commit f736e0f1a55a88cb258b73da77463573739e9ac9
Merge: e2f3ccfa6200 6be7436d2245 e5a971d76d70 33b2b93bd831 3c80b4024579
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Thu May 7 10:18:32 2020 -0700

    Merge branches 'fixes.2020.04.27a', 'kfree_rcu.2020.04.27a', 'rcu-tasks.2020.04.27a', 'stall.2020.04.27a' and 'torture.2020.05.07a' into HEAD
    
    fixes.2020.04.27a:  Miscellaneous fixes.
    kfree_rcu.2020.04.27a:  Changes related to kfree_rcu().
    rcu-tasks.2020.04.27a:  Addition of new RCU-tasks flavors.
    stall.2020.04.27a:  RCU CPU stall-warning updates.
    torture.2020.05.07a:  Torture-test updates.

commit 7d0c9c50c5a109acd7a5cf589fc5563f9ef7149a
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Thu Mar 19 15:33:12 2020 -0700

    rcu-tasks: Avoid IPIing userspace/idle tasks if kernel is so built
    
    Systems running CPU-bound real-time task do not want IPIs sent to CPUs
    executing nohz_full userspace tasks.  Battery-powered systems don't
    want IPIs sent to idle CPUs in low-power mode.  Unfortunately, RCU tasks
    trace can and will send such IPIs in some cases.
    
    Both of these situations occur only when the target CPU is in RCU
    dyntick-idle mode, in other words, when RCU is not watching the
    target CPU.  This suggests that CPUs in dyntick-idle mode should use
    memory barriers in outermost invocations of rcu_read_lock_trace()
    and rcu_read_unlock_trace(), which would allow the RCU tasks trace
    grace period to directly read out the target CPU's read-side state.
    One challenge is that RCU tasks trace is not targeting a specific
    CPU, but rather a task.  And that task could switch from one CPU to
    another at any time.
    
    This commit therefore uses try_invoke_on_locked_down_task()
    and checks for task_curr() in trc_inspect_reader_notrunning().
    When this condition holds, the target task is running and cannot move.
    If CONFIG_TASKS_TRACE_RCU_READ_MB=y, the new rcu_dynticks_zero_in_eqs()
    function can be used to check if the specified integer (in this case,
    t->trc_reader_nesting) is zero while the target CPU remains in that same
    dyntick-idle sojourn.  If so, the target task is in a quiescent state.
    If not, trc_read_check_handler() must indicate failure so that the
    grace-period kthread can take appropriate action or retry after an
    appropriate delay, as the case may be.
    
    With this change, given CONFIG_TASKS_TRACE_RCU_READ_MB=y, if a given
    CPU remains idle or a given task continues executing in nohz_full mode,
    the RCU tasks trace grace-period kthread will detect this without the
    need to send an IPI.
    
    Suggested-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 9dc2ec021da5..29ba79989802 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -454,6 +454,8 @@ static void rcu_bind_gp_kthread(void);
 static bool rcu_nohz_full_cpu(void);
 static void rcu_dynticks_task_enter(void);
 static void rcu_dynticks_task_exit(void);
+static void rcu_dynticks_task_trace_enter(void);
+static void rcu_dynticks_task_trace_exit(void);
 
 /* Forward declarations for tree_stall.h */
 static void record_gp_stall_check_time(void);

commit 1fca4d12f46371a34bf90af87f49548dd026c3ca
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Sat Feb 22 20:07:09 2020 -0800

    rcu: Expedite first two FQS scans under callback-overload conditions
    
    Even if some CPUs have excessive numbers of callbacks, RCU's grace-period
    kthread will still wait normally between successive force-quiescent-state
    scans.  The first two are the most important, as they are the ones that
    enlist aid from the scheduler when overloaded.  This commit therefore
    omits the wait before the first and the second force-quiescent-state
    scan under callback-overload conditions.
    
    This approach was inspired by a discussion with Jeff Roberson.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 9dc2ec021da5..44edd0a98ffe 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -359,6 +359,7 @@ struct rcu_state {
 /* Values for rcu_state structure's gp_flags field. */
 #define RCU_GP_FLAG_INIT 0x1	/* Need grace-period initialization. */
 #define RCU_GP_FLAG_FQS  0x2	/* Need grace-period quiescent-state forcing. */
+#define RCU_GP_FLAG_OVLD 0x4	/* Experiencing callback overload. */
 
 /* Values for rcu_state structure's gp_state field. */
 #define RCU_GP_IDLE	 0	/* Initial state and no GP in progress. */

commit b2b00ddf193bf83dc561d965c67b18eb54ebcd83
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Wed Oct 30 11:56:10 2019 -0700

    rcu: React to callback overload by aggressively seeking quiescent states
    
    In default configutions, RCU currently waits at least 100 milliseconds
    before asking cond_resched() and/or resched_rcu() for help seeking
    quiescent states to end a grace period.  But 100 milliseconds can be
    one good long time during an RCU callback flood, for example, as can
    happen when user processes repeatedly open and close files in a tight
    loop.  These 100-millisecond gaps in successive grace periods during a
    callback flood can result in excessive numbers of callbacks piling up,
    unnecessarily increasing memory footprint.
    
    This commit therefore asks cond_resched() and/or resched_rcu() for help
    as early as the first FQS scan when at least one of the CPUs has more
    than 20,000 callbacks queued, a number that can be changed using the new
    rcutree.qovld kernel boot parameter.  An auxiliary qovld_calc variable
    is used to avoid acquisition of locks that have not yet been initialized.
    Early tests indicate that this reduces the RCU-callback memory footprint
    during rcutorture floods by from 50% to 4x, depending on configuration.
    
    Reported-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Reported-by: Tejun Heo <tj@kernel.org>
    [ paulmck: Fix bug located by Qian Cai. ]
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    Tested-by: Dexuan Cui <decui@microsoft.com>
    Tested-by: Qian Cai <cai@lca.pw>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 0c87e4c161c2..9dc2ec021da5 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -68,6 +68,8 @@ struct rcu_node {
 				/* Online CPUs for next expedited GP. */
 				/*  Any CPU that has ever been online will */
 				/*  have its bit set. */
+	unsigned long cbovldmask;
+				/* CPUs experiencing callback overload. */
 	unsigned long ffmask;	/* Fully functional CPUs. */
 	unsigned long grpmask;	/* Mask to apply to parent qsmask. */
 				/*  Only one bit will be set in this mask. */
@@ -321,6 +323,8 @@ struct rcu_state {
 	atomic_t expedited_need_qs;		/* # CPUs left to check in. */
 	struct swait_queue_head expedited_wq;	/* Wait for check-ins. */
 	int ncpus_snap;				/* # CPUs seen last time. */
+	u8 cbovld;				/* Callback overload now? */
+	u8 cbovldnext;				/* ^        ^  next time? */
 
 	unsigned long jiffies_force_qs;		/* Time at which to invoke */
 						/*  force_quiescent_state(). */

commit 0e247386d9ed5ab8b7dad010cf4b183efeb1e47d
Merge: 6e6eca2ee79a df1e849ae455 f6105fc2a9c0 189a6883dcf7 afa47fdfa29f 5b14557b073c b22eb7cefb9d
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Fri Jan 24 10:37:27 2020 -0800

    Merge branches 'doc.2019.12.10a', 'exp.2019.12.09a', 'fixes.2020.01.24a', 'kfree_rcu.2020.01.24a', 'list.2020.01.10a', 'preempt.2020.01.24a' and 'torture.2019.12.09a' into HEAD
    
    doc.2019.12.10a: Documentations updates
    exp.2019.12.09a: Expedited grace-period updates
    fixes.2020.01.24a: Miscellaneous fixes
    kfree_rcu.2020.01.24a: Batch kfree_rcu() work
    list.2020.01.10a: RCU-protected-list updates
    preempt.2020.01.24a: Preemptible RCU updates
    torture.2019.12.09a: Torture-test updates

commit f6105fc2a9c0ea5be6b9e5c19b2551af0b8b4eac
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Wed Nov 27 11:36:07 2019 -0800

    rcu: Remove unused stop-machine #include
    
    Long ago, RCU used the stop-machine mechanism to implement expedited
    grace periods, but no longer does so.  This commit therefore removes
    the no-longer-needed #includes of linux/stop_machine.h.
    
    Link: https://lwn.net/Articles/805317/
    Reported-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 9d5986abfc67..ce90c68c184b 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -16,7 +16,6 @@
 #include <linux/cpumask.h>
 #include <linux/seqlock.h>
 #include <linux/swait.h>
-#include <linux/stop_machine.h>
 #include <linux/rcu_node_tree.h>
 
 #include "rcu_segcblist.h"

commit e2167b38c87a0c9e85c342a823dae1e6f67b11d9
Author: Lai Jiangshan <jiangshanlai@gmail.com>
Date:   Tue Oct 15 10:28:47 2019 +0000

    rcu: Move gp_state_names[] and gp_state_getname() to tree_stall.h
    
    Only tree_stall.h needs to get name from GP state, so this commit
    moves the gp_state_names[] array and the gp_state_getname()
    from kernel/rcu/tree.h and kernel/rcu/tree.c, respectively, to
    kernel/rcu/tree_stall.h.  While moving gp_state_names[], this commit
    uses the GCC syntax to ensure that the right string is associated with
    the right CPP macro.
    
    Signed-off-by: Lai Jiangshan <jiangshanlai@gmail.com>
    Signed-off-by: Lai Jiangshan <laijs@linux.alibaba.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 54ff9896ae31..9d5986abfc67 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -368,18 +368,6 @@ struct rcu_state {
 #define RCU_GP_CLEANUP   7	/* Grace-period cleanup started. */
 #define RCU_GP_CLEANED   8	/* Grace-period cleanup complete. */
 
-static const char * const gp_state_names[] = {
-	"RCU_GP_IDLE",
-	"RCU_GP_WAIT_GPS",
-	"RCU_GP_DONE_GPS",
-	"RCU_GP_ONOFF",
-	"RCU_GP_INIT",
-	"RCU_GP_WAIT_FQS",
-	"RCU_GP_DOING_FQS",
-	"RCU_GP_CLEANUP",
-	"RCU_GP_CLEANED",
-};
-
 /*
  * In order to export the rcu_state name to the tracing tools, it
  * needs to be added in the __tracepoint_string section.

commit 4778339df0ee361dbf2cbdcb87abaec9dfbc841d
Author: Lai Jiangshan <jiangshanlai@gmail.com>
Date:   Tue Oct 15 10:28:46 2019 +0000

    rcu: Remove the declaration of call_rcu() in tree.h
    
    The call_rcu() function is an external RCU API that is declared in
    include/linux/rcupdate.h.  There is thus no point in redeclaring it
    in kernel/rcu/tree.h, so this commit removes that redundant declaration.
    
    Signed-off-by: Lai Jiangshan <jiangshanlai@gmail.com>
    Signed-off-by: Lai Jiangshan <laijs@linux.alibaba.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index e4dc5debfc84..54ff9896ae31 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -413,7 +413,6 @@ static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 static int rcu_print_task_exp_stall(struct rcu_node *rnp);
 static void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp);
 static void rcu_flavor_sched_clock_irq(int user);
-void call_rcu(struct rcu_head *head, rcu_callback_t func);
 static void dump_blkd_tasks(struct rcu_node *rnp, int ncheck);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
 static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);

commit 77a40f97030b27b3fc1640a3ed203870f0817f57
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Fri Aug 30 12:36:32 2019 -0400

    rcu: Remove kfree_rcu() special casing and lazy-callback handling
    
    This commit removes kfree_rcu() special-casing and the lazy-callback
    handling from Tree RCU.  It moves some of this special casing to Tiny RCU,
    the removal of which will be the subject of later commits.
    
    This results in a nice negative delta.
    
    Suggested-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    [ paulmck: Add slab.h #include, thanks to kbuild test robot <lkp@intel.com>. ]
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 055c31781d3a..15405420b40c 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -183,7 +183,6 @@ struct rcu_data {
 	bool rcu_urgent_qs;		/* GP old need light quiescent state. */
 	bool rcu_forced_tick;		/* Forced tick to provide QS. */
 #ifdef CONFIG_RCU_FAST_NO_HZ
-	bool all_lazy;			/* All CPU's CBs lazy at idle start? */
 	unsigned long last_accelerate;	/* Last jiffy CBs were accelerated. */
 	unsigned long last_advance_all;	/* Last jiffy CBs were all advanced. */
 	int tick_nohz_enabled_snap;	/* Previously seen value from sysfs. */

commit c30fe541896440667bd9e9068aedd1d440fbbcd2
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Fri Oct 11 21:40:09 2019 -0700

    rcu: Mark non-global functions and variables as static
    
    Each of rcu_state, rcu_rnp_online_cpus(), rcu_dynticks_curr_cpu_in_eqs(),
    and rcu_dynticks_snap() are used only in the kernel/rcu/tree.o translation
    unit, and may thus be marked static.  This commit therefore makes this
    change.
    
    Reported-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 055c31781d3a..e4dc5debfc84 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -403,8 +403,6 @@ static const char *tp_rcu_varname __used __tracepoint_string = rcu_name;
 #define RCU_NAME rcu_name
 #endif /* #else #ifdef CONFIG_TRACING */
 
-int rcu_dynticks_snap(struct rcu_data *rdp);
-
 /* Forward declarations for tree_plugin.h */
 static void rcu_bootup_announce(void);
 static void rcu_qs(void);

commit df1e849ae4559544ff00ff5052eefe2479750539
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Wed Nov 27 16:36:45 2019 -0800

    rcu: Enable tick for nohz_full CPUs slow to provide expedited QS
    
    An expedited grace period can be stalled by a nohz_full CPU looping
    in kernel context.  This possibility is currently handled by some
    carefully crafted checks in rcu_read_unlock_special() that enlist help
    from ksoftirqd when permitted by the scheduler.  However, it is exactly
    these checks that require the scheduler avoid holding any of its rq or
    pi locks across rcu_read_unlock() without also having held them across
    the entire RCU read-side critical section.
    
    It would therefore be very nice if expedited grace periods could
    handle nohz_full CPUs looping in kernel context without such checks.
    This commit therefore adds code to the expedited grace period's wait
    and cleanup code that forces the scheduler-clock interrupt on for CPUs
    that fail to quickly supply a quiescent state.  "Quickly" is currently
    a hard-coded single-jiffy delay.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 055c31781d3a..f9253ed406ba 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -182,6 +182,7 @@ struct rcu_data {
 	bool rcu_need_heavy_qs;		/* GP old, so heavy quiescent state! */
 	bool rcu_urgent_qs;		/* GP old need light quiescent state. */
 	bool rcu_forced_tick;		/* Forced tick to provide QS. */
+	bool rcu_forced_tick_exp;	/*   ... provide QS to expedited GP. */
 #ifdef CONFIG_RCU_FAST_NO_HZ
 	bool all_lazy;			/* All CPU's CBs lazy at idle start? */
 	unsigned long last_accelerate;	/* Last jiffy CBs were accelerated. */

commit 66e4c33b51bc515ca803c0948cf1525b53ffd631
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Mon Aug 12 16:14:00 2019 -0700

    rcu: Force tick on for nohz_full CPUs not reaching quiescent states
    
    CPUs running for long time periods in the kernel in nohz_full mode
    might leave the scheduling-clock interrupt disabled for then full
    duration of their in-kernel execution.  This can (among other things)
    delay grace periods.  This commit therefore forces the tick back on
    for any nohz_full CPU that is failing to pass through a quiescent state
    upon return from interrupt, which the resched_cpu() will induce.
    
    Reported-by: Joel Fernandes <joel@joelfernandes.org>
    [ paulmck: Clear ->rcu_forced_tick as reported by Joel Fernandes testing. ]
    [ paulmck: Apply Joel Fernandes TICK_DEP_MASK_RCU->TICK_DEP_BIT_RCU fix. ]
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index c612f306fe89..055c31781d3a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -181,6 +181,7 @@ struct rcu_data {
 	atomic_t dynticks;		/* Even value for idle, else odd. */
 	bool rcu_need_heavy_qs;		/* GP old, so heavy quiescent state! */
 	bool rcu_urgent_qs;		/* GP old need light quiescent state. */
+	bool rcu_forced_tick;		/* Forced tick to provide QS. */
 #ifdef CONFIG_RCU_FAST_NO_HZ
 	bool all_lazy;			/* All CPU's CBs lazy at idle start? */
 	unsigned long last_accelerate;	/* Last jiffy CBs were accelerated. */

commit f7a81b12d6af42a9d09be1e5f041169f04b0b67a
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue Jun 25 13:32:51 2019 -0700

    rcu/nocb: Print no-CBs diagnostics when rcutorture writer unduly delayed
    
    This commit causes locking, sleeping, and callback state to be printed
    for no-CBs CPUs when the rcutorture writer is delayed sufficiently for
    rcutorture to complain.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index e4df86db8137..c612f306fe89 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -212,7 +212,11 @@ struct rcu_data {
 	/* The following fields are used by GP kthread, hence own cacheline. */
 	raw_spinlock_t nocb_gp_lock ____cacheline_internodealigned_in_smp;
 	struct timer_list nocb_bypass_timer; /* Force nocb_bypass flush. */
-	bool nocb_gp_sleep;		/* Is the nocb GP thread asleep? */
+	u8 nocb_gp_sleep;		/* Is the nocb GP thread asleep? */
+	u8 nocb_gp_bypass;		/* Found a bypass on last scan? */
+	u8 nocb_gp_gp;			/* GP to wait for on last scan? */
+	unsigned long nocb_gp_seq;	/*  If so, ->gp_seq to wait for. */
+	unsigned long nocb_gp_loops;	/* # passes through wait code. */
 	struct swait_queue_head nocb_gp_wq; /* For nocb kthreads to sleep on. */
 	bool nocb_cb_sleep;		/* Is the nocb CB thread asleep? */
 	struct task_struct *nocb_cb_kthread;
@@ -438,6 +442,7 @@ static void do_nocb_deferred_wakeup(struct rcu_data *rdp);
 static void rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp);
 static void rcu_spawn_cpu_nocb_kthread(int cpu);
 static void __init rcu_spawn_nocb_kthreads(void);
+static void show_rcu_nocb_state(struct rcu_data *rdp);
 static void rcu_nocb_lock(struct rcu_data *rdp);
 static void rcu_nocb_unlock(struct rcu_data *rdp);
 static void rcu_nocb_unlock_irqrestore(struct rcu_data *rdp,

commit d1b222c6be1f8bfc77099e034219732ecaeaaf96
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue Jul 2 16:03:33 2019 -0700

    rcu/nocb: Add bypass callback queueing
    
    Use of the rcu_data structure's segmented ->cblist for no-CBs CPUs
    takes advantage of unrelated grace periods, thus reducing the memory
    footprint in the face of floods of call_rcu() invocations.  However,
    the ->cblist field is a more-complex rcu_segcblist structure which must
    be protected via locking.  Even though there are only three entities
    which can acquire this lock (the CPU invoking call_rcu(), the no-CBs
    grace-period kthread, and the no-CBs callbacks kthread), the contention
    on this lock is excessive under heavy stress.
    
    This commit therefore greatly reduces contention by provisioning
    an rcu_cblist structure field named ->nocb_bypass within the
    rcu_data structure.  Each no-CBs CPU is permitted only a limited
    number of enqueues onto the ->cblist per jiffy, controlled by a new
    nocb_nobypass_lim_per_jiffy kernel boot parameter that defaults to
    about 16 enqueues per millisecond (16 * 1000 / HZ).  When that limit is
    exceeded, the CPU instead enqueues onto the new ->nocb_bypass.
    
    The ->nocb_bypass is flushed into the ->cblist every jiffy or when
    the number of callbacks on ->nocb_bypass exceeds qhimark, whichever
    happens first.  During call_rcu() floods, this flushing is carried out
    by the CPU during the course of its call_rcu() invocations.  However,
    a CPU could simply stop invoking call_rcu() at any time.  The no-CBs
    grace-period kthread therefore carries out less-aggressive flushing
    (every few jiffies or when the number of callbacks on ->nocb_bypass
    exceeds (2 * qhimark), whichever comes first).  This means that the
    no-CBs grace-period kthread cannot be permitted to do unbounded waits
    while there are callbacks on ->nocb_bypass.  A ->nocb_bypass_timer is
    used to provide the needed wakeups.
    
    [ paulmck: Apply Coverity feedback reported by Colin Ian King. ]
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 2c3e9068671c..e4df86db8137 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -200,18 +200,26 @@ struct rcu_data {
 	atomic_t nocb_lock_contended;	/* Contention experienced. */
 	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
 	struct timer_list nocb_timer;	/* Enforce finite deferral. */
+	unsigned long nocb_gp_adv_time;	/* Last call_rcu() CB adv (jiffies). */
+
+	/* The following fields are used by call_rcu, hence own cacheline. */
+	raw_spinlock_t nocb_bypass_lock ____cacheline_internodealigned_in_smp;
+	struct rcu_cblist nocb_bypass;	/* Lock-contention-bypass CB list. */
+	unsigned long nocb_bypass_first; /* Time (jiffies) of first enqueue. */
+	unsigned long nocb_nobypass_last; /* Last ->cblist enqueue (jiffies). */
+	int nocb_nobypass_count;	/* # ->cblist enqueues at ^^^ time. */
 
 	/* The following fields are used by GP kthread, hence own cacheline. */
 	raw_spinlock_t nocb_gp_lock ____cacheline_internodealigned_in_smp;
-	bool nocb_gp_sleep;
-					/* Is the nocb GP thread asleep? */
+	struct timer_list nocb_bypass_timer; /* Force nocb_bypass flush. */
+	bool nocb_gp_sleep;		/* Is the nocb GP thread asleep? */
 	struct swait_queue_head nocb_gp_wq; /* For nocb kthreads to sleep on. */
 	bool nocb_cb_sleep;		/* Is the nocb CB thread asleep? */
 	struct task_struct *nocb_cb_kthread;
 	struct rcu_data *nocb_next_cb_rdp;
 					/* Next rcu_data in wakeup chain. */
 
-	/* The following fields are used by CB kthread, hence new cachline. */
+	/* The following fields are used by CB kthread, hence new cacheline. */
 	struct rcu_data *nocb_gp_rdp ____cacheline_internodealigned_in_smp;
 					/* GP rdp takes GP-end wakeups. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
@@ -419,6 +427,10 @@ static void zero_cpu_stall_ticks(struct rcu_data *rdp);
 static struct swait_queue_head *rcu_nocb_gp_get(struct rcu_node *rnp);
 static void rcu_nocb_gp_cleanup(struct swait_queue_head *sq);
 static void rcu_init_one_nocb(struct rcu_node *rnp);
+static bool rcu_nocb_flush_bypass(struct rcu_data *rdp, struct rcu_head *rhp,
+				  unsigned long j);
+static bool rcu_nocb_try_bypass(struct rcu_data *rdp, struct rcu_head *rhp,
+				bool *was_alldone, unsigned long flags);
 static void __call_rcu_nocb_wake(struct rcu_data *rdp, bool was_empty,
 				 unsigned long flags);
 static int rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp);
@@ -430,19 +442,15 @@ static void rcu_nocb_lock(struct rcu_data *rdp);
 static void rcu_nocb_unlock(struct rcu_data *rdp);
 static void rcu_nocb_unlock_irqrestore(struct rcu_data *rdp,
 				       unsigned long flags);
+static void rcu_lockdep_assert_cblist_protected(struct rcu_data *rdp);
 #ifdef CONFIG_RCU_NOCB_CPU
 static void __init rcu_organize_nocb_kthreads(void);
 #define rcu_nocb_lock_irqsave(rdp, flags)				\
 do {									\
-	if (!rcu_segcblist_is_offloaded(&(rdp)->cblist)) {		\
+	if (!rcu_segcblist_is_offloaded(&(rdp)->cblist))		\
 		local_irq_save(flags);					\
-	} else if (!raw_spin_trylock_irqsave(&(rdp)->nocb_lock, (flags))) {\
-		atomic_inc(&(rdp)->nocb_lock_contended);		\
-		smp_mb__after_atomic(); /* atomic_inc() before lock. */	\
+	else								\
 		raw_spin_lock_irqsave(&(rdp)->nocb_lock, (flags));	\
-		smp_mb__before_atomic(); /* atomic_dec() after lock. */	\
-		atomic_dec(&(rdp)->nocb_lock_contended);		\
-	}								\
 } while (0)
 #else /* #ifdef CONFIG_RCU_NOCB_CPU */
 #define rcu_nocb_lock_irqsave(rdp, flags) local_irq_save(flags)

commit 4fd8c5f153bc41ae847b9ddb1539b34f70c18278
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Sun Jun 2 13:41:08 2019 -0700

    rcu/nocb: Reduce ->nocb_lock contention with separate ->nocb_gp_lock
    
    The sleep/wakeup of the no-CBs grace-period kthreads is synchronized
    using the ->nocb_lock of the first CPU corresponding to that kthread.
    This commit provides a separate ->nocb_gp_lock for this purpose, thus
    reducing contention on ->nocb_lock.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7062f9d9c053..2c3e9068671c 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -202,7 +202,8 @@ struct rcu_data {
 	struct timer_list nocb_timer;	/* Enforce finite deferral. */
 
 	/* The following fields are used by GP kthread, hence own cacheline. */
-	bool nocb_gp_sleep ____cacheline_internodealigned_in_smp;
+	raw_spinlock_t nocb_gp_lock ____cacheline_internodealigned_in_smp;
+	bool nocb_gp_sleep;
 					/* Is the nocb GP thread asleep? */
 	struct swait_queue_head nocb_gp_wq; /* For nocb kthreads to sleep on. */
 	bool nocb_cb_sleep;		/* Is the nocb CB thread asleep? */

commit 81c0b3d724f419c0524f432c1ac22b9f518c2899
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue May 28 07:18:08 2019 -0700

    rcu/nocb: Avoid ->nocb_lock capture by corresponding CPU
    
    A given rcu_data structure's ->nocb_lock can be acquired very frequently
    by the corresponding CPU and occasionally by the corresponding no-CBs
    grace-period and callbacks kthreads.  In particular, these two kthreads
    will have frequent gaps between ->nocb_lock acquisitions that are roughly
    a grace period in duration.  This means that any excessive ->nocb_lock
    contention will be due to the CPU's acquisitions, and this in turn
    enables a very naive contention-avoidance strategy to be quite effective.
    
    This commit therefore modifies rcu_nocb_lock() to first
    attempt a raw_spin_trylock(), and to atomically increment a
    separate ->nocb_lock_contended across a raw_spin_lock().  This new
    ->nocb_lock_contended field is checked in __call_rcu_nocb_wake() when
    interrupts are enabled, with a spin-wait for contending acquisitions
    to complete, thus allowing the kthreads a chance to acquire the lock.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index c12e85c12310..7062f9d9c053 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -197,6 +197,7 @@ struct rcu_data {
 	struct swait_queue_head nocb_cb_wq; /* For nocb kthreads to sleep on. */
 	struct task_struct *nocb_gp_kthread;
 	raw_spinlock_t nocb_lock;	/* Guard following pair of fields. */
+	atomic_t nocb_lock_contended;	/* Contention experienced. */
 	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
 	struct timer_list nocb_timer;	/* Enforce finite deferral. */
 
@@ -430,7 +431,22 @@ static void rcu_nocb_unlock_irqrestore(struct rcu_data *rdp,
 				       unsigned long flags);
 #ifdef CONFIG_RCU_NOCB_CPU
 static void __init rcu_organize_nocb_kthreads(void);
-#endif /* #ifdef CONFIG_RCU_NOCB_CPU */
+#define rcu_nocb_lock_irqsave(rdp, flags)				\
+do {									\
+	if (!rcu_segcblist_is_offloaded(&(rdp)->cblist)) {		\
+		local_irq_save(flags);					\
+	} else if (!raw_spin_trylock_irqsave(&(rdp)->nocb_lock, (flags))) {\
+		atomic_inc(&(rdp)->nocb_lock_contended);		\
+		smp_mb__after_atomic(); /* atomic_inc() before lock. */	\
+		raw_spin_lock_irqsave(&(rdp)->nocb_lock, (flags));	\
+		smp_mb__before_atomic(); /* atomic_dec() after lock. */	\
+		atomic_dec(&(rdp)->nocb_lock_contended);		\
+	}								\
+} while (0)
+#else /* #ifdef CONFIG_RCU_NOCB_CPU */
+#define rcu_nocb_lock_irqsave(rdp, flags) local_irq_save(flags)
+#endif /* #else #ifdef CONFIG_RCU_NOCB_CPU */
+
 static void rcu_bind_gp_kthread(void);
 static bool rcu_nohz_full_cpu(void);
 static void rcu_dynticks_task_enter(void);

commit 4f9c1bc727f917c8c32ee1decc88e89057e0dffc
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue May 21 09:20:10 2019 -0700

    rcu/nocb: Remove obsolete nocb_gp_head and nocb_gp_tail fields
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6e4cf7de303f..c12e85c12310 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -201,10 +201,8 @@ struct rcu_data {
 	struct timer_list nocb_timer;	/* Enforce finite deferral. */
 
 	/* The following fields are used by GP kthread, hence own cacheline. */
-	struct rcu_head *nocb_gp_head ____cacheline_internodealigned_in_smp;
-					/* CBs waiting for GP. */
-	struct rcu_head **nocb_gp_tail;
-	bool nocb_gp_sleep;		/* Is the nocb GP thread asleep? */
+	bool nocb_gp_sleep ____cacheline_internodealigned_in_smp;
+					/* Is the nocb GP thread asleep? */
 	struct swait_queue_head nocb_gp_wq; /* For nocb kthreads to sleep on. */
 	bool nocb_cb_sleep;		/* Is the nocb CB thread asleep? */
 	struct task_struct *nocb_cb_kthread;

commit 2a777de757f4c7050997c6232a585eff59c5ea36
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue May 21 09:10:24 2019 -0700

    rcu/nocb: Remove obsolete nocb_cb_tail and nocb_cb_head fields
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d1df192272fb..6e4cf7de303f 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -194,8 +194,6 @@ struct rcu_data {
 
 	/* 5) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU
-	struct rcu_head *nocb_cb_head;	/* CBs ready to invoke. */
-	struct rcu_head **nocb_cb_tail;
 	struct swait_queue_head nocb_cb_wq; /* For nocb kthreads to sleep on. */
 	struct task_struct *nocb_gp_kthread;
 	raw_spinlock_t nocb_lock;	/* Guard following pair of fields. */

commit c035280f1761b3336f4dad336906c19735d7ba5f
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue May 21 08:28:41 2019 -0700

    rcu/nocb: Remove obsolete nocb_q_count and nocb_q_count_lazy fields
    
    This commit removes the obsolete nocb_q_count and nocb_q_count_lazy
    fields, also removing rcu_get_n_cbs_nocb_cpu(), adjusting
    rcu_get_n_cbs_cpu(), and making rcutree_migrate_callbacks() once again
    disable the ->cblist fields of offline CPUs.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 74e3a4ab8095..d1df192272fb 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -194,8 +194,6 @@ struct rcu_data {
 
 	/* 5) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU
-	atomic_long_t nocb_q_count;	/* # CBs waiting for nocb */
-	atomic_long_t nocb_q_count_lazy; /*  invocation (all stages). */
 	struct rcu_head *nocb_cb_head;	/* CBs ready to invoke. */
 	struct rcu_head **nocb_cb_tail;
 	struct swait_queue_head nocb_cb_wq; /* For nocb kthreads to sleep on. */
@@ -437,7 +435,6 @@ static void rcu_nocb_unlock_irqrestore(struct rcu_data *rdp,
 #ifdef CONFIG_RCU_NOCB_CPU
 static void __init rcu_organize_nocb_kthreads(void);
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
-static unsigned long rcu_get_n_cbs_nocb_cpu(struct rcu_data *rdp);
 static void rcu_bind_gp_kthread(void);
 static bool rcu_nohz_full_cpu(void);
 static void rcu_dynticks_task_enter(void);

commit e7f4c5b3998a3cf1bd8dbf110948075b47ac9b78
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue May 21 07:18:00 2019 -0700

    rcu/nocb: Remove obsolete nocb_head and nocb_tail fields
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 529eec2aa74d..74e3a4ab8095 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -194,8 +194,6 @@ struct rcu_data {
 
 	/* 5) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU
-	struct rcu_head *nocb_head;	/* CBs waiting for kthread. */
-	struct rcu_head **nocb_tail;
 	atomic_long_t nocb_q_count;	/* # CBs waiting for nocb */
 	atomic_long_t nocb_q_count_lazy; /*  invocation (all stages). */
 	struct rcu_head *nocb_cb_head;	/* CBs ready to invoke. */
@@ -211,7 +209,6 @@ struct rcu_data {
 					/* CBs waiting for GP. */
 	struct rcu_head **nocb_gp_tail;
 	bool nocb_gp_sleep;		/* Is the nocb GP thread asleep? */
-	bool nocb_gp_forced;		/* Forced nocb GP thread wakeup? */
 	struct swait_queue_head nocb_gp_wq; /* For nocb kthreads to sleep on. */
 	bool nocb_cb_sleep;		/* Is the nocb CB thread asleep? */
 	struct task_struct *nocb_cb_kthread;

commit 5d6742b37727e111f4755155e59c5319cf5caa7b
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Wed May 15 09:56:40 2019 -0700

    rcu/nocb: Use rcu_segcblist for no-CBs CPUs
    
    Currently the RCU callbacks for no-CBs CPUs are queued on a series of
    ad-hoc linked lists, which means that these callbacks cannot benefit
    from "drive-by" grace periods, thus suffering needless delays prior
    to invocation.  In addition, the no-CBs grace-period kthreads first
    wait for callbacks to appear and later wait for a new grace period,
    which means that callbacks appearing during a grace-period wait can
    be delayed.  These delays increase memory footprint, and could even
    result in an out-of-memory condition.
    
    This commit therefore enqueues RCU callbacks from no-CBs CPUs on the
    rcu_segcblist structure that is already used by non-no-CBs CPUs.  It also
    restructures the no-CBs grace-period kthread to be checking for incoming
    callbacks while waiting for grace periods.  Also, instead of waiting
    for a new grace period, it waits for the closest grace period that will
    cause some of the callbacks to be safe to invoke.  All of these changes
    reduce callback latency and thus the number of outstanding callbacks,
    in turn reducing the probability of an out-of-memory condition.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8d9cfcac6757..529eec2aa74d 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -211,7 +211,9 @@ struct rcu_data {
 					/* CBs waiting for GP. */
 	struct rcu_head **nocb_gp_tail;
 	bool nocb_gp_sleep;		/* Is the nocb GP thread asleep? */
+	bool nocb_gp_forced;		/* Forced nocb GP thread wakeup? */
 	struct swait_queue_head nocb_gp_wq; /* For nocb kthreads to sleep on. */
+	bool nocb_cb_sleep;		/* Is the nocb CB thread asleep? */
 	struct task_struct *nocb_cb_kthread;
 	struct rcu_data *nocb_next_cb_rdp;
 					/* Next rcu_data in wakeup chain. */
@@ -421,20 +423,20 @@ static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 static bool rcu_preempt_need_deferred_qs(struct task_struct *t);
 static void rcu_preempt_deferred_qs(struct task_struct *t);
 static void zero_cpu_stall_ticks(struct rcu_data *rdp);
-static bool rcu_nocb_cpu_needs_barrier(int cpu);
 static struct swait_queue_head *rcu_nocb_gp_get(struct rcu_node *rnp);
 static void rcu_nocb_gp_cleanup(struct swait_queue_head *sq);
 static void rcu_init_one_nocb(struct rcu_node *rnp);
-static bool __call_rcu_nocb(struct rcu_data *rdp, struct rcu_head *rhp,
-			    bool lazy, unsigned long flags);
-static bool rcu_nocb_adopt_orphan_cbs(struct rcu_data *my_rdp,
-				      struct rcu_data *rdp,
-				      unsigned long flags);
+static void __call_rcu_nocb_wake(struct rcu_data *rdp, bool was_empty,
+				 unsigned long flags);
 static int rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp);
 static void do_nocb_deferred_wakeup(struct rcu_data *rdp);
 static void rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp);
 static void rcu_spawn_cpu_nocb_kthread(int cpu);
 static void __init rcu_spawn_nocb_kthreads(void);
+static void rcu_nocb_lock(struct rcu_data *rdp);
+static void rcu_nocb_unlock(struct rcu_data *rdp);
+static void rcu_nocb_unlock_irqrestore(struct rcu_data *rdp,
+				       unsigned long flags);
 #ifdef CONFIG_RCU_NOCB_CPU
 static void __init rcu_organize_nocb_kthreads(void);
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */

commit e83e73f5b0f8de6a8978ba64185e80fdf48a2a63
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue May 14 09:50:49 2019 -0700

    rcu/nocb: Leave ->cblist enabled for no-CBs CPUs
    
    As a first step towards making no-CBs CPUs use the ->cblist, this commit
    leaves the ->cblist enabled for these CPUs.  The main reason to make
    no-CBs CPUs use ->cblist is to take advantage of callback numbering,
    which will reduce the effects of missed grace periods which in turn will
    reduce forward-progress problems for no-CBs CPUs.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index dc3c53cb9608..8d9cfcac6757 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -438,7 +438,6 @@ static void __init rcu_spawn_nocb_kthreads(void);
 #ifdef CONFIG_RCU_NOCB_CPU
 static void __init rcu_organize_nocb_kthreads(void);
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
-static bool init_nocb_callback_list(struct rcu_data *rdp);
 static unsigned long rcu_get_n_cbs_nocb_cpu(struct rcu_data *rdp);
 static void rcu_bind_gp_kthread(void);
 static bool rcu_nohz_full_cpu(void);

commit 12f54c3a8410102afb96ed437aebe7f1d87f399f
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Mar 29 16:43:51 2019 -0700

    rcu/nocb: Provide separate no-CBs grace-period kthreads
    
    Currently, there is one no-CBs rcuo kthread per CPU, and these kthreads
    are divided into groups.  The first rcuo kthread to come online in a
    given group is that group's leader, and the leader both waits for grace
    periods and invokes its CPU's callbacks.  The non-leader rcuo kthreads
    only invoke callbacks.
    
    This works well in the real-time/embedded environments for which it was
    intended because such environments tend not to generate all that many
    callbacks.  However, given huge floods of callbacks, it is possible for
    the leader kthread to be stuck invoking callbacks while its followers
    wait helplessly while their callbacks pile up.  This is a good recipe
    for an OOM, and rcutorture's new callback-flood capability does generate
    such OOMs.
    
    One strategy would be to wait until such OOMs start happening in
    production, but similar OOMs have in fact happened starting in 2018.
    It would therefore be wise to take a more proactive approach.
    
    This commit therefore features per-CPU rcuo kthreads that do nothing
    but invoke callbacks.  Instead of having one of these kthreads act as
    leader, each group has a separate rcog kthread that handles grace periods
    for its group.  Because these rcuog kthreads do not invoke callbacks,
    callback floods on one CPU no longer block callbacks from reaching the
    rcuc callback-invocation kthreads on other CPUs.
    
    This change does introduce additional kthreads, however:
    
    1.      The number of additional kthreads is about the square root of
            the number of CPUs, so that a 4096-CPU system would have only
            about 64 additional kthreads.  Note that recent changes
            decreased the number of rcuo kthreads by a factor of two
            (CONFIG_PREEMPT=n) or even three (CONFIG_PREEMPT=y), so
            this still represents a significant improvement on most systems.
    
    2.      The leading "rcuo" of the rcuog kthreads should allow existing
            scripting to affinity these additional kthreads as needed, the
            same as for the rcuop and rcuos kthreads.  (There are no longer
            any rcuob kthreads.)
    
    3.      A state-machine approach was considered and rejected.  Although
            this would allow the rcuo kthreads to continue their dual
            leader/follower roles, it complicates callback invocation
            and makes it more difficult to consolidate rcuo callback
            invocation with existing softirq callback invocation.
    
    The introduction of rcuog kthreads should thus be acceptable.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 32b3348d3a4d..dc3c53cb9608 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -200,8 +200,8 @@ struct rcu_data {
 	atomic_long_t nocb_q_count_lazy; /*  invocation (all stages). */
 	struct rcu_head *nocb_cb_head;	/* CBs ready to invoke. */
 	struct rcu_head **nocb_cb_tail;
-	struct swait_queue_head nocb_wq; /* For nocb kthreads to sleep on. */
-	struct task_struct *nocb_cb_kthread;
+	struct swait_queue_head nocb_cb_wq; /* For nocb kthreads to sleep on. */
+	struct task_struct *nocb_gp_kthread;
 	raw_spinlock_t nocb_lock;	/* Guard following pair of fields. */
 	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
 	struct timer_list nocb_timer;	/* Enforce finite deferral. */
@@ -211,6 +211,8 @@ struct rcu_data {
 					/* CBs waiting for GP. */
 	struct rcu_head **nocb_gp_tail;
 	bool nocb_gp_sleep;		/* Is the nocb GP thread asleep? */
+	struct swait_queue_head nocb_gp_wq; /* For nocb kthreads to sleep on. */
+	struct task_struct *nocb_cb_kthread;
 	struct rcu_data *nocb_next_cb_rdp;
 					/* Next rcu_data in wakeup chain. */
 

commit 6484fe54b5c64e9a388f369001508ab8df85a646
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Thu Mar 28 15:44:18 2019 -0700

    rcu/nocb: Update comments to prepare for forward-progress work
    
    This commit simply rewords comments to prepare for leader nocb kthreads
    doing only grace-period work and callback shuffling.  This will mean
    the addition of replacement kthreads to invoke callbacks.  The "leader"
    and "follower" thus become less meaningful, so the commit changes no-CB
    comments with these strings to "GP" and "CB", respectively.  (Give or
    take the usual grammatical transformations.)
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index e4e59b627c5a..32b3348d3a4d 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -206,17 +206,17 @@ struct rcu_data {
 	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
 	struct timer_list nocb_timer;	/* Enforce finite deferral. */
 
-	/* The following fields are used by the leader, hence own cacheline. */
+	/* The following fields are used by GP kthread, hence own cacheline. */
 	struct rcu_head *nocb_gp_head ____cacheline_internodealigned_in_smp;
 					/* CBs waiting for GP. */
 	struct rcu_head **nocb_gp_tail;
-	bool nocb_gp_sleep;		/* Is the nocb leader thread asleep? */
+	bool nocb_gp_sleep;		/* Is the nocb GP thread asleep? */
 	struct rcu_data *nocb_next_cb_rdp;
 					/* Next rcu_data in wakeup chain. */
 
-	/* The following fields are used by the follower, hence new cachline. */
+	/* The following fields are used by CB kthread, hence new cachline. */
 	struct rcu_data *nocb_gp_rdp ____cacheline_internodealigned_in_smp;
-					/* Leader CPU takes GP-end wakeups. */
+					/* GP rdp takes GP-end wakeups. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 
 	/* 6) RCU priority boosting. */

commit 58bf6f77c6fb0abe8e1330d8375dddd52711ef4c
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Thu Mar 28 15:33:59 2019 -0700

    rcu/nocb: Rename rcu_data fields to prepare for forward-progress work
    
    This commit simply renames rcu_data fields to prepare for leader
    nocb kthreads doing only grace-period work and callback shuffling.
    This will mean the addition of replacement kthreads to invoke callbacks.
    The "leader" and "follower" thus become less meaningful, so the commit
    changes no-CB fields with these strings to "gp" and "cb", respectively.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7acaf3a62d39..e4e59b627c5a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -198,10 +198,10 @@ struct rcu_data {
 	struct rcu_head **nocb_tail;
 	atomic_long_t nocb_q_count;	/* # CBs waiting for nocb */
 	atomic_long_t nocb_q_count_lazy; /*  invocation (all stages). */
-	struct rcu_head *nocb_follower_head; /* CBs ready to invoke. */
-	struct rcu_head **nocb_follower_tail;
+	struct rcu_head *nocb_cb_head;	/* CBs ready to invoke. */
+	struct rcu_head **nocb_cb_tail;
 	struct swait_queue_head nocb_wq; /* For nocb kthreads to sleep on. */
-	struct task_struct *nocb_kthread;
+	struct task_struct *nocb_cb_kthread;
 	raw_spinlock_t nocb_lock;	/* Guard following pair of fields. */
 	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
 	struct timer_list nocb_timer;	/* Enforce finite deferral. */
@@ -210,12 +210,12 @@ struct rcu_data {
 	struct rcu_head *nocb_gp_head ____cacheline_internodealigned_in_smp;
 					/* CBs waiting for GP. */
 	struct rcu_head **nocb_gp_tail;
-	bool nocb_leader_sleep;		/* Is the nocb leader thread asleep? */
-	struct rcu_data *nocb_next_follower;
-					/* Next follower in wakeup chain. */
+	bool nocb_gp_sleep;		/* Is the nocb leader thread asleep? */
+	struct rcu_data *nocb_next_cb_rdp;
+					/* Next rcu_data in wakeup chain. */
 
 	/* The following fields are used by the follower, hence new cachline. */
-	struct rcu_data *nocb_leader ____cacheline_internodealigned_in_smp;
+	struct rcu_data *nocb_gp_rdp ____cacheline_internodealigned_in_smp;
 					/* Leader CPU takes GP-end wakeups. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 

commit 1bb336443cde1154600bd147a45a30baa59c57db
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Wed Mar 27 15:51:25 2019 -0700

    rcu: Rename rcu_data's ->deferred_qs to ->exp_deferred_qs
    
    The rcu_data structure's ->deferred_qs field is used to indicate that the
    current CPU is blocking an expedited grace period (perhaps a future one).
    Given that it is used only for expedited grace periods, its current name
    is misleading, so this commit renames it to ->exp_deferred_qs.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 21d740f0b8dc..7acaf3a62d39 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -154,7 +154,7 @@ struct rcu_data {
 	bool		core_needs_qs;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */
 	bool		gpwrap;		/* Possible ->gp_seq wrap. */
-	bool		deferred_qs;	/* This CPU awaiting a deferred QS? */
+	bool		exp_deferred_qs; /* This CPU awaiting a deferred QS? */
 	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
 	unsigned long grpmask;		/* Mask to apply to leaf qsmask. */
 	unsigned long	ticks_this_gp;	/* The number of scheduling-clock */

commit 0864f057b050bc6dd68106b3185e02db5140012d
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Thu Apr 4 12:19:25 2019 -0700

    rcu: Use irq_work to get scheduler's attention in clean context
    
    When rcu_read_unlock_special() is invoked with interrupts disabled, is
    either not in an interrupt handler or is not using RCU_SOFTIRQ, is not
    the first RCU read-side critical section in the chain, and either there
    is an expedited grace period in flight or this is a NO_HZ_FULL kernel,
    the end of the grace period can be unduly delayed.  The reason for this
    is that it is not safe to do wakeups in this situation.
    
    This commit fixes this problem by using the irq_work subsystem to
    force a later interrupt handler in a clean environment.  Because
    set_tsk_need_resched(current) and set_preempt_need_resched() are
    invoked prior to this, the scheduler will force a context switch
    upon return from this interrupt (though perhaps at the end of any
    interrupted preempt-disable or BH-disable region of code), which will
    invoke rcu_note_context_switch() (again in a clean environment), which
    will in turn give RCU the chance to report the deferred quiescent state.
    
    Of course, by then this task might be within another RCU read-side
    critical section.  But that will be detected at that time and reporting
    will be further deferred to the outermost rcu_read_unlock().  See
    rcu_preempt_need_deferred_qs() and rcu_preempt_deferred_qs() for more
    details on the checking.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a1a72a1ecb02..21d740f0b8dc 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -161,6 +161,8 @@ struct rcu_data {
 					/*  ticks this CPU has handled */
 					/*  during and after the last grace */
 					/* period it is aware of. */
+	struct irq_work defer_qs_iw;	/* Obtain later scheduler attention. */
+	bool defer_qs_iw_pending;	/* Scheduler attention pending? */
 
 	/* 2) batch handling */
 	struct rcu_segcblist cblist;	/* Segmented callback list, with */

commit 48d07c04b4cc1dc1221965312f58fd84926212fe
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Mar 20 22:13:33 2019 +0100

    rcu: Enable elimination of Tree-RCU softirq processing
    
    Some workloads need to change kthread priority for RCU core processing
    without affecting other softirq work.  This commit therefore introduces
    the rcutree.use_softirq kernel boot parameter, which moves the RCU core
    work from softirq to a per-CPU SCHED_OTHER kthread named rcuc.  Use of
    SCHED_OTHER approach avoids the scalability problems that appeared
    with the earlier attempt to move RCU core processing to from softirq
    to kthreads.  That said, kernels built with RCU_BOOST=y will run the
    rcuc kthreads at the RCU-boosting priority.
    
    Note that rcutree.use_softirq=0 must be specified to move RCU core
    processing to the rcuc kthreads: rcutree.use_softirq=1 is the default.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    [ paulmck: Adjust for invoke_rcu_callbacks() only ever being invoked
      from RCU core processing, in contrast to softirq->rcuc transition
      in old mainline RCU priority boosting. ]
    [ paulmck: Avoid wakeups when scheduler might have invoked rcu_read_unlock()
      while holding rq or pi locks, also possibly fixing a pre-existing latent
      bug involving raise_softirq()-induced wakeups. ]
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index e253d11af3c4..a1a72a1ecb02 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -407,8 +407,8 @@ void call_rcu(struct rcu_head *head, rcu_callback_t func);
 static void dump_blkd_tasks(struct rcu_node *rnp, int ncheck);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
 static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);
-static void invoke_rcu_callbacks_kthread(void);
 static bool rcu_is_callbacks_kthread(void);
+static void rcu_cpu_kthread_setup(unsigned int cpu);
 static void __init rcu_spawn_boost_kthreads(void);
 static void rcu_prepare_kthreads(int cpu);
 static void rcu_cleanup_after_idle(void);

commit b51bcbbf16ef0ea352e8b924dd8638112e4037a5
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue Jan 15 07:01:33 2019 -0800

    rcu: Move forward-progress checkers into tree_stall.h
    
    This commit further consolidates stall-warning functionality by moving
    forward-progress checkers into kernel/rcu/tree_stall.h, updating a
    comment or two while in the area.  More specifically, this commit moves
    show_rcu_gp_kthreads(), rcu_check_gp_start_stall(), rcu_fwd_progress_check(),
    sysrq_rcu, sysrq_show_rcu(), sysrq_rcudump_op, and rcu_sysrq_init() from
    kernel/rcu/tree.c to kernel/rcu/tree_stall.h.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index f882ce3ca5a5..e253d11af3c4 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -445,3 +445,5 @@ static void rcu_dynticks_task_exit(void);
 static void record_gp_stall_check_time(void);
 static void rcu_iw_handler(struct irq_work *iwp);
 static void check_cpu_stall(struct rcu_data *rdp);
+static void rcu_check_gp_start_stall(struct rcu_node *rnp, struct rcu_data *rdp,
+				     const unsigned long gpssdelay);

commit 7ac1907c9e7ba5f80d3c298fe9d2dbf620566a49
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Mon Jan 14 10:19:20 2019 -0800

    rcu: Move irq-disabled stall-warning checking to tree_stall.h
    
    The rcu_iw_handler() function's sole purpose in life is to indicate
    whether a stalled CPU had interrupts disabled, so it belongs in
    kernel/rcu/tree_stall.h.  This commit therefore makes that move,
    clarifying its header comment while in the area.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 099410dbcbe9..f882ce3ca5a5 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -443,4 +443,5 @@ static void rcu_dynticks_task_exit(void);
 
 /* Forward declarations for tree_stall.h */
 static void record_gp_stall_check_time(void);
+static void rcu_iw_handler(struct irq_work *iwp);
 static void check_cpu_stall(struct rcu_data *rdp);

commit e23344c2ca42d0083596bb39964675bef00ad691
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Sat Jan 12 09:35:44 2019 -0800

    rcu: Organize functions in tree_stall.h
    
    This commit does only code movement, removal of now-unneeded forward
    declarations, and addition of comments.  It organizes the functions
    that implement RCU CPU stall warnings for normal grace periods into
    three categories:
    
    1.      Control of RCU CPU stall warnings, including computing timeouts.
    
    2.      Interaction of stall warnings with grace periods.
    
    3.      Actual printing of the RCU CPU stall-warning messages.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 49bf3b00bb50..099410dbcbe9 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -442,6 +442,5 @@ static void rcu_dynticks_task_enter(void);
 static void rcu_dynticks_task_exit(void);
 
 /* Forward declarations for tree_stall.h */
-static int rcu_print_task_stall(struct rcu_node *rnp);
 static void record_gp_stall_check_time(void);
 static void check_cpu_stall(struct rcu_data *rdp);

commit 59b73a27681c5841440391f970a9a085228ba975
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Jan 11 21:05:17 2019 -0800

    rcu: Move FAST_NO_HZ stall-warning code to tree_stall.h
    
    This commit further consolidates the stall-warning code by moving
    print_cpu_stall_info() and its helper functions along with
    zero_cpu_stall_ticks() to kernel/rcu/tree_stall.h.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d73472af49e7..49bf3b00bb50 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -416,7 +416,6 @@ static void rcu_prepare_for_idle(void);
 static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 static bool rcu_preempt_need_deferred_qs(struct task_struct *t);
 static void rcu_preempt_deferred_qs(struct task_struct *t);
-static void print_cpu_stall_info(int cpu);
 static void zero_cpu_stall_ticks(struct rcu_data *rdp);
 static bool rcu_nocb_cpu_needs_barrier(int cpu);
 static struct swait_queue_head *rcu_nocb_gp_get(struct rcu_node *rnp);

commit 40e69ac7d0c5a19ea14656bc3131c55719baec96
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Jan 11 20:58:58 2019 -0800

    rcu: Inline RCU stall-warning info helper functions
    
    The print_cpu_stall_info_begin() and print_cpu_stall_info_end() print a
    single character each onto the console, and are a holdover from a time
    when RCU CPU stall warning messages could be abbreviated using a long-gone
    Kconfig option.  This commit therefore adds these single characters to
    already-printed strings in the calling functions, and then eliminates
    both print_cpu_stall_info_begin() and print_cpu_stall_info_end().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index c6df9a13dd06..d73472af49e7 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -416,9 +416,7 @@ static void rcu_prepare_for_idle(void);
 static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 static bool rcu_preempt_need_deferred_qs(struct task_struct *t);
 static void rcu_preempt_deferred_qs(struct task_struct *t);
-static void print_cpu_stall_info_begin(void);
 static void print_cpu_stall_info(int cpu);
-static void print_cpu_stall_info_end(void);
 static void zero_cpu_stall_ticks(struct rcu_data *rdp);
 static bool rcu_nocb_cpu_needs_barrier(int cpu);
 static struct swait_queue_head *rcu_nocb_gp_get(struct rcu_node *rnp);

commit 21d0d79ab051bf9facb9960a30e58b93a31c75a5
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Jan 11 20:36:45 2019 -0800

    rcu: Inline RCU task stall-warning helper functions
    
    The rcu_print_detail_task_stall(), rcu_print_task_stall_begin(), and
    rcu_print_task_stall_end() functions were defined to allow long-gone
    Kconfig options to provide an abbreviated RCU CPU stall warning printout.
    This commit saves a few lines of code by inlining them into their sole
    callers.
    
    While in the area, a useless call of rcu_print_detail_task_stall_rnp()
    on the root rcu_node structure was eliminated.  If there is only one
    rcu_node structure, its tasks get printed twice, but if there are more,
    the root rcu_node structure is guaranteed to have an empty list of blocked
    tasks, hence the uselessness.  (Long ago, root rcu_node structures with
    non-empty ->blkd_tasks lists could happen, but no longer.)
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 3c4e26fff806..c6df9a13dd06 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -445,7 +445,6 @@ static void rcu_dynticks_task_enter(void);
 static void rcu_dynticks_task_exit(void);
 
 /* Forward declarations for tree_stall.h */
-static void rcu_print_detail_task_stall(void);
 static int rcu_print_task_stall(struct rcu_node *rnp);
 static void record_gp_stall_check_time(void);
 static void check_cpu_stall(struct rcu_data *rdp);

commit 32255d51b6ed00de2b88970ceea8db0ec3bae6f8
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Jan 11 16:57:41 2019 -0800

    rcu: Move RCU CPU stall-warning code out of tree.c
    
    This commit completes the process of consolidating the code for RCU CPU
    stall warnings for normal grace periods by moving the remaining such
    code from kernel/rcu/tree.c to kernel/rcu/tree_stall.h.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index bb4f995f2d3f..3c4e26fff806 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -393,15 +393,13 @@ static const char *tp_rcu_varname __used __tracepoint_string = rcu_name;
 
 int rcu_dynticks_snap(struct rcu_data *rdp);
 
-/* Forward declarations for rcutree_plugin.h */
+/* Forward declarations for tree_plugin.h */
 static void rcu_bootup_announce(void);
 static void rcu_qs(void);
 static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
 #ifdef CONFIG_HOTPLUG_CPU
 static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 #endif /* #ifdef CONFIG_HOTPLUG_CPU */
-static void rcu_print_detail_task_stall(void);
-static int rcu_print_task_stall(struct rcu_node *rnp);
 static int rcu_print_task_exp_stall(struct rcu_node *rnp);
 static void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp);
 static void rcu_flavor_sched_clock_irq(int user);
@@ -445,3 +443,9 @@ static void rcu_bind_gp_kthread(void);
 static bool rcu_nohz_full_cpu(void);
 static void rcu_dynticks_task_enter(void);
 static void rcu_dynticks_task_exit(void);
+
+/* Forward declarations for tree_stall.h */
+static void rcu_print_detail_task_stall(void);
+static int rcu_print_task_stall(struct rcu_node *rnp);
+static void record_gp_stall_check_time(void);
+static void check_cpu_stall(struct rcu_data *rdp);

commit e7ffb4eb9a6d89678e7f62461737899f88dab64e
Merge: 87d1779dc48f 423a86a610ca 6706dae90d85 5a4eb3cb2012 e81baf4cb19a 6684880a8b16
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Sat Feb 9 08:47:52 2019 -0800

    Merge branches 'doc.2019.01.26a', 'fixes.2019.01.26a', 'sil.2019.01.26a', 'spdx.2019.02.09a', 'srcu.2019.01.26a' and 'torture.2019.01.26a' into HEAD
    
    doc.2019.01.26a:  Documentation updates.
    fixes.2019.01.26a:  Miscellaneous fixes.
    sil.2019.01.26a:  Removal of a few more spin_is_locked() instances.
    spdx.2019.02.09a:  Add SPDX identifiers to RCU files
    srcu.2019.01.26a:  SRCU updates.
    torture.2019.01.26a: Torture-test updates.

commit 22e409253144d564ec7c8a425fa84b598e36f03c
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Thu Jan 17 10:23:39 2019 -0800

    rcu/tree: Convert to SPDX license identifier
    
    Replace the license boiler plate with a SPDX license identifier.
    While in the area, update an email address.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    [ paulmck: Update .h file SPDX comment format per Joe Perches. ]
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 149557b7c39c..f22c21ff3a89 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -1,25 +1,12 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
 /*
  * Read-Copy Update mechanism for mutual exclusion (tree-based version)
  * Internal non-public definitions.
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
  * Copyright IBM Corporation, 2008
  *
  * Author: Ingo Molnar <mingo@elte.hu>
- *	   Paul E. McKenney <paulmck@linux.vnet.ibm.com>
+ *	   Paul E. McKenney <paulmck@linux.ibm.com>
  */
 
 #include <linux/cache.h>

commit e81baf4cb19a9b428ba477fd0423f81672a58817
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Dec 11 12:12:38 2018 +0100

    srcu: Remove srcu_queue_delayed_work_on()
    
    srcu_queue_delayed_work_on() disables preemption (and therefore CPU
    hotplug in RCU's case) and then checks based on its own accounting if a
    CPU is online. If the CPU is online it uses queue_delayed_work_on()
    otherwise it fallbacks to queue_delayed_work().
    The problem here is that queue_work() on -RT does not work with disabled
    preemption.
    
    queue_work_on() works also on an offlined CPU. queue_delayed_work_on()
    has the problem that it is possible to program a timer on an offlined
    CPU. This timer will fire once the CPU is online again. But until then,
    the timer remains programmed and nothing will happen.
    
    Add a local timer which will fire (as requested per delay) on the local
    CPU and then enqueue the work on the specific CPU.
    
    RCUtorture testing with SRCU-P for 24h showed no problems.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 149557b7c39c..4bba017c703c 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -458,11 +458,3 @@ static void rcu_bind_gp_kthread(void);
 static bool rcu_nohz_full_cpu(void);
 static void rcu_dynticks_task_enter(void);
 static void rcu_dynticks_task_exit(void);
-
-#ifdef CONFIG_SRCU
-void srcu_online_cpu(unsigned int cpu);
-void srcu_offline_cpu(unsigned int cpu);
-#else /* #ifdef CONFIG_SRCU */
-void srcu_online_cpu(unsigned int cpu) { }
-void srcu_offline_cpu(unsigned int cpu) { }
-#endif /* #else #ifdef CONFIG_SRCU */

commit c98cac603f1ce7d00e2a802b5640bced3bc3c1f2
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Wed Nov 21 11:35:03 2018 -0800

    rcu: Rename rcu_check_callbacks() to rcu_sched_clock_irq()
    
    The name rcu_check_callbacks() arguably made sense back in the early
    2000s when RCU was quite a bit simpler than it is today, but it has
    become quite misleading, especially with the advent of dyntick-idle
    and NO_HZ_FULL.  The rcu_check_callbacks() function is RCU's hook into
    the scheduling-clock interrupt, and is now but one of many ways that
    callbacks get promoted to invocable state.
    
    This commit therefore changes the name to rcu_sched_clock_irq(),
    which is the same number of characters and clearly indicates this
    function's relation to the rest of the Linux kernel.  In addition, for
    the sake of consistency, rcu_flavor_check_callbacks() is also renamed
    to rcu_flavor_sched_clock_irq().
    
    While in the area, the header comments for both functions are reworked.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 149557b7c39c..f37f54cc9080 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -417,7 +417,7 @@ static void rcu_print_detail_task_stall(void);
 static int rcu_print_task_stall(struct rcu_node *rnp);
 static int rcu_print_task_exp_stall(struct rcu_node *rnp);
 static void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp);
-static void rcu_flavor_check_callbacks(int user);
+static void rcu_flavor_sched_clock_irq(int user);
 void call_rcu(struct rcu_head *head, rcu_callback_t func);
 static void dump_blkd_tasks(struct rcu_node *rnp, int ncheck);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);

commit 7a968bb26a9b09b6e44c48a8036a73e0bc497dbc
Merge: 6ba7d681aca2 13dc7d0c7a2e
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Jan 25 15:32:01 2019 -0800

    Merge branches 'consolidate.2019.01.26a' and 'fwd.2019.01.26a' into HEAD
    
    consolidate.2019.01.26a: RCU flavor consolidation cleanups.
    fwd.2019.01.26a: RCU grace-period forward-progress fixes.

commit fd897573fa4cfe66ebddf5f4444f36710cf0cad0
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Mon Dec 10 16:09:49 2018 -0800

    rcu: Improve diagnostics for failed RCU grace-period start
    
    If a grace period fails to start (for example, because you commented
    out the last two lines of rcu_accelerate_cbs_unlocked()), rcu_core()
    will invoke rcu_check_gp_start_stall(), which will notice and complain.
    However, this complaint is lacking crucial debugging information such
    as when the last wakeup executed and what the value of ->gp_seq was at
    that time.  This commit therefore removes the current pr_alert() from
    rcu_check_gp_start_stall(), instead invoking show_rcu_gp_kthreads(),
    which has been updated to print the needed information, which is collected
    by rcu_gp_kthread_wake().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 008c356c7033..1f2ada7ef7d7 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -309,6 +309,8 @@ struct rcu_state {
 	struct swait_queue_head gp_wq;		/* Where GP task waits. */
 	short gp_flags;				/* Commands for GP task. */
 	short gp_state;				/* GP kthread sleep state. */
+	unsigned long gp_wake_time;		/* Last GP kthread wake. */
+	unsigned long gp_wake_seq;		/* ->gp_seq at ^^^. */
 
 	/* End of fields guarded by root rcu_node's lock. */
 

commit b2c1955b88495c1531b2080ba4fad119c0a03cc1
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Nov 30 19:12:04 2018 -0800

    rcu: Remove unused rcu_cpu_kthread_cpu per-CPU variable
    
    The rcu_cpu_kthread_cpu used to provide debugfs information, but is no
    longer used.  This commit therefore removes it.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7ae6774a920e..008c356c7033 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -408,10 +408,6 @@ static const char *tp_rcu_varname __used __tracepoint_string = rcu_name;
 
 int rcu_dynticks_snap(struct rcu_data *rdp);
 
-#ifdef CONFIG_RCU_BOOST
-DECLARE_PER_CPU(int, rcu_cpu_kthread_cpu);
-#endif /* #ifdef CONFIG_RCU_BOOST */
-
 /* Forward declarations for rcutree_plugin.h */
 static void rcu_bootup_announce(void);
 static void rcu_qs(void);

commit f7e972ee128e0a65784b13ec1459fe35b817eed7
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Nov 30 18:21:32 2018 -0800

    rcu: Move rcu_cpu_has_work to rcu_data structure
    
    Given that RCU has a perfectly good per-CPU rcu_data structure, most
    per-CPU quantities should be stored there.
    
    This commit therefore moves the rcu_cpu_has_work per-CPU variable to
    the rcu_data structure.  This also makes this variable unconditionally
    present, which should be acceptable given the memory reduction due to the
    RCU flavor consolidation and also due to simplifications this will enable.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index e50b0a5a94bc..7ae6774a920e 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -238,7 +238,7 @@ struct rcu_data {
 	struct task_struct *rcu_cpu_kthread_task;
 					/* rcuc per-CPU kthread or NULL. */
 	unsigned int rcu_cpu_kthread_status;
-					/* Running status for rcuc. */
+	char rcu_cpu_has_work;
 
 	/* 7) Diagnostic data, including RCU CPU stall warnings. */
 	unsigned int softirq_snap;	/* Snapshot of softirq activity. */
@@ -410,7 +410,6 @@ int rcu_dynticks_snap(struct rcu_data *rdp);
 
 #ifdef CONFIG_RCU_BOOST
 DECLARE_PER_CPU(int, rcu_cpu_kthread_cpu);
-DECLARE_PER_CPU(char, rcu_cpu_has_work);
 #endif /* #ifdef CONFIG_RCU_BOOST */
 
 /* Forward declarations for rcutree_plugin.h */

commit 8b4d0f4858864af6a0753740f00353035bd058de
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Nov 30 17:17:21 2018 -0800

    rcu: Remove unused rcu_cpu_kthread_loops per-CPU variable
    
    The rcu_cpu_kthread_loops variable used to provide debugfs information,
    but is no longer used.  This commit therefore removes it.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 047f5e9350d1..e50b0a5a94bc 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -410,7 +410,6 @@ int rcu_dynticks_snap(struct rcu_data *rdp);
 
 #ifdef CONFIG_RCU_BOOST
 DECLARE_PER_CPU(int, rcu_cpu_kthread_cpu);
-DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_loops);
 DECLARE_PER_CPU(char, rcu_cpu_has_work);
 #endif /* #ifdef CONFIG_RCU_BOOST */
 

commit 6ffdde28b7558ec48f4e0eee01821a66a67a8e25
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Nov 30 16:43:05 2018 -0800

    rcu: Move rcu_cpu_kthread_status to rcu_data structure
    
    Given that RCU has a perfectly good per-CPU rcu_data structure, most
    per-CPU quantities should be stored there.
    
    This commit therefore moves the rcu_cpu_kthread_status per-CPU variable
    to the rcu_data structure.  This also makes this variable unconditionally
    present, which should be acceptable given the memory reduction due to the
    RCU flavor consolidation and also due to simplifications this will enable.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index ef517ba25192..047f5e9350d1 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -237,6 +237,8 @@ struct rcu_data {
 	/* 6) RCU priority boosting. */
 	struct task_struct *rcu_cpu_kthread_task;
 					/* rcuc per-CPU kthread or NULL. */
+	unsigned int rcu_cpu_kthread_status;
+					/* Running status for rcuc. */
 
 	/* 7) Diagnostic data, including RCU CPU stall warnings. */
 	unsigned int softirq_snap;	/* Snapshot of softirq activity. */
@@ -407,7 +409,6 @@ static const char *tp_rcu_varname __used __tracepoint_string = rcu_name;
 int rcu_dynticks_snap(struct rcu_data *rdp);
 
 #ifdef CONFIG_RCU_BOOST
-DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_status);
 DECLARE_PER_CPU(int, rcu_cpu_kthread_cpu);
 DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_loops);
 DECLARE_PER_CPU(char, rcu_cpu_has_work);

commit 37f62d7cf00c085e1d7a91a6af286c4e8d32e1e1
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Fri Nov 30 16:11:14 2018 -0800

    rcu: Move rcu_cpu_kthread_task to rcu_data structure
    
    Given that RCU has a perfectly good per-CPU rcu_data structure, most
    per-CPU quantities should be stored there.
    
    This commit therefore moves the rcu_cpu_kthread_task per-CPU variable to
    the rcu_data structure.  This also makes this variable unconditionally
    present, which should be acceptable given the memory reduction due to the
    RCU flavor consolidation and also due to simplifications this will enable.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d90b02b53c0e..ef517ba25192 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -234,7 +234,11 @@ struct rcu_data {
 					/* Leader CPU takes GP-end wakeups. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 
-	/* 6) Diagnostic data, including RCU CPU stall warnings. */
+	/* 6) RCU priority boosting. */
+	struct task_struct *rcu_cpu_kthread_task;
+					/* rcuc per-CPU kthread or NULL. */
+
+	/* 7) Diagnostic data, including RCU CPU stall warnings. */
 	unsigned int softirq_snap;	/* Snapshot of softirq activity. */
 	/* ->rcu_iw* fields protected by leaf rcu_node ->lock. */
 	struct irq_work rcu_iw;		/* Check for non-irq activity. */

commit 260e1e4fd826a8e47c8976efba6a54d62b4a57de
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Thu Nov 29 13:28:49 2018 -0800

    rcu: Discard separate per-CPU callback counts
    
    Back when there were multiple flavors of RCU, it was necessary to
    separately count lazy and non-lazy callbacks for each CPU.  These counts
    were used in CONFIG_RCU_FAST_NO_HZ kernels to determine how long a newly
    idle CPU should be allowed to sleep before handling its RCU callbacks.
    But now that there is only one flavor, the callback counts for a given
    CPU's sole rcu_data structure are the counts for that CPU.
    
    This commit therefore removes the rcu_data structure's ->nonlazy_posted
    and ->nonlazy_posted_snap fields, the rcu_idle_count_callbacks_posted()
    and rcu_cpu_has_callbacks() functions, repurposes the rcu_data structure's
    ->all_lazy field to record the laziness state at the beginning of the
    latest idle sojourn, and modifies CONFIG_RCU_FAST_NO_HZ RCU CPU stall
    warnings accordingly.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 50bb41cdc5fb..20feecbb0ab6 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -193,10 +193,7 @@ struct rcu_data {
 	bool rcu_need_heavy_qs;		/* GP old, so heavy quiescent state! */
 	bool rcu_urgent_qs;		/* GP old need light quiescent state. */
 #ifdef CONFIG_RCU_FAST_NO_HZ
-	bool all_lazy;			/* Are all CPU's CBs lazy? */
-	unsigned long nonlazy_posted;	/* # times non-lazy CB posted to CPU. */
-	unsigned long nonlazy_posted_snap;
-					/* Nonlazy_posted snapshot. */
+	bool all_lazy;			/* All CPU's CBs lazy at idle start? */
 	unsigned long last_accelerate;	/* Last jiffy CBs were accelerated. */
 	unsigned long last_advance_all;	/* Last jiffy CBs were all advanced. */
 	int tick_nohz_enabled_snap;	/* Previously seen value from sysfs. */
@@ -430,7 +427,6 @@ static void __init rcu_spawn_boost_kthreads(void);
 static void rcu_prepare_kthreads(int cpu);
 static void rcu_cleanup_after_idle(void);
 static void rcu_prepare_for_idle(void);
-static void rcu_idle_count_callbacks_posted(void);
 static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 static bool rcu_preempt_need_deferred_qs(struct task_struct *t);
 static void rcu_preempt_deferred_qs(struct task_struct *t);

commit 142d106d5e62ff2cf0dfd2dfe1adfcaff1c2ed85
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Thu Nov 29 09:15:54 2018 -0800

    rcu: Determine expedited-GP IPI handler at build time
    
    Back when there could be multiple RCU flavors running in the same kernel
    at the same time, it was necessary to specify the expedited grace-period
    IPI handler at runtime.  Now that there is only one RCU flavor, the
    IPI handler can be determined at build time.  There is therefore no
    longer any reason for the RCU-preempt and RCU-sched IPI handlers to
    have different names, nor is there any reason to pass these handlers in
    function arguments and in the data structures enclosing workqueues.
    
    This commit therefore makes all these changes, pushing the specification
    of the expedited grace-period IPI handler down to the point of use.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index bcfd684a5c57..50bb41cdc5fb 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -36,7 +36,6 @@
 
 /* Communicate arguments to a workqueue handler. */
 struct rcu_exp_work {
-	smp_call_func_t rew_func;
 	unsigned long rew_s;
 	struct work_struct rew_work;
 };

commit ad368d15b08ad22509a56bdfd6ee3a04da91ce10
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue Nov 27 13:55:53 2018 -0800

    rcu: Rename and comment changes due to only one rcuo kthread per CPU
    
    Given RCU flavor consolidation, the name rcu_spawn_all_nocb_kthreads()
    is quite misleading.  It no longer ever creates more than one kthread,
    and it does so only for the specified CPU.  This commit therefore changes
    this name to the more descriptive rcu_spawn_cpu_nocb_kthread(), and also
    fixes up a similar issue in its header comment while in the area.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d90b02b53c0e..bcfd684a5c57 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -451,7 +451,7 @@ static bool rcu_nocb_adopt_orphan_cbs(struct rcu_data *my_rdp,
 static int rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp);
 static void do_nocb_deferred_wakeup(struct rcu_data *rdp);
 static void rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp);
-static void rcu_spawn_all_nocb_kthreads(int cpu);
+static void rcu_spawn_cpu_nocb_kthread(int cpu);
 static void __init rcu_spawn_nocb_kthreads(void);
 #ifdef CONFIG_RCU_NOCB_CPU
 static void __init rcu_organize_nocb_kthreads(void);

commit c51d7b5e6c94aa6b554c27bd2b0eb64ebef02334
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Wed Oct 3 17:25:33 2018 -0700

    rcutorture: Print time since GP end upon forward-progress failure
    
    If rcutorture's forward-progress tests fail while a grace period is not
    in progress, it is useful to print the time since the last grace period
    ended as a way to detect failure to launch a new grace period.  This
    commit therefore makes this change.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a8f82b7dc5e2..d90b02b53c0e 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -328,6 +328,8 @@ struct rcu_state {
 						/*  force_quiescent_state(). */
 	unsigned long gp_start;			/* Time at which GP started, */
 						/*  but in jiffies. */
+	unsigned long gp_end;			/* Time last GP ended, again */
+						/*  in jiffies. */
 	unsigned long gp_activity;		/* Time of last GP kthread */
 						/*  activity in jiffies. */
 	unsigned long gp_req_activity;		/* Time of last GP request */

commit 903ee83d91776bc72d856147743687d4b6c99286
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue Oct 2 16:05:46 2018 -0700

    rcu: Account for nocb-CPU callback counts in RCU CPU stall warnings
    
    The RCU CPU stall warnings print an estimate of the total number of
    RCU callbacks queued in the system, but this estimate leaves out
    the callbacks queued for nocbs CPUs.  This commit therefore introduces
    rcu_get_n_cbs_cpu(), which gives an accurate callback estimate for
    both nocbs and normal CPUs, and uses this new function as needed.
    
    This commit also introduces a rcu_get_n_cbs_nocb_cpu() helper function
    that returns the number of callbacks for nocbs CPUs or zero otherwise,
    and also uses this function in place of direct access to ->nocb_q_count
    while in the area (fewer characters, you see).
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index c3e2807a834a..a8f82b7dc5e2 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -455,6 +455,7 @@ static void __init rcu_spawn_nocb_kthreads(void);
 static void __init rcu_organize_nocb_kthreads(void);
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 static bool init_nocb_callback_list(struct rcu_data *rdp);
+static unsigned long rcu_get_n_cbs_nocb_cpu(struct rcu_data *rdp);
 static void rcu_bind_gp_kthread(void);
 static bool rcu_nohz_full_cpu(void);
 static void rcu_dynticks_task_enter(void);

commit adbccddb4a16b1dbf047d330ae1e78fd1ec80352
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Sat Sep 22 19:41:26 2018 -0400

    rcu: Fix rcu_{node,data} comments about gp_seq_needed
    
    Recent changes have removed the old ->gp_seq_needed field from the
    rcu_state structure, which in turn obsoleted a couple of comments in
    the rcu_node and rcu_data structures.  This commit therefore updates
    these comments accordingly.
    
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Cc: <kernel-team@android.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 57a937ac51c2..c3e2807a834a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -57,7 +57,7 @@ struct rcu_node {
 					/*  some rcu_state fields as well as */
 					/*  following. */
 	unsigned long gp_seq;	/* Track rsp->rcu_gp_seq. */
-	unsigned long gp_seq_needed; /* Track rsp->rcu_gp_seq_needed. */
+	unsigned long gp_seq_needed; /* Track furthest future GP request. */
 	unsigned long completedqs; /* All QSes done for this node. */
 	unsigned long qsmask;	/* CPUs or groups that need to switch in */
 				/*  order for current grace period to proceed.*/
@@ -163,7 +163,7 @@ union rcu_noqs {
 struct rcu_data {
 	/* 1) quiescent-state and grace-period handling : */
 	unsigned long	gp_seq;		/* Track rsp->rcu_gp_seq counter. */
-	unsigned long	gp_seq_needed;	/* Track rsp->rcu_gp_seq_needed ctr. */
+	unsigned long	gp_seq_needed;	/* Track furthest future GP request. */
 	union rcu_noqs	cpu_no_qs;	/* No QSes yet for this CPU. */
 	bool		core_needs_qs;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */

commit 75a8f72245223cce1571b0405b0881ca3c046df7
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Sat Sep 22 19:41:25 2018 -0400

    rcu: Remove unused rcu_state externs
    
    The rcu_bh_state and rcu_sched_state variables were removed during the
    RCU flavor consolidations, but external declarations remain in tree.h.
    This commit therefore removes these obsolete declarations.
    
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Cc: <kernel-team@android.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 703e19ff532d..57a937ac51c2 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -398,17 +398,6 @@ static const char *tp_rcu_varname __used __tracepoint_string = rcu_name;
 #define RCU_NAME rcu_name
 #endif /* #else #ifdef CONFIG_TRACING */
 
-/*
- * RCU implementation internal declarations:
- */
-extern struct rcu_state rcu_sched_state;
-
-extern struct rcu_state rcu_bh_state;
-
-#ifdef CONFIG_PREEMPT_RCU
-extern struct rcu_state rcu_preempt_state;
-#endif /* #ifdef CONFIG_PREEMPT_RCU */
-
 int rcu_dynticks_snap(struct rcu_data *rdp);
 
 #ifdef CONFIG_RCU_BOOST

commit 894d45bbf7e7569ec2aa845155801fd503b5f1bf
Author: Mike Galbraith <efault@gmx.de>
Date:   Wed Aug 15 09:05:29 2018 -0700

    rcu: Convert rcu_state.ofl_lock to raw_spinlock_t
    
    1e64b15a4b10 ("rcu: Fix grace-period hangs due to race with CPU offline")
    added spinlock_t ofl_lock to the rcu_state structure, then takes it with
    preemption disabled during CPU offline, which gives the -rt patchset's
    sleeping spinlock heartburn.
    
    This commit therefore converts ->ofl_lock to raw_spinlock_t.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index bfbf97a1c29d..703e19ff532d 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -343,7 +343,7 @@ struct rcu_state {
 	const char *name;			/* Name of structure. */
 	char abbr;				/* Abbreviated name. */
 
-	spinlock_t ofl_lock ____cacheline_internodealigned_in_smp;
+	raw_spinlock_t ofl_lock ____cacheline_internodealigned_in_smp;
 						/* Synchronize offline with */
 						/*  GP pre-initialization. */
 };

commit 8d8a9d0e7eda9feeee4af7be31932e14b512d3ad
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 4 20:32:07 2018 -0700

    rcu: Remove obsolete ->dynticks_fqs and ->cond_resched_completed
    
    The rcu_data structure's ->dynticks_fqs is incremented but never
    accesses.  Its ->cond_resched_completed field isn't used at all.
    This commit therefore removes both fields.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index af8681fec23b..bfbf97a1c29d 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -203,17 +203,11 @@ struct rcu_data {
 	int tick_nohz_enabled_snap;	/* Previously seen value from sysfs. */
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 
-	/* 4) reasons this CPU needed to be kicked by force_quiescent_state */
-	unsigned long dynticks_fqs;	/* Kicked due to dynticks idle. */
-	unsigned long cond_resched_completed;
-					/* Grace period that needs help */
-					/*  from cond_resched(). */
-
-	/* 5) rcu_barrier(), OOM callbacks, and expediting. */
+	/* 4) rcu_barrier(), OOM callbacks, and expediting. */
 	struct rcu_head barrier_head;
 	int exp_dynticks_snap;		/* Double-check need for IPI. */
 
-	/* 6) Callback offloading. */
+	/* 5) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU
 	struct rcu_head *nocb_head;	/* CBs waiting for kthread. */
 	struct rcu_head **nocb_tail;
@@ -240,7 +234,7 @@ struct rcu_data {
 					/* Leader CPU takes GP-end wakeups. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 
-	/* 7) Diagnostic data, including RCU CPU stall warnings. */
+	/* 6) Diagnostic data, including RCU CPU stall warnings. */
 	unsigned int softirq_snap;	/* Snapshot of softirq activity. */
 	/* ->rcu_iw* fields protected by leaf rcu_node ->lock. */
 	struct irq_work rcu_iw;		/* Check for non-irq activity. */

commit dc5a4f2932f18568bb9d8cdbe2139a8ddbc28bb8
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 3 21:00:38 2018 -0700

    rcu: Switch ->dynticks to rcu_data structure, remove rcu_dynticks
    
    This commit move ->dynticks from the rcu_dynticks structure to the
    rcu_data structure, replacing the field of the same name.  It also updates
    the code to access ->dynticks from the rcu_data structure and to use the
    rcu_data structure rather than following to now-gone ->dynticks field
    to the now-gone rcu_dynticks structure.  While in the area, this commit
    also fixes up comments.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 2e5eec48a94a..af8681fec23b 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -34,13 +34,6 @@
 
 #include "rcu_segcblist.h"
 
-/*
- * Dynticks per-CPU state.
- */
-struct rcu_dynticks {
-	atomic_t dynticks;	    /* Even value for idle, else odd. */
-};
-
 /* Communicate arguments to a workqueue handler. */
 struct rcu_exp_work {
 	smp_call_func_t rew_func;
@@ -194,24 +187,20 @@ struct rcu_data {
 	long		blimit;		/* Upper limit on a processed batch */
 
 	/* 3) dynticks interface. */
-	struct rcu_dynticks *dynticks;	/* Shared per-CPU dynticks state. */
 	int dynticks_snap;		/* Per-GP tracking for dynticks. */
-	long dynticks_nesting;      /* Track process nesting level. */
-	long dynticks_nmi_nesting;  /* Track irq/NMI nesting level. */
-	// atomic_t dynticks;	    /* Even value for idle, else odd. */
-	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
-	bool rcu_urgent_qs;	    /* GP old need light quiescent state. */
+	long dynticks_nesting;		/* Track process nesting level. */
+	long dynticks_nmi_nesting;	/* Track irq/NMI nesting level. */
+	atomic_t dynticks;		/* Even value for idle, else odd. */
+	bool rcu_need_heavy_qs;		/* GP old, so heavy quiescent state! */
+	bool rcu_urgent_qs;		/* GP old need light quiescent state. */
 #ifdef CONFIG_RCU_FAST_NO_HZ
-	bool all_lazy;		    /* Are all CPU's CBs lazy? */
-	unsigned long nonlazy_posted;
-				    /* # times non-lazy CBs posted to CPU. */
+	bool all_lazy;			/* Are all CPU's CBs lazy? */
+	unsigned long nonlazy_posted;	/* # times non-lazy CB posted to CPU. */
 	unsigned long nonlazy_posted_snap;
-				    /* idle-period nonlazy_posted snapshot. */
-	unsigned long last_accelerate;
-				    /* Last jiffy CBs were accelerated. */
-	unsigned long last_advance_all;
-				    /* Last jiffy CBs were all advanced. */
-	int tick_nohz_enabled_snap; /* Previously seen value from sysfs. */
+					/* Nonlazy_posted snapshot. */
+	unsigned long last_accelerate;	/* Last jiffy CBs were accelerated. */
+	unsigned long last_advance_all;	/* Last jiffy CBs were all advanced. */
+	int tick_nohz_enabled_snap;	/* Previously seen value from sysfs. */
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 
 	/* 4) reasons this CPU needed to be kicked by force_quiescent_state */
@@ -426,7 +415,7 @@ extern struct rcu_state rcu_bh_state;
 extern struct rcu_state rcu_preempt_state;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 
-int rcu_dynticks_snap(struct rcu_dynticks *rdtp);
+int rcu_dynticks_snap(struct rcu_data *rdp);
 
 #ifdef CONFIG_RCU_BOOST
 DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_status);

commit 4c5273bf2b5ed9b585e470dda19c09c875a9fbbd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 3 21:00:38 2018 -0700

    rcu: Switch dyntick nesting counters to rcu_data structure
    
    This commit removes ->dynticks_nesting and ->dynticks_nmi_nesting from
    the rcu_dynticks structure and updates the code to access them from the
    rcu_data structure.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 4c31066ddb94..2e5eec48a94a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -38,8 +38,6 @@
  * Dynticks per-CPU state.
  */
 struct rcu_dynticks {
-	long dynticks_nesting;      /* Track process nesting level. */
-	long dynticks_nmi_nesting;  /* Track irq/NMI nesting level. */
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
 };
 

commit 2dba13f0b6c2b26ff371b8927ac58d20a7d94713
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 3 21:00:38 2018 -0700

    rcu: Switch urgent quiescent-state requests to rcu_data structure
    
    This commit removes ->rcu_need_heavy_qs and ->rcu_urgent_qs from the
    rcu_dynticks structure and updates the code to access them from the
    rcu_data structure.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 36a47c7bd882..4c31066ddb94 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -41,8 +41,6 @@ struct rcu_dynticks {
 	long dynticks_nesting;      /* Track process nesting level. */
 	long dynticks_nmi_nesting;  /* Track irq/NMI nesting level. */
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
-	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
-	bool rcu_urgent_qs;	    /* GP old need light quiescent state. */
 };
 
 /* Communicate arguments to a workqueue handler. */

commit c458a89e964dbf3c56b23eca2018bd0e2380969d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 3 21:00:38 2018 -0700

    rcu: Switch lazy counts to rcu_data structure
    
    This commit removes ->all_lazy, ->nonlazy_posted and ->nonlazy_posted_snap
    from the rcu_dynticks structure and updates the code to access them from
    the rcu_data structure.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 69bd6bec05bb..36a47c7bd882 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -43,13 +43,6 @@ struct rcu_dynticks {
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
 	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
 	bool rcu_urgent_qs;	    /* GP old need light quiescent state. */
-#ifdef CONFIG_RCU_FAST_NO_HZ
-	bool all_lazy;		    /* Are all CPU's CBs lazy? */
-	unsigned long nonlazy_posted;
-				    /* # times non-lazy CBs posted to CPU. */
-	unsigned long nonlazy_posted_snap;
-				    /* idle-period nonlazy_posted snapshot. */
-#endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 };
 
 /* Communicate arguments to a workqueue handler. */

commit 5998a75adbf4f85e63b06fa7723633cc84d7129b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 3 21:00:38 2018 -0700

    rcu: Switch last accelerate/advance to rcu_data structure
    
    This commit removes ->last_accelerate and ->last_advance_all from the
    rcu_dynticks structure and updates the code to access them from the
    rcu_data structure.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 5d447ceba769..69bd6bec05bb 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -49,10 +49,6 @@ struct rcu_dynticks {
 				    /* # times non-lazy CBs posted to CPU. */
 	unsigned long nonlazy_posted_snap;
 				    /* idle-period nonlazy_posted snapshot. */
-	unsigned long last_accelerate;
-				    /* Last jiffy CBs were accelerated. */
-	unsigned long last_advance_all;
-				    /* Last jiffy CBs were all advanced. */
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 };
 

commit 0fd79e7521bc944522c3c97f40f3d25619e329f4
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 3 21:00:38 2018 -0700

    rcu: Switch ->tick_nohz_enabled_snap to rcu_data structure
    
    This commit removes ->tick_nohz_enabled_snap from the rcu_dynticks
    structure and updates the code to access it from the rcu_data
    structure.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d35cd9677b08..5d447ceba769 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -53,7 +53,6 @@ struct rcu_dynticks {
 				    /* Last jiffy CBs were accelerated. */
 	unsigned long last_advance_all;
 				    /* Last jiffy CBs were all advanced. */
-	int tick_nohz_enabled_snap; /* Previously seen value from sysfs. */
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 };
 

commit cc72046cc3cce8438778f306d59442febf4b7683
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Aug 3 19:31:39 2018 -0700

    rcu: Merge rcu_dynticks structure into rcu_data structure
    
    Now that there is only ever one rcu_data structure per CPU, there is no
    need for a separate rcu_dynticks structure.  This commit therefore adds
    the rcu_dynticks fields into the rcu_data structure in preparation for
    removing the rcu_dynticks structure entirely.  Note that the ->dynticks
    field will be handled specially because there is a field by that name
    in both structures.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 5e561f1339d4..d35cd9677b08 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -212,6 +212,23 @@ struct rcu_data {
 	/* 3) dynticks interface. */
 	struct rcu_dynticks *dynticks;	/* Shared per-CPU dynticks state. */
 	int dynticks_snap;		/* Per-GP tracking for dynticks. */
+	long dynticks_nesting;      /* Track process nesting level. */
+	long dynticks_nmi_nesting;  /* Track irq/NMI nesting level. */
+	// atomic_t dynticks;	    /* Even value for idle, else odd. */
+	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
+	bool rcu_urgent_qs;	    /* GP old need light quiescent state. */
+#ifdef CONFIG_RCU_FAST_NO_HZ
+	bool all_lazy;		    /* Are all CPU's CBs lazy? */
+	unsigned long nonlazy_posted;
+				    /* # times non-lazy CBs posted to CPU. */
+	unsigned long nonlazy_posted_snap;
+				    /* idle-period nonlazy_posted snapshot. */
+	unsigned long last_accelerate;
+				    /* Last jiffy CBs were accelerated. */
+	unsigned long last_advance_all;
+				    /* Last jiffy CBs were all advanced. */
+	int tick_nohz_enabled_snap; /* Previously seen value from sysfs. */
+#endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 
 	/* 4) reasons this CPU needed to be kicked by force_quiescent_state */
 	unsigned long dynticks_fqs;	/* Kicked due to dynticks idle. */

commit df63fa5bc11aadf81126d4a1785080c800e2ece3
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 31 09:49:20 2018 -0700

    rcu: Convert "1UL << x" to "BIT(x)"
    
    This commit saves a few characters by converting "1UL << x" to "BIT(x)".
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8f053bb1eec8..5e561f1339d4 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -168,7 +168,7 @@ struct rcu_node {
  * are indexed relative to this interval rather than the global CPU ID space.
  * This generates the bit for a CPU in node-local masks.
  */
-#define leaf_node_cpu_bit(rnp, cpu) (1UL << ((cpu) - (rnp)->grplo))
+#define leaf_node_cpu_bit(rnp, cpu) (BIT((cpu) - (rnp)->grplo))
 
 /*
  * Union to allow "aggregate OR" operation on the need for a quiescent

commit d3052109c0bc9e536d17d627ae628ed8ceb6928c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jul 25 11:49:47 2018 -0700

    rcu: More aggressively enlist scheduler aid for nohz_full CPUs
    
    Because nohz_full CPUs can leave the scheduler-clock interrupt disabled
    even when in kernel mode, RCU cannot rely on rcu_check_callbacks() to
    enlist the scheduler's aid in extracting a quiescent state from such CPUs.
    This commit therefore more aggressively uses resched_cpu() on nohz_full
    CPUs that fail to pass through a quiescent state in a timely manner.
    By default, the resched_cpu() beating starts 300 milliseconds into the
    quiescent state.
    
    While in the neighborhood, add a ->last_fqs_resched field to the rcu_data
    structure in order to rate-limit resched_cpu() calls from the RCU
    grace-period kthread.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 4866fa44ab0b..8f053bb1eec8 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -260,6 +260,7 @@ struct rcu_data {
 	short rcu_ofl_gp_flags;		/* ->gp_flags at last offline. */
 	unsigned long rcu_onl_gp_seq;	/* ->gp_seq at last online. */
 	short rcu_onl_gp_flags;		/* ->gp_flags at last online. */
+	unsigned long last_fqs_resched;	/* Time of last rcu_resched(). */
 
 	int cpu;
 };

commit 7e28c5af4ef6b539334aa5de40feca0c041c94df
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jul 11 08:09:28 2018 -0700

    rcu: Eliminate ->rcu_qs_ctr from the rcu_dynticks structure
    
    The ->rcu_qs_ctr counter was intended to allow providing a lightweight
    report of a quiescent state to all RCU flavors.  But now that there is
    only one flavor of RCU in any one running kernel, there is no point in
    having this feature.  This commit therefore removes the ->rcu_qs_ctr
    field from the rcu_dynticks structure and the ->rcu_qs_ctr_snap field
    from the rcu_data structure.  This results in the "rqc" option to the
    rcu_fqs trace event no longer being used, so this commit also removes the
    "rqc" description from the header comment.
    
    While in the neighborhood, this commit also causes the forward-progress
    request .rcu_need_heavy_qs be set one jiffies_till_sched_qs interval
    later in the grace period than the first setting of .rcu_urgent_qs.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8cf93ac277ec..4866fa44ab0b 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -42,7 +42,6 @@ struct rcu_dynticks {
 	long dynticks_nmi_nesting;  /* Track irq/NMI nesting level. */
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
 	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
-	unsigned long rcu_qs_ctr;   /* Light universal quiescent state ctr. */
 	bool rcu_urgent_qs;	    /* GP old need light quiescent state. */
 #ifdef CONFIG_RCU_FAST_NO_HZ
 	bool all_lazy;		    /* Are all CPU's CBs lazy? */
@@ -188,8 +187,6 @@ struct rcu_data {
 	/* 1) quiescent-state and grace-period handling : */
 	unsigned long	gp_seq;		/* Track rsp->rcu_gp_seq counter. */
 	unsigned long	gp_seq_needed;	/* Track rsp->rcu_gp_seq_needed ctr. */
-	unsigned long	rcu_qs_ctr_snap;/* Snapshot of rcu_qs_ctr to check */
-					/*  for rcu_all_qs() invocations. */
 	union rcu_noqs	cpu_no_qs;	/* No QSes yet for this CPU. */
 	bool		core_needs_qs;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */

commit dd46a7882c2c2006201e053ebf5e9ad761860cc0
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 10 18:37:30 2018 -0700

    rcu: Inline _rcu_barrier() into its sole remaining caller
    
    Because rcu_barrier() is a one-line wrapper function for _rcu_barrier()
    and because nothing else calls _rcu_barrier(), this commit inlines
    _rcu_barrier() into rcu_barrier().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 46452d3d0fad..8cf93ac277ec 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -222,7 +222,7 @@ struct rcu_data {
 					/* Grace period that needs help */
 					/*  from cond_resched(). */
 
-	/* 5) _rcu_barrier(), OOM callbacks, and expediting. */
+	/* 5) rcu_barrier(), OOM callbacks, and expediting. */
 	struct rcu_head barrier_head;
 	int exp_dynticks_snap;		/* Double-check need for IPI. */
 
@@ -328,7 +328,7 @@ struct rcu_state {
 	atomic_t barrier_cpu_count;		/* # CPUs waiting on. */
 	struct completion barrier_completion;	/* Wake at barrier end. */
 	unsigned long barrier_sequence;		/* ++ at start and end of */
-						/*  _rcu_barrier(). */
+						/*  rcu_barrier(). */
 	/* End of fields guarded by barrier_mutex. */
 
 	struct mutex exp_mutex;			/* Serialize expedited GP. */

commit 4e95020cdd34bbfc86f9c705f4d46ed63fa2e231
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jul 5 17:59:36 2018 -0700

    rcu: Inline increment_cpu_stall_ticks() into its sole caller
    
    Consolidation of the RCU flavors into one makes increment_cpu_stall_ticks()
    a trivial one-line function with only one caller.  This commit therefore
    inlines it.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8abc15c42d84..46452d3d0fad 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -466,7 +466,6 @@ static void print_cpu_stall_info_begin(void);
 static void print_cpu_stall_info(int cpu);
 static void print_cpu_stall_info_end(void);
 static void zero_cpu_stall_ticks(struct rcu_data *rdp);
-static void increment_cpu_stall_ticks(void);
 static bool rcu_nocb_cpu_needs_barrier(int cpu);
 static struct swait_queue_head *rcu_nocb_gp_get(struct rcu_node *rnp);
 static void rcu_nocb_gp_cleanup(struct swait_queue_head *sq);

commit b97d23c51c9fee56b0c7598c323ab2846d873f2d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jul 4 15:35:00 2018 -0700

    rcu: Remove for_each_rcu_flavor() flavor-traversal macro
    
    Now that there is only ever a single flavor of RCU in a given kernel
    build, there isn't a whole lot of point in having a flavor-traversal
    macro.  This commit therefore removes it and converts calls to it to
    straightline code, inlining trivial functions as appropriate.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6f1b1a3fc23d..8abc15c42d84 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -360,7 +360,6 @@ struct rcu_state {
 						/*  jiffies. */
 	const char *name;			/* Name of structure. */
 	char abbr;				/* Abbreviated name. */
-	struct list_head flavors;		/* List of RCU flavors. */
 
 	spinlock_t ofl_lock ____cacheline_internodealigned_in_smp;
 						/* Synchronize offline with */
@@ -417,12 +416,6 @@ static const char *tp_rcu_varname __used __tracepoint_string = rcu_name;
 #define RCU_NAME rcu_name
 #endif /* #else #ifdef CONFIG_TRACING */
 
-extern struct list_head rcu_struct_flavors;
-
-/* Sequence through rcu_state structures for each RCU flavor. */
-#define for_each_rcu_flavor(rsp) \
-	list_for_each_entry((rsp), &rcu_struct_flavors, flavors)
-
 /*
  * RCU implementation internal declarations:
  */

commit 88d1bead858d88cdda92ed8f3388eea8ee3a9675
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jul 4 14:45:00 2018 -0700

    rcu: Remove rcu_data structure's ->rsp field
    
    Now that there is only one rcu_state structure, there is no need for the
    rcu_data structure to indicate which it corresponds to.  This commit
    therefore removes the rcu_data structure's ->rsp field, replacing all
    remaining uses of it with &rcu_state.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index b21d79bdab23..6f1b1a3fc23d 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -265,7 +265,6 @@ struct rcu_data {
 	short rcu_onl_gp_flags;		/* ->gp_flags at last online. */
 
 	int cpu;
-	struct rcu_state *rsp;
 };
 
 /* Values for nocb_defer_wakeup field in struct rcu_data. */

commit 63d4c8c97948b0be8cb7ef3b7b943c25864eae4b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 17:22:34 2018 -0700

    rcu: Remove rsp parameter from expedited grace-period functions
    
    There now is only one rcu_state structure in a given build of the
    Linux kernel, so there is no need to pass it as a parameter to
    RCU's functions.  This commit therefore removes the rsp parameter
    from the code in kernel/rcu/tree_exp.h, and removes all of the
    rsp local variables while in the area.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7c6033d71e9d..b21d79bdab23 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -61,7 +61,6 @@ struct rcu_dynticks {
 /* Communicate arguments to a workqueue handler. */
 struct rcu_exp_work {
 	smp_call_func_t rew_func;
-	struct rcu_state *rew_rsp;
 	unsigned long rew_s;
 	struct work_struct rew_work;
 };

commit 4580b0541beac895a9ba9a4b6f60aec94355bfdd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 17:22:34 2018 -0700

    rcu: Remove rsp parameter from no-CBs CPU functions
    
    There now is only one rcu_state structure in a given build of the
    Linux kernel, so there is no need to pass it as a parameter to
    RCU's functions.  This commit therefore removes the rsp parameter
    from rcu_nocb_cpu_needs_barrier(), rcu_spawn_one_nocb_kthread(),
    rcu_organize_nocb_kthreads(), rcu_nocb_cpu_needs_barrier(), and
    rcu_nohz_full_cpu().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 2bf57de9f78a..7c6033d71e9d 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -476,7 +476,7 @@ static void print_cpu_stall_info(int cpu);
 static void print_cpu_stall_info_end(void);
 static void zero_cpu_stall_ticks(struct rcu_data *rdp);
 static void increment_cpu_stall_ticks(void);
-static bool rcu_nocb_cpu_needs_barrier(struct rcu_state *rsp, int cpu);
+static bool rcu_nocb_cpu_needs_barrier(int cpu);
 static struct swait_queue_head *rcu_nocb_gp_get(struct rcu_node *rnp);
 static void rcu_nocb_gp_cleanup(struct swait_queue_head *sq);
 static void rcu_init_one_nocb(struct rcu_node *rnp);
@@ -491,11 +491,11 @@ static void rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp);
 static void rcu_spawn_all_nocb_kthreads(int cpu);
 static void __init rcu_spawn_nocb_kthreads(void);
 #ifdef CONFIG_RCU_NOCB_CPU
-static void __init rcu_organize_nocb_kthreads(struct rcu_state *rsp);
+static void __init rcu_organize_nocb_kthreads(void);
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 static bool init_nocb_callback_list(struct rcu_data *rdp);
 static void rcu_bind_gp_kthread(void);
-static bool rcu_nohz_full_cpu(struct rcu_state *rsp);
+static bool rcu_nohz_full_cpu(void);
 static void rcu_dynticks_task_enter(void);
 static void rcu_dynticks_task_exit(void);
 

commit b21ebed951010acccbe9a55337d16cf4da4cce0a
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 17:22:34 2018 -0700

    rcu: Remove rsp parameter from print_cpu_stall_info()
    
    There now is only one rcu_state structure in a given build of the
    Linux kernel, so there is no need to pass it as a parameter to RCU's
    functions.  This commit therefore removes the rsp parameter from
    print_cpu_stall_info().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index dc1c337f6da9..2bf57de9f78a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -472,7 +472,7 @@ static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 static bool rcu_preempt_need_deferred_qs(struct task_struct *t);
 static void rcu_preempt_deferred_qs(struct task_struct *t);
 static void print_cpu_stall_info_begin(void);
-static void print_cpu_stall_info(struct rcu_state *rsp, int cpu);
+static void print_cpu_stall_info(int cpu);
 static void print_cpu_stall_info_end(void);
 static void zero_cpu_stall_ticks(struct rcu_data *rdp);
 static void increment_cpu_stall_ticks(void);

commit 6dbfdc1409cf07accf7c97475c3b58d46daa319b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 17:22:34 2018 -0700

    rcu: Remove rsp parameter from rcu_spawn_one_boost_kthread()
    
    There now is only one rcu_state structure in a given build of the
    Linux kernel, so there is no need to pass it as a parameter to RCU's
    functions.  This commit therefore removes the rsp parameter from
    rcu_spawn_one_boost_kthread().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index ccdee6bd3919..dc1c337f6da9 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -463,10 +463,6 @@ static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
 static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);
 static void invoke_rcu_callbacks_kthread(void);
 static bool rcu_is_callbacks_kthread(void);
-#ifdef CONFIG_RCU_BOOST
-static int rcu_spawn_one_boost_kthread(struct rcu_state *rsp,
-						 struct rcu_node *rnp);
-#endif /* #ifdef CONFIG_RCU_BOOST */
 static void __init rcu_spawn_boost_kthreads(void);
 static void rcu_prepare_kthreads(int cpu);
 static void rcu_cleanup_after_idle(void);

commit 81ab59a3ad8656620d7106e855085bc12dc13a4c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 17:22:34 2018 -0700

    rcu: Remove rsp parameter from dump_blkd_tasks() and friend
    
    There now is only one rcu_state structure in a given build of the
    Linux kernel, so there is no need to pass it as a parameter to RCU's
    functions.  This commit therefore removes the rsp parameter from
    dump_blkd_tasks() and rcu_preempt_blocked_readers_cgp().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 00d268cb4d04..ccdee6bd3919 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -455,12 +455,10 @@ static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 static void rcu_print_detail_task_stall(void);
 static int rcu_print_task_stall(struct rcu_node *rnp);
 static int rcu_print_task_exp_stall(struct rcu_node *rnp);
-static void rcu_preempt_check_blocked_tasks(struct rcu_state *rsp,
-					    struct rcu_node *rnp);
+static void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp);
 static void rcu_flavor_check_callbacks(int user);
 void call_rcu(struct rcu_head *head, rcu_callback_t func);
-static void dump_blkd_tasks(struct rcu_state *rsp, struct rcu_node *rnp,
-			    int ncheck);
+static void dump_blkd_tasks(struct rcu_node *rnp, int ncheck);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
 static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);
 static void invoke_rcu_callbacks_kthread(void);

commit a2887cd85f38cf2fdbf42bad97e5c412d99ff5ca
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 17:22:34 2018 -0700

    rcu: Remove rsp parameter from rcu_print_detail_task_stall()
    
    There now is only one rcu_state structure in a given build of the
    Linux kernel, so there is no need to pass it as a parameter to RCU's
    functions.  This commit therefore removes the rsp parameter from
    rcu_print_detail_task_stall().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d60304f1ef56..00d268cb4d04 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -452,7 +452,7 @@ static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
 #ifdef CONFIG_HOTPLUG_CPU
 static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 #endif /* #ifdef CONFIG_HOTPLUG_CPU */
-static void rcu_print_detail_task_stall(struct rcu_state *rsp);
+static void rcu_print_detail_task_stall(void);
 static int rcu_print_task_stall(struct rcu_node *rnp);
 static int rcu_print_task_exp_stall(struct rcu_node *rnp);
 static void rcu_preempt_check_blocked_tasks(struct rcu_state *rsp,

commit da1df50d16171f4c65da18093d5b5652423f5b99
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 15:37:16 2018 -0700

    rcu: Remove rcu_state structure's ->rda field
    
    The rcu_state structure's ->rda field was used to find the per-CPU
    rcu_data structures corresponding to that rcu_state structure.  But now
    there is only one rcu_state structure (creatively named "rcu_state")
    and one set of per-CPU rcu_data structures (creatively named "rcu_data").
    Therefore, uses of the ->rda field can always be replaced by "rcu_data,
    and this commit makes that change and removes the ->rda field.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index c50060567146..d60304f1ef56 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -312,7 +312,6 @@ struct rcu_state {
 	struct rcu_node *level[RCU_NUM_LVLS + 1];
 						/* Hierarchy levels (+1 to */
 						/*  shut bogus gcc warning) */
-	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */
 	int ncpus;				/* # CPUs seen so far. */
 
 	/* The following fields are guarded by the root rcu_node's lock. */

commit ec5dd444b678b1305d9af34ebb4cca17e0ef88e6
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 15:02:28 2018 -0700

    rcu: Eliminate rcu_state structure's ->call field
    
    The rcu_state structure's ->call field references the corresponding RCU
    flavor's call_rcu() function.  However, now that there is only ever one
    rcu_state structure in a given build of the Linux kernel, and that flavor
    uses plain old call_rcu(), there is not a lot of point in continuing to
    have the ->call field.  This commit therefore removes it.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 3f36562d3118..c50060567146 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -313,7 +313,6 @@ struct rcu_state {
 						/* Hierarchy levels (+1 to */
 						/*  shut bogus gcc warning) */
 	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */
-	call_rcu_func_t call;			/* call_rcu() flavor. */
 	int ncpus;				/* # CPUs seen so far. */
 
 	/* The following fields are guarded by the root rcu_node's lock. */

commit 358be2d3685cb0cca49c914e89824467ee0b589c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 14:15:31 2018 -0700

    rcu: Remove RCU_STATE_INITIALIZER()
    
    Now that a given build of the Linux kernel has only one set of rcu_state,
    rcu_node, and rcu_data structures, there is no point in creating a macro
    to declare and compile-time initialize them.  This commit therefore
    just does normal declaration and compile-time initialization of these
    structures.  While in the area, this commit also removes #ifndefs of
    the no-longer-ever-defined preprocessor macro RCU_TREE_NONCORE.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 38658ca87dcb..3f36562d3118 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -386,7 +386,6 @@ struct rcu_state {
 #define RCU_GP_CLEANUP   7	/* Grace-period cleanup started. */
 #define RCU_GP_CLEANED   8	/* Grace-period cleanup complete. */
 
-#ifndef RCU_TREE_NONCORE
 static const char * const gp_state_names[] = {
 	"RCU_GP_IDLE",
 	"RCU_GP_WAIT_GPS",
@@ -398,7 +397,29 @@ static const char * const gp_state_names[] = {
 	"RCU_GP_CLEANUP",
 	"RCU_GP_CLEANED",
 };
-#endif /* #ifndef RCU_TREE_NONCORE */
+
+/*
+ * In order to export the rcu_state name to the tracing tools, it
+ * needs to be added in the __tracepoint_string section.
+ * This requires defining a separate variable tp_<sname>_varname
+ * that points to the string being used, and this will allow
+ * the tracing userspace tools to be able to decipher the string
+ * address to the matching string.
+ */
+#ifdef CONFIG_PREEMPT_RCU
+#define RCU_ABBR 'p'
+#define RCU_NAME_RAW "rcu_preempt"
+#else /* #ifdef CONFIG_PREEMPT_RCU */
+#define RCU_ABBR 's'
+#define RCU_NAME_RAW "rcu_sched"
+#endif /* #else #ifdef CONFIG_PREEMPT_RCU */
+#ifndef CONFIG_TRACING
+#define RCU_NAME RCU_NAME_RAW
+#else /* #ifdef CONFIG_TRACING */
+static char rcu_name[] = RCU_NAME_RAW;
+static const char *tp_rcu_varname __used __tracepoint_string = rcu_name;
+#define RCU_NAME rcu_name
+#endif /* #else #ifdef CONFIG_TRACING */
 
 extern struct list_head rcu_struct_flavors;
 
@@ -426,8 +447,6 @@ DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_loops);
 DECLARE_PER_CPU(char, rcu_cpu_has_work);
 #endif /* #ifdef CONFIG_RCU_BOOST */
 
-#ifndef RCU_TREE_NONCORE
-
 /* Forward declarations for rcutree_plugin.h */
 static void rcu_bootup_announce(void);
 static void rcu_qs(void);
@@ -495,5 +514,3 @@ void srcu_offline_cpu(unsigned int cpu);
 void srcu_online_cpu(unsigned int cpu) { }
 void srcu_offline_cpu(unsigned int cpu) { }
 #endif /* #else #ifdef CONFIG_SRCU */
-
-#endif /* #ifndef RCU_TREE_NONCORE */

commit 45975c7d21a1c0aba97e3d8007e2a7c123145748
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jul 2 14:30:37 2018 -0700

    rcu: Define RCU-sched API in terms of RCU for Tree RCU PREEMPT builds
    
    Now that RCU-preempt knows about preemption disabling, its implementation
    of synchronize_rcu() works for synchronize_sched(), and likewise for the
    other RCU-sched update-side API members.  This commit therefore confines
    the RCU-sched update-side code to CONFIG_PREEMPT=n builds, and defines
    RCU-sched's update-side API members in terms of those of RCU-preempt.
    
    This means that any given build of the Linux kernel has only one
    update-side flavor of RCU, namely RCU-preempt for CONFIG_PREEMPT=y builds
    and RCU-sched for CONFIG_PREEMPT=n builds.  This in turn means that kernels
    built with CONFIG_RCU_NOCB_CPU=y have only one rcuo kthread per CPU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index e02c882861eb..38658ca87dcb 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -225,9 +225,6 @@ struct rcu_data {
 
 	/* 5) _rcu_barrier(), OOM callbacks, and expediting. */
 	struct rcu_head barrier_head;
-#ifdef CONFIG_RCU_FAST_NO_HZ
-	struct rcu_head oom_head;
-#endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 	int exp_dynticks_snap;		/* Double-check need for IPI. */
 
 	/* 6) Callback offloading. */
@@ -433,8 +430,7 @@ DECLARE_PER_CPU(char, rcu_cpu_has_work);
 
 /* Forward declarations for rcutree_plugin.h */
 static void rcu_bootup_announce(void);
-static void rcu_preempt_qs(void);
-static void rcu_preempt_note_context_switch(bool preempt);
+static void rcu_qs(void);
 static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
 #ifdef CONFIG_HOTPLUG_CPU
 static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
@@ -444,9 +440,8 @@ static int rcu_print_task_stall(struct rcu_node *rnp);
 static int rcu_print_task_exp_stall(struct rcu_node *rnp);
 static void rcu_preempt_check_blocked_tasks(struct rcu_state *rsp,
 					    struct rcu_node *rnp);
-static void rcu_preempt_check_callbacks(void);
+static void rcu_flavor_check_callbacks(int user);
 void call_rcu(struct rcu_head *head, rcu_callback_t func);
-static void __init __rcu_init_preempt(void);
 static void dump_blkd_tasks(struct rcu_state *rsp, struct rcu_node *rnp,
 			    int ncheck);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);

commit d28139c4e96713d52a300fb9036c5be2f45e0741
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 28 14:45:25 2018 -0700

    rcu: Apply RCU-bh QSes to RCU-sched and RCU-preempt when safe
    
    One necessary step towards consolidating the three flavors of RCU is to
    make sure that the resulting consolidated "one flavor to rule them all"
    correctly handles networking denial-of-service attacks.  One thing that
    allows RCU-bh to do so is that __do_softirq() invokes rcu_bh_qs() every
    so often, and so something similar has to happen for consolidated RCU.
    
    This must be done carefully.  For example, if a preemption-disabled
    region of code takes an interrupt which does softirq processing before
    returning, consolidated RCU must ignore the resulting rcu_bh_qs()
    invocations -- preemption is still disabled, and that means an RCU
    reader for the consolidated flavor.
    
    This commit therefore creates a new rcu_softirq_qs() that is called only
    from the ksoftirqd task, thus avoiding the interrupted-a-preempted-region
    problem.  This new rcu_softirq_qs() function invokes rcu_sched_qs(),
    rcu_preempt_qs(), and rcu_preempt_deferred_qs().  The latter call handles
    any deferred quiescent states.
    
    Note that __do_softirq() still invokes rcu_bh_qs().  It will continue to
    do so until a later stage of cleanup when the RCU-bh flavor is removed.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Fix !SMP issue located by kbuild test robot. ]

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 025bd2e5592b..e02c882861eb 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -433,6 +433,7 @@ DECLARE_PER_CPU(char, rcu_cpu_has_work);
 
 /* Forward declarations for rcutree_plugin.h */
 static void rcu_bootup_announce(void);
+static void rcu_preempt_qs(void);
 static void rcu_preempt_note_context_switch(bool preempt);
 static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
 #ifdef CONFIG_HOTPLUG_CPU

commit 3e31009898699dfca823893054748d85048dc7b3
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 21 12:50:01 2018 -0700

    rcu: Defer reporting RCU-preempt quiescent states when disabled
    
    This commit defers reporting of RCU-preempt quiescent states at
    rcu_read_unlock_special() time when any of interrupts, softirq, or
    preemption are disabled.  These deferred quiescent states are reported
    at a later RCU_SOFTIRQ, context switch, idle entry, or CPU-hotplug
    offline operation.  Of course, if another RCU read-side critical
    section has started in the meantime, the reporting of the quiescent
    state will be further deferred.
    
    This also means that disabling preemption, interrupts, and/or
    softirqs will act as an RCU-preempt read-side critical section.
    This is enforced by checking preempt_count() as needed.
    
    Some special cases must be handled on an ad-hoc basis, for example,
    context switch is a quiescent state even though both the scheduler and
    do_exit() disable preemption.  In these cases, additional calls to
    rcu_preempt_deferred_qs() override the preemption disabling.  Similar
    logic overrides disabled interrupts in rcu_preempt_check_callbacks()
    because in this case the quiescent state happened just before the
    corresponding scheduling-clock interrupt.
    
    In theory, this change lifts a long-standing restriction that required
    that if interrupts were disabled across a call to rcu_read_unlock()
    that the matching rcu_read_lock() also be contained within that
    interrupts-disabled region of code.  Because the reporting of the
    corresponding RCU-preempt quiescent state is now deferred until
    after interrupts have been enabled, it is no longer possible for this
    situation to result in deadlocks involving the scheduler's runqueue and
    priority-inheritance locks.  This may allow some code simplification that
    might reduce interrupt latency a bit.  Unfortunately, in practice this
    would also defer deboosting a low-priority task that had been subjected
    to RCU priority boosting, so real-time-response considerations might
    well force this restriction to remain in place.
    
    Because RCU-preempt grace periods are now blocked not only by RCU
    read-side critical sections, but also by disabling of interrupts,
    preemption, and softirqs, it will be possible to eliminate RCU-bh and
    RCU-sched in favor of RCU-preempt in CONFIG_PREEMPT=y kernels.  This may
    require some additional plumbing to provide the network denial-of-service
    guarantees that have been traditionally provided by RCU-bh.  Once these
    are in place, CONFIG_PREEMPT=n kernels will be able to fold RCU-bh
    into RCU-sched.  This would mean that all kernels would have but
    one flavor of RCU, which would open the door to significant code
    cleanup.
    
    Moving to a single flavor of RCU would also have the beneficial effect
    of reducing the NOCB kthreads by at least a factor of two.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Apply rcu_read_unlock_special() preempt_count() feedback
      from Joel Fernandes. ]
    [ paulmck: Adjust rcu_eqs_enter() call to rcu_preempt_deferred_qs() in
      response to bug reports from kbuild test robot. ]
    [ paulmck: Fix bug located by kbuild test robot involving recursion
      via rcu_preempt_deferred_qs(). ]

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 4e74df768c57..025bd2e5592b 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -195,6 +195,7 @@ struct rcu_data {
 	bool		core_needs_qs;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */
 	bool		gpwrap;		/* Possible ->gp_seq wrap. */
+	bool		deferred_qs;	/* This CPU awaiting a deferred QS? */
 	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
 	unsigned long grpmask;		/* Mask to apply to leaf qsmask. */
 	unsigned long	ticks_this_gp;	/* The number of scheduling-clock */
@@ -461,6 +462,8 @@ static void rcu_cleanup_after_idle(void);
 static void rcu_prepare_for_idle(void);
 static void rcu_idle_count_callbacks_posted(void);
 static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
+static bool rcu_preempt_need_deferred_qs(struct task_struct *t);
+static void rcu_preempt_deferred_qs(struct task_struct *t);
 static void print_cpu_stall_info_begin(void);
 static void print_cpu_stall_info(struct rcu_state *rsp, int cpu);
 static void print_cpu_stall_info_end(void);

commit 164ba3fc4864346dbc365f8b89d8888e1b6cd38c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 16 20:41:36 2018 -0700

    rcu: Remove unused rcu_kick_nohz_cpu() function
    
    The rcu_kick_nohz_cpu() function is no longer used, and the functionality
    it used to provide is now provided by a call to resched_cpu() in the
    force-quiescent-state function rcu_implicit_dynticks_qs().  This commit
    therefore removes rcu_kick_nohz_cpu().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d51e6edc8e83..4e74df768c57 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -483,7 +483,6 @@ static void __init rcu_spawn_nocb_kthreads(void);
 #ifdef CONFIG_RCU_NOCB_CPU
 static void __init rcu_organize_nocb_kthreads(struct rcu_state *rsp);
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
-static void __maybe_unused rcu_kick_nohz_cpu(int cpu);
 static bool init_nocb_callback_list(struct rcu_data *rdp);
 static void rcu_bind_gp_kthread(void);
 static bool rcu_nohz_full_cpu(struct rcu_state *rsp);

commit f2e2df59786d7bd52e6e7e2d10c1c6ba433a0ee7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 15 16:23:23 2018 -0700

    rcu: Add diagnostics for offline CPUs failing to report QS
    
    CPUs are expected to report quiescent states when coming online and
    when going offline, and grace-period initialization is supposed to
    handle any race conditions where a CPU's ->qsmask bit is set just after
    it goes offline.  This commit adds diagnostics for the case where an
    offline CPU nevertheless has a grace period waiting on it.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8077aff7ab40..d51e6edc8e83 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -90,6 +90,7 @@ struct rcu_node {
 				/*  an rcu_data structure, otherwise, each */
 				/*  bit corresponds to a child rcu_node */
 				/*  structure. */
+	unsigned long rcu_gp_init_mask;	/* Mask of offline CPUs at GP init. */
 	unsigned long qsmaskinit;
 				/* Per-GP initial value for qsmask. */
 				/*  Initialized from ->qsmaskinitnext at the */

commit fea3f222d3523dfdd0e86b11227d3cda20765102
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 15 15:47:30 2018 -0700

    rcu: Record ->gp_state for both phases of grace-period initialization
    
    Grace-period initialization first processes any recent CPU-hotplug
    operations, and then initializes state for the new grace period.  These
    two phases of initialization are currently not distinguished in debug
    prints, but the distinction is valuable in a number of debug situations.
    This commit therefore introduces two new values for ->gp_state,
    RCU_GP_ONOFF and RCU_GP_INIT, in order to make this distinction.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index f52bc059bfec..8077aff7ab40 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -380,16 +380,20 @@ struct rcu_state {
 #define RCU_GP_IDLE	 0	/* Initial state and no GP in progress. */
 #define RCU_GP_WAIT_GPS  1	/* Wait for grace-period start. */
 #define RCU_GP_DONE_GPS  2	/* Wait done for grace-period start. */
-#define RCU_GP_WAIT_FQS  3	/* Wait for force-quiescent-state time. */
-#define RCU_GP_DOING_FQS 4	/* Wait done for force-quiescent-state time. */
-#define RCU_GP_CLEANUP   5	/* Grace-period cleanup started. */
-#define RCU_GP_CLEANED   6	/* Grace-period cleanup complete. */
+#define RCU_GP_ONOFF     3	/* Grace-period initialization hotplug. */
+#define RCU_GP_INIT      4	/* Grace-period initialization. */
+#define RCU_GP_WAIT_FQS  5	/* Wait for force-quiescent-state time. */
+#define RCU_GP_DOING_FQS 6	/* Wait done for force-quiescent-state time. */
+#define RCU_GP_CLEANUP   7	/* Grace-period cleanup started. */
+#define RCU_GP_CLEANED   8	/* Grace-period cleanup complete. */
 
 #ifndef RCU_TREE_NONCORE
 static const char * const gp_state_names[] = {
 	"RCU_GP_IDLE",
 	"RCU_GP_WAIT_GPS",
 	"RCU_GP_DONE_GPS",
+	"RCU_GP_ONOFF",
+	"RCU_GP_INIT",
 	"RCU_GP_WAIT_FQS",
 	"RCU_GP_DOING_FQS",
 	"RCU_GP_CLEANUP",

commit 577389423187d8b51dfe6199297e579a3419b72b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 8 14:18:57 2018 -0700

    rcu: Add CPU online/offline state to dump_blkd_tasks()
    
    Interactions between CPU-hotplug operations and grace-period
    initialization can result in dump_blkd_tasks().  One of the first
    debugging actions in this case is to search back in dmesg to work
    out which of the affected rcu_node structure's CPUs are online and to
    determine the last CPU-hotplug operation affecting any of those CPUs.
    This can be laborious and error-prone, especially when console output
    is lost.
    
    This commit therefore causes dump_blkd_tasks() to dump the state of
    the affected rcu_node structure's CPUs and the last grace period during
    which the last offline and online operation affected each of these CPUs.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 795d469c6f67..f52bc059bfec 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -255,12 +255,16 @@ struct rcu_data {
 					/* Leader CPU takes GP-end wakeups. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 
-	/* 7) RCU CPU stall data. */
+	/* 7) Diagnostic data, including RCU CPU stall warnings. */
 	unsigned int softirq_snap;	/* Snapshot of softirq activity. */
 	/* ->rcu_iw* fields protected by leaf rcu_node ->lock. */
 	struct irq_work rcu_iw;		/* Check for non-irq activity. */
 	bool rcu_iw_pending;		/* Is ->rcu_iw pending? */
 	unsigned long rcu_iw_gp_seq;	/* ->gp_seq associated with ->rcu_iw. */
+	unsigned long rcu_ofl_gp_seq;	/* ->gp_seq at last offline. */
+	short rcu_ofl_gp_flags;		/* ->gp_flags at last offline. */
+	unsigned long rcu_onl_gp_seq;	/* ->gp_seq at last online. */
+	short rcu_onl_gp_flags;		/* ->gp_flags at last online. */
 
 	int cpu;
 	struct rcu_state *rsp;
@@ -431,11 +435,13 @@ static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 static void rcu_print_detail_task_stall(struct rcu_state *rsp);
 static int rcu_print_task_stall(struct rcu_node *rnp);
 static int rcu_print_task_exp_stall(struct rcu_node *rnp);
-static void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp);
+static void rcu_preempt_check_blocked_tasks(struct rcu_state *rsp,
+					    struct rcu_node *rnp);
 static void rcu_preempt_check_callbacks(void);
 void call_rcu(struct rcu_head *head, rcu_callback_t func);
 static void __init __rcu_init_preempt(void);
-static void dump_blkd_tasks(struct rcu_node *rnp, int ncheck);
+static void dump_blkd_tasks(struct rcu_state *rsp, struct rcu_node *rnp,
+			    int ncheck);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
 static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);
 static void invoke_rcu_callbacks_kthread(void);

commit e05121ba5b81e2f85349f038642410578457f6db
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon May 7 12:07:48 2018 -0700

    rcu: Remove CPU-hotplug failsafe from force-quiescent-state code path
    
    Now that quiescent states for newly offlined CPUs are reported either
    when that CPU goes offline or at the end of grace-period initialization,
    the CPU-hotplug failsafe in the force-quiescent-state code path is no
    longer needed.
    
    This commit therefore removes this failsafe.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6683da6e4ecc..795d469c6f67 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -217,7 +217,6 @@ struct rcu_data {
 
 	/* 4) reasons this CPU needed to be kicked by force_quiescent_state */
 	unsigned long dynticks_fqs;	/* Kicked due to dynticks idle. */
-	unsigned long offline_fqs;	/* Kicked due to being offline. */
 	unsigned long cond_resched_completed;
 					/* Grace period that needs help */
 					/*  from cond_resched(). */

commit 1e64b15a4b102e1cd059d4d798b7a78f93341333
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri May 25 19:23:09 2018 -0700

    rcu: Fix grace-period hangs due to race with CPU offline
    
    Without special fail-safe quiescent-state-propagation checks, grace-period
    hangs can result from the following scenario:
    
    1.      CPU 1 goes offline.
    
    2.      Because CPU 1 is the only CPU in the system blocking the current
            grace period, the grace period ends as soon as
            rcu_cleanup_dying_idle_cpu()'s call to rcu_report_qs_rnp()
            returns.
    
    3.      At this point, the leaf rcu_node structure's ->lock is no longer
            held: rcu_report_qs_rnp() has released it, as it must in order
            to awaken the RCU grace-period kthread.
    
    4.      At this point, that same leaf rcu_node structure's ->qsmaskinitnext
            field still records CPU 1 as being online.  This is absolutely
            necessary because the scheduler uses RCU (in this case on the
            wake-up path while awakening RCU's grace-period kthread), and
            ->qsmaskinitnext contains RCU's idea as to which CPUs are online.
            Therefore, invoking rcu_report_qs_rnp() after clearing CPU 1's
            bit from ->qsmaskinitnext would result in a lockdep-RCU splat
            due to RCU being used from an offline CPU.
    
    5.      RCU's grace-period kthread awakens, sees that the old grace period
            has completed and that a new one is needed.  It therefore starts
            a new grace period, but because CPU 1's leaf rcu_node structure's
            ->qsmaskinitnext field still shows CPU 1 as being online, this new
            grace period is initialized to wait for a quiescent state from the
            now-offline CPU 1.
    
    6.      Without the fail-safe force-quiescent-state checks, there would
            be no quiescent state from the now-offline CPU 1, which would
            eventually result in RCU CPU stall warnings and memory exhaustion.
    
    It would be good to get rid of the special fail-safe quiescent-state
    propagation checks, and thus it would be good to fix things so that
    the above scenario cannot happen.  This commit therefore adds a new
    ->ofl_lock to the rcu_state structure.  This lock is held by rcu_gp_init()
    across the applying of buffered online and offline operations to the
    rcu_node tree, and it is also held by rcu_cleanup_dying_idle_cpu()
    when buffering a new offline operation.  This prevents rcu_gp_init()
    from acquiring the leaf rcu_node structure's lock during the interval
    between when rcu_cleanup_dying_idle_cpu() invokes rcu_report_qs_rnp(),
    which releases ->lock and the re-acquisition of that same lock.
    This in turn prevents the failure scenario outlined above, and will
    hopefully eventually allow removal of the offline-CPU checks from the
    force-quiescent-state code path.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 3def94fc9c74..6683da6e4ecc 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -363,6 +363,10 @@ struct rcu_state {
 	const char *name;			/* Name of structure. */
 	char abbr;				/* Abbreviated name. */
 	struct list_head flavors;		/* List of RCU flavors. */
+
+	spinlock_t ofl_lock ____cacheline_internodealigned_in_smp;
+						/* Synchronize offline with */
+						/*  GP pre-initialization. */
 };
 
 /* Values for rcu_state structure's gp_flags field. */

commit ff3bb6f4d06247508489345ee90a8a9b6f3ffd3b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 1 14:34:08 2018 -0700

    rcu: Remove ->gpnum and ->completed
    
    Now that everything has been converted to use ->gp_seq instead of
    ->gpnum and ->completed, this commit removes ->gpnum and ->completed.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 9329c1ff695f..3def94fc9c74 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -81,12 +81,6 @@ struct rcu_node {
 	raw_spinlock_t __private lock;	/* Root rcu_node's lock protects */
 					/*  some rcu_state fields as well as */
 					/*  following. */
-	unsigned long gpnum;	/* Current grace period for this node. */
-				/*  This will either be equal to or one */
-				/*  behind the root rcu_node's gpnum. */
-	unsigned long completed; /* Last GP completed for this node. */
-				/*  This will either be equal to or one */
-				/*  behind the root rcu_node's gpnum. */
 	unsigned long gp_seq;	/* Track rsp->rcu_gp_seq. */
 	unsigned long gp_seq_needed; /* Track rsp->rcu_gp_seq_needed. */
 	unsigned long completedqs; /* All QSes done for this node. */
@@ -192,10 +186,6 @@ union rcu_noqs {
 /* Per-CPU data for read-copy update. */
 struct rcu_data {
 	/* 1) quiescent-state and grace-period handling : */
-	unsigned long	completed;	/* Track rsp->completed gp number */
-					/*  in order to detect GP end. */
-	unsigned long	gpnum;		/* Highest gp number that this CPU */
-					/*  is aware of having started. */
 	unsigned long	gp_seq;		/* Track rsp->rcu_gp_seq counter. */
 	unsigned long	gp_seq_needed;	/* Track rsp->rcu_gp_seq_needed ctr. */
 	unsigned long	rcu_qs_ctr_snap;/* Snapshot of rcu_qs_ctr to check */
@@ -203,7 +193,7 @@ struct rcu_data {
 	union rcu_noqs	cpu_no_qs;	/* No QSes yet for this CPU. */
 	bool		core_needs_qs;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */
-	bool		gpwrap;		/* Possible gpnum/completed wrap. */
+	bool		gpwrap;		/* Possible ->gp_seq wrap. */
 	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
 	unsigned long grpmask;		/* Mask to apply to leaf qsmask. */
 	unsigned long	ticks_this_gp;	/* The number of scheduling-clock */
@@ -328,8 +318,6 @@ struct rcu_state {
 
 	u8	boost ____cacheline_internodealigned_in_smp;
 						/* Subject to priority boost. */
-	unsigned long gpnum;			/* Current gp number. */
-	unsigned long completed;		/* # of last completed gp. */
 	unsigned long gp_seq;			/* Grace-period sequence #. */
 	struct task_struct *gp_kthread;		/* Task for grace periods. */
 	struct swait_queue_head gp_wq;		/* Where GP task waits. */

commit 7a1d0f23ad70cd4813bf4b72735ea2c26a4f53fe
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 1 10:26:57 2018 -0700

    rcu: Move from ->need_future_gp[] to ->gp_seq_needed
    
    One problem with the ->need_future_gp[] array is that the grace-period
    assignment of each element changes as the grace periods complete.
    This means that it is necessary to hold a lock when checking this
    array to learn if a given grace period has already been requested.
    This increase lock contention, which is the opposite of helpful.
    This commit therefore replaces the ->need_future_gp[] with a single
    ->gp_seq_needed value and keeps it updated in the rcu_data structure.
    
    This will enable reliable lockless checking of whether or not a given
    grace period has already been requested.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a21d403a6010..9329c1ff695f 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -88,6 +88,7 @@ struct rcu_node {
 				/*  This will either be equal to or one */
 				/*  behind the root rcu_node's gpnum. */
 	unsigned long gp_seq;	/* Track rsp->rcu_gp_seq. */
+	unsigned long gp_seq_needed; /* Track rsp->rcu_gp_seq_needed. */
 	unsigned long completedqs; /* All QSes done for this node. */
 	unsigned long qsmask;	/* CPUs or groups that need to switch in */
 				/*  order for current grace period to proceed.*/
@@ -160,7 +161,6 @@ struct rcu_node {
 	struct swait_queue_head nocb_gp_wq[2];
 				/* Place for rcu_nocb_kthread() to wait GP. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
-	u8 need_future_gp[4];	/* Counts of upcoming GP requests. */
 	raw_spinlock_t fqslock ____cacheline_internodealigned_in_smp;
 
 	spinlock_t exp_lock ____cacheline_internodealigned_in_smp;
@@ -170,22 +170,6 @@ struct rcu_node {
 	bool exp_need_flush;	/* Need to flush workitem? */
 } ____cacheline_internodealigned_in_smp;
 
-/* Accessors for ->need_future_gp[] array. */
-#define need_future_gp_mask() \
-	(ARRAY_SIZE(((struct rcu_node *)NULL)->need_future_gp) - 1)
-#define need_future_gp_element(rnp, c) \
-	((rnp)->need_future_gp[(c >> RCU_SEQ_CTR_SHIFT) & need_future_gp_mask()])
-#define need_any_future_gp(rnp)						\
-({									\
-	int __i;							\
-	bool __nonzero = false;						\
-									\
-	for (__i = 0; __i < ARRAY_SIZE((rnp)->need_future_gp); __i++)	\
-		__nonzero = __nonzero ||				\
-			    READ_ONCE((rnp)->need_future_gp[__i]);	\
-	__nonzero;							\
-})
-
 /*
  * Bitmasks in an rcu_node cover the interval [grplo, grphi] of CPU IDs, and
  * are indexed relative to this interval rather than the global CPU ID space.
@@ -213,6 +197,7 @@ struct rcu_data {
 	unsigned long	gpnum;		/* Highest gp number that this CPU */
 					/*  is aware of having started. */
 	unsigned long	gp_seq;		/* Track rsp->rcu_gp_seq counter. */
+	unsigned long	gp_seq_needed;	/* Track rsp->rcu_gp_seq_needed ctr. */
 	unsigned long	rcu_qs_ctr_snap;/* Snapshot of rcu_qs_ctr to check */
 					/*  for rcu_all_qs() invocations. */
 	union rcu_noqs	cpu_no_qs;	/* No QSes yet for this CPU. */

commit 29365e563b1e4e5bfde211280d37dc6127c019ed
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Apr 30 10:57:36 2018 -0700

    rcu: Convert grace-period requests to ->gp_seq
    
    This commit converts the grace-period request code paths from ->completed
    and ->gpnum to ->gp_seq.  The need_future_gp_element() macro encapsulates
    the shift operation required to use ->gp_seq as an index to the
    ->need_future_gp[] array.  The rcu_cbs_completed() function is removed
    in favor of the rcu_seq_snap() function.  The rcu_start_this_gp()
    gets some temporary consistency checks and uses rcu_seq_done(),
    rcu_seq_current(), rcu_seq_state(), and rcu_gp_in_progress() in place
    of the earlier open-coded comparisons of ->gpnum and ->completed.
    The rcu_future_gp_cleanup() function replaces use of ->completed
    with ->gp_seq.  The rcu_accelerate_cbs() function replaces a call to
    rcu_cbs_completed() with one to rcu_seq_snap().  The rcu_advance_cbs()
    function replaces an access to >completed with one to ->gp_seq and adds
    some temporary warnings.  The rcu_nocb_wait_gp() function replaces a
    call to rcu_cbs_completed() with one to rcu_seq_snap() and an open-coded
    comparison with rcu_seq_done().
    
    The temporary warnings will be removed when the various ->gpnum and
    ->completed fields are removed.  Their purpose is to locate code who
    might still be using ->gpnum and ->completed.  (Much easier that way
    than trying to trace down the causes of too-short grace periods and
    grace-period hangs!)
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6d6cbc8b3a9c..a21d403a6010 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -174,7 +174,7 @@ struct rcu_node {
 #define need_future_gp_mask() \
 	(ARRAY_SIZE(((struct rcu_node *)NULL)->need_future_gp) - 1)
 #define need_future_gp_element(rnp, c) \
-	((rnp)->need_future_gp[(c) & need_future_gp_mask()])
+	((rnp)->need_future_gp[(c >> RCU_SEQ_CTR_SHIFT) & need_future_gp_mask()])
 #define need_any_future_gp(rnp)						\
 ({									\
 	int __i;							\

commit 8aa670cdacc1820cb0597e4b4b413ef91ede2dd9
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Apr 28 14:15:40 2018 -0700

    rcu: Convert ->rcu_iw_gpnum to ->gp_seq
    
    This commit switches the interrupt-disabled detection mechanism to
    ->gp_seq.  This mechanism is used as part of RCU CPU stall warnings,
    and detects cases where the stall is due to a CPU having interrupts
    disabled.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 50a28d1cf5a1..6d6cbc8b3a9c 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -286,7 +286,7 @@ struct rcu_data {
 	/* ->rcu_iw* fields protected by leaf rcu_node ->lock. */
 	struct irq_work rcu_iw;		/* Check for non-irq activity. */
 	bool rcu_iw_pending;		/* Is ->rcu_iw pending? */
-	unsigned long rcu_iw_gpnum;	/* ->gpnum associated with ->rcu_iw. */
+	unsigned long rcu_iw_gp_seq;	/* ->gp_seq associated with ->rcu_iw. */
 
 	int cpu;
 	struct rcu_state *rsp;

commit de30ad512a668b56e7ad7a5a7c379d7c5d138a94
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 26 11:52:09 2018 -0700

    rcu: Introduce grace-period sequence numbers
    
    This commit adds grace-period sequence numbers (->gp_seq) to the
    rcu_state, rcu_node, and rcu_data structures, and updates them.
    It also checks for consistency between rsp->gpnum and rsp->gp_seq.
    These ->gp_seq counters will eventually replace the existing ->gpnum
    and ->completed counters, allowing a single memory access to determine
    whether or not a grace period is in progress and if so, which one.
    This in turn will enable changes that will reduce ->lock contention on
    the leaf rcu_node structures.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 3c1942174c56..50a28d1cf5a1 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -87,6 +87,7 @@ struct rcu_node {
 	unsigned long completed; /* Last GP completed for this node. */
 				/*  This will either be equal to or one */
 				/*  behind the root rcu_node's gpnum. */
+	unsigned long gp_seq;	/* Track rsp->rcu_gp_seq. */
 	unsigned long completedqs; /* All QSes done for this node. */
 	unsigned long qsmask;	/* CPUs or groups that need to switch in */
 				/*  order for current grace period to proceed.*/
@@ -211,6 +212,7 @@ struct rcu_data {
 					/*  in order to detect GP end. */
 	unsigned long	gpnum;		/* Highest gp number that this CPU */
 					/*  is aware of having started. */
+	unsigned long	gp_seq;		/* Track rsp->rcu_gp_seq counter. */
 	unsigned long	rcu_qs_ctr_snap;/* Snapshot of rcu_qs_ctr to check */
 					/*  for rcu_all_qs() invocations. */
 	union rcu_noqs	cpu_no_qs;	/* No QSes yet for this CPU. */
@@ -343,6 +345,7 @@ struct rcu_state {
 						/* Subject to priority boost. */
 	unsigned long gpnum;			/* Current gp number. */
 	unsigned long completed;		/* # of last completed gp. */
+	unsigned long gp_seq;			/* Grace-period sequence #. */
 	struct task_struct *gp_kthread;		/* Task for grace periods. */
 	struct swait_queue_head gp_wq;		/* Where GP task waits. */
 	short gp_flags;				/* Commands for GP task. */

commit 26d950a9451336a6b5abc1c8ca6c21df58e8d89f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Apr 21 20:44:11 2018 -0700

    rcu: Diagnostics for grace-period startup hangs
    
    This commit causes a splat if RCU is idle and a request for a new grace
    period is ignored for more than one second.  This splat normally indicates
    that some code path asked for a new grace period, but failed to wake up
    the RCU grace-period kthread.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Fix bug located by Dan Carpenter and his static checker. ]
    [ paulmck: Fix self-deadlock bug located 0day test robot. ]
    [ paulmck: Disable unless CONFIG_PROVE_RCU=y. ]

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7365ac53fdd9..3c1942174c56 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -374,6 +374,8 @@ struct rcu_state {
 						/*  but in jiffies. */
 	unsigned long gp_activity;		/* Time of last GP kthread */
 						/*  activity in jiffies. */
+	unsigned long gp_req_activity;		/* Time of last GP request */
+						/*  in jiffies. */
 	unsigned long jiffies_stall;		/* Time at which to check */
 						/*  for CPU stalls. */
 	unsigned long jiffies_resched;		/* Time at which to resched */

commit 4bc8d55574dd316e43975651b9259c5c18d741fc
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Nov 27 15:13:56 2017 -0800

    rcu: Add debugging info to assertion
    
    The WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp()) in
    rcu_gp_cleanup() triggers (inexplicably, of course) every so often.
    This commit therefore extracts more information.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 78e051dffc5b..7365ac53fdd9 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -87,6 +87,7 @@ struct rcu_node {
 	unsigned long completed; /* Last GP completed for this node. */
 				/*  This will either be equal to or one */
 				/*  behind the root rcu_node's gpnum. */
+	unsigned long completedqs; /* All QSes done for this node. */
 	unsigned long qsmask;	/* CPUs or groups that need to switch in */
 				/*  order for current grace period to proceed.*/
 				/*  In leaf rcu_node, each bit corresponds to */
@@ -453,6 +454,7 @@ static void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp);
 static void rcu_preempt_check_callbacks(void);
 void call_rcu(struct rcu_head *head, rcu_callback_t func);
 static void __init __rcu_init_preempt(void);
+static void dump_blkd_tasks(struct rcu_node *rnp, int ncheck);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
 static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);
 static void invoke_rcu_callbacks_kthread(void);

commit 22df7316ac71dc1ac57415349938737d2a229c59
Merge: 55ebfce0605a 628c08420bc6 c21b8b3ed0eb da1a1e192306
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 15 10:33:05 2018 -0700

    Merge branches 'exp.2018.05.15a', 'fixes.2018.05.15a', 'lock.2018.05.15a' and 'torture.2018.05.15a' into HEAD
    
    exp.2018.05.15a: Parallelize expedited grace-period initialization.
    fixes.2018.05.15a: Miscellaneous fixes.
    lock.2018.05.15a: Decrease lock contention on root rcu_node structure,
            which is a step towards merging RCU flavors.
    torture.2018.05.15a: Torture-test updates.

commit 6f576e281690316270275bbef17c79ea304ad511
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Apr 18 16:50:31 2018 -0700

    rcu: Convert ->need_future_gp[] array to boolean
    
    There is no longer any need for ->need_future_gp[] to count the number of
    requests for future grace periods, so this commit converts the additions
    to assignments to "true" and reduces the size of each element to one byte.
    While we are in the area, fix an obsolete comment.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 123c30eac8b5..9f97fd7f648c 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -150,7 +150,7 @@ struct rcu_node {
 	struct swait_queue_head nocb_gp_wq[2];
 				/* Place for rcu_nocb_kthread() to wait GP. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
-	int need_future_gp[4];	/* Counts of upcoming no-CB GP requests. */
+	u8 need_future_gp[4];	/* Counts of upcoming GP requests. */
 	raw_spinlock_t fqslock ____cacheline_internodealigned_in_smp;
 
 	spinlock_t exp_lock ____cacheline_internodealigned_in_smp;

commit 0ae94e00ce40e4447080ab7675220f725c690330
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Apr 18 14:14:42 2018 -0700

    rcu: Make rcu_future_needs_gp() check all ->need_future_gps[] elements
    
    Currently, the rcu_future_needs_gp() function checks only the current
    element of the ->need_future_gps[] array, which might miss elements that
    were offset from the expected element, for example, due to races with
    the start or the end of a grace period.  This commit therefore makes
    rcu_future_needs_gp() use the need_any_future_gp() macro to check all
    of the elements of this array.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 952cd0c223fe..123c30eac8b5 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -169,7 +169,8 @@ struct rcu_node {
 	bool __nonzero = false;						\
 									\
 	for (__i = 0; __i < ARRAY_SIZE((rnp)->need_future_gp); __i++)	\
-		__nonzero = __nonzero || (rnp)->need_future_gp[__i];	\
+		__nonzero = __nonzero ||				\
+			    READ_ONCE((rnp)->need_future_gp[__i]);	\
 	__nonzero;							\
 })
 

commit 51af970d19f395fc57b82514022126de6c5420cd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Apr 14 10:40:57 2018 -0700

    rcu: Avoid losing ->need_future_gp[] values due to GP start/end races
    
    The rcu_cbs_completed() function provides the value of ->completed
    at which new callbacks can safely be invoked.  This is recorded in
    two-element ->need_future_gp[] arrays in the rcu_node structure, and
    the elements of these arrays corresponding to the just-completed grace
    period are zeroed at the end of that grace period.  However, the
    rcu_cbs_completed() function can return the current ->completed value
    plus either one or two, so it is possible for the corresponding
    ->need_future_gp[] entry to be cleared just after it was set, thus
    losing a request for a future grace period.
    
    This commit avoids this race by expanding ->need_future_gp[] to four
    elements.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index bd1103763551..952cd0c223fe 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -150,8 +150,7 @@ struct rcu_node {
 	struct swait_queue_head nocb_gp_wq[2];
 				/* Place for rcu_nocb_kthread() to wait GP. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
-	int need_future_gp[2];
-				/* Counts of upcoming no-CB GP requests. */
+	int need_future_gp[4];	/* Counts of upcoming no-CB GP requests. */
 	raw_spinlock_t fqslock ____cacheline_internodealigned_in_smp;
 
 	spinlock_t exp_lock ____cacheline_internodealigned_in_smp;

commit fb31340f8a43a6f2e871164822ef4979b36232ae
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 12 07:20:30 2018 -0700

    rcu: Make rcu_gp_cleanup() more accurately predict need for new GP
    
    Currently, rcu_gp_cleanup() scans the rcu_node tree in order to reset
    state to reflect the end of the grace period.  It also checks to see
    whether a new grace period is needed, but in a number of cases, rather
    than directly cause the new grace period to be immediately started, it
    instead leaves the grace-period-needed state where various fail-safes
    can find it.  This works fine, but results in higher contention on the
    root rcu_node structure's ->lock, which is undesirable, and contention
    on that lock has recently become noticeable.
    
    This commit therefore makes rcu_gp_cleanup() immediately start a new
    grace period if there is any need for one.
    
    It is quite possible that it will later be necessary to throttle the
    grace-period rate, but that can be dealt with when and if.
    
    Reported-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 18b091474ffa..bd1103763551 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -469,7 +469,6 @@ static void print_cpu_stall_info_end(void);
 static void zero_cpu_stall_ticks(struct rcu_data *rdp);
 static void increment_cpu_stall_ticks(void);
 static bool rcu_nocb_cpu_needs_barrier(struct rcu_state *rsp, int cpu);
-static void rcu_nocb_gp_set(struct rcu_node *rnp, int nrq);
 static struct swait_queue_head *rcu_nocb_gp_get(struct rcu_node *rnp);
 static void rcu_nocb_gp_cleanup(struct swait_queue_head *sq);
 static void rcu_init_one_nocb(struct rcu_node *rnp);

commit c91a8675b9cc697c725b6d97fcc7f157f4a989d0
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Apr 18 11:11:39 2018 -0700

    rcu: Add accessor macros for the ->need_future_gp[] array
    
    Accessors for the ->need_future_gp[] array are currently open-coded,
    which makes them difficult to change.  To improve maintainability, this
    commit adds need_future_gp_mask() to compute the indexing mask from the
    array size, need_future_gp_element() to access the element corresponding
    to the specified grace-period number, and need_any_future_gp() to
    determine if any future grace period is needed.  This commit also applies
    need_future_gp_element() to existing open-coded single-element accesses.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index f491ab4f2e8e..18b091474ffa 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -159,6 +159,21 @@ struct rcu_node {
 	wait_queue_head_t exp_wq[4];
 } ____cacheline_internodealigned_in_smp;
 
+/* Accessors for ->need_future_gp[] array. */
+#define need_future_gp_mask() \
+	(ARRAY_SIZE(((struct rcu_node *)NULL)->need_future_gp) - 1)
+#define need_future_gp_element(rnp, c) \
+	((rnp)->need_future_gp[(c) & need_future_gp_mask()])
+#define need_any_future_gp(rnp)						\
+({									\
+	int __i;							\
+	bool __nonzero = false;						\
+									\
+	for (__i = 0; __i < ARRAY_SIZE((rnp)->need_future_gp); __i++)	\
+		__nonzero = __nonzero || (rnp)->need_future_gp[__i];	\
+	__nonzero;							\
+})
+
 /*
  * Bitmasks in an rcu_node cover the interval [grplo, grphi] of CPU IDs, and
  * are indexed relative to this interval rather than the global CPU ID space.

commit 17672480fb1e953f999623b598a98130f8aacfbc
Author: Yury Norov <ynorov@caviumnetworks.com>
Date:   Sun Mar 25 20:50:03 2018 +0300

    rcu: Declare rcu_eqs_special_set() in public header
    
    Because rcu_eqs_special_set() is declared only in internal header
    kernel/rcu/tree.h and stubbed in include/linux/rcutiny.h, it is
    inaccessible outside of the RCU implementation.  This patch therefore
    moves the  rcu_eqs_special_set() declaration to include/linux/rcutree.h,
    which allows it to be used in non-rcu kernel code.
    
    Signed-off-by: Yury Norov <ynorov@caviumnetworks.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 5fd374c71404..0b3a90ebe225 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -404,7 +404,6 @@ extern struct rcu_state rcu_preempt_state;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 
 int rcu_dynticks_snap(struct rcu_dynticks *rdtp);
-bool rcu_eqs_special_set(int cpu);
 
 #ifdef CONFIG_RCU_BOOST
 DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_status);

commit 6fba2b3767ea6e3e1204855031492415cc4dce4f
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Fri Mar 2 16:39:12 2018 +0900

    rcu: Remove deprecated RCU debugfs tracing code
    
    Commit ae91aa0adb14 ("rcu: Remove debugfs tracing") removed the
    RCU debugfs tracing code, but did not remove the no-longer used
    ->exp_workdone{0,1,2,3} fields in the srcu_data structure.  This commit
    therefore removes these fields along with the code that uselessly
    updates them.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 3a0dc30100e8..5fd374c71404 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -224,10 +224,6 @@ struct rcu_data {
 #ifdef CONFIG_RCU_FAST_NO_HZ
 	struct rcu_head oom_head;
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
-	atomic_long_t exp_workdone0;	/* # done by workqueue. */
-	atomic_long_t exp_workdone1;	/* # done by others #1. */
-	atomic_long_t exp_workdone2;	/* # done by others #2. */
-	atomic_long_t exp_workdone3;	/* # done by others #3. */
 	int exp_dynticks_snap;		/* Double-check need for IPI. */
 
 	/* 6) Callback offloading. */

commit be01b4cab118e7a2d9747c71b3bef8f28fdda193
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Feb 26 14:11:36 2018 +0900

    rcu: Inline rcu_preempt_do_callback() into its sole caller
    
    The rcu_preempt_do_callbacks() function was introduced in commit
    09223371dea(rcu: Use softirq to address performance regression), where it
    was necessary to handle kernel builds both containing and not containing
    RCU-preempt.  Since then, various changes (most notably f8b7fc6b51
    ("rcu: use softirq instead of kthreads except when RCU_BOOST=y")) have
    resulted in this function being invoked only from rcu_kthread_do_work(),
    which is present only in kernels containing RCU-preempt, which in turn
    means that the rcu_preempt_do_callbacks() function is no longer needed.
    
    This commit therefore inlines rcu_preempt_do_callbacks() into its
    sole remaining caller and also removes the rcu_state_p and rcu_data_p
    indirection for added clarity.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    [ paulmck: Remove the rcu_state_p and rcu_data_p indirection. ]
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index f491ab4f2e8e..3a0dc30100e8 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -438,7 +438,6 @@ static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);
 static void invoke_rcu_callbacks_kthread(void);
 static bool rcu_is_callbacks_kthread(void);
 #ifdef CONFIG_RCU_BOOST
-static void rcu_preempt_do_callbacks(void);
 static int rcu_spawn_one_boost_kthread(struct rcu_state *rsp,
 						 struct rcu_node *rnp);
 #endif /* #ifdef CONFIG_RCU_BOOST */

commit 25f3d7effab632eb10d145f1a5aebf6515a04b98
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Feb 1 22:05:38 2018 -0800

    rcu: Parallelize expedited grace-period initialization
    
    The latency of RCU expedited grace periods grows with increasing numbers
    of CPUs, eventually failing to be all that expedited.  Much of the growth
    in latency is in the initialization phase, so this commit uses workqueues
    to carry out this initialization concurrently on a rcu_node-by-rcu_node
    basis.
    
    This change makes use of a new rcu_par_gp_wq because flushing a work
    item from another work item running from the same workqueue can result
    in deadlock.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index f491ab4f2e8e..98d33902b65c 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -58,6 +58,14 @@ struct rcu_dynticks {
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 };
 
+/* Communicate arguments to a workqueue handler. */
+struct rcu_exp_work {
+	smp_call_func_t rew_func;
+	struct rcu_state *rew_rsp;
+	unsigned long rew_s;
+	struct work_struct rew_work;
+};
+
 /* RCU's kthread states for tracing. */
 #define RCU_KTHREAD_STOPPED  0
 #define RCU_KTHREAD_RUNNING  1
@@ -157,6 +165,8 @@ struct rcu_node {
 	spinlock_t exp_lock ____cacheline_internodealigned_in_smp;
 	unsigned long exp_seq_rq;
 	wait_queue_head_t exp_wq[4];
+	struct rcu_exp_work rew;
+	bool exp_need_flush;	/* Need to flush workitem? */
 } ____cacheline_internodealigned_in_smp;
 
 /*

commit 65518db86b9ed1180b013c8a34c73c6ff7275886
Author: Liu, Changcheng <changcheng.liu@intel.com>
Date:   Tue Jan 16 17:05:14 2018 +0800

    rcu: Remove redundant nxttail index macro define
    
    RCU's nxttail has been optimized to be a rcu_segcblist, which is
    a multi-tailed linked list with macros defined for the indexes for
    each tail.  The indexes have been defined in linux/rcu_segcblist.h,
    so this commit removes the redundant definitions in kernel/rcu/tree.h.
    
    Signed-off-by: Liu Changcheng <changcheng.liu@intel.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 2edd56397b00..f491ab4f2e8e 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -178,13 +178,6 @@ union rcu_noqs {
 	u16 s; /* Set of bits, aggregate OR here. */
 };
 
-/* Index values for nxttail array in struct rcu_data. */
-#define RCU_DONE_TAIL		0	/* Also RCU_WAIT head. */
-#define RCU_WAIT_TAIL		1	/* Also RCU_NEXT_READY head. */
-#define RCU_NEXT_READY_TAIL	2	/* Also RCU_NEXT head. */
-#define RCU_NEXT_TAIL		3
-#define RCU_NEXT_SIZE		4
-
 /* Per-CPU data for read-copy update. */
 struct rcu_data {
 	/* 1) quiescent-state and grace-period handling : */

commit d62df57370a5215fbf1b088d2ee51fa5d69bd0c3
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 10 13:10:49 2018 -0800

    rcu: Remove obsolete force-quiescent-state statistics for debugfs
    
    The debugfs interface displayed statistics on RCU-pending checks but
    this interface has since been removed.  This commit therefore removes the
    no-longer-used rcu_state structure's ->n_force_qs_lh and ->n_force_qs_ngp
    fields along with their updates.  (Though the ->n_force_qs_ngp field
    was actually not used at all, embarrassingly enough.)
    
    If this information proves necessary in the future, the corresponding
    event traces will be added.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d29bab8dea28..2edd56397b00 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -355,10 +355,6 @@ struct rcu_state {
 						/*  kthreads, if configured. */
 	unsigned long n_force_qs;		/* Number of calls to */
 						/*  force_quiescent_state(). */
-	unsigned long n_force_qs_lh;		/* ~Number of calls leaving */
-						/*  due to lock unavailable. */
-	unsigned long n_force_qs_ngp;		/* Number of calls leaving */
-						/*  due to no GP active. */
 	unsigned long gp_start;			/* Time at which GP started, */
 						/*  but in jiffies. */
 	unsigned long gp_activity;		/* Time of last GP kthread */

commit 01c495f72a3b5a210e5689deba1ef33c82e8aa30
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 10 12:36:00 2018 -0800

    rcu: Remove obsolete __rcu_pending() statistics for debugfs
    
    The debugfs interface displayed statistics on RCU-pending checks
    but this interface has since been removed.  This commit therefore
    removes the no-longer-used rcu_data structure's ->n_rcu_pending,
    ->n_rp_core_needs_qs, ->n_rp_report_qs, ->n_rp_cb_ready,
    ->n_rp_cpu_needs_gp, ->n_rp_gp_completed, ->n_rp_gp_started,
    ->n_rp_nocb_defer_wakeup, and ->n_rp_need_nothing fields along with
    their updates.
    
    If this information proves necessary in the future, the corresponding
    event traces will be added.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index b258fac73524..d29bab8dea28 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -226,18 +226,7 @@ struct rcu_data {
 					/* Grace period that needs help */
 					/*  from cond_resched(). */
 
-	/* 5) __rcu_pending() statistics. */
-	unsigned long n_rcu_pending;	/* rcu_pending() calls since boot. */
-	unsigned long n_rp_core_needs_qs;
-	unsigned long n_rp_report_qs;
-	unsigned long n_rp_cb_ready;
-	unsigned long n_rp_cpu_needs_gp;
-	unsigned long n_rp_gp_completed;
-	unsigned long n_rp_gp_started;
-	unsigned long n_rp_nocb_defer_wakeup;
-	unsigned long n_rp_need_nothing;
-
-	/* 6) _rcu_barrier(), OOM callbacks, and expediting. */
+	/* 5) _rcu_barrier(), OOM callbacks, and expediting. */
 	struct rcu_head barrier_head;
 #ifdef CONFIG_RCU_FAST_NO_HZ
 	struct rcu_head oom_head;
@@ -248,7 +237,7 @@ struct rcu_data {
 	atomic_long_t exp_workdone3;	/* # done by others #3. */
 	int exp_dynticks_snap;		/* Double-check need for IPI. */
 
-	/* 7) Callback offloading. */
+	/* 6) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU
 	struct rcu_head *nocb_head;	/* CBs waiting for kthread. */
 	struct rcu_head **nocb_tail;
@@ -275,7 +264,7 @@ struct rcu_data {
 					/* Leader CPU takes GP-end wakeups. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 
-	/* 8) RCU CPU stall data. */
+	/* 7) RCU CPU stall data. */
 	unsigned int softirq_snap;	/* Snapshot of softirq activity. */
 	/* ->rcu_iw* fields protected by leaf rcu_node ->lock. */
 	struct irq_work rcu_iw;		/* Check for non-irq activity. */

commit 62df63e048daf4e29373bbbaae3751e5af5d9502
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 10 12:21:40 2018 -0800

    rcu: Remove obsolete callback-invocation statistics for debugfs
    
    The debugfs interface displayed statistics on RCU callback invocation but
    this interface has since been removed.  This commit therefore removes the
    no-longer-used rcu_data structure's ->n_cbs_invoked and ->n_nocbs_invoked
    fields along with their updates.
    
    If this information proves necessary in the future, the corresponding
    event traces will be added.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 01abd1c4e5da..b258fac73524 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -211,8 +211,6 @@ struct rcu_data {
 					/* different grace periods. */
 	long		qlen_last_fqs_check;
 					/* qlen at last check for QS forcing */
-	unsigned long	n_cbs_invoked;	/* count of RCU cbs invoked. */
-	unsigned long	n_nocbs_invoked; /* count of no-CBs RCU cbs invoked. */
 	unsigned long	n_force_qs_snap;
 					/* did other CPU force QS recently? */
 	long		blimit;		/* Upper limit on a processed batch */

commit bec06785fe2c866ccf39cafa18935d88d77d559a
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 10 12:16:42 2018 -0800

    rcu: Remove obsolete boost statistics for debugfs
    
    The debugfs interface displayed statistics on RCU priority boosting,
    but this interface has since been removed.  This commit therefore
    removes the no-longer-used rcu_data structure's ->n_tasks_boosted,
    ->n_exp_boosts, and ->n_exp_boosts and their updates.
    
    If this information proves necessary in the future, the corresponding
    event traces will be added.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6488a3b0e729..01abd1c4e5da 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -146,12 +146,6 @@ struct rcu_node {
 				/*  boosting for this rcu_node structure. */
 	unsigned int boost_kthread_status;
 				/* State of boost_kthread_task for tracing. */
-	unsigned long n_tasks_boosted;
-				/* Total number of tasks boosted. */
-	unsigned long n_exp_boosts;
-				/* Number of tasks boosted for expedited GP. */
-	unsigned long n_normal_boosts;
-				/* Number of tasks boosted for normal GP. */
 #ifdef CONFIG_RCU_NOCB_CPU
 	struct swait_queue_head nocb_gp_wq[2];
 				/* Place for rcu_nocb_kthread() to wait GP. */

commit 84585aa8b6ad24e5bdfba9db4a320a6aeed192ab
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Oct 4 15:55:16 2017 -0700

    rcu: Shrink ->dynticks_{nmi_,}nesting from long long to long
    
    Because the ->dynticks_nesting field now only contains the process-based
    nesting level instead of a value encoding both the process nesting level
    and the irq "nesting" level, we no longer need a long long, even on
    32-bit systems.  This commit therefore changes both the ->dynticks_nesting
    and ->dynticks_nmi_nesting fields to long.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index dbd7e3753bed..6488a3b0e729 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -38,8 +38,8 @@
  * Dynticks per-CPU state.
  */
 struct rcu_dynticks {
-	long long dynticks_nesting; /* Track irq/process nesting level. */
-	int dynticks_nmi_nesting;   /* Track NMI nesting level. */
+	long dynticks_nesting;      /* Track process nesting level. */
+	long dynticks_nmi_nesting;  /* Track irq/NMI nesting level. */
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
 	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
 	unsigned long rcu_qs_ctr;   /* Light universal quiescent state ctr. */

commit 51a1fd30f13090be7750fed86cf3728afaf4e394
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 3 14:43:40 2017 -0700

    rcu: Make ->dynticks_nesting be a simple counter
    
    Now that ->dynticks_nesting counts only process-level dyntick-idle
    entry and exit, there is no need for the elaborate segmented counter
    with its guard fields and overflow checking.  This commit therefore
    makes ->dynticks_nesting be a simple counter.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 46a5d1991450..dbd7e3753bed 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -39,7 +39,6 @@
  */
 struct rcu_dynticks {
 	long long dynticks_nesting; /* Track irq/process nesting level. */
-				    /* Process level is worth LLONG_MAX/2. */
 	int dynticks_nmi_nesting;   /* Track NMI nesting level. */
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
 	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */

commit 9b9500da81502738efa1b485a8835f174ff7be6d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 17 17:05:59 2017 -0700

    rcu: Make RCU CPU stall warnings check for irq-disabled CPUs
    
    One common question upon seeing an RCU CPU stall warning is "did
    the stalled CPUs have interrupts disabled?"  However, the current
    stall warnings are silent on this point.  This commit therefore
    uses irq_work to check whether stalled CPUs still respond to IPIs,
    and flags this state in the RCU CPU stall warning console messages.
    
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8e1f285f0a70..46a5d1991450 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -103,6 +103,7 @@ struct rcu_node {
 				/* Online CPUs for next expedited GP. */
 				/*  Any CPU that has ever been online will */
 				/*  have its bit set. */
+	unsigned long ffmask;	/* Fully functional CPUs. */
 	unsigned long grpmask;	/* Mask to apply to parent qsmask. */
 				/*  Only one bit will be set in this mask. */
 	int	grplo;		/* lowest-numbered CPU or group here. */
@@ -285,6 +286,10 @@ struct rcu_data {
 
 	/* 8) RCU CPU stall data. */
 	unsigned int softirq_snap;	/* Snapshot of softirq activity. */
+	/* ->rcu_iw* fields protected by leaf rcu_node ->lock. */
+	struct irq_work rcu_iw;		/* Check for non-irq activity. */
+	bool rcu_iw_pending;		/* Is ->rcu_iw pending? */
+	unsigned long rcu_iw_gpnum;	/* ->gpnum associated with ->rcu_iw. */
 
 	int cpu;
 	struct rcu_state *rsp;

commit 656e7c0c0a2e8d899f87fd7f081ea7a711146604
Merge: 850bf6d59265 16c0b106070f 09efeeee173e 22e4ebb97582 952111d7db02 35732cf9dd38 f34c8585ed70
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 17 08:10:04 2017 -0700

    Merge branches 'doc.2017.08.17a', 'fixes.2017.08.17a', 'hotplug.2017.07.25b', 'misc.2017.08.17a', 'spin_unlock_wait_no.2017.08.17a', 'srcu.2017.07.27c' and 'torture.2017.07.24c' into HEAD
    
    doc.2017.08.17a: Documentation updates.
    fixes.2017.08.17a: RCU fixes.
    hotplug.2017.07.25b: CPU-hotplug updates.
    misc.2017.08.17a: Miscellaneous fixes outside of RCU (give or take conflicts).
    spin_unlock_wait_no.2017.08.17a: Remove spin_unlock_wait().
    srcu.2017.07.27c: SRCU updates.
    torture.2017.07.24c: Torture-test updates.

commit f2dbe4a562d4f17cc1bad3e36a9d1ccb19c86604
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 27 07:44:06 2017 -0700

    rcu: Localize rcu_state ->orphan_pend and ->orphan_done
    
    Given that the rcu_state structure's >orphan_pend and ->orphan_done
    fields are used only during migration of callbacks from the recently
    offlined CPU to a surviving CPU, if rcu_send_cbs_to_orphanage() and
    rcu_adopt_orphan_cbs() are combined, these fields can become local
    variables in the combined function.  This commit therefore combines
    rcu_send_cbs_to_orphanage() and rcu_adopt_orphan_cbs() into a new
    rcu_segcblist_merge() function and removes the ->orphan_pend and
    ->orphan_done fields.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 62b1d0b0d47c..b99f31c2b0c3 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -348,12 +348,6 @@ struct rcu_state {
 
 	/* End of fields guarded by root rcu_node's lock. */
 
-	struct rcu_cblist orphan_pend;		/* Orphaned callbacks that */
-						/*  need a grace period. */
-	struct rcu_cblist orphan_done;		/* Orphaned callbacks that */
-						/*  are ready to invoke. */
-						/* (Contains counts.) */
-
 	struct mutex barrier_mutex;		/* Guards barrier fields. */
 	atomic_t barrier_cpu_count;		/* # CPUs waiting on. */
 	struct completion barrier_completion;	/* Wake at barrier end. */

commit 537b85c870babacc1cf13235e92bee9de86210e0
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jun 26 17:59:02 2017 -0700

    rcu: Eliminate rcu_state ->orphan_lock
    
    The ->orphan_lock is acquired and released only within the
    rcu_migrate_callbacks() function, which now acquires the root rcu_node
    structure's ->lock.  This commit therefore eliminates the ->orphan_lock
    in favor of the root rcu_node structure's ->lock.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 574513cf49b4..62b1d0b0d47c 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -348,14 +348,11 @@ struct rcu_state {
 
 	/* End of fields guarded by root rcu_node's lock. */
 
-	raw_spinlock_t orphan_lock ____cacheline_internodealigned_in_smp;
-						/* Protect following fields. */
 	struct rcu_cblist orphan_pend;		/* Orphaned callbacks that */
 						/*  need a grace period. */
 	struct rcu_cblist orphan_done;		/* Orphaned callbacks that */
 						/*  are ready to invoke. */
 						/* (Contains counts.) */
-	/* End of fields guarded by orphan_lock. */
 
 	struct mutex barrier_mutex;		/* Guards barrier fields. */
 	atomic_t barrier_cpu_count;		/* # CPUs waiting on. */

commit b1a2d79fe7d210c114003362d93d529912d244df
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jun 26 12:23:46 2017 -0700

    rcu: Make NOCB CPUs migrate CBs directly from outgoing CPU
    
    RCU's CPU-hotplug callback-migration code first moves the outgoing
    CPU's callbacks to ->orphan_done and ->orphan_pend, and only then
    moves them to the NOCB callback list.  This commit avoids the
    extra step (and simplifies the code) by moving the callbacks directly
    from the outgoing CPU's callback list to the NOCB callback list.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index aec53c4d4aec..574513cf49b4 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -493,7 +493,7 @@ static void rcu_nocb_gp_cleanup(struct swait_queue_head *sq);
 static void rcu_init_one_nocb(struct rcu_node *rnp);
 static bool __call_rcu_nocb(struct rcu_data *rdp, struct rcu_head *rhp,
 			    bool lazy, unsigned long flags);
-static bool rcu_nocb_adopt_orphan_cbs(struct rcu_state *rsp,
+static bool rcu_nocb_adopt_orphan_cbs(struct rcu_data *my_rdp,
 				      struct rcu_data *rdp,
 				      unsigned long flags);
 static int rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp);

commit c47e067a3c57835fe5ce24d50482f5c325a64efd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jun 23 20:30:08 2017 -0700

    rcu: Remove orphan/adopt event-tracing fields
    
    The rcu_node structure's ->n_cbs_orphaned and ->n_cbs_adopted fields
    are updated, but never read.  This commit therefore removes them.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 9af0f31d6847..aec53c4d4aec 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -219,8 +219,6 @@ struct rcu_data {
 					/* qlen at last check for QS forcing */
 	unsigned long	n_cbs_invoked;	/* count of RCU cbs invoked. */
 	unsigned long	n_nocbs_invoked; /* count of no-CBs RCU cbs invoked. */
-	unsigned long   n_cbs_orphaned; /* RCU cbs orphaned by dying CPU */
-	unsigned long   n_cbs_adopted;  /* RCU cbs adopted from dying CPU */
 	unsigned long	n_force_qs_snap;
 					/* did other CPU force QS recently? */
 	long		blimit;		/* Upper limit on a processed batch */

commit 8be6e1b15c54402106e6ba9bc706e685458b2d2d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Apr 29 20:03:20 2017 -0700

    rcu: Use timer as backstop for NOCB deferred wakeups
    
    The handling of RCU's no-CBs CPUs has a maintenance headache, namely
    that if call_rcu() is invoked with interrupts disabled, the rcuo kthread
    wakeup must be defered to a point where we can be sure that scheduler
    locks are not held.  Of course, there are a lot of code paths leading
    from an interrupts-disabled invocation of call_rcu(), and missing any
    one of these can result in excessive callback-invocation latency, and
    potentially even system hangs.
    
    This commit therefore uses a timer to guarantee that the wakeup will
    eventually occur.  If one of the deferred-wakeup points kicks in, then
    the timer is simply cancelled.
    
    This commit also fixes up an incomplete removal of commits that were
    intended to plug remaining exit paths, which should have the added
    benefit of reducing the overhead of RCU's context-switch hooks.  In
    addition, it simplifies leader-to-follower callback-list handoff by
    introducing locking.  The call_rcu()-to-leader handoff continues to
    use atomic operations in order to maintain good real-time latency for
    common-case use of call_rcu().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Dan Carpenter fix for mod_timer() usage bug found by smatch. ]

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 9af0f31d6847..fe83f684ddcd 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -268,7 +268,9 @@ struct rcu_data {
 	struct rcu_head **nocb_follower_tail;
 	struct swait_queue_head nocb_wq; /* For nocb kthreads to sleep on. */
 	struct task_struct *nocb_kthread;
+	raw_spinlock_t nocb_lock;	/* Guard following pair of fields. */
 	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
+	struct timer_list nocb_timer;	/* Enforce finite deferral. */
 
 	/* The following fields are used by the leader, hence own cacheline. */
 	struct rcu_head *nocb_gp_head ____cacheline_internodealigned_in_smp;

commit ae91aa0adb14dc33114d566feca2f7cb7a96b8b7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon May 15 15:30:32 2017 -0700

    rcu: Remove debugfs tracing
    
    RCU's debugfs tracing used to be the only reasonable low-level debug
    information available, but ftrace and event tracing has since surpassed
    the RCU debugfs level of usefulness.  This commit therefore removes
    RCU's debugfs tracing.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 2c112bb11aa8..9af0f31d6847 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -152,19 +152,6 @@ struct rcu_node {
 				/* Number of tasks boosted for expedited GP. */
 	unsigned long n_normal_boosts;
 				/* Number of tasks boosted for normal GP. */
-	unsigned long n_balk_blkd_tasks;
-				/* Refused to boost: no blocked tasks. */
-	unsigned long n_balk_exp_gp_tasks;
-				/* Refused to boost: nothing blocking GP. */
-	unsigned long n_balk_boost_tasks;
-				/* Refused to boost: already boosting. */
-	unsigned long n_balk_notblocked;
-				/* Refused to boost: RCU RS CS still running. */
-	unsigned long n_balk_notyet;
-				/* Refused to boost: not yet time. */
-	unsigned long n_balk_nos;
-				/* Refused to boost: not sure why, though. */
-				/*  This can happen due to race conditions. */
 #ifdef CONFIG_RCU_NOCB_CPU
 	struct swait_queue_head nocb_gp_wq[2];
 				/* Place for rcu_nocb_kthread() to wait GP. */
@@ -535,17 +522,3 @@ void srcu_offline_cpu(unsigned int cpu) { }
 #endif /* #else #ifdef CONFIG_SRCU */
 
 #endif /* #ifndef RCU_TREE_NONCORE */
-
-#ifdef CONFIG_RCU_TRACE
-/* Read out queue lengths for tracing. */
-static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
-{
-#ifdef CONFIG_RCU_NOCB_CPU
-	*ql = atomic_long_read(&rdp->nocb_q_count);
-	*qll = atomic_long_read(&rdp->nocb_q_count_lazy);
-#else /* #ifdef CONFIG_RCU_NOCB_CPU */
-	*ql = 0;
-	*qll = 0;
-#endif /* #else #ifdef CONFIG_RCU_NOCB_CPU */
-}
-#endif /* #ifdef CONFIG_RCU_TRACE */

commit fe5ac724d81a3c7803e60c2232718f212f3f38d4
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 11 11:26:22 2017 -0700

    rcu: Remove nohz_full full-system-idle state machine
    
    The NO_HZ_FULL_SYSIDLE full-system-idle capability was added in 2013
    by commit 0edd1b1784cb ("nohz_full: Add full-system-idle state machine"),
    but has not been used.  This commit therefore removes it.
    
    If it turns out to be needed later, this commit can always be reverted.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index baa0bac8da2a..2c112bb11aa8 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -45,14 +45,6 @@ struct rcu_dynticks {
 	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
 	unsigned long rcu_qs_ctr;   /* Light universal quiescent state ctr. */
 	bool rcu_urgent_qs;	    /* GP old need light quiescent state. */
-#ifdef CONFIG_NO_HZ_FULL_SYSIDLE
-	long long dynticks_idle_nesting;
-				    /* irq/process nesting level from idle. */
-	atomic_t dynticks_idle;	    /* Even value for idle, else odd. */
-				    /*  "Idle" excludes userspace execution. */
-	unsigned long dynticks_idle_jiffies;
-				    /* End of last non-NMI non-idle period. */
-#endif /* #ifdef CONFIG_NO_HZ_FULL_SYSIDLE */
 #ifdef CONFIG_RCU_FAST_NO_HZ
 	bool all_lazy;		    /* Are all CPU's CBs lazy? */
 	unsigned long nonlazy_posted;
@@ -529,15 +521,7 @@ static void __init rcu_organize_nocb_kthreads(struct rcu_state *rsp);
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 static void __maybe_unused rcu_kick_nohz_cpu(int cpu);
 static bool init_nocb_callback_list(struct rcu_data *rdp);
-static void rcu_sysidle_enter(int irq);
-static void rcu_sysidle_exit(int irq);
-static void rcu_sysidle_check_cpu(struct rcu_data *rdp, bool *isidle,
-				  unsigned long *maxj);
-static bool is_sysidle_rcu_state(struct rcu_state *rsp);
-static void rcu_sysidle_report_gp(struct rcu_state *rsp, int isidle,
-				  unsigned long maxj);
 static void rcu_bind_gp_kthread(void);
-static void rcu_sysidle_init_percpu_data(struct rcu_dynticks *rdtp);
 static bool rcu_nohz_full_cpu(struct rcu_state *rsp);
 static void rcu_dynticks_task_enter(void);
 static void rcu_dynticks_task_exit(void);

commit 83d40bd3bc3ab3d6b5a4a331f7667d627948a099
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 9 13:28:51 2017 -0700

    rcu: Move rnp->lock wrappers for SRCU use
    
    This commit moves the now-generic rnp->lock wrapper macros from
    kernel/rcu/tree.h to kernel/rcu/rcu.h, thus allowing SRCU to use them.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a7f63f1074b4..baa0bac8da2a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -565,56 +565,3 @@ static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
 #endif /* #else #ifdef CONFIG_RCU_NOCB_CPU */
 }
 #endif /* #ifdef CONFIG_RCU_TRACE */
-
-/*
- * Wrappers for the rcu_node::lock acquire and release.
- *
- * Because the rcu_nodes form a tree, the tree traversal locking will observe
- * different lock values, this in turn means that an UNLOCK of one level
- * followed by a LOCK of another level does not imply a full memory barrier;
- * and most importantly transitivity is lost.
- *
- * In order to restore full ordering between tree levels, augment the regular
- * lock acquire functions with smp_mb__after_unlock_lock().
- *
- * As ->lock of struct rcu_node is a __private field, therefore one should use
- * these wrappers rather than directly call raw_spin_{lock,unlock}* on ->lock.
- */
-#define raw_spin_lock_rcu_node(p)					\
-do {									\
-	raw_spin_lock(&ACCESS_PRIVATE(p, lock));			\
-	smp_mb__after_unlock_lock();					\
-} while (0)
-
-#define raw_spin_unlock_rcu_node(p) raw_spin_unlock(&ACCESS_PRIVATE(p, lock))
-
-#define raw_spin_lock_irq_rcu_node(p)					\
-do {									\
-	raw_spin_lock_irq(&ACCESS_PRIVATE(p, lock));			\
-	smp_mb__after_unlock_lock();					\
-} while (0)
-
-#define raw_spin_unlock_irq_rcu_node(p)					\
-	raw_spin_unlock_irq(&ACCESS_PRIVATE(p, lock))
-
-#define raw_spin_lock_irqsave_rcu_node(rnp, flags)			\
-do {									\
-	typecheck(unsigned long, flags);				\
-	raw_spin_lock_irqsave(&ACCESS_PRIVATE(rnp, lock), flags);	\
-	smp_mb__after_unlock_lock();					\
-} while (0)
-
-#define raw_spin_unlock_irqrestore_rcu_node(rnp, flags)			\
-do {									\
-	typecheck(unsigned long, flags);				\
-	raw_spin_unlock_irqrestore(&ACCESS_PRIVATE(rnp, lock), flags);	\
-} while (0)
-
-#define raw_spin_trylock_rcu_node(p)					\
-({									\
-	bool ___locked = raw_spin_trylock(&ACCESS_PRIVATE(p, lock));	\
-									\
-	if (___locked)							\
-		smp_mb__after_unlock_lock();				\
-	___locked;							\
-})

commit bf32c76540257f9f5f2cf661dbdd8bb4a4bd8c82
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 9 12:05:46 2017 -0700

    rcu: Convert rnp->lock wrappers to macros for SRCU use
    
    Use of smp_mb__after_unlock_lock() would allow SRCU to omit a full
    memory barrier during callback execution, so this commit converts
    raw_spin_lock_rcu_node() from inline functions to type-generic macros
    to allow them to handle locks in srcu_node structures as well as
    rcu_node structures.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index ddfa34d020ba..a7f63f1074b4 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -580,27 +580,22 @@ static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
  * As ->lock of struct rcu_node is a __private field, therefore one should use
  * these wrappers rather than directly call raw_spin_{lock,unlock}* on ->lock.
  */
-static inline void raw_spin_lock_rcu_node(struct rcu_node *rnp)
-{
-	raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
-	smp_mb__after_unlock_lock();
-}
+#define raw_spin_lock_rcu_node(p)					\
+do {									\
+	raw_spin_lock(&ACCESS_PRIVATE(p, lock));			\
+	smp_mb__after_unlock_lock();					\
+} while (0)
 
-static inline void raw_spin_unlock_rcu_node(struct rcu_node *rnp)
-{
-	raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
-}
+#define raw_spin_unlock_rcu_node(p) raw_spin_unlock(&ACCESS_PRIVATE(p, lock))
 
-static inline void raw_spin_lock_irq_rcu_node(struct rcu_node *rnp)
-{
-	raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
-	smp_mb__after_unlock_lock();
-}
+#define raw_spin_lock_irq_rcu_node(p)					\
+do {									\
+	raw_spin_lock_irq(&ACCESS_PRIVATE(p, lock));			\
+	smp_mb__after_unlock_lock();					\
+} while (0)
 
-static inline void raw_spin_unlock_irq_rcu_node(struct rcu_node *rnp)
-{
-	raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
-}
+#define raw_spin_unlock_irq_rcu_node(p)					\
+	raw_spin_unlock_irq(&ACCESS_PRIVATE(p, lock))
 
 #define raw_spin_lock_irqsave_rcu_node(rnp, flags)			\
 do {									\
@@ -615,11 +610,11 @@ do {									\
 	raw_spin_unlock_irqrestore(&ACCESS_PRIVATE(rnp, lock), flags);	\
 } while (0)
 
-static inline bool raw_spin_trylock_rcu_node(struct rcu_node *rnp)
-{
-	bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
-
-	if (locked)
-		smp_mb__after_unlock_lock();
-	return locked;
-}
+#define raw_spin_trylock_rcu_node(p)					\
+({									\
+	bool ___locked = raw_spin_trylock(&ACCESS_PRIVATE(p, lock));	\
+									\
+	if (___locked)							\
+		smp_mb__after_unlock_lock();				\
+	___locked;							\
+})

commit 511324e462a12ea8be1a7e5fc63a992134db80d7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Apr 28 17:04:09 2017 -0700

    rcu: Use RCU_NOCB_WAKE rather than RCU_NOGP_WAKE
    
    The RCU_NOGP_WAKE_NOT, RCU_NOGP_WAKE, and RCU_NOGP_WAKE_FORCE flags
    are used to mediate wakeups for the no-CBs CPU kthreads.  The "NOGP"
    really doesn't make any sense, so this commit does s/NOGP/NOCB/.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 0fa7aee9ef55..ddfa34d020ba 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -312,9 +312,9 @@ struct rcu_data {
 };
 
 /* Values for nocb_defer_wakeup field in struct rcu_data. */
-#define RCU_NOGP_WAKE_NOT	0
-#define RCU_NOGP_WAKE		1
-#define RCU_NOGP_WAKE_FORCE	2
+#define RCU_NOCB_WAKE_NOT	0
+#define RCU_NOCB_WAKE		1
+#define RCU_NOCB_WAKE_FORCE	2
 
 #define RCU_JIFFIES_TILL_FORCE_QS (1 + (HZ > 250) + (HZ > 500))
 					/* For jiffies_till_first_fqs and */

commit 5b72f9643b52a5148bb8ced126e20563adfa3466
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Apr 12 15:29:14 2017 -0700

    rcu: Complain if blocking in preemptible RCU read-side critical section
    
    Although preemptible RCU allows its read-side critical sections to be
    preempted, general blocking is forbidden.  The reason for this is that
    excessive preemption times can be handled by CONFIG_RCU_BOOST=y, but a
    voluntarily blocked task doesn't care how high you boost its priority.
    Because preemptible RCU is a global mechanism, one ill-behaved reader
    hurts everyone.  Hence the prohibition against general blocking in
    RCU-preempt read-side critical sections.  Preemption yes, blocking no.
    
    This commit enforces this prohibition.
    
    There is a special exception for the -rt patchset (which they kindly
    volunteered to implement):  It is OK to block (as opposed to merely being
    preempted) within an RCU-preempt read-side critical section, but only if
    the blocking is subject to priority inheritance.  This exception permits
    CONFIG_RCU_BOOST=y to get -rt RCU readers out of trouble.
    
    Why doesn't this exception also apply to mainline's rt_mutex?  Because
    of the possibility that someone does general blocking while holding
    an rt_mutex.  Yes, the priority boosting will affect the rt_mutex,
    but it won't help with the task doing general blocking while holding
    that rt_mutex.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index ba38262c3554..0fa7aee9ef55 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -477,7 +477,7 @@ DECLARE_PER_CPU(char, rcu_cpu_has_work);
 
 /* Forward declarations for rcutree_plugin.h */
 static void rcu_bootup_announce(void);
-static void rcu_preempt_note_context_switch(void);
+static void rcu_preempt_note_context_switch(bool preempt);
 static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
 #ifdef CONFIG_HOTPLUG_CPU
 static bool rcu_preempt_has_tasks(struct rcu_node *rnp);

commit 45753c5f315749711b935a2506ee5c10eef5c23d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue May 2 10:31:18 2017 +0200

    srcu: Debloat the <linux/rcu_segcblist.h> header
    
    Linus noticed that the <linux/rcu_segcblist.h> has huge inline functions
    which should not be inline at all.
    
    As a first step in cleaning this up, move them all to kernel/rcu/ and
    only keep an absolute minimum of data type defines in the header:
    
      before:   -rw-r--r-- 1 mingo mingo 22284 May  2 10:25 include/linux/rcu_segcblist.h
       after:   -rw-r--r-- 1 mingo mingo  3180 May  2 10:22 include/linux/rcu_segcblist.h
    
    More can be done, such as uninlining the large functions, which inlining
    is unjustified even if it's an RCU internal matter.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 0e598ab08fea..ba38262c3554 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -30,9 +30,10 @@
 #include <linux/seqlock.h>
 #include <linux/swait.h>
 #include <linux/stop_machine.h>
-#include <linux/rcu_segcblist.h>
 #include <linux/rcu_node_tree.h>
 
+#include "rcu_segcblist.h"
+
 /*
  * Dynticks per-CPU state.
  */

commit da915ad5cf25b5f5d358dd3670c3378d8ae8c03e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Apr 5 09:01:53 2017 -0700

    srcu: Parallelize callback handling
    
    Peter Zijlstra proposed using SRCU to reduce mmap_sem contention [1,2],
    however, there are workloads that could result in a high volume of
    concurrent invocations of call_srcu(), which with current SRCU would
    result in excessive lock contention on the srcu_struct structure's
    ->queue_lock, which protects SRCU's callback lists.  This commit therefore
    moves SRCU to per-CPU callback lists, thus greatly reducing contention.
    
    Because a given SRCU instance no longer has a single centralized callback
    list, starting grace periods and invoking callbacks are both more complex
    than in the single-list Classic SRCU implementation.  Starting grace
    periods and handling callbacks are now handled using an srcu_node tree
    that is in some ways similar to the rcu_node trees used by RCU-bh,
    RCU-preempt, and RCU-sched (for example, the srcu_node tree shape is
    controlled by exactly the same Kconfig options and boot parameters that
    control the shape of the rcu_node tree).
    
    In addition, the old per-CPU srcu_array structure is now named srcu_data
    and contains an rcu_segcblist structure named ->srcu_cblist for its
    callbacks (and a spinlock to protect this).  The srcu_struct gets
    an srcu_gp_seq that is used to associate callback segments with the
    corresponding completion-time grace-period number.  These completion-time
    grace-period numbers are propagated up the srcu_node tree so that the
    grace-period workqueue handler can determine whether additional grace
    periods are needed on the one hand and where to look for callbacks that
    are ready to be invoked.
    
    The srcu_barrier() function must now wait on all instances of the per-CPU
    ->srcu_cblist.  Because each ->srcu_cblist is protected by ->lock,
    srcu_barrier() can remotely add the needed callbacks.  In theory,
    it could also remotely start grace periods, but in practice doing so
    is complex and racy.  And interestingly enough, it is never necessary
    for srcu_barrier() to start a grace period because srcu_barrier() only
    enqueues a callback when a callback is already present--and it turns out
    that a grace period has to have already been started for this pre-existing
    callback.  Furthermore, it is only the callback that srcu_barrier()
    needs to wait on, not any particular grace period.  Therefore, a new
    rcu_segcblist_entrain() function enqueues the srcu_barrier() function's
    callback into the same segment occupied by the last pre-existing callback
    in the list.  The special case where all the pre-existing callbacks are
    on a different list (because they are in the process of being invoked)
    is handled by enqueuing srcu_barrier()'s callback into the RCU_DONE_TAIL
    segment, relying on the done-callbacks check that takes place after all
    callbacks are inovked.
    
    Note that the readers use the same algorithm as before.  Note that there
    is a separate srcu_idx that tells the readers what counter to increment.
    This unfortunately cannot be combined with srcu_gp_seq because they
    need to be incremented at different times.
    
    This commit introduces some ugly #ifdefs in rcutorture.  These will go
    away when I feel good enough about Tree SRCU to ditch Classic SRCU.
    
    Some crude performance comparisons, courtesy of a quickly hacked rcuperf
    asynchronous-grace-period capability:
    
                            Callback Queuing Overhead
                            -------------------------
            # CPUS          Classic SRCU    Tree SRCU
            ------          ------------    ---------
                 2              0.349 us     0.342 us
                16             31.66  us     0.4   us
                41             ---------     0.417 us
    
    The times are the 90th percentiles, a statistic that was chosen to reject
    the overheads of the occasional srcu_barrier() call needed to avoid OOMing
    the test machine.  The rcuperf test hangs when running Classic SRCU at 41
    CPUs, hence the line of dashes.  Despite the hacks to both the rcuperf code
    and that statistics, this is a convincing demonstration of Tree SRCU's
    performance and scalability advantages.
    
    [1] https://lwn.net/Articles/309030/
    [2] https://patchwork.kernel.org/patch/5108281/
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Fix initialization if synchronize_srcu_expedited() called first. ]

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a2a45cb629d6..0e598ab08fea 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -541,6 +541,14 @@ static bool rcu_nohz_full_cpu(struct rcu_state *rsp);
 static void rcu_dynticks_task_enter(void);
 static void rcu_dynticks_task_exit(void);
 
+#ifdef CONFIG_SRCU
+void srcu_online_cpu(unsigned int cpu);
+void srcu_offline_cpu(unsigned int cpu);
+#else /* #ifdef CONFIG_SRCU */
+void srcu_online_cpu(unsigned int cpu) { }
+void srcu_offline_cpu(unsigned int cpu) { }
+#endif /* #else #ifdef CONFIG_SRCU */
+
 #endif /* #ifndef RCU_TREE_NONCORE */
 
 #ifdef CONFIG_RCU_TRACE

commit efbe451d46af62369226e42b98dbcd95b6940a63
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 15 13:07:53 2017 -0700

    srcu: Move rcu_node traversal macros to rcu.h
    
    This commit moves rcu_for_each_node_breadth_first(),
    rcu_for_each_nonleaf_node_breadth_first(), and
    rcu_for_each_leaf_node() from kernel/rcu/tree.h to
    kernel/rcu/rcu.h so that SRCU can access them.
    This commit is code-movement only.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 1bec3958d44f..a2a45cb629d6 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -192,41 +192,6 @@ struct rcu_node {
  */
 #define leaf_node_cpu_bit(rnp, cpu) (1UL << ((cpu) - (rnp)->grplo))
 
-/*
- * Do a full breadth-first scan of the rcu_node structures for the
- * specified rcu_state structure.
- */
-#define rcu_for_each_node_breadth_first(rsp, rnp) \
-	for ((rnp) = &(rsp)->node[0]; \
-	     (rnp) < &(rsp)->node[rcu_num_nodes]; (rnp)++)
-
-/*
- * Do a breadth-first scan of the non-leaf rcu_node structures for the
- * specified rcu_state structure.  Note that if there is a singleton
- * rcu_node tree with but one rcu_node structure, this loop is a no-op.
- */
-#define rcu_for_each_nonleaf_node_breadth_first(rsp, rnp) \
-	for ((rnp) = &(rsp)->node[0]; \
-	     (rnp) < (rsp)->level[rcu_num_lvls - 1]; (rnp)++)
-
-/*
- * Scan the leaves of the rcu_node hierarchy for the specified rcu_state
- * structure.  Note that if there is a singleton rcu_node tree with but
- * one rcu_node structure, this loop -will- visit the rcu_node structure.
- * It is still a leaf node, even if it is also the root node.
- */
-#define rcu_for_each_leaf_node(rsp, rnp) \
-	for ((rnp) = (rsp)->level[rcu_num_lvls - 1]; \
-	     (rnp) < &(rsp)->node[rcu_num_nodes]; (rnp)++)
-
-/*
- * Iterate over all possible CPUs in a leaf RCU node.
- */
-#define for_each_leaf_node_possible_cpu(rnp, cpu) \
-	for ((cpu) = cpumask_next(rnp->grplo - 1, cpu_possible_mask); \
-	     cpu <= rnp->grphi; \
-	     cpu = cpumask_next((cpu), cpu_possible_mask))
-
 /*
  * Union to allow "aggregate OR" operation on the need for a quiescent
  * state by the normal and expedited grace periods.

commit f2425b4efb0c69e77c0b9666b605ae4a1ecaae47
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Mar 14 12:42:30 2017 -0700

    srcu: Move combining-tree definitions for SRCU's benefit
    
    This commit moves the C preprocessor code that defines the default shape
    of the rcu_node combining tree to a new include/linux/rcu_node_tree.h
    file as a first step towards enabling SRCU to create its own combining
    tree, which in turn enables SRCU to implement per-CPU callback handling,
    thus avoiding contention on the lock currently guarding the single list
    of callbacks.  Note that users of SRCU still need to know the size of
    the srcu_struct structure, hence include/linux rather than kernel/rcu.
    
    This commit is code-movement only.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 4f62651588ea..1bec3958d44f 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -31,76 +31,7 @@
 #include <linux/swait.h>
 #include <linux/stop_machine.h>
 #include <linux/rcu_segcblist.h>
-
-/*
- * Define shape of hierarchy based on NR_CPUS, CONFIG_RCU_FANOUT, and
- * CONFIG_RCU_FANOUT_LEAF.
- * In theory, it should be possible to add more levels straightforwardly.
- * In practice, this did work well going from three levels to four.
- * Of course, your mileage may vary.
- */
-
-#ifdef CONFIG_RCU_FANOUT
-#define RCU_FANOUT CONFIG_RCU_FANOUT
-#else /* #ifdef CONFIG_RCU_FANOUT */
-# ifdef CONFIG_64BIT
-# define RCU_FANOUT 64
-# else
-# define RCU_FANOUT 32
-# endif
-#endif /* #else #ifdef CONFIG_RCU_FANOUT */
-
-#ifdef CONFIG_RCU_FANOUT_LEAF
-#define RCU_FANOUT_LEAF CONFIG_RCU_FANOUT_LEAF
-#else /* #ifdef CONFIG_RCU_FANOUT_LEAF */
-#define RCU_FANOUT_LEAF 16
-#endif /* #else #ifdef CONFIG_RCU_FANOUT_LEAF */
-
-#define RCU_FANOUT_1	      (RCU_FANOUT_LEAF)
-#define RCU_FANOUT_2	      (RCU_FANOUT_1 * RCU_FANOUT)
-#define RCU_FANOUT_3	      (RCU_FANOUT_2 * RCU_FANOUT)
-#define RCU_FANOUT_4	      (RCU_FANOUT_3 * RCU_FANOUT)
-
-#if NR_CPUS <= RCU_FANOUT_1
-#  define RCU_NUM_LVLS	      1
-#  define NUM_RCU_LVL_0	      1
-#  define NUM_RCU_NODES	      NUM_RCU_LVL_0
-#  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0 }
-#  define RCU_NODE_NAME_INIT  { "rcu_node_0" }
-#  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0" }
-#elif NR_CPUS <= RCU_FANOUT_2
-#  define RCU_NUM_LVLS	      2
-#  define NUM_RCU_LVL_0	      1
-#  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
-#  define NUM_RCU_NODES	      (NUM_RCU_LVL_0 + NUM_RCU_LVL_1)
-#  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1 }
-#  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1" }
-#  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1" }
-#elif NR_CPUS <= RCU_FANOUT_3
-#  define RCU_NUM_LVLS	      3
-#  define NUM_RCU_LVL_0	      1
-#  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
-#  define NUM_RCU_LVL_2	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
-#  define NUM_RCU_NODES	      (NUM_RCU_LVL_0 + NUM_RCU_LVL_1 + NUM_RCU_LVL_2)
-#  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1, NUM_RCU_LVL_2 }
-#  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2" }
-#  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2" }
-#elif NR_CPUS <= RCU_FANOUT_4
-#  define RCU_NUM_LVLS	      4
-#  define NUM_RCU_LVL_0	      1
-#  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_3)
-#  define NUM_RCU_LVL_2	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
-#  define NUM_RCU_LVL_3	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
-#  define NUM_RCU_NODES	      (NUM_RCU_LVL_0 + NUM_RCU_LVL_1 + NUM_RCU_LVL_2 + NUM_RCU_LVL_3)
-#  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1, NUM_RCU_LVL_2, NUM_RCU_LVL_3 }
-#  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2", "rcu_node_3" }
-#  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2", "rcu_node_fqs_3" }
-#else
-# error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
-#endif /* #if (NR_CPUS) <= RCU_FANOUT_1 */
-
-extern int rcu_num_lvls;
-extern int rcu_num_nodes;
+#include <linux/rcu_node_tree.h>
 
 /*
  * Dynticks per-CPU state.

commit 8660b7d8a545227fd9ee80508aa82528ea9947d7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Mar 13 16:48:18 2017 -0700

    srcu: Use rcu_segcblist to track SRCU callbacks
    
    This commit switches SRCU from custom-built callback queues to the new
    rcu_segcblist structure.  This change associates grace-period sequence
    numbers with groups of callbacks, which will be needed for efficient
    processing of per-CPU callbacks.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 93889ff21dbb..4f62651588ea 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -30,7 +30,7 @@
 #include <linux/seqlock.h>
 #include <linux/swait.h>
 #include <linux/stop_machine.h>
-#include "rcu_segcblist.h"
+#include <linux/rcu_segcblist.h>
 
 /*
  * Define shape of hierarchy based on NR_CPUS, CONFIG_RCU_FANOUT, and

commit 15fecf89e46a962ccda583d919e25d9da7bf0723
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Feb 8 12:36:42 2017 -0800

    srcu: Abstract multi-tail callback list handling
    
    RCU has only one multi-tail callback list, which is implemented via
    the nxtlist, nxttail, nxtcompleted, qlen_lazy, and qlen fields in the
    rcu_data structure, and whose operations are open-code throughout the
    Tree RCU implementation.  This has been more or less OK in the past,
    but upcoming callback-list optimizations in SRCU could really use
    a multi-tail callback list there as well.
    
    This commit therefore abstracts the multi-tail callback list handling
    into a new kernel/rcu/rcu_segcblist.h file, and uses this new API.
    The simple head-and-tail pointer callback list is also abstracted and
    applied everywhere except for the NOCB callback-offload lists.  (Yes,
    the plan is to apply them there as well, but this commit is already
    bigger than would be good.)
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 376c01e539c7..93889ff21dbb 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -30,6 +30,7 @@
 #include <linux/seqlock.h>
 #include <linux/swait.h>
 #include <linux/stop_machine.h>
+#include "rcu_segcblist.h"
 
 /*
  * Define shape of hierarchy based on NR_CPUS, CONFIG_RCU_FANOUT, and
@@ -335,34 +336,9 @@ struct rcu_data {
 					/* period it is aware of. */
 
 	/* 2) batch handling */
-	/*
-	 * If nxtlist is not NULL, it is partitioned as follows.
-	 * Any of the partitions might be empty, in which case the
-	 * pointer to that partition will be equal to the pointer for
-	 * the following partition.  When the list is empty, all of
-	 * the nxttail elements point to the ->nxtlist pointer itself,
-	 * which in that case is NULL.
-	 *
-	 * [nxtlist, *nxttail[RCU_DONE_TAIL]):
-	 *	Entries that batch # <= ->completed
-	 *	The grace period for these entries has completed, and
-	 *	the other grace-period-completed entries may be moved
-	 *	here temporarily in rcu_process_callbacks().
-	 * [*nxttail[RCU_DONE_TAIL], *nxttail[RCU_WAIT_TAIL]):
-	 *	Entries that batch # <= ->completed - 1: waiting for current GP
-	 * [*nxttail[RCU_WAIT_TAIL], *nxttail[RCU_NEXT_READY_TAIL]):
-	 *	Entries known to have arrived before current GP ended
-	 * [*nxttail[RCU_NEXT_READY_TAIL], *nxttail[RCU_NEXT_TAIL]):
-	 *	Entries that might have arrived after current GP ended
-	 *	Note that the value of *nxttail[RCU_NEXT_TAIL] will
-	 *	always be NULL, as this is the end of the list.
-	 */
-	struct rcu_head *nxtlist;
-	struct rcu_head **nxttail[RCU_NEXT_SIZE];
-	unsigned long	nxtcompleted[RCU_NEXT_SIZE];
-					/* grace periods for sublists. */
-	long		qlen_lazy;	/* # of lazy queued callbacks */
-	long		qlen;		/* # of queued callbacks, incl lazy */
+	struct rcu_segcblist cblist;	/* Segmented callback list, with */
+					/* different callbacks waiting for */
+					/* different grace periods. */
 	long		qlen_last_fqs_check;
 					/* qlen at last check for QS forcing */
 	unsigned long	n_cbs_invoked;	/* count of RCU cbs invoked. */
@@ -500,14 +476,11 @@ struct rcu_state {
 
 	raw_spinlock_t orphan_lock ____cacheline_internodealigned_in_smp;
 						/* Protect following fields. */
-	struct rcu_head *orphan_nxtlist;	/* Orphaned callbacks that */
+	struct rcu_cblist orphan_pend;		/* Orphaned callbacks that */
 						/*  need a grace period. */
-	struct rcu_head **orphan_nxttail;	/* Tail of above. */
-	struct rcu_head *orphan_donelist;	/* Orphaned callbacks that */
+	struct rcu_cblist orphan_done;		/* Orphaned callbacks that */
 						/*  are ready to invoke. */
-	struct rcu_head **orphan_donetail;	/* Tail of above. */
-	long qlen_lazy;				/* Number of lazy callbacks. */
-	long qlen;				/* Total number of callbacks. */
+						/* (Contains counts.) */
 	/* End of fields guarded by orphan_lock. */
 
 	struct mutex barrier_mutex;		/* Guards barrier fields. */

commit b8c78d3afc6aac1c722af3bec18959c6bd93231c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Feb 3 09:54:05 2017 -0800

    rcu: Default RCU_FANOUT_LEAF to 16 unless explicitly changed
    
    If the RCU_EXPERT Kconfig option is not set (the default), then the
    RCU_FANOUT_LEAF Kconfig option will not be defined, which will cause
    the leaf-level rcu_node tree fanout to default to 32 on 32-bit systems
    and 64 on 64-bit systems.  This can result in excessive lock contention.
    This commit therefore changes the computation of the leaf-level rcu_node
    tree fanout so that the result will be 16 unless an explicit Kconfig or
    kernel-boot setting says otherwise.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d2f276fc2edc..376c01e539c7 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -52,11 +52,7 @@
 #ifdef CONFIG_RCU_FANOUT_LEAF
 #define RCU_FANOUT_LEAF CONFIG_RCU_FANOUT_LEAF
 #else /* #ifdef CONFIG_RCU_FANOUT_LEAF */
-# ifdef CONFIG_64BIT
-# define RCU_FANOUT_LEAF 64
-# else
-# define RCU_FANOUT_LEAF 32
-# endif
+#define RCU_FANOUT_LEAF 16
 #endif /* #else #ifdef CONFIG_RCU_FANOUT_LEAF */
 
 #define RCU_FANOUT_1	      (RCU_FANOUT_LEAF)

commit 9226b10d78ffe7895549045fe388dc5e73b87eac
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jan 27 14:17:50 2017 -0800

    rcu: Place guard on rcu_all_qs() and rcu_note_context_switch() actions
    
    The rcu_all_qs() and rcu_note_context_switch() do a series of checks,
    taking various actions to supply RCU with quiescent states, depending
    on the outcomes of the various checks.  This is a bit much for scheduling
    fastpaths, so this commit creates a separate ->rcu_urgent_qs field in
    the rcu_dynticks structure that acts as a global guard for these checks.
    Thus, in the common case, rcu_all_qs() and rcu_note_context_switch()
    check the ->rcu_urgent_qs field, find it false, and simply return.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index b212cd0f22c7..d2f276fc2edc 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -113,8 +113,9 @@ struct rcu_dynticks {
 				    /* Process level is worth LLONG_MAX/2. */
 	int dynticks_nmi_nesting;   /* Track NMI nesting level. */
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
-	bool rcu_need_heavy_qs;      /* GP old, need heavy quiescent state. */
+	bool rcu_need_heavy_qs;     /* GP old, need heavy quiescent state. */
 	unsigned long rcu_qs_ctr;   /* Light universal quiescent state ctr. */
+	bool rcu_urgent_qs;	    /* GP old need light quiescent state. */
 #ifdef CONFIG_NO_HZ_FULL_SYSIDLE
 	long long dynticks_idle_nesting;
 				    /* irq/process nesting level from idle. */

commit 0f9be8cabbc343218dd2807af7308656be113045
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jan 27 13:17:02 2017 -0800

    rcu: Eliminate flavor scan in rcu_momentary_dyntick_idle()
    
    The rcu_momentary_dyntick_idle() function scans the RCU flavors, checking
    that one of them still needs a quiescent state before doing an expensive
    atomic operation on the ->dynticks counter.  However, this check reduces
    overhead only after a rare race condition, and increases complexity.  This
    commit therefore removes the scan and the mechanism enabling the scan.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 76e4467bc765..b212cd0f22c7 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -113,7 +113,7 @@ struct rcu_dynticks {
 				    /* Process level is worth LLONG_MAX/2. */
 	int dynticks_nmi_nesting;   /* Track NMI nesting level. */
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
-	int rcu_sched_qs_mask;      /* GP old, need heavy quiescent state. */
+	bool rcu_need_heavy_qs;      /* GP old, need heavy quiescent state. */
 	unsigned long rcu_qs_ctr;   /* Light universal quiescent state ctr. */
 #ifdef CONFIG_NO_HZ_FULL_SYSIDLE
 	long long dynticks_idle_nesting;
@@ -484,7 +484,6 @@ struct rcu_state {
 	struct rcu_node *level[RCU_NUM_LVLS + 1];
 						/* Hierarchy levels (+1 to */
 						/*  shut bogus gcc warning) */
-	u8 flavor_mask;				/* bit in flavor mask. */
 	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */
 	call_rcu_func_t call;			/* call_rcu() flavor. */
 	int ncpus;				/* # CPUs seen so far. */

commit 9577df9a3122af08fff84b8a1a60dccf524a3891
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jan 26 16:18:07 2017 -0800

    rcu: Pull rcu_qs_ctr into rcu_dynticks structure
    
    The rcu_qs_ctr variable is yet another isolated per-CPU variable,
    so this commit pulls it into the pre-existing rcu_dynticks per-CPU
    structure.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index e298281984dc..76e4467bc765 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -113,7 +113,8 @@ struct rcu_dynticks {
 				    /* Process level is worth LLONG_MAX/2. */
 	int dynticks_nmi_nesting;   /* Track NMI nesting level. */
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
-	int rcu_sched_qs_mask;      /* GP old, need quiescent state. */
+	int rcu_sched_qs_mask;      /* GP old, need heavy quiescent state. */
+	unsigned long rcu_qs_ctr;   /* Light universal quiescent state ctr. */
 #ifdef CONFIG_NO_HZ_FULL_SYSIDLE
 	long long dynticks_idle_nesting;
 				    /* irq/process nesting level from idle. */

commit abb06b99484a9f5af05c7147c289faf835f68e8e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jan 26 13:45:38 2017 -0800

    rcu: Pull rcu_sched_qs_mask into rcu_dynticks structure
    
    The rcu_sched_qs_mask variable is yet another isolated per-CPU variable,
    so this commit pulls it into the pre-existing rcu_dynticks per-CPU
    structure.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7468b4de7e0c..e298281984dc 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -113,6 +113,7 @@ struct rcu_dynticks {
 				    /* Process level is worth LLONG_MAX/2. */
 	int dynticks_nmi_nesting;   /* Track NMI nesting level. */
 	atomic_t dynticks;	    /* Even value for idle, else odd. */
+	int rcu_sched_qs_mask;      /* GP old, need quiescent state. */
 #ifdef CONFIG_NO_HZ_FULL_SYSIDLE
 	long long dynticks_idle_nesting;
 				    /* irq/process nesting level from idle. */

commit b8c17e6664c461e4aed545a943304c3b32dd309c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Nov 8 14:25:21 2016 -0800

    rcu: Maintain special bits at bottom of ->dynticks counter
    
    Currently, IPIs are used to force other CPUs to invalidate their TLBs
    in response to a kernel virtual-memory mapping change.  This works, but
    degrades both battery lifetime (for idle CPUs) and real-time response
    (for nohz_full CPUs), and in addition results in unnecessary IPIs due to
    the fact that CPUs executing in usermode are unaffected by stale kernel
    mappings.  It would be better to cause a CPU executing in usermode to
    wait until it is entering kernel mode to do the flush, first to avoid
    interrupting usemode tasks and second to handle multiple flush requests
    with a single flush in the case of a long-running user task.
    
    This commit therefore reserves a bit at the bottom of the ->dynticks
    counter, which is checked upon exit from extended quiescent states.
    If it is set, it is cleared and then a new rcu_eqs_special_exit() macro is
    invoked, which, if not supplied, is an empty single-pass do-while loop.
    If this bottom bit is set on -entry- to an extended quiescent state,
    then a WARN_ON_ONCE() triggers.
    
    This bottom bit may be set using a new rcu_eqs_special_set() function,
    which returns true if the bit was set, or false if the CPU turned
    out to not be in an extended quiescent state.  Please note that this
    function refuses to set the bit for a non-nohz_full CPU when that CPU
    is executing in usermode because usermode execution is tracked by RCU
    as a dyntick-idle extended quiescent state only for nohz_full CPUs.
    
    Reported-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index ec62a05bfdb3..7468b4de7e0c 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -596,6 +596,7 @@ extern struct rcu_state rcu_preempt_state;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 
 int rcu_dynticks_snap(struct rcu_dynticks *rdtp);
+bool rcu_eqs_special_set(int cpu);
 
 #ifdef CONFIG_RCU_BOOST
 DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_status);

commit 037741a6d4ab2e4b0580dae97aca963d8a8dc68f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 10:08:30 2017 +0100

    sched/headers: Prepare for the removal of <linux/rtmutex.h> from <linux/sched.h>
    
    Fix up missing #includes in other places that rely on sched.h doing that for them.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index b60f2b6caa14..ec62a05bfdb3 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -24,6 +24,7 @@
 
 #include <linux/cache.h>
 #include <linux/spinlock.h>
+#include <linux/rtmutex.h>
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 #include <linux/seqlock.h>

commit 31945aa9f14085c81cb3257e51bb210698b78626
Merge: e3c50dfbbd6f 38d30b336ccf 8dc79888a792 7f554a3d05be 7d025948e498
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 25 12:56:05 2017 -0800

    Merge branches 'doc.2017.01.15b', 'dyntick.2017.01.23a', 'fixes.2017.01.23a', 'srcu.2017.01.25a' and 'torture.2017.01.15b' into HEAD
    
    doc.2017.01.15b: Documentation updates
    dyntick.2017.01.23a: Dyntick tracking consolidation
    fixes.2017.01.23a: Miscellaneous fixes
    srcu.2017.01.25a: SRCU rewrite, fixes, and verification
    torture.2017.01.15b: Torture-test updates

commit d85b62f18d543c663cbdd6061054efeb9e66cee7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Nov 28 12:08:49 2016 -0800

    srcu: Force full grace-period ordering
    
    If a process invokes synchronize_srcu(), is delayed just the right amount
    of time, and thus does not sleep when waiting for the grace period to
    complete, there is no ordering between the end of the grace period and
    the code following the synchronize_srcu().  Similarly, there can be a
    lack of ordering between the end of the SRCU grace period and callback
    invocation.
    
    This commit adds the necessary ordering.
    
    Reported-by: Lance Roy <ldr709@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Further smp_mb() adjustment per email with Lance Roy. ]

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index fe98dd24adf8..abcc25bdcb29 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -687,18 +687,6 @@ static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
 }
 #endif /* #ifdef CONFIG_RCU_TRACE */
 
-/*
- * Place this after a lock-acquisition primitive to guarantee that
- * an UNLOCK+LOCK pair act as a full barrier.  This guarantee applies
- * if the UNLOCK and LOCK are executed by the same CPU or if the
- * UNLOCK and LOCK operate on the same lock variable.
- */
-#ifdef CONFIG_PPC
-#define smp_mb__after_unlock_lock()	smp_mb()  /* Full ordering for lock. */
-#else /* #ifdef CONFIG_PPC */
-#define smp_mb__after_unlock_lock()	do { } while (0)
-#endif /* #else #ifdef CONFIG_PPC */
-
 /*
  * Wrappers for the rcu_node::lock acquire and release.
  *

commit 02a5c550b2738f2bfea8e1e00aa75944d71c9e18
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Nov 2 17:25:06 2016 -0700

    rcu: Abstract extended quiescent state determination
    
    This commit is the fourth step towards full abstraction of all accesses
    to the ->dynticks counter, implementing previously open-coded checks and
    comparisons in new rcu_dynticks_in_eqs() and rcu_dynticks_in_eqs_since()
    functions.  This abstraction will ease changes to the ->dynticks counter
    operation.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index fe98dd24adf8..3b953dcf6afc 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -595,6 +595,8 @@ extern struct rcu_state rcu_bh_state;
 extern struct rcu_state rcu_preempt_state;
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 
+int rcu_dynticks_snap(struct rcu_dynticks *rdtp);
+
 #ifdef CONFIG_RCU_BOOST
 DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_status);
 DECLARE_PER_CPU(int, rcu_cpu_kthread_cpu);

commit bb4e2c08bbfa8eb032db7814f6100086aac102d3
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jan 6 22:04:22 2017 -0800

    rcu: Eliminate unused expedited_normal counter
    
    Expedited grace periods no longer fall back to normal grace periods
    in response to lock contention, given that expedited grace periods
    now use the rcu_node tree so as to avoid contention.  This commit
    therfore removes the expedited_normal counter.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index fe98dd24adf8..8f750dffb0dd 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -521,7 +521,6 @@ struct rcu_state {
 	struct mutex exp_mutex;			/* Serialize expedited GP. */
 	struct mutex exp_wake_mutex;		/* Serialize wakeup. */
 	unsigned long expedited_sequence;	/* Take a ticket. */
-	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
 	atomic_t expedited_need_qs;		/* # CPUs left to check in. */
 	struct swait_queue_head expedited_wq;	/* Wait for check-ins. */
 	int ncpus_snap;				/* # CPUs seen last time. */

commit 0742ac3e2f9f4b8a3a394a270d8685078837662b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 11 06:09:59 2016 -0700

    rcu: Make expedited grace periods recheck dyntick idle state
    
    Expedited grace periods check dyntick-idle state, and avoid sending
    IPIs to idle CPUs, including those running guest OSes, and, on NOHZ_FULL
    kernels, nohz_full CPUs.  However, the kernel has been observed checking
    a CPU while it was non-idle, but sending the IPI after it has gone
    idle.  This commit therefore rechecks idle state immediately before
    sending the IPI, refraining from IPIing CPUs that have since gone idle.
    
    Reported-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index e99a5234d9ed..fe98dd24adf8 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -404,6 +404,7 @@ struct rcu_data {
 	atomic_long_t exp_workdone1;	/* # done by others #1. */
 	atomic_long_t exp_workdone2;	/* # done by others #2. */
 	atomic_long_t exp_workdone3;	/* # done by others #3. */
+	int exp_dynticks_snap;		/* Double-check need for IPI. */
 
 	/* 7) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU

commit 8b355e3bc1408be238ae4695fb6318ae502cae8e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jun 29 13:46:25 2016 -0700

    rcu: Drive expedited grace periods from workqueue
    
    The current implementation of expedited grace periods has the user
    task drive the grace period.  This works, but has downsides: (1) The
    user task must awaken tasks piggybacking on this grace period, which
    can result in latencies rivaling that of the grace period itself, and
    (2) User tasks can receive signals, which interfere with RCU CPU stall
    warnings.
    
    This commit therefore uses workqueues to drive the grace periods, so
    that the user task need not do the awakening.  A subsequent commit
    will remove the now-unnecessary code allowing for signals.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index f714f873bf9d..e99a5234d9ed 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -400,6 +400,7 @@ struct rcu_data {
 #ifdef CONFIG_RCU_FAST_NO_HZ
 	struct rcu_head oom_head;
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
+	atomic_long_t exp_workdone0;	/* # done by workqueue. */
 	atomic_long_t exp_workdone1;	/* # done by others #1. */
 	atomic_long_t exp_workdone2;	/* # done by others #2. */
 	atomic_long_t exp_workdone3;	/* # done by others #3. */

commit bc75e99983df1efd977a5cd468893d55d52b8d70
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jun 3 15:20:04 2016 +0100

    rcu: Correctly handle sparse possible cpus
    
    In many cases in the RCU tree code, we iterate over the set of cpus for
    a leaf node described by rcu_node::grplo and rcu_node::grphi, checking
    per-cpu data for each cpu in this range. However, if the set of possible
    cpus is sparse, some cpus described in this range are not possible, and
    thus no per-cpu region will have been allocated (or initialised) for
    them by the generic percpu code.
    
    Erroneous accesses to a per-cpu area for these !possible cpus may fault
    or may hit other data depending on the addressed generated when the
    erroneous per cpu offset is applied. In practice, both cases have been
    observed on arm64 hardware (the former being silent, but detectable with
    additional patches).
    
    To avoid issues resulting from this, we must iterate over the set of
    *possible* cpus for a given leaf node. This patch add a new helper,
    for_each_leaf_node_possible_cpu, to enable this. As iteration is often
    intertwined with rcu_node local bitmask manipulation, a new
    leaf_node_cpu_bit helper is added to make this simpler and more
    consistent. The RCU tree code is made to use both of these where
    appropriate.
    
    Without this patch, running reboot at a shell can result in an oops
    like:
    
    [ 3369.075979] Unable to handle kernel paging request at virtual address ffffff8008b21b4c
    [ 3369.083881] pgd = ffffffc3ecdda000
    [ 3369.087270] [ffffff8008b21b4c] *pgd=00000083eca48003, *pud=00000083eca48003, *pmd=0000000000000000
    [ 3369.096222] Internal error: Oops: 96000007 [#1] PREEMPT SMP
    [ 3369.101781] Modules linked in:
    [ 3369.104825] CPU: 2 PID: 1817 Comm: NetworkManager Tainted: G        W       4.6.0+ #3
    [ 3369.121239] task: ffffffc0fa13e000 ti: ffffffc3eb940000 task.ti: ffffffc3eb940000
    [ 3369.128708] PC is at sync_rcu_exp_select_cpus+0x188/0x510
    [ 3369.134094] LR is at sync_rcu_exp_select_cpus+0x104/0x510
    [ 3369.139479] pc : [<ffffff80081109a8>] lr : [<ffffff8008110924>] pstate: 200001c5
    [ 3369.146860] sp : ffffffc3eb9435a0
    [ 3369.150162] x29: ffffffc3eb9435a0 x28: ffffff8008be4f88
    [ 3369.155465] x27: ffffff8008b66c80 x26: ffffffc3eceb2600
    [ 3369.160767] x25: 0000000000000001 x24: ffffff8008be4f88
    [ 3369.166070] x23: ffffff8008b51c3c x22: ffffff8008b66c80
    [ 3369.171371] x21: 0000000000000001 x20: ffffff8008b21b40
    [ 3369.176673] x19: ffffff8008b66c80 x18: 0000000000000000
    [ 3369.181975] x17: 0000007fa951a010 x16: ffffff80086a30f0
    [ 3369.187278] x15: 0000007fa9505590 x14: 0000000000000000
    [ 3369.192580] x13: ffffff8008b51000 x12: ffffffc3eb940000
    [ 3369.197882] x11: 0000000000000006 x10: ffffff8008b51b78
    [ 3369.203184] x9 : 0000000000000001 x8 : ffffff8008be4000
    [ 3369.208486] x7 : ffffff8008b21b40 x6 : 0000000000001003
    [ 3369.213788] x5 : 0000000000000000 x4 : ffffff8008b27280
    [ 3369.219090] x3 : ffffff8008b21b4c x2 : 0000000000000001
    [ 3369.224406] x1 : 0000000000000001 x0 : 0000000000000140
    ...
    [ 3369.972257] [<ffffff80081109a8>] sync_rcu_exp_select_cpus+0x188/0x510
    [ 3369.978685] [<ffffff80081128b4>] synchronize_rcu_expedited+0x64/0xa8
    [ 3369.985026] [<ffffff80086b987c>] synchronize_net+0x24/0x30
    [ 3369.990499] [<ffffff80086ddb54>] dev_deactivate_many+0x28c/0x298
    [ 3369.996493] [<ffffff80086b6bb8>] __dev_close_many+0x60/0xd0
    [ 3370.002052] [<ffffff80086b6d48>] __dev_close+0x28/0x40
    [ 3370.007178] [<ffffff80086bf62c>] __dev_change_flags+0x8c/0x158
    [ 3370.012999] [<ffffff80086bf718>] dev_change_flags+0x20/0x60
    [ 3370.018558] [<ffffff80086cf7f0>] do_setlink+0x288/0x918
    [ 3370.023771] [<ffffff80086d0798>] rtnl_newlink+0x398/0x6a8
    [ 3370.029158] [<ffffff80086cee84>] rtnetlink_rcv_msg+0xe4/0x220
    [ 3370.034891] [<ffffff80086e274c>] netlink_rcv_skb+0xc4/0xf8
    [ 3370.040364] [<ffffff80086ced8c>] rtnetlink_rcv+0x2c/0x40
    [ 3370.045663] [<ffffff80086e1fe8>] netlink_unicast+0x160/0x238
    [ 3370.051309] [<ffffff80086e24b8>] netlink_sendmsg+0x2f0/0x358
    [ 3370.056956] [<ffffff80086a0070>] sock_sendmsg+0x18/0x30
    [ 3370.062168] [<ffffff80086a21cc>] ___sys_sendmsg+0x26c/0x280
    [ 3370.067728] [<ffffff80086a30ac>] __sys_sendmsg+0x44/0x88
    [ 3370.073027] [<ffffff80086a3100>] SyS_sendmsg+0x10/0x20
    [ 3370.078153] [<ffffff8008085e70>] el0_svc_naked+0x24/0x28
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reported-by: Dennis Chen <dennis.chen@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Steve Capper <steve.capper@arm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index e3959f5e6ddf..f714f873bf9d 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -253,6 +253,13 @@ struct rcu_node {
 	wait_queue_head_t exp_wq[4];
 } ____cacheline_internodealigned_in_smp;
 
+/*
+ * Bitmasks in an rcu_node cover the interval [grplo, grphi] of CPU IDs, and
+ * are indexed relative to this interval rather than the global CPU ID space.
+ * This generates the bit for a CPU in node-local masks.
+ */
+#define leaf_node_cpu_bit(rnp, cpu) (1UL << ((cpu) - (rnp)->grplo))
+
 /*
  * Do a full breadth-first scan of the rcu_node structures for the
  * specified rcu_state structure.
@@ -280,6 +287,14 @@ struct rcu_node {
 	for ((rnp) = (rsp)->level[rcu_num_lvls - 1]; \
 	     (rnp) < &(rsp)->node[rcu_num_nodes]; (rnp)++)
 
+/*
+ * Iterate over all possible CPUs in a leaf RCU node.
+ */
+#define for_each_leaf_node_possible_cpu(rnp, cpu) \
+	for ((cpu) = cpumask_next(rnp->grplo - 1, cpu_possible_mask); \
+	     cpu <= rnp->grphi; \
+	     cpu = cpumask_next((cpu), cpu_possible_mask))
+
 /*
  * Union to allow "aggregate OR" operation on the need for a quiescent
  * state by the normal and expedited grace periods.

commit dcd36d01fb3f99d1d5df01714f6ccbe3fbbaf81f
Merge: 5c1458478c49 3b5f668e715b 5dffed1e5721 a54062c0d959
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 21 13:48:20 2016 -0700

    Merge branches 'doc.2016.04.19a', 'exp.2016.03.31d', 'fixes.2016.03.31d' and 'torture.2016.04.21a' into HEAD
    
    doc.2016.04.19a: Documentation updates
    exp.2016.03.31d: Expedited grace-period updates
    fixes.2016.03.31d: Miscellaneous fixes
    torture.2016.004.21a Torture-test updates

commit 8c7c4829a81c1838f18c12ce5a3a5c29a08bf0a8
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Jan 3 20:29:57 2016 -0800

    rcu: Awaken grace-period kthread if too long since FQS
    
    Recent kernels can fail to awaken the grace-period kthread for
    quiescent-state forcing.  This commit is a crude hack that does
    a wakeup if a scheduling-clock interrupt sees that it has been
    too long since force-quiescent-state (FQS) processing.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index df668c0f9e64..34d3973f7223 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -513,6 +513,8 @@ struct rcu_state {
 
 	unsigned long jiffies_force_qs;		/* Time at which to invoke */
 						/*  force_quiescent_state(). */
+	unsigned long jiffies_kick_kthreads;	/* Time at which to kick */
+						/*  kthreads, if configured. */
 	unsigned long n_force_qs;		/* Number of calls to */
 						/*  force_quiescent_state(). */
 	unsigned long n_force_qs_lh;		/* ~Number of calls leaving */

commit 3b5f668e715bc19610ad967ef97a7e8c55a186ec
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 16 16:47:55 2016 -0700

    rcu: Overlap wakeups with next expedited grace period
    
    The current expedited grace-period implementation makes subsequent grace
    periods wait on wakeups for the prior grace period.  This does not fit
    the dictionary definition of "expedited", so this commit allows these two
    phases to overlap.  Doing this requires four waitqueues rather than two
    because tasks can now be waiting on the previous, current, and next grace
    periods.  The fourth waitqueue makes the bit masking work out nicely.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index f9d4fbb1e014..1194ab0da56a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -250,7 +250,7 @@ struct rcu_node {
 
 	spinlock_t exp_lock ____cacheline_internodealigned_in_smp;
 	unsigned long exp_seq_rq;
-	wait_queue_head_t exp_wq[2];
+	wait_queue_head_t exp_wq[4];
 } ____cacheline_internodealigned_in_smp;
 
 /*
@@ -502,6 +502,7 @@ struct rcu_state {
 	/* End of fields guarded by barrier_mutex. */
 
 	struct mutex exp_mutex;			/* Serialize expedited GP. */
+	struct mutex exp_wake_mutex;		/* Serialize wakeup. */
 	unsigned long expedited_sequence;	/* Take a ticket. */
 	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
 	atomic_t expedited_need_qs;		/* # CPUs left to check in. */

commit f6a12f34a448cc8a624070fd365c29c890138a48
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Jan 30 17:57:35 2016 -0800

    rcu: Enforce expedited-GP fairness via funnel wait queue
    
    The current mutex-based funnel-locking approach used by expedited grace
    periods is subject to severe unfairness.  The problem arises when a
    few tasks, making a path from leaves to root, all wake up before other
    tasks do.  A new task can then follow this path all the way to the root,
    which needlessly delays tasks whose grace period is done, but who do
    not happen to acquire the lock quickly enough.
    
    This commit avoids this problem by maintaining per-rcu_node wait queues,
    along with a per-rcu_node counter that tracks the latest grace period
    sought by an earlier task to visit this node.  If that grace period
    would satisfy the current task, instead of proceeding up the tree,
    it waits on the current rcu_node structure using a pair of wait queues
    provided for that purpose.  This decouples awakening of old tasks from
    the arrival of new tasks.
    
    If the wakeups prove to be a bottleneck, additional kthreads can be
    brought to bear for that purpose.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6a8f09446924..f9d4fbb1e014 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -70,7 +70,6 @@
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0" }
-#  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0" }
 #elif NR_CPUS <= RCU_FANOUT_2
 #  define RCU_NUM_LVLS	      2
 #  define NUM_RCU_LVL_0	      1
@@ -79,7 +78,6 @@
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1" }
-#  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1" }
 #elif NR_CPUS <= RCU_FANOUT_3
 #  define RCU_NUM_LVLS	      3
 #  define NUM_RCU_LVL_0	      1
@@ -89,7 +87,6 @@
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1, NUM_RCU_LVL_2 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2" }
-#  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1", "rcu_node_exp_2" }
 #elif NR_CPUS <= RCU_FANOUT_4
 #  define RCU_NUM_LVLS	      4
 #  define NUM_RCU_LVL_0	      1
@@ -100,7 +97,6 @@
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1, NUM_RCU_LVL_2, NUM_RCU_LVL_3 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2", "rcu_node_3" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2", "rcu_node_fqs_3" }
-#  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1", "rcu_node_exp_2", "rcu_node_exp_3" }
 #else
 # error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
 #endif /* #if (NR_CPUS) <= RCU_FANOUT_1 */
@@ -252,7 +248,9 @@ struct rcu_node {
 				/* Counts of upcoming no-CB GP requests. */
 	raw_spinlock_t fqslock ____cacheline_internodealigned_in_smp;
 
-	struct mutex exp_funnel_mutex ____cacheline_internodealigned_in_smp;
+	spinlock_t exp_lock ____cacheline_internodealigned_in_smp;
+	unsigned long exp_seq_rq;
+	wait_queue_head_t exp_wq[2];
 } ____cacheline_internodealigned_in_smp;
 
 /*
@@ -387,7 +385,6 @@ struct rcu_data {
 #ifdef CONFIG_RCU_FAST_NO_HZ
 	struct rcu_head oom_head;
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
-	struct mutex exp_funnel_mutex;
 	atomic_long_t exp_workdone1;	/* # done by others #1. */
 	atomic_long_t exp_workdone2;	/* # done by others #2. */
 	atomic_long_t exp_workdone3;	/* # done by others #3. */
@@ -504,6 +501,7 @@ struct rcu_state {
 						/*  _rcu_barrier(). */
 	/* End of fields guarded by barrier_mutex. */
 
+	struct mutex exp_mutex;			/* Serialize expedited GP. */
 	unsigned long expedited_sequence;	/* Take a ticket. */
 	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
 	atomic_t expedited_need_qs;		/* # CPUs left to check in. */

commit d40a4f09a448382961fa9b1a2f7d4f34813f0273
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Mar 8 14:43:44 2016 -0800

    rcu: Shorten expedited_workdone* to exp_workdone*
    
    Just a name change to save a few lines and a bit of typing.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index ac9a7b0c36ae..6a8f09446924 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -388,9 +388,9 @@ struct rcu_data {
 	struct rcu_head oom_head;
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 	struct mutex exp_funnel_mutex;
-	atomic_long_t expedited_workdone1;	/* # done by others #1. */
-	atomic_long_t expedited_workdone2;	/* # done by others #2. */
-	atomic_long_t expedited_workdone3;	/* # done by others #3. */
+	atomic_long_t exp_workdone1;	/* # done by others #1. */
+	atomic_long_t exp_workdone2;	/* # done by others #2. */
+	atomic_long_t exp_workdone3;	/* # done by others #3. */
 
 	/* 7) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU

commit e2fd9d35847d1936398d44c4df68dceb3d7f64e7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Jan 30 17:23:19 2016 -0800

    rcu: Remove expedited GP funnel-lock bypass
    
    Commit #cdacbe1f91264 ("rcu: Add fastpath bypassing funnel locking")
    turns out to be a pessimization at high load because it forces a tree
    full of tasks to wait for an expedited grace period that they probably
    do not need.  This commit therefore removes this optimization.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index df668c0f9e64..ac9a7b0c36ae 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -388,7 +388,6 @@ struct rcu_data {
 	struct rcu_head oom_head;
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 	struct mutex exp_funnel_mutex;
-	atomic_long_t expedited_workdone0;	/* # done by others #0. */
 	atomic_long_t expedited_workdone1;	/* # done by others #1. */
 	atomic_long_t expedited_workdone2;	/* # done by others #2. */
 	atomic_long_t expedited_workdone3;	/* # done by others #3. */

commit 8bc6782fe20bd2584c73a35c47329c9fd0a8d34c
Merge: e23604edac2a 3500efae4410
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 15 09:00:12 2016 +0100

    Merge commit 'fixes.2015.02.23a' into core/rcu
    
     Conflicts:
            kernel/rcu/tree.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit abedf8e2419fb873d919dd74de2e84b510259339
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Feb 19 09:46:41 2016 +0100

    rcu: Use simple wait queues where possible in rcutree
    
    As of commit dae6e64d2bcfd ("rcu: Introduce proper blocking to no-CBs kthreads
    GP waits") the RCU subsystem started making use of wait queues.
    
    Here we convert all additions of RCU wait queues to use simple wait queues,
    since they don't need the extra overhead of the full wait queue features.
    
    Originally this was done for RT kernels[1], since we would get things like...
    
      BUG: sleeping function called from invalid context at kernel/rtmutex.c:659
      in_atomic(): 1, irqs_disabled(): 1, pid: 8, name: rcu_preempt
      Pid: 8, comm: rcu_preempt Not tainted
      Call Trace:
       [<ffffffff8106c8d0>] __might_sleep+0xd0/0xf0
       [<ffffffff817d77b4>] rt_spin_lock+0x24/0x50
       [<ffffffff8106fcf6>] __wake_up+0x36/0x70
       [<ffffffff810c4542>] rcu_gp_kthread+0x4d2/0x680
       [<ffffffff8105f910>] ? __init_waitqueue_head+0x50/0x50
       [<ffffffff810c4070>] ? rcu_gp_fqs+0x80/0x80
       [<ffffffff8105eabb>] kthread+0xdb/0xe0
       [<ffffffff8106b912>] ? finish_task_switch+0x52/0x100
       [<ffffffff817e0754>] kernel_thread_helper+0x4/0x10
       [<ffffffff8105e9e0>] ? __init_kthread_worker+0x60/0x60
       [<ffffffff817e0750>] ? gs_change+0xb/0xb
    
    ...and hence simple wait queues were deployed on RT out of necessity
    (as simple wait uses a raw lock), but mainline might as well take
    advantage of the more streamline support as well.
    
    [1] This is a carry forward of work from v3.10-rt; the original conversion
    was by Thomas on an earlier -rt version, and Sebastian extended it to
    additional post-3.10 added RCU waiters; here I've added a commit log and
    unified the RCU changes into one, and uprev'd it to match mainline RCU.
    
    Signed-off-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-rt-users@vger.kernel.org
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1455871601-27484-6-git-send-email-wagi@monom.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 10dedfbef09d..bbd235d0e71f 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -27,6 +27,7 @@
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 #include <linux/seqlock.h>
+#include <linux/swait.h>
 #include <linux/stop_machine.h>
 
 /*
@@ -243,7 +244,7 @@ struct rcu_node {
 				/* Refused to boost: not sure why, though. */
 				/*  This can happen due to race conditions. */
 #ifdef CONFIG_RCU_NOCB_CPU
-	wait_queue_head_t nocb_gp_wq[2];
+	struct swait_queue_head nocb_gp_wq[2];
 				/* Place for rcu_nocb_kthread() to wait GP. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 	int need_future_gp[2];
@@ -399,7 +400,7 @@ struct rcu_data {
 	atomic_long_t nocb_q_count_lazy; /*  invocation (all stages). */
 	struct rcu_head *nocb_follower_head; /* CBs ready to invoke. */
 	struct rcu_head **nocb_follower_tail;
-	wait_queue_head_t nocb_wq;	/* For nocb kthreads to sleep on. */
+	struct swait_queue_head nocb_wq; /* For nocb kthreads to sleep on. */
 	struct task_struct *nocb_kthread;
 	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
 
@@ -478,7 +479,7 @@ struct rcu_state {
 	unsigned long gpnum;			/* Current gp number. */
 	unsigned long completed;		/* # of last completed gp. */
 	struct task_struct *gp_kthread;		/* Task for grace periods. */
-	wait_queue_head_t gp_wq;		/* Where GP task waits. */
+	struct swait_queue_head gp_wq;		/* Where GP task waits. */
 	short gp_flags;				/* Commands for GP task. */
 	short gp_state;				/* GP kthread sleep state. */
 
@@ -506,7 +507,7 @@ struct rcu_state {
 	unsigned long expedited_sequence;	/* Take a ticket. */
 	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
 	atomic_t expedited_need_qs;		/* # CPUs left to check in. */
-	wait_queue_head_t expedited_wq;		/* Wait for check-ins. */
+	struct swait_queue_head expedited_wq;	/* Wait for check-ins. */
 	int ncpus_snap;				/* # CPUs seen last time. */
 
 	unsigned long jiffies_force_qs;		/* Time at which to invoke */
@@ -621,8 +622,8 @@ static void zero_cpu_stall_ticks(struct rcu_data *rdp);
 static void increment_cpu_stall_ticks(void);
 static bool rcu_nocb_cpu_needs_barrier(struct rcu_state *rsp, int cpu);
 static void rcu_nocb_gp_set(struct rcu_node *rnp, int nrq);
-static wait_queue_head_t *rcu_nocb_gp_get(struct rcu_node *rnp);
-static void rcu_nocb_gp_cleanup(wait_queue_head_t *sq);
+static struct swait_queue_head *rcu_nocb_gp_get(struct rcu_node *rnp);
+static void rcu_nocb_gp_cleanup(struct swait_queue_head *sq);
 static void rcu_init_one_nocb(struct rcu_node *rnp);
 static bool __call_rcu_nocb(struct rcu_data *rdp, struct rcu_head *rhp,
 			    bool lazy, unsigned long flags);

commit 065bb78c5b09df54d1c32e03227deb777ddff57b
Author: Daniel Wagner <daniel.wagner@bmw-carit.de>
Date:   Fri Feb 19 09:46:40 2016 +0100

    rcu: Do not call rcu_nocb_gp_cleanup() while holding rnp->lock
    
    rcu_nocb_gp_cleanup() is called while holding rnp->lock. Currently,
    this is okay because the wake_up_all() in rcu_nocb_gp_cleanup() will
    not enable the IRQs. lockdep is happy.
    
    By switching over using swait this is not true anymore. swake_up_all()
    enables the IRQs while processing the waiters. __do_softirq() can now
    run and will eventually call rcu_process_callbacks() which wants to
    grap nrp->lock.
    
    Let's move the rcu_nocb_gp_cleanup() call outside the lock before we
    switch over to swait.
    
    If we would hold the rnp->lock and use swait, lockdep reports
    following:
    
     =================================
     [ INFO: inconsistent lock state ]
     4.2.0-rc5-00025-g9a73ba0 #136 Not tainted
     ---------------------------------
     inconsistent {IN-SOFTIRQ-W} -> {SOFTIRQ-ON-W} usage.
     rcu_preempt/8 [HC0[0]:SC0[0]:HE1:SE1] takes:
      (rcu_node_1){+.?...}, at: [<ffffffff811387c7>] rcu_gp_kthread+0xb97/0xeb0
     {IN-SOFTIRQ-W} state was registered at:
       [<ffffffff81109b9f>] __lock_acquire+0xd5f/0x21e0
       [<ffffffff8110be0f>] lock_acquire+0xdf/0x2b0
       [<ffffffff81841cc9>] _raw_spin_lock_irqsave+0x59/0xa0
       [<ffffffff81136991>] rcu_process_callbacks+0x141/0x3c0
       [<ffffffff810b1a9d>] __do_softirq+0x14d/0x670
       [<ffffffff810b2214>] irq_exit+0x104/0x110
       [<ffffffff81844e96>] smp_apic_timer_interrupt+0x46/0x60
       [<ffffffff81842e70>] apic_timer_interrupt+0x70/0x80
       [<ffffffff810dba66>] rq_attach_root+0xa6/0x100
       [<ffffffff810dbc2d>] cpu_attach_domain+0x16d/0x650
       [<ffffffff810e4b42>] build_sched_domains+0x942/0xb00
       [<ffffffff821777c2>] sched_init_smp+0x509/0x5c1
       [<ffffffff821551e3>] kernel_init_freeable+0x172/0x28f
       [<ffffffff8182cdce>] kernel_init+0xe/0xe0
       [<ffffffff8184231f>] ret_from_fork+0x3f/0x70
     irq event stamp: 76
     hardirqs last  enabled at (75): [<ffffffff81841330>] _raw_spin_unlock_irq+0x30/0x60
     hardirqs last disabled at (76): [<ffffffff8184116f>] _raw_spin_lock_irq+0x1f/0x90
     softirqs last  enabled at (0): [<ffffffff810a8df2>] copy_process.part.26+0x602/0x1cf0
     softirqs last disabled at (0): [<          (null)>]           (null)
     other info that might help us debug this:
      Possible unsafe locking scenario:
            CPU0
            ----
       lock(rcu_node_1);
       <Interrupt>
         lock(rcu_node_1);
      *** DEADLOCK ***
     1 lock held by rcu_preempt/8:
      #0:  (rcu_node_1){+.?...}, at: [<ffffffff811387c7>] rcu_gp_kthread+0xb97/0xeb0
     stack backtrace:
     CPU: 0 PID: 8 Comm: rcu_preempt Not tainted 4.2.0-rc5-00025-g9a73ba0 #136
     Hardware name: Dell Inc. PowerEdge R820/066N7P, BIOS 2.0.20 01/16/2014
      0000000000000000 000000006d7e67d8 ffff881fb081fbd8 ffffffff818379e0
      0000000000000000 ffff881fb0812a00 ffff881fb081fc38 ffffffff8110813b
      0000000000000000 0000000000000001 ffff881f00000001 ffffffff8102fa4f
     Call Trace:
      [<ffffffff818379e0>] dump_stack+0x4f/0x7b
      [<ffffffff8110813b>] print_usage_bug+0x1db/0x1e0
      [<ffffffff8102fa4f>] ? save_stack_trace+0x2f/0x50
      [<ffffffff811087ad>] mark_lock+0x66d/0x6e0
      [<ffffffff81107790>] ? check_usage_forwards+0x150/0x150
      [<ffffffff81108898>] mark_held_locks+0x78/0xa0
      [<ffffffff81841330>] ? _raw_spin_unlock_irq+0x30/0x60
      [<ffffffff81108a28>] trace_hardirqs_on_caller+0x168/0x220
      [<ffffffff81108aed>] trace_hardirqs_on+0xd/0x10
      [<ffffffff81841330>] _raw_spin_unlock_irq+0x30/0x60
      [<ffffffff810fd1c7>] swake_up_all+0xb7/0xe0
      [<ffffffff811386e1>] rcu_gp_kthread+0xab1/0xeb0
      [<ffffffff811089bf>] ? trace_hardirqs_on_caller+0xff/0x220
      [<ffffffff81841341>] ? _raw_spin_unlock_irq+0x41/0x60
      [<ffffffff81137c30>] ? rcu_barrier+0x20/0x20
      [<ffffffff810d2014>] kthread+0x104/0x120
      [<ffffffff81841330>] ? _raw_spin_unlock_irq+0x30/0x60
      [<ffffffff810d1f10>] ? kthread_create_on_node+0x260/0x260
      [<ffffffff8184231f>] ret_from_fork+0x3f/0x70
      [<ffffffff810d1f10>] ? kthread_create_on_node+0x260/0x260
    
    Signed-off-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-rt-users@vger.kernel.org
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1455871601-27484-5-git-send-email-wagi@monom.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 83360b4f4352..10dedfbef09d 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -621,7 +621,8 @@ static void zero_cpu_stall_ticks(struct rcu_data *rdp);
 static void increment_cpu_stall_ticks(void);
 static bool rcu_nocb_cpu_needs_barrier(struct rcu_state *rsp, int cpu);
 static void rcu_nocb_gp_set(struct rcu_node *rnp, int nrq);
-static void rcu_nocb_gp_cleanup(struct rcu_state *rsp, struct rcu_node *rnp);
+static wait_queue_head_t *rcu_nocb_gp_get(struct rcu_node *rnp);
+static void rcu_nocb_gp_cleanup(wait_queue_head_t *sq);
 static void rcu_init_one_nocb(struct rcu_node *rnp);
 static bool __call_rcu_nocb(struct rcu_data *rdp, struct rcu_head *rhp,
 			    bool lazy, unsigned long flags);

commit 67c583a7de3433a971983490b37ad2bff3c55463
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Tue Dec 29 12:18:47 2015 +0800

    RCU: Privatize rcu_node::lock
    
    In patch:
    
    "rcu: Add transitivity to remaining rcu_node ->lock acquisitions"
    
    All locking operations on rcu_node::lock are replaced with the wrappers
    because of the need of transitivity, which indicates we should never
    write code using LOCK primitives alone(i.e. without a proper barrier
    following) on rcu_node::lock outside those wrappers. We could detect
    this kind of misuses on rcu_node::lock in the future by adding __private
    modifier on rcu_node::lock.
    
    To privatize rcu_node::lock, unlock wrappers are also needed. Replacing
    spinlock unlocks with these wrappers not only privatizes rcu_node::lock
    but also makes it easier to figure out critical sections of rcu_node.
    
    This patch adds __private modifier to rcu_node::lock and makes every
    access to it wrapped by ACCESS_PRIVATE(). Besides, unlock wrappers are
    added and raw_spin_unlock(&rnp->lock) and its friends are replaced with
    those wrappers.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 83360b4f4352..4886d6a03353 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -149,8 +149,9 @@ struct rcu_dynticks {
  * Definition for node within the RCU grace-period-detection hierarchy.
  */
 struct rcu_node {
-	raw_spinlock_t lock;	/* Root rcu_node's lock protects some */
-				/*  rcu_state fields as well as following. */
+	raw_spinlock_t __private lock;	/* Root rcu_node's lock protects */
+					/*  some rcu_state fields as well as */
+					/*  following. */
 	unsigned long gpnum;	/* Current grace period for this node. */
 				/*  This will either be equal to or one */
 				/*  behind the root rcu_node's gpnum. */
@@ -680,7 +681,7 @@ static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
 #endif /* #else #ifdef CONFIG_PPC */
 
 /*
- * Wrappers for the rcu_node::lock acquire.
+ * Wrappers for the rcu_node::lock acquire and release.
  *
  * Because the rcu_nodes form a tree, the tree traversal locking will observe
  * different lock values, this in turn means that an UNLOCK of one level
@@ -689,29 +690,48 @@ static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
  *
  * In order to restore full ordering between tree levels, augment the regular
  * lock acquire functions with smp_mb__after_unlock_lock().
+ *
+ * As ->lock of struct rcu_node is a __private field, therefore one should use
+ * these wrappers rather than directly call raw_spin_{lock,unlock}* on ->lock.
  */
 static inline void raw_spin_lock_rcu_node(struct rcu_node *rnp)
 {
-	raw_spin_lock(&rnp->lock);
+	raw_spin_lock(&ACCESS_PRIVATE(rnp, lock));
 	smp_mb__after_unlock_lock();
 }
 
+static inline void raw_spin_unlock_rcu_node(struct rcu_node *rnp)
+{
+	raw_spin_unlock(&ACCESS_PRIVATE(rnp, lock));
+}
+
 static inline void raw_spin_lock_irq_rcu_node(struct rcu_node *rnp)
 {
-	raw_spin_lock_irq(&rnp->lock);
+	raw_spin_lock_irq(&ACCESS_PRIVATE(rnp, lock));
 	smp_mb__after_unlock_lock();
 }
 
-#define raw_spin_lock_irqsave_rcu_node(rnp, flags)	\
-do {							\
-	typecheck(unsigned long, flags);		\
-	raw_spin_lock_irqsave(&(rnp)->lock, flags);	\
-	smp_mb__after_unlock_lock();			\
+static inline void raw_spin_unlock_irq_rcu_node(struct rcu_node *rnp)
+{
+	raw_spin_unlock_irq(&ACCESS_PRIVATE(rnp, lock));
+}
+
+#define raw_spin_lock_irqsave_rcu_node(rnp, flags)			\
+do {									\
+	typecheck(unsigned long, flags);				\
+	raw_spin_lock_irqsave(&ACCESS_PRIVATE(rnp, lock), flags);	\
+	smp_mb__after_unlock_lock();					\
+} while (0)
+
+#define raw_spin_unlock_irqrestore_rcu_node(rnp, flags)			\
+do {									\
+	typecheck(unsigned long, flags);				\
+	raw_spin_unlock_irqrestore(&ACCESS_PRIVATE(rnp, lock), flags);	\
 } while (0)
 
 static inline bool raw_spin_trylock_rcu_node(struct rcu_node *rnp)
 {
-	bool locked = raw_spin_trylock(&rnp->lock);
+	bool locked = raw_spin_trylock(&ACCESS_PRIVATE(rnp, lock));
 
 	if (locked)
 		smp_mb__after_unlock_lock();

commit 648c630c642a4869c7fc30345880675020298852
Merge: f84cfbb0ff26 79cfea027387 69b907297f4e 2f073848c3cc c979ff991764
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Dec 7 17:02:54 2015 -0800

    Merge branches 'doc.2015.12.05a', 'exp.2015.12.07a', 'fixes.2015.12.07a', 'list.2015.12.04b' and 'torture.2015.12.05a' into HEAD
    
    doc.2015.12.05a:  Documentation updates
    exp.2015.12.07a:  Expedited grace-period updates
    fixes.2015.12.07a:  Miscellaneous fixes
    list.2015.12.04b:  Linked-list updates
    torture.2015.12.05a:  Torture-test updates

commit 6b50e119c440b7532ed749b635a58b3839f62992
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Nov 17 14:39:26 2015 -0800

    rcutorture: Print symbolic name for ->gp_state
    
    Currently, ->gp_state is printed as an integer, which slows debugging.
    This commit therefore prints a symbolic name in addition to the integer.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Updated to fix relational operator called out by Dan Carpenter. ]
    [ paulmck: More "const", as suggested by Josh Triplett. ]
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index f32bebb6bc90..a3fb6fe94127 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -545,6 +545,18 @@ struct rcu_state {
 #define RCU_GP_CLEANUP   5	/* Grace-period cleanup started. */
 #define RCU_GP_CLEANED   6	/* Grace-period cleanup complete. */
 
+#ifndef RCU_TREE_NONCORE
+static const char * const gp_state_names[] = {
+	"RCU_GP_IDLE",
+	"RCU_GP_WAIT_GPS",
+	"RCU_GP_DONE_GPS",
+	"RCU_GP_WAIT_FQS",
+	"RCU_GP_DOING_FQS",
+	"RCU_GP_CLEANUP",
+	"RCU_GP_CLEANED",
+};
+#endif /* #ifndef RCU_TREE_NONCORE */
+
 extern struct list_head rcu_struct_flavors;
 
 /* Sequence through rcu_state structures for each RCU flavor. */

commit df5bd5144a80a9f6c3807383b11f735dae9caf9d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Oct 1 10:26:24 2015 -0700

    rcu: Reduce expedited GP memory contention via per-CPU variables
    
    Currently, the piggybacked-work checks carried out by sync_exp_work_done()
    atomically increment a small set of variables (the ->expedited_workdone0,
    ->expedited_workdone1, ->expedited_workdone2, ->expedited_workdone3
    fields in the rcu_state structure), which will form a memory-contention
    bottleneck given a sufficiently large number of CPUs concurrently invoking
    either synchronize_rcu_expedited() or synchronize_sched_expedited().
    
    This commit therefore moves these for fields to the per-CPU rcu_data
    structure, eliminating the memory contention.  The show_rcuexp() function
    also changes to sum up each field in the rcu_data structures.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8151971a8978..6cbec31f99d6 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -386,6 +386,10 @@ struct rcu_data {
 	struct rcu_head oom_head;
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 	struct mutex exp_funnel_mutex;
+	atomic_long_t expedited_workdone0;	/* # done by others #0. */
+	atomic_long_t expedited_workdone1;	/* # done by others #1. */
+	atomic_long_t expedited_workdone2;	/* # done by others #2. */
+	atomic_long_t expedited_workdone3;	/* # done by others #3. */
 
 	/* 7) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU
@@ -500,10 +504,6 @@ struct rcu_state {
 	/* End of fields guarded by barrier_mutex. */
 
 	unsigned long expedited_sequence;	/* Take a ticket. */
-	atomic_long_t expedited_workdone0;	/* # done by others #0. */
-	atomic_long_t expedited_workdone1;	/* # done by others #1. */
-	atomic_long_t expedited_workdone2;	/* # done by others #2. */
-	atomic_long_t expedited_workdone3;	/* # done by others #3. */
 	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
 	atomic_t expedited_need_qs;		/* # CPUs left to check in. */
 	wait_queue_head_t expedited_wq;		/* Wait for check-ins. */

commit 1de6e56ddc043437d335ee0455a1b34b73510c91
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Sep 29 09:45:00 2015 -0700

    rcu: Clarify role of ->expmaskinitnext
    
    Analogy with the ->qsmaskinitnext field might lead one to believe that
    ->expmaskinitnext tracks online CPUs.  This belief is incorrect: Any CPU
    that has ever been online will have its bit set in the ->expmaskinitnext
    field.  This commit therefore adds a comment to make this clear, at
    least to people who read comments.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index f32bebb6bc90..8151971a8978 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -178,6 +178,8 @@ struct rcu_node {
 				/*  beginning of each expedited GP. */
 	unsigned long expmaskinitnext;
 				/* Online CPUs for next expedited GP. */
+				/*  Any CPU that has ever been online will */
+				/*  have its bit set. */
 	unsigned long grpmask;	/* Mask to apply to parent qsmask. */
 				/*  Only one bit will be set in this mask. */
 	int	grplo;		/* lowest-numbered CPU or group here. */

commit 2a67e741bbbc022e0fadf8c6dbc3a76019ecd0cf
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 8 12:24:23 2015 +0200

    rcu: Create transitive rnp->lock acquisition functions
    
    Providing RCU's memory-ordering guarantees requires that the rcu_node
    tree's locking provide transitive memory ordering, which the Linux kernel's
    spinlocks currently do not provide unless smp_mb__after_unlock_lock()
    is used.  Having a separate smp_mb__after_unlock_lock() after each and
    every lock acquisition is error-prone, hard to read, and a bit annoying,
    so this commit provides wrapper functions that pull in the
    smp_mb__after_unlock_lock() invocations.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 9fb4e238d4dc..f32bebb6bc90 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -664,3 +664,42 @@ static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
 #else /* #ifdef CONFIG_PPC */
 #define smp_mb__after_unlock_lock()	do { } while (0)
 #endif /* #else #ifdef CONFIG_PPC */
+
+/*
+ * Wrappers for the rcu_node::lock acquire.
+ *
+ * Because the rcu_nodes form a tree, the tree traversal locking will observe
+ * different lock values, this in turn means that an UNLOCK of one level
+ * followed by a LOCK of another level does not imply a full memory barrier;
+ * and most importantly transitivity is lost.
+ *
+ * In order to restore full ordering between tree levels, augment the regular
+ * lock acquire functions with smp_mb__after_unlock_lock().
+ */
+static inline void raw_spin_lock_rcu_node(struct rcu_node *rnp)
+{
+	raw_spin_lock(&rnp->lock);
+	smp_mb__after_unlock_lock();
+}
+
+static inline void raw_spin_lock_irq_rcu_node(struct rcu_node *rnp)
+{
+	raw_spin_lock_irq(&rnp->lock);
+	smp_mb__after_unlock_lock();
+}
+
+#define raw_spin_lock_irqsave_rcu_node(rnp, flags)	\
+do {							\
+	typecheck(unsigned long, flags);		\
+	raw_spin_lock_irqsave(&(rnp)->lock, flags);	\
+	smp_mb__after_unlock_lock();			\
+} while (0)
+
+static inline bool raw_spin_trylock_rcu_node(struct rcu_node *rnp)
+{
+	bool locked = raw_spin_trylock(&rnp->lock);
+
+	if (locked)
+		smp_mb__after_unlock_lock();
+	return locked;
+}

commit d2856b046d2ce2bfb664727cb8671ad0e371bd6c
Merge: 7f5f873c6a07 338b0f760e84
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Oct 7 16:05:21 2015 -0700

    Merge branches 'fixes.2015.10.06a' and 'exp.2015.10.07a' into HEAD
    
    exp.2015.10.07a:  Reduce OS jitter of RCU-sched expedited grace periods.
    fixes.2015.10.06a:  Miscellaneous fixes.

commit 74611ecb0fc4c850a8f89a744ce99cbf0dd43cb2
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Aug 18 10:20:43 2015 -0700

    rcu: Add online/offline info to expedited stall warning message
    
    This commit makes the RCU CPU stall warning message print online/offline
    indications immediately after the CPU number.  A "O" indicates global
    offline, a "." global online, and a "o" indicates RCU believes that the
    CPU is offline for the current grace period and "." otherwise, and an
    "N" indicates that RCU believes that the CPU will be offline for the
    next grace period, and "." otherwise, all right after the CPU number.
    So for CPU 10, you would normally see "10-...:" indicating that everything
    believes that the CPU is online.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6f3b63b68886..191aa3678575 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -589,6 +589,7 @@ static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 #endif /* #ifdef CONFIG_HOTPLUG_CPU */
 static void rcu_print_detail_task_stall(struct rcu_state *rsp);
 static int rcu_print_task_stall(struct rcu_node *rnp);
+static int rcu_print_task_exp_stall(struct rcu_node *rnp);
 static void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp);
 static void rcu_preempt_check_callbacks(void);
 void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu));

commit 83c2c735e78da1a0d994911f730f6e1d36c88d7a
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 6 20:43:02 2015 -0700

    rcu: Stop silencing lockdep false positive for expedited grace periods
    
    This reverts commit af859beaaba4 (rcu: Silence lockdep false positive
    for expedited grace periods).  Because synchronize_rcu_expedited()
    no longer invokes synchronize_sched_expedited(), ->exp_funnel_mutex
    acquisition is no longer nested, so the false positive no longer happens.
    This commit therefore removes the extra lockdep data structures, as they
    are no longer needed.

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 1b969cef8fe4..6f3b63b68886 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -70,8 +70,6 @@
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0" }
 #  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0" }
-#  define RCU_EXP_SCHED_NAME_INIT \
-			      { "rcu_node_exp_sched_0" }
 #elif NR_CPUS <= RCU_FANOUT_2
 #  define RCU_NUM_LVLS	      2
 #  define NUM_RCU_LVL_0	      1
@@ -81,8 +79,6 @@
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1" }
 #  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1" }
-#  define RCU_EXP_SCHED_NAME_INIT \
-			      { "rcu_node_exp_sched_0", "rcu_node_exp_sched_1" }
 #elif NR_CPUS <= RCU_FANOUT_3
 #  define RCU_NUM_LVLS	      3
 #  define NUM_RCU_LVL_0	      1
@@ -93,8 +89,6 @@
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2" }
 #  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1", "rcu_node_exp_2" }
-#  define RCU_EXP_SCHED_NAME_INIT \
-			      { "rcu_node_exp_sched_0", "rcu_node_exp_sched_1", "rcu_node_exp_sched_2" }
 #elif NR_CPUS <= RCU_FANOUT_4
 #  define RCU_NUM_LVLS	      4
 #  define NUM_RCU_LVL_0	      1
@@ -106,8 +100,6 @@
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2", "rcu_node_3" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2", "rcu_node_fqs_3" }
 #  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1", "rcu_node_exp_2", "rcu_node_exp_3" }
-#  define RCU_EXP_SCHED_NAME_INIT \
-			      { "rcu_node_exp_sched_0", "rcu_node_exp_sched_1", "rcu_node_exp_sched_2", "rcu_node_exp_sched_3" }
 #else
 # error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
 #endif /* #if (NR_CPUS) <= RCU_FANOUT_1 */

commit 6587a23b6b9bdb47205ec96c703e5bf8a2d39701
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 6 16:50:39 2015 -0700

    rcu: Switch synchronize_sched_expedited() to IPI
    
    This commit switches synchronize_sched_expedited() from stop_one_cpu_nowait()
    to smp_call_function_single(), thus moving from an IPI and a pair of
    context switches to an IPI and a single pass through the scheduler.
    Of course, if the scheduler actually does decide to switch to a different
    task, there will still be a pair of context switches, but there would
    likely have been a pair of context switches anyway, just a bit later.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 3eee48bcf52b..1b969cef8fe4 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -324,9 +324,6 @@ struct rcu_data {
 					/*  ticks this CPU has handled */
 					/*  during and after the last grace */
 					/* period it is aware of. */
-	struct cpu_stop_work exp_stop_work;
-					/* Expedited grace-period control */
-					/*  for CPU stopping. */
 
 	/* 2) batch handling */
 	/*

commit c34d2f418485a5d710bc002e490148b8ea53f456
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Sep 10 11:21:28 2015 -0700

    rcu: Correct comment for values of ->gp_state field
    
    This commit corrects the comment for the values of the ->gp_state field,
    which previously incorrectly said that these were for the ->gp_flags
    field.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index be6d1e8eeb79..674ebbc3e406 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -529,7 +529,7 @@ struct rcu_state {
 #define RCU_GP_FLAG_INIT 0x1	/* Need grace-period initialization. */
 #define RCU_GP_FLAG_FQS  0x2	/* Need grace-period quiescent-state forcing. */
 
-/* Values for rcu_state structure's gp_flags field. */
+/* Values for rcu_state structure's gp_state field. */
 #define RCU_GP_IDLE	 0	/* Initial state and no GP in progress. */
 #define RCU_GP_WAIT_GPS  1	/* Wait for grace-period start. */
 #define RCU_GP_DONE_GPS  2	/* Wait done for grace-period start. */

commit 77f81fe08ebd99d7e0eefde42ddac06a675bc4ad
Author: Petr Mladek <pmladek@suse.com>
Date:   Wed Sep 9 12:09:49 2015 -0700

    rcu: Finish folding ->fqs_state into ->gp_state
    
    Commit commit 4cdfc175c25c89ee ("rcu: Move quiescent-state forcing
    into kthread") started the process of folding the old ->fqs_state into
    ->gp_state, but did not complete it.  This situation does not cause
    any malfunction, but can result in extremely confusing trace output.
    This commit completes this task of eliminating ->fqs_state in favor
    of ->gp_state.
    
    The old ->fqs_state was also used to decide when to collect dyntick-idle
    snapshots.  For this purpose, we add a boolean variable into the kthread,
    which is set on the first call to rcu_gp_fqs() for a given grace period
    and clear otherwise.
    
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 0c33c82cec64..be6d1e8eeb79 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -412,13 +412,6 @@ struct rcu_data {
 	struct rcu_state *rsp;
 };
 
-/* Values for fqs_state field in struct rcu_state. */
-#define RCU_GP_IDLE		0	/* No grace period in progress. */
-#define RCU_GP_INIT		1	/* Grace period being initialized. */
-#define RCU_SAVE_DYNTICK	2	/* Need to scan dyntick state. */
-#define RCU_FORCE_QS		3	/* Need to force quiescent state. */
-#define RCU_SIGNAL_INIT		RCU_SAVE_DYNTICK
-
 /* Values for nocb_defer_wakeup field in struct rcu_data. */
 #define RCU_NOGP_WAKE_NOT	0
 #define RCU_NOGP_WAKE		1
@@ -468,9 +461,8 @@ struct rcu_state {
 
 	/* The following fields are guarded by the root rcu_node's lock. */
 
-	u8	fqs_state ____cacheline_internodealigned_in_smp;
-						/* Force QS state. */
-	u8	boost;				/* Subject to priority boost. */
+	u8	boost ____cacheline_internodealigned_in_smp;
+						/* Subject to priority boost. */
 	unsigned long gpnum;			/* Current gp number. */
 	unsigned long completed;		/* # of last completed gp. */
 	struct task_struct *gp_kthread;		/* Task for grace periods. */
@@ -538,7 +530,7 @@ struct rcu_state {
 #define RCU_GP_FLAG_FQS  0x2	/* Need grace-period quiescent-state forcing. */
 
 /* Values for rcu_state structure's gp_flags field. */
-#define RCU_GP_WAIT_INIT 0	/* Initial state. */
+#define RCU_GP_IDLE	 0	/* Initial state and no GP in progress. */
 #define RCU_GP_WAIT_GPS  1	/* Wait for grace-period start. */
 #define RCU_GP_DONE_GPS  2	/* Wait done for grace-period start. */
 #define RCU_GP_WAIT_FQS  3	/* Wait for force-quiescent-state time. */

commit db3e8db45e1cbf8cc22f1083a559d78eb91ccd63
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Wed Jul 29 13:29:39 2015 +0800

    rcu: Use call_rcu_func_t to replace explicit type equivalents
    
    We have had the call_rcu_func_t typedef for a quite awhile, but we still
    use explicit function pointer types in some places.  These types can
    confuse cscope and can be hard to read.  This patch therefore replaces
    these types with the call_rcu_func_t typedef.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index ad11529375cc..0c33c82cec64 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -464,8 +464,7 @@ struct rcu_state {
 						/*  shut bogus gcc warning) */
 	u8 flavor_mask;				/* bit in flavor mask. */
 	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */
-	void (*call)(struct rcu_head *head,	/* call_rcu() flavor. */
-		     void (*func)(struct rcu_head *head));
+	call_rcu_func_t call;			/* call_rcu() flavor. */
 
 	/* The following fields are guarded by the root rcu_node's lock. */
 

commit b6a4ae766e3133a4db73fabc81e522d1601cb623
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Wed Jul 29 13:29:38 2015 +0800

    rcu: Use rcu_callback_t in call_rcu*() and friends
    
    As we now have rcu_callback_t typedefs as the type of rcu callbacks, we
    should use it in call_rcu*() and friends as the type of parameters. This
    could save us a few lines of code and make it clear which function
    requires an rcu callbacks rather than other callbacks as its argument.
    
    Besides, this can also help cscope to generate a better database for
    code reading.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 2e991f8361e4..ad11529375cc 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -584,7 +584,7 @@ static void rcu_print_detail_task_stall(struct rcu_state *rsp);
 static int rcu_print_task_stall(struct rcu_node *rnp);
 static void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp);
 static void rcu_preempt_check_callbacks(void);
-void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu));
+void call_rcu(struct rcu_head *head, rcu_callback_t func);
 static void __init __rcu_init_preempt(void);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
 static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);

commit 5b74c458906fc4a62f932ee8bb801d29baf15fec
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 6 15:16:57 2015 -0700

    rcu: Make ->cpu_no_qs be a union for aggregate OR
    
    This commit converts the rcu_data structure's ->cpu_no_qs field
    to a union.  The bytewise side of this union allows individual access
    to indications as to whether this CPU needs to find a quiescent state
    for a normal (.norm) and/or expedited (.exp) grace period.  The setwise
    side of the union allows testing whether or not a quiescent state is
    needed at all, for either type of grace period.
    
    For now, only .norm is used.  A later commit will introduce the expedited
    usage.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index ded4ceebed76..3eee48bcf52b 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -286,6 +286,18 @@ struct rcu_node {
 	for ((rnp) = (rsp)->level[rcu_num_lvls - 1]; \
 	     (rnp) < &(rsp)->node[rcu_num_nodes]; (rnp)++)
 
+/*
+ * Union to allow "aggregate OR" operation on the need for a quiescent
+ * state by the normal and expedited grace periods.
+ */
+union rcu_noqs {
+	struct {
+		u8 norm;
+		u8 exp;
+	} b; /* Bits. */
+	u16 s; /* Set of bits, aggregate OR here. */
+};
+
 /* Index values for nxttail array in struct rcu_data. */
 #define RCU_DONE_TAIL		0	/* Also RCU_WAIT head. */
 #define RCU_WAIT_TAIL		1	/* Also RCU_NEXT_READY head. */
@@ -302,7 +314,7 @@ struct rcu_data {
 					/*  is aware of having started. */
 	unsigned long	rcu_qs_ctr_snap;/* Snapshot of rcu_qs_ctr to check */
 					/*  for rcu_all_qs() invocations. */
-	bool		cpu_no_qs;	/* No QS yet for this CPU. */
+	union rcu_noqs	cpu_no_qs;	/* No QSes yet for this CPU. */
 	bool		core_needs_qs;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */
 	bool		gpwrap;		/* Possible gpnum/completed wrap. */

commit 0d43eb34f9aabcddf41c99b7af2d0ced33e9a3cc
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 6 14:17:29 2015 -0700

    rcu: Invert passed_quiesce and rename to cpu_no_qs
    
    This commit inverts the sense of the rcu_data structure's ->passed_quiesce
    field and renames it to ->cpu_no_qs.  This will allow a later commit to
    use an "aggregate OR" operation to test expedited as well as normal grace
    periods without added overhead.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 4a0f30676ba8..ded4ceebed76 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -302,7 +302,7 @@ struct rcu_data {
 					/*  is aware of having started. */
 	unsigned long	rcu_qs_ctr_snap;/* Snapshot of rcu_qs_ctr to check */
 					/*  for rcu_all_qs() invocations. */
-	bool		passed_quiesce;	/* User-mode/idle loop etc. */
+	bool		cpu_no_qs;	/* No QS yet for this CPU. */
 	bool		core_needs_qs;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */
 	bool		gpwrap;		/* Possible gpnum/completed wrap. */

commit 97c668b8e983b722e2ed765b98b05f644aff1b13
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 6 11:31:51 2015 -0700

    rcu: Rename qs_pending to core_needs_qs
    
    An upcoming commit needs to invert the sense of the ->passed_quiesce
    rcu_data structure field, so this commit is taking this opportunity
    to clarify things a bit by renaming ->qs_pending to ->core_needs_qs.
    
    So if !rdp->core_needs_qs, then this CPU need not concern itself with
    quiescent states, in particular, it need not acquire its leaf rcu_node
    structure's ->lock to check.  Otherwise, it needs to report the next
    quiescent state.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index efe361c764ab..4a0f30676ba8 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -303,7 +303,7 @@ struct rcu_data {
 	unsigned long	rcu_qs_ctr_snap;/* Snapshot of rcu_qs_ctr to check */
 					/*  for rcu_all_qs() invocations. */
 	bool		passed_quiesce;	/* User-mode/idle loop etc. */
-	bool		qs_pending;	/* Core waits for quiesc state. */
+	bool		core_needs_qs;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */
 	bool		gpwrap;		/* Possible gpnum/completed wrap. */
 	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
@@ -368,7 +368,7 @@ struct rcu_data {
 
 	/* 5) __rcu_pending() statistics. */
 	unsigned long n_rcu_pending;	/* rcu_pending() calls since boot. */
-	unsigned long n_rp_qs_pending;
+	unsigned long n_rp_core_needs_qs;
 	unsigned long n_rp_report_qs;
 	unsigned long n_rp_cb_ready;
 	unsigned long n_rp_cpu_needs_gp;

commit bce5fa12aad148e15efd9bc0015dc4898b6e723b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Aug 5 16:03:54 2015 -0700

    rcu: Move synchronize_sched_expedited() to combining tree
    
    Currently, synchronize_sched_expedited() uses a single global counter
    to track the number of remaining context switches that the current
    expedited grace period must wait on.  This is problematic on large
    systems, where the resulting memory contention can be pathological.
    This commit therefore makes synchronize_sched_expedited() instead use
    the combining tree in the same manner as synchronize_rcu_expedited(),
    keeping memory contention down to a dull roar.
    
    This commit creates a temporary function sync_sched_exp_select_cpus()
    that is very similar to sync_rcu_exp_select_cpus().  A later commit
    will consolidate these two functions, which becomes possible when
    synchronize_sched_expedited() switches from stop_one_cpu_nowait() to
    smp_call_function_single().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a57f25ecca58..efe361c764ab 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -383,7 +383,6 @@ struct rcu_data {
 	struct rcu_head oom_head;
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 	struct mutex exp_funnel_mutex;
-	bool exp_done;			/* Expedited QS for this CPU? */
 
 	/* 7) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU

commit b9585e940a0d78770cda8f9aebf81b17b4d19e6d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jul 31 16:04:45 2015 -0700

    rcu: Consolidate tree setup for synchronize_rcu_expedited()
    
    This commit replaces sync_rcu_preempt_exp_init1(() and
    sync_rcu_preempt_exp_init2() with sync_exp_reset_tree_hotplug()
    and sync_exp_reset_tree(), which will also be used by
    synchronize_sched_expedited(), and sync_rcu_exp_select_nodes(), which
    contains code specific to synchronize_rcu_expedited().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 2e991f8361e4..a57f25ecca58 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -171,16 +171,21 @@ struct rcu_node {
 				/*  an rcu_data structure, otherwise, each */
 				/*  bit corresponds to a child rcu_node */
 				/*  structure. */
-	unsigned long expmask;	/* Groups that have ->blkd_tasks */
-				/*  elements that need to drain to allow the */
-				/*  current expedited grace period to */
-				/*  complete (only for PREEMPT_RCU). */
 	unsigned long qsmaskinit;
-				/* Per-GP initial value for qsmask & expmask. */
+				/* Per-GP initial value for qsmask. */
 				/*  Initialized from ->qsmaskinitnext at the */
 				/*  beginning of each grace period. */
 	unsigned long qsmaskinitnext;
 				/* Online CPUs for next grace period. */
+	unsigned long expmask;	/* CPUs or groups that need to check in */
+				/*  to allow the current expedited GP */
+				/*  to complete. */
+	unsigned long expmaskinit;
+				/* Per-GP initial values for expmask. */
+				/*  Initialized from ->expmaskinitnext at the */
+				/*  beginning of each expedited GP. */
+	unsigned long expmaskinitnext;
+				/* Online CPUs for next expedited GP. */
 	unsigned long grpmask;	/* Mask to apply to parent qsmask. */
 				/*  Only one bit will be set in this mask. */
 	int	grplo;		/* lowest-numbered CPU or group here. */
@@ -466,6 +471,7 @@ struct rcu_state {
 	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */
 	void (*call)(struct rcu_head *head,	/* call_rcu() flavor. */
 		     void (*func)(struct rcu_head *head));
+	int ncpus;				/* # CPUs seen so far. */
 
 	/* The following fields are guarded by the root rcu_node's lock. */
 
@@ -508,6 +514,7 @@ struct rcu_state {
 	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
 	atomic_t expedited_need_qs;		/* # CPUs left to check in. */
 	wait_queue_head_t expedited_wq;		/* Wait for check-ins. */
+	int ncpus_snap;				/* # CPUs seen last time. */
 
 	unsigned long jiffies_force_qs;		/* Time at which to invoke */
 						/*  force_quiescent_state(). */

commit 12d560f4ea87030667438a169912380be00cea4b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 14 18:35:23 2015 -0700

    rcu,locking: Privatize smp_mb__after_unlock_lock()
    
    RCU is the only thing that uses smp_mb__after_unlock_lock(), and is
    likely the only thing that ever will use it, so this commit makes this
    macro private to RCU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "linux-arch@vger.kernel.org" <linux-arch@vger.kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 0412030ca882..2e991f8361e4 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -653,3 +653,15 @@ static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
 #endif /* #else #ifdef CONFIG_RCU_NOCB_CPU */
 }
 #endif /* #ifdef CONFIG_RCU_TRACE */
+
+/*
+ * Place this after a lock-acquisition primitive to guarantee that
+ * an UNLOCK+LOCK pair act as a full barrier.  This guarantee applies
+ * if the UNLOCK and LOCK are executed by the same CPU or if the
+ * UNLOCK and LOCK operate on the same lock variable.
+ */
+#ifdef CONFIG_PPC
+#define smp_mb__after_unlock_lock()	smp_mb()  /* Full ordering for lock. */
+#else /* #ifdef CONFIG_PPC */
+#define smp_mb__after_unlock_lock()	do { } while (0)
+#endif /* #else #ifdef CONFIG_PPC */

commit af859beaaba4d57883b08f4acbcb3974bc1f975e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Jul 19 15:13:40 2015 -0700

    rcu: Silence lockdep false positive for expedited grace periods
    
    In a CONFIG_PREEMPT=y kernel, synchronize_rcu_expedited()
    acquires the ->exp_funnel_mutex in rcu_preempt_state, then invokes
    synchronize_sched_expedited, which acquires the ->exp_funnel_mutex in
    rcu_sched_state.  There can be no deadlock because rcu_preempt_state
    ->exp_funnel_mutex acquisition always precedes that of rcu_sched_state.
    But lockdep does not know that, so it gives false-positive splats.
    
    This commit therefore associates a separate lock_class_key structure
    with the rcu_sched_state structure's ->exp_funnel_mutex, allowing
    lockdep to see the lock ordering, avoiding the false positives.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 80d974df0ea0..0412030ca882 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -70,6 +70,8 @@
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0" }
 #  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0" }
+#  define RCU_EXP_SCHED_NAME_INIT \
+			      { "rcu_node_exp_sched_0" }
 #elif NR_CPUS <= RCU_FANOUT_2
 #  define RCU_NUM_LVLS	      2
 #  define NUM_RCU_LVL_0	      1
@@ -79,6 +81,8 @@
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1" }
 #  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1" }
+#  define RCU_EXP_SCHED_NAME_INIT \
+			      { "rcu_node_exp_sched_0", "rcu_node_exp_sched_1" }
 #elif NR_CPUS <= RCU_FANOUT_3
 #  define RCU_NUM_LVLS	      3
 #  define NUM_RCU_LVL_0	      1
@@ -89,6 +93,8 @@
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2" }
 #  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1", "rcu_node_exp_2" }
+#  define RCU_EXP_SCHED_NAME_INIT \
+			      { "rcu_node_exp_sched_0", "rcu_node_exp_sched_1", "rcu_node_exp_sched_2" }
 #elif NR_CPUS <= RCU_FANOUT_4
 #  define RCU_NUM_LVLS	      4
 #  define NUM_RCU_LVL_0	      1
@@ -100,6 +106,8 @@
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2", "rcu_node_3" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2", "rcu_node_fqs_3" }
 #  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1", "rcu_node_exp_2", "rcu_node_exp_3" }
+#  define RCU_EXP_SCHED_NAME_INIT \
+			      { "rcu_node_exp_sched_0", "rcu_node_exp_sched_1", "rcu_node_exp_sched_2", "rcu_node_exp_sched_3" }
 #else
 # error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
 #endif /* #if (NR_CPUS) <= RCU_FANOUT_1 */

commit cdacbe1f91264687af956e810278030f2ab5a3d0
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Jul 11 16:24:45 2015 -0700

    rcu: Add fastpath bypassing funnel locking
    
    In the common case, there will be only one expedited grace period in
    the system at a given time, in which case it is not helpful to use
    funnel locking.  This commit therefore adds a fastpath that bypasses
    funnel locking when the root ->exp_funnel_mutex is not held.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 543ba726396c..80d974df0ea0 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -493,7 +493,7 @@ struct rcu_state {
 	/* End of fields guarded by barrier_mutex. */
 
 	unsigned long expedited_sequence;	/* Take a ticket. */
-	atomic_long_t expedited_tryfail;	/* # acquisition failures. */
+	atomic_long_t expedited_workdone0;	/* # done by others #0. */
 	atomic_long_t expedited_workdone1;	/* # done by others #1. */
 	atomic_long_t expedited_workdone2;	/* # done by others #2. */
 	atomic_long_t expedited_workdone3;	/* # done by others #3. */

commit 32bb1c79996069ef9e4e53b428050749f9841c3f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jul 2 12:27:31 2015 -0700

    rcu: Rename RCU_GP_DONE_FQS to RCU_GP_DOING_FQS
    
    The grace-period kthread sleeps waiting to do a force-quiescent-state
    scan, and when awakened sets rsp->gp_state to RCU_GP_DONE_FQS.
    However, this is confusing because the kthread has not done the
    force-quiescent-state, but is instead just starting to do it.  This commit
    therefore renames RCU_GP_DONE_FQS to RCU_GP_DOING_FQS in order to make
    things a bit easier on reviewers.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index b3ae8d3cffbc..543ba726396c 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -535,7 +535,7 @@ struct rcu_state {
 #define RCU_GP_WAIT_GPS  1	/* Wait for grace-period start. */
 #define RCU_GP_DONE_GPS  2	/* Wait done for grace-period start. */
 #define RCU_GP_WAIT_FQS  3	/* Wait for force-quiescent-state time. */
-#define RCU_GP_DONE_FQS  4	/* Wait done for force-quiescent-state time. */
+#define RCU_GP_DOING_FQS 4	/* Wait done for force-quiescent-state time. */
 #define RCU_GP_CLEANUP   5	/* Grace-period cleanup started. */
 #define RCU_GP_CLEANED   6	/* Grace-period cleanup complete. */
 

commit cf3620a6c7798be3395163d3bb863ab378a6aa80
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 30 11:14:32 2015 -0700

    rcu: Add stall warnings to synchronize_sched_expedited()
    
    Although synchronize_sched_expedited() historically has no RCU CPU stall
    warnings, the availability of the rcupdate.rcu_expedited boot parameter
    invalidates the old assumption that synchronize_sched()'s stall warnings
    would suffice.  This commit therefore adds RCU CPU stall warnings to
    synchronize_sched_expedited().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index efee84ce1e08..b3ae8d3cffbc 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -370,6 +370,7 @@ struct rcu_data {
 	struct rcu_head oom_head;
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
 	struct mutex exp_funnel_mutex;
+	bool exp_done;			/* Expedited QS for this CPU? */
 
 	/* 7) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU

commit 2cd6ffafec066118365f6d7eb7a42ea16c1f032c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jun 29 17:06:39 2015 -0700

    rcu: Extend expedited funnel locking to rcu_data structure
    
    The strictly rcu_node based funnel-locking scheme works well in many
    cases, but systems with CONFIG_RCU_FANOUT_LEAF=64 won't necessarily get
    all that much concurrency.  This commit therefore extends the funnel
    locking into the per-CPU rcu_data structure, providing concurrency equal
    to the number of CPUs.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 5c1042d9c310..efee84ce1e08 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -364,11 +364,12 @@ struct rcu_data {
 	unsigned long n_rp_nocb_defer_wakeup;
 	unsigned long n_rp_need_nothing;
 
-	/* 6) _rcu_barrier() and OOM callbacks. */
+	/* 6) _rcu_barrier(), OOM callbacks, and expediting. */
 	struct rcu_head barrier_head;
 #ifdef CONFIG_RCU_FAST_NO_HZ
 	struct rcu_head oom_head;
 #endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
+	struct mutex exp_funnel_mutex;
 
 	/* 7) Callback offloading. */
 #ifdef CONFIG_RCU_NOCB_CPU
@@ -494,6 +495,7 @@ struct rcu_state {
 	atomic_long_t expedited_tryfail;	/* # acquisition failures. */
 	atomic_long_t expedited_workdone1;	/* # done by others #1. */
 	atomic_long_t expedited_workdone2;	/* # done by others #2. */
+	atomic_long_t expedited_workdone3;	/* # done by others #3. */
 	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
 	atomic_t expedited_need_qs;		/* # CPUs left to check in. */
 	wait_queue_head_t expedited_wq;		/* Wait for check-ins. */

commit 4f525a528b9e75571c6bedc6202beff1ced24c32
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jun 26 11:20:00 2015 -0700

    rcu: Apply rcu_seq operations to _rcu_barrier()
    
    The rcu_seq operations were open-coded in _rcu_barrier(), so this commit
    replaces the open-coding with the shiny new rcu_seq operations.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 4edc277d08eb..5c1042d9c310 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -486,7 +486,7 @@ struct rcu_state {
 	struct mutex barrier_mutex;		/* Guards barrier fields. */
 	atomic_t barrier_cpu_count;		/* # CPUs waiting on. */
 	struct completion barrier_completion;	/* Wake at barrier end. */
-	unsigned long n_barrier_done;		/* ++ at start and end of */
+	unsigned long barrier_sequence;		/* ++ at start and end of */
 						/*  _rcu_barrier(). */
 	/* End of fields guarded by barrier_mutex. */
 

commit 3a6d7c64d78a78d279851524d39999637a549363
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jun 25 11:27:10 2015 -0700

    rcu: Make expedited GP CPU stoppage asynchronous
    
    Sequentially stopping the CPUs slows down expedited grace periods by
    at least a factor of two, based on rcutorture's grace-period-per-second
    rate.  This is a conservative measure because rcutorture uses unusually
    long RCU read-side critical sections and because rcutorture periodically
    quiesces the system in order to test RCU's ability to ramp down to and
    up from the idle state.  This commit therefore replaces the stop_one_cpu()
    with stop_one_cpu_nowait(), using an atomic-counter scheme to determine
    when all CPUs have passed through the stopped state.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 2ef036b356f7..4edc277d08eb 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -27,6 +27,7 @@
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 #include <linux/seqlock.h>
+#include <linux/stop_machine.h>
 
 /*
  * Define shape of hierarchy based on NR_CPUS, CONFIG_RCU_FANOUT, and
@@ -298,6 +299,9 @@ struct rcu_data {
 					/*  ticks this CPU has handled */
 					/*  during and after the last grace */
 					/* period it is aware of. */
+	struct cpu_stop_work exp_stop_work;
+					/* Expedited grace-period control */
+					/*  for CPU stopping. */
 
 	/* 2) batch handling */
 	/*
@@ -491,6 +495,8 @@ struct rcu_state {
 	atomic_long_t expedited_workdone1;	/* # done by others #1. */
 	atomic_long_t expedited_workdone2;	/* # done by others #2. */
 	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
+	atomic_t expedited_need_qs;		/* # CPUs left to check in. */
+	wait_queue_head_t expedited_wq;		/* Wait for check-ins. */
 
 	unsigned long jiffies_force_qs;		/* Time at which to invoke */
 						/*  force_quiescent_state(). */

commit 385b73c06f6a733547d0a7714d0c4cb4c8788b88
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jun 24 14:20:08 2015 -0700

    rcu: Get rid of synchronize_sched_expedited()'s polling loop
    
    This commit gets rid of synchronize_sched_expedited()'s mutex_trylock()
    polling loop in favor of a funnel-locking scheme based on the rcu_node
    tree.  The work-done check is done at each level of the tree, allowing
    high-contention situations to be resolved quickly with reasonable levels
    of mutex contention.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6a2b741436de..2ef036b356f7 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -68,6 +68,7 @@
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0" }
+#  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0" }
 #elif NR_CPUS <= RCU_FANOUT_2
 #  define RCU_NUM_LVLS	      2
 #  define NUM_RCU_LVL_0	      1
@@ -76,6 +77,7 @@
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1" }
+#  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1" }
 #elif NR_CPUS <= RCU_FANOUT_3
 #  define RCU_NUM_LVLS	      3
 #  define NUM_RCU_LVL_0	      1
@@ -85,6 +87,7 @@
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1, NUM_RCU_LVL_2 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2" }
+#  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1", "rcu_node_exp_2" }
 #elif NR_CPUS <= RCU_FANOUT_4
 #  define RCU_NUM_LVLS	      4
 #  define NUM_RCU_LVL_0	      1
@@ -95,6 +98,7 @@
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1, NUM_RCU_LVL_2, NUM_RCU_LVL_3 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2", "rcu_node_3" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2", "rcu_node_fqs_3" }
+#  define RCU_EXP_NAME_INIT   { "rcu_node_exp_0", "rcu_node_exp_1", "rcu_node_exp_2", "rcu_node_exp_3" }
 #else
 # error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
 #endif /* #if (NR_CPUS) <= RCU_FANOUT_1 */
@@ -237,6 +241,8 @@ struct rcu_node {
 	int need_future_gp[2];
 				/* Counts of upcoming no-CB GP requests. */
 	raw_spinlock_t fqslock ____cacheline_internodealigned_in_smp;
+
+	struct mutex exp_funnel_mutex ____cacheline_internodealigned_in_smp;
 } ____cacheline_internodealigned_in_smp;
 
 /*
@@ -480,12 +486,10 @@ struct rcu_state {
 						/*  _rcu_barrier(). */
 	/* End of fields guarded by barrier_mutex. */
 
-	struct mutex  expedited_mutex;		/* Serializes expediting. */
 	unsigned long expedited_sequence;	/* Take a ticket. */
 	atomic_long_t expedited_tryfail;	/* # acquisition failures. */
 	atomic_long_t expedited_workdone1;	/* # done by others #1. */
 	atomic_long_t expedited_workdone2;	/* # done by others #2. */
-	unsigned long expedited_workdone3;	/* # done by others #3. */
 	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
 
 	unsigned long jiffies_force_qs;		/* Time at which to invoke */

commit d6ada2cf2f81dab8a231d0ef8fb5dec4f5ac8379
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jun 24 10:46:30 2015 -0700

    rcu: Rework synchronize_sched_expedited() counter handling
    
    Now that synchronize_sched_expedited() have a mutex, it can use simpler
    work-already-done detection scheme.  This commit simplifies this scheme
    by using something similar to the sequence-locking counter scheme.
    A counter is incremented before and after each grace period, so that
    the counter is odd in the midst of the grace period and even otherwise.
    So if the counter has advanced to the second even number that is
    greater than or equal to the snapshot, the required grace period has
    already happened.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7c25fe473ad9..6a2b741436de 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -481,17 +481,12 @@ struct rcu_state {
 	/* End of fields guarded by barrier_mutex. */
 
 	struct mutex  expedited_mutex;		/* Serializes expediting. */
-	atomic_long_t expedited_start;		/* Starting ticket. */
-	atomic_long_t expedited_done;		/* Done ticket. */
-	atomic_long_t expedited_wrap;		/* # near-wrap incidents. */
+	unsigned long expedited_sequence;	/* Take a ticket. */
 	atomic_long_t expedited_tryfail;	/* # acquisition failures. */
 	atomic_long_t expedited_workdone1;	/* # done by others #1. */
 	atomic_long_t expedited_workdone2;	/* # done by others #2. */
+	unsigned long expedited_workdone3;	/* # done by others #3. */
 	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
-	atomic_long_t expedited_stoppedcpus;	/* # successful stop_cpus. */
-	atomic_long_t expedited_done_tries;	/* # tries to update _done. */
-	atomic_long_t expedited_done_lost;	/* # times beaten to _done. */
-	atomic_long_t expedited_done_exit;	/* # times exited _done loop. */
 
 	unsigned long jiffies_force_qs;		/* Time at which to invoke */
 						/*  force_quiescent_state(). */

commit c190c3b16c0f56ff338df12df53c03859155951b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 23 19:03:45 2015 -0700

    rcu: Switch synchronize_sched_expedited() to stop_one_cpu()
    
    The synchronize_sched_expedited() currently invokes try_stop_cpus(),
    which schedules the stopper kthreads on each online non-idle CPU,
    and waits until all those kthreads are running before letting any
    of them stop.  This is disastrous for real-time workloads, which
    get hit with a preemption that is as long as the longest scheduling
    latency on any CPU, including any non-realtime housekeeping CPUs.
    This commit therefore switches to using stop_one_cpu() on each CPU
    in turn.  This avoids inflicting the worst-case scheduling latency
    on the worst-case CPU onto all other CPUs, and also simplifies the
    code a little bit.
    
    Follow-up commits will simplify the counter-snapshotting algorithm
    and convert a number of the counters that are now protected by the
    new ->expedited_mutex to non-atomic.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [ paulmck: Kept stop_one_cpu(), dropped disabling of "guardrails". ]
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7c0b09d754a1..7c25fe473ad9 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -480,6 +480,7 @@ struct rcu_state {
 						/*  _rcu_barrier(). */
 	/* End of fields guarded by barrier_mutex. */
 
+	struct mutex  expedited_mutex;		/* Serializes expediting. */
 	atomic_long_t expedited_start;		/* Starting ticket. */
 	atomic_long_t expedited_done;		/* Done ticket. */
 	atomic_long_t expedited_wrap;		/* # near-wrap incidents. */

commit 75c27f119b6475d95374bdad872c6938b5c26196
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 11 15:22:43 2015 -0700

    rcu: Remove CONFIG_RCU_CPU_STALL_INFO
    
    The CONFIG_RCU_CPU_STALL_INFO has been default-y for a couple of
    releases with no complaints, so it is time to eliminate this Kconfig
    option entirely, so that the long-form RCU CPU stall warnings cannot
    be disabled.  This commit does just that.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index faee5242d6ff..7c0b09d754a1 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -288,12 +288,10 @@ struct rcu_data {
 	bool		gpwrap;		/* Possible gpnum/completed wrap. */
 	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
 	unsigned long grpmask;		/* Mask to apply to leaf qsmask. */
-#ifdef CONFIG_RCU_CPU_STALL_INFO
 	unsigned long	ticks_this_gp;	/* The number of scheduling-clock */
 					/*  ticks this CPU has handled */
 					/*  during and after the last grace */
 					/* period it is aware of. */
-#endif /* #ifdef CONFIG_RCU_CPU_STALL_INFO */
 
 	/* 2) batch handling */
 	/*
@@ -388,9 +386,7 @@ struct rcu_data {
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 
 	/* 8) RCU CPU stall data. */
-#ifdef CONFIG_RCU_CPU_STALL_INFO
 	unsigned int softirq_snap;	/* Snapshot of softirq activity. */
-#endif /* #ifdef CONFIG_RCU_CPU_STALL_INFO */
 
 	int cpu;
 	struct rcu_state *rsp;

commit 032dfc87225c96ec1771e5967436c4b23d1dc5d6
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Thu Jul 9 15:34:23 2015 +0200

    rcu: Shut up bogus gcc array bounds warning
    
    Because gcc does not realize a loop would not be entered ever
    (i.e. in case of rcu_num_lvls == 1):
    
      for (i = 1; i < rcu_num_lvls; i++)
              rsp->level[i] = rsp->level[i - 1] + levelcnt[i - 1];
    
    some compiler (pre- 5.x?) versions give a bogus warning:
    
      kernel/rcu/tree.c: In function rcu_init_one.isra.55:
      kernel/rcu/tree.c:4108:13: warning: array subscript is above array bounds [-Warray-bounds]
         rsp->level[i] = rsp->level[i - 1] + rsp->levelcnt[i - 1];
                   ^
    Fix that warning by adding an extra item to rcu_state::level[]
    array. Once the bogus warning is fixed in gcc and kernel drops
    support of older versions, the dummy item may be removed from
    the array.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Suggested-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 581f8d3c5b28..faee5242d6ff 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -443,7 +443,9 @@ do {									\
  */
 struct rcu_state {
 	struct rcu_node node[NUM_RCU_NODES];	/* Hierarchy. */
-	struct rcu_node *level[RCU_NUM_LVLS];	/* Hierarchy levels. */
+	struct rcu_node *level[RCU_NUM_LVLS + 1];
+						/* Hierarchy levels (+1 to */
+						/*  shut bogus gcc warning) */
 	u8 flavor_mask;				/* bit in flavor mask. */
 	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */
 	void (*call)(struct rcu_head *head,	/* call_rcu() flavor. */

commit 426216970e0458c1f507860f4837cbde66a72263
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Wed Jun 3 08:18:31 2015 +0200

    rcu: Simplify arithmetic to calculate number of RCU nodes
    
    This update makes arithmetic to calculate number of RCU nodes
    more straight and easy to read.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d44856b6170a..581f8d3c5b28 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -64,10 +64,7 @@
 #if NR_CPUS <= RCU_FANOUT_1
 #  define RCU_NUM_LVLS	      1
 #  define NUM_RCU_LVL_0	      1
-#  define NUM_RCU_LVL_1	      (NR_CPUS)
-#  define NUM_RCU_LVL_2	      0
-#  define NUM_RCU_LVL_3	      0
-#  define NUM_RCU_LVL_4	      0
+#  define NUM_RCU_NODES	      NUM_RCU_LVL_0
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0" }
@@ -75,9 +72,7 @@
 #  define RCU_NUM_LVLS	      2
 #  define NUM_RCU_LVL_0	      1
 #  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
-#  define NUM_RCU_LVL_2	      (NR_CPUS)
-#  define NUM_RCU_LVL_3	      0
-#  define NUM_RCU_LVL_4	      0
+#  define NUM_RCU_NODES	      (NUM_RCU_LVL_0 + NUM_RCU_LVL_1)
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1" }
@@ -86,8 +81,7 @@
 #  define NUM_RCU_LVL_0	      1
 #  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
 #  define NUM_RCU_LVL_2	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
-#  define NUM_RCU_LVL_3	      (NR_CPUS)
-#  define NUM_RCU_LVL_4	      0
+#  define NUM_RCU_NODES	      (NUM_RCU_LVL_0 + NUM_RCU_LVL_1 + NUM_RCU_LVL_2)
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1, NUM_RCU_LVL_2 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2" }
@@ -97,7 +91,7 @@
 #  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_3)
 #  define NUM_RCU_LVL_2	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
 #  define NUM_RCU_LVL_3	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
-#  define NUM_RCU_LVL_4	      (NR_CPUS)
+#  define NUM_RCU_NODES	      (NUM_RCU_LVL_0 + NUM_RCU_LVL_1 + NUM_RCU_LVL_2 + NUM_RCU_LVL_3)
 #  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1, NUM_RCU_LVL_2, NUM_RCU_LVL_3 }
 #  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2", "rcu_node_3" }
 #  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2", "rcu_node_fqs_3" }
@@ -105,9 +99,6 @@
 # error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
 #endif /* #if (NR_CPUS) <= RCU_FANOUT_1 */
 
-#define RCU_SUM (NUM_RCU_LVL_0 + NUM_RCU_LVL_1 + NUM_RCU_LVL_2 + NUM_RCU_LVL_3 + NUM_RCU_LVL_4)
-#define NUM_RCU_NODES (RCU_SUM - NR_CPUS)
-
 extern int rcu_num_lvls;
 extern int rcu_num_nodes;
 

commit cb007102398edd06ffc4488bf841c2e10f14d2e7
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Wed Jun 3 08:18:30 2015 +0200

    rcu: Limit count of static data to the number of RCU levels
    
    Although a number of RCU levels may be less than the current
    maximum of four, some static data associated with each level
    are allocated for all four levels. As result, the extra data
    never get accessed and just wast memory. This update limits
    count of allocated items to the number of used RCU levels.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 3413f3c5c8b2..d44856b6170a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -68,6 +68,9 @@
 #  define NUM_RCU_LVL_2	      0
 #  define NUM_RCU_LVL_3	      0
 #  define NUM_RCU_LVL_4	      0
+#  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0 }
+#  define RCU_NODE_NAME_INIT  { "rcu_node_0" }
+#  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0" }
 #elif NR_CPUS <= RCU_FANOUT_2
 #  define RCU_NUM_LVLS	      2
 #  define NUM_RCU_LVL_0	      1
@@ -75,6 +78,9 @@
 #  define NUM_RCU_LVL_2	      (NR_CPUS)
 #  define NUM_RCU_LVL_3	      0
 #  define NUM_RCU_LVL_4	      0
+#  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1 }
+#  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1" }
+#  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1" }
 #elif NR_CPUS <= RCU_FANOUT_3
 #  define RCU_NUM_LVLS	      3
 #  define NUM_RCU_LVL_0	      1
@@ -82,6 +88,9 @@
 #  define NUM_RCU_LVL_2	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
 #  define NUM_RCU_LVL_3	      (NR_CPUS)
 #  define NUM_RCU_LVL_4	      0
+#  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1, NUM_RCU_LVL_2 }
+#  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2" }
+#  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2" }
 #elif NR_CPUS <= RCU_FANOUT_4
 #  define RCU_NUM_LVLS	      4
 #  define NUM_RCU_LVL_0	      1
@@ -89,6 +98,9 @@
 #  define NUM_RCU_LVL_2	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
 #  define NUM_RCU_LVL_3	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
 #  define NUM_RCU_LVL_4	      (NR_CPUS)
+#  define NUM_RCU_LVL_INIT    { NUM_RCU_LVL_0, NUM_RCU_LVL_1, NUM_RCU_LVL_2, NUM_RCU_LVL_3 }
+#  define RCU_NODE_NAME_INIT  { "rcu_node_0", "rcu_node_1", "rcu_node_2", "rcu_node_3" }
+#  define RCU_FQS_NAME_INIT   { "rcu_node_fqs_0", "rcu_node_fqs_1", "rcu_node_fqs_2", "rcu_node_fqs_3" }
 #else
 # error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
 #endif /* #if (NR_CPUS) <= RCU_FANOUT_1 */

commit 199977bff9efceec649d74510fa9754e107ce0c5
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Wed Jun 3 08:18:29 2015 +0200

    rcu: Remove unnecessary fields from rcu_state structure
    
    Members rcu_state::levelcnt[] and rcu_state::levelspread[]
    are only used at init. There is no reason to keep them
    afterwards.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d625e9ff0faf..3413f3c5c8b2 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -441,8 +441,6 @@ do {									\
 struct rcu_state {
 	struct rcu_node node[NUM_RCU_NODES];	/* Hierarchy. */
 	struct rcu_node *level[RCU_NUM_LVLS];	/* Hierarchy levels. */
-	u32 levelcnt[RCU_NUM_LVLS];		/* # nodes in each level. */
-	u8 levelspread[RCU_NUM_LVLS];		/* kids/node in each level. */
 	u8 flavor_mask;				/* bit in flavor mask. */
 	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */
 	void (*call)(struct rcu_head *head,	/* call_rcu() flavor. */

commit 05b84aec465c34da242a224d2438d192ca0feec7
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Wed Jun 3 08:18:28 2015 +0200

    rcu: Limit rcu_capacity[] size to RCU_NUM_LVLS items
    
    Number of items in rcu_capacity[] array is defined by macro
    MAX_RCU_LVLS. However, that array is never accessed beyond
    RCU_NUM_LVLS index. Therefore, we can limit the array to
    RCU_NUM_LVLS items and eliminate MAX_RCU_LVLS. As result,
    in most cases the memory is conserved.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a6faae53ea8f..d625e9ff0faf 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -36,8 +36,6 @@
  * Of course, your mileage may vary.
  */
 
-#define MAX_RCU_LVLS 4
-
 #ifdef CONFIG_RCU_FANOUT
 #define RCU_FANOUT CONFIG_RCU_FANOUT
 #else /* #ifdef CONFIG_RCU_FANOUT */

commit a6d77081e266605c9f4d8c11e0ee00468b9dc614
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Wed Jun 3 08:18:27 2015 +0200

    rcu: Limit rcu_state::levelcnt[] to RCU_NUM_LVLS items
    
    Variable rcu_num_lvls is limited by RCU_NUM_LVLS macro.
    In turn, rcu_state::levelcnt[] array is never accessed
    beyond rcu_num_lvls. Thus, rcu_state::levelcnt[] is safe
    to limit to RCU_NUM_LVLS items.
    
    Since rcu_num_lvls could be changed during boot (as result
    of rcutree.rcu_fanout_leaf kernel parameter update) one might
    assume a new value could overflow the value of RCU_NUM_LVLS.
    However, that is not the case, since leaf-level fanout is only
    permitted to increase, resulting in rcu_num_lvls possibly to
    decrease.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index f1f4784f9107..a6faae53ea8f 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -443,7 +443,7 @@ do {									\
 struct rcu_state {
 	struct rcu_node node[NUM_RCU_NODES];	/* Hierarchy. */
 	struct rcu_node *level[RCU_NUM_LVLS];	/* Hierarchy levels. */
-	u32 levelcnt[MAX_RCU_LVLS + 1];		/* # nodes in each level. */
+	u32 levelcnt[RCU_NUM_LVLS];		/* # nodes in each level. */
 	u8 levelspread[RCU_NUM_LVLS];		/* kids/node in each level. */
 	u8 flavor_mask;				/* bit in flavor mask. */
 	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */

commit 319362c90f7af92d03714759bea5d2ba7c88e388
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 19 14:16:52 2015 -0700

    rcu: Provide more diagnostics for stalled GP kthread
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 4adb7ca0bf47..f1f4784f9107 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -527,7 +527,11 @@ struct rcu_state {
 /* Values for rcu_state structure's gp_flags field. */
 #define RCU_GP_WAIT_INIT 0	/* Initial state. */
 #define RCU_GP_WAIT_GPS  1	/* Wait for grace-period start. */
-#define RCU_GP_WAIT_FQS  2	/* Wait for force-quiescent-state time. */
+#define RCU_GP_DONE_GPS  2	/* Wait done for grace-period start. */
+#define RCU_GP_WAIT_FQS  3	/* Wait for force-quiescent-state time. */
+#define RCU_GP_DONE_FQS  4	/* Wait done for force-quiescent-state time. */
+#define RCU_GP_CLEANUP   5	/* Grace-period cleanup started. */
+#define RCU_GP_CLEANED   6	/* Grace-period cleanup complete. */
 
 extern struct list_head rcu_struct_flavors;
 

commit 0868aa22167d93dd974c253d259c3e6fd47a16c8
Merge: 29c6820f5164 ed38446424dd f517700cce37 927da9dfd13a 1ce46ee597bc 6e91f8cb1386 7d3bb54adeb1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 27 13:00:49 2015 -0700

    Merge branches 'array.2015.05.27a', 'doc.2015.05.27a', 'fixes.2015.05.27a', 'hotplug.2015.05.27a', 'init.2015.05.27a', 'tiny.2015.05.27a' and 'torture.2015.05.27a' into HEAD
    
    array.2015.05.27a:  Remove all uses of RCU-protected array indexes.
    doc.2015.05.27a:  Docuemntation updates.
    fixes.2015.05.27a:  Miscellaneous fixes.
    hotplug.2015.05.27a:  CPU-hotplug updates.
    init.2015.05.27a:  Initialization/Kconfig updates.
    tiny.2015.05.27a:  Updates to Tiny RCU.
    torture.2015.05.27a:  Torture-testing updates.

commit 47d631af58bb9b2f2dd3d0da8c98a79a5e75c738
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Apr 21 09:12:13 2015 -0700

    rcu: Make RCU able to tolerate undefined CONFIG_RCU_FANOUT_LEAF
    
    This commit introduces an RCU_FANOUT_LEAF C-preprocessor macro so
    that RCU will build even when CONFIG_RCU_FANOUT_LEAF is undefined.
    The RCU_FANOUT_LEAF macro is set to the value of CONFIG_RCU_FANOUT_LEAF
    when defined, otherwise it is set to 32 for 32-bit systems and 64 for
    64-bit systems.  This commit then makes CONFIG_RCU_FANOUT_LEAF depend
    on CONFIG_RCU_EXPERT, so that Kconfig users won't be asked about
    CONFIG_RCU_FANOUT_LEAF unless they want to be.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index ac3020fff028..7d949c186302 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -48,7 +48,17 @@
 # endif
 #endif /* #else #ifdef CONFIG_RCU_FANOUT */
 
-#define RCU_FANOUT_1	      (CONFIG_RCU_FANOUT_LEAF)
+#ifdef CONFIG_RCU_FANOUT_LEAF
+#define RCU_FANOUT_LEAF CONFIG_RCU_FANOUT_LEAF
+#else /* #ifdef CONFIG_RCU_FANOUT_LEAF */
+# ifdef CONFIG_64BIT
+# define RCU_FANOUT_LEAF 64
+# else
+# define RCU_FANOUT_LEAF 32
+# endif
+#endif /* #else #ifdef CONFIG_RCU_FANOUT_LEAF */
+
+#define RCU_FANOUT_1	      (RCU_FANOUT_LEAF)
 #define RCU_FANOUT_2	      (RCU_FANOUT_1 * RCU_FANOUT)
 #define RCU_FANOUT_3	      (RCU_FANOUT_2 * RCU_FANOUT)
 #define RCU_FANOUT_4	      (RCU_FANOUT_3 * RCU_FANOUT)

commit 05c5df31afd1092ca6322094d22aff6351fa67fe
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Apr 20 14:27:43 2015 -0700

    rcu: Make RCU able to tolerate undefined CONFIG_RCU_FANOUT
    
    This commit introduces an RCU_FANOUT C-preprocessor macro so that RCU will
    build even when CONFIG_RCU_FANOUT is undefined.  The RCU_FANOUT macro is
    set to the value of CONFIG_RCU_FANOUT when defined, otherwise it is set
    to 32 for 32-bit systems and 64 for 64-bit systems.  This commit then
    makes CONFIG_RCU_FANOUT depend on CONFIG_RCU_EXPERT, so that Kconfig
    users won't be asked about CONFIG_RCU_FANOUT unless they want to be.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a69d3dab2ec4..ac3020fff028 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -35,11 +35,23 @@
  * In practice, this did work well going from three levels to four.
  * Of course, your mileage may vary.
  */
+
 #define MAX_RCU_LVLS 4
+
+#ifdef CONFIG_RCU_FANOUT
+#define RCU_FANOUT CONFIG_RCU_FANOUT
+#else /* #ifdef CONFIG_RCU_FANOUT */
+# ifdef CONFIG_64BIT
+# define RCU_FANOUT 64
+# else
+# define RCU_FANOUT 32
+# endif
+#endif /* #else #ifdef CONFIG_RCU_FANOUT */
+
 #define RCU_FANOUT_1	      (CONFIG_RCU_FANOUT_LEAF)
-#define RCU_FANOUT_2	      (RCU_FANOUT_1 * CONFIG_RCU_FANOUT)
-#define RCU_FANOUT_3	      (RCU_FANOUT_2 * CONFIG_RCU_FANOUT)
-#define RCU_FANOUT_4	      (RCU_FANOUT_3 * CONFIG_RCU_FANOUT)
+#define RCU_FANOUT_2	      (RCU_FANOUT_1 * RCU_FANOUT)
+#define RCU_FANOUT_3	      (RCU_FANOUT_2 * RCU_FANOUT)
+#define RCU_FANOUT_4	      (RCU_FANOUT_3 * RCU_FANOUT)
 
 #if NR_CPUS <= RCU_FANOUT_1
 #  define RCU_NUM_LVLS	      1

commit c92fb05795f57463cb763a82f9053d294f77ea87
Author: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
Date:   Tue May 5 21:57:06 2015 +0800

    rcu: Make rcu_*_data variables static
    
    rcu_bh_data, rcu_sched_data and rcu_preempt_data are never used outside
    kernel/rcu/tree.c and thus can be made static.
    
    Doing so fixes a section mismatch warning reported by clang when
    building LLVMLinux with -Wsection, because these variables were declared
    in .data..percpu and defined in .data..percpu..shared_aligned since
    commit 11bbb235c26f ("rcu: Use DEFINE_PER_CPU_SHARED_ALIGNED for
    rcu_data").
    
    Signed-off-by: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index dd5ce4034875..8079c5b22a8f 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -517,14 +517,11 @@ extern struct list_head rcu_struct_flavors;
  * RCU implementation internal declarations:
  */
 extern struct rcu_state rcu_sched_state;
-DECLARE_PER_CPU(struct rcu_data, rcu_sched_data);
 
 extern struct rcu_state rcu_bh_state;
-DECLARE_PER_CPU(struct rcu_data, rcu_bh_data);
 
 #ifdef CONFIG_PREEMPT_RCU
 extern struct rcu_state rcu_preempt_state;
-DECLARE_PER_CPU(struct rcu_data, rcu_preempt_data);
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
 
 #ifdef CONFIG_RCU_BOOST

commit 727b705baf7d091a9bc5494d7f1d9699b6932531
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Mar 3 14:49:26 2015 -0800

    rcu: Eliminate a few RCU_BOOST #ifdefs in favor of IS_ENABLED()
    
    This commit removes a few RCU_BOOST #ifdefs, replacing them with
    IS_ENABLED()-protected return statements.  This relies on the
    optimizer to remove any resulting dead code.  There are several other
    RCU_BOOST #ifdefs, however these rely on some per-CPU variables that
    are available only under RCU_BOOST.  These might be converted later,
    if the simplification proves to outweigh the increase in memory footprint.
    One hoped-for advantage is more easily locating compiler errors in
    obscure combinations of Kconfig parameters.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: <linux-rt-users@vger.kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a69d3dab2ec4..dd5ce4034875 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -170,7 +170,6 @@ struct rcu_node {
 				/*  if there is no such task.  If there */
 				/*  is no current expedited grace period, */
 				/*  then there can cannot be any such task. */
-#ifdef CONFIG_RCU_BOOST
 	struct list_head *boost_tasks;
 				/* Pointer to first task that needs to be */
 				/*  priority boosted, or NULL if no priority */
@@ -208,7 +207,6 @@ struct rcu_node {
 	unsigned long n_balk_nos;
 				/* Refused to boost: not sure why, though. */
 				/*  This can happen due to race conditions. */
-#endif /* #ifdef CONFIG_RCU_BOOST */
 #ifdef CONFIG_RCU_NOCB_CPU
 	wait_queue_head_t nocb_gp_wq[2];
 				/* Place for rcu_nocb_kthread() to wait GP. */

commit c199068913c9c5cbb5498e289bb387703e087ea8
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jan 23 22:29:37 2015 -0800

    rcu: Eliminate ->onoff_mutex from rcu_node structure
    
    Because that RCU grace-period initialization need no longer exclude
    CPU-hotplug operations, this commit eliminates the ->onoff_mutex and
    its uses.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index aa42562ff5b2..a69d3dab2ec4 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -456,8 +456,6 @@ struct rcu_state {
 	long qlen;				/* Total number of callbacks. */
 	/* End of fields guarded by orphan_lock. */
 
-	struct mutex onoff_mutex;		/* Coordinate hotplug & GPs. */
-
 	struct mutex barrier_mutex;		/* Guards barrier fields. */
 	atomic_t barrier_cpu_count;		/* # CPUs waiting on. */
 	struct completion barrier_completion;	/* Wake at barrier end. */

commit 0aa04b055e71bd3b8040dd71a126126c66b6f01e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jan 23 21:52:37 2015 -0800

    rcu: Process offlining and onlining only at grace-period start
    
    Races between CPU hotplug and grace periods can be difficult to resolve,
    so the ->onoff_mutex is used to exclude the two events.  Unfortunately,
    this means that it is impossible for an outgoing CPU to perform the
    last bits of its offlining from its last pass through the idle loop,
    because sleeplocks cannot be acquired in that context.
    
    This commit avoids these problems by buffering online and offline events
    in a new ->qsmaskinitnext field in the leaf rcu_node structures.  When a
    grace period starts, the events accumulated in this mask are applied to
    the ->qsmaskinit field, and, if needed, up the rcu_node tree.  The special
    case of all CPUs corresponding to a given leaf rcu_node structure being
    offline while there are still elements in that structure's ->blkd_tasks
    list is handled using a new ->wait_blkd_tasks field.  In this case,
    propagating the offline bits up the tree is deferred until the beginning
    of the grace period after all of the tasks have exited their RCU read-side
    critical sections and removed themselves from the list, at which point
    the ->wait_blkd_tasks flag is cleared.  If one of that leaf rcu_node
    structure's CPUs comes back online before the list empties, then the
    ->wait_blkd_tasks flag is simply cleared.
    
    This of course means that RCU's notion of which CPUs are offline can be
    out of date.  This is OK because RCU need only wait on CPUs that were
    online at the time that the grace period started.  In addition, RCU's
    force-quiescent-state actions will handle the case where a CPU goes
    offline after the grace period starts.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 119de399eb2f..aa42562ff5b2 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -141,12 +141,20 @@ struct rcu_node {
 				/*  complete (only for PREEMPT_RCU). */
 	unsigned long qsmaskinit;
 				/* Per-GP initial value for qsmask & expmask. */
+				/*  Initialized from ->qsmaskinitnext at the */
+				/*  beginning of each grace period. */
+	unsigned long qsmaskinitnext;
+				/* Online CPUs for next grace period. */
 	unsigned long grpmask;	/* Mask to apply to parent qsmask. */
 				/*  Only one bit will be set in this mask. */
 	int	grplo;		/* lowest-numbered CPU or group here. */
 	int	grphi;		/* highest-numbered CPU or group here. */
 	u8	grpnum;		/* CPU/group number for next level up. */
 	u8	level;		/* root is at level 0. */
+	bool	wait_blkd_tasks;/* Necessary to wait for blocked tasks to */
+				/*  exit RCU read-side critical sections */
+				/*  before propagating offline up the */
+				/*  rcu_node tree? */
 	struct rcu_node *parent;
 	struct list_head blkd_tasks;
 				/* Tasks blocked in RCU read-side critical */
@@ -559,6 +567,7 @@ static void rcu_prepare_kthreads(int cpu);
 static void rcu_cleanup_after_idle(void);
 static void rcu_prepare_for_idle(void);
 static void rcu_idle_count_callbacks_posted(void);
+static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 static void print_cpu_stall_info_begin(void);
 static void print_cpu_stall_info(struct rcu_state *rsp, int cpu);
 static void print_cpu_stall_info_end(void);

commit 78e691f4ae2d5edea0199ca802bb505b9cdced88
Merge: d87510c5a6e3 60479676eb6e ab954c167ed9 83fe27ea5311 630181c4a915 7602de4af192
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jan 15 23:34:34 2015 -0800

    Merge branches 'doc.2015.01.07a', 'fixes.2015.01.15a', 'preempt.2015.01.06a', 'srcu.2015.01.06a', 'stall.2015.01.16a' and 'torture.2015.01.11a' into HEAD
    
    doc.2015.01.07a: Documentation updates.
    fixes.2015.01.15a: Miscellaneous fixes.
    preempt.2015.01.06a: Changes to handling of lists of preempted tasks.
    srcu.2015.01.06a: SRCU updates.
    stall.2015.01.16a: RCU CPU stall-warning updates and fixes.
    torture.2015.01.11a: RCU torture-test updates and fixes.

commit 5cd37193ce8539be1e6ef76be226f4bcc984e0f5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Dec 13 20:32:04 2014 -0800

    rcu: Make cond_resched_rcu_qs() apply to normal RCU flavors
    
    Although cond_resched_rcu_qs() only applies to TASKS_RCU, it is used
    in places where it would be useful for it to apply to the normal RCU
    flavors, rcu_preempt, rcu_sched, and rcu_bh.  This is especially the
    case for workloads that aggressively overload the system, particularly
    those that generate large numbers of RCU updates on systems running
    NO_HZ_FULL CPUs.  This commit therefore communicates quiescent states
    from cond_resched_rcu_qs() to the normal RCU flavors.
    
    Note that it is unfortunately necessary to leave the old ->passed_quiesce
    mechanism in place to allow quiescent states that apply to only one
    flavor to be recorded.  (Yes, we could decrement ->rcu_qs_ctr_snap in
    that case, but that is not so good for debugging of RCU internals.)
    In addition, if one of the RCU flavor's grace period has stalled, this
    will invoke rcu_momentary_dyntick_idle(), resulting in a heavy-weight
    quiescent state visible from other CPUs.
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Merge commit from Sasha Levin fixing a bug where __this_cpu()
      was used in preemptible code. ]

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7472ff388d55..1e7f8b05714e 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -257,6 +257,8 @@ struct rcu_data {
 					/*  in order to detect GP end. */
 	unsigned long	gpnum;		/* Highest gp number that this CPU */
 					/*  is aware of having started. */
+	unsigned long	rcu_qs_ctr_snap;/* Snapshot of rcu_qs_ctr to check */
+					/*  for rcu_all_qs() invocations. */
 	bool		passed_quiesce;	/* User-mode/idle loop etc. */
 	bool		qs_pending;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */

commit f9103c390257d06c162d9e3c2a90d2bdedadfe17
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Nov 21 16:48:20 2014 -0800

    rcu: Remove redundant rcu_batches_completed() declaration
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 1a07d7379ac6..69eb422445f9 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -546,7 +546,6 @@ DECLARE_PER_CPU(char, rcu_cpu_has_work);
 
 /* Forward declarations for rcutree_plugin.h */
 static void rcu_bootup_announce(void);
-unsigned long rcu_batches_completed(void);
 static void rcu_preempt_note_context_switch(void);
 static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
 #ifdef CONFIG_HOTPLUG_CPU

commit 9733e4f0a973a354034f5dd603b4142a3095c85f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Nov 21 12:49:13 2014 -0800

    rcu: Make _batches_completed() functions return unsigned long
    
    Long ago, the various ->completed fields were of type long, but now are
    unsigned long due to signed-integer-overflow concerns.  However, the
    various _batches_completed() functions remained of type long, even though
    their only purpose in life is to return the corresponding ->completed
    field.  This patch cleans this up by changing these functions' return
    types to unsigned long.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8e7b1843896e..1a07d7379ac6 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -546,7 +546,7 @@ DECLARE_PER_CPU(char, rcu_cpu_has_work);
 
 /* Forward declarations for rcutree_plugin.h */
 static void rcu_bootup_announce(void);
-long rcu_batches_completed(void);
+unsigned long rcu_batches_completed(void);
 static void rcu_preempt_note_context_switch(void);
 static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
 #ifdef CONFIG_HOTPLUG_CPU

commit e3663b1024d1f94688e5233440ad67a9bc10b94e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Dec 8 20:26:55 2014 -0800

    rcu: Handle gpnum/completed wrap while dyntick idle
    
    Subtle race conditions can result if a CPU stays in dyntick-idle mode
    long enough for the ->gpnum and ->completed fields to wrap.  For
    example, consider the following sequence of events:
    
    o       CPU 1 encounters a quiescent state while waiting for grace period
            5 to complete, but then enters dyntick-idle mode.
    
    o       While CPU 1 is in dyntick-idle mode, the grace-period counters
            wrap around so that the grace period number is now 4.
    
    o       Just as CPU 1 exits dyntick-idle mode, grace period 4 completes
            and grace period 5 begins.
    
    o       The quiescent state that CPU 1 passed through during the old
            grace period 5 looks like it applies to the new grace period
            5.  Therefore, the new grace period 5 completes without CPU 1
            having passed through a quiescent state.
    
    This could clearly be a fatal surprise to any long-running RCU read-side
    critical section that happened to be running on CPU 1 at the time.  At one
    time, this was not a problem, given that it takes significant time for
    the grace-period counters to overflow even on 32-bit systems.  However,
    with the advent of NO_HZ_FULL and SMP embedded systems, arbitrarily long
    idle periods are now becoming quite feasible.  It is therefore time to
    close this race.
    
    This commit therefore avoids this race condition by having the
    quiescent-state forcing code detect when a CPU is falling too far
    behind, and setting a new rcu_data field ->gpwrap when this happens.
    Whenever this new ->gpwrap field is set, the CPU's ->gpnum and ->completed
    fields are known to be untrustworthy, and can be ignored, along with
    any associated quiescent states.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 5ec81cf938fd..7472ff388d55 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -260,6 +260,7 @@ struct rcu_data {
 	bool		passed_quiesce;	/* User-mode/idle loop etc. */
 	bool		qs_pending;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */
+	bool		gpwrap;		/* Possible gpnum/completed wrap. */
 	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
 	unsigned long grpmask;		/* Mask to apply to leaf qsmask. */
 #ifdef CONFIG_RCU_CPU_STALL_INFO

commit 6ccd2ecd422644277b7d8b37222e3af3f43ea9ae
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Dec 11 10:20:59 2014 -0800

    rcu: Improve diagnostics for spurious RCU CPU stall warnings
    
    The current RCU CPU stall warning code will print "Stall ended before
    state dump start" any time that the stall-warning code is triggered on
    a CPU that has already reported a quiescent state for the current grace
    period and if all quiescent states have been reported for the current
    grace period.  However, a true stall can result in these symptoms, for
    example, by preventing RCU's grace-period kthreads from ever running
    
    This commit therefore checks for this condition, reporting the end of
    the stall only if one of the grace-period counters has actually advanced.
    Otherwise, it reports the last time that the grace-period kthread made
    meaningful progress.  (In normal situations, the grace-period kthread
    should make meaningful progress at least every jiffies_till_next_fqs
    jiffies.)
    
    Reported-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Miroslav Benes <mbenes@suse.cz>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index e300848cc0cf..5ec81cf938fd 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -488,6 +488,8 @@ struct rcu_state {
 						/*  due to no GP active. */
 	unsigned long gp_start;			/* Time at which GP started, */
 						/*  but in jiffies. */
+	unsigned long gp_activity;		/* Time of last GP kthread */
+						/*  activity in jiffies. */
 	unsigned long jiffies_stall;		/* Time at which to check */
 						/*  for CPU stalls. */
 	unsigned long jiffies_resched;		/* Time at which to resched */

commit fc908ed33e7c1428f799abb12399f906da03b397
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Dec 8 09:57:48 2014 -0800

    rcu: Make RCU_CPU_STALL_INFO include number of fqs attempts
    
    One way that an RCU CPU stall warning can happen is if the grace-period
    kthread is not allowed to execute.  One proxy for this kthread's
    forward progress is the number of force-quiescent-state (fqs) scans.
    This commit therefore adds the number of fqs scans to the RCU CPU stall
    warning printouts when CONFIG_RCU_CPU_STALL_INFO=y.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8e7b1843896e..e300848cc0cf 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -492,6 +492,8 @@ struct rcu_state {
 						/*  for CPU stalls. */
 	unsigned long jiffies_resched;		/* Time at which to resched */
 						/*  a reluctant CPU. */
+	unsigned long n_force_qs_gpstart;	/* Snapshot of n_force_qs at */
+						/*  GP start. */
 	unsigned long gp_max;			/* Maximum GP duration in */
 						/*  jiffies. */
 	const char *name;			/* Name of structure. */

commit abaf3f9d275b8d856ae5e47531e40c0bfeac012b
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Nov 18 16:30:01 2014 +0800

    rcu: Revert "Allow post-unlock reference for rt_mutex" to avoid priority-inversion
    
    The patch dfeb9765ce3c ("Allow post-unlock reference for rt_mutex")
    ensured rcu-boost safe even the rt_mutex has post-unlock reference.
    
    But rt_mutex allowing post-unlock reference is definitely a bug and it was
    fixed by the commit 27e35715df54 ("rtmutex: Plug slow unlock race").
    This fix made the previous patch (dfeb9765ce3c) useless.
    
    And even worse, the priority-inversion introduced by the the previous
    patch still exists.
    
    rcu_read_unlock_special() {
            rt_mutex_unlock(&rnp->boost_mtx);
            /* Priority-Inversion:
             * the current task had been deboosted and preempted as a low
             * priority task immediately, it could wait long before reschedule in,
             * and the rcu-booster also waits on this low priority task and sleeps.
             * This priority-inversion makes rcu-booster can't work
             * as expected.
             */
            complete(&rnp->boost_completion);
    }
    
    Just revert the patch to avoid it.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 883ebc8e2b6e..95356477d560 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -172,11 +172,6 @@ struct rcu_node {
 				/*  queued on this rcu_node structure that */
 				/*  are blocking the current grace period, */
 				/*  there can be no such task. */
-	struct completion boost_completion;
-				/* Used to ensure that the rt_mutex used */
-				/*  to carry out the boosting is fully */
-				/*  released with no future boostee accesses */
-				/*  before that rt_mutex is re-initialized. */
 	struct rt_mutex boost_mtx;
 				/* Used only for the priority-boosting */
 				/*  side effect, not as a lock. */

commit d19fb8d1f3f66cc342d30aa48f090c70afb753ed
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Oct 31 12:56:16 2014 -0700

    rcu: Don't migrate blocked tasks even if all corresponding CPUs offline
    
    When the last CPU associated with a given leaf rcu_node structure
    goes offline, something must be done about the tasks queued on that
    rcu_node structure.  Each of these tasks has been preempted on one of
    the leaf rcu_node structure's CPUs while in an RCU read-side critical
    section that it have not yet exited.  Handling these tasks is the job of
    rcu_preempt_offline_tasks(), which migrates them from the leaf rcu_node
    structure to the root rcu_node structure.
    
    Unfortunately, this migration has to be done one task at a time because
    each tasks allegiance must be shifted from the original leaf rcu_node to
    the root, so that future attempts to deal with these tasks will acquire
    the root rcu_node structure's ->lock rather than that of the leaf.
    Worse yet, this migration must be done with interrupts disabled, which
    is not so good for realtime response, especially given that there is
    no bound on the number of tasks on a given rcu_node structure's list.
    (OK, OK, there is a bound, it is just that it is unreasonably large,
    especially on 64-bit systems.)  This was not considered a problem back
    when rcu_preempt_offline_tasks() was first written because realtime
    systems were assumed not to do CPU-hotplug operations while real-time
    applications were running.  This assumption has proved of dubious validity
    given that people are starting to run multiple realtime applications
    on a single SMP system and that it is common practice to offline then
    online a CPU before starting its real-time application in order to clear
    extraneous processing off of that CPU.  So we now need CPU hotplug
    operations to avoid undue latencies.
    
    This commit therefore avoids migrating these tasks, instead letting
    them be dequeued one by one from the original leaf rcu_node structure
    by rcu_read_unlock_special().  This means that the clearing of bits
    from the upper-level rcu_node structures must be deferred until the
    last such task has been dequeued, because otherwise subsequent grace
    periods won't wait on them.  This commit has the beneficial side effect
    of simplifying the CPU-hotplug code for TREE_PREEMPT_RCU, especially in
    CONFIG_RCU_BOOST builds.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 9315477b47d9..883ebc8e2b6e 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -514,13 +514,6 @@ extern struct list_head rcu_struct_flavors;
 #define for_each_rcu_flavor(rsp) \
 	list_for_each_entry((rsp), &rcu_struct_flavors, flavors)
 
-/* Return values for rcu_preempt_offline_tasks(). */
-
-#define RCU_OFL_TASKS_NORM_GP	0x1		/* Tasks blocking normal */
-						/*  GP were moved to root. */
-#define RCU_OFL_TASKS_EXP_GP	0x2		/* Tasks blocking expedited */
-						/*  GP were moved to root. */
-
 /*
  * RCU implementation internal declarations:
  */
@@ -550,24 +543,13 @@ long rcu_batches_completed(void);
 static void rcu_preempt_note_context_switch(void);
 static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
 #ifdef CONFIG_HOTPLUG_CPU
-static void rcu_report_unblock_qs_rnp(struct rcu_node *rnp,
-				      unsigned long flags);
 static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 #endif /* #ifdef CONFIG_HOTPLUG_CPU */
 static void rcu_print_detail_task_stall(struct rcu_state *rsp);
 static int rcu_print_task_stall(struct rcu_node *rnp);
 static void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp);
-#ifdef CONFIG_HOTPLUG_CPU
-static int rcu_preempt_offline_tasks(struct rcu_state *rsp,
-				     struct rcu_node *rnp,
-				     struct rcu_data *rdp);
-#endif /* #ifdef CONFIG_HOTPLUG_CPU */
 static void rcu_preempt_check_callbacks(void);
 void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu));
-#if defined(CONFIG_HOTPLUG_CPU) || defined(CONFIG_PREEMPT_RCU)
-static void rcu_report_exp_rnp(struct rcu_state *rsp, struct rcu_node *rnp,
-			       bool wake);
-#endif /* #if defined(CONFIG_HOTPLUG_CPU) || defined(CONFIG_PREEMPT_RCU) */
 static void __init __rcu_init_preempt(void);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
 static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);

commit 8af3a5e78cfb63abe8813743946b7bd5a8a3134c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Oct 31 11:22:37 2014 -0700

    rcu: Abstract rcu_cleanup_dead_rnp() from rcu_cleanup_dead_cpu()
    
    This commit abstracts rcu_cleanup_dead_rnp() from rcu_cleanup_dead_cpu()
    in preparation for the rework of RCU priority boosting.  This new function
    will be invoked from rcu_read_unlock_special() in the reworked scheme,
    which is why rcu_cleanup_dead_rnp() assumes that the leaf rcu_node
    structure's ->qsmaskinit field has already been updated.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8e7b1843896e..9315477b47d9 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -552,6 +552,7 @@ static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
 #ifdef CONFIG_HOTPLUG_CPU
 static void rcu_report_unblock_qs_rnp(struct rcu_node *rnp,
 				      unsigned long flags);
+static bool rcu_preempt_has_tasks(struct rcu_node *rnp);
 #endif /* #ifdef CONFIG_HOTPLUG_CPU */
 static void rcu_print_detail_task_stall(struct rcu_state *rsp);
 static int rcu_print_task_stall(struct rcu_node *rnp);

commit 5a43b88e98eaca88fa12abcacd0000adc879dd2c
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Dec 24 17:55:28 2014 +0800

    rcu: Remove "select IRQ_WORK" from config TREE_RCU
    
    The 48a7639ce80c ("rcu: Make callers awaken grace-period kthread")
    removed the irq_work_queue(), so the TREE_RCU doesn't need
    irq work any more.  This commit therefore updates RCU's Kconfig and
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index cb5908672f11..41860b9a04a2 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -27,7 +27,6 @@
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 #include <linux/seqlock.h>
-#include <linux/irq_work.h>
 
 /*
  * Define shape of hierarchy based on NR_CPUS, CONFIG_RCU_FANOUT, and

commit 41050a009640f9f330a5b916563ca7faf853a98c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Dec 18 12:31:27 2014 -0800

    rcu: Fix rcu_barrier() race that could result in too-short wait
    
    The rcu_barrier() no-callbacks check for no-CBs CPUs has race conditions.
    It checks a given CPU's lists of callbacks, and if all three no-CBs lists
    are empty, ignores that CPU.  However, these three lists could potentially
    be empty even when callbacks are present if the check executed just as
    the callbacks were being moved from one list to another.  It turns out
    that recent versions of rcutorture can spot this race.
    
    This commit plugs this hole by consolidating the per-list counts of
    no-CBs callbacks into a single count, which is incremented before
    the corresponding callback is posted and after it is invoked.  Then
    rcu_barrier() checks this single count to reliably determine whether
    the corresponding CPU has no-CBs callbacks.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8e7b1843896e..cb5908672f11 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -340,14 +340,10 @@ struct rcu_data {
 #ifdef CONFIG_RCU_NOCB_CPU
 	struct rcu_head *nocb_head;	/* CBs waiting for kthread. */
 	struct rcu_head **nocb_tail;
-	atomic_long_t nocb_q_count;	/* # CBs waiting for kthread */
-	atomic_long_t nocb_q_count_lazy; /*  (approximate). */
+	atomic_long_t nocb_q_count;	/* # CBs waiting for nocb */
+	atomic_long_t nocb_q_count_lazy; /*  invocation (all stages). */
 	struct rcu_head *nocb_follower_head; /* CBs ready to invoke. */
 	struct rcu_head **nocb_follower_tail;
-	atomic_long_t nocb_follower_count; /* # CBs ready to invoke. */
-	atomic_long_t nocb_follower_count_lazy; /*  (approximate). */
-	int nocb_p_count;		/* # CBs being invoked by kthread */
-	int nocb_p_count_lazy;		/*  (approximate). */
 	wait_queue_head_t nocb_wq;	/* For nocb kthreads to sleep on. */
 	struct task_struct *nocb_kthread;
 	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
@@ -356,8 +352,6 @@ struct rcu_data {
 	struct rcu_head *nocb_gp_head ____cacheline_internodealigned_in_smp;
 					/* CBs waiting for GP. */
 	struct rcu_head **nocb_gp_tail;
-	long nocb_gp_count;
-	long nocb_gp_count_lazy;
 	bool nocb_leader_sleep;		/* Is the nocb leader thread asleep? */
 	struct rcu_data *nocb_next_follower;
 					/* Next follower in wakeup chain. */
@@ -622,24 +616,15 @@ static void rcu_dynticks_task_exit(void);
 #endif /* #ifndef RCU_TREE_NONCORE */
 
 #ifdef CONFIG_RCU_TRACE
-#ifdef CONFIG_RCU_NOCB_CPU
-/* Sum up queue lengths for tracing. */
+/* Read out queue lengths for tracing. */
 static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
 {
-	*ql = atomic_long_read(&rdp->nocb_q_count) +
-	      rdp->nocb_p_count +
-	      atomic_long_read(&rdp->nocb_follower_count) +
-	      rdp->nocb_p_count + rdp->nocb_gp_count;
-	*qll = atomic_long_read(&rdp->nocb_q_count_lazy) +
-	       rdp->nocb_p_count_lazy +
-	       atomic_long_read(&rdp->nocb_follower_count_lazy) +
-	       rdp->nocb_p_count_lazy + rdp->nocb_gp_count_lazy;
-}
+#ifdef CONFIG_RCU_NOCB_CPU
+	*ql = atomic_long_read(&rdp->nocb_q_count);
+	*qll = atomic_long_read(&rdp->nocb_q_count_lazy);
 #else /* #ifdef CONFIG_RCU_NOCB_CPU */
-static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
-{
 	*ql = 0;
 	*qll = 0;
-}
 #endif /* #else #ifdef CONFIG_RCU_NOCB_CPU */
+}
 #endif /* #ifdef CONFIG_RCU_TRACE */

commit 9ea6c5885681e3d9ce9844ba9dc57371a5cfc6d2
Merge: b8969d1a506c 62db99f4783e 8ab8b3e1837f 60ced4950c50 392809b25833 bbe5d7a93a39
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Nov 13 10:39:04 2014 -0800

    Merge branches 'torture.2014.11.03a', 'cpu.2014.11.03a', 'doc.2014.11.13a', 'fixes.2014.11.13a', 'signal.2014.10.29a' and 'rt.2014.10.29a' into HEAD
    
    cpu.2014.11.03a: Changes for per-CPU variables.
    doc.2014.11.13a: Documentation updates.
    fixes.2014.11.13a: Miscellaneous fixes.
    signal.2014.10.29a: Signal changes.
    rt.2014.10.29a: Real-time changes.
    torture.2014.11.03a: torture-test changes.

commit 8fa7845df539105a8962c3173c866483da74ff6d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Oct 22 15:07:37 2014 -0700

    rcu: Remove "cpu" argument to rcu_cleanup_after_idle()
    
    The "cpu" argument to rcu_cleanup_after_idle() is always the current
    CPU, so drop it.  This moves the smp_processor_id() from the caller to
    rcu_cleanup_after_idle(), saving argument-passing overhead.  Again,
    the anticipated cross-CPU uses of these functions has been replaced
    by NO_HZ_FULL.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8bc3cacbe14a..36c30390e4e9 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -579,7 +579,7 @@ static int rcu_spawn_one_boost_kthread(struct rcu_state *rsp,
 #endif /* #ifdef CONFIG_RCU_BOOST */
 static void __init rcu_spawn_boost_kthreads(void);
 static void rcu_prepare_kthreads(int cpu);
-static void rcu_cleanup_after_idle(int cpu);
+static void rcu_cleanup_after_idle(void);
 static void rcu_prepare_for_idle(void);
 static void rcu_idle_count_callbacks_posted(void);
 static void print_cpu_stall_info_begin(void);

commit 198bbf8127dd2dec6661622981a4e1f9b8f967b7
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Oct 22 15:03:43 2014 -0700

    rcu: Remove "cpu" argument to rcu_prepare_for_idle()
    
    The "cpu" argument to rcu_prepare_for_idle() is always the current
    CPU, so drop it.  This in turn allows two of the uses of "cpu" in
    this function to be replaced with a this_cpu_ptr() and the third by
    smp_processor_id(), replacing that of the call to rcu_prepare_for_idle().
    Again, the anticipated cross-CPU uses of these functions has been replaced
    by NO_HZ_FULL.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 238ac39053fc..8bc3cacbe14a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -580,7 +580,7 @@ static int rcu_spawn_one_boost_kthread(struct rcu_state *rsp,
 static void __init rcu_spawn_boost_kthreads(void);
 static void rcu_prepare_kthreads(int cpu);
 static void rcu_cleanup_after_idle(int cpu);
-static void rcu_prepare_for_idle(int cpu);
+static void rcu_prepare_for_idle(void);
 static void rcu_idle_count_callbacks_posted(void);
 static void print_cpu_stall_info_begin(void);
 static void print_cpu_stall_info(struct rcu_state *rsp, int cpu);

commit 38200cf24702e5d79ce6c8f4c62036c41845c62d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 21 12:50:04 2014 -0700

    rcu: Remove "cpu" argument to rcu_note_context_switch()
    
    The "cpu" argument to rcu_note_context_switch() is always the current
    CPU, so drop it.  This in turn allows the "cpu" argument to
    rcu_preempt_note_context_switch() to be removed, which allows the sole
    use of "cpu" in both functions to be replaced with a this_cpu_ptr().
    Again, the anticipated cross-CPU uses of these functions has been
    replaced by NO_HZ_FULL.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 94a26e330c1b..238ac39053fc 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -547,7 +547,7 @@ DECLARE_PER_CPU(char, rcu_cpu_has_work);
 /* Forward declarations for rcutree_plugin.h */
 static void rcu_bootup_announce(void);
 long rcu_batches_completed(void);
-static void rcu_preempt_note_context_switch(int cpu);
+static void rcu_preempt_note_context_switch(void);
 static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
 #ifdef CONFIG_HOTPLUG_CPU
 static void rcu_report_unblock_qs_rnp(struct rcu_node *rnp,

commit 86aea0e6e7d571a9452082a7198d0603cc5dd965
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 21 08:12:00 2014 -0700

    rcu: Remove "cpu" argument to rcu_preempt_check_callbacks()
    
    Because rcu_preempt_check_callbacks()'s argument is guaranteed to
    always be the current CPU, drop the argument and replace per_cpu()
    with __this_cpu_read().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8e90562010ec..94a26e330c1b 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -561,7 +561,7 @@ static int rcu_preempt_offline_tasks(struct rcu_state *rsp,
 				     struct rcu_node *rnp,
 				     struct rcu_data *rdp);
 #endif /* #ifdef CONFIG_HOTPLUG_CPU */
-static void rcu_preempt_check_callbacks(int cpu);
+static void rcu_preempt_check_callbacks(void);
 void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu));
 #if defined(CONFIG_HOTPLUG_CPU) || defined(CONFIG_TREE_PREEMPT_RCU)
 static void rcu_report_exp_rnp(struct rcu_state *rsp, struct rcu_node *rnp,

commit 28ced795cbb43f01146feb96d03a72facdee9911
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Sep 2 14:13:44 2014 -0700

    rcu: Remove rcu_dynticks * parameters when they are always this_cpu_ptr(&rcu_dynticks)
    
    For some functions in kernel/rcu/tree* the rdtp parameter is always
    this_cpu_ptr(rdtp).  Remove the parameter if constant and calculate the
    pointer in function.
    
    This will have the advantage that it is obvious that the address are
    all per cpu offsets and thus it will enable the use of this_cpu_ops in
    the future.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    [ paulmck: Forward-ported to rcu/dev, whitespace adjustment. ]
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index bbdc45d8d74f..8e90562010ec 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -606,8 +606,8 @@ static void __init rcu_organize_nocb_kthreads(struct rcu_state *rsp);
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 static void __maybe_unused rcu_kick_nohz_cpu(int cpu);
 static bool init_nocb_callback_list(struct rcu_data *rdp);
-static void rcu_sysidle_enter(struct rcu_dynticks *rdtp, int irq);
-static void rcu_sysidle_exit(struct rcu_dynticks *rdtp, int irq);
+static void rcu_sysidle_enter(int irq);
+static void rcu_sysidle_exit(int irq);
 static void rcu_sysidle_check_cpu(struct rcu_data *rdp, bool *isidle,
 				  unsigned long *maxj);
 static bool is_sysidle_rcu_state(struct rcu_state *rsp);

commit 28f6569ab7d036cd4ee94c26bb76dc1b3f3fc056
Author: Pranith Kumar <bobby.prani@gmail.com>
Date:   Mon Sep 22 14:00:48 2014 -0400

    rcu: Remove redundant TREE_PREEMPT_RCU config option
    
    PREEMPT_RCU and TREE_PREEMPT_RCU serve the same function after
    TINY_PREEMPT_RCU has been removed. This patch removes TREE_PREEMPT_RCU
    and uses PREEMPT_RCU config option in its place.
    
    Signed-off-by: Pranith Kumar <bobby.prani@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index bbdc45d8d74f..66cde5109c7d 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -139,7 +139,7 @@ struct rcu_node {
 	unsigned long expmask;	/* Groups that have ->blkd_tasks */
 				/*  elements that need to drain to allow the */
 				/*  current expedited grace period to */
-				/*  complete (only for TREE_PREEMPT_RCU). */
+				/*  complete (only for PREEMPT_RCU). */
 	unsigned long qsmaskinit;
 				/* Per-GP initial value for qsmask & expmask. */
 	unsigned long grpmask;	/* Mask to apply to parent qsmask. */
@@ -530,10 +530,10 @@ DECLARE_PER_CPU(struct rcu_data, rcu_sched_data);
 extern struct rcu_state rcu_bh_state;
 DECLARE_PER_CPU(struct rcu_data, rcu_bh_data);
 
-#ifdef CONFIG_TREE_PREEMPT_RCU
+#ifdef CONFIG_PREEMPT_RCU
 extern struct rcu_state rcu_preempt_state;
 DECLARE_PER_CPU(struct rcu_data, rcu_preempt_data);
-#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
+#endif /* #ifdef CONFIG_PREEMPT_RCU */
 
 #ifdef CONFIG_RCU_BOOST
 DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_status);
@@ -563,10 +563,10 @@ static int rcu_preempt_offline_tasks(struct rcu_state *rsp,
 #endif /* #ifdef CONFIG_HOTPLUG_CPU */
 static void rcu_preempt_check_callbacks(int cpu);
 void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu));
-#if defined(CONFIG_HOTPLUG_CPU) || defined(CONFIG_TREE_PREEMPT_RCU)
+#if defined(CONFIG_HOTPLUG_CPU) || defined(CONFIG_PREEMPT_RCU)
 static void rcu_report_exp_rnp(struct rcu_state *rsp, struct rcu_node *rnp,
 			       bool wake);
-#endif /* #if defined(CONFIG_HOTPLUG_CPU) || defined(CONFIG_TREE_PREEMPT_RCU) */
+#endif /* #if defined(CONFIG_HOTPLUG_CPU) || defined(CONFIG_PREEMPT_RCU) */
 static void __init __rcu_init_preempt(void);
 static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
 static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);

commit d7e29933969e5ca7c112ce1368a07911f4485dc2
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Oct 27 09:15:54 2014 -0700

    rcu: Make rcu_barrier() understand about missing rcuo kthreads
    
    Commit 35ce7f29a44a (rcu: Create rcuo kthreads only for onlined CPUs)
    avoids creating rcuo kthreads for CPUs that never come online.  This
    fixes a bug in many instances of firmware: Instead of lying about their
    age, these systems instead lie about the number of CPUs that they have.
    Before commit 35ce7f29a44a, this could result in huge numbers of useless
    rcuo kthreads being created.
    
    It appears that experience indicates that I should have told the
    people suffering from this problem to fix their broken firmware, but
    I instead produced what turned out to be a partial fix.   The missing
    piece supplied by this commit makes sure that rcu_barrier() knows not to
    post callbacks for no-CBs CPUs that have not yet come online, because
    otherwise rcu_barrier() will hang on systems having firmware that lies
    about the number of CPUs.
    
    It is tempting to simply have rcu_barrier() refuse to post a callback on
    any no-CBs CPU that does not have an rcuo kthread.  This unfortunately
    does not work because rcu_barrier() is required to wait for all pending
    callbacks.  It is therefore required to wait even for those callbacks
    that cannot possibly be invoked.  Even if doing so hangs the system.
    
    Given that posting a callback to a no-CBs CPU that does not yet have an
    rcuo kthread can hang rcu_barrier(), It is tempting to report an error
    in this case.  Unfortunately, this will result in false positives at
    boot time, when it is perfectly legal to post callbacks to the boot CPU
    before the scheduler has started, in other words, before it is legal
    to invoke rcu_barrier().
    
    So this commit instead has rcu_barrier() avoid posting callbacks to
    CPUs having neither rcuo kthread nor pending callbacks, and has it
    complain bitterly if it finds CPUs having no rcuo kthread but some
    pending callbacks.  And when rcu_barrier() does find CPUs having no rcuo
    kthread but pending callbacks, as noted earlier, it has no choice but
    to hang indefinitely.
    
    Reported-by: Yanko Kaneti <yaneti@declera.com>
    Reported-by: Jay Vosburgh <jay.vosburgh@canonical.com>
    Reported-by: Meelis Roos <mroos@linux.ee>
    Reported-by: Eric B Munson <emunson@akamai.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Eric B Munson <emunson@akamai.com>
    Tested-by: Jay Vosburgh <jay.vosburgh@canonical.com>
    Tested-by: Yanko Kaneti <yaneti@declera.com>
    Tested-by: Kevin Fenzi <kevin@scrye.com>
    Tested-by: Meelis Roos <mroos@linux.ee>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index d03764652d91..bbdc45d8d74f 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -587,6 +587,7 @@ static void print_cpu_stall_info(struct rcu_state *rsp, int cpu);
 static void print_cpu_stall_info_end(void);
 static void zero_cpu_stall_ticks(struct rcu_data *rdp);
 static void increment_cpu_stall_ticks(void);
+static bool rcu_nocb_cpu_needs_barrier(struct rcu_state *rsp, int cpu);
 static void rcu_nocb_gp_set(struct rcu_node *rnp, int nrq);
 static void rcu_nocb_gp_cleanup(struct rcu_state *rsp, struct rcu_node *rnp);
 static void rcu_init_one_nocb(struct rcu_node *rnp);

commit 96b4672703ed4538c7fc25de36df4415a0ee237c
Merge: e98d06dd6cd7 a53dd6a65668
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Sep 16 10:10:44 2014 -0700

    Merge branch 'rcu-tasks.2014.09.10a' into HEAD
    
    rcu-tasks.2014.09.10a: Add RCU-tasks flavor of RCU.

commit e98d06dd6cd791b5138b0fc6c14a9c0b4d1f2e72
Merge: 2456d2a617de 73a860cd58a1 c847f14217d5 bc51896da2ce
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Sep 16 10:08:34 2014 -0700

    Merge branches 'doc.2014.09.07a', 'fixes.2014.09.10a', 'nocb-nohz.2014.09.16b' and 'torture.2014.09.07a' into HEAD
    
    doc.2014.09.07a: Documentation updates.
    fixes.2014.09.10a: Miscellaneous fixes.
    nocb-nohz.2014.09.16b: No-CBs CPUs and NO_HZ_FULL updates.
    torture.2014.09.07a: Torture-test updates.

commit 35ce7f29a44a888c45c0a9f202f69e10613c5306
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jul 11 11:30:24 2014 -0700

    rcu: Create rcuo kthreads only for onlined CPUs
    
    RCU currently uses for_each_possible_cpu() to spawn rcuo kthreads,
    which can result in more rcuo kthreads than one would expect, for
    example, derRichard reported 64 CPUs worth of rcuo kthreads on an
    8-CPU image.  This commit therefore creates rcuo kthreads only for
    those CPUs that actually come online.
    
    This was reported by derRichard on the OFTC IRC network.
    
    Reported-by: Richard Weinberger <richard@nod.at>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Tested-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a966092fdfd7..a9a226d2e80a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -593,7 +593,11 @@ static bool rcu_nocb_adopt_orphan_cbs(struct rcu_state *rsp,
 static bool rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp);
 static void do_nocb_deferred_wakeup(struct rcu_data *rdp);
 static void rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp);
-static void rcu_spawn_nocb_kthreads(struct rcu_state *rsp);
+static void rcu_spawn_all_nocb_kthreads(int cpu);
+static void __init rcu_spawn_nocb_kthreads(void);
+#ifdef CONFIG_RCU_NOCB_CPU
+static void __init rcu_organize_nocb_kthreads(struct rcu_state *rsp);
+#endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 static void __maybe_unused rcu_kick_nohz_cpu(int cpu);
 static bool init_nocb_callback_list(struct rcu_data *rdp);
 static void rcu_sysidle_enter(struct rcu_dynticks *rdtp, int irq);

commit 9386c0b75dda05f535a10ea1abf1817fe292c81c
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Jul 13 12:00:53 2014 -0700

    rcu: Rationalize kthread spawning
    
    Currently, RCU spawns kthreads from several different early_initcall()
    functions.  Although this has served RCU well for quite some time,
    as more kthreads are added a more deterministic approach is required.
    This commit therefore causes all of RCU's early-boot kthreads to be
    spawned from a single early_initcall() function.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Tested-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6a86eb7bac45..a966092fdfd7 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -572,6 +572,7 @@ static void rcu_preempt_do_callbacks(void);
 static int rcu_spawn_one_boost_kthread(struct rcu_state *rsp,
 						 struct rcu_node *rnp);
 #endif /* #ifdef CONFIG_RCU_BOOST */
+static void __init rcu_spawn_boost_kthreads(void);
 static void rcu_prepare_kthreads(int cpu);
 static void rcu_cleanup_after_idle(int cpu);
 static void rcu_prepare_for_idle(int cpu);

commit 176f8f7a52cc6d09d686f0d900abda6942a52fbb
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Aug 4 17:43:50 2014 -0700

    rcu: Make TASKS_RCU handle nohz_full= CPUs
    
    Currently TASKS_RCU would ignore a CPU running a task in nohz_full=
    usermode execution.  There would be neither a context switch nor a
    scheduling-clock interrupt to tell TASKS_RCU that the task in question
    had passed through a quiescent state.  The grace period would therefore
    extend indefinitely.  This commit therefore makes RCU's dyntick-idle
    subsystem record the task_struct structure of the task that is running
    in dyntick-idle mode on each CPU.  The TASKS_RCU grace period can
    then access this information and record a quiescent state on
    behalf of any CPU running in dyntick-idle usermode.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6a86eb7bac45..3a92000c354f 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -605,6 +605,8 @@ static void rcu_sysidle_report_gp(struct rcu_state *rsp, int isidle,
 static void rcu_bind_gp_kthread(void);
 static void rcu_sysidle_init_percpu_data(struct rcu_dynticks *rdtp);
 static bool rcu_nohz_full_cpu(struct rcu_state *rsp);
+static void rcu_dynticks_task_enter(void);
+static void rcu_dynticks_task_exit(void);
 
 #endif /* #ifndef RCU_TREE_NONCORE */
 

commit 9fdd3bc9005824704f9802bec7b3e06f5edae434
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 29 14:50:47 2014 -0700

    rcu: Break more call_rcu() deadlock involving scheduler and perf
    
    Commit 96d3fd0d315a9 (rcu: Break call_rcu() deadlock involving scheduler
    and perf) covered the case where __call_rcu_nocb_enqueue() needs to wake
    the rcuo kthread due to the queue being initially empty, but did not
    do anything for the case where the queue was overflowing.  This commit
    therefore also defers wakeup for the overflow case.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 6a86eb7bac45..e33562f2a655 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -350,7 +350,7 @@ struct rcu_data {
 	int nocb_p_count_lazy;		/*  (approximate). */
 	wait_queue_head_t nocb_wq;	/* For nocb kthreads to sleep on. */
 	struct task_struct *nocb_kthread;
-	bool nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
+	int nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
 
 	/* The following fields are used by the leader, hence own cacheline. */
 	struct rcu_head *nocb_gp_head ____cacheline_internodealigned_in_smp;
@@ -383,6 +383,11 @@ struct rcu_data {
 #define RCU_FORCE_QS		3	/* Need to force quiescent state. */
 #define RCU_SIGNAL_INIT		RCU_SAVE_DYNTICK
 
+/* Values for nocb_defer_wakeup field in struct rcu_data. */
+#define RCU_NOGP_WAKE_NOT	0
+#define RCU_NOGP_WAKE		1
+#define RCU_NOGP_WAKE_FORCE	2
+
 #define RCU_JIFFIES_TILL_FORCE_QS (1 + (HZ > 250) + (HZ > 500))
 					/* For jiffies_till_first_fqs and */
 					/*  and jiffies_till_next_fqs. */
@@ -589,7 +594,7 @@ static bool __call_rcu_nocb(struct rcu_data *rdp, struct rcu_head *rhp,
 static bool rcu_nocb_adopt_orphan_cbs(struct rcu_state *rsp,
 				      struct rcu_data *rdp,
 				      unsigned long flags);
-static bool rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp);
+static int rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp);
 static void do_nocb_deferred_wakeup(struct rcu_data *rdp);
 static void rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp);
 static void rcu_spawn_nocb_kthreads(struct rcu_state *rsp);

commit 11ed7f934cb807f26da09547b5946c2e534d1dac
Author: Pranith Kumar <bobby.prani@gmail.com>
Date:   Wed Aug 27 16:43:40 2014 -0400

    rcu: Make nocb leader kthreads process pending callbacks after spawning
    
    The nocb callbacks generated before the nocb kthreads are spawned are
    enqueued in the nocb queue for later processing. Commit fbce7497ee5af ("rcu:
    Parallelize and economize NOCB kthread wakeups") introduced nocb leader kthreads
    which checked the nocb_leader_wake flag to see if there were any such pending
    callbacks. A case was reported in which newly spawned leader kthreads were not
    processing the pending callbacks as this flag was not set, which led to a boot
    hang.
    
    The following commit ensures that the newly spawned nocb kthreads process the
    pending callbacks by allowing the kthreads to run immediately after spawning
    instead of waiting. This is done by inverting the logic of nocb_leader_wake
    tests to nocb_leader_sleep which allows us to use the default initialization of
    this flag to 0 to let the kthreads run.
    
    Reported-by: Amit Shah <amit.shah@redhat.com>
    Signed-off-by: Pranith Kumar <bobby.prani@gmail.com>
    Link: http://www.spinics.net/lists/kernel/msg1802899.html
    [ paulmck: Backported to v3.17-rc2. ]
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Amit Shah <amit.shah@redhat.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 71e64c718f75..6a86eb7bac45 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -358,7 +358,7 @@ struct rcu_data {
 	struct rcu_head **nocb_gp_tail;
 	long nocb_gp_count;
 	long nocb_gp_count_lazy;
-	bool nocb_leader_wake;		/* Is the nocb leader thread awake? */
+	bool nocb_leader_sleep;		/* Is the nocb leader thread asleep? */
 	struct rcu_data *nocb_next_follower;
 					/* Next follower in wakeup chain. */
 

commit 1823172ab582eb54456fd1354869bb850a1196ba
Merge: 9963185c0472 b41d1b924d0b c1c3f2c906e3 b58cc46c5f6b 6945915e7f94
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jul 9 09:16:54 2014 -0700

    Merge branches 'doc.2014.07.08a', 'fixes.2014.07.09a', 'maintainers.2014.07.08b', 'nocbs.2014.07.07a' and 'torture.2014.07.07a' into HEAD
    
    doc.2014.07.08a: Documentation updates.
    fixes.2014.07.09a: Miscellaneous fixes.
    maintainers.2014.07.08b: Maintainership updates.
    nocbs.2014.07.07a: Callback-offloading fixes.
    torture.2014.07.07a: Torture-test updates.

commit abaa93d9e1de2c29297e69ddba8ddd38f15064cf
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 12 13:30:25 2014 -0700

    rcu: Simplify priority boosting by putting rt_mutex in rcu_node
    
    RCU priority boosting currently checks for boosting via a pointer in
    task_struct.  However, this is not needed: As Oleg noted, if the
    rt_mutex is placed in the rcu_node instead of on the booster's stack,
    the boostee can simply check it see if it owns the lock.  This commit
    makes this change, shrinking task_struct by one pointer and the kernel
    by thirteen lines.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 3eeb919e26a2..60fb0eaa2d16 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -177,6 +177,9 @@ struct rcu_node {
 				/*  to carry out the boosting is fully */
 				/*  released with no future boostee accesses */
 				/*  before that rt_mutex is re-initialized. */
+	struct rt_mutex boost_mtx;
+				/* Used only for the priority-boosting */
+				/*  side effect, not as a lock. */
 	unsigned long boost_time;
 				/* When to start boosting (jiffies). */
 	struct task_struct *boost_kthread_task;

commit dfeb9765ce3c33cb3cbc5f16db423f1c58a4cc55
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 10 16:31:55 2014 -0700

    rcu: Allow post-unlock reference for rt_mutex
    
    The current approach to RCU priority boosting uses an rt_mutex strictly
    for its priority-boosting side effects.  The rt_mutex_init_proxy_locked()
    function is used by the booster to initialize the lock as held by the
    boostee.  The booster then uses rt_mutex_lock() to acquire this rt_mutex,
    which priority-boosts the boostee.  When the boostee reaches the end
    of its outermost RCU read-side critical section, it checks a field in
    its task structure to see whether it has been boosted, and, if so, uses
    rt_mutex_unlock() to release the rt_mutex.  The booster can then go on
    to boost the next task that is blocking the current RCU grace period.
    
    But reasonable implementations of rt_mutex_unlock() might result in the
    boostee referencing the rt_mutex's data after releasing it.  But the
    booster might have re-initialized the rt_mutex between the time that the
    boostee released it and the time that it later referenced it.  This is
    clearly asking for trouble, so this commit introduces a completion that
    forces the booster to wait until the boostee has completely finished with
    the rt_mutex, thus avoiding the case where the booster is re-initializing
    the rt_mutex before the last boostee's last reference to that rt_mutex.
    
    This of course does introduce some overhead, but the priority-boosting
    code paths are miles from any possible fastpath, and the overhead of
    executing the completion will normally be quite small compared to the
    overhead of priority boosting and deboosting, so this should be OK.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 0f69a79c5b7d..3eeb919e26a2 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -172,6 +172,11 @@ struct rcu_node {
 				/*  queued on this rcu_node structure that */
 				/*  are blocking the current grace period, */
 				/*  there can be no such task. */
+	struct completion boost_completion;
+				/* Used to ensure that the rt_mutex used */
+				/*  to carry out the boosting is fully */
+				/*  released with no future boostee accesses */
+				/*  before that rt_mutex is re-initialized. */
 	unsigned long boost_time;
 				/* When to start boosting (jiffies). */
 	struct task_struct *boost_kthread_task;

commit fbce7497ee5af800a1c350c73f3c3f103cb27a15
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jun 24 09:26:11 2014 -0700

    rcu: Parallelize and economize NOCB kthread wakeups
    
    An 80-CPU system with a context-switch-heavy workload can require so
    many NOCB kthread wakeups that the RCU grace-period kthreads spend several
    tens of percent of a CPU just awakening things.  This clearly will not
    scale well: If you add enough CPUs, the RCU grace-period kthreads would
    get behind, increasing grace-period latency.
    
    To avoid this problem, this commit divides the NOCB kthreads into leaders
    and followers, where the grace-period kthreads awaken the leaders each of
    whom in turn awakens its followers.  By default, the number of groups of
    kthreads is the square root of the number of CPUs, but this default may
    be overridden using the rcutree.rcu_nocb_leader_stride boot parameter.
    This reduces the number of wakeups done per grace period by the RCU
    grace-period kthread by the square root of the number of CPUs, but of
    course by shifting those wakeups to the leaders.  In addition, because
    the leaders do grace periods on behalf of their respective followers,
    the number of wakeups of the followers decreases by up to a factor of two.
    Instead of being awakened once when new callbacks arrive and again
    at the end of the grace period, the followers are awakened only at
    the end of the grace period.
    
    For a numerical example, in a 4096-CPU system, the grace-period kthread
    would awaken 64 leaders, each of which would awaken its 63 followers
    at the end of the grace period.  This compares favorably with the 79
    wakeups for the grace-period kthread on an 80-CPU system.
    
    Reported-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 0f69a79c5b7d..e996d1e53c84 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -334,11 +334,29 @@ struct rcu_data {
 	struct rcu_head **nocb_tail;
 	atomic_long_t nocb_q_count;	/* # CBs waiting for kthread */
 	atomic_long_t nocb_q_count_lazy; /*  (approximate). */
+	struct rcu_head *nocb_follower_head; /* CBs ready to invoke. */
+	struct rcu_head **nocb_follower_tail;
+	atomic_long_t nocb_follower_count; /* # CBs ready to invoke. */
+	atomic_long_t nocb_follower_count_lazy; /*  (approximate). */
 	int nocb_p_count;		/* # CBs being invoked by kthread */
 	int nocb_p_count_lazy;		/*  (approximate). */
 	wait_queue_head_t nocb_wq;	/* For nocb kthreads to sleep on. */
 	struct task_struct *nocb_kthread;
 	bool nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
+
+	/* The following fields are used by the leader, hence own cacheline. */
+	struct rcu_head *nocb_gp_head ____cacheline_internodealigned_in_smp;
+					/* CBs waiting for GP. */
+	struct rcu_head **nocb_gp_tail;
+	long nocb_gp_count;
+	long nocb_gp_count_lazy;
+	bool nocb_leader_wake;		/* Is the nocb leader thread awake? */
+	struct rcu_data *nocb_next_follower;
+					/* Next follower in wakeup chain. */
+
+	/* The following fields are used by the follower, hence new cachline. */
+	struct rcu_data *nocb_leader ____cacheline_internodealigned_in_smp;
+					/* Leader CPU takes GP-end wakeups. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 
 	/* 8) RCU CPU stall data. */
@@ -587,8 +605,14 @@ static bool rcu_nohz_full_cpu(struct rcu_state *rsp);
 /* Sum up queue lengths for tracing. */
 static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
 {
-	*ql = atomic_long_read(&rdp->nocb_q_count) + rdp->nocb_p_count;
-	*qll = atomic_long_read(&rdp->nocb_q_count_lazy) + rdp->nocb_p_count_lazy;
+	*ql = atomic_long_read(&rdp->nocb_q_count) +
+	      rdp->nocb_p_count +
+	      atomic_long_read(&rdp->nocb_follower_count) +
+	      rdp->nocb_p_count + rdp->nocb_gp_count;
+	*qll = atomic_long_read(&rdp->nocb_q_count_lazy) +
+	       rdp->nocb_p_count_lazy +
+	       atomic_long_read(&rdp->nocb_follower_count_lazy) +
+	       rdp->nocb_p_count_lazy + rdp->nocb_gp_count_lazy;
 }
 #else /* #ifdef CONFIG_RCU_NOCB_CPU */
 static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)

commit 4a81e8328d3791a4f99bf5b436d050f6dc5ffea3
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jun 20 16:49:01 2014 -0700

    rcu: Reduce overhead of cond_resched() checks for RCU
    
    Commit ac1bea85781e (Make cond_resched() report RCU quiescent states)
    fixed a problem where a CPU looping in the kernel with but one runnable
    task would give RCU CPU stall warnings, even if the in-kernel loop
    contained cond_resched() calls.  Unfortunately, in so doing, it introduced
    performance regressions in Anton Blanchard's will-it-scale "open1" test.
    The problem appears to be not so much the increased cond_resched() path
    length as an increase in the rate at which grace periods complete, which
    increased per-update grace-period overhead.
    
    This commit takes a different approach to fixing this bug, mainly by
    moving the RCU-visible quiescent state from cond_resched() to
    rcu_note_context_switch(), and by further reducing the check to a
    simple non-zero test of a single per-CPU variable.  However, this
    approach requires that the force-quiescent-state processing send
    resched IPIs to the offending CPUs.  These will be sent only once
    the grace period has reached an age specified by the boot/sysfs
    parameter rcutree.jiffies_till_sched_qs, or once the grace period
    reaches an age halfway to the point at which RCU CPU stall warnings
    will be emitted, whichever comes first.
    
    Reported-by: Dave Hansen <dave.hansen@intel.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Christoph Lameter <cl@gentwo.org>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    [ paulmck: Made rcu_momentary_dyntick_idle() as suggested by the
      ktest build robot.  Also fixed smp_mb() comment as noted by
      Oleg Nesterov. ]
    
    Merge with e552592e (Reduce overhead of cond_resched() checks for RCU)
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index bf2c1e669691..0f69a79c5b7d 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -307,6 +307,9 @@ struct rcu_data {
 	/* 4) reasons this CPU needed to be kicked by force_quiescent_state */
 	unsigned long dynticks_fqs;	/* Kicked due to dynticks idle. */
 	unsigned long offline_fqs;	/* Kicked due to being offline. */
+	unsigned long cond_resched_completed;
+					/* Grace period that needs help */
+					/*  from cond_resched(). */
 
 	/* 5) __rcu_pending() statistics. */
 	unsigned long n_rcu_pending;	/* rcu_pending() calls since boot. */
@@ -392,6 +395,7 @@ struct rcu_state {
 	struct rcu_node *level[RCU_NUM_LVLS];	/* Hierarchy levels. */
 	u32 levelcnt[MAX_RCU_LVLS + 1];		/* # nodes in each level. */
 	u8 levelspread[RCU_NUM_LVLS];		/* kids/node in each level. */
+	u8 flavor_mask;				/* bit in flavor mask. */
 	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */
 	void (*call)(struct rcu_head *head,	/* call_rcu() flavor. */
 		     void (*func)(struct rcu_head *head));
@@ -563,7 +567,7 @@ static bool rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp);
 static void do_nocb_deferred_wakeup(struct rcu_data *rdp);
 static void rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp);
 static void rcu_spawn_nocb_kthreads(struct rcu_state *rsp);
-static void rcu_kick_nohz_cpu(int cpu);
+static void __maybe_unused rcu_kick_nohz_cpu(int cpu);
 static bool init_nocb_callback_list(struct rcu_data *rdp);
 static void rcu_sysidle_enter(struct rcu_dynticks *rdtp, int irq);
 static void rcu_sysidle_exit(struct rcu_dynticks *rdtp, int irq);

commit f5d2a0450ddfda337ffee04abf4f7b40f132c509
Merge: d07e6d080d92 a5d6d3a1b00a 6348675c4e36
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 14 10:57:31 2014 -0700

    Merge branches 'doc.2014.04.29a', 'fixes.2014.04.29a' and 'torture.2014.05.14a' into HEAD
    
    doc.2014.04.29a:  Documentation updates.
    fixes.2014.04.29a:  Miscellaneous fixes.
    torture.2014.05.14a:  RCU/Lock torture tests.

commit afea227fd4acf4f097a9e77bbc2f07d4856ebd01
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 12 07:10:41 2014 -0700

    rcutorture: Export RCU grace-period kthread wait state to rcutorture
    
    This commit allows rcutorture to print additional state for the
    RCU grace-period kthreads in cases where RCU seems reluctant to
    start a new grace period.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 75dc3c39a02a..c2fd1e722879 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -406,7 +406,8 @@ struct rcu_state {
 	unsigned long completed;		/* # of last completed gp. */
 	struct task_struct *gp_kthread;		/* Task for grace periods. */
 	wait_queue_head_t gp_wq;		/* Where GP task waits. */
-	int gp_flags;				/* Commands for GP task. */
+	short gp_flags;				/* Commands for GP task. */
+	short gp_state;				/* GP kthread sleep state. */
 
 	/* End of fields guarded by root rcu_node's lock. */
 
@@ -469,6 +470,11 @@ struct rcu_state {
 #define RCU_GP_FLAG_INIT 0x1	/* Need grace-period initialization. */
 #define RCU_GP_FLAG_FQS  0x2	/* Need grace-period quiescent-state forcing. */
 
+/* Values for rcu_state structure's gp_flags field. */
+#define RCU_GP_WAIT_INIT 0	/* Initial state. */
+#define RCU_GP_WAIT_GPS  1	/* Wait for grace-period start. */
+#define RCU_GP_WAIT_FQS  2	/* Wait for force-quiescent-state time. */
+
 extern struct list_head rcu_struct_flavors;
 
 /* Sequence through rcu_state structures for each RCU flavor. */

commit 48a7639ce80cf279834d0d44865e49ecd714f37d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Mar 11 13:02:16 2014 -0700

    rcu: Make callers awaken grace-period kthread
    
    The rcu_start_gp_advanced() function currently uses irq_work_queue()
    to defer wakeups of the RCU grace-period kthread.  This deferring
    is necessary to avoid RCU-scheduler deadlocks involving the rcu_node
    structure's lock, meaning that RCU cannot call any of the scheduler's
    wake-up functions while holding one of these locks.
    
    Unfortunately, the second and subsequent calls to irq_work_queue() are
    ignored, and the first call will be ignored (aside from queuing the work
    item) if the scheduler-clock tick is turned off.  This is OK for many
    uses, especially those where irq_work_queue() is called from an interrupt
    or softirq handler, because in those cases the scheduler-clock-tick state
    will be re-evaluated, which will turn the scheduler-clock tick back on.
    On the next tick, any deferred work will then be processed.
    
    However, this strategy does not always work for RCU, which can be invoked
    at process level from idle CPUs.  In this case, the tick might never
    be turned back on, indefinitely defering a grace-period start request.
    Note that the RCU CPU stall detector cannot see this condition, because
    there is no RCU grace period in progress.  Therefore, we can (and do!)
    see long tens-of-seconds stalls in grace-period handling.  In theory,
    we could see a full grace-period hang, but rcutorture testing to date
    has seen only the tens-of-seconds stalls.  Event tracing demonstrates
    that irq_work_queue() is being called repeatedly to no effect during
    these stalls: The "newreq" event appears repeatedly from a task that is
    not one of the grace-period kthreads.
    
    In theory, irq_work_queue() might be fixed to avoid this sort of issue,
    but RCU's requirements are unusual and it is quite straightforward to pass
    wake-up responsibility up through RCU's call chain, so that the wakeup
    happens when the offending locks are released.
    
    This commit therefore makes this change.  The rcu_start_gp_advanced(),
    rcu_start_future_gp(), rcu_accelerate_cbs(), rcu_advance_cbs(),
    __note_gp_changes(), and rcu_start_gp() functions now return a boolean
    which indicates when a wake-up is needed.  A new rcu_gp_kthread_wake()
    does the wakeup when it is necessary and safe to do so: No self-wakes,
    no wake-ups if the ->gp_flags field indicates there is no need (as in
    someone else did the wake-up before we got around to it), and no wake-ups
    before the grace-period kthread has been created.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 13766ad2f4ee..cdbb392d5b4b 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -461,7 +461,6 @@ struct rcu_state {
 	const char *name;			/* Name of structure. */
 	char abbr;				/* Abbreviated name. */
 	struct list_head flavors;		/* List of RCU flavors. */
-	struct irq_work wakeup_work;		/* Postponed wakeups */
 };
 
 /* Values for rcu_state structure's gp_flags field. */

commit 9b67122ae3da3018c966148233739116ed89502a
Author: Iulia Manda <iulia.manda21@gmail.com>
Date:   Tue Mar 11 13:18:22 2014 +0200

    rcu: Remove unused rcu_data structure field
    
    The ->preemptible field in rcu_data is only initialized in the function
    rcu_init_percpu_data(), and never used.  This commit therefore removes
    this field.
    
    Signed-off-by: Iulia Manda <iulia.manda21@gmail.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 7b572c5c65e1..13766ad2f4ee 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -252,7 +252,6 @@ struct rcu_data {
 	bool		passed_quiesce;	/* User-mode/idle loop etc. */
 	bool		qs_pending;	/* Core waits for quiesc state. */
 	bool		beenonline;	/* CPU online at least once. */
-	bool		preemptible;	/* Preemptible RCU? */
 	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
 	unsigned long grpmask;		/* Mask to apply to leaf qsmask. */
 #ifdef CONFIG_RCU_CPU_STALL_INFO

commit 365187fbc04fd55766bf6a94e37e558505bf480a
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Mar 10 10:55:52 2014 -0700

    rcu: Update cpu_needs_another_gp() for futures from non-NOCB CPUs
    
    In the old days, the only source of requests for future grace periods
    was NOCB CPUs.  This has changed: CPUs routinely post requests for
    future grace periods in order to promote power efficiency and reduce
    OS jitter with minimal impact on grace-period latency.  This commit
    therefore updates cpu_needs_another_gp() to invoke rcu_future_needs_gp()
    instead of rcu_nocb_needs_gp().  The latter is no longer used, so is
    now removed.  This commit also adds tracing for the irq_work_queue()
    wakeup case.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 75dc3c39a02a..7b572c5c65e1 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -547,7 +547,6 @@ static void print_cpu_stall_info(struct rcu_state *rsp, int cpu);
 static void print_cpu_stall_info_end(void);
 static void zero_cpu_stall_ticks(struct rcu_data *rdp);
 static void increment_cpu_stall_ticks(void);
-static int rcu_nocb_needs_gp(struct rcu_state *rsp);
 static void rcu_nocb_gp_set(struct rcu_node *rnp, int nrq);
 static void rcu_nocb_gp_cleanup(struct rcu_state *rsp, struct rcu_node *rnp);
 static void rcu_init_one_nocb(struct rcu_node *rnp);

commit 87de1cfdc55b16b794e245b07322340725149d62
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Dec 3 10:02:52 2013 -0800

    rcu: Stop tracking FSF's postal address
    
    All of the RCU source files have the usual GPL header, which contains a
    long-obsolete postal address for FSF.  To avoid the need to track the
    FSF office's movements, this commit substitutes the URL where GPL may
    be found.
    
    Reported-by: Greg KH <gregkh@linuxfoundation.org>
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8c19873f1ac9..75dc3c39a02a 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -13,8 +13,8 @@
  * GNU General Public License for more details.
  *
  * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
  *
  * Copyright IBM Corporation, 2008
  *

commit a096932f0c9c9dca9cce72f1c0fb2395df8f2dff
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Nov 8 09:03:10 2013 -0800

    rcu: Don't activate RCU core on NO_HZ_FULL CPUs
    
    Whenever a CPU receives a scheduling-clock interrupt, RCU checks to see
    if the RCU core needs anything from this CPU.  If so, RCU raises
    RCU_SOFTIRQ to carry out any needed processing.
    
    This approach has worked well historically, but it is undesirable on
    NO_HZ_FULL CPUs.  Such CPUs are expected to spend almost all of their time
    in userspace, so that scheduling-clock interrupts can be disabled while
    there is only one runnable task on the CPU in question.  Unfortunately,
    raising any softirq has the potential to wake up ksoftirqd, which would
    provide the second runnable task on that CPU, preventing disabling of
    scheduling-clock interrupts.
    
    What is needed instead is for RCU to leave NO_HZ_FULL CPUs alone,
    relying on the grace-period kthreads' quiescent-state forcing to
    do any needed RCU work on behalf of those CPUs.
    
    This commit therefore refrains from raising RCU_SOFTIRQ on any
    NO_HZ_FULL CPUs during any grace periods that have been in effect
    for less than one second.  The one-second limit handles the case
    where an inappropriate workload is running on a NO_HZ_FULL CPU
    that features lots of scheduling-clock interrupts, but no idle
    or userspace time.
    
    Reported-by: Mike Galbraith <bitbucket@online.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Mike Galbraith <bitbucket@online.de>
    Toasted-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index a87adfc2916b..8c19873f1ac9 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -571,6 +571,7 @@ static void rcu_sysidle_report_gp(struct rcu_state *rsp, int isidle,
 				  unsigned long maxj);
 static void rcu_bind_gp_kthread(void);
 static void rcu_sysidle_init_percpu_data(struct rcu_dynticks *rdtp);
+static bool rcu_nohz_full_cpu(struct rcu_state *rsp);
 
 #endif /* #ifndef RCU_TREE_NONCORE */
 

commit 96d3fd0d315a949e30adc80f086031c5cdf070d1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Oct 4 14:33:34 2013 -0700

    rcu: Break call_rcu() deadlock involving scheduler and perf
    
    Dave Jones got the following lockdep splat:
    
    >  ======================================================
    >  [ INFO: possible circular locking dependency detected ]
    >  3.12.0-rc3+ #92 Not tainted
    >  -------------------------------------------------------
    >  trinity-child2/15191 is trying to acquire lock:
    >   (&rdp->nocb_wq){......}, at: [<ffffffff8108ff43>] __wake_up+0x23/0x50
    >
    > but task is already holding lock:
    >   (&ctx->lock){-.-...}, at: [<ffffffff81154c19>] perf_event_exit_task+0x109/0x230
    >
    > which lock already depends on the new lock.
    >
    >
    > the existing dependency chain (in reverse order) is:
    >
    > -> #3 (&ctx->lock){-.-...}:
    >         [<ffffffff810cc243>] lock_acquire+0x93/0x200
    >         [<ffffffff81733f90>] _raw_spin_lock+0x40/0x80
    >         [<ffffffff811500ff>] __perf_event_task_sched_out+0x2df/0x5e0
    >         [<ffffffff81091b83>] perf_event_task_sched_out+0x93/0xa0
    >         [<ffffffff81732052>] __schedule+0x1d2/0xa20
    >         [<ffffffff81732f30>] preempt_schedule_irq+0x50/0xb0
    >         [<ffffffff817352b6>] retint_kernel+0x26/0x30
    >         [<ffffffff813eed04>] tty_flip_buffer_push+0x34/0x50
    >         [<ffffffff813f0504>] pty_write+0x54/0x60
    >         [<ffffffff813e900d>] n_tty_write+0x32d/0x4e0
    >         [<ffffffff813e5838>] tty_write+0x158/0x2d0
    >         [<ffffffff811c4850>] vfs_write+0xc0/0x1f0
    >         [<ffffffff811c52cc>] SyS_write+0x4c/0xa0
    >         [<ffffffff8173d4e4>] tracesys+0xdd/0xe2
    >
    > -> #2 (&rq->lock){-.-.-.}:
    >         [<ffffffff810cc243>] lock_acquire+0x93/0x200
    >         [<ffffffff81733f90>] _raw_spin_lock+0x40/0x80
    >         [<ffffffff810980b2>] wake_up_new_task+0xc2/0x2e0
    >         [<ffffffff81054336>] do_fork+0x126/0x460
    >         [<ffffffff81054696>] kernel_thread+0x26/0x30
    >         [<ffffffff8171ff93>] rest_init+0x23/0x140
    >         [<ffffffff81ee1e4b>] start_kernel+0x3f6/0x403
    >         [<ffffffff81ee1571>] x86_64_start_reservations+0x2a/0x2c
    >         [<ffffffff81ee1664>] x86_64_start_kernel+0xf1/0xf4
    >
    > -> #1 (&p->pi_lock){-.-.-.}:
    >         [<ffffffff810cc243>] lock_acquire+0x93/0x200
    >         [<ffffffff8173419b>] _raw_spin_lock_irqsave+0x4b/0x90
    >         [<ffffffff810979d1>] try_to_wake_up+0x31/0x350
    >         [<ffffffff81097d62>] default_wake_function+0x12/0x20
    >         [<ffffffff81084af8>] autoremove_wake_function+0x18/0x40
    >         [<ffffffff8108ea38>] __wake_up_common+0x58/0x90
    >         [<ffffffff8108ff59>] __wake_up+0x39/0x50
    >         [<ffffffff8110d4f8>] __call_rcu_nocb_enqueue+0xa8/0xc0
    >         [<ffffffff81111450>] __call_rcu+0x140/0x820
    >         [<ffffffff81111b8d>] call_rcu+0x1d/0x20
    >         [<ffffffff81093697>] cpu_attach_domain+0x287/0x360
    >         [<ffffffff81099d7e>] build_sched_domains+0xe5e/0x10a0
    >         [<ffffffff81efa7fc>] sched_init_smp+0x3b7/0x47a
    >         [<ffffffff81ee1f4e>] kernel_init_freeable+0xf6/0x202
    >         [<ffffffff817200be>] kernel_init+0xe/0x190
    >         [<ffffffff8173d22c>] ret_from_fork+0x7c/0xb0
    >
    > -> #0 (&rdp->nocb_wq){......}:
    >         [<ffffffff810cb7ca>] __lock_acquire+0x191a/0x1be0
    >         [<ffffffff810cc243>] lock_acquire+0x93/0x200
    >         [<ffffffff8173419b>] _raw_spin_lock_irqsave+0x4b/0x90
    >         [<ffffffff8108ff43>] __wake_up+0x23/0x50
    >         [<ffffffff8110d4f8>] __call_rcu_nocb_enqueue+0xa8/0xc0
    >         [<ffffffff81111450>] __call_rcu+0x140/0x820
    >         [<ffffffff81111bb0>] kfree_call_rcu+0x20/0x30
    >         [<ffffffff81149abf>] put_ctx+0x4f/0x70
    >         [<ffffffff81154c3e>] perf_event_exit_task+0x12e/0x230
    >         [<ffffffff81056b8d>] do_exit+0x30d/0xcc0
    >         [<ffffffff8105893c>] do_group_exit+0x4c/0xc0
    >         [<ffffffff810589c4>] SyS_exit_group+0x14/0x20
    >         [<ffffffff8173d4e4>] tracesys+0xdd/0xe2
    >
    > other info that might help us debug this:
    >
    > Chain exists of:
    >   &rdp->nocb_wq --> &rq->lock --> &ctx->lock
    >
    >   Possible unsafe locking scenario:
    >
    >         CPU0                    CPU1
    >         ----                    ----
    >    lock(&ctx->lock);
    >                                 lock(&rq->lock);
    >                                 lock(&ctx->lock);
    >    lock(&rdp->nocb_wq);
    >
    >  *** DEADLOCK ***
    >
    > 1 lock held by trinity-child2/15191:
    >  #0:  (&ctx->lock){-.-...}, at: [<ffffffff81154c19>] perf_event_exit_task+0x109/0x230
    >
    > stack backtrace:
    > CPU: 2 PID: 15191 Comm: trinity-child2 Not tainted 3.12.0-rc3+ #92
    >  ffffffff82565b70 ffff880070c2dbf8 ffffffff8172a363 ffffffff824edf40
    >  ffff880070c2dc38 ffffffff81726741 ffff880070c2dc90 ffff88022383b1c0
    >  ffff88022383aac0 0000000000000000 ffff88022383b188 ffff88022383b1c0
    > Call Trace:
    >  [<ffffffff8172a363>] dump_stack+0x4e/0x82
    >  [<ffffffff81726741>] print_circular_bug+0x200/0x20f
    >  [<ffffffff810cb7ca>] __lock_acquire+0x191a/0x1be0
    >  [<ffffffff810c6439>] ? get_lock_stats+0x19/0x60
    >  [<ffffffff8100b2f4>] ? native_sched_clock+0x24/0x80
    >  [<ffffffff810cc243>] lock_acquire+0x93/0x200
    >  [<ffffffff8108ff43>] ? __wake_up+0x23/0x50
    >  [<ffffffff8173419b>] _raw_spin_lock_irqsave+0x4b/0x90
    >  [<ffffffff8108ff43>] ? __wake_up+0x23/0x50
    >  [<ffffffff8108ff43>] __wake_up+0x23/0x50
    >  [<ffffffff8110d4f8>] __call_rcu_nocb_enqueue+0xa8/0xc0
    >  [<ffffffff81111450>] __call_rcu+0x140/0x820
    >  [<ffffffff8109bc8f>] ? local_clock+0x3f/0x50
    >  [<ffffffff81111bb0>] kfree_call_rcu+0x20/0x30
    >  [<ffffffff81149abf>] put_ctx+0x4f/0x70
    >  [<ffffffff81154c3e>] perf_event_exit_task+0x12e/0x230
    >  [<ffffffff81056b8d>] do_exit+0x30d/0xcc0
    >  [<ffffffff810c9af5>] ? trace_hardirqs_on_caller+0x115/0x1e0
    >  [<ffffffff810c9bcd>] ? trace_hardirqs_on+0xd/0x10
    >  [<ffffffff8105893c>] do_group_exit+0x4c/0xc0
    >  [<ffffffff810589c4>] SyS_exit_group+0x14/0x20
    >  [<ffffffff8173d4e4>] tracesys+0xdd/0xe2
    
    The underlying problem is that perf is invoking call_rcu() with the
    scheduler locks held, but in NOCB mode, call_rcu() will with high
    probability invoke the scheduler -- which just might want to use its
    locks.  The reason that call_rcu() needs to invoke the scheduler is
    to wake up the corresponding rcuo callback-offload kthread, which
    does the job of starting up a grace period and invoking the callbacks
    afterwards.
    
    One solution (championed on a related problem by Lai Jiangshan) is to
    simply defer the wakeup to some point where scheduler locks are no longer
    held.  Since we don't want to unnecessarily incur the cost of such
    deferral, the task before us is threefold:
    
    1.      Determine when it is likely that a relevant scheduler lock is held.
    
    2.      Defer the wakeup in such cases.
    
    3.      Ensure that all deferred wakeups eventually happen, preferably
            sooner rather than later.
    
    We use irqs_disabled_flags() as a proxy for relevant scheduler locks
    being held.  This works because the relevant locks are always acquired
    with interrupts disabled.  We may defer more often than needed, but that
    is at least safe.
    
    The wakeup deferral is tracked via a new field in the per-CPU and
    per-RCU-flavor rcu_data structure, namely ->nocb_defer_wakeup.
    
    This flag is checked by the RCU core processing.  The __rcu_pending()
    function now checks this flag, which causes rcu_check_callbacks()
    to initiate RCU core processing at each scheduling-clock interrupt
    where this flag is set.  Of course this is not sufficient because
    scheduling-clock interrupts are often turned off (the things we used to
    be able to count on!).  So the flags are also checked on entry to any
    state that RCU considers to be idle, which includes both NO_HZ_IDLE idle
    state and NO_HZ_FULL user-mode-execution state.
    
    This approach should allow call_rcu() to be invoked regardless of what
    locks you might be holding, the key word being "should".
    
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 8e34d8674a4e..a87adfc2916b 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -317,6 +317,7 @@ struct rcu_data {
 	unsigned long n_rp_cpu_needs_gp;
 	unsigned long n_rp_gp_completed;
 	unsigned long n_rp_gp_started;
+	unsigned long n_rp_nocb_defer_wakeup;
 	unsigned long n_rp_need_nothing;
 
 	/* 6) _rcu_barrier() and OOM callbacks. */
@@ -335,6 +336,7 @@ struct rcu_data {
 	int nocb_p_count_lazy;		/*  (approximate). */
 	wait_queue_head_t nocb_wq;	/* For nocb kthreads to sleep on. */
 	struct task_struct *nocb_kthread;
+	bool nocb_defer_wakeup;		/* Defer wakeup of nocb_kthread. */
 #endif /* #ifdef CONFIG_RCU_NOCB_CPU */
 
 	/* 8) RCU CPU stall data. */
@@ -550,9 +552,12 @@ static void rcu_nocb_gp_set(struct rcu_node *rnp, int nrq);
 static void rcu_nocb_gp_cleanup(struct rcu_state *rsp, struct rcu_node *rnp);
 static void rcu_init_one_nocb(struct rcu_node *rnp);
 static bool __call_rcu_nocb(struct rcu_data *rdp, struct rcu_head *rhp,
-			    bool lazy);
+			    bool lazy, unsigned long flags);
 static bool rcu_nocb_adopt_orphan_cbs(struct rcu_state *rsp,
-				      struct rcu_data *rdp);
+				      struct rcu_data *rdp,
+				      unsigned long flags);
+static bool rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp);
+static void do_nocb_deferred_wakeup(struct rcu_data *rdp);
 static void rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp);
 static void rcu_spawn_nocb_kthreads(struct rcu_state *rsp);
 static void rcu_kick_nohz_cpu(int cpu);

commit 6193c76aba8ec3cc5f083c35efbab9ed924125f6
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Sep 23 13:57:18 2013 -0700

    rcu: Kick CPU halfway to RCU CPU stall warning
    
    When an RCU CPU stall warning occurs, the CPU invokes resched_cpu() on
    itself.  This can help move the grace period forward in some situations,
    but it would be even better to do this -before- the RCU CPU stall warning.
    This commit therefore causes resched_cpu() to be called every five jiffies
    once the system is halfway to an RCU CPU stall warning.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
index 52be957c9fe2..8e34d8674a4e 100644
--- a/kernel/rcu/tree.h
+++ b/kernel/rcu/tree.h
@@ -453,6 +453,8 @@ struct rcu_state {
 						/*  but in jiffies. */
 	unsigned long jiffies_stall;		/* Time at which to check */
 						/*  for CPU stalls. */
+	unsigned long jiffies_resched;		/* Time at which to resched */
+						/*  a reluctant CPU. */
 	unsigned long gp_max;			/* Maximum GP duration in */
 						/*  jiffies. */
 	const char *name;			/* Name of structure. */

commit 4102adab9189c8ea2f0cdd2f88345fd25d2790f1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 8 20:23:47 2013 -0700

    rcu: Move RCU-related source code to kernel/rcu directory
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/rcu/tree.h b/kernel/rcu/tree.h
new file mode 100644
index 000000000000..52be957c9fe2
--- /dev/null
+++ b/kernel/rcu/tree.h
@@ -0,0 +1,585 @@
+/*
+ * Read-Copy Update mechanism for mutual exclusion (tree-based version)
+ * Internal non-public definitions.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright IBM Corporation, 2008
+ *
+ * Author: Ingo Molnar <mingo@elte.hu>
+ *	   Paul E. McKenney <paulmck@linux.vnet.ibm.com>
+ */
+
+#include <linux/cache.h>
+#include <linux/spinlock.h>
+#include <linux/threads.h>
+#include <linux/cpumask.h>
+#include <linux/seqlock.h>
+#include <linux/irq_work.h>
+
+/*
+ * Define shape of hierarchy based on NR_CPUS, CONFIG_RCU_FANOUT, and
+ * CONFIG_RCU_FANOUT_LEAF.
+ * In theory, it should be possible to add more levels straightforwardly.
+ * In practice, this did work well going from three levels to four.
+ * Of course, your mileage may vary.
+ */
+#define MAX_RCU_LVLS 4
+#define RCU_FANOUT_1	      (CONFIG_RCU_FANOUT_LEAF)
+#define RCU_FANOUT_2	      (RCU_FANOUT_1 * CONFIG_RCU_FANOUT)
+#define RCU_FANOUT_3	      (RCU_FANOUT_2 * CONFIG_RCU_FANOUT)
+#define RCU_FANOUT_4	      (RCU_FANOUT_3 * CONFIG_RCU_FANOUT)
+
+#if NR_CPUS <= RCU_FANOUT_1
+#  define RCU_NUM_LVLS	      1
+#  define NUM_RCU_LVL_0	      1
+#  define NUM_RCU_LVL_1	      (NR_CPUS)
+#  define NUM_RCU_LVL_2	      0
+#  define NUM_RCU_LVL_3	      0
+#  define NUM_RCU_LVL_4	      0
+#elif NR_CPUS <= RCU_FANOUT_2
+#  define RCU_NUM_LVLS	      2
+#  define NUM_RCU_LVL_0	      1
+#  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
+#  define NUM_RCU_LVL_2	      (NR_CPUS)
+#  define NUM_RCU_LVL_3	      0
+#  define NUM_RCU_LVL_4	      0
+#elif NR_CPUS <= RCU_FANOUT_3
+#  define RCU_NUM_LVLS	      3
+#  define NUM_RCU_LVL_0	      1
+#  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
+#  define NUM_RCU_LVL_2	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
+#  define NUM_RCU_LVL_3	      (NR_CPUS)
+#  define NUM_RCU_LVL_4	      0
+#elif NR_CPUS <= RCU_FANOUT_4
+#  define RCU_NUM_LVLS	      4
+#  define NUM_RCU_LVL_0	      1
+#  define NUM_RCU_LVL_1	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_3)
+#  define NUM_RCU_LVL_2	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_2)
+#  define NUM_RCU_LVL_3	      DIV_ROUND_UP(NR_CPUS, RCU_FANOUT_1)
+#  define NUM_RCU_LVL_4	      (NR_CPUS)
+#else
+# error "CONFIG_RCU_FANOUT insufficient for NR_CPUS"
+#endif /* #if (NR_CPUS) <= RCU_FANOUT_1 */
+
+#define RCU_SUM (NUM_RCU_LVL_0 + NUM_RCU_LVL_1 + NUM_RCU_LVL_2 + NUM_RCU_LVL_3 + NUM_RCU_LVL_4)
+#define NUM_RCU_NODES (RCU_SUM - NR_CPUS)
+
+extern int rcu_num_lvls;
+extern int rcu_num_nodes;
+
+/*
+ * Dynticks per-CPU state.
+ */
+struct rcu_dynticks {
+	long long dynticks_nesting; /* Track irq/process nesting level. */
+				    /* Process level is worth LLONG_MAX/2. */
+	int dynticks_nmi_nesting;   /* Track NMI nesting level. */
+	atomic_t dynticks;	    /* Even value for idle, else odd. */
+#ifdef CONFIG_NO_HZ_FULL_SYSIDLE
+	long long dynticks_idle_nesting;
+				    /* irq/process nesting level from idle. */
+	atomic_t dynticks_idle;	    /* Even value for idle, else odd. */
+				    /*  "Idle" excludes userspace execution. */
+	unsigned long dynticks_idle_jiffies;
+				    /* End of last non-NMI non-idle period. */
+#endif /* #ifdef CONFIG_NO_HZ_FULL_SYSIDLE */
+#ifdef CONFIG_RCU_FAST_NO_HZ
+	bool all_lazy;		    /* Are all CPU's CBs lazy? */
+	unsigned long nonlazy_posted;
+				    /* # times non-lazy CBs posted to CPU. */
+	unsigned long nonlazy_posted_snap;
+				    /* idle-period nonlazy_posted snapshot. */
+	unsigned long last_accelerate;
+				    /* Last jiffy CBs were accelerated. */
+	unsigned long last_advance_all;
+				    /* Last jiffy CBs were all advanced. */
+	int tick_nohz_enabled_snap; /* Previously seen value from sysfs. */
+#endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
+};
+
+/* RCU's kthread states for tracing. */
+#define RCU_KTHREAD_STOPPED  0
+#define RCU_KTHREAD_RUNNING  1
+#define RCU_KTHREAD_WAITING  2
+#define RCU_KTHREAD_OFFCPU   3
+#define RCU_KTHREAD_YIELDING 4
+#define RCU_KTHREAD_MAX      4
+
+/*
+ * Definition for node within the RCU grace-period-detection hierarchy.
+ */
+struct rcu_node {
+	raw_spinlock_t lock;	/* Root rcu_node's lock protects some */
+				/*  rcu_state fields as well as following. */
+	unsigned long gpnum;	/* Current grace period for this node. */
+				/*  This will either be equal to or one */
+				/*  behind the root rcu_node's gpnum. */
+	unsigned long completed; /* Last GP completed for this node. */
+				/*  This will either be equal to or one */
+				/*  behind the root rcu_node's gpnum. */
+	unsigned long qsmask;	/* CPUs or groups that need to switch in */
+				/*  order for current grace period to proceed.*/
+				/*  In leaf rcu_node, each bit corresponds to */
+				/*  an rcu_data structure, otherwise, each */
+				/*  bit corresponds to a child rcu_node */
+				/*  structure. */
+	unsigned long expmask;	/* Groups that have ->blkd_tasks */
+				/*  elements that need to drain to allow the */
+				/*  current expedited grace period to */
+				/*  complete (only for TREE_PREEMPT_RCU). */
+	unsigned long qsmaskinit;
+				/* Per-GP initial value for qsmask & expmask. */
+	unsigned long grpmask;	/* Mask to apply to parent qsmask. */
+				/*  Only one bit will be set in this mask. */
+	int	grplo;		/* lowest-numbered CPU or group here. */
+	int	grphi;		/* highest-numbered CPU or group here. */
+	u8	grpnum;		/* CPU/group number for next level up. */
+	u8	level;		/* root is at level 0. */
+	struct rcu_node *parent;
+	struct list_head blkd_tasks;
+				/* Tasks blocked in RCU read-side critical */
+				/*  section.  Tasks are placed at the head */
+				/*  of this list and age towards the tail. */
+	struct list_head *gp_tasks;
+				/* Pointer to the first task blocking the */
+				/*  current grace period, or NULL if there */
+				/*  is no such task. */
+	struct list_head *exp_tasks;
+				/* Pointer to the first task blocking the */
+				/*  current expedited grace period, or NULL */
+				/*  if there is no such task.  If there */
+				/*  is no current expedited grace period, */
+				/*  then there can cannot be any such task. */
+#ifdef CONFIG_RCU_BOOST
+	struct list_head *boost_tasks;
+				/* Pointer to first task that needs to be */
+				/*  priority boosted, or NULL if no priority */
+				/*  boosting is needed for this rcu_node */
+				/*  structure.  If there are no tasks */
+				/*  queued on this rcu_node structure that */
+				/*  are blocking the current grace period, */
+				/*  there can be no such task. */
+	unsigned long boost_time;
+				/* When to start boosting (jiffies). */
+	struct task_struct *boost_kthread_task;
+				/* kthread that takes care of priority */
+				/*  boosting for this rcu_node structure. */
+	unsigned int boost_kthread_status;
+				/* State of boost_kthread_task for tracing. */
+	unsigned long n_tasks_boosted;
+				/* Total number of tasks boosted. */
+	unsigned long n_exp_boosts;
+				/* Number of tasks boosted for expedited GP. */
+	unsigned long n_normal_boosts;
+				/* Number of tasks boosted for normal GP. */
+	unsigned long n_balk_blkd_tasks;
+				/* Refused to boost: no blocked tasks. */
+	unsigned long n_balk_exp_gp_tasks;
+				/* Refused to boost: nothing blocking GP. */
+	unsigned long n_balk_boost_tasks;
+				/* Refused to boost: already boosting. */
+	unsigned long n_balk_notblocked;
+				/* Refused to boost: RCU RS CS still running. */
+	unsigned long n_balk_notyet;
+				/* Refused to boost: not yet time. */
+	unsigned long n_balk_nos;
+				/* Refused to boost: not sure why, though. */
+				/*  This can happen due to race conditions. */
+#endif /* #ifdef CONFIG_RCU_BOOST */
+#ifdef CONFIG_RCU_NOCB_CPU
+	wait_queue_head_t nocb_gp_wq[2];
+				/* Place for rcu_nocb_kthread() to wait GP. */
+#endif /* #ifdef CONFIG_RCU_NOCB_CPU */
+	int need_future_gp[2];
+				/* Counts of upcoming no-CB GP requests. */
+	raw_spinlock_t fqslock ____cacheline_internodealigned_in_smp;
+} ____cacheline_internodealigned_in_smp;
+
+/*
+ * Do a full breadth-first scan of the rcu_node structures for the
+ * specified rcu_state structure.
+ */
+#define rcu_for_each_node_breadth_first(rsp, rnp) \
+	for ((rnp) = &(rsp)->node[0]; \
+	     (rnp) < &(rsp)->node[rcu_num_nodes]; (rnp)++)
+
+/*
+ * Do a breadth-first scan of the non-leaf rcu_node structures for the
+ * specified rcu_state structure.  Note that if there is a singleton
+ * rcu_node tree with but one rcu_node structure, this loop is a no-op.
+ */
+#define rcu_for_each_nonleaf_node_breadth_first(rsp, rnp) \
+	for ((rnp) = &(rsp)->node[0]; \
+	     (rnp) < (rsp)->level[rcu_num_lvls - 1]; (rnp)++)
+
+/*
+ * Scan the leaves of the rcu_node hierarchy for the specified rcu_state
+ * structure.  Note that if there is a singleton rcu_node tree with but
+ * one rcu_node structure, this loop -will- visit the rcu_node structure.
+ * It is still a leaf node, even if it is also the root node.
+ */
+#define rcu_for_each_leaf_node(rsp, rnp) \
+	for ((rnp) = (rsp)->level[rcu_num_lvls - 1]; \
+	     (rnp) < &(rsp)->node[rcu_num_nodes]; (rnp)++)
+
+/* Index values for nxttail array in struct rcu_data. */
+#define RCU_DONE_TAIL		0	/* Also RCU_WAIT head. */
+#define RCU_WAIT_TAIL		1	/* Also RCU_NEXT_READY head. */
+#define RCU_NEXT_READY_TAIL	2	/* Also RCU_NEXT head. */
+#define RCU_NEXT_TAIL		3
+#define RCU_NEXT_SIZE		4
+
+/* Per-CPU data for read-copy update. */
+struct rcu_data {
+	/* 1) quiescent-state and grace-period handling : */
+	unsigned long	completed;	/* Track rsp->completed gp number */
+					/*  in order to detect GP end. */
+	unsigned long	gpnum;		/* Highest gp number that this CPU */
+					/*  is aware of having started. */
+	bool		passed_quiesce;	/* User-mode/idle loop etc. */
+	bool		qs_pending;	/* Core waits for quiesc state. */
+	bool		beenonline;	/* CPU online at least once. */
+	bool		preemptible;	/* Preemptible RCU? */
+	struct rcu_node *mynode;	/* This CPU's leaf of hierarchy */
+	unsigned long grpmask;		/* Mask to apply to leaf qsmask. */
+#ifdef CONFIG_RCU_CPU_STALL_INFO
+	unsigned long	ticks_this_gp;	/* The number of scheduling-clock */
+					/*  ticks this CPU has handled */
+					/*  during and after the last grace */
+					/* period it is aware of. */
+#endif /* #ifdef CONFIG_RCU_CPU_STALL_INFO */
+
+	/* 2) batch handling */
+	/*
+	 * If nxtlist is not NULL, it is partitioned as follows.
+	 * Any of the partitions might be empty, in which case the
+	 * pointer to that partition will be equal to the pointer for
+	 * the following partition.  When the list is empty, all of
+	 * the nxttail elements point to the ->nxtlist pointer itself,
+	 * which in that case is NULL.
+	 *
+	 * [nxtlist, *nxttail[RCU_DONE_TAIL]):
+	 *	Entries that batch # <= ->completed
+	 *	The grace period for these entries has completed, and
+	 *	the other grace-period-completed entries may be moved
+	 *	here temporarily in rcu_process_callbacks().
+	 * [*nxttail[RCU_DONE_TAIL], *nxttail[RCU_WAIT_TAIL]):
+	 *	Entries that batch # <= ->completed - 1: waiting for current GP
+	 * [*nxttail[RCU_WAIT_TAIL], *nxttail[RCU_NEXT_READY_TAIL]):
+	 *	Entries known to have arrived before current GP ended
+	 * [*nxttail[RCU_NEXT_READY_TAIL], *nxttail[RCU_NEXT_TAIL]):
+	 *	Entries that might have arrived after current GP ended
+	 *	Note that the value of *nxttail[RCU_NEXT_TAIL] will
+	 *	always be NULL, as this is the end of the list.
+	 */
+	struct rcu_head *nxtlist;
+	struct rcu_head **nxttail[RCU_NEXT_SIZE];
+	unsigned long	nxtcompleted[RCU_NEXT_SIZE];
+					/* grace periods for sublists. */
+	long		qlen_lazy;	/* # of lazy queued callbacks */
+	long		qlen;		/* # of queued callbacks, incl lazy */
+	long		qlen_last_fqs_check;
+					/* qlen at last check for QS forcing */
+	unsigned long	n_cbs_invoked;	/* count of RCU cbs invoked. */
+	unsigned long	n_nocbs_invoked; /* count of no-CBs RCU cbs invoked. */
+	unsigned long   n_cbs_orphaned; /* RCU cbs orphaned by dying CPU */
+	unsigned long   n_cbs_adopted;  /* RCU cbs adopted from dying CPU */
+	unsigned long	n_force_qs_snap;
+					/* did other CPU force QS recently? */
+	long		blimit;		/* Upper limit on a processed batch */
+
+	/* 3) dynticks interface. */
+	struct rcu_dynticks *dynticks;	/* Shared per-CPU dynticks state. */
+	int dynticks_snap;		/* Per-GP tracking for dynticks. */
+
+	/* 4) reasons this CPU needed to be kicked by force_quiescent_state */
+	unsigned long dynticks_fqs;	/* Kicked due to dynticks idle. */
+	unsigned long offline_fqs;	/* Kicked due to being offline. */
+
+	/* 5) __rcu_pending() statistics. */
+	unsigned long n_rcu_pending;	/* rcu_pending() calls since boot. */
+	unsigned long n_rp_qs_pending;
+	unsigned long n_rp_report_qs;
+	unsigned long n_rp_cb_ready;
+	unsigned long n_rp_cpu_needs_gp;
+	unsigned long n_rp_gp_completed;
+	unsigned long n_rp_gp_started;
+	unsigned long n_rp_need_nothing;
+
+	/* 6) _rcu_barrier() and OOM callbacks. */
+	struct rcu_head barrier_head;
+#ifdef CONFIG_RCU_FAST_NO_HZ
+	struct rcu_head oom_head;
+#endif /* #ifdef CONFIG_RCU_FAST_NO_HZ */
+
+	/* 7) Callback offloading. */
+#ifdef CONFIG_RCU_NOCB_CPU
+	struct rcu_head *nocb_head;	/* CBs waiting for kthread. */
+	struct rcu_head **nocb_tail;
+	atomic_long_t nocb_q_count;	/* # CBs waiting for kthread */
+	atomic_long_t nocb_q_count_lazy; /*  (approximate). */
+	int nocb_p_count;		/* # CBs being invoked by kthread */
+	int nocb_p_count_lazy;		/*  (approximate). */
+	wait_queue_head_t nocb_wq;	/* For nocb kthreads to sleep on. */
+	struct task_struct *nocb_kthread;
+#endif /* #ifdef CONFIG_RCU_NOCB_CPU */
+
+	/* 8) RCU CPU stall data. */
+#ifdef CONFIG_RCU_CPU_STALL_INFO
+	unsigned int softirq_snap;	/* Snapshot of softirq activity. */
+#endif /* #ifdef CONFIG_RCU_CPU_STALL_INFO */
+
+	int cpu;
+	struct rcu_state *rsp;
+};
+
+/* Values for fqs_state field in struct rcu_state. */
+#define RCU_GP_IDLE		0	/* No grace period in progress. */
+#define RCU_GP_INIT		1	/* Grace period being initialized. */
+#define RCU_SAVE_DYNTICK	2	/* Need to scan dyntick state. */
+#define RCU_FORCE_QS		3	/* Need to force quiescent state. */
+#define RCU_SIGNAL_INIT		RCU_SAVE_DYNTICK
+
+#define RCU_JIFFIES_TILL_FORCE_QS (1 + (HZ > 250) + (HZ > 500))
+					/* For jiffies_till_first_fqs and */
+					/*  and jiffies_till_next_fqs. */
+
+#define RCU_JIFFIES_FQS_DIV	256	/* Very large systems need more */
+					/*  delay between bouts of */
+					/*  quiescent-state forcing. */
+
+#define RCU_STALL_RAT_DELAY	2	/* Allow other CPUs time to take */
+					/*  at least one scheduling clock */
+					/*  irq before ratting on them. */
+
+#define rcu_wait(cond)							\
+do {									\
+	for (;;) {							\
+		set_current_state(TASK_INTERRUPTIBLE);			\
+		if (cond)						\
+			break;						\
+		schedule();						\
+	}								\
+	__set_current_state(TASK_RUNNING);				\
+} while (0)
+
+/*
+ * RCU global state, including node hierarchy.  This hierarchy is
+ * represented in "heap" form in a dense array.  The root (first level)
+ * of the hierarchy is in ->node[0] (referenced by ->level[0]), the second
+ * level in ->node[1] through ->node[m] (->node[1] referenced by ->level[1]),
+ * and the third level in ->node[m+1] and following (->node[m+1] referenced
+ * by ->level[2]).  The number of levels is determined by the number of
+ * CPUs and by CONFIG_RCU_FANOUT.  Small systems will have a "hierarchy"
+ * consisting of a single rcu_node.
+ */
+struct rcu_state {
+	struct rcu_node node[NUM_RCU_NODES];	/* Hierarchy. */
+	struct rcu_node *level[RCU_NUM_LVLS];	/* Hierarchy levels. */
+	u32 levelcnt[MAX_RCU_LVLS + 1];		/* # nodes in each level. */
+	u8 levelspread[RCU_NUM_LVLS];		/* kids/node in each level. */
+	struct rcu_data __percpu *rda;		/* pointer of percu rcu_data. */
+	void (*call)(struct rcu_head *head,	/* call_rcu() flavor. */
+		     void (*func)(struct rcu_head *head));
+
+	/* The following fields are guarded by the root rcu_node's lock. */
+
+	u8	fqs_state ____cacheline_internodealigned_in_smp;
+						/* Force QS state. */
+	u8	boost;				/* Subject to priority boost. */
+	unsigned long gpnum;			/* Current gp number. */
+	unsigned long completed;		/* # of last completed gp. */
+	struct task_struct *gp_kthread;		/* Task for grace periods. */
+	wait_queue_head_t gp_wq;		/* Where GP task waits. */
+	int gp_flags;				/* Commands for GP task. */
+
+	/* End of fields guarded by root rcu_node's lock. */
+
+	raw_spinlock_t orphan_lock ____cacheline_internodealigned_in_smp;
+						/* Protect following fields. */
+	struct rcu_head *orphan_nxtlist;	/* Orphaned callbacks that */
+						/*  need a grace period. */
+	struct rcu_head **orphan_nxttail;	/* Tail of above. */
+	struct rcu_head *orphan_donelist;	/* Orphaned callbacks that */
+						/*  are ready to invoke. */
+	struct rcu_head **orphan_donetail;	/* Tail of above. */
+	long qlen_lazy;				/* Number of lazy callbacks. */
+	long qlen;				/* Total number of callbacks. */
+	/* End of fields guarded by orphan_lock. */
+
+	struct mutex onoff_mutex;		/* Coordinate hotplug & GPs. */
+
+	struct mutex barrier_mutex;		/* Guards barrier fields. */
+	atomic_t barrier_cpu_count;		/* # CPUs waiting on. */
+	struct completion barrier_completion;	/* Wake at barrier end. */
+	unsigned long n_barrier_done;		/* ++ at start and end of */
+						/*  _rcu_barrier(). */
+	/* End of fields guarded by barrier_mutex. */
+
+	atomic_long_t expedited_start;		/* Starting ticket. */
+	atomic_long_t expedited_done;		/* Done ticket. */
+	atomic_long_t expedited_wrap;		/* # near-wrap incidents. */
+	atomic_long_t expedited_tryfail;	/* # acquisition failures. */
+	atomic_long_t expedited_workdone1;	/* # done by others #1. */
+	atomic_long_t expedited_workdone2;	/* # done by others #2. */
+	atomic_long_t expedited_normal;		/* # fallbacks to normal. */
+	atomic_long_t expedited_stoppedcpus;	/* # successful stop_cpus. */
+	atomic_long_t expedited_done_tries;	/* # tries to update _done. */
+	atomic_long_t expedited_done_lost;	/* # times beaten to _done. */
+	atomic_long_t expedited_done_exit;	/* # times exited _done loop. */
+
+	unsigned long jiffies_force_qs;		/* Time at which to invoke */
+						/*  force_quiescent_state(). */
+	unsigned long n_force_qs;		/* Number of calls to */
+						/*  force_quiescent_state(). */
+	unsigned long n_force_qs_lh;		/* ~Number of calls leaving */
+						/*  due to lock unavailable. */
+	unsigned long n_force_qs_ngp;		/* Number of calls leaving */
+						/*  due to no GP active. */
+	unsigned long gp_start;			/* Time at which GP started, */
+						/*  but in jiffies. */
+	unsigned long jiffies_stall;		/* Time at which to check */
+						/*  for CPU stalls. */
+	unsigned long gp_max;			/* Maximum GP duration in */
+						/*  jiffies. */
+	const char *name;			/* Name of structure. */
+	char abbr;				/* Abbreviated name. */
+	struct list_head flavors;		/* List of RCU flavors. */
+	struct irq_work wakeup_work;		/* Postponed wakeups */
+};
+
+/* Values for rcu_state structure's gp_flags field. */
+#define RCU_GP_FLAG_INIT 0x1	/* Need grace-period initialization. */
+#define RCU_GP_FLAG_FQS  0x2	/* Need grace-period quiescent-state forcing. */
+
+extern struct list_head rcu_struct_flavors;
+
+/* Sequence through rcu_state structures for each RCU flavor. */
+#define for_each_rcu_flavor(rsp) \
+	list_for_each_entry((rsp), &rcu_struct_flavors, flavors)
+
+/* Return values for rcu_preempt_offline_tasks(). */
+
+#define RCU_OFL_TASKS_NORM_GP	0x1		/* Tasks blocking normal */
+						/*  GP were moved to root. */
+#define RCU_OFL_TASKS_EXP_GP	0x2		/* Tasks blocking expedited */
+						/*  GP were moved to root. */
+
+/*
+ * RCU implementation internal declarations:
+ */
+extern struct rcu_state rcu_sched_state;
+DECLARE_PER_CPU(struct rcu_data, rcu_sched_data);
+
+extern struct rcu_state rcu_bh_state;
+DECLARE_PER_CPU(struct rcu_data, rcu_bh_data);
+
+#ifdef CONFIG_TREE_PREEMPT_RCU
+extern struct rcu_state rcu_preempt_state;
+DECLARE_PER_CPU(struct rcu_data, rcu_preempt_data);
+#endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */
+
+#ifdef CONFIG_RCU_BOOST
+DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_status);
+DECLARE_PER_CPU(int, rcu_cpu_kthread_cpu);
+DECLARE_PER_CPU(unsigned int, rcu_cpu_kthread_loops);
+DECLARE_PER_CPU(char, rcu_cpu_has_work);
+#endif /* #ifdef CONFIG_RCU_BOOST */
+
+#ifndef RCU_TREE_NONCORE
+
+/* Forward declarations for rcutree_plugin.h */
+static void rcu_bootup_announce(void);
+long rcu_batches_completed(void);
+static void rcu_preempt_note_context_switch(int cpu);
+static int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp);
+#ifdef CONFIG_HOTPLUG_CPU
+static void rcu_report_unblock_qs_rnp(struct rcu_node *rnp,
+				      unsigned long flags);
+#endif /* #ifdef CONFIG_HOTPLUG_CPU */
+static void rcu_print_detail_task_stall(struct rcu_state *rsp);
+static int rcu_print_task_stall(struct rcu_node *rnp);
+static void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp);
+#ifdef CONFIG_HOTPLUG_CPU
+static int rcu_preempt_offline_tasks(struct rcu_state *rsp,
+				     struct rcu_node *rnp,
+				     struct rcu_data *rdp);
+#endif /* #ifdef CONFIG_HOTPLUG_CPU */
+static void rcu_preempt_check_callbacks(int cpu);
+void call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu));
+#if defined(CONFIG_HOTPLUG_CPU) || defined(CONFIG_TREE_PREEMPT_RCU)
+static void rcu_report_exp_rnp(struct rcu_state *rsp, struct rcu_node *rnp,
+			       bool wake);
+#endif /* #if defined(CONFIG_HOTPLUG_CPU) || defined(CONFIG_TREE_PREEMPT_RCU) */
+static void __init __rcu_init_preempt(void);
+static void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags);
+static void rcu_preempt_boost_start_gp(struct rcu_node *rnp);
+static void invoke_rcu_callbacks_kthread(void);
+static bool rcu_is_callbacks_kthread(void);
+#ifdef CONFIG_RCU_BOOST
+static void rcu_preempt_do_callbacks(void);
+static int rcu_spawn_one_boost_kthread(struct rcu_state *rsp,
+						 struct rcu_node *rnp);
+#endif /* #ifdef CONFIG_RCU_BOOST */
+static void rcu_prepare_kthreads(int cpu);
+static void rcu_cleanup_after_idle(int cpu);
+static void rcu_prepare_for_idle(int cpu);
+static void rcu_idle_count_callbacks_posted(void);
+static void print_cpu_stall_info_begin(void);
+static void print_cpu_stall_info(struct rcu_state *rsp, int cpu);
+static void print_cpu_stall_info_end(void);
+static void zero_cpu_stall_ticks(struct rcu_data *rdp);
+static void increment_cpu_stall_ticks(void);
+static int rcu_nocb_needs_gp(struct rcu_state *rsp);
+static void rcu_nocb_gp_set(struct rcu_node *rnp, int nrq);
+static void rcu_nocb_gp_cleanup(struct rcu_state *rsp, struct rcu_node *rnp);
+static void rcu_init_one_nocb(struct rcu_node *rnp);
+static bool __call_rcu_nocb(struct rcu_data *rdp, struct rcu_head *rhp,
+			    bool lazy);
+static bool rcu_nocb_adopt_orphan_cbs(struct rcu_state *rsp,
+				      struct rcu_data *rdp);
+static void rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp);
+static void rcu_spawn_nocb_kthreads(struct rcu_state *rsp);
+static void rcu_kick_nohz_cpu(int cpu);
+static bool init_nocb_callback_list(struct rcu_data *rdp);
+static void rcu_sysidle_enter(struct rcu_dynticks *rdtp, int irq);
+static void rcu_sysidle_exit(struct rcu_dynticks *rdtp, int irq);
+static void rcu_sysidle_check_cpu(struct rcu_data *rdp, bool *isidle,
+				  unsigned long *maxj);
+static bool is_sysidle_rcu_state(struct rcu_state *rsp);
+static void rcu_sysidle_report_gp(struct rcu_state *rsp, int isidle,
+				  unsigned long maxj);
+static void rcu_bind_gp_kthread(void);
+static void rcu_sysidle_init_percpu_data(struct rcu_dynticks *rdtp);
+
+#endif /* #ifndef RCU_TREE_NONCORE */
+
+#ifdef CONFIG_RCU_TRACE
+#ifdef CONFIG_RCU_NOCB_CPU
+/* Sum up queue lengths for tracing. */
+static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
+{
+	*ql = atomic_long_read(&rdp->nocb_q_count) + rdp->nocb_p_count;
+	*qll = atomic_long_read(&rdp->nocb_q_count_lazy) + rdp->nocb_p_count_lazy;
+}
+#else /* #ifdef CONFIG_RCU_NOCB_CPU */
+static inline void rcu_nocb_q_lengths(struct rcu_data *rdp, long *ql, long *qll)
+{
+	*ql = 0;
+	*qll = 0;
+}
+#endif /* #else #ifdef CONFIG_RCU_NOCB_CPU */
+#endif /* #ifdef CONFIG_RCU_TRACE */
