commit 5cfd92e12e13432251981b9d0cd68dbd7aa8d690
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 20 16:59:14 2019 -0400

    locking/rwsem: Adaptive disabling of reader optimistic spinning
    
    Reader optimistic spinning is helpful when the reader critical section
    is short and there aren't that many readers around. It makes readers
    relatively more preferred than writers. When a writer times out spinning
    on a reader-owned lock and set the nospinnable bits, there are two main
    reasons for that.
    
     1) The reader critical section is long, perhaps the task sleeps after
        acquiring the read lock.
     2) There are just too many readers contending the lock causing it to
        take a while to service all of them.
    
    In the former case, long reader critical section will impede the progress
    of writers which is usually more important for system performance.
    In the later case, reader optimistic spinning tends to make the reader
    groups that contain readers that acquire the lock together smaller
    leading to more of them. That may hurt performance in some cases. In
    other words, the setting of nonspinnable bits indicates that reader
    optimistic spinning may not be helpful for those workloads that cause it.
    
    Therefore, any writers that have observed the setting of the writer
    nonspinnable bit for a given rwsem after they fail to acquire the lock
    via optimistic spinning will set the reader nonspinnable bit once they
    acquire the write lock. Similarly, readers that observe the setting
    of reader nonspinnable bit at slowpath entry will also set the reader
    nonspinnable bit when they acquire the read lock via the wakeup path.
    
    Once the reader nonspinnable bit is on, it will only be reset when
    a writer is able to acquire the rwsem in the fast path or somehow a
    reader or writer in the slowpath doesn't observe the nonspinable bit.
    
    This is to discourage reader optmistic spinning on that particular
    rwsem and make writers more preferred. This adaptive disabling of reader
    optimistic spinning will alleviate some of the negative side effect of
    this feature.
    
    In addition, this patch tries to make readers in the spinning queue
    follow the phase-fair principle after quitting optimistic spinning
    by checking if another reader has somehow acquired a read lock after
    this reader enters the optimistic spinning queue. If so and the rwsem
    is still reader-owned, this reader is in the right read-phase and can
    attempt to acquire the lock.
    
    On a 2-socket 40-core 80-thread Skylake system, the page_fault1 test of
    the will-it-scale benchmark was run with various number of threads. The
    number of operations done before reader optimistic spinning patches,
    this patch and after this patch were:
    
      Threads  Before rspin  Before patch  After patch    %change
      -------  ------------  ------------  -----------    -------
        20        5541068      5345484       5455667    -3.5%/ +2.1%
        40       10185150      7292313       9219276   -28.5%/+26.4%
        60        8196733      6460517       7181209   -21.2%/+11.2%
        80        9508864      6739559       8107025   -29.1%/+20.3%
    
    This patch doesn't recover all the lost performance, but it is more
    than half. Given the fact that reader optimistic spinning does benefit
    some workloads, this is a good compromise.
    
    Using the rwsem locking microbenchmark with very short critical section,
    this patch doesn't have too much impact on locking performance as shown
    by the locking rates (kops/s) below with equal numbers of readers and
    writers before and after this patch:
    
       # of Threads  Pre-patch    Post-patch
       ------------  ---------    ----------
            2          4,730        4,969
            4          4,814        4,786
            8          4,866        4,815
           16          4,715        4,511
           32          3,338        3,500
           64          3,212        3,389
           80          3,110        3,044
    
    When running the locking microbenchmark with 40 dedicated reader and writer
    threads, however, the reader performance is curtailed to favor the writer.
    
    Before patch:
    
      40 readers, Iterations Min/Mean/Max = 204,026/234,309/254,816
      40 writers, Iterations Min/Mean/Max = 88,515/95,884/115,644
    
    After patch:
    
      40 readers, Iterations Min/Mean/Max = 33,813/35,260/36,791
      40 writers, Iterations Min/Mean/Max = 95,368/96,565/97,798
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: huang ying <huang.ying.caritas@gmail.com>
    Link: https://lkml.kernel.org/r/20190520205918.22251-16-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/locking/lock_events_list.h b/kernel/locking/lock_events_list.h
index baa998401052..239039d0ce21 100644
--- a/kernel/locking/lock_events_list.h
+++ b/kernel/locking/lock_events_list.h
@@ -56,10 +56,12 @@ LOCK_EVENT(rwsem_sleep_reader)	/* # of reader sleeps			*/
 LOCK_EVENT(rwsem_sleep_writer)	/* # of writer sleeps			*/
 LOCK_EVENT(rwsem_wake_reader)	/* # of reader wakeups			*/
 LOCK_EVENT(rwsem_wake_writer)	/* # of writer wakeups			*/
-LOCK_EVENT(rwsem_opt_rlock)	/* # of read locks opt-spin acquired	*/
-LOCK_EVENT(rwsem_opt_wlock)	/* # of write locks opt-spin acquired	*/
-LOCK_EVENT(rwsem_opt_fail)	/* # of failed opt-spinnings		*/
-LOCK_EVENT(rwsem_opt_nospin)	/* # of disabled reader opt-spinnings	*/
+LOCK_EVENT(rwsem_opt_rlock)	/* # of opt-acquired read locks		*/
+LOCK_EVENT(rwsem_opt_wlock)	/* # of opt-acquired write locks	*/
+LOCK_EVENT(rwsem_opt_fail)	/* # of failed optspins			*/
+LOCK_EVENT(rwsem_opt_nospin)	/* # of disabled optspins		*/
+LOCK_EVENT(rwsem_opt_norspin)	/* # of disabled reader-only optspins	*/
+LOCK_EVENT(rwsem_opt_rlock2)	/* # of opt-acquired 2ndary read locks	*/
 LOCK_EVENT(rwsem_rlock)		/* # of read locks acquired		*/
 LOCK_EVENT(rwsem_rlock_fast)	/* # of fast read locks acquired	*/
 LOCK_EVENT(rwsem_rlock_fail)	/* # of failed read lock acquisitions	*/

commit 7d43f1ce9dd075d8b2aa3ad1f3970ef386a5c358
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 20 16:59:13 2019 -0400

    locking/rwsem: Enable time-based spinning on reader-owned rwsem
    
    When the rwsem is owned by reader, writers stop optimistic spinning
    simply because there is no easy way to figure out if all the readers
    are actively running or not. However, there are scenarios where
    the readers are unlikely to sleep and optimistic spinning can help
    performance.
    
    This patch provides a simple mechanism for spinning on a reader-owned
    rwsem by a writer. It is a time threshold based spinning where the
    allowable spinning time can vary from 10us to 25us depending on the
    condition of the rwsem.
    
    When the time threshold is exceeded, the nonspinnable bits will be set
    in the owner field to indicate that no more optimistic spinning will
    be allowed on this rwsem until it becomes writer owned again. Not even
    readers is allowed to acquire the reader-locked rwsem by optimistic
    spinning for fairness.
    
    We also want a writer to acquire the lock after the readers hold the
    lock for a relatively long time. In order to give preference to writers
    under such a circumstance, the single RWSEM_NONSPINNABLE bit is now split
    into two - one for reader and one for writer. When optimistic spinning
    is disabled, both bits will be set. When the reader count drop down
    to 0, the writer nonspinnable bit will be cleared to allow writers to
    spin on the lock, but not the readers. When a writer acquires the lock,
    it will write its own task structure pointer into sem->owner and clear
    the reader nonspinnable bit in the process.
    
    The time taken for each iteration of the reader-owned rwsem spinning
    loop varies. Below are sample minimum elapsed times for 16 iterations
    of the loop.
    
          System                 Time for 16 Iterations
          ------                 ----------------------
      1-socket Skylake                  ~800ns
      4-socket Broadwell                ~300ns
      2-socket ThunderX2 (arm64)        ~250ns
    
    When the lock cacheline is contended, we can see up to almost 10X
    increase in elapsed time.  So 25us will be at most 500, 1300 and 1600
    iterations for each of the above systems.
    
    With a locking microbenchmark running on 5.1 based kernel, the total
    locking rates (in kops/s) on a 8-socket IvyBridge-EX system with
    equal numbers of readers and writers before and after this patch were
    as follows:
    
       # of Threads  Pre-patch    Post-patch
       ------------  ---------    ----------
            2          1,759        6,684
            4          1,684        6,738
            8          1,074        7,222
           16            900        7,163
           32            458        7,316
           64            208          520
          128            168          425
          240            143          474
    
    This patch gives a big boost in performance for mixed reader/writer
    workloads.
    
    With 32 locking threads, the rwsem lock event data were:
    
    rwsem_opt_fail=79850
    rwsem_opt_nospin=5069
    rwsem_opt_rlock=597484
    rwsem_opt_wlock=957339
    rwsem_sleep_reader=57782
    rwsem_sleep_writer=55663
    
    With 64 locking threads, the data looked like:
    
    rwsem_opt_fail=346723
    rwsem_opt_nospin=6293
    rwsem_opt_rlock=1127119
    rwsem_opt_wlock=1400628
    rwsem_sleep_reader=308201
    rwsem_sleep_writer=72281
    
    So a lot more threads acquired the lock in the slowpath and more threads
    went to sleep.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: huang ying <huang.ying.caritas@gmail.com>
    Link: https://lkml.kernel.org/r/20190520205918.22251-15-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/locking/lock_events_list.h b/kernel/locking/lock_events_list.h
index ca954e4e00e4..baa998401052 100644
--- a/kernel/locking/lock_events_list.h
+++ b/kernel/locking/lock_events_list.h
@@ -59,6 +59,7 @@ LOCK_EVENT(rwsem_wake_writer)	/* # of writer wakeups			*/
 LOCK_EVENT(rwsem_opt_rlock)	/* # of read locks opt-spin acquired	*/
 LOCK_EVENT(rwsem_opt_wlock)	/* # of write locks opt-spin acquired	*/
 LOCK_EVENT(rwsem_opt_fail)	/* # of failed opt-spinnings		*/
+LOCK_EVENT(rwsem_opt_nospin)	/* # of disabled reader opt-spinnings	*/
 LOCK_EVENT(rwsem_rlock)		/* # of read locks acquired		*/
 LOCK_EVENT(rwsem_rlock_fast)	/* # of fast read locks acquired	*/
 LOCK_EVENT(rwsem_rlock_fail)	/* # of failed read lock acquisitions	*/

commit cf69482d62d996d3ce840eeead8e160de281ac6c
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 20 16:59:11 2019 -0400

    locking/rwsem: Enable readers spinning on writer
    
    This patch enables readers to optimistically spin on a
    rwsem when it is owned by a writer instead of going to sleep
    directly.  The rwsem_can_spin_on_owner() function is extracted
    out of rwsem_optimistic_spin() and is called directly by
    rwsem_down_read_slowpath() and rwsem_down_write_slowpath().
    
    With a locking microbenchmark running on 5.1 based kernel, the total
    locking rates (in kops/s) on a 8-socket IvyBrige-EX system with equal
    numbers of readers and writers before and after the patch were as
    follows:
    
       # of Threads  Pre-patch    Post-patch
       ------------  ---------    ----------
            4          1,674        1,684
            8          1,062        1,074
           16            924          900
           32            300          458
           64            195          208
          128            164          168
          240            149          143
    
    The performance change wasn't significant in this case, but this change
    is required by a follow-on patch.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: huang ying <huang.ying.caritas@gmail.com>
    Link: https://lkml.kernel.org/r/20190520205918.22251-13-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/locking/lock_events_list.h b/kernel/locking/lock_events_list.h
index 634b47fd8b5e..ca954e4e00e4 100644
--- a/kernel/locking/lock_events_list.h
+++ b/kernel/locking/lock_events_list.h
@@ -56,6 +56,7 @@ LOCK_EVENT(rwsem_sleep_reader)	/* # of reader sleeps			*/
 LOCK_EVENT(rwsem_sleep_writer)	/* # of writer sleeps			*/
 LOCK_EVENT(rwsem_wake_reader)	/* # of reader wakeups			*/
 LOCK_EVENT(rwsem_wake_writer)	/* # of writer wakeups			*/
+LOCK_EVENT(rwsem_opt_rlock)	/* # of read locks opt-spin acquired	*/
 LOCK_EVENT(rwsem_opt_wlock)	/* # of write locks opt-spin acquired	*/
 LOCK_EVENT(rwsem_opt_fail)	/* # of failed opt-spinnings		*/
 LOCK_EVENT(rwsem_rlock)		/* # of read locks acquired		*/

commit 4f23dbc1e657951e5d94c60369bc1db065961fb3
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 20 16:59:06 2019 -0400

    locking/rwsem: Implement lock handoff to prevent lock starvation
    
    Because of writer lock stealing, it is possible that a constant
    stream of incoming writers will cause a waiting writer or reader to
    wait indefinitely leading to lock starvation.
    
    This patch implements a lock handoff mechanism to disable lock stealing
    and force lock handoff to the first waiter or waiters (for readers)
    in the queue after at least a 4ms waiting period unless it is a RT
    writer task which doesn't need to wait. The waiting period is used to
    avoid discouraging lock stealing too much to affect performance.
    
    The setting and clearing of the handoff bit is serialized by the
    wait_lock. So racing is not possible.
    
    A rwsem microbenchmark was run for 5 seconds on a 2-socket 40-core
    80-thread Skylake system with a v5.1 based kernel and 240 write_lock
    threads with 5us sleep critical section.
    
    Before the patch, the min/mean/max numbers of locking operations for
    the locking threads were 1/7,792/173,696. After the patch, the figures
    became 5,842/6,542/7,458.  It can be seen that the rwsem became much
    more fair, though there was a drop of about 16% in the mean locking
    operations done which was a tradeoff of having better fairness.
    
    Making the waiter set the handoff bit right after the first wakeup can
    impact performance especially with a mixed reader/writer workload. With
    the same microbenchmark with short critical section and equal number of
    reader and writer threads (40/40), the reader/writer locking operation
    counts with the current patch were:
    
      40 readers, Iterations Min/Mean/Max = 1,793/1,794/1,796
      40 writers, Iterations Min/Mean/Max = 1,793/34,956/86,081
    
    By making waiter set handoff bit immediately after wakeup:
    
      40 readers, Iterations Min/Mean/Max = 43/44/46
      40 writers, Iterations Min/Mean/Max = 43/1,263/3,191
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: huang ying <huang.ying.caritas@gmail.com>
    Link: https://lkml.kernel.org/r/20190520205918.22251-8-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/locking/lock_events_list.h b/kernel/locking/lock_events_list.h
index 11187a1d40b8..634b47fd8b5e 100644
--- a/kernel/locking/lock_events_list.h
+++ b/kernel/locking/lock_events_list.h
@@ -61,5 +61,7 @@ LOCK_EVENT(rwsem_opt_fail)	/* # of failed opt-spinnings		*/
 LOCK_EVENT(rwsem_rlock)		/* # of read locks acquired		*/
 LOCK_EVENT(rwsem_rlock_fast)	/* # of fast read locks acquired	*/
 LOCK_EVENT(rwsem_rlock_fail)	/* # of failed read lock acquisitions	*/
+LOCK_EVENT(rwsem_rlock_handoff)	/* # of read lock handoffs		*/
 LOCK_EVENT(rwsem_wlock)		/* # of write locks acquired		*/
 LOCK_EVENT(rwsem_wlock_fail)	/* # of failed write lock acquisitions	*/
+LOCK_EVENT(rwsem_wlock_handoff)	/* # of write lock handoffs		*/

commit 6cef7ff6e43cbdb9fa8eb91eb9a6b25d45ae11e3
Author: Waiman Long <longman@redhat.com>
Date:   Mon May 20 16:59:04 2019 -0400

    locking/rwsem: Code cleanup after files merging
    
    After merging all the relevant rwsem code into one single file, there
    are a number of optimizations and cleanups that can be done:
    
     1) Remove all the EXPORT_SYMBOL() calls for functions that are not
        accessed elsewhere.
     2) Remove all the __visible tags as none of the functions will be
        called from assembly code anymore.
     3) Make all the internal functions static.
     4) Remove some unneeded blank lines.
     5) Remove the intermediate rwsem_down_{read|write}_failed*() functions
        and rename __rwsem_down_{read|write}_failed_common() to
        rwsem_down_{read|write}_slowpath().
     6) Remove "__" prefix of __rwsem_mark_wake().
     7) Use atomic_long_try_cmpxchg_acquire() as much as possible.
     8) Remove the rwsem_rtrylock and rwsem_wtrylock lock events as they
        are not that useful.
    
    That enables the compiler to do better optimization and reduce code
    size. The text+data size of rwsem.o on an x86-64 machine with gcc8 was
    reduced from 10237 bytes to 5030 bytes with this change.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Waiman Long <longman@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: huang ying <huang.ying.caritas@gmail.com>
    Link: https://lkml.kernel.org/r/20190520205918.22251-6-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/locking/lock_events_list.h b/kernel/locking/lock_events_list.h
index ad7668cfc9da..11187a1d40b8 100644
--- a/kernel/locking/lock_events_list.h
+++ b/kernel/locking/lock_events_list.h
@@ -61,7 +61,5 @@ LOCK_EVENT(rwsem_opt_fail)	/* # of failed opt-spinnings		*/
 LOCK_EVENT(rwsem_rlock)		/* # of read locks acquired		*/
 LOCK_EVENT(rwsem_rlock_fast)	/* # of fast read locks acquired	*/
 LOCK_EVENT(rwsem_rlock_fail)	/* # of failed read lock acquisitions	*/
-LOCK_EVENT(rwsem_rtrylock)	/* # of read trylock calls		*/
 LOCK_EVENT(rwsem_wlock)		/* # of write locks acquired		*/
 LOCK_EVENT(rwsem_wlock_fail)	/* # of failed write lock acquisitions	*/
-LOCK_EVENT(rwsem_wtrylock)	/* # of write trylock calls		*/

commit a8654596f0371c2604c4d475422c48f4fc6a56c9
Author: Waiman Long <longman@redhat.com>
Date:   Thu Apr 4 13:43:19 2019 -0400

    locking/rwsem: Enable lock event counting
    
    Add lock event counting calls so that we can track the number of lock
    events happening in the rwsem code.
    
    With CONFIG_LOCK_EVENT_COUNTS on and booting a 4-socket 112-thread x86-64
    system, the rwsem counts after system bootup were as follows:
    
      rwsem_opt_fail=261
      rwsem_opt_wlock=50636
      rwsem_rlock=445
      rwsem_rlock_fail=0
      rwsem_rlock_fast=22
      rwsem_rtrylock=810144
      rwsem_sleep_reader=441
      rwsem_sleep_writer=310
      rwsem_wake_reader=355
      rwsem_wake_writer=2335
      rwsem_wlock=261
      rwsem_wlock_fail=0
      rwsem_wtrylock=20583
    
    It can be seen that most of the lock acquisitions in the slowpath were
    write-locks in the optimistic spinning code path with no sleeping at
    all. For this system, over 97% of the locks are acquired via optimistic
    spinning. It illustrates the importance of optimistic spinning in
    improving the performance of rwsem.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/20190404174320.22416-11-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/locking/lock_events_list.h b/kernel/locking/lock_events_list.h
index 8b4d2e180475..ad7668cfc9da 100644
--- a/kernel/locking/lock_events_list.h
+++ b/kernel/locking/lock_events_list.h
@@ -48,3 +48,20 @@ LOCK_EVENT(lock_use_node3)	/* # of locking ops that use 3rd percpu node */
 LOCK_EVENT(lock_use_node4)	/* # of locking ops that use 4th percpu node */
 LOCK_EVENT(lock_no_node)	/* # of locking ops w/o using percpu node    */
 #endif /* CONFIG_QUEUED_SPINLOCKS */
+
+/*
+ * Locking events for rwsem
+ */
+LOCK_EVENT(rwsem_sleep_reader)	/* # of reader sleeps			*/
+LOCK_EVENT(rwsem_sleep_writer)	/* # of writer sleeps			*/
+LOCK_EVENT(rwsem_wake_reader)	/* # of reader wakeups			*/
+LOCK_EVENT(rwsem_wake_writer)	/* # of writer wakeups			*/
+LOCK_EVENT(rwsem_opt_wlock)	/* # of write locks opt-spin acquired	*/
+LOCK_EVENT(rwsem_opt_fail)	/* # of failed opt-spinnings		*/
+LOCK_EVENT(rwsem_rlock)		/* # of read locks acquired		*/
+LOCK_EVENT(rwsem_rlock_fast)	/* # of fast read locks acquired	*/
+LOCK_EVENT(rwsem_rlock_fail)	/* # of failed read lock acquisitions	*/
+LOCK_EVENT(rwsem_rtrylock)	/* # of read trylock calls		*/
+LOCK_EVENT(rwsem_wlock)		/* # of write locks acquired		*/
+LOCK_EVENT(rwsem_wlock_fail)	/* # of failed write lock acquisitions	*/
+LOCK_EVENT(rwsem_wtrylock)	/* # of write trylock calls		*/

commit ad53fa10fa9e816067bbae7109845940f5e6df50
Author: Waiman Long <longman@redhat.com>
Date:   Thu Apr 4 13:43:16 2019 -0400

    locking/qspinlock_stat: Introduce generic lockevent_*() counting APIs
    
    The percpu event counts used by qspinlock code can be useful for
    other locking code as well. So a new set of lockevent_* counting APIs
    is introduced with the lock event names extracted out into the new
    lock_events_list.h header file for easier addition in the future.
    
    The existing qstat_inc() calls are replaced by either lockevent_inc() or
    lockevent_cond_inc() calls.
    
    The qstat_hop() call is renamed to lockevent_pv_hop(). The "reset_counters"
    debugfs file is also renamed to ".reset_counts".
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/20190404174320.22416-8-longman@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/locking/lock_events_list.h b/kernel/locking/lock_events_list.h
new file mode 100644
index 000000000000..8b4d2e180475
--- /dev/null
+++ b/kernel/locking/lock_events_list.h
@@ -0,0 +1,50 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Authors: Waiman Long <longman@redhat.com>
+ */
+
+#ifndef LOCK_EVENT
+#define LOCK_EVENT(name)	LOCKEVENT_ ## name,
+#endif
+
+#ifdef CONFIG_QUEUED_SPINLOCKS
+#ifdef CONFIG_PARAVIRT_SPINLOCKS
+/*
+ * Locking events for PV qspinlock.
+ */
+LOCK_EVENT(pv_hash_hops)	/* Average # of hops per hashing operation */
+LOCK_EVENT(pv_kick_unlock)	/* # of vCPU kicks issued at unlock time   */
+LOCK_EVENT(pv_kick_wake)	/* # of vCPU kicks for pv_latency_wake	   */
+LOCK_EVENT(pv_latency_kick)	/* Average latency (ns) of vCPU kick	   */
+LOCK_EVENT(pv_latency_wake)	/* Average latency (ns) of kick-to-wakeup  */
+LOCK_EVENT(pv_lock_stealing)	/* # of lock stealing operations	   */
+LOCK_EVENT(pv_spurious_wakeup)	/* # of spurious wakeups in non-head vCPUs */
+LOCK_EVENT(pv_wait_again)	/* # of wait's after queue head vCPU kick  */
+LOCK_EVENT(pv_wait_early)	/* # of early vCPU wait's		   */
+LOCK_EVENT(pv_wait_head)	/* # of vCPU wait's at the queue head	   */
+LOCK_EVENT(pv_wait_node)	/* # of vCPU wait's at non-head queue node */
+#endif /* CONFIG_PARAVIRT_SPINLOCKS */
+
+/*
+ * Locking events for qspinlock
+ *
+ * Subtracting lock_use_node[234] from lock_slowpath will give you
+ * lock_use_node1.
+ */
+LOCK_EVENT(lock_pending)	/* # of locking ops via pending code	     */
+LOCK_EVENT(lock_slowpath)	/* # of locking ops via MCS lock queue	     */
+LOCK_EVENT(lock_use_node2)	/* # of locking ops that use 2nd percpu node */
+LOCK_EVENT(lock_use_node3)	/* # of locking ops that use 3rd percpu node */
+LOCK_EVENT(lock_use_node4)	/* # of locking ops that use 4th percpu node */
+LOCK_EVENT(lock_no_node)	/* # of locking ops w/o using percpu node    */
+#endif /* CONFIG_QUEUED_SPINLOCKS */
