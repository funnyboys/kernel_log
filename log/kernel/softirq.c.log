commit 59bc300b712998d10254ee20e24f2e7ec09c560a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 29 23:27:39 2020 +0200

    x86/entry: Clarify irq_{enter,exit}_rcu()
    
    Because:
    
      irq_enter_rcu() includes lockdep_hardirq_enter()
      irq_exit_rcu() does *NOT* include lockdep_hardirq_exit()
    
    Which resulted in two 'stray' lockdep_hardirq_exit() calls in
    idtentry.h, and me spending a long time trying to find the matching
    enter calls.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20200529213321.359433429@infradead.org

diff --git a/kernel/softirq.c b/kernel/softirq.c
index beb8e3a66c7c..c4201b7f42b1 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -404,12 +404,7 @@ static inline void tick_irq_exit(void)
 #endif
 }
 
-/**
- * irq_exit_rcu() - Exit an interrupt context without updating RCU
- *
- * Also processes softirqs if needed and possible.
- */
-void irq_exit_rcu(void)
+static inline void __irq_exit_rcu(void)
 {
 #ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED
 	local_irq_disable();
@@ -424,6 +419,18 @@ void irq_exit_rcu(void)
 	tick_irq_exit();
 }
 
+/**
+ * irq_exit_rcu() - Exit an interrupt context without updating RCU
+ *
+ * Also processes softirqs if needed and possible.
+ */
+void irq_exit_rcu(void)
+{
+	__irq_exit_rcu();
+	 /* must be last! */
+	lockdep_hardirq_exit();
+}
+
 /**
  * irq_exit - Exit an interrupt context, update RCU and lockdep
  *
@@ -431,7 +438,7 @@ void irq_exit_rcu(void)
  */
 void irq_exit(void)
 {
-	irq_exit_rcu();
+	__irq_exit_rcu();
 	rcu_irq_exit();
 	 /* must be last! */
 	lockdep_hardirq_exit();

commit 8a6bc4787f05d087fda8e11ead225c8830250703
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 21 22:05:21 2020 +0200

    genirq: Provide irq_enter/exit_rcu()
    
    irq_enter()/exit() currently include RCU handling. To properly separate the RCU
    handling code, provide variants which contain only the non-RCU related
    functionality.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lore.kernel.org/r/20200521202117.567023613@linutronix.de

diff --git a/kernel/softirq.c b/kernel/softirq.c
index a47c6dd57452..beb8e3a66c7c 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -339,12 +339,11 @@ asmlinkage __visible void do_softirq(void)
 	local_irq_restore(flags);
 }
 
-/*
- * Enter an interrupt context.
+/**
+ * irq_enter_rcu - Enter an interrupt context with RCU watching
  */
-void irq_enter(void)
+void irq_enter_rcu(void)
 {
-	rcu_irq_enter();
 	if (is_idle_task(current) && !in_interrupt()) {
 		/*
 		 * Prevent raise_softirq from needlessly waking up ksoftirqd
@@ -354,10 +353,18 @@ void irq_enter(void)
 		tick_irq_enter();
 		_local_bh_enable();
 	}
-
 	__irq_enter();
 }
 
+/**
+ * irq_enter - Enter an interrupt context including RCU update
+ */
+void irq_enter(void)
+{
+	rcu_irq_enter();
+	irq_enter_rcu();
+}
+
 static inline void invoke_softirq(void)
 {
 	if (ksoftirqd_running(local_softirq_pending()))
@@ -397,10 +404,12 @@ static inline void tick_irq_exit(void)
 #endif
 }
 
-/*
- * Exit an interrupt context. Process softirqs if needed and possible:
+/**
+ * irq_exit_rcu() - Exit an interrupt context without updating RCU
+ *
+ * Also processes softirqs if needed and possible.
  */
-void irq_exit(void)
+void irq_exit_rcu(void)
 {
 #ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED
 	local_irq_disable();
@@ -413,6 +422,16 @@ void irq_exit(void)
 		invoke_softirq();
 
 	tick_irq_exit();
+}
+
+/**
+ * irq_exit - Exit an interrupt context, update RCU and lockdep
+ *
+ * Also processes softirqs if needed and possible.
+ */
+void irq_exit(void)
+{
+	irq_exit_rcu();
 	rcu_irq_exit();
 	 /* must be last! */
 	lockdep_hardirq_exit();

commit ef996916e78e03d25e56c2d372e5e21fdb471882
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 20 12:56:42 2020 +0100

    lockdep: Rename trace_{hard,soft}{irq_context,irqs_enabled}()
    
    Continue what commit:
    
      d820ac4c2fa8 ("locking: rename trace_softirq_[enter|exit] => lockdep_softirq_[enter|exit]")
    
    started, rename these to avoid confusing them with tracepoints.
    
    git grep -l "trace_\(soft\|hard\)\(irq_context\|irqs_enabled\)" | while read file;
    do
            sed -ie 's/trace_\(soft\|hard\)\(irq_context\|irqs_enabled\)/lockdep_\1\2/g' $file;
    done
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20200320115859.178626842@infradead.org

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 0112ca01694b..a47c6dd57452 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -224,7 +224,7 @@ static inline bool lockdep_softirq_start(void)
 {
 	bool in_hardirq = false;
 
-	if (trace_hardirq_context(current)) {
+	if (lockdep_hardirq_context(current)) {
 		in_hardirq = true;
 		lockdep_hardirq_exit();
 	}

commit 0d38453c85b426e47375346812d2271680c47988
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 20 12:56:41 2020 +0100

    lockdep: Rename trace_softirqs_{on,off}()
    
    Continue what commit:
    
      d820ac4c2fa8 ("locking: rename trace_softirq_[enter|exit] => lockdep_softirq_[enter|exit]")
    
    started, rename these to avoid confusing them with tracepoints.
    
    git grep -l "trace_softirqs_\(on\|off\)" | while read file;
    do
            sed -ie 's/trace_softirqs_\(on\|off\)/lockdep_softirqs_\1/g' $file;
    done
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20200320115859.119434738@infradead.org

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b3286892abac..0112ca01694b 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -126,7 +126,7 @@ void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
 	 * Were softirqs turned off above:
 	 */
 	if (softirq_count() == (cnt & SOFTIRQ_MASK))
-		trace_softirqs_off(ip);
+		lockdep_softirqs_off(ip);
 	raw_local_irq_restore(flags);
 
 	if (preempt_count() == cnt) {
@@ -147,7 +147,7 @@ static void __local_bh_enable(unsigned int cnt)
 		trace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());
 
 	if (softirq_count() == (cnt & SOFTIRQ_MASK))
-		trace_softirqs_on(_RET_IP_);
+		lockdep_softirqs_on(_RET_IP_);
 
 	__preempt_count_sub(cnt);
 }
@@ -174,7 +174,7 @@ void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
 	 * Are softirqs going to be turned on now:
 	 */
 	if (softirq_count() == SOFTIRQ_DISABLE_OFFSET)
-		trace_softirqs_on(ip);
+		lockdep_softirqs_on(ip);
 	/*
 	 * Keep preemption disabled until we are done with
 	 * softirq processing:

commit 2502ec37a7b228b34c1e2e89480f98b92f53046a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 20 12:56:40 2020 +0100

    lockdep: Rename trace_hardirq_{enter,exit}()
    
    Continue what commit:
    
      d820ac4c2fa8 ("locking: rename trace_softirq_[enter|exit] => lockdep_softirq_[enter|exit]")
    
    started, rename these to avoid confusing them with tracepoints.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20200320115859.060481361@infradead.org

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 0427a86743a4..b3286892abac 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -226,7 +226,7 @@ static inline bool lockdep_softirq_start(void)
 
 	if (trace_hardirq_context(current)) {
 		in_hardirq = true;
-		trace_hardirq_exit();
+		lockdep_hardirq_exit();
 	}
 
 	lockdep_softirq_enter();
@@ -239,7 +239,7 @@ static inline void lockdep_softirq_end(bool in_hardirq)
 	lockdep_softirq_exit();
 
 	if (in_hardirq)
-		trace_hardirq_enter();
+		lockdep_hardirq_enter();
 }
 #else
 static inline bool lockdep_softirq_start(void) { return false; }
@@ -414,7 +414,8 @@ void irq_exit(void)
 
 	tick_irq_exit();
 	rcu_irq_exit();
-	trace_hardirq_exit(); /* must be last! */
+	 /* must be last! */
+	lockdep_hardirq_exit();
 }
 
 /*

commit 2a1ccd31420a7b1acd6ca37b2bec2d723aa093e4
Merge: e0e86b111bca 3a1d24ca9573
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 11:01:13 2019 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq updates from Thomas Gleixner:
     "The irq departement provides the usual mixed bag:
    
      Core:
    
       - Further improvements to the irq timings code which aims to predict
         the next interrupt for power state selection to achieve better
         latency/power balance
    
       - Add interrupt statistics to the core NMI handlers
    
       - The usual small fixes and cleanups
    
      Drivers:
    
       - Support for Renesas RZ/A1, Annapurna Labs FIC, Meson-G12A SoC and
         Amazon Gravition AMR/GIC interrupt controllers.
    
       - Rework of the Renesas INTC controller driver
    
       - ACPI support for Socionext SoCs
    
       - Enhancements to the CSKY interrupt controller
    
       - The usual small fixes and cleanups"
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (39 commits)
      irq/irqdomain: Fix comment typo
      genirq: Update irq stats from NMI handlers
      irqchip/gic-pm: Remove PM_CLK dependency
      irqchip/al-fic: Introduce Amazon's Annapurna Labs Fabric Interrupt Controller Driver
      dt-bindings: interrupt-controller: Add Amazon's Annapurna Labs FIC
      softirq: Use __this_cpu_write() in takeover_tasklets()
      irqchip/mbigen: Stop printing kernel addresses
      irqchip/gic: Add dependency for ARM_GIC_MAX_NR
      genirq/affinity: Remove unused argument from [__]irq_build_affinity_masks()
      genirq/timings: Add selftest for next event computation
      genirq/timings: Add selftest for irqs circular buffer
      genirq/timings: Add selftest for circular array
      genirq/timings: Encapsulate storing function
      genirq/timings: Encapsulate timings push
      genirq/timings: Optimize the period detection speed
      genirq/timings: Fix timings buffer inspection
      genirq/timings: Fix next event index function
      irqchip/qcom: Use struct_size() in devm_kzalloc()
      irqchip/irq-csky-mpintc: Remove unnecessary loop in interrupt handler
      dt-bindings: interrupt-controller: Update csky mpintc
      ...

commit 8afecaa68df1e94a9d634f1f961533a925f239fc
Author: Muchun Song <smuchun@gmail.com>
Date:   Tue Jun 18 22:33:05 2019 +0800

    softirq: Use __this_cpu_write() in takeover_tasklets()
    
    The code is executed with interrupts disabled, so it's safe to use
    __this_cpu_write().
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Muchun Song <smuchun@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: joel@joelfernandes.org
    Cc: rostedt@goodmis.org
    Cc: frederic@kernel.org
    Cc: paulmck@linux.vnet.ibm.com
    Cc: alexander.levin@verizon.com
    Cc: peterz@infradead.org
    Link: https://lkml.kernel.org/r/20190618143305.2038-1-smuchun@gmail.com

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 2c3382378d94..eaf3bdf7c749 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -650,7 +650,7 @@ static int takeover_tasklets(unsigned int cpu)
 	/* Find end, append list for that CPU. */
 	if (&per_cpu(tasklet_vec, cpu).head != per_cpu(tasklet_vec, cpu).tail) {
 		*__this_cpu_read(tasklet_vec.tail) = per_cpu(tasklet_vec, cpu).head;
-		this_cpu_write(tasklet_vec.tail, per_cpu(tasklet_vec, cpu).tail);
+		__this_cpu_write(tasklet_vec.tail, per_cpu(tasklet_vec, cpu).tail);
 		per_cpu(tasklet_vec, cpu).head = NULL;
 		per_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;
 	}

commit 767a67b0b35520348dc3b28dcba06454b0f9023d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jun 1 10:08:44 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 430
    
    Based on 1 normalized pattern(s):
    
      distribute under gplv2
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 8 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190531190114.475576622@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 2c3382378d94..a6b81c6b6bff 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -1,10 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *	linux/kernel/softirq.c
  *
  *	Copyright (C) 1992 Linus Torvalds
  *
- *	Distribute under GPLv2.
- *
  *	Rewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)
  */
 

commit d7dcf26ff0ffd7b56fe2b09ed7f1867589f3cdf1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 1 23:48:21 2019 +0100

    softirq: Remove tasklet_hrtimer
    
    There are no more users of this interface.
    Remove it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: netdev@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190301224821.29843-4-bigeasy@linutronix.de

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 10277429ed84..2c3382378d94 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -573,57 +573,6 @@ void tasklet_kill(struct tasklet_struct *t)
 }
 EXPORT_SYMBOL(tasklet_kill);
 
-/*
- * tasklet_hrtimer
- */
-
-/*
- * The trampoline is called when the hrtimer expires. It schedules a tasklet
- * to run __tasklet_hrtimer_trampoline() which in turn will call the intended
- * hrtimer callback, but from softirq context.
- */
-static enum hrtimer_restart __hrtimer_tasklet_trampoline(struct hrtimer *timer)
-{
-	struct tasklet_hrtimer *ttimer =
-		container_of(timer, struct tasklet_hrtimer, timer);
-
-	tasklet_hi_schedule(&ttimer->tasklet);
-	return HRTIMER_NORESTART;
-}
-
-/*
- * Helper function which calls the hrtimer callback from
- * tasklet/softirq context
- */
-static void __tasklet_hrtimer_trampoline(unsigned long data)
-{
-	struct tasklet_hrtimer *ttimer = (void *)data;
-	enum hrtimer_restart restart;
-
-	restart = ttimer->function(&ttimer->timer);
-	if (restart != HRTIMER_NORESTART)
-		hrtimer_restart(&ttimer->timer);
-}
-
-/**
- * tasklet_hrtimer_init - Init a tasklet/hrtimer combo for softirq callbacks
- * @ttimer:	 tasklet_hrtimer which is initialized
- * @function:	 hrtimer callback function which gets called from softirq context
- * @which_clock: clock id (CLOCK_MONOTONIC/CLOCK_REALTIME)
- * @mode:	 hrtimer mode (HRTIMER_MODE_ABS/HRTIMER_MODE_REL)
- */
-void tasklet_hrtimer_init(struct tasklet_hrtimer *ttimer,
-			  enum hrtimer_restart (*function)(struct hrtimer *),
-			  clockid_t which_clock, enum hrtimer_mode mode)
-{
-	hrtimer_init(&ttimer->timer, which_clock, mode);
-	ttimer->timer.function = __hrtimer_tasklet_trampoline;
-	tasklet_init(&ttimer->tasklet, __tasklet_hrtimer_trampoline,
-		     (unsigned long)ttimer);
-	ttimer->function = function;
-}
-EXPORT_SYMBOL_GPL(tasklet_hrtimer_init);
-
 void __init softirq_init(void)
 {
 	int cpu;

commit 1342d8080f6183b0419a9246c6e6545e21fa1e05
Author: Matthias Kaehlcke <mka@chromium.org>
Date:   Mon Jan 28 15:46:25 2019 -0800

    softirq: Don't skip softirq execution when softirq thread is parking
    
    When a CPU is unplugged the kernel threads of this CPU are parked (see
    smpboot_park_threads()). kthread_park() is used to mark each thread as
    parked and wake it up, so it can complete the process of parking itselfs
    (see smpboot_thread_fn()).
    
    If local softirqs are pending on interrupt exit invoke_softirq() is called
    to process the softirqs, however it skips processing when the softirq
    kernel thread of the local CPU is scheduled to run. The softirq kthread is
    one of the threads that is parked when a CPU is unplugged. Parking the
    kthread wakes it up, however only to complete the parking process, not to
    process the pending softirqs. Hence processing of softirqs at the end of an
    interrupt is skipped, but not done elsewhere, which can result in warnings
    about pending softirqs when a CPU is unplugged:
    
    /sys/devices/system/cpu # echo 0 > cpu4/online
    [ ... ] NOHZ: local_softirq_pending 02
    [ ... ] NOHZ: local_softirq_pending 202
    [ ... ] CPU4: shutdown
    [ ... ] psci: CPU4 killed.
    
    Don't skip processing of softirqs at the end of an interrupt when the
    softirq thread of the CPU is parking.
    
    Signed-off-by: Matthias Kaehlcke <mka@chromium.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Douglas Anderson <dianders@chromium.org>
    Cc: Stephen Boyd <swboyd@chromium.org>
    Link: https://lkml.kernel.org/r/20190128234625.78241-3-mka@chromium.org

diff --git a/kernel/softirq.c b/kernel/softirq.c
index d28813306b2c..10277429ed84 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -89,7 +89,8 @@ static bool ksoftirqd_running(unsigned long pending)
 
 	if (pending & SOFTIRQ_NOW_MASK)
 		return false;
-	return tsk && (tsk->state == TASK_RUNNING);
+	return tsk && (tsk->state == TASK_RUNNING) &&
+		!__kthread_should_park(tsk);
 }
 
 /*

commit 5947a64a7e0c70cc16d5d1e5af3cf3b44535047a
Merge: 4dcb9239dad6 746a923b863a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 25 11:43:47 2018 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq updates from Thomas Gleixner:
     "The interrupt brigade came up with the following updates:
    
       - Driver for the Marvell System Error Interrupt machinery
    
       - Overhaul of the GIC-V3 ITS driver
    
       - Small updates and fixes all over the place"
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (31 commits)
      genirq: Fix race on spurious interrupt detection
      softirq: Fix typo in __do_softirq() comments
      genirq: Fix grammar s/an /a /
      irqchip/gic: Unify GIC priority definitions
      irqchip/gic-v3: Remove acknowledge loop
      dt-bindings/interrupt-controller: Add documentation for Marvell SEI controller
      dt-bindings/interrupt-controller: Update Marvell ICU bindings
      irqchip/irq-mvebu-icu: Add support for System Error Interrupts (SEI)
      arm64: marvell: Enable SEI driver
      irqchip/irq-mvebu-sei: Add new driver for Marvell SEI
      irqchip/irq-mvebu-icu: Support ICU subnodes
      irqchip/irq-mvebu-icu: Disociate ICU and NSR
      irqchip/irq-mvebu-icu: Clarify the reset operation of configured interrupts
      irqchip/irq-mvebu-icu: Fix wrong private data retrieval
      dt-bindings/interrupt-controller: Fix Marvell ICU length in the example
      genirq/msi: Allow creation of a tree-based irqdomain for platform-msi
      dt-bindings: irqchip: renesas-irqc: Document r8a7744 support
      dt-bindings: irqchip: renesas-irqc: Document R-Car E3 support
      irqchip/pdc: Setup all edge interrupts as rising edge at GIC
      irqchip/gic-v3-its: Allow use of LPI tables in reserved memory
      ...

commit e45506ac0af9b56b221863e9649fe122d8bb42ff
Author: Yangtao Li <tiny.windzz@gmail.com>
Date:   Thu Oct 18 10:21:33 2018 -0400

    softirq: Fix typo in __do_softirq() comments
    
    s/s/as
    
    [ mingo: Also add a missing 'the', add proper punctuation and clarify what 'swap' means here. ]
    
    Signed-off-by: Yangtao Li <tiny.windzz@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: alexander.levin@verizon.com
    Cc: frederic@kernel.org
    Cc: joel@joelfernandes.org
    Cc: paulmck@linux.vnet.ibm.com
    Cc: rostedt@goodmis.org
    Link: http://lkml.kernel.org/r/20181018142133.12341-1-tiny.windzz@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 6f584861d329..9526895fe4ac 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -257,9 +257,9 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 	int softirq_bit;
 
 	/*
-	 * Mask out PF_MEMALLOC s current task context is borrowed for the
-	 * softirq. A softirq handled such as network RX might set PF_MEMALLOC
-	 * again if the socket is related to swap
+	 * Mask out PF_MEMALLOC as the current task context is borrowed for the
+	 * softirq. A softirq handled, such as network RX, might set PF_MEMALLOC
+	 * again if the socket is related to swapping.
 	 */
 	current->flags &= ~PF_MEMALLOC;
 

commit 65cfe3583b612a22e12fba9a7bbd2d37ca5ad941
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Jul 1 07:40:52 2018 -0700

    rcu: Define RCU-bh update API in terms of RCU
    
    Now that the main RCU API knows about softirq disabling and softirq's
    quiescent states, the RCU-bh update code can be dispensed with.
    This commit therefore removes the RCU-bh update-side implementation and
    defines RCU-bh's update-side API in terms of that of either RCU-preempt or
    RCU-sched, depending on the setting of the CONFIG_PREEMPT Kconfig option.
    
    In kernels built with CONFIG_RCU_NOCB_CPU=y this has the knock-on effect
    of reducing by one the number of rcuo kthreads per CPU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index ebd69694144a..7a0720a20003 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -301,7 +301,6 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 		pending >>= softirq_bit;
 	}
 
-	rcu_bh_qs();
 	if (__this_cpu_read(ksoftirqd) == current)
 		rcu_softirq_qs();
 	local_irq_disable();

commit d28139c4e96713d52a300fb9036c5be2f45e0741
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 28 14:45:25 2018 -0700

    rcu: Apply RCU-bh QSes to RCU-sched and RCU-preempt when safe
    
    One necessary step towards consolidating the three flavors of RCU is to
    make sure that the resulting consolidated "one flavor to rule them all"
    correctly handles networking denial-of-service attacks.  One thing that
    allows RCU-bh to do so is that __do_softirq() invokes rcu_bh_qs() every
    so often, and so something similar has to happen for consolidated RCU.
    
    This must be done carefully.  For example, if a preemption-disabled
    region of code takes an interrupt which does softirq processing before
    returning, consolidated RCU must ignore the resulting rcu_bh_qs()
    invocations -- preemption is still disabled, and that means an RCU
    reader for the consolidated flavor.
    
    This commit therefore creates a new rcu_softirq_qs() that is called only
    from the ksoftirqd task, thus avoiding the interrupted-a-preempted-region
    problem.  This new rcu_softirq_qs() function invokes rcu_sched_qs(),
    rcu_preempt_qs(), and rcu_preempt_deferred_qs().  The latter call handles
    any deferred quiescent states.
    
    Note that __do_softirq() still invokes rcu_bh_qs().  It will continue to
    do so until a later stage of cleanup when the RCU-bh flavor is removed.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Fix !SMP issue located by kbuild test robot. ]

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 6f584861d329..ebd69694144a 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -302,6 +302,8 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 	}
 
 	rcu_bh_qs();
+	if (__this_cpu_read(ksoftirqd) == current)
+		rcu_softirq_qs();
 	local_irq_disable();
 
 	pending = local_softirq_pending();

commit 0a0e0829f990120cef165bbb804237f400953ec2
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Fri Aug 3 15:31:34 2018 +0200

    nohz: Fix missing tick reprogram when interrupting an inline softirq
    
    The full nohz tick is reprogrammed in irq_exit() only if the exit is not in
    a nesting interrupt. This stands as an optimization: whether a hardirq or a
    softirq is interrupted, the tick is going to be reprogrammed when necessary
    at the end of the inner interrupt, with even potential new updates on the
    timer queue.
    
    When soft interrupts are interrupted, it's assumed that they are executing
    on the tail of an interrupt return. In that case tick_nohz_irq_exit() is
    called after softirq processing to take care of the tick reprogramming.
    
    But the assumption is wrong: softirqs can be processed inline as well, ie:
    outside of an interrupt, like in a call to local_bh_enable() or from
    ksoftirqd.
    
    Inline softirqs don't reprogram the tick once they are done, as opposed to
    interrupt tail softirq processing. So if a tick interrupts an inline
    softirq processing, the next timer will neither be reprogrammed from the
    interrupting tick's irq_exit() nor after the interrupted softirq
    processing. This situation may leave the tick unprogrammed while timers are
    armed.
    
    To fix this, simply keep reprogramming the tick even if a softirq has been
    interrupted. That can be optimized further, but for now correctness is more
    important.
    
    Note that new timers enqueued in nohz_full mode after a softirq gets
    interrupted will still be handled just fine through self-IPIs triggered by
    the timer code.
    
    Reported-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: stable@vger.kernel.org # 4.14+
    Link: https://lkml.kernel.org/r/1533303094-15855-1-git-send-email-frederic@kernel.org

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 75ffc1d1a2e0..6f584861d329 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -390,7 +390,7 @@ static inline void tick_irq_exit(void)
 
 	/* Make sure that timer wheel updates are propagated */
 	if ((idle_cpu(cpu) && !need_resched()) || tick_nohz_full_cpu(cpu)) {
-		if (!in_interrupt())
+		if (!in_irq())
 			tick_nohz_irq_exit();
 	}
 #endif

commit 3c53776e29f81719efcf8f7a6e30cdf753bee94d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 8 11:51:04 2018 -0800

    Mark HI and TASKLET softirq synchronous
    
    Way back in 4.9, we committed 4cd13c21b207 ("softirq: Let ksoftirqd do
    its job"), and ever since we've had small nagging issues with it.  For
    example, we've had:
    
      1ff688209e2e ("watchdog: core: make sure the watchdog_worker is not deferred")
      8d5755b3f77b ("watchdog: softdog: fire watchdog even if softirqs do not get to run")
      217f69743681 ("net: busy-poll: allow preemption in sk_busy_loop()")
    
    all of which worked around some of the effects of that commit.
    
    The DVB people have also complained that the commit causes excessive USB
    URB latencies, which seems to be due to the USB code using tasklets to
    schedule USB traffic.  This seems to be an issue mainly when already
    living on the edge, but waiting for ksoftirqd to handle it really does
    seem to cause excessive latencies.
    
    Now Hanna Hawa reports that this issue isn't just limited to USB URB and
    DVB, but also causes timeout problems for the Marvell SoC team:
    
     "I'm facing kernel panic issue while running raid 5 on sata disks
      connected to Macchiatobin (Marvell community board with Armada-8040
      SoC with 4 ARMv8 cores of CA72) Raid 5 built with Marvell DMA engine
      and async_tx mechanism (ASYNC_TX_DMA [=y]); the DMA driver (mv_xor_v2)
      uses a tasklet to clean the done descriptors from the queue"
    
    The latency problem causes a panic:
    
      mv_xor_v2 f0400000.xor: dma_sync_wait: timeout!
      Kernel panic - not syncing: async_tx_quiesce: DMA error waiting for transaction
    
    We've discussed simply just reverting the original commit entirely, and
    also much more involved solutions (with per-softirq threads etc).  This
    patch is intentionally stupid and fairly limited, because the issue
    still remains, and the other solutions either got sidetracked or had
    other issues.
    
    We should probably also consider the timer softirqs to be synchronous
    and not be delayed to ksoftirqd (since they were the issue with the
    earlier watchdog problems), but that should be done as a separate patch.
    This does only the tasklet cases.
    
    Reported-and-tested-by: Hanna Hawa <hannah@marvell.com>
    Reported-and-tested-by: Josef Griebichler <griebichler.josef@gmx.at>
    Reported-by: Mauro Carvalho Chehab <mchehab@s-opensource.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 900dcfee542c..75ffc1d1a2e0 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -79,12 +79,16 @@ static void wakeup_softirqd(void)
 
 /*
  * If ksoftirqd is scheduled, we do not want to process pending softirqs
- * right now. Let ksoftirqd handle this at its own rate, to get fairness.
+ * right now. Let ksoftirqd handle this at its own rate, to get fairness,
+ * unless we're doing some of the synchronous softirqs.
  */
-static bool ksoftirqd_running(void)
+#define SOFTIRQ_NOW_MASK ((1 << HI_SOFTIRQ) | (1 << TASKLET_SOFTIRQ))
+static bool ksoftirqd_running(unsigned long pending)
 {
 	struct task_struct *tsk = __this_cpu_read(ksoftirqd);
 
+	if (pending & SOFTIRQ_NOW_MASK)
+		return false;
 	return tsk && (tsk->state == TASK_RUNNING);
 }
 
@@ -328,7 +332,7 @@ asmlinkage __visible void do_softirq(void)
 
 	pending = local_softirq_pending();
 
-	if (pending && !ksoftirqd_running())
+	if (pending && !ksoftirqd_running(pending))
 		do_softirq_own_stack();
 
 	local_irq_restore(flags);
@@ -355,7 +359,7 @@ void irq_enter(void)
 
 static inline void invoke_softirq(void)
 {
-	if (ksoftirqd_running())
+	if (ksoftirqd_running(local_softirq_pending()))
 		return;
 
 	if (!force_irqthreads) {

commit 1a63dcd8765bc8680481dc2f9acf6ef13cee6d27
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Thu Jun 7 13:11:43 2018 -0700

    softirq: Reorder trace_softirqs_on to prevent lockdep splat
    
    I'm able to reproduce a lockdep splat with config options:
    CONFIG_PROVE_LOCKING=y,
    CONFIG_DEBUG_LOCK_ALLOC=y and
    CONFIG_PREEMPTIRQ_EVENTS=y
    
    $ echo 1 > /d/tracing/events/preemptirq/preempt_enable/enable
    
    [   26.112609] DEBUG_LOCKS_WARN_ON(current->softirqs_enabled)
    [   26.112636] WARNING: CPU: 0 PID: 118 at kernel/locking/lockdep.c:3854
    [...]
    [   26.144229] Call Trace:
    [   26.144926]  <IRQ>
    [   26.145506]  lock_acquire+0x55/0x1b0
    [   26.146499]  ? __do_softirq+0x46f/0x4d9
    [   26.147571]  ? __do_softirq+0x46f/0x4d9
    [   26.148646]  trace_preempt_on+0x8f/0x240
    [   26.149744]  ? trace_preempt_on+0x4d/0x240
    [   26.150862]  ? __do_softirq+0x46f/0x4d9
    [   26.151930]  preempt_count_sub+0x18a/0x1a0
    [   26.152985]  __do_softirq+0x46f/0x4d9
    [   26.153937]  irq_exit+0x68/0xe0
    [   26.154755]  smp_apic_timer_interrupt+0x271/0x280
    [   26.156056]  apic_timer_interrupt+0xf/0x20
    [   26.157105]  </IRQ>
    
    The issue was this:
    
    preempt_count = 1 << SOFTIRQ_SHIFT
    
            __local_bh_enable(cnt = 1 << SOFTIRQ_SHIFT) {
                    if (softirq_count() == (cnt && SOFTIRQ_MASK)) {
                            trace_softirqs_on() {
                                    current->softirqs_enabled = 1;
                            }
                    }
                    preempt_count_sub(cnt) {
                            trace_preempt_on() {
                                    tracepoint() {
                                            rcu_read_lock_sched() {
                                                    // jumps into lockdep
    
    Where preempt_count still has softirqs disabled, but
    current->softirqs_enabled is true, and we get a splat.
    
    Link: http://lkml.kernel.org/r/20180607201143.247775-1-joel@joelfernandes.org
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Thomas Glexiner <tglx@linutronix.de>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Erick Reyes <erickreyes@google.com>
    Cc: Julia Cartwright <julia@ni.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: stable@vger.kernel.org
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Fixes: d59158162e032 ("tracing: Add support for preempt and irq enable/disable events")
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index de2f57fddc04..900dcfee542c 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -139,9 +139,13 @@ static void __local_bh_enable(unsigned int cnt)
 {
 	lockdep_assert_irqs_disabled();
 
+	if (preempt_count() == cnt)
+		trace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());
+
 	if (softirq_count() == (cnt & SOFTIRQ_MASK))
 		trace_softirqs_on(_RET_IP_);
-	preempt_count_sub(cnt);
+
+	__preempt_count_sub(cnt);
 }
 
 /*

commit db020be9f7a0eb667761f0b762c1aadef2d7bd24
Merge: d09a8e6f2c0a 65441ba9c4e4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 19:59:22 2018 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq updates from Thomas Gleixner:
    
     - Consolidation of softirq pending:
    
       The softirq mask and its accessors/mutators have many implementations
       scattered around many architectures. Most do the same things
       consisting in a field in a per-cpu struct (often irq_cpustat_t)
       accessed through per-cpu ops. We can provide instead a generic
       efficient version that most of them can use. In fact s390 is the only
       exception because the field is stored in lowcore.
    
     - Support for level!?! triggered MSI (ARM)
    
       Over the past couple of years, we've seen some SoCs coming up with
       ways of signalling level interrupts using a new flavor of MSIs, where
       the MSI controller uses two distinct messages: one that raises a
       virtual line, and one that lowers it. The target MSI controller is in
       charge of maintaining the state of the line.
    
       This allows for a much simplified HW signal routing (no need to have
       hundreds of discrete lines to signal level interrupts if you already
       have a memory bus), but results in a departure from the current idea
       the kernel has of MSIs.
    
     - Support for Meson-AXG GPIO irqchip
    
     - Large stm32 irqchip rework (suspend/resume, hierarchical domains)
    
     - More SPDX conversions
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (36 commits)
      ARM: dts: stm32: Add exti support to stm32mp157 pinctrl
      ARM: dts: stm32: Add exti support for stm32mp157c
      pinctrl/stm32: Add irq_eoi for stm32gpio irqchip
      irqchip/stm32: Add suspend/resume support for hierarchy domain
      irqchip/stm32: Add stm32mp1 support with hierarchy domain
      irqchip/stm32: Prepare common functions
      irqchip/stm32: Add host and driver data structures
      irqchip/stm32: Add suspend support
      irqchip/stm32: Add falling pending register support
      irqchip/stm32: Checkpatch fix
      irqchip/stm32: Optimizes and cleans up stm32-exti irq_domain
      irqchip/meson-gpio: Add support for Meson-AXG SoCs
      dt-bindings: interrupt-controller: New binding for Meson-AXG SoC
      dt-bindings: interrupt-controller: Fix the double quotes
      softirq/s390: Move default mutators of overwritten softirq mask to s390
      softirq/x86: Switch to generic local_softirq_pending() implementation
      softirq/sparc: Switch to generic local_softirq_pending() implementation
      softirq/powerpc: Switch to generic local_softirq_pending() implementation
      softirq/parisc: Switch to generic local_softirq_pending() implementation
      softirq/ia64: Switch to generic local_softirq_pending() implementation
      ...

commit c3442697c2d73d3cdb9d4135cf630ad36ba8552f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Mar 5 11:29:40 2018 -0800

    softirq: Eliminate unused cond_resched_softirq() macro
    
    The cond_resched_softirq() macro is not used anywhere in mainline, so
    this commit simplifies the kernel by eliminating it.
    
    Suggested-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Eric Dumazet <edumazet@google.com>
    Tested-by: Nicholas Piggin <npiggin@gmail.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 177de3640c78..03981f1c39ea 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -145,8 +145,7 @@ static void __local_bh_enable(unsigned int cnt)
 }
 
 /*
- * Special-case - softirqs can safely be enabled in
- * cond_resched_softirq(), or by __do_softirq(),
+ * Special-case - softirqs can safely be enabled by __do_softirq(),
  * without processing still-pending softirqs:
  */
 void _local_bh_enable(void)

commit 0f6f47bacba514f4e9f61de0d85940dfb41498cc
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Tue May 8 15:38:19 2018 +0200

    softirq/core: Turn default irq_cpustat_t to standard per-cpu
    
    In order to optimize and consolidate softirq mask accesses, let's
    convert the default irq_cpustat_t implementation to per-CPU standard API.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/1525786706-22846-5-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 177de3640c78..c5fafd792df1 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -49,8 +49,8 @@
  */
 
 #ifndef __ARCH_IRQ_STAT
-irq_cpustat_t irq_stat[NR_CPUS] ____cacheline_aligned;
-EXPORT_SYMBOL(irq_stat);
+DEFINE_PER_CPU_ALIGNED(irq_cpustat_t, irq_stat);
+EXPORT_PER_CPU_SYMBOL(irq_stat);
 #endif
 
 static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;

commit 82b691bedf05f258f1c86c96ee574b0d7795c0a1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Feb 27 17:48:08 2018 +0100

    softirq: Consolidate common code in tasklet_[hi]_action()
    
    tasklet_action() + tasklet_hi_action() are almost identical.  Move the
    common code from both function into __tasklet_action_common() and let
    both functions invoke it with different arguments.
    
    [ bigeasy: Splitted out from RT's "tasklet: Prevent tasklets from going
               into infinite spin in RT" and added commit message]
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Julia Cartwright <juliac@eso.teric.us>
    Link: https://lkml.kernel.org/r/20180227164808.10093-3-bigeasy@linutronix.de

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 2394b009994f..177de3640c78 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -490,14 +490,16 @@ void __tasklet_hi_schedule(struct tasklet_struct *t)
 }
 EXPORT_SYMBOL(__tasklet_hi_schedule);
 
-static __latent_entropy void tasklet_action(struct softirq_action *a)
+static void tasklet_action_common(struct softirq_action *a,
+				  struct tasklet_head *tl_head,
+				  unsigned int softirq_nr)
 {
 	struct tasklet_struct *list;
 
 	local_irq_disable();
-	list = __this_cpu_read(tasklet_vec.head);
-	__this_cpu_write(tasklet_vec.head, NULL);
-	__this_cpu_write(tasklet_vec.tail, this_cpu_ptr(&tasklet_vec.head));
+	list = tl_head->head;
+	tl_head->head = NULL;
+	tl_head->tail = &tl_head->head;
 	local_irq_enable();
 
 	while (list) {
@@ -519,47 +521,21 @@ static __latent_entropy void tasklet_action(struct softirq_action *a)
 
 		local_irq_disable();
 		t->next = NULL;
-		*__this_cpu_read(tasklet_vec.tail) = t;
-		__this_cpu_write(tasklet_vec.tail, &(t->next));
-		__raise_softirq_irqoff(TASKLET_SOFTIRQ);
+		*tl_head->tail = t;
+		tl_head->tail = &t->next;
+		__raise_softirq_irqoff(softirq_nr);
 		local_irq_enable();
 	}
 }
 
-static __latent_entropy void tasklet_hi_action(struct softirq_action *a)
+static __latent_entropy void tasklet_action(struct softirq_action *a)
 {
-	struct tasklet_struct *list;
-
-	local_irq_disable();
-	list = __this_cpu_read(tasklet_hi_vec.head);
-	__this_cpu_write(tasklet_hi_vec.head, NULL);
-	__this_cpu_write(tasklet_hi_vec.tail, this_cpu_ptr(&tasklet_hi_vec.head));
-	local_irq_enable();
-
-	while (list) {
-		struct tasklet_struct *t = list;
-
-		list = list->next;
-
-		if (tasklet_trylock(t)) {
-			if (!atomic_read(&t->count)) {
-				if (!test_and_clear_bit(TASKLET_STATE_SCHED,
-							&t->state))
-					BUG();
-				t->func(t->data);
-				tasklet_unlock(t);
-				continue;
-			}
-			tasklet_unlock(t);
-		}
+	tasklet_action_common(a, this_cpu_ptr(&tasklet_vec), TASKLET_SOFTIRQ);
+}
 
-		local_irq_disable();
-		t->next = NULL;
-		*__this_cpu_read(tasklet_hi_vec.tail) = t;
-		__this_cpu_write(tasklet_hi_vec.tail, &(t->next));
-		__raise_softirq_irqoff(HI_SOFTIRQ);
-		local_irq_enable();
-	}
+static __latent_entropy void tasklet_hi_action(struct softirq_action *a)
+{
+	tasklet_action_common(a, this_cpu_ptr(&tasklet_hi_vec), HI_SOFTIRQ);
 }
 
 void tasklet_init(struct tasklet_struct *t,

commit 6498ddad301c7a94162915d06d1efe2e5d20f6dc
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Feb 27 17:48:07 2018 +0100

    softirq: Consolidate common code in __tasklet_[hi]_schedule()
    
    __tasklet_schedule() and __tasklet_hi_schedule() are almost identical.
    Move the common code from both function into __tasklet_schedule_common()
    and let both functions invoke it with different arguments.
    
    [ bigeasy: Splitted out from RT's "tasklet: Prevent tasklets from going
               into infinite spin in RT" and added commit message. Use
               this_cpu_ptr(headp) in __tasklet_schedule_common() as suggested
               by Julia Cartwright ]
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Julia Cartwright <juliac@eso.teric.us>
    Link: https://lkml.kernel.org/r/20180227164808.10093-2-bigeasy@linutronix.de

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 24d243ef8e71..2394b009994f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -460,29 +460,33 @@ struct tasklet_head {
 static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
 static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);
 
-void __tasklet_schedule(struct tasklet_struct *t)
+static void __tasklet_schedule_common(struct tasklet_struct *t,
+				      struct tasklet_head __percpu *headp,
+				      unsigned int softirq_nr)
 {
+	struct tasklet_head *head;
 	unsigned long flags;
 
 	local_irq_save(flags);
+	head = this_cpu_ptr(headp);
 	t->next = NULL;
-	*__this_cpu_read(tasklet_vec.tail) = t;
-	__this_cpu_write(tasklet_vec.tail, &(t->next));
-	raise_softirq_irqoff(TASKLET_SOFTIRQ);
+	*head->tail = t;
+	head->tail = &(t->next);
+	raise_softirq_irqoff(softirq_nr);
 	local_irq_restore(flags);
 }
+
+void __tasklet_schedule(struct tasklet_struct *t)
+{
+	__tasklet_schedule_common(t, &tasklet_vec,
+				  TASKLET_SOFTIRQ);
+}
 EXPORT_SYMBOL(__tasklet_schedule);
 
 void __tasklet_hi_schedule(struct tasklet_struct *t)
 {
-	unsigned long flags;
-
-	local_irq_save(flags);
-	t->next = NULL;
-	*__this_cpu_read(tasklet_hi_vec.tail) = t;
-	__this_cpu_write(tasklet_hi_vec.tail,  &(t->next));
-	raise_softirq_irqoff(HI_SOFTIRQ);
-	local_irq_restore(flags);
+	__tasklet_schedule_common(t, &tasklet_hi_vec,
+				  HI_SOFTIRQ);
 }
 EXPORT_SYMBOL(__tasklet_hi_schedule);
 

commit edf22f4ca26babcd8cba4f049c6be53f0e73bcc1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 24 08:31:12 2017 -0700

    softirq: Eliminate cond_resched_rcu_qs() in favor of cond_resched()
    
    Now that cond_resched() also provides RCU quiescent states when
    needed, it can be used in place of cond_resched_rcu_qs().  This
    commit therefore makes this change.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: NeilBrown <neilb@suse.com>
    Cc: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 2f5e87f1bae2..24d243ef8e71 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -665,7 +665,7 @@ static void run_ksoftirqd(unsigned int cpu)
 		 */
 		__do_softirq();
 		local_irq_enable();
-		cond_resched_rcu_qs();
+		cond_resched();
 		return;
 	}
 	local_irq_enable();

commit 4675ff05de2d76d167336b368bd07f3fef6ed5a6
Author: Levin, Alexander (Sasha Levin) <alexander.levin@verizon.com>
Date:   Wed Nov 15 17:36:02 2017 -0800

    kmemcheck: rip it out
    
    Fix up makefiles, remove references, and git rm kmemcheck.
    
    Link: http://lkml.kernel.org/r/20171007030159.22241-4-alexander.levin@verizon.com
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Tim Hansen <devtimhansen@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 662f7b1b7a78..2f5e87f1bae2 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -486,16 +486,6 @@ void __tasklet_hi_schedule(struct tasklet_struct *t)
 }
 EXPORT_SYMBOL(__tasklet_hi_schedule);
 
-void __tasklet_hi_schedule_first(struct tasklet_struct *t)
-{
-	lockdep_assert_irqs_disabled();
-
-	t->next = __this_cpu_read(tasklet_hi_vec.head);
-	__this_cpu_write(tasklet_hi_vec.head, t);
-	__raise_softirq_irqoff(HI_SOFTIRQ);
-}
-EXPORT_SYMBOL(__tasklet_hi_schedule_first);
-
 static __latent_entropy void tasklet_action(struct softirq_action *a)
 {
 	struct tasklet_struct *list;

commit f71b74bca637fca7de78f1d44eaefde5bc900f9f
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Mon Nov 6 16:01:18 2017 +0100

    irq/softirqs: Use lockdep to assert IRQs are disabled/enabled
    
    Use lockdep to check that IRQs are enabled or disabled as expected. This
    way the sanity check only shows overhead when concurrency correctness
    debug code is enabled.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: David S . Miller <davem@davemloft.net>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/r/1509980490-4285-3-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 4e09821f9d9e..662f7b1b7a78 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -137,7 +137,7 @@ EXPORT_SYMBOL(__local_bh_disable_ip);
 
 static void __local_bh_enable(unsigned int cnt)
 {
-	WARN_ON_ONCE(!irqs_disabled());
+	lockdep_assert_irqs_disabled();
 
 	if (softirq_count() == (cnt & SOFTIRQ_MASK))
 		trace_softirqs_on(_RET_IP_);
@@ -158,7 +158,8 @@ EXPORT_SYMBOL(_local_bh_enable);
 
 void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
 {
-	WARN_ON_ONCE(in_irq() || irqs_disabled());
+	WARN_ON_ONCE(in_irq());
+	lockdep_assert_irqs_enabled();
 #ifdef CONFIG_TRACE_IRQFLAGS
 	local_irq_disable();
 #endif
@@ -396,9 +397,8 @@ void irq_exit(void)
 #ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED
 	local_irq_disable();
 #else
-	WARN_ON_ONCE(!irqs_disabled());
+	lockdep_assert_irqs_disabled();
 #endif
-
 	account_irq_exit_time(current);
 	preempt_count_sub(HARDIRQ_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())
@@ -488,7 +488,7 @@ EXPORT_SYMBOL(__tasklet_hi_schedule);
 
 void __tasklet_hi_schedule_first(struct tasklet_struct *t)
 {
-	BUG_ON(!irqs_disabled());
+	lockdep_assert_irqs_disabled();
 
 	t->next = __this_cpu_read(tasklet_hi_vec.head);
 	__this_cpu_write(tasklet_hi_vec.head, t);

commit 717a94b5fc7092afebe9c93791f29b2d8e5d297a
Author: NeilBrown <neilb@suse.com>
Date:   Fri Apr 7 10:03:26 2017 +1000

    sched/core: Remove 'task' parameter and rename tsk_restore_flags() to current_restore_flags()
    
    It is not safe for one thread to modify the ->flags
    of another thread as there is no locking that can protect
    the update.
    
    So tsk_restore_flags(), which takes a task pointer and modifies
    the flags, is an invitation to do the wrong thing.
    
    All current users pass "current" as the task, so no developers have
    accepted that invitation.  It would be best to ensure it remains
    that way.
    
    So rename tsk_restore_flags() to current_restore_flags() and don't
    pass in a task_struct pointer.  Always operate on current->flags.
    
    Signed-off-by: NeilBrown <neilb@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 744fa611cae0..4e09821f9d9e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -309,7 +309,7 @@ asmlinkage __visible void __softirq_entry __do_softirq(void)
 	account_irq_exit_time(current);
 	__local_bh_enable(SOFTIRQ_OFFSET);
 	WARN_ON_ONCE(in_interrupt());
-	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
+	current_restore_flags(old_flags, PF_MEMALLOC);
 }
 
 asmlinkage __visible void do_softirq(void)

commit f660f6066716b700148f60dba3461e65efff3123
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Mon Oct 10 15:10:51 2016 +0300

    softirq: Display IRQ_POLL for irq-poll statistics
    
    This library was moved to the generic area and was
    renamed to irq-poll. Hence, update proc/softirqs output accordingly.
    
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 1bf81ef91375..744fa611cae0 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -58,7 +58,7 @@ static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp
 DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
 
 const char * const softirq_to_name[NR_SOFTIRQS] = {
-	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL",
+	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "IRQ_POLL",
 	"TASKLET", "SCHED", "HRTIMER", "RCU"
 };
 

commit 9ffc66941df278c9f4df979b6bcf6c6ddafedd16
Merge: 133d970e0dad 0766f788eb72
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 15 10:03:15 2016 -0700

    Merge tag 'gcc-plugins-v4.9-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull gcc plugins update from Kees Cook:
     "This adds a new gcc plugin named "latent_entropy". It is designed to
      extract as much possible uncertainty from a running system at boot
      time as possible, hoping to capitalize on any possible variation in
      CPU operation (due to runtime data differences, hardware differences,
      SMP ordering, thermal timing variation, cache behavior, etc).
    
      At the very least, this plugin is a much more comprehensive example
      for how to manipulate kernel code using the gcc plugin internals"
    
    * tag 'gcc-plugins-v4.9-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      latent_entropy: Mark functions with __latent_entropy
      gcc-plugins: Add latent_entropy plugin

commit 0766f788eb727e2e330d55d30545db65bcf2623f
Author: Emese Revfy <re.emese@gmail.com>
Date:   Mon Jun 20 20:42:34 2016 +0200

    latent_entropy: Mark functions with __latent_entropy
    
    The __latent_entropy gcc attribute can be used only on functions and
    variables.  If it is on a function then the plugin will instrument it for
    gathering control-flow entropy. If the attribute is on a variable then
    the plugin will initialize it with random contents.  The variable must
    be an integer, an integer array type or a structure with integer fields.
    
    These specific functions have been selected because they are init
    functions (to help gather boot-time entropy), are called at unpredictable
    times, or they have variable loops, each of which provide some level of
    latent entropy.
    
    Signed-off-by: Emese Revfy <re.emese@gmail.com>
    [kees: expanded commit message]
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 17caf4b63342..34033fd09c8c 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -482,7 +482,7 @@ void __tasklet_hi_schedule_first(struct tasklet_struct *t)
 }
 EXPORT_SYMBOL(__tasklet_hi_schedule_first);
 
-static void tasklet_action(struct softirq_action *a)
+static __latent_entropy void tasklet_action(struct softirq_action *a)
 {
 	struct tasklet_struct *list;
 
@@ -518,7 +518,7 @@ static void tasklet_action(struct softirq_action *a)
 	}
 }
 
-static void tasklet_hi_action(struct softirq_action *a)
+static __latent_entropy void tasklet_hi_action(struct softirq_action *a)
 {
 	struct tasklet_struct *list;
 

commit 597f03f9d133e9837d00965016170271d4f87dcf
Merge: 999dcbe2414e 0bf71e4d02ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 19:43:08 2016 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull CPU hotplug updates from Thomas Gleixner:
     "Yet another batch of cpu hotplug core updates and conversions:
    
       - Provide core infrastructure for multi instance drivers so the
         drivers do not have to keep custom lists.
    
       - Convert custom lists to the new infrastructure. The block-mq custom
         list conversion comes through the block tree and makes the diffstat
         tip over to more lines removed than added.
    
       - Handle unbalanced hotplug enable/disable calls more gracefully.
    
       - Remove the obsolete CPU_STARTING/DYING notifier support.
    
       - Convert another batch of notifier users.
    
       The relayfs changes which conflicted with the conversion have been
       shipped to me by Andrew.
    
       The remaining lot is targeted for 4.10 so that we finally can remove
       the rest of the notifiers"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (46 commits)
      cpufreq: Fix up conversion to hotplug state machine
      blk/mq: Reserve hotplug states for block multiqueue
      x86/apic/uv: Convert to hotplug state machine
      s390/mm/pfault: Convert to hotplug state machine
      mips/loongson/smp: Convert to hotplug state machine
      mips/octeon/smp: Convert to hotplug state machine
      fault-injection/cpu: Convert to hotplug state machine
      padata: Convert to hotplug state machine
      cpufreq: Convert to hotplug state machine
      ACPI/processor: Convert to hotplug state machine
      virtio scsi: Convert to hotplug state machine
      oprofile/timer: Convert to hotplug state machine
      block/softirq: Convert to hotplug state machine
      lib/irq_poll: Convert to hotplug state machine
      x86/microcode: Convert to hotplug state machine
      sh/SH-X3 SMP: Convert to hotplug state machine
      ia64/mca: Convert to hotplug state machine
      ARM/OMAP/wakeupgen: Convert to hotplug state machine
      ARM/shmobile: Convert to hotplug state machine
      arm64/FP/SIMD: Convert to hotplug state machine
      ...

commit 4cd13c21b207e80ddb1144c576500098f2d5f882
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Aug 31 10:42:29 2016 -0700

    softirq: Let ksoftirqd do its job
    
    A while back, Paolo and Hannes sent an RFC patch adding threaded-able
    napi poll loop support : (https://patchwork.ozlabs.org/patch/620657/)
    
    The problem seems to be that softirqs are very aggressive and are often
    handled by the current process, even if we are under stress and that
    ksoftirqd was scheduled, so that innocent threads would have more chance
    to make progress.
    
    This patch makes sure that if ksoftirq is running, we let it
    perform the softirq work.
    
    Jonathan Corbet summarized the issue in https://lwn.net/Articles/687617/
    
    Tested:
    
     - NIC receiving traffic handled by CPU 0
     - UDP receiver running on CPU 0, using a single UDP socket.
     - Incoming flood of UDP packets targeting the UDP socket.
    
    Before the patch, the UDP receiver could almost never get CPU cycles and
    could only receive ~2,000 packets per second.
    
    After the patch, CPU cycles are split 50/50 between user application and
    ksoftirqd/0, and we can effectively read ~900,000 packets per second,
    a huge improvement in DOS situation. (Note that more packets are now
    dropped by the NIC itself, since the BH handlers get less CPU cycles to
    drain RX ring buffer)
    
    Since the load runs in well identified threads context, an admin can
    more easily tune process scheduling parameters if needed.
    
    Reported-by: Paolo Abeni <pabeni@redhat.com>
    Reported-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Hannes Frederic Sowa <hannes@redhat.com>
    Cc: Jesper Dangaard Brouer <jbrouer@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1472665349.14381.356.camel@edumazet-glaptop3.roam.corp.google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 17caf4b63342..8ed90e3a88d6 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -77,6 +77,17 @@ static void wakeup_softirqd(void)
 		wake_up_process(tsk);
 }
 
+/*
+ * If ksoftirqd is scheduled, we do not want to process pending softirqs
+ * right now. Let ksoftirqd handle this at its own rate, to get fairness.
+ */
+static bool ksoftirqd_running(void)
+{
+	struct task_struct *tsk = __this_cpu_read(ksoftirqd);
+
+	return tsk && (tsk->state == TASK_RUNNING);
+}
+
 /*
  * preempt_count and SOFTIRQ_OFFSET usage:
  * - preempt_count is changed by SOFTIRQ_OFFSET on entering or leaving
@@ -313,7 +324,7 @@ asmlinkage __visible void do_softirq(void)
 
 	pending = local_softirq_pending();
 
-	if (pending)
+	if (pending && !ksoftirqd_running())
 		do_softirq_own_stack();
 
 	local_irq_restore(flags);
@@ -340,6 +351,9 @@ void irq_enter(void)
 
 static inline void invoke_softirq(void)
 {
+	if (ksoftirqd_running())
+		return;
+
 	if (!force_irqthreads) {
 #ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
 		/*

commit c4544dbc7a9bce3da6fa2361cd68cadb34e9221f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Aug 18 14:57:21 2016 +0200

    kernel/softirq: Convert to hotplug state machine
    
    Install the callbacks via the state machine.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160818125731.27256-7-bigeasy@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 17caf4b63342..c372114494f5 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -700,7 +700,7 @@ void tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu)
 	BUG();
 }
 
-static void takeover_tasklets(unsigned int cpu)
+static int takeover_tasklets(unsigned int cpu)
 {
 	/* CPU is dead, so no lock needed. */
 	local_irq_disable();
@@ -723,27 +723,12 @@ static void takeover_tasklets(unsigned int cpu)
 	raise_softirq_irqoff(HI_SOFTIRQ);
 
 	local_irq_enable();
+	return 0;
 }
+#else
+#define takeover_tasklets	NULL
 #endif /* CONFIG_HOTPLUG_CPU */
 
-static int cpu_callback(struct notifier_block *nfb, unsigned long action,
-			void *hcpu)
-{
-	switch (action) {
-#ifdef CONFIG_HOTPLUG_CPU
-	case CPU_DEAD:
-	case CPU_DEAD_FROZEN:
-		takeover_tasklets((unsigned long)hcpu);
-		break;
-#endif /* CONFIG_HOTPLUG_CPU */
-	}
-	return NOTIFY_OK;
-}
-
-static struct notifier_block cpu_nfb = {
-	.notifier_call = cpu_callback
-};
-
 static struct smp_hotplug_thread softirq_threads = {
 	.store			= &ksoftirqd,
 	.thread_should_run	= ksoftirqd_should_run,
@@ -753,8 +738,8 @@ static struct smp_hotplug_thread softirq_threads = {
 
 static __init int spawn_ksoftirqd(void)
 {
-	register_cpu_notifier(&cpu_nfb);
-
+	cpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, "softirq:dead", NULL,
+				  takeover_tasklets);
 	BUG_ON(smpboot_register_percpu_thread(&softirq_threads));
 
 	return 0;

commit be7635e7287e0e8013af3c89a6354a9e0182594c
Author: Alexander Potapenko <glider@google.com>
Date:   Fri Mar 25 14:22:05 2016 -0700

    arch, ftrace: for KASAN put hard/soft IRQ entries into separate sections
    
    KASAN needs to know whether the allocation happens in an IRQ handler.
    This lets us strip everything below the IRQ entry point to reduce the
    number of unique stack traces needed to be stored.
    
    Move the definition of __irq_entry to <linux/interrupt.h> so that the
    users don't need to pull in <linux/ftrace.h>.  Also introduce the
    __softirq_entry macro which is similar to __irq_entry, but puts the
    corresponding functions to the .softirqentry.text section.
    
    Signed-off-by: Alexander Potapenko <glider@google.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrey Konovalov <adech.fo@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Dmitry Chernenkov <dmitryc@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 8aae49dd7da8..17caf4b63342 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -227,7 +227,7 @@ static inline bool lockdep_softirq_start(void) { return false; }
 static inline void lockdep_softirq_end(bool in_hardirq) { }
 #endif
 
-asmlinkage __visible void __do_softirq(void)
+asmlinkage __visible void __softirq_entry __do_softirq(void)
 {
 	unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
 	unsigned long old_flags = current->flags;

commit f904f58263e1df5f70feb8b283f4bbe662847334
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri Feb 26 14:54:56 2016 +0100

    sched/debug: Fix preempt_disable_ip recording for preempt_disable()
    
    The preempt_disable() invokes preempt_count_add() which saves the caller
    in ->preempt_disable_ip. It uses CALLER_ADDR1 which does not look for
    its caller but for the parent of the caller. Which means we get the correct
    caller for something like spin_lock() unless the architectures inlines
    those invocations. It is always wrong for preempt_disable() or
    local_bh_disable().
    
    This patch makes the function get_lock_parent_ip() which tries
    CALLER_ADDR0,1,2 if the former is a locking function.
    This seems to record the preempt_disable() caller properly for
    preempt_disable() itself as well as for get_cpu_var() or
    local_bh_disable().
    
    Steven asked for the get_parent_ip() -> get_lock_parent_ip() rename.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160226135456.GB18244@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 479e4436f787..8aae49dd7da8 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -116,9 +116,9 @@ void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
 
 	if (preempt_count() == cnt) {
 #ifdef CONFIG_DEBUG_PREEMPT
-		current->preempt_disable_ip = get_parent_ip(CALLER_ADDR1);
+		current->preempt_disable_ip = get_lock_parent_ip();
 #endif
-		trace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));
+		trace_preempt_off(CALLER_ADDR0, get_lock_parent_ip());
 	}
 }
 EXPORT_SYMBOL(__local_bh_disable_ip);

commit 8308756f45a12e2ff4f7749c2694fc83cdef0be9
Merge: 23e8fe2e1658 afffc6c1805d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 9 15:24:03 2015 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core locking updates from Ingo Molnar:
     "The main changes are:
    
       - mutex, completions and rtmutex micro-optimizations
       - lock debugging fix
       - various cleanups in the MCS and the futex code"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      locking/rtmutex: Optimize setting task running after being blocked
      locking/rwsem: Use task->state helpers
      sched/completion: Add lock-free checking of the blocking case
      sched/completion: Remove unnecessary ->wait.lock serialization when reading completion state
      locking/mutex: Explicitly mark task as running after wakeup
      futex: Fix argument handling in futex_lock_pi() calls
      doc: Fix misnamed FUTEX_CMP_REQUEUE_PI op constants
      locking/Documentation: Update code path
      softirq/preempt: Add missing current->preempt_disable_ip update
      locking/osq: No need for load/acquire when acquire-polling
      locking/mcs: Better differentiate between MCS variants
      locking/mutex: Introduce ww_mutex_set_context_slowpath()
      locking/mutex: Move MCS related comments to proper location
      locking/mutex: Checking the stamp is WW only

commit 60479676eb6e9913176d93ebad194e3dc167bc63
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 14 13:20:26 2015 -0800

    ksoftirqd: Use new cond_resched_rcu_qs() function
    
    Simplify run_ksoftirqd() by using the new cond_resched_rcu_qs() function
    that conditionally reschedules, but unconditionally supplies an RCU
    quiescent state.  This commit is separate from the previous commit by
    Calvin Owens because Calvin's approach can be backported, while this
    commit cannot be.  The reason that this commit cannot be backported is
    that cond_resched_rcu_qs() does not always provide the needed quiescent
    state in earlier kernels.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index c497fcdf0d1e..8cdb98847c7b 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -657,12 +657,7 @@ static void run_ksoftirqd(unsigned int cpu)
 		 */
 		__do_softirq();
 		local_irq_enable();
-		cond_resched();
-
-		preempt_disable();
-		rcu_note_context_switch();
-		preempt_enable();
-
+		cond_resched_rcu_qs();
 		return;
 	}
 	local_irq_enable();

commit 28423ad283d5348793b0c45cc9b1af058e776fd6
Author: Calvin Owens <calvinowens@fb.com>
Date:   Tue Jan 13 13:16:18 2015 -0800

    ksoftirqd: Enable IRQs and call cond_resched() before poking RCU
    
    While debugging an issue with excessive softirq usage, I encountered the
    following note in commit 3e339b5dae24a706 ("softirq: Use hotplug thread
    infrastructure"):
    
        [ paulmck: Call rcu_note_context_switch() with interrupts enabled. ]
    
    ...but despite this note, the patch still calls RCU with IRQs disabled.
    
    This seemingly innocuous change caused a significant regression in softirq
    CPU usage on the sending side of a large TCP transfer (~1 GB/s): when
    introducing 0.01% packet loss, the softirq usage would jump to around 25%,
    spiking as high as 50%. Before the change, the usage would never exceed 5%.
    
    Moving the call to rcu_note_context_switch() after the cond_sched() call,
    as it was originally before the hotplug patch, completely eliminated this
    problem.
    
    Signed-off-by: Calvin Owens <calvinowens@fb.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 501baa9ac1be..c497fcdf0d1e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -656,9 +656,13 @@ static void run_ksoftirqd(unsigned int cpu)
 		 * in the task stack here.
 		 */
 		__do_softirq();
-		rcu_note_context_switch();
 		local_irq_enable();
 		cond_resched();
+
+		preempt_disable();
+		rcu_note_context_switch();
+		preempt_enable();
+
 		return;
 	}
 	local_irq_enable();

commit 0f1ba9a2cea52896448ef4e14a4cb1880b8e5bee
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 7 10:04:41 2015 +0100

    softirq/preempt: Add missing current->preempt_disable_ip update
    
    While debugging some "sleeping function called from invalid context" bug I
    realized that the debugging message "Preemption disabled at:" pointed to
    an incorrect function.
    
    In particular if the last function/action that disabled preemption was
    spin_lock_bh() then current->preempt_disable_ip won't be updated.
    
    The reason for this is that __local_bh_disable_ip() will increase
    preempt_count manually instead of calling preempt_count_add(), which
    would handle the update correctly.
    
    It look like the manual handling was done to work around some lockdep issue.
    
    So add the missing update of current->preempt_disable_ip to
    __local_bh_disable_ip() as well.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20150107090441.GC4365@osiris
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 501baa9ac1be..b9ae81643f1d 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -114,8 +114,12 @@ void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
 		trace_softirqs_off(ip);
 	raw_local_irq_restore(flags);
 
-	if (preempt_count() == cnt)
+	if (preempt_count() == cnt) {
+#ifdef CONFIG_DEBUG_PREEMPT
+		current->preempt_disable_ip = get_parent_ip(CALLER_ADDR1);
+#endif
 		trace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));
+	}
 }
 EXPORT_SYMBOL(__local_bh_disable_ip);
 #endif /* CONFIG_TRACE_IRQFLAGS */

commit 38200cf24702e5d79ce6c8f4c62036c41845c62d
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 21 12:50:04 2014 -0700

    rcu: Remove "cpu" argument to rcu_note_context_switch()
    
    The "cpu" argument to rcu_note_context_switch() is always the current
    CPU, so drop it.  This in turn allows the "cpu" argument to
    rcu_preempt_note_context_switch() to be removed, which allows the sole
    use of "cpu" in both functions to be replaced with a this_cpu_ptr().
    Again, the anticipated cross-CPU uses of these functions has been
    replaced by NO_HZ_FULL.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 0699add19164..501baa9ac1be 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -656,7 +656,7 @@ static void run_ksoftirqd(unsigned int cpu)
 		 * in the task stack here.
 		 */
 		__do_softirq();
-		rcu_note_context_switch(cpu);
+		rcu_note_context_switch();
 		local_irq_enable();
 		cond_resched();
 		return;

commit 0429fbc0bdc297d64188483ba029a23773ae07b0
Merge: 6929c358972f 513d1a2884a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 15 07:48:18 2014 +0200

    Merge branch 'for-3.18-consistent-ops' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    Pull percpu consistent-ops changes from Tejun Heo:
     "Way back, before the current percpu allocator was implemented, static
      and dynamic percpu memory areas were allocated and handled separately
      and had their own accessors.  The distinction has been gone for many
      years now; however, the now duplicate two sets of accessors remained
      with the pointer based ones - this_cpu_*() - evolving various other
      operations over time.  During the process, we also accumulated other
      inconsistent operations.
    
      This pull request contains Christoph's patches to clean up the
      duplicate accessor situation.  __get_cpu_var() uses are replaced with
      with this_cpu_ptr() and __this_cpu_ptr() with raw_cpu_ptr().
    
      Unfortunately, the former sometimes is tricky thanks to C being a bit
      messy with the distinction between lvalues and pointers, which led to
      a rather ugly solution for cpumask_var_t involving the introduction of
      this_cpu_cpumask_var_ptr().
    
      This converts most of the uses but not all.  Christoph will follow up
      with the remaining conversions in this merge window and hopefully
      remove the obsolete accessors"
    
    * 'for-3.18-consistent-ops' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (38 commits)
      irqchip: Properly fetch the per cpu offset
      percpu: Resolve ambiguities in __get_cpu_var/cpumask_var_t -fix
      ia64: sn_nodepda cannot be assigned to after this_cpu conversion. Use __this_cpu_write.
      percpu: Resolve ambiguities in __get_cpu_var/cpumask_var_t
      Revert "powerpc: Replace __get_cpu_var uses"
      percpu: Remove __this_cpu_ptr
      clocksource: Replace __this_cpu_ptr with raw_cpu_ptr
      sparc: Replace __get_cpu_var uses
      avr32: Replace __get_cpu_var with __this_cpu_write
      blackfin: Replace __get_cpu_var uses
      tile: Use this_cpu_ptr() for hardware counters
      tile: Replace __get_cpu_var uses
      powerpc: Replace __get_cpu_var uses
      alpha: Replace __get_cpu_var
      ia64: Replace __get_cpu_var uses
      s390: cio driver &__get_cpu_var replacements
      s390: Replace __get_cpu_var uses
      mips: Replace __get_cpu_var uses
      MIPS: Replace __get_cpu_var uses in FPU emulator.
      arm: Replace __this_cpu_ptr with raw_cpu_ptr
      ...

commit 284a8c93af47306beed967a303d84730b32bab39
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 14 16:38:46 2014 -0700

    rcu: Per-CPU operation cleanups to rcu_*_qs() functions
    
    The rcu_bh_qs(), rcu_preempt_qs(), and rcu_sched_qs() functions use
    old-style per-CPU variable access and write to ->passed_quiesce even
    if it is already set.  This commit therefore updates to use the new-style
    per-CPU variable access functions and avoids the spurious writes.
    This commit also eliminates the "cpu" argument to these functions because
    they are always invoked on the indicated CPU.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 5918d227730f..348ec763b104 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -278,7 +278,7 @@ asmlinkage __visible void __do_softirq(void)
 		pending >>= softirq_bit;
 	}
 
-	rcu_bh_qs(smp_processor_id());
+	rcu_bh_qs();
 	local_irq_disable();
 
 	pending = local_softirq_pending();

commit 22127e93c587afa01e4f7225d2d1cf1d26ae7dfe
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Aug 17 12:30:25 2014 -0500

    time: Replace __get_cpu_var uses
    
    Convert uses of __get_cpu_var for creating a address from a percpu
    offset to this_cpu_ptr.
    
    The two cases where get_cpu_var is used to actually access a percpu
    variable are changed to use this_cpu_read/raw_cpu_read.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 5918d227730f..2d44b5714fe6 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -485,7 +485,7 @@ static void tasklet_action(struct softirq_action *a)
 	local_irq_disable();
 	list = __this_cpu_read(tasklet_vec.head);
 	__this_cpu_write(tasklet_vec.head, NULL);
-	__this_cpu_write(tasklet_vec.tail, &__get_cpu_var(tasklet_vec).head);
+	__this_cpu_write(tasklet_vec.tail, this_cpu_ptr(&tasklet_vec.head));
 	local_irq_enable();
 
 	while (list) {
@@ -521,7 +521,7 @@ static void tasklet_hi_action(struct softirq_action *a)
 	local_irq_disable();
 	list = __this_cpu_read(tasklet_hi_vec.head);
 	__this_cpu_write(tasklet_hi_vec.head, NULL);
-	__this_cpu_write(tasklet_hi_vec.tail, &__get_cpu_var(tasklet_hi_vec).head);
+	__this_cpu_write(tasklet_hi_vec.tail, this_cpu_ptr(&tasklet_hi_vec.head));
 	local_irq_enable();
 
 	while (list) {

commit e14505a8d50882ff3bdd4b791b14d90a0881fa4d
Merge: 4b660a7f5c80 61f38db3e3c0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu May 22 11:36:10 2014 +0200

    Merge branch 'rcu/next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
    Pull RCU updates from Paul E. McKenney:
    
    " 1.      Update RCU documentation.  These were posted to LKML at
              https://lkml.org/lkml/2014/4/28/634.
    
      2.      Miscellaneous fixes.  These were posted to LKML at
              https://lkml.org/lkml/2014/4/28/645.
    
      3.      Torture-test changes.  These were posted to LKML at
              https://lkml.org/lkml/2014/4/28/667.
    
      4.      Variable-name renaming cleanup, sent separately due to conflicts.
              This was posted to LKML at https://lkml.org/lkml/2014/5/13/854.
    
      5.      Patch to suppress RCU stall warnings while sysrq requests are
              being processed.  This patch is the RCU portions of the patch
              that Rik posted to LKML at https://lkml.org/lkml/2014/4/29/457.
              The reason for pushing this patch ahead instead of waiting until
              3.17 is that the NMI-based stack traces are messing up sysrq
              output, and in some cases also messing up the system as well."
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 722a9f9299ca720a3f14660e7c0dce7b76a9cb42
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri May 2 00:44:38 2014 +0200

    asmlinkage: Add explicit __visible to drivers/*, lib/*, kernel/*
    
    As requested by Linus add explicit __visible to the asmlinkage users.
    This marks functions visible to assembler.
    
    Tree sweep for rest of tree.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1398984278-29319-4-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 33e4648ae0e7..92f24f5e8d52 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -223,7 +223,7 @@ static inline bool lockdep_softirq_start(void) { return false; }
 static inline void lockdep_softirq_end(bool in_hardirq) { }
 #endif
 
-asmlinkage void __do_softirq(void)
+asmlinkage __visible void __do_softirq(void)
 {
 	unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
 	unsigned long old_flags = current->flags;
@@ -299,7 +299,7 @@ asmlinkage void __do_softirq(void)
 	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
 }
 
-asmlinkage void do_softirq(void)
+asmlinkage __visible void do_softirq(void)
 {
 	__u32 pending;
 	unsigned long flags;

commit a5d6d3a1b00a0ad88f07c3a727c79b27915278e3
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Apr 16 09:06:24 2014 -0700

    softirq: A single rcu_bh_qs() call per softirq set is enough
    
    Calling rcu_bh_qs() after every softirq action is not really needed.
    What RCU needs is at least one rcu_bh_qs() per softirq round to note a
    quiescent state was passed for rcu_bh.
    
    Note for Paul and myself : this could be inlined as a single instruction
    and avoid smp_processor_id()
    (sone this_cpu_write(rcu_bh_data.passed_quiesce, 1))
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b50990a5bea0..b9b2d4906848 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -232,7 +232,6 @@ asmlinkage void __do_softirq(void)
 	bool in_hardirq;
 	__u32 pending;
 	int softirq_bit;
-	int cpu;
 
 	/*
 	 * Mask out PF_MEMALLOC s current task context is borrowed for the
@@ -247,7 +246,6 @@ asmlinkage void __do_softirq(void)
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
 	in_hardirq = lockdep_softirq_start();
 
-	cpu = smp_processor_id();
 restart:
 	/* Reset the pending bitmask before enabling irqs */
 	set_softirq_pending(0);
@@ -276,11 +274,11 @@ asmlinkage void __do_softirq(void)
 			       prev_count, preempt_count());
 			preempt_count_set(prev_count);
 		}
-		rcu_bh_qs(cpu);
 		h++;
 		pending >>= softirq_bit;
 	}
 
+	rcu_bh_qs(smp_processor_id());
 	local_irq_disable();
 
 	pending = local_softirq_pending();

commit 62a08ae2a5763aabeee98264605236b001503e0c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 24 09:50:53 2014 +0200

    genirq: x86: Ensure that dynamic irq allocation does not conflict
    
    On x86 the allocation of irq descriptors may allocate interrupts which
    are in the range of the GSI interrupts. That's wrong as those
    interrupts are hardwired and we don't have the irq domain translation
    like PPC. So one of these interrupts can be hooked up later to one of
    the devices which are hard wired to it and the io_apic init code for
    that particular interrupt line happily reuses that descriptor with a
    completely different configuration so hell breaks lose.
    
    Inside x86 we allocate dynamic interrupts from above nr_gsi_irqs,
    except for a few usage sites which have not yet blown up in our face
    for whatever reason. But for drivers which need an irq range, like the
    GPIO drivers, we have no limit in place and we don't want to expose
    such a detail to a driver.
    
    To cure this introduce a function which an architecture can implement
    to impose a lower bound on the dynamic interrupt allocations.
    
    Implement it for x86 and set the lower bound to nr_gsi_irqs, which is
    the end of the hardwired interrupt space, so all dynamic allocations
    happen above.
    
    That not only allows the GPIO driver to work sanely, it also protects
    the bogus callsites of create_irq_nr() in hpet, uv, irq_remapping and
    htirq code. They need to be cleaned up as well, but that's a separate
    issue.
    
    Reported-by: Jin Yao <yao.jin@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Mika Westerberg <mika.westerberg@linux.intel.com>
    Cc: Mathias Nyman <mathias.nyman@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: H. Peter Anvin <hpa@linux.intel.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Krogerus Heikki <heikki.krogerus@intel.com>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1404241617360.28206@ionos.tec.linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b50990a5bea0..33e4648ae0e7 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -779,3 +779,8 @@ int __init __weak arch_early_irq_init(void)
 {
 	return 0;
 }
+
+unsigned int __weak arch_dynirq_lower_bound(unsigned int from)
+{
+	return from;
+}

commit d532676cc7329e1088702ccb0015942cc370b954
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 19 11:19:52 2014 +0100

    softirq: Add linux/irq.h to make it compile again
    
    On Sparc and S390 the removal of irq.h from kernel_stat.h causes:
    
       kernel/softirq.c:774:9: error: 'NR_IRQS_LEGACY' undeclared
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 490fcbb1dc5b..b50990a5bea0 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -25,6 +25,7 @@
 #include <linux/smp.h>
 #include <linux/smpboot.h>
 #include <linux/tick.h>
+#include <linux/irq.h>
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/irq.h>

commit aafd9d6a46745926648cb5d0b68b108e79ceb8d4
Merge: 595bf999e3a8 a2b4c607c93a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 31 09:02:51 2014 -0800

    Merge branch 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer/dynticks updates from Ingo Molnar:
     "This tree contains misc dynticks updates: a fix and three cleanups"
    
    * 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/nohz: Fix overflow error in scheduler_tick_max_deferment()
      nohz_full: fix code style issue of tick_nohz_full_stop_tick
      nohz: Get timekeeping max deferment outside jiffies_lock
      tick: Rename tick_check_idle() to tick_irq_enter()

commit ce85b4f2eab663dfd4ff2cb5b603ba03f595922e
Author: Joe Perches <joe@perches.com>
Date:   Mon Jan 27 17:07:16 2014 -0800

    softirq: use const char * const for softirq_to_name, whitespace neatening
    
    Reduce data size a little.
    Reduce checkpatch noise.
    
    $ size kernel/softirq.o*
       text    data     bss     dec     hex filename
      11554    6013    4008   21575    5447 kernel/softirq.o.new
      11474    6093    4008   21575    5447 kernel/softirq.o.old
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 2674354a5cb8..850967068aaf 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -56,7 +56,7 @@ static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp
 
 DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
 
-char *softirq_to_name[NR_SOFTIRQS] = {
+const char * const softirq_to_name[NR_SOFTIRQS] = {
 	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL",
 	"TASKLET", "SCHED", "HRTIMER", "RCU"
 };
@@ -138,7 +138,6 @@ void _local_bh_enable(void)
 	WARN_ON_ONCE(in_irq());
 	__local_bh_enable(SOFTIRQ_DISABLE_OFFSET);
 }
-
 EXPORT_SYMBOL(_local_bh_enable);
 
 void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
@@ -155,7 +154,7 @@ void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
 	/*
 	 * Keep preemption disabled until we are done with
 	 * softirq processing:
- 	 */
+	 */
 	preempt_count_sub(cnt - 1);
 
 	if (unlikely(!in_interrupt() && local_softirq_pending())) {
@@ -436,8 +435,7 @@ void open_softirq(int nr, void (*action)(struct softirq_action *))
 /*
  * Tasklets
  */
-struct tasklet_head
-{
+struct tasklet_head {
 	struct tasklet_struct *head;
 	struct tasklet_struct **tail;
 };
@@ -456,7 +454,6 @@ void __tasklet_schedule(struct tasklet_struct *t)
 	raise_softirq_irqoff(TASKLET_SOFTIRQ);
 	local_irq_restore(flags);
 }
-
 EXPORT_SYMBOL(__tasklet_schedule);
 
 void __tasklet_hi_schedule(struct tasklet_struct *t)
@@ -470,7 +467,6 @@ void __tasklet_hi_schedule(struct tasklet_struct *t)
 	raise_softirq_irqoff(HI_SOFTIRQ);
 	local_irq_restore(flags);
 }
-
 EXPORT_SYMBOL(__tasklet_hi_schedule);
 
 void __tasklet_hi_schedule_first(struct tasklet_struct *t)
@@ -481,7 +477,6 @@ void __tasklet_hi_schedule_first(struct tasklet_struct *t)
 	__this_cpu_write(tasklet_hi_vec.head, t);
 	__raise_softirq_irqoff(HI_SOFTIRQ);
 }
-
 EXPORT_SYMBOL(__tasklet_hi_schedule_first);
 
 static void tasklet_action(struct softirq_action *a)
@@ -501,7 +496,8 @@ static void tasklet_action(struct softirq_action *a)
 
 		if (tasklet_trylock(t)) {
 			if (!atomic_read(&t->count)) {
-				if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
+				if (!test_and_clear_bit(TASKLET_STATE_SCHED,
+							&t->state))
 					BUG();
 				t->func(t->data);
 				tasklet_unlock(t);
@@ -536,7 +532,8 @@ static void tasklet_hi_action(struct softirq_action *a)
 
 		if (tasklet_trylock(t)) {
 			if (!atomic_read(&t->count)) {
-				if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
+				if (!test_and_clear_bit(TASKLET_STATE_SCHED,
+							&t->state))
 					BUG();
 				t->func(t->data);
 				tasklet_unlock(t);
@@ -554,7 +551,6 @@ static void tasklet_hi_action(struct softirq_action *a)
 	}
 }
 
-
 void tasklet_init(struct tasklet_struct *t,
 		  void (*func)(unsigned long), unsigned long data)
 {
@@ -564,7 +560,6 @@ void tasklet_init(struct tasklet_struct *t,
 	t->func = func;
 	t->data = data;
 }
-
 EXPORT_SYMBOL(tasklet_init);
 
 void tasklet_kill(struct tasklet_struct *t)
@@ -580,7 +575,6 @@ void tasklet_kill(struct tasklet_struct *t)
 	tasklet_unlock_wait(t);
 	clear_bit(TASKLET_STATE_SCHED, &t->state);
 }
-
 EXPORT_SYMBOL(tasklet_kill);
 
 /*
@@ -730,9 +724,8 @@ static void takeover_tasklets(unsigned int cpu)
 }
 #endif /* CONFIG_HOTPLUG_CPU */
 
-static int cpu_callback(struct notifier_block *nfb,
-				  unsigned long action,
-				  void *hcpu)
+static int cpu_callback(struct notifier_block *nfb, unsigned long action,
+			void *hcpu)
 {
 	switch (action) {
 #ifdef CONFIG_HOTPLUG_CPU

commit 403227641533c4227d44d14f25c8f3676f6e7436
Author: Joe Perches <joe@perches.com>
Date:   Mon Jan 27 17:07:15 2014 -0800

    softirq: convert printks to pr_<level>
    
    Use a more current logging style.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index ba79bbfe380e..2674354a5cb8 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -8,6 +8,8 @@
  *	Rewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)
  */
 
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
 #include <linux/export.h>
 #include <linux/kernel_stat.h>
 #include <linux/interrupt.h>
@@ -269,7 +271,7 @@ asmlinkage void __do_softirq(void)
 		h->action(h);
 		trace_softirq_exit(vec_nr);
 		if (unlikely(prev_count != preempt_count())) {
-			printk(KERN_ERR "huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
+			pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
 			       vec_nr, softirq_to_name[vec_nr], h->action,
 			       prev_count, preempt_count());
 			preempt_count_set(prev_count);
@@ -568,7 +570,7 @@ EXPORT_SYMBOL(tasklet_init);
 void tasklet_kill(struct tasklet_struct *t)
 {
 	if (in_interrupt())
-		printk("Attempt to kill tasklet from interrupt\n");
+		pr_notice("Attempt to kill tasklet from interrupt\n");
 
 	while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
 		do {

commit 2e702b9f6cba4eb87d90e2a2d425a1fc05eec803
Author: Joe Perches <joe@perches.com>
Date:   Mon Jan 27 17:07:14 2014 -0800

    softirq: use ffs() in __do_softirq()
    
    Possible speed improvement of __do_softirq() by using ffs() instead of
    using a while loop with an & 1 test then single bit shift.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 8a1e6e104892..ba79bbfe380e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -229,6 +229,7 @@ asmlinkage void __do_softirq(void)
 	struct softirq_action *h;
 	bool in_hardirq;
 	__u32 pending;
+	int softirq_bit;
 	int cpu;
 
 	/*
@@ -253,30 +254,30 @@ asmlinkage void __do_softirq(void)
 
 	h = softirq_vec;
 
-	do {
-		if (pending & 1) {
-			unsigned int vec_nr = h - softirq_vec;
-			int prev_count = preempt_count();
-
-			kstat_incr_softirqs_this_cpu(vec_nr);
-
-			trace_softirq_entry(vec_nr);
-			h->action(h);
-			trace_softirq_exit(vec_nr);
-			if (unlikely(prev_count != preempt_count())) {
-				printk(KERN_ERR "huh, entered softirq %u %s %p"
-				       "with preempt_count %08x,"
-				       " exited with %08x?\n", vec_nr,
-				       softirq_to_name[vec_nr], h->action,
-				       prev_count, preempt_count());
-				preempt_count_set(prev_count);
-			}
+	while ((softirq_bit = ffs(pending))) {
+		unsigned int vec_nr;
+		int prev_count;
+
+		h += softirq_bit - 1;
+
+		vec_nr = h - softirq_vec;
+		prev_count = preempt_count();
 
-			rcu_bh_qs(cpu);
+		kstat_incr_softirqs_this_cpu(vec_nr);
+
+		trace_softirq_entry(vec_nr);
+		h->action(h);
+		trace_softirq_exit(vec_nr);
+		if (unlikely(prev_count != preempt_count())) {
+			printk(KERN_ERR "huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
+			       vec_nr, softirq_to_name[vec_nr], h->action,
+			       prev_count, preempt_count());
+			preempt_count_set(prev_count);
 		}
+		rcu_bh_qs(cpu);
 		h++;
-		pending >>= 1;
-	} while (pending);
+		pending >>= softirq_bit;
+	}
 
 	local_irq_disable();
 

commit a2b4c607c93a0850c8e3d90688cf3bd08576b986
Merge: 15c81026204d 8fe8ff09ce3b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Jan 25 08:27:26 2014 +0100

    Merge branch 'timers/core' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/linux-dynticks into timers/urgent
    
    Pull dynticks cleanups from Frederic Weisbecker.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6c6461435611e1d4843516f2d55e8316c009112e
Merge: a0fa1dd3cdbc 00e2bcd6d35f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 11:34:26 2014 -0800

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer changes from Ingo Molnar:
      - ARM clocksource/clockevent improvements and fixes
      - generic timekeeping updates: TAI fixes/improvements, cleanups
      - Posix cpu timer cleanups and improvements
      - dynticks updates: full dynticks bugfixes, optimizations and cleanups
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (46 commits)
      clocksource: Timer-sun5i: Switch to sched_clock_register()
      timekeeping: Remove comment that's mostly out of date
      rtc-cmos: Add an alarm disable quirk
      timekeeper: fix comment typo for tk_setup_internals()
      timekeeping: Fix missing timekeeping_update in suspend path
      timekeeping: Fix CLOCK_TAI timer/nanosleep delays
      tick/timekeeping: Call update_wall_time outside the jiffies lock
      timekeeping: Avoid possible deadlock from clock_was_set_delayed
      timekeeping: Fix potential lost pv notification of time change
      timekeeping: Fix lost updates to tai adjustment
      clocksource: sh_cmt: Add clk_prepare/unprepare support
      clocksource: bcm_kona_timer: Remove unused bcm_timer_ids
      clocksource: vt8500: Remove deprecated IRQF_DISABLED
      clocksource: tegra: Remove deprecated IRQF_DISABLED
      clocksource: misc drivers: Remove deprecated IRQF_DISABLED
      clocksource: sh_mtu2: Remove unnecessary platform_set_drvdata()
      clocksource: sh_tmu: Remove unnecessary platform_set_drvdata()
      clocksource: armada-370-xp: Enable timer divider only when needed
      clocksource: clksrc-of: Warn if no clock sources are found
      clocksource: orion: Switch to sched_clock_register()
      ...

commit a0fa1dd3cdbccec9597fe53b6177a9aa6e20f2f8
Merge: 9326657abe1a eaad45132c56
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 10:42:08 2014 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
    
     - Add the initial implementation of SCHED_DEADLINE support: a real-time
       scheduling policy where tasks that meet their deadlines and
       periodically execute their instances in less than their runtime quota
       see real-time scheduling and won't miss any of their deadlines.
       Tasks that go over their quota get delayed (Available to privileged
       users for now)
    
     - Clean up and fix preempt_enable_no_resched() abuse all around the
       tree
    
     - Do sched_clock() performance optimizations on x86 and elsewhere
    
     - Fix and improve auto-NUMA balancing
    
     - Fix and clean up the idle loop
    
     - Apply various cleanups and fixes
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (60 commits)
      sched: Fix __sched_setscheduler() nice test
      sched: Move SCHED_RESET_ON_FORK into attr::sched_flags
      sched: Fix up attr::sched_priority warning
      sched: Fix up scheduler syscall LTP fails
      sched: Preserve the nice level over sched_setscheduler() and sched_setparam() calls
      sched/core: Fix htmldocs warnings
      sched/deadline: No need to check p if dl_se is valid
      sched/deadline: Remove unused variables
      sched/deadline: Fix sparse static warnings
      m68k: Fix build warning in mac_via.h
      sched, thermal: Clean up preempt_enable_no_resched() abuse
      sched, net: Fixup busy_loop_us_clock()
      sched, net: Clean up preempt_enable_no_resched() abuse
      sched/preempt: Fix up missed PREEMPT_NEED_RESCHED folding
      sched/preempt, locking: Rework local_bh_{dis,en}able()
      sched/clock, x86: Avoid a runtime condition in native_sched_clock()
      sched/clock: Fix up clear_sched_clock_stable()
      sched/clock, x86: Use a static_key for sched_clock_stable
      sched/clock: Remove local_irq_disable() from the clocks
      sched/clock, x86: Rewrite cyc2ns() to avoid the need to disable IRQs
      ...

commit 5acac1be499d979e3aa463ea73a498888faefcbe
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 4 18:28:20 2013 +0100

    tick: Rename tick_check_idle() to tick_irq_enter()
    
    This makes the code more symetric against the existing tick functions
    called on irq exit: tick_irq_exit() and tick_nohz_irq_exit().
    
    These function are also symetric as they mirror each other's action:
    we start to account idle time on irq exit and we stop this accounting
    on irq entry. Also the tick is stopped on irq exit and timekeeping
    catches up with the tickless time elapsed until we reach irq entry.
    
    This rename was suggested by Peter Zijlstra a long while ago but it
    got forgotten in the mass.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Alex Shi <alex.shi@linaro.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Link: http://lkml.kernel.org/r/1387320692-28460-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 11348de09400..ca9cb35a96d4 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -318,7 +318,7 @@ void irq_enter(void)
 		 * here, as softirq will be serviced on return from interrupt.
 		 */
 		local_bh_disable();
-		tick_check_idle();
+		tick_irq_enter();
 		_local_bh_enable();
 	}
 

commit 0bd3a173d711857fc9f583eb5825386cc08f3948
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Nov 19 16:13:38 2013 +0100

    sched/preempt, locking: Rework local_bh_{dis,en}able()
    
    Currently local_bh_disable() is out-of-line for no apparent reason.
    So inline it to save a few cycles on call/return nonsense, the
    function body is a single add on x86 (a few loads and store extra on
    load/store archs).
    
    Also expose two new local_bh functions:
    
      __local_bh_{dis,en}able_ip(unsigned long ip, unsigned int cnt);
    
    Which implement the actual local_bh_{dis,en}able() behaviour.
    
    The next patch uses the exposed @cnt argument to optimize bh lock
    functions.
    
    With build fixes from Jacob Pan.
    
    Cc: rjw@rjwysocki.net
    Cc: rui.zhang@intel.com
    Cc: jacob.jun.pan@linux.intel.com
    Cc: Mike Galbraith <bitbucket@online.de>
    Cc: hpa@zytor.com
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: lenb@kernel.org
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20131119151338.GF3694@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 7500cce1ebfd..9e368ef35f9a 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -89,7 +89,7 @@ static void wakeup_softirqd(void)
  * where hardirqs are disabled legitimately:
  */
 #ifdef CONFIG_TRACE_IRQFLAGS
-static void __local_bh_disable(unsigned long ip, unsigned int cnt)
+void __local_bh_disable_ip(unsigned long ip, unsigned int cnt)
 {
 	unsigned long flags;
 
@@ -114,21 +114,9 @@ static void __local_bh_disable(unsigned long ip, unsigned int cnt)
 	if (preempt_count() == cnt)
 		trace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));
 }
-#else /* !CONFIG_TRACE_IRQFLAGS */
-static inline void __local_bh_disable(unsigned long ip, unsigned int cnt)
-{
-	preempt_count_add(cnt);
-	barrier();
-}
+EXPORT_SYMBOL(__local_bh_disable_ip);
 #endif /* CONFIG_TRACE_IRQFLAGS */
 
-void local_bh_disable(void)
-{
-	__local_bh_disable(_RET_IP_, SOFTIRQ_DISABLE_OFFSET);
-}
-
-EXPORT_SYMBOL(local_bh_disable);
-
 static void __local_bh_enable(unsigned int cnt)
 {
 	WARN_ON_ONCE(!irqs_disabled());
@@ -151,7 +139,7 @@ void _local_bh_enable(void)
 
 EXPORT_SYMBOL(_local_bh_enable);
 
-static inline void _local_bh_enable_ip(unsigned long ip)
+void __local_bh_enable_ip(unsigned long ip, unsigned int cnt)
 {
 	WARN_ON_ONCE(in_irq() || irqs_disabled());
 #ifdef CONFIG_TRACE_IRQFLAGS
@@ -166,7 +154,7 @@ static inline void _local_bh_enable_ip(unsigned long ip)
 	 * Keep preemption disabled until we are done with
 	 * softirq processing:
  	 */
-	preempt_count_sub(SOFTIRQ_DISABLE_OFFSET - 1);
+	preempt_count_sub(cnt - 1);
 
 	if (unlikely(!in_interrupt() && local_softirq_pending())) {
 		/*
@@ -182,18 +170,7 @@ static inline void _local_bh_enable_ip(unsigned long ip)
 #endif
 	preempt_check_resched();
 }
-
-void local_bh_enable(void)
-{
-	_local_bh_enable_ip(_RET_IP_);
-}
-EXPORT_SYMBOL(local_bh_enable);
-
-void local_bh_enable_ip(unsigned long ip)
-{
-	_local_bh_enable_ip(ip);
-}
-EXPORT_SYMBOL(local_bh_enable_ip);
+EXPORT_SYMBOL(__local_bh_enable_ip);
 
 /*
  * We restart softirq processing for at most MAX_SOFTIRQ_RESTART times,
@@ -230,7 +207,7 @@ asmlinkage void __do_softirq(void)
 	pending = local_softirq_pending();
 	account_irq_enter_time(current);
 
-	__local_bh_disable(_RET_IP_, SOFTIRQ_OFFSET);
+	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);
 	lockdep_softirq_enter();
 
 	cpu = smp_processor_id();

commit 9ea4c380066fbe23fe0da7f4abfabc444f2467f4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Nov 19 16:13:38 2013 +0100

    locking: Optimize lock_bh functions
    
    Currently all _bh_ lock functions do two preempt_count operations:
    
      local_bh_disable();
      preempt_disable();
    
    and for the unlock:
    
      preempt_enable_no_resched();
      local_bh_enable();
    
    Since its a waste of perfectly good cycles to modify the same variable
    twice when you can do it in one go; use the new
    __local_bh_{dis,en}able_ip() functions that allow us to provide a
    preempt_count value to add/sub.
    
    So define SOFTIRQ_LOCK_OFFSET as the offset a _bh_ lock needs to
    add/sub to be done in one go.
    
    As a bonus it gets rid of the preempt_enable_no_resched() usage.
    
    This reduces a 1000 loops of:
    
      spin_lock_bh(&bh_lock);
      spin_unlock_bh(&bh_lock);
    
    from 53596 cycles to 51995 cycles. I didn't do enough measurements to
    say for absolute sure that the result is significant but the the few
    runs I did for each suggest it is so.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: jacob.jun.pan@linux.intel.com
    Cc: Mike Galbraith <bitbucket@online.de>
    Cc: hpa@zytor.com
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: lenb@kernel.org
    Cc: rjw@rjwysocki.net
    Cc: rui.zhang@intel.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20131119151338.GF3694@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 11025ccc06dd..7500cce1ebfd 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -107,7 +107,7 @@ static void __local_bh_disable(unsigned long ip, unsigned int cnt)
 	/*
 	 * Were softirqs turned off above:
 	 */
-	if (softirq_count() == cnt)
+	if (softirq_count() == (cnt & SOFTIRQ_MASK))
 		trace_softirqs_off(ip);
 	raw_local_irq_restore(flags);
 
@@ -133,7 +133,7 @@ static void __local_bh_enable(unsigned int cnt)
 {
 	WARN_ON_ONCE(!irqs_disabled());
 
-	if (softirq_count() == cnt)
+	if (softirq_count() == (cnt & SOFTIRQ_MASK))
 		trace_softirqs_on(_RET_IP_);
 	preempt_count_sub(cnt);
 }

commit e8fcaa5c54e3b0371230e5d43a6f650c667da9c5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Aug 7 22:28:01 2013 +0200

    nohz: Convert a few places to use local per cpu accesses
    
    A few functions use remote per CPU access APIs when they
    deal with local values.
    
    Just do the right conversion to improve performance, code
    readability and debug checks.
    
    While at it, lets extend some of these function names with *_this_cpu()
    suffix in order to display their purpose more clearly.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 11025ccc06dd..11348de09400 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -311,8 +311,6 @@ asmlinkage void do_softirq(void)
  */
 void irq_enter(void)
 {
-	int cpu = smp_processor_id();
-
 	rcu_irq_enter();
 	if (is_idle_task(current) && !in_interrupt()) {
 		/*
@@ -320,7 +318,7 @@ void irq_enter(void)
 		 * here, as softirq will be serviced on return from interrupt.
 		 */
 		local_bh_disable();
-		tick_check_idle(cpu);
+		tick_check_idle();
 		_local_bh_enable();
 	}
 

commit 5c4853b60ca8ec3d989ce05a5e995d15c3ed52c0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 20 01:07:34 2013 +0100

    lockdep: Simplify a bit hardirq <-> softirq transitions
    
    Instead of saving the hardirq state on a per CPU variable, which require
    an explicit call before the softirq handling and some complication,
    just save and restore the hardirq tracing state through functions
    return values and parameters.
    
    It simplifies a bit the black magic that works around the fact that
    softirqs can be called from hardirqs while hardirqs can nest on softirqs
    but those two cases have very different semantics and only the latter
    case assume both states.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1384906054-30676-1-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index f84aa48c0e66..9a4500e4c189 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -213,40 +213,35 @@ EXPORT_SYMBOL(local_bh_enable_ip);
 
 #ifdef CONFIG_TRACE_IRQFLAGS
 /*
- * Convoluted means of passing __do_softirq() a message through the various
- * architecture execute_on_stack() bits.
- *
  * When we run softirqs from irq_exit() and thus on the hardirq stack we need
  * to keep the lockdep irq context tracking as tight as possible in order to
  * not miss-qualify lock contexts and miss possible deadlocks.
  */
-static DEFINE_PER_CPU(int, softirq_from_hardirq);
 
-static inline void lockdep_softirq_from_hardirq(void)
+static inline bool lockdep_softirq_start(void)
 {
-	this_cpu_write(softirq_from_hardirq, 1);
-}
+	bool in_hardirq = false;
 
-static inline void lockdep_softirq_start(void)
-{
-	if (this_cpu_read(softirq_from_hardirq))
+	if (trace_hardirq_context(current)) {
+		in_hardirq = true;
 		trace_hardirq_exit();
+	}
+
 	lockdep_softirq_enter();
+
+	return in_hardirq;
 }
 
-static inline void lockdep_softirq_end(void)
+static inline void lockdep_softirq_end(bool in_hardirq)
 {
 	lockdep_softirq_exit();
-	if (this_cpu_read(softirq_from_hardirq)) {
-		this_cpu_write(softirq_from_hardirq, 0);
+
+	if (in_hardirq)
 		trace_hardirq_enter();
-	}
 }
-
 #else
-static inline void lockdep_softirq_from_hardirq(void) { }
-static inline void lockdep_softirq_start(void) { }
-static inline void lockdep_softirq_end(void) { }
+static inline bool lockdep_softirq_start(void) { return false; }
+static inline void lockdep_softirq_end(bool in_hardirq) { }
 #endif
 
 asmlinkage void __do_softirq(void)
@@ -255,6 +250,7 @@ asmlinkage void __do_softirq(void)
 	unsigned long old_flags = current->flags;
 	int max_restart = MAX_SOFTIRQ_RESTART;
 	struct softirq_action *h;
+	bool in_hardirq;
 	__u32 pending;
 	int cpu;
 
@@ -269,7 +265,7 @@ asmlinkage void __do_softirq(void)
 	account_irq_enter_time(current);
 
 	__local_bh_disable(_RET_IP_, SOFTIRQ_OFFSET);
-	lockdep_softirq_start();
+	in_hardirq = lockdep_softirq_start();
 
 	cpu = smp_processor_id();
 restart:
@@ -316,7 +312,7 @@ asmlinkage void __do_softirq(void)
 		wakeup_softirqd();
 	}
 
-	lockdep_softirq_end();
+	lockdep_softirq_end(in_hardirq);
 	account_irq_exit_time(current);
 	__local_bh_enable(SOFTIRQ_OFFSET);
 	WARN_ON_ONCE(in_interrupt());
@@ -365,7 +361,6 @@ void irq_enter(void)
 static inline void invoke_softirq(void)
 {
 	if (!force_irqthreads) {
-		lockdep_softirq_from_hardirq();
 #ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
 		/*
 		 * We can safely execute softirq on the current stack if

commit 7d5b15831039837f3551bf40f188385ed8ad5e16
Merge: e0edc78f25c0 f1a83e652bed
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Nov 27 11:09:19 2013 +0100

    Merge branch 'core/urgent' into core/locking
    
    Prepare for dependent patch.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f1a83e652bedef88d6d77d3dc58250e08e7062bd
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Nov 19 16:42:47 2013 +0100

    lockdep: Correctly annotate hardirq context in irq_exit()
    
    There was a reported deadlock on -rt which lockdep didn't report.
    
    It turns out that in irq_exit() we tell lockdep that the hardirq
    context ends and then do all kinds of locking afterwards.
    
    To fix it, move trace_hardirq_exit() to the very end of irq_exit(), this
    ensures all locking in tick_irq_exit() and rcu_irq_exit() are properly
    recorded as happening from hardirq context.
    
    This however leads to the 'fun' little problem of running softirqs
    while in hardirq context. To cure this make the softirq code a little
    more complex (in the CONFIG_TRACE_IRQFLAGS case).
    
    Due to stack swizzling arch dependent trickery we cannot pass an
    argument to __do_softirq() to tell it if it was done from hardirq
    context or not; so use a side-band argument.
    
    When we do __do_softirq() from hardirq context, 'atomically' flip to
    softirq context and back, so that no locking goes without being in
    either hard- or soft-irq context.
    
    I didn't find any new problems in mainline using this patch, but it
    did show the -rt problem.
    
    Reported-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-dgwc5cdksbn0jk09vbmcc9sa@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b24988353458..eb0acf44b063 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -213,14 +213,52 @@ EXPORT_SYMBOL(local_bh_enable_ip);
 #define MAX_SOFTIRQ_TIME  msecs_to_jiffies(2)
 #define MAX_SOFTIRQ_RESTART 10
 
+#ifdef CONFIG_TRACE_IRQFLAGS
+/*
+ * Convoluted means of passing __do_softirq() a message through the various
+ * architecture execute_on_stack() bits.
+ *
+ * When we run softirqs from irq_exit() and thus on the hardirq stack we need
+ * to keep the lockdep irq context tracking as tight as possible in order to
+ * not miss-qualify lock contexts and miss possible deadlocks.
+ */
+static DEFINE_PER_CPU(int, softirq_from_hardirq);
+
+static inline void lockdep_softirq_from_hardirq(void)
+{
+	this_cpu_write(softirq_from_hardirq, 1);
+}
+
+static inline void lockdep_softirq_start(void)
+{
+	if (this_cpu_read(softirq_from_hardirq))
+		trace_hardirq_exit();
+	lockdep_softirq_enter();
+}
+
+static inline void lockdep_softirq_end(void)
+{
+	lockdep_softirq_exit();
+	if (this_cpu_read(softirq_from_hardirq)) {
+		this_cpu_write(softirq_from_hardirq, 0);
+		trace_hardirq_enter();
+	}
+}
+
+#else
+static inline void lockdep_softirq_from_hardirq(void) { }
+static inline void lockdep_softirq_start(void) { }
+static inline void lockdep_softirq_end(void) { }
+#endif
+
 asmlinkage void __do_softirq(void)
 {
-	struct softirq_action *h;
-	__u32 pending;
 	unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
-	int cpu;
 	unsigned long old_flags = current->flags;
 	int max_restart = MAX_SOFTIRQ_RESTART;
+	struct softirq_action *h;
+	__u32 pending;
+	int cpu;
 
 	/*
 	 * Mask out PF_MEMALLOC s current task context is borrowed for the
@@ -233,7 +271,7 @@ asmlinkage void __do_softirq(void)
 	account_irq_enter_time(current);
 
 	__local_bh_disable(_RET_IP_, SOFTIRQ_OFFSET);
-	lockdep_softirq_enter();
+	lockdep_softirq_start();
 
 	cpu = smp_processor_id();
 restart:
@@ -280,16 +318,13 @@ asmlinkage void __do_softirq(void)
 		wakeup_softirqd();
 	}
 
-	lockdep_softirq_exit();
-
+	lockdep_softirq_end();
 	account_irq_exit_time(current);
 	__local_bh_enable(SOFTIRQ_OFFSET);
 	WARN_ON_ONCE(in_interrupt());
 	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
 }
 
-
-
 asmlinkage void do_softirq(void)
 {
 	__u32 pending;
@@ -332,6 +367,7 @@ void irq_enter(void)
 static inline void invoke_softirq(void)
 {
 	if (!force_irqthreads) {
+		lockdep_softirq_from_hardirq();
 #ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
 		/*
 		 * We can safely execute softirq on the current stack if
@@ -377,13 +413,13 @@ void irq_exit(void)
 #endif
 
 	account_irq_exit_time(current);
-	trace_hardirq_exit();
 	preempt_count_sub(HARDIRQ_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
 
 	tick_irq_exit();
 	rcu_irq_exit();
+	trace_hardirq_exit(); /* must be last! */
 }
 
 /*

commit fc21c0cff2f425891b28ff6fb6b03b325c977428
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Nov 14 14:32:06 2013 -0800

    revert "softirq: Add support for triggering softirq work on softirqs"
    
    This commit was incomplete in that code to remove items from the per-cpu
    lists was missing and never acquired a user in the 5 years it has been in
    the tree.  We're going to implement what it seems to try to archive in a
    simpler way, and this code is in the way of doing so.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b24988353458..11025ccc06dd 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -6,8 +6,6 @@
  *	Distribute under GPLv2.
  *
  *	Rewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)
- *
- *	Remote softirq infrastructure is by Jens Axboe.
  */
 
 #include <linux/export.h>
@@ -627,146 +625,17 @@ void tasklet_hrtimer_init(struct tasklet_hrtimer *ttimer,
 }
 EXPORT_SYMBOL_GPL(tasklet_hrtimer_init);
 
-/*
- * Remote softirq bits
- */
-
-DEFINE_PER_CPU(struct list_head [NR_SOFTIRQS], softirq_work_list);
-EXPORT_PER_CPU_SYMBOL(softirq_work_list);
-
-static void __local_trigger(struct call_single_data *cp, int softirq)
-{
-	struct list_head *head = &__get_cpu_var(softirq_work_list[softirq]);
-
-	list_add_tail(&cp->list, head);
-
-	/* Trigger the softirq only if the list was previously empty.  */
-	if (head->next == &cp->list)
-		raise_softirq_irqoff(softirq);
-}
-
-#ifdef CONFIG_USE_GENERIC_SMP_HELPERS
-static void remote_softirq_receive(void *data)
-{
-	struct call_single_data *cp = data;
-	unsigned long flags;
-	int softirq;
-
-	softirq = *(int *)cp->info;
-	local_irq_save(flags);
-	__local_trigger(cp, softirq);
-	local_irq_restore(flags);
-}
-
-static int __try_remote_softirq(struct call_single_data *cp, int cpu, int softirq)
-{
-	if (cpu_online(cpu)) {
-		cp->func = remote_softirq_receive;
-		cp->info = &softirq;
-		cp->flags = 0;
-
-		__smp_call_function_single(cpu, cp, 0);
-		return 0;
-	}
-	return 1;
-}
-#else /* CONFIG_USE_GENERIC_SMP_HELPERS */
-static int __try_remote_softirq(struct call_single_data *cp, int cpu, int softirq)
-{
-	return 1;
-}
-#endif
-
-/**
- * __send_remote_softirq - try to schedule softirq work on a remote cpu
- * @cp: private SMP call function data area
- * @cpu: the remote cpu
- * @this_cpu: the currently executing cpu
- * @softirq: the softirq for the work
- *
- * Attempt to schedule softirq work on a remote cpu.  If this cannot be
- * done, the work is instead queued up on the local cpu.
- *
- * Interrupts must be disabled.
- */
-void __send_remote_softirq(struct call_single_data *cp, int cpu, int this_cpu, int softirq)
-{
-	if (cpu == this_cpu || __try_remote_softirq(cp, cpu, softirq))
-		__local_trigger(cp, softirq);
-}
-EXPORT_SYMBOL(__send_remote_softirq);
-
-/**
- * send_remote_softirq - try to schedule softirq work on a remote cpu
- * @cp: private SMP call function data area
- * @cpu: the remote cpu
- * @softirq: the softirq for the work
- *
- * Like __send_remote_softirq except that disabling interrupts and
- * computing the current cpu is done for the caller.
- */
-void send_remote_softirq(struct call_single_data *cp, int cpu, int softirq)
-{
-	unsigned long flags;
-	int this_cpu;
-
-	local_irq_save(flags);
-	this_cpu = smp_processor_id();
-	__send_remote_softirq(cp, cpu, this_cpu, softirq);
-	local_irq_restore(flags);
-}
-EXPORT_SYMBOL(send_remote_softirq);
-
-static int remote_softirq_cpu_notify(struct notifier_block *self,
-					       unsigned long action, void *hcpu)
-{
-	/*
-	 * If a CPU goes away, splice its entries to the current CPU
-	 * and trigger a run of the softirq
-	 */
-	if (action == CPU_DEAD || action == CPU_DEAD_FROZEN) {
-		int cpu = (unsigned long) hcpu;
-		int i;
-
-		local_irq_disable();
-		for (i = 0; i < NR_SOFTIRQS; i++) {
-			struct list_head *head = &per_cpu(softirq_work_list[i], cpu);
-			struct list_head *local_head;
-
-			if (list_empty(head))
-				continue;
-
-			local_head = &__get_cpu_var(softirq_work_list[i]);
-			list_splice_init(head, local_head);
-			raise_softirq_irqoff(i);
-		}
-		local_irq_enable();
-	}
-
-	return NOTIFY_OK;
-}
-
-static struct notifier_block remote_softirq_cpu_notifier = {
-	.notifier_call	= remote_softirq_cpu_notify,
-};
-
 void __init softirq_init(void)
 {
 	int cpu;
 
 	for_each_possible_cpu(cpu) {
-		int i;
-
 		per_cpu(tasklet_vec, cpu).tail =
 			&per_cpu(tasklet_vec, cpu).head;
 		per_cpu(tasklet_hi_vec, cpu).tail =
 			&per_cpu(tasklet_hi_vec, cpu).head;
-		for (i = 0; i < NR_SOFTIRQS; i++)
-			INIT_LIST_HEAD(&per_cpu(softirq_work_list[i], cpu));
 	}
 
-	register_hotcpu_notifier(&remote_softirq_cpu_notifier);
-
 	open_softirq(TASKLET_SOFTIRQ, tasklet_action);
 	open_softirq(HI_SOFTIRQ, tasklet_hi_action);
 }

commit 39cf275a1a18ba3c7eb9b986c5c9b35b57332798
Merge: ad5d69899e52 e5137b50a064
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 12 10:20:12 2013 +0900

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The main changes in this cycle are:
    
       - (much) improved CONFIG_NUMA_BALANCING support from Mel Gorman, Rik
         van Riel, Peter Zijlstra et al.  Yay!
    
       - optimize preemption counter handling: merge the NEED_RESCHED flag
         into the preempt_count variable, by Peter Zijlstra.
    
       - wait.h fixes and code reorganization from Peter Zijlstra
    
       - cfs_bandwidth fixes from Ben Segall
    
       - SMP load-balancer cleanups from Peter Zijstra
    
       - idle balancer improvements from Jason Low
    
       - other fixes and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (129 commits)
      ftrace, sched: Add TRACE_FLAG_PREEMPT_RESCHED
      stop_machine: Fix race between stop_two_cpus() and stop_cpus()
      sched: Remove unnecessary iteration over sched domains to update nr_busy_cpus
      sched: Fix asymmetric scheduling for POWER7
      sched: Move completion code from core.c to completion.c
      sched: Move wait code from core.c to wait.c
      sched: Move wait.c into kernel/sched/
      sched/wait: Fix __wait_event_interruptible_lock_irq_timeout()
      sched: Avoid throttle_cfs_rq() racing with period_timer stopping
      sched: Guarantee new group-entities always have weight
      sched: Fix hrtimer_cancel()/rq->lock deadlock
      sched: Fix cfs_bandwidth misuse of hrtimer_expires_remaining
      sched: Fix race on toggling cfs_bandwidth_used
      sched: Remove extra put_online_cpus() inside sched_setaffinity()
      sched/rt: Fix task_tick_rt() comment
      sched/wait: Fix build breakage
      sched/wait: Introduce prepare_to_wait_event()
      sched/wait: Add ___wait_cond_timeout() to wait_event*_timeout() too
      sched: Remove get_online_cpus() usage
      sched: Fix race in migrate_swap_stop()
      ...

commit 37bf06375c90a42fe07b9bebdb07bc316ae5a0ce
Merge: 6bfa687c19b7 d0e639c9e06d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Oct 9 12:36:13 2013 +0200

    Merge tag 'v3.12-rc4' into sched/core
    
    Merge Linux v3.12-rc4 to fix a conflict and also to refresh the tree
    before applying more scheduler patches.
    
    Conflicts:
            arch/avr32/include/asm/Kbuild
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit cc1f027454929924471bea2f362431072e3c71be
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Sep 24 17:17:47 2013 +0200

    irq: Optimize softirq stack selection in irq exit
    
    If irq_exit() is called on the arch's specified irq stack,
    it should be safe to run softirqs inline under that same
    irq stack as it is near empty by the time we call irq_exit().
    
    For example if we use the same stack for both hard and soft irqs here,
    the worst case scenario is:
    hardirq -> softirq -> hardirq. But then the softirq supersedes the
    first hardirq as the stack user since irq_exit() is called in
    a mostly empty stack. So the stack merge in this case looks acceptable.
    
    Stack overrun still have a chance to happen if hardirqs have more
    opportunities to nest, but then it's another problem to solve.
    
    So lets adapt the irq exit's softirq stack on top of a new Kconfig symbol
    that can be defined when irq_exit() runs on the irq stack. That way
    we can spare some stack switch on irq processing and all the cache
    issues that come along.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 2b4328ea769f..dacd0ab51df4 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -332,15 +332,21 @@ void irq_enter(void)
 static inline void invoke_softirq(void)
 {
 	if (!force_irqthreads) {
+#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK
 		/*
 		 * We can safely execute softirq on the current stack if
 		 * it is the irq stack, because it should be near empty
-		 * at this stage. But we have no way to know if the arch
-		 * calls irq_exit() on the irq stack. So call softirq
-		 * in its own stack to prevent from any overrun on top
-		 * of a potentially deep task stack.
+		 * at this stage.
+		 */
+		__do_softirq();
+#else
+		/*
+		 * Otherwise, irq_exit() is called on the task stack that can
+		 * be potentially deep already. So call softirq in its own stack
+		 * to prevent from any overrun.
 		 */
 		do_softirq_own_stack();
+#endif
 	} else {
 		wakeup_softirqd();
 	}

commit 0bed698a334766ed07bacd6cb33f0228003a7f61
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 5 16:14:00 2013 +0200

    irq: Justify the various softirq stack choices
    
    For clarity, comment the various stack choices for softirqs
    processing, whether we execute them from ksoftirqd or
    local_irq_enable() calls.
    
    Their use on irq_exit() is already commented.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 9f8092b82a94..2b4328ea769f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -170,8 +170,13 @@ static inline void _local_bh_enable_ip(unsigned long ip)
  	 */
 	sub_preempt_count(SOFTIRQ_DISABLE_OFFSET - 1);
 
-	if (unlikely(!in_interrupt() && local_softirq_pending()))
+	if (unlikely(!in_interrupt() && local_softirq_pending())) {
+		/*
+		 * Run softirq if any pending. And do it in its own stack
+		 * as we may be calling this deep in a task call stack already.
+		 */
 		do_softirq();
+	}
 
 	dec_preempt_count();
 #ifdef CONFIG_TRACE_IRQFLAGS
@@ -769,6 +774,10 @@ static void run_ksoftirqd(unsigned int cpu)
 {
 	local_irq_disable();
 	if (local_softirq_pending()) {
+		/*
+		 * We can safely run softirq on inline stack, as we are not deep
+		 * in the task stack here.
+		 */
 		__do_softirq();
 		rcu_note_context_switch(cpu);
 		local_irq_enable();

commit 5d60d3e7c08a46643e902e39d9743cf394382151
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Sep 24 04:11:35 2013 +0200

    irq: Improve a bit softirq debugging
    
    do_softirq() has a debug check that verifies that it is not nesting
    on softirqs processing, nor miscounting the softirq part of the preempt
    count.
    
    But making sure that softirqs processing don't nest is actually a more
    generic concern that applies to any caller of __do_softirq().
    
    Do take it one step further and generalize that debug check to
    any softirq processing.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 17c5cd2e3dae..9f8092b82a94 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -133,7 +133,6 @@ EXPORT_SYMBOL(local_bh_disable);
 
 static void __local_bh_enable(unsigned int cnt)
 {
-	WARN_ON_ONCE(in_irq());
 	WARN_ON_ONCE(!irqs_disabled());
 
 	if (softirq_count() == cnt)
@@ -148,6 +147,7 @@ static void __local_bh_enable(unsigned int cnt)
  */
 void _local_bh_enable(void)
 {
+	WARN_ON_ONCE(in_irq());
 	__local_bh_enable(SOFTIRQ_DISABLE_OFFSET);
 }
 
@@ -279,6 +279,7 @@ asmlinkage void __do_softirq(void)
 
 	account_irq_exit_time(current);
 	__local_bh_enable(SOFTIRQ_OFFSET);
+	WARN_ON_ONCE(in_interrupt());
 	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
 }
 
@@ -299,7 +300,6 @@ asmlinkage void do_softirq(void)
 	if (pending)
 		do_softirq_own_stack();
 
-	WARN_ON_ONCE(softirq_count());
 	local_irq_restore(flags);
 }
 

commit be6e1016440860fc4ec098b2d0aed3d0397b5d6e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Sep 24 16:39:41 2013 +0200

    irq: Optimize call to softirq on hardirq exit
    
    Before processing softirqs on hardirq exit, we already
    do the check for pending softirqs while hardirqs are
    guaranteed to be disabled.
    
    So we can take a shortcut and safely jump to the arch
    specific implementation directly.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 26ee72725d29..17c5cd2e3dae 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -335,7 +335,7 @@ static inline void invoke_softirq(void)
 		 * in its own stack to prevent from any overrun on top
 		 * of a potentially deep task stack.
 		 */
-		do_softirq();
+		do_softirq_own_stack();
 	} else {
 		wakeup_softirqd();
 	}

commit 7d65f4a6553203da6a22097821d151fbbe7e4956
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 5 15:49:45 2013 +0200

    irq: Consolidate do_softirq() arch overriden implementations
    
    All arch overriden implementations of do_softirq() share the following
    common code: disable irqs (to avoid races with the pending check),
    check if there are softirqs pending, then execute __do_softirq() on
    a specific stack.
    
    Consolidate the common parts such that archs only worry about the
    stack switch.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index d7d498d8cc4f..26ee72725d29 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -29,7 +29,6 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/irq.h>
 
-#include <asm/irq.h>
 /*
    - No shared variables, all the data are CPU local.
    - If a softirq needs serialization, let it serialize itself
@@ -283,7 +282,7 @@ asmlinkage void __do_softirq(void)
 	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
 }
 
-#ifndef __ARCH_HAS_DO_SOFTIRQ
+
 
 asmlinkage void do_softirq(void)
 {
@@ -298,13 +297,12 @@ asmlinkage void do_softirq(void)
 	pending = local_softirq_pending();
 
 	if (pending)
-		__do_softirq();
+		do_softirq_own_stack();
 
+	WARN_ON_ONCE(softirq_count());
 	local_irq_restore(flags);
 }
 
-#endif
-
 /*
  * Enter an interrupt context.
  */

commit ded797547548a5b8e7b92383a41e4c0e6b0ecb7f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Sep 24 00:50:25 2013 +0200

    irq: Force hardirq exit's softirq processing on its own stack
    
    The commit facd8b80c67a3cf64a467c4a2ac5fb31f2e6745b
    ("irq: Sanitize invoke_softirq") converted irq exit
    calls of do_softirq() to __do_softirq() on all architectures,
    assuming it was only used there for its irq disablement
    properties.
    
    But as a side effect, the softirqs processed in the end
    of the hardirq are always called on the inline current
    stack that is used by irq_exit() instead of the softirq
    stack provided by the archs that override do_softirq().
    
    The result is mostly safe if the architecture runs irq_exit()
    on a separate irq stack because then softirqs are processed
    on that same stack that is near empty at this stage (assuming
    hardirq aren't nesting).
    
    Otherwise irq_exit() runs in the task stack and so does the softirq
    too. The interrupted call stack can be randomly deep already and
    the softirq can dig through it even further. To add insult to the
    injury, this softirq can be interrupted by a new hardirq, maximizing
    the chances for a stack overrun as reported in powerpc for example:
    
            do_IRQ: stack overflow: 1920
            CPU: 0 PID: 1602 Comm: qemu-system-ppc Not tainted 3.10.4-300.1.fc19.ppc64p7 #1
            Call Trace:
            [c0000000050a8740] .show_stack+0x130/0x200 (unreliable)
            [c0000000050a8810] .dump_stack+0x28/0x3c
            [c0000000050a8880] .do_IRQ+0x2b8/0x2c0
            [c0000000050a8930] hardware_interrupt_common+0x154/0x180
            --- Exception: 501 at .cp_start_xmit+0x3a4/0x820 [8139cp]
                    LR = .cp_start_xmit+0x390/0x820 [8139cp]
            [c0000000050a8d40] .dev_hard_start_xmit+0x394/0x640
            [c0000000050a8e00] .sch_direct_xmit+0x110/0x260
            [c0000000050a8ea0] .dev_queue_xmit+0x260/0x630
            [c0000000050a8f40] .br_dev_queue_push_xmit+0xc4/0x130 [bridge]
            [c0000000050a8fc0] .br_dev_xmit+0x198/0x270 [bridge]
            [c0000000050a9070] .dev_hard_start_xmit+0x394/0x640
            [c0000000050a9130] .dev_queue_xmit+0x428/0x630
            [c0000000050a91d0] .ip_finish_output+0x2a4/0x550
            [c0000000050a9290] .ip_local_out+0x50/0x70
            [c0000000050a9310] .ip_queue_xmit+0x148/0x420
            [c0000000050a93b0] .tcp_transmit_skb+0x4e4/0xaf0
            [c0000000050a94a0] .__tcp_ack_snd_check+0x7c/0xf0
            [c0000000050a9520] .tcp_rcv_established+0x1e8/0x930
            [c0000000050a95f0] .tcp_v4_do_rcv+0x21c/0x570
            [c0000000050a96c0] .tcp_v4_rcv+0x734/0x930
            [c0000000050a97a0] .ip_local_deliver_finish+0x184/0x360
            [c0000000050a9840] .ip_rcv_finish+0x148/0x400
            [c0000000050a98d0] .__netif_receive_skb_core+0x4f8/0xb00
            [c0000000050a99d0] .netif_receive_skb+0x44/0x110
            [c0000000050a9a70] .br_handle_frame_finish+0x2bc/0x3f0 [bridge]
            [c0000000050a9b20] .br_nf_pre_routing_finish+0x2ac/0x420 [bridge]
            [c0000000050a9bd0] .br_nf_pre_routing+0x4dc/0x7d0 [bridge]
            [c0000000050a9c70] .nf_iterate+0x114/0x130
            [c0000000050a9d30] .nf_hook_slow+0xb4/0x1e0
            [c0000000050a9e00] .br_handle_frame+0x290/0x330 [bridge]
            [c0000000050a9ea0] .__netif_receive_skb_core+0x34c/0xb00
            [c0000000050a9fa0] .netif_receive_skb+0x44/0x110
            [c0000000050aa040] .napi_gro_receive+0xe8/0x120
            [c0000000050aa0c0] .cp_rx_poll+0x31c/0x590 [8139cp]
            [c0000000050aa1d0] .net_rx_action+0x1dc/0x310
            [c0000000050aa2b0] .__do_softirq+0x158/0x330
            [c0000000050aa3b0] .irq_exit+0xc8/0x110
            [c0000000050aa430] .do_IRQ+0xdc/0x2c0
            [c0000000050aa4e0] hardware_interrupt_common+0x154/0x180
             --- Exception: 501 at .bad_range+0x1c/0x110
                     LR = .get_page_from_freelist+0x908/0xbb0
            [c0000000050aa7d0] .list_del+0x18/0x50 (unreliable)
            [c0000000050aa850] .get_page_from_freelist+0x908/0xbb0
            [c0000000050aa9e0] .__alloc_pages_nodemask+0x21c/0xae0
            [c0000000050aaba0] .alloc_pages_vma+0xd0/0x210
            [c0000000050aac60] .handle_pte_fault+0x814/0xb70
            [c0000000050aad50] .__get_user_pages+0x1a4/0x640
            [c0000000050aae60] .get_user_pages_fast+0xec/0x160
            [c0000000050aaf10] .__gfn_to_pfn_memslot+0x3b0/0x430 [kvm]
            [c0000000050aafd0] .kvmppc_gfn_to_pfn+0x64/0x130 [kvm]
            [c0000000050ab070] .kvmppc_mmu_map_page+0x94/0x530 [kvm]
            [c0000000050ab190] .kvmppc_handle_pagefault+0x174/0x610 [kvm]
            [c0000000050ab270] .kvmppc_handle_exit_pr+0x464/0x9b0 [kvm]
            [c0000000050ab320]  kvm_start_lightweight+0x1ec/0x1fc [kvm]
            [c0000000050ab4f0] .kvmppc_vcpu_run_pr+0x168/0x3b0 [kvm]
            [c0000000050ab9c0] .kvmppc_vcpu_run+0xc8/0xf0 [kvm]
            [c0000000050aba50] .kvm_arch_vcpu_ioctl_run+0x5c/0x1a0 [kvm]
            [c0000000050abae0] .kvm_vcpu_ioctl+0x478/0x730 [kvm]
            [c0000000050abc90] .do_vfs_ioctl+0x4ec/0x7c0
            [c0000000050abd80] .SyS_ioctl+0xd4/0xf0
            [c0000000050abe30] syscall_exit+0x0/0x98
    
    Since this is a regression, this patch proposes a minimalistic
    and low-risk solution by blindly forcing the hardirq exit processing of
    softirqs on the softirq stack. This way we should reduce significantly
    the opportunities for task stack overflow dug by softirqs.
    
    Longer term solutions may involve extending the hardirq stack coverage to
    irq_exit(), etc...
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: #3.9.. <stable@vger.kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Mackerras <paulus@au1.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 53cc09ceb0b8..d7d498d8cc4f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -328,10 +328,19 @@ void irq_enter(void)
 
 static inline void invoke_softirq(void)
 {
-	if (!force_irqthreads)
-		__do_softirq();
-	else
+	if (!force_irqthreads) {
+		/*
+		 * We can safely execute softirq on the current stack if
+		 * it is the irq stack, because it should be near empty
+		 * at this stage. But we have no way to know if the arch
+		 * calls irq_exit() on the irq stack. So call softirq
+		 * in its own stack to prevent from any overrun on top
+		 * of a potentially deep task stack.
+		 */
+		do_softirq();
+	} else {
 		wakeup_softirqd();
+	}
 }
 
 static inline void tick_irq_exit(void)

commit bdb43806589096ac4272fe1307e789846ac08d7c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 10 12:15:23 2013 +0200

    sched: Extract the basic add/sub preempt_count modifiers
    
    Rewrite the preempt_count macros in order to extract the 3 basic
    preempt_count value modifiers:
    
      __preempt_count_add()
      __preempt_count_sub()
    
    and the new:
    
      __preempt_count_dec_and_test()
    
    And since we're at it anyway, replace the unconventional
    $op_preempt_count names with the more conventional preempt_count_$op.
    
    Since these basic operators are equivalent to the previous _notrace()
    variants, do away with the _notrace() versions.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-ewbpdbupy9xpsjhg960zwbv8@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index a90de70cf1f3..3e88612fc87e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -100,13 +100,13 @@ static void __local_bh_disable(unsigned long ip, unsigned int cnt)
 
 	raw_local_irq_save(flags);
 	/*
-	 * The preempt tracer hooks into add_preempt_count and will break
+	 * The preempt tracer hooks into preempt_count_add and will break
 	 * lockdep because it calls back into lockdep after SOFTIRQ_OFFSET
 	 * is set and before current->softirq_enabled is cleared.
 	 * We must manually increment preempt_count here and manually
 	 * call the trace_preempt_off later.
 	 */
-	add_preempt_count_notrace(cnt);
+	__preempt_count_add(cnt);
 	/*
 	 * Were softirqs turned off above:
 	 */
@@ -120,7 +120,7 @@ static void __local_bh_disable(unsigned long ip, unsigned int cnt)
 #else /* !CONFIG_TRACE_IRQFLAGS */
 static inline void __local_bh_disable(unsigned long ip, unsigned int cnt)
 {
-	add_preempt_count(cnt);
+	preempt_count_add(cnt);
 	barrier();
 }
 #endif /* CONFIG_TRACE_IRQFLAGS */
@@ -139,7 +139,7 @@ static void __local_bh_enable(unsigned int cnt)
 
 	if (softirq_count() == cnt)
 		trace_softirqs_on(_RET_IP_);
-	sub_preempt_count(cnt);
+	preempt_count_sub(cnt);
 }
 
 /*
@@ -169,12 +169,12 @@ static inline void _local_bh_enable_ip(unsigned long ip)
 	 * Keep preemption disabled until we are done with
 	 * softirq processing:
  	 */
-	sub_preempt_count(SOFTIRQ_DISABLE_OFFSET - 1);
+	preempt_count_sub(SOFTIRQ_DISABLE_OFFSET - 1);
 
 	if (unlikely(!in_interrupt() && local_softirq_pending()))
 		do_softirq();
 
-	dec_preempt_count();
+	preempt_count_dec();
 #ifdef CONFIG_TRACE_IRQFLAGS
 	local_irq_enable();
 #endif
@@ -360,7 +360,7 @@ void irq_exit(void)
 
 	account_irq_exit_time(current);
 	trace_hardirq_exit();
-	sub_preempt_count(HARDIRQ_OFFSET);
+	preempt_count_sub(HARDIRQ_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
 

commit 4a2b4b222743bb07fedf985b884550f2ca067ea9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Aug 14 14:55:24 2013 +0200

    sched: Introduce preempt_count accessor functions
    
    Replace the single preempt_count() 'function' that's an lvalue with
    two proper functions:
    
     preempt_count() - returns the preempt_count value as rvalue
     preempt_count_set() - Allows setting the preempt-count value
    
    Also provide preempt_count_ptr() as a convenience wrapper to implement
    all modifying operations.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-orxrbycjozopqfhb4dxdkdvb@git.kernel.org
    [ Fixed build failure. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 53cc09ceb0b8..a90de70cf1f3 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -106,7 +106,7 @@ static void __local_bh_disable(unsigned long ip, unsigned int cnt)
 	 * We must manually increment preempt_count here and manually
 	 * call the trace_preempt_off later.
 	 */
-	preempt_count() += cnt;
+	add_preempt_count_notrace(cnt);
 	/*
 	 * Were softirqs turned off above:
 	 */
@@ -256,7 +256,7 @@ asmlinkage void __do_softirq(void)
 				       " exited with %08x?\n", vec_nr,
 				       softirq_to_name[vec_nr], h->action,
 				       prev_count, preempt_count());
-				preempt_count() = prev_count;
+				preempt_count_set(prev_count);
 			}
 
 			rcu_bh_qs(cpu);

commit 0244ad004a54e39308d495fee0a2e637f8b5c317
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Aug 30 09:39:53 2013 +0200

    Remove GENERIC_HARDIRQ config option
    
    After the last architecture switched to generic hard irqs the config
    options HAVE_GENERIC_HARDIRQS & GENERIC_HARDIRQS and the related code
    for !CONFIG_GENERIC_HARDIRQS can be removed.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index be3d3514c325..53cc09ceb0b8 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -876,7 +876,6 @@ int __init __weak early_irq_init(void)
 	return 0;
 }
 
-#ifdef CONFIG_GENERIC_HARDIRQS
 int __init __weak arch_probe_nr_irqs(void)
 {
 	return NR_IRQS_LEGACY;
@@ -886,4 +885,3 @@ int __init __weak arch_early_irq_init(void)
 {
 	return 0;
 }
-#endif

commit 0db0628d90125193280eabb501c94feaf48fa9ab
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jun 19 14:53:51 2013 -0400

    kernel: delete __cpuinit usage from all core kernel files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    This removes all the uses of the __cpuinit macros from C files in
    the core kernel directories (kernel, init, lib, mm, and include)
    that don't really have a specific maintainer.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index ca25e6e704a2..be3d3514c325 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -699,7 +699,7 @@ void send_remote_softirq(struct call_single_data *cp, int cpu, int softirq)
 }
 EXPORT_SYMBOL(send_remote_softirq);
 
-static int __cpuinit remote_softirq_cpu_notify(struct notifier_block *self,
+static int remote_softirq_cpu_notify(struct notifier_block *self,
 					       unsigned long action, void *hcpu)
 {
 	/*
@@ -728,7 +728,7 @@ static int __cpuinit remote_softirq_cpu_notify(struct notifier_block *self,
 	return NOTIFY_OK;
 }
 
-static struct notifier_block __cpuinitdata remote_softirq_cpu_notifier = {
+static struct notifier_block remote_softirq_cpu_notifier = {
 	.notifier_call	= remote_softirq_cpu_notify,
 };
 
@@ -830,7 +830,7 @@ static void takeover_tasklets(unsigned int cpu)
 }
 #endif /* CONFIG_HOTPLUG_CPU */
 
-static int __cpuinit cpu_callback(struct notifier_block *nfb,
+static int cpu_callback(struct notifier_block *nfb,
 				  unsigned long action,
 				  void *hcpu)
 {
@@ -845,7 +845,7 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 	return NOTIFY_OK;
 }
 
-static struct notifier_block __cpuinitdata cpu_nfb = {
+static struct notifier_block cpu_nfb = {
 	.notifier_call = cpu_callback
 };
 

commit a4883ef6af5e513a1e8c2ab9aab721604aa3a4f5
Merge: ab3d681e9d41 d2e08473f248
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 2 16:14:35 2013 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core irq changes from Ingo Molnar:
     "The main changes:
    
      - generic-irqchip driver additions, cleanups and fixes
    
      - 3 new irqchip drivers: ARMv7-M NVIC, TB10x and Marvell Orion SoCs
    
      - irq_get_trigger_type() simplification and cross-arch cleanup
    
      - various cleanups, simplifications
    
      - documentation updates"
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (26 commits)
      softirq: Use _RET_IP_
      genirq: Add the generic chip to the genirq docbook
      genirq: generic-chip: Export some irq_gc_ functions
      genirq: Fix can_request_irq() for IRQs without an action
      irqchip: exynos-combiner: Staticize combiner_init
      irqchip: Add support for ARMv7-M NVIC
      irqchip: Add TB10x interrupt controller driver
      irqdomain: Use irq_get_trigger_type() to get IRQ flags
      MIPS: octeon: Use irq_get_trigger_type() to get IRQ flags
      arm: orion: Use irq_get_trigger_type() to get IRQ flags
      mfd: stmpe: use irq_get_trigger_type() to get IRQ flags
      mfd: twl4030-irq: Use irq_get_trigger_type() to get IRQ flags
      gpio: mvebu: Use irq_get_trigger_type() to get IRQ flags
      genirq: Add irq_get_trigger_type() to get IRQ flags
      genirq: Irqchip: document gcflags arg of irq_alloc_domain_generic_chips
      genirq: Set irq thread to RT priority on creation
      irqchip: Add support for Marvell Orion SoCs
      genirq: Add kerneldoc for irq_disable.
      genirq: irqchip: Add mask to block out invalid irqs
      genirq: Generic chip: Add linear irq domain support
      ...

commit d2e08473f2488d53a71c2f53455f934ec6c44c53
Author: Davidlohr Bueso <davidlohr.bueso@hp.com>
Date:   Tue Apr 30 11:46:09 2013 -0700

    softirq: Use _RET_IP_
    
    Use the already defined macro to pass the function return address.
    
    Signed-off-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/1367347569.1784.3.camel@buesod1.americas.hpqcorp.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b5197dcb0dad..a5f88362589b 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -127,8 +127,7 @@ static inline void __local_bh_disable(unsigned long ip, unsigned int cnt)
 
 void local_bh_disable(void)
 {
-	__local_bh_disable((unsigned long)__builtin_return_address(0),
-				SOFTIRQ_DISABLE_OFFSET);
+	__local_bh_disable(_RET_IP_, SOFTIRQ_DISABLE_OFFSET);
 }
 
 EXPORT_SYMBOL(local_bh_disable);
@@ -139,7 +138,7 @@ static void __local_bh_enable(unsigned int cnt)
 	WARN_ON_ONCE(!irqs_disabled());
 
 	if (softirq_count() == cnt)
-		trace_softirqs_on((unsigned long)__builtin_return_address(0));
+		trace_softirqs_on(_RET_IP_);
 	sub_preempt_count(cnt);
 }
 
@@ -184,7 +183,7 @@ static inline void _local_bh_enable_ip(unsigned long ip)
 
 void local_bh_enable(void)
 {
-	_local_bh_enable_ip((unsigned long)__builtin_return_address(0));
+	_local_bh_enable_ip(_RET_IP_);
 }
 EXPORT_SYMBOL(local_bh_enable);
 
@@ -223,8 +222,7 @@ asmlinkage void __do_softirq(void)
 	pending = local_softirq_pending();
 	account_irq_enter_time(current);
 
-	__local_bh_disable((unsigned long)__builtin_return_address(0),
-				SOFTIRQ_OFFSET);
+	__local_bh_disable(_RET_IP_, SOFTIRQ_OFFSET);
 	lockdep_softirq_enter();
 
 	cpu = smp_processor_id();

commit 34376a50fb1fa095b9d0636fa41ed2e73125f214
Author: Ben Greear <greearb@candelatech.com>
Date:   Thu Jun 6 14:29:49 2013 -0700

    Fix lockup related to stop_machine being stuck in __do_softirq.
    
    The stop machine logic can lock up if all but one of the migration
    threads make it through the disable-irq step and the one remaining
    thread gets stuck in __do_softirq.  The reason __do_softirq can hang is
    that it has a bail-out based on jiffies timeout, but in the lockup case,
    jiffies itself is not incremented.
    
    To work around this, re-add the max_restart counter in __do_irq and stop
    processing irqs after 10 restarts.
    
    Thanks to Tejun Heo and Rusty Russell and others for helping me track
    this down.
    
    This was introduced in 3.9 by commit c10d73671ad3 ("softirq: reduce
    latencies").
    
    It may be worth looking into ath9k to see if it has issues with its irq
    handler at a later date.
    
    The hang stack traces look something like this:
    
        ------------[ cut here ]------------
        WARNING: at kernel/watchdog.c:245 watchdog_overflow_callback+0x9c/0xa7()
        Watchdog detected hard LOCKUP on cpu 2
        Modules linked in: ath9k ath9k_common ath9k_hw ath mac80211 cfg80211 nfsv4 auth_rpcgss nfs fscache nf_nat_ipv4 nf_nat veth 8021q garp stp mrp llc pktgen lockd sunrpc]
        Pid: 23, comm: migration/2 Tainted: G         C   3.9.4+ #11
        Call Trace:
         <NMI>   warn_slowpath_common+0x85/0x9f
          warn_slowpath_fmt+0x46/0x48
          watchdog_overflow_callback+0x9c/0xa7
          __perf_event_overflow+0x137/0x1cb
          perf_event_overflow+0x14/0x16
          intel_pmu_handle_irq+0x2dc/0x359
          perf_event_nmi_handler+0x19/0x1b
          nmi_handle+0x7f/0xc2
          do_nmi+0xbc/0x304
          end_repeat_nmi+0x1e/0x2e
         <<EOE>>
          cpu_stopper_thread+0xae/0x162
          smpboot_thread_fn+0x258/0x260
          kthread+0xc7/0xcf
          ret_from_fork+0x7c/0xb0
        ---[ end trace 4947dfa9b0a4cec3 ]---
        BUG: soft lockup - CPU#1 stuck for 22s! [migration/1:17]
        Modules linked in: ath9k ath9k_common ath9k_hw ath mac80211 cfg80211 nfsv4 auth_rpcgss nfs fscache nf_nat_ipv4 nf_nat veth 8021q garp stp mrp llc pktgen lockd sunrpc]
        irq event stamp: 835637905
        hardirqs last  enabled at (835637904): __do_softirq+0x9f/0x257
        hardirqs last disabled at (835637905): apic_timer_interrupt+0x6d/0x80
        softirqs last  enabled at (5654720): __do_softirq+0x1ff/0x257
        softirqs last disabled at (5654725): irq_exit+0x5f/0xbb
        CPU 1
        Pid: 17, comm: migration/1 Tainted: G        WC   3.9.4+ #11 To be filled by O.E.M. To be filled by O.E.M./To be filled by O.E.M.
        RIP: tasklet_hi_action+0xf0/0xf0
        Process migration/1
        Call Trace:
         <IRQ>
          __do_softirq+0x117/0x257
          irq_exit+0x5f/0xbb
          smp_apic_timer_interrupt+0x8a/0x98
          apic_timer_interrupt+0x72/0x80
         <EOI>
          printk+0x4d/0x4f
          stop_machine_cpu_stop+0x22c/0x274
          cpu_stopper_thread+0xae/0x162
          smpboot_thread_fn+0x258/0x260
          kthread+0xc7/0xcf
          ret_from_fork+0x7c/0xb0
    
    Signed-off-by: Ben Greear <greearb@candelatech.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Pekka Riikonen <priikone@iki.fi>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b5197dcb0dad..3d6833f125d3 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -195,8 +195,12 @@ void local_bh_enable_ip(unsigned long ip)
 EXPORT_SYMBOL(local_bh_enable_ip);
 
 /*
- * We restart softirq processing for at most 2 ms,
- * and if need_resched() is not set.
+ * We restart softirq processing for at most MAX_SOFTIRQ_RESTART times,
+ * but break the loop if need_resched() is set or after 2 ms.
+ * The MAX_SOFTIRQ_TIME provides a nice upper bound in most cases, but in
+ * certain cases, such as stop_machine(), jiffies may cease to
+ * increment and so we need the MAX_SOFTIRQ_RESTART limit as
+ * well to make sure we eventually return from this method.
  *
  * These limits have been established via experimentation.
  * The two things to balance is latency against fairness -
@@ -204,6 +208,7 @@ EXPORT_SYMBOL(local_bh_enable_ip);
  * should not be able to lock up the box.
  */
 #define MAX_SOFTIRQ_TIME  msecs_to_jiffies(2)
+#define MAX_SOFTIRQ_RESTART 10
 
 asmlinkage void __do_softirq(void)
 {
@@ -212,6 +217,7 @@ asmlinkage void __do_softirq(void)
 	unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
 	int cpu;
 	unsigned long old_flags = current->flags;
+	int max_restart = MAX_SOFTIRQ_RESTART;
 
 	/*
 	 * Mask out PF_MEMALLOC s current task context is borrowed for the
@@ -265,7 +271,8 @@ asmlinkage void __do_softirq(void)
 
 	pending = local_softirq_pending();
 	if (pending) {
-		if (time_before(jiffies, end) && !need_resched())
+		if (time_before(jiffies, end) && !need_resched() &&
+		    --max_restart)
 			goto restart;
 
 		wakeup_softirqd();

commit 534c97b0950b1967bca1c753aeaed32f5db40264
Merge: 64049d1973c1 265f22a975c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 5 13:23:27 2013 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull 'full dynticks' support from Ingo Molnar:
     "This tree from Frederic Weisbecker adds a new, (exciting! :-) core
      kernel feature to the timer and scheduler subsystems: 'full dynticks',
      or CONFIG_NO_HZ_FULL=y.
    
      This feature extends the nohz variable-size timer tick feature from
      idle to busy CPUs (running at most one task) as well, potentially
      reducing the number of timer interrupts significantly.
    
      This feature got motivated by real-time folks and the -rt tree, but
      the general utility and motivation of full-dynticks runs wider than
      that:
    
       - HPC workloads get faster: CPUs running a single task should be able
         to utilize a maximum amount of CPU power.  A periodic timer tick at
         HZ=1000 can cause a constant overhead of up to 1.0%.  This feature
         removes that overhead - and speeds up the system by 0.5%-1.0% on
         typical distro configs even on modern systems.
    
       - Real-time workload latency reduction: CPUs running critical tasks
         should experience as little jitter as possible.  The last remaining
         source of kernel-related jitter was the periodic timer tick.
    
       - A single task executing on a CPU is a pretty common situation,
         especially with an increasing number of cores/CPUs, so this feature
         helps desktop and mobile workloads as well.
    
      The cost of the feature is mainly related to increased timer
      reprogramming overhead when a CPU switches its tick period, and thus
      slightly longer to-idle and from-idle latency.
    
      Configuration-wise a third mode of operation is added to the existing
      two NOHZ kconfig modes:
    
       - CONFIG_HZ_PERIODIC: [formerly !CONFIG_NO_HZ], now explicitly named
         as a config option.  This is the traditional Linux periodic tick
         design: there's a HZ tick going on all the time, regardless of
         whether a CPU is idle or not.
    
       - CONFIG_NO_HZ_IDLE: [formerly CONFIG_NO_HZ=y], this turns off the
         periodic tick when a CPU enters idle mode.
    
       - CONFIG_NO_HZ_FULL: this new mode, in addition to turning off the
         tick when a CPU is idle, also slows the tick down to 1 Hz (one
         timer interrupt per second) when only a single task is running on a
         CPU.
    
      The .config behavior is compatible: existing !CONFIG_NO_HZ and
      CONFIG_NO_HZ=y settings get translated to the new values, without the
      user having to configure anything.  CONFIG_NO_HZ_FULL is turned off by
      default.
    
      This feature is based on a lot of infrastructure work that has been
      steadily going upstream in the last 2-3 cycles: related RCU support
      and non-periodic cputime support in particular is upstream already.
    
      This tree adds the final pieces and activates the feature.  The pull
      request is marked RFC because:
    
       - it's marked 64-bit only at the moment - the 32-bit support patch is
         small but did not get ready in time.
    
       - it has a number of fresh commits that came in after the merge
         window.  The overwhelming majority of commits are from before the
         merge window, but still some aspects of the tree are fresh and so I
         marked it RFC.
    
       - it's a pretty wide-reaching feature with lots of effects - and
         while the components have been in testing for some time, the full
         combination is still not very widely used.  That it's default-off
         should reduce its regression abilities and obviously there are no
         known regressions with CONFIG_NO_HZ_FULL=y enabled either.
    
       - the feature is not completely idempotent: there is no 100%
         equivalent replacement for a periodic scheduler/timer tick.  In
         particular there's ongoing work to map out and reduce its effects
         on scheduler load-balancing and statistics.  This should not impact
         correctness though, there are no known regressions related to this
         feature at this point.
    
       - it's a pretty ambitious feature that with time will likely be
         enabled by most Linux distros, and we'd like you to make input on
         its design/implementation, if you dislike some aspect we missed.
         Without flaming us to crisp! :-)
    
      Future plans:
    
       - there's ongoing work to reduce 1Hz to 0Hz, to essentially shut off
         the periodic tick altogether when there's a single busy task on a
         CPU.  We'd first like 1 Hz to be exposed more widely before we go
         for the 0 Hz target though.
    
       - once we reach 0 Hz we can remove the periodic tick assumption from
         nr_running>=2 as well, by essentially interrupting busy tasks only
         as frequently as the sched_latency constraints require us to do -
         once every 4-40 msecs, depending on nr_running.
    
      I am personally leaning towards biting the bullet and doing this in
      v3.10, like the -rt tree this effort has been going on for too long -
      but the final word is up to you as usual.
    
      More technical details can be found in Documentation/timers/NO_HZ.txt"
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (39 commits)
      sched: Keep at least 1 tick per second for active dynticks tasks
      rcu: Fix full dynticks' dependency on wide RCU nocb mode
      nohz: Protect smp_processor_id() in tick_nohz_task_switch()
      nohz_full: Add documentation.
      cputime_nsecs: use math64.h for nsec resolution conversion helpers
      nohz: Select VIRT_CPU_ACCOUNTING_GEN from full dynticks config
      nohz: Reduce overhead under high-freq idling patterns
      nohz: Remove full dynticks' superfluous dependency on RCU tree
      nohz: Fix unavailable tick_stop tracepoint in dynticks idle
      nohz: Add basic tracing
      nohz: Select wide RCU nocb for full dynticks
      nohz: Disable the tick when irq resume in full dynticks CPU
      nohz: Re-evaluate the tick for the new task after a context switch
      nohz: Prepare to stop the tick on irq exit
      nohz: Implement full dynticks kick
      nohz: Re-evaluate the tick from the scheduler IPI
      sched: New helper to prevent from stopping the tick in full dynticks
      sched: Kick full dynticks CPU that have more than one task enqueued.
      perf: New helper to prevent full dynticks CPUs from stopping tick
      perf: Kick full dynticks CPU if events rotation is needed
      ...

commit c032862fba51a3ca504752d3a25186b324c5ce83
Merge: fda76e074c77 8700c95adb03
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 2 17:37:49 2013 +0200

    Merge commit '8700c95adb03' into timers/nohz
    
    The full dynticks tree needs the latest RCU and sched
    upstream updates in order to fix some dependencies.
    
    Merge a common upstream merge point that has these
    updates.
    
    Conflicts:
            include/linux/perf_event.h
            kernel/rcutree.h
            kernel/rcutree_plugin.h
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit 3440a1ca99707f093e9568ba9762764d3162dd8f
Author: liguang <lig.fnst@cn.fujitsu.com>
Date:   Tue Apr 30 15:27:26 2013 -0700

    kernel/smp.c: remove 'priv' of call_single_data
    
    The 'priv' field is redundant; we can pass data via 'info'.
    
    Signed-off-by: liguang <lig.fnst@cn.fujitsu.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 14d7758074aa..aa82723c7202 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -620,8 +620,7 @@ static void remote_softirq_receive(void *data)
 	unsigned long flags;
 	int softirq;
 
-	softirq = cp->priv;
-
+	softirq = *(int *)cp->info;
 	local_irq_save(flags);
 	__local_trigger(cp, softirq);
 	local_irq_restore(flags);
@@ -631,9 +630,8 @@ static int __try_remote_softirq(struct call_single_data *cp, int cpu, int softir
 {
 	if (cpu_online(cpu)) {
 		cp->func = remote_softirq_receive;
-		cp->info = cp;
+		cp->info = &softirq;
 		cp->flags = 0;
-		cp->priv = softirq;
 
 		__smp_call_function_single(cpu, cp, 0);
 		return 0;

commit 67826eae8c16dbf00c262be6ec15021bb42f69c4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Apr 20 17:43:13 2013 +0200

    nohz: Disable the tick when irq resume in full dynticks CPU
    
    Eventually try to disable tick on irq exit, now that the
    fundamental infrastructure is in place.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index de15813f2a66..8b1446d4a4db 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -337,6 +337,19 @@ static inline void invoke_softirq(void)
 	}
 }
 
+static inline void tick_irq_exit(void)
+{
+#ifdef CONFIG_NO_HZ_COMMON
+	int cpu = smp_processor_id();
+
+	/* Make sure that timer wheel updates are propagated */
+	if ((idle_cpu(cpu) && !need_resched()) || tick_nohz_full_cpu(cpu)) {
+		if (!in_interrupt())
+			tick_nohz_irq_exit();
+	}
+#endif
+}
+
 /*
  * Exit an interrupt context. Process softirqs if needed and possible:
  */
@@ -348,11 +361,7 @@ void irq_exit(void)
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
 
-#ifdef CONFIG_NO_HZ_COMMON
-	/* Make sure that timer wheel updates are propagated */
-	if (idle_cpu(smp_processor_id()) && !in_interrupt() && !need_resched())
-		tick_nohz_irq_exit();
-#endif
+	tick_irq_exit();
 	rcu_irq_exit();
 	sched_preempt_enable_no_resched();
 }

commit 3451d0243c3cdfd729b36f9684a14659d4895ca3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Aug 10 23:21:01 2011 +0200

    nohz: Rename CONFIG_NO_HZ to CONFIG_NO_HZ_COMMON
    
    We are planning to convert the dynticks Kconfig options layout
    into a choice menu. The user must be able to easily pick
    any of the following implementations: constant periodic tick,
    idle dynticks, full dynticks.
    
    As this implies a mutual exclusion, the two dynticks implementions
    need to converge on the selection of a common Kconfig option in order
    to ease the sharing of a common infrastructure.
    
    It would thus seem pretty natural to reuse CONFIG_NO_HZ to
    that end. It already implements all the idle dynticks code
    and the full dynticks depends on all that code for now.
    So ideally the choice menu would propose CONFIG_NO_HZ_IDLE and
    CONFIG_NO_HZ_EXTENDED then both would select CONFIG_NO_HZ.
    
    On the other hand we want to stay backward compatible: if
    CONFIG_NO_HZ is set in an older config file, we want to
    enable CONFIG_NO_HZ_IDLE by default.
    
    But we can't afford both at the same time or we run into
    a circular dependency:
    
    1) CONFIG_NO_HZ_IDLE and CONFIG_NO_HZ_EXTENDED both select
       CONFIG_NO_HZ
    2) If CONFIG_NO_HZ is set, we default to CONFIG_NO_HZ_IDLE
    
    We might be able to support that from Kconfig/Kbuild but it
    may not be wise to introduce such a confusing behaviour.
    
    So to solve this, create a new CONFIG_NO_HZ_COMMON option
    which gathers the common code between idle and full dynticks
    (that common code for now is simply the idle dynticks code)
    and select it from their referring Kconfig.
    
    Then we'll later create CONFIG_NO_HZ_IDLE and map CONFIG_NO_HZ
    to it for backward compatibility.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b4d252fd195b..de15813f2a66 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -348,7 +348,7 @@ void irq_exit(void)
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
 
-#ifdef CONFIG_NO_HZ
+#ifdef CONFIG_NO_HZ_COMMON
 	/* Make sure that timer wheel updates are propagated */
 	if (idle_cpu(smp_processor_id()) && !in_interrupt() && !need_resched())
 		tick_nohz_irq_exit();

commit e3b59518c10e08eeb06215abf06f50e8f83b51dc
Merge: 6516ab6fdffb 4cd5d1115c2f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 5 18:10:04 2013 -0800

    Merge branch 'irq-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq fixes and cleanups from Thomas Gleixner:
     "Commit e5ab012c3271 ("nohz: Make tick_nohz_irq_exit() irq safe") is
      the first commit in the series and the minimal necessary bugfix, which
      needs to go back into stable.
    
      The remanining commits enforce irq disabling in irq_exit(), sanitize
      the hardirq/softirq preempt count transition and remove a bunch of no
      longer necessary conditionals."
    
    I personally love getting rid of the very subtle and confusing
    IRQ_EXIT_OFFSET thing.  Even apart from the whole "more lines removed
    than added" thing.
    
    * 'irq-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      irq: Don't re-enable interrupts at the end of irq_exit
      irq: Remove IRQ_EXIT_OFFSET workaround
      Revert "nohz: Make tick_nohz_irq_exit() irq safe"
      irq: Sanitize invoke_softirq
      irq: Ensure irq_exit() code runs with interrupts disabled
      nohz: Make tick_nohz_irq_exit() irq safe

commit 4cd5d1115c2f752ca94a0eb461b36d88fb37ed1e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Feb 28 20:00:43 2013 +0100

    irq: Don't re-enable interrupts at the end of irq_exit
    
    Commit 74eed0163d0def3fce27228d9ccf3d36e207b286
    "irq: Ensure irq_exit() code runs with interrupts disabled"
    restore interrupts flags in the end of irq_exit() for archs
    that don't define __ARCH_IRQ_EXIT_IRQS_DISABLED.
    
    However always returning from irq_exit() with interrupts
    disabled should not be a problem for these archs. Prior to
    this commit this was already happening anytime we processed
    pending softirqs anyway.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index f42ff97e1f8f..dce38fac4f32 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -334,9 +334,7 @@ static inline void invoke_softirq(void)
 void irq_exit(void)
 {
 #ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED
-	unsigned long flags;
-
-	local_irq_save(flags);
+	local_irq_disable();
 #else
 	WARN_ON_ONCE(!irqs_disabled());
 #endif
@@ -353,9 +351,6 @@ void irq_exit(void)
 		tick_nohz_irq_exit();
 #endif
 	rcu_irq_exit();
-#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED
-	local_irq_restore(flags);
-#endif
 }
 
 /*

commit 4d4c4e24cf48400a24d33feffc7cca4f4e8cabe1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Feb 22 00:05:07 2013 +0100

    irq: Remove IRQ_EXIT_OFFSET workaround
    
    The IRQ_EXIT_OFFSET trick was used to make sure the irq
    doesn't get preempted after we substract the HARDIRQ_OFFSET
    until we are entirely done with any code in irq_exit().
    
    This workaround was necessary because some archs may call
    irq_exit() with irqs enabled and there is still some code
    in the end of this function that is not covered by the
    HARDIRQ_OFFSET but want to stay non-preemptible.
    
    Now that irq are always disabled in irq_exit(), the whole code
    is guaranteed not to be preempted. We can thus remove this hack.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 24a921bcf04f..f42ff97e1f8f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -343,7 +343,7 @@ void irq_exit(void)
 
 	account_irq_exit_time(current);
 	trace_hardirq_exit();
-	sub_preempt_count(IRQ_EXIT_OFFSET);
+	sub_preempt_count(HARDIRQ_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
 
@@ -353,7 +353,6 @@ void irq_exit(void)
 		tick_nohz_irq_exit();
 #endif
 	rcu_irq_exit();
-	sched_preempt_enable_no_resched();
 #ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED
 	local_irq_restore(flags);
 #endif

commit facd8b80c67a3cf64a467c4a2ac5fb31f2e6745b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Feb 21 18:17:42 2013 +0100

    irq: Sanitize invoke_softirq
    
    With the irq protection in irq_exit, we can remove the #ifdeffery and
    the bh_disable/enable dance in invoke_softirq()
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linuxfoundation.org>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1302202155320.22263@ionos

diff --git a/kernel/softirq.c b/kernel/softirq.c
index f2a934673008..24a921bcf04f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -322,18 +322,10 @@ void irq_enter(void)
 
 static inline void invoke_softirq(void)
 {
-	if (!force_irqthreads) {
-#ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED
+	if (!force_irqthreads)
 		__do_softirq();
-#else
-		do_softirq();
-#endif
-	} else {
-		__local_bh_disable((unsigned long)__builtin_return_address(0),
-				SOFTIRQ_OFFSET);
+	else
 		wakeup_softirqd();
-		__local_bh_enable(SOFTIRQ_OFFSET);
-	}
 }
 
 /*

commit 74eed0163d0def3fce27228d9ccf3d36e207b286
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 20 22:00:48 2013 +0100

    irq: Ensure irq_exit() code runs with interrupts disabled
    
    We had already a few problems with code called from irq_exit() when
    interrupted from a nesting interrupt. This can happen on architectures
    which do not define __ARCH_IRQ_EXIT_IRQS_DISABLED.
    
    __ARCH_IRQ_EXIT_IRQS_DISABLED should go away and we want to make it
    mandatory to call irq_exit() with interrupts disabled.
    
    As a temporary protection disable interrupts for those architectures
    which do not define __ARCH_IRQ_EXIT_IRQS_DISABLED and add a WARN_ONCE
    when an architecture which defines __ARCH_IRQ_EXIT_IRQS_DISABLED calls
    irq_exit() with interrupts enabled.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linuxfoundation.org>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1302202155320.22263@ionos

diff --git a/kernel/softirq.c b/kernel/softirq.c
index f5cc25f147a6..f2a934673008 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -341,6 +341,14 @@ static inline void invoke_softirq(void)
  */
 void irq_exit(void)
 {
+#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED
+	unsigned long flags;
+
+	local_irq_save(flags);
+#else
+	WARN_ON_ONCE(!irqs_disabled());
+#endif
+
 	account_irq_exit_time(current);
 	trace_hardirq_exit();
 	sub_preempt_count(IRQ_EXIT_OFFSET);
@@ -354,6 +362,9 @@ void irq_exit(void)
 #endif
 	rcu_irq_exit();
 	sched_preempt_enable_no_resched();
+#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED
+	local_irq_restore(flags);
+#endif
 }
 
 /*

commit a0b1c42951dd06ec83cc1bc2c9788131d9fefcd8
Merge: 8ec4942212a6 ecd9883724b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 20 18:58:50 2013 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking update from David Miller:
    
     1) Checkpoint/restarted TCP sockets now can properly propagate the TCP
        timestamp offset.  From Andrey Vagin.
    
     2) VMWARE VM VSOCK layer, from Andy King.
    
     3) Much improved support for virtual functions and SR-IOV in bnx2x,
        from Ariel ELior.
    
     4) All protocols on ipv4 and ipv6 are now network namespace aware, and
        all the compatability checks for initial-namespace-only protocols is
        removed.  Thanks to Tom Parkin for helping deal with the last major
        holdout, L2TP.
    
     5) IPV6 support in netpoll and network namespace support in pktgen,
        from Cong Wang.
    
     6) Multiple Registration Protocol (MRP) and Multiple VLAN Registration
        Protocol (MVRP) support, from David Ward.
    
     7) Compute packet lengths more accurately in the packet scheduler, from
        Eric Dumazet.
    
     8) Use per-task page fragment allocator in skb_append_datato_frags(),
        also from Eric Dumazet.
    
     9) Add support for connection tracking labels in netfilter, from
        Florian Westphal.
    
    10) Fix default multicast group joining on ipv6, and add anti-spoofing
        checks to 6to4 and 6rd.  From Hannes Frederic Sowa.
    
    11) Make ipv4/ipv6 fragmentation memory limits more reasonable in modern
        times, rearrange inet frag datastructures for better cacheline
        locality, and move more operations outside of locking.  From Jesper
        Dangaard Brouer.
    
    12) Instead of strict master <--> slave relationships, allow arbitrary
        scenerios with "upper device lists".  From Jiri Pirko.
    
    13) Improve rate limiting accuracy in TBF and act_police, also from Jiri
        Pirko.
    
    14) Add a BPF filter netfilter match target, from Willem de Bruijn.
    
    15) Orphan and delete a bunch of pre-historic networking drivers from
        Paul Gortmaker.
    
    16) Add TSO support for GRE tunnels, from Pravin B SHelar.  Although
        this still needs some minor bug fixing before it's %100 correct in
        all cases.
    
    17) Handle unresolved IPSEC states like ARP, with a resolution packet
        queue.  From Steffen Klassert.
    
    18) Remove TCP Appropriate Byte Count support (ABC), from Stephen
        Hemminger.  This was long overdue.
    
    19) Support SO_REUSEPORT, from Tom Herbert.
    
    20) Allow locking a socket BPF filter, so that it cannot change after a
        process drops capabilities.
    
    21) Add VLAN filtering to bridge, from Vlad Yasevich.
    
    22) Bring ipv6 on-par with ipv4 and do not cache neighbour entries in
        the ipv6 routes, from YOSHIFUJI Hideaki.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1538 commits)
      ipv6: fix race condition regarding dst->expires and dst->from.
      net: fix a wrong assignment in skb_split()
      ip_gre: remove an extra dst_release()
      ppp: set qdisc_tx_busylock to avoid LOCKDEP splat
      atl1c: restore buffer state
      net: fix a build failure when !CONFIG_PROC_FS
      net: ipv4: fix waring -Wunused-variable
      net: proc: fix build failed when procfs is not configured
      Revert "xen: netback: remove redundant xenvif_put"
      net: move procfs code to net/core/net-procfs.c
      qmi_wwan, cdc-ether: add ADU960S
      bonding: set sysfs device_type to 'bond'
      bonding: fix bond_release_all inconsistencies
      b44: use netdev_alloc_skb_ip_align()
      xen: netback: remove redundant xenvif_put
      net: fec: Do a sanity check on the gpio number
      ip_gre: propogate target device GSO capability to the tunnel device
      ip_gre: allow CSUM capable devices to handle packets
      bonding: Fix initialize after use for 3ad machine state spinlock
      bonding: Fix race condition between bond_enslave() and bond_3ad_update_lacp_rate()
      ...

commit 6a61671bb2f3a1bd12cd17b8fca811a624782632
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Dec 16 20:00:34 2012 +0100

    cputime: Safely read cputime of full dynticks CPUs
    
    While remotely reading the cputime of a task running in a
    full dynticks CPU, the values stored in utime/stime fields
    of struct task_struct may be stale. Its values may be those
    of the last kernel <-> user transition time snapshot and
    we need to add the tickless time spent since this snapshot.
    
    To fix this, flush the cputime of the dynticks CPUs on
    kernel <-> user transition and record the time / context
    where we did this. Then on top of this snapshot and the current
    time, perform the fixup on the reader side from task_times()
    accessors.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [fixed kvm module related build errors]
    Signed-off-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index ed567babe789..f5cc25f147a6 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -221,7 +221,7 @@ asmlinkage void __do_softirq(void)
 	current->flags &= ~PF_MEMALLOC;
 
 	pending = local_softirq_pending();
-	vtime_account_irq_enter(current);
+	account_irq_enter_time(current);
 
 	__local_bh_disable((unsigned long)__builtin_return_address(0),
 				SOFTIRQ_OFFSET);
@@ -272,7 +272,7 @@ asmlinkage void __do_softirq(void)
 
 	lockdep_softirq_exit();
 
-	vtime_account_irq_exit(current);
+	account_irq_exit_time(current);
 	__local_bh_enable(SOFTIRQ_OFFSET);
 	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
 }
@@ -341,7 +341,7 @@ static inline void invoke_softirq(void)
  */
 void irq_exit(void)
 {
-	vtime_account_irq_exit(current);
+	account_irq_exit_time(current);
 	trace_hardirq_exit();
 	sub_preempt_count(IRQ_EXIT_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())

commit c10d73671ad30f54692f7f69f0e09e75d3a8926a
Author: Eric Dumazet <edumazet@google.com>
Date:   Thu Jan 10 15:26:34 2013 -0800

    softirq: reduce latencies
    
    In various network workloads, __do_softirq() latencies can be up
    to 20 ms if HZ=1000, and 200 ms if HZ=100.
    
    This is because we iterate 10 times in the softirq dispatcher,
    and some actions can consume a lot of cycles.
    
    This patch changes the fallback to ksoftirqd condition to :
    
    - A time limit of 2 ms.
    - need_resched() being set on current task
    
    When one of this condition is met, we wakeup ksoftirqd for further
    softirq processing if we still have pending softirqs.
    
    Using need_resched() as the only condition can trigger RCU stalls,
    as we can keep BH disabled for too long.
    
    I ran several benchmarks and got no significant difference in
    throughput, but a very significant reduction of latencies (one order
    of magnitude) :
    
    In following bench, 200 antagonist "netperf -t TCP_RR" are started in
    background, using all available cpus.
    
    Then we start one "netperf -t TCP_RR", bound to the cpu handling the NIC
    IRQ (hard+soft)
    
    Before patch :
    
    # netperf -H 7.7.7.84 -t TCP_RR -T2,2 -- -k
    RT_LATENCY,MIN_LATENCY,MAX_LATENCY,P50_LATENCY,P90_LATENCY,P99_LATENCY,MEAN_LATENCY,STDDEV_LATENCY
    MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET
    to 7.7.7.84 () port 0 AF_INET : first burst 0 : cpu bind
    RT_LATENCY=550110.424
    MIN_LATENCY=146858
    MAX_LATENCY=997109
    P50_LATENCY=305000
    P90_LATENCY=550000
    P99_LATENCY=710000
    MEAN_LATENCY=376989.12
    STDDEV_LATENCY=184046.92
    
    After patch :
    
    # netperf -H 7.7.7.84 -t TCP_RR -T2,2 -- -k
    RT_LATENCY,MIN_LATENCY,MAX_LATENCY,P50_LATENCY,P90_LATENCY,P99_LATENCY,MEAN_LATENCY,STDDEV_LATENCY
    MIGRATED TCP REQUEST/RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET
    to 7.7.7.84 () port 0 AF_INET : first burst 0 : cpu bind
    RT_LATENCY=40545.492
    MIN_LATENCY=9834
    MAX_LATENCY=78366
    P50_LATENCY=33583
    P90_LATENCY=59000
    P99_LATENCY=69000
    MEAN_LATENCY=38364.67
    STDDEV_LATENCY=12865.26
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index ed567babe789..47cb991c6ba4 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -195,21 +195,21 @@ void local_bh_enable_ip(unsigned long ip)
 EXPORT_SYMBOL(local_bh_enable_ip);
 
 /*
- * We restart softirq processing MAX_SOFTIRQ_RESTART times,
- * and we fall back to softirqd after that.
+ * We restart softirq processing for at most 2 ms,
+ * and if need_resched() is not set.
  *
- * This number has been established via experimentation.
+ * These limits have been established via experimentation.
  * The two things to balance is latency against fairness -
  * we want to handle softirqs as soon as possible, but they
  * should not be able to lock up the box.
  */
-#define MAX_SOFTIRQ_RESTART 10
+#define MAX_SOFTIRQ_TIME  msecs_to_jiffies(2)
 
 asmlinkage void __do_softirq(void)
 {
 	struct softirq_action *h;
 	__u32 pending;
-	int max_restart = MAX_SOFTIRQ_RESTART;
+	unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
 	int cpu;
 	unsigned long old_flags = current->flags;
 
@@ -264,11 +264,12 @@ asmlinkage void __do_softirq(void)
 	local_irq_disable();
 
 	pending = local_softirq_pending();
-	if (pending && --max_restart)
-		goto restart;
+	if (pending) {
+		if (time_before(jiffies, end) && !need_resched())
+			goto restart;
 
-	if (pending)
 		wakeup_softirqd();
+	}
 
 	lockdep_softirq_exit();
 

commit fa5058f3b63153e0147ef65bcdb3a4ee63581346
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Oct 6 04:07:19 2012 +0200

    cputime: Specialize irq vtime hooks
    
    With CONFIG_VIRT_CPU_ACCOUNTING, when vtime_account()
    is called in irq entry/exit, we perform a check on the
    context: if we are interrupting the idle task we
    account the pending cputime to idle, otherwise account
    to system time or its sub-areas: tsk->stime, hardirq time,
    softirq time, ...
    
    However this check for idle only concerns the hardirq entry
    and softirq entry:
    
    * Hardirq may directly interrupt the idle task, in which case
    we need to flush the pending CPU time to idle.
    
    * The idle task may be directly interrupted by a softirq if
    it calls local_bh_enable(). There is probably no such call
    in any idle task but we need to cover every case. Ksoftirqd
    is not concerned because the idle time is flushed on context
    switch and softirq in the end of hardirq have the idle time
    already flushed from the hardirq entry.
    
    In the other cases we always account to system/irq time:
    
    * On hardirq exit we account the time to hardirq time.
    * On softirq exit we account the time to softirq time.
    
    To optimize this and avoid the indirect call to vtime_account()
    and the checks it performs, specialize the vtime irq APIs and
    only perform the check on irq entry. Irq exit can directly call
    vtime_account_system().
    
    CONFIG_IRQ_TIME_ACCOUNTING behaviour doesn't change and directly
    maps to its own vtime_account() implementation. One may want
    to take benefits from the new APIs to optimize irq time accounting
    as well in the future.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index cc96bdc0c2c9..ed567babe789 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -221,7 +221,7 @@ asmlinkage void __do_softirq(void)
 	current->flags &= ~PF_MEMALLOC;
 
 	pending = local_softirq_pending();
-	vtime_account(current);
+	vtime_account_irq_enter(current);
 
 	__local_bh_disable((unsigned long)__builtin_return_address(0),
 				SOFTIRQ_OFFSET);
@@ -272,7 +272,7 @@ asmlinkage void __do_softirq(void)
 
 	lockdep_softirq_exit();
 
-	vtime_account(current);
+	vtime_account_irq_exit(current);
 	__local_bh_enable(SOFTIRQ_OFFSET);
 	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
 }
@@ -341,7 +341,7 @@ static inline void invoke_softirq(void)
  */
 void irq_exit(void)
 {
-	vtime_account(current);
+	vtime_account_irq_exit(current);
 	trace_hardirq_exit();
 	sub_preempt_count(IRQ_EXIT_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())

commit 0b981cb94bc63a2d0e5eccccdca75fe57643ffce
Merge: 4cba3335826c fdf9c356502a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 10:43:39 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "Continued quest to clean up and enhance the cputime code by Frederic
      Weisbecker, in preparation for future tickless kernel features.
    
      Other than that, smallish changes."
    
    Fix up trivial conflicts due to additions next to each other in arch/{x86/}Kconfig
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      cputime: Make finegrained irqtime accounting generally available
      cputime: Gather time/stats accounting config options into a single menu
      ia64: Reuse system and user vtime accounting functions on task switch
      ia64: Consolidate user vtime accounting
      vtime: Consolidate system/idle context detection
      cputime: Use a proper subsystem naming for vtime related APIs
      sched: cpu_power: enable ARCH_POWER
      sched/nohz: Clean up select_nohz_load_balancer()
      sched: Fix load avg vs. cpu-hotplug
      sched: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW
      sched: Fix nohz_idle_balance()
      sched: Remove useless code in yield_to()
      sched: Add time unit suffix to sched sysctl knobs
      sched/debug: Limit sd->*_idx range on sysctl
      sched: Remove AFFINE_WAKEUPS feature flag
      s390: Remove leftover account_tick_vtime() header
      cputime: Consolidate vtime handling on context switch
      sched: Move cputime code to its own file
      cputime: Generalize CONFIG_VIRT_CPU_ACCOUNTING
      tile: Remove SD_PREFER_LOCAL leftover
      ...

commit bf9fae9f5e4ca8dce4708812f9ad6281e61df109
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Sep 8 15:23:11 2012 +0200

    cputime: Use a proper subsystem naming for vtime related APIs
    
    Use a naming based on vtime as a prefix for virtual based
    cputime accounting APIs:
    
    - account_system_vtime() -> vtime_account()
    - account_switch_vtime() -> vtime_task_switch()
    
    It makes it easier to allow for further declension such
    as vtime_account_system(), vtime_account_idle(), ... if we
    want to find out the context we account to from generic code.
    
    This also make it better to know on which subsystem these APIs
    refer to.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b73e681df09e..d55e3159f928 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -220,7 +220,7 @@ asmlinkage void __do_softirq(void)
 	current->flags &= ~PF_MEMALLOC;
 
 	pending = local_softirq_pending();
-	account_system_vtime(current);
+	vtime_account(current);
 
 	__local_bh_disable((unsigned long)__builtin_return_address(0),
 				SOFTIRQ_OFFSET);
@@ -271,7 +271,7 @@ asmlinkage void __do_softirq(void)
 
 	lockdep_softirq_exit();
 
-	account_system_vtime(current);
+	vtime_account(current);
 	__local_bh_enable(SOFTIRQ_OFFSET);
 	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
 }
@@ -340,7 +340,7 @@ static inline void invoke_softirq(void)
  */
 void irq_exit(void)
 {
-	account_system_vtime(current);
+	vtime_account(current);
 	trace_hardirq_exit();
 	sub_preempt_count(IRQ_EXIT_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())

commit 3e339b5dae24a7065e196eb8d0145ab2f8cc2d2d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:37 2012 +0000

    softirq: Use hotplug thread infrastructure
    
    [ paulmck: Call rcu_note_context_switch() with interrupts enabled. ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20120716103948.456416747@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b73e681df09e..5c6a5bd8462f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -23,6 +23,7 @@
 #include <linux/rcupdate.h>
 #include <linux/ftrace.h>
 #include <linux/smp.h>
+#include <linux/smpboot.h>
 #include <linux/tick.h>
 
 #define CREATE_TRACE_POINTS
@@ -742,49 +743,22 @@ void __init softirq_init(void)
 	open_softirq(HI_SOFTIRQ, tasklet_hi_action);
 }
 
-static int run_ksoftirqd(void * __bind_cpu)
+static int ksoftirqd_should_run(unsigned int cpu)
 {
-	set_current_state(TASK_INTERRUPTIBLE);
-
-	while (!kthread_should_stop()) {
-		preempt_disable();
-		if (!local_softirq_pending()) {
-			schedule_preempt_disabled();
-		}
-
-		__set_current_state(TASK_RUNNING);
-
-		while (local_softirq_pending()) {
-			/* Preempt disable stops cpu going offline.
-			   If already offline, we'll be on wrong CPU:
-			   don't process */
-			if (cpu_is_offline((long)__bind_cpu))
-				goto wait_to_die;
-			local_irq_disable();
-			if (local_softirq_pending())
-				__do_softirq();
-			local_irq_enable();
-			sched_preempt_enable_no_resched();
-			cond_resched();
-			preempt_disable();
-			rcu_note_context_switch((long)__bind_cpu);
-		}
-		preempt_enable();
-		set_current_state(TASK_INTERRUPTIBLE);
-	}
-	__set_current_state(TASK_RUNNING);
-	return 0;
+	return local_softirq_pending();
+}
 
-wait_to_die:
-	preempt_enable();
-	/* Wait for kthread_stop */
-	set_current_state(TASK_INTERRUPTIBLE);
-	while (!kthread_should_stop()) {
-		schedule();
-		set_current_state(TASK_INTERRUPTIBLE);
+static void run_ksoftirqd(unsigned int cpu)
+{
+	local_irq_disable();
+	if (local_softirq_pending()) {
+		__do_softirq();
+		rcu_note_context_switch(cpu);
+		local_irq_enable();
+		cond_resched();
+		return;
 	}
-	__set_current_state(TASK_RUNNING);
-	return 0;
+	local_irq_enable();
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
@@ -850,50 +824,14 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 				  unsigned long action,
 				  void *hcpu)
 {
-	int hotcpu = (unsigned long)hcpu;
-	struct task_struct *p;
-
 	switch (action) {
-	case CPU_UP_PREPARE:
-	case CPU_UP_PREPARE_FROZEN:
-		p = kthread_create_on_node(run_ksoftirqd,
-					   hcpu,
-					   cpu_to_node(hotcpu),
-					   "ksoftirqd/%d", hotcpu);
-		if (IS_ERR(p)) {
-			printk("ksoftirqd for %i failed\n", hotcpu);
-			return notifier_from_errno(PTR_ERR(p));
-		}
-		kthread_bind(p, hotcpu);
-  		per_cpu(ksoftirqd, hotcpu) = p;
- 		break;
-	case CPU_ONLINE:
-	case CPU_ONLINE_FROZEN:
-		wake_up_process(per_cpu(ksoftirqd, hotcpu));
-		break;
 #ifdef CONFIG_HOTPLUG_CPU
-	case CPU_UP_CANCELED:
-	case CPU_UP_CANCELED_FROZEN:
-		if (!per_cpu(ksoftirqd, hotcpu))
-			break;
-		/* Unbind so it can run.  Fall thru. */
-		kthread_bind(per_cpu(ksoftirqd, hotcpu),
-			     cpumask_any(cpu_online_mask));
 	case CPU_DEAD:
-	case CPU_DEAD_FROZEN: {
-		static const struct sched_param param = {
-			.sched_priority = MAX_RT_PRIO-1
-		};
-
-		p = per_cpu(ksoftirqd, hotcpu);
-		per_cpu(ksoftirqd, hotcpu) = NULL;
-		sched_setscheduler_nocheck(p, SCHED_FIFO, &param);
-		kthread_stop(p);
-		takeover_tasklets(hotcpu);
+	case CPU_DEAD_FROZEN:
+		takeover_tasklets((unsigned long)hcpu);
 		break;
-	}
 #endif /* CONFIG_HOTPLUG_CPU */
- 	}
+	}
 	return NOTIFY_OK;
 }
 
@@ -901,14 +839,19 @@ static struct notifier_block __cpuinitdata cpu_nfb = {
 	.notifier_call = cpu_callback
 };
 
+static struct smp_hotplug_thread softirq_threads = {
+	.store			= &ksoftirqd,
+	.thread_should_run	= ksoftirqd_should_run,
+	.thread_fn		= run_ksoftirqd,
+	.thread_comm		= "ksoftirqd/%u",
+};
+
 static __init int spawn_ksoftirqd(void)
 {
-	void *cpu = (void *)(long)smp_processor_id();
-	int err = cpu_callback(&cpu_nfb, CPU_UP_PREPARE, cpu);
-
-	BUG_ON(err != NOTIFY_OK);
-	cpu_callback(&cpu_nfb, CPU_ONLINE, cpu);
 	register_cpu_notifier(&cpu_nfb);
+
+	BUG_ON(smpboot_register_percpu_thread(&softirq_threads));
+
 	return 0;
 }
 early_initcall(spawn_ksoftirqd);

commit 907aed48f65efeecf91575397e3d79335d93a466
Author: Mel Gorman <mgorman@suse.de>
Date:   Tue Jul 31 16:44:07 2012 -0700

    mm: allow PF_MEMALLOC from softirq context
    
    This is needed to allow network softirq packet processing to make use of
    PF_MEMALLOC.
    
    Currently softirq context cannot use PF_MEMALLOC due to it not being
    associated with a task, and therefore not having task flags to fiddle with
    - thus the gfp to alloc flag mapping ignores the task flags when in
    interrupts (hard or soft) context.
    
    Allowing softirqs to make use of PF_MEMALLOC therefore requires some
    trickery.  This patch borrows the task flags from whatever process happens
    to be preempted by the softirq.  It then modifies the gfp to alloc flags
    mapping to not exclude task flags in softirq context, and modify the
    softirq code to save, clear and restore the PF_MEMALLOC flag.
    
    The save and clear, ensures the preempted task's PF_MEMALLOC flag doesn't
    leak into the softirq.  The restore ensures a softirq's PF_MEMALLOC flag
    cannot leak back into the preempted process.  This should be safe due to
    the following reasons
    
    Softirqs can run on multiple CPUs sure but the same task should not be
            executing the same softirq code. Neither should the softirq
            handler be preempted by any other softirq handler so the flags
            should not leak to an unrelated softirq.
    
    Softirqs re-enable hardware interrupts in __do_softirq() so can be
            preempted by hardware interrupts so PF_MEMALLOC is inherited
            by the hard IRQ. However, this is similar to a process in
            reclaim being preempted by a hardirq. While PF_MEMALLOC is
            set, gfp_to_alloc_flags() distinguishes between hard and
            soft irqs and avoids giving a hardirq the ALLOC_NO_WATERMARKS
            flag.
    
    If the softirq is deferred to ksoftirq then its flags may be used
            instead of a normal tasks but as the softirq cannot be preempted,
            the PF_MEMALLOC flag does not leak to other code by accident.
    
    [davem@davemloft.net: Document why PF_MEMALLOC is safe]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: David Miller <davem@davemloft.net>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Mike Christie <michaelc@cs.wisc.edu>
    Cc: Eric B Munson <emunson@mgebm.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 671f9594e368..b73e681df09e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -210,6 +210,14 @@ asmlinkage void __do_softirq(void)
 	__u32 pending;
 	int max_restart = MAX_SOFTIRQ_RESTART;
 	int cpu;
+	unsigned long old_flags = current->flags;
+
+	/*
+	 * Mask out PF_MEMALLOC s current task context is borrowed for the
+	 * softirq. A softirq handled such as network RX might set PF_MEMALLOC
+	 * again if the socket is related to swap
+	 */
+	current->flags &= ~PF_MEMALLOC;
 
 	pending = local_softirq_pending();
 	account_system_vtime(current);
@@ -265,6 +273,7 @@ asmlinkage void __do_softirq(void)
 
 	account_system_vtime(current);
 	__local_bh_enable(SOFTIRQ_OFFSET);
+	tsk_restore_flags(current, old_flags, PF_MEMALLOC);
 }
 
 #ifndef __ARCH_HAS_DO_SOFTIRQ

commit 161f7a7161191ab9c2e97f787829ef8dd2b95771
Merge: 2ba68940c893 a078c6d0e628
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 20 10:32:09 2012 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer changes for v3.4 from Ingo Molnar
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (32 commits)
      ntp: Fix integer overflow when setting time
      math: Introduce div64_long
      cs5535-clockevt: Allow the MFGPT IRQ to be shared
      cs5535-clockevt: Don't ignore MFGPT on SMP-capable kernels
      x86/time: Eliminate unused irq0_irqs counter
      clocksource: scx200_hrt: Fix the build
      x86/tsc: Reduce the TSC sync check time for core-siblings
      timer: Fix bad idle check on irq entry
      nohz: Remove ts->Einidle checks before restarting the tick
      nohz: Remove update_ts_time_stat from tick_nohz_start_idle
      clockevents: Leave the broadcast device in shutdown mode when not needed
      clocksource: Load the ACPI PM clocksource asynchronously
      clocksource: scx200_hrt: Convert scx200 to use clocksource_register_hz
      clocksource: Get rid of clocksource_calc_mult_shift()
      clocksource: dbx500: convert to clocksource_register_hz()
      clocksource: scx200_hrt:  use pr_<level> instead of printk
      time: Move common updates to a function
      time: Reorder so the hot data is together
      time: Remove most of xtime_lock usage in timekeeping.c
      ntp: Add ntp_lock to replace xtime_locking
      ...

commit 2ba68940c893c8f0bfc8573c041254251bb6aeab
Merge: 9c2b957db177 600e14588280
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 20 10:31:44 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes for v3.4 from Ingo Molnar
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      printk: Make it compile with !CONFIG_PRINTK
      sched/x86: Fix overflow in cyc2ns_offset
      sched: Fix nohz load accounting -- again!
      sched: Update yield() docs
      printk/sched: Introduce special printk_sched() for those awkward moments
      sched/nohz: Correctly initialize 'next_balance' in 'nohz' idle balancer
      sched: Cleanup cpu_active madness
      sched: Fix load-balance wreckage
      sched: Clean up parameter passing of proc_sched_autogroup_set_nice()
      sched: Ditch per cgroup task lists for load-balancing
      sched: Rename load-balancing fields
      sched: Move load-balancing arguments into helper struct
      sched/rt: Do not submit new work when PI-blocked
      sched/rt: Prevent idle task boosting
      sched/wait: Add __wake_up_all_locked() API
      sched/rt: Document scheduler related skip-resched-check sites
      sched/rt: Use schedule_preempt_disabled()
      sched/rt: Add schedule_preempt_disabled()
      sched/rt: Do not throttle when PI boosting
      sched/rt: Keep period timer ticking when rt throttling is active
      ...

commit 9c2b957db1772ebf942ae7a9346b14eba6c8ca66
Merge: 0bbfcaff9b2a bea95c152dee
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 20 10:29:15 2012 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf events changes for v3.4 from Ingo Molnar:
    
     - New "hardware based branch profiling" feature both on the kernel and
       the tooling side, on CPUs that support it.  (modern x86 Intel CPUs
       with the 'LBR' hardware feature currently.)
    
       This new feature is basically a sophisticated 'magnifying glass' for
       branch execution - something that is pretty difficult to extract from
       regular, function histogram centric profiles.
    
       The simplest mode is activated via 'perf record -b', and the result
       looks like this in perf report:
    
            $ perf record -b any_call,u -e cycles:u branchy
    
            $ perf report -b --sort=symbol
                52.34%  [.] main                   [.] f1
                24.04%  [.] f1                     [.] f3
                23.60%  [.] f1                     [.] f2
                 0.01%  [k] _IO_new_file_xsputn    [k] _IO_file_overflow
                 0.01%  [k] _IO_vfprintf_internal  [k] _IO_new_file_xsputn
                 0.01%  [k] _IO_vfprintf_internal  [k] strchrnul
                 0.01%  [k] __printf               [k] _IO_vfprintf_internal
                 0.01%  [k] main                   [k] __printf
    
       This output shows from/to branch columns and shows the highest
       percentage (from,to) jump combinations - i.e.  the most likely taken
       branches in the system.  "branches" can also include function calls
       and any other synchronous and asynchronous transitions of the
       instruction pointer that are not 'next instruction' - such as system
       calls, traps, interrupts, etc.
    
       This feature comes with (hopefully intuitive) flat ascii and TUI
       support in perf report.
    
     - Various 'perf annotate' visual improvements for us assembly junkies.
       It will now recognize function calls in the TUI and by hitting enter
       you can follow the call (recursively) and back, amongst other
       improvements.
    
     - Multiple threads/processes recording support in perf record, perf
       stat, perf top - which is activated via a comma-list of PIDs:
    
            perf top -p 21483,21485
            perf stat -p 21483,21485 -ddd
            perf record -p 21483,21485
    
     - Support for per UID views, via the --uid paramter to perf top, perf
       report, etc.  For example 'perf top --uid mingo' will only show the
       tasks that I am running, excluding other users, root, etc.
    
     - Jump label restructurings and improvements - this includes the
       factoring out of the (hopefully much clearer) include/linux/static_key.h
       generic facility:
    
            struct static_key key = STATIC_KEY_INIT_FALSE;
    
            ...
    
            if (static_key_false(&key))
                    do unlikely code
            else
                    do likely code
    
            ...
            static_key_slow_inc();
            ...
            static_key_slow_inc();
            ...
    
       The static_key_false() branch will be generated into the code with as
       little impact to the likely code path as possible.  the
       static_key_slow_*() APIs flip the branch via live kernel code patching.
    
       This facility can now be used more widely within the kernel to
       micro-optimize hot branches whose likelihood matches the static-key
       usage and fast/slow cost patterns.
    
     - SW function tracer improvements: perf support and filtering support.
    
     - Various hardenings of the perf.data ABI, to make older perf.data's
       smoother on newer tool versions, to make new features integrate more
       smoothly, to support cross-endian recording/analyzing workflows
       better, etc.
    
     - Restructuring of the kprobes code, the splitting out of 'optprobes',
       and a corner case bugfix.
    
     - Allow the tracing of kernel console output (printk).
    
     - Improvements/fixes to user-space RDPMC support, allowing user-space
       self-profiling code to extract PMU counts without performing any
       system calls, while playing nice with the kernel side.
    
     - 'perf bench' improvements
    
     - ... and lots of internal restructurings, cleanups and fixes that made
       these features possible.  And, as usual this list is incomplete as
       there were also lots of other improvements
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (120 commits)
      perf report: Fix annotate double quit issue in branch view mode
      perf report: Remove duplicate annotate choice in branch view mode
      perf/x86: Prettify pmu config literals
      perf report: Enable TUI in branch view mode
      perf report: Auto-detect branch stack sampling mode
      perf record: Add HEADER_BRANCH_STACK tag
      perf record: Provide default branch stack sampling mode option
      perf tools: Make perf able to read files from older ABIs
      perf tools: Fix ABI compatibility bug in print_event_desc()
      perf tools: Enable reading of perf.data files from different ABI rev
      perf: Add ABI reference sizes
      perf report: Add support for taken branch sampling
      perf record: Add support for sampling taken branch
      perf tools: Add code to support PERF_SAMPLE_BRANCH_STACK
      x86/kprobes: Split out optprobe related code to kprobes-opt.c
      x86/kprobes: Fix a bug which can modify kernel code permanently
      x86/kprobes: Fix instruction recovery on optimized path
      perf: Add callback to flush branch_stack on context switch
      perf: Disable PERF_SAMPLE_BRANCH_* when not supported
      perf/x86: Add LBR software filter support for Intel CPUs
      ...

commit b2a00178614e2cdd981a708d22a05c1ce4eadfd7
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Mar 5 15:07:25 2012 -0800

    softirq: Reduce invoke_softirq() code duplication
    
    The two invoke_softirq() variants are identical except for a single
    line. So move the #ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED inside one of
    the functions and get rid of the other one.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 4eb3a0fa351e..c82d95a022ef 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -310,31 +310,21 @@ void irq_enter(void)
 	__irq_enter();
 }
 
-#ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED
 static inline void invoke_softirq(void)
 {
-	if (!force_irqthreads)
+	if (!force_irqthreads) {
+#ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED
 		__do_softirq();
-	else {
-		__local_bh_disable((unsigned long)__builtin_return_address(0),
-				SOFTIRQ_OFFSET);
-		wakeup_softirqd();
-		__local_bh_enable(SOFTIRQ_OFFSET);
-	}
-}
 #else
-static inline void invoke_softirq(void)
-{
-	if (!force_irqthreads)
 		do_softirq();
-	else {
+#endif
+	} else {
 		__local_bh_disable((unsigned long)__builtin_return_address(0),
 				SOFTIRQ_OFFSET);
 		wakeup_softirqd();
 		__local_bh_enable(SOFTIRQ_OFFSET);
 	}
 }
-#endif
 
 /*
  * Exit an interrupt context. Process softirqs if needed and possible:

commit ba74c1448f127649046615ec017bded7b2a76f29
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 21 13:32:17 2011 +0100

    sched/rt: Document scheduler related skip-resched-check sites
    
    Create a distinction between scheduler related preempt_enable_no_resched()
    calls and the nearly one hundred other places in the kernel that do not
    want to reschedule, for one reason or another.
    
    This distinction matters for -rt, where the scheduler and the non-scheduler
    preempt models (and checks) are different. For upstream it's purely
    documentational.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/n/tip-gs88fvx2mdv5psnzxnv575ke@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 79b524767a24..f268369ebe1f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -353,7 +353,7 @@ void irq_exit(void)
 		tick_nohz_irq_exit();
 #endif
 	rcu_irq_exit();
-	preempt_enable_no_resched();
+	sched_preempt_enable_no_resched();
 }
 
 /*
@@ -759,7 +759,7 @@ static int run_ksoftirqd(void * __bind_cpu)
 			if (local_softirq_pending())
 				__do_softirq();
 			local_irq_enable();
-			preempt_enable_no_resched();
+			sched_preempt_enable_no_resched();
 			cond_resched();
 			preempt_disable();
 			rcu_note_context_switch((long)__bind_cpu);

commit bd2f55361f18347e890d52ff9cfd8895455ec11b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 21 12:33:18 2011 +0100

    sched/rt: Use schedule_preempt_disabled()
    
    Coccinelle based conversion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-24swm5zut3h9c4a6s46x8rws@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 4eb3a0fa351e..79b524767a24 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -744,9 +744,7 @@ static int run_ksoftirqd(void * __bind_cpu)
 	while (!kthread_should_stop()) {
 		preempt_disable();
 		if (!local_softirq_pending()) {
-			preempt_enable_no_resched();
-			schedule();
-			preempt_disable();
+			schedule_preempt_disabled();
 		}
 
 		__set_current_state(TASK_RUNNING);

commit 0a8a2e78b7eece7c65884fcff9f98dc0fce89ee4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 24 18:59:44 2012 +0100

    timer: Fix bad idle check on irq entry
    
    idle_cpu() is called on irq entry to guess if we need to call
    tick_check_idle(). This way we can catch up with jiffies if the tick
    was stopped, stop accounting idle time during the interrupt and
    maintain the sched clock if it is unstable.
    
    But if we are going to exit the idle loop to schedule a new task (ie:
    if we have a task in the runqueue or a remotely enqueued ttwu to
    perform), the idle_cpu() check will return 0 such that we miss the
    call to tick_check_idle() for all interrupts happening before we
    schedule the new task.
    
    As a result these interrupts and the softirqs coming along may deal
    with stale jiffies values, bad sched clock values, and won't substract
    their time from the idle time accounting.
    
    Fix this with using is_idle_task() instead that strictly checks that
    we are running the idle task, without caring about the fact we are
    going to schedule a task soon.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Link: http://lkml.kernel.org/r/1327427984-23282-3-git-send-email-fweisbec@gmail.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 4eb3a0fa351e..5ace266bc0e6 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -297,7 +297,7 @@ void irq_enter(void)
 	int cpu = smp_processor_id();
 
 	rcu_irq_enter();
-	if (idle_cpu(cpu) && !in_interrupt()) {
+	if (is_idle_task(current) && !in_interrupt()) {
 		/*
 		 * Prevent raise_softirq from needlessly waking up ksoftirqd
 		 * here, as softirq will be serviced on return from interrupt.

commit f069686e4bdc60a637d210ea3eea25fcdb82df88
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Jan 25 20:18:55 2012 -0500

    tracing/softirq: Move __raise_softirq_irqoff() out of header
    
    The __raise_softirq_irqoff() contains a tracepoint. As tracepoints in headers
    can cause issues, and not to mention, bloats the kernel when they are
    in a static inline, it is best to move the function that contains the
    tracepoint out of the header and into softirq.c.
    
    Link: http://lkml.kernel.org/r/20120118120711.GB14863@elte.hu
    
    Suggested-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 4eb3a0fa351e..06d40993594a 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -385,6 +385,12 @@ void raise_softirq(unsigned int nr)
 	local_irq_restore(flags);
 }
 
+void __raise_softirq_irqoff(unsigned int nr)
+{
+	trace_softirq_raise(nr);
+	or_softirq_pending(1UL << nr);
+}
+
 void open_softirq(int nr, void (*action)(struct softirq_action *))
 {
 	softirq_vec[nr].action = action;

commit 416eb33cd60ef405e2860a186364e57bcb2d89f6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 7 16:31:02 2011 -0700

    rcu: Fix early call to rcu_idle_enter()
    
    On the irq exit path, tick_nohz_irq_exit()
    may raise a softirq, which action leads to the wake up
    path and select_task_rq_fair() that makes use of rcu
    to iterate the domains.
    
    This is an illegal use of RCU because we may be in RCU
    extended quiescent state if we interrupted an RCU-idle
    window in the idle loop:
    
    [  132.978883] ===============================
    [  132.978883] [ INFO: suspicious RCU usage. ]
    [  132.978883] -------------------------------
    [  132.978883] kernel/sched_fair.c:1707 suspicious rcu_dereference_check() usage!
    [  132.978883]
    [  132.978883] other info that might help us debug this:
    [  132.978883]
    [  132.978883]
    [  132.978883] rcu_scheduler_active = 1, debug_locks = 0
    [  132.978883] RCU used illegally from extended quiescent state!
    [  132.978883] 2 locks held by swapper/0:
    [  132.978883]  #0:  (&p->pi_lock){-.-.-.}, at: [<ffffffff8105a729>] try_to_wake_up+0x39/0x2f0
    [  132.978883]  #1:  (rcu_read_lock){.+.+..}, at: [<ffffffff8105556a>] select_task_rq_fair+0x6a/0xec0
    [  132.978883]
    [  132.978883] stack backtrace:
    [  132.978883] Pid: 0, comm: swapper Tainted: G        W   3.0.0+ #178
    [  132.978883] Call Trace:
    [  132.978883]  <IRQ>  [<ffffffff810a01f6>] lockdep_rcu_suspicious+0xe6/0x100
    [  132.978883]  [<ffffffff81055c49>] select_task_rq_fair+0x749/0xec0
    [  132.978883]  [<ffffffff8105556a>] ? select_task_rq_fair+0x6a/0xec0
    [  132.978883]  [<ffffffff812fe494>] ? do_raw_spin_lock+0x54/0x150
    [  132.978883]  [<ffffffff810a1f2d>] ? trace_hardirqs_on+0xd/0x10
    [  132.978883]  [<ffffffff8105a7c3>] try_to_wake_up+0xd3/0x2f0
    [  132.978883]  [<ffffffff81094f98>] ? ktime_get+0x68/0xf0
    [  132.978883]  [<ffffffff8105aa35>] wake_up_process+0x15/0x20
    [  132.978883]  [<ffffffff81069dd5>] raise_softirq_irqoff+0x65/0x110
    [  132.978883]  [<ffffffff8108eb65>] __hrtimer_start_range_ns+0x415/0x5a0
    [  132.978883]  [<ffffffff812fe3ee>] ? do_raw_spin_unlock+0x5e/0xb0
    [  132.978883]  [<ffffffff8108ed08>] hrtimer_start+0x18/0x20
    [  132.978883]  [<ffffffff8109c9c3>] tick_nohz_stop_sched_tick+0x393/0x450
    [  132.978883]  [<ffffffff810694f2>] irq_exit+0xd2/0x100
    [  132.978883]  [<ffffffff81829e96>] do_IRQ+0x66/0xe0
    [  132.978883]  [<ffffffff81820d53>] common_interrupt+0x13/0x13
    [  132.978883]  <EOI>  [<ffffffff8103434b>] ? native_safe_halt+0xb/0x10
    [  132.978883]  [<ffffffff810a1f2d>] ? trace_hardirqs_on+0xd/0x10
    [  132.978883]  [<ffffffff810144ea>] default_idle+0xba/0x370
    [  132.978883]  [<ffffffff810147fe>] amd_e400_idle+0x5e/0x130
    [  132.978883]  [<ffffffff8100a9f6>] cpu_idle+0xb6/0x120
    [  132.978883]  [<ffffffff817f217f>] rest_init+0xef/0x150
    [  132.978883]  [<ffffffff817f20e2>] ? rest_init+0x52/0x150
    [  132.978883]  [<ffffffff81ed9cf3>] start_kernel+0x3da/0x3e5
    [  132.978883]  [<ffffffff81ed9346>] x86_64_start_reservations+0x131/0x135
    [  132.978883]  [<ffffffff81ed944d>] x86_64_start_kernel+0x103/0x112
    
    Fix this by calling rcu_idle_enter() after tick_nohz_irq_exit().
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index f9f2aa81ce53..4eb3a0fa351e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -347,12 +347,12 @@ void irq_exit(void)
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
 
-	rcu_irq_exit();
 #ifdef CONFIG_NO_HZ
 	/* Make sure that timer wheel updates are propagated */
 	if (idle_cpu(smp_processor_id()) && !in_interrupt() && !need_resched())
 		tick_nohz_irq_exit();
 #endif
+	rcu_irq_exit();
 	preempt_enable_no_resched();
 }
 

commit 280f06774afedf849f0b34248ed6aff57d0f6908
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 7 18:22:06 2011 +0200

    nohz: Separate out irq exit and idle loop dyntick logic
    
    The tick_nohz_stop_sched_tick() function, which tries to delay
    the next timer tick as long as possible, can be called from two
    places:
    
    - From the idle loop to start the dytick idle mode
    - From interrupt exit if we have interrupted the dyntick
    idle mode, so that we reprogram the next tick event in
    case the irq changed some internal state that requires this
    action.
    
    There are only few minor differences between both that
    are handled by that function, driven by the ts->inidle
    cpu variable and the inidle parameter. The whole guarantees
    that we only update the dyntick mode on irq exit if we actually
    interrupted the dyntick idle mode, and that we enter in RCU extended
    quiescent state from idle loop entry only.
    
    Split this function into:
    
    - tick_nohz_idle_enter(), which sets ts->inidle to 1, enters
    dynticks idle mode unconditionally if it can, and enters into RCU
    extended quiescent state.
    
    - tick_nohz_irq_exit() which only updates the dynticks idle mode
    when ts->inidle is set (ie: if tick_nohz_idle_enter() has been called).
    
    To maintain symmetry, tick_nohz_restart_sched_tick() has been renamed
    into tick_nohz_idle_exit().
    
    This simplifies the code and micro-optimize the irq exit path (no need
    for local_irq_save there). This also prepares for the split between
    dynticks and rcu extended quiescent state logics. We'll need this split to
    further fix illegal uses of RCU in extended quiescent states in the idle
    loop.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: David Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Hans-Christian Egtvedt <hans-christian.egtvedt@atmel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 2c71d91efff0..f9f2aa81ce53 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -351,7 +351,7 @@ void irq_exit(void)
 #ifdef CONFIG_NO_HZ
 	/* Make sure that timer wheel updates are propagated */
 	if (idle_cpu(smp_processor_id()) && !in_interrupt() && !need_resched())
-		tick_nohz_stop_sched_tick(0);
+		tick_nohz_irq_exit();
 #endif
 	preempt_enable_no_resched();
 }

commit 9984de1a5a8a96275fcab818f7419af5a3c86e71
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon May 23 14:51:41 2011 -0400

    kernel: Map most files to use export.h instead of module.h
    
    The changed files were only including linux/module.h for the
    EXPORT_SYMBOL infrastructure, and nothing else.  Revector them
    onto the isolated export header for faster compile times.
    
    Nothing to see here but a whole lot of instances of:
    
      -#include <linux/module.h>
      +#include <linux/export.h>
    
    This commit is only changing the kernel dir; next targets
    will probably be mm, fs, the arch dirs, etc.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index fca82c32042b..2c71d91efff0 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -10,7 +10,7 @@
  *	Remote softirq infrastructure is by Jens Axboe.
  */
 
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/kernel_stat.h>
 #include <linux/interrupt.h>
 #include <linux/init.h>

commit ec433f0c51527426989ea8a38a856d810d739414
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Jul 19 15:32:00 2011 -0700

    softirq,rcu: Inform RCU of irq_exit() activity
    
    The rcu_read_unlock_special() function relies on in_irq() to exclude
    scheduler activity from interrupt level.  This fails because exit_irq()
    can invoke the scheduler after clearing the preempt_count() bits that
    in_irq() uses to determine that it is at interrupt level.  This situation
    can result in failures as follows:
    
     $task                  IRQ             SoftIRQ
    
     rcu_read_lock()
    
     /* do stuff */
    
     <preempt> |= UNLOCK_BLOCKED
    
     rcu_read_unlock()
       --t->rcu_read_lock_nesting
    
                            irq_enter();
                            /* do stuff, don't use RCU */
                            irq_exit();
                              sub_preempt_count(IRQ_EXIT_OFFSET);
                              invoke_softirq()
    
                                            ttwu();
                                              spin_lock_irq(&pi->lock)
                                              rcu_read_lock();
                                              /* do stuff */
                                              rcu_read_unlock();
                                                rcu_read_unlock_special()
                                                  rcu_report_exp_rnp()
                                                    ttwu()
                                                      spin_lock_irq(&pi->lock) /* deadlock */
    
       rcu_read_unlock_special(t);
    
    Ed can simply trigger this 'easy' because invoke_softirq() immediately
    does a ttwu() of ksoftirqd/# instead of doing the in-place softirq stuff
    first, but even without that the above happens.
    
    Cure this by also excluding softirqs from the
    rcu_read_unlock_special() handler and ensuring the force_irqthreads
    ksoftirqd/# wakeup is done from full softirq context.
    
    [ Alternatively, delaying the ->rcu_read_lock_nesting decrement
      until after the special handling would make the thing more robust
      in the face of interrupts as well.  And there is a separate patch
      for that. ]
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Reported-and-tested-by: Ed Tomlinson <edt@aei.ca>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 40cf63ddd4b3..fca82c32042b 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -315,16 +315,24 @@ static inline void invoke_softirq(void)
 {
 	if (!force_irqthreads)
 		__do_softirq();
-	else
+	else {
+		__local_bh_disable((unsigned long)__builtin_return_address(0),
+				SOFTIRQ_OFFSET);
 		wakeup_softirqd();
+		__local_bh_enable(SOFTIRQ_OFFSET);
+	}
 }
 #else
 static inline void invoke_softirq(void)
 {
 	if (!force_irqthreads)
 		do_softirq();
-	else
+	else {
+		__local_bh_disable((unsigned long)__builtin_return_address(0),
+				SOFTIRQ_OFFSET);
 		wakeup_softirqd();
+		__local_bh_enable(SOFTIRQ_OFFSET);
+	}
 }
 #endif
 

commit 09223371deac67d08ca0b70bd18787920284c967
Author: Shaohua Li <shaohua.li@intel.com>
Date:   Tue Jun 14 13:26:25 2011 +0800

    rcu: Use softirq to address performance regression
    
    Commit a26ac2455ffcf3(rcu: move TREE_RCU from softirq to kthread)
    introduced performance regression. In an AIM7 test, this commit degraded
    performance by about 40%.
    
    The commit runs rcu callbacks in a kthread instead of softirq. We observed
    high rate of context switch which is caused by this. Out test system has
    64 CPUs and HZ is 1000, so we saw more than 64k context switch per second
    which is caused by RCU's per-CPU kthread.  A trace showed that most of
    the time the RCU per-CPU kthread doesn't actually handle any callbacks,
    but instead just does a very small amount of work handling grace periods.
    This means that RCU's per-CPU kthreads are making the scheduler do quite
    a bit of work in order to allow a very small amount of RCU-related
    processing to be done.
    
    Alex Shi's analysis determined that this slowdown is due to lock
    contention within the scheduler.  Unfortunately, as Peter Zijlstra points
    out, the scheduler's real-time semantics require global action, which
    means that this contention is inherent in real-time scheduling.  (Yes,
    perhaps someone will come up with a workaround -- otherwise, -rt is not
    going to do well on large SMP systems -- but this patch will work around
    this issue in the meantime.  And "the meantime" might well be forever.)
    
    This patch therefore re-introduces softirq processing to RCU, but only
    for core RCU work.  RCU callbacks are still executed in kthread context,
    so that only a small amount of RCU work runs in softirq context in the
    common case.  This should minimize ksoftirqd execution, allowing us to
    skip boosting of ksoftirqd for CONFIG_RCU_BOOST=y kernels.
    
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Tested-by: "Alex,Shi" <alex.shi@intel.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 13960170cad4..40cf63ddd4b3 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -58,7 +58,7 @@ DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
 
 char *softirq_to_name[NR_SOFTIRQS] = {
 	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL",
-	"TASKLET", "SCHED", "HRTIMER"
+	"TASKLET", "SCHED", "HRTIMER", "RCU"
 };
 
 /*

commit a26ac2455ffcf3be5c6ef92bc6df7182700f2114
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Wed Jan 12 14:10:23 2011 -0800

    rcu: move TREE_RCU from softirq to kthread
    
    If RCU priority boosting is to be meaningful, callback invocation must
    be boosted in addition to preempted RCU readers.  Otherwise, in presence
    of CPU real-time threads, the grace period ends, but the callbacks don't
    get invoked.  If the callbacks don't get invoked, the associated memory
    doesn't get freed, so the system is still subject to OOM.
    
    But it is not reasonable to priority-boost RCU_SOFTIRQ, so this commit
    moves the callback invocations to a kthread, which can be boosted easily.
    
    Also add comments and properly synchronized all accesses to
    rcu_cpu_kthread_task, as suggested by Lai Jiangshan.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 174f976c2874..13960170cad4 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -58,7 +58,7 @@ DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
 
 char *softirq_to_name[NR_SOFTIRQS] = {
 	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL",
-	"TASKLET", "SCHED", "HRTIMER",	"RCU"
+	"TASKLET", "SCHED", "HRTIMER"
 };
 
 /*

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 735d87095172..174f976c2874 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -567,7 +567,7 @@ static void __tasklet_hrtimer_trampoline(unsigned long data)
 /**
  * tasklet_hrtimer_init - Init a tasklet/hrtimer combo for softirq callbacks
  * @ttimer:	 tasklet_hrtimer which is initialized
- * @function:	 hrtimer callback funtion which gets called from softirq context
+ * @function:	 hrtimer callback function which gets called from softirq context
  * @which_clock: clock id (CLOCK_MONOTONIC/CLOCK_REALTIME)
  * @mode:	 hrtimer mode (HRTIMER_MODE_ABS/HRTIMER_MODE_REL)
  */

commit 94dcf29a11b3d20a28790598d701f98484a969da
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Mar 22 16:30:45 2011 -0700

    kthread: use kthread_create_on_node()
    
    ksoftirqd, kworker, migration, and pktgend kthreads can be created with
    kthread_create_on_node(), to get proper NUMA affinities for their stack and
    task_struct.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 56e5dec837f0..735d87095172 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -845,7 +845,10 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 	switch (action) {
 	case CPU_UP_PREPARE:
 	case CPU_UP_PREPARE_FROZEN:
-		p = kthread_create(run_ksoftirqd, hcpu, "ksoftirqd/%d", hotcpu);
+		p = kthread_create_on_node(run_ksoftirqd,
+					   hcpu,
+					   cpu_to_node(hotcpu),
+					   "ksoftirqd/%d", hotcpu);
 		if (IS_ERR(p)) {
 			printk("ksoftirqd for %i failed\n", hotcpu);
 			return notifier_from_errno(PTR_ERR(p));

commit 5f6fb45466b2273ffb91c9cf209f164f666c33b1
Merge: 3904afb41d43 c0185808eb85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 19:23:40 2011 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (116 commits)
      x86: Enable forced interrupt threading support
      x86: Mark low level interrupts IRQF_NO_THREAD
      x86: Use generic show_interrupts
      x86: ioapic: Avoid redundant lookup of irq_cfg
      x86: ioapic: Use new move_irq functions
      x86: Use the proper accessors in fixup_irqs()
      x86: ioapic: Use irq_data->state
      x86: ioapic: Simplify irq chip and handler setup
      x86: Cleanup the genirq name space
      genirq: Add chip flag to force mask on suspend
      genirq: Add desc->irq_data accessor
      genirq: Add comments to Kconfig switches
      genirq: Fixup fasteoi handler for oneshot mode
      genirq: Provide forced interrupt threading
      sched: Switch wait_task_inactive to schedule_hrtimeout()
      genirq: Add IRQF_NO_THREAD
      genirq: Allow shared oneshot interrupts
      genirq: Prepare the handling of shared oneshot interrupts
      genirq: Make warning in handle_percpu_event useful
      x86: ioapic: Move trigger defines to io_apic.h
      ...
    
    Fix up trivial(?) conflicts in arch/x86/pci/xen.c due to genirq name
    space changes clashing with the Xen cleanups.  The set_irq_msi() had
    moved to xen_bind_pirq_msi_to_irq().

commit 8d32a307e4faa8b123dc8a9cd56d1a7525f69ad3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 23 23:52:23 2011 +0000

    genirq: Provide forced interrupt threading
    
    Add a commandline parameter "threadirqs" which forces all interrupts except
    those marked IRQF_NO_THREAD to run threaded. That's mostly a debug option to
    allow retrieving better debug data from crashing interrupt handlers. If
    "threadirqs" is not enabled on the kernel command line, then there is no
    impact in the interrupt hotpath.
    
    Architecture code needs to select CONFIG_IRQ_FORCED_THREADING after
    marking the interrupts which cant be threaded IRQF_NO_THREAD. All
    interrupts which have IRQF_TIMER set are implict marked
    IRQF_NO_THREAD. Also all PER_CPU interrupts are excluded.
    
    Forced threading hard interrupts also forces all soft interrupt
    handling into thread context.
    
    When enabled it might slow down things a bit, but for debugging problems in
    interrupt code it's a reasonable penalty as it does not immediately
    crash and burn the machine when an interrupt handler is buggy.
    
    Some test results on a Core2Duo machine:
    
    Cache cold run of:
     # time git grep irq_desc
    
          non-threaded       threaded
     real 1m18.741s          1m19.061s
     user 0m1.874s           0m1.757s
     sys  0m5.843s           0m5.427s
    
     # iperf -c server
    non-threaded
    [  3]  0.0-10.0 sec  1.09 GBytes   933 Mbits/sec
    [  3]  0.0-10.0 sec  1.09 GBytes   934 Mbits/sec
    [  3]  0.0-10.0 sec  1.09 GBytes   933 Mbits/sec
    threaded
    [  3]  0.0-10.0 sec  1.09 GBytes   939 Mbits/sec
    [  3]  0.0-10.0 sec  1.09 GBytes   934 Mbits/sec
    [  3]  0.0-10.0 sec  1.09 GBytes   937 Mbits/sec
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <20110223234956.772668648@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index c0490464e92f..a33fb2911248 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -311,9 +311,21 @@ void irq_enter(void)
 }
 
 #ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED
-# define invoke_softirq()	__do_softirq()
+static inline void invoke_softirq(void)
+{
+	if (!force_irqthreads)
+		__do_softirq();
+	else
+		wakeup_softirqd();
+}
 #else
-# define invoke_softirq()	do_softirq()
+static inline void invoke_softirq(void)
+{
+	if (!force_irqthreads)
+		do_softirq();
+	else
+		wakeup_softirqd();
+}
 #endif
 
 /*

commit c305d524e5dd3c3c7a6035083e30950bea1b52dc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 2 17:10:48 2011 +0100

    softirq: Avoid stack switch from ksoftirqd
    
    ksoftirqd() calls do_softirq() which switches stacks on several
    architectures. That makes no sense at all. ksoftirqd's stack is
    sufficient.
    
    Call __do_softirq() directly.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: David Miller <davem@davemloft.net>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    LKML-Reference: <alpine.LFD.2.00.1102021704530.31804@localhost6.localdomain6>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 68eb5efec388..c0490464e92f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -738,7 +738,10 @@ static int run_ksoftirqd(void * __bind_cpu)
 			   don't process */
 			if (cpu_is_offline((long)__bind_cpu))
 				goto wait_to_die;
-			do_softirq();
+			local_irq_disable();
+			if (local_softirq_pending())
+				__do_softirq();
+			local_irq_enable();
 			preempt_enable_no_resched();
 			cond_resched();
 			preempt_disable();

commit 4dd53d891ca46dcc1fde0376a33540d3fd83cb9a
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Tue Dec 21 17:09:00 2010 -0800

    softirqs: Free up pf flag PF_KSOFTIRQD
    
    Cleanup patch, freeing up PF_KSOFTIRQD and use per_cpu ksoftirqd pointer
    instead, as suggested by Eric Dumazet.
    
    Tested-by: Shaun Ruffell <sruffell@digium.com>
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1292980144-28796-2-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 68eb5efec388..0cee50487629 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -54,7 +54,7 @@ EXPORT_SYMBOL(irq_stat);
 
 static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;
 
-static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
+DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
 
 char *softirq_to_name[NR_SOFTIRQS] = {
 	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL",
@@ -721,7 +721,6 @@ static int run_ksoftirqd(void * __bind_cpu)
 {
 	set_current_state(TASK_INTERRUPTIBLE);
 
-	current->flags |= PF_KSOFTIRQD;
 	while (!kthread_should_stop()) {
 		preempt_disable();
 		if (!local_softirq_pending()) {

commit 351f8f8e6499ae4fff40f5e3a8fe16d9e1903646
Author: Amerigo Wang <amwang@redhat.com>
Date:   Wed Jan 12 16:59:39 2011 -0800

    kernel: clean up USE_GENERIC_SMP_HELPERS
    
    For arch which needs USE_GENERIC_SMP_HELPERS, it has to select
    USE_GENERIC_SMP_HELPERS, rather than leaving a choice to user, since they
    don't provide their own implementions.
    
    Also, move on_each_cpu() to kernel/smp.c, it is strange to put it in
    kernel/softirq.c.
    
    For arch which doesn't use USE_GENERIC_SMP_HELPERS, e.g.  blackfin, only
    on_each_cpu() is compiled.
    
    Signed-off-by: Amerigo Wang <amwang@redhat.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 0823778f87fc..68eb5efec388 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -885,25 +885,6 @@ static __init int spawn_ksoftirqd(void)
 }
 early_initcall(spawn_ksoftirqd);
 
-#ifdef CONFIG_SMP
-/*
- * Call a function on all processors
- */
-int on_each_cpu(void (*func) (void *info), void *info, int wait)
-{
-	int ret = 0;
-
-	preempt_disable();
-	ret = smp_call_function(func, info, wait);
-	local_irq_disable();
-	func(info);
-	local_irq_enable();
-	preempt_enable();
-	return ret;
-}
-EXPORT_SYMBOL(on_each_cpu);
-#endif
-
 /*
  * [ These __weak aliases are kept in a separate compilation unit, so that
  *   GCC does not inline them incorrectly. ]

commit 72eb6a791459c87a0340318840bb3bd9252b627b
Merge: 23d69b09b78c 55ee4ef30241
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 7 17:02:58 2011 -0800

    Merge branch 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (30 commits)
      gameport: use this_cpu_read instead of lookup
      x86: udelay: Use this_cpu_read to avoid address calculation
      x86: Use this_cpu_inc_return for nmi counter
      x86: Replace uses of current_cpu_data with this_cpu ops
      x86: Use this_cpu_ops to optimize code
      vmstat: User per cpu atomics to avoid interrupt disable / enable
      irq_work: Use per cpu atomics instead of regular atomics
      cpuops: Use cmpxchg for xchg to avoid lock semantics
      x86: this_cpu_cmpxchg and this_cpu_xchg operations
      percpu: Generic this_cpu_cmpxchg() and this_cpu_xchg support
      percpu,x86: relocate this_cpu_add_return() and friends
      connector: Use this_cpu operations
      xen: Use this_cpu_inc_return
      taskstats: Use this_cpu_ops
      random: Use this_cpu_inc_return
      fs: Use this_cpu_inc_return in buffer.c
      highmem: Use this_cpu_xx_return() operations
      vmstat: Use this_cpu_inc_return for vm statistics
      x86: Support for this_cpu_add, sub, dec, inc_return
      percpu: Generic support for this_cpu_add, sub, dec, inc_return
      ...
    
    Fixed up conflicts: in arch/x86/kernel/{apic/nmi.c, apic/x2apic_uv_x.c, process.c}
    as per Tejun.

commit c9b5f501ef1580faa30c40c644b7691870462201
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 7 13:41:40 2011 +0100

    sched: Constify function scope static struct sched_param usage
    
    Function-scope statics are discouraged because they are
    easily overlooked and can cause subtle bugs/races due to
    their global (non-SMP safe) nature.
    
    Linus noticed that we did this for sched_param - at minimum
    make the const.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: Message-ID: <AANLkTinotRxScOHEb0HgFgSpGPkq_6jKTv5CfvnQM=ee@mail.gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index d4d918a91881..c10150cb456b 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -853,7 +853,7 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 			     cpumask_any(cpu_online_mask));
 	case CPU_DEAD:
 	case CPU_DEAD_FROZEN: {
-		static struct sched_param param = {
+		static const struct sched_param param = {
 			.sched_priority = MAX_RT_PRIO-1
 		};
 

commit 909ea96468096b07fbb41aaf69be060d92bd9271
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Dec 8 16:22:55 2010 +0100

    core: Replace __get_cpu_var with __this_cpu_read if not used for an address.
    
    __get_cpu_var() can be replaced with this_cpu_read and will then use a
    single read instruction with implied address calculation to access the
    correct per cpu instance.
    
    However, the address of a per cpu variable passed to __this_cpu_read()
    cannot be determined (since it's an implied address conversion through
    segment prefixes).  Therefore apply this only to uses of __get_cpu_var
    where the address of the variable is not used.
    
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 18f4be0d5fe0..d0a0dda52c1a 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -70,7 +70,7 @@ char *softirq_to_name[NR_SOFTIRQS] = {
 static void wakeup_softirqd(void)
 {
 	/* Interrupts are disabled: no need to stop preemption */
-	struct task_struct *tsk = __get_cpu_var(ksoftirqd);
+	struct task_struct *tsk = __this_cpu_read(ksoftirqd);
 
 	if (tsk && tsk->state != TASK_RUNNING)
 		wake_up_process(tsk);
@@ -388,8 +388,8 @@ void __tasklet_schedule(struct tasklet_struct *t)
 
 	local_irq_save(flags);
 	t->next = NULL;
-	*__get_cpu_var(tasklet_vec).tail = t;
-	__get_cpu_var(tasklet_vec).tail = &(t->next);
+	*__this_cpu_read(tasklet_vec.tail) = t;
+	__this_cpu_write(tasklet_vec.tail, &(t->next));
 	raise_softirq_irqoff(TASKLET_SOFTIRQ);
 	local_irq_restore(flags);
 }
@@ -402,8 +402,8 @@ void __tasklet_hi_schedule(struct tasklet_struct *t)
 
 	local_irq_save(flags);
 	t->next = NULL;
-	*__get_cpu_var(tasklet_hi_vec).tail = t;
-	__get_cpu_var(tasklet_hi_vec).tail = &(t->next);
+	*__this_cpu_read(tasklet_hi_vec.tail) = t;
+	__this_cpu_write(tasklet_hi_vec.tail,  &(t->next));
 	raise_softirq_irqoff(HI_SOFTIRQ);
 	local_irq_restore(flags);
 }
@@ -414,8 +414,8 @@ void __tasklet_hi_schedule_first(struct tasklet_struct *t)
 {
 	BUG_ON(!irqs_disabled());
 
-	t->next = __get_cpu_var(tasklet_hi_vec).head;
-	__get_cpu_var(tasklet_hi_vec).head = t;
+	t->next = __this_cpu_read(tasklet_hi_vec.head);
+	__this_cpu_write(tasklet_hi_vec.head, t);
 	__raise_softirq_irqoff(HI_SOFTIRQ);
 }
 
@@ -426,9 +426,9 @@ static void tasklet_action(struct softirq_action *a)
 	struct tasklet_struct *list;
 
 	local_irq_disable();
-	list = __get_cpu_var(tasklet_vec).head;
-	__get_cpu_var(tasklet_vec).head = NULL;
-	__get_cpu_var(tasklet_vec).tail = &__get_cpu_var(tasklet_vec).head;
+	list = __this_cpu_read(tasklet_vec.head);
+	__this_cpu_write(tasklet_vec.head, NULL);
+	__this_cpu_write(tasklet_vec.tail, &__get_cpu_var(tasklet_vec).head);
 	local_irq_enable();
 
 	while (list) {
@@ -449,8 +449,8 @@ static void tasklet_action(struct softirq_action *a)
 
 		local_irq_disable();
 		t->next = NULL;
-		*__get_cpu_var(tasklet_vec).tail = t;
-		__get_cpu_var(tasklet_vec).tail = &(t->next);
+		*__this_cpu_read(tasklet_vec.tail) = t;
+		__this_cpu_write(tasklet_vec.tail, &(t->next));
 		__raise_softirq_irqoff(TASKLET_SOFTIRQ);
 		local_irq_enable();
 	}
@@ -461,9 +461,9 @@ static void tasklet_hi_action(struct softirq_action *a)
 	struct tasklet_struct *list;
 
 	local_irq_disable();
-	list = __get_cpu_var(tasklet_hi_vec).head;
-	__get_cpu_var(tasklet_hi_vec).head = NULL;
-	__get_cpu_var(tasklet_hi_vec).tail = &__get_cpu_var(tasklet_hi_vec).head;
+	list = __this_cpu_read(tasklet_hi_vec.head);
+	__this_cpu_write(tasklet_hi_vec.head, NULL);
+	__this_cpu_write(tasklet_hi_vec.tail, &__get_cpu_var(tasklet_hi_vec).head);
 	local_irq_enable();
 
 	while (list) {
@@ -484,8 +484,8 @@ static void tasklet_hi_action(struct softirq_action *a)
 
 		local_irq_disable();
 		t->next = NULL;
-		*__get_cpu_var(tasklet_hi_vec).tail = t;
-		__get_cpu_var(tasklet_hi_vec).tail = &(t->next);
+		*__this_cpu_read(tasklet_hi_vec.tail) = t;
+		__this_cpu_write(tasklet_hi_vec.tail, &(t->next));
 		__raise_softirq_irqoff(HI_SOFTIRQ);
 		local_irq_enable();
 	}
@@ -802,16 +802,16 @@ static void takeover_tasklets(unsigned int cpu)
 
 	/* Find end, append list for that CPU. */
 	if (&per_cpu(tasklet_vec, cpu).head != per_cpu(tasklet_vec, cpu).tail) {
-		*(__get_cpu_var(tasklet_vec).tail) = per_cpu(tasklet_vec, cpu).head;
-		__get_cpu_var(tasklet_vec).tail = per_cpu(tasklet_vec, cpu).tail;
+		*__this_cpu_read(tasklet_vec.tail) = per_cpu(tasklet_vec, cpu).head;
+		this_cpu_write(tasklet_vec.tail, per_cpu(tasklet_vec, cpu).tail);
 		per_cpu(tasklet_vec, cpu).head = NULL;
 		per_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;
 	}
 	raise_softirq_irqoff(TASKLET_SOFTIRQ);
 
 	if (&per_cpu(tasklet_hi_vec, cpu).head != per_cpu(tasklet_hi_vec, cpu).tail) {
-		*__get_cpu_var(tasklet_hi_vec).tail = per_cpu(tasklet_hi_vec, cpu).head;
-		__get_cpu_var(tasklet_hi_vec).tail = per_cpu(tasklet_hi_vec, cpu).tail;
+		*__this_cpu_read(tasklet_hi_vec.tail) = per_cpu(tasklet_hi_vec, cpu).head;
+		__this_cpu_write(tasklet_hi_vec.tail, per_cpu(tasklet_hi_vec, cpu).tail);
 		per_cpu(tasklet_hi_vec, cpu).head = NULL;
 		per_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;
 	}

commit 92fd4d4d67b945c0766416284d4ab236b31542c4
Merge: fe7de49f9d4e e53beacd23d9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Nov 18 13:22:14 2010 +0100

    Merge commit 'v2.6.37-rc2' into sched/core
    
    Merge reason: Move to a .37-rc base.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a042e26137d7674ac04b1cd2d5c06b9ebc1ee2d5
Merge: f66dd539feb8 e25804a0327d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 27 18:48:00 2010 -0700

    Merge branch 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (50 commits)
      perf python scripting: Add futex-contention script
      perf python scripting: Fixup cut'n'paste error in sctop script
      perf scripting: Shut up 'perf record' final status
      perf record: Remove newline character from perror() argument
      perf python scripting: Support fedora 11 (audit 1.7.17)
      perf python scripting: Improve the syscalls-by-pid script
      perf python scripting: print the syscall name on sctop
      perf python scripting: Improve the syscalls-counts script
      perf python scripting: Improve the failed-syscalls-by-pid script
      kprobes: Remove redundant text_mutex lock in optimize
      x86/oprofile: Fix uninitialized variable use in debug printk
      tracing: Fix 'faild' -> 'failed' typo
      perf probe: Fix format specified for Dwarf_Off parameter
      perf trace: Fix detection of script extension
      perf trace: Use $PERF_EXEC_PATH in canned report scripts
      perf tools: Document event modifiers
      perf tools: Remove direct slang.h include
      perf_events: Fix for transaction recovery in group_sched_in()
      perf_events: Revert: Fix transaction recovery in group_sched_in()
      perf, x86: Use NUMA aware allocations for PEBS/BTS/DS allocations
      ...

commit b8ecad8b2f8757d51632b1ea6d602c1f7b9760a2
Merge: eea4a0b19a27 8bfb5e7d6a14
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Oct 23 20:05:43 2010 +0200

    Merge branch 'perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/acme/linux-2.6 into perf/urgent

commit fe7de49f9d4e53f24ec9ef762a503f70b562341c
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Wed Oct 20 16:01:12 2010 -0700

    sched: Make sched_param argument static in sched_setscheduler() callers
    
    Andrew Morton pointed out almost all sched_setscheduler() callers are
    using fixed parameters and can be converted to static.  It reduces runtime
    memory use a little.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: James Morris <jmorris@namei.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index fc978889b194..081869ed3a9f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -851,7 +851,9 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 			     cpumask_any(cpu_online_mask));
 	case CPU_DEAD:
 	case CPU_DEAD_FROZEN: {
-		struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+		static struct sched_param param = {
+			.sched_priority = MAX_RT_PRIO-1
+		};
 
 		p = per_cpu(ksoftirqd, hotcpu);
 		per_cpu(ksoftirqd, hotcpu) = NULL;

commit 02f36038c568111ad4fc433f6fa760ff5e38fab4
Merge: 6c2754c28f23 676cb02dc32a 07bd8516a2f9 50f2d7f682f9 892df7f81c31 68f4d5a00ada b365a85c6816 6554287b1de0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 23 08:25:36 2010 -0700

    Merge branches 'softirq-for-linus', 'x86-debug-for-linus', 'x86-numa-for-linus', 'x86-quirks-for-linus', 'x86-setup-for-linus', 'x86-uv-for-linus' and 'x86-vm86-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'softirq-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      softirqs: Make wakeup_softirqd static
    
    * 'x86-debug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, asm: Restore parentheses around one pushl_cfi argument
      x86, asm: Fix ancient-GAS workaround
      x86, asm: Fix CFI macro invocations to deal with shortcomings in gas
    
    * 'x86-numa-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, numa: Assign CPUs to nodes in round-robin manner on fake NUMA
    
    * 'x86-quirks-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86: HPET force enable for CX700 / VIA Epia LT
    
    * 'x86-setup-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, setup: Use string copy operation to optimze copy in kernel compression
    
    * 'x86-uv-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, UV: Use allocated buffer in tlb_uv.c:tunables_read()
    
    * 'x86-vm86-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, vm86: Fix preemption bug for int1 debug and int3 breakpoint handlers.

commit 4a60cfa9457749f7987fd4f3c956dbba5a281129
Merge: 62bea97f54d8 27afdf2008da
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 21 14:11:46 2010 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (96 commits)
      apic, x86: Use BIOS settings for IBS and MCE threshold interrupt LVT offsets
      apic, x86: Check if EILVT APIC registers are available (AMD only)
      x86: ioapic: Call free_irte only if interrupt remapping enabled
      arm: Use ARCH_IRQ_INIT_FLAGS
      genirq, ARM: Fix boot on ARM platforms
      genirq: Fix CONFIG_GENIRQ_NO_DEPRECATED=y build
      x86: Switch sparse_irq allocations to GFP_KERNEL
      genirq: Switch sparse_irq allocator to GFP_KERNEL
      genirq: Make sparse_lock a mutex
      x86: lguest: Use new irq allocator
      genirq: Remove the now unused sparse irq leftovers
      genirq: Sanitize dynamic irq handling
      genirq: Remove arch_init_chip_data()
      x86: xen: Sanitise sparse_irq handling
      x86: Use sane enumeration
      x86: uv: Clean up the direct access to irq_desc
      x86: Make io_apic.c local functions static
      genirq: Remove irq_2_iommu
      x86: Speed up the irq_remapped check in hot pathes
      intr_remap: Simplify the code further
      ...
    
    Fix up trivial conflicts in arch/x86/Kconfig

commit f4bc6bb2d562703eafc895c37e7be20906de139d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Oct 19 15:00:13 2010 +0200

    tracing: Cleanup the convoluted softirq tracepoints
    
    With the addition of trace_softirq_raise() the softirq tracepoint got
    even more convoluted. Why the tracepoints take two pointers to assign
    an integer is beyond my comprehension.
    
    But adding an extra case which treats the first pointer as an unsigned
    long when the second pointer is NULL including the back and forth
    type casting is just horrible.
    
    Convert the softirq tracepoints to take a single unsigned int argument
    for the softirq vector number and fix the call sites.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <alpine.LFD.2.00.1010191428560.6815@localhost6.localdomain6>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: mathieu.desnoyers@efficios.com
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 07b4f1b1a73a..b3cb1dc15795 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -212,18 +212,20 @@ asmlinkage void __do_softirq(void)
 
 	do {
 		if (pending & 1) {
+			unsigned int vec_nr = h - softirq_vec;
 			int prev_count = preempt_count();
-			kstat_incr_softirqs_this_cpu(h - softirq_vec);
 
-			trace_softirq_entry(h, softirq_vec);
+			kstat_incr_softirqs_this_cpu(vec_nr);
+
+			trace_softirq_entry(vec_nr);
 			h->action(h);
-			trace_softirq_exit(h, softirq_vec);
+			trace_softirq_exit(vec_nr);
 			if (unlikely(prev_count != preempt_count())) {
-				printk(KERN_ERR "huh, entered softirq %td %s %p"
+				printk(KERN_ERR "huh, entered softirq %u %s %p"
 				       "with preempt_count %08x,"
-				       " exited with %08x?\n", h - softirq_vec,
-				       softirq_to_name[h - softirq_vec],
-				       h->action, prev_count, preempt_count());
+				       " exited with %08x?\n", vec_nr,
+				       softirq_to_name[vec_nr], h->action,
+				       prev_count, preempt_count());
 				preempt_count() = prev_count;
 			}
 

commit d267f87fb8179c6dba03d08b91952e81bc3723c7
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Mon Oct 4 17:03:23 2010 -0700

    sched: Call tick_check_idle before __irq_enter
    
    When CPU is idle and on first interrupt, irq_enter calls tick_check_idle()
    to notify interruption from idle. But, there is a problem if this call
    is done after __irq_enter, as all routines in __irq_enter may find
    stale time due to yet to be done tick_check_idle.
    
    Specifically, trace calls in __irq_enter when they use global clock and also
    account_system_vtime change in this patch as it wants to use sched_clock_cpu()
    to do proper irq timing.
    
    But, tick_check_idle was moved after __irq_enter intentionally to
    prevent problem of unneeded ksoftirqd wakeups by the commit ee5f80a:
    
        irq: call __irq_enter() before calling the tick_idle_check
        Impact: avoid spurious ksoftirqd wakeups
    
    Moving tick_check_idle() before __irq_enter and wrapping it with
    local_bh_enable/disable would solve both the problems.
    
    Fixed-by: Yong Zhang <yong.zhang0@gmail.com>
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286237003-12406-9-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 267f7b763ebb..79ee8f1fc0e7 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -296,10 +296,16 @@ void irq_enter(void)
 
 	rcu_irq_enter();
 	if (idle_cpu(cpu) && !in_interrupt()) {
-		__irq_enter();
+		/*
+		 * Prevent raise_softirq from needlessly waking up ksoftirqd
+		 * here, as softirq will be serviced on return from interrupt.
+		 */
+		local_bh_disable();
 		tick_check_idle(cpu);
-	} else
-		__irq_enter();
+		_local_bh_enable();
+	}
+
+	__irq_enter();
 }
 
 #ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED

commit 6cdd5199daf0cb7b0fcc8dca941af08492612887
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Mon Oct 4 17:03:18 2010 -0700

    sched: Add a PF flag for ksoftirqd identification
    
    To account softirq time cleanly in scheduler, we need to identify whether
    softirq is invoked in ksoftirqd context or softirq at hardirq tail context.
    Add PF_KSOFTIRQD for that purpose.
    
    As all PF flag bits are currently taken, create space by moving one of the
    infrequently used bits (PF_THREAD_BOUND) down in task_struct to be along
    with some other state fields.
    
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286237003-12406-4-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 988dfbe6bbe8..267f7b763ebb 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -713,6 +713,7 @@ static int run_ksoftirqd(void * __bind_cpu)
 {
 	set_current_state(TASK_INTERRUPTIBLE);
 
+	current->flags |= PF_KSOFTIRQD;
 	while (!kthread_should_stop()) {
 		preempt_disable();
 		if (!local_softirq_pending()) {

commit 75e1056f5c57050415b64cb761a3acc35d91f013
Author: Venkatesh Pallipadi <venki@google.com>
Date:   Mon Oct 4 17:03:16 2010 -0700

    sched: Fix softirq time accounting
    
    Peter Zijlstra found a bug in the way softirq time is accounted in
    VIRT_CPU_ACCOUNTING on this thread:
    
       http://lkml.indiana.edu/hypermail//linux/kernel/1009.2/01366.html
    
    The problem is, softirq processing uses local_bh_disable internally. There
    is no way, later in the flow, to differentiate between whether softirq is
    being processed or is it just that bh has been disabled. So, a hardirq when bh
    is disabled results in time being wrongly accounted as softirq.
    
    Looking at the code a bit more, the problem exists in !VIRT_CPU_ACCOUNTING
    as well. As account_system_time() in normal tick based accouting also uses
    softirq_count, which will be set even when not in softirq with bh disabled.
    
    Peter also suggested solution of using 2*SOFTIRQ_OFFSET as irq count
    for local_bh_{disable,enable} and using just SOFTIRQ_OFFSET while softirq
    processing. The patch below does that and adds API in_serving_softirq() which
    returns whether we are currently processing softirq or not.
    
    Also changes one of the usages of softirq_count in net/sched/cls_cgroup.c
    to in_serving_softirq.
    
    Looks like many usages of in_softirq really want in_serving_softirq. Those
    changes can be made individually on a case by case basis.
    
    Signed-off-by: Venkatesh Pallipadi <venki@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1286237003-12406-2-git-send-email-venki@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 07b4f1b1a73a..988dfbe6bbe8 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -76,12 +76,22 @@ void wakeup_softirqd(void)
 		wake_up_process(tsk);
 }
 
+/*
+ * preempt_count and SOFTIRQ_OFFSET usage:
+ * - preempt_count is changed by SOFTIRQ_OFFSET on entering or leaving
+ *   softirq processing.
+ * - preempt_count is changed by SOFTIRQ_DISABLE_OFFSET (= 2 * SOFTIRQ_OFFSET)
+ *   on local_bh_disable or local_bh_enable.
+ * This lets us distinguish between whether we are currently processing
+ * softirq and whether we just have bh disabled.
+ */
+
 /*
  * This one is for softirq.c-internal use,
  * where hardirqs are disabled legitimately:
  */
 #ifdef CONFIG_TRACE_IRQFLAGS
-static void __local_bh_disable(unsigned long ip)
+static void __local_bh_disable(unsigned long ip, unsigned int cnt)
 {
 	unsigned long flags;
 
@@ -95,32 +105,43 @@ static void __local_bh_disable(unsigned long ip)
 	 * We must manually increment preempt_count here and manually
 	 * call the trace_preempt_off later.
 	 */
-	preempt_count() += SOFTIRQ_OFFSET;
+	preempt_count() += cnt;
 	/*
 	 * Were softirqs turned off above:
 	 */
-	if (softirq_count() == SOFTIRQ_OFFSET)
+	if (softirq_count() == cnt)
 		trace_softirqs_off(ip);
 	raw_local_irq_restore(flags);
 
-	if (preempt_count() == SOFTIRQ_OFFSET)
+	if (preempt_count() == cnt)
 		trace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));
 }
 #else /* !CONFIG_TRACE_IRQFLAGS */
-static inline void __local_bh_disable(unsigned long ip)
+static inline void __local_bh_disable(unsigned long ip, unsigned int cnt)
 {
-	add_preempt_count(SOFTIRQ_OFFSET);
+	add_preempt_count(cnt);
 	barrier();
 }
 #endif /* CONFIG_TRACE_IRQFLAGS */
 
 void local_bh_disable(void)
 {
-	__local_bh_disable((unsigned long)__builtin_return_address(0));
+	__local_bh_disable((unsigned long)__builtin_return_address(0),
+				SOFTIRQ_DISABLE_OFFSET);
 }
 
 EXPORT_SYMBOL(local_bh_disable);
 
+static void __local_bh_enable(unsigned int cnt)
+{
+	WARN_ON_ONCE(in_irq());
+	WARN_ON_ONCE(!irqs_disabled());
+
+	if (softirq_count() == cnt)
+		trace_softirqs_on((unsigned long)__builtin_return_address(0));
+	sub_preempt_count(cnt);
+}
+
 /*
  * Special-case - softirqs can safely be enabled in
  * cond_resched_softirq(), or by __do_softirq(),
@@ -128,12 +149,7 @@ EXPORT_SYMBOL(local_bh_disable);
  */
 void _local_bh_enable(void)
 {
-	WARN_ON_ONCE(in_irq());
-	WARN_ON_ONCE(!irqs_disabled());
-
-	if (softirq_count() == SOFTIRQ_OFFSET)
-		trace_softirqs_on((unsigned long)__builtin_return_address(0));
-	sub_preempt_count(SOFTIRQ_OFFSET);
+	__local_bh_enable(SOFTIRQ_DISABLE_OFFSET);
 }
 
 EXPORT_SYMBOL(_local_bh_enable);
@@ -147,13 +163,13 @@ static inline void _local_bh_enable_ip(unsigned long ip)
 	/*
 	 * Are softirqs going to be turned on now:
 	 */
-	if (softirq_count() == SOFTIRQ_OFFSET)
+	if (softirq_count() == SOFTIRQ_DISABLE_OFFSET)
 		trace_softirqs_on(ip);
 	/*
 	 * Keep preemption disabled until we are done with
 	 * softirq processing:
  	 */
- 	sub_preempt_count(SOFTIRQ_OFFSET - 1);
+	sub_preempt_count(SOFTIRQ_DISABLE_OFFSET - 1);
 
 	if (unlikely(!in_interrupt() && local_softirq_pending()))
 		do_softirq();
@@ -198,7 +214,8 @@ asmlinkage void __do_softirq(void)
 	pending = local_softirq_pending();
 	account_system_vtime(current);
 
-	__local_bh_disable((unsigned long)__builtin_return_address(0));
+	__local_bh_disable((unsigned long)__builtin_return_address(0),
+				SOFTIRQ_OFFSET);
 	lockdep_softirq_enter();
 
 	cpu = smp_processor_id();
@@ -245,7 +262,7 @@ asmlinkage void __do_softirq(void)
 	lockdep_softirq_exit();
 
 	account_system_vtime(current);
-	_local_bh_enable();
+	__local_bh_enable(SOFTIRQ_OFFSET);
 }
 
 #ifndef __ARCH_HAS_DO_SOFTIRQ

commit b7d0d8258a9f71949b810e0f82a3d75088f4d364
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 29 18:44:23 2010 +0200

    genirq: Remove arch_init_chip_data()
    
    This function should have not been there in the first place.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 14a7b80b2cce..d19b1c9aa7c5 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -896,9 +896,4 @@ int __init __weak arch_early_irq_init(void)
 {
 	return 0;
 }
-
-int __weak arch_init_chip_data(struct irq_desc *desc, int node)
-{
-	return 0;
-}
 #endif

commit b683de2b3cb17bb10fa6fd4af614dc75b5749fe0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 27 20:55:03 2010 +0200

    genirq: Query arch for number of early descriptors
    
    sparse irq sets up NR_IRQS_LEGACY irq descriptors and archs then go
    ahead and allocate more.
    
    Use the unused return value of arch_probe_nr_irqs() to let the
    architecture return the number of early allocations. Fix up all users.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 07b4f1b1a73a..14a7b80b2cce 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -886,9 +886,10 @@ int __init __weak early_irq_init(void)
 	return 0;
 }
 
+#ifdef CONFIG_GENERIC_HARDIRQS
 int __init __weak arch_probe_nr_irqs(void)
 {
-	return 0;
+	return NR_IRQS_LEGACY;
 }
 
 int __init __weak arch_early_irq_init(void)
@@ -900,3 +901,4 @@ int __weak arch_init_chip_data(struct irq_desc *desc, int node)
 {
 	return 0;
 }
+#endif

commit 676cb02dc32adef13d9efb5ea52079e4ede1e3ec
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 20 23:33:49 2009 +0200

    softirqs: Make wakeup_softirqd static
    
    No users outside of kernel/softirq.c
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 07b4f1b1a73a..80f6e3bd1d2a 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -67,7 +67,7 @@ char *softirq_to_name[NR_SOFTIRQS] = {
  * to the pending events, so lets the scheduler to balance
  * the softirq load for us.
  */
-void wakeup_softirqd(void)
+static void wakeup_softirqd(void)
 {
 	/* Interrupts are disabled: no need to stop preemption */
 	struct task_struct *tsk = __get_cpu_var(ksoftirqd);

commit 9e506f7adce8e6165a104d3d78fddd8ff0cdccf8
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Fri Jun 4 14:15:04 2010 -0700

    kernel/: fix BUG_ON checks for cpu notifier callbacks direct call
    
    The commit 80b5184cc537718122e036afe7e62d202b70d077 ("kernel/: convert cpu
    notifier to return encapsulate errno value") changed the return value of
    cpu notifier callbacks.
    
    Those callbacks don't return NOTIFY_BAD on failures anymore.  But there
    are a few callbacks which are called directly at init time and checking
    the return value.
    
    I forgot to change BUG_ON checking by the direct callers in the commit.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 825e1126008f..07b4f1b1a73a 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -850,7 +850,7 @@ static __init int spawn_ksoftirqd(void)
 	void *cpu = (void *)(long)smp_processor_id();
 	int err = cpu_callback(&cpu_nfb, CPU_UP_PREPARE, cpu);
 
-	BUG_ON(err == NOTIFY_BAD);
+	BUG_ON(err != NOTIFY_OK);
 	cpu_callback(&cpu_nfb, CPU_ONLINE, cpu);
 	register_cpu_notifier(&cpu_nfb);
 	return 0;

commit 80b5184cc537718122e036afe7e62d202b70d077
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Wed May 26 14:43:32 2010 -0700

    kernel/: convert cpu notifier to return encapsulate errno value
    
    By the previous modification, the cpu notifier can return encapsulate
    errno value.  This converts the cpu notifiers for kernel/*.c
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 0db913a5c60f..825e1126008f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -808,7 +808,7 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 		p = kthread_create(run_ksoftirqd, hcpu, "ksoftirqd/%d", hotcpu);
 		if (IS_ERR(p)) {
 			printk("ksoftirqd for %i failed\n", hotcpu);
-			return NOTIFY_BAD;
+			return notifier_from_errno(PTR_ERR(p));
 		}
 		kthread_bind(p, hotcpu);
   		per_cpu(ksoftirqd, hotcpu) = p;

commit 25502a6c13745f4650cc59322bd198194f55e796
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Apr 1 17:37:01 2010 -0700

    rcu: refactor RCU's context-switch handling
    
    The addition of preemptible RCU to treercu resulted in a bit of
    confusion and inefficiency surrounding the handling of context switches
    for RCU-sched and for RCU-preempt.  For RCU-sched, a context switch
    is a quiescent state, pure and simple, just like it always has been.
    For RCU-preempt, a context switch is in no way a quiescent state, but
    special handling is required when a task blocks in an RCU read-side
    critical section.
    
    However, the callout from the scheduler and the outer loop in ksoftirqd
    still calls something named rcu_sched_qs(), whose name is no longer
    accurate.  Furthermore, when rcu_check_callbacks() notes an RCU-sched
    quiescent state, it ends up unnecessarily (though harmlessly, aside
    from the performance hit) enqueuing the current task if it happens to
    be running in an RCU-preempt read-side critical section.  This not only
    increases the maximum latency of scheduler_tick(), it also needlessly
    increases the overhead of the next outermost rcu_read_unlock() invocation.
    
    This patch addresses this situation by separating the notion of RCU's
    context-switch handling from that of RCU-sched's quiescent states.
    The context-switch handling is covered by rcu_note_context_switch() in
    general and by rcu_preempt_note_context_switch() for preemptible RCU.
    This permits rcu_sched_qs() to handle quiescent states and only quiescent
    states.  It also reduces the maximum latency of scheduler_tick(), though
    probably by much less than a microsecond.  Finally, it means that tasks
    within preemptible-RCU read-side critical sections avoid incurring the
    overhead of queuing unless there really is a context switch.
    
    Suggested-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Acked-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 7c1a67ef0274..0db913a5c60f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -716,7 +716,7 @@ static int run_ksoftirqd(void * __bind_cpu)
 			preempt_enable_no_resched();
 			cond_resched();
 			preempt_disable();
-			rcu_sched_qs((long)__bind_cpu);
+			rcu_note_context_switch((long)__bind_cpu);
 		}
 		preempt_enable();
 		set_current_state(TASK_INTERRUPTIBLE);

commit b9c3032277f756e73f6c673419dc414155e04e46
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Feb 3 18:08:52 2010 +0100

    hrtimer, softirq: Fix hrtimer->softirq trampoline
    
    hrtimers callbacks are always done from hardirq context, either the
    jiffy tick interrupt or the hrtimer device interrupt.
    
    [ there is currently one exception that can still call a hrtimer
      callback from softirq, but even in that case this will still
      work correctly. ]
    
    Reported-by: Wei Yongjun <yjwei@cn.fujitsu.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Yury Polyanskiy <ypolyans@princeton.edu>
    Tested-by: Wei Yongjun <yjwei@cn.fujitsu.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    LKML-Reference: <1265120401.24455.306.camel@laptop>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index a09502e2ef75..7c1a67ef0274 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -500,22 +500,17 @@ EXPORT_SYMBOL(tasklet_kill);
  */
 
 /*
- * The trampoline is called when the hrtimer expires. If this is
- * called from the hrtimer interrupt then we schedule the tasklet as
- * the timer callback function expects to run in softirq context. If
- * it's called in softirq context anyway (i.e. high resolution timers
- * disabled) then the hrtimer callback is called right away.
+ * The trampoline is called when the hrtimer expires. It schedules a tasklet
+ * to run __tasklet_hrtimer_trampoline() which in turn will call the intended
+ * hrtimer callback, but from softirq context.
  */
 static enum hrtimer_restart __hrtimer_tasklet_trampoline(struct hrtimer *timer)
 {
 	struct tasklet_hrtimer *ttimer =
 		container_of(timer, struct tasklet_hrtimer, timer);
 
-	if (hrtimer_is_hres_active(timer)) {
-		tasklet_hi_schedule(&ttimer->tasklet);
-		return HRTIMER_NORESTART;
-	}
-	return ttimer->function(timer);
+	tasklet_hi_schedule(&ttimer->tasklet);
+	return HRTIMER_NORESTART;
 }
 
 /*

commit d0316554d3586cbea60592a41391b5def2553d6f
Merge: fb0bbb92d42d 51e99be00ce2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 14 09:58:24 2009 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (34 commits)
      m68k: rename global variable vmalloc_end to m68k_vmalloc_end
      percpu: add missing per_cpu_ptr_to_phys() definition for UP
      percpu: Fix kdump failure if booted with percpu_alloc=page
      percpu: make misc percpu symbols unique
      percpu: make percpu symbols in ia64 unique
      percpu: make percpu symbols in powerpc unique
      percpu: make percpu symbols in x86 unique
      percpu: make percpu symbols in xen unique
      percpu: make percpu symbols in cpufreq unique
      percpu: make percpu symbols in oprofile unique
      percpu: make percpu symbols in tracer unique
      percpu: make percpu symbols under kernel/ and mm/ unique
      percpu: remove some sparse warnings
      percpu: make alloc_percpu() handle array types
      vmalloc: fix use of non-existent percpu variable in put_cpu_var()
      this_cpu: Use this_cpu_xx in trace_functions_graph.c
      this_cpu: Use this_cpu_xx for ftrace
      this_cpu: Use this_cpu_xx in nmi handling
      this_cpu: Use this_cpu operations in RCU
      this_cpu: Use this_cpu ops for VM statistics
      ...
    
    Fix up trivial (famous last words) global per-cpu naming conflicts in
            arch/x86/kvm/svm.c
            mm/slab.c

commit c5e0cb3ddc5f14cedcfc50c0fb3b5fc6b56576da
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Oct 28 08:14:48 2009 -0700

    rcu: Cleanup: balance rcu_irq_enter()/rcu_irq_exit() calls
    
    Currently, rcu_irq_exit() is invoked only for CONFIG_NO_HZ,
    while rcu_irq_enter() is invoked unconditionally.  This patch
    moves rcu_irq_exit() out from under CONFIG_NO_HZ so that the
    calls are balanced.
    
    This patch has no effect on the behavior of the kernel because
    both rcu_irq_enter() and rcu_irq_exit() are empty for
    !CONFIG_NO_HZ, but the code is easier to understand if the calls
    are obviously balanced in all cases.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <12567428891605-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index f8749e5216e0..21939d9e830e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -302,9 +302,9 @@ void irq_exit(void)
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
 
+	rcu_irq_exit();
 #ifdef CONFIG_NO_HZ
 	/* Make sure that timer wheel updates are propagated */
-	rcu_irq_exit();
 	if (idle_cpu(smp_processor_id()) && !in_interrupt() && !need_resched())
 		tick_nohz_stop_sched_tick(0);
 #endif

commit 1871e52c76dd95895caeb772f845a1718dcbcd75
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 29 22:34:13 2009 +0900

    percpu: make percpu symbols under kernel/ and mm/ unique
    
    This patch updates percpu related symbols under kernel/ and mm/ such
    that percpu symbols are unique and don't clash with local symbols.
    This serves two purposes of decreasing the possibility of global
    percpu symbol collision and allowing dropping per_cpu__ prefix from
    percpu symbols.
    
    * kernel/lockdep.c: s/lock_stats/cpu_lock_stats/
    
    * kernel/sched.c: s/init_rq_rt/init_rt_rq_var/  (any better idea?)
                      s/sched_group_cpus/sched_groups/
    
    * kernel/softirq.c: s/ksoftirqd/run_ksoftirqd/a
    
    * kernel/softlockup.c: s/(*)_timestamp/softlockup_\1_ts/
                           s/watchdog_task/softlockup_watchdog/
                           s/timestamp/ts/ for local variables
    
    * kernel/time/timer_stats: s/lookup_lock/tstats_lookup_lock/
    
    * mm/slab.c: s/reap_work/slab_reap_work/
                 s/reap_node/slab_reap_node/
    
    * mm/vmstat.c: local variable changed to avoid collision with vmstat_work
    
    Partly based on Rusty Russell's "alloc_percpu: rename percpu vars
    which cause name clashes" patch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: (slab/vmstat) Christoph Lameter <cl@linux-foundation.org>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index f8749e5216e0..0740dfd55c51 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -697,7 +697,7 @@ void __init softirq_init(void)
 	open_softirq(HI_SOFTIRQ, tasklet_hi_action);
 }
 
-static int ksoftirqd(void * __bind_cpu)
+static int run_ksoftirqd(void * __bind_cpu)
 {
 	set_current_state(TASK_INTERRUPTIBLE);
 
@@ -810,7 +810,7 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 	switch (action) {
 	case CPU_UP_PREPARE:
 	case CPU_UP_PREPARE_FROZEN:
-		p = kthread_create(ksoftirqd, hcpu, "ksoftirqd/%d", hotcpu);
+		p = kthread_create(run_ksoftirqd, hcpu, "ksoftirqd/%d", hotcpu);
 		if (IS_ERR(p)) {
 			printk("ksoftirqd for %i failed\n", hotcpu);
 			return NOTIFY_BAD;

commit 5dd4de587fd9c25cb32a7a0fe9feec3647509b6f
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Sep 17 17:38:32 2009 +0800

    softirq: add BLOCK_IOPOLL to softirq_to_name
    
    With BLOCK_IOPOLL_SOFTIRQ added, softirq_to_name[] and
    show_softirq_name() needs to be updated.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4AB20398.8070209@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 7db25067cd2d..f8749e5216e0 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -57,7 +57,7 @@ static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp
 static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
 
 char *softirq_to_name[NR_SOFTIRQS] = {
-	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK",
+	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK", "BLOCK_IOPOLL",
 	"TASKLET", "SCHED", "HRTIMER",	"RCU"
 };
 

commit d6714c22b43fbcbead7e7b706ff270e15f04a791
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:46 2009 -0700

    rcu: Renamings to increase RCU clarity
    
    Make RCU-sched, RCU-bh, and RCU-preempt be underlying
    implementations, with "RCU" defined in terms of one of the
    three.  Update the outdated rcu_qsctr_inc() names, as these
    functions no longer increment anything.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <12509746132696-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index eb5e131a0485..7db25067cd2d 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -227,7 +227,7 @@ asmlinkage void __do_softirq(void)
 				preempt_count() = prev_count;
 			}
 
-			rcu_bh_qsctr_inc(cpu);
+			rcu_bh_qs(cpu);
 		}
 		h++;
 		pending >>= 1;
@@ -721,7 +721,7 @@ static int ksoftirqd(void * __bind_cpu)
 			preempt_enable_no_resched();
 			cond_resched();
 			preempt_disable();
-			rcu_qsctr_inc((long)__bind_cpu);
+			rcu_sched_qs((long)__bind_cpu);
 		}
 		preempt_enable();
 		set_current_state(TASK_INTERRUPTIBLE);

commit 9ba5f005c994ad28e266a0cd14ef29354be382c9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jul 22 14:18:35 2009 +0200

    softirq: introduce tasklet_hrtimer infrastructure
    
    commit ca109491f (hrtimer: removing all ur callback modes) moved all
    hrtimer callbacks into hard interrupt context when high resolution
    timers are active. That breaks code which relied on the assumption
    that the callback happens in softirq context.
    
    Provide a generic infrastructure which combines tasklets and hrtimers
    together to provide an in-softirq hrtimer experience.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: torvalds@linux-foundation.org
    Cc: kaber@trash.net
    Cc: David Miller <davem@davemloft.net>
    LKML-Reference: <1248265724.27058.1366.camel@twins>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 3a94905fa5d2..eb5e131a0485 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -345,7 +345,9 @@ void open_softirq(int nr, void (*action)(struct softirq_action *))
 	softirq_vec[nr].action = action;
 }
 
-/* Tasklets */
+/*
+ * Tasklets
+ */
 struct tasklet_head
 {
 	struct tasklet_struct *head;
@@ -493,6 +495,66 @@ void tasklet_kill(struct tasklet_struct *t)
 
 EXPORT_SYMBOL(tasklet_kill);
 
+/*
+ * tasklet_hrtimer
+ */
+
+/*
+ * The trampoline is called when the hrtimer expires. If this is
+ * called from the hrtimer interrupt then we schedule the tasklet as
+ * the timer callback function expects to run in softirq context. If
+ * it's called in softirq context anyway (i.e. high resolution timers
+ * disabled) then the hrtimer callback is called right away.
+ */
+static enum hrtimer_restart __hrtimer_tasklet_trampoline(struct hrtimer *timer)
+{
+	struct tasklet_hrtimer *ttimer =
+		container_of(timer, struct tasklet_hrtimer, timer);
+
+	if (hrtimer_is_hres_active(timer)) {
+		tasklet_hi_schedule(&ttimer->tasklet);
+		return HRTIMER_NORESTART;
+	}
+	return ttimer->function(timer);
+}
+
+/*
+ * Helper function which calls the hrtimer callback from
+ * tasklet/softirq context
+ */
+static void __tasklet_hrtimer_trampoline(unsigned long data)
+{
+	struct tasklet_hrtimer *ttimer = (void *)data;
+	enum hrtimer_restart restart;
+
+	restart = ttimer->function(&ttimer->timer);
+	if (restart != HRTIMER_NORESTART)
+		hrtimer_restart(&ttimer->timer);
+}
+
+/**
+ * tasklet_hrtimer_init - Init a tasklet/hrtimer combo for softirq callbacks
+ * @ttimer:	 tasklet_hrtimer which is initialized
+ * @function:	 hrtimer callback funtion which gets called from softirq context
+ * @which_clock: clock id (CLOCK_MONOTONIC/CLOCK_REALTIME)
+ * @mode:	 hrtimer mode (HRTIMER_MODE_ABS/HRTIMER_MODE_REL)
+ */
+void tasklet_hrtimer_init(struct tasklet_hrtimer *ttimer,
+			  enum hrtimer_restart (*function)(struct hrtimer *),
+			  clockid_t which_clock, enum hrtimer_mode mode)
+{
+	hrtimer_init(&ttimer->timer, which_clock, mode);
+	ttimer->timer.function = __hrtimer_tasklet_trampoline;
+	tasklet_init(&ttimer->tasklet, __tasklet_hrtimer_trampoline,
+		     (unsigned long)ttimer);
+	ttimer->function = function;
+}
+EXPORT_SYMBOL_GPL(tasklet_hrtimer_init);
+
+/*
+ * Remote softirq bits
+ */
+
 DEFINE_PER_CPU(struct list_head [NR_SOFTIRQS], softirq_work_list);
 EXPORT_PER_CPU_SYMBOL(softirq_work_list);
 

commit aa0ce5bbc2dbb1853bd0c6d13f17716fcc38ac5a
Author: Keika Kobayashi <kobayashi.kk@ncos.nec.co.jp>
Date:   Wed Jun 17 16:25:52 2009 -0700

    softirq: introduce statistics for softirq
    
    Statistics for softirq doesn't exist.
    It will be helpful like statistics for interrupts.
    This patch introduces counting the number of softirq,
    which will be exported in /proc/softirqs.
    
    When softirq handler consumes much CPU time,
    /proc/stat is like the following.
    
    $ while :; do  cat /proc/stat | head -n1 ; sleep 10 ; done
    cpu  88 0 408 739665 583 28 2 0 0
    cpu  450 0 1090 740970 594 28 1294 0 0
                                  ^^^^
                                 softirq
    
    In such a situation,
    /proc/softirqs shows us which softirq handler is invoked.
    We can see the increase rate of softirqs.
    
    <before>
    $ cat /proc/softirqs
                    CPU0       CPU1       CPU2       CPU3
    HI                 0          0          0          0
    TIMER         462850     462805     462782     462718
    NET_TX             0          0          0        365
    NET_RX          2472          2          2         40
    BLOCK              0          0        381       1164
    TASKLET            0          0          0        224
    SCHED         462654     462689     462698     462427
    RCU             3046       2423       3367       3173
    
    <after>
    $ cat /proc/softirqs
                    CPU0       CPU1       CPU2       CPU3
    HI                 0          0          0          0
    TIMER         463361     465077     465056     464991
    NET_TX            53          0          1        365
    NET_RX          3757          2          2         40
    BLOCK              0          0        398       1170
    TASKLET            0          0          0        224
    SCHED         463074     464318     464612     463330
    RCU             3505       2948       3947       3673
    
    When CPU TIME of softirq is high,
    the rates of increase is the following.
      TIMER  : 220/sec     : CPU1-3
      NET_TX : 5/sec       : CPU0
      NET_RX : 120/sec     : CPU0
      SCHED  : 40-200/sec  : all CPU
      RCU    : 45-58/sec   : all CPU
    
    The rates of increase in an idle mode is the following.
      TIMER  : 250/sec
      SCHED  : 250/sec
      RCU    : 2/sec
    
    It seems many softirqs for receiving packets and rcu are invoked.  This
    gives us help for checking system.
    
    Signed-off-by: Keika Kobayashi <kobayashi.kk@ncos.nec.co.jp>
    Reviewed-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Eric Dumazet <dada1@cosmosbay.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b41fb710e114..3a94905fa5d2 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -213,6 +213,7 @@ asmlinkage void __do_softirq(void)
 	do {
 		if (pending & 1) {
 			int prev_count = preempt_count();
+			kstat_incr_softirqs_this_cpu(h - softirq_vec);
 
 			trace_softirq_entry(h, softirq_vec);
 			h->action(h);

commit 7c692cbade8b8884f1c20500393bcc7cd6d24ef8
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Wed May 21 22:53:13 2008 +0200

    tasklets: new tasklet scheduling function
    
    Rationale: kmemcheck needs to be able to schedule a tasklet without
    touching any dynamically allocated memory _at_ _all_ (since that would
    lead to a recursive page fault). This tasklet is used for writing the
    error reports to the kernel log.
    
    The new scheduling function avoids touching any other tasklets by
    inserting the new tasklist as the head of the "tasklet_hi" list instead
    of on the tail.
    
    Also don't wake up the softirq thread lest the scheduler access some
    tracked memory and we go down with a recursive page fault.
    
    In this case, we'd better just wait for the maximum time of 1/HZ for the
    message to appear.
    
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 258885a543db..b41fb710e114 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -382,6 +382,17 @@ void __tasklet_hi_schedule(struct tasklet_struct *t)
 
 EXPORT_SYMBOL(__tasklet_hi_schedule);
 
+void __tasklet_hi_schedule_first(struct tasklet_struct *t)
+{
+	BUG_ON(!irqs_disabled());
+
+	t->next = __get_cpu_var(tasklet_hi_vec).head;
+	__get_cpu_var(tasklet_hi_vec).head = t;
+	__raise_softirq_irqoff(HI_SOFTIRQ);
+}
+
+EXPORT_SYMBOL(__tasklet_hi_schedule_first);
+
 static void tasklet_action(struct softirq_action *a)
 {
 	struct tasklet_struct *list;

commit 862366118026a358882eefc70238dbcc3db37aac
Merge: 57eee9ae7bbc 511b01bdf64a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 19:53:40 2009 -0700

    Merge branch 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (244 commits)
      Revert "x86, bts: reenable ptrace branch trace support"
      tracing: do not translate event helper macros in print format
      ftrace/documentation: fix typo in function grapher name
      tracing/events: convert block trace points to TRACE_EVENT(), fix !CONFIG_BLOCK
      tracing: add protection around module events unload
      tracing: add trace_seq_vprint interface
      tracing: fix the block trace points print size
      tracing/events: convert block trace points to TRACE_EVENT()
      ring-buffer: fix ret in rb_add_time_stamp
      ring-buffer: pass in lockdep class key for reader_lock
      tracing: add annotation to what type of stack trace is recorded
      tracing: fix multiple use of __print_flags and __print_symbolic
      tracing/events: fix output format of user stack
      tracing/events: fix output format of kernel stack
      tracing/trace_stack: fix the number of entries in the header
      ring-buffer: discard timestamps that are at the start of the buffer
      ring-buffer: try to discard unneeded timestamps
      ring-buffer: fix bug in ring_buffer_discard_commit
      ftrace: do not profile functions when disabled
      tracing: make trace pipe recognize latency format flag
      ...

commit 44347d947f628060b92449702071bfe1d31dfb75
Merge: d94fc523f3c3 413f81eba35d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 7 11:17:13 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Merge reason: tracing/core was on a .30-rc1 base and was missing out on
                  on a handful of tracing fixes present in .30-rc5-almost.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a0e39ed378fb6ba916522764cd508fa7d42ad495
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Apr 29 13:51:39 2009 +0200

    tracing: fix build failure on s390
    
    "tracing: create automated trace defines" causes this compile error on s390,
    as reported by Sachin Sant against linux-next:
    
     kernel/built-in.o: In function `__do_softirq':
     (.text+0x1c680): undefined reference to `__tracepoint_softirq_entry'
    
    This happens because the definitions of the softirq tracepoints were moved
    from kernel/softirq.c to kernel/irq/handle.c. Since s390 doesn't support
    generic hardirqs handle.c doesn't get compiled and the definitions are
    missing.
    
    So move the tracepoints to softirq.c again.
    
    [ Impact: fix build failure on s390 ]
    
    Reported-by: Sachin Sant <sachinp@in.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: fweisbec@gmail.com
    LKML-Reference: <20090429135139.5fac79b8@osiris.boeblingen.de.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 7ab9dfd8d082..d4ba347a872d 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -24,6 +24,8 @@
 #include <linux/ftrace.h>
 #include <linux/smp.h>
 #include <linux/tick.h>
+
+#define CREATE_TRACE_POINTS
 #include <trace/events/irq.h>
 
 #include <asm/irq.h>

commit 85ac16d033370caf6f48d743c8dc8103700f5cc5
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Apr 27 18:00:38 2009 -0700

    x86/irq: change irq_desc_alloc() to take node instead of cpu
    
    This simplifies the node awareness of the code. All our allocators
    only deal with a NUMA node ID locality not with CPU ids anyway - so
    there's no need to maintain (and transform) a CPU id all across the
    IRq layer.
    
    v2: keep move_irq_desc related
    
    [ Impact: cleanup, prepare IRQ code to be NUMA-aware ]
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    LKML-Reference: <49F65536.2020300@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b525dd348511..f674f332a024 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -828,7 +828,7 @@ int __init __weak arch_early_irq_init(void)
 	return 0;
 }
 
-int __weak arch_init_chip_data(struct irq_desc *desc, int cpu)
+int __weak arch_init_chip_data(struct irq_desc *desc, int node)
 {
 	return 0;
 }

commit 79d381c9f2354b594dcab9b04dfcc0debf7294fe
Author: H Hartley Sweeten <hartleys@visionengravers.com>
Date:   Thu Apr 16 19:30:18 2009 -0400

    kernel/softirq.c: fix sparse warning
    
    Fix sparse warning in kernel/softirq.c.
    
      warning: do-while statement is not a compound statement
    
    Signed-off-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    LKML-Reference: <BD79186B4FD85F4B8E60E381CAEE1909015F9033@mi8nycmail19.Mi8.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 2fecefacdc5b..b525dd348511 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -472,9 +472,9 @@ void tasklet_kill(struct tasklet_struct *t)
 		printk("Attempt to kill tasklet from interrupt\n");
 
 	while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
-		do
+		do {
 			yield();
-		while (test_bit(TASKLET_STATE_SCHED, &t->state));
+		} while (test_bit(TASKLET_STATE_SCHED, &t->state));
 	}
 	tasklet_unlock_wait(t);
 	clear_bit(TASKLET_STATE_SCHED, &t->state);

commit ad8d75fff811a6a230f7f43b05a6483099349533
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Apr 14 19:39:12 2009 -0400

    tracing/events: move trace point headers into include/trace/events
    
    Impact: clean up
    
    Create a sub directory in include/trace called events to keep the
    trace point headers in their own separate directory. Only headers that
    declare trace points should be defined in this directory.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Neil Horman <nhorman@tuxdriver.com>
    Cc: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index a2d9b458ac2b..7ab9dfd8d082 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -24,7 +24,7 @@
 #include <linux/ftrace.h>
 #include <linux/smp.h>
 #include <linux/tick.h>
-#include <trace/irq.h>
+#include <trace/events/irq.h>
 
 #include <asm/irq.h>
 /*

commit a8d154b009168337494fbf345671bab74d3e4b8b
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 10 09:36:00 2009 -0400

    tracing: create automated trace defines
    
    This patch lowers the number of places a developer must modify to add
    new tracepoints. The current method to add a new tracepoint
    into an existing system is to write the trace point macro in the
    trace header with one of the macros TRACE_EVENT, TRACE_FORMAT or
    DECLARE_TRACE, then they must add the same named item into the C file
    with the macro DEFINE_TRACE(name) and then add the trace point.
    
    This change cuts out the needing to add the DEFINE_TRACE(name).
    Every file that uses the tracepoint must still include the trace/<type>.h
    file, but the one C file must also add a define before the including
    of that file.
    
     #define CREATE_TRACE_POINTS
     #include <trace/mytrace.h>
    
    This will cause the trace/mytrace.h file to also produce the C code
    necessary to implement the trace point.
    
    Note, if more than one trace/<type>.h is used to create the C code
    it is best to list them all together.
    
     #define CREATE_TRACE_POINTS
     #include <trace/foo.h>
     #include <trace/bar.h>
     #include <trace/fido.h>
    
    Thanks to Mathieu Desnoyers and Christoph Hellwig for coming up with
    the cleaner solution of the define above the includes over my first
    design to have the C code include a "special" header.
    
    This patch converts sched, irq and lockdep and skb to use this new
    method.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Neil Horman <nhorman@tuxdriver.com>
    Cc: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 2fecefacdc5b..a2d9b458ac2b 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -186,9 +186,6 @@ EXPORT_SYMBOL(local_bh_enable_ip);
  */
 #define MAX_SOFTIRQ_RESTART 10
 
-DEFINE_TRACE(softirq_entry);
-DEFINE_TRACE(softirq_exit);
-
 asmlinkage void __do_softirq(void)
 {
 	struct softirq_action *h;

commit 609862be074cc20e007c640fd936ffe798b41abc
Merge: 12fe32e4f942 eedeeabdeead
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 6 13:37:30 2009 -0700

    Merge branch 'locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      lockdep: add stack dumps to asserts
      hrtimer: fix rq->lock inversion (again)

commit 714f83d5d9f7c785f622259dad1f4fad12d64664
Merge: 8901e7ffc2fa 645dae969c3b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 5 11:04:19 2009 -0700

    Merge branch 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (413 commits)
      tracing, net: fix net tree and tracing tree merge interaction
      tracing, powerpc: fix powerpc tree and tracing tree interaction
      ring-buffer: do not remove reader page from list on ring buffer free
      function-graph: allow unregistering twice
      trace: make argument 'mem' of trace_seq_putmem() const
      tracing: add missing 'extern' keywords to trace_output.h
      tracing: provide trace_seq_reserve()
      blktrace: print out BLK_TN_MESSAGE properly
      blktrace: extract duplidate code
      blktrace: fix memory leak when freeing struct blk_io_trace
      blktrace: fix blk_probes_ref chaos
      blktrace: make classic output more classic
      blktrace: fix off-by-one bug
      blktrace: fix the original blktrace
      blktrace: fix a race when creating blk_tree_root in debugfs
      blktrace: fix timestamp in binary output
      tracing, Text Edit Lock: cleanup
      tracing: filter fix for TRACE_EVENT_FORMAT events
      ftrace: Using FTRACE_WARN_ON() to check "freed record" in ftrace_release()
      x86: kretprobe-booster interrupt emulation code fix
      ...
    
    Fix up trivial conflicts in
     arch/parisc/include/asm/ftrace.h
     include/linux/memory.h
     kernel/extable.c
     kernel/module.c

commit b1dbb67911fecb290db3f566281bcd9ccc9dc6df
Merge: 492f59f526d9 70f454408e68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 3 17:33:30 2009 -0700

    Merge branch 'ipi-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'ipi-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      s390: remove arch specific smp_send_stop()
      panic: clean up kernel/panic.c
      panic, smp: provide smp_send_stop() wrapper on UP too
      panic: decrease oops_in_progress only after having done the panic
      generic-ipi: eliminate WARN_ON()s during oops/panic
      generic-ipi: cleanups
      generic-ipi: remove CSD_FLAG_WAIT
      generic-ipi: remove kmalloc()
      generic IPI: simplify barriers and locking

commit 7f1e2ca9f04b02794597f60e7b1d43f0a1317939
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Mar 13 12:21:27 2009 +0100

    hrtimer: fix rq->lock inversion (again)
    
    It appears I inadvertly introduced rq->lock recursion to the
    hrtimer_start() path when I delegated running already expired
    timers to softirq context.
    
    This patch fixes it by introducing a __hrtimer_start_range_ns()
    method that will not use raise_softirq_irqoff() but
    __raise_softirq_irqoff() which avoids the wakeup.
    
    It then also changes schedule() to check for pending softirqs and
    do the wakeup then, I'm not quite sure I like this last bit, nor
    am I convinced its really needed.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus@samba.org
    LKML-Reference: <20090313112301.096138802@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 487751604300..accc85197c49 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -58,7 +58,7 @@ static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
  * to the pending events, so lets the scheduler to balance
  * the softirq load for us.
  */
-static inline void wakeup_softirqd(void)
+void wakeup_softirqd(void)
 {
 	/* Interrupts are disabled: no need to stop preemption */
 	struct task_struct *tsk = __get_cpu_var(ksoftirqd);

commit 7bee946358c3cb957d4aa648fc5ab3cad0b232d0
Merge: d820ac4c2fa8 15f7176eb1cc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 31 13:53:43 2009 +0200

    Merge branch 'linus' into locking-for-linus
    
    Conflicts:
            lib/Kconfig.debug

commit 6e15cf04860074ad032e88c306bea656bbdd0f22
Merge: be0ea69674ed 60db56422043
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 26 21:39:17 2009 +0100

    Merge branch 'core/percpu' into percpu-cpumask-x86-for-linus-2
    
    Conflicts:
            arch/parisc/kernel/irq.c
            arch/x86/include/asm/fixmap_64.h
            arch/x86/include/asm/setup.h
            kernel/irq/handle.c
    
    Semantic merge:
            arch/x86/include/asm/fixmap.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit cd80a8142efa3468c2cd9fb52845f334c3220d54
Merge: 641cd4cfcdc7 a98fe7f3425c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 11:05:58 2009 +0100

    Merge branch 'x86/core' into core/ipi

commit 899039e8746bb9a09b6487ddb8ab2275ce9d0256
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Mar 13 00:43:33 2009 -0400

    softirq: no need to have SOFTIRQ in softirq name
    
    Impact: clean up
    
    It is redundant to have 'SOFTIRQ' in the softirq names.
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index a5e81231ca7a..65ff3e3961b4 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -55,9 +55,8 @@ static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp
 static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
 
 char *softirq_to_name[NR_SOFTIRQS] = {
-	"HI_SOFTIRQ", "TIMER_SOFTIRQ", "NET_TX_SOFTIRQ", "NET_RX_SOFTIRQ",
-	"BLOCK_SOFTIRQ", "TASKLET_SOFTIRQ", "SCHED_SOFTIRQ", "HRTIMER_SOFTIRQ",
-	"RCU_SOFTIRQ"
+	"HI", "TIMER", "NET_TX", "NET_RX", "BLOCK",
+	"TASKLET", "SCHED", "HRTIMER",	"RCU"
 };
 
 /*

commit 39842323ceb368d2ea36ab7696aedbe296e13b61
Author: Jason Baron <jbaron@redhat.com>
Date:   Thu Mar 12 14:36:03 2009 -0400

    tracing: tracepoints for softirq entry/exit - tracepoints
    
    Introduce softirq entry/exit tracepoints. These are useful for
    augmenting existing tracers, and to figure out softirq frequencies and
    timings.
    
    [
      s/irq_softirq_/softirq_/ for trace point names and
      Fixed printf format in TRACE_FORMAT macro
       - Steven Rostedt
    ]
    
    LKML-Reference: <20090312183603.GC3352@redhat.com>
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 9f90fdc039f4..a5e81231ca7a 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -24,6 +24,7 @@
 #include <linux/ftrace.h>
 #include <linux/smp.h>
 #include <linux/tick.h>
+#include <trace/irq.h>
 
 #include <asm/irq.h>
 /*
@@ -186,6 +187,9 @@ EXPORT_SYMBOL(local_bh_enable_ip);
  */
 #define MAX_SOFTIRQ_RESTART 10
 
+DEFINE_TRACE(softirq_entry);
+DEFINE_TRACE(softirq_exit);
+
 asmlinkage void __do_softirq(void)
 {
 	struct softirq_action *h;
@@ -212,8 +216,9 @@ asmlinkage void __do_softirq(void)
 		if (pending & 1) {
 			int prev_count = preempt_count();
 
+			trace_softirq_entry(h, softirq_vec);
 			h->action(h);
-
+			trace_softirq_exit(h, softirq_vec);
 			if (unlikely(prev_count != preempt_count())) {
 				printk(KERN_ERR "huh, entered softirq %td %s %p"
 				       "with preempt_count %08x,"

commit 5d592b44b29a1d73e13d5c9e3426eed843bdc359
Author: Jason Baron <jbaron@redhat.com>
Date:   Thu Mar 12 14:33:36 2009 -0400

    tracing: tracepoints for softirq entry/exit - add softirq-to-name array
    
    Create a 'softirq_to_name' array, which is indexed by softirq #, so
    that we can easily convert between the softirq index # and its name, in
    order to get more meaningful output messages.
    
    LKML-Reference: <20090312183336.GB3352@redhat.com>
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 7571bcb71be4..9f90fdc039f4 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -53,6 +53,12 @@ static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp
 
 static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
 
+char *softirq_to_name[NR_SOFTIRQS] = {
+	"HI_SOFTIRQ", "TIMER_SOFTIRQ", "NET_TX_SOFTIRQ", "NET_RX_SOFTIRQ",
+	"BLOCK_SOFTIRQ", "TASKLET_SOFTIRQ", "SCHED_SOFTIRQ", "HRTIMER_SOFTIRQ",
+	"RCU_SOFTIRQ"
+};
+
 /*
  * we cannot loop indefinitely here to avoid userspace starvation,
  * but we also don't want to introduce a worst case 1/HZ latency
@@ -209,9 +215,10 @@ asmlinkage void __do_softirq(void)
 			h->action(h);
 
 			if (unlikely(prev_count != preempt_count())) {
-				printk(KERN_ERR "huh, entered softirq %td %p"
+				printk(KERN_ERR "huh, entered softirq %td %s %p"
 				       "with preempt_count %08x,"
 				       " exited with %08x?\n", h - softirq_vec,
+				       softirq_to_name[h - softirq_vec],
 				       h->action, prev_count, preempt_count());
 				preempt_count() = prev_count;
 			}

commit 25d500067d5a666d1336598d1b324793554e5496
Merge: 0b13fda1e093 9ead64974b05
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 02:14:25 2009 +0100

    Merge branch 'linus' into core/ipi

commit 480c93df5b99699390f93a7024c9f60d09da0e96
Merge: aecfcde920da d820ac4c2fa8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 01:33:21 2009 +0100

    Merge branch 'core/locking' into tracing/ftrace

commit d820ac4c2fa881079e6b689d2098adce337558ae
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 01:30:40 2009 +0100

    locking: rename trace_softirq_[enter|exit] => lockdep_softirq_[enter|exit]
    
    Impact: cleanup
    
    The naming clashes with upcoming softirq tracepoints, so rename the
    APIs to lockdep_*().
    
    Requested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 9041ea7948fe..08a030f85416 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -180,7 +180,7 @@ asmlinkage void __do_softirq(void)
 	account_system_vtime(current);
 
 	__local_bh_disable((unsigned long)__builtin_return_address(0));
-	trace_softirq_enter();
+	lockdep_softirq_enter();
 
 	cpu = smp_processor_id();
 restart:
@@ -220,7 +220,7 @@ asmlinkage void __do_softirq(void)
 	if (pending)
 		wakeup_softirqd();
 
-	trace_softirq_exit();
+	lockdep_softirq_exit();
 
 	account_system_vtime(current);
 	_local_bh_enable();

commit 12e87e36e0141c08dbc8b2177c93c75fb18ad7e5
Merge: 42b40b3d55f5 c3ffc7a40b7e 7bffc23e56e9 7a203f3b089b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 10 09:56:25 2009 +0100

    Merge branches 'tracing/doc', 'tracing/ftrace', 'tracing/printk' and 'linus' into tracing/core

commit 467c88fee51e2ae862e9485245687da0730e29aa
Merge: 1f442d70c84a 7ab152470e84 8827247ffcc9 d1a8e7792047 0feca851c1b3 d0fc63f7bd07 7a203f3b089b 3a450de1365d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 10 09:26:38 2009 +0100

    Merge branches 'x86/apic', 'x86/asm', 'x86/fixmap', 'x86/memtest', 'x86/mm', 'x86/urgent', 'linus' and 'core/percpu' into x86/core

commit f0ef03985130287c6c84ebe69416cf790e6cc00e
Merge: 16097439703b 31bbed527e70
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 6 16:44:14 2009 +0100

    Merge branch 'x86/core' into tracing/textedit
    
    Conflicts:
            arch/x86/Kconfig
            block/blktrace.c
            kernel/irq/handle.c
    
    Semantic conflict:
            kernel/trace/blktrace.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 64ca5ab913f1594ef316556e65f5eae63ff50cee
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Wed Mar 4 12:11:56 2009 -0800

    rcu: increment quiescent state counter in ksoftirqd()
    
    If a machine is flooded by network frames, a cpu can loop
    100% of its time inside ksoftirqd() without calling schedule().
    This can delay RCU grace period to insane values.
    
    Adding rcu_qsctr_inc() call in ksoftirqd() solves this problem.
    
    Paul: "This regression was a result of the recent change from
    "schedule()" to "cond_resched()", which got rid of that quiescent
    state in the common case where a reschedule is not needed".
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index bdbe9de9cd8d..9041ea7948fe 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -626,6 +626,7 @@ static int ksoftirqd(void * __bind_cpu)
 			preempt_enable_no_resched();
 			cond_resched();
 			preempt_disable();
+			rcu_qsctr_inc((long)__bind_cpu);
 		}
 		preempt_enable();
 		set_current_state(TASK_INTERRUPTIBLE);

commit 6e2756376c706e4da3454a272947983f92e80a7e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Feb 25 13:59:48 2009 +0100

    generic-ipi: remove CSD_FLAG_WAIT
    
    Oleg noticed that we don't strictly need CSD_FLAG_WAIT, rework
    the code so that we can use CSD_FLAG_LOCK for both purposes.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index bdbe9de9cd8d..48c3d5d627a8 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -496,7 +496,7 @@ static int __try_remote_softirq(struct call_single_data *cp, int cpu, int softir
 		cp->flags = 0;
 		cp->priv = softirq;
 
-		__smp_call_function_single(cpu, cp);
+		__smp_call_function_single(cpu, cp, 0);
 		return 0;
 	}
 	return 1;

commit 7e49fcce1bdadd723ae6a0b3b324c4daced61563
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 22 19:01:40 2009 -0500

    trace, lockdep: manual preempt count adding for local_bh_disable
    
    Impact: fix to preempt trace triggering lockdep check_flag failure
    
    In local_bh_disable, the use of add_preempt_count causes the
    preempt tracer to start recording the time preemption is off.
    But because it already modified the preempt_count to show
    softirqs disabled, and before it called the lockdep code to
    handle this, it causes a state that lockdep can not handle.
    
    The preempt tracer will reset the ring buffer on start of a trace,
    and the ring buffer reset code does a spin_lock_irqsave. This
    calls into lockdep and lockdep will fail when it detects the
    invalid state of having softirqs disabled but the internal
    current->softirqs_enabled is still set.
    
    The fix is to manually add the SOFTIRQ_OFFSET to preempt count
    and call the preempt tracer code outside the lockdep critical
    area.
    
    Thanks to Peter Zijlstra for suggesting this solution.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index bdbe9de9cd8d..6edfc2c11d99 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -21,6 +21,7 @@
 #include <linux/freezer.h>
 #include <linux/kthread.h>
 #include <linux/rcupdate.h>
+#include <linux/ftrace.h>
 #include <linux/smp.h>
 #include <linux/tick.h>
 
@@ -79,13 +80,23 @@ static void __local_bh_disable(unsigned long ip)
 	WARN_ON_ONCE(in_irq());
 
 	raw_local_irq_save(flags);
-	add_preempt_count(SOFTIRQ_OFFSET);
+	/*
+	 * The preempt tracer hooks into add_preempt_count and will break
+	 * lockdep because it calls back into lockdep after SOFTIRQ_OFFSET
+	 * is set and before current->softirq_enabled is cleared.
+	 * We must manually increment preempt_count here and manually
+	 * call the trace_preempt_off later.
+	 */
+	preempt_count() += SOFTIRQ_OFFSET;
 	/*
 	 * Were softirqs turned off above:
 	 */
 	if (softirq_count() == SOFTIRQ_OFFSET)
 		trace_softirqs_off(ip);
 	raw_local_irq_restore(flags);
+
+	if (preempt_count() == SOFTIRQ_OFFSET)
+		trace_preempt_off(CALLER_ADDR0, get_parent_ip(CALLER_ADDR1));
 }
 #else /* !CONFIG_TRACE_IRQFLAGS */
 static inline void __local_bh_disable(unsigned long ip)

commit 4a046d1754ee6ebb6f399696805ed61ea0444d4c
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Jan 12 17:39:24 2009 -0800

    x86: arch_probe_nr_irqs
    
    Impact: save RAM with large NR_CPUS, get smaller nr_irqs
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index bdbe9de9cd8d..0365b4899a3d 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -795,6 +795,11 @@ int __init __weak early_irq_init(void)
 	return 0;
 }
 
+int __init __weak arch_probe_nr_irqs(void)
+{
+	return 0;
+}
+
 int __init __weak arch_early_irq_init(void)
 {
 	return 0;

commit 7d3b56ba37a95f1f370f50258ed3954c304c524b
Merge: 269b012321f2 ab14398abd19
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 3 12:04:39 2009 -0800

    Merge branch 'cpus4096-for-linus-3' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'cpus4096-for-linus-3' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (77 commits)
      x86: setup_per_cpu_areas() cleanup
      cpumask: fix compile error when CONFIG_NR_CPUS is not defined
      cpumask: use alloc_cpumask_var_node where appropriate
      cpumask: convert shared_cpu_map in acpi_processor* structs to cpumask_var_t
      x86: use cpumask_var_t in acpi/boot.c
      x86: cleanup some remaining usages of NR_CPUS where s/b nr_cpu_ids
      sched: put back some stack hog changes that were undone in kernel/sched.c
      x86: enable cpus display of kernel_max and offlined cpus
      ia64: cpumask fix for is_affinity_mask_valid()
      cpumask: convert RCU implementations, fix
      xtensa: define __fls
      mn10300: define __fls
      m32r: define __fls
      h8300: define __fls
      frv: define __fls
      cris: define __fls
      cpumask: CONFIG_DISABLE_OBSOLETE_CPUMASK_FUNCTIONS
      cpumask: zero extra bits in alloc_cpumask_var_node
      cpumask: replace for_each_cpu_mask_nr with for_each_cpu in kernel/time/
      cpumask: convert mm/
      ...

commit f1fc057c79cb2d27602fb3ad08a031f13459ef27
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Jan 1 10:12:23 2009 +1030

    cpumask: remove any_online_cpu() users: kernel/
    
    Impact: Remove obsolete API usage
    
    any_online_cpu() is a good name, but it takes a cpumask_t, not a
    pointer.
    
    There are several places where any_online_cpu() doesn't really want a
    mask arg at all.  Replace all callers with cpumask_any() and
    cpumask_any_and().
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 466e75ce271a..b7568d7def23 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -733,7 +733,7 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 			break;
 		/* Unbind so it can run.  Fall thru. */
 		kthread_bind(per_cpu(ksoftirqd, hotcpu),
-			     any_online_cpu(cpu_online_map));
+			     cpumask_any(cpu_online_mask));
 	case CPU_DEAD:
 	case CPU_DEAD_FROZEN: {
 		struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };

commit db200df0b3530f673d8e9f5bd535e9e10305842a
Merge: ec270e59a74e 43a256322ac1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 31 09:00:59 2008 -0800

    Merge branch 'irq-fixes-for-linus-4' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'irq-fixes-for-linus-4' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      sparseirq: move __weak symbols into separate compilation unit
      sparseirq: work around __weak alias bug
      sparseirq: fix hang with !SPARSE_IRQ
      sparseirq: set lock_class for legacy irq when sparse_irq is selected
      sparseirq: work around compiler optimizing away __weak functions
      sparseirq: fix desc->lock init
      sparseirq: do not printk when migrating IRQ descriptors
      sparseirq: remove duplicated arch_early_irq_init()
      irq: simplify for_each_irq_desc() usage
      proc: remove ifdef CONFIG_SPARSE_IRQ from stat.c
      irq: for_each_irq_desc() move to irqnr.h
      hrtimer: remove #include <linux/irq.h>

commit 43a256322ac1fc105c181b3cade3b9bfc0b63ca1
Author: Yinghai Lu <yhlu.kernel@gmail.com>
Date:   Sun Dec 28 16:01:13 2008 -0800

    sparseirq: move __weak symbols into separate compilation unit
    
    GCC has a bug with __weak alias functions: if the functions are in
    the same compilation unit as their call site, GCC can decide to
    inline them - and thus rob the linker of the opportunity to override
    the weak alias with the real thing.
    
    So move all the IRQ handling related __weak symbols to kernel/irq/chip.c.
    
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index e7c69a720d69..daf46358d2dd 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -797,3 +797,23 @@ int on_each_cpu(void (*func) (void *info), void *info, int wait)
 }
 EXPORT_SYMBOL(on_each_cpu);
 #endif
+
+/*
+ * [ These __weak aliases are kept in a separate compilation unit, so that
+ *   GCC does not inline them incorrectly. ]
+ */
+
+int __init __weak early_irq_init(void)
+{
+	return 0;
+}
+
+int __init __weak arch_early_irq_init(void)
+{
+	return 0;
+}
+
+int __weak arch_init_chip_data(struct irq_desc *desc, int cpu)
+{
+	return 0;
+}

commit 6638101c1124c19c8a65b1645e4ecd09e0572f3e
Merge: cc37d3d20604 3ae7020543db a08636690d06 00ef9f7348df 26cc271db798 12d79bafb756 3ac52669c7a2 8b752e3ef6e3 9212ddb5eada
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 25 14:06:29 2008 +0100

    Merge branches 'core/debugobjects', 'core/iommu', 'core/locking', 'core/printk', 'core/rcu', 'core/resources', 'core/softirq' and 'core/stacktrace' into core/core

commit 64db4cfff99c04cd5f550357edcc8780f96b54a2
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Dec 18 21:55:32 2008 +0100

    "Tree RCU": scalable classic RCU implementation
    
    This patch fixes a long-standing performance bug in classic RCU that
    results in massive internal-to-RCU lock contention on systems with
    more than a few hundred CPUs.  Although this patch creates a separate
    flavor of RCU for ease of review and patch maintenance, it is intended
    to replace classic RCU.
    
    This patch still handles stress better than does mainline, so I am still
    calling it ready for inclusion.  This patch is against the -tip tree.
    Nevertheless, experience on an actual 1000+ CPU machine would still be
    most welcome.
    
    Most of the changes noted below were found while creating an rcutiny
    (which should permit ejecting the current rcuclassic) and while doing
    detailed line-by-line documentation.
    
    Updates from v9 (http://lkml.org/lkml/2008/12/2/334):
    
    o       Fixes from remainder of line-by-line code walkthrough,
            including comment spelling, initialization, undesirable
            narrowing due to type conversion, removing redundant memory
            barriers, removing redundant local-variable initialization,
            and removing redundant local variables.
    
            I do not believe that any of these fixes address the CPU-hotplug
            issues that Andi Kleen was seeing, but please do give it a whirl
            in case the machine is smarter than I am.
    
            A writeup from the walkthrough may be found at the following
            URL, in case you are suffering from terminal insomnia or
            masochism:
    
            http://www.kernel.org/pub/linux/kernel/people/paulmck/tmp/rcutree-walkthrough.2008.12.16a.pdf
    
    o       Made rcutree tracing use seq_file, as suggested some time
            ago by Lai Jiangshan.
    
    o       Added a .csv variant of the rcudata debugfs trace file, to allow
            people having thousands of CPUs to drop the data into
            a spreadsheet.  Tested with oocalc and gnumeric.  Updated
            documentation to suit.
    
    Updates from v8 (http://lkml.org/lkml/2008/11/15/139):
    
    o       Fix a theoretical race between grace-period initialization and
            force_quiescent_state() that could occur if more than three
            jiffies were required to carry out the grace-period
            initialization.  Which it might, if you had enough CPUs.
    
    o       Apply Ingo's printk-standardization patch.
    
    o       Substitute local variables for repeated accesses to global
            variables.
    
    o       Fix comment misspellings and redundant (but harmless) increments
            of ->n_rcu_pending (this latter after having explicitly added it).
    
    o       Apply checkpatch fixes.
    
    Updates from v7 (http://lkml.org/lkml/2008/10/10/291):
    
    o       Fixed a number of problems noted by Gautham Shenoy, including
            the cpu-stall-detection bug that he was having difficulty
            convincing me was real.  ;-)
    
    o       Changed cpu-stall detection to wait for ten seconds rather than
            three in order to reduce false positive, as suggested by Ingo
            Molnar.
    
    o       Produced a design document (http://lwn.net/Articles/305782/).
            The act of writing this document uncovered a number of both
            theoretical and "here and now" bugs as noted below.
    
    o       Fix dynticks_nesting accounting confusion, simplify WARN_ON()
            condition, fix kerneldoc comments, and add memory barriers
            in dynticks interface functions.
    
    o       Add more data to tracing.
    
    o       Remove unused "rcu_barrier" field from rcu_data structure.
    
    o       Count calls to rcu_pending() from scheduling-clock interrupt
            to use as a surrogate timebase should jiffies stop counting.
    
    o       Fix a theoretical race between force_quiescent_state() and
            grace-period initialization.  Yes, initialization does have to
            go on for some jiffies for this race to occur, but given enough
            CPUs...
    
    Updates from v6 (http://lkml.org/lkml/2008/9/23/448):
    
    o       Fix a number of checkpatch.pl complaints.
    
    o       Apply review comments from Ingo Molnar and Lai Jiangshan
            on the stall-detection code.
    
    o       Fix several bugs in !CONFIG_SMP builds.
    
    o       Fix a misspelled config-parameter name so that RCU now announces
            at boot time if stall detection is configured.
    
    o       Run tests on numerous combinations of configurations parameters,
            which after the fixes above, now build and run correctly.
    
    Updates from v5 (http://lkml.org/lkml/2008/9/15/92, bad subject line):
    
    o       Fix a compiler error in the !CONFIG_FANOUT_EXACT case (blew a
            changeset some time ago, and finally got around to retesting
            this option).
    
    o       Fix some tracing bugs in rcupreempt that caused incorrect
            totals to be printed.
    
    o       I now test with a more brutal random-selection online/offline
            script (attached).  Probably more brutal than it needs to be
            on the people reading it as well, but so it goes.
    
    o       A number of optimizations and usability improvements:
    
            o       Make rcu_pending() ignore the grace-period timeout when
                    there is no grace period in progress.
    
            o       Make force_quiescent_state() avoid going for a global
                    lock in the case where there is no grace period in
                    progress.
    
            o       Rearrange struct fields to improve struct layout.
    
            o       Make call_rcu() initiate a grace period if RCU was
                    idle, rather than waiting for the next scheduling
                    clock interrupt.
    
            o       Invoke rcu_irq_enter() and rcu_irq_exit() only when
                    idle, as suggested by Andi Kleen.  I still don't
                    completely trust this change, and might back it out.
    
            o       Make CONFIG_RCU_TRACE be the single config variable
                    manipulated for all forms of RCU, instead of the prior
                    confusion.
    
            o       Document tracing files and formats for both rcupreempt
                    and rcutree.
    
    Updates from v4 for those missing v5 given its bad subject line:
    
    o       Separated dynticks interface so that NMIs and irqs call separate
            functions, greatly simplifying it.  In particular, this code
            no longer requires a proof of correctness.  ;-)
    
    o       Separated dynticks state out into its own per-CPU structure,
            avoiding the duplicated accounting.
    
    o       The case where a dynticks-idle CPU runs an irq handler that
            invokes call_rcu() is now correctly handled, forcing that CPU
            out of dynticks-idle mode.
    
    o       Review comments have been applied (thank you all!!!).
            For but one example, fixed the dynticks-ordering issue that
            Manfred pointed out, saving me much debugging.  ;-)
    
    o       Adjusted rcuclassic and rcupreempt to handle dynticks changes.
    
    Attached is an updated patch to Classic RCU that applies a hierarchy,
    greatly reducing the contention on the top-level lock for large machines.
    This passes 10-hour concurrent rcutorture and online-offline testing on
    128-CPU ppc64 without dynticks enabled, and exposes some timekeeping
    bugs in presence of dynticks (exciting working on a system where
    "sleep 1" hangs until interrupted...), which were fixed in the
    2.6.27 kernel.  It is getting more reliable than mainline by some
    measures, so the next version will be against -tip for inclusion.
    See also Manfred Spraul's recent patches (or his earlier work from
    2004 at http://marc.info/?l=linux-kernel&m=108546384711797&w=2).
    We will converge onto a common patch in the fullness of time, but are
    currently exploring different regions of the design space.  That said,
    I have already gratefully stolen quite a few of Manfred's ideas.
    
    This patch provides CONFIG_RCU_FANOUT, which controls the bushiness
    of the RCU hierarchy.  Defaults to 32 on 32-bit machines and 64 on
    64-bit machines.  If CONFIG_NR_CPUS is less than CONFIG_RCU_FANOUT,
    there is no hierarchy.  By default, the RCU initialization code will
    adjust CONFIG_RCU_FANOUT to balance the hierarchy, so strongly NUMA
    architectures may choose to set CONFIG_RCU_FANOUT_EXACT to disable
    this balancing, allowing the hierarchy to be exactly aligned to the
    underlying hardware.  Up to two levels of hierarchy are permitted
    (in addition to the root node), allowing up to 16,384 CPUs on 32-bit
    systems and up to 262,144 CPUs on 64-bit systems.  I just know that I
    am going to regret saying this, but this seems more than sufficient
    for the foreseeable future.  (Some architectures might wish to set
    CONFIG_RCU_FANOUT=4, which would limit such architectures to 64 CPUs.
    If this becomes a real problem, additional levels can be added, but I
    doubt that it will make a significant difference on real hardware.)
    
    In the common case, a given CPU will manipulate its private rcu_data
    structure and the rcu_node structure that it shares with its immediate
    neighbors.  This can reduce both lock and memory contention by multiple
    orders of magnitude, which should eliminate the need for the strange
    manipulations that are reported to be required when running Linux on
    very large systems.
    
    Some shortcomings:
    
    o       More bugs will probably surface as a result of an ongoing
            line-by-line code inspection.
    
            Patches will be provided as required.
    
    o       There are probably hangs, rcutorture failures, &c.  Seems
            quite stable on a 128-CPU machine, but that is kind of small
            compared to 4096 CPUs.  However, seems to do better than
            mainline.
    
            Patches will be provided as required.
    
    o       The memory footprint of this version is several KB larger
            than rcuclassic.
    
            A separate UP-only rcutiny patch will be provided, which will
            reduce the memory footprint significantly, even compared
            to the old rcuclassic.  One such patch passes light testing,
            and has a memory footprint smaller even than rcuclassic.
            Initial reaction from various embedded guys was "it is not
            worth it", so am putting it aside.
    
    Credits:
    
    o       Manfred Spraul for ideas, review comments, and bugs spotted,
            as well as some good friendly competition.  ;-)
    
    o       Josh Triplett, Ingo Molnar, Peter Zijlstra, Mathieu Desnoyers,
            Lai Jiangshan, Andi Kleen, Andy Whitcroft, and Andrew Morton
            for reviews and comments.
    
    o       Thomas Gleixner for much-needed help with some timer issues
            (see patches below).
    
    o       Jon M. Tollefson, Tim Pepper, Andrew Theurer, Jose R. Santos,
            Andy Whitcroft, Darrick Wong, Nishanth Aravamudan, Anton
            Blanchard, Dave Kleikamp, and Nathan Lynch for keeping machines
            alive despite my heavy abuse^Wtesting.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index e7c69a720d69..80d323e6f61a 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -269,6 +269,7 @@ void irq_enter(void)
 {
 	int cpu = smp_processor_id();
 
+	rcu_irq_enter();
 	if (idle_cpu(cpu) && !in_interrupt()) {
 		__irq_enter();
 		tick_check_idle(cpu);
@@ -295,9 +296,9 @@ void irq_exit(void)
 
 #ifdef CONFIG_NO_HZ
 	/* Make sure that timer wheel updates are propagated */
-	if (!in_interrupt() && idle_cpu(smp_processor_id()) && !need_resched())
-		tick_nohz_stop_sched_tick(0);
 	rcu_irq_exit();
+	if (idle_cpu(smp_processor_id()) && !in_interrupt() && !need_resched())
+		tick_nohz_stop_sched_tick(0);
 #endif
 	preempt_enable_no_resched();
 }

commit 8b752e3ef6e3f5cde87afc649dd51d92b1e549c1
Author: Liming Wang <liming.wang@windriver.com>
Date:   Fri Nov 28 09:52:40 2008 +0800

    softirq: remove useless function __local_bh_enable
    
    Impact: remove unused code
    
    __local_bh_enable has been replaced with _local_bh_enable.
    As comments says "it always nests inside local_bh_enable() sections"
    has not been valid now. Also there is no reason to use __local_bh_enable
    anywhere, so we can remove this useless function.
    
    Signed-off-by: Liming Wang <liming.wang@windriver.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index e7c69a720d69..8d9934b4162a 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -102,20 +102,6 @@ void local_bh_disable(void)
 
 EXPORT_SYMBOL(local_bh_disable);
 
-void __local_bh_enable(void)
-{
-	WARN_ON_ONCE(in_irq());
-
-	/*
-	 * softirqs should never be enabled by __local_bh_enable(),
-	 * it always nests inside local_bh_enable() sections:
-	 */
-	WARN_ON_ONCE(softirq_count() == SOFTIRQ_OFFSET);
-
-	sub_preempt_count(SOFTIRQ_OFFSET);
-}
-EXPORT_SYMBOL_GPL(__local_bh_enable);
-
 /*
  * Special-case - softirqs can safely be enabled in
  * cond_resched_softirq(), or by __do_softirq(),

commit ee5f80a993539490a07477ff2526bf62c503fbb4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Nov 7 11:06:00 2008 +0100

    irq: call __irq_enter() before calling the tick_idle_check
    
    Impact: avoid spurious ksoftirqd wakeups
    
    The tick idle check which is called from irq_enter() was run before
    the call to __irq_enter() which did not set the in_interrupt() bits in
    preempt_count. That way the raise of a softirq woke up softirqd for
    nothing as the softirq was handled on return from interrupt.
    
    Call __irq_enter() before calling into the tick idle check code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 7110daeb9a90..e7c69a720d69 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -269,10 +269,11 @@ void irq_enter(void)
 {
 	int cpu = smp_processor_id();
 
-	if (idle_cpu(cpu) && !in_interrupt())
+	if (idle_cpu(cpu) && !in_interrupt()) {
+		__irq_enter();
 		tick_check_idle(cpu);
-
-	__irq_enter();
+	} else
+		__irq_enter();
 }
 
 #ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED

commit c465a76af658b443075d6efee1c3131257643020
Merge: 2d42244ae71d 1b02469088ac fb02fbc14d17 d40e944c25fb 1508487e7f16 322acf6585f3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Oct 20 13:14:06 2008 +0200

    Merge branches 'timers/clocksource', 'timers/hrtimers', 'timers/nohz', 'timers/ntp', 'timers/posixtimers' and 'timers/debug' into v28-timers-for-linus

commit 719254faa17ffedc87ba0fadb9b34e535c9758d5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Oct 17 09:59:47 2008 +0200

    NOHZ: unify the nohz function calls in irq_enter()
    
    We have two separate nohz function calls in irq_enter() for no good
    reason. Just call a single NOHZ function from irq_enter() and call
    the bits in the tick code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 37d67aa2d56f..d410014279e7 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -265,16 +265,12 @@ asmlinkage void do_softirq(void)
  */
 void irq_enter(void)
 {
-#ifdef CONFIG_NO_HZ
 	int cpu = smp_processor_id();
+
 	if (idle_cpu(cpu) && !in_interrupt())
-		tick_nohz_stop_idle(cpu);
-#endif
+		tick_check_idle(cpu);
+
 	__irq_enter();
-#ifdef CONFIG_NO_HZ
-	if (idle_cpu(cpu))
-		tick_nohz_update_jiffies();
-#endif
 }
 
 #ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED

commit 54514a70adefe356afe854e2d3912d46668068e6
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Sep 23 22:15:57 2008 -0700

    softirq: Add support for triggering softirq work on softirqs.
    
    This is basically a genericization of Jens Axboe's block layer
    remote softirq changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 37d67aa2d56f..83ba21a13bd4 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -6,6 +6,8 @@
  *	Distribute under GPLv2.
  *
  *	Rewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)
+ *
+ *	Remote softirq infrastructure is by Jens Axboe.
  */
 
 #include <linux/module.h>
@@ -474,17 +476,144 @@ void tasklet_kill(struct tasklet_struct *t)
 
 EXPORT_SYMBOL(tasklet_kill);
 
+DEFINE_PER_CPU(struct list_head [NR_SOFTIRQS], softirq_work_list);
+EXPORT_PER_CPU_SYMBOL(softirq_work_list);
+
+static void __local_trigger(struct call_single_data *cp, int softirq)
+{
+	struct list_head *head = &__get_cpu_var(softirq_work_list[softirq]);
+
+	list_add_tail(&cp->list, head);
+
+	/* Trigger the softirq only if the list was previously empty.  */
+	if (head->next == &cp->list)
+		raise_softirq_irqoff(softirq);
+}
+
+#ifdef CONFIG_USE_GENERIC_SMP_HELPERS
+static void remote_softirq_receive(void *data)
+{
+	struct call_single_data *cp = data;
+	unsigned long flags;
+	int softirq;
+
+	softirq = cp->priv;
+
+	local_irq_save(flags);
+	__local_trigger(cp, softirq);
+	local_irq_restore(flags);
+}
+
+static int __try_remote_softirq(struct call_single_data *cp, int cpu, int softirq)
+{
+	if (cpu_online(cpu)) {
+		cp->func = remote_softirq_receive;
+		cp->info = cp;
+		cp->flags = 0;
+		cp->priv = softirq;
+
+		__smp_call_function_single(cpu, cp);
+		return 0;
+	}
+	return 1;
+}
+#else /* CONFIG_USE_GENERIC_SMP_HELPERS */
+static int __try_remote_softirq(struct call_single_data *cp, int cpu, int softirq)
+{
+	return 1;
+}
+#endif
+
+/**
+ * __send_remote_softirq - try to schedule softirq work on a remote cpu
+ * @cp: private SMP call function data area
+ * @cpu: the remote cpu
+ * @this_cpu: the currently executing cpu
+ * @softirq: the softirq for the work
+ *
+ * Attempt to schedule softirq work on a remote cpu.  If this cannot be
+ * done, the work is instead queued up on the local cpu.
+ *
+ * Interrupts must be disabled.
+ */
+void __send_remote_softirq(struct call_single_data *cp, int cpu, int this_cpu, int softirq)
+{
+	if (cpu == this_cpu || __try_remote_softirq(cp, cpu, softirq))
+		__local_trigger(cp, softirq);
+}
+EXPORT_SYMBOL(__send_remote_softirq);
+
+/**
+ * send_remote_softirq - try to schedule softirq work on a remote cpu
+ * @cp: private SMP call function data area
+ * @cpu: the remote cpu
+ * @softirq: the softirq for the work
+ *
+ * Like __send_remote_softirq except that disabling interrupts and
+ * computing the current cpu is done for the caller.
+ */
+void send_remote_softirq(struct call_single_data *cp, int cpu, int softirq)
+{
+	unsigned long flags;
+	int this_cpu;
+
+	local_irq_save(flags);
+	this_cpu = smp_processor_id();
+	__send_remote_softirq(cp, cpu, this_cpu, softirq);
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL(send_remote_softirq);
+
+static int __cpuinit remote_softirq_cpu_notify(struct notifier_block *self,
+					       unsigned long action, void *hcpu)
+{
+	/*
+	 * If a CPU goes away, splice its entries to the current CPU
+	 * and trigger a run of the softirq
+	 */
+	if (action == CPU_DEAD || action == CPU_DEAD_FROZEN) {
+		int cpu = (unsigned long) hcpu;
+		int i;
+
+		local_irq_disable();
+		for (i = 0; i < NR_SOFTIRQS; i++) {
+			struct list_head *head = &per_cpu(softirq_work_list[i], cpu);
+			struct list_head *local_head;
+
+			if (list_empty(head))
+				continue;
+
+			local_head = &__get_cpu_var(softirq_work_list[i]);
+			list_splice_init(head, local_head);
+			raise_softirq_irqoff(i);
+		}
+		local_irq_enable();
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block __cpuinitdata remote_softirq_cpu_notifier = {
+	.notifier_call	= remote_softirq_cpu_notify,
+};
+
 void __init softirq_init(void)
 {
 	int cpu;
 
 	for_each_possible_cpu(cpu) {
+		int i;
+
 		per_cpu(tasklet_vec, cpu).tail =
 			&per_cpu(tasklet_vec, cpu).head;
 		per_cpu(tasklet_hi_vec, cpu).tail =
 			&per_cpu(tasklet_hi_vec, cpu).head;
+		for (i = 0; i < NR_SOFTIRQS; i++)
+			INIT_LIST_HEAD(&per_cpu(softirq_work_list[i], cpu));
 	}
 
+	register_hotcpu_notifier(&remote_softirq_cpu_notifier);
+
 	open_softirq(TASKLET_SOFTIRQ, tasklet_action);
 	open_softirq(HI_SOFTIRQ, tasklet_hi_action);
 }

commit 1c95e1b69073cff5ff179e592fa1a1e182c78a17
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 16 15:32:46 2008 -0700

    Fix kernel/softirq.c printk format warning properly
    
    This fixes the broken 77af7e3403e7314c47b0c07fbc5e4ef21d939532
    ("softirq, warning fix: correct a format to avoid a warning") fix
    correctly.
    
    The type of a pointer subtraction is not "int", nor is it "long".  It
    can be either (or something else).  It's "ptrdiff_t", and the printk
    format for it is "%td".
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index be7a8292f992..37d67aa2d56f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -210,7 +210,7 @@ asmlinkage void __do_softirq(void)
 			h->action(h);
 
 			if (unlikely(prev_count != preempt_count())) {
-				printk(KERN_ERR "huh, entered softirq %d %p"
+				printk(KERN_ERR "huh, entered softirq %td %p"
 				       "with preempt_count %08x,"
 				       " exited with %08x?\n", h - softirq_vec,
 				       h->action, prev_count, preempt_count());

commit 77af7e3403e7314c47b0c07fbc5e4ef21d939532
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 3 11:39:46 2008 +0200

    softirq, warning fix: correct a format to avoid a warning
    
    Last -tip gives this warning:
    
    kernel/softirq.c: Dans la fonction __do_softirq :
    kernel/softirq.c:216: attention : format %ld expects type long int, but argument 2 has type int
    
    This patch corrects the format type, and a small mistake in the "softirq" word.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 1cf1e2f2c406..be7a8292f992 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -210,7 +210,7 @@ asmlinkage void __do_softirq(void)
 			h->action(h);
 
 			if (unlikely(prev_count != preempt_count())) {
-				printk(KERN_ERR "huh, entered sotfirq %ld %p"
+				printk(KERN_ERR "huh, entered softirq %d %p"
 				       "with preempt_count %08x,"
 				       " exited with %08x?\n", h - softirq_vec,
 				       h->action, prev_count, preempt_count());

commit 8e85b4b553fc932e1c5141feb5fda389b7f5db01
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 2 10:50:53 2008 +0200

    softirqs, debug: preemption check
    
    if a preempt count leaks out of a softirq handler it can be very hard
    to figure it out. Add a debug check for this.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 82e32aadedd8..1cf1e2f2c406 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -205,7 +205,18 @@ asmlinkage void __do_softirq(void)
 
 	do {
 		if (pending & 1) {
+			int prev_count = preempt_count();
+
 			h->action(h);
+
+			if (unlikely(prev_count != preempt_count())) {
+				printk(KERN_ERR "huh, entered sotfirq %ld %p"
+				       "with preempt_count %08x,"
+				       " exited with %08x?\n", h - softirq_vec,
+				       h->action, prev_count, preempt_count());
+				preempt_count() = prev_count;
+			}
+
 			rcu_bh_qsctr_inc(cpu);
 		}
 		h++;

commit 978b0116cd225682a29e3d1d5010319bf2de32c2
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sat Sep 6 20:04:36 2008 +0200

    softirq: allocate less vectors
    
    We don't need whole 32 of them, only NR_SOFTIRQS.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index c506f266a6b9..82e32aadedd8 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -46,7 +46,7 @@ irq_cpustat_t irq_stat[NR_CPUS] ____cacheline_aligned;
 EXPORT_SYMBOL(irq_stat);
 #endif
 
-static struct softirq_action softirq_vec[32] __cacheline_aligned_in_smp;
+static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;
 
 static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
 

commit 7babe8db99d305340cf4828ce1f5a1481d5622ef
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Fri Jul 25 19:45:11 2008 -0700

    Full conversion to early_initcall() interface, remove old interface
    
    A previous patch added the early_initcall(), to allow a cleaner hooking of
    pre-SMP initcalls.  Now we remove the older interface, converting all
    existing users to the new one.
    
    [akpm@linux-foundation.org: cleanups]
    [akpm@linux-foundation.org: build fix]
    [kosaki.motohiro@jp.fujitsu.com: warning fix]
    [kosaki.motohiro@jp.fujitsu.com: warning fix]
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index f6b03d56c2bf..c506f266a6b9 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -630,7 +630,7 @@ static struct notifier_block __cpuinitdata cpu_nfb = {
 	.notifier_call = cpu_callback
 };
 
-__init int spawn_ksoftirqd(void)
+static __init int spawn_ksoftirqd(void)
 {
 	void *cpu = (void *)(long)smp_processor_id();
 	int err = cpu_callback(&cpu_nfb, CPU_UP_PREPARE, cpu);
@@ -640,6 +640,7 @@ __init int spawn_ksoftirqd(void)
 	register_cpu_notifier(&cpu_nfb);
 	return 0;
 }
+early_initcall(spawn_ksoftirqd);
 
 #ifdef CONFIG_SMP
 /*

commit 9b610fda0df5d0f0b0c64242e37441ad1b384aac
Merge: b8f8c3cf0a4a 5b664cb235e9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 18 19:53:16 2008 +0200

    Merge branch 'linus' into timers/nohz

commit b8f8c3cf0a4ac0632ec3f0e15e9dc0c29de917af
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 18 17:27:28 2008 +0200

    nohz: prevent tick stop outside of the idle loop
    
    Jack Ren and Eric Miao tracked down the following long standing
    problem in the NOHZ code:
    
            scheduler switch to idle task
            enable interrupts
    
    Window starts here
    
            ----> interrupt happens (does not set NEED_RESCHED)
                    irq_exit() stops the tick
    
            ----> interrupt happens (does set NEED_RESCHED)
    
            return from schedule()
    
            cpu_idle(): preempt_disable();
    
    Window ends here
    
    The interrupts can happen at any point inside the race window. The
    first interrupt stops the tick, the second one causes the scheduler to
    rerun and switch away from idle again and we end up with the tick
    disabled.
    
    The fact that it needs two interrupts where the first one does not set
    NEED_RESCHED and the second one does made the bug obscure and extremly
    hard to reproduce and analyse. Kudos to Jack and Eric.
    
    Solution: Limit the NOHZ functionality to the idle loop to make sure
    that we can not run into such a situation ever again.
    
    cpu_idle()
    {
            preempt_disable();
    
            while(1) {
                     tick_nohz_stop_sched_tick(1); <- tell NOHZ code that we
                                                      are in the idle loop
    
                     while (!need_resched())
                           halt();
    
                     tick_nohz_restart_sched_tick(); <- disables NOHZ mode
                     preempt_enable_no_resched();
                     schedule();
                     preempt_disable();
            }
    }
    
    In hindsight we should have done this forever, but ...
    
    /me grabs a large brown paperbag.
    
    Debugged-by: Jack Ren <jack.ren@marvell.com>,
    Debugged-by: eric miao <eric.y.miao@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 36e061740047..05f248039d77 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -312,7 +312,7 @@ void irq_exit(void)
 #ifdef CONFIG_NO_HZ
 	/* Make sure that timer wheel updates are propagated */
 	if (!in_interrupt() && idle_cpu(smp_processor_id()) && !need_resched())
-		tick_nohz_stop_sched_tick();
+		tick_nohz_stop_sched_tick(0);
 	rcu_irq_exit();
 #endif
 	preempt_enable_no_resched();

commit 1a781a777b2f6ac46523fe92396215762ced624d
Merge: b9d2252c1e44 42a2f217a5e3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 15 21:55:59 2008 +0200

    Merge branch 'generic-ipi' into generic-ipi-for-linus
    
    Conflicts:
    
            arch/powerpc/Kconfig
            arch/s390/kernel/time.c
            arch/x86/kernel/apic_32.c
            arch/x86/kernel/cpu/perfctr-watchdog.c
            arch/x86/kernel/i8259_64.c
            arch/x86/kernel/ldt.c
            arch/x86/kernel/nmi_64.c
            arch/x86/kernel/smpboot.c
            arch/x86/xen/smp.c
            include/asm-x86/hw_irq_32.h
            include/asm-x86/hw_irq_64.h
            include/asm-x86/mach-default/irq_vectors.h
            include/asm-x86/mach-voyager/irq_vectors.h
            include/asm-x86/smp.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 666484f0250db2e016948d63b3ef33e202e3b8d0
Merge: d18bb9a548e5 ace7f1b79670
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 14 15:28:42 2008 -0700

    Merge branch 'core/softirq' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core/softirq' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      softirq: remove irqs_disabled warning from local_bh_enable
      softirq: remove initialization of static per-cpu variable
      Remove argument from open_softirq which is always NULL

commit 15c8b6c1aaaf1c4edd67e2f02e4d8e1bd1a51c0d
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri May 9 09:39:44 2008 +0200

    on_each_cpu(): kill unused 'retry' parameter
    
    It's not even passed on to smp_call_function() anymore, since that
    was removed. So kill it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index d73afb4764ef..c159fd094772 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -674,7 +674,7 @@ __init int spawn_ksoftirqd(void)
 /*
  * Call a function on all processors
  */
-int on_each_cpu(void (*func) (void *info), void *info, int retry, int wait)
+int on_each_cpu(void (*func) (void *info), void *info, int wait)
 {
 	int ret = 0;
 

commit 8691e5a8f691cc2a4fda0651e8d307aaba0e7d68
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jun 6 11:18:06 2008 +0200

    smp_call_function: get rid of the unused nonatomic/retry argument
    
    It's never used and the comments refer to nonatomic and retry
    interchangably. So get rid of it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 36e061740047..d73afb4764ef 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -679,7 +679,7 @@ int on_each_cpu(void (*func) (void *info), void *info, int retry, int wait)
 	int ret = 0;
 
 	preempt_disable();
-	ret = smp_call_function(func, info, retry, wait);
+	ret = smp_call_function(func, info, wait);
 	local_irq_disable();
 	func(info);
 	local_irq_enable();

commit 961ccddd59d627b89bd3dc284b6517833bbdf25d
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Jun 23 13:55:38 2008 +1000

    sched: add new API sched_setscheduler_nocheck: add a flag to control access checks
    
    Hidehiro Kawai noticed that sched_setscheduler() can fail in
    stop_machine: it calls sched_setscheduler() from insmod, which can
    have CAP_SYS_MODULE without CAP_SYS_NICE.
    
    Two cases could have failed, so are changed to sched_setscheduler_nocheck:
      kernel/softirq.c:cpu_callback()
            - CPU hotplug callback
      kernel/stop_machine.c:__stop_machine_run()
            - Called from various places, including modprobe()
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-mm@kvack.org
    Cc: sugita <yumiko.sugita.yf@hitachi.com>
    Cc: Satoshi OSHIMA <satoshi.oshima.fk@hitachi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 36e061740047..afd9120c2fc4 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -645,7 +645,7 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 
 		p = per_cpu(ksoftirqd, hotcpu);
 		per_cpu(ksoftirqd, hotcpu) = NULL;
-		sched_setscheduler(p, SCHED_FIFO, &param);
+		sched_setscheduler_nocheck(p, SCHED_FIFO, &param);
 		kthread_stop(p);
 		takeover_tasklets(hotcpu);
 		break;

commit 0f476b6d91a1395bda6464e653ce66ea9bea7167
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Wed Jun 18 09:29:37 2008 +0200

    softirq: remove irqs_disabled warning from local_bh_enable
    
    There's no need to use local_irq_save() over local_irq_disable() in the
    local_bh_enable code since it is a bug to call it with irqs disabled and
    do_softirq will enable irqs if there is any pending work.
    
    Consolidate the code from local_bh_enable and ..._ip to avoid having a
    disconnect between them in the warnings they trigger that is currently
    there.
    
    Also always trigger the warning on in_irq(), not just in the
    trace-irqflags case.
    
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Cc: Michael Buesch <mb@bu3sch.de>
    Cc: David Ellingsworth <david@identd.dyndns.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 86775340ef1d..2cf2502b7c73 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -131,23 +131,17 @@ void _local_bh_enable(void)
 
 EXPORT_SYMBOL(_local_bh_enable);
 
-void local_bh_enable(void)
+static inline void _local_bh_enable_ip(unsigned long ip)
 {
+	WARN_ON_ONCE(in_irq() || irqs_disabled());
 #ifdef CONFIG_TRACE_IRQFLAGS
-	unsigned long flags;
-
-	WARN_ON_ONCE(in_irq());
-#endif
-	WARN_ON_ONCE(irqs_disabled());
-
-#ifdef CONFIG_TRACE_IRQFLAGS
-	local_irq_save(flags);
+	local_irq_disable();
 #endif
 	/*
 	 * Are softirqs going to be turned on now:
 	 */
 	if (softirq_count() == SOFTIRQ_OFFSET)
-		trace_softirqs_on((unsigned long)__builtin_return_address(0));
+		trace_softirqs_on(ip);
 	/*
 	 * Keep preemption disabled until we are done with
 	 * softirq processing:
@@ -159,40 +153,20 @@ void local_bh_enable(void)
 
 	dec_preempt_count();
 #ifdef CONFIG_TRACE_IRQFLAGS
-	local_irq_restore(flags);
+	local_irq_enable();
 #endif
 	preempt_check_resched();
 }
+
+void local_bh_enable(void)
+{
+	_local_bh_enable_ip((unsigned long)__builtin_return_address(0));
+}
 EXPORT_SYMBOL(local_bh_enable);
 
 void local_bh_enable_ip(unsigned long ip)
 {
-#ifdef CONFIG_TRACE_IRQFLAGS
-	unsigned long flags;
-
-	WARN_ON_ONCE(in_irq());
-
-	local_irq_save(flags);
-#endif
-	/*
-	 * Are softirqs going to be turned on now:
-	 */
-	if (softirq_count() == SOFTIRQ_OFFSET)
-		trace_softirqs_on(ip);
-	/*
-	 * Keep preemption disabled until we are done with
-	 * softirq processing:
- 	 */
- 	sub_preempt_count(SOFTIRQ_OFFSET - 1);
-
-	if (unlikely(!in_interrupt() && local_softirq_pending()))
-		do_softirq();
-
-	dec_preempt_count();
-#ifdef CONFIG_TRACE_IRQFLAGS
-	local_irq_restore(flags);
-#endif
-	preempt_check_resched();
+	_local_bh_enable_ip(ip);
 }
 EXPORT_SYMBOL(local_bh_enable_ip);
 

commit 4620b49f76096fa5183eecad7d689faa898a4c82
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Thu Jun 12 23:21:53 2008 +0200

    softirq: remove initialization of static per-cpu variable
    
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 059256874e9b..86775340ef1d 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -359,10 +359,8 @@ struct tasklet_head
 	struct tasklet_struct **tail;
 };
 
-/* Some compilers disobey section attribute on statics when not
-   initialized -- RR */
-static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec) = { NULL };
-static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec) = { NULL };
+static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);
+static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);
 
 void __tasklet_schedule(struct tasklet_struct *t)
 {

commit 962cf36c5bf6d2840b8d66ee9a606fae2f540bbd
Author: Carlos R. Mafra <crmafra2@gmail.com>
Date:   Thu May 15 11:15:37 2008 -0300

    Remove argument from open_softirq which is always NULL
    
    As git-grep shows, open_softirq() is always called with the last argument
    being NULL
    
    block/blk-core.c:       open_softirq(BLOCK_SOFTIRQ, blk_done_softirq, NULL);
    kernel/hrtimer.c:       open_softirq(HRTIMER_SOFTIRQ, run_hrtimer_softirq, NULL);
    kernel/rcuclassic.c:    open_softirq(RCU_SOFTIRQ, rcu_process_callbacks, NULL);
    kernel/rcupreempt.c:    open_softirq(RCU_SOFTIRQ, rcu_process_callbacks, NULL);
    kernel/sched.c: open_softirq(SCHED_SOFTIRQ, run_rebalance_domains, NULL);
    kernel/softirq.c:       open_softirq(TASKLET_SOFTIRQ, tasklet_action, NULL);
    kernel/softirq.c:       open_softirq(HI_SOFTIRQ, tasklet_hi_action, NULL);
    kernel/timer.c: open_softirq(TIMER_SOFTIRQ, run_timer_softirq, NULL);
    net/core/dev.c: open_softirq(NET_TX_SOFTIRQ, net_tx_action, NULL);
    net/core/dev.c: open_softirq(NET_RX_SOFTIRQ, net_rx_action, NULL);
    
    This observation has already been made by Matthew Wilcox in June 2002
    (http://www.cs.helsinki.fi/linux/linux-kernel/2002-25/0687.html)
    
    "I notice that none of the current softirq routines use the data element
    passed to them."
    
    and the situation hasn't changed since them. So it appears we can safely
    remove that extra argument to save 128 (54) bytes of kernel data (text).
    
    Signed-off-by: Carlos R. Mafra <crmafra@ift.unesp.br>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 36e061740047..059256874e9b 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -347,9 +347,8 @@ void raise_softirq(unsigned int nr)
 	local_irq_restore(flags);
 }
 
-void open_softirq(int nr, void (*action)(struct softirq_action*), void *data)
+void open_softirq(int nr, void (*action)(struct softirq_action *))
 {
-	softirq_vec[nr].data = data;
 	softirq_vec[nr].action = action;
 }
 
@@ -503,8 +502,8 @@ void __init softirq_init(void)
 			&per_cpu(tasklet_hi_vec, cpu).head;
 	}
 
-	open_softirq(TASKLET_SOFTIRQ, tasklet_action, NULL);
-	open_softirq(HI_SOFTIRQ, tasklet_hi_action, NULL);
+	open_softirq(TASKLET_SOFTIRQ, tasklet_action);
+	open_softirq(HI_SOFTIRQ, tasklet_hi_action);
 }
 
 static int ksoftirqd(void * __bind_cpu)

commit e5e417232e7c9ecc58a77902d2e8dd46792cd092
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu May 1 04:34:23 2008 -0700

    Fix cpu hotplug problem in softirq code
    
    currently cpu hotplug (unplug) seems broken on s390 and likely others. On cpu
    unplug the system starts to behave very strange and hangs.
    
    I bisected the problem to the following commit:
    
    commit 48f20a9a9488c432fc86df1ff4b7f4fa895d1183
    Author: Olof Johansson <olof@lixom.net>
    Date: Tue Mar 4 15:23:25 2008 -0800
            tasklets: execute tasklets in the same order they were queued
    
    Reverting this patch seems to fix the problem.  I looked into takeover_tasklet
    and it seems that there is a way to corrupt the tail pointer of the current
    cpu.  If the tasklet list of the frozen cpu is empty, the tail pointer of the
    current cpu points to the address of the head pointer of the stopped cpu and
    not to the next pointer of a tasklet_struct.
    
    This patch avoids the list splice of the list is empty and cpu hotplug seems
    to work as the tail pointer is not corrupted.  Olof, can you look into that
    patch and ACK/NACK it so Andrew can push this to Linus, if appropriate?
    Please note that some lines are longer than 80 chars, but line-wrapping looked
    worse that this version.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 3c44956ee7e2..36e061740047 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -589,16 +589,20 @@ static void takeover_tasklets(unsigned int cpu)
 	local_irq_disable();
 
 	/* Find end, append list for that CPU. */
-	*__get_cpu_var(tasklet_vec).tail = per_cpu(tasklet_vec, cpu).head;
-	__get_cpu_var(tasklet_vec).tail = per_cpu(tasklet_vec, cpu).tail;
-	per_cpu(tasklet_vec, cpu).head = NULL;
-	per_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;
+	if (&per_cpu(tasklet_vec, cpu).head != per_cpu(tasklet_vec, cpu).tail) {
+		*(__get_cpu_var(tasklet_vec).tail) = per_cpu(tasklet_vec, cpu).head;
+		__get_cpu_var(tasklet_vec).tail = per_cpu(tasklet_vec, cpu).tail;
+		per_cpu(tasklet_vec, cpu).head = NULL;
+		per_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;
+	}
 	raise_softirq_irqoff(TASKLET_SOFTIRQ);
 
-	*__get_cpu_var(tasklet_hi_vec).tail = per_cpu(tasklet_hi_vec, cpu).head;
-	__get_cpu_var(tasklet_hi_vec).tail = per_cpu(tasklet_hi_vec, cpu).tail;
-	per_cpu(tasklet_hi_vec, cpu).head = NULL;
-	per_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;
+	if (&per_cpu(tasklet_hi_vec, cpu).head != per_cpu(tasklet_hi_vec, cpu).tail) {
+		*__get_cpu_var(tasklet_hi_vec).tail = per_cpu(tasklet_hi_vec, cpu).head;
+		__get_cpu_var(tasklet_hi_vec).tail = per_cpu(tasklet_hi_vec, cpu).tail;
+		per_cpu(tasklet_hi_vec, cpu).head = NULL;
+		per_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;
+	}
 	raise_softirq_irqoff(HI_SOFTIRQ);
 
 	local_irq_enable();

commit 48f20a9a9488c432fc86df1ff4b7f4fa895d1183
Author: Olof Johansson <olof@lixom.net>
Date:   Tue Mar 4 15:23:25 2008 -0800

    tasklets: execute tasklets in the same order they were queued
    
    I noticed this when looking at an openswan issue.  Openswan (ab?)uses the
    tasklet API to defer processing of packets in some situations, with one
    packet per tasklet_action().  I started noticing sequences of
    backwards-ordered sequence numbers coming over the wire, since new tasklets
    are always queued at the head of the list but processed sequentially.
    
    Convert it to instead append new entries to the tail of the list.  As an
    extra bonus, the splicing code in takeover_tasklets() no longer has to
    iterate over the list.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 31e9f2a47928..3c44956ee7e2 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -356,7 +356,8 @@ void open_softirq(int nr, void (*action)(struct softirq_action*), void *data)
 /* Tasklets */
 struct tasklet_head
 {
-	struct tasklet_struct *list;
+	struct tasklet_struct *head;
+	struct tasklet_struct **tail;
 };
 
 /* Some compilers disobey section attribute on statics when not
@@ -369,8 +370,9 @@ void __tasklet_schedule(struct tasklet_struct *t)
 	unsigned long flags;
 
 	local_irq_save(flags);
-	t->next = __get_cpu_var(tasklet_vec).list;
-	__get_cpu_var(tasklet_vec).list = t;
+	t->next = NULL;
+	*__get_cpu_var(tasklet_vec).tail = t;
+	__get_cpu_var(tasklet_vec).tail = &(t->next);
 	raise_softirq_irqoff(TASKLET_SOFTIRQ);
 	local_irq_restore(flags);
 }
@@ -382,8 +384,9 @@ void __tasklet_hi_schedule(struct tasklet_struct *t)
 	unsigned long flags;
 
 	local_irq_save(flags);
-	t->next = __get_cpu_var(tasklet_hi_vec).list;
-	__get_cpu_var(tasklet_hi_vec).list = t;
+	t->next = NULL;
+	*__get_cpu_var(tasklet_hi_vec).tail = t;
+	__get_cpu_var(tasklet_hi_vec).tail = &(t->next);
 	raise_softirq_irqoff(HI_SOFTIRQ);
 	local_irq_restore(flags);
 }
@@ -395,8 +398,9 @@ static void tasklet_action(struct softirq_action *a)
 	struct tasklet_struct *list;
 
 	local_irq_disable();
-	list = __get_cpu_var(tasklet_vec).list;
-	__get_cpu_var(tasklet_vec).list = NULL;
+	list = __get_cpu_var(tasklet_vec).head;
+	__get_cpu_var(tasklet_vec).head = NULL;
+	__get_cpu_var(tasklet_vec).tail = &__get_cpu_var(tasklet_vec).head;
 	local_irq_enable();
 
 	while (list) {
@@ -416,8 +420,9 @@ static void tasklet_action(struct softirq_action *a)
 		}
 
 		local_irq_disable();
-		t->next = __get_cpu_var(tasklet_vec).list;
-		__get_cpu_var(tasklet_vec).list = t;
+		t->next = NULL;
+		*__get_cpu_var(tasklet_vec).tail = t;
+		__get_cpu_var(tasklet_vec).tail = &(t->next);
 		__raise_softirq_irqoff(TASKLET_SOFTIRQ);
 		local_irq_enable();
 	}
@@ -428,8 +433,9 @@ static void tasklet_hi_action(struct softirq_action *a)
 	struct tasklet_struct *list;
 
 	local_irq_disable();
-	list = __get_cpu_var(tasklet_hi_vec).list;
-	__get_cpu_var(tasklet_hi_vec).list = NULL;
+	list = __get_cpu_var(tasklet_hi_vec).head;
+	__get_cpu_var(tasklet_hi_vec).head = NULL;
+	__get_cpu_var(tasklet_hi_vec).tail = &__get_cpu_var(tasklet_hi_vec).head;
 	local_irq_enable();
 
 	while (list) {
@@ -449,8 +455,9 @@ static void tasklet_hi_action(struct softirq_action *a)
 		}
 
 		local_irq_disable();
-		t->next = __get_cpu_var(tasklet_hi_vec).list;
-		__get_cpu_var(tasklet_hi_vec).list = t;
+		t->next = NULL;
+		*__get_cpu_var(tasklet_hi_vec).tail = t;
+		__get_cpu_var(tasklet_hi_vec).tail = &(t->next);
 		__raise_softirq_irqoff(HI_SOFTIRQ);
 		local_irq_enable();
 	}
@@ -487,6 +494,15 @@ EXPORT_SYMBOL(tasklet_kill);
 
 void __init softirq_init(void)
 {
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		per_cpu(tasklet_vec, cpu).tail =
+			&per_cpu(tasklet_vec, cpu).head;
+		per_cpu(tasklet_hi_vec, cpu).tail =
+			&per_cpu(tasklet_hi_vec, cpu).head;
+	}
+
 	open_softirq(TASKLET_SOFTIRQ, tasklet_action, NULL);
 	open_softirq(HI_SOFTIRQ, tasklet_hi_action, NULL);
 }
@@ -555,9 +571,12 @@ void tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu)
 		return;
 
 	/* CPU is dead, so no lock needed. */
-	for (i = &per_cpu(tasklet_vec, cpu).list; *i; i = &(*i)->next) {
+	for (i = &per_cpu(tasklet_vec, cpu).head; *i; i = &(*i)->next) {
 		if (*i == t) {
 			*i = t->next;
+			/* If this was the tail element, move the tail ptr */
+			if (*i == NULL)
+				per_cpu(tasklet_vec, cpu).tail = i;
 			return;
 		}
 	}
@@ -566,20 +585,20 @@ void tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu)
 
 static void takeover_tasklets(unsigned int cpu)
 {
-	struct tasklet_struct **i;
-
 	/* CPU is dead, so no lock needed. */
 	local_irq_disable();
 
 	/* Find end, append list for that CPU. */
-	for (i = &__get_cpu_var(tasklet_vec).list; *i; i = &(*i)->next);
-	*i = per_cpu(tasklet_vec, cpu).list;
-	per_cpu(tasklet_vec, cpu).list = NULL;
+	*__get_cpu_var(tasklet_vec).tail = per_cpu(tasklet_vec, cpu).head;
+	__get_cpu_var(tasklet_vec).tail = per_cpu(tasklet_vec, cpu).tail;
+	per_cpu(tasklet_vec, cpu).head = NULL;
+	per_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;
 	raise_softirq_irqoff(TASKLET_SOFTIRQ);
 
-	for (i = &__get_cpu_var(tasklet_hi_vec).list; *i; i = &(*i)->next);
-	*i = per_cpu(tasklet_hi_vec, cpu).list;
-	per_cpu(tasklet_hi_vec, cpu).list = NULL;
+	*__get_cpu_var(tasklet_hi_vec).tail = per_cpu(tasklet_hi_vec, cpu).head;
+	__get_cpu_var(tasklet_hi_vec).tail = per_cpu(tasklet_hi_vec, cpu).tail;
+	per_cpu(tasklet_hi_vec, cpu).head = NULL;
+	per_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;
 	raise_softirq_irqoff(HI_SOFTIRQ);
 
 	local_irq_enable();

commit 2232c2d8e0a6a31061dec311f3d1cf7624bc14f1
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Feb 29 18:46:50 2008 +0100

    rcu: add support for dynamic ticks and preempt rcu
    
    The PREEMPT-RCU can get stuck if a CPU goes idle and NO_HZ is set. The
    idle CPU will not progress the RCU through its grace period and a
    synchronize_rcu my get stuck. Without this patch I have a box that will
    not boot when PREEMPT_RCU and NO_HZ are set. That same box boots fine
    with this patch.
    
    This patch comes from the -rt kernel where it has been tested for
    several months.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 5b3aea5f471e..31e9f2a47928 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -313,6 +313,7 @@ void irq_exit(void)
 	/* Make sure that timer wheel updates are propagated */
 	if (!in_interrupt() && idle_cpu(smp_processor_id()) && !need_resched())
 		tick_nohz_stop_sched_tick();
+	rcu_irq_exit();
 #endif
 	preempt_enable_no_resched();
 }

commit 7ad5b3a505e68cfdc342933d6e0fc0eaa5e0a4f7
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Fri Feb 8 04:19:53 2008 -0800

    kernel: remove fastcall in kernel/*
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index d7837d45419e..5b3aea5f471e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -320,7 +320,7 @@ void irq_exit(void)
 /*
  * This function must run with irqs disabled!
  */
-inline fastcall void raise_softirq_irqoff(unsigned int nr)
+inline void raise_softirq_irqoff(unsigned int nr)
 {
 	__raise_softirq_irqoff(nr);
 
@@ -337,7 +337,7 @@ inline fastcall void raise_softirq_irqoff(unsigned int nr)
 		wakeup_softirqd();
 }
 
-void fastcall raise_softirq(unsigned int nr)
+void raise_softirq(unsigned int nr)
 {
 	unsigned long flags;
 
@@ -363,7 +363,7 @@ struct tasklet_head
 static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec) = { NULL };
 static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec) = { NULL };
 
-void fastcall __tasklet_schedule(struct tasklet_struct *t)
+void __tasklet_schedule(struct tasklet_struct *t)
 {
 	unsigned long flags;
 
@@ -376,7 +376,7 @@ void fastcall __tasklet_schedule(struct tasklet_struct *t)
 
 EXPORT_SYMBOL(__tasklet_schedule);
 
-void fastcall __tasklet_hi_schedule(struct tasklet_struct *t)
+void __tasklet_hi_schedule(struct tasklet_struct *t)
 {
 	unsigned long flags;
 

commit 6378ddb592158db4b42197f1bc8666228800e379
Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Wed Jan 30 13:30:04 2008 +0100

    time: track accurate idle time with tick_sched.idle_sleeptime
    
    Current idle time in kstat is based on jiffies and is coarse grained.
    tick_sched.idle_sleeptime is making some attempt to keep track of idle time
    in a fine grained manner.  But, it is not handling the time spent in
    interrupts fully.
    
    Make tick_sched.idle_sleeptime accurate with respect to time spent on
    handling interrupts and also add tick_sched.idle_lastupdate, which keeps
    track of last time when idle_sleeptime was updated.
    
    This statistics will be crucial for cpufreq-ondemand governor, which can
    shed some conservative gaurd band that is uses today while setting the
    frequency.  The ondemand changes that uses the exact idle time is coming
    soon.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 8fe1ff40102d..d7837d45419e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -280,9 +280,14 @@ asmlinkage void do_softirq(void)
  */
 void irq_enter(void)
 {
+#ifdef CONFIG_NO_HZ
+	int cpu = smp_processor_id();
+	if (idle_cpu(cpu) && !in_interrupt())
+		tick_nohz_stop_idle(cpu);
+#endif
 	__irq_enter();
 #ifdef CONFIG_NO_HZ
-	if (idle_cpu(smp_processor_id()))
+	if (idle_cpu(cpu))
 		tick_nohz_update_jiffies();
 #endif
 }

commit b10db7f0d2b589a7f88dc3026e150756cb437a28
Author: Pavel Machek <pavel@ucw.cz>
Date:   Wed Jan 30 13:30:00 2008 +0100

    time: more timer related cleanups
    
    I was confused by FSEC = 10^15 NSEC statement, plus small whitespace
    fixes. When there's copyright, there should be GPL.
    
    Signed-off-by: Pavel Machek <pavel@suse.cz>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index bd89bc4eb0b9..8fe1ff40102d 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -3,7 +3,9 @@
  *
  *	Copyright (C) 1992 Linus Torvalds
  *
- * Rewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)
+ *	Distribute under GPLv2.
+ *
+ *	Rewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)
  */
 
 #include <linux/module.h>

commit 464771fe4743afd00ebff65aee0983fa1aa1da4f
Author: Adrian Bunk <bunk@kernel.org>
Date:   Wed Sep 12 15:14:45 2007 +0200

    [KERNEL]: Unexport raise_softirq_irqoff
    
    raise_softirq_irqoff no longer has any modular user.
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index dbbdcd7f3c2e..bd89bc4eb0b9 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -330,8 +330,6 @@ inline fastcall void raise_softirq_irqoff(unsigned int nr)
 		wakeup_softirqd();
 }
 
-EXPORT_SYMBOL(raise_softirq_irqoff);
-
 void fastcall raise_softirq(unsigned int nr)
 {
 	unsigned long flags;

commit c45248c70125cc374fdf264659643276c72801bf
Author: Robert Olsson <robert.olsson@its.uu.se>
Date:   Mon Sep 17 11:47:12 2007 -0700

    [SOFTIRQ]: Remove do_softirq() symbol export.
    
    As noted by Christoph Hellwig, pktgen was the only user so
    it can now be removed.
    
    [ Add missing cases caught by Adrian Bunk. -DaveM ]
    
    Signed-off-by: Robert Olsson <robert.olsson@its.uu.se>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 0f546ddea43d..dbbdcd7f3c2e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -271,8 +271,6 @@ asmlinkage void do_softirq(void)
 	local_irq_restore(flags);
 }
 
-EXPORT_SYMBOL(do_softirq);
-
 #endif
 
 /*

commit 831441862956fffa17b9801db37e6ea1650b0f69
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Tue Jul 17 04:03:35 2007 -0700

    Freezer: make kernel threads nonfreezable by default
    
    Currently, the freezer treats all tasks as freezable, except for the kernel
    threads that explicitly set the PF_NOFREEZE flag for themselves.  This
    approach is problematic, since it requires every kernel thread to either
    set PF_NOFREEZE explicitly, or call try_to_freeze(), even if it doesn't
    care for the freezing of tasks at all.
    
    It seems better to only require the kernel threads that want to or need to
    be frozen to use some freezer-related code and to remove any
    freezer-related code from the other (nonfreezable) kernel threads, which is
    done in this patch.
    
    The patch causes all kernel threads to be nonfreezable by default (ie.  to
    have PF_NOFREEZE set by default) and introduces the set_freezable()
    function that should be called by the freezable kernel threads in order to
    unset PF_NOFREEZE.  It also makes all of the currently freezable kernel
    threads call set_freezable(), so it shouldn't cause any (intentional)
    change of behaviour to appear.  Additionally, it updates documentation to
    describe the freezing of tasks more accurately.
    
    [akpm@linux-foundation.org: build fixes]
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Nigel Cunningham <nigel@nigel.suspend2.net>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 8de267790166..0f546ddea43d 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -14,6 +14,7 @@
 #include <linux/notifier.h>
 #include <linux/percpu.h>
 #include <linux/cpu.h>
+#include <linux/freezer.h>
 #include <linux/kthread.h>
 #include <linux/rcupdate.h>
 #include <linux/smp.h>
@@ -488,8 +489,6 @@ void __init softirq_init(void)
 
 static int ksoftirqd(void * __bind_cpu)
 {
-	current->flags |= PF_NOFREEZE;
-
 	set_current_state(TASK_INTERRUPTIBLE);
 
 	while (!kthread_should_stop()) {

commit 1c6b4aa94576eee6dec3b8011f60d7f666db90b0
Author: Satoru Takeuchi <takeuchi_satoru@jp.fujitsu.com>
Date:   Sun Jul 15 23:39:48 2007 -0700

    cpu hotplug: fix ksoftirqd termination on cpu hotplug with naughty realtime process
    
    Fix ksoftirqd termination on cpu hotplug with naughty real time process.
    
    Assuming the following case:
    
     - Try to hot remove CPU2 from CPU1.
     - There is a real time process on CPU2, and that process doesn't sleep at all.
     - That rt process and ksoftirqd/2 is migrated to the CPU0
    
    Then ksoftirqd/2 can't stop becasue that rt process runs everlastingly on
    CPU0, and CPU1 waiting the ksoftirqd/2's termination hangs up.  To fix this
    problem, set the priority of ksoftirqd/2 to max one before kthread_stop().
    
    [akpm@linux-foundation.org: fix warning]
    Signed-off-by: Satoru Takeuchi <takeuchi_satoru@jp.fujitsu.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ashok Raj <ashok.raj@intel.com>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 73217a9e2875..8de267790166 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -614,12 +614,16 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 		kthread_bind(per_cpu(ksoftirqd, hotcpu),
 			     any_online_cpu(cpu_online_map));
 	case CPU_DEAD:
-	case CPU_DEAD_FROZEN:
+	case CPU_DEAD_FROZEN: {
+		struct sched_param param = { .sched_priority = MAX_RT_PRIO-1 };
+
 		p = per_cpu(ksoftirqd, hotcpu);
 		per_cpu(ksoftirqd, hotcpu) = NULL;
+		sched_setscheduler(p, SCHED_FIFO, &param);
 		kthread_stop(p);
 		takeover_tasklets(hotcpu);
 		break;
+	}
 #endif /* CONFIG_HOTPLUG_CPU */
  	}
 	return NOTIFY_OK;

commit 23bdd703a585a869f2eb32fb9f66749d0476d71e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:52:00 2007 +0200

    sched: do not set softirqs to nice +19
    
    do not set softirqs to nice +19. _If_ for whatever reason
    we missed to process some high-prio softirq and woke up
    ksoftirqd, we should give it a fair chance to actually
    get some work done, even if the system is under load.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 0b9886a00e74..73217a9e2875 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -488,7 +488,6 @@ void __init softirq_init(void)
 
 static int ksoftirqd(void * __bind_cpu)
 {
-	set_user_nice(current, 19);
 	current->flags |= PF_NOFREEZE;
 
 	set_current_state(TASK_INTERRUPTIBLE);

commit 8bb7844286fb8c9fce6f65d8288aeb09d03a5e0d
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed May 9 02:35:10 2007 -0700

    Add suspend-related notifications for CPU hotplug
    
    Since nonboot CPUs are now disabled after tasks and devices have been
    frozen and the CPU hotplug infrastructure is used for this purpose, we need
    special CPU hotplug notifications that will help the CPU-hotplug-aware
    subsystems distinguish normal CPU hotplug events from CPU hotplug events
    related to a system-wide suspend or resume operation in progress.  This
    patch introduces such notifications and causes them to be used during
    suspend and resume transitions.  It also changes all of the
    CPU-hotplug-aware subsystems to take these notifications into consideration
    (for now they are handled in the same way as the corresponding "normal"
    ones).
    
    [oleg@tv-sign.ru: cleanups]
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 8b75008e2bd8..0b9886a00e74 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -593,6 +593,7 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 
 	switch (action) {
 	case CPU_UP_PREPARE:
+	case CPU_UP_PREPARE_FROZEN:
 		p = kthread_create(ksoftirqd, hcpu, "ksoftirqd/%d", hotcpu);
 		if (IS_ERR(p)) {
 			printk("ksoftirqd for %i failed\n", hotcpu);
@@ -602,16 +603,19 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
   		per_cpu(ksoftirqd, hotcpu) = p;
  		break;
 	case CPU_ONLINE:
+	case CPU_ONLINE_FROZEN:
 		wake_up_process(per_cpu(ksoftirqd, hotcpu));
 		break;
 #ifdef CONFIG_HOTPLUG_CPU
 	case CPU_UP_CANCELED:
+	case CPU_UP_CANCELED_FROZEN:
 		if (!per_cpu(ksoftirqd, hotcpu))
 			break;
 		/* Unbind so it can run.  Fall thru. */
 		kthread_bind(per_cpu(ksoftirqd, hotcpu),
 			     any_online_cpu(cpu_online_map));
 	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
 		p = per_cpu(ksoftirqd, hotcpu);
 		per_cpu(ksoftirqd, hotcpu) = NULL;
 		kthread_stop(p);

commit 79bf2bb335b85db25d27421c798595a2fa2a0e82
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 16 01:28:03 2007 -0800

    [PATCH] tick-management: dyntick / highres functionality
    
    With Ingo Molnar <mingo@elte.hu>
    
    Add functions to provide dynamic ticks and high resolution timers.  The code
    which keeps track of jiffies and handles the long idle periods is shared
    between tick based and high resolution timer based dynticks.  The dyntick
    functionality can be disabled on the kernel commandline.  Provide also the
    infrastructure to support high resolution timers.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 14e1a14f94d2..8b75008e2bd8 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -17,6 +17,7 @@
 #include <linux/kthread.h>
 #include <linux/rcupdate.h>
 #include <linux/smp.h>
+#include <linux/tick.h>
 
 #include <asm/irq.h>
 /*
@@ -278,9 +279,11 @@ EXPORT_SYMBOL(do_softirq);
  */
 void irq_enter(void)
 {
-	account_system_vtime(current);
-	add_preempt_count(HARDIRQ_OFFSET);
-	trace_hardirq_enter();
+	__irq_enter();
+#ifdef CONFIG_NO_HZ
+	if (idle_cpu(smp_processor_id()))
+		tick_nohz_update_jiffies();
+#endif
 }
 
 #ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED
@@ -299,6 +302,12 @@ void irq_exit(void)
 	sub_preempt_count(IRQ_EXIT_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();
+
+#ifdef CONFIG_NO_HZ
+	/* Make sure that timer wheel updates are propagated */
+	if (!in_interrupt() && idle_cpu(smp_processor_id()) && !need_resched())
+		tick_nohz_stop_sched_tick();
+#endif
 	preempt_enable_no_resched();
 }
 

commit dde4b2b5f4ed275250488dabdaf282d9c6e7e2b8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 16 01:27:45 2007 -0800

    [PATCH] uninline irq_enter()
    
    Uninline irq_enter().  [dynticks adds more stuff to it]
    
    No functional changes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 918e52df090e..14e1a14f94d2 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -273,6 +273,16 @@ EXPORT_SYMBOL(do_softirq);
 
 #endif
 
+/*
+ * Enter an interrupt context.
+ */
+void irq_enter(void)
+{
+	account_system_vtime(current);
+	add_preempt_count(HARDIRQ_OFFSET);
+	trace_hardirq_enter();
+}
+
 #ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED
 # define invoke_softirq()	__do_softirq()
 #else

commit 3908fd2ed920af818aa596672da68ba26173ff27
Author: Zachary Amsden <zach@vmware.com>
Date:   Wed Dec 6 20:39:39 2006 -0800

    [PATCH] softirq: remove BUG_ONs which can incorrectly trigger
    
    It is possible to have tasklets get scheduled before softirqd has had a chance
    to spawn on all CPUs.  This is totally harmless; after success during action
    CPU_UP_PREPARE, action CPU_ONLINE will be called, which immediately wakes
    softirqd on the appropriate CPU to process the already pending tasklets.  So
    there is no danger of having a missed wakeup for any tasklets that were
    already pending.
    
    In particular, i386 is affected by this during startup, and is visible when
    using a very large initrd; during the time it takes for the initrd to be
    decompressed, a timer IRQ can come in and schedule RCU callbacks.  It is also
    possible that resending of a hardware IRQ via a softirq triggers the same bug.
    
    Because of different timing conditions, this shows up in all emulators and
    virtual machines tested, including Xen, VMware, Virtual PC, and Qemu.  It is
    also possible to trigger on native hardware with a large enough initrd,
    although I don't have a reliable case demonstrating that.
    
    Signed-off-by: Zachary Amsden <zach@vmware.com>
    Cc: <caglar@pardus.org.tr>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index bf25015dce16..918e52df090e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -574,8 +574,6 @@ static int __cpuinit cpu_callback(struct notifier_block *nfb,
 
 	switch (action) {
 	case CPU_UP_PREPARE:
-		BUG_ON(per_cpu(tasklet_vec, hotcpu).list);
-		BUG_ON(per_cpu(tasklet_hi_vec, hotcpu).list);
 		p = kthread_create(ksoftirqd, hcpu, "ksoftirqd/%d", hotcpu);
 		if (IS_ERR(p)) {
 			printk("ksoftirqd for %i failed\n", hotcpu);

commit 07dccf3344010f9b9df7fe725da7e73bca2992df
Author: Akinobu Mita <mita@miraclelinux.com>
Date:   Fri Sep 29 02:00:22 2006 -0700

    [PATCH] check return value of cpu_callback
    
    Spawing ksoftirqd, migration, or watchdog, and calling init_timers_cpu()
    may fail with small memory.  If it happens in initcalls, kernel NULL
    pointer dereference happens later.  This patch makes crash happen
    immediately in such cases.  It seems a bit better than getting kernel NULL
    pointer dereference later.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Akinobu Mita <mita@miraclelinux.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 3789ca98197c..bf25015dce16 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -612,7 +612,9 @@ static struct notifier_block __cpuinitdata cpu_nfb = {
 __init int spawn_ksoftirqd(void)
 {
 	void *cpu = (void *)(long)smp_processor_id();
-	cpu_callback(&cpu_nfb, CPU_UP_PREPARE, cpu);
+	int err = cpu_callback(&cpu_nfb, CPU_UP_PREPARE, cpu);
+
+	BUG_ON(err == NOTIFY_BAD);
 	cpu_callback(&cpu_nfb, CPU_ONLINE, cpu);
 	register_cpu_notifier(&cpu_nfb);
 	return 0;

commit 3c829c367a1a52550378584a657768217971e587
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Sun Jul 30 03:04:02 2006 -0700

    [PATCH] Reducing local_bh_enable/disable overhead in irqtrace
    
    The recent changes from irqtrace feature has added overheads to
    local_bh_disable and local_bh_enable that reduces UDP performance across
    x86_64 and IA64, even though IA64 does not support the irqtrace feature.
    Patch in question is
    
    [PATCH]lockdep: irqtrace subsystem, core
    http://www.kernel.org/git/?p=linux/kernel/git/torvalds/linux-2.6.git;a=c
    ommit;h=de30a2b355ea85350ca2f58f3b9bf4e5bc007986
    
    Prior to this patch, local_bh_disable was a short macro.  Now it is a
    function which calls __local_bh_disable with added irq flags save and
    restore.  The irq flags save and restore were also added to
    local_bh_enable, probably for injecting the trace irqs code.
    
    This overhead is on the generic code path across all architectures.  On a
    IA_64 test machine (Itanium-2 1.6 GHz) running a benchmark like netperf's
    UDP streaming test, the added overhead results in a drop of 3% in
    throughput, as udp_sendmsg calls the local_bh_enable/disable several times.
    
    Other workloads that have heavy usages of local_bh_enable/disable could
    also be affected.  The patch ideally should not have affected IA-64
    performance as it does not have IRQ tracing support.  A significant portion
    of the overhead is in the added irq flags save and restore, which I think
    is not needed if IRQ tracing is unused.  A suggested patch is attached
    below that recovers the lost performance.  However, the "ifdef"s in the
    patch are a bit ugly.
    
    Signed-off-by: Tim Chen <tim.c.chen@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index aab880677ce0..3789ca98197c 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -65,6 +65,7 @@ static inline void wakeup_softirqd(void)
  * This one is for softirq.c-internal use,
  * where hardirqs are disabled legitimately:
  */
+#ifdef CONFIG_TRACE_IRQFLAGS
 static void __local_bh_disable(unsigned long ip)
 {
 	unsigned long flags;
@@ -80,6 +81,13 @@ static void __local_bh_disable(unsigned long ip)
 		trace_softirqs_off(ip);
 	raw_local_irq_restore(flags);
 }
+#else /* !CONFIG_TRACE_IRQFLAGS */
+static inline void __local_bh_disable(unsigned long ip)
+{
+	add_preempt_count(SOFTIRQ_OFFSET);
+	barrier();
+}
+#endif /* CONFIG_TRACE_IRQFLAGS */
 
 void local_bh_disable(void)
 {
@@ -121,12 +129,16 @@ EXPORT_SYMBOL(_local_bh_enable);
 
 void local_bh_enable(void)
 {
+#ifdef CONFIG_TRACE_IRQFLAGS
 	unsigned long flags;
 
 	WARN_ON_ONCE(in_irq());
+#endif
 	WARN_ON_ONCE(irqs_disabled());
 
+#ifdef CONFIG_TRACE_IRQFLAGS
 	local_irq_save(flags);
+#endif
 	/*
 	 * Are softirqs going to be turned on now:
 	 */
@@ -142,18 +154,22 @@ void local_bh_enable(void)
 		do_softirq();
 
 	dec_preempt_count();
+#ifdef CONFIG_TRACE_IRQFLAGS
 	local_irq_restore(flags);
+#endif
 	preempt_check_resched();
 }
 EXPORT_SYMBOL(local_bh_enable);
 
 void local_bh_enable_ip(unsigned long ip)
 {
+#ifdef CONFIG_TRACE_IRQFLAGS
 	unsigned long flags;
 
 	WARN_ON_ONCE(in_irq());
 
 	local_irq_save(flags);
+#endif
 	/*
 	 * Are softirqs going to be turned on now:
 	 */
@@ -169,7 +185,9 @@ void local_bh_enable_ip(unsigned long ip)
 		do_softirq();
 
 	dec_preempt_count();
+#ifdef CONFIG_TRACE_IRQFLAGS
 	local_irq_restore(flags);
+#endif
 	preempt_check_resched();
 }
 EXPORT_SYMBOL(local_bh_enable_ip);

commit 8c78f3075dab4be279e283f901f00e33ce44890a
Author: Chandra Seetharaman <sekharan@us.ibm.com>
Date:   Sun Jul 30 03:03:35 2006 -0700

    [PATCH] cpu hotplug: replace __devinit* with __cpuinit* for cpu notifications
    
    Few of the callback functions and notifier blocks that are associated with cpu
    notifications incorrectly have __devinit and __devinitdata.  They should be
    __cpuinit and __cpuinitdata instead.
    
    It makes no functional difference but wastes text area when CONFIG_HOTPLUG is
    enabled and CONFIG_HOTPLUG_CPU is not.
    
    This patch fixes all those instances.
    
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Cc: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 0f08a84ae307..aab880677ce0 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -547,7 +547,7 @@ static void takeover_tasklets(unsigned int cpu)
 }
 #endif /* CONFIG_HOTPLUG_CPU */
 
-static int __devinit cpu_callback(struct notifier_block *nfb,
+static int __cpuinit cpu_callback(struct notifier_block *nfb,
 				  unsigned long action,
 				  void *hcpu)
 {
@@ -587,7 +587,7 @@ static int __devinit cpu_callback(struct notifier_block *nfb,
 	return NOTIFY_OK;
 }
 
-static struct notifier_block __devinitdata cpu_nfb = {
+static struct notifier_block __cpuinitdata cpu_nfb = {
 	.notifier_call = cpu_callback
 };
 

commit 6bc02d8412b422388f86b09ae40d762c0bc05290
Author: Adrian Bunk <bunk@stusta.de>
Date:   Fri Jul 14 00:24:15 2006 -0700

    [PATCH] unexport open_softirq
    
    Christoph Hellwig:
    open_softirq just enables a softirq.  The softirq array is statically
    allocated so to add a new one you would have to patch the kernel.  So
    there's no point to keep this export at all as any user would have to
    patch the enum in include/linux/interrupt.h anyway.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index fd12f2556f0d..0f08a84ae307 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -311,8 +311,6 @@ void open_softirq(int nr, void (*action)(struct softirq_action*), void *data)
 	softirq_vec[nr].action = action;
 }
 
-EXPORT_UNUSED_SYMBOL(open_softirq);  /*  June 2006  */
-
 /* Tasklets */
 struct tasklet_head
 {

commit 80d6679a62fe45f440d042099d997a42e4e8c59d
Author: Adrian Bunk <bunk@stusta.de>
Date:   Mon Jul 10 04:44:24 2006 -0700

    [PATCH] kernel/softirq.c: EXPORT_UNUSED_SYMBOL
    
    This patch marks an unused export as EXPORT_UNUSED_SYMBOL.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 215541e26c1a..fd12f2556f0d 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -311,7 +311,7 @@ void open_softirq(int nr, void (*action)(struct softirq_action*), void *data)
 	softirq_vec[nr].action = action;
 }
 
-EXPORT_SYMBOL(open_softirq);
+EXPORT_UNUSED_SYMBOL(open_softirq);  /*  June 2006  */
 
 /* Tasklets */
 struct tasklet_head

commit 829035fd709119d9def124a6d40b94d317573e6f
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jul 3 00:25:40 2006 -0700

    [PATCH] lockdep: irqtrace subsystem, move account_system_vtime() calls into kernel/softirq.c
    
    At the moment, powerpc and s390 have their own versions of do_softirq which
    include local_bh_disable() and __local_bh_enable() calls.  They end up
    calling __do_softirq (in kernel/softirq.c) which also does
    local_bh_disable/enable.
    
    Apparently the two levels of disable/enable trigger a warning from some
    validation code that Ingo is working on, and he would like to see the outer
    level removed.  But to do that, we have to move the account_system_vtime
    calls that are currently in the arch do_softirq() implementations for
    powerpc and s390 into the generic __do_softirq() (this is a no-op for other
    archs because account_system_vtime is defined to be an empty inline
    function on all other archs).  This patch does that.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 584609b6a66e..215541e26c1a 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -193,6 +193,8 @@ asmlinkage void __do_softirq(void)
 	int cpu;
 
 	pending = local_softirq_pending();
+	account_system_vtime(current);
+
 	__local_bh_disable((unsigned long)__builtin_return_address(0));
 	trace_softirq_enter();
 
@@ -224,6 +226,8 @@ asmlinkage void __do_softirq(void)
 		wakeup_softirqd();
 
 	trace_softirq_exit();
+
+	account_system_vtime(current);
 	_local_bh_enable();
 }
 

commit de30a2b355ea85350ca2f58f3b9bf4e5bc007986
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:42 2006 -0700

    [PATCH] lockdep: irqtrace subsystem, core
    
    Accurate hard-IRQ-flags and softirq-flags state tracing.
    
    This allows us to attach extra functionality to IRQ flags on/off
    events (such as trace-on/off).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 8f03e3b89b55..584609b6a66e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -61,6 +61,119 @@ static inline void wakeup_softirqd(void)
 		wake_up_process(tsk);
 }
 
+/*
+ * This one is for softirq.c-internal use,
+ * where hardirqs are disabled legitimately:
+ */
+static void __local_bh_disable(unsigned long ip)
+{
+	unsigned long flags;
+
+	WARN_ON_ONCE(in_irq());
+
+	raw_local_irq_save(flags);
+	add_preempt_count(SOFTIRQ_OFFSET);
+	/*
+	 * Were softirqs turned off above:
+	 */
+	if (softirq_count() == SOFTIRQ_OFFSET)
+		trace_softirqs_off(ip);
+	raw_local_irq_restore(flags);
+}
+
+void local_bh_disable(void)
+{
+	__local_bh_disable((unsigned long)__builtin_return_address(0));
+}
+
+EXPORT_SYMBOL(local_bh_disable);
+
+void __local_bh_enable(void)
+{
+	WARN_ON_ONCE(in_irq());
+
+	/*
+	 * softirqs should never be enabled by __local_bh_enable(),
+	 * it always nests inside local_bh_enable() sections:
+	 */
+	WARN_ON_ONCE(softirq_count() == SOFTIRQ_OFFSET);
+
+	sub_preempt_count(SOFTIRQ_OFFSET);
+}
+EXPORT_SYMBOL_GPL(__local_bh_enable);
+
+/*
+ * Special-case - softirqs can safely be enabled in
+ * cond_resched_softirq(), or by __do_softirq(),
+ * without processing still-pending softirqs:
+ */
+void _local_bh_enable(void)
+{
+	WARN_ON_ONCE(in_irq());
+	WARN_ON_ONCE(!irqs_disabled());
+
+	if (softirq_count() == SOFTIRQ_OFFSET)
+		trace_softirqs_on((unsigned long)__builtin_return_address(0));
+	sub_preempt_count(SOFTIRQ_OFFSET);
+}
+
+EXPORT_SYMBOL(_local_bh_enable);
+
+void local_bh_enable(void)
+{
+	unsigned long flags;
+
+	WARN_ON_ONCE(in_irq());
+	WARN_ON_ONCE(irqs_disabled());
+
+	local_irq_save(flags);
+	/*
+	 * Are softirqs going to be turned on now:
+	 */
+	if (softirq_count() == SOFTIRQ_OFFSET)
+		trace_softirqs_on((unsigned long)__builtin_return_address(0));
+	/*
+	 * Keep preemption disabled until we are done with
+	 * softirq processing:
+ 	 */
+ 	sub_preempt_count(SOFTIRQ_OFFSET - 1);
+
+	if (unlikely(!in_interrupt() && local_softirq_pending()))
+		do_softirq();
+
+	dec_preempt_count();
+	local_irq_restore(flags);
+	preempt_check_resched();
+}
+EXPORT_SYMBOL(local_bh_enable);
+
+void local_bh_enable_ip(unsigned long ip)
+{
+	unsigned long flags;
+
+	WARN_ON_ONCE(in_irq());
+
+	local_irq_save(flags);
+	/*
+	 * Are softirqs going to be turned on now:
+	 */
+	if (softirq_count() == SOFTIRQ_OFFSET)
+		trace_softirqs_on(ip);
+	/*
+	 * Keep preemption disabled until we are done with
+	 * softirq processing:
+ 	 */
+ 	sub_preempt_count(SOFTIRQ_OFFSET - 1);
+
+	if (unlikely(!in_interrupt() && local_softirq_pending()))
+		do_softirq();
+
+	dec_preempt_count();
+	local_irq_restore(flags);
+	preempt_check_resched();
+}
+EXPORT_SYMBOL(local_bh_enable_ip);
+
 /*
  * We restart softirq processing MAX_SOFTIRQ_RESTART times,
  * and we fall back to softirqd after that.
@@ -80,8 +193,9 @@ asmlinkage void __do_softirq(void)
 	int cpu;
 
 	pending = local_softirq_pending();
+	__local_bh_disable((unsigned long)__builtin_return_address(0));
+	trace_softirq_enter();
 
-	local_bh_disable();
 	cpu = smp_processor_id();
 restart:
 	/* Reset the pending bitmask before enabling irqs */
@@ -109,7 +223,8 @@ asmlinkage void __do_softirq(void)
 	if (pending)
 		wakeup_softirqd();
 
-	__local_bh_enable();
+	trace_softirq_exit();
+	_local_bh_enable();
 }
 
 #ifndef __ARCH_HAS_DO_SOFTIRQ
@@ -136,23 +251,6 @@ EXPORT_SYMBOL(do_softirq);
 
 #endif
 
-void local_bh_enable(void)
-{
-	WARN_ON(irqs_disabled());
-	/*
-	 * Keep preemption disabled until we are done with
-	 * softirq processing:
- 	 */
- 	sub_preempt_count(SOFTIRQ_OFFSET - 1);
-
-	if (unlikely(!in_interrupt() && local_softirq_pending()))
-		do_softirq();
-
-	dec_preempt_count();
-	preempt_check_resched();
-}
-EXPORT_SYMBOL(local_bh_enable);
-
 #ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED
 # define invoke_softirq()	__do_softirq()
 #else
@@ -165,6 +263,7 @@ EXPORT_SYMBOL(local_bh_enable);
 void irq_exit(void)
 {
 	account_system_vtime(current);
+	trace_hardirq_exit();
 	sub_preempt_count(IRQ_EXIT_OFFSET);
 	if (!in_interrupt() && local_softirq_pending())
 		invoke_softirq();

commit 054cc8a2d808822dadf488a61729e3e550f114c4
Author: Chandra Seetharaman <sekharan@us.ibm.com>
Date:   Tue Jun 27 02:54:07 2006 -0700

    [PATCH] cpu hotplug: revert initdata patch submitted for 2.6.17
    
    This patch reverts notifier_block changes made in 2.6.17
    
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Cc: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index db65a311f14e..8f03e3b89b55 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -486,7 +486,7 @@ static int __devinit cpu_callback(struct notifier_block *nfb,
 	return NOTIFY_OK;
 }
 
-static struct notifier_block cpu_nfb = {
+static struct notifier_block __devinitdata cpu_nfb = {
 	.notifier_call = cpu_callback
 };
 

commit 9c7b216d23e820e0e148d5be01bbb5bd2d8378fe
Author: Chandra Seetharaman <sekharan@us.ibm.com>
Date:   Tue Jun 27 02:54:07 2006 -0700

    [PATCH] cpu hotplug: revert init patch submitted for 2.6.17
    
    In 2.6.17, there was a problem with cpu_notifiers and XFS.  I provided a
    band-aid solution to solve that problem.  In the process, i undid all the
    changes you both were making to ensure that these notifiers were available
    only at init time (unless CONFIG_HOTPLUG_CPU is defined).
    
    We deferred the real fix to 2.6.18.  Here is a set of patches that fixes the
    XFS problem cleanly and makes the cpu notifiers available only at init time
    (unless CONFIG_HOTPLUG_CPU is defined).
    
    If CONFIG_HOTPLUG_CPU is defined then cpu notifiers are available at run
    time.
    
    This patch reverts the notifier_call changes made in 2.6.17
    
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Cc: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 9e2f1c6e73d7..db65a311f14e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -446,7 +446,7 @@ static void takeover_tasklets(unsigned int cpu)
 }
 #endif /* CONFIG_HOTPLUG_CPU */
 
-static int cpu_callback(struct notifier_block *nfb,
+static int __devinit cpu_callback(struct notifier_block *nfb,
 				  unsigned long action,
 				  void *hcpu)
 {

commit fc75cdfa5b43ac4d3232b490800cd35063adafd3
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Sun Jun 25 05:49:10 2006 -0700

    [PATCH] cpu hotplug: fix CPU_UP_CANCEL handling
    
    If a cpu hotplug callback fails on CPU_UP_PREPARE, all callbacks will be
    called with CPU_UP_CANCELED.  A few of these callbacks assume that on
    CPU_UP_PREPARE a pointer to task has been stored in a percpu array.  This
    assumption is not true if CPU_UP_PREPARE fails and the following calls to
    kthread_bind() in CPU_UP_CANCELED will cause an addressing exception
    because of passing a NULL pointer.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 336f92d64e2e..9e2f1c6e73d7 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -470,6 +470,8 @@ static int cpu_callback(struct notifier_block *nfb,
 		break;
 #ifdef CONFIG_HOTPLUG_CPU
 	case CPU_UP_CANCELED:
+		if (!per_cpu(ksoftirqd, hotcpu))
+			break;
 		/* Unbind so it can run.  Fall thru. */
 		kthread_bind(per_cpu(ksoftirqd, hotcpu),
 			     any_online_cpu(cpu_online_map));

commit 83d722f7e198b034699b1500d98729beff930efd
Author: Chandra Seetharaman <sekharan@us.ibm.com>
Date:   Mon Apr 24 19:35:21 2006 -0700

    [PATCH] Remove __devinit and __cpuinit from notifier_call definitions
    
    Few of the notifier_chain_register() callers use __init in the definition
    of notifier_call.  It is incorrect as the function definition should be
    available after the initializations (they do not unregister them during
    initializations).
    
    This patch fixes all such usages to _not_ have the notifier_call __init
    section.
    
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index a13f2b342e4b..336f92d64e2e 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -446,7 +446,7 @@ static void takeover_tasklets(unsigned int cpu)
 }
 #endif /* CONFIG_HOTPLUG_CPU */
 
-static int __devinit cpu_callback(struct notifier_block *nfb,
+static int cpu_callback(struct notifier_block *nfb,
 				  unsigned long action,
 				  void *hcpu)
 {

commit 649bbaa484bcdce94f40a1b97a6a2ded0549e8a2
Author: Chandra Seetharaman <sekharan@us.ibm.com>
Date:   Mon Apr 24 19:35:15 2006 -0700

    [PATCH] Remove __devinitdata from notifier block definitions
    
    Few of the notifier_chain_register() callers use __devinitdata in the
    definition of notifier_block data structure.  It is incorrect as the
    data structure should be available after the initializations (they do
    not unregister them during initializations).
    
    This was leading to an oops when notifier_chain_register() call is
    invoked for those callback chains after initialization.
    
    This patch fixes all such usages to _not_ have the notifier_block data
    structure in the init data section.
    
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index ec8fed42a86f..a13f2b342e4b 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -484,7 +484,7 @@ static int __devinit cpu_callback(struct notifier_block *nfb,
 	return NOTIFY_OK;
 }
 
-static struct notifier_block __devinitdata cpu_nfb = {
+static struct notifier_block cpu_nfb = {
 	.notifier_call = cpu_callback
 };
 

commit 78eef01b0fae087c5fadbd85dd4fe2918c3a015f
Author: Andrew Morton <akpm@osdl.org>
Date:   Wed Mar 22 00:08:16 2006 -0800

    [PATCH] on_each_cpu(): disable local interrupts
    
    When on_each_cpu() runs the callback on other CPUs, it runs with local
    interrupts disabled.  So we should run the function with local interrupts
    disabled on this CPU, too.
    
    And do the same for UP, so the callback is run in the same environment on both
    UP and SMP.  (strictly it should do preempt_disable() too, but I think
    local_irq_disable is sufficiently equivalent).
    
    Also uninlines on_each_cpu().  softirq.c was the most appropriate file I could
    find, but it doesn't seem to justify creating a new file.
    
    Oh, and fix up that comment over (under?) x86's smp_call_function().  It
    drives me nuts.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index ad3295cdded5..ec8fed42a86f 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -16,6 +16,7 @@
 #include <linux/cpu.h>
 #include <linux/kthread.h>
 #include <linux/rcupdate.h>
+#include <linux/smp.h>
 
 #include <asm/irq.h>
 /*
@@ -495,3 +496,22 @@ __init int spawn_ksoftirqd(void)
 	register_cpu_notifier(&cpu_nfb);
 	return 0;
 }
+
+#ifdef CONFIG_SMP
+/*
+ * Call a function on all processors
+ */
+int on_each_cpu(void (*func) (void *info), void *info, int retry, int wait)
+{
+	int ret = 0;
+
+	preempt_disable();
+	ret = smp_call_function(func, info, retry, wait);
+	local_irq_disable();
+	func(info);
+	local_irq_enable();
+	preempt_enable();
+	return ret;
+}
+EXPORT_SYMBOL(on_each_cpu);
+#endif

commit a4c4af7c8dc1eccdfb8c57e1684f08179b4407e6
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Nov 7 00:58:38 2005 -0800

    [PATCH] cpu hoptlug: avoid usage of smp_processor_id() in preemptible code
    
    Replace smp_processor_id() with any_online_cpu(cpu_online_map) in order to
    avoid lots of "BUG: using smp_processor_id() in preemptible [00000001]
    code:..." messages in case taking a cpu online fails.
    
    All the traces start at the last notifier_call_chain(...) in kernel/cpu.c.
    Since we hold the cpu_control semaphore it shouldn't be any problem to access
    cpu_online_map.
    
    The reason why cpu_up failed is simply that the cpu that was supposed to be
    taken online wasn't even there.  That is because on s390 we never know when a
    new cpu comes and therefore cpu_possible_map consists of only ones and doesn't
    reflect reality.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index f766b2fc48be..ad3295cdded5 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -470,7 +470,8 @@ static int __devinit cpu_callback(struct notifier_block *nfb,
 #ifdef CONFIG_HOTPLUG_CPU
 	case CPU_UP_CANCELED:
 		/* Unbind so it can run.  Fall thru. */
-		kthread_bind(per_cpu(ksoftirqd, hotcpu), smp_processor_id());
+		kthread_bind(per_cpu(ksoftirqd, hotcpu),
+			     any_online_cpu(cpu_online_map));
 	case CPU_DEAD:
 		p = per_cpu(ksoftirqd, hotcpu);
 		per_cpu(ksoftirqd, hotcpu) = NULL;

commit 3f74478b5fd7263e9311cdb320923d599c73a792
Author: Andi Kleen <ak@suse.de>
Date:   Mon Sep 12 18:49:24 2005 +0200

    [PATCH] x86-64: Some cleanup and optimization to the processor data area.
    
    - Remove unused irqrsp field
    - Remove pda->me
    - Optimize set_softirq_pending slightly
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b4ab6af1dea8..f766b2fc48be 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -84,7 +84,7 @@ asmlinkage void __do_softirq(void)
 	cpu = smp_processor_id();
 restart:
 	/* Reset the pending bitmask before enabling irqs */
-	local_softirq_pending() = 0;
+	set_softirq_pending(0);
 
 	local_irq_enable();
 

commit c70f5d6610c601ea2ae4ae4e49f66c80801e895f
Author: Andrew Morton <akpm@osdl.org>
Date:   Sat Jul 30 10:22:49 2005 -0700

    [PATCH] revert bogus softirq changes
    
    This snuck in with an x86_64 change.  Thanks to Richard Purdie
    <rpurdie@rpsys.net> for spotting it.
    
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index 31007d6542cc..b4ab6af1dea8 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -86,7 +86,7 @@ asmlinkage void __do_softirq(void)
 	/* Reset the pending bitmask before enabling irqs */
 	local_softirq_pending() = 0;
 
-	//local_irq_enable();
+	local_irq_enable();
 
 	h = softirq_vec;
 
@@ -99,7 +99,7 @@ asmlinkage void __do_softirq(void)
 		pending >>= 1;
 	} while (pending);
 
-	//local_irq_disable();
+	local_irq_disable();
 
 	pending = local_softirq_pending();
 	if (pending && --max_restart)

commit ed6b676ca8b50e0b538e61c283d52fd04f007abf
Author: Andi Kleen <ak@suse.de>
Date:   Thu Jul 28 21:15:49 2005 -0700

    [PATCH] x86_64: Switch to the interrupt stack when running a softirq in local_bh_enable()
    
    This avoids some potential stack overflows with very deep softirq callchains.
    i386 does this too.
    
    TOADD CFI annotation
    
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/softirq.c b/kernel/softirq.c
index b4ab6af1dea8..31007d6542cc 100644
--- a/kernel/softirq.c
+++ b/kernel/softirq.c
@@ -86,7 +86,7 @@ asmlinkage void __do_softirq(void)
 	/* Reset the pending bitmask before enabling irqs */
 	local_softirq_pending() = 0;
 
-	local_irq_enable();
+	//local_irq_enable();
 
 	h = softirq_vec;
 
@@ -99,7 +99,7 @@ asmlinkage void __do_softirq(void)
 		pending >>= 1;
 	} while (pending);
 
-	local_irq_disable();
+	//local_irq_disable();
 
 	pending = local_softirq_pending();
 	if (pending && --max_restart)

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/kernel/softirq.c b/kernel/softirq.c
new file mode 100644
index 000000000000..b4ab6af1dea8
--- /dev/null
+++ b/kernel/softirq.c
@@ -0,0 +1,496 @@
+/*
+ *	linux/kernel/softirq.c
+ *
+ *	Copyright (C) 1992 Linus Torvalds
+ *
+ * Rewritten. Old one was good in 2.2, but in 2.3 it was immoral. --ANK (990903)
+ */
+
+#include <linux/module.h>
+#include <linux/kernel_stat.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/mm.h>
+#include <linux/notifier.h>
+#include <linux/percpu.h>
+#include <linux/cpu.h>
+#include <linux/kthread.h>
+#include <linux/rcupdate.h>
+
+#include <asm/irq.h>
+/*
+   - No shared variables, all the data are CPU local.
+   - If a softirq needs serialization, let it serialize itself
+     by its own spinlocks.
+   - Even if softirq is serialized, only local cpu is marked for
+     execution. Hence, we get something sort of weak cpu binding.
+     Though it is still not clear, will it result in better locality
+     or will not.
+
+   Examples:
+   - NET RX softirq. It is multithreaded and does not require
+     any global serialization.
+   - NET TX softirq. It kicks software netdevice queues, hence
+     it is logically serialized per device, but this serialization
+     is invisible to common code.
+   - Tasklets: serialized wrt itself.
+ */
+
+#ifndef __ARCH_IRQ_STAT
+irq_cpustat_t irq_stat[NR_CPUS] ____cacheline_aligned;
+EXPORT_SYMBOL(irq_stat);
+#endif
+
+static struct softirq_action softirq_vec[32] __cacheline_aligned_in_smp;
+
+static DEFINE_PER_CPU(struct task_struct *, ksoftirqd);
+
+/*
+ * we cannot loop indefinitely here to avoid userspace starvation,
+ * but we also don't want to introduce a worst case 1/HZ latency
+ * to the pending events, so lets the scheduler to balance
+ * the softirq load for us.
+ */
+static inline void wakeup_softirqd(void)
+{
+	/* Interrupts are disabled: no need to stop preemption */
+	struct task_struct *tsk = __get_cpu_var(ksoftirqd);
+
+	if (tsk && tsk->state != TASK_RUNNING)
+		wake_up_process(tsk);
+}
+
+/*
+ * We restart softirq processing MAX_SOFTIRQ_RESTART times,
+ * and we fall back to softirqd after that.
+ *
+ * This number has been established via experimentation.
+ * The two things to balance is latency against fairness -
+ * we want to handle softirqs as soon as possible, but they
+ * should not be able to lock up the box.
+ */
+#define MAX_SOFTIRQ_RESTART 10
+
+asmlinkage void __do_softirq(void)
+{
+	struct softirq_action *h;
+	__u32 pending;
+	int max_restart = MAX_SOFTIRQ_RESTART;
+	int cpu;
+
+	pending = local_softirq_pending();
+
+	local_bh_disable();
+	cpu = smp_processor_id();
+restart:
+	/* Reset the pending bitmask before enabling irqs */
+	local_softirq_pending() = 0;
+
+	local_irq_enable();
+
+	h = softirq_vec;
+
+	do {
+		if (pending & 1) {
+			h->action(h);
+			rcu_bh_qsctr_inc(cpu);
+		}
+		h++;
+		pending >>= 1;
+	} while (pending);
+
+	local_irq_disable();
+
+	pending = local_softirq_pending();
+	if (pending && --max_restart)
+		goto restart;
+
+	if (pending)
+		wakeup_softirqd();
+
+	__local_bh_enable();
+}
+
+#ifndef __ARCH_HAS_DO_SOFTIRQ
+
+asmlinkage void do_softirq(void)
+{
+	__u32 pending;
+	unsigned long flags;
+
+	if (in_interrupt())
+		return;
+
+	local_irq_save(flags);
+
+	pending = local_softirq_pending();
+
+	if (pending)
+		__do_softirq();
+
+	local_irq_restore(flags);
+}
+
+EXPORT_SYMBOL(do_softirq);
+
+#endif
+
+void local_bh_enable(void)
+{
+	WARN_ON(irqs_disabled());
+	/*
+	 * Keep preemption disabled until we are done with
+	 * softirq processing:
+ 	 */
+ 	sub_preempt_count(SOFTIRQ_OFFSET - 1);
+
+	if (unlikely(!in_interrupt() && local_softirq_pending()))
+		do_softirq();
+
+	dec_preempt_count();
+	preempt_check_resched();
+}
+EXPORT_SYMBOL(local_bh_enable);
+
+#ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED
+# define invoke_softirq()	__do_softirq()
+#else
+# define invoke_softirq()	do_softirq()
+#endif
+
+/*
+ * Exit an interrupt context. Process softirqs if needed and possible:
+ */
+void irq_exit(void)
+{
+	account_system_vtime(current);
+	sub_preempt_count(IRQ_EXIT_OFFSET);
+	if (!in_interrupt() && local_softirq_pending())
+		invoke_softirq();
+	preempt_enable_no_resched();
+}
+
+/*
+ * This function must run with irqs disabled!
+ */
+inline fastcall void raise_softirq_irqoff(unsigned int nr)
+{
+	__raise_softirq_irqoff(nr);
+
+	/*
+	 * If we're in an interrupt or softirq, we're done
+	 * (this also catches softirq-disabled code). We will
+	 * actually run the softirq once we return from
+	 * the irq or softirq.
+	 *
+	 * Otherwise we wake up ksoftirqd to make sure we
+	 * schedule the softirq soon.
+	 */
+	if (!in_interrupt())
+		wakeup_softirqd();
+}
+
+EXPORT_SYMBOL(raise_softirq_irqoff);
+
+void fastcall raise_softirq(unsigned int nr)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	raise_softirq_irqoff(nr);
+	local_irq_restore(flags);
+}
+
+void open_softirq(int nr, void (*action)(struct softirq_action*), void *data)
+{
+	softirq_vec[nr].data = data;
+	softirq_vec[nr].action = action;
+}
+
+EXPORT_SYMBOL(open_softirq);
+
+/* Tasklets */
+struct tasklet_head
+{
+	struct tasklet_struct *list;
+};
+
+/* Some compilers disobey section attribute on statics when not
+   initialized -- RR */
+static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec) = { NULL };
+static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec) = { NULL };
+
+void fastcall __tasklet_schedule(struct tasklet_struct *t)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	t->next = __get_cpu_var(tasklet_vec).list;
+	__get_cpu_var(tasklet_vec).list = t;
+	raise_softirq_irqoff(TASKLET_SOFTIRQ);
+	local_irq_restore(flags);
+}
+
+EXPORT_SYMBOL(__tasklet_schedule);
+
+void fastcall __tasklet_hi_schedule(struct tasklet_struct *t)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	t->next = __get_cpu_var(tasklet_hi_vec).list;
+	__get_cpu_var(tasklet_hi_vec).list = t;
+	raise_softirq_irqoff(HI_SOFTIRQ);
+	local_irq_restore(flags);
+}
+
+EXPORT_SYMBOL(__tasklet_hi_schedule);
+
+static void tasklet_action(struct softirq_action *a)
+{
+	struct tasklet_struct *list;
+
+	local_irq_disable();
+	list = __get_cpu_var(tasklet_vec).list;
+	__get_cpu_var(tasklet_vec).list = NULL;
+	local_irq_enable();
+
+	while (list) {
+		struct tasklet_struct *t = list;
+
+		list = list->next;
+
+		if (tasklet_trylock(t)) {
+			if (!atomic_read(&t->count)) {
+				if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
+					BUG();
+				t->func(t->data);
+				tasklet_unlock(t);
+				continue;
+			}
+			tasklet_unlock(t);
+		}
+
+		local_irq_disable();
+		t->next = __get_cpu_var(tasklet_vec).list;
+		__get_cpu_var(tasklet_vec).list = t;
+		__raise_softirq_irqoff(TASKLET_SOFTIRQ);
+		local_irq_enable();
+	}
+}
+
+static void tasklet_hi_action(struct softirq_action *a)
+{
+	struct tasklet_struct *list;
+
+	local_irq_disable();
+	list = __get_cpu_var(tasklet_hi_vec).list;
+	__get_cpu_var(tasklet_hi_vec).list = NULL;
+	local_irq_enable();
+
+	while (list) {
+		struct tasklet_struct *t = list;
+
+		list = list->next;
+
+		if (tasklet_trylock(t)) {
+			if (!atomic_read(&t->count)) {
+				if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
+					BUG();
+				t->func(t->data);
+				tasklet_unlock(t);
+				continue;
+			}
+			tasklet_unlock(t);
+		}
+
+		local_irq_disable();
+		t->next = __get_cpu_var(tasklet_hi_vec).list;
+		__get_cpu_var(tasklet_hi_vec).list = t;
+		__raise_softirq_irqoff(HI_SOFTIRQ);
+		local_irq_enable();
+	}
+}
+
+
+void tasklet_init(struct tasklet_struct *t,
+		  void (*func)(unsigned long), unsigned long data)
+{
+	t->next = NULL;
+	t->state = 0;
+	atomic_set(&t->count, 0);
+	t->func = func;
+	t->data = data;
+}
+
+EXPORT_SYMBOL(tasklet_init);
+
+void tasklet_kill(struct tasklet_struct *t)
+{
+	if (in_interrupt())
+		printk("Attempt to kill tasklet from interrupt\n");
+
+	while (test_and_set_bit(TASKLET_STATE_SCHED, &t->state)) {
+		do
+			yield();
+		while (test_bit(TASKLET_STATE_SCHED, &t->state));
+	}
+	tasklet_unlock_wait(t);
+	clear_bit(TASKLET_STATE_SCHED, &t->state);
+}
+
+EXPORT_SYMBOL(tasklet_kill);
+
+void __init softirq_init(void)
+{
+	open_softirq(TASKLET_SOFTIRQ, tasklet_action, NULL);
+	open_softirq(HI_SOFTIRQ, tasklet_hi_action, NULL);
+}
+
+static int ksoftirqd(void * __bind_cpu)
+{
+	set_user_nice(current, 19);
+	current->flags |= PF_NOFREEZE;
+
+	set_current_state(TASK_INTERRUPTIBLE);
+
+	while (!kthread_should_stop()) {
+		preempt_disable();
+		if (!local_softirq_pending()) {
+			preempt_enable_no_resched();
+			schedule();
+			preempt_disable();
+		}
+
+		__set_current_state(TASK_RUNNING);
+
+		while (local_softirq_pending()) {
+			/* Preempt disable stops cpu going offline.
+			   If already offline, we'll be on wrong CPU:
+			   don't process */
+			if (cpu_is_offline((long)__bind_cpu))
+				goto wait_to_die;
+			do_softirq();
+			preempt_enable_no_resched();
+			cond_resched();
+			preempt_disable();
+		}
+		preempt_enable();
+		set_current_state(TASK_INTERRUPTIBLE);
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+
+wait_to_die:
+	preempt_enable();
+	/* Wait for kthread_stop */
+	set_current_state(TASK_INTERRUPTIBLE);
+	while (!kthread_should_stop()) {
+		schedule();
+		set_current_state(TASK_INTERRUPTIBLE);
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+/*
+ * tasklet_kill_immediate is called to remove a tasklet which can already be
+ * scheduled for execution on @cpu.
+ *
+ * Unlike tasklet_kill, this function removes the tasklet
+ * _immediately_, even if the tasklet is in TASKLET_STATE_SCHED state.
+ *
+ * When this function is called, @cpu must be in the CPU_DEAD state.
+ */
+void tasklet_kill_immediate(struct tasklet_struct *t, unsigned int cpu)
+{
+	struct tasklet_struct **i;
+
+	BUG_ON(cpu_online(cpu));
+	BUG_ON(test_bit(TASKLET_STATE_RUN, &t->state));
+
+	if (!test_bit(TASKLET_STATE_SCHED, &t->state))
+		return;
+
+	/* CPU is dead, so no lock needed. */
+	for (i = &per_cpu(tasklet_vec, cpu).list; *i; i = &(*i)->next) {
+		if (*i == t) {
+			*i = t->next;
+			return;
+		}
+	}
+	BUG();
+}
+
+static void takeover_tasklets(unsigned int cpu)
+{
+	struct tasklet_struct **i;
+
+	/* CPU is dead, so no lock needed. */
+	local_irq_disable();
+
+	/* Find end, append list for that CPU. */
+	for (i = &__get_cpu_var(tasklet_vec).list; *i; i = &(*i)->next);
+	*i = per_cpu(tasklet_vec, cpu).list;
+	per_cpu(tasklet_vec, cpu).list = NULL;
+	raise_softirq_irqoff(TASKLET_SOFTIRQ);
+
+	for (i = &__get_cpu_var(tasklet_hi_vec).list; *i; i = &(*i)->next);
+	*i = per_cpu(tasklet_hi_vec, cpu).list;
+	per_cpu(tasklet_hi_vec, cpu).list = NULL;
+	raise_softirq_irqoff(HI_SOFTIRQ);
+
+	local_irq_enable();
+}
+#endif /* CONFIG_HOTPLUG_CPU */
+
+static int __devinit cpu_callback(struct notifier_block *nfb,
+				  unsigned long action,
+				  void *hcpu)
+{
+	int hotcpu = (unsigned long)hcpu;
+	struct task_struct *p;
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+		BUG_ON(per_cpu(tasklet_vec, hotcpu).list);
+		BUG_ON(per_cpu(tasklet_hi_vec, hotcpu).list);
+		p = kthread_create(ksoftirqd, hcpu, "ksoftirqd/%d", hotcpu);
+		if (IS_ERR(p)) {
+			printk("ksoftirqd for %i failed\n", hotcpu);
+			return NOTIFY_BAD;
+		}
+		kthread_bind(p, hotcpu);
+  		per_cpu(ksoftirqd, hotcpu) = p;
+ 		break;
+	case CPU_ONLINE:
+		wake_up_process(per_cpu(ksoftirqd, hotcpu));
+		break;
+#ifdef CONFIG_HOTPLUG_CPU
+	case CPU_UP_CANCELED:
+		/* Unbind so it can run.  Fall thru. */
+		kthread_bind(per_cpu(ksoftirqd, hotcpu), smp_processor_id());
+	case CPU_DEAD:
+		p = per_cpu(ksoftirqd, hotcpu);
+		per_cpu(ksoftirqd, hotcpu) = NULL;
+		kthread_stop(p);
+		takeover_tasklets(hotcpu);
+		break;
+#endif /* CONFIG_HOTPLUG_CPU */
+ 	}
+	return NOTIFY_OK;
+}
+
+static struct notifier_block __devinitdata cpu_nfb = {
+	.notifier_call = cpu_callback
+};
+
+__init int spawn_ksoftirqd(void)
+{
+	void *cpu = (void *)(long)smp_processor_id();
+	cpu_callback(&cpu_nfb, CPU_UP_PREPARE, cpu);
+	cpu_callback(&cpu_nfb, CPU_ONLINE, cpu);
+	register_cpu_notifier(&cpu_nfb);
+	return 0;
+}
