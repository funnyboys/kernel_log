commit fe557319aa06c23cffc9346000f119547e0f289a
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 17 09:37:53 2020 +0200

    maccess: rename probe_kernel_{read,write} to copy_{from,to}_kernel_nofault
    
    Better describe what these functions do.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 8e3d2d7fdf5e..132f84a5fde3 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -201,7 +201,7 @@ void *kthread_probe_data(struct task_struct *task)
 	struct kthread *kthread = to_kthread(task);
 	void *data = NULL;
 
-	probe_kernel_read(&data, &kthread->data, sizeof(data));
+	copy_from_kernel_nofault(&data, &kthread->data, sizeof(data));
 	return data;
 }
 

commit 623f6dc593eaf98b91916836785278eddddaacf8
Merge: 5cd221e837d6 37c54f9bd486
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 13:25:53 2020 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge some more updates from Andrew Morton:
    
     - various hotfixes and minor things
    
     - hch's use_mm/unuse_mm clearnups
    
    Subsystems affected by this patch series: mm/hugetlb, scripts, kcov,
    lib, nilfs, checkpatch, lib, mm/debug, ocfs2, lib, misc.
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>:
      kernel: set USER_DS in kthread_use_mm
      kernel: better document the use_mm/unuse_mm API contract
      kernel: move use_mm/unuse_mm to kthread.c
      kernel: move use_mm/unuse_mm to kthread.c
      stacktrace: cleanup inconsistent variable type
      lib: test get_count_order/long in test_bitops.c
      mm: add comments on pglist_data zones
      ocfs2: fix spelling mistake and grammar
      mm/debug_vm_pgtable: fix kernel crash by checking for THP support
      lib: fix bitmap_parse() on 64-bit big endian archs
      checkpatch: correct check for kernel parameters doc
      nilfs2: fix null pointer dereference at nilfs_segctor_do_construct()
      lib/lz4/lz4_decompress.c: document deliberate use of `&'
      kcov: check kcov_softirq in kcov_remote_stop()
      scripts/spelling: add a few more typos
      khugepaged: selftests: fix timeout condition in wait_for_scan()

commit 37c54f9bd48663f7657a9178fe08c47e4f5b537b
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 10 18:42:10 2020 -0700

    kernel: set USER_DS in kthread_use_mm
    
    Some architectures like arm64 and s390 require USER_DS to be set for
    kernel threads to access user address space, which is the whole purpose of
    kthread_use_mm, but other like x86 don't.  That has lead to a huge mess
    where some callers are fixed up once they are tested on said
    architectures, while others linger around and yet other like io_uring try
    to do "clever" optimizations for what usually is just a trivial asignment
    to a member in the thread_struct for most architectures.
    
    Make kthread_use_mm set USER_DS, and kthread_unuse_mm restore to the
    previous value instead.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Jens Axboe <axboe@kernel.dk>
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Acked-by: Michael S. Tsirkin <mst@redhat.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Felipe Balbi <balbi@kernel.org>
    Cc: Felix Kuehling <Felix.Kuehling@amd.com>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Link: http://lkml.kernel.org/r/20200404094101.672954-7-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 8ed4b4fbec7c..86357cd38eb2 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -52,6 +52,7 @@ struct kthread {
 	unsigned long flags;
 	unsigned int cpu;
 	void *data;
+	mm_segment_t oldfs;
 	struct completion parked;
 	struct completion exited;
 #ifdef CONFIG_BLK_CGROUP
@@ -1235,6 +1236,9 @@ void kthread_use_mm(struct mm_struct *mm)
 
 	if (active_mm != mm)
 		mmdrop(active_mm);
+
+	to_kthread(tsk)->oldfs = get_fs();
+	set_fs(USER_DS);
 }
 EXPORT_SYMBOL_GPL(kthread_use_mm);
 
@@ -1249,6 +1253,8 @@ void kthread_unuse_mm(struct mm_struct *mm)
 	WARN_ON_ONCE(!(tsk->flags & PF_KTHREAD));
 	WARN_ON_ONCE(!tsk->mm);
 
+	set_fs(to_kthread(tsk)->oldfs);
+
 	task_lock(tsk);
 	sync_mm_rss(mm);
 	tsk->mm = NULL;

commit f5678e7f2ac31c270334b936352f0ef2fe7dd2b3
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 10 18:42:06 2020 -0700

    kernel: better document the use_mm/unuse_mm API contract
    
    Switch the function documentation to kerneldoc comments, and add
    WARN_ON_ONCE asserts that the calling thread is a kernel thread and does
    not have ->mm set (or has ->mm set in the case of unuse_mm).
    
    Also give the functions a kthread_ prefix to better document the use case.
    
    [hch@lst.de: fix a comment typo, cover the newly merged use_mm/unuse_mm caller in vfio]
      Link: http://lkml.kernel.org/r/20200416053158.586887-3-hch@lst.de
    [sfr@canb.auug.org.au: powerpc/vas: fix up for {un}use_mm() rename]
      Link: http://lkml.kernel.org/r/20200422163935.5aa93ba5@canb.auug.org.au
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Jens Axboe <axboe@kernel.dk>
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Acked-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org> [usb]
    Acked-by: Haren Myneni <haren@linux.ibm.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Felipe Balbi <balbi@kernel.org>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Link: http://lkml.kernel.org/r/20200404094101.672954-6-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index ce4610316377..8ed4b4fbec7c 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -1208,18 +1208,18 @@ void kthread_destroy_worker(struct kthread_worker *worker)
 }
 EXPORT_SYMBOL(kthread_destroy_worker);
 
-/*
- * use_mm
- *	Makes the calling kernel thread take on the specified
- *	mm context.
- *	(Note: this routine is intended to be called only
- *	from a kernel thread context)
+/**
+ * kthread_use_mm - make the calling kthread operate on an address space
+ * @mm: address space to operate on
  */
-void use_mm(struct mm_struct *mm)
+void kthread_use_mm(struct mm_struct *mm)
 {
 	struct mm_struct *active_mm;
 	struct task_struct *tsk = current;
 
+	WARN_ON_ONCE(!(tsk->flags & PF_KTHREAD));
+	WARN_ON_ONCE(tsk->mm);
+
 	task_lock(tsk);
 	active_mm = tsk->active_mm;
 	if (active_mm != mm) {
@@ -1236,20 +1236,19 @@ void use_mm(struct mm_struct *mm)
 	if (active_mm != mm)
 		mmdrop(active_mm);
 }
-EXPORT_SYMBOL_GPL(use_mm);
+EXPORT_SYMBOL_GPL(kthread_use_mm);
 
-/*
- * unuse_mm
- *	Reverses the effect of use_mm, i.e. releases the
- *	specified mm context which was earlier taken on
- *	by the calling kernel thread
- *	(Note: this routine is intended to be called only
- *	from a kernel thread context)
+/**
+ * kthread_unuse_mm - reverse the effect of kthread_use_mm()
+ * @mm: address space to operate on
  */
-void unuse_mm(struct mm_struct *mm)
+void kthread_unuse_mm(struct mm_struct *mm)
 {
 	struct task_struct *tsk = current;
 
+	WARN_ON_ONCE(!(tsk->flags & PF_KTHREAD));
+	WARN_ON_ONCE(!tsk->mm);
+
 	task_lock(tsk);
 	sync_mm_rss(mm);
 	tsk->mm = NULL;
@@ -1257,7 +1256,7 @@ void unuse_mm(struct mm_struct *mm)
 	enter_lazy_tlb(mm, tsk);
 	task_unlock(tsk);
 }
-EXPORT_SYMBOL_GPL(unuse_mm);
+EXPORT_SYMBOL_GPL(kthread_unuse_mm);
 
 #ifdef CONFIG_BLK_CGROUP
 /**

commit 9bf5b9eb232b34738800868e30bea3bad4a6a1ba
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 10 18:41:59 2020 -0700

    kernel: move use_mm/unuse_mm to kthread.c
    
    Patch series "improve use_mm / unuse_mm", v2.
    
    This series improves the use_mm / unuse_mm interface by better documenting
    the assumptions, and my taking the set_fs manipulations spread over the
    callers into the core API.
    
    This patch (of 3):
    
    Use the proper API instead.
    
    Link: http://lkml.kernel.org/r/20200404094101.672954-1-hch@lst.de
    
    These helpers are only for use with kernel threads, and I will tie them
    more into the kthread infrastructure going forward.  Also move the
    prototypes to kthread.h - mmu_context.h was a little weird to start with
    as it otherwise contains very low-level MM bits.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Jens Axboe <axboe@kernel.dk>
    Reviewed-by: Jens Axboe <axboe@kernel.dk>
    Acked-by: Felix Kuehling <Felix.Kuehling@amd.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Felipe Balbi <balbi@kernel.org>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Zhenyu Wang <zhenyuw@linux.intel.com>
    Cc: Zhi Wang <zhi.a.wang@intel.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Link: http://lkml.kernel.org/r/20200404094101.672954-1-hch@lst.de
    Link: http://lkml.kernel.org/r/20200416053158.586887-1-hch@lst.de
    Link: http://lkml.kernel.org/r/20200404094101.672954-5-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index bfbfa481be3a..ce4610316377 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -1,13 +1,17 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /* Kernel thread helper functions.
  *   Copyright (C) 2004 IBM Corporation, Rusty Russell.
+ *   Copyright (C) 2009 Red Hat, Inc.
  *
  * Creation is done via kthreadd, so that we get a clean environment
  * even if we're invoked from userspace (think modprobe, hotplug cpu,
  * etc.).
  */
 #include <uapi/linux/sched/types.h>
+#include <linux/mm.h>
+#include <linux/mmu_context.h>
 #include <linux/sched.h>
+#include <linux/sched/mm.h>
 #include <linux/sched/task.h>
 #include <linux/kthread.h>
 #include <linux/completion.h>
@@ -25,6 +29,7 @@
 #include <linux/numa.h>
 #include <trace/events/sched.h>
 
+
 static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
 struct task_struct *kthreadd_task;
@@ -1203,6 +1208,57 @@ void kthread_destroy_worker(struct kthread_worker *worker)
 }
 EXPORT_SYMBOL(kthread_destroy_worker);
 
+/*
+ * use_mm
+ *	Makes the calling kernel thread take on the specified
+ *	mm context.
+ *	(Note: this routine is intended to be called only
+ *	from a kernel thread context)
+ */
+void use_mm(struct mm_struct *mm)
+{
+	struct mm_struct *active_mm;
+	struct task_struct *tsk = current;
+
+	task_lock(tsk);
+	active_mm = tsk->active_mm;
+	if (active_mm != mm) {
+		mmgrab(mm);
+		tsk->active_mm = mm;
+	}
+	tsk->mm = mm;
+	switch_mm(active_mm, mm, tsk);
+	task_unlock(tsk);
+#ifdef finish_arch_post_lock_switch
+	finish_arch_post_lock_switch();
+#endif
+
+	if (active_mm != mm)
+		mmdrop(active_mm);
+}
+EXPORT_SYMBOL_GPL(use_mm);
+
+/*
+ * unuse_mm
+ *	Reverses the effect of use_mm, i.e. releases the
+ *	specified mm context which was earlier taken on
+ *	by the calling kernel thread
+ *	(Note: this routine is intended to be called only
+ *	from a kernel thread context)
+ */
+void unuse_mm(struct mm_struct *mm)
+{
+	struct task_struct *tsk = current;
+
+	task_lock(tsk);
+	sync_mm_rss(mm);
+	tsk->mm = NULL;
+	/* active_mm is still 'mm' */
+	enter_lazy_tlb(mm, tsk);
+	task_unlock(tsk);
+}
+EXPORT_SYMBOL_GPL(unuse_mm);
+
 #ifdef CONFIG_BLK_CGROUP
 /**
  * kthread_associate_blkcg - associate blkcg to current kthread

commit 52782c92ac85c4e393eb4a903a62e6c24afa633f
Author: J. Bruce Fields <bfields@redhat.com>
Date:   Wed May 6 12:09:34 2020 -0400

    kthread: save thread function
    
    It's handy to keep the kthread_fn just as a unique cookie to identify
    classes of kthreads.  E.g. if you can verify that a given task is
    running your thread_fn, then you may know what sort of type kthread_data
    points to.
    
    We'll use this in nfsd to pass some information into the vfs.  Note it
    will need kthread_data() exported too.
    
    Original-patch-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index bfbfa481be3a..b84fc7eec035 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -46,6 +46,7 @@ struct kthread_create_info
 struct kthread {
 	unsigned long flags;
 	unsigned int cpu;
+	int (*threadfn)(void *);
 	void *data;
 	struct completion parked;
 	struct completion exited;
@@ -152,6 +153,20 @@ bool kthread_freezable_should_stop(bool *was_frozen)
 }
 EXPORT_SYMBOL_GPL(kthread_freezable_should_stop);
 
+/**
+ * kthread_func - return the function specified on kthread creation
+ * @task: kthread task in question
+ *
+ * Returns NULL if the task is not a kthread.
+ */
+void *kthread_func(struct task_struct *task)
+{
+	if (task->flags & PF_KTHREAD)
+		return to_kthread(task)->threadfn;
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(kthread_func);
+
 /**
  * kthread_data - return data value specified on kthread creation
  * @task: kthread task in question
@@ -164,6 +179,7 @@ void *kthread_data(struct task_struct *task)
 {
 	return to_kthread(task)->data;
 }
+EXPORT_SYMBOL_GPL(kthread_data);
 
 /**
  * kthread_probe_data - speculative version of kthread_data()
@@ -244,6 +260,7 @@ static int kthread(void *_create)
 		do_exit(-ENOMEM);
 	}
 
+	self->threadfn = threadfn;
 	self->data = data;
 	init_completion(&self->exited);
 	init_completion(&self->parked);

commit 26c7295be0c5e6da3fa45970e9748be983175b1b
Author: Liang Chen <cl@rock-chips.com>
Date:   Fri Mar 6 15:01:33 2020 +0800

    kthread: Do not preempt current task if it is going to call schedule()
    
    when we create a kthread with ktrhead_create_on_cpu(),the child thread
    entry is ktread.c:ktrhead() which will be preempted by the parent after
    call complete(done) while schedule() is not called yet,then the parent
    will call wait_task_inactive(child) but the child is still on the runqueue,
    so the parent will schedule_hrtimeout() for 1 jiffy,it will waste a lot of
    time,especially on startup.
    
      parent                             child
    ktrhead_create_on_cpu()
      wait_fo_completion(&done) -----> ktread.c:ktrhead()
                                 |----- complete(done);--wakeup and preempted by parent
     kthread_bind() <------------|  |-> schedule();--dequeue here
      wait_task_inactive(child)     |
       schedule_hrtimeout(1 jiffy) -|
    
    So we hope the child just wakeup parent but not preempted by parent, and the
    child is going to call schedule() soon,then the parent will not call
    schedule_hrtimeout(1 jiffy) as the child is already dequeue.
    
    The same issue for ktrhead_park()&&kthread_parkme().
    This patch can save 120ms on rk312x startup with CONFIG_HZ=300.
    
    Signed-off-by: Liang Chen <cl@rock-chips.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Link: https://lkml.kernel.org/r/20200306070133.18335-2-cl@rock-chips.com

diff --git a/kernel/kthread.c b/kernel/kthread.c
index b262f47046ca..bfbfa481be3a 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -199,8 +199,15 @@ static void __kthread_parkme(struct kthread *self)
 		if (!test_bit(KTHREAD_SHOULD_PARK, &self->flags))
 			break;
 
+		/*
+		 * Thread is going to call schedule(), do not preempt it,
+		 * or the caller of kthread_park() may spend more time in
+		 * wait_task_inactive().
+		 */
+		preempt_disable();
 		complete(&self->parked);
-		schedule();
+		schedule_preempt_disabled();
+		preempt_enable();
 	}
 	__set_current_state(TASK_RUNNING);
 }
@@ -245,8 +252,14 @@ static int kthread(void *_create)
 	/* OK, tell user we're spawned, wait for stop or wakeup */
 	__set_current_state(TASK_UNINTERRUPTIBLE);
 	create->result = current;
+	/*
+	 * Thread is going to call schedule(), do not preempt it,
+	 * or the creator may spend more time in wait_task_inactive().
+	 */
+	preempt_disable();
 	complete(done);
-	schedule();
+	schedule_preempt_disabled();
+	preempt_enable();
 
 	ret = -EINTR;
 	if (!test_bit(KTHREAD_SHOULD_STOP, &self->flags)) {

commit bc88f85c6c09306bd21917e1ae28205e9cd775a7
Author: Ben Dooks <ben.dooks@codethink.co.uk>
Date:   Wed Oct 16 12:24:58 2019 +0100

    kthread: make __kthread_queue_delayed_work static
    
    The __kthread_queue_delayed_work is not exported so
    make it static, to avoid the following sparse warning:
    
      kernel/kthread.c:869:6: warning: symbol '__kthread_queue_delayed_work' was not declared. Should it be static?
    
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 621467c33fef..b262f47046ca 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -866,9 +866,9 @@ void kthread_delayed_work_timer_fn(struct timer_list *t)
 }
 EXPORT_SYMBOL(kthread_delayed_work_timer_fn);
 
-void __kthread_queue_delayed_work(struct kthread_worker *worker,
-				  struct kthread_delayed_work *dwork,
-				  unsigned long delay)
+static void __kthread_queue_delayed_work(struct kthread_worker *worker,
+					 struct kthread_delayed_work *dwork,
+					 unsigned long delay)
 {
 	struct timer_list *timer = &dwork->timer;
 	struct kthread_work *work = &dwork->work;

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index be4e8795561a..621467c33fef 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /* Kernel thread helper functions.
  *   Copyright (C) 2004 IBM Corporation, Rusty Russell.
  *

commit 8af0c18af1425fc70686c0fdcfc0072cd8431aa0
Author: Suren Baghdasaryan <surenb@google.com>
Date:   Tue May 14 15:41:12 2019 -0700

    include/: refactor headers to allow kthread.h inclusion in psi_types.h
    
    kthread.h can't be included in psi_types.h because it creates a circular
    inclusion with kthread.h eventually including psi_types.h and
    complaining on kthread structures not being defined because they are
    defined further in the kthread.h.  Resolve this by removing psi_types.h
    inclusion from the headers included from kthread.h.
    
    Link: http://lkml.kernel.org/r/20190319235619.260832-7-surenb@google.com
    Signed-off-by: Suren Baghdasaryan <surenb@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Dennis Zhou <dennis@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 5942eeafb9ac..be4e8795561a 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -11,6 +11,7 @@
 #include <linux/kthread.h>
 #include <linux/completion.h>
 #include <linux/err.h>
+#include <linux/cgroup.h>
 #include <linux/cpuset.h>
 #include <linux/unistd.h>
 #include <linux/file.h>

commit 8dcd175bc3d50b78413c56d5b17d4bddd77412ef
Merge: afe6fe7036c6 fff04900ea79
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 10:31:36 2019 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge misc updates from Andrew Morton:
    
     - a few misc things
    
     - ocfs2 updates
    
     - most of MM
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (159 commits)
      tools/testing/selftests/proc/proc-self-syscall.c: remove duplicate include
      proc: more robust bulk read test
      proc: test /proc/*/maps, smaps, smaps_rollup, statm
      proc: use seq_puts() everywhere
      proc: read kernel cpu stat pointer once
      proc: remove unused argument in proc_pid_lookup()
      fs/proc/thread_self.c: code cleanup for proc_setup_thread_self()
      fs/proc/self.c: code cleanup for proc_setup_self()
      proc: return exit code 4 for skipped tests
      mm,mremap: bail out earlier in mremap_to under map pressure
      mm/sparse: fix a bad comparison
      mm/memory.c: do_fault: avoid usage of stale vm_area_struct
      writeback: fix inode cgroup switching comment
      mm/huge_memory.c: fix "orig_pud" set but not used
      mm/hotplug: fix an imbalance with DEBUG_PAGEALLOC
      mm/memcontrol.c: fix bad line in comment
      mm/cma.c: cma_declare_contiguous: correct err handling
      mm/page_ext.c: fix an imbalance with kmemleak
      mm/compaction: pass pgdat to too_many_isolated() instead of zone
      mm: remove zone_lru_lock() function, access ->lru_lock directly
      ...

commit 45802da05e666a81b421422d3e302930c0e24e77
Merge: 203b6609e0ed ad01423aedaa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 08:14:05 2019 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - refcount conversions
    
       - Solve the rq->leaf_cfs_rq_list can of worms for real.
    
       - improve power-aware scheduling
    
       - add sysctl knob for Energy Aware Scheduling
    
       - documentation updates
    
       - misc other changes"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (34 commits)
      kthread: Do not use TIMER_IRQSAFE
      kthread: Convert worker lock to raw spinlock
      sched/fair: Use non-atomic cpumask_{set,clear}_cpu()
      sched/fair: Remove unused 'sd' parameter from select_idle_smt()
      sched/wait: Use freezable_schedule() when possible
      sched/fair: Prune, fix and simplify the nohz_balancer_kick() comment block
      sched/fair: Explain LLC nohz kick condition
      sched/fair: Simplify nohz_balancer_kick()
      sched/topology: Fix percpu data types in struct sd_data & struct s_data
      sched/fair: Simplify post_init_entity_util_avg() by calling it with a task_struct pointer argument
      sched/fair: Fix O(nr_cgroups) in the load balancing path
      sched/fair: Optimize update_blocked_averages()
      sched/fair: Fix insertion in rq->leaf_cfs_rq_list
      sched/fair: Add tmp_alone_branch assertion
      sched/core: Use READ_ONCE()/WRITE_ONCE() in move_queued_task()/task_rq_lock()
      sched/debug: Initialize sd_sysctl_cpus if !CONFIG_CPUMASK_OFFSTACK
      sched/pelt: Skip updating util_est when utilization is higher than CPU's capacity
      sched/fair: Update scale invariance of PELT
      sched/fair: Move the rq_of() helper function
      sched/core: Convert task_struct.stack_refcount to refcount_t
      ...

commit 98fa15f34cb379864757670b8e8743b21456a20e
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Mar 5 15:42:58 2019 -0800

    mm: replace all open encodings for NUMA_NO_NODE
    
    Patch series "Replace all open encodings for NUMA_NO_NODE", v3.
    
    All these places for replacement were found by running the following
    grep patterns on the entire kernel code.  Please let me know if this
    might have missed some instances.  This might also have replaced some
    false positives.  I will appreciate suggestions, inputs and review.
    
    1. git grep "nid == -1"
    2. git grep "node == -1"
    3. git grep "nid = -1"
    4. git grep "node = -1"
    
    This patch (of 2):
    
    At present there are multiple places where invalid node number is
    encoded as -1.  Even though implicitly understood it is always better to
    have macros in there.  Replace these open encodings for an invalid node
    number with the global macro NUMA_NO_NODE.  This helps remove NUMA
    related assumptions like 'invalid node' from various places redirecting
    them to a common definition.
    
    Link: http://lkml.kernel.org/r/1545127933-10711-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>    [ixgbe]
    Acked-by: Jens Axboe <axboe@kernel.dk>                  [mtip32xx]
    Acked-by: Vinod Koul <vkoul@kernel.org>                 [dmaengine.c]
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>         [powerpc]
    Acked-by: Doug Ledford <dledford@redhat.com>            [drivers/infiniband]
    Cc: Joseph Qi <jiangqi903@gmail.com>
    Cc: Hans Verkuil <hverkuil@xs4all.nl>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 087d18d771b5..ebebbcf3c5de 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -20,6 +20,7 @@
 #include <linux/freezer.h>
 #include <linux/ptrace.h>
 #include <linux/uaccess.h>
+#include <linux/numa.h>
 #include <trace/events/sched.h>
 
 static DEFINE_SPINLOCK(kthread_create_lock);
@@ -675,7 +676,7 @@ __kthread_create_worker(int cpu, unsigned int flags,
 {
 	struct kthread_worker *worker;
 	struct task_struct *task;
-	int node = -1;
+	int node = NUMA_NO_NODE;
 
 	worker = kzalloc(sizeof(*worker), GFP_KERNEL);
 	if (!worker)

commit ad01423aedaa7c6dd62d560b73a3cb39e6da3901
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Feb 12 17:25:54 2019 +0100

    kthread: Do not use TIMER_IRQSAFE
    
    The TIMER_IRQSAFE usage was introduced in commit 22597dc3d97b1 ("kthread:
    initial support for delayed kthread work") which modelled the delayed
    kthread code after workqueue's code. The workqueue code requires the flag
    TIMER_IRQSAFE for synchronisation purpose. This is not true for kthread's
    delay timer since all operations occur under a lock.
    
    Remove TIMER_IRQSAFE from the timer initialisation and use timer_setup()
    for initialisation purpose which is the official function.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Link: https://lkml.kernel.org/r/20190212162554.19779-2-bigeasy@linutronix.de

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 5641b55783a6..537335541267 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -835,6 +835,7 @@ void kthread_delayed_work_timer_fn(struct timer_list *t)
 	struct kthread_delayed_work *dwork = from_timer(dwork, t, timer);
 	struct kthread_work *work = &dwork->work;
 	struct kthread_worker *worker = work->worker;
+	unsigned long flags;
 
 	/*
 	 * This might happen when a pending work is reinitialized.
@@ -843,7 +844,7 @@ void kthread_delayed_work_timer_fn(struct timer_list *t)
 	if (WARN_ON_ONCE(!worker))
 		return;
 
-	raw_spin_lock(&worker->lock);
+	raw_spin_lock_irqsave(&worker->lock, flags);
 	/* Work must not be used with >1 worker, see kthread_queue_work(). */
 	WARN_ON_ONCE(work->worker != worker);
 
@@ -852,7 +853,7 @@ void kthread_delayed_work_timer_fn(struct timer_list *t)
 	list_del_init(&work->node);
 	kthread_insert_work(worker, work, &worker->work_list);
 
-	raw_spin_unlock(&worker->lock);
+	raw_spin_unlock_irqrestore(&worker->lock, flags);
 }
 EXPORT_SYMBOL(kthread_delayed_work_timer_fn);
 

commit fe99a4f4d6022ec92f9b52a5528cb9b77513e7d1
Author: Julia Cartwright <julia@ni.com>
Date:   Tue Feb 12 17:25:53 2019 +0100

    kthread: Convert worker lock to raw spinlock
    
    In order to enable the queuing of kthread work items from hardirq context
    even when PREEMPT_RT_FULL is enabled, convert the worker spin_lock to a
    raw_spin_lock.
    
    This is only acceptable to do because the work performed under the lock is
    well-bounded and minimal.
    
    Reported-by: Steffen Trumtrar <s.trumtrar@pengutronix.de>
    Reported-by: Tim Sander <tim@krieglstein.org>
    Signed-off-by: Julia Cartwright <julia@ni.com>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Steffen Trumtrar <s.trumtrar@pengutronix.de>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Link: https://lkml.kernel.org/r/20190212162554.19779-1-bigeasy@linutronix.de

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 087d18d771b5..5641b55783a6 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -599,7 +599,7 @@ void __kthread_init_worker(struct kthread_worker *worker,
 				struct lock_class_key *key)
 {
 	memset(worker, 0, sizeof(struct kthread_worker));
-	spin_lock_init(&worker->lock);
+	raw_spin_lock_init(&worker->lock);
 	lockdep_set_class_and_name(&worker->lock, key, name);
 	INIT_LIST_HEAD(&worker->work_list);
 	INIT_LIST_HEAD(&worker->delayed_work_list);
@@ -641,21 +641,21 @@ int kthread_worker_fn(void *worker_ptr)
 
 	if (kthread_should_stop()) {
 		__set_current_state(TASK_RUNNING);
-		spin_lock_irq(&worker->lock);
+		raw_spin_lock_irq(&worker->lock);
 		worker->task = NULL;
-		spin_unlock_irq(&worker->lock);
+		raw_spin_unlock_irq(&worker->lock);
 		return 0;
 	}
 
 	work = NULL;
-	spin_lock_irq(&worker->lock);
+	raw_spin_lock_irq(&worker->lock);
 	if (!list_empty(&worker->work_list)) {
 		work = list_first_entry(&worker->work_list,
 					struct kthread_work, node);
 		list_del_init(&work->node);
 	}
 	worker->current_work = work;
-	spin_unlock_irq(&worker->lock);
+	raw_spin_unlock_irq(&worker->lock);
 
 	if (work) {
 		__set_current_state(TASK_RUNNING);
@@ -812,12 +812,12 @@ bool kthread_queue_work(struct kthread_worker *worker,
 	bool ret = false;
 	unsigned long flags;
 
-	spin_lock_irqsave(&worker->lock, flags);
+	raw_spin_lock_irqsave(&worker->lock, flags);
 	if (!queuing_blocked(worker, work)) {
 		kthread_insert_work(worker, work, &worker->work_list);
 		ret = true;
 	}
-	spin_unlock_irqrestore(&worker->lock, flags);
+	raw_spin_unlock_irqrestore(&worker->lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL_GPL(kthread_queue_work);
@@ -843,7 +843,7 @@ void kthread_delayed_work_timer_fn(struct timer_list *t)
 	if (WARN_ON_ONCE(!worker))
 		return;
 
-	spin_lock(&worker->lock);
+	raw_spin_lock(&worker->lock);
 	/* Work must not be used with >1 worker, see kthread_queue_work(). */
 	WARN_ON_ONCE(work->worker != worker);
 
@@ -852,7 +852,7 @@ void kthread_delayed_work_timer_fn(struct timer_list *t)
 	list_del_init(&work->node);
 	kthread_insert_work(worker, work, &worker->work_list);
 
-	spin_unlock(&worker->lock);
+	raw_spin_unlock(&worker->lock);
 }
 EXPORT_SYMBOL(kthread_delayed_work_timer_fn);
 
@@ -908,14 +908,14 @@ bool kthread_queue_delayed_work(struct kthread_worker *worker,
 	unsigned long flags;
 	bool ret = false;
 
-	spin_lock_irqsave(&worker->lock, flags);
+	raw_spin_lock_irqsave(&worker->lock, flags);
 
 	if (!queuing_blocked(worker, work)) {
 		__kthread_queue_delayed_work(worker, dwork, delay);
 		ret = true;
 	}
 
-	spin_unlock_irqrestore(&worker->lock, flags);
+	raw_spin_unlock_irqrestore(&worker->lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL_GPL(kthread_queue_delayed_work);
@@ -951,7 +951,7 @@ void kthread_flush_work(struct kthread_work *work)
 	if (!worker)
 		return;
 
-	spin_lock_irq(&worker->lock);
+	raw_spin_lock_irq(&worker->lock);
 	/* Work must not be used with >1 worker, see kthread_queue_work(). */
 	WARN_ON_ONCE(work->worker != worker);
 
@@ -963,7 +963,7 @@ void kthread_flush_work(struct kthread_work *work)
 	else
 		noop = true;
 
-	spin_unlock_irq(&worker->lock);
+	raw_spin_unlock_irq(&worker->lock);
 
 	if (!noop)
 		wait_for_completion(&fwork.done);
@@ -996,9 +996,9 @@ static bool __kthread_cancel_work(struct kthread_work *work, bool is_dwork,
 		 * any queuing is blocked by setting the canceling counter.
 		 */
 		work->canceling++;
-		spin_unlock_irqrestore(&worker->lock, *flags);
+		raw_spin_unlock_irqrestore(&worker->lock, *flags);
 		del_timer_sync(&dwork->timer);
-		spin_lock_irqsave(&worker->lock, *flags);
+		raw_spin_lock_irqsave(&worker->lock, *flags);
 		work->canceling--;
 	}
 
@@ -1045,7 +1045,7 @@ bool kthread_mod_delayed_work(struct kthread_worker *worker,
 	unsigned long flags;
 	int ret = false;
 
-	spin_lock_irqsave(&worker->lock, flags);
+	raw_spin_lock_irqsave(&worker->lock, flags);
 
 	/* Do not bother with canceling when never queued. */
 	if (!work->worker)
@@ -1062,7 +1062,7 @@ bool kthread_mod_delayed_work(struct kthread_worker *worker,
 fast_queue:
 	__kthread_queue_delayed_work(worker, dwork, delay);
 out:
-	spin_unlock_irqrestore(&worker->lock, flags);
+	raw_spin_unlock_irqrestore(&worker->lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL_GPL(kthread_mod_delayed_work);
@@ -1076,7 +1076,7 @@ static bool __kthread_cancel_work_sync(struct kthread_work *work, bool is_dwork)
 	if (!worker)
 		goto out;
 
-	spin_lock_irqsave(&worker->lock, flags);
+	raw_spin_lock_irqsave(&worker->lock, flags);
 	/* Work must not be used with >1 worker, see kthread_queue_work(). */
 	WARN_ON_ONCE(work->worker != worker);
 
@@ -1090,13 +1090,13 @@ static bool __kthread_cancel_work_sync(struct kthread_work *work, bool is_dwork)
 	 * In the meantime, block any queuing by setting the canceling counter.
 	 */
 	work->canceling++;
-	spin_unlock_irqrestore(&worker->lock, flags);
+	raw_spin_unlock_irqrestore(&worker->lock, flags);
 	kthread_flush_work(work);
-	spin_lock_irqsave(&worker->lock, flags);
+	raw_spin_lock_irqsave(&worker->lock, flags);
 	work->canceling--;
 
 out_fast:
-	spin_unlock_irqrestore(&worker->lock, flags);
+	raw_spin_unlock_irqrestore(&worker->lock, flags);
 out:
 	return ret;
 }

commit 0121805d9d2b1fff371e195c28e9b86ae38b5e47
Author: Matthias Kaehlcke <mka@chromium.org>
Date:   Mon Jan 28 15:46:24 2019 -0800

    kthread: Add __kthread_should_park()
    
    kthread_should_park() is used to check if the calling kthread ('current')
    should park, but there is no function to check whether an arbitrary kthread
    should be parked. The latter is required to plug a CPU hotplug race vs. a
    parking ksoftirqd thread.
    
    The new __kthread_should_park() receives a task_struct as parameter to
    check if the corresponding kernel thread should be parked.
    
    Call __kthread_should_park() from kthread_should_park() to avoid code
    duplication.
    
    Signed-off-by: Matthias Kaehlcke <mka@chromium.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Douglas Anderson <dianders@chromium.org>
    Cc: Stephen Boyd <swboyd@chromium.org>
    Link: https://lkml.kernel.org/r/20190128234625.78241-2-mka@chromium.org

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 087d18d771b5..65234c89d85b 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -101,6 +101,12 @@ bool kthread_should_stop(void)
 }
 EXPORT_SYMBOL(kthread_should_stop);
 
+bool __kthread_should_park(struct task_struct *k)
+{
+	return test_bit(KTHREAD_SHOULD_PARK, &to_kthread(k)->flags);
+}
+EXPORT_SYMBOL_GPL(__kthread_should_park);
+
 /**
  * kthread_should_park - should this kthread park now?
  *
@@ -114,7 +120,7 @@ EXPORT_SYMBOL(kthread_should_stop);
  */
 bool kthread_should_park(void)
 {
-	return test_bit(KTHREAD_SHOULD_PARK, &to_kthread(current)->flags);
+	return __kthread_should_park(current);
 }
 EXPORT_SYMBOL_GPL(kthread_should_park);
 

commit f7951c33f0fed14ee26651a70a46899a59a31e18
Merge: 2406fb8d94fb 1b6266ebe3da
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 13 11:25:07 2018 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Thomas Gleixner:
    
     - Cleanup and improvement of NUMA balancing
    
     - Refactoring and improvements to the PELT (Per Entity Load Tracking)
       code
    
     - Watchdog simplification and related cleanups
    
     - The usual pile of small incremental fixes and improvements
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (41 commits)
      watchdog: Reduce message verbosity
      stop_machine: Reflow cpu_stop_queue_two_works()
      sched/numa: Move task_numa_placement() closer to numa_migrate_preferred()
      sched/numa: Use group_weights to identify if migration degrades locality
      sched/numa: Update the scan period without holding the numa_group lock
      sched/numa: Remove numa_has_capacity()
      sched/numa: Modify migrate_swap() to accept additional parameters
      sched/numa: Remove unused task_capacity from 'struct numa_stats'
      sched/numa: Skip nodes that are at 'hoplimit'
      sched/debug: Reverse the order of printing faults
      sched/numa: Use task faults only if numa_group is not yet set up
      sched/numa: Set preferred_node based on best_cpu
      sched/numa: Simplify load_too_imbalanced()
      sched/numa: Evaluate move once per node
      sched/numa: Remove redundant field
      sched/debug: Show the sum wait time of a task group
      sched/fair: Remove #ifdefs from scale_rt_capacity()
      sched/core: Remove get_cpu() from sched_fork()
      sched/cpufreq: Clarify sugov_get_util()
      sched/sysctl: Remove unused sched_time_avg_ms sysctl
      ...

commit 3e536e222f2930534c252c1cc7ae799c725c5ff9
Author: Snild Dolkow <snild@sony.com>
Date:   Thu Jul 26 09:15:39 2018 +0200

    kthread, tracing: Don't expose half-written comm when creating kthreads
    
    There is a window for racing when printing directly to task->comm,
    allowing other threads to see a non-terminated string. The vsnprintf
    function fills the buffer, counts the truncated chars, then finally
    writes the \0 at the end.
    
            creator                     other
            vsnprintf:
              fill (not terminated)
              count the rest            trace_sched_waking(p):
              ...                         memcpy(comm, p->comm, TASK_COMM_LEN)
              write \0
    
    The consequences depend on how 'other' uses the string. In our case,
    it was copied into the tracing system's saved cmdlines, a buffer of
    adjacent TASK_COMM_LEN-byte buffers (note the 'n' where 0 should be):
    
            crash-arm64> x/1024s savedcmd->saved_cmdlines | grep 'evenk'
            0xffffffd5b3818640:     "irq/497-pwr_evenkworker/u16:12"
    
    ...and a strcpy out of there would cause stack corruption:
    
            [224761.522292] Kernel panic - not syncing: stack-protector:
                Kernel stack is corrupted in: ffffff9bf9783c78
    
            crash-arm64> kbt | grep 'comm\|trace_print_context'
            #6  0xffffff9bf9783c78 in trace_print_context+0x18c(+396)
                  comm (char [16]) =  "irq/497-pwr_even"
    
            crash-arm64> rd 0xffffffd4d0e17d14 8
            ffffffd4d0e17d14:  2f71726900000000 5f7277702d373934   ....irq/497-pwr_
            ffffffd4d0e17d24:  726f776b6e657665 3a3631752f72656b   evenkworker/u16:
            ffffffd4d0e17d34:  f9780248ff003231 cede60e0ffffff9b   12..H.x......`..
            ffffffd4d0e17d44:  cede60c8ffffffd4 00000fffffffffd4   .....`..........
    
    The workaround in e09e28671 (use strlcpy in __trace_find_cmdline) was
    likely needed because of this same bug.
    
    Solved by vsnprintf:ing to a local buffer, then using set_task_comm().
    This way, there won't be a window where comm is not terminated.
    
    Link: http://lkml.kernel.org/r/20180726071539.188015-1-snild@sony.com
    
    Cc: stable@vger.kernel.org
    Fixes: bc0c38d139ec7 ("ftrace: latency tracer infrastructure")
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Snild Dolkow <snild@sony.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 750cb8082694..486dedbd9af5 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -325,8 +325,14 @@ struct task_struct *__kthread_create_on_node(int (*threadfn)(void *data),
 	task = create->result;
 	if (!IS_ERR(task)) {
 		static const struct sched_param param = { .sched_priority = 0 };
+		char name[TASK_COMM_LEN];
 
-		vsnprintf(task->comm, sizeof(task->comm), namefmt, args);
+		/*
+		 * task is already visible to other tasks, so updating
+		 * COMM must be protected.
+		 */
+		vsnprintf(name, sizeof(name), namefmt, args);
+		set_task_comm(task, name);
 		/*
 		 * root may have changed our (kthreadd's) priority or CPU mask.
 		 * The kernel thread should not inherit these properties.

commit f83ee19be4272564ad592ef90145db7295229490
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jun 7 10:55:56 2018 +0200

    kthread: Simplify kthread_park() completion
    
    Oleg explains the reason we could hit park+park is that
    smpboot_update_cpumask_percpu_thread()'s
    
      for_each_cpu_and(cpu, &tmp, cpu_online_mask)
            smpboot_park_kthread();
    
    turns into:
    
      for ((cpu) = 0; (cpu) < 1; (cpu)++, (void)mask, (void)and)
            smpboot_park_kthread();
    
    on UP, ignoring the mask. But since we just completely removed that
    function, this is no longer relevant.
    
    So revert commit:
    
      b1f5b378e126 ("kthread: Allow kthread_park() on a parked kthread")
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 750cb8082694..11b591ee51ab 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -190,7 +190,7 @@ static void __kthread_parkme(struct kthread *self)
 		if (!test_bit(KTHREAD_SHOULD_PARK, &self->flags))
 			break;
 
-		complete_all(&self->parked);
+		complete(&self->parked);
 		schedule();
 	}
 	__set_current_state(TASK_RUNNING);
@@ -465,7 +465,6 @@ void kthread_unpark(struct task_struct *k)
 	if (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))
 		__kthread_bind(k, kthread->cpu, TASK_PARKED);
 
-	reinit_completion(&kthread->parked);
 	clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
 	/*
 	 * __kthread_parkme() will either see !SHOULD_PARK or get the wakeup.
@@ -493,6 +492,9 @@ int kthread_park(struct task_struct *k)
 	if (WARN_ON(k->flags & PF_EXITING))
 		return -ENOSYS;
 
+	if (WARN_ON_ONCE(test_bit(KTHREAD_SHOULD_PARK, &kthread->flags)))
+		return -EBUSY;
+
 	set_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
 	if (k != current) {
 		wake_up_process(k);

commit 1cef1150ef40ec52f507436a14230cbc2623299c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jun 7 11:45:49 2018 +0200

    kthread, sched/core: Fix kthread_parkme() (again...)
    
    Gaurav reports that commit:
    
      85f1abe0019f ("kthread, sched/wait: Fix kthread_parkme() completion issue")
    
    isn't working for him. Because of the following race:
    
    > controller Thread                               CPUHP Thread
    > takedown_cpu
    > kthread_park
    > kthread_parkme
    > Set KTHREAD_SHOULD_PARK
    >                                                 smpboot_thread_fn
    >                                                 set Task interruptible
    >
    >
    > wake_up_process
    >  if (!(p->state & state))
    >                 goto out;
    >
    >                                                 Kthread_parkme
    >                                                 SET TASK_PARKED
    >                                                 schedule
    >                                                 raw_spin_lock(&rq->lock)
    > ttwu_remote
    > waiting for __task_rq_lock
    >                                                 context_switch
    >
    >                                                 finish_lock_switch
    >
    >
    >
    >                                                 Case TASK_PARKED
    >                                                 kthread_park_complete
    >
    >
    > SET Running
    
    Furthermore, Oleg noticed that the whole scheduler TASK_PARKED
    handling is buggered because the TASK_DEAD thing is done with
    preemption disabled, the current code can still complete early on
    preemption :/
    
    So basically revert that earlier fix and go with a variant of the
    alternative mentioned in the commit. Promote TASK_PARKED to special
    state to avoid the store-store issue on task->state leading to the
    WARN in kthread_unpark() -> __kthread_bind().
    
    But in addition, add wait_task_inactive() to kthread_park() to ensure
    the task really is PARKED when we return from kthread_park(). This
    avoids the whole kthread still gets migrated nonsense -- although it
    would be really good to get this done differently.
    
    Reported-by: Gaurav Kohli <gkohli@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 85f1abe0019f ("kthread, sched/wait: Fix kthread_parkme() completion issue")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 481951bf091d..750cb8082694 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -177,9 +177,20 @@ void *kthread_probe_data(struct task_struct *task)
 static void __kthread_parkme(struct kthread *self)
 {
 	for (;;) {
-		set_current_state(TASK_PARKED);
+		/*
+		 * TASK_PARKED is a special state; we must serialize against
+		 * possible pending wakeups to avoid store-store collisions on
+		 * task->state.
+		 *
+		 * Such a collision might possibly result in the task state
+		 * changin from TASK_PARKED and us failing the
+		 * wait_task_inactive() in kthread_park().
+		 */
+		set_special_state(TASK_PARKED);
 		if (!test_bit(KTHREAD_SHOULD_PARK, &self->flags))
 			break;
+
+		complete_all(&self->parked);
 		schedule();
 	}
 	__set_current_state(TASK_RUNNING);
@@ -191,11 +202,6 @@ void kthread_parkme(void)
 }
 EXPORT_SYMBOL_GPL(kthread_parkme);
 
-void kthread_park_complete(struct task_struct *k)
-{
-	complete_all(&to_kthread(k)->parked);
-}
-
 static int kthread(void *_create)
 {
 	/* Copy data: it's on kthread's stack */
@@ -461,6 +467,9 @@ void kthread_unpark(struct task_struct *k)
 
 	reinit_completion(&kthread->parked);
 	clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
+	/*
+	 * __kthread_parkme() will either see !SHOULD_PARK or get the wakeup.
+	 */
 	wake_up_state(k, TASK_PARKED);
 }
 EXPORT_SYMBOL_GPL(kthread_unpark);
@@ -487,7 +496,16 @@ int kthread_park(struct task_struct *k)
 	set_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
 	if (k != current) {
 		wake_up_process(k);
+		/*
+		 * Wait for __kthread_parkme() to complete(), this means we
+		 * _will_ have TASK_PARKED and are about to call schedule().
+		 */
 		wait_for_completion(&kthread->parked);
+		/*
+		 * Now wait for that schedule() to complete and the task to
+		 * get scheduled out.
+		 */
+		WARN_ON_ONCE(!wait_task_inactive(k, TASK_PARKED));
 	}
 
 	return 0;

commit b1f5b378e126133521df668379249fb8265121f1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 4 11:11:42 2018 +0200

    kthread: Allow kthread_park() on a parked kthread
    
    The following commit:
    
      85f1abe0019f ("kthread, sched/wait: Fix kthread_parkme() completion issue")
    
    added a WARN() in the case where we call kthread_park() on an already
    parked thread, because the old code wasn't doing the right thing there
    and it wasn't at all clear that would happen.
    
    It turns out, this does in fact happen, so we have to deal with it.
    
    Instead of potentially returning early, also wait for the completion.
    This does however mean we have to use complete_all() and re-initialize
    the completion on re-use.
    
    Reported-by: LKP <lkp@01.org>
    Tested-by: Meelis Roos <mroos@linux.ee>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: kernel test robot <lkp@intel.com>
    Cc: wfg@linux.intel.com
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 85f1abe0019f ("kthread, sched/wait: Fix kthread_parkme() completion issue")
    Link: http://lkml.kernel.org/r/20180504091142.GI12235@hirez.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 2017a39ab490..481951bf091d 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -193,7 +193,7 @@ EXPORT_SYMBOL_GPL(kthread_parkme);
 
 void kthread_park_complete(struct task_struct *k)
 {
-	complete(&to_kthread(k)->parked);
+	complete_all(&to_kthread(k)->parked);
 }
 
 static int kthread(void *_create)
@@ -459,6 +459,7 @@ void kthread_unpark(struct task_struct *k)
 	if (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))
 		__kthread_bind(k, kthread->cpu, TASK_PARKED);
 
+	reinit_completion(&kthread->parked);
 	clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
 	wake_up_state(k, TASK_PARKED);
 }
@@ -483,9 +484,6 @@ int kthread_park(struct task_struct *k)
 	if (WARN_ON(k->flags & PF_EXITING))
 		return -ENOSYS;
 
-	if (WARN_ON_ONCE(test_bit(KTHREAD_SHOULD_PARK, &kthread->flags)))
-		return -EBUSY;
-
 	set_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
 	if (k != current) {
 		wake_up_process(k);

commit 85f1abe0019fcb3ea10df7029056cf42702283a8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 1 18:14:45 2018 +0200

    kthread, sched/wait: Fix kthread_parkme() completion issue
    
    Even with the wait-loop fixed, there is a further issue with
    kthread_parkme(). Upon hotplug, when we do takedown_cpu(),
    smpboot_park_threads() can return before all those threads are in fact
    blocked, due to the placement of the complete() in __kthread_parkme().
    
    When that happens, sched_cpu_dying() -> migrate_tasks() can end up
    migrating such a still runnable task onto another CPU.
    
    Normally the task will have hit schedule() and gone to sleep by the
    time we do kthread_unpark(), which will then do __kthread_bind() to
    re-bind the task to the correct CPU.
    
    However, when we loose the initial TASK_PARKED store to the concurrent
    wakeup issue described previously, do the complete(), get migrated, it
    is possible to either:
    
     - observe kthread_unpark()'s clearing of SHOULD_PARK and terminate
       the park and set TASK_RUNNING, or
    
     - __kthread_bind()'s wait_task_inactive() to observe the competing
       TASK_RUNNING store.
    
    Either way the WARN() in __kthread_bind() will trigger and fail to
    correctly set the CPU affinity.
    
    Fix this by only issuing the complete() when the kthread has scheduled
    out. This does away with all the icky 'still running' nonsense.
    
    The alternative is to promote TASK_PARKED to a special state, this
    guarantees wait_task_inactive() cannot observe a 'stale' TASK_RUNNING
    and we'll end up doing the right thing, but this preserves the whole
    icky business of potentially migating the still runnable thing.
    
    Reported-by: Gaurav Kohli <gkohli@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index cbee858e5815..2017a39ab490 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -55,7 +55,6 @@ enum KTHREAD_BITS {
 	KTHREAD_IS_PER_CPU = 0,
 	KTHREAD_SHOULD_STOP,
 	KTHREAD_SHOULD_PARK,
-	KTHREAD_IS_PARKED,
 };
 
 static inline void set_kthread_struct(void *kthread)
@@ -181,11 +180,8 @@ static void __kthread_parkme(struct kthread *self)
 		set_current_state(TASK_PARKED);
 		if (!test_bit(KTHREAD_SHOULD_PARK, &self->flags))
 			break;
-		if (!test_and_set_bit(KTHREAD_IS_PARKED, &self->flags))
-			complete(&self->parked);
 		schedule();
 	}
-	clear_bit(KTHREAD_IS_PARKED, &self->flags);
 	__set_current_state(TASK_RUNNING);
 }
 
@@ -195,6 +191,11 @@ void kthread_parkme(void)
 }
 EXPORT_SYMBOL_GPL(kthread_parkme);
 
+void kthread_park_complete(struct task_struct *k)
+{
+	complete(&to_kthread(k)->parked);
+}
+
 static int kthread(void *_create)
 {
 	/* Copy data: it's on kthread's stack */
@@ -451,22 +452,15 @@ void kthread_unpark(struct task_struct *k)
 {
 	struct kthread *kthread = to_kthread(k);
 
-	clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
 	/*
-	 * We clear the IS_PARKED bit here as we don't wait
-	 * until the task has left the park code. So if we'd
-	 * park before that happens we'd see the IS_PARKED bit
-	 * which might be about to be cleared.
+	 * Newly created kthread was parked when the CPU was offline.
+	 * The binding was lost and we need to set it again.
 	 */
-	if (test_and_clear_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
-		/*
-		 * Newly created kthread was parked when the CPU was offline.
-		 * The binding was lost and we need to set it again.
-		 */
-		if (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))
-			__kthread_bind(k, kthread->cpu, TASK_PARKED);
-		wake_up_state(k, TASK_PARKED);
-	}
+	if (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))
+		__kthread_bind(k, kthread->cpu, TASK_PARKED);
+
+	clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
+	wake_up_state(k, TASK_PARKED);
 }
 EXPORT_SYMBOL_GPL(kthread_unpark);
 
@@ -489,12 +483,13 @@ int kthread_park(struct task_struct *k)
 	if (WARN_ON(k->flags & PF_EXITING))
 		return -ENOSYS;
 
-	if (!test_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
-		set_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
-		if (k != current) {
-			wake_up_process(k);
-			wait_for_completion(&kthread->parked);
-		}
+	if (WARN_ON_ONCE(test_bit(KTHREAD_SHOULD_PARK, &kthread->flags)))
+		return -EBUSY;
+
+	set_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
+	if (k != current) {
+		wake_up_process(k);
+		wait_for_completion(&kthread->parked);
 	}
 
 	return 0;

commit 741a76b350897604c48fb12beff1c9b77724dc96
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Apr 30 14:50:22 2018 +0200

    kthread, sched/wait: Fix kthread_parkme() wait-loop
    
    Gaurav reported a problem with __kthread_parkme() where a concurrent
    try_to_wake_up() could result in competing stores to ->state which,
    when the TASK_PARKED store got lost bad things would happen.
    
    The comment near set_current_state() actually mentions this competing
    store, but only mentions the case against TASK_RUNNING. This same
    store, with different timing, can happen against a subsequent !RUNNING
    store.
    
    This normally is not a problem, because as per that same comment, the
    !RUNNING state store is inside a condition based wait-loop:
    
      for (;;) {
        set_current_state(TASK_UNINTERRUPTIBLE);
        if (!need_sleep)
          break;
        schedule();
      }
      __set_current_state(TASK_RUNNING);
    
    If we loose the (first) TASK_UNINTERRUPTIBLE store to a previous
    (concurrent) wakeup, the schedule() will NO-OP and we'll go around the
    loop once more.
    
    The problem here is that the TASK_PARKED store is not inside the
    KTHREAD_SHOULD_PARK condition wait-loop.
    
    There is a genuine issue with sleeps that do not have a condition;
    this is addressed in a subsequent patch.
    
    Reported-by: Gaurav Kohli <gkohli@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index cd50e99202b0..cbee858e5815 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -177,12 +177,13 @@ void *kthread_probe_data(struct task_struct *task)
 
 static void __kthread_parkme(struct kthread *self)
 {
-	__set_current_state(TASK_PARKED);
-	while (test_bit(KTHREAD_SHOULD_PARK, &self->flags)) {
+	for (;;) {
+		set_current_state(TASK_PARKED);
+		if (!test_bit(KTHREAD_SHOULD_PARK, &self->flags))
+			break;
 		if (!test_and_set_bit(KTHREAD_IS_PARKED, &self->flags))
 			complete(&self->parked);
 		schedule();
-		__set_current_state(TASK_PARKED);
 	}
 	clear_bit(KTHREAD_IS_PARKED, &self->flags);
 	__set_current_state(TASK_RUNNING);

commit 841b86f3289dbe858daeceec36423d4ea286fac2
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Oct 23 09:40:42 2017 +0200

    treewide: Remove TIMER_FUNC_TYPE and TIMER_DATA_TYPE casts
    
    With all callbacks converted, and the timer callback prototype
    switched over, the TIMER_FUNC_TYPE cast is no longer needed,
    so remove it. Conversion was done with the following scripts:
    
        perl -pi -e 's|\(TIMER_FUNC_TYPE\)||g' \
            $(git grep TIMER_FUNC_TYPE | cut -d: -f1 | sort -u)
    
        perl -pi -e 's|\(TIMER_DATA_TYPE\)||g' \
            $(git grep TIMER_DATA_TYPE | cut -d: -f1 | sort -u)
    
    The now unused macros are also dropped from include/linux/timer.h.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 8af313081b0d..cd50e99202b0 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -843,7 +843,7 @@ void __kthread_queue_delayed_work(struct kthread_worker *worker,
 	struct timer_list *timer = &dwork->timer;
 	struct kthread_work *work = &dwork->work;
 
-	WARN_ON_ONCE(timer->function != (TIMER_FUNC_TYPE)kthread_delayed_work_timer_fn);
+	WARN_ON_ONCE(timer->function != kthread_delayed_work_timer_fn);
 
 	/*
 	 * If @delay is 0, queue @dwork->work immediately.  This is for

commit e2c5923c349c1738fe8fda980874d93f6fb2e5b6
Merge: abc36be23635 a04b5de5050a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 14 15:32:19 2017 -0800

    Merge branch 'for-4.15/block' of git://git.kernel.dk/linux-block
    
    Pull core block layer updates from Jens Axboe:
     "This is the main pull request for block storage for 4.15-rc1.
    
      Nothing out of the ordinary in here, and no API changes or anything
      like that. Just various new features for drivers, core changes, etc.
      In particular, this pull request contains:
    
       - A patch series from Bart, closing the whole on blk/scsi-mq queue
         quescing.
    
       - A series from Christoph, building towards hidden gendisks (for
         multipath) and ability to move bio chains around.
    
       - NVMe
            - Support for native multipath for NVMe (Christoph).
            - Userspace notifications for AENs (Keith).
            - Command side-effects support (Keith).
            - SGL support (Chaitanya Kulkarni)
            - FC fixes and improvements (James Smart)
            - Lots of fixes and tweaks (Various)
    
       - bcache
            - New maintainer (Michael Lyle)
            - Writeback control improvements (Michael)
            - Various fixes (Coly, Elena, Eric, Liang, et al)
    
       - lightnvm updates, mostly centered around the pblk interface
         (Javier, Hans, and Rakesh).
    
       - Removal of unused bio/bvec kmap atomic interfaces (me, Christoph)
    
       - Writeback series that fix the much discussed hundreds of millions
         of sync-all units. This goes all the way, as discussed previously
         (me).
    
       - Fix for missing wakeup on writeback timer adjustments (Yafang
         Shao).
    
       - Fix laptop mode on blk-mq (me).
    
       - {mq,name} tupple lookup for IO schedulers, allowing us to have
         alias names. This means you can use 'deadline' on both !mq and on
         mq (where it's called mq-deadline). (me).
    
       - blktrace race fix, oopsing on sg load (me).
    
       - blk-mq optimizations (me).
    
       - Obscure waitqueue race fix for kyber (Omar).
    
       - NBD fixes (Josef).
    
       - Disable writeback throttling by default on bfq, like we do on cfq
         (Luca Miccio).
    
       - Series from Ming that enable us to treat flush requests on blk-mq
         like any other request. This is a really nice cleanup.
    
       - Series from Ming that improves merging on blk-mq with schedulers,
         getting us closer to flipping the switch on scsi-mq again.
    
       - BFQ updates (Paolo).
    
       - blk-mq atomic flags memory ordering fixes (Peter Z).
    
       - Loop cgroup support (Shaohua).
    
       - Lots of minor fixes from lots of different folks, both for core and
         driver code"
    
    * 'for-4.15/block' of git://git.kernel.dk/linux-block: (294 commits)
      nvme: fix visibility of "uuid" ns attribute
      blk-mq: fixup some comment typos and lengths
      ide: ide-atapi: fix compile error with defining macro DEBUG
      blk-mq: improve tag waiting setup for non-shared tags
      brd: remove unused brd_mutex
      blk-mq: only run the hardware queue if IO is pending
      block: avoid null pointer dereference on null disk
      fs: guard_bio_eod() needs to consider partitions
      xtensa/simdisk: fix compile error
      nvme: expose subsys attribute to sysfs
      nvme: create 'slaves' and 'holders' entries for hidden controllers
      block: create 'slaves' and 'holders' entries for hidden gendisks
      nvme: also expose the namespace identification sysfs files for mpath nodes
      nvme: implement multipath access to nvme subsystems
      nvme: track shared namespaces
      nvme: introduce a nvme_ns_ids structure
      nvme: track subsystems
      block, nvme: Introduce blk_mq_req_flags_t
      block, scsi: Make SCSI quiesce and resume work reliably
      block: Add the QUEUE_FLAG_PREEMPT_ONLY request queue flag
      ...

commit e10237cc76ef9a4066a84aa2cc710bfd708cc341
Author: Shaohua Li <shli@fb.com>
Date:   Tue Nov 7 11:09:50 2017 -0800

    kthread: zero the kthread data structure
    
    kthread() could bail out early before we initialize blkcg_css (if the
    kthread is killed very early. Please see xchg() statement in kthread()),
    which confuses free_kthread_struct. Instead of moving the blkcg_css
    initialization early, we simply zero the whole 'self' data structure,
    which doesn't sound much overhead.
    
    Reported-by: syzbot <syzkaller@googlegroups.com>
    Fixes: 05e3db95ebfc ("kthread: add a mechanism to store cgroup info")
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index f87cd8b4eb2a..8dbe2454cb1d 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -204,7 +204,7 @@ static int kthread(void *_create)
 	struct kthread *self;
 	int ret;
 
-	self = kmalloc(sizeof(*self), GFP_KERNEL);
+	self = kzalloc(sizeof(*self), GFP_KERNEL);
 	set_kthread_struct(self);
 
 	/* If user was SIGKILLed, I release the structure. */
@@ -220,13 +220,9 @@ static int kthread(void *_create)
 		do_exit(-ENOMEM);
 	}
 
-	self->flags = 0;
 	self->data = data;
 	init_completion(&self->exited);
 	init_completion(&self->parked);
-#ifdef CONFIG_BLK_CGROUP
-	self->blkcg_css = NULL;
-#endif
 	current->vfork_done = &self->exited;
 
 	/* OK, tell user we're spawned, wait for stop or wakeup */

commit fe5c3b69b540e3387223a696f327c1bb8880d1ac
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Oct 4 16:27:06 2017 -0700

    kthread: Convert callback to use from_timer()
    
    In preparation for unconditionally passing the struct timer_list pointer
    to all timer callbacks, switch kthread to use from_timer() and pass the
    timer pointer explicitly.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mips@linux-mips.org
    Cc: Len Brown <len.brown@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Sebastian Reichel <sre@kernel.org>
    Cc: Kalle Valo <kvalo@qca.qualcomm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: linux1394-devel@lists.sourceforge.net
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: linux-s390@vger.kernel.org
    Cc: linux-wireless@vger.kernel.org
    Cc: "James E.J. Bottomley" <jejb@linux.vnet.ibm.com>
    Cc: Wim Van Sebroeck <wim@iguana.be>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Ursula Braun <ubraun@linux.vnet.ibm.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Harish Patil <harish.patil@cavium.com>
    Cc: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: Manish Chopra <manish.chopra@cavium.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: linux-pm@vger.kernel.org
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Julian Wiedmann <jwi@linux.vnet.ibm.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Mark Gross <mark.gross@intel.com>
    Cc: linux-watchdog@vger.kernel.org
    Cc: linux-scsi@vger.kernel.org
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Stefan Richter <stefanr@s5r6.in-berlin.de>
    Cc: Michael Reed <mdr@sgi.com>
    Cc: netdev@vger.kernel.org
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Sudip Mukherjee <sudipm.mukherjee@gmail.com>
    Link: https://lkml.kernel.org/r/1507159627-127660-13-git-send-email-keescook@chromium.org

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 1c19edf82427..ba3992c8c375 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -798,15 +798,14 @@ EXPORT_SYMBOL_GPL(kthread_queue_work);
 /**
  * kthread_delayed_work_timer_fn - callback that queues the associated kthread
  *	delayed work when the timer expires.
- * @__data: pointer to the data associated with the timer
+ * @t: pointer to the expired timer
  *
  * The format of the function is defined by struct timer_list.
  * It should have been called from irqsafe timer with irq already off.
  */
-void kthread_delayed_work_timer_fn(unsigned long __data)
+void kthread_delayed_work_timer_fn(struct timer_list *t)
 {
-	struct kthread_delayed_work *dwork =
-		(struct kthread_delayed_work *)__data;
+	struct kthread_delayed_work *dwork = from_timer(dwork, t, timer);
 	struct kthread_work *work = &dwork->work;
 	struct kthread_worker *worker = work->worker;
 
@@ -837,8 +836,7 @@ void __kthread_queue_delayed_work(struct kthread_worker *worker,
 	struct timer_list *timer = &dwork->timer;
 	struct kthread_work *work = &dwork->work;
 
-	WARN_ON_ONCE(timer->function != kthread_delayed_work_timer_fn ||
-		     timer->data != (unsigned long)dwork);
+	WARN_ON_ONCE(timer->function != (TIMER_FUNC_TYPE)kthread_delayed_work_timer_fn);
 
 	/*
 	 * If @delay is 0, queue @dwork->work immediately.  This is for

commit 0b508bc926bdced678febee2a2b8cdba0a19e481
Author: Shaohua Li <shli@fb.com>
Date:   Tue Sep 26 11:02:12 2017 -0700

    block: fix a build error
    
    The code is only for blkcg not for all cgroups
    
    Fixes: d4478e92d618 ("block/loop: make loop cgroup aware")
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index b011ea08967f..f87cd8b4eb2a 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -46,7 +46,7 @@ struct kthread {
 	void *data;
 	struct completion parked;
 	struct completion exited;
-#ifdef CONFIG_CGROUPS
+#ifdef CONFIG_BLK_CGROUP
 	struct cgroup_subsys_state *blkcg_css;
 #endif
 };
@@ -83,7 +83,7 @@ void free_kthread_struct(struct task_struct *k)
 	 * or if kmalloc() in kthread() failed.
 	 */
 	kthread = to_kthread(k);
-#ifdef CONFIG_CGROUPS
+#ifdef CONFIG_BLK_CGROUP
 	WARN_ON_ONCE(kthread && kthread->blkcg_css);
 #endif
 	kfree(kthread);
@@ -224,7 +224,7 @@ static int kthread(void *_create)
 	self->data = data;
 	init_completion(&self->exited);
 	init_completion(&self->parked);
-#ifdef CONFIG_CGROUPS
+#ifdef CONFIG_BLK_CGROUP
 	self->blkcg_css = NULL;
 #endif
 	current->vfork_done = &self->exited;
@@ -1166,7 +1166,7 @@ void kthread_destroy_worker(struct kthread_worker *worker)
 }
 EXPORT_SYMBOL(kthread_destroy_worker);
 
-#ifdef CONFIG_CGROUPS
+#ifdef CONFIG_BLK_CGROUP
 /**
  * kthread_associate_blkcg - associate blkcg to current kthread
  * @css: the cgroup info

commit 05e3db95ebfc5c06a29a1d8c7a3e02f46f3a25a7
Author: Shaohua Li <shli@fb.com>
Date:   Thu Sep 14 14:02:04 2017 -0700

    kthread: add a mechanism to store cgroup info
    
    kthread usually runs jobs on behalf of other threads. The jobs should be
    charged to cgroup of original threads. But the jobs run in a kthread,
    where we lose the cgroup context of original threads. The patch adds a
    machanism to record cgroup info of original threads in kthread context.
    Later we can retrieve the cgroup info and attach the cgroup info to jobs.
    
    Since this mechanism is only required by kthread, we store the cgroup
    info in kthread data instead of generic task_struct.
    
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 1c19edf82427..b011ea08967f 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -20,7 +20,6 @@
 #include <linux/freezer.h>
 #include <linux/ptrace.h>
 #include <linux/uaccess.h>
-#include <linux/cgroup.h>
 #include <trace/events/sched.h>
 
 static DEFINE_SPINLOCK(kthread_create_lock);
@@ -47,6 +46,9 @@ struct kthread {
 	void *data;
 	struct completion parked;
 	struct completion exited;
+#ifdef CONFIG_CGROUPS
+	struct cgroup_subsys_state *blkcg_css;
+#endif
 };
 
 enum KTHREAD_BITS {
@@ -74,11 +76,17 @@ static inline struct kthread *to_kthread(struct task_struct *k)
 
 void free_kthread_struct(struct task_struct *k)
 {
+	struct kthread *kthread;
+
 	/*
 	 * Can be NULL if this kthread was created by kernel_thread()
 	 * or if kmalloc() in kthread() failed.
 	 */
-	kfree(to_kthread(k));
+	kthread = to_kthread(k);
+#ifdef CONFIG_CGROUPS
+	WARN_ON_ONCE(kthread && kthread->blkcg_css);
+#endif
+	kfree(kthread);
 }
 
 /**
@@ -216,6 +224,9 @@ static int kthread(void *_create)
 	self->data = data;
 	init_completion(&self->exited);
 	init_completion(&self->parked);
+#ifdef CONFIG_CGROUPS
+	self->blkcg_css = NULL;
+#endif
 	current->vfork_done = &self->exited;
 
 	/* OK, tell user we're spawned, wait for stop or wakeup */
@@ -1154,3 +1165,54 @@ void kthread_destroy_worker(struct kthread_worker *worker)
 	kfree(worker);
 }
 EXPORT_SYMBOL(kthread_destroy_worker);
+
+#ifdef CONFIG_CGROUPS
+/**
+ * kthread_associate_blkcg - associate blkcg to current kthread
+ * @css: the cgroup info
+ *
+ * Current thread must be a kthread. The thread is running jobs on behalf of
+ * other threads. In some cases, we expect the jobs attach cgroup info of
+ * original threads instead of that of current thread. This function stores
+ * original thread's cgroup info in current kthread context for later
+ * retrieval.
+ */
+void kthread_associate_blkcg(struct cgroup_subsys_state *css)
+{
+	struct kthread *kthread;
+
+	if (!(current->flags & PF_KTHREAD))
+		return;
+	kthread = to_kthread(current);
+	if (!kthread)
+		return;
+
+	if (kthread->blkcg_css) {
+		css_put(kthread->blkcg_css);
+		kthread->blkcg_css = NULL;
+	}
+	if (css) {
+		css_get(css);
+		kthread->blkcg_css = css;
+	}
+}
+EXPORT_SYMBOL(kthread_associate_blkcg);
+
+/**
+ * kthread_blkcg - get associated blkcg css of current kthread
+ *
+ * Current thread must be a kthread.
+ */
+struct cgroup_subsys_state *kthread_blkcg(void)
+{
+	struct kthread *kthread;
+
+	if (current->flags & PF_KTHREAD) {
+		kthread = to_kthread(current);
+		if (kthread)
+			return kthread->blkcg_css;
+	}
+	return NULL;
+}
+EXPORT_SYMBOL(kthread_blkcg);
+#endif

commit 22cf8bc6cb0d48c6d31da8eccbd4201ab9b0c4c6
Author: Shaohua Li <shli@fb.com>
Date:   Thu Aug 31 16:15:23 2017 -0700

    kernel/kthread.c: kthread_worker: don't hog the cpu
    
    If the worker thread continues getting work, it will hog the cpu and rcu
    stall complains.  Make it a good citizen.  This is triggered in a loop
    block device test.
    
    Link: http://lkml.kernel.org/r/5de0a179b3184e1a2183fc503448b0269f24d75b.1503697127.git.shli@fb.com
    Signed-off-by: Shaohua Li <shli@fb.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 26db528c1d88..1c19edf82427 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -637,6 +637,7 @@ int kthread_worker_fn(void *worker_ptr)
 		schedule();
 
 	try_to_freeze();
+	cond_resched();
 	goto repeat;
 }
 EXPORT_SYMBOL_GPL(kthread_worker_fn);

commit 77f88796cee819b9c4562b0b6b44691b3b7755b1
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Mar 16 16:54:24 2017 -0400

    cgroup, kthread: close race window where new kthreads can be migrated to non-root cgroups
    
    Creation of a kthread goes through a couple interlocked stages between
    the kthread itself and its creator.  Once the new kthread starts
    running, it initializes itself and wakes up the creator.  The creator
    then can further configure the kthread and then let it start doing its
    job by waking it up.
    
    In this configuration-by-creator stage, the creator is the only one
    that can wake it up but the kthread is visible to userland.  When
    altering the kthread's attributes from userland is allowed, this is
    fine; however, for cases where CPU affinity is critical,
    kthread_bind() is used to first disable affinity changes from userland
    and then set the affinity.  This also prevents the kthread from being
    migrated into non-root cgroups as that can affect the CPU affinity and
    many other things.
    
    Unfortunately, the cgroup side of protection is racy.  While the
    PF_NO_SETAFFINITY flag prevents further migrations, userland can win
    the race before the creator sets the flag with kthread_bind() and put
    the kthread in a non-root cgroup, which can lead to all sorts of
    problems including incorrect CPU affinity and starvation.
    
    This bug got triggered by userland which periodically tries to migrate
    all processes in the root cpuset cgroup to a non-root one.  Per-cpu
    workqueue workers got caught while being created and ended up with
    incorrected CPU affinity breaking concurrency management and sometimes
    stalling workqueue execution.
    
    This patch adds task->no_cgroup_migration which disallows the task to
    be migrated by userland.  kthreadd starts with the flag set making
    every child kthread start in the root cgroup with migration
    disallowed.  The flag is cleared after the kthread finishes
    initialization by which time PF_NO_SETAFFINITY is set if the kthread
    should stay in the root cgroup.
    
    It'd be better to wait for the initialization instead of failing but I
    couldn't think of a way of implementing that without adding either a
    new PF flag, or sleeping and retrying from waiting side.  Even if
    userland depends on changing cgroup membership of a kthread, it either
    has to be synchronized with kthread_create() or periodically repeat,
    so it's unlikely that this would break anything.
    
    v2: Switch to a simpler implementation using a new task_struct bit
        field suggested by Oleg.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Reported-and-debugged-by: Chris Mason <clm@fb.com>
    Cc: stable@vger.kernel.org # v4.3+ (we can't close the race on < v4.3)
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 2f26adea0f84..26db528c1d88 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -20,6 +20,7 @@
 #include <linux/freezer.h>
 #include <linux/ptrace.h>
 #include <linux/uaccess.h>
+#include <linux/cgroup.h>
 #include <trace/events/sched.h>
 
 static DEFINE_SPINLOCK(kthread_create_lock);
@@ -225,6 +226,7 @@ static int kthread(void *_create)
 
 	ret = -EINTR;
 	if (!test_bit(KTHREAD_SHOULD_STOP, &self->flags)) {
+		cgroup_kthread_ready();
 		__kthread_parkme(self);
 		ret = threadfn(data);
 	}
@@ -538,6 +540,7 @@ int kthreadd(void *unused)
 	set_mems_allowed(node_states[N_MEMORY]);
 
 	current->flags |= PF_NOFREEZE;
+	cgroup_init_kthreadd();
 
 	for (;;) {
 		set_current_state(TASK_INTERRUPTIBLE);

commit 299300258d1bc4e997b7db340a2e06636757fe2e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task.h>
    
    We are going to split <linux/sched/task.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index ef9b9eb809c7..2f26adea0f84 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -7,6 +7,7 @@
  */
 #include <uapi/linux/sched/types.h>
 #include <linux/sched.h>
+#include <linux/sched/task.h>
 #include <linux/kthread.h>
 #include <linux/completion.h>
 #include <linux/err.h>

commit ae7e81c077d60507dcec139e40a6d10cf932cf4b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 18:07:51 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <uapi/linux/sched/types.h>
    
    We are going to move scheduler ABI details to <uapi/linux/sched/types.h>,
    which will be used from a number of .c files.
    
    Create empty placeholder header that maps to <linux/types.h>.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 8461a4372e8a..ef9b9eb809c7 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -5,6 +5,7 @@
  * even if we're invoked from userspace (think modprobe, hotplug cpu,
  * etc.).
  */
+#include <uapi/linux/sched/types.h>
 #include <linux/sched.h>
 #include <linux/kthread.h>
 #include <linux/completion.h>

commit dfb4357da6ddbdf57d583ba64361c9d792b0e0b1
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Feb 8 11:26:59 2017 -0800

    time: Remove CONFIG_TIMER_STATS
    
    Currently CONFIG_TIMER_STATS exposes process information across namespaces:
    
    kernel/time/timer_list.c print_timer():
    
            SEQ_printf(m, ", %s/%d", tmp, timer->start_pid);
    
    /proc/timer_list:
    
     #11: <0000000000000000>, hrtimer_wakeup, S:01, do_nanosleep, cron/2570
    
    Given that the tracer can give the same information, this patch entirely
    removes CONFIG_TIMER_STATS.
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: linux-doc@vger.kernel.org
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Shuah Khan <shuah@kernel.org>
    Cc: Xing Gao <xgao01@email.wm.edu>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Jessica Frazelle <me@jessfraz.com>
    Cc: kernel-hardening@lists.openwall.com
    Cc: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Michal Marek <mmarek@suse.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Olof Johansson <olof@lixom.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-api@vger.kernel.org
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Link: http://lkml.kernel.org/r/20170208192659.GA32582@beast
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 2318fba86277..8461a4372e8a 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -850,7 +850,6 @@ void __kthread_queue_delayed_work(struct kthread_worker *worker,
 
 	list_add(&work->node, &worker->delayed_work_list);
 	work->worker = worker;
-	timer_stats_timer_set_start_info(&dwork->timer);
 	timer->expires = jiffies + delay;
 	add_timer(timer);
 }

commit c0b942a76361e08fc9fb17989e0f266e64ff0688
Author: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
Date:   Mon Dec 12 16:40:39 2016 -0800

    kthread: add __printf attributes
    
    When commit fbae2d44aa1d ("kthread: add kthread_create_worker*()")
    introduced some kthread_create_...() functions which were taking
    printf-like parametter, it introduced __printf attributes to some
    functions (e.g.  kthread_create_worker()).  Nevertheless some new
    functions were forgotten (they have been detected thanks to
    -Wmissing-format-attribute warning flag).
    
    Add the missing __printf attributes to the newly-introduced functions in
    order to detect formatting issues at build-time with -Wformat flag.
    
    Link: http://lkml.kernel.org/r/20161126193543.22672-1-nicolas.iooss_linux@m4x.org
    Signed-off-by: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 956495f0efaf..2318fba86277 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -261,7 +261,8 @@ static void create_kthread(struct kthread_create_info *create)
 	}
 }
 
-static struct task_struct *__kthread_create_on_node(int (*threadfn)(void *data),
+static __printf(4, 0)
+struct task_struct *__kthread_create_on_node(int (*threadfn)(void *data),
 						    void *data, int node,
 						    const char namefmt[],
 						    va_list args)
@@ -635,7 +636,7 @@ int kthread_worker_fn(void *worker_ptr)
 }
 EXPORT_SYMBOL_GPL(kthread_worker_fn);
 
-static struct kthread_worker *
+static __printf(3, 0) struct kthread_worker *
 __kthread_create_worker(int cpu, unsigned int flags,
 			const char namefmt[], va_list args)
 {

commit 8fb9dcbdc3619741c10c573199d804161c34c89a
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Nov 29 18:51:10 2016 +0100

    kthread: Don't abuse kthread_create_on_cpu() in __kthread_create_worker()
    
    kthread_create_on_cpu() sets KTHREAD_IS_PER_CPU and kthread->cpu, this
    only makes sense if this kthread can be parked/unparked by cpuhp code.
    kthread workers never call kthread_parkme() so this has no effect.
    
    Change __kthread_create_worker() to simply call kthread_bind(task, cpu).
    The very fact that kthread_create_on_cpu() doesn't accept a generic fmt
    shows that it should not be used outside of smpboot.c.
    
    Now, the only reason we can not unexport this helper and move it into
    smpboot.c is that it sets kthread->cpu and struct kthread is not exported.
    And the only reason we can not kill kthread->cpu is that kthread_unpark()
    is used by drivers/gpu/drm/amd/scheduler/gpu_scheduler.c and thus we can
    not turn _unpark into kthread_unpark(struct smp_hotplug_thread *, cpu).
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Tested-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Cc: Chunming Zhou <David1.Zhou@amd.com>
    Cc: Roman Pen <roman.penyaev@profitbricks.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20161129175110.GA5342@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 01d27164e5b7..956495f0efaf 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -641,6 +641,7 @@ __kthread_create_worker(int cpu, unsigned int flags,
 {
 	struct kthread_worker *worker;
 	struct task_struct *task;
+	int node = -1;
 
 	worker = kzalloc(sizeof(*worker), GFP_KERNEL);
 	if (!worker)
@@ -648,25 +649,17 @@ __kthread_create_worker(int cpu, unsigned int flags,
 
 	kthread_init_worker(worker);
 
-	if (cpu >= 0) {
-		char name[TASK_COMM_LEN];
-
-		/*
-		 * kthread_create_worker_on_cpu() allows to pass a generic
-		 * namefmt in compare with kthread_create_on_cpu. We need
-		 * to format it here.
-		 */
-		vsnprintf(name, sizeof(name), namefmt, args);
-		task = kthread_create_on_cpu(kthread_worker_fn, worker,
-					     cpu, name);
-	} else {
-		task = __kthread_create_on_node(kthread_worker_fn, worker,
-						-1, namefmt, args);
-	}
+	if (cpu >= 0)
+		node = cpu_to_node(cpu);
 
+	task = __kthread_create_on_node(kthread_worker_fn, worker,
+						node, namefmt, args);
 	if (IS_ERR(task))
 		goto fail_task;
 
+	if (cpu >= 0)
+		kthread_bind(task, cpu);
+
 	worker->flags = flags;
 	worker->task = task;
 	wake_up_process(task);

commit cf380a4a96e2260742051fa7fc831596bb26cc8b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Nov 29 18:51:07 2016 +0100

    kthread: Don't use to_live_kthread() in kthread_[un]park()
    
    Now that to_kthread() is always validm change kthread_park() and
    kthread_unpark() to use it and kill to_live_kthread().
    
    The conversion of kthread_unpark() is trivial. If KTHREAD_IS_PARKED is set
    then the task has called complete(&self->parked) and there the function
    cannot race against a concurrent kthread_stop() and exit.
    
    kthread_park() is more tricky, because its semantics are not well
    defined. It returns -ENOSYS if the thread exited but this can never happen
    and as Roman pointed out kthread_park() can obviously block forever if it
    would race with the exiting kthread.
    
    The usage of kthread_park() in cpuhp code (cpu.c, smpboot.c, stop_machine.c)
    is fine. It can never see an exiting/exited kthread, smpboot_destroy_threads()
    clears *ht->store, smpboot_park_thread() checks it is not NULL under the same
    smpboot_threads_lock. cpuhp_threads and cpu_stop_threads never exit, so other
    callers are fine too.
    
    But it has two more users:
    
    - watchdog_park_threads():
    
      The code is actually correct, get_online_cpus() ensures that
      kthread_park() can't race with itself (note that kthread_park() can't
      handle this race correctly), but it should not use kthread_park()
      directly.
    
    - drivers/gpu/drm/amd/scheduler/gpu_scheduler.c should not use
      kthread_park() either.
    
      kthread_park() must not be called after amd_sched_fini() which does
      kthread_stop(), otherwise even to_live_kthread() is not safe because
      task_struct can be already freed and sched->thread can point to nowhere.
    
    The usage of kthread_park/unpark should either be restricted to core code
    which is properly protected against the exit race or made more robust so it
    is safe to use it in drivers.
    
    To catch eventual exit issues, add a WARN_ON(PF_EXITING) for now.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chunming Zhou <David1.Zhou@amd.com>
    Cc: Roman Pen <roman.penyaev@profitbricks.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20161129175107.GA5339@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 4dcbc8b5d6b6..01d27164e5b7 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -78,21 +78,6 @@ void free_kthread_struct(struct task_struct *k)
 	kfree(to_kthread(k));
 }
 
-#define __to_kthread(vfork)	\
-	container_of(vfork, struct kthread, exited)
-
-/*
- * TODO: kill it and use to_kthread(). But we still need the users
- * like kthread_stop() which has to sync with the exiting kthread.
- */
-static struct kthread *to_live_kthread(struct task_struct *k)
-{
-	struct completion *vfork = ACCESS_ONCE(k->vfork_done);
-	if (likely(vfork))
-		return __to_kthread(vfork);
-	return NULL;
-}
-
 /**
  * kthread_should_stop - should this kthread return now?
  *
@@ -441,8 +426,18 @@ struct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),
 	return p;
 }
 
-static void __kthread_unpark(struct task_struct *k, struct kthread *kthread)
+/**
+ * kthread_unpark - unpark a thread created by kthread_create().
+ * @k:		thread created by kthread_create().
+ *
+ * Sets kthread_should_park() for @k to return false, wakes it, and
+ * waits for it to return. If the thread is marked percpu then its
+ * bound to the cpu again.
+ */
+void kthread_unpark(struct task_struct *k)
 {
+	struct kthread *kthread = to_kthread(k);
+
 	clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
 	/*
 	 * We clear the IS_PARKED bit here as we don't wait
@@ -460,22 +455,6 @@ static void __kthread_unpark(struct task_struct *k, struct kthread *kthread)
 		wake_up_state(k, TASK_PARKED);
 	}
 }
-
-/**
- * kthread_unpark - unpark a thread created by kthread_create().
- * @k:		thread created by kthread_create().
- *
- * Sets kthread_should_park() for @k to return false, wakes it, and
- * waits for it to return. If the thread is marked percpu then its
- * bound to the cpu again.
- */
-void kthread_unpark(struct task_struct *k)
-{
-	struct kthread *kthread = to_live_kthread(k);
-
-	if (kthread)
-		__kthread_unpark(k, kthread);
-}
 EXPORT_SYMBOL_GPL(kthread_unpark);
 
 /**
@@ -492,20 +471,20 @@ EXPORT_SYMBOL_GPL(kthread_unpark);
  */
 int kthread_park(struct task_struct *k)
 {
-	struct kthread *kthread = to_live_kthread(k);
-	int ret = -ENOSYS;
-
-	if (kthread) {
-		if (!test_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
-			set_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
-			if (k != current) {
-				wake_up_process(k);
-				wait_for_completion(&kthread->parked);
-			}
+	struct kthread *kthread = to_kthread(k);
+
+	if (WARN_ON(k->flags & PF_EXITING))
+		return -ENOSYS;
+
+	if (!test_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
+		set_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
+		if (k != current) {
+			wake_up_process(k);
+			wait_for_completion(&kthread->parked);
 		}
-		ret = 0;
 	}
-	return ret;
+
+	return 0;
 }
 EXPORT_SYMBOL_GPL(kthread_park);
 
@@ -534,7 +513,7 @@ int kthread_stop(struct task_struct *k)
 	get_task_struct(k);
 	kthread = to_kthread(k);
 	set_bit(KTHREAD_SHOULD_STOP, &kthread->flags);
-	__kthread_unpark(k, kthread);
+	kthread_unpark(k);
 	wake_up_process(k);
 	wait_for_completion(&kthread->exited);
 	ret = k->exit_code;

commit efb29fbfa50c490dac64a9418ebe553be82df781
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Nov 29 18:51:03 2016 +0100

    kthread: Don't use to_live_kthread() in kthread_stop()
    
    kthread_stop() had to use to_live_kthread() simply because it was not
    possible to access kthread->exited after the exiting task clears
    task_struct->vfork_done. Now that to_kthread() is always valid,
    wake_up_process() + wait_for_completion() can be done
    ununconditionally. It's not an issue anymore if the task has already issued
    complete_vfork_done() or died.
    
    The exiting task can get the spurious wakeup after mm_release() but this is
    possible without this change too and is fine; do_task_dead() ensures that
    this can't make any harm.
    
    As a further enhancement this could be converted to task_work_add() later,
    so ->vfork_done can be avoided completely.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chunming Zhou <David1.Zhou@amd.com>
    Cc: Roman Pen <roman.penyaev@profitbricks.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20161129175103.GA5336@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 7891a940007d..4dcbc8b5d6b6 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -532,13 +532,11 @@ int kthread_stop(struct task_struct *k)
 	trace_sched_kthread_stop(k);
 
 	get_task_struct(k);
-	kthread = to_live_kthread(k);
-	if (kthread) {
-		set_bit(KTHREAD_SHOULD_STOP, &kthread->flags);
-		__kthread_unpark(k, kthread);
-		wake_up_process(k);
-		wait_for_completion(&kthread->exited);
-	}
+	kthread = to_kthread(k);
+	set_bit(KTHREAD_SHOULD_STOP, &kthread->flags);
+	__kthread_unpark(k, kthread);
+	wake_up_process(k);
+	wait_for_completion(&kthread->exited);
 	ret = k->exit_code;
 	put_task_struct(k);
 

commit eff9662547f358239b98dfc4a8e6905b494e14d6
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Nov 29 18:51:00 2016 +0100

    Revert "kthread: Pin the stack via try_get_task_stack()/put_task_stack() in to_live_kthread() function"
    
    This reverts commit 23196f2e5f5d810578a772785807dcdc2b9fdce9.
    
    Now that struct kthread is kmalloc'ed and not longer on the task stack
    there is no need anymore to pin the stack.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chunming Zhou <David1.Zhou@amd.com>
    Cc: Roman Pen <roman.penyaev@profitbricks.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20161129175100.GA5333@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 9d64b6526d0b..7891a940007d 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -88,7 +88,7 @@ void free_kthread_struct(struct task_struct *k)
 static struct kthread *to_live_kthread(struct task_struct *k)
 {
 	struct completion *vfork = ACCESS_ONCE(k->vfork_done);
-	if (likely(vfork) && try_get_task_stack(k))
+	if (likely(vfork))
 		return __to_kthread(vfork);
 	return NULL;
 }
@@ -473,10 +473,8 @@ void kthread_unpark(struct task_struct *k)
 {
 	struct kthread *kthread = to_live_kthread(k);
 
-	if (kthread) {
+	if (kthread)
 		__kthread_unpark(k, kthread);
-		put_task_stack(k);
-	}
 }
 EXPORT_SYMBOL_GPL(kthread_unpark);
 
@@ -505,7 +503,6 @@ int kthread_park(struct task_struct *k)
 				wait_for_completion(&kthread->parked);
 			}
 		}
-		put_task_stack(k);
 		ret = 0;
 	}
 	return ret;
@@ -541,7 +538,6 @@ int kthread_stop(struct task_struct *k)
 		__kthread_unpark(k, kthread);
 		wake_up_process(k);
 		wait_for_completion(&kthread->exited);
-		put_task_stack(k);
 	}
 	ret = k->exit_code;
 	put_task_struct(k);

commit 1da5c46fa965ff90f5ffc080b6ab3fae5e227bc3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Nov 29 18:50:57 2016 +0100

    kthread: Make struct kthread kmalloc'ed
    
    commit 23196f2e5f5d "kthread: Pin the stack via try_get_task_stack() /
    put_task_stack() in to_live_kthread() function" is a workaround for the
    fragile design of struct kthread being allocated on the task stack.
    
    struct kthread in its current form should be removed, but this needs
    cleanups outside of kthread.c.
    
    As a first step move struct kthread away from the task stack by making it
    kmalloc'ed. This allows to access kthread.exited without the magic of
    trying to pin task stack and the try logic in to_live_kthread().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chunming Zhou <David1.Zhou@amd.com>
    Cc: Roman Pen <roman.penyaev@profitbricks.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20161129175057.GA5330@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index be2cc1f9dd57..9d64b6526d0b 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -53,14 +53,38 @@ enum KTHREAD_BITS {
 	KTHREAD_IS_PARKED,
 };
 
-#define __to_kthread(vfork)	\
-	container_of(vfork, struct kthread, exited)
+static inline void set_kthread_struct(void *kthread)
+{
+	/*
+	 * We abuse ->set_child_tid to avoid the new member and because it
+	 * can't be wrongly copied by copy_process(). We also rely on fact
+	 * that the caller can't exec, so PF_KTHREAD can't be cleared.
+	 */
+	current->set_child_tid = (__force void __user *)kthread;
+}
 
 static inline struct kthread *to_kthread(struct task_struct *k)
 {
-	return __to_kthread(k->vfork_done);
+	WARN_ON(!(k->flags & PF_KTHREAD));
+	return (__force void *)k->set_child_tid;
+}
+
+void free_kthread_struct(struct task_struct *k)
+{
+	/*
+	 * Can be NULL if this kthread was created by kernel_thread()
+	 * or if kmalloc() in kthread() failed.
+	 */
+	kfree(to_kthread(k));
 }
 
+#define __to_kthread(vfork)	\
+	container_of(vfork, struct kthread, exited)
+
+/*
+ * TODO: kill it and use to_kthread(). But we still need the users
+ * like kthread_stop() which has to sync with the exiting kthread.
+ */
 static struct kthread *to_live_kthread(struct task_struct *k)
 {
 	struct completion *vfork = ACCESS_ONCE(k->vfork_done);
@@ -181,14 +205,11 @@ static int kthread(void *_create)
 	int (*threadfn)(void *data) = create->threadfn;
 	void *data = create->data;
 	struct completion *done;
-	struct kthread self;
+	struct kthread *self;
 	int ret;
 
-	self.flags = 0;
-	self.data = data;
-	init_completion(&self.exited);
-	init_completion(&self.parked);
-	current->vfork_done = &self.exited;
+	self = kmalloc(sizeof(*self), GFP_KERNEL);
+	set_kthread_struct(self);
 
 	/* If user was SIGKILLed, I release the structure. */
 	done = xchg(&create->done, NULL);
@@ -196,6 +217,19 @@ static int kthread(void *_create)
 		kfree(create);
 		do_exit(-EINTR);
 	}
+
+	if (!self) {
+		create->result = ERR_PTR(-ENOMEM);
+		complete(done);
+		do_exit(-ENOMEM);
+	}
+
+	self->flags = 0;
+	self->data = data;
+	init_completion(&self->exited);
+	init_completion(&self->parked);
+	current->vfork_done = &self->exited;
+
 	/* OK, tell user we're spawned, wait for stop or wakeup */
 	__set_current_state(TASK_UNINTERRUPTIBLE);
 	create->result = current;
@@ -203,12 +237,10 @@ static int kthread(void *_create)
 	schedule();
 
 	ret = -EINTR;
-
-	if (!test_bit(KTHREAD_SHOULD_STOP, &self.flags)) {
-		__kthread_parkme(&self);
+	if (!test_bit(KTHREAD_SHOULD_STOP, &self->flags)) {
+		__kthread_parkme(self);
 		ret = threadfn(data);
 	}
-	/* we can't just return, we must preserve "self" on stack */
 	do_exit(ret);
 }
 

commit dbf52682cb02863d22b15e3742988c7c6e3f1710
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:50 2016 -0700

    kthread: better support freezable kthread workers
    
    This patch allows to make kthread worker freezable via a new @flags
    parameter. It will allow to avoid an init work in some kthreads.
    
    It currently does not affect the function of kthread_worker_fn()
    but it might help to do some optimization or fixes eventually.
    
    I currently do not know about any other use for the @flags
    parameter but I believe that we will want more flags
    in the future.
    
    Finally, I hope that it will not cause confusion with @flags member
    in struct kthread. Well, I guess that we will want to rework the
    basic kthreads implementation once all kthreads are converted into
    kthread workers or workqueues. It is possible that we will merge
    the two structures.
    
    Link: http://lkml.kernel.org/r/1470754545-17632-12-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index c1fcc63fa605..be2cc1f9dd57 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -560,11 +560,11 @@ void __kthread_init_worker(struct kthread_worker *worker,
 				const char *name,
 				struct lock_class_key *key)
 {
+	memset(worker, 0, sizeof(struct kthread_worker));
 	spin_lock_init(&worker->lock);
 	lockdep_set_class_and_name(&worker->lock, key, name);
 	INIT_LIST_HEAD(&worker->work_list);
 	INIT_LIST_HEAD(&worker->delayed_work_list);
-	worker->task = NULL;
 }
 EXPORT_SYMBOL_GPL(__kthread_init_worker);
 
@@ -594,6 +594,10 @@ int kthread_worker_fn(void *worker_ptr)
 	 */
 	WARN_ON(worker->task && worker->task != current);
 	worker->task = current;
+
+	if (worker->flags & KTW_FREEZABLE)
+		set_freezable();
+
 repeat:
 	set_current_state(TASK_INTERRUPTIBLE);	/* mb paired w/ kthread_stop */
 
@@ -627,7 +631,8 @@ int kthread_worker_fn(void *worker_ptr)
 EXPORT_SYMBOL_GPL(kthread_worker_fn);
 
 static struct kthread_worker *
-__kthread_create_worker(int cpu, const char namefmt[], va_list args)
+__kthread_create_worker(int cpu, unsigned int flags,
+			const char namefmt[], va_list args)
 {
 	struct kthread_worker *worker;
 	struct task_struct *task;
@@ -657,6 +662,7 @@ __kthread_create_worker(int cpu, const char namefmt[], va_list args)
 	if (IS_ERR(task))
 		goto fail_task;
 
+	worker->flags = flags;
 	worker->task = task;
 	wake_up_process(task);
 	return worker;
@@ -668,6 +674,7 @@ __kthread_create_worker(int cpu, const char namefmt[], va_list args)
 
 /**
  * kthread_create_worker - create a kthread worker
+ * @flags: flags modifying the default behavior of the worker
  * @namefmt: printf-style name for the kthread worker (task).
  *
  * Returns a pointer to the allocated worker on success, ERR_PTR(-ENOMEM)
@@ -675,13 +682,13 @@ __kthread_create_worker(int cpu, const char namefmt[], va_list args)
  * when the worker was SIGKILLed.
  */
 struct kthread_worker *
-kthread_create_worker(const char namefmt[], ...)
+kthread_create_worker(unsigned int flags, const char namefmt[], ...)
 {
 	struct kthread_worker *worker;
 	va_list args;
 
 	va_start(args, namefmt);
-	worker = __kthread_create_worker(-1, namefmt, args);
+	worker = __kthread_create_worker(-1, flags, namefmt, args);
 	va_end(args);
 
 	return worker;
@@ -692,6 +699,7 @@ EXPORT_SYMBOL(kthread_create_worker);
  * kthread_create_worker_on_cpu - create a kthread worker and bind it
  *	it to a given CPU and the associated NUMA node.
  * @cpu: CPU number
+ * @flags: flags modifying the default behavior of the worker
  * @namefmt: printf-style name for the kthread worker (task).
  *
  * Use a valid CPU number if you want to bind the kthread worker
@@ -705,13 +713,14 @@ EXPORT_SYMBOL(kthread_create_worker);
  * when the worker was SIGKILLed.
  */
 struct kthread_worker *
-kthread_create_worker_on_cpu(int cpu, const char namefmt[], ...)
+kthread_create_worker_on_cpu(int cpu, unsigned int flags,
+			     const char namefmt[], ...)
 {
 	struct kthread_worker *worker;
 	va_list args;
 
 	va_start(args, namefmt);
-	worker = __kthread_create_worker(cpu, namefmt, args);
+	worker = __kthread_create_worker(cpu, flags, namefmt, args);
 	va_end(args);
 
 	return worker;

commit 9a6b06c8d9a220860468aadb2f1c726570813bf9
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:46 2016 -0700

    kthread: allow to modify delayed kthread work
    
    There are situations when we need to modify the delay of a delayed kthread
    work. For example, when the work depends on an event and the initial delay
    means a timeout. Then we want to queue the work immediately when the event
    happens.
    
    This patch implements kthread_mod_delayed_work() as inspired workqueues.
    It cancels the timer, removes the work from any worker list and queues it
    again with the given timeout.
    
    A very special case is when the work is being canceled at the same time.
    It might happen because of the regular kthread_cancel_delayed_work_sync()
    or by another kthread_mod_delayed_work(). In this case, we do nothing and
    let the other operation win. This should not normally happen as the caller
    is supposed to synchronize these operations a reasonable way.
    
    Link: http://lkml.kernel.org/r/1470754545-17632-11-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 20505c632bc3..c1fcc63fa605 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -976,6 +976,59 @@ static bool __kthread_cancel_work(struct kthread_work *work, bool is_dwork,
 	return false;
 }
 
+/**
+ * kthread_mod_delayed_work - modify delay of or queue a kthread delayed work
+ * @worker: kthread worker to use
+ * @dwork: kthread delayed work to queue
+ * @delay: number of jiffies to wait before queuing
+ *
+ * If @dwork is idle, equivalent to kthread_queue_delayed_work(). Otherwise,
+ * modify @dwork's timer so that it expires after @delay. If @delay is zero,
+ * @work is guaranteed to be queued immediately.
+ *
+ * Return: %true if @dwork was pending and its timer was modified,
+ * %false otherwise.
+ *
+ * A special case is when the work is being canceled in parallel.
+ * It might be caused either by the real kthread_cancel_delayed_work_sync()
+ * or yet another kthread_mod_delayed_work() call. We let the other command
+ * win and return %false here. The caller is supposed to synchronize these
+ * operations a reasonable way.
+ *
+ * This function is safe to call from any context including IRQ handler.
+ * See __kthread_cancel_work() and kthread_delayed_work_timer_fn()
+ * for details.
+ */
+bool kthread_mod_delayed_work(struct kthread_worker *worker,
+			      struct kthread_delayed_work *dwork,
+			      unsigned long delay)
+{
+	struct kthread_work *work = &dwork->work;
+	unsigned long flags;
+	int ret = false;
+
+	spin_lock_irqsave(&worker->lock, flags);
+
+	/* Do not bother with canceling when never queued. */
+	if (!work->worker)
+		goto fast_queue;
+
+	/* Work must not be used with >1 worker, see kthread_queue_work() */
+	WARN_ON_ONCE(work->worker != worker);
+
+	/* Do not fight with another command that is canceling this work. */
+	if (work->canceling)
+		goto out;
+
+	ret = __kthread_cancel_work(work, true, &flags);
+fast_queue:
+	__kthread_queue_delayed_work(worker, dwork, delay);
+out:
+	spin_unlock_irqrestore(&worker->lock, flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(kthread_mod_delayed_work);
+
 static bool __kthread_cancel_work_sync(struct kthread_work *work, bool is_dwork)
 {
 	struct kthread_worker *worker = work->worker;

commit 37be45d49dec2a411e29d50c9597cfe8184b5645
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:43 2016 -0700

    kthread: allow to cancel kthread work
    
    We are going to use kthread workers more widely and sometimes we will need
    to make sure that the work is neither pending nor running.
    
    This patch implements cancel_*_sync() operations as inspired by
    workqueues.  Well, we are synchronized against the other operations via
    the worker lock, we use del_timer_sync() and a counter to count parallel
    cancel operations.  Therefore the implementation might be easier.
    
    First, we check if a worker is assigned.  If not, the work has newer been
    queued after it was initialized.
    
    Second, we take the worker lock.  It must be the right one.  The work must
    not be assigned to another worker unless it is initialized in between.
    
    Third, we try to cancel the timer when it exists.  The timer is deleted
    synchronously to make sure that the timer call back is not running.  We
    need to temporary release the worker->lock to avoid a possible deadlock
    with the callback.  In the meantime, we set work->canceling counter to
    avoid any queuing.
    
    Fourth, we try to remove the work from a worker list. It might be
    the list of either normal or delayed works.
    
    Fifth, if the work is running, we call kthread_flush_work().  It might
    take an arbitrary time.  We need to release the worker-lock again.  In the
    meantime, we again block any queuing by the canceling counter.
    
    As already mentioned, the check for a pending kthread work is done under a
    lock.  In compare with workqueues, we do not need to fight for a single
    PENDING bit to block other operations.  Therefore we do not suffer from
    the thundering storm problem and all parallel canceling jobs might use
    kthread_flush_work().  Any queuing is blocked until the counter gets zero.
    
    Link: http://lkml.kernel.org/r/1470754545-17632-10-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index a5c6d709349d..20505c632bc3 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -718,6 +718,19 @@ kthread_create_worker_on_cpu(int cpu, const char namefmt[], ...)
 }
 EXPORT_SYMBOL(kthread_create_worker_on_cpu);
 
+/*
+ * Returns true when the work could not be queued at the moment.
+ * It happens when it is already pending in a worker list
+ * or when it is being cancelled.
+ */
+static inline bool queuing_blocked(struct kthread_worker *worker,
+				   struct kthread_work *work)
+{
+	lockdep_assert_held(&worker->lock);
+
+	return !list_empty(&work->node) || work->canceling;
+}
+
 static void kthread_insert_work_sanity_check(struct kthread_worker *worker,
 					     struct kthread_work *work)
 {
@@ -759,7 +772,7 @@ bool kthread_queue_work(struct kthread_worker *worker,
 	unsigned long flags;
 
 	spin_lock_irqsave(&worker->lock, flags);
-	if (list_empty(&work->node)) {
+	if (!queuing_blocked(worker, work)) {
 		kthread_insert_work(worker, work, &worker->work_list);
 		ret = true;
 	}
@@ -859,7 +872,7 @@ bool kthread_queue_delayed_work(struct kthread_worker *worker,
 
 	spin_lock_irqsave(&worker->lock, flags);
 
-	if (list_empty(&work->node)) {
+	if (!queuing_blocked(worker, work)) {
 		__kthread_queue_delayed_work(worker, dwork, delay);
 		ret = true;
 	}
@@ -919,6 +932,121 @@ void kthread_flush_work(struct kthread_work *work)
 }
 EXPORT_SYMBOL_GPL(kthread_flush_work);
 
+/*
+ * This function removes the work from the worker queue. Also it makes sure
+ * that it won't get queued later via the delayed work's timer.
+ *
+ * The work might still be in use when this function finishes. See the
+ * current_work proceed by the worker.
+ *
+ * Return: %true if @work was pending and successfully canceled,
+ *	%false if @work was not pending
+ */
+static bool __kthread_cancel_work(struct kthread_work *work, bool is_dwork,
+				  unsigned long *flags)
+{
+	/* Try to cancel the timer if exists. */
+	if (is_dwork) {
+		struct kthread_delayed_work *dwork =
+			container_of(work, struct kthread_delayed_work, work);
+		struct kthread_worker *worker = work->worker;
+
+		/*
+		 * del_timer_sync() must be called to make sure that the timer
+		 * callback is not running. The lock must be temporary released
+		 * to avoid a deadlock with the callback. In the meantime,
+		 * any queuing is blocked by setting the canceling counter.
+		 */
+		work->canceling++;
+		spin_unlock_irqrestore(&worker->lock, *flags);
+		del_timer_sync(&dwork->timer);
+		spin_lock_irqsave(&worker->lock, *flags);
+		work->canceling--;
+	}
+
+	/*
+	 * Try to remove the work from a worker list. It might either
+	 * be from worker->work_list or from worker->delayed_work_list.
+	 */
+	if (!list_empty(&work->node)) {
+		list_del_init(&work->node);
+		return true;
+	}
+
+	return false;
+}
+
+static bool __kthread_cancel_work_sync(struct kthread_work *work, bool is_dwork)
+{
+	struct kthread_worker *worker = work->worker;
+	unsigned long flags;
+	int ret = false;
+
+	if (!worker)
+		goto out;
+
+	spin_lock_irqsave(&worker->lock, flags);
+	/* Work must not be used with >1 worker, see kthread_queue_work(). */
+	WARN_ON_ONCE(work->worker != worker);
+
+	ret = __kthread_cancel_work(work, is_dwork, &flags);
+
+	if (worker->current_work != work)
+		goto out_fast;
+
+	/*
+	 * The work is in progress and we need to wait with the lock released.
+	 * In the meantime, block any queuing by setting the canceling counter.
+	 */
+	work->canceling++;
+	spin_unlock_irqrestore(&worker->lock, flags);
+	kthread_flush_work(work);
+	spin_lock_irqsave(&worker->lock, flags);
+	work->canceling--;
+
+out_fast:
+	spin_unlock_irqrestore(&worker->lock, flags);
+out:
+	return ret;
+}
+
+/**
+ * kthread_cancel_work_sync - cancel a kthread work and wait for it to finish
+ * @work: the kthread work to cancel
+ *
+ * Cancel @work and wait for its execution to finish.  This function
+ * can be used even if the work re-queues itself. On return from this
+ * function, @work is guaranteed to be not pending or executing on any CPU.
+ *
+ * kthread_cancel_work_sync(&delayed_work->work) must not be used for
+ * delayed_work's. Use kthread_cancel_delayed_work_sync() instead.
+ *
+ * The caller must ensure that the worker on which @work was last
+ * queued can't be destroyed before this function returns.
+ *
+ * Return: %true if @work was pending, %false otherwise.
+ */
+bool kthread_cancel_work_sync(struct kthread_work *work)
+{
+	return __kthread_cancel_work_sync(work, false);
+}
+EXPORT_SYMBOL_GPL(kthread_cancel_work_sync);
+
+/**
+ * kthread_cancel_delayed_work_sync - cancel a kthread delayed work and
+ *	wait for it to finish.
+ * @dwork: the kthread delayed work to cancel
+ *
+ * This is kthread_cancel_work_sync() for delayed works.
+ *
+ * Return: %true if @dwork was pending, %false otherwise.
+ */
+bool kthread_cancel_delayed_work_sync(struct kthread_delayed_work *dwork)
+{
+	return __kthread_cancel_work_sync(&dwork->work, true);
+}
+EXPORT_SYMBOL_GPL(kthread_cancel_delayed_work_sync);
+
 /**
  * kthread_flush_worker - flush all current works on a kthread_worker
  * @worker: worker to flush

commit 22597dc3d97b1ead2aca201397415a1a84bf2b26
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:40 2016 -0700

    kthread: initial support for delayed kthread work
    
    We are going to use kthread_worker more widely and delayed works
    will be pretty useful.
    
    The implementation is inspired by workqueues.  It uses a timer to queue
    the work after the requested delay.  If the delay is zero, the work is
    queued immediately.
    
    In compare with workqueues, each work is associated with a single worker
    (kthread).  Therefore the implementation could be much easier.  In
    particular, we use the worker->lock to synchronize all the operations with
    the work.  We do not need any atomic operation with a flags variable.
    
    In fact, we do not need any state variable at all.  Instead, we add a list
    of delayed works into the worker.  Then the pending work is listed either
    in the list of queued or delayed works.  And the existing check of pending
    works is the same even for the delayed ones.
    
    A work must not be assigned to another worker unless reinitialized.
    Therefore the timer handler might expect that dwork->work->worker is valid
    and it could simply take the lock.  We just add some sanity checks to help
    with debugging a potential misuse.
    
    Link: http://lkml.kernel.org/r/1470754545-17632-9-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 524dfc8247d7..a5c6d709349d 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -563,6 +563,7 @@ void __kthread_init_worker(struct kthread_worker *worker,
 	spin_lock_init(&worker->lock);
 	lockdep_set_class_and_name(&worker->lock, key, name);
 	INIT_LIST_HEAD(&worker->work_list);
+	INIT_LIST_HEAD(&worker->delayed_work_list);
 	worker->task = NULL;
 }
 EXPORT_SYMBOL_GPL(__kthread_init_worker);
@@ -767,6 +768,107 @@ bool kthread_queue_work(struct kthread_worker *worker,
 }
 EXPORT_SYMBOL_GPL(kthread_queue_work);
 
+/**
+ * kthread_delayed_work_timer_fn - callback that queues the associated kthread
+ *	delayed work when the timer expires.
+ * @__data: pointer to the data associated with the timer
+ *
+ * The format of the function is defined by struct timer_list.
+ * It should have been called from irqsafe timer with irq already off.
+ */
+void kthread_delayed_work_timer_fn(unsigned long __data)
+{
+	struct kthread_delayed_work *dwork =
+		(struct kthread_delayed_work *)__data;
+	struct kthread_work *work = &dwork->work;
+	struct kthread_worker *worker = work->worker;
+
+	/*
+	 * This might happen when a pending work is reinitialized.
+	 * It means that it is used a wrong way.
+	 */
+	if (WARN_ON_ONCE(!worker))
+		return;
+
+	spin_lock(&worker->lock);
+	/* Work must not be used with >1 worker, see kthread_queue_work(). */
+	WARN_ON_ONCE(work->worker != worker);
+
+	/* Move the work from worker->delayed_work_list. */
+	WARN_ON_ONCE(list_empty(&work->node));
+	list_del_init(&work->node);
+	kthread_insert_work(worker, work, &worker->work_list);
+
+	spin_unlock(&worker->lock);
+}
+EXPORT_SYMBOL(kthread_delayed_work_timer_fn);
+
+void __kthread_queue_delayed_work(struct kthread_worker *worker,
+				  struct kthread_delayed_work *dwork,
+				  unsigned long delay)
+{
+	struct timer_list *timer = &dwork->timer;
+	struct kthread_work *work = &dwork->work;
+
+	WARN_ON_ONCE(timer->function != kthread_delayed_work_timer_fn ||
+		     timer->data != (unsigned long)dwork);
+
+	/*
+	 * If @delay is 0, queue @dwork->work immediately.  This is for
+	 * both optimization and correctness.  The earliest @timer can
+	 * expire is on the closest next tick and delayed_work users depend
+	 * on that there's no such delay when @delay is 0.
+	 */
+	if (!delay) {
+		kthread_insert_work(worker, work, &worker->work_list);
+		return;
+	}
+
+	/* Be paranoid and try to detect possible races already now. */
+	kthread_insert_work_sanity_check(worker, work);
+
+	list_add(&work->node, &worker->delayed_work_list);
+	work->worker = worker;
+	timer_stats_timer_set_start_info(&dwork->timer);
+	timer->expires = jiffies + delay;
+	add_timer(timer);
+}
+
+/**
+ * kthread_queue_delayed_work - queue the associated kthread work
+ *	after a delay.
+ * @worker: target kthread_worker
+ * @dwork: kthread_delayed_work to queue
+ * @delay: number of jiffies to wait before queuing
+ *
+ * If the work has not been pending it starts a timer that will queue
+ * the work after the given @delay. If @delay is zero, it queues the
+ * work immediately.
+ *
+ * Return: %false if the @work has already been pending. It means that
+ * either the timer was running or the work was queued. It returns %true
+ * otherwise.
+ */
+bool kthread_queue_delayed_work(struct kthread_worker *worker,
+				struct kthread_delayed_work *dwork,
+				unsigned long delay)
+{
+	struct kthread_work *work = &dwork->work;
+	unsigned long flags;
+	bool ret = false;
+
+	spin_lock_irqsave(&worker->lock, flags);
+
+	if (list_empty(&work->node)) {
+		__kthread_queue_delayed_work(worker, dwork, delay);
+		ret = true;
+	}
+
+	spin_unlock_irqrestore(&worker->lock, flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(kthread_queue_delayed_work);
+
 struct kthread_flush_work {
 	struct kthread_work	work;
 	struct completion	done;

commit 8197b3d43b250b8214b9913878e9227eef8bd85f
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:36 2016 -0700

    kthread: detect when a kthread work is used by more workers
    
    Nothing currently prevents a work from queuing for a kthread worker when
    it is already running on another one.  This means that the work might run
    in parallel on more than one worker.  Also some operations are not
    reliable, e.g.  flush.
    
    This problem will be even more visible after we add kthread_cancel_work()
    function.  It will only have "work" as the parameter and will use
    worker->lock to synchronize with others.
    
    Well, normally this is not a problem because the API users are sane.
    But bugs might happen and users also might be crazy.
    
    This patch adds a warning when we try to insert the work for another
    worker.  It does not fully prevent the misuse because it would make the
    code much more complicated without a big benefit.
    
    It adds the same warning also into kthread_flush_work() instead of the
    repeated attempts to get the right lock.
    
    A side effect is that one needs to explicitly reinitialize the work if it
    must be queued into another worker.  This is needed, for example, when the
    worker is stopped and started again.  It is a bit inconvenient.  But it
    looks like a good compromise between the stability and complexity.
    
    I have double checked all existing users of the kthread worker API and
    they all seems to initialize the work after the worker gets started.
    
    Just for completeness, the patch adds a check that the work is not already
    in a queue.
    
    The patch also puts all the checks into a separate function.  It will be
    reused when implementing delayed works.
    
    Link: http://lkml.kernel.org/r/1470754545-17632-8-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index f874300dfce5..524dfc8247d7 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -578,6 +578,9 @@ EXPORT_SYMBOL_GPL(__kthread_init_worker);
  * The works are not allowed to keep any locks, disable preemption or interrupts
  * when they finish. There is defined a safe point for freezing when one work
  * finishes and before a new one is started.
+ *
+ * Also the works must not be handled by more than one worker at the same time,
+ * see also kthread_queue_work().
  */
 int kthread_worker_fn(void *worker_ptr)
 {
@@ -714,12 +717,21 @@ kthread_create_worker_on_cpu(int cpu, const char namefmt[], ...)
 }
 EXPORT_SYMBOL(kthread_create_worker_on_cpu);
 
+static void kthread_insert_work_sanity_check(struct kthread_worker *worker,
+					     struct kthread_work *work)
+{
+	lockdep_assert_held(&worker->lock);
+	WARN_ON_ONCE(!list_empty(&work->node));
+	/* Do not use a work with >1 worker, see kthread_queue_work() */
+	WARN_ON_ONCE(work->worker && work->worker != worker);
+}
+
 /* insert @work before @pos in @worker */
 static void kthread_insert_work(struct kthread_worker *worker,
-			       struct kthread_work *work,
-			       struct list_head *pos)
+				struct kthread_work *work,
+				struct list_head *pos)
 {
-	lockdep_assert_held(&worker->lock);
+	kthread_insert_work_sanity_check(worker, work);
 
 	list_add_tail(&work->node, pos);
 	work->worker = worker;
@@ -735,6 +747,9 @@ static void kthread_insert_work(struct kthread_worker *worker,
  * Queue @work to work processor @task for async execution.  @task
  * must have been created with kthread_worker_create().  Returns %true
  * if @work was successfully queued, %false if it was already pending.
+ *
+ * Reinitialize the work if it needs to be used by another worker.
+ * For example, when the worker was stopped and started again.
  */
 bool kthread_queue_work(struct kthread_worker *worker,
 			struct kthread_work *work)
@@ -779,16 +794,13 @@ void kthread_flush_work(struct kthread_work *work)
 	struct kthread_worker *worker;
 	bool noop = false;
 
-retry:
 	worker = work->worker;
 	if (!worker)
 		return;
 
 	spin_lock_irq(&worker->lock);
-	if (work->worker != worker) {
-		spin_unlock_irq(&worker->lock);
-		goto retry;
-	}
+	/* Work must not be used with >1 worker, see kthread_queue_work(). */
+	WARN_ON_ONCE(work->worker != worker);
 
 	if (!list_empty(&work->node))
 		kthread_insert_work(worker, &fwork.work, work->node.next);

commit 35033fe9cbbf18415dfeb7e27f0d4228dfc7458a
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:33 2016 -0700

    kthread: add kthread_destroy_worker()
    
    The current kthread worker users call flush() and stop() explicitly.
    This function does the same plus it frees the kthread_worker struct
    in one call.
    
    It is supposed to be used together with kthread_create_worker*() that
    allocates struct kthread_worker.
    
    Link: http://lkml.kernel.org/r/1470754545-17632-7-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 12695e68ed00..f874300dfce5 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -823,3 +823,26 @@ void kthread_flush_worker(struct kthread_worker *worker)
 	wait_for_completion(&fwork.done);
 }
 EXPORT_SYMBOL_GPL(kthread_flush_worker);
+
+/**
+ * kthread_destroy_worker - destroy a kthread worker
+ * @worker: worker to be destroyed
+ *
+ * Flush and destroy @worker.  The simple flush is enough because the kthread
+ * worker API is used only in trivial scenarios.  There are no multi-step state
+ * machines needed.
+ */
+void kthread_destroy_worker(struct kthread_worker *worker)
+{
+	struct task_struct *task;
+
+	task = worker->task;
+	if (WARN_ON(!task))
+		return;
+
+	kthread_flush_worker(worker);
+	kthread_stop(task);
+	WARN_ON(!list_empty(&worker->work_list));
+	kfree(worker);
+}
+EXPORT_SYMBOL(kthread_destroy_worker);

commit fbae2d44aa1df72d0154be77eb4d71e1e34c0f8f
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:30 2016 -0700

    kthread: add kthread_create_worker*()
    
    Kthread workers are currently created using the classic kthread API,
    namely kthread_run().  kthread_worker_fn() is passed as the @threadfn
    parameter.
    
    This patch defines kthread_create_worker() and
    kthread_create_worker_on_cpu() functions that hide implementation details.
    
    They enforce using kthread_worker_fn() for the main thread.  But I doubt
    that there are any plans to create any alternative.  In fact, I think that
    we do not want any alternative main thread because it would be hard to
    support consistency with the rest of the kthread worker API.
    
    The naming and function of kthread_create_worker() is inspired by the
    workqueues API like the rest of the kthread worker API.
    
    The kthread_create_worker_on_cpu() variant is motivated by the original
    kthread_create_on_cpu().  Note that we need to bind per-CPU kthread
    workers already when they are created.  It makes the life easier.
    kthread_bind() could not be used later for an already running worker.
    
    This patch does _not_ convert existing kthread workers.  The kthread
    worker API need more improvements first, e.g.  a function to destroy the
    worker.
    
    IMPORTANT:
    
    kthread_create_worker_on_cpu() allows to use any format of the worker
    name, in compare with kthread_create_on_cpu().  The good thing is that it
    is more generic.  The bad thing is that most users will need to pass the
    cpu number in two parameters, e.g.  kthread_create_worker_on_cpu(cpu,
    "helper/%d", cpu).
    
    To be honest, the main motivation was to avoid the need for an empty
    va_list.  The only legal way was to create a helper function that would be
    called with an empty list.  Other attempts caused compilation warnings or
    even errors on different architectures.
    
    There were also other alternatives, for example, using #define or
    splitting __kthread_create_worker().  The used solution looked like the
    least ugly.
    
    Link: http://lkml.kernel.org/r/1470754545-17632-6-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 36fd751a769f..12695e68ed00 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -571,23 +571,24 @@ EXPORT_SYMBOL_GPL(__kthread_init_worker);
  * kthread_worker_fn - kthread function to process kthread_worker
  * @worker_ptr: pointer to initialized kthread_worker
  *
- * This function can be used as @threadfn to kthread_create() or
- * kthread_run() with @worker_ptr argument pointing to an initialized
- * kthread_worker.  The started kthread will process work_list until
- * the it is stopped with kthread_stop().  A kthread can also call
- * this function directly after extra initialization.
+ * This function implements the main cycle of kthread worker. It processes
+ * work_list until it is stopped with kthread_stop(). It sleeps when the queue
+ * is empty.
  *
- * Different kthreads can be used for the same kthread_worker as long
- * as there's only one kthread attached to it at any given time.  A
- * kthread_worker without an attached kthread simply collects queued
- * kthread_works.
+ * The works are not allowed to keep any locks, disable preemption or interrupts
+ * when they finish. There is defined a safe point for freezing when one work
+ * finishes and before a new one is started.
  */
 int kthread_worker_fn(void *worker_ptr)
 {
 	struct kthread_worker *worker = worker_ptr;
 	struct kthread_work *work;
 
-	WARN_ON(worker->task);
+	/*
+	 * FIXME: Update the check and remove the assignment when all kthread
+	 * worker users are created using kthread_create_worker*() functions.
+	 */
+	WARN_ON(worker->task && worker->task != current);
 	worker->task = current;
 repeat:
 	set_current_state(TASK_INTERRUPTIBLE);	/* mb paired w/ kthread_stop */
@@ -621,6 +622,98 @@ int kthread_worker_fn(void *worker_ptr)
 }
 EXPORT_SYMBOL_GPL(kthread_worker_fn);
 
+static struct kthread_worker *
+__kthread_create_worker(int cpu, const char namefmt[], va_list args)
+{
+	struct kthread_worker *worker;
+	struct task_struct *task;
+
+	worker = kzalloc(sizeof(*worker), GFP_KERNEL);
+	if (!worker)
+		return ERR_PTR(-ENOMEM);
+
+	kthread_init_worker(worker);
+
+	if (cpu >= 0) {
+		char name[TASK_COMM_LEN];
+
+		/*
+		 * kthread_create_worker_on_cpu() allows to pass a generic
+		 * namefmt in compare with kthread_create_on_cpu. We need
+		 * to format it here.
+		 */
+		vsnprintf(name, sizeof(name), namefmt, args);
+		task = kthread_create_on_cpu(kthread_worker_fn, worker,
+					     cpu, name);
+	} else {
+		task = __kthread_create_on_node(kthread_worker_fn, worker,
+						-1, namefmt, args);
+	}
+
+	if (IS_ERR(task))
+		goto fail_task;
+
+	worker->task = task;
+	wake_up_process(task);
+	return worker;
+
+fail_task:
+	kfree(worker);
+	return ERR_CAST(task);
+}
+
+/**
+ * kthread_create_worker - create a kthread worker
+ * @namefmt: printf-style name for the kthread worker (task).
+ *
+ * Returns a pointer to the allocated worker on success, ERR_PTR(-ENOMEM)
+ * when the needed structures could not get allocated, and ERR_PTR(-EINTR)
+ * when the worker was SIGKILLed.
+ */
+struct kthread_worker *
+kthread_create_worker(const char namefmt[], ...)
+{
+	struct kthread_worker *worker;
+	va_list args;
+
+	va_start(args, namefmt);
+	worker = __kthread_create_worker(-1, namefmt, args);
+	va_end(args);
+
+	return worker;
+}
+EXPORT_SYMBOL(kthread_create_worker);
+
+/**
+ * kthread_create_worker_on_cpu - create a kthread worker and bind it
+ *	it to a given CPU and the associated NUMA node.
+ * @cpu: CPU number
+ * @namefmt: printf-style name for the kthread worker (task).
+ *
+ * Use a valid CPU number if you want to bind the kthread worker
+ * to the given CPU and the associated NUMA node.
+ *
+ * A good practice is to add the cpu number also into the worker name.
+ * For example, use kthread_create_worker_on_cpu(cpu, "helper/%d", cpu).
+ *
+ * Returns a pointer to the allocated worker on success, ERR_PTR(-ENOMEM)
+ * when the needed structures could not get allocated, and ERR_PTR(-EINTR)
+ * when the worker was SIGKILLed.
+ */
+struct kthread_worker *
+kthread_create_worker_on_cpu(int cpu, const char namefmt[], ...)
+{
+	struct kthread_worker *worker;
+	va_list args;
+
+	va_start(args, namefmt);
+	worker = __kthread_create_worker(cpu, namefmt, args);
+	va_end(args);
+
+	return worker;
+}
+EXPORT_SYMBOL(kthread_create_worker_on_cpu);
+
 /* insert @work before @pos in @worker */
 static void kthread_insert_work(struct kthread_worker *worker,
 			       struct kthread_work *work,

commit 255451e45345bcd19e79f25e9afc0b9e14ccb104
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:27 2016 -0700

    kthread: allow to call __kthread_create_on_node() with va_list args
    
    kthread_create_on_node() implements a bunch of logic to create the
    kthread.  It is already called by kthread_create_on_cpu().
    
    We are going to extend the kthread worker API and will need to call
    kthread_create_on_node() with va_list args there.
    
    This patch does only a refactoring and does not modify the existing
    behavior.
    
    Link: http://lkml.kernel.org/r/1470754545-17632-5-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index f5dc0b151d61..36fd751a769f 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -244,33 +244,10 @@ static void create_kthread(struct kthread_create_info *create)
 	}
 }
 
-/**
- * kthread_create_on_node - create a kthread.
- * @threadfn: the function to run until signal_pending(current).
- * @data: data ptr for @threadfn.
- * @node: task and thread structures for the thread are allocated on this node
- * @namefmt: printf-style name for the thread.
- *
- * Description: This helper function creates and names a kernel
- * thread.  The thread will be stopped: use wake_up_process() to start
- * it.  See also kthread_run().  The new thread has SCHED_NORMAL policy and
- * is affine to all CPUs.
- *
- * If thread is going to be bound on a particular cpu, give its node
- * in @node, to get NUMA affinity for kthread stack, or else give NUMA_NO_NODE.
- * When woken, the thread will run @threadfn() with @data as its
- * argument. @threadfn() can either call do_exit() directly if it is a
- * standalone thread for which no one will call kthread_stop(), or
- * return when 'kthread_should_stop()' is true (which means
- * kthread_stop() has been called).  The return value should be zero
- * or a negative error number; it will be passed to kthread_stop().
- *
- * Returns a task_struct or ERR_PTR(-ENOMEM) or ERR_PTR(-EINTR).
- */
-struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
-					   void *data, int node,
-					   const char namefmt[],
-					   ...)
+static struct task_struct *__kthread_create_on_node(int (*threadfn)(void *data),
+						    void *data, int node,
+						    const char namefmt[],
+						    va_list args)
 {
 	DECLARE_COMPLETION_ONSTACK(done);
 	struct task_struct *task;
@@ -311,11 +288,8 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 	task = create->result;
 	if (!IS_ERR(task)) {
 		static const struct sched_param param = { .sched_priority = 0 };
-		va_list args;
 
-		va_start(args, namefmt);
 		vsnprintf(task->comm, sizeof(task->comm), namefmt, args);
-		va_end(args);
 		/*
 		 * root may have changed our (kthreadd's) priority or CPU mask.
 		 * The kernel thread should not inherit these properties.
@@ -326,6 +300,44 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 	kfree(create);
 	return task;
 }
+
+/**
+ * kthread_create_on_node - create a kthread.
+ * @threadfn: the function to run until signal_pending(current).
+ * @data: data ptr for @threadfn.
+ * @node: task and thread structures for the thread are allocated on this node
+ * @namefmt: printf-style name for the thread.
+ *
+ * Description: This helper function creates and names a kernel
+ * thread.  The thread will be stopped: use wake_up_process() to start
+ * it.  See also kthread_run().  The new thread has SCHED_NORMAL policy and
+ * is affine to all CPUs.
+ *
+ * If thread is going to be bound on a particular cpu, give its node
+ * in @node, to get NUMA affinity for kthread stack, or else give NUMA_NO_NODE.
+ * When woken, the thread will run @threadfn() with @data as its
+ * argument. @threadfn() can either call do_exit() directly if it is a
+ * standalone thread for which no one will call kthread_stop(), or
+ * return when 'kthread_should_stop()' is true (which means
+ * kthread_stop() has been called).  The return value should be zero
+ * or a negative error number; it will be passed to kthread_stop().
+ *
+ * Returns a task_struct or ERR_PTR(-ENOMEM) or ERR_PTR(-EINTR).
+ */
+struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
+					   void *data, int node,
+					   const char namefmt[],
+					   ...)
+{
+	struct task_struct *task;
+	va_list args;
+
+	va_start(args, namefmt);
+	task = __kthread_create_on_node(threadfn, data, node, namefmt, args);
+	va_end(args);
+
+	return task;
+}
 EXPORT_SYMBOL(kthread_create_on_node);
 
 static void __kthread_bind_mask(struct task_struct *p, const struct cpumask *mask, long state)

commit a65d40961dc70d15dec046c2ee88c556f57940b2
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:23 2016 -0700

    kthread/smpboot: do not park in kthread_create_on_cpu()
    
    kthread_create_on_cpu() was added by the commit 2a1d446019f9a5983e
    ("kthread: Implement park/unpark facility").  It is currently used only
    when enabling new CPU.  For this purpose, the newly created kthread has to
    be parked.
    
    The CPU binding is a bit tricky.  The kthread is parked when the CPU has
    not been allowed yet.  And the CPU is bound when the kthread is unparked.
    
    The function would be useful for more per-CPU kthreads, e.g.
    bnx2fc_thread, fcoethread.  For this purpose, the newly created kthread
    should stay in the uninterruptible state.
    
    This patch moves the parking into smpboot.  It binds the thread already
    when created.  Then the function might be used universally.  Also the
    behavior is consistent with kthread_create() and kthread_create_on_node().
    
    Link: http://lkml.kernel.org/r/1470754545-17632-4-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index c52a05a8ec52..f5dc0b151d61 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -390,10 +390,10 @@ struct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),
 				   cpu);
 	if (IS_ERR(p))
 		return p;
+	kthread_bind(p, cpu);
+	/* CPU hotplug need to bind once again when unparking the thread. */
 	set_bit(KTHREAD_IS_PER_CPU, &to_kthread(p)->flags);
 	to_kthread(p)->cpu = cpu;
-	/* Park the thread to get it out of TASK_UNINTERRUPTIBLE state */
-	kthread_park(p);
 	return p;
 }
 
@@ -407,6 +407,10 @@ static void __kthread_unpark(struct task_struct *k, struct kthread *kthread)
 	 * which might be about to be cleared.
 	 */
 	if (test_and_clear_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
+		/*
+		 * Newly created kthread was parked when the CPU was offline.
+		 * The binding was lost and we need to set it again.
+		 */
 		if (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))
 			__kthread_bind(k, kthread->cpu, TASK_PARKED);
 		wake_up_state(k, TASK_PARKED);

commit 3989144f863ac576e6efba298d24b0b02a10d4bb
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:20 2016 -0700

    kthread: kthread worker API cleanup
    
    A good practice is to prefix the names of functions by the name
    of the subsystem.
    
    The kthread worker API is a mix of classic kthreads and workqueues.  Each
    worker has a dedicated kthread.  It runs a generic function that process
    queued works.  It is implemented as part of the kthread subsystem.
    
    This patch renames the existing kthread worker API to use
    the corresponding name from the workqueues API prefixed by
    kthread_:
    
    __init_kthread_worker()         -> __kthread_init_worker()
    init_kthread_worker()           -> kthread_init_worker()
    init_kthread_work()             -> kthread_init_work()
    insert_kthread_work()           -> kthread_insert_work()
    queue_kthread_work()            -> kthread_queue_work()
    flush_kthread_work()            -> kthread_flush_work()
    flush_kthread_worker()          -> kthread_flush_worker()
    
    Note that the names of DEFINE_KTHREAD_WORK*() macros stay
    as they are. It is common that the "DEFINE_" prefix has
    precedence over the subsystem names.
    
    Note that INIT() macros and init() functions use different
    naming scheme. There is no good solution. There are several
    reasons for this solution:
    
      + "init" in the function names stands for the verb "initialize"
        aka "initialize worker". While "INIT" in the macro names
        stands for the noun "INITIALIZER" aka "worker initializer".
    
      + INIT() macros are used only in DEFINE() macros
    
      + init() functions are used close to the other kthread()
        functions. It looks much better if all the functions
        use the same scheme.
    
      + There will be also kthread_destroy_worker() that will
        be used close to kthread_cancel_work(). It is related
        to the init() function. Again it looks better if all
        functions use the same naming scheme.
    
      + there are several precedents for such init() function
        names, e.g. amd_iommu_init_device(), free_area_init_node(),
        jump_label_init_type(),  regmap_init_mmio_clk(),
    
      + It is not an argument but it was inconsistent even before.
    
    [arnd@arndb.de: fix linux-next merge conflict]
     Link: http://lkml.kernel.org/r/20160908135724.1311726-1-arnd@arndb.de
    Link: http://lkml.kernel.org/r/1470754545-17632-3-git-send-email-pmladek@suse.com
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 7e77b728f96b..c52a05a8ec52 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -540,7 +540,7 @@ int kthreadd(void *unused)
 	return 0;
 }
 
-void __init_kthread_worker(struct kthread_worker *worker,
+void __kthread_init_worker(struct kthread_worker *worker,
 				const char *name,
 				struct lock_class_key *key)
 {
@@ -549,7 +549,7 @@ void __init_kthread_worker(struct kthread_worker *worker,
 	INIT_LIST_HEAD(&worker->work_list);
 	worker->task = NULL;
 }
-EXPORT_SYMBOL_GPL(__init_kthread_worker);
+EXPORT_SYMBOL_GPL(__kthread_init_worker);
 
 /**
  * kthread_worker_fn - kthread function to process kthread_worker
@@ -606,7 +606,7 @@ int kthread_worker_fn(void *worker_ptr)
 EXPORT_SYMBOL_GPL(kthread_worker_fn);
 
 /* insert @work before @pos in @worker */
-static void insert_kthread_work(struct kthread_worker *worker,
+static void kthread_insert_work(struct kthread_worker *worker,
 			       struct kthread_work *work,
 			       struct list_head *pos)
 {
@@ -619,7 +619,7 @@ static void insert_kthread_work(struct kthread_worker *worker,
 }
 
 /**
- * queue_kthread_work - queue a kthread_work
+ * kthread_queue_work - queue a kthread_work
  * @worker: target kthread_worker
  * @work: kthread_work to queue
  *
@@ -627,7 +627,7 @@ static void insert_kthread_work(struct kthread_worker *worker,
  * must have been created with kthread_worker_create().  Returns %true
  * if @work was successfully queued, %false if it was already pending.
  */
-bool queue_kthread_work(struct kthread_worker *worker,
+bool kthread_queue_work(struct kthread_worker *worker,
 			struct kthread_work *work)
 {
 	bool ret = false;
@@ -635,13 +635,13 @@ bool queue_kthread_work(struct kthread_worker *worker,
 
 	spin_lock_irqsave(&worker->lock, flags);
 	if (list_empty(&work->node)) {
-		insert_kthread_work(worker, work, &worker->work_list);
+		kthread_insert_work(worker, work, &worker->work_list);
 		ret = true;
 	}
 	spin_unlock_irqrestore(&worker->lock, flags);
 	return ret;
 }
-EXPORT_SYMBOL_GPL(queue_kthread_work);
+EXPORT_SYMBOL_GPL(kthread_queue_work);
 
 struct kthread_flush_work {
 	struct kthread_work	work;
@@ -656,12 +656,12 @@ static void kthread_flush_work_fn(struct kthread_work *work)
 }
 
 /**
- * flush_kthread_work - flush a kthread_work
+ * kthread_flush_work - flush a kthread_work
  * @work: work to flush
  *
  * If @work is queued or executing, wait for it to finish execution.
  */
-void flush_kthread_work(struct kthread_work *work)
+void kthread_flush_work(struct kthread_work *work)
 {
 	struct kthread_flush_work fwork = {
 		KTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),
@@ -682,9 +682,10 @@ void flush_kthread_work(struct kthread_work *work)
 	}
 
 	if (!list_empty(&work->node))
-		insert_kthread_work(worker, &fwork.work, work->node.next);
+		kthread_insert_work(worker, &fwork.work, work->node.next);
 	else if (worker->current_work == work)
-		insert_kthread_work(worker, &fwork.work, worker->work_list.next);
+		kthread_insert_work(worker, &fwork.work,
+				    worker->work_list.next);
 	else
 		noop = true;
 
@@ -693,23 +694,23 @@ void flush_kthread_work(struct kthread_work *work)
 	if (!noop)
 		wait_for_completion(&fwork.done);
 }
-EXPORT_SYMBOL_GPL(flush_kthread_work);
+EXPORT_SYMBOL_GPL(kthread_flush_work);
 
 /**
- * flush_kthread_worker - flush all current works on a kthread_worker
+ * kthread_flush_worker - flush all current works on a kthread_worker
  * @worker: worker to flush
  *
  * Wait until all currently executing or pending works on @worker are
  * finished.
  */
-void flush_kthread_worker(struct kthread_worker *worker)
+void kthread_flush_worker(struct kthread_worker *worker)
 {
 	struct kthread_flush_work fwork = {
 		KTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),
 		COMPLETION_INITIALIZER_ONSTACK(fwork.done),
 	};
 
-	queue_kthread_work(worker, &fwork.work);
+	kthread_queue_work(worker, &fwork.work);
 	wait_for_completion(&fwork.done);
 }
-EXPORT_SYMBOL_GPL(flush_kthread_worker);
+EXPORT_SYMBOL_GPL(kthread_flush_worker);

commit e700591ae03896c16974d4e1ab58eb296aaa5f59
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:17 2016 -0700

    kthread: rename probe_kthread_data() to kthread_probe_data()
    
    Patch series "kthread: Kthread worker API improvements"
    
    The intention of this patchset is to make it easier to manipulate and
    maintain kthreads.  Especially, I want to replace all the custom main
    cycles with a generic one.  Also I want to make the kthreads sleep in a
    consistent state in a common place when there is no work.
    
    This patch (of 11):
    
    A good practice is to prefix the names of functions by the name of the
    subsystem.
    
    This patch fixes the name of probe_kthread_data().  The other wrong
    functions names are part of the kthread worker API and will be fixed
    separately.
    
    Link: http://lkml.kernel.org/r/1470754545-17632-2-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 4ab4c3766a80..7e77b728f96b 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -138,7 +138,7 @@ void *kthread_data(struct task_struct *task)
 }
 
 /**
- * probe_kthread_data - speculative version of kthread_data()
+ * kthread_probe_data - speculative version of kthread_data()
  * @task: possible kthread task in question
  *
  * @task could be a kthread task.  Return the data value specified when it
@@ -146,7 +146,7 @@ void *kthread_data(struct task_struct *task)
  * inaccessible for any reason, %NULL is returned.  This function requires
  * that @task itself is safe to dereference.
  */
-void *probe_kthread_data(struct task_struct *task)
+void *kthread_probe_data(struct task_struct *task)
 {
 	struct kthread *kthread = to_kthread(task);
 	void *data = NULL;

commit 23196f2e5f5d810578a772785807dcdc2b9fdce9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Sep 15 22:45:44 2016 -0700

    kthread: Pin the stack via try_get_task_stack()/put_task_stack() in to_live_kthread() function
    
    get_task_struct(tsk) no longer pins tsk->stack so all users of
    to_live_kthread() should do try_get_task_stack/put_task_stack to protect
    "struct kthread" which lives on kthread's stack.
    
    TODO: Kill to_live_kthread(), perhaps we can even kill "struct kthread" too,
    and rework kthread_stop(), it can use task_work_add() to sync with the exiting
    kernel thread.
    
    Message-Id: <20160629180357.GA7178@redhat.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/cb9b16bbc19d4aea4507ab0552e4644c1211d130.1474003868.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 9ff173dca1ae..4ab4c3766a80 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -64,7 +64,7 @@ static inline struct kthread *to_kthread(struct task_struct *k)
 static struct kthread *to_live_kthread(struct task_struct *k)
 {
 	struct completion *vfork = ACCESS_ONCE(k->vfork_done);
-	if (likely(vfork))
+	if (likely(vfork) && try_get_task_stack(k))
 		return __to_kthread(vfork);
 	return NULL;
 }
@@ -425,8 +425,10 @@ void kthread_unpark(struct task_struct *k)
 {
 	struct kthread *kthread = to_live_kthread(k);
 
-	if (kthread)
+	if (kthread) {
 		__kthread_unpark(k, kthread);
+		put_task_stack(k);
+	}
 }
 EXPORT_SYMBOL_GPL(kthread_unpark);
 
@@ -455,6 +457,7 @@ int kthread_park(struct task_struct *k)
 				wait_for_completion(&kthread->parked);
 			}
 		}
+		put_task_stack(k);
 		ret = 0;
 	}
 	return ret;
@@ -490,6 +493,7 @@ int kthread_stop(struct task_struct *k)
 		__kthread_unpark(k, kthread);
 		wake_up_process(k);
 		wait_for_completion(&kthread->exited);
+		put_task_stack(k);
 	}
 	ret = k->exit_code;
 	put_task_struct(k);

commit e9f069868d60550c4b46f084ac9276a57c1b4711
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Fri Sep 4 15:42:42 2015 -0700

    kernel/kthread.c:kthread_create_on_node(): clarify documentation
    
    - Make it clear that the `node' arg refers to memory allocations only:
      kthread_create_on_node() does not pin the new thread to that node's
      CPUs.
    
    - Encourage the use of NUMA_NO_NODE.
    
    [nzimmer@sgi.com: use NUMA_NO_NODE in kthread_create() also]
    Cc: Nathan Zimmer <nzimmer@sgi.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 490924cc9e7c..9ff173dca1ae 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -248,15 +248,16 @@ static void create_kthread(struct kthread_create_info *create)
  * kthread_create_on_node - create a kthread.
  * @threadfn: the function to run until signal_pending(current).
  * @data: data ptr for @threadfn.
- * @node: memory node number.
+ * @node: task and thread structures for the thread are allocated on this node
  * @namefmt: printf-style name for the thread.
  *
  * Description: This helper function creates and names a kernel
  * thread.  The thread will be stopped: use wake_up_process() to start
- * it.  See also kthread_run().
+ * it.  See also kthread_run().  The new thread has SCHED_NORMAL policy and
+ * is affine to all CPUs.
  *
  * If thread is going to be bound on a particular cpu, give its node
- * in @node, to get NUMA affinity for kthread stack, or else give -1.
+ * in @node, to get NUMA affinity for kthread stack, or else give NUMA_NO_NODE.
  * When woken, the thread will run @threadfn() with @data as its
  * argument. @threadfn() can either call do_exit() directly if it is a
  * standalone thread for which no one will call kthread_stop(), or

commit a1d8561172f369ba56d636df49a6b4d6d77e2123
Merge: 3959df1dfb95 ff277d4250fe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 31 20:26:22 2015 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The biggest change in this cycle is the rewrite of the main SMP load
      balancing metric: the CPU load/utilization.  The main goal was to make
      the metric more precise and more representative - see the changelog of
      this commit for the gory details:
    
        9d89c257dfb9 ("sched/fair: Rewrite runnable load and utilization average tracking")
    
      It is done in a way that significantly reduces complexity of the code:
    
        5 files changed, 249 insertions(+), 494 deletions(-)
    
      and the performance testing results are encouraging.  Nevertheless we
      need to keep an eye on potential regressions, since this potentially
      affects every SMP workload in existence.
    
      This work comes from Yuyang Du.
    
      Other changes:
    
       - SCHED_DL updates.  (Andrea Parri)
    
       - Simplify architecture callbacks by removing finish_arch_switch().
         (Peter Zijlstra et al)
    
       - cputime accounting: guarantee stime + utime == rtime.  (Peter
         Zijlstra)
    
       - optimize idle CPU wakeups some more - inspired by Facebook server
         loads.  (Mike Galbraith)
    
       - stop_machine fixes and updates.  (Oleg Nesterov)
    
       - Introduce the 'trace_sched_waking' tracepoint.  (Peter Zijlstra)
    
       - sched/numa tweaks.  (Srikar Dronamraju)
    
       - misc fixes and small cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      sched/deadline: Fix comment in enqueue_task_dl()
      sched/deadline: Fix comment in push_dl_tasks()
      sched: Change the sched_class::set_cpus_allowed() calling context
      sched: Make sched_class::set_cpus_allowed() unconditional
      sched: Fix a race between __kthread_bind() and sched_setaffinity()
      sched: Ensure a task has a non-normalized vruntime when returning back to CFS
      sched/numa: Fix NUMA_DIRECT topology identification
      tile: Reorganize _switch_to()
      sched, sparc32: Update scheduler comments in copy_thread()
      sched: Remove finish_arch_switch()
      sched, tile: Remove finish_arch_switch
      sched, sh: Fold finish_arch_switch() into switch_to()
      sched, score: Remove finish_arch_switch()
      sched, avr32: Remove finish_arch_switch()
      sched, MIPS: Get rid of finish_arch_switch()
      sched, arm: Remove finish_arch_switch()
      sched/fair: Clean up load average references
      sched/fair: Provide runnable_load_avg back to cfs_rq
      sched/fair: Remove task and group entity load when they are dead
      sched/fair: Init cfs_rq's sched_entity load average
      ...

commit 25834c73f93af7f0712c98ca4593691592e6b360
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 15 17:43:34 2015 +0200

    sched: Fix a race between __kthread_bind() and sched_setaffinity()
    
    Because sched_setscheduler() checks p->flags & PF_NO_SETAFFINITY
    without locks, a caller might observe an old value and race with the
    set_cpus_allowed_ptr() call from __kthread_bind() and effectively undo
    it:
    
            __kthread_bind()
              do_set_cpus_allowed()
                                                    <SYSCALL>
                                                      sched_setaffinity()
                                                        if (p->flags & PF_NO_SETAFFINITIY)
                                                        set_cpus_allowed_ptr()
              p->flags |= PF_NO_SETAFFINITY
    
    Fix the bug by putting everything under the regular scheduler locks.
    
    This also closes a hole in the serialization of task_struct::{nr_,}cpus_allowed.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dedekind1@gmail.com
    Cc: juri.lelli@arm.com
    Cc: mgorman@suse.de
    Cc: riel@redhat.com
    Cc: rostedt@goodmis.org
    Link: http://lkml.kernel.org/r/20150515154833.545640346@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 10e489c448fe..7c40a189becc 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -325,16 +325,30 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 }
 EXPORT_SYMBOL(kthread_create_on_node);
 
-static void __kthread_bind(struct task_struct *p, unsigned int cpu, long state)
+static void __kthread_bind_mask(struct task_struct *p, const struct cpumask *mask, long state)
 {
-	/* Must have done schedule() in kthread() before we set_task_cpu */
+	unsigned long flags;
+
 	if (!wait_task_inactive(p, state)) {
 		WARN_ON(1);
 		return;
 	}
+
 	/* It's safe because the task is inactive. */
-	do_set_cpus_allowed(p, cpumask_of(cpu));
+	raw_spin_lock_irqsave(&p->pi_lock, flags);
+	do_set_cpus_allowed(p, mask);
 	p->flags |= PF_NO_SETAFFINITY;
+	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+}
+
+static void __kthread_bind(struct task_struct *p, unsigned int cpu, long state)
+{
+	__kthread_bind_mask(p, cpumask_of(cpu), state);
+}
+
+void kthread_bind_mask(struct task_struct *p, const struct cpumask *mask)
+{
+	__kthread_bind_mask(p, mask, TASK_UNINTERRUPTIBLE);
 }
 
 /**

commit 18896451eaeee497ef5c397d76902c6376a8787d
Author: David Kershner <david.kershner@unisys.com>
Date:   Thu Aug 6 15:46:45 2015 -0700

    kthread: export kthread functions
    
    The s-Par visornic driver, currently in staging, processes a queue being
    serviced by the an s-Par service partition.  We can get a message that
    something has happened with the Service Partition, when that happens, we
    must not access the channel until we get a message that the service
    partition is back again.
    
    The visornic driver has a thread for processing the channel, when we get
    the message, we need to be able to park the thread and then resume it
    when the problem clears.
    
    We can do this with kthread_park and unpark but they are not exported
    from the kernel, this patch exports the needed functions.
    
    Signed-off-by: David Kershner <david.kershner@unisys.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Neil Horman <nhorman@tuxdriver.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Weinberger <richard.weinberger@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 10e489c448fe..fdea0bee7b5a 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -97,6 +97,7 @@ bool kthread_should_park(void)
 {
 	return test_bit(KTHREAD_SHOULD_PARK, &to_kthread(current)->flags);
 }
+EXPORT_SYMBOL_GPL(kthread_should_park);
 
 /**
  * kthread_freezable_should_stop - should this freezable kthread return now?
@@ -171,6 +172,7 @@ void kthread_parkme(void)
 {
 	__kthread_parkme(to_kthread(current));
 }
+EXPORT_SYMBOL_GPL(kthread_parkme);
 
 static int kthread(void *_create)
 {
@@ -411,6 +413,7 @@ void kthread_unpark(struct task_struct *k)
 	if (kthread)
 		__kthread_unpark(k, kthread);
 }
+EXPORT_SYMBOL_GPL(kthread_unpark);
 
 /**
  * kthread_park - park a thread created by kthread_create().
@@ -441,6 +444,7 @@ int kthread_park(struct task_struct *k)
 	}
 	return ret;
 }
+EXPORT_SYMBOL_GPL(kthread_park);
 
 /**
  * kthread_stop - stop a thread created by kthread_create().

commit 109228389a943edd7e5c6ae94a7fda119691baec
Author: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
Date:   Thu Oct 9 15:26:18 2014 -0700

    kernel/kthread.c: partial revert of 81c98869faa5 ("kthread: ensure locality of task_struct allocations")
    
    After discussions with Tejun, we don't want to spread the use of
    cpu_to_mem() (and thus knowledge of allocators/NUMA topology details) into
    callers, but would rather ensure the callees correctly handle memoryless
    nodes.  With the previous patches ("topology: add support for
    node_to_mem_node() to determine the fallback node" and "slub: fallback to
    node_to_mem_node() node if allocating on memoryless node") adding and
    using node_to_mem_node(), we can safely undo part of the change to the
    kthread logic from 81c98869faa5.
    
    Signed-off-by: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Han Pingtian <hanpt@linux.vnet.ibm.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index ef483220e855..10e489c448fe 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -369,7 +369,7 @@ struct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),
 {
 	struct task_struct *p;
 
-	p = kthread_create_on_node(threadfn, data, cpu_to_mem(cpu), namefmt,
+	p = kthread_create_on_node(threadfn, data, cpu_to_node(cpu), namefmt,
 				   cpu);
 	if (IS_ERR(p))
 		return p;

commit ed1403ec2bd463050e481cd29be40c504021676c
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Sat Jul 26 12:03:59 2014 +0800

    kthread_work: wake up worker only when the worker is idle
    
    If the worker is already executing a work item when another is queued,
    we can safely skip wakeup without worrying about stalling queue thus
    avoiding waking up the busy worker spuriously.  Spurious wakeups
    should be fine but still isn't nice and avoiding it is trivial here.
    
    tj: Updated description.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index c2390f41307b..ef483220e855 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -591,7 +591,7 @@ static void insert_kthread_work(struct kthread_worker *worker,
 
 	list_add_tail(&work->node, pos);
 	work->worker = worker;
-	if (likely(worker->task))
+	if (!worker->current_work && likely(worker->task))
 		wake_up_process(worker->task);
 }
 

commit 8fe6929cfd43c44834858a53e129ffdc7c166298
Author: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
Date:   Wed Jun 4 16:05:36 2014 -0700

    kthread: fix return value of kthread_create() upon SIGKILL.
    
    Commit 786235eeba0e ("kthread: make kthread_create() killable") meant
    for allowing kthread_create() to abort as soon as killed by the
    OOM-killer.  But returning -ENOMEM is wrong if killed by SIGKILL from
    userspace.  Change kthread_create() to return -EINTR upon SIGKILL.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: <stable@vger.kernel.org> [3.13+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 9a130ec06f7a..c2390f41307b 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -262,7 +262,7 @@ static void create_kthread(struct kthread_create_info *create)
  * kthread_stop() has been called).  The return value should be zero
  * or a negative error number; it will be passed to kthread_stop().
  *
- * Returns a task_struct or ERR_PTR(-ENOMEM).
+ * Returns a task_struct or ERR_PTR(-ENOMEM) or ERR_PTR(-EINTR).
  */
 struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 					   void *data, int node,
@@ -298,7 +298,7 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 		 * that thread.
 		 */
 		if (xchg(&create->done, NULL))
-			return ERR_PTR(-ENOMEM);
+			return ERR_PTR(-EINTR);
 		/*
 		 * kthreadd (or new kernel thread) will call complete()
 		 * shortly.

commit 81c98869faa5f3a9457c93efef908ef476326b31
Author: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
Date:   Thu Apr 3 14:46:25 2014 -0700

    kthread: ensure locality of task_struct allocations
    
    In the presence of memoryless nodes, numa_node_id() will return the
    current CPU's NUMA node, but that may not be where we expect to allocate
    from memory from.  Instead, we should rely on the fallback code in the
    memory allocator itself, by using NUMA_NO_NODE.  Also, when calling
    kthread_create_on_node(), use the nearest node with memory to the cpu in
    question, rather than the node it is running on.
    
    Signed-off-by: Nishanth Aravamudan <nacc@linux.vnet.ibm.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Ben Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index b5ae3ee860a9..9a130ec06f7a 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -217,7 +217,7 @@ int tsk_fork_get_node(struct task_struct *tsk)
 	if (tsk == kthreadd_task)
 		return tsk->pref_node_fork;
 #endif
-	return numa_node_id();
+	return NUMA_NO_NODE;
 }
 
 static void create_kthread(struct kthread_create_info *create)
@@ -369,7 +369,7 @@ struct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),
 {
 	struct task_struct *p;
 
-	p = kthread_create_on_node(threadfn, data, cpu_to_node(cpu), namefmt,
+	p = kthread_create_on_node(threadfn, data, cpu_to_mem(cpu), namefmt,
 				   cpu);
 	if (IS_ERR(p))
 		return p;

commit 786235eeba0e1e85e5cbbb9f97d1087ad03dfa21
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Tue Nov 12 15:06:45 2013 -0800

    kthread: make kthread_create() killable
    
    Any user process callers of wait_for_completion() except global init
    process might be chosen by the OOM killer while waiting for completion()
    call by some other process which does memory allocation.  See
    CVE-2012-4398 "kernel: request_module() OOM local DoS" can happen.
    
    When such users are chosen by the OOM killer when they are waiting for
    completion() in TASK_UNINTERRUPTIBLE, the system will be kept stressed
    due to memory starvation because the OOM killer cannot kill such users.
    
    kthread_create() is one of such users and this patch fixes the problem
    for kthreadd by making kthread_create() killable - the same approach
    used for fixing CVE-2012-4398.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 760e86df8c20..b5ae3ee860a9 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -33,7 +33,7 @@ struct kthread_create_info
 
 	/* Result passed back to kthread_create() from kthreadd. */
 	struct task_struct *result;
-	struct completion done;
+	struct completion *done;
 
 	struct list_head list;
 };
@@ -178,6 +178,7 @@ static int kthread(void *_create)
 	struct kthread_create_info *create = _create;
 	int (*threadfn)(void *data) = create->threadfn;
 	void *data = create->data;
+	struct completion *done;
 	struct kthread self;
 	int ret;
 
@@ -187,10 +188,16 @@ static int kthread(void *_create)
 	init_completion(&self.parked);
 	current->vfork_done = &self.exited;
 
+	/* If user was SIGKILLed, I release the structure. */
+	done = xchg(&create->done, NULL);
+	if (!done) {
+		kfree(create);
+		do_exit(-EINTR);
+	}
 	/* OK, tell user we're spawned, wait for stop or wakeup */
 	__set_current_state(TASK_UNINTERRUPTIBLE);
 	create->result = current;
-	complete(&create->done);
+	complete(done);
 	schedule();
 
 	ret = -EINTR;
@@ -223,8 +230,15 @@ static void create_kthread(struct kthread_create_info *create)
 	/* We want our own signal handler (we take no signals by default). */
 	pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
 	if (pid < 0) {
+		/* If user was SIGKILLed, I release the structure. */
+		struct completion *done = xchg(&create->done, NULL);
+
+		if (!done) {
+			kfree(create);
+			return;
+		}
 		create->result = ERR_PTR(pid);
-		complete(&create->done);
+		complete(done);
 	}
 }
 
@@ -255,36 +269,59 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 					   const char namefmt[],
 					   ...)
 {
-	struct kthread_create_info create;
-
-	create.threadfn = threadfn;
-	create.data = data;
-	create.node = node;
-	init_completion(&create.done);
+	DECLARE_COMPLETION_ONSTACK(done);
+	struct task_struct *task;
+	struct kthread_create_info *create = kmalloc(sizeof(*create),
+						     GFP_KERNEL);
+
+	if (!create)
+		return ERR_PTR(-ENOMEM);
+	create->threadfn = threadfn;
+	create->data = data;
+	create->node = node;
+	create->done = &done;
 
 	spin_lock(&kthread_create_lock);
-	list_add_tail(&create.list, &kthread_create_list);
+	list_add_tail(&create->list, &kthread_create_list);
 	spin_unlock(&kthread_create_lock);
 
 	wake_up_process(kthreadd_task);
-	wait_for_completion(&create.done);
-
-	if (!IS_ERR(create.result)) {
+	/*
+	 * Wait for completion in killable state, for I might be chosen by
+	 * the OOM killer while kthreadd is trying to allocate memory for
+	 * new kernel thread.
+	 */
+	if (unlikely(wait_for_completion_killable(&done))) {
+		/*
+		 * If I was SIGKILLed before kthreadd (or new kernel thread)
+		 * calls complete(), leave the cleanup of this structure to
+		 * that thread.
+		 */
+		if (xchg(&create->done, NULL))
+			return ERR_PTR(-ENOMEM);
+		/*
+		 * kthreadd (or new kernel thread) will call complete()
+		 * shortly.
+		 */
+		wait_for_completion(&done);
+	}
+	task = create->result;
+	if (!IS_ERR(task)) {
 		static const struct sched_param param = { .sched_priority = 0 };
 		va_list args;
 
 		va_start(args, namefmt);
-		vsnprintf(create.result->comm, sizeof(create.result->comm),
-			  namefmt, args);
+		vsnprintf(task->comm, sizeof(task->comm), namefmt, args);
 		va_end(args);
 		/*
 		 * root may have changed our (kthreadd's) priority or CPU mask.
 		 * The kernel thread should not inherit these properties.
 		 */
-		sched_setscheduler_nocheck(create.result, SCHED_NORMAL, &param);
-		set_cpus_allowed_ptr(create.result, cpu_all_mask);
+		sched_setscheduler_nocheck(task, SCHED_NORMAL, &param);
+		set_cpus_allowed_ptr(task, cpu_all_mask);
 	}
-	return create.result;
+	kfree(create);
+	return task;
 }
 EXPORT_SYMBOL(kthread_create_on_node);
 

commit cd42d559e45e3563c74403e453f8954b593db69d
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 30 15:27:21 2013 -0700

    kthread: implement probe_kthread_data()
    
    One of the problems that arise when converting dedicated custom threadpool
    to workqueue is that the shared worker pool used by workqueue anonimizes
    each worker making it more difficult to identify what the worker was doing
    on which target from the output of sysrq-t or debug dump from oops, BUG()
    and friends.
    
    For example, after writeback is converted to use workqueue instead of
    priviate thread pool, there's no easy to tell which backing device a
    writeback work item was working on at the time of task dump, which,
    according to our writeback brethren, is important in tracking down issues
    with a lot of mounted file systems on a lot of different devices.
    
    This patchset implements a way for a work function to mark its execution
    instance so that task dump of the worker task includes information to
    indicate what the work item was doing.
    
    An example WARN dump would look like the following.
    
     WARNING: at fs/fs-writeback.c:1015 bdi_writeback_workfn+0x2b4/0x3c0()
     Modules linked in:
     CPU: 0 Pid: 28 Comm: kworker/u18:0 Not tainted 3.9.0-rc1-work+ #24
     Hardware name: empty empty/S3992, BIOS 080011  10/26/2007
     Workqueue: writeback bdi_writeback_workfn (flush-8:16)
      ffffffff820a3a98 ffff88015b927cb8 ffffffff81c61855 ffff88015b927cf8
      ffffffff8108f500 0000000000000000 ffff88007a171948 ffff88007a1716b0
      ffff88015b49df00 ffff88015b8d3940 0000000000000000 ffff88015b927d08
     Call Trace:
      [<ffffffff81c61855>] dump_stack+0x19/0x1b
      [<ffffffff8108f500>] warn_slowpath_common+0x70/0xa0
      ...
    
    This patch:
    
    Implement probe_kthread_data() which returns kthread_data if accessible.
    The function is equivalent to kthread_data() except that the specified
    @task may not be a kthread or its vfork_done is already cleared rendering
    struct kthread inaccessible.  In the former case, probe_kthread_data() may
    return any value.  In the latter, NULL.
    
    This will be used to safely print debug information without affecting
    synchronization in the normal paths.  Workqueue debug info printing on
    dump_stack() and friends will make use of it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Jan Kara <jack@suse.cz>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 16d8ddd268b1..760e86df8c20 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -17,6 +17,7 @@
 #include <linux/slab.h>
 #include <linux/freezer.h>
 #include <linux/ptrace.h>
+#include <linux/uaccess.h>
 #include <trace/events/sched.h>
 
 static DEFINE_SPINLOCK(kthread_create_lock);
@@ -135,6 +136,24 @@ void *kthread_data(struct task_struct *task)
 	return to_kthread(task)->data;
 }
 
+/**
+ * probe_kthread_data - speculative version of kthread_data()
+ * @task: possible kthread task in question
+ *
+ * @task could be a kthread task.  Return the data value specified when it
+ * was created if accessible.  If @task isn't a kthread task or its data is
+ * inaccessible for any reason, %NULL is returned.  This function requires
+ * that @task itself is safe to dereference.
+ */
+void *probe_kthread_data(struct task_struct *task)
+{
+	struct kthread *kthread = to_kthread(task);
+	void *data = NULL;
+
+	probe_kernel_read(&data, &kthread->data, sizeof(data));
+	return data;
+}
+
 static void __kthread_parkme(struct kthread *self)
 {
 	__set_current_state(TASK_PARKED);

commit 46d9be3e5eb01f71fc02653755d970247174b400
Merge: ce8aa4892944 cece95dfe5aa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 29 19:07:40 2013 -0700

    Merge branch 'for-3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq
    
    Pull workqueue updates from Tejun Heo:
     "A lot of activities on workqueue side this time.  The changes achieve
      the followings.
    
       - WQ_UNBOUND workqueues - the workqueues which are per-cpu - are
         updated to be able to interface with multiple backend worker pools.
         This involved a lot of churning but the end result seems actually
         neater as unbound workqueues are now a lot closer to per-cpu ones.
    
       - The ability to interface with multiple backend worker pools are
         used to implement unbound workqueues with custom attributes.
         Currently the supported attributes are the nice level and CPU
         affinity.  It may be expanded to include cgroup association in
         future.  The attributes can be specified either by calling
         apply_workqueue_attrs() or through /sys/bus/workqueue/WQ_NAME/* if
         the workqueue in question is exported through sysfs.
    
         The backend worker pools are keyed by the actual attributes and
         shared by any workqueues which share the same attributes.  When
         attributes of a workqueue are changed, the workqueue binds to the
         worker pool with the specified attributes while leaving the work
         items which are already executing in its previous worker pools
         alone.
    
         This allows converting custom worker pool implementations which
         want worker attribute tuning to use workqueues.  The writeback pool
         is already converted in block tree and there are a couple others
         are likely to follow including btrfs io workers.
    
       - WQ_UNBOUND's ability to bind to multiple worker pools is also used
         to make it NUMA-aware.  Because there's no association between work
         item issuer and the specific worker assigned to execute it, before
         this change, using unbound workqueue led to unnecessary cross-node
         bouncing and it couldn't be helped by autonuma as it requires tasks
         to have implicit node affinity and workers are assigned randomly.
    
         After these changes, an unbound workqueue now binds to multiple
         NUMA-affine worker pools so that queued work items are executed in
         the same node.  This is turned on by default but can be disabled
         system-wide or for individual workqueues.
    
         Crypto was requesting NUMA affinity as encrypting data across
         different nodes can contribute noticeable overhead and doing it
         per-cpu was too limiting for certain cases and IO throughput could
         be bottlenecked by one CPU being fully occupied while others have
         idle cycles.
    
      While the new features required a lot of changes including
      restructuring locking, it didn't complicate the execution paths much.
      The unbound workqueue handling is now closer to per-cpu ones and the
      new features are implemented by simply associating a workqueue with
      different sets of backend worker pools without changing queue,
      execution or flush paths.
    
      As such, even though the amount of change is very high, I feel
      relatively safe in that it isn't likely to cause subtle issues with
      basic correctness of work item execution and handling.  If something
      is wrong, it's likely to show up as being associated with worker pools
      with the wrong attributes or OOPS while workqueue attributes are being
      changed or during CPU hotplug.
    
      While this creates more backend worker pools, it doesn't add too many
      more workers unless, of course, there are many workqueues with unique
      combinations of attributes.  Assuming everything else is the same,
      NUMA awareness costs an extra worker pool per NUMA node with online
      CPUs.
    
      There are also a couple things which are being routed outside the
      workqueue tree.
    
       - block tree pulled in workqueue for-3.10 so that writeback worker
         pool can be converted to unbound workqueue with sysfs control
         exposed.  This simplifies the code, makes writeback workers
         NUMA-aware and allows tuning nice level and CPU affinity via sysfs.
    
       - The conversion to workqueue means that there's no 1:1 association
         between a specific worker, which makes writeback folks unhappy as
         they want to be able to tell which filesystem caused a problem from
         backtrace on systems with many filesystems mounted.  This is
         resolved by allowing work items to set debug info string which is
         printed when the task is dumped.  As this change involves unifying
         implementations of dump_stack() and friends in arch codes, it's
         being routed through Andrew's -mm tree."
    
    * 'for-3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/wq: (84 commits)
      workqueue: use kmem_cache_free() instead of kfree()
      workqueue: avoid false negative WARN_ON() in destroy_workqueue()
      workqueue: update sysfs interface to reflect NUMA awareness and a kernel param to disable NUMA affinity
      workqueue: implement NUMA affinity for unbound workqueues
      workqueue: introduce put_pwq_unlocked()
      workqueue: introduce numa_pwq_tbl_install()
      workqueue: use NUMA-aware allocation for pool_workqueues
      workqueue: break init_and_link_pwq() into two functions and introduce alloc_unbound_pwq()
      workqueue: map an unbound workqueues to multiple per-node pool_workqueues
      workqueue: move hot fields of workqueue_struct to the end
      workqueue: make workqueue->name[] fixed len
      workqueue: add workqueue->unbound_attrs
      workqueue: determine NUMA node of workers accourding to the allowed cpumask
      workqueue: drop 'H' from kworker names of unbound worker pools
      workqueue: add wq_numa_tbl_len and wq_numa_possible_cpumask[]
      workqueue: move pwq_pool_locking outside of get/put_unbound_pool()
      workqueue: fix memory leak in apply_workqueue_attrs()
      workqueue: fix unbound workqueue attrs hashing / comparison
      workqueue: fix race condition in unbound workqueue free path
      workqueue: remove pwq_lock which is no longer used
      ...

commit b5c5442bb6bce0c67701d55124be561043a51faf
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 29 15:05:12 2013 -0700

    kthread: kill task_get_live_kthread()
    
    task_get_live_kthread() looks confusing and unneeded.  It does
    get_task_struct() but only kthread_stop() needs this, it can be called
    even if the calller doesn't have a reference when we know that this
    kthread can't exit until we do kthread_stop().
    
    kthread_park() and kthread_unpark() do not need get_task_struct(), the
    callers already have the reference.  And it can not help if we can race
    with the exiting kthread anyway, kthread_park() can hang forever in this
    case.
    
    Change kthread_park() and kthread_unpark() to use to_live_kthread(),
    change kthread_stop() to do get_task_struct() by hand and remove
    task_get_live_kthread().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Srivatsa S. Bhat" <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index a84dee265f94..9b12d65186f7 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -324,12 +324,6 @@ struct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),
 	return p;
 }
 
-static struct kthread *task_get_live_kthread(struct task_struct *k)
-{
-	get_task_struct(k);
-	return to_live_kthread(k);
-}
-
 static void __kthread_unpark(struct task_struct *k, struct kthread *kthread)
 {
 	clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
@@ -356,11 +350,10 @@ static void __kthread_unpark(struct task_struct *k, struct kthread *kthread)
  */
 void kthread_unpark(struct task_struct *k)
 {
-	struct kthread *kthread = task_get_live_kthread(k);
+	struct kthread *kthread = to_live_kthread(k);
 
 	if (kthread)
 		__kthread_unpark(k, kthread);
-	put_task_struct(k);
 }
 
 /**
@@ -377,7 +370,7 @@ void kthread_unpark(struct task_struct *k)
  */
 int kthread_park(struct task_struct *k)
 {
-	struct kthread *kthread = task_get_live_kthread(k);
+	struct kthread *kthread = to_live_kthread(k);
 	int ret = -ENOSYS;
 
 	if (kthread) {
@@ -390,7 +383,6 @@ int kthread_park(struct task_struct *k)
 		}
 		ret = 0;
 	}
-	put_task_struct(k);
 	return ret;
 }
 
@@ -411,10 +403,13 @@ int kthread_park(struct task_struct *k)
  */
 int kthread_stop(struct task_struct *k)
 {
-	struct kthread *kthread = task_get_live_kthread(k);
+	struct kthread *kthread;
 	int ret;
 
 	trace_sched_kthread_stop(k);
+
+	get_task_struct(k);
+	kthread = to_live_kthread(k);
 	if (kthread) {
 		set_bit(KTHREAD_SHOULD_STOP, &kthread->flags);
 		__kthread_unpark(k, kthread);
@@ -422,10 +417,9 @@ int kthread_stop(struct task_struct *k)
 		wait_for_completion(&kthread->exited);
 	}
 	ret = k->exit_code;
-
 	put_task_struct(k);
-	trace_sched_kthread_stop_ret(ret);
 
+	trace_sched_kthread_stop_ret(ret);
 	return ret;
 }
 EXPORT_SYMBOL(kthread_stop);

commit 4ecdafc8084fe1d95bb59ed7753b345abcd586fb
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 29 15:05:01 2013 -0700

    kthread: introduce to_live_kthread()
    
    "k->vfork_done != NULL" with a barrier() after to_kthread(k) in
    task_get_live_kthread(k) looks unclear, and sub-optimal because we load
    ->vfork_done twice.
    
    All we need is to ensure that we do not return to_kthread(NULL).  Add a
    new trivial helper which loads/checks ->vfork_done once, this also looks
    more understandable.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Srivatsa S. Bhat" <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 9eb7fed0bbaa..a84dee265f94 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -52,8 +52,21 @@ enum KTHREAD_BITS {
 	KTHREAD_IS_PARKED,
 };
 
-#define to_kthread(tsk)	\
-	container_of((tsk)->vfork_done, struct kthread, exited)
+#define __to_kthread(vfork)	\
+	container_of(vfork, struct kthread, exited)
+
+static inline struct kthread *to_kthread(struct task_struct *k)
+{
+	return __to_kthread(k->vfork_done);
+}
+
+static struct kthread *to_live_kthread(struct task_struct *k)
+{
+	struct completion *vfork = ACCESS_ONCE(k->vfork_done);
+	if (likely(vfork))
+		return __to_kthread(vfork);
+	return NULL;
+}
 
 /**
  * kthread_should_stop - should this kthread return now?
@@ -313,15 +326,8 @@ struct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),
 
 static struct kthread *task_get_live_kthread(struct task_struct *k)
 {
-	struct kthread *kthread;
-
 	get_task_struct(k);
-	kthread = to_kthread(k);
-	/* It might have exited */
-	barrier();
-	if (k->vfork_done != NULL)
-		return kthread;
-	return NULL;
+	return to_live_kthread(k);
 }
 
 static void __kthread_unpark(struct task_struct *k, struct kthread *kthread)

commit f2530dc71cf0822f90bb63ea4600caaef33a66bb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 9 09:33:34 2013 +0200

    kthread: Prevent unpark race which puts threads on the wrong cpu
    
    The smpboot threads rely on the park/unpark mechanism which binds per
    cpu threads on a particular core. Though the functionality is racy:
    
    CPU0                    CPU1                CPU2
    unpark(T)                                   wake_up_process(T)
      clear(SHOULD_PARK)    T runs
                            leave parkme() due to !SHOULD_PARK
      bind_to(CPU2)         BUG_ON(wrong CPU)
    
    We cannot let the tasks move themself to the target CPU as one of
    those tasks is actually the migration thread itself, which requires
    that it starts running on the target cpu right away.
    
    The solution to this problem is to prevent wakeups in park mode which
    are not from unpark(). That way we can guarantee that the association
    of the task to the target cpu is working correctly.
    
    Add a new task state (TASK_PARKED) which prevents other wakeups and
    use this state explicitly for the unpark wakeup.
    
    Peter noticed: Also, since the task state is visible to userspace and
    all the parked tasks are still in the PID space, its a good hint in ps
    and friends that these tasks aren't really there for the moment.
    
    The migration thread has another related issue.
    
    CPU0                     CPU1
    Bring up CPU2
    create_thread(T)
    park(T)
     wait_for_completion()
                             parkme()
                             complete()
    sched_set_stop_task()
                             schedule(TASK_PARKED)
    
    The sched_set_stop_task() call is issued while the task is on the
    runqueue of CPU1 and that confuses the hell out of the stop_task class
    on that cpu. So we need the same synchronizaion before
    sched_set_stop_task().
    
    Reported-by: Dave Jones <davej@redhat.com>
    Reported-and-tested-by: Dave Hansen <dave@sr71.net>
    Reported-and-tested-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Peter Ziljstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: dhillf@gmail.com
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1304091635430.21884@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 691dc2ef9baf..9eb7fed0bbaa 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -124,12 +124,12 @@ void *kthread_data(struct task_struct *task)
 
 static void __kthread_parkme(struct kthread *self)
 {
-	__set_current_state(TASK_INTERRUPTIBLE);
+	__set_current_state(TASK_PARKED);
 	while (test_bit(KTHREAD_SHOULD_PARK, &self->flags)) {
 		if (!test_and_set_bit(KTHREAD_IS_PARKED, &self->flags))
 			complete(&self->parked);
 		schedule();
-		__set_current_state(TASK_INTERRUPTIBLE);
+		__set_current_state(TASK_PARKED);
 	}
 	clear_bit(KTHREAD_IS_PARKED, &self->flags);
 	__set_current_state(TASK_RUNNING);
@@ -256,8 +256,13 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 }
 EXPORT_SYMBOL(kthread_create_on_node);
 
-static void __kthread_bind(struct task_struct *p, unsigned int cpu)
+static void __kthread_bind(struct task_struct *p, unsigned int cpu, long state)
 {
+	/* Must have done schedule() in kthread() before we set_task_cpu */
+	if (!wait_task_inactive(p, state)) {
+		WARN_ON(1);
+		return;
+	}
 	/* It's safe because the task is inactive. */
 	do_set_cpus_allowed(p, cpumask_of(cpu));
 	p->flags |= PF_THREAD_BOUND;
@@ -274,12 +279,7 @@ static void __kthread_bind(struct task_struct *p, unsigned int cpu)
  */
 void kthread_bind(struct task_struct *p, unsigned int cpu)
 {
-	/* Must have done schedule() in kthread() before we set_task_cpu */
-	if (!wait_task_inactive(p, TASK_UNINTERRUPTIBLE)) {
-		WARN_ON(1);
-		return;
-	}
-	__kthread_bind(p, cpu);
+	__kthread_bind(p, cpu, TASK_UNINTERRUPTIBLE);
 }
 EXPORT_SYMBOL(kthread_bind);
 
@@ -324,6 +324,22 @@ static struct kthread *task_get_live_kthread(struct task_struct *k)
 	return NULL;
 }
 
+static void __kthread_unpark(struct task_struct *k, struct kthread *kthread)
+{
+	clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
+	/*
+	 * We clear the IS_PARKED bit here as we don't wait
+	 * until the task has left the park code. So if we'd
+	 * park before that happens we'd see the IS_PARKED bit
+	 * which might be about to be cleared.
+	 */
+	if (test_and_clear_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
+		if (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))
+			__kthread_bind(k, kthread->cpu, TASK_PARKED);
+		wake_up_state(k, TASK_PARKED);
+	}
+}
+
 /**
  * kthread_unpark - unpark a thread created by kthread_create().
  * @k:		thread created by kthread_create().
@@ -336,20 +352,8 @@ void kthread_unpark(struct task_struct *k)
 {
 	struct kthread *kthread = task_get_live_kthread(k);
 
-	if (kthread) {
-		clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
-		/*
-		 * We clear the IS_PARKED bit here as we don't wait
-		 * until the task has left the park code. So if we'd
-		 * park before that happens we'd see the IS_PARKED bit
-		 * which might be about to be cleared.
-		 */
-		if (test_and_clear_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
-			if (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))
-				__kthread_bind(k, kthread->cpu);
-			wake_up_process(k);
-		}
-	}
+	if (kthread)
+		__kthread_unpark(k, kthread);
 	put_task_struct(k);
 }
 
@@ -407,7 +411,7 @@ int kthread_stop(struct task_struct *k)
 	trace_sched_kthread_stop(k);
 	if (kthread) {
 		set_bit(KTHREAD_SHOULD_STOP, &kthread->flags);
-		clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
+		__kthread_unpark(k, kthread);
 		wake_up_process(k);
 		wait_for_completion(&kthread->exited);
 	}

commit 14a40ffccd6163bbcd1d6f32b28a88ffe6149fc6
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Mar 19 13:45:20 2013 -0700

    sched: replace PF_THREAD_BOUND with PF_NO_SETAFFINITY
    
    PF_THREAD_BOUND was originally used to mark kernel threads which were
    bound to a specific CPU using kthread_bind() and a task with the flag
    set allows cpus_allowed modifications only to itself.  Workqueue is
    currently abusing it to prevent userland from meddling with
    cpus_allowed of workqueue workers.
    
    What we need is a flag to prevent userland from messing with
    cpus_allowed of certain kernel tasks.  In kernel, anyone can
    (incorrectly) squash the flag, and, for worker-type usages,
    restricting cpus_allowed modification to the task itself doesn't
    provide meaningful extra proection as other tasks can inject work
    items to the task anyway.
    
    This patch replaces PF_THREAD_BOUND with PF_NO_SETAFFINITY.
    sched_setaffinity() checks the flag and return -EINVAL if set.
    set_cpus_allowed_ptr() is no longer affected by the flag.
    
    This will allow simplifying workqueue worker CPU affinity management.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 691dc2ef9baf..a2fbbb782bad 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -260,7 +260,7 @@ static void __kthread_bind(struct task_struct *p, unsigned int cpu)
 {
 	/* It's safe because the task is inactive. */
 	do_set_cpus_allowed(p, cpumask_of(cpu));
-	p->flags |= PF_THREAD_BOUND;
+	p->flags |= PF_NO_SETAFFINITY;
 }
 
 /**

commit aee4faa499dc58fdb126885f68d447f2eba79b43
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Dec 12 13:51:39 2012 -0800

    kthread: use N_MEMORY instead N_HIGH_MEMORY
    
    N_HIGH_MEMORY stands for the nodes that has normal or high memory.
    N_MEMORY stands for the nodes that has any memory.
    
    The code here need to handle with the nodes which have memory, we should
    use N_MEMORY instead.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Lin Feng <linfeng@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 29fb60caecb5..691dc2ef9baf 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -428,7 +428,7 @@ int kthreadd(void *unused)
 	set_task_comm(tsk, "kthreadd");
 	ignore_signals(tsk);
 	set_cpus_allowed_ptr(tsk, cpu_all_mask);
-	set_mems_allowed(node_states[N_HIGH_MEMORY]);
+	set_mems_allowed(node_states[N_MEMORY]);
 
 	current->flags |= PF_NOFREEZE;
 

commit 4e21fc138bfd7fe625ff5dc81541399aaf9d429b
Merge: 8418263e3547 5522be6a4624
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 13 10:05:52 2012 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull third pile of kernel_execve() patches from Al Viro:
     "The last bits of infrastructure for kernel_thread() et.al., with
      alpha/arm/x86 use of those.  Plus sanitizing the asm glue and
      do_notify_resume() on alpha, fixing the "disabled irq while running
      task_work stuff" breakage there.
    
      At that point the rest of kernel_thread/kernel_execve/sys_execve work
      can be done independently for different architectures.  The only
      pending bits that do depend on having all architectures converted are
      restrictred to fs/* and kernel/* - that'll obviously have to wait for
      the next cycle.
    
      I thought we'd have to wait for all of them done before we start
      eliminating the longjump-style insanity in kernel_execve(), but it
      turned out there's a very simple way to do that without flagday-style
      changes."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal:
      alpha: switch to saner kernel_execve() semantics
      arm: switch to saner kernel_execve() semantics
      x86, um: convert to saner kernel_execve() semantics
      infrastructure for saner ret_from_kernel_thread semantics
      make sure that kernel_thread() callbacks call do_exit() themselves
      make sure that we always have a return path from kernel_execve()
      ppc: eeh_event should just use kthread_run()
      don't bother with kernel_thread/kernel_execve for launching linuxrc
      alpha: get rid of switch_stack argument of do_work_pending()
      alpha: don't bother passing switch_stack separately from regs
      alpha: take SIGPENDING/NOTIFY_RESUME loop into signal.c
      alpha: simplify TIF_NEED_RESCHED handling

commit a74fb73c12398b250fdc5e333a11e15a9e3a84fc
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Oct 10 21:28:25 2012 -0400

    infrastructure for saner ret_from_kernel_thread semantics
    
    * allow kernel_execve() leave the actual return to userland to
    caller (selected by CONFIG_GENERIC_KERNEL_EXECVE).  Callers
    updated accordingly.
    * architecture that does select GENERIC_KERNEL_EXECVE in its
    Kconfig should have its ret_from_kernel_thread() do this:
            call schedule_tail
            call the callback left for it by copy_thread(); if it ever
    returns, that's because it has just done successful kernel_execve()
            jump to return from syscall
    IOW, its only difference from ret_from_fork() is that it does call the
    callback.
    * such an architecture should also get rid of ret_from_kernel_execve()
    and __ARCH_WANT_KERNEL_EXECVE
    
    This is the last part of infrastructure patches in that area - from
    that point on work on different architectures can live independently.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index b579af57ea10..7ba65c1aa6b3 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -16,6 +16,7 @@
 #include <linux/mutex.h>
 #include <linux/slab.h>
 #include <linux/freezer.h>
+#include <linux/ptrace.h>
 #include <trace/events/sched.h>
 
 static DEFINE_SPINLOCK(kthread_create_lock);

commit 2a1d446019f9a5983ec5a335b95e8593fdb6fa2e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:36 2012 +0000

    kthread: Implement park/unpark facility
    
    To avoid the full teardown/setup of per cpu kthreads in the case of
    cpu hot(un)plug, provide a facility which allows to put the kthread
    into a park position and unpark it when the cpu comes online again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20120716103948.236618824@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index b579af57ea10..146a6fa96825 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -37,11 +37,20 @@ struct kthread_create_info
 };
 
 struct kthread {
-	int should_stop;
+	unsigned long flags;
+	unsigned int cpu;
 	void *data;
+	struct completion parked;
 	struct completion exited;
 };
 
+enum KTHREAD_BITS {
+	KTHREAD_IS_PER_CPU = 0,
+	KTHREAD_SHOULD_STOP,
+	KTHREAD_SHOULD_PARK,
+	KTHREAD_IS_PARKED,
+};
+
 #define to_kthread(tsk)	\
 	container_of((tsk)->vfork_done, struct kthread, exited)
 
@@ -52,12 +61,28 @@ struct kthread {
  * and this will return true.  You should then return, and your return
  * value will be passed through to kthread_stop().
  */
-int kthread_should_stop(void)
+bool kthread_should_stop(void)
 {
-	return to_kthread(current)->should_stop;
+	return test_bit(KTHREAD_SHOULD_STOP, &to_kthread(current)->flags);
 }
 EXPORT_SYMBOL(kthread_should_stop);
 
+/**
+ * kthread_should_park - should this kthread park now?
+ *
+ * When someone calls kthread_park() on your kthread, it will be woken
+ * and this will return true.  You should then do the necessary
+ * cleanup and call kthread_parkme()
+ *
+ * Similar to kthread_should_stop(), but this keeps the thread alive
+ * and in a park position. kthread_unpark() "restarts" the thread and
+ * calls the thread function again.
+ */
+bool kthread_should_park(void)
+{
+	return test_bit(KTHREAD_SHOULD_PARK, &to_kthread(current)->flags);
+}
+
 /**
  * kthread_freezable_should_stop - should this freezable kthread return now?
  * @was_frozen: optional out parameter, indicates whether %current was frozen
@@ -96,6 +121,24 @@ void *kthread_data(struct task_struct *task)
 	return to_kthread(task)->data;
 }
 
+static void __kthread_parkme(struct kthread *self)
+{
+	__set_current_state(TASK_INTERRUPTIBLE);
+	while (test_bit(KTHREAD_SHOULD_PARK, &self->flags)) {
+		if (!test_and_set_bit(KTHREAD_IS_PARKED, &self->flags))
+			complete(&self->parked);
+		schedule();
+		__set_current_state(TASK_INTERRUPTIBLE);
+	}
+	clear_bit(KTHREAD_IS_PARKED, &self->flags);
+	__set_current_state(TASK_RUNNING);
+}
+
+void kthread_parkme(void)
+{
+	__kthread_parkme(to_kthread(current));
+}
+
 static int kthread(void *_create)
 {
 	/* Copy data: it's on kthread's stack */
@@ -105,9 +148,10 @@ static int kthread(void *_create)
 	struct kthread self;
 	int ret;
 
-	self.should_stop = 0;
+	self.flags = 0;
 	self.data = data;
 	init_completion(&self.exited);
+	init_completion(&self.parked);
 	current->vfork_done = &self.exited;
 
 	/* OK, tell user we're spawned, wait for stop or wakeup */
@@ -117,9 +161,11 @@ static int kthread(void *_create)
 	schedule();
 
 	ret = -EINTR;
-	if (!self.should_stop)
-		ret = threadfn(data);
 
+	if (!test_bit(KTHREAD_SHOULD_STOP, &self.flags)) {
+		__kthread_parkme(&self);
+		ret = threadfn(data);
+	}
 	/* we can't just return, we must preserve "self" on stack */
 	do_exit(ret);
 }
@@ -172,8 +218,7 @@ static void create_kthread(struct kthread_create_info *create)
  * Returns a task_struct or ERR_PTR(-ENOMEM).
  */
 struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
-					   void *data,
-					   int node,
+					   void *data, int node,
 					   const char namefmt[],
 					   ...)
 {
@@ -210,6 +255,13 @@ struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
 }
 EXPORT_SYMBOL(kthread_create_on_node);
 
+static void __kthread_bind(struct task_struct *p, unsigned int cpu)
+{
+	/* It's safe because the task is inactive. */
+	do_set_cpus_allowed(p, cpumask_of(cpu));
+	p->flags |= PF_THREAD_BOUND;
+}
+
 /**
  * kthread_bind - bind a just-created kthread to a cpu.
  * @p: thread created by kthread_create().
@@ -226,13 +278,111 @@ void kthread_bind(struct task_struct *p, unsigned int cpu)
 		WARN_ON(1);
 		return;
 	}
-
-	/* It's safe because the task is inactive. */
-	do_set_cpus_allowed(p, cpumask_of(cpu));
-	p->flags |= PF_THREAD_BOUND;
+	__kthread_bind(p, cpu);
 }
 EXPORT_SYMBOL(kthread_bind);
 
+/**
+ * kthread_create_on_cpu - Create a cpu bound kthread
+ * @threadfn: the function to run until signal_pending(current).
+ * @data: data ptr for @threadfn.
+ * @cpu: The cpu on which the thread should be bound,
+ * @namefmt: printf-style name for the thread. Format is restricted
+ *	     to "name.*%u". Code fills in cpu number.
+ *
+ * Description: This helper function creates and names a kernel thread
+ * The thread will be woken and put into park mode.
+ */
+struct task_struct *kthread_create_on_cpu(int (*threadfn)(void *data),
+					  void *data, unsigned int cpu,
+					  const char *namefmt)
+{
+	struct task_struct *p;
+
+	p = kthread_create_on_node(threadfn, data, cpu_to_node(cpu), namefmt,
+				   cpu);
+	if (IS_ERR(p))
+		return p;
+	set_bit(KTHREAD_IS_PER_CPU, &to_kthread(p)->flags);
+	to_kthread(p)->cpu = cpu;
+	/* Park the thread to get it out of TASK_UNINTERRUPTIBLE state */
+	kthread_park(p);
+	return p;
+}
+
+static struct kthread *task_get_live_kthread(struct task_struct *k)
+{
+	struct kthread *kthread;
+
+	get_task_struct(k);
+	kthread = to_kthread(k);
+	/* It might have exited */
+	barrier();
+	if (k->vfork_done != NULL)
+		return kthread;
+	return NULL;
+}
+
+/**
+ * kthread_unpark - unpark a thread created by kthread_create().
+ * @k:		thread created by kthread_create().
+ *
+ * Sets kthread_should_park() for @k to return false, wakes it, and
+ * waits for it to return. If the thread is marked percpu then its
+ * bound to the cpu again.
+ */
+void kthread_unpark(struct task_struct *k)
+{
+	struct kthread *kthread = task_get_live_kthread(k);
+
+	if (kthread) {
+		clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
+		/*
+		 * We clear the IS_PARKED bit here as we don't wait
+		 * until the task has left the park code. So if we'd
+		 * park before that happens we'd see the IS_PARKED bit
+		 * which might be about to be cleared.
+		 */
+		if (test_and_clear_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
+			if (test_bit(KTHREAD_IS_PER_CPU, &kthread->flags))
+				__kthread_bind(k, kthread->cpu);
+			wake_up_process(k);
+		}
+	}
+	put_task_struct(k);
+}
+
+/**
+ * kthread_park - park a thread created by kthread_create().
+ * @k: thread created by kthread_create().
+ *
+ * Sets kthread_should_park() for @k to return true, wakes it, and
+ * waits for it to return. This can also be called after kthread_create()
+ * instead of calling wake_up_process(): the thread will park without
+ * calling threadfn().
+ *
+ * Returns 0 if the thread is parked, -ENOSYS if the thread exited.
+ * If called by the kthread itself just the park bit is set.
+ */
+int kthread_park(struct task_struct *k)
+{
+	struct kthread *kthread = task_get_live_kthread(k);
+	int ret = -ENOSYS;
+
+	if (kthread) {
+		if (!test_bit(KTHREAD_IS_PARKED, &kthread->flags)) {
+			set_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
+			if (k != current) {
+				wake_up_process(k);
+				wait_for_completion(&kthread->parked);
+			}
+		}
+		ret = 0;
+	}
+	put_task_struct(k);
+	return ret;
+}
+
 /**
  * kthread_stop - stop a thread created by kthread_create().
  * @k: thread created by kthread_create().
@@ -250,16 +400,13 @@ EXPORT_SYMBOL(kthread_bind);
  */
 int kthread_stop(struct task_struct *k)
 {
-	struct kthread *kthread;
+	struct kthread *kthread = task_get_live_kthread(k);
 	int ret;
 
 	trace_sched_kthread_stop(k);
-	get_task_struct(k);
-
-	kthread = to_kthread(k);
-	barrier(); /* it might have exited */
-	if (k->vfork_done != NULL) {
-		kthread->should_stop = 1;
+	if (kthread) {
+		set_bit(KTHREAD_SHOULD_STOP, &kthread->flags);
+		clear_bit(KTHREAD_SHOULD_PARK, &kthread->flags);
 		wake_up_process(k);
 		wait_for_completion(&kthread->exited);
 	}

commit 46f3d976213452350f9d10b0c2780c2681f7075b
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 19 13:52:53 2012 -0700

    kthread_worker: reimplement flush_kthread_work() to allow freeing the work item being executed
    
    kthread_worker provides minimalistic workqueue-like interface for
    users which need a dedicated worker thread (e.g. for realtime
    priority).  It has basic queue, flush_work, flush_worker operations
    which mostly match the workqueue counterparts; however, due to the way
    flush_work() is implemented, it has a noticeable difference of not
    allowing work items to be freed while being executed.
    
    While the current users of kthread_worker are okay with the current
    behavior, the restriction does impede some valid use cases.  Also,
    removing this difference isn't difficult and actually makes the code
    easier to understand.
    
    This patch reimplements flush_kthread_work() such that it uses a
    flush_work item instead of queue/done sequence numbers.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 4bfbff36d447..b579af57ea10 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -360,16 +360,12 @@ int kthread_worker_fn(void *worker_ptr)
 					struct kthread_work, node);
 		list_del_init(&work->node);
 	}
+	worker->current_work = work;
 	spin_unlock_irq(&worker->lock);
 
 	if (work) {
 		__set_current_state(TASK_RUNNING);
 		work->func(work);
-		smp_wmb();	/* wmb worker-b0 paired with flush-b1 */
-		work->done_seq = work->queue_seq;
-		smp_mb();	/* mb worker-b1 paired with flush-b0 */
-		if (atomic_read(&work->flushing))
-			wake_up_all(&work->done);
 	} else if (!freezing(current))
 		schedule();
 
@@ -386,7 +382,7 @@ static void insert_kthread_work(struct kthread_worker *worker,
 	lockdep_assert_held(&worker->lock);
 
 	list_add_tail(&work->node, pos);
-	work->queue_seq++;
+	work->worker = worker;
 	if (likely(worker->task))
 		wake_up_process(worker->task);
 }
@@ -436,25 +432,35 @@ static void kthread_flush_work_fn(struct kthread_work *work)
  */
 void flush_kthread_work(struct kthread_work *work)
 {
-	int seq = work->queue_seq;
+	struct kthread_flush_work fwork = {
+		KTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),
+		COMPLETION_INITIALIZER_ONSTACK(fwork.done),
+	};
+	struct kthread_worker *worker;
+	bool noop = false;
+
+retry:
+	worker = work->worker;
+	if (!worker)
+		return;
 
-	atomic_inc(&work->flushing);
+	spin_lock_irq(&worker->lock);
+	if (work->worker != worker) {
+		spin_unlock_irq(&worker->lock);
+		goto retry;
+	}
 
-	/*
-	 * mb flush-b0 paired with worker-b1, to make sure either
-	 * worker sees the above increment or we see done_seq update.
-	 */
-	smp_mb__after_atomic_inc();
+	if (!list_empty(&work->node))
+		insert_kthread_work(worker, &fwork.work, work->node.next);
+	else if (worker->current_work == work)
+		insert_kthread_work(worker, &fwork.work, worker->work_list.next);
+	else
+		noop = true;
 
-	/* A - B <= 0 tests whether B is in front of A regardless of overflow */
-	wait_event(work->done, seq - work->done_seq <= 0);
-	atomic_dec(&work->flushing);
+	spin_unlock_irq(&worker->lock);
 
-	/*
-	 * rmb flush-b1 paired with worker-b0, to make sure our caller
-	 * sees every change made by work->func().
-	 */
-	smp_mb__after_atomic_dec();
+	if (!noop)
+		wait_for_completion(&fwork.done);
 }
 EXPORT_SYMBOL_GPL(flush_kthread_work);
 

commit 9a2e03d8ed518a61154f18d83d6466628e519f94
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Jul 19 13:52:53 2012 -0700

    kthread_worker: reorganize to prepare for flush_kthread_work() reimplementation
    
    Make the following two non-functional changes.
    
    * Separate out insert_kthread_work() from queue_kthread_work().
    
    * Relocate struct kthread_flush_work and kthread_flush_work_fn()
      definitions above flush_kthread_work().
    
    v2: Added lockdep_assert_held() in insert_kthread_work() as suggested
        by Andy Walls.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Andy Walls <awalls@md.metrocast.net>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 3d3de633702e..4bfbff36d447 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -378,6 +378,19 @@ int kthread_worker_fn(void *worker_ptr)
 }
 EXPORT_SYMBOL_GPL(kthread_worker_fn);
 
+/* insert @work before @pos in @worker */
+static void insert_kthread_work(struct kthread_worker *worker,
+			       struct kthread_work *work,
+			       struct list_head *pos)
+{
+	lockdep_assert_held(&worker->lock);
+
+	list_add_tail(&work->node, pos);
+	work->queue_seq++;
+	if (likely(worker->task))
+		wake_up_process(worker->task);
+}
+
 /**
  * queue_kthread_work - queue a kthread_work
  * @worker: target kthread_worker
@@ -395,10 +408,7 @@ bool queue_kthread_work(struct kthread_worker *worker,
 
 	spin_lock_irqsave(&worker->lock, flags);
 	if (list_empty(&work->node)) {
-		list_add_tail(&work->node, &worker->work_list);
-		work->queue_seq++;
-		if (likely(worker->task))
-			wake_up_process(worker->task);
+		insert_kthread_work(worker, work, &worker->work_list);
 		ret = true;
 	}
 	spin_unlock_irqrestore(&worker->lock, flags);
@@ -406,6 +416,18 @@ bool queue_kthread_work(struct kthread_worker *worker,
 }
 EXPORT_SYMBOL_GPL(queue_kthread_work);
 
+struct kthread_flush_work {
+	struct kthread_work	work;
+	struct completion	done;
+};
+
+static void kthread_flush_work_fn(struct kthread_work *work)
+{
+	struct kthread_flush_work *fwork =
+		container_of(work, struct kthread_flush_work, work);
+	complete(&fwork->done);
+}
+
 /**
  * flush_kthread_work - flush a kthread_work
  * @work: work to flush
@@ -436,18 +458,6 @@ void flush_kthread_work(struct kthread_work *work)
 }
 EXPORT_SYMBOL_GPL(flush_kthread_work);
 
-struct kthread_flush_work {
-	struct kthread_work	work;
-	struct completion	done;
-};
-
-static void kthread_flush_work_fn(struct kthread_work *work)
-{
-	struct kthread_flush_work *fwork =
-		container_of(work, struct kthread_flush_work, work);
-	complete(&fwork->done);
-}
-
 /**
  * flush_kthread_worker - flush all current works on a kthread_worker
  * @worker: worker to flush

commit 34b087e48367c252e343c2f8de65676a78af1e4a
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Nov 23 09:28:17 2011 -0800

    freezer: kill unused set_freezable_with_signal()
    
    There's no in-kernel user of set_freezable_with_signal() left.  Mixing
    TIF_SIGPENDING with kernel threads can lead to nasty corner cases as
    kernel threads never travel signal delivery path on their own.
    
    e.g. the current implementation is buggy in the cancelation path of
    __thaw_task().  It calls recalc_sigpending_and_wake() in an attempt to
    clear TIF_SIGPENDING but the function never clears it regardless of
    sigpending state.  This means that signallable freezable kthreads may
    continue executing with !freezing() && stuck TIF_SIGPENDING, which can
    be troublesome.
    
    This patch removes set_freezable_with_signal() along with
    PF_FREEZER_NOSIG and recalc_sigpending*() calls in freezer.  User
    tasks get TIF_SIGPENDING, kernel tasks get woken up and the spurious
    sigpending is dealt with in the usual signal delivery path.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 1c36deaae2f1..3d3de633702e 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -282,7 +282,7 @@ int kthreadd(void *unused)
 	set_cpus_allowed_ptr(tsk, cpu_all_mask);
 	set_mems_allowed(node_states[N_HIGH_MEMORY]);
 
-	current->flags |= PF_NOFREEZE | PF_FREEZER_NOSIG;
+	current->flags |= PF_NOFREEZE;
 
 	for (;;) {
 		set_current_state(TASK_INTERRUPTIBLE);

commit 8a32c441c1609f80e55df75422324a1151208f40
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Nov 21 12:32:23 2011 -0800

    freezer: implement and use kthread_freezable_should_stop()
    
    Writeback and thinkpad_acpi have been using thaw_process() to prevent
    deadlock between the freezer and kthread_stop(); unfortunately, this
    is inherently racy - nothing prevents freezing from happening between
    thaw_process() and kthread_stop().
    
    This patch implements kthread_freezable_should_stop() which enters
    refrigerator if necessary but is guaranteed to return if
    kthread_stop() is invoked.  Both thaw_process() users are converted to
    use the new function.
    
    Note that this deadlock condition exists for many of freezable
    kthreads.  They need to be converted to use the new should_stop or
    freezable workqueue.
    
    Tested with synthetic test case.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Henrique de Moraes Holschuh <ibm-acpi@hmh.eng.br>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index b6d216a92639..1c36deaae2f1 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -58,6 +58,31 @@ int kthread_should_stop(void)
 }
 EXPORT_SYMBOL(kthread_should_stop);
 
+/**
+ * kthread_freezable_should_stop - should this freezable kthread return now?
+ * @was_frozen: optional out parameter, indicates whether %current was frozen
+ *
+ * kthread_should_stop() for freezable kthreads, which will enter
+ * refrigerator if necessary.  This function is safe from kthread_stop() /
+ * freezer deadlock and freezable kthreads should use this function instead
+ * of calling try_to_freeze() directly.
+ */
+bool kthread_freezable_should_stop(bool *was_frozen)
+{
+	bool frozen = false;
+
+	might_sleep();
+
+	if (unlikely(freezing(current)))
+		frozen = __refrigerator(true);
+
+	if (was_frozen)
+		*was_frozen = frozen;
+
+	return kthread_should_stop();
+}
+EXPORT_SYMBOL_GPL(kthread_freezable_should_stop);
+
 /**
  * kthread_data - return data value specified on kthread creation
  * @task: kthread task in question

commit 9984de1a5a8a96275fcab818f7419af5a3c86e71
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon May 23 14:51:41 2011 -0400

    kernel: Map most files to use export.h instead of module.h
    
    The changed files were only including linux/module.h for the
    EXPORT_SYMBOL infrastructure, and nothing else.  Revector them
    onto the isolated export header for faster compile times.
    
    Nothing to see here but a whole lot of instances of:
    
      -#include <linux/module.h>
      +#include <linux/export.h>
    
    This commit is only changing the kernel dir; next targets
    will probably be mm, fs, the arch dirs, etc.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 4ba7cccb4994..b6d216a92639 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -12,7 +12,7 @@
 #include <linux/cpuset.h>
 #include <linux/unistd.h>
 #include <linux/file.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/mutex.h>
 #include <linux/slab.h>
 #include <linux/freezer.h>

commit 1e1b6c511d1b23cb7c3b619d82fc7bd9f620565d
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Thu May 19 15:08:58 2011 +0900

    cpuset: Fix cpuset_cpus_allowed_fallback(), don't update tsk->rt.nr_cpus_allowed
    
    The rule is, we have to update tsk->rt.nr_cpus_allowed if we change
    tsk->cpus_allowed. Otherwise RT scheduler may confuse.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4DD4B3FA.5060901@jp.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 3b34d2732bce..4ba7cccb4994 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -202,8 +202,8 @@ void kthread_bind(struct task_struct *p, unsigned int cpu)
 		return;
 	}
 
-	p->cpus_allowed = cpumask_of_cpu(cpu);
-	p->rt.nr_cpus_allowed = 1;
+	/* It's safe because the task is inactive. */
+	do_set_cpus_allowed(p, cpumask_of(cpu));
 	p->flags |= PF_THREAD_BOUND;
 }
 EXPORT_SYMBOL(kthread_bind);

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 684ab3f7dd72..3b34d2732bce 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -139,7 +139,7 @@ static void create_kthread(struct kthread_create_info *create)
  * in @node, to get NUMA affinity for kthread stack, or else give -1.
  * When woken, the thread will run @threadfn() with @data as its
  * argument. @threadfn() can either call do_exit() directly if it is a
- * standalone thread for which noone will call kthread_stop(), or
+ * standalone thread for which no one will call kthread_stop(), or
  * return when 'kthread_should_stop()' is true (which means
  * kthread_stop() has been called).  The return value should be zero
  * or a negative error number; it will be passed to kthread_stop().

commit 207205a2ba2655652fe46a60b49838af6c16a919
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Mar 22 16:30:44 2011 -0700

    kthread: NUMA aware kthread_create_on_node()
    
    All kthreads being created from a single helper task, they all use memory
    from a single node for their kernel stack and task struct.
    
    This patch suite creates kthread_create_on_node(), adding a 'cpu' parameter
    to parameters already used by kthread_create().
    
    This parameter serves in allocating memory for the new kthread on its
    memory node if possible.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index c55afba990a3..684ab3f7dd72 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -27,6 +27,7 @@ struct kthread_create_info
 	/* Information passed to kthread() from kthreadd. */
 	int (*threadfn)(void *data);
 	void *data;
+	int node;
 
 	/* Result passed back to kthread_create() from kthreadd. */
 	struct task_struct *result;
@@ -98,10 +99,23 @@ static int kthread(void *_create)
 	do_exit(ret);
 }
 
+/* called from do_fork() to get node information for about to be created task */
+int tsk_fork_get_node(struct task_struct *tsk)
+{
+#ifdef CONFIG_NUMA
+	if (tsk == kthreadd_task)
+		return tsk->pref_node_fork;
+#endif
+	return numa_node_id();
+}
+
 static void create_kthread(struct kthread_create_info *create)
 {
 	int pid;
 
+#ifdef CONFIG_NUMA
+	current->pref_node_fork = create->node;
+#endif
 	/* We want our own signal handler (we take no signals by default). */
 	pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
 	if (pid < 0) {
@@ -111,15 +125,18 @@ static void create_kthread(struct kthread_create_info *create)
 }
 
 /**
- * kthread_create - create a kthread.
+ * kthread_create_on_node - create a kthread.
  * @threadfn: the function to run until signal_pending(current).
  * @data: data ptr for @threadfn.
+ * @node: memory node number.
  * @namefmt: printf-style name for the thread.
  *
  * Description: This helper function creates and names a kernel
  * thread.  The thread will be stopped: use wake_up_process() to start
  * it.  See also kthread_run().
  *
+ * If thread is going to be bound on a particular cpu, give its node
+ * in @node, to get NUMA affinity for kthread stack, or else give -1.
  * When woken, the thread will run @threadfn() with @data as its
  * argument. @threadfn() can either call do_exit() directly if it is a
  * standalone thread for which noone will call kthread_stop(), or
@@ -129,15 +146,17 @@ static void create_kthread(struct kthread_create_info *create)
  *
  * Returns a task_struct or ERR_PTR(-ENOMEM).
  */
-struct task_struct *kthread_create(int (*threadfn)(void *data),
-				   void *data,
-				   const char namefmt[],
-				   ...)
+struct task_struct *kthread_create_on_node(int (*threadfn)(void *data),
+					   void *data,
+					   int node,
+					   const char namefmt[],
+					   ...)
 {
 	struct kthread_create_info create;
 
 	create.threadfn = threadfn;
 	create.data = data;
+	create.node = node;
 	init_completion(&create.done);
 
 	spin_lock(&kthread_create_lock);
@@ -164,7 +183,7 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 	}
 	return create.result;
 }
-EXPORT_SYMBOL(kthread_create);
+EXPORT_SYMBOL(kthread_create_on_node);
 
 /**
  * kthread_bind - bind a just-created kthread to a cpu.

commit c9b5f501ef1580faa30c40c644b7691870462201
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 7 13:41:40 2011 +0100

    sched: Constify function scope static struct sched_param usage
    
    Function-scope statics are discouraged because they are
    easily overlooked and can cause subtle bugs/races due to
    their global (non-SMP safe) nature.
    
    Linus noticed that we did this for sched_param - at minimum
    make the const.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: Message-ID: <AANLkTinotRxScOHEb0HgFgSpGPkq_6jKTv5CfvnQM=ee@mail.gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 5355cfd44a3f..c55afba990a3 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -148,7 +148,7 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 	wait_for_completion(&create.done);
 
 	if (!IS_ERR(create.result)) {
-		static struct sched_param param = { .sched_priority = 0 };
+		static const struct sched_param param = { .sched_priority = 0 };
 		va_list args;
 
 		va_start(args, namefmt);

commit 27066fd484a32c80630136aa2b91c980f3198f9d
Merge: 101e5f77bf35 3c0eee3fe6a3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 5 14:14:42 2011 +0100

    Merge commit 'v2.6.37' into sched/core
    
    Merge reason: Merge the final .37 tree.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 4f32e9b1f812fd6c00cc85a127583fefbdedaedc
Author: Yong Zhang <yong.zhang0@gmail.com>
Date:   Wed Dec 22 10:27:53 2010 +0100

    kthread_work: make lockdep happy
    
    spinlock in kthread_worker and wait_queue_head in kthread_work both
    should be lockdep sensible, so change the interface to make it
    suiltable for CONFIG_LOCKDEP.
    
    tj: comment update
    
    Reported-by: Nicolas <nicolas.mailhot@laposte.net>
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Signed-off-by: Andy Walls <awalls@md.metrocast.net>
    Tested-by: Andy Walls <awalls@md.metrocast.net>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 2dc3786349d1..ca61bbdd44b2 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -265,6 +265,17 @@ int kthreadd(void *unused)
 	return 0;
 }
 
+void __init_kthread_worker(struct kthread_worker *worker,
+				const char *name,
+				struct lock_class_key *key)
+{
+	spin_lock_init(&worker->lock);
+	lockdep_set_class_and_name(&worker->lock, key, name);
+	INIT_LIST_HEAD(&worker->work_list);
+	worker->task = NULL;
+}
+EXPORT_SYMBOL_GPL(__init_kthread_worker);
+
 /**
  * kthread_worker_fn - kthread function to process kthread_worker
  * @worker_ptr: pointer to initialized kthread_worker

commit fe7de49f9d4e53f24ec9ef762a503f70b562341c
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Wed Oct 20 16:01:12 2010 -0700

    sched: Make sched_param argument static in sched_setscheduler() callers
    
    Andrew Morton pointed out almost all sched_setscheduler() callers are
    using fixed parameters and can be converted to static.  It reduces runtime
    memory use a little.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: James Morris <jmorris@namei.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 2dc3786349d1..74cf6f5e7ade 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -148,7 +148,7 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 	wait_for_completion(&create.done);
 
 	if (!IS_ERR(create.result)) {
-		struct sched_param param = { .sched_priority = 0 };
+		static struct sched_param param = { .sched_priority = 0 };
 		va_list args;
 
 		va_start(args, namefmt);

commit 82805ab77d25643f579d90397dcd34f05d1b750a
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:09 2010 +0200

    kthread: implement kthread_data()
    
    Implement kthread_data() which takes @task pointing to a kthread and
    returns @data specified when creating the kthread.  The caller is
    responsible for ensuring the validity of @task when calling this
    function.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 8b63c7fee73b..2dc3786349d1 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -37,6 +37,7 @@ struct kthread_create_info
 
 struct kthread {
 	int should_stop;
+	void *data;
 	struct completion exited;
 };
 
@@ -56,6 +57,19 @@ int kthread_should_stop(void)
 }
 EXPORT_SYMBOL(kthread_should_stop);
 
+/**
+ * kthread_data - return data value specified on kthread creation
+ * @task: kthread task in question
+ *
+ * Return the data value specified when kthread @task was created.
+ * The caller is responsible for ensuring the validity of @task when
+ * calling this function.
+ */
+void *kthread_data(struct task_struct *task)
+{
+	return to_kthread(task)->data;
+}
+
 static int kthread(void *_create)
 {
 	/* Copy data: it's on kthread's stack */
@@ -66,6 +80,7 @@ static int kthread(void *_create)
 	int ret;
 
 	self.should_stop = 0;
+	self.data = data;
 	init_completion(&self.exited);
 	current->vfork_done = &self.exited;
 

commit b56c0d8937e665a27d90517ee7a746d0aa05af46
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 29 10:07:09 2010 +0200

    kthread: implement kthread_worker
    
    Implement simple work processor for kthread.  This is to ease using
    kthread.  Single thread workqueue used to be used for things like this
    but workqueue won't guarantee fixed kthread association anymore to
    enable worker sharing.
    
    This can be used in cases where specific kthread association is
    necessary, for example, when it should have RT priority or be assigned
    to certain cgroup.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 83911c780175..8b63c7fee73b 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -14,6 +14,8 @@
 #include <linux/file.h>
 #include <linux/module.h>
 #include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/freezer.h>
 #include <trace/events/sched.h>
 
 static DEFINE_SPINLOCK(kthread_create_lock);
@@ -247,3 +249,150 @@ int kthreadd(void *unused)
 
 	return 0;
 }
+
+/**
+ * kthread_worker_fn - kthread function to process kthread_worker
+ * @worker_ptr: pointer to initialized kthread_worker
+ *
+ * This function can be used as @threadfn to kthread_create() or
+ * kthread_run() with @worker_ptr argument pointing to an initialized
+ * kthread_worker.  The started kthread will process work_list until
+ * the it is stopped with kthread_stop().  A kthread can also call
+ * this function directly after extra initialization.
+ *
+ * Different kthreads can be used for the same kthread_worker as long
+ * as there's only one kthread attached to it at any given time.  A
+ * kthread_worker without an attached kthread simply collects queued
+ * kthread_works.
+ */
+int kthread_worker_fn(void *worker_ptr)
+{
+	struct kthread_worker *worker = worker_ptr;
+	struct kthread_work *work;
+
+	WARN_ON(worker->task);
+	worker->task = current;
+repeat:
+	set_current_state(TASK_INTERRUPTIBLE);	/* mb paired w/ kthread_stop */
+
+	if (kthread_should_stop()) {
+		__set_current_state(TASK_RUNNING);
+		spin_lock_irq(&worker->lock);
+		worker->task = NULL;
+		spin_unlock_irq(&worker->lock);
+		return 0;
+	}
+
+	work = NULL;
+	spin_lock_irq(&worker->lock);
+	if (!list_empty(&worker->work_list)) {
+		work = list_first_entry(&worker->work_list,
+					struct kthread_work, node);
+		list_del_init(&work->node);
+	}
+	spin_unlock_irq(&worker->lock);
+
+	if (work) {
+		__set_current_state(TASK_RUNNING);
+		work->func(work);
+		smp_wmb();	/* wmb worker-b0 paired with flush-b1 */
+		work->done_seq = work->queue_seq;
+		smp_mb();	/* mb worker-b1 paired with flush-b0 */
+		if (atomic_read(&work->flushing))
+			wake_up_all(&work->done);
+	} else if (!freezing(current))
+		schedule();
+
+	try_to_freeze();
+	goto repeat;
+}
+EXPORT_SYMBOL_GPL(kthread_worker_fn);
+
+/**
+ * queue_kthread_work - queue a kthread_work
+ * @worker: target kthread_worker
+ * @work: kthread_work to queue
+ *
+ * Queue @work to work processor @task for async execution.  @task
+ * must have been created with kthread_worker_create().  Returns %true
+ * if @work was successfully queued, %false if it was already pending.
+ */
+bool queue_kthread_work(struct kthread_worker *worker,
+			struct kthread_work *work)
+{
+	bool ret = false;
+	unsigned long flags;
+
+	spin_lock_irqsave(&worker->lock, flags);
+	if (list_empty(&work->node)) {
+		list_add_tail(&work->node, &worker->work_list);
+		work->queue_seq++;
+		if (likely(worker->task))
+			wake_up_process(worker->task);
+		ret = true;
+	}
+	spin_unlock_irqrestore(&worker->lock, flags);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(queue_kthread_work);
+
+/**
+ * flush_kthread_work - flush a kthread_work
+ * @work: work to flush
+ *
+ * If @work is queued or executing, wait for it to finish execution.
+ */
+void flush_kthread_work(struct kthread_work *work)
+{
+	int seq = work->queue_seq;
+
+	atomic_inc(&work->flushing);
+
+	/*
+	 * mb flush-b0 paired with worker-b1, to make sure either
+	 * worker sees the above increment or we see done_seq update.
+	 */
+	smp_mb__after_atomic_inc();
+
+	/* A - B <= 0 tests whether B is in front of A regardless of overflow */
+	wait_event(work->done, seq - work->done_seq <= 0);
+	atomic_dec(&work->flushing);
+
+	/*
+	 * rmb flush-b1 paired with worker-b0, to make sure our caller
+	 * sees every change made by work->func().
+	 */
+	smp_mb__after_atomic_dec();
+}
+EXPORT_SYMBOL_GPL(flush_kthread_work);
+
+struct kthread_flush_work {
+	struct kthread_work	work;
+	struct completion	done;
+};
+
+static void kthread_flush_work_fn(struct kthread_work *work)
+{
+	struct kthread_flush_work *fwork =
+		container_of(work, struct kthread_flush_work, work);
+	complete(&fwork->done);
+}
+
+/**
+ * flush_kthread_worker - flush all current works on a kthread_worker
+ * @worker: worker to flush
+ *
+ * Wait until all currently executing or pending works on @worker are
+ * finished.
+ */
+void flush_kthread_worker(struct kthread_worker *worker)
+{
+	struct kthread_flush_work fwork = {
+		KTHREAD_WORK_INIT(fwork.work, kthread_flush_work_fn),
+		COMPLETION_INITIALIZER_ONSTACK(fwork.done),
+	};
+
+	queue_kthread_work(worker, &fwork.work);
+	wait_for_completion(&fwork.done);
+}
+EXPORT_SYMBOL_GPL(flush_kthread_worker);

commit 5ab116c9349ef52d6fbd2e2917a53f13194b048e
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Tue Mar 23 13:35:34 2010 -0700

    cpuset: fix the problem that cpuset_mem_spread_node() returns an offline node
    
    cpuset_mem_spread_node() returns an offline node, and causes an oops.
    
    This patch fixes it by initializing task->mems_allowed to
    node_states[N_HIGH_MEMORY], and updating task->mems_allowed when doing
    memory hotplug.
    
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Reported-by: Nick Piggin <npiggin@suse.de>
    Tested-by: Nick Piggin <npiggin@suse.de>
    Cc: Paul Menage <menage@google.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 82ed0ea15194..83911c780175 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -219,7 +219,7 @@ int kthreadd(void *unused)
 	set_task_comm(tsk, "kthreadd");
 	ignore_signals(tsk);
 	set_cpus_allowed_ptr(tsk, cpu_all_mask);
-	set_mems_allowed(node_possible_map);
+	set_mems_allowed(node_states[N_HIGH_MEMORY]);
 
 	current->flags |= PF_NOFREEZE | PF_FREEZER_NOSIG;
 

commit 301ba0457f1ed853fc08e57785f8c87fe7e49c68
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Feb 9 15:07:40 2010 +1100

    kthread, sched: Remove reference to kthread_create_on_cpu
    
    kthread_create_on_cpu doesn't exist so update a comment in
    kthread.c to reflect this.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20100209040740.GB3702@kryten>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index fbb6222fe7e0..82ed0ea15194 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -101,7 +101,7 @@ static void create_kthread(struct kthread_create_info *create)
  *
  * Description: This helper function creates and names a kernel
  * thread.  The thread will be stopped: use wake_up_process() to start
- * it.  See also kthread_run(), kthread_create_on_cpu().
+ * it.  See also kthread_run().
  *
  * When woken, the thread will run @threadfn() with @data as its
  * argument. @threadfn() can either call do_exit() directly if it is a

commit 881232b70b195768a71cd74ff4b4e8ab9502997b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Dec 16 18:04:39 2009 +0100

    sched: Move kthread_bind() back to kthread.c
    
    Since kthread_bind() lost its dependencies on sched.c, move it
    back where it came from.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <20091216170518.039524041@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index ab7ae57773e1..fbb6222fe7e0 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -149,6 +149,29 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 }
 EXPORT_SYMBOL(kthread_create);
 
+/**
+ * kthread_bind - bind a just-created kthread to a cpu.
+ * @p: thread created by kthread_create().
+ * @cpu: cpu (might not be online, must be possible) for @k to run on.
+ *
+ * Description: This function is equivalent to set_cpus_allowed(),
+ * except that @cpu doesn't need to be online, and the thread must be
+ * stopped (i.e., just returned from kthread_create()).
+ */
+void kthread_bind(struct task_struct *p, unsigned int cpu)
+{
+	/* Must have done schedule() in kthread() before we set_task_cpu */
+	if (!wait_task_inactive(p, TASK_UNINTERRUPTIBLE)) {
+		WARN_ON(1);
+		return;
+	}
+
+	p->cpus_allowed = cpumask_of_cpu(cpu);
+	p->rt.nr_cpus_allowed = 1;
+	p->flags |= PF_THREAD_BOUND;
+}
+EXPORT_SYMBOL(kthread_bind);
+
 /**
  * kthread_stop - stop a thread created by kthread_create().
  * @k: thread created by kthread_create().

commit b84ff7d6f1b7f8a43414e74d972ec4c8f3361db4
Author: Mike Galbraith <efault@gmx.de>
Date:   Thu Oct 29 11:48:30 2009 +0100

    sched: Fix kthread_bind() by moving the body of kthread_bind() to sched.c
    
    Eric Paris reported that commit
    f685ceacab07d3f6c236f04803e2f2f0dbcc5afb causes boot time
    PREEMPT_DEBUG complaints.
    
     [    4.590699] BUG: using smp_processor_id() in preemptible [00000000] code: rmmod/1314
     [    4.593043] caller is task_hot+0x86/0xd0
    
    Since kthread_bind() messes with scheduler internals, move the
    body to sched.c, and lock the runqueue.
    
    Reported-by: Eric Paris <eparis@redhat.com>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Tested-by: Eric Paris <eparis@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1256813310.7574.3.camel@marge.simson.net>
    [ v2: fix !SMP build and clean up ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 5fe709982caa..ab7ae57773e1 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -149,29 +149,6 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 }
 EXPORT_SYMBOL(kthread_create);
 
-/**
- * kthread_bind - bind a just-created kthread to a cpu.
- * @k: thread created by kthread_create().
- * @cpu: cpu (might not be online, must be possible) for @k to run on.
- *
- * Description: This function is equivalent to set_cpus_allowed(),
- * except that @cpu doesn't need to be online, and the thread must be
- * stopped (i.e., just returned from kthread_create()).
- */
-void kthread_bind(struct task_struct *k, unsigned int cpu)
-{
-	/* Must have done schedule() in kthread() before we set_task_cpu */
-	if (!wait_task_inactive(k, TASK_UNINTERRUPTIBLE)) {
-		WARN_ON(1);
-		return;
-	}
-	set_task_cpu(k, cpu);
-	k->cpus_allowed = cpumask_of_cpu(cpu);
-	k->rt.nr_cpus_allowed = 1;
-	k->flags |= PF_THREAD_BOUND;
-}
-EXPORT_SYMBOL(kthread_bind);
-
 /**
  * kthread_stop - stop a thread created by kthread_create().
  * @k: thread created by kthread_create().

commit 61cbe54d9479ad98283b2dda686deae4c34b2d59
Author: Mike Galbraith <efault@gmx.de>
Date:   Wed Sep 9 15:41:37 2009 +0200

    sched: Keep kthreads at default priority
    
    Removes kthread/workqueue priority boost, they increase worst-case
    desktop latencies.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1252486344.28645.18.camel@marge.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index eb8751aa0418..5fe709982caa 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -16,8 +16,6 @@
 #include <linux/mutex.h>
 #include <trace/events/sched.h>
 
-#define KTHREAD_NICE_LEVEL (-5)
-
 static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
 struct task_struct *kthreadd_task;
@@ -145,7 +143,6 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 		 * The kernel thread should not inherit these properties.
 		 */
 		sched_setscheduler_nocheck(create.result, SCHED_NORMAL, &param);
-		set_user_nice(create.result, KTHREAD_NICE_LEVEL);
 		set_cpus_allowed_ptr(create.result, cpu_all_mask);
 	}
 	return create.result;
@@ -221,7 +218,6 @@ int kthreadd(void *unused)
 	/* Setup a clean context for our children to inherit. */
 	set_task_comm(tsk, "kthreadd");
 	ignore_signals(tsk);
-	set_user_nice(tsk, KTHREAD_NICE_LEVEL);
 	set_cpus_allowed_ptr(tsk, cpu_all_mask);
 	set_mems_allowed(node_possible_map);
 

commit 9ae260270c90643156cda73427aa1f04c923e627
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jun 19 02:51:13 2009 +0200

    update the comment in kthread_stop()
    
    Commit 63706172f332fd3f6e7458ebfb35fa6de9c21dc5 ("kthreads: rework
    kthread_stop()") removed the limitation that the thread function mysr
    not call do_exit() itself, but forgot to update the comment.
    
    Since that commit it is OK to use kthread_stop() even if kthread can
    exit itself.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 9b1a7de26979..eb8751aa0418 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -180,10 +180,12 @@ EXPORT_SYMBOL(kthread_bind);
  * @k: thread created by kthread_create().
  *
  * Sets kthread_should_stop() for @k to return true, wakes it, and
- * waits for it to exit.  Your threadfn() must not call do_exit()
- * itself if you use this function!  This can also be called after
- * kthread_create() instead of calling wake_up_process(): the thread
- * will exit without calling threadfn().
+ * waits for it to exit. This can also be called after kthread_create()
+ * instead of calling wake_up_process(): the thread will exit without
+ * calling threadfn().
+ *
+ * If threadfn() may call do_exit() itself, the caller must ensure
+ * task_struct can't go away.
  *
  * Returns the result of threadfn(), or %-EINTR if wake_up_process()
  * was never called.

commit 63706172f332fd3f6e7458ebfb35fa6de9c21dc5
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:45 2009 -0700

    kthreads: rework kthread_stop()
    
    Based on Eric's patch which in turn was based on my patch.
    
    kthread_stop() has the nasty problems:
    
    - it runs unpredictably long with the global semaphore held.
    
    - it deadlocks if kthread itself does kthread_stop() before it obeys
      the kthread_should_stop() request.
    
    - it is not useable if kthread exits on its own, see for example the
      ugly "wait_to_die:" hack in migration_thread()
    
    - it is not possible to just tell kthread it should stop, we must always
      wait for its exit.
    
    With this patch kthread() allocates all neccesary data (struct kthread) on
    its own stack, globals kthread_stop_xxx are deleted.  ->vfork_done is used
    as a pointer into "struct kthread", this means kthread_stop() can easily
    wait for kthread's exit.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Vitaliy Gusev <vgusev@openvz.org
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index bc5d1f0b25a4..9b1a7de26979 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -35,17 +35,13 @@ struct kthread_create_info
 	struct list_head list;
 };
 
-struct kthread_stop_info
-{
-	struct task_struct *k;
-	int err;
-	struct completion done;
+struct kthread {
+	int should_stop;
+	struct completion exited;
 };
 
-/* Thread stopping is done by setthing this var: lock serializes
- * multiple kthread_stop calls. */
-static DEFINE_MUTEX(kthread_stop_lock);
-static struct kthread_stop_info kthread_stop_info;
+#define to_kthread(tsk)	\
+	container_of((tsk)->vfork_done, struct kthread, exited)
 
 /**
  * kthread_should_stop - should this kthread return now?
@@ -56,20 +52,22 @@ static struct kthread_stop_info kthread_stop_info;
  */
 int kthread_should_stop(void)
 {
-	return (kthread_stop_info.k == current);
+	return to_kthread(current)->should_stop;
 }
 EXPORT_SYMBOL(kthread_should_stop);
 
 static int kthread(void *_create)
 {
+	/* Copy data: it's on kthread's stack */
 	struct kthread_create_info *create = _create;
-	int (*threadfn)(void *data);
-	void *data;
-	int ret = -EINTR;
+	int (*threadfn)(void *data) = create->threadfn;
+	void *data = create->data;
+	struct kthread self;
+	int ret;
 
-	/* Copy data: it's on kthread's stack */
-	threadfn = create->threadfn;
-	data = create->data;
+	self.should_stop = 0;
+	init_completion(&self.exited);
+	current->vfork_done = &self.exited;
 
 	/* OK, tell user we're spawned, wait for stop or wakeup */
 	__set_current_state(TASK_UNINTERRUPTIBLE);
@@ -77,15 +75,12 @@ static int kthread(void *_create)
 	complete(&create->done);
 	schedule();
 
-	if (!kthread_should_stop())
+	ret = -EINTR;
+	if (!self.should_stop)
 		ret = threadfn(data);
 
-	/* It might have exited on its own, w/o kthread_stop.  Check. */
-	if (kthread_should_stop()) {
-		kthread_stop_info.err = ret;
-		complete(&kthread_stop_info.done);
-	}
-	return 0;
+	/* we can't just return, we must preserve "self" on stack */
+	do_exit(ret);
 }
 
 static void create_kthread(struct kthread_create_info *create)
@@ -195,30 +190,22 @@ EXPORT_SYMBOL(kthread_bind);
  */
 int kthread_stop(struct task_struct *k)
 {
+	struct kthread *kthread;
 	int ret;
 
-	mutex_lock(&kthread_stop_lock);
-
-	/* It could exit after stop_info.k set, but before wake_up_process. */
-	get_task_struct(k);
-
 	trace_sched_kthread_stop(k);
+	get_task_struct(k);
 
-	/* Must init completion *before* thread sees kthread_stop_info.k */
-	init_completion(&kthread_stop_info.done);
-	smp_wmb();
+	kthread = to_kthread(k);
+	barrier(); /* it might have exited */
+	if (k->vfork_done != NULL) {
+		kthread->should_stop = 1;
+		wake_up_process(k);
+		wait_for_completion(&kthread->exited);
+	}
+	ret = k->exit_code;
 
-	/* Now set kthread_should_stop() to true, and wake it up. */
-	kthread_stop_info.k = k;
-	wake_up_process(k);
 	put_task_struct(k);
-
-	/* Once it dies, reset stop ptr, gather result and we're done. */
-	wait_for_completion(&kthread_stop_info.done);
-	kthread_stop_info.k = NULL;
-	ret = kthread_stop_info.err;
-	mutex_unlock(&kthread_stop_lock);
-
 	trace_sched_kthread_stop_ret(ret);
 
 	return ret;

commit cdd140bdd6c7bc6395f08877a73c39941501af93
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:43 2009 -0700

    kthreads: simplify the startup synchronization
    
    We use two completions two create the kernel thread, this is a bit ugly.
    kthread() wakes up create_kthread() via ->started, then create_kthread()
    wakes up the caller kthread_create() via ->done.  But kthread() does not
    need to wait for kthread(), it can just return.  Instead kthread() itself
    can wake up the caller of kthread_create().
    
    Kill kthread_create_info->started, ->done is enough.  This improves the
    scalability a bit and sijmplifies the code.
    
    The only problem if kernel_thread() fails, in that case create_kthread()
    must do complete(&create->done).
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Vitaliy Gusev <vgusev@openvz.org
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 7fa441333529..bc5d1f0b25a4 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -27,7 +27,6 @@ struct kthread_create_info
 	/* Information passed to kthread() from kthreadd. */
 	int (*threadfn)(void *data);
 	void *data;
-	struct completion started;
 
 	/* Result passed back to kthread_create() from kthreadd. */
 	struct task_struct *result;
@@ -75,7 +74,7 @@ static int kthread(void *_create)
 	/* OK, tell user we're spawned, wait for stop or wakeup */
 	__set_current_state(TASK_UNINTERRUPTIBLE);
 	create->result = current;
-	complete(&create->started);
+	complete(&create->done);
 	schedule();
 
 	if (!kthread_should_stop())
@@ -95,11 +94,10 @@ static void create_kthread(struct kthread_create_info *create)
 
 	/* We want our own signal handler (we take no signals by default). */
 	pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
-	if (pid < 0)
+	if (pid < 0) {
 		create->result = ERR_PTR(pid);
-	else
-		wait_for_completion(&create->started);
-	complete(&create->done);
+		complete(&create->done);
+	}
 }
 
 /**
@@ -130,7 +128,6 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 
 	create.threadfn = threadfn;
 	create.data = data;
-	init_completion(&create.started);
 	init_completion(&create.done);
 
 	spin_lock(&kthread_create_lock);

commit 58568d2a8215cb6f55caf2332017d7bdff954e1c
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Tue Jun 16 15:31:49 2009 -0700

    cpuset,mm: update tasks' mems_allowed in time
    
    Fix allocating page cache/slab object on the unallowed node when memory
    spread is set by updating tasks' mems_allowed after its cpuset's mems is
    changed.
    
    In order to update tasks' mems_allowed in time, we must modify the code of
    memory policy.  Because the memory policy is applied in the process's
    context originally.  After applying this patch, one task directly
    manipulates anothers mems_allowed, and we use alloc_lock in the
    task_struct to protect mems_allowed and memory policy of the task.
    
    But in the fast path, we didn't use lock to protect them, because adding a
    lock may lead to performance regression.  But if we don't add a lock,the
    task might see no nodes when changing cpuset's mems_allowed to some
    non-overlapping set.  In order to avoid it, we set all new allowed nodes,
    then clear newly disallowed ones.
    
    [lee.schermerhorn@hp.com:
      The rework of mpol_new() to extract the adjusting of the node mask to
      apply cpuset and mpol flags "context" breaks set_mempolicy() and mbind()
      with MPOL_PREFERRED and a NULL nodemask--i.e., explicit local
      allocation.  Fix this by adding the check for MPOL_PREFERRED and empty
      node mask to mpol_new_mpolicy().
    
      Remove the now unneeded 'nodes = NULL' from mpol_new().
    
      Note that mpol_new_mempolicy() is always called with a non-NULL
      'nodes' parameter now that it has been removed from mpol_new().
      Therefore, we don't need to test nodes for NULL before testing it for
      'empty'.  However, just to be extra paranoid, add a VM_BUG_ON() to
      verify this assumption.]
    [lee.schermerhorn@hp.com:
    
      I don't think the function name 'mpol_new_mempolicy' is descriptive
      enough to differentiate it from mpol_new().
    
      This function applies cpuset set context, usually constraining nodes
      to those allowed by the cpuset.  However, when the 'RELATIVE_NODES flag
      is set, it also translates the nodes.  So I settled on
      'mpol_set_nodemask()', because the comment block for mpol_new() mentions
      that we need to call this function to "set nodes".
    
      Some additional minor line length, whitespace and typo cleanup.]
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Paul Menage <menage@google.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 41c88fe40500..7fa441333529 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -9,6 +9,7 @@
 #include <linux/kthread.h>
 #include <linux/completion.h>
 #include <linux/err.h>
+#include <linux/cpuset.h>
 #include <linux/unistd.h>
 #include <linux/file.h>
 #include <linux/module.h>
@@ -236,6 +237,7 @@ int kthreadd(void *unused)
 	ignore_signals(tsk);
 	set_user_nice(tsk, KTHREAD_NICE_LEVEL);
 	set_cpus_allowed_ptr(tsk, cpu_all_mask);
+	set_mems_allowed(node_possible_map);
 
 	current->flags |= PF_NOFREEZE | PF_FREEZER_NOSIG;
 

commit ad8d75fff811a6a230f7f43b05a6483099349533
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Apr 14 19:39:12 2009 -0400

    tracing/events: move trace point headers into include/trace/events
    
    Impact: clean up
    
    Create a sub directory in include/trace called events to keep the
    trace point headers in their own separate directory. Only headers that
    declare trace points should be defined in this directory.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Neil Horman <nhorman@tuxdriver.com>
    Cc: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index e1c76924545b..41c88fe40500 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -13,7 +13,7 @@
 #include <linux/file.h>
 #include <linux/module.h>
 #include <linux/mutex.h>
-#include <trace/sched.h>
+#include <trace/events/sched.h>
 
 #define KTHREAD_NICE_LEVEL (-5)
 

commit a8d154b009168337494fbf345671bab74d3e4b8b
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 10 09:36:00 2009 -0400

    tracing: create automated trace defines
    
    This patch lowers the number of places a developer must modify to add
    new tracepoints. The current method to add a new tracepoint
    into an existing system is to write the trace point macro in the
    trace header with one of the macros TRACE_EVENT, TRACE_FORMAT or
    DECLARE_TRACE, then they must add the same named item into the C file
    with the macro DEFINE_TRACE(name) and then add the trace point.
    
    This change cuts out the needing to add the DEFINE_TRACE(name).
    Every file that uses the tracepoint must still include the trace/<type>.h
    file, but the one C file must also add a define before the including
    of that file.
    
     #define CREATE_TRACE_POINTS
     #include <trace/mytrace.h>
    
    This will cause the trace/mytrace.h file to also produce the C code
    necessary to implement the trace point.
    
    Note, if more than one trace/<type>.h is used to create the C code
    it is best to list them all together.
    
     #define CREATE_TRACE_POINTS
     #include <trace/foo.h>
     #include <trace/bar.h>
     #include <trace/fido.h>
    
    Thanks to Mathieu Desnoyers and Christoph Hellwig for coming up with
    the cleaner solution of the define above the includes over my first
    design to have the C code include a "special" header.
    
    This patch converts sched, irq and lockdep and skb to use this new
    method.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Neil Horman <nhorman@tuxdriver.com>
    Cc: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 4ebaf8519abf..e1c76924545b 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -21,9 +21,6 @@ static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
 struct task_struct *kthreadd_task;
 
-DEFINE_TRACE(sched_kthread_stop);
-DEFINE_TRACE(sched_kthread_stop_ret);
-
 struct kthread_create_info
 {
 	/* Information passed to kthread() from kthreadd. */

commit 1c99315bb36b5d776210546d438ca928dc9b1f22
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 9 09:50:36 2009 -0600

    kthread: move sched-realeted initialization from kthreadd context
    
    kthreadd is the single thread which implements ths "create" request, move
    sched_setscheduler/etc from create_kthread() to kthread_create() to
    improve the scalability.
    
    We should be careful with sched_setscheduler(), use _nochek helper.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Vitaliy Gusev <vgusev@openvz.org
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index c013bf000ce6..4ebaf8519abf 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -97,19 +97,10 @@ static void create_kthread(struct kthread_create_info *create)
 
 	/* We want our own signal handler (we take no signals by default). */
 	pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
-	if (pid < 0) {
+	if (pid < 0)
 		create->result = ERR_PTR(pid);
-	} else {
-		struct sched_param param = { .sched_priority = 0 };
+	else
 		wait_for_completion(&create->started);
-		/*
-		 * root may have changed our (kthreadd's) priority or CPU mask.
-		 * The kernel thread should not inherit these properties.
-		 */
-		sched_setscheduler(create->result, SCHED_NORMAL, &param);
-		set_user_nice(create->result, KTHREAD_NICE_LEVEL);
-		set_cpus_allowed_ptr(create->result, cpu_all_mask);
-	}
 	complete(&create->done);
 }
 
@@ -152,11 +143,20 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 	wait_for_completion(&create.done);
 
 	if (!IS_ERR(create.result)) {
+		struct sched_param param = { .sched_priority = 0 };
 		va_list args;
+
 		va_start(args, namefmt);
 		vsnprintf(create.result->comm, sizeof(create.result->comm),
 			  namefmt, args);
 		va_end(args);
+		/*
+		 * root may have changed our (kthreadd's) priority or CPU mask.
+		 * The kernel thread should not inherit these properties.
+		 */
+		sched_setscheduler_nocheck(create.result, SCHED_NORMAL, &param);
+		set_user_nice(create.result, KTHREAD_NICE_LEVEL);
+		set_cpus_allowed_ptr(create.result, cpu_all_mask);
 	}
 	return create.result;
 }

commit 3217ab97f14c5c8f9f975ed8c40c351164b0b10e
Author: Vitaliy Gusev <vgusev@openvz.org>
Date:   Thu Apr 9 09:50:35 2009 -0600

    kthread: Don't looking for a task in create_kthread() #2
    
    Remove the unnecessary find_task_by_pid_ns(). kthread() can just
    use "current" to get the same result.
    
    Signed-off-by: Vitaliy Gusev <vgusev@openvz.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 84bbadd4d021..c013bf000ce6 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -76,6 +76,7 @@ static int kthread(void *_create)
 
 	/* OK, tell user we're spawned, wait for stop or wakeup */
 	__set_current_state(TASK_UNINTERRUPTIBLE);
+	create->result = current;
 	complete(&create->started);
 	schedule();
 
@@ -101,9 +102,6 @@ static void create_kthread(struct kthread_create_info *create)
 	} else {
 		struct sched_param param = { .sched_priority = 0 };
 		wait_for_completion(&create->started);
-		read_lock(&tasklist_lock);
-		create->result = find_task_by_pid_ns(pid, &init_pid_ns);
-		read_unlock(&tasklist_lock);
 		/*
 		 * root may have changed our (kthreadd's) priority or CPU mask.
 		 * The kernel thread should not inherit these properties.

commit 1a2142afa5646ad5af44bbe1febaa5e0b7e71156
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Mar 30 22:05:10 2009 -0600

    cpumask: remove dangerous CPU_MASK_ALL_PTR, &CPU_MASK_ALL
    
    Impact: cleanup
    
    (Thanks to Al Viro for reminding me of this, via Ingo)
    
    CPU_MASK_ALL is the (deprecated) "all bits set" cpumask, defined as so:
    
            #define CPU_MASK_ALL (cpumask_t) { { ... } }
    
    Taking the address of such a temporary is questionable at best,
    unfortunately 321a8e9d (cpumask: add CPU_MASK_ALL_PTR macro) added
    CPU_MASK_ALL_PTR:
    
            #define CPU_MASK_ALL_PTR (&CPU_MASK_ALL)
    
    Which formalizes this practice.  One day gcc could bite us over this
    usage (though we seem to have gotten away with it so far).
    
    So replace everywhere which used &CPU_MASK_ALL or CPU_MASK_ALL_PTR
    with the modern "cpu_all_mask" (a real const struct cpumask *).
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Reported-by: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Mike Travis <travis@sgi.com>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 4fbc456f393d..84bbadd4d021 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -110,7 +110,7 @@ static void create_kthread(struct kthread_create_info *create)
 		 */
 		sched_setscheduler(create->result, SCHED_NORMAL, &param);
 		set_user_nice(create->result, KTHREAD_NICE_LEVEL);
-		set_cpus_allowed_ptr(create->result, CPU_MASK_ALL_PTR);
+		set_cpus_allowed_ptr(create->result, cpu_all_mask);
 	}
 	complete(&create->done);
 }
@@ -240,7 +240,7 @@ int kthreadd(void *unused)
 	set_task_comm(tsk, "kthreadd");
 	ignore_signals(tsk);
 	set_user_nice(tsk, KTHREAD_NICE_LEVEL);
-	set_cpus_allowed_ptr(tsk, CPU_MASK_ALL_PTR);
+	set_cpus_allowed_ptr(tsk, cpu_all_mask);
 
 	current->flags |= PF_NOFREEZE | PF_FREEZER_NOSIG;
 

commit 7e066fb870fcd1025ec3ba7bbde5d541094f4ce1
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Fri Nov 14 17:47:47 2008 -0500

    tracepoints: add DECLARE_TRACE() and DEFINE_TRACE()
    
    Impact: API *CHANGE*. Must update all tracepoint users.
    
    Add DEFINE_TRACE() to tracepoints to let them declare the tracepoint
    structure in a single spot for all the kernel. It helps reducing memory
    consumption, especially when declaring a lot of tracepoints, e.g. for
    kmalloc tracing.
    
    *API CHANGE WARNING*: now, DECLARE_TRACE() must be used in headers for
    tracepoint declarations rather than DEFINE_TRACE(). This is the sane way
    to do it. The name previously used was misleading.
    
    Updates scheduler instrumentation to follow this API change.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 8e7a7ce3ed0a..4fbc456f393d 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -21,6 +21,9 @@ static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
 struct task_struct *kthreadd_task;
 
+DEFINE_TRACE(sched_kthread_stop);
+DEFINE_TRACE(sched_kthread_stop_ret);
+
 struct kthread_create_info
 {
 	/* Information passed to kthread() from kthreadd. */

commit 92b29b86fe2e183d44eb467e5e74a5f718ef2e43
Merge: b9d7ccf56be1 98d9c66ab074
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 20 13:35:07 2008 -0700

    Merge branch 'tracing-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (131 commits)
      tracing/fastboot: improve help text
      tracing/stacktrace: improve help text
      tracing/fastboot: fix initcalls disposition in bootgraph.pl
      tracing/fastboot: fix bootgraph.pl initcall name regexp
      tracing/fastboot: fix issues and improve output of bootgraph.pl
      tracepoints: synchronize unregister static inline
      tracepoints: tracepoint_synchronize_unregister()
      ftrace: make ftrace_test_p6nop disassembler-friendly
      markers: fix synchronize marker unregister static inline
      tracing/fastboot: add better resolution to initcall debug/tracing
      trace: add build-time check to avoid overrunning hex buffer
      ftrace: fix hex output mode of ftrace
      tracing/fastboot: fix initcalls disposition in bootgraph.pl
      tracing/fastboot: fix printk format typo in boot tracer
      ftrace: return an error when setting a nonexistent tracer
      ftrace: make some tracers reentrant
      ring-buffer: make reentrant
      ring-buffer: move page indexes into page headers
      tracing/fastboot: only trace non-module initcalls
      ftrace: move pc counter in irqtrace
      ...
    
    Manually fix conflicts:
     - init/main.c: initcall tracing
     - kernel/module.c: verbose level vs tracepoints
     - scripts/bootgraph.pl: fallout from cherry-picking commits.

commit 293adee601bcd4cdb5076a9bda187137de17e96e
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sat Oct 18 20:28:24 2008 -0700

    kthread_bind: use wait_task_inactive(TASK_UNINTERRUPTIBLE)
    
    Now that wait_task_inactive(task, state) checks task->state == state,
    we can simplify the code and make this debugging check more robust.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 96cff2f8710b..14ec64fe175a 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -171,12 +171,11 @@ EXPORT_SYMBOL(kthread_create);
  */
 void kthread_bind(struct task_struct *k, unsigned int cpu)
 {
-	if (k->state != TASK_UNINTERRUPTIBLE) {
+	/* Must have done schedule() in kthread() before we set_task_cpu */
+	if (!wait_task_inactive(k, TASK_UNINTERRUPTIBLE)) {
 		WARN_ON(1);
 		return;
 	}
-	/* Must have done schedule() in kthread() before we set_task_cpu */
-	wait_task_inactive(k, 0);
 	set_task_cpu(k, cpu);
 	k->cpus_allowed = cpumask_of_cpu(cpu);
 	k->rt.nr_cpus_allowed = 1;

commit 0a16b6075843325dc402edf80c1662838b929aff
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Fri Jul 18 12:16:17 2008 -0400

    tracing, sched: LTTng instrumentation - scheduler
    
    Instrument the scheduler activity (sched_switch, migration, wakeups,
    wait for a task, signal delivery) and process/thread
    creation/destruction (fork, exit, kthread stop). Actually, kthread
    creation is not instrumented in this patch because it is architecture
    dependent. It allows to connect tracers such as ftrace which detects
    scheduling latencies, good/bad scheduler decisions. Tools like LTTng can
    export this scheduler information along with instrumentation of the rest
    of the kernel activity to perform post-mortem analysis on the scheduler
    activity.
    
    About the performance impact of tracepoints (which is comparable to
    markers), even without immediate values optimizations, tests done by
    Hideo Aoki on ia64 show no regression. His test case was using hackbench
    on a kernel where scheduler instrumentation (about 5 events in code
    scheduler code) was added. See the "Tracepoints" patch header for
    performance result detail.
    
    Changelog :
    
    - Change instrumentation location and parameter to match ftrace
      instrumentation, previously done with kernel markers.
    
    [ mingo@elte.hu: conflict resolutions ]
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Acked-by: 'Peter Zijlstra' <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 96cff2f8710b..50598e29439a 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -13,6 +13,7 @@
 #include <linux/file.h>
 #include <linux/module.h>
 #include <linux/mutex.h>
+#include <trace/sched.h>
 
 #define KTHREAD_NICE_LEVEL (-5)
 
@@ -206,6 +207,8 @@ int kthread_stop(struct task_struct *k)
 	/* It could exit after stop_info.k set, but before wake_up_process. */
 	get_task_struct(k);
 
+	trace_sched_kthread_stop(k);
+
 	/* Must init completion *before* thread sees kthread_stop_info.k */
 	init_completion(&kthread_stop_info.done);
 	smp_wmb();
@@ -221,6 +224,8 @@ int kthread_stop(struct task_struct *k)
 	ret = kthread_stop_info.err;
 	mutex_unlock(&kthread_stop_lock);
 
+	trace_sched_kthread_stop_ret(ret);
+
 	return ret;
 }
 EXPORT_SYMBOL(kthread_stop);

commit 85ba2d862e521375a8ee01526c5c46b1f24bb4af
Author: Roland McGrath <roland@redhat.com>
Date:   Fri Jul 25 19:45:58 2008 -0700

    tracehook: wait_task_inactive
    
    This extends wait_task_inactive() with a new argument so it can be used in
    a "soft" mode where it will check for the task changing state unexpectedly
    and back off.  There is no change to existing callers.  This lays the
    groundwork to allow robust, noninvasive tracing that can try to sample a
    blocked thread but back off safely if it wakes up.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 6111c27491b1..96cff2f8710b 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -176,7 +176,7 @@ void kthread_bind(struct task_struct *k, unsigned int cpu)
 		return;
 	}
 	/* Must have done schedule() in kthread() before we set_task_cpu */
-	wait_task_inactive(k);
+	wait_task_inactive(k, 0);
 	set_task_cpu(k, cpu);
 	k->cpus_allowed = cpumask_of_cpu(cpu);
 	k->rt.nr_cpus_allowed = 1;

commit 8df185a95c9b84fc0c3c02224e64fdc5b83bae34
Author: Mike Travis <travis@sgi.com>
Date:   Tue Jul 8 15:55:48 2008 -0700

    kthread: reduce stack pressure in create_kthread and kthreadd
    
      * Replace:
    
            set_cpus_allowed(..., CPU_MASK_ALL)
    
        with:
    
            set_cpus_allowed_ptr(..., CPU_MASK_ALL_PTR)
    
        to remove excessive stack requirements when NR_CPUS=4096.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index ac3fb7326641..6111c27491b1 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -106,7 +106,7 @@ static void create_kthread(struct kthread_create_info *create)
 		 */
 		sched_setscheduler(create->result, SCHED_NORMAL, &param);
 		set_user_nice(create->result, KTHREAD_NICE_LEVEL);
-		set_cpus_allowed(create->result, CPU_MASK_ALL);
+		set_cpus_allowed_ptr(create->result, CPU_MASK_ALL_PTR);
 	}
 	complete(&create->done);
 }
@@ -233,7 +233,7 @@ int kthreadd(void *unused)
 	set_task_comm(tsk, "kthreadd");
 	ignore_signals(tsk);
 	set_user_nice(tsk, KTHREAD_NICE_LEVEL);
-	set_cpus_allowed(tsk, CPU_MASK_ALL);
+	set_cpus_allowed_ptr(tsk, CPU_MASK_ALL_PTR);
 
 	current->flags |= PF_NOFREEZE | PF_FREEZER_NOSIG;
 

commit ebb12db51f6c13b30752fcf506baad4c617b153c
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Jun 11 22:04:29 2008 +0200

    Freezer: Introduce PF_FREEZER_NOSIG
    
    The freezer currently attempts to distinguish kernel threads from
    user space tasks by checking if their mm pointer is unset and it
    does not send fake signals to kernel threads.  However, there are
    kernel threads, mostly related to networking, that behave like
    user space tasks and may want to be sent a fake signal to be frozen.
    
    Introduce the new process flag PF_FREEZER_NOSIG that will be set
    by default for all kernel threads and make the freezer only send
    fake signals to the tasks having PF_FREEZER_NOSIG unset.  Provide
    the set_freezable_with_signal() function to be called by the kernel
    threads that want to be sent a fake signal for freezing.
    
    This patch should not change the freezer's observable behavior.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Pavel Machek <pavel@suse.cz>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 97747cdd37c9..ac3fb7326641 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -235,7 +235,7 @@ int kthreadd(void *unused)
 	set_user_nice(tsk, KTHREAD_NICE_LEVEL);
 	set_cpus_allowed(tsk, CPU_MASK_ALL);
 
-	current->flags |= PF_NOFREEZE;
+	current->flags |= PF_NOFREEZE | PF_FREEZER_NOSIG;
 
 	for (;;) {
 		set_current_state(TASK_INTERRUPTIBLE);

commit 9985b0bab332289f14837eff3c6e0bcc658b58f7
Author: David Rientjes <rientjes@google.com>
Date:   Thu Jun 5 12:57:11 2008 -0700

    sched: prevent bound kthreads from changing cpus_allowed
    
    Kthreads that have called kthread_bind() are bound to specific cpus, so
    other tasks should not be able to change their cpus_allowed from under
    them.  Otherwise, it is possible to move kthreads, such as the migration
    or software watchdog threads, so they are not allowed access to the cpu
    they work on.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Menage <menage@google.com>
    Cc: Paul Jackson <pj@sgi.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index bd1b9ea024e1..97747cdd37c9 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -180,6 +180,7 @@ void kthread_bind(struct task_struct *k, unsigned int cpu)
 	set_task_cpu(k, cpu);
 	k->cpus_allowed = cpumask_of_cpu(cpu);
 	k->rt.nr_cpus_allowed = 1;
+	k->flags |= PF_THREAD_BOUND;
 }
 EXPORT_SYMBOL(kthread_bind);
 

commit 5cd204550b1a006f2b0c986b0e0f53220ebfd391
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Wed Apr 30 00:54:24 2008 -0700

    Deprecate find_task_by_pid()
    
    There are some places that are known to operate on tasks'
    global pids only:
    
    * the rest_init() call (called on boot)
    * the kgdb's getthread
    * the create_kthread() (since the kthread is run in init ns)
    
    So use the find_task_by_pid_ns(..., &init_pid_ns) there
    and schedule the find_task_by_pid for removal.
    
    [sukadev@us.ibm.com: Fix warning in kernel/pid.c]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index ac72eea48339..bd1b9ea024e1 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -98,7 +98,7 @@ static void create_kthread(struct kthread_create_info *create)
 		struct sched_param param = { .sched_priority = 0 };
 		wait_for_completion(&create->started);
 		read_lock(&tasklist_lock);
-		create->result = find_task_by_pid(pid);
+		create->result = find_task_by_pid_ns(pid, &init_pid_ns);
 		read_unlock(&tasklist_lock);
 		/*
 		 * root may have changed our (kthreadd's) priority or CPU mask.

commit cbd9b67bd3883dff0ef4b8ec9229d315a9ba38f0
Author: Dmitry Adamushko <dmitry.adamushko@gmail.com>
Date:   Tue Apr 29 00:59:23 2008 -0700

    kthread: call wake_up_process() without the lock being held
    
    From the POV of synchronization, there should be no need to call
    wake_up_process() with the 'kthread_create_lock' being held.
    
    Signed-off-by: Dmitry Adamushko <dmitry.adamushko@gmail.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 92cf6930ab51..ac72eea48339 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -144,9 +144,9 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 
 	spin_lock(&kthread_create_lock);
 	list_add_tail(&create.list, &kthread_create_list);
-	wake_up_process(kthreadd_task);
 	spin_unlock(&kthread_create_lock);
 
+	wake_up_process(kthreadd_task);
 	wait_for_completion(&create.done);
 
 	if (!IS_ERR(create.result)) {

commit 429f731dea577bdd43693940cdca524135287e6a
Merge: 85b375a61308 d2f5e80862d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 21 15:41:27 2008 -0700

    Merge branch 'semaphore' of git://git.kernel.org/pub/scm/linux/kernel/git/willy/misc
    
    * 'semaphore' of git://git.kernel.org/pub/scm/linux/kernel/git/willy/misc:
      Deprecate the asm/semaphore.h files in feature-removal-schedule.
      Convert asm/semaphore.h users to linux/semaphore.h
      security: Remove unnecessary inclusions of asm/semaphore.h
      lib: Remove unnecessary inclusions of asm/semaphore.h
      kernel: Remove unnecessary inclusions of asm/semaphore.h
      include: Remove unnecessary inclusions of asm/semaphore.h
      fs: Remove unnecessary inclusions of asm/semaphore.h
      drivers: Remove unnecessary inclusions of asm/semaphore.h
      net: Remove unnecessary inclusions of asm/semaphore.h
      arch: Remove unnecessary inclusions of asm/semaphore.h

commit 9f0e738f492522a2f70ad9a2a0287e4e966c633a
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Tue Feb 12 13:30:05 2008 -0500

    sched: fix cpus_allowed settings
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Acked-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 0ac887882f90..25241d6ec8cd 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -180,6 +180,7 @@ void kthread_bind(struct task_struct *k, unsigned int cpu)
 	wait_task_inactive(k);
 	set_task_cpu(k, cpu);
 	k->cpus_allowed = cpumask_of_cpu(cpu);
+	k->rt.nr_cpus_allowed = 1;
 }
 EXPORT_SYMBOL(kthread_bind);
 

commit a6550207538619bc9b90bac2e1d5e54902a432ad
Author: Matthew Wilcox <matthew@wil.cx>
Date:   Tue Feb 26 10:47:18 2008 -0500

    kernel: Remove unnecessary inclusions of asm/semaphore.h
    
    None of these files use any of the functionality promised by
    asm/semaphore.h.
    
    Signed-off-by: Matthew Wilcox <willy@linux.intel.com>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 0ac887882f90..92dd1bcaa0df 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -13,7 +13,6 @@
 #include <linux/file.h>
 #include <linux/module.h>
 #include <linux/mutex.h>
-#include <asm/semaphore.h>
 
 #define KTHREAD_NICE_LEVEL (-5)
 

commit 4f05b98d54b140ed3c5851d5d5156e9918c6305d
Author: Michal Schmidt <mschmidt@redhat.com>
Date:   Fri Jan 25 21:08:33 2008 +0100

    sched: fix, always create kernel threads with normal priority
    
    Ensure that the kernel threads are created with the usual nice level
    and affinity even if kthreadd's properties were changed from the
    default by root.
    
    Signed-off-by: Michal Schmidt <mschmidt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index dcfe724300eb..0ac887882f90 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -15,6 +15,8 @@
 #include <linux/mutex.h>
 #include <asm/semaphore.h>
 
+#define KTHREAD_NICE_LEVEL (-5)
+
 static DEFINE_SPINLOCK(kthread_create_lock);
 static LIST_HEAD(kthread_create_list);
 struct task_struct *kthreadd_task;
@@ -94,10 +96,18 @@ static void create_kthread(struct kthread_create_info *create)
 	if (pid < 0) {
 		create->result = ERR_PTR(pid);
 	} else {
+		struct sched_param param = { .sched_priority = 0 };
 		wait_for_completion(&create->started);
 		read_lock(&tasklist_lock);
 		create->result = find_task_by_pid(pid);
 		read_unlock(&tasklist_lock);
+		/*
+		 * root may have changed our (kthreadd's) priority or CPU mask.
+		 * The kernel thread should not inherit these properties.
+		 */
+		sched_setscheduler(create->result, SCHED_NORMAL, &param);
+		set_user_nice(create->result, KTHREAD_NICE_LEVEL);
+		set_cpus_allowed(create->result, CPU_MASK_ALL);
 	}
 	complete(&create->done);
 }
@@ -221,7 +231,7 @@ int kthreadd(void *unused)
 	/* Setup a clean context for our children to inherit. */
 	set_task_comm(tsk, "kthreadd");
 	ignore_signals(tsk);
-	set_user_nice(tsk, -5);
+	set_user_nice(tsk, KTHREAD_NICE_LEVEL);
 	set_cpus_allowed(tsk, CPU_MASK_ALL);
 
 	current->flags |= PF_NOFREEZE;

commit e804a4a4dd596d853f6d6f814fbdcf97b8efcdea
Author: Satyam Sharma <ssatyam@cse.iitk.ac.in>
Date:   Tue Jul 31 00:39:16 2007 -0700

    kthread: silence bogus section mismatch warning
    
    WARNING: kernel/built-in.o(.text+0x16910): Section mismatch:
    reference to .init.text: (between 'kthreadd' and 'init_waitqueue_head')
    
    comes because kernel/kthread.c:kthreadd() is not __init but calls
    kthreadd_setup() which is __init. But this is ok, because kthreadd_setup()
    is only ever called at init time, and then kthreadd() proceeds into its
    "for (;;)" loop. We could mark kthreadd __init_refok, but kthreadd_setup()
    with just one callsite and 4 lines in it (it's been that small since
    10ab825bdef8df51) doesn't need to be a separate function at all -- so let's
    just move those four lines at beginning of kthreadd() itself.
    
    Signed-off-by: Satyam Sharma <ssatyam@cse.iitk.ac.in>
    Acked-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index a404f7ee7395..dcfe724300eb 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -214,23 +214,15 @@ int kthread_stop(struct task_struct *k)
 }
 EXPORT_SYMBOL(kthread_stop);
 
-
-static noinline __init_refok void kthreadd_setup(void)
+int kthreadd(void *unused)
 {
 	struct task_struct *tsk = current;
 
+	/* Setup a clean context for our children to inherit. */
 	set_task_comm(tsk, "kthreadd");
-
 	ignore_signals(tsk);
-
 	set_user_nice(tsk, -5);
 	set_cpus_allowed(tsk, CPU_MASK_ALL);
-}
-
-int kthreadd(void *unused)
-{
-	/* Setup a clean context for our children to inherit. */
-	kthreadd_setup();
 
 	current->flags |= PF_NOFREEZE;
 

commit 98011f569e2ae1e4ae394f6e23faa16676d50de4
Author: Jan Beulich <jbeulich@novell.com>
Date:   Sun Jul 15 23:38:17 2007 -0700

    mm: fix improper .init-type section references
    
    .. which modpost started warning about.
    
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index bbd51b81a3e8..a404f7ee7395 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -215,7 +215,7 @@ int kthread_stop(struct task_struct *k)
 EXPORT_SYMBOL(kthread_stop);
 
 
-static __init void kthreadd_setup(void)
+static noinline __init_refok void kthreadd_setup(void)
 {
 	struct task_struct *tsk = current;
 

commit a076e4bca2fdabb9e45d86722cc72c0944da5f94
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 23 13:57:27 2007 -0700

    freezer: fix kthread_create vs freezer theoretical race
    
    kthread() sleeps in TASK_INTERRUPTIBLE state waiting for the first wakeup.  In
    theory, this wakeup may come from freeze_process()->signal_wake_up(), so the
    task can disappear even before kthread_create() sets its ->comm.
    
    Change kthread() to use TASK_UNINTERRUPTIBLE.
    
    [akpm@linux-foundation.org: s/BUG_ON/WARN_ON+recover]
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index df8a8e8f6ca4..bbd51b81a3e8 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -70,7 +70,7 @@ static int kthread(void *_create)
 	data = create->data;
 
 	/* OK, tell user we're spawned, wait for stop or wakeup */
-	__set_current_state(TASK_INTERRUPTIBLE);
+	__set_current_state(TASK_UNINTERRUPTIBLE);
 	complete(&create->started);
 	schedule();
 
@@ -162,7 +162,10 @@ EXPORT_SYMBOL(kthread_create);
  */
 void kthread_bind(struct task_struct *k, unsigned int cpu)
 {
-	BUG_ON(k->state != TASK_INTERRUPTIBLE);
+	if (k->state != TASK_UNINTERRUPTIBLE) {
+		WARN_ON(1);
+		return;
+	}
 	/* Must have done schedule() in kthread() before we set_task_cpu */
 	wait_task_inactive(k);
 	set_task_cpu(k, cpu);

commit 10ab825bdef8df510f99c703a5a2d9b13a4e31a5
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 9 02:34:37 2007 -0700

    change kernel threads to ignore signals instead of blocking them
    
    Currently kernel threads use sigprocmask(SIG_BLOCK) to protect against
    signals.  This doesn't prevent the signal delivery, this only blocks
    signal_wake_up().  Every "killall -33 kthreadd" means a "struct siginfo"
    leak.
    
    Change kthreadd_setup() to set all handlers to SIG_IGN instead of blocking
    them (make a new helper ignore_signals() for that).  If the kernel thread
    needs some signal, it should use allow_signal() anyway, and in that case it
    should not use CLONE_SIGHAND.
    
    Note that we can't change daemonize() (should die!) in the same way,
    because it can be used along with CLONE_SIGHAND.  This means that
    allow_signal() still should unblock the signal to work correctly with
    daemonize()ed threads.
    
    However, disallow_signal() doesn't block the signal any longer but ignores
    it.
    
    NOTE: with or without this patch the kernel threads are not protected from
    handle_stop_signal(), this seems harmless, but not good.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 0eb0070a3c57..df8a8e8f6ca4 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -215,24 +215,13 @@ EXPORT_SYMBOL(kthread_stop);
 static __init void kthreadd_setup(void)
 {
 	struct task_struct *tsk = current;
-	struct k_sigaction sa;
-	sigset_t blocked;
 
 	set_task_comm(tsk, "kthreadd");
 
-	/* Block and flush all signals */
-	sigfillset(&blocked);
-	sigprocmask(SIG_BLOCK, &blocked, NULL);
-	flush_signals(tsk);
+	ignore_signals(tsk);
 
-	/* SIG_IGN makes children autoreap: see do_notify_parent(). */
-	sa.sa.sa_handler = SIG_IGN;
-	sa.sa.sa_flags = 0;
-	siginitset(&sa.sa.sa_mask, sigmask(SIGCHLD));
-	do_sigaction(SIGCHLD, &sa, (struct k_sigaction *)0);
-
-	set_user_nice(current, -5);
-	set_cpus_allowed(current, CPU_MASK_ALL);
+	set_user_nice(tsk, -5);
+	set_cpus_allowed(tsk, CPU_MASK_ALL);
 }
 
 int kthreadd(void *unused)

commit 73c279927f89561ecb45b2dfdf9314bafcfd9f67
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed May 9 02:34:32 2007 -0700

    kthread: don't depend on work queues
    
    Currently there is a circular reference between work queue initialization
    and kthread initialization.  This prevents the kthread infrastructure from
    initializing until after work queues have been initialized.
    
    We want the properties of tasks created with kthread_create to be as close
    as possible to the init_task and to not be contaminated by user processes.
    The later we start our kthreadd that creates these tasks the harder it is
    to avoid contamination from user processes and the more of a mess we have
    to clean up because the defaults have changed on us.
    
    So this patch modifies the kthread support to not use work queues but to
    instead use a simple list of structures, and to have kthreadd start from
    init_task immediately after our kernel thread that execs /sbin/init.
    
    By being a true child of init_task we only have to change those process
    settings that we want to have different from init_task, such as our process
    name, the cpus that are allowed, blocking all signals and setting SIGCHLD
    to SIG_IGN so that all of our children are reaped automatically.
    
    By being a true child of init_task we also naturally get our ppid set to 0
    and do not wind up as a child of PID == 1.  Ensuring that tasks generated
    by kthread_create will not slow down the functioning of the wait family of
    functions.
    
    [akpm@linux-foundation.org: use interruptible sleeps]
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 87c50ccd1d4e..0eb0070a3c57 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -1,7 +1,7 @@
 /* Kernel thread helper functions.
  *   Copyright (C) 2004 IBM Corporation, Rusty Russell.
  *
- * Creation is done via keventd, so that we get a clean environment
+ * Creation is done via kthreadd, so that we get a clean environment
  * even if we're invoked from userspace (think modprobe, hotplug cpu,
  * etc.).
  */
@@ -15,24 +15,22 @@
 #include <linux/mutex.h>
 #include <asm/semaphore.h>
 
-/*
- * We dont want to execute off keventd since it might
- * hold a semaphore our callers hold too:
- */
-static struct workqueue_struct *helper_wq;
+static DEFINE_SPINLOCK(kthread_create_lock);
+static LIST_HEAD(kthread_create_list);
+struct task_struct *kthreadd_task;
 
 struct kthread_create_info
 {
-	/* Information passed to kthread() from keventd. */
+	/* Information passed to kthread() from kthreadd. */
 	int (*threadfn)(void *data);
 	void *data;
 	struct completion started;
 
-	/* Result passed back to kthread_create() from keventd. */
+	/* Result passed back to kthread_create() from kthreadd. */
 	struct task_struct *result;
 	struct completion done;
 
-	struct work_struct work;
+	struct list_head list;
 };
 
 struct kthread_stop_info
@@ -60,42 +58,17 @@ int kthread_should_stop(void)
 }
 EXPORT_SYMBOL(kthread_should_stop);
 
-static void kthread_exit_files(void)
-{
-	struct fs_struct *fs;
-	struct task_struct *tsk = current;
-
-	exit_fs(tsk);		/* current->fs->count--; */
-	fs = init_task.fs;
-	tsk->fs = fs;
-	atomic_inc(&fs->count);
- 	exit_files(tsk);
-	current->files = init_task.files;
-	atomic_inc(&tsk->files->count);
-}
-
 static int kthread(void *_create)
 {
 	struct kthread_create_info *create = _create;
 	int (*threadfn)(void *data);
 	void *data;
-	sigset_t blocked;
 	int ret = -EINTR;
 
-	kthread_exit_files();
-
-	/* Copy data: it's on keventd's stack */
+	/* Copy data: it's on kthread's stack */
 	threadfn = create->threadfn;
 	data = create->data;
 
-	/* Block and flush all signals (in case we're not from keventd). */
-	sigfillset(&blocked);
-	sigprocmask(SIG_BLOCK, &blocked, NULL);
-	flush_signals(current);
-
-	/* By default we can run anywhere, unlike keventd. */
-	set_cpus_allowed(current, CPU_MASK_ALL);
-
 	/* OK, tell user we're spawned, wait for stop or wakeup */
 	__set_current_state(TASK_INTERRUPTIBLE);
 	complete(&create->started);
@@ -112,11 +85,8 @@ static int kthread(void *_create)
 	return 0;
 }
 
-/* We are keventd: create a thread. */
-static void keventd_create_kthread(struct work_struct *work)
+static void create_kthread(struct kthread_create_info *create)
 {
-	struct kthread_create_info *create =
-		container_of(work, struct kthread_create_info, work);
 	int pid;
 
 	/* We want our own signal handler (we take no signals by default). */
@@ -162,17 +132,14 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 	create.data = data;
 	init_completion(&create.started);
 	init_completion(&create.done);
-	INIT_WORK(&create.work, keventd_create_kthread);
-
-	/*
-	 * The workqueue needs to start up first:
-	 */
-	if (!helper_wq)
-		create.work.func(&create.work);
-	else {
-		queue_work(helper_wq, &create.work);
-		wait_for_completion(&create.done);
-	}
+
+	spin_lock(&kthread_create_lock);
+	list_add_tail(&create.list, &kthread_create_list);
+	wake_up_process(kthreadd_task);
+	spin_unlock(&kthread_create_lock);
+
+	wait_for_completion(&create.done);
+
 	if (!IS_ERR(create.result)) {
 		va_list args;
 		va_start(args, namefmt);
@@ -180,7 +147,6 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 			  namefmt, args);
 		va_end(args);
 	}
-
 	return create.result;
 }
 EXPORT_SYMBOL(kthread_create);
@@ -245,12 +211,58 @@ int kthread_stop(struct task_struct *k)
 }
 EXPORT_SYMBOL(kthread_stop);
 
-static __init int helper_init(void)
+
+static __init void kthreadd_setup(void)
 {
-	helper_wq = create_singlethread_workqueue("kthread");
-	BUG_ON(!helper_wq);
+	struct task_struct *tsk = current;
+	struct k_sigaction sa;
+	sigset_t blocked;
 
-	return 0;
+	set_task_comm(tsk, "kthreadd");
+
+	/* Block and flush all signals */
+	sigfillset(&blocked);
+	sigprocmask(SIG_BLOCK, &blocked, NULL);
+	flush_signals(tsk);
+
+	/* SIG_IGN makes children autoreap: see do_notify_parent(). */
+	sa.sa.sa_handler = SIG_IGN;
+	sa.sa.sa_flags = 0;
+	siginitset(&sa.sa.sa_mask, sigmask(SIGCHLD));
+	do_sigaction(SIGCHLD, &sa, (struct k_sigaction *)0);
+
+	set_user_nice(current, -5);
+	set_cpus_allowed(current, CPU_MASK_ALL);
 }
 
-core_initcall(helper_init);
+int kthreadd(void *unused)
+{
+	/* Setup a clean context for our children to inherit. */
+	kthreadd_setup();
+
+	current->flags |= PF_NOFREEZE;
+
+	for (;;) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		if (list_empty(&kthread_create_list))
+			schedule();
+		__set_current_state(TASK_RUNNING);
+
+		spin_lock(&kthread_create_lock);
+		while (!list_empty(&kthread_create_list)) {
+			struct kthread_create_info *create;
+
+			create = list_entry(kthread_create_list.next,
+					    struct kthread_create_info, list);
+			list_del_init(&create->list);
+			spin_unlock(&kthread_create_lock);
+
+			create_kthread(create);
+
+			spin_lock(&kthread_create_lock);
+		}
+		spin_unlock(&kthread_create_lock);
+	}
+
+	return 0;
+}

commit 72fd4a35a824331d7a0f4168d7576502d95d34b3
Author: Robert P. J. Day <rpjday@mindspring.com>
Date:   Sat Feb 10 01:45:59 2007 -0800

    [PATCH] Numerous fixes to kernel-doc info in source files.
    
    A variety of (mostly) innocuous fixes to the embedded kernel-doc content in
    source files, including:
    
      * make multi-line initial descriptions single line
      * denote some function names, constants and structs as such
      * change erroneous opening '/*' to '/**' in a few places
      * reword some text for clarity
    
    Signed-off-by: Robert P. J. Day <rpjday@mindspring.com>
    Cc: "Randy.Dunlap" <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 1db8c72d0d38..87c50ccd1d4e 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -50,7 +50,7 @@ static struct kthread_stop_info kthread_stop_info;
 /**
  * kthread_should_stop - should this kthread return now?
  *
- * When someone calls kthread_stop on your kthread, it will be woken
+ * When someone calls kthread_stop() on your kthread, it will be woken
  * and this will return true.  You should then return, and your return
  * value will be passed through to kthread_stop().
  */
@@ -143,7 +143,7 @@ static void keventd_create_kthread(struct work_struct *work)
  * it.  See also kthread_run(), kthread_create_on_cpu().
  *
  * When woken, the thread will run @threadfn() with @data as its
- * argument. @threadfn can either call do_exit() directly if it is a
+ * argument. @threadfn() can either call do_exit() directly if it is a
  * standalone thread for which noone will call kthread_stop(), or
  * return when 'kthread_should_stop()' is true (which means
  * kthread_stop() has been called).  The return value should be zero
@@ -192,7 +192,7 @@ EXPORT_SYMBOL(kthread_create);
  *
  * Description: This function is equivalent to set_cpus_allowed(),
  * except that @cpu doesn't need to be online, and the thread must be
- * stopped (i.e., just returned from kthread_create().
+ * stopped (i.e., just returned from kthread_create()).
  */
 void kthread_bind(struct task_struct *k, unsigned int cpu)
 {

commit 65f27f38446e1976cc98fd3004b110fedcddd189
Author: David Howells <dhowells@redhat.com>
Date:   Wed Nov 22 14:55:48 2006 +0000

    WorkStruct: Pass the work_struct pointer instead of context data
    
    Pass the work_struct pointer to the work function rather than context data.
    The work function can use container_of() to work out the data.
    
    For the cases where the container of the work_struct may go away the moment the
    pending bit is cleared, it is made possible to defer the release of the
    structure by deferring the clearing of the pending bit.
    
    To make this work, an extra flag is introduced into the management side of the
    work_struct.  This governs auto-release of the structure upon execution.
    
    Ordinarily, the work queue executor would release the work_struct for further
    scheduling or deallocation by clearing the pending bit prior to jumping to the
    work function.  This means that, unless the driver makes some guarantee itself
    that the work_struct won't go away, the work function may not access anything
    else in the work_struct or its container lest they be deallocated..  This is a
    problem if the auxiliary data is taken away (as done by the last patch).
    
    However, if the pending bit is *not* cleared before jumping to the work
    function, then the work function *may* access the work_struct and its container
    with no problems.  But then the work function must itself release the
    work_struct by calling work_release().
    
    In most cases, automatic release is fine, so this is the default.  Special
    initiators exist for the non-auto-release case (ending in _NAR).
    
    
    Signed-Off-By: David Howells <dhowells@redhat.com>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 4f9c60ef95e8..1db8c72d0d38 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -31,6 +31,8 @@ struct kthread_create_info
 	/* Result passed back to kthread_create() from keventd. */
 	struct task_struct *result;
 	struct completion done;
+
+	struct work_struct work;
 };
 
 struct kthread_stop_info
@@ -111,9 +113,10 @@ static int kthread(void *_create)
 }
 
 /* We are keventd: create a thread. */
-static void keventd_create_kthread(void *_create)
+static void keventd_create_kthread(struct work_struct *work)
 {
-	struct kthread_create_info *create = _create;
+	struct kthread_create_info *create =
+		container_of(work, struct kthread_create_info, work);
 	int pid;
 
 	/* We want our own signal handler (we take no signals by default). */
@@ -154,20 +157,20 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 				   ...)
 {
 	struct kthread_create_info create;
-	DECLARE_WORK(work, keventd_create_kthread, &create);
 
 	create.threadfn = threadfn;
 	create.data = data;
 	init_completion(&create.started);
 	init_completion(&create.done);
+	INIT_WORK(&create.work, keventd_create_kthread);
 
 	/*
 	 * The workqueue needs to start up first:
 	 */
 	if (!helper_wq)
-		work.func(work.data);
+		create.work.func(&create.work);
 	else {
-		queue_work(helper_wq, &work);
+		queue_work(helper_wq, &create.work);
 		wait_for_completion(&create.done);
 	}
 	if (!IS_ERR(create.result)) {

commit 52e92e5788139921352213fa6faf6e30ff1f2f5a
Author: Adrian Bunk <bunk@stusta.de>
Date:   Fri Jul 14 00:24:05 2006 -0700

    [PATCH] remove kernel/kthread.c:kthread_stop_sem()
    
    Remove the now-unneeded kthread_stop_sem().
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Acked-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 24be714b04c7..4f9c60ef95e8 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -215,23 +215,6 @@ EXPORT_SYMBOL(kthread_bind);
  * was never called.
  */
 int kthread_stop(struct task_struct *k)
-{
-	return kthread_stop_sem(k, NULL);
-}
-EXPORT_SYMBOL(kthread_stop);
-
-/**
- * kthread_stop_sem - stop a thread created by kthread_create().
- * @k: thread created by kthread_create().
- * @s: semaphore that @k waits on while idle.
- *
- * Does essentially the same thing as kthread_stop() above, but wakes
- * @k by calling up(@s).
- *
- * Returns the result of threadfn(), or %-EINTR if wake_up_process()
- * was never called.
- */
-int kthread_stop_sem(struct task_struct *k, struct semaphore *s)
 {
 	int ret;
 
@@ -246,10 +229,7 @@ int kthread_stop_sem(struct task_struct *k, struct semaphore *s)
 
 	/* Now set kthread_should_stop() to true, and wake it up. */
 	kthread_stop_info.k = k;
-	if (s)
-		up(s);
-	else
-		wake_up_process(k);
+	wake_up_process(k);
 	put_task_struct(k);
 
 	/* Once it dies, reset stop ptr, gather result and we're done. */
@@ -260,7 +240,7 @@ int kthread_stop_sem(struct task_struct *k, struct semaphore *s)
 
 	return ret;
 }
-EXPORT_SYMBOL(kthread_stop_sem);
+EXPORT_SYMBOL(kthread_stop);
 
 static __init int helper_init(void)
 {

commit 9e37bd301ee130598fa1406c1281caa159473bf8
Author: Randy Dunlap <rdunlap@xenotime.net>
Date:   Sun Jun 25 05:49:19 2006 -0700

    [PATCH] kthread: move kernel-doc and put it into DocBook
    
    Move kthread API kernel-doc from kthread.h to kthread.c & fix it.
    Add kthread API to kernel-api DocBook.
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index c5f3c6613b6d..24be714b04c7 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -45,6 +45,13 @@ struct kthread_stop_info
 static DEFINE_MUTEX(kthread_stop_lock);
 static struct kthread_stop_info kthread_stop_info;
 
+/**
+ * kthread_should_stop - should this kthread return now?
+ *
+ * When someone calls kthread_stop on your kthread, it will be woken
+ * and this will return true.  You should then return, and your return
+ * value will be passed through to kthread_stop().
+ */
 int kthread_should_stop(void)
 {
 	return (kthread_stop_info.k == current);
@@ -122,6 +129,25 @@ static void keventd_create_kthread(void *_create)
 	complete(&create->done);
 }
 
+/**
+ * kthread_create - create a kthread.
+ * @threadfn: the function to run until signal_pending(current).
+ * @data: data ptr for @threadfn.
+ * @namefmt: printf-style name for the thread.
+ *
+ * Description: This helper function creates and names a kernel
+ * thread.  The thread will be stopped: use wake_up_process() to start
+ * it.  See also kthread_run(), kthread_create_on_cpu().
+ *
+ * When woken, the thread will run @threadfn() with @data as its
+ * argument. @threadfn can either call do_exit() directly if it is a
+ * standalone thread for which noone will call kthread_stop(), or
+ * return when 'kthread_should_stop()' is true (which means
+ * kthread_stop() has been called).  The return value should be zero
+ * or a negative error number; it will be passed to kthread_stop().
+ *
+ * Returns a task_struct or ERR_PTR(-ENOMEM).
+ */
 struct task_struct *kthread_create(int (*threadfn)(void *data),
 				   void *data,
 				   const char namefmt[],
@@ -156,6 +182,15 @@ struct task_struct *kthread_create(int (*threadfn)(void *data),
 }
 EXPORT_SYMBOL(kthread_create);
 
+/**
+ * kthread_bind - bind a just-created kthread to a cpu.
+ * @k: thread created by kthread_create().
+ * @cpu: cpu (might not be online, must be possible) for @k to run on.
+ *
+ * Description: This function is equivalent to set_cpus_allowed(),
+ * except that @cpu doesn't need to be online, and the thread must be
+ * stopped (i.e., just returned from kthread_create().
+ */
 void kthread_bind(struct task_struct *k, unsigned int cpu)
 {
 	BUG_ON(k->state != TASK_INTERRUPTIBLE);
@@ -166,12 +201,36 @@ void kthread_bind(struct task_struct *k, unsigned int cpu)
 }
 EXPORT_SYMBOL(kthread_bind);
 
+/**
+ * kthread_stop - stop a thread created by kthread_create().
+ * @k: thread created by kthread_create().
+ *
+ * Sets kthread_should_stop() for @k to return true, wakes it, and
+ * waits for it to exit.  Your threadfn() must not call do_exit()
+ * itself if you use this function!  This can also be called after
+ * kthread_create() instead of calling wake_up_process(): the thread
+ * will exit without calling threadfn().
+ *
+ * Returns the result of threadfn(), or %-EINTR if wake_up_process()
+ * was never called.
+ */
 int kthread_stop(struct task_struct *k)
 {
 	return kthread_stop_sem(k, NULL);
 }
 EXPORT_SYMBOL(kthread_stop);
 
+/**
+ * kthread_stop_sem - stop a thread created by kthread_create().
+ * @k: thread created by kthread_create().
+ * @s: semaphore that @k waits on while idle.
+ *
+ * Does essentially the same thing as kthread_stop() above, but wakes
+ * @k by calling up(@s).
+ *
+ * Returns the result of threadfn(), or %-EINTR if wake_up_process()
+ * was never called.
+ */
 int kthread_stop_sem(struct task_struct *k, struct semaphore *s)
 {
 	int ret;
@@ -210,5 +269,5 @@ static __init int helper_init(void)
 
 	return 0;
 }
-core_initcall(helper_init);
 
+core_initcall(helper_init);

commit 05eeae208d08a05a6980cf2ff61f02843c0955fd
Author: Andrew Morton <akpm@osdl.org>
Date:   Sat Mar 25 03:07:48 2006 -0800

    [PATCH] find_task_by_pid() needs tasklist_lock
    
    A couple of places are forgetting to take it.
    
    The kswapd case is probably unimportant.  keventd_create_kthread() was racy.
    
    The whole thing is a bit flakey: you start a kernel thread, get its pid from
    kernel_thread() then look up its task_struct.
    
    a) It assumes that pid recycling takes a "long" time.
    
    b) We get a task_struct but no reference was taken on it.  The owner of the
       kswapd and kthread task_struct*'s must assume that the new thread won't
       exit unexpectedly.  Because if it does, they're left holding dead memory
       and any attempt to control or stop that task will crash.
    
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index 6a5373868a98..c5f3c6613b6d 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -115,7 +115,9 @@ static void keventd_create_kthread(void *_create)
 		create->result = ERR_PTR(pid);
 	} else {
 		wait_for_completion(&create->started);
+		read_lock(&tasklist_lock);
 		create->result = find_task_by_pid(pid);
+		read_unlock(&tasklist_lock);
 	}
 	complete(&create->done);
 }

commit 97d1f15b7ef52c1e9c28dc48b454024bb53a5fd2
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Thu Mar 23 03:00:24 2006 -0800

    [PATCH] sem2mutex: kernel/
    
    Semaphore to mutex conversion.
    
    The conversion was generated via scripts, and the result was validated
    automatically via a script as well.
    
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index e75950a1092c..6a5373868a98 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -12,6 +12,7 @@
 #include <linux/unistd.h>
 #include <linux/file.h>
 #include <linux/module.h>
+#include <linux/mutex.h>
 #include <asm/semaphore.h>
 
 /*
@@ -41,7 +42,7 @@ struct kthread_stop_info
 
 /* Thread stopping is done by setthing this var: lock serializes
  * multiple kthread_stop calls. */
-static DECLARE_MUTEX(kthread_stop_lock);
+static DEFINE_MUTEX(kthread_stop_lock);
 static struct kthread_stop_info kthread_stop_info;
 
 int kthread_should_stop(void)
@@ -173,7 +174,7 @@ int kthread_stop_sem(struct task_struct *k, struct semaphore *s)
 {
 	int ret;
 
-	down(&kthread_stop_lock);
+	mutex_lock(&kthread_stop_lock);
 
 	/* It could exit after stop_info.k set, but before wake_up_process. */
 	get_task_struct(k);
@@ -194,7 +195,7 @@ int kthread_stop_sem(struct task_struct *k, struct semaphore *s)
 	wait_for_completion(&kthread_stop_info.done);
 	kthread_stop_info.k = NULL;
 	ret = kthread_stop_info.err;
-	up(&kthread_stop_lock);
+	mutex_unlock(&kthread_stop_lock);
 
 	return ret;
 }

commit 61e1a9ea4b425eb8c3b4965c35fe953bd881728f
Author: Alan Stern <stern@rowland.harvard.edu>
Date:   Sun Oct 30 15:01:40 2005 -0800

    [PATCH] Add kthread_stop_sem()
    
    Enhance the kthread API by adding kthread_stop_sem, for use in stopping
    threads that spend their idle time waiting on a semaphore.
    
    Signed-off-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index f50f174e92da..e75950a1092c 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -164,6 +164,12 @@ void kthread_bind(struct task_struct *k, unsigned int cpu)
 EXPORT_SYMBOL(kthread_bind);
 
 int kthread_stop(struct task_struct *k)
+{
+	return kthread_stop_sem(k, NULL);
+}
+EXPORT_SYMBOL(kthread_stop);
+
+int kthread_stop_sem(struct task_struct *k, struct semaphore *s)
 {
 	int ret;
 
@@ -178,7 +184,10 @@ int kthread_stop(struct task_struct *k)
 
 	/* Now set kthread_should_stop() to true, and wake it up. */
 	kthread_stop_info.k = k;
-	wake_up_process(k);
+	if (s)
+		up(s);
+	else
+		wake_up_process(k);
 	put_task_struct(k);
 
 	/* Once it dies, reset stop ptr, gather result and we're done. */
@@ -189,7 +198,7 @@ int kthread_stop(struct task_struct *k)
 
 	return ret;
 }
-EXPORT_SYMBOL(kthread_stop);
+EXPORT_SYMBOL(kthread_stop_sem);
 
 static __init int helper_init(void)
 {

commit d59dd4620fb8d6422555a9e2b82a707718e68327
Author: akpm@osdl.org <akpm@osdl.org>
Date:   Sun May 1 08:58:47 2005 -0700

    [PATCH] use smp_mb/wmb/rmb where possible
    
    Replace a number of memory barriers with smp_ variants.  This means we won't
    take the unnecessary hit on UP machines.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/kthread.c b/kernel/kthread.c
index e377e2244103..f50f174e92da 100644
--- a/kernel/kthread.c
+++ b/kernel/kthread.c
@@ -174,7 +174,7 @@ int kthread_stop(struct task_struct *k)
 
 	/* Must init completion *before* thread sees kthread_stop_info.k */
 	init_completion(&kthread_stop_info.done);
-	wmb();
+	smp_wmb();
 
 	/* Now set kthread_should_stop() to true, and wake it up. */
 	kthread_stop_info.k = k;

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/kernel/kthread.c b/kernel/kthread.c
new file mode 100644
index 000000000000..e377e2244103
--- /dev/null
+++ b/kernel/kthread.c
@@ -0,0 +1,202 @@
+/* Kernel thread helper functions.
+ *   Copyright (C) 2004 IBM Corporation, Rusty Russell.
+ *
+ * Creation is done via keventd, so that we get a clean environment
+ * even if we're invoked from userspace (think modprobe, hotplug cpu,
+ * etc.).
+ */
+#include <linux/sched.h>
+#include <linux/kthread.h>
+#include <linux/completion.h>
+#include <linux/err.h>
+#include <linux/unistd.h>
+#include <linux/file.h>
+#include <linux/module.h>
+#include <asm/semaphore.h>
+
+/*
+ * We dont want to execute off keventd since it might
+ * hold a semaphore our callers hold too:
+ */
+static struct workqueue_struct *helper_wq;
+
+struct kthread_create_info
+{
+	/* Information passed to kthread() from keventd. */
+	int (*threadfn)(void *data);
+	void *data;
+	struct completion started;
+
+	/* Result passed back to kthread_create() from keventd. */
+	struct task_struct *result;
+	struct completion done;
+};
+
+struct kthread_stop_info
+{
+	struct task_struct *k;
+	int err;
+	struct completion done;
+};
+
+/* Thread stopping is done by setthing this var: lock serializes
+ * multiple kthread_stop calls. */
+static DECLARE_MUTEX(kthread_stop_lock);
+static struct kthread_stop_info kthread_stop_info;
+
+int kthread_should_stop(void)
+{
+	return (kthread_stop_info.k == current);
+}
+EXPORT_SYMBOL(kthread_should_stop);
+
+static void kthread_exit_files(void)
+{
+	struct fs_struct *fs;
+	struct task_struct *tsk = current;
+
+	exit_fs(tsk);		/* current->fs->count--; */
+	fs = init_task.fs;
+	tsk->fs = fs;
+	atomic_inc(&fs->count);
+ 	exit_files(tsk);
+	current->files = init_task.files;
+	atomic_inc(&tsk->files->count);
+}
+
+static int kthread(void *_create)
+{
+	struct kthread_create_info *create = _create;
+	int (*threadfn)(void *data);
+	void *data;
+	sigset_t blocked;
+	int ret = -EINTR;
+
+	kthread_exit_files();
+
+	/* Copy data: it's on keventd's stack */
+	threadfn = create->threadfn;
+	data = create->data;
+
+	/* Block and flush all signals (in case we're not from keventd). */
+	sigfillset(&blocked);
+	sigprocmask(SIG_BLOCK, &blocked, NULL);
+	flush_signals(current);
+
+	/* By default we can run anywhere, unlike keventd. */
+	set_cpus_allowed(current, CPU_MASK_ALL);
+
+	/* OK, tell user we're spawned, wait for stop or wakeup */
+	__set_current_state(TASK_INTERRUPTIBLE);
+	complete(&create->started);
+	schedule();
+
+	if (!kthread_should_stop())
+		ret = threadfn(data);
+
+	/* It might have exited on its own, w/o kthread_stop.  Check. */
+	if (kthread_should_stop()) {
+		kthread_stop_info.err = ret;
+		complete(&kthread_stop_info.done);
+	}
+	return 0;
+}
+
+/* We are keventd: create a thread. */
+static void keventd_create_kthread(void *_create)
+{
+	struct kthread_create_info *create = _create;
+	int pid;
+
+	/* We want our own signal handler (we take no signals by default). */
+	pid = kernel_thread(kthread, create, CLONE_FS | CLONE_FILES | SIGCHLD);
+	if (pid < 0) {
+		create->result = ERR_PTR(pid);
+	} else {
+		wait_for_completion(&create->started);
+		create->result = find_task_by_pid(pid);
+	}
+	complete(&create->done);
+}
+
+struct task_struct *kthread_create(int (*threadfn)(void *data),
+				   void *data,
+				   const char namefmt[],
+				   ...)
+{
+	struct kthread_create_info create;
+	DECLARE_WORK(work, keventd_create_kthread, &create);
+
+	create.threadfn = threadfn;
+	create.data = data;
+	init_completion(&create.started);
+	init_completion(&create.done);
+
+	/*
+	 * The workqueue needs to start up first:
+	 */
+	if (!helper_wq)
+		work.func(work.data);
+	else {
+		queue_work(helper_wq, &work);
+		wait_for_completion(&create.done);
+	}
+	if (!IS_ERR(create.result)) {
+		va_list args;
+		va_start(args, namefmt);
+		vsnprintf(create.result->comm, sizeof(create.result->comm),
+			  namefmt, args);
+		va_end(args);
+	}
+
+	return create.result;
+}
+EXPORT_SYMBOL(kthread_create);
+
+void kthread_bind(struct task_struct *k, unsigned int cpu)
+{
+	BUG_ON(k->state != TASK_INTERRUPTIBLE);
+	/* Must have done schedule() in kthread() before we set_task_cpu */
+	wait_task_inactive(k);
+	set_task_cpu(k, cpu);
+	k->cpus_allowed = cpumask_of_cpu(cpu);
+}
+EXPORT_SYMBOL(kthread_bind);
+
+int kthread_stop(struct task_struct *k)
+{
+	int ret;
+
+	down(&kthread_stop_lock);
+
+	/* It could exit after stop_info.k set, but before wake_up_process. */
+	get_task_struct(k);
+
+	/* Must init completion *before* thread sees kthread_stop_info.k */
+	init_completion(&kthread_stop_info.done);
+	wmb();
+
+	/* Now set kthread_should_stop() to true, and wake it up. */
+	kthread_stop_info.k = k;
+	wake_up_process(k);
+	put_task_struct(k);
+
+	/* Once it dies, reset stop ptr, gather result and we're done. */
+	wait_for_completion(&kthread_stop_info.done);
+	kthread_stop_info.k = NULL;
+	ret = kthread_stop_info.err;
+	up(&kthread_stop_lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(kthread_stop);
+
+static __init int helper_init(void)
+{
+	helper_wq = create_singlethread_workqueue("kthread");
+	BUG_ON(!helper_wq);
+
+	return 0;
+}
+core_initcall(helper_init);
+
