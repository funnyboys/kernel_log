commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index c230c2dd48e1..2efe1e206167 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Common SMP CPU bringup/teardown functions
  */

commit 167a88677b05d6a810f23b871cfb2b5db1808e60
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jun 7 10:53:01 2018 +0200

    smpboot: Remove cpumask from the API
    
    Now that the sole use of the whole smpboot_*cpumask() API is gone,
    remove it.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 5043e7433f4b..c230c2dd48e1 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -238,8 +238,7 @@ int smpboot_unpark_threads(unsigned int cpu)
 
 	mutex_lock(&smpboot_threads_lock);
 	list_for_each_entry(cur, &hotplug_threads, list)
-		if (cpumask_test_cpu(cpu, cur->cpumask))
-			smpboot_unpark_thread(cur, cpu);
+		smpboot_unpark_thread(cur, cpu);
 	mutex_unlock(&smpboot_threads_lock);
 	return 0;
 }
@@ -280,34 +279,26 @@ static void smpboot_destroy_threads(struct smp_hotplug_thread *ht)
 }
 
 /**
- * smpboot_register_percpu_thread_cpumask - Register a per_cpu thread related
+ * smpboot_register_percpu_thread - Register a per_cpu thread related
  * 					    to hotplug
  * @plug_thread:	Hotplug thread descriptor
- * @cpumask:		The cpumask where threads run
  *
  * Creates and starts the threads on all online cpus.
  */
-int smpboot_register_percpu_thread_cpumask(struct smp_hotplug_thread *plug_thread,
-					   const struct cpumask *cpumask)
+int smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)
 {
 	unsigned int cpu;
 	int ret = 0;
 
-	if (!alloc_cpumask_var(&plug_thread->cpumask, GFP_KERNEL))
-		return -ENOMEM;
-	cpumask_copy(plug_thread->cpumask, cpumask);
-
 	get_online_cpus();
 	mutex_lock(&smpboot_threads_lock);
 	for_each_online_cpu(cpu) {
 		ret = __smpboot_create_thread(plug_thread, cpu);
 		if (ret) {
 			smpboot_destroy_threads(plug_thread);
-			free_cpumask_var(plug_thread->cpumask);
 			goto out;
 		}
-		if (cpumask_test_cpu(cpu, cpumask))
-			smpboot_unpark_thread(plug_thread, cpu);
+		smpboot_unpark_thread(plug_thread, cpu);
 	}
 	list_add(&plug_thread->list, &hotplug_threads);
 out:
@@ -315,7 +306,7 @@ int smpboot_register_percpu_thread_cpumask(struct smp_hotplug_thread *plug_threa
 	put_online_cpus();
 	return ret;
 }
-EXPORT_SYMBOL_GPL(smpboot_register_percpu_thread_cpumask);
+EXPORT_SYMBOL_GPL(smpboot_register_percpu_thread);
 
 /**
  * smpboot_unregister_percpu_thread - Unregister a per_cpu thread related to hotplug
@@ -331,44 +322,9 @@ void smpboot_unregister_percpu_thread(struct smp_hotplug_thread *plug_thread)
 	smpboot_destroy_threads(plug_thread);
 	mutex_unlock(&smpboot_threads_lock);
 	put_online_cpus();
-	free_cpumask_var(plug_thread->cpumask);
 }
 EXPORT_SYMBOL_GPL(smpboot_unregister_percpu_thread);
 
-/**
- * smpboot_update_cpumask_percpu_thread - Adjust which per_cpu hotplug threads stay parked
- * @plug_thread:	Hotplug thread descriptor
- * @new:		Revised mask to use
- *
- * The cpumask field in the smp_hotplug_thread must not be updated directly
- * by the client, but only by calling this function.
- * This function can only be called on a registered smp_hotplug_thread.
- */
-void smpboot_update_cpumask_percpu_thread(struct smp_hotplug_thread *plug_thread,
-					  const struct cpumask *new)
-{
-	struct cpumask *old = plug_thread->cpumask;
-	static struct cpumask tmp;
-	unsigned int cpu;
-
-	lockdep_assert_cpus_held();
-	mutex_lock(&smpboot_threads_lock);
-
-	/* Park threads that were exclusively enabled on the old mask. */
-	cpumask_andnot(&tmp, old, new);
-	for_each_cpu_and(cpu, &tmp, cpu_online_mask)
-		smpboot_park_thread(plug_thread, cpu);
-
-	/* Unpark threads that are exclusively enabled on the new mask. */
-	cpumask_andnot(&tmp, new, old);
-	for_each_cpu_and(cpu, &tmp, cpu_online_mask)
-		smpboot_unpark_thread(plug_thread, cpu);
-
-	cpumask_copy(old, new);
-
-	mutex_unlock(&smpboot_threads_lock);
-}
-
 static DEFINE_PER_CPU(atomic_t, cpu_hotplug_state) = ATOMIC_INIT(CPU_POST_DEAD);
 
 /*

commit e31d6883f21c1cdfe5bc64e28411f8a92b783fde
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Oct 3 16:37:53 2017 +0200

    watchdog/core, powerpc: Lock cpus across reconfiguration
    
    Instead of dropping the cpu hotplug lock after stopping NMI watchdog and
    threads and reaquiring for restart, the code and the protection rules
    become more obvious when holding cpu hotplug lock across the full
    reconfiguration.
    
    Suggested-by: Linus Torvalds <torvalds@linuxfoundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1710022105570.2114@nanos

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index ed7507b69b48..5043e7433f4b 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -351,7 +351,7 @@ void smpboot_update_cpumask_percpu_thread(struct smp_hotplug_thread *plug_thread
 	static struct cpumask tmp;
 	unsigned int cpu;
 
-	get_online_cpus();
+	lockdep_assert_cpus_held();
 	mutex_lock(&smpboot_threads_lock);
 
 	/* Park threads that were exclusively enabled on the old mask. */
@@ -367,7 +367,6 @@ void smpboot_update_cpumask_percpu_thread(struct smp_hotplug_thread *plug_thread
 	cpumask_copy(old, new);
 
 	mutex_unlock(&smpboot_threads_lock);
-	put_online_cpus();
 }
 
 static DEFINE_PER_CPU(atomic_t, cpu_hotplug_state) = ATOMIC_INIT(CPU_POST_DEAD);

commit 0d85923c7a81719567311ba0eae8ecb2efd4c8a0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:09 2017 +0200

    smpboot/threads, watchdog/core: Avoid runtime allocation
    
    smpboot_update_cpumask_threads_percpu() allocates a temporary cpumask at
    runtime. This is suboptimal because the call site needs more code size for
    proper error handling than a statically allocated temporary mask requires
    data size.
    
    Add static temporary cpumask. The function is globaly serialized, so no
    further protection required.
    
    Remove the half baken error handling in the watchdog code and get rid of
    the export as there are no in tree modular users of that function.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194147.297288838@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 1d71c051a951..ed7507b69b48 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -344,39 +344,31 @@ EXPORT_SYMBOL_GPL(smpboot_unregister_percpu_thread);
  * by the client, but only by calling this function.
  * This function can only be called on a registered smp_hotplug_thread.
  */
-int smpboot_update_cpumask_percpu_thread(struct smp_hotplug_thread *plug_thread,
-					 const struct cpumask *new)
+void smpboot_update_cpumask_percpu_thread(struct smp_hotplug_thread *plug_thread,
+					  const struct cpumask *new)
 {
 	struct cpumask *old = plug_thread->cpumask;
-	cpumask_var_t tmp;
+	static struct cpumask tmp;
 	unsigned int cpu;
 
-	if (!alloc_cpumask_var(&tmp, GFP_KERNEL))
-		return -ENOMEM;
-
 	get_online_cpus();
 	mutex_lock(&smpboot_threads_lock);
 
 	/* Park threads that were exclusively enabled on the old mask. */
-	cpumask_andnot(tmp, old, new);
-	for_each_cpu_and(cpu, tmp, cpu_online_mask)
+	cpumask_andnot(&tmp, old, new);
+	for_each_cpu_and(cpu, &tmp, cpu_online_mask)
 		smpboot_park_thread(plug_thread, cpu);
 
 	/* Unpark threads that are exclusively enabled on the new mask. */
-	cpumask_andnot(tmp, new, old);
-	for_each_cpu_and(cpu, tmp, cpu_online_mask)
+	cpumask_andnot(&tmp, new, old);
+	for_each_cpu_and(cpu, &tmp, cpu_online_mask)
 		smpboot_unpark_thread(plug_thread, cpu);
 
 	cpumask_copy(old, new);
 
 	mutex_unlock(&smpboot_threads_lock);
 	put_online_cpus();
-
-	free_cpumask_var(tmp);
-
-	return 0;
 }
-EXPORT_SYMBOL_GPL(smpboot_update_cpumask_percpu_thread);
 
 static DEFINE_PER_CPU(atomic_t, cpu_hotplug_state) = ATOMIC_INIT(CPU_POST_DEAD);
 

commit 299300258d1bc4e997b7db340a2e06636757fe2e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task.h>
    
    We are going to split <linux/sched/task.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 4a5c6e73ecd4..1d71c051a951 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -9,6 +9,7 @@
 #include <linux/list.h>
 #include <linux/slab.h>
 #include <linux/sched.h>
+#include <linux/sched/task.h>
 #include <linux/export.h>
 #include <linux/percpu.h>
 #include <linux/kthread.h>

commit a65d40961dc70d15dec046c2ee88c556f57940b2
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Oct 11 13:55:23 2016 -0700

    kthread/smpboot: do not park in kthread_create_on_cpu()
    
    kthread_create_on_cpu() was added by the commit 2a1d446019f9a5983e
    ("kthread: Implement park/unpark facility").  It is currently used only
    when enabling new CPU.  For this purpose, the newly created kthread has to
    be parked.
    
    The CPU binding is a bit tricky.  The kthread is parked when the CPU has
    not been allowed yet.  And the CPU is bound when the kthread is unparked.
    
    The function would be useful for more per-CPU kthreads, e.g.
    bnx2fc_thread, fcoethread.  For this purpose, the newly created kthread
    should stay in the uninterruptible state.
    
    This patch moves the parking into smpboot.  It binds the thread already
    when created.  Then the function might be used universally.  Also the
    behavior is consistent with kthread_create() and kthread_create_on_node().
    
    Link: http://lkml.kernel.org/r/1470754545-17632-4-git-send-email-pmladek@suse.com
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 13bc43d1fb22..4a5c6e73ecd4 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -186,6 +186,11 @@ __smpboot_create_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
 		kfree(td);
 		return PTR_ERR(tsk);
 	}
+	/*
+	 * Park the thread so that it could start right on the CPU
+	 * when it is available.
+	 */
+	kthread_park(tsk);
 	get_task_struct(tsk);
 	*per_cpu_ptr(ht->store, cpu) = tsk;
 	if (ht->create) {

commit be6a2e4c46cc122ba9113ba569fbc50fad075fff
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Oct 4 09:55:57 2016 +0200

    Revert "sched/core: Do not use smp_processor_id() with preempt enabled in smpboot_thread_fn()"
    
    This reverts commit 4fa5cd5245b627db88c9ca08ae442373b02596b4.
    
    The original change widens a preempt-off section, to avoid a seemingly unsafe
    smp_processor_id() use.
    
    During review I overlooked two facts:
    
     - The code to calls a non-trivial function callback:
    
                                    ht->park(td->cpu);
    
       ... which might (and does occasionally) sleep, triggering the warning.
    
     - More importantly, as pointed out by Peter Zijlstra, using
       smp_processor_id() in that context is safe, if it's done from
       a kernel thread that is pinned to a single CPU - which is the
       case here.
    
    So revert to the original code that enables preemption sooner.
    
    Reported-by: kernel test robot <xiaolong.ye@intel.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Con Kolivas <kernel@kolivas.org>
    Cc: Alfred Chen <cchalpha@gmail.com>
    Link: http://lkml.kernel.org/r/20160930015102.GB20189@yexl-desktop
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index fc0d8270f69e..13bc43d1fb22 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -122,12 +122,12 @@ static int smpboot_thread_fn(void *data)
 
 		if (kthread_should_park()) {
 			__set_current_state(TASK_RUNNING);
+			preempt_enable();
 			if (ht->park && td->status == HP_THREAD_ACTIVE) {
 				BUG_ON(td->cpu != smp_processor_id());
 				ht->park(td->cpu);
 				td->status = HP_THREAD_PARKED;
 			}
-			preempt_enable();
 			kthread_parkme();
 			/* We might have been woken for stop */
 			continue;

commit 4fa5cd5245b627db88c9ca08ae442373b02596b4
Author: Con Kolivas <kernel@kolivas.org>
Date:   Tue Sep 13 16:27:05 2016 +1000

    sched/core: Do not use smp_processor_id() with preempt enabled in smpboot_thread_fn()
    
    We should not be using smp_processor_id() with preempt enabled.
    
    Bug identified and fix provided by Alfred Chen.
    
    Reported-by: Alfred Chen <cchalpha@gmail.com>
    Signed-off-by: Con Kolivas <kernel@kolivas.org>
    Cc: Alfred Chen <cchalpha@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/2042051.3vvUWIM0vs@hex
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 13bc43d1fb22..fc0d8270f69e 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -122,12 +122,12 @@ static int smpboot_thread_fn(void *data)
 
 		if (kthread_should_park()) {
 			__set_current_state(TASK_RUNNING);
-			preempt_enable();
 			if (ht->park && td->status == HP_THREAD_ACTIVE) {
 				BUG_ON(td->cpu != smp_processor_id());
 				ht->park(td->cpu);
 				td->status = HP_THREAD_PARKED;
 			}
+			preempt_enable();
 			kthread_parkme();
 			/* We might have been woken for stop */
 			continue;

commit 931ef163309ee955611f287dc65248b39a65fc9d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 26 18:43:36 2016 +0000

    cpu/hotplug: Unpark smpboot threads from the state machine
    
    Handle the smpboot threads in the state machine.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: "Srivatsa S. Bhat" <srivatsa@mit.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/20160226182341.295777684@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index d264f59bff56..13bc43d1fb22 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -226,7 +226,7 @@ static void smpboot_unpark_thread(struct smp_hotplug_thread *ht, unsigned int cp
 		kthread_unpark(tsk);
 }
 
-void smpboot_unpark_threads(unsigned int cpu)
+int smpboot_unpark_threads(unsigned int cpu)
 {
 	struct smp_hotplug_thread *cur;
 
@@ -235,6 +235,7 @@ void smpboot_unpark_threads(unsigned int cpu)
 		if (cpumask_test_cpu(cpu, cur->cpumask))
 			smpboot_unpark_thread(cur, cpu);
 	mutex_unlock(&smpboot_threads_lock);
+	return 0;
 }
 
 static void smpboot_park_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
@@ -245,7 +246,7 @@ static void smpboot_park_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
 		kthread_park(tsk);
 }
 
-void smpboot_park_threads(unsigned int cpu)
+int smpboot_park_threads(unsigned int cpu)
 {
 	struct smp_hotplug_thread *cur;
 
@@ -253,6 +254,7 @@ void smpboot_park_threads(unsigned int cpu)
 	list_for_each_entry_reverse(cur, &hotplug_threads, list)
 		smpboot_park_thread(cur, cpu);
 	mutex_unlock(&smpboot_threads_lock);
+	return 0;
 }
 
 static void smpboot_destroy_threads(struct smp_hotplug_thread *ht)

commit c00166d87e730088d919814020e96ffed129d0d1
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Oct 9 18:00:49 2015 +0200

    stop_machine: Kill smp_hotplug_thread->pre_unpark, introduce stop_machine_unpark()
    
    1. Change smpboot_unpark_thread() to check ->selfparking, just
       like smpboot_park_thread() does.
    
    2. Introduce stop_machine_unpark() which sets ->enabled and calls
       kthread_unpark().
    
    3. Change smpboot_thread_call() and cpu_stop_init() to call
       stop_machine_unpark() by hand.
    
    This way:
    
        - IMO the ->selfparking logic becomes more consistent.
    
        - We can kill the smp_hotplug_thread->pre_unpark() method.
    
        - We can easily unpark the stopper thread earlier. Say, we
          can move stop_machine_unpark() from smpboot_thread_call()
          to sched_cpu_active() as Peter suggests.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: heiko.carstens@de.ibm.com
    Link: http://lkml.kernel.org/r/20151009160049.GA10166@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index a818cbc73e14..d264f59bff56 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -222,9 +222,8 @@ static void smpboot_unpark_thread(struct smp_hotplug_thread *ht, unsigned int cp
 {
 	struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
 
-	if (ht->pre_unpark)
-		ht->pre_unpark(cpu);
-	kthread_unpark(tsk);
+	if (!ht->selfparking)
+		kthread_unpark(tsk);
 }
 
 void smpboot_unpark_threads(unsigned int cpu)

commit 230ec93909f00678401cb2d63b8b95f1dea68e40
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Sep 4 15:45:06 2015 -0700

    smpboot: allow passing the cpumask on per-cpu thread registration
    
    It makes the registration cheaper and simpler for the smpboot per-cpu
    kthread users that don't need to always update the cpumask after threads
    creation.
    
    [sfr@canb.auug.org.au: fix for allow passing the cpumask on per-cpu thread registration]
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 60aa858a6a07..a818cbc73e14 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -273,19 +273,22 @@ static void smpboot_destroy_threads(struct smp_hotplug_thread *ht)
 }
 
 /**
- * smpboot_register_percpu_thread - Register a per_cpu thread related to hotplug
+ * smpboot_register_percpu_thread_cpumask - Register a per_cpu thread related
+ * 					    to hotplug
  * @plug_thread:	Hotplug thread descriptor
+ * @cpumask:		The cpumask where threads run
  *
  * Creates and starts the threads on all online cpus.
  */
-int smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)
+int smpboot_register_percpu_thread_cpumask(struct smp_hotplug_thread *plug_thread,
+					   const struct cpumask *cpumask)
 {
 	unsigned int cpu;
 	int ret = 0;
 
 	if (!alloc_cpumask_var(&plug_thread->cpumask, GFP_KERNEL))
 		return -ENOMEM;
-	cpumask_copy(plug_thread->cpumask, cpu_possible_mask);
+	cpumask_copy(plug_thread->cpumask, cpumask);
 
 	get_online_cpus();
 	mutex_lock(&smpboot_threads_lock);
@@ -296,7 +299,8 @@ int smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)
 			free_cpumask_var(plug_thread->cpumask);
 			goto out;
 		}
-		smpboot_unpark_thread(plug_thread, cpu);
+		if (cpumask_test_cpu(cpu, cpumask))
+			smpboot_unpark_thread(plug_thread, cpu);
 	}
 	list_add(&plug_thread->list, &hotplug_threads);
 out:
@@ -304,7 +308,7 @@ int smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)
 	put_online_cpus();
 	return ret;
 }
-EXPORT_SYMBOL_GPL(smpboot_register_percpu_thread);
+EXPORT_SYMBOL_GPL(smpboot_register_percpu_thread_cpumask);
 
 /**
  * smpboot_unregister_percpu_thread - Unregister a per_cpu thread related to hotplug

commit 3dd08c0c918f9bf058572ddbf26e7d6fb5674a5c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Sep 4 15:45:03 2015 -0700

    smpboot: make cleanup to mirror setup
    
    The per-cpu kthread cleanup() callback is the mirror of the setup()
    callback.  When the per-cpu kthread is started, it first calls setup()
    to initialize the resources which are then released by cleanup() when
    the kthread exits.
    
    Now since the introduction of a per-cpu kthread cpumask, the kthreads
    excluded by the cpumask on boot may happen to be parked immediately
    after their creation without taking the setup() stage, waiting to be
    asked to unpark to do so.  Then when smpboot_unregister_percpu_thread()
    is later called, the kthread is stopped without having ever called
    setup().
    
    But this triggers a bug as the kthread unconditionally calls cleanup()
    on exit but this doesn't mirror any setup().  Thus the kernel crashes
    because we try to free resources that haven't been initialized, as in
    the watchdog case:
    
        WATCHDOG disable 0
        WATCHDOG disable 1
        WATCHDOG disable 2
        BUG: unable to handle kernel NULL pointer dereference at           (null)
        IP: hrtimer_active+0x26/0x60
        [...]
        Call Trace:
          hrtimer_try_to_cancel+0x1c/0x280
          hrtimer_cancel+0x1d/0x30
          watchdog_disable+0x56/0x70
          watchdog_cleanup+0xe/0x10
          smpboot_thread_fn+0x23c/0x2c0
          kthread+0xf8/0x110
          ret_from_fork+0x3f/0x70
    
    This bug is currently masked with explicit kthread unparking before
    kthread_stop() on smpboot_destroy_threads(). This forces a call to
    setup() and then unpark().
    
    We could fix this by unconditionally calling setup() on kthread entry.
    But setup() isn't always cheap.  In the case of watchdog it launches
    hrtimer, perf events, etc...  So we may as well like to skip it if there
    are chances the kthread will never be used, as in a reduced cpumask value.
    
    So let's simply do a state machine check before calling cleanup() that
    makes sure setup() has been called before mirroring it.
    
    And remove the nasty hack workaround.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 71aa90b69f8f..60aa858a6a07 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -113,7 +113,8 @@ static int smpboot_thread_fn(void *data)
 		if (kthread_should_stop()) {
 			__set_current_state(TASK_RUNNING);
 			preempt_enable();
-			if (ht->cleanup)
+			/* cleanup must mirror setup */
+			if (ht->cleanup && td->status != HP_THREAD_NONE)
 				ht->cleanup(td->cpu, cpu_online(td->cpu));
 			kfree(td);
 			return 0;
@@ -259,15 +260,6 @@ static void smpboot_destroy_threads(struct smp_hotplug_thread *ht)
 {
 	unsigned int cpu;
 
-	/* Unpark any threads that were voluntarily parked. */
-	for_each_cpu_not(cpu, ht->cpumask) {
-		if (cpu_online(cpu)) {
-			struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
-			if (tsk)
-				kthread_unpark(tsk);
-		}
-	}
-
 	/* We need to destroy also the parked threads of offline cpus */
 	for_each_possible_cpu(cpu) {
 		struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);

commit 5869b5064b0950afc447610c0f5d4134b71d9e94
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Sep 4 15:45:00 2015 -0700

    smpboot: fix memory leak on error handling
    
    The cpumask is allocated before threads get created. If the latter step
    fails, we need to free the cpumask.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 7c434c39f02a..71aa90b69f8f 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -301,6 +301,7 @@ int smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)
 		ret = __smpboot_create_thread(plug_thread, cpu);
 		if (ret) {
 			smpboot_destroy_threads(plug_thread);
+			free_cpumask_var(plug_thread->cpumask);
 			goto out;
 		}
 		smpboot_unpark_thread(plug_thread, cpu);

commit fe4ba3c34352b7e8068b7f18eb233444aed17011
Author: Chris Metcalf <cmetcalf@ezchip.com>
Date:   Wed Jun 24 16:55:45 2015 -0700

    watchdog: add watchdog_cpumask sysctl to assist nohz
    
    Change the default behavior of watchdog so it only runs on the
    housekeeping cores when nohz_full is enabled at build and boot time.
    Allow modifying the set of cores the watchdog is currently running on
    with a new kernel.watchdog_cpumask sysctl.
    
    In the current system, the watchdog subsystem runs a periodic timer that
    schedules the watchdog kthread to run.  However, nohz_full cores are
    designed to allow userspace application code running on those cores to
    have 100% access to the CPU.  So the watchdog system prevents the
    nohz_full application code from being able to run the way it wants to,
    thus the motivation to suppress the watchdog on nohz_full cores, which
    this patchset provides by default.
    
    However, if we disable the watchdog globally, then the housekeeping
    cores can't benefit from the watchdog functionality.  So we allow
    disabling it only on some cores.  See Documentation/lockup-watchdogs.txt
    for more information.
    
    [jhubbard@nvidia.com: fix a watchdog crash in some configurations]
    Signed-off-by: Chris Metcalf <cmetcalf@ezchip.com>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 5e46c2a75d59..7c434c39f02a 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -338,6 +338,7 @@ EXPORT_SYMBOL_GPL(smpboot_unregister_percpu_thread);
  *
  * The cpumask field in the smp_hotplug_thread must not be updated directly
  * by the client, but only by calling this function.
+ * This function can only be called on a registered smp_hotplug_thread.
  */
 int smpboot_update_cpumask_percpu_thread(struct smp_hotplug_thread *plug_thread,
 					 const struct cpumask *new)

commit b5242e98c1cb834feb1e84026f09a4796b49eb4d
Author: Chris Metcalf <cmetcalf@ezchip.com>
Date:   Wed Jun 24 16:55:42 2015 -0700

    smpboot: allow excluding cpus from the smpboot threads
    
    This patch series allows the watchdog to run by default only on the
    housekeeping cores when nohz_full is in effect; this seems to be a good
    compromise short of turning it off completely (since the nohz_full cores
    can't tolerate a watchdog).
    
    To provide customizability, we add /proc/sys/kernel/watchdog_cpumask so
    that the set of cores running the watchdog can be tuned to different
    values after bootup.
    
    To implement this customizability, we add a new
    smpboot_update_cpumask_percpu_thread() API to the smpboot_thread
    subsystem that lets us park or unpark "unwanted" threads.
    
    And now that threads can be parked for long periods of time, we tweak the
    /proc/<pid>/stat and /proc/<pid>/status code so parked threads aren't
    reported as running, which is otherwise confusing.
    
    This patch (of 3):
    
    This change allows some cores to be excluded from running the
    smp_hotplug_thread tasks.  The following commit to update
    kernel/watchdog.c to use this functionality is the motivating example, and
    more information on the motivation is provided there.
    
    A new smp_hotplug_thread field is introduced, "cpumask", which is cpumask
    field managed by the smpboot subsystem that indicates whether or not the
    given smp_hotplug_thread should run on that core; the cpumask is checked
    when deciding whether to unpark the thread.
    
    To limit the cpumask to less than cpu_possible, you must call
    smpboot_update_cpumask_percpu_thread() after registering.
    
    Signed-off-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Don Zickus <dzickus@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index c697f73d82d6..5e46c2a75d59 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -232,7 +232,8 @@ void smpboot_unpark_threads(unsigned int cpu)
 
 	mutex_lock(&smpboot_threads_lock);
 	list_for_each_entry(cur, &hotplug_threads, list)
-		smpboot_unpark_thread(cur, cpu);
+		if (cpumask_test_cpu(cpu, cur->cpumask))
+			smpboot_unpark_thread(cur, cpu);
 	mutex_unlock(&smpboot_threads_lock);
 }
 
@@ -258,6 +259,15 @@ static void smpboot_destroy_threads(struct smp_hotplug_thread *ht)
 {
 	unsigned int cpu;
 
+	/* Unpark any threads that were voluntarily parked. */
+	for_each_cpu_not(cpu, ht->cpumask) {
+		if (cpu_online(cpu)) {
+			struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
+			if (tsk)
+				kthread_unpark(tsk);
+		}
+	}
+
 	/* We need to destroy also the parked threads of offline cpus */
 	for_each_possible_cpu(cpu) {
 		struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
@@ -281,6 +291,10 @@ int smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)
 	unsigned int cpu;
 	int ret = 0;
 
+	if (!alloc_cpumask_var(&plug_thread->cpumask, GFP_KERNEL))
+		return -ENOMEM;
+	cpumask_copy(plug_thread->cpumask, cpu_possible_mask);
+
 	get_online_cpus();
 	mutex_lock(&smpboot_threads_lock);
 	for_each_online_cpu(cpu) {
@@ -313,9 +327,52 @@ void smpboot_unregister_percpu_thread(struct smp_hotplug_thread *plug_thread)
 	smpboot_destroy_threads(plug_thread);
 	mutex_unlock(&smpboot_threads_lock);
 	put_online_cpus();
+	free_cpumask_var(plug_thread->cpumask);
 }
 EXPORT_SYMBOL_GPL(smpboot_unregister_percpu_thread);
 
+/**
+ * smpboot_update_cpumask_percpu_thread - Adjust which per_cpu hotplug threads stay parked
+ * @plug_thread:	Hotplug thread descriptor
+ * @new:		Revised mask to use
+ *
+ * The cpumask field in the smp_hotplug_thread must not be updated directly
+ * by the client, but only by calling this function.
+ */
+int smpboot_update_cpumask_percpu_thread(struct smp_hotplug_thread *plug_thread,
+					 const struct cpumask *new)
+{
+	struct cpumask *old = plug_thread->cpumask;
+	cpumask_var_t tmp;
+	unsigned int cpu;
+
+	if (!alloc_cpumask_var(&tmp, GFP_KERNEL))
+		return -ENOMEM;
+
+	get_online_cpus();
+	mutex_lock(&smpboot_threads_lock);
+
+	/* Park threads that were exclusively enabled on the old mask. */
+	cpumask_andnot(tmp, old, new);
+	for_each_cpu_and(cpu, tmp, cpu_online_mask)
+		smpboot_park_thread(plug_thread, cpu);
+
+	/* Unpark threads that are exclusively enabled on the new mask. */
+	cpumask_andnot(tmp, new, old);
+	for_each_cpu_and(cpu, tmp, cpu_online_mask)
+		smpboot_unpark_thread(plug_thread, cpu);
+
+	cpumask_copy(old, new);
+
+	mutex_unlock(&smpboot_threads_lock);
+	put_online_cpus();
+
+	free_cpumask_var(tmp);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(smpboot_update_cpumask_percpu_thread);
+
 static DEFINE_PER_CPU(atomic_t, cpu_hotplug_state) = ATOMIC_INIT(CPU_POST_DEAD);
 
 /*

commit 8038dad7e888581266c76df15d70ca457a3c5910
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Feb 25 10:34:39 2015 -0800

    smpboot: Add common code for notification from dying CPU
    
    RCU ignores offlined CPUs, so they cannot safely run RCU read-side code.
    (They -can- use SRCU, but not RCU.)  This means that any use of RCU
    during or after the call to arch_cpu_idle_dead().  Unfortunately,
    commit 2ed53c0d6cc99 added a complete() call, which will contain RCU
    read-side critical sections if there is a task waiting to be awakened.
    
    Which, as it turns out, there almost never is.  In my qemu/KVM testing,
    the to-be-awakened task is not yet asleep more than 99.5% of the time.
    In current mainline, failure is even harder to reproduce, requiring a
    virtualized environment that delays the outgoing CPU by at least three
    jiffies between the time it exits its stop_machine() task at CPU_DYING
    time and the time it calls arch_cpu_idle_dead() from the idle loop.
    However, this problem really can occur, especially in virtualized
    environments, and therefore really does need to be fixed
    
    This suggests moving back to the polling loop, but using a much shorter
    wait, with gentle exponential backoff instead of the old 100-millisecond
    wait.  Most of the time, the loop will exit without waiting at all,
    and almost all of the remaining uses will wait only five microseconds.
    If the outgoing CPU is preempted, a loop will wait one jiffy, then
    increase the wait by a factor of 11/10ths, rounding up.  As before, there
    is a five-second timeout.
    
    This commit therefore provides common-code infrastructure to do the
    dying-to-surviving CPU handoff in a safe manner.  This code also
    provides an indication at CPU-online of whether the CPU to be onlined
    previously timed out on offline.  The new cpu_check_up_prepare() function
    returns -EBUSY if this CPU previously took more than five seconds to
    go offline, or -EAGAIN if it has not yet managed to go offline.  The
    rationale for -EAGAIN is that it might still be preempted, so an additional
    wait might well find it correctly offlined.  Architecture-specific code
    can decide how to handle these conditions.  Systems in which CPUs take
    themselves completely offline might respond to an -EBUSY return as if
    it was a zero (success) return.  Systems in which the surviving CPU must
    take some action might take it at this time, or might simply mark the
    other CPU as unusable.
    
    Note that architectures that take the easy way out and simply pass the
    -EBUSY and -EAGAIN upwards will change the sysfs API.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: <linux-api@vger.kernel.org>
    Cc: <linux-arch@vger.kernel.org>
    [ paulmck: Fixed state machine for architectures that don't check earlier
      CPU-hotplug results as suggested by James Hogan. ]

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 40190f28db35..c697f73d82d6 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -4,6 +4,7 @@
 #include <linux/cpu.h>
 #include <linux/err.h>
 #include <linux/smp.h>
+#include <linux/delay.h>
 #include <linux/init.h>
 #include <linux/list.h>
 #include <linux/slab.h>
@@ -314,3 +315,158 @@ void smpboot_unregister_percpu_thread(struct smp_hotplug_thread *plug_thread)
 	put_online_cpus();
 }
 EXPORT_SYMBOL_GPL(smpboot_unregister_percpu_thread);
+
+static DEFINE_PER_CPU(atomic_t, cpu_hotplug_state) = ATOMIC_INIT(CPU_POST_DEAD);
+
+/*
+ * Called to poll specified CPU's state, for example, when waiting for
+ * a CPU to come online.
+ */
+int cpu_report_state(int cpu)
+{
+	return atomic_read(&per_cpu(cpu_hotplug_state, cpu));
+}
+
+/*
+ * If CPU has died properly, set its state to CPU_UP_PREPARE and
+ * return success.  Otherwise, return -EBUSY if the CPU died after
+ * cpu_wait_death() timed out.  And yet otherwise again, return -EAGAIN
+ * if cpu_wait_death() timed out and the CPU still hasn't gotten around
+ * to dying.  In the latter two cases, the CPU might not be set up
+ * properly, but it is up to the arch-specific code to decide.
+ * Finally, -EIO indicates an unanticipated problem.
+ *
+ * Note that it is permissible to omit this call entirely, as is
+ * done in architectures that do no CPU-hotplug error checking.
+ */
+int cpu_check_up_prepare(int cpu)
+{
+	if (!IS_ENABLED(CONFIG_HOTPLUG_CPU)) {
+		atomic_set(&per_cpu(cpu_hotplug_state, cpu), CPU_UP_PREPARE);
+		return 0;
+	}
+
+	switch (atomic_read(&per_cpu(cpu_hotplug_state, cpu))) {
+
+	case CPU_POST_DEAD:
+
+		/* The CPU died properly, so just start it up again. */
+		atomic_set(&per_cpu(cpu_hotplug_state, cpu), CPU_UP_PREPARE);
+		return 0;
+
+	case CPU_DEAD_FROZEN:
+
+		/*
+		 * Timeout during CPU death, so let caller know.
+		 * The outgoing CPU completed its processing, but after
+		 * cpu_wait_death() timed out and reported the error. The
+		 * caller is free to proceed, in which case the state
+		 * will be reset properly by cpu_set_state_online().
+		 * Proceeding despite this -EBUSY return makes sense
+		 * for systems where the outgoing CPUs take themselves
+		 * offline, with no post-death manipulation required from
+		 * a surviving CPU.
+		 */
+		return -EBUSY;
+
+	case CPU_BROKEN:
+
+		/*
+		 * The most likely reason we got here is that there was
+		 * a timeout during CPU death, and the outgoing CPU never
+		 * did complete its processing.  This could happen on
+		 * a virtualized system if the outgoing VCPU gets preempted
+		 * for more than five seconds, and the user attempts to
+		 * immediately online that same CPU.  Trying again later
+		 * might return -EBUSY above, hence -EAGAIN.
+		 */
+		return -EAGAIN;
+
+	default:
+
+		/* Should not happen.  Famous last words. */
+		return -EIO;
+	}
+}
+
+/*
+ * Mark the specified CPU online.
+ *
+ * Note that it is permissible to omit this call entirely, as is
+ * done in architectures that do no CPU-hotplug error checking.
+ */
+void cpu_set_state_online(int cpu)
+{
+	(void)atomic_xchg(&per_cpu(cpu_hotplug_state, cpu), CPU_ONLINE);
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+
+/*
+ * Wait for the specified CPU to exit the idle loop and die.
+ */
+bool cpu_wait_death(unsigned int cpu, int seconds)
+{
+	int jf_left = seconds * HZ;
+	int oldstate;
+	bool ret = true;
+	int sleep_jf = 1;
+
+	might_sleep();
+
+	/* The outgoing CPU will normally get done quite quickly. */
+	if (atomic_read(&per_cpu(cpu_hotplug_state, cpu)) == CPU_DEAD)
+		goto update_state;
+	udelay(5);
+
+	/* But if the outgoing CPU dawdles, wait increasingly long times. */
+	while (atomic_read(&per_cpu(cpu_hotplug_state, cpu)) != CPU_DEAD) {
+		schedule_timeout_uninterruptible(sleep_jf);
+		jf_left -= sleep_jf;
+		if (jf_left <= 0)
+			break;
+		sleep_jf = DIV_ROUND_UP(sleep_jf * 11, 10);
+	}
+update_state:
+	oldstate = atomic_read(&per_cpu(cpu_hotplug_state, cpu));
+	if (oldstate == CPU_DEAD) {
+		/* Outgoing CPU died normally, update state. */
+		smp_mb(); /* atomic_read() before update. */
+		atomic_set(&per_cpu(cpu_hotplug_state, cpu), CPU_POST_DEAD);
+	} else {
+		/* Outgoing CPU still hasn't died, set state accordingly. */
+		if (atomic_cmpxchg(&per_cpu(cpu_hotplug_state, cpu),
+				   oldstate, CPU_BROKEN) != oldstate)
+			goto update_state;
+		ret = false;
+	}
+	return ret;
+}
+
+/*
+ * Called by the outgoing CPU to report its successful death.  Return
+ * false if this report follows the surviving CPU's timing out.
+ *
+ * A separate "CPU_DEAD_FROZEN" is used when the surviving CPU
+ * timed out.  This approach allows architectures to omit calls to
+ * cpu_check_up_prepare() and cpu_set_state_online() without defeating
+ * the next cpu_wait_death()'s polling loop.
+ */
+bool cpu_report_death(void)
+{
+	int oldstate;
+	int newstate;
+	int cpu = smp_processor_id();
+
+	do {
+		oldstate = atomic_read(&per_cpu(cpu_hotplug_state, cpu));
+		if (oldstate != CPU_BROKEN)
+			newstate = CPU_DEAD;
+		else
+			newstate = CPU_DEAD_FROZEN;
+	} while (atomic_cmpxchg(&per_cpu(cpu_hotplug_state, cpu),
+				oldstate, newstate) != oldstate);
+	return newstate == CPU_DEAD;
+}
+
+#endif /* #ifdef CONFIG_HOTPLUG_CPU */

commit 4bee96860a65c3a62d332edac331b3cf936ba3ad
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Thu Jul 31 11:30:17 2014 +0800

    smpboot: Add missing get_online_cpus() in smpboot_register_percpu_thread()
    
    The following race exists in the smpboot percpu threads management:
    
    CPU0                         CPU1
    cpu_up(2)
      get_online_cpus();
      smpboot_create_threads(2);
                                 smpboot_register_percpu_thread();
                                 for_each_online_cpu();
                                   __smpboot_create_thread();
      __cpu_up(2);
    
    This results in a missing per cpu thread for the newly onlined cpu2 and
    in a NULL pointer dereference on a consecutive offline of that cpu.
    
    Proctect smpboot_register_percpu_thread() with get_online_cpus() to
    prevent that.
    
    [ tglx: Massaged changelog and removed the change in
            smpboot_unregister_percpu_thread() because that's an
            optimization and therefor not stable material. ]
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/1406777421-12830-1-git-send-email-laijs@cn.fujitsu.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index f032fb5284e3..40190f28db35 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -280,6 +280,7 @@ int smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)
 	unsigned int cpu;
 	int ret = 0;
 
+	get_online_cpus();
 	mutex_lock(&smpboot_threads_lock);
 	for_each_online_cpu(cpu) {
 		ret = __smpboot_create_thread(plug_thread, cpu);
@@ -292,6 +293,7 @@ int smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)
 	list_add(&plug_thread->list, &hotplug_threads);
 out:
 	mutex_unlock(&smpboot_threads_lock);
+	put_online_cpus();
 	return ret;
 }
 EXPORT_SYMBOL_GPL(smpboot_register_percpu_thread);

commit 7d4d26966e0b6443c78123a8a8b602e8eaf67694
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 24 10:18:52 2014 +0200

    sched, smp: Correctly deal with nested sleeps
    
    smp_hotplug_thread::{setup,unpark} functions can sleep too, so be
    consistent and do the same for all callbacks.
    
     __might_sleep+0x74/0x80
     kmem_cache_alloc_trace+0x4e/0x1c0
     perf_event_alloc+0x55/0x450
     perf_event_create_kernel_counter+0x2f/0x100
     watchdog_nmi_enable+0x8d/0x160
     watchdog_enable+0x45/0x90
     smpboot_thread_fn+0xec/0x2b0
     kthread+0xe4/0x100
     ret_from_fork+0x7c/0xb0
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: tglx@linutronix.de
    Cc: ilya.dryomov@inktank.com
    Cc: umgwanakikbuti@gmail.com
    Cc: oleg@redhat.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20140924082242.392279328@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index eb89e1807408..f032fb5284e3 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -110,7 +110,7 @@ static int smpboot_thread_fn(void *data)
 		set_current_state(TASK_INTERRUPTIBLE);
 		preempt_disable();
 		if (kthread_should_stop()) {
-			set_current_state(TASK_RUNNING);
+			__set_current_state(TASK_RUNNING);
 			preempt_enable();
 			if (ht->cleanup)
 				ht->cleanup(td->cpu, cpu_online(td->cpu));
@@ -136,26 +136,27 @@ static int smpboot_thread_fn(void *data)
 		/* Check for state change setup */
 		switch (td->status) {
 		case HP_THREAD_NONE:
+			__set_current_state(TASK_RUNNING);
 			preempt_enable();
 			if (ht->setup)
 				ht->setup(td->cpu);
 			td->status = HP_THREAD_ACTIVE;
-			preempt_disable();
-			break;
+			continue;
+
 		case HP_THREAD_PARKED:
+			__set_current_state(TASK_RUNNING);
 			preempt_enable();
 			if (ht->unpark)
 				ht->unpark(td->cpu);
 			td->status = HP_THREAD_ACTIVE;
-			preempt_disable();
-			break;
+			continue;
 		}
 
 		if (!ht->thread_should_run(td->cpu)) {
-			preempt_enable();
+			preempt_enable_no_resched();
 			schedule();
 		} else {
-			set_current_state(TASK_RUNNING);
+			__set_current_state(TASK_RUNNING);
 			preempt_enable();
 			ht->thread_fn(td->cpu);
 		}

commit 0db0628d90125193280eabb501c94feaf48fa9ab
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jun 19 14:53:51 2013 -0400

    kernel: delete __cpuinit usage from all core kernel files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    This removes all the uses of the __cpuinit macros from C files in
    the core kernel directories (kernel, init, lib, mm, and include)
    that don't really have a specific maintainer.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 02fc5c933673..eb89e1807408 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -24,7 +24,7 @@
  */
 static DEFINE_PER_CPU(struct task_struct *, idle_threads);
 
-struct task_struct * __cpuinit idle_thread_get(unsigned int cpu)
+struct task_struct *idle_thread_get(unsigned int cpu)
 {
 	struct task_struct *tsk = per_cpu(idle_threads, cpu);
 

commit f2530dc71cf0822f90bb63ea4600caaef33a66bb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 9 09:33:34 2013 +0200

    kthread: Prevent unpark race which puts threads on the wrong cpu
    
    The smpboot threads rely on the park/unpark mechanism which binds per
    cpu threads on a particular core. Though the functionality is racy:
    
    CPU0                    CPU1                CPU2
    unpark(T)                                   wake_up_process(T)
      clear(SHOULD_PARK)    T runs
                            leave parkme() due to !SHOULD_PARK
      bind_to(CPU2)         BUG_ON(wrong CPU)
    
    We cannot let the tasks move themself to the target CPU as one of
    those tasks is actually the migration thread itself, which requires
    that it starts running on the target cpu right away.
    
    The solution to this problem is to prevent wakeups in park mode which
    are not from unpark(). That way we can guarantee that the association
    of the task to the target cpu is working correctly.
    
    Add a new task state (TASK_PARKED) which prevents other wakeups and
    use this state explicitly for the unpark wakeup.
    
    Peter noticed: Also, since the task state is visible to userspace and
    all the parked tasks are still in the PID space, its a good hint in ps
    and friends that these tasks aren't really there for the moment.
    
    The migration thread has another related issue.
    
    CPU0                     CPU1
    Bring up CPU2
    create_thread(T)
    park(T)
     wait_for_completion()
                             parkme()
                             complete()
    sched_set_stop_task()
                             schedule(TASK_PARKED)
    
    The sched_set_stop_task() call is issued while the task is on the
    runqueue of CPU1 and that confuses the hell out of the stop_task class
    on that cpu. So we need the same synchronizaion before
    sched_set_stop_task().
    
    Reported-by: Dave Jones <davej@redhat.com>
    Reported-and-tested-by: Dave Hansen <dave@sr71.net>
    Reported-and-tested-by: Borislav Petkov <bp@alien8.de>
    Acked-by: Peter Ziljstra <peterz@infradead.org>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: dhillf@gmail.com
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1304091635430.21884@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 8eaed9aa9cf0..02fc5c933673 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -185,8 +185,18 @@ __smpboot_create_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
 	}
 	get_task_struct(tsk);
 	*per_cpu_ptr(ht->store, cpu) = tsk;
-	if (ht->create)
-		ht->create(cpu);
+	if (ht->create) {
+		/*
+		 * Make sure that the task has actually scheduled out
+		 * into park position, before calling the create
+		 * callback. At least the migration thread callback
+		 * requires that the task is off the runqueue.
+		 */
+		if (!wait_task_inactive(tsk, TASK_PARKED))
+			WARN_ON(1);
+		else
+			ht->create(cpu);
+	}
 	return 0;
 }
 

commit dc893e19b5800d7743fb58235877bfa9091805ff
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Mar 8 12:43:31 2013 -0800

    Revert parts of "hlist: drop the node parameter from iterators"
    
    Commit b67bfe0d42ca ("hlist: drop the node parameter from iterators")
    did a lot of nice changes but also contains two small hunks that seem to
    have slipped in accidentally and have no apparent connection to the
    intent of the patch.
    
    This reverts the two extraneous changes.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Peter Senna Tschudin <peter.senna@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 25d3d8b6e4e1..8eaed9aa9cf0 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -131,7 +131,7 @@ static int smpboot_thread_fn(void *data)
 			continue;
 		}
 
-		//BUG_ON(td->cpu != smp_processor_id());
+		BUG_ON(td->cpu != smp_processor_id());
 
 		/* Check for state change setup */
 		switch (td->status) {

commit 6516ab6fdffbda656253d4e1231660c3f87f7889
Merge: 06e79d3b45df 46c498c2cdee
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 5 18:07:12 2013 -0800

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull smpboot bugfix from Thomas Gleixner:
     "A single bugfix for a regression introduced with the conversion of the
      stop machine threads to the generic smpboot thread management
      facility"
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      stop_machine: Mark per cpu stopper enabled early

commit b67bfe0d42cac56c512dd5da4b1b347a23f4b70a
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Wed Feb 27 17:06:00 2013 -0800

    hlist: drop the node parameter from iterators
    
    I'm not sure why, but the hlist for each entry iterators were conceived
    
            list_for_each_entry(pos, head, member)
    
    The hlist ones were greedy and wanted an extra parameter:
    
            hlist_for_each_entry(tpos, pos, head, member)
    
    Why did they need an extra pos parameter? I'm not quite sure. Not only
    they don't really need it, it also prevents the iterator from looking
    exactly like the list iterator, which is unfortunate.
    
    Besides the semantic patch, there was some manual work required:
    
     - Fix up the actual hlist iterators in linux/list.h
     - Fix up the declaration of other iterators based on the hlist ones.
     - A very small amount of places were using the 'node' parameter, this
     was modified to use 'obj->member' instead.
     - Coccinelle didn't handle the hlist_for_each_entry_safe iterator
     properly, so those had to be fixed up manually.
    
    The semantic patch which is mostly the work of Peter Senna Tschudin is here:
    
    @@
    iterator name hlist_for_each_entry, hlist_for_each_entry_continue, hlist_for_each_entry_from, hlist_for_each_entry_rcu, hlist_for_each_entry_rcu_bh, hlist_for_each_entry_continue_rcu_bh, for_each_busy_worker, ax25_uid_for_each, ax25_for_each, inet_bind_bucket_for_each, sctp_for_each_hentry, sk_for_each, sk_for_each_rcu, sk_for_each_from, sk_for_each_safe, sk_for_each_bound, hlist_for_each_entry_safe, hlist_for_each_entry_continue_rcu, nr_neigh_for_each, nr_neigh_for_each_safe, nr_node_for_each, nr_node_for_each_safe, for_each_gfn_indirect_valid_sp, for_each_gfn_sp, for_each_host;
    
    type T;
    expression a,c,d,e;
    identifier b;
    statement S;
    @@
    
    -T b;
        <+... when != b
    (
    hlist_for_each_entry(a,
    - b,
    c, d) S
    |
    hlist_for_each_entry_continue(a,
    - b,
    c) S
    |
    hlist_for_each_entry_from(a,
    - b,
    c) S
    |
    hlist_for_each_entry_rcu(a,
    - b,
    c, d) S
    |
    hlist_for_each_entry_rcu_bh(a,
    - b,
    c, d) S
    |
    hlist_for_each_entry_continue_rcu_bh(a,
    - b,
    c) S
    |
    for_each_busy_worker(a, c,
    - b,
    d) S
    |
    ax25_uid_for_each(a,
    - b,
    c) S
    |
    ax25_for_each(a,
    - b,
    c) S
    |
    inet_bind_bucket_for_each(a,
    - b,
    c) S
    |
    sctp_for_each_hentry(a,
    - b,
    c) S
    |
    sk_for_each(a,
    - b,
    c) S
    |
    sk_for_each_rcu(a,
    - b,
    c) S
    |
    sk_for_each_from
    -(a, b)
    +(a)
    S
    + sk_for_each_from(a) S
    |
    sk_for_each_safe(a,
    - b,
    c, d) S
    |
    sk_for_each_bound(a,
    - b,
    c) S
    |
    hlist_for_each_entry_safe(a,
    - b,
    c, d, e) S
    |
    hlist_for_each_entry_continue_rcu(a,
    - b,
    c) S
    |
    nr_neigh_for_each(a,
    - b,
    c) S
    |
    nr_neigh_for_each_safe(a,
    - b,
    c, d) S
    |
    nr_node_for_each(a,
    - b,
    c) S
    |
    nr_node_for_each_safe(a,
    - b,
    c, d) S
    |
    - for_each_gfn_sp(a, c, d, b) S
    + for_each_gfn_sp(a, c, d) S
    |
    - for_each_gfn_indirect_valid_sp(a, c, d, b) S
    + for_each_gfn_indirect_valid_sp(a, c, d) S
    |
    for_each_host(a,
    - b,
    c) S
    |
    for_each_host_safe(a,
    - b,
    c, d) S
    |
    for_each_mesh_entry(a,
    - b,
    c, d) S
    )
        ...+>
    
    [akpm@linux-foundation.org: drop bogus change from net/ipv4/raw.c]
    [akpm@linux-foundation.org: drop bogus hunk from net/ipv6/raw.c]
    [akpm@linux-foundation.org: checkpatch fixes]
    [akpm@linux-foundation.org: fix warnings]
    [akpm@linux-foudnation.org: redo intrusive kvm changes]
    Tested-by: Peter Senna Tschudin <peter.senna@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index d4abac261779..b9bde5727829 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -131,7 +131,7 @@ static int smpboot_thread_fn(void *data)
 			continue;
 		}
 
-		BUG_ON(td->cpu != smp_processor_id());
+		//BUG_ON(td->cpu != smp_processor_id());
 
 		/* Check for state change setup */
 		switch (td->status) {

commit 46c498c2cdee5efe44f617bcd4f388179be36115
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 26 18:44:33 2013 +0100

    stop_machine: Mark per cpu stopper enabled early
    
    commit 14e568e78 (stop_machine: Use smpboot threads) introduced the
    following regression:
    
    Before this commit the stopper enabled bit was set in the online
    notifier.
    
    CPU0                            CPU1
    cpu_up
                                    cpu online
    hotplug_notifier(ONLINE)
      stopper(CPU1)->enabled = true;
    ...
    stop_machine()
    
    The conversion to smpboot threads moved the enablement to the wakeup
    path of the parked thread. The majority of users seem to have the
    following working order:
    
    CPU0                            CPU1
    cpu_up
                                    cpu online
    unpark_threads()
      wakeup(stopper[CPU1])
    ....
                                    stopper thread runs
                                      stopper(CPU1)->enabled = true;
    stop_machine()
    
    But Konrad and Sander have observed:
    
    CPU0                            CPU1
    cpu_up
                                    cpu online
    unpark_threads()
      wakeup(stopper[CPU1])
    ....
    stop_machine()
                                    stopper thread runs
                                      stopper(CPU1)->enabled = true;
    
    Now the stop machinery kicks CPU0 into the stop loop, where it gets
    stuck forever because the queue code saw stopper(CPU1)->enabled ==
    false, so CPU0 waits for CPU1 to enter stomp_machine, but the CPU1
    stopper work got discarded due to enabled == false.
    
    Add a pre_unpark function to the smpboot thread descriptor and call it
    before waking the thread.
    
    This fixes the problem at hand, but the stop_machine code should be
    more robust. The stopper->enabled flag smells fishy at best.
    
    Thanks to Konrad for going through a loop of debug patches and
    providing the information to decode this issue.
    
    Reported-and-tested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reported-and-tested-by: Sander Eikelenboom <linux@eikelenboom.it>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1302261843240.22263@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index d4abac261779..8eaed9aa9cf0 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -209,6 +209,8 @@ static void smpboot_unpark_thread(struct smp_hotplug_thread *ht, unsigned int cp
 {
 	struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
 
+	if (ht->pre_unpark)
+		ht->pre_unpark(cpu);
 	kthread_unpark(tsk);
 }
 

commit 7d7e499f7333f68b7e7f67d14b9c1480913b4afb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jan 31 12:11:12 2013 +0000

    smpboot: Allow selfparking per cpu threads
    
    The stop machine threads are still killed when a cpu goes offline. The
    reason is that the thread is used to bring the cpu down, so it can't
    be parked along with the other per cpu threads.
    
    Allow a per cpu thread to be excluded from automatic parking, so it
    can park itself once it's done
    
    Add a create callback function as well.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Arjan van de Veen <arjan@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Richard Weinberger <rw@linutronix.de>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130131120741.553993267@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index d6c5fc054242..d4abac261779 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -183,9 +183,10 @@ __smpboot_create_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
 		kfree(td);
 		return PTR_ERR(tsk);
 	}
-
 	get_task_struct(tsk);
 	*per_cpu_ptr(ht->store, cpu) = tsk;
+	if (ht->create)
+		ht->create(cpu);
 	return 0;
 }
 
@@ -225,7 +226,7 @@ static void smpboot_park_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
 {
 	struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
 
-	if (tsk)
+	if (tsk && !ht->selfparking)
 		kthread_park(tsk);
 }
 

commit 3180d89b47701072cf129f800a735baf3acdbb8a
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Jul 12 01:55:54 2012 -0700

    hotplug: Fix UP bug in smpboot hotplug code
    
    Because kernel subsystems need their per-CPU kthreads on UP systems as
    well as on SMP systems, the smpboot hotplug kthread functions must be
    provided in UP builds as well as in SMP builds.  This commit therefore
    adds smpboot.c to UP builds and excludes irrelevant code via #ifdef.
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 9d5f7b04025d..d6c5fc054242 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -15,6 +15,8 @@
 
 #include "smpboot.h"
 
+#ifdef CONFIG_SMP
+
 #ifdef CONFIG_GENERIC_SMP_IDLE_THREAD
 /*
  * For the hotplug case we keep the task structs around and reuse
@@ -72,6 +74,8 @@ void __init idle_threads_init(void)
 }
 #endif
 
+#endif /* #ifdef CONFIG_SMP */
+
 static LIST_HEAD(hotplug_threads);
 static DEFINE_MUTEX(smpboot_threads_lock);
 

commit f97f8f06a49febbc3cb3635172efbe64ddc79700
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 16 10:42:36 2012 +0000

    smpboot: Provide infrastructure for percpu hotplug threads
    
    Provide a generic interface for setting up and tearing down percpu
    threads.
    
    On registration the threads for already online cpus are created and
    started. On deregistration (modules) the threads are stoppped.
    
    During hotplug operations the threads are created, started, parked and
    unparked. The datastructure for registration provides a pointer to
    percpu storage space and optional setup, cleanup, park, unpark
    functions. These functions are called when the thread state changes.
    
    Each implementation has to provide a function which is queried and
    returns whether the thread should run and the thread function itself.
    
    The core code handles all state transitions and avoids duplicated code
    in the call sites.
    
    [ paulmck: Preemption leak fix ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20120716103948.352501068@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 98f60c5caa1b..9d5f7b04025d 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -1,11 +1,17 @@
 /*
  * Common SMP CPU bringup/teardown functions
  */
+#include <linux/cpu.h>
 #include <linux/err.h>
 #include <linux/smp.h>
 #include <linux/init.h>
+#include <linux/list.h>
+#include <linux/slab.h>
 #include <linux/sched.h>
+#include <linux/export.h>
 #include <linux/percpu.h>
+#include <linux/kthread.h>
+#include <linux/smpboot.h>
 
 #include "smpboot.h"
 
@@ -65,3 +71,226 @@ void __init idle_threads_init(void)
 	}
 }
 #endif
+
+static LIST_HEAD(hotplug_threads);
+static DEFINE_MUTEX(smpboot_threads_lock);
+
+struct smpboot_thread_data {
+	unsigned int			cpu;
+	unsigned int			status;
+	struct smp_hotplug_thread	*ht;
+};
+
+enum {
+	HP_THREAD_NONE = 0,
+	HP_THREAD_ACTIVE,
+	HP_THREAD_PARKED,
+};
+
+/**
+ * smpboot_thread_fn - percpu hotplug thread loop function
+ * @data:	thread data pointer
+ *
+ * Checks for thread stop and park conditions. Calls the necessary
+ * setup, cleanup, park and unpark functions for the registered
+ * thread.
+ *
+ * Returns 1 when the thread should exit, 0 otherwise.
+ */
+static int smpboot_thread_fn(void *data)
+{
+	struct smpboot_thread_data *td = data;
+	struct smp_hotplug_thread *ht = td->ht;
+
+	while (1) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		preempt_disable();
+		if (kthread_should_stop()) {
+			set_current_state(TASK_RUNNING);
+			preempt_enable();
+			if (ht->cleanup)
+				ht->cleanup(td->cpu, cpu_online(td->cpu));
+			kfree(td);
+			return 0;
+		}
+
+		if (kthread_should_park()) {
+			__set_current_state(TASK_RUNNING);
+			preempt_enable();
+			if (ht->park && td->status == HP_THREAD_ACTIVE) {
+				BUG_ON(td->cpu != smp_processor_id());
+				ht->park(td->cpu);
+				td->status = HP_THREAD_PARKED;
+			}
+			kthread_parkme();
+			/* We might have been woken for stop */
+			continue;
+		}
+
+		BUG_ON(td->cpu != smp_processor_id());
+
+		/* Check for state change setup */
+		switch (td->status) {
+		case HP_THREAD_NONE:
+			preempt_enable();
+			if (ht->setup)
+				ht->setup(td->cpu);
+			td->status = HP_THREAD_ACTIVE;
+			preempt_disable();
+			break;
+		case HP_THREAD_PARKED:
+			preempt_enable();
+			if (ht->unpark)
+				ht->unpark(td->cpu);
+			td->status = HP_THREAD_ACTIVE;
+			preempt_disable();
+			break;
+		}
+
+		if (!ht->thread_should_run(td->cpu)) {
+			preempt_enable();
+			schedule();
+		} else {
+			set_current_state(TASK_RUNNING);
+			preempt_enable();
+			ht->thread_fn(td->cpu);
+		}
+	}
+}
+
+static int
+__smpboot_create_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
+{
+	struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
+	struct smpboot_thread_data *td;
+
+	if (tsk)
+		return 0;
+
+	td = kzalloc_node(sizeof(*td), GFP_KERNEL, cpu_to_node(cpu));
+	if (!td)
+		return -ENOMEM;
+	td->cpu = cpu;
+	td->ht = ht;
+
+	tsk = kthread_create_on_cpu(smpboot_thread_fn, td, cpu,
+				    ht->thread_comm);
+	if (IS_ERR(tsk)) {
+		kfree(td);
+		return PTR_ERR(tsk);
+	}
+
+	get_task_struct(tsk);
+	*per_cpu_ptr(ht->store, cpu) = tsk;
+	return 0;
+}
+
+int smpboot_create_threads(unsigned int cpu)
+{
+	struct smp_hotplug_thread *cur;
+	int ret = 0;
+
+	mutex_lock(&smpboot_threads_lock);
+	list_for_each_entry(cur, &hotplug_threads, list) {
+		ret = __smpboot_create_thread(cur, cpu);
+		if (ret)
+			break;
+	}
+	mutex_unlock(&smpboot_threads_lock);
+	return ret;
+}
+
+static void smpboot_unpark_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
+{
+	struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
+
+	kthread_unpark(tsk);
+}
+
+void smpboot_unpark_threads(unsigned int cpu)
+{
+	struct smp_hotplug_thread *cur;
+
+	mutex_lock(&smpboot_threads_lock);
+	list_for_each_entry(cur, &hotplug_threads, list)
+		smpboot_unpark_thread(cur, cpu);
+	mutex_unlock(&smpboot_threads_lock);
+}
+
+static void smpboot_park_thread(struct smp_hotplug_thread *ht, unsigned int cpu)
+{
+	struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
+
+	if (tsk)
+		kthread_park(tsk);
+}
+
+void smpboot_park_threads(unsigned int cpu)
+{
+	struct smp_hotplug_thread *cur;
+
+	mutex_lock(&smpboot_threads_lock);
+	list_for_each_entry_reverse(cur, &hotplug_threads, list)
+		smpboot_park_thread(cur, cpu);
+	mutex_unlock(&smpboot_threads_lock);
+}
+
+static void smpboot_destroy_threads(struct smp_hotplug_thread *ht)
+{
+	unsigned int cpu;
+
+	/* We need to destroy also the parked threads of offline cpus */
+	for_each_possible_cpu(cpu) {
+		struct task_struct *tsk = *per_cpu_ptr(ht->store, cpu);
+
+		if (tsk) {
+			kthread_stop(tsk);
+			put_task_struct(tsk);
+			*per_cpu_ptr(ht->store, cpu) = NULL;
+		}
+	}
+}
+
+/**
+ * smpboot_register_percpu_thread - Register a per_cpu thread related to hotplug
+ * @plug_thread:	Hotplug thread descriptor
+ *
+ * Creates and starts the threads on all online cpus.
+ */
+int smpboot_register_percpu_thread(struct smp_hotplug_thread *plug_thread)
+{
+	unsigned int cpu;
+	int ret = 0;
+
+	mutex_lock(&smpboot_threads_lock);
+	for_each_online_cpu(cpu) {
+		ret = __smpboot_create_thread(plug_thread, cpu);
+		if (ret) {
+			smpboot_destroy_threads(plug_thread);
+			goto out;
+		}
+		smpboot_unpark_thread(plug_thread, cpu);
+	}
+	list_add(&plug_thread->list, &hotplug_threads);
+out:
+	mutex_unlock(&smpboot_threads_lock);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(smpboot_register_percpu_thread);
+
+/**
+ * smpboot_unregister_percpu_thread - Unregister a per_cpu thread related to hotplug
+ * @plug_thread:	Hotplug thread descriptor
+ *
+ * Stops all threads on all possible cpus.
+ */
+void smpboot_unregister_percpu_thread(struct smp_hotplug_thread *plug_thread)
+{
+	get_online_cpus();
+	mutex_lock(&smpboot_threads_lock);
+	list_del(&plug_thread->list);
+	smpboot_destroy_threads(plug_thread);
+	mutex_unlock(&smpboot_threads_lock);
+	put_online_cpus();
+}
+EXPORT_SYMBOL_GPL(smpboot_unregister_percpu_thread);

commit 4a70d2d9909b43ed88043b98cabe2c7fbd563021
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 20:41:00 2012 +0530

    smpboot, idle: Fix comment mismatch over idle_threads_init()
    
    The comment over idle_threads_init() really talks about the functionality
    of idle_init(). Move that comment to idle_init(), and add a suitable
    comment over idle_threads_init().
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: suresh.b.siddha@intel.com
    Cc: venki@google.com
    Cc: nikunj@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/20120524151100.2549.66501.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 0f2162f808a7..98f60c5caa1b 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -31,6 +31,12 @@ void __init idle_thread_set_boot_cpu(void)
 	per_cpu(idle_threads, smp_processor_id()) = current;
 }
 
+/**
+ * idle_init - Initialize the idle thread for a cpu
+ * @cpu:	The cpu for which the idle thread should be initialized
+ *
+ * Creates the thread if it does not exist.
+ */
 static inline void idle_init(unsigned int cpu)
 {
 	struct task_struct *tsk = per_cpu(idle_threads, cpu);
@@ -45,10 +51,7 @@ static inline void idle_init(unsigned int cpu)
 }
 
 /**
- * idle_thread_init - Initialize the idle thread for a cpu
- * @cpu:	The cpu for which the idle thread should be initialized
- *
- * Creates the thread if it does not exist.
+ * idle_threads_init - Initialize idle threads for all cpus
  */
 void __init idle_threads_init(void)
 {

commit ee74d13229fb606353ff56f4927fa93b37e95bbe
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Thu May 24 20:40:55 2012 +0530

    smpboot, idle: Optimize calls to smp_processor_id() in idle_threads_init()
    
    While trying to initialize idle threads for all cpus, idle_threads_init()
    calls smp_processor_id() in a loop, which is unnecessary. The intent
    is to initialize idle threads for all non-boot cpus. So just use a variable
    to note the boot cpu and use it in the loop.
    
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: suresh.b.siddha@intel.com
    Cc: venki@google.com
    Cc: nikunj@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/20120524151055.2549.64309.stgit@srivatsabhat.in.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index e1a797e028a3..0f2162f808a7 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -52,10 +52,12 @@ static inline void idle_init(unsigned int cpu)
  */
 void __init idle_threads_init(void)
 {
-	unsigned int cpu;
+	unsigned int cpu, boot_cpu;
+
+	boot_cpu = smp_processor_id();
 
 	for_each_possible_cpu(cpu) {
-		if (cpu != smp_processor_id())
+		if (cpu != boot_cpu)
 			idle_init(cpu);
 	}
 }

commit 3bb5d2ee396aabaa4e318f17e94d13e2ee0e5a88
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Apr 20 17:08:50 2012 -0700

    smp, idle: Allocate idle thread for each possible cpu during boot
    
    percpu areas are already allocated during boot for each possible cpu.
    percpu idle threads can be considered as an extension of the percpu areas,
    and allocate them for each possible cpu during boot.
    
    This will eliminate the need for workqueue based idle thread allocation.
    In future we can move the idle thread area into the percpu area too.
    
    [ tglx: Moved the loop into smpboot.c and added an error check when
      the init code failed to allocate an idle thread for a cpu which
      should be onlined ]
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: venki@google.com
    Link: http://lkml.kernel.org/r/1334966930.28674.245.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index ed1576981801..e1a797e028a3 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -6,64 +6,42 @@
 #include <linux/init.h>
 #include <linux/sched.h>
 #include <linux/percpu.h>
-#include <linux/workqueue.h>
 
 #include "smpboot.h"
 
 #ifdef CONFIG_GENERIC_SMP_IDLE_THREAD
-struct create_idle {
-	struct work_struct	work;
-	struct task_struct	*idle;
-	struct completion	done;
-	unsigned int cpu;
-};
-
-static void __cpuinit do_fork_idle(struct work_struct *work)
-{
-	struct create_idle *c = container_of(work, struct create_idle, work);
-
-	c->idle = fork_idle(c->cpu);
-	complete(&c->done);
-}
-
-static struct task_struct * __cpuinit idle_thread_create(unsigned int cpu)
-{
-	struct create_idle c_idle = {
-		.cpu	= cpu,
-		.done	= COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
-	};
-
-	INIT_WORK_ONSTACK(&c_idle.work, do_fork_idle);
-	schedule_work(&c_idle.work);
-	wait_for_completion(&c_idle.done);
-	destroy_work_on_stack(&c_idle.work);
-	return c_idle.idle;
-}
-
 /*
  * For the hotplug case we keep the task structs around and reuse
  * them.
  */
 static DEFINE_PER_CPU(struct task_struct *, idle_threads);
 
-static inline struct task_struct *get_idle_for_cpu(unsigned int cpu)
+struct task_struct * __cpuinit idle_thread_get(unsigned int cpu)
 {
 	struct task_struct *tsk = per_cpu(idle_threads, cpu);
 
 	if (!tsk)
-		return idle_thread_create(cpu);
+		return ERR_PTR(-ENOMEM);
 	init_idle(tsk, cpu);
 	return tsk;
 }
 
-struct task_struct * __cpuinit idle_thread_get(unsigned int cpu)
+void __init idle_thread_set_boot_cpu(void)
 {
-	return per_cpu(idle_threads, cpu);
+	per_cpu(idle_threads, smp_processor_id()) = current;
 }
 
-void __init idle_thread_set_boot_cpu(void)
+static inline void idle_init(unsigned int cpu)
 {
-	per_cpu(idle_threads, smp_processor_id()) = current;
+	struct task_struct *tsk = per_cpu(idle_threads, cpu);
+
+	if (!tsk) {
+		tsk = fork_idle(cpu);
+		if (IS_ERR(tsk))
+			pr_err("SMP: fork_idle() failed for CPU %u\n", cpu);
+		else
+			per_cpu(idle_threads, cpu) = tsk;
+	}
 }
 
 /**
@@ -72,25 +50,13 @@ void __init idle_thread_set_boot_cpu(void)
  *
  * Creates the thread if it does not exist.
  */
-static int __cpuinit idle_thread_init(unsigned int cpu)
+void __init idle_threads_init(void)
 {
-	struct task_struct *idle = get_idle_for_cpu(cpu);
+	unsigned int cpu;
 
-	if (IS_ERR(idle)) {
-		printk(KERN_ERR "failed fork for CPU %u\n", cpu);
-		return PTR_ERR(idle);
+	for_each_possible_cpu(cpu) {
+		if (cpu != smp_processor_id())
+			idle_init(cpu);
 	}
-	per_cpu(idle_threads, cpu) = idle;
-	return 0;
 }
-#else
-static inline int idle_thread_init(unsigned int cpu) { return 0; }
 #endif
-
-/**
- * smpboot_prepare - generic smpboot preparation
- */
-int __cpuinit smpboot_prepare(unsigned int cpu)
-{
-	return idle_thread_init(cpu);
-}

commit 29d5e0476e1c4a513859e7858845ad172f560389
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:45 2012 +0000

    smp: Provide generic idle thread allocation
    
    All SMP architectures have magic to fork the idle task and to store it
    for reusage when cpu hotplug is enabled. Provide a generic
    infrastructure for it.
    
    Create/reinit the idle thread for the cpu which is brought up in the
    generic code and hand the thread pointer to the architecture code via
    __cpu_up().
    
    Note, that fork_idle() is called via a workqueue, because this
    guarantees that the idle thread does not get a reference to a user
    space VM. This can happen when the boot process did not bring up all
    possible cpus and a later cpu_up() is initiated via the sysfs
    interface. In that case fork_idle() would be called in the context of
    the user space task and take a reference on the user space VM.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Acked-by: Venkatesh Pallipadi <venki@google.com>
    Link: http://lkml.kernel.org/r/20120420124557.102478630@linutronix.de

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
index 6dae6a3d2d59..ed1576981801 100644
--- a/kernel/smpboot.c
+++ b/kernel/smpboot.c
@@ -1,14 +1,96 @@
 /*
  * Common SMP CPU bringup/teardown functions
  */
+#include <linux/err.h>
+#include <linux/smp.h>
 #include <linux/init.h>
+#include <linux/sched.h>
+#include <linux/percpu.h>
+#include <linux/workqueue.h>
 
 #include "smpboot.h"
 
+#ifdef CONFIG_GENERIC_SMP_IDLE_THREAD
+struct create_idle {
+	struct work_struct	work;
+	struct task_struct	*idle;
+	struct completion	done;
+	unsigned int cpu;
+};
+
+static void __cpuinit do_fork_idle(struct work_struct *work)
+{
+	struct create_idle *c = container_of(work, struct create_idle, work);
+
+	c->idle = fork_idle(c->cpu);
+	complete(&c->done);
+}
+
+static struct task_struct * __cpuinit idle_thread_create(unsigned int cpu)
+{
+	struct create_idle c_idle = {
+		.cpu	= cpu,
+		.done	= COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
+	};
+
+	INIT_WORK_ONSTACK(&c_idle.work, do_fork_idle);
+	schedule_work(&c_idle.work);
+	wait_for_completion(&c_idle.done);
+	destroy_work_on_stack(&c_idle.work);
+	return c_idle.idle;
+}
+
+/*
+ * For the hotplug case we keep the task structs around and reuse
+ * them.
+ */
+static DEFINE_PER_CPU(struct task_struct *, idle_threads);
+
+static inline struct task_struct *get_idle_for_cpu(unsigned int cpu)
+{
+	struct task_struct *tsk = per_cpu(idle_threads, cpu);
+
+	if (!tsk)
+		return idle_thread_create(cpu);
+	init_idle(tsk, cpu);
+	return tsk;
+}
+
+struct task_struct * __cpuinit idle_thread_get(unsigned int cpu)
+{
+	return per_cpu(idle_threads, cpu);
+}
+
+void __init idle_thread_set_boot_cpu(void)
+{
+	per_cpu(idle_threads, smp_processor_id()) = current;
+}
+
+/**
+ * idle_thread_init - Initialize the idle thread for a cpu
+ * @cpu:	The cpu for which the idle thread should be initialized
+ *
+ * Creates the thread if it does not exist.
+ */
+static int __cpuinit idle_thread_init(unsigned int cpu)
+{
+	struct task_struct *idle = get_idle_for_cpu(cpu);
+
+	if (IS_ERR(idle)) {
+		printk(KERN_ERR "failed fork for CPU %u\n", cpu);
+		return PTR_ERR(idle);
+	}
+	per_cpu(idle_threads, cpu) = idle;
+	return 0;
+}
+#else
+static inline int idle_thread_init(unsigned int cpu) { return 0; }
+#endif
+
 /**
  * smpboot_prepare - generic smpboot preparation
  */
 int __cpuinit smpboot_prepare(unsigned int cpu)
 {
-	return 0;
+	return idle_thread_init(cpu);
 }

commit 38498a67aa2cf8c80754b8d304bfacc10bc582b5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:44 2012 +0000

    smp: Add generic smpboot facility
    
    Start a new file, which will hold SMP and CPU hotplug related generic
    infrastructure.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20120420124557.035417523@linutronix.de

diff --git a/kernel/smpboot.c b/kernel/smpboot.c
new file mode 100644
index 000000000000..6dae6a3d2d59
--- /dev/null
+++ b/kernel/smpboot.c
@@ -0,0 +1,14 @@
+/*
+ * Common SMP CPU bringup/teardown functions
+ */
+#include <linux/init.h>
+
+#include "smpboot.h"
+
+/**
+ * smpboot_prepare - generic smpboot preparation
+ */
+int __cpuinit smpboot_prepare(unsigned int cpu)
+{
+	return 0;
+}
