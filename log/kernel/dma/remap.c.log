commit 8e36baf97b252cdcafa53589e8227cbb1e85f0b0
Author: Eric Auger <eric.auger@redhat.com>
Date:   Tue Jun 23 14:07:55 2020 +0200

    dma-remap: align the size in dma_common_*_remap()
    
    Running a guest with a virtio-iommu protecting virtio devices
    is broken since commit 515e5b6d90d4 ("dma-mapping: use vmap insted
    of reimplementing it"). Before the conversion, the size was
    page aligned in __get_vm_area_node(). Doing so fixes the
    regression.
    
    Fixes: 515e5b6d90d4 ("dma-mapping: use vmap insted of reimplementing it")
    Signed-off-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index e739a6eea6e7..78b23f089cf1 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -24,7 +24,8 @@ void *dma_common_pages_remap(struct page **pages, size_t size,
 {
 	void *vaddr;
 
-	vaddr = vmap(pages, size >> PAGE_SHIFT, VM_DMA_COHERENT, prot);
+	vaddr = vmap(pages, PAGE_ALIGN(size) >> PAGE_SHIFT,
+		     VM_DMA_COHERENT, prot);
 	if (vaddr)
 		find_vm_area(vaddr)->pages = pages;
 	return vaddr;
@@ -37,7 +38,7 @@ void *dma_common_pages_remap(struct page **pages, size_t size,
 void *dma_common_contiguous_remap(struct page *page, size_t size,
 			pgprot_t prot, const void *caller)
 {
-	int count = size >> PAGE_SHIFT;
+	int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
 	struct page **pages;
 	void *vaddr;
 	int i;

commit 1ee18de92927f37e6948d5a6fc73cbf89f806905
Merge: e542e0dc3ee3 298f3db6ee69
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 6 11:43:23 2020 -0700

    Merge tag 'dma-mapping-5.8' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma-mapping updates from Christoph Hellwig:
    
     - enhance the dma pool to allow atomic allocation on x86 with AMD SEV
       (David Rientjes)
    
     - two small cleanups (Jason Yan and Peter Collingbourne)
    
    * tag 'dma-mapping-5.8' of git://git.infradead.org/users/hch/dma-mapping:
      dma-contiguous: fix comment for dma_release_from_contiguous
      dma-pool: scale the default DMA coherent pool size with memory capacity
      x86/mm: unencrypted non-blocking DMA allocations use coherent pools
      dma-pool: add pool sizes to debugfs
      dma-direct: atomic allocations must come from atomic coherent pools
      dma-pool: dynamically expanding atomic pools
      dma-pool: add additional coherent pools to map to gfp mask
      dma-remap: separate DMA atomic pools from direct remap code
      dma-debug: make __dma_entry_alloc_check_leak() static

commit 515e5b6d90d410a3b0b433853c367936830a45a4
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:50:32 2020 -0700

    dma-mapping: use vmap insted of reimplementing it
    
    Replace the open coded instance of vmap with the actual function.  In
    the non-contiguous (IOMMU) case this requires an extra find_vm_area,
    but given that this isn't a fast path function that is a small price
    to pay.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200414131348.444715-6-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index d14cbc83986a..914ff5a58dd5 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -20,23 +20,6 @@ struct page **dma_common_find_pages(void *cpu_addr)
 	return area->pages;
 }
 
-static struct vm_struct *__dma_common_pages_remap(struct page **pages,
-			size_t size, pgprot_t prot, const void *caller)
-{
-	struct vm_struct *area;
-
-	area = get_vm_area_caller(size, VM_DMA_COHERENT, caller);
-	if (!area)
-		return NULL;
-
-	if (map_vm_area(area, prot, pages)) {
-		vunmap(area->addr);
-		return NULL;
-	}
-
-	return area;
-}
-
 /*
  * Remaps an array of PAGE_SIZE pages into another vm_area.
  * Cannot be used in non-sleeping contexts
@@ -44,15 +27,12 @@ static struct vm_struct *__dma_common_pages_remap(struct page **pages,
 void *dma_common_pages_remap(struct page **pages, size_t size,
 			 pgprot_t prot, const void *caller)
 {
-	struct vm_struct *area;
+	void *vaddr;
 
-	area = __dma_common_pages_remap(pages, size, prot, caller);
-	if (!area)
-		return NULL;
-
-	area->pages = pages;
-
-	return area->addr;
+	vaddr = vmap(pages, size >> PAGE_SHIFT, VM_DMA_COHERENT, prot);
+	if (vaddr)
+		find_vm_area(vaddr)->pages = pages;
+	return vaddr;
 }
 
 /*
@@ -62,24 +42,20 @@ void *dma_common_pages_remap(struct page **pages, size_t size,
 void *dma_common_contiguous_remap(struct page *page, size_t size,
 			pgprot_t prot, const void *caller)
 {
-	int i;
+	int count = size >> PAGE_SHIFT;
 	struct page **pages;
-	struct vm_struct *area;
+	void *vaddr;
+	int i;
 
-	pages = kmalloc(sizeof(struct page *) << get_order(size), GFP_KERNEL);
+	pages = kmalloc_array(count, sizeof(struct page *), GFP_KERNEL);
 	if (!pages)
 		return NULL;
-
-	for (i = 0; i < (size >> PAGE_SHIFT); i++)
+	for (i = 0; i < count; i++)
 		pages[i] = nth_page(page, i);
-
-	area = __dma_common_pages_remap(pages, size, prot, caller);
-
+	vaddr = vmap(pages, count, VM_DMA_COHERENT, prot);
 	kfree(pages);
 
-	if (!area)
-		return NULL;
-	return area->addr;
+	return vaddr;
 }
 
 /*

commit e860c299ac0d738b44ff91693f11e63080a29698
Author: David Rientjes <rientjes@google.com>
Date:   Tue Apr 14 17:04:52 2020 -0700

    dma-remap: separate DMA atomic pools from direct remap code
    
    DMA atomic pools will be needed beyond only CONFIG_DMA_DIRECT_REMAP so
    separate them out into their own file.
    
    This also adds a new Kconfig option that can be subsequently used for
    options, such as CONFIG_AMD_MEM_ENCRYPT, that will utilize the coherent
    pools but do not have a dependency on direct remapping.
    
    For this patch alone, there is no functional change introduced.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: David Rientjes <rientjes@google.com>
    [hch: fixup copyrights and remove unused includes]
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index d14cbc83986a..f7b402849891 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -1,13 +1,8 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
- * Copyright (C) 2012 ARM Ltd.
  * Copyright (c) 2014 The Linux Foundation
  */
-#include <linux/dma-direct.h>
-#include <linux/dma-noncoherent.h>
-#include <linux/dma-contiguous.h>
-#include <linux/init.h>
-#include <linux/genalloc.h>
+#include <linux/dma-mapping.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
 
@@ -97,117 +92,3 @@ void dma_common_free_remap(void *cpu_addr, size_t size)
 	unmap_kernel_range((unsigned long)cpu_addr, PAGE_ALIGN(size));
 	vunmap(cpu_addr);
 }
-
-#ifdef CONFIG_DMA_DIRECT_REMAP
-static struct gen_pool *atomic_pool __ro_after_init;
-
-#define DEFAULT_DMA_COHERENT_POOL_SIZE  SZ_256K
-static size_t atomic_pool_size __initdata = DEFAULT_DMA_COHERENT_POOL_SIZE;
-
-static int __init early_coherent_pool(char *p)
-{
-	atomic_pool_size = memparse(p, &p);
-	return 0;
-}
-early_param("coherent_pool", early_coherent_pool);
-
-static gfp_t dma_atomic_pool_gfp(void)
-{
-	if (IS_ENABLED(CONFIG_ZONE_DMA))
-		return GFP_DMA;
-	if (IS_ENABLED(CONFIG_ZONE_DMA32))
-		return GFP_DMA32;
-	return GFP_KERNEL;
-}
-
-static int __init dma_atomic_pool_init(void)
-{
-	unsigned int pool_size_order = get_order(atomic_pool_size);
-	unsigned long nr_pages = atomic_pool_size >> PAGE_SHIFT;
-	struct page *page;
-	void *addr;
-	int ret;
-
-	if (dev_get_cma_area(NULL))
-		page = dma_alloc_from_contiguous(NULL, nr_pages,
-						 pool_size_order, false);
-	else
-		page = alloc_pages(dma_atomic_pool_gfp(), pool_size_order);
-	if (!page)
-		goto out;
-
-	arch_dma_prep_coherent(page, atomic_pool_size);
-
-	atomic_pool = gen_pool_create(PAGE_SHIFT, -1);
-	if (!atomic_pool)
-		goto free_page;
-
-	addr = dma_common_contiguous_remap(page, atomic_pool_size,
-					   pgprot_dmacoherent(PAGE_KERNEL),
-					   __builtin_return_address(0));
-	if (!addr)
-		goto destroy_genpool;
-
-	ret = gen_pool_add_virt(atomic_pool, (unsigned long)addr,
-				page_to_phys(page), atomic_pool_size, -1);
-	if (ret)
-		goto remove_mapping;
-	gen_pool_set_algo(atomic_pool, gen_pool_first_fit_order_align, NULL);
-
-	pr_info("DMA: preallocated %zu KiB pool for atomic allocations\n",
-		atomic_pool_size / 1024);
-	return 0;
-
-remove_mapping:
-	dma_common_free_remap(addr, atomic_pool_size);
-destroy_genpool:
-	gen_pool_destroy(atomic_pool);
-	atomic_pool = NULL;
-free_page:
-	if (!dma_release_from_contiguous(NULL, page, nr_pages))
-		__free_pages(page, pool_size_order);
-out:
-	pr_err("DMA: failed to allocate %zu KiB pool for atomic coherent allocation\n",
-		atomic_pool_size / 1024);
-	return -ENOMEM;
-}
-postcore_initcall(dma_atomic_pool_init);
-
-bool dma_in_atomic_pool(void *start, size_t size)
-{
-	if (unlikely(!atomic_pool))
-		return false;
-
-	return gen_pool_has_addr(atomic_pool, (unsigned long)start, size);
-}
-
-void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)
-{
-	unsigned long val;
-	void *ptr = NULL;
-
-	if (!atomic_pool) {
-		WARN(1, "coherent pool not initialised!\n");
-		return NULL;
-	}
-
-	val = gen_pool_alloc(atomic_pool, size);
-	if (val) {
-		phys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val);
-
-		*ret_page = pfn_to_page(__phys_to_pfn(phys));
-		ptr = (void *)val;
-		memset(ptr, 0, size);
-	}
-
-	return ptr;
-}
-
-bool dma_free_from_pool(void *start, size_t size)
-{
-	if (!dma_in_atomic_pool(start, size))
-		return false;
-	gen_pool_free(atomic_pool, (unsigned long)start, size);
-	return true;
-}
-#endif /* CONFIG_DMA_DIRECT_REMAP */

commit 964975ac6677c97ae61ec9d6969dd5d03f18d1c3
Author: Huang Shijie <sjhuang@iluvatar.ai>
Date:   Wed Dec 4 16:52:03 2019 -0800

    lib/genalloc.c: rename addr_in_gen_pool to gen_pool_has_addr
    
    Follow the kernel conventions, rename addr_in_gen_pool to
    gen_pool_has_addr.
    
    [sjhuang@iluvatar.ai: fix Documentation/ too]
     Link: http://lkml.kernel.org/r/20181229015914.5573-1-sjhuang@iluvatar.ai
    Link: http://lkml.kernel.org/r/20181228083950.20398-1-sjhuang@iluvatar.ai
    Signed-off-by: Huang Shijie <sjhuang@iluvatar.ai>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index d47bd40fc0f5..d14cbc83986a 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -178,7 +178,7 @@ bool dma_in_atomic_pool(void *start, size_t size)
 	if (unlikely(!atomic_pool))
 		return false;
 
-	return addr_in_gen_pool(atomic_pool, (unsigned long)start, size);
+	return gen_pool_has_addr(atomic_pool, (unsigned long)start, size);
 }
 
 void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)

commit 3acac065508f6cc60ac9d3e4b7c6cc37fd91d531
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Oct 29 11:06:32 2019 +0100

    dma-mapping: merge the generic remapping helpers into dma-direct
    
    Integrate the generic dma remapping implementation into the main flow.
    This prepares for architectures like xtensa that use an uncached
    segment for pages in the kernel mapping, but can also remap highmem
    from CMA.  To simplify that implementation we now always deduct the
    page from the physical address via the DMA address instead of the
    virtual address.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index 3c49499ee6b0..d47bd40fc0f5 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -210,53 +210,4 @@ bool dma_free_from_pool(void *start, size_t size)
 	gen_pool_free(atomic_pool, (unsigned long)start, size);
 	return true;
 }
-
-void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
-		gfp_t flags, unsigned long attrs)
-{
-	struct page *page = NULL;
-	void *ret;
-
-	size = PAGE_ALIGN(size);
-
-	if (!gfpflags_allow_blocking(flags)) {
-		ret = dma_alloc_from_pool(size, &page, flags);
-		if (!ret)
-			return NULL;
-		goto done;
-	}
-
-	page = __dma_direct_alloc_pages(dev, size, flags, attrs);
-	if (!page)
-		return NULL;
-
-	/* remove any dirty cache lines on the kernel alias */
-	arch_dma_prep_coherent(page, size);
-
-	/* create a coherent mapping */
-	ret = dma_common_contiguous_remap(page, size,
-			dma_pgprot(dev, PAGE_KERNEL, attrs),
-			__builtin_return_address(0));
-	if (!ret) {
-		dma_free_contiguous(dev, page, size);
-		return ret;
-	}
-
-	memset(ret, 0, size);
-done:
-	*dma_handle = phys_to_dma(dev, page_to_phys(page));
-	return ret;
-}
-
-void arch_dma_free(struct device *dev, size_t size, void *vaddr,
-		dma_addr_t dma_handle, unsigned long attrs)
-{
-	if (!dma_free_from_pool(vaddr, PAGE_ALIGN(size))) {
-		phys_addr_t phys = dma_to_phys(dev, dma_handle);
-		struct page *page = pfn_to_page(__phys_to_pfn(phys));
-
-		vunmap(vaddr);
-		dma_free_contiguous(dev, page, size);
-	}
-}
 #endif /* CONFIG_DMA_DIRECT_REMAP */

commit 34dc0ea6bc960f1f57b2148f01a3f4da23f87013
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Oct 29 11:01:37 2019 +0100

    dma-direct: provide mmap and get_sgtable method overrides
    
    For dma-direct we know that the DMA address is an encoding of the
    physical address that we can trivially decode.  Use that fact to
    provide implementations that do not need the arch_dma_coherent_to_pfn
    architecture hook.  Note that we still can only support mmap of
    non-coherent memory only if the architecture provides a way to set an
    uncached bit in the page tables.  This must be true for architectures
    that use the generic remap helpers, but other architectures can also
    manually select it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index 90d5ce77c189..3c49499ee6b0 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -259,10 +259,4 @@ void arch_dma_free(struct device *dev, size_t size, void *vaddr,
 		dma_free_contiguous(dev, page, size);
 	}
 }
-
-long arch_dma_coherent_to_pfn(struct device *dev, void *cpu_addr,
-		dma_addr_t dma_addr)
-{
-	return __phys_to_pfn(dma_to_phys(dev, dma_addr));
-}
 #endif /* CONFIG_DMA_DIRECT_REMAP */

commit 4e1003aa56a7d60ddb048e43a7a51368fcfe36af
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Oct 29 09:57:32 2019 +0100

    dma-direct: remove the dma_handle argument to __dma_direct_alloc_pages
    
    The argument isn't used anywhere, so stop passing it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index fb1e50c2d48a..90d5ce77c189 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -226,7 +226,7 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		goto done;
 	}
 
-	page = __dma_direct_alloc_pages(dev, size, dma_handle, flags, attrs);
+	page = __dma_direct_alloc_pages(dev, size, flags, attrs);
 	if (!page)
 		return NULL;
 

commit acaade1af3587132e7ea585f470a95261e14f60c
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Oct 29 09:57:09 2019 +0100

    dma-direct: remove __dma_direct_free_pages
    
    We can just call dma_free_contiguous directly instead of wrapping it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Max Filippov <jcmvbkbc@gmail.com>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index c00b9258fa6a..fb1e50c2d48a 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -238,7 +238,7 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 			dma_pgprot(dev, PAGE_KERNEL, attrs),
 			__builtin_return_address(0));
 	if (!ret) {
-		__dma_direct_free_pages(dev, size, page);
+		dma_free_contiguous(dev, page, size);
 		return ret;
 	}
 
@@ -256,7 +256,7 @@ void arch_dma_free(struct device *dev, size_t size, void *vaddr,
 		struct page *page = pfn_to_page(__phys_to_pfn(phys));
 
 		vunmap(vaddr);
-		__dma_direct_free_pages(dev, size, page);
+		dma_free_contiguous(dev, page, size);
 	}
 }
 

commit 2cf2aa6a69db0b17b3979144287af8775c1c1534
Author: Andrey Smirnov <andrew.smirnov@gmail.com>
Date:   Sat Oct 5 10:23:30 2019 +0200

    dma-mapping: fix false positivse warnings in dma_common_free_remap()
    
    Commit 5cf4537975bb ("dma-mapping: introduce a dma_common_find_pages
    helper") changed invalid input check in dma_common_free_remap() from:
    
        if (!area || !area->flags != VM_DMA_COHERENT)
    
    to
    
        if (!area || !area->flags != VM_DMA_COHERENT || !area->pages)
    
    which seem to produce false positives for memory obtained via
    dma_common_contiguous_remap()
    
    This triggers the following warning message when doing "reboot" on ZII
    VF610 Dev Board Rev B:
    
    WARNING: CPU: 0 PID: 1 at kernel/dma/remap.c:112 dma_common_free_remap+0x88/0x8c
    trying to free invalid coherent area: 9ef82980
    Modules linked in:
    CPU: 0 PID: 1 Comm: systemd-shutdow Not tainted 5.3.0-rc6-next-20190820 #119
    Hardware name: Freescale Vybrid VF5xx/VF6xx (Device Tree)
    Backtrace:
    [<8010d1ec>] (dump_backtrace) from [<8010d588>] (show_stack+0x20/0x24)
     r7:8015ed78 r6:00000009 r5:00000000 r4:9f4d9b14
    [<8010d568>] (show_stack) from [<8077e3f0>] (dump_stack+0x24/0x28)
    [<8077e3cc>] (dump_stack) from [<801197a0>] (__warn.part.3+0xcc/0xe4)
    [<801196d4>] (__warn.part.3) from [<80119830>] (warn_slowpath_fmt+0x78/0x94)
     r6:00000070 r5:808e540c r4:81c03048
    [<801197bc>] (warn_slowpath_fmt) from [<8015ed78>] (dma_common_free_remap+0x88/0x8c)
     r3:9ef82980 r2:808e53e0
     r7:00001000 r6:a0b1e000 r5:a0b1e000 r4:00001000
    [<8015ecf0>] (dma_common_free_remap) from [<8010fa9c>] (remap_allocator_free+0x60/0x68)
     r5:81c03048 r4:9f4d9b78
    [<8010fa3c>] (remap_allocator_free) from [<801100d0>] (__arm_dma_free.constprop.3+0xf8/0x148)
     r5:81c03048 r4:9ef82900
    [<8010ffd8>] (__arm_dma_free.constprop.3) from [<80110144>] (arm_dma_free+0x24/0x2c)
     r5:9f563410 r4:80110120
    [<80110120>] (arm_dma_free) from [<8015d80c>] (dma_free_attrs+0xa0/0xdc)
    [<8015d76c>] (dma_free_attrs) from [<8020f3e4>] (dma_pool_destroy+0xc0/0x154)
     r8:9efa8860 r7:808f02f0 r6:808f02d0 r5:9ef82880 r4:9ef82780
    [<8020f324>] (dma_pool_destroy) from [<805525d0>] (ehci_mem_cleanup+0x6c/0x150)
     r7:9f563410 r6:9efa8810 r5:00000000 r4:9efd0148
    [<80552564>] (ehci_mem_cleanup) from [<80558e0c>] (ehci_stop+0xac/0xc0)
     r5:9efd0148 r4:9efd0000
    [<80558d60>] (ehci_stop) from [<8053c4bc>] (usb_remove_hcd+0xf4/0x1b0)
     r7:9f563410 r6:9efd0074 r5:81c03048 r4:9efd0000
    [<8053c3c8>] (usb_remove_hcd) from [<8056361c>] (host_stop+0x48/0xb8)
     r7:9f563410 r6:9efd0000 r5:9f5f4040 r4:9f5f5040
    [<805635d4>] (host_stop) from [<80563d0c>] (ci_hdrc_host_destroy+0x34/0x38)
     r7:9f563410 r6:9f5f5040 r5:9efa8800 r4:9f5f4040
    [<80563cd8>] (ci_hdrc_host_destroy) from [<8055ef18>] (ci_hdrc_remove+0x50/0x10c)
    [<8055eec8>] (ci_hdrc_remove) from [<804a2ed8>] (platform_drv_remove+0x34/0x4c)
     r7:9f563410 r6:81c4f99c r5:9efa8810 r4:9efa8810
    [<804a2ea4>] (platform_drv_remove) from [<804a18a8>] (device_release_driver_internal+0xec/0x19c)
     r5:00000000 r4:9efa8810
    [<804a17bc>] (device_release_driver_internal) from [<804a1978>] (device_release_driver+0x20/0x24)
     r7:9f563410 r6:81c41ed0 r5:9efa8810 r4:9f4a1dac
    [<804a1958>] (device_release_driver) from [<804a01b8>] (bus_remove_device+0xdc/0x108)
    [<804a00dc>] (bus_remove_device) from [<8049c204>] (device_del+0x150/0x36c)
     r7:9f563410 r6:81c03048 r5:9efa8854 r4:9efa8810
    [<8049c0b4>] (device_del) from [<804a3368>] (platform_device_del.part.2+0x20/0x84)
     r10:9f563414 r9:809177e0 r8:81cb07dc r7:81c78320 r6:9f563454 r5:9efa8800
     r4:9efa8800
    [<804a3348>] (platform_device_del.part.2) from [<804a3420>] (platform_device_unregister+0x28/0x34)
     r5:9f563400 r4:9efa8800
    [<804a33f8>] (platform_device_unregister) from [<8055dce0>] (ci_hdrc_remove_device+0x1c/0x30)
     r5:9f563400 r4:00000001
    [<8055dcc4>] (ci_hdrc_remove_device) from [<805652ac>] (ci_hdrc_imx_remove+0x38/0x118)
     r7:81c78320 r6:9f563454 r5:9f563410 r4:9f541010
    [<8056538c>] (ci_hdrc_imx_shutdown) from [<804a2970>] (platform_drv_shutdown+0x2c/0x30)
    [<804a2944>] (platform_drv_shutdown) from [<8049e4fc>] (device_shutdown+0x158/0x1f0)
    [<8049e3a4>] (device_shutdown) from [<8013ac80>] (kernel_restart_prepare+0x44/0x48)
     r10:00000058 r9:9f4d8000 r8:fee1dead r7:379ce700 r6:81c0b280 r5:81c03048
     r4:00000000
    [<8013ac3c>] (kernel_restart_prepare) from [<8013ad14>] (kernel_restart+0x1c/0x60)
    [<8013acf8>] (kernel_restart) from [<8013af84>] (__do_sys_reboot+0xe0/0x1d8)
     r5:81c03048 r4:00000000
    [<8013aea4>] (__do_sys_reboot) from [<8013b0ec>] (sys_reboot+0x18/0x1c)
     r8:80101204 r7:00000058 r6:00000000 r5:00000000 r4:00000000
    [<8013b0d4>] (sys_reboot) from [<80101000>] (ret_fast_syscall+0x0/0x54)
    Exception stack(0x9f4d9fa8 to 0x9f4d9ff0)
    9fa0:                   00000000 00000000 fee1dead 28121969 01234567 379ce700
    9fc0: 00000000 00000000 00000000 00000058 00000000 00000000 00000000 00016d04
    9fe0: 00028e0c 7ec87c64 000135ec 76c1f410
    
    Restore original invalid input check in dma_common_free_remap() to
    avoid this problem.
    
    Fixes: 5cf4537975bb ("dma-mapping: introduce a dma_common_find_pages helper")
    Signed-off-by: Andrey Smirnov <andrew.smirnov@gmail.com>
    [hch: just revert the offending hunk instead of creating a new helper]
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index ca4e5d44b571..c00b9258fa6a 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -87,9 +87,9 @@ void *dma_common_contiguous_remap(struct page *page, size_t size,
  */
 void dma_common_free_remap(void *cpu_addr, size_t size)
 {
-	struct page **pages = dma_common_find_pages(cpu_addr);
+	struct vm_struct *area = find_vm_area(cpu_addr);
 
-	if (!pages) {
+	if (!area || area->flags != VM_DMA_COHERENT) {
 		WARN(1, "trying to free invalid coherent area: %p\n", cpu_addr);
 		return;
 	}

commit 5cf4537975bbd5691b9ddd015d540bb92f61e322
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 3 09:14:31 2019 +0200

    dma-mapping: introduce a dma_common_find_pages helper
    
    A helper to find the backing page array based on a virtual address.
    This also ensures we do the same vm_flags check everywhere instead
    of slightly different or missing ones in a few places.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index f6b90521a7e4..ca4e5d44b571 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -11,6 +11,15 @@
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
 
+struct page **dma_common_find_pages(void *cpu_addr)
+{
+	struct vm_struct *area = find_vm_area(cpu_addr);
+
+	if (!area || area->flags != VM_DMA_COHERENT)
+		return NULL;
+	return area->pages;
+}
+
 static struct vm_struct *__dma_common_pages_remap(struct page **pages,
 			size_t size, pgprot_t prot, const void *caller)
 {
@@ -78,9 +87,9 @@ void *dma_common_contiguous_remap(struct page *page, size_t size,
  */
 void dma_common_free_remap(void *cpu_addr, size_t size)
 {
-	struct vm_struct *area = find_vm_area(cpu_addr);
+	struct page **pages = dma_common_find_pages(cpu_addr);
 
-	if (!area || area->flags != VM_DMA_COHERENT) {
+	if (!pages) {
 		WARN(1, "trying to free invalid coherent area: %p\n", cpu_addr);
 		return;
 	}

commit 512317401f6a337e617ec284d20dec5fa3a951ec
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 30 08:51:01 2019 +0200

    dma-mapping: always use VM_DMA_COHERENT for generic DMA remap
    
    Currently the generic dma remap allocator gets a vm_flags passed by
    the caller that is a little confusing.  We just introduced a generic
    vmalloc-level flag to identify the dma coherent allocations, so use
    that everywhere and remove the now pointless argument.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index 838123f79639..f6b90521a7e4 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -12,12 +12,11 @@
 #include <linux/vmalloc.h>
 
 static struct vm_struct *__dma_common_pages_remap(struct page **pages,
-			size_t size, unsigned long vm_flags, pgprot_t prot,
-			const void *caller)
+			size_t size, pgprot_t prot, const void *caller)
 {
 	struct vm_struct *area;
 
-	area = get_vm_area_caller(size, vm_flags, caller);
+	area = get_vm_area_caller(size, VM_DMA_COHERENT, caller);
 	if (!area)
 		return NULL;
 
@@ -34,12 +33,11 @@ static struct vm_struct *__dma_common_pages_remap(struct page **pages,
  * Cannot be used in non-sleeping contexts
  */
 void *dma_common_pages_remap(struct page **pages, size_t size,
-			unsigned long vm_flags, pgprot_t prot,
-			const void *caller)
+			 pgprot_t prot, const void *caller)
 {
 	struct vm_struct *area;
 
-	area = __dma_common_pages_remap(pages, size, vm_flags, prot, caller);
+	area = __dma_common_pages_remap(pages, size, prot, caller);
 	if (!area)
 		return NULL;
 
@@ -53,7 +51,6 @@ void *dma_common_pages_remap(struct page **pages, size_t size,
  * Cannot be used in non-sleeping contexts
  */
 void *dma_common_contiguous_remap(struct page *page, size_t size,
-			unsigned long vm_flags,
 			pgprot_t prot, const void *caller)
 {
 	int i;
@@ -67,7 +64,7 @@ void *dma_common_contiguous_remap(struct page *page, size_t size,
 	for (i = 0; i < (size >> PAGE_SHIFT); i++)
 		pages[i] = nth_page(page, i);
 
-	area = __dma_common_pages_remap(pages, size, vm_flags, prot, caller);
+	area = __dma_common_pages_remap(pages, size, prot, caller);
 
 	kfree(pages);
 
@@ -79,11 +76,11 @@ void *dma_common_contiguous_remap(struct page *page, size_t size,
 /*
  * Unmaps a range previously mapped by dma_common_*_remap
  */
-void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags)
+void dma_common_free_remap(void *cpu_addr, size_t size)
 {
 	struct vm_struct *area = find_vm_area(cpu_addr);
 
-	if (!area || (area->flags & vm_flags) != vm_flags) {
+	if (!area || area->flags != VM_DMA_COHERENT) {
 		WARN(1, "trying to free invalid coherent area: %p\n", cpu_addr);
 		return;
 	}
@@ -136,7 +133,7 @@ static int __init dma_atomic_pool_init(void)
 	if (!atomic_pool)
 		goto free_page;
 
-	addr = dma_common_contiguous_remap(page, atomic_pool_size, VM_USERMAP,
+	addr = dma_common_contiguous_remap(page, atomic_pool_size,
 					   pgprot_dmacoherent(PAGE_KERNEL),
 					   __builtin_return_address(0));
 	if (!addr)
@@ -153,7 +150,7 @@ static int __init dma_atomic_pool_init(void)
 	return 0;
 
 remove_mapping:
-	dma_common_free_remap(addr, atomic_pool_size, VM_USERMAP);
+	dma_common_free_remap(addr, atomic_pool_size);
 destroy_genpool:
 	gen_pool_destroy(atomic_pool);
 	atomic_pool = NULL;
@@ -228,7 +225,7 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 	arch_dma_prep_coherent(page, size);
 
 	/* create a coherent mapping */
-	ret = dma_common_contiguous_remap(page, size, VM_USERMAP,
+	ret = dma_common_contiguous_remap(page, size,
 			dma_pgprot(dev, PAGE_KERNEL, attrs),
 			__builtin_return_address(0));
 	if (!ret) {

commit 8e3a68fb55e00e0760bd8023883e064f1f93c62d
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Aug 3 12:42:15 2019 +0300

    dma-mapping: make dma_atomic_pool_init self-contained
    
    The memory allocated for the atomic pool needs to have the same
    mapping attributes that we use for remapping, so use
    pgprot_dmacoherent instead of open coding it.  Also deduct a
    suitable zone to allocate the memory from based on the presence
    of the DMA zones.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index ffe78f0b2fe4..838123f79639 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -105,7 +105,16 @@ static int __init early_coherent_pool(char *p)
 }
 early_param("coherent_pool", early_coherent_pool);
 
-int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
+static gfp_t dma_atomic_pool_gfp(void)
+{
+	if (IS_ENABLED(CONFIG_ZONE_DMA))
+		return GFP_DMA;
+	if (IS_ENABLED(CONFIG_ZONE_DMA32))
+		return GFP_DMA32;
+	return GFP_KERNEL;
+}
+
+static int __init dma_atomic_pool_init(void)
 {
 	unsigned int pool_size_order = get_order(atomic_pool_size);
 	unsigned long nr_pages = atomic_pool_size >> PAGE_SHIFT;
@@ -117,7 +126,7 @@ int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
 		page = dma_alloc_from_contiguous(NULL, nr_pages,
 						 pool_size_order, false);
 	else
-		page = alloc_pages(gfp, pool_size_order);
+		page = alloc_pages(dma_atomic_pool_gfp(), pool_size_order);
 	if (!page)
 		goto out;
 
@@ -128,7 +137,8 @@ int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
 		goto free_page;
 
 	addr = dma_common_contiguous_remap(page, atomic_pool_size, VM_USERMAP,
-					   prot, __builtin_return_address(0));
+					   pgprot_dmacoherent(PAGE_KERNEL),
+					   __builtin_return_address(0));
 	if (!addr)
 		goto destroy_genpool;
 
@@ -155,6 +165,7 @@ int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
 		atomic_pool_size / 1024);
 	return -ENOMEM;
 }
+postcore_initcall(dma_atomic_pool_init);
 
 bool dma_in_atomic_pool(void *start, size_t size)
 {

commit 33dcb37cef741294b481f4d889a465b8091f11bf
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jul 26 09:26:40 2019 +0200

    dma-mapping: fix page attributes for dma_mmap_*
    
    All the way back to introducing dma_common_mmap we've defaulted to mark
    the pages as uncached.  But this is wrong for DMA coherent devices.
    Later on DMA_ATTR_WRITE_COMBINE also got incorrect treatment as that
    flag is only treated special on the alloc side for non-coherent devices.
    
    Introduce a new dma_pgprot helper that deals with the check for coherent
    devices so that only the remapping cases ever reach arch_dma_mmap_pgprot
    and we thus ensure no aliasing of page attributes happens, which makes
    the powerpc version of arch_dma_mmap_pgprot obsolete and simplifies the
    remaining ones.
    
    Note that this means arch_dma_mmap_pgprot is a bit misnamed now, but
    we'll phase it out soon.
    
    Fixes: 64ccc9c033c6 ("common: dma-mapping: add support for generic dma_mmap_* calls")
    Reported-by: Shawn Anastasio <shawn@anastas.io>
    Reported-by: Gavin Li <git@thegavinli.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com> # arm64

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index a594aec07882..ffe78f0b2fe4 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -218,7 +218,7 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 
 	/* create a coherent mapping */
 	ret = dma_common_contiguous_remap(page, size, VM_USERMAP,
-			arch_dma_mmap_pgprot(dev, PAGE_KERNEL, attrs),
+			dma_pgprot(dev, PAGE_KERNEL, attrs),
 			__builtin_return_address(0));
 	if (!ret) {
 		__dma_direct_free_pages(dev, size, page);

commit d98849aff87911013aadb730138ab728b52fc547
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 14 16:17:27 2019 +0200

    dma-direct: handle DMA_ATTR_NO_KERNEL_MAPPING in common code
    
    DMA_ATTR_NO_KERNEL_MAPPING is generally implemented by allocating
    normal cacheable pages or CMA memory, and then returning the page
    pointer as the opaque handle.  Lift that code from the xtensa and
    generic dma remapping implementations into the generic dma-direct
    code so that we don't even call arch_dma_alloc for these allocations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index 0207e3764d52..a594aec07882 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -202,8 +202,7 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 
 	size = PAGE_ALIGN(size);
 
-	if (!gfpflags_allow_blocking(flags) &&
-	    !(attrs & DMA_ATTR_NO_KERNEL_MAPPING)) {
+	if (!gfpflags_allow_blocking(flags)) {
 		ret = dma_alloc_from_pool(size, &page, flags);
 		if (!ret)
 			return NULL;
@@ -217,11 +216,6 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 	/* remove any dirty cache lines on the kernel alias */
 	arch_dma_prep_coherent(page, size);
 
-	if (attrs & DMA_ATTR_NO_KERNEL_MAPPING) {
-		ret = page; /* opaque cookie */
-		goto done;
-	}
-
 	/* create a coherent mapping */
 	ret = dma_common_contiguous_remap(page, size, VM_USERMAP,
 			arch_dma_mmap_pgprot(dev, PAGE_KERNEL, attrs),
@@ -240,10 +234,7 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 void arch_dma_free(struct device *dev, size_t size, void *vaddr,
 		dma_addr_t dma_handle, unsigned long attrs)
 {
-	if (attrs & DMA_ATTR_NO_KERNEL_MAPPING) {
-		/* vaddr is a struct page cookie, not a kernel address */
-		__dma_direct_free_pages(dev, size, vaddr);
-	} else if (!dma_free_from_pool(vaddr, PAGE_ALIGN(size))) {
+	if (!dma_free_from_pool(vaddr, PAGE_ALIGN(size))) {
 		phys_addr_t phys = dma_to_phys(dev, dma_handle);
 		struct page *page = pfn_to_page(__phys_to_pfn(phys));
 

commit 4b4b077cbd0a998aebaa72c199e06b8a4c8dcfee
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Mon Jun 10 15:54:37 2019 -0700

    dma-remap: Avoid de-referencing NULL atomic_pool
    
    With architectures allowing the kernel to be placed almost arbitrarily
    in memory (e.g.: ARM64), it is possible to have the kernel resides at
    physical addresses above 4GB, resulting in neither the default CMA area,
    nor the atomic pool from successfully allocating. This does not prevent
    specific peripherals from working though, one example is XHCI, which
    still operates correctly.
    
    Trouble comes when the XHCI driver gets suspended and resumed, since we
    can now trigger the following NPD:
    
    [   12.664170] usb usb1: root hub lost power or was reset
    [   12.669387] usb usb2: root hub lost power or was reset
    [   12.674662] Unable to handle kernel NULL pointer dereference at virtual address 00000008
    [   12.682896] pgd = ffffffc1365a7000
    [   12.686386] [00000008] *pgd=0000000136500003, *pud=0000000136500003, *pmd=0000000000000000
    [   12.694897] Internal error: Oops: 96000006 [#1] SMP
    [   12.699843] Modules linked in:
    [   12.702980] CPU: 0 PID: 1499 Comm: pml Not tainted 4.9.135-1.13pre #51
    [   12.709577] Hardware name: BCM97268DV (DT)
    [   12.713736] task: ffffffc136bb6540 task.stack: ffffffc1366cc000
    [   12.719740] PC is at addr_in_gen_pool+0x4/0x48
    [   12.724253] LR is at __dma_free+0x64/0xbc
    [   12.728325] pc : [<ffffff80083c0df8>] lr : [<ffffff80080979e0>] pstate: 60000145
    [   12.735825] sp : ffffffc1366cf990
    [   12.739196] x29: ffffffc1366cf990 x28: ffffffc1366cc000
    [   12.744608] x27: 0000000000000000 x26: ffffffc13a8568c8
    [   12.750020] x25: 0000000000000000 x24: ffffff80098f9000
    [   12.755433] x23: 000000013a5ff000 x22: ffffff8009c57000
    [   12.760844] x21: ffffffc13a856810 x20: 0000000000000000
    [   12.766255] x19: 0000000000001000 x18: 000000000000000a
    [   12.771667] x17: 0000007f917553e0 x16: 0000000000001002
    [   12.777078] x15: 00000000000a36cb x14: ffffff80898feb77
    [   12.782490] x13: ffffffffffffffff x12: 0000000000000030
    [   12.787899] x11: 00000000fffffffe x10: ffffff80098feb7f
    [   12.793311] x9 : 0000000005f5e0ff x8 : 65776f702074736f
    [   12.798723] x7 : 6c2062756820746f x6 : ffffff80098febb1
    [   12.804134] x5 : ffffff800809797c x4 : 0000000000000000
    [   12.809545] x3 : 000000013a5ff000 x2 : 0000000000000fff
    [   12.814955] x1 : ffffff8009c57000 x0 : 0000000000000000
    [   12.820363]
    [   12.821907] Process pml (pid: 1499, stack limit = 0xffffffc1366cc020)
    [   12.828421] Stack: (0xffffffc1366cf990 to 0xffffffc1366d0000)
    [   12.834240] f980:                                   ffffffc1366cf9e0 ffffff80086004d0
    [   12.842186] f9a0: ffffffc13ab08238 0000000000000010 ffffff80097c2218 ffffffc13a856810
    [   12.850131] f9c0: ffffff8009c57000 000000013a5ff000 0000000000000008 000000013a5ff000
    [   12.858076] f9e0: ffffffc1366cfa50 ffffff80085f9250 ffffffc13ab08238 0000000000000004
    [   12.866021] fa00: ffffffc13ab08000 ffffff80097b6000 ffffffc13ab08130 0000000000000001
    [   12.873966] fa20: 0000000000000008 ffffffc13a8568c8 0000000000000000 ffffffc1366cc000
    [   12.881911] fa40: ffffffc13ab08130 0000000000000001 ffffffc1366cfa90 ffffff80085e3de8
    [   12.889856] fa60: ffffffc13ab08238 0000000000000000 ffffffc136b75b00 0000000000000000
    [   12.897801] fa80: 0000000000000010 ffffff80089ccb92 ffffffc1366cfac0 ffffff80084ad040
    [   12.905746] faa0: ffffffc13a856810 0000000000000000 ffffff80084ad004 ffffff80084b91a8
    [   12.913691] fac0: ffffffc1366cfae0 ffffff80084b91b4 ffffffc13a856810 ffffff80080db5cc
    [   12.921636] fae0: ffffffc1366cfb20 ffffff80084b96bc ffffffc13a856810 0000000000000010
    [   12.929581] fb00: ffffffc13a856870 0000000000000000 ffffffc13a856810 ffffff800984d2b8
    [   12.937526] fb20: ffffffc1366cfb50 ffffff80084baa70 ffffff8009932ad0 ffffff800984d260
    [   12.945471] fb40: 0000000000000010 00000002eff0a065 ffffffc1366cfbb0 ffffff80084bafbc
    [   12.953415] fb60: 0000000000000010 0000000000000003 ffffff80098fe000 0000000000000000
    [   12.961360] fb80: ffffff80097b6000 ffffff80097b6dc8 ffffff80098c12b8 ffffff80098c12f8
    [   12.969306] fba0: ffffff8008842000 ffffff80097b6dc8 ffffffc1366cfbd0 ffffff80080e0d88
    [   12.977251] fbc0: 00000000fffffffb ffffff80080e10bc ffffffc1366cfc60 ffffff80080e16a8
    [   12.985196] fbe0: 0000000000000000 0000000000000003 ffffff80097b6000 ffffff80098fe9f0
    [   12.993140] fc00: ffffff80097d4000 ffffff8008983802 0000000000000123 0000000000000040
    [   13.001085] fc20: ffffff8008842000 ffffffc1366cc000 ffffff80089803c2 00000000ffffffff
    [   13.009029] fc40: 0000000000000000 0000000000000000 ffffffc1366cfc60 0000000000040987
    [   13.016974] fc60: ffffffc1366cfcc0 ffffff80080dfd08 0000000000000003 0000000000000004
    [   13.024919] fc80: 0000000000000003 ffffff80098fea08 ffffffc136577ec0 ffffff80089803c2
    [   13.032864] fca0: 0000000000000123 0000000000000001 0000000500000002 0000000000040987
    [   13.040809] fcc0: ffffffc1366cfd00 ffffff80083a89d4 0000000000000004 ffffffc136577ec0
    [   13.048754] fce0: ffffffc136610cc0 ffffffffffffffea ffffffc1366cfeb0 ffffffc136610cd8
    [   13.056700] fd00: ffffffc1366cfd10 ffffff800822a614 ffffffc1366cfd40 ffffff80082295d4
    [   13.064645] fd20: 0000000000000004 ffffffc136577ec0 ffffffc136610cc0 0000000021670570
    [   13.072590] fd40: ffffffc1366cfd80 ffffff80081b5d10 ffffff80097b6000 ffffffc13aae4200
    [   13.080536] fd60: ffffffc1366cfeb0 0000000000000004 0000000021670570 0000000000000004
    [   13.088481] fd80: ffffffc1366cfe30 ffffff80081b6b20 ffffffc13aae4200 0000000000000000
    [   13.096427] fda0: 0000000000000004 0000000021670570 ffffffc1366cfeb0 ffffffc13a838200
    [   13.104371] fdc0: 0000000000000000 000000000000000a ffffff80097b6000 0000000000040987
    [   13.112316] fde0: ffffffc1366cfe20 ffffff80081b3af0 ffffffc13a838200 0000000000000000
    [   13.120261] fe00: ffffffc1366cfe30 ffffff80081b6b0c ffffffc13aae4200 0000000000000000
    [   13.128206] fe20: 0000000000000004 0000000000040987 ffffffc1366cfe70 ffffff80081b7dd8
    [   13.136151] fe40: ffffff80097b6000 ffffffc13aae4200 ffffffc13aae4200 fffffffffffffff7
    [   13.144096] fe60: 0000000021670570 ffffffc13a8c63c0 0000000000000000 ffffff8008083180
    [   13.152042] fe80: ffffffffffffff1d 0000000021670570 ffffffffffffffff 0000007f917ad9b8
    [   13.159986] fea0: 0000000020000000 0000000000000015 0000000000000000 0000000000040987
    [   13.167930] fec0: 0000000000000001 0000000021670570 0000000000000004 0000000000000000
    [   13.175874] fee0: 0000000000000888 0000440110000000 000000000000006d 0000000000000003
    [   13.183819] ff00: 0000000000000040 ffffff80ffffffc8 0000000000000000 0000000000000020
    [   13.191762] ff20: 0000000000000000 0000000000000000 0000000000000001 0000000000000000
    [   13.199707] ff40: 0000000000000000 0000007f917553e0 0000000000000000 0000000000000004
    [   13.207651] ff60: 0000000021670570 0000007f91835480 0000000000000004 0000007f91831638
    [   13.215595] ff80: 0000000000000004 00000000004b0de0 00000000004b0000 0000000000000000
    [   13.223539] ffa0: 0000000000000000 0000007fc92ac8c0 0000007f9175d178 0000007fc92ac8c0
    [   13.231483] ffc0: 0000007f917ad9b8 0000000020000000 0000000000000001 0000000000000040
    [   13.239427] ffe0: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
    [   13.247360] Call trace:
    [   13.249866] Exception stack(0xffffffc1366cf7a0 to 0xffffffc1366cf8d0)
    [   13.256386] f7a0: 0000000000001000 0000007fffffffff ffffffc1366cf990 ffffff80083c0df8
    [   13.264331] f7c0: 0000000060000145 ffffff80089b5001 ffffffc13ab08130 0000000000000001
    [   13.272275] f7e0: 0000000000000008 ffffffc13a8568c8 0000000000000000 0000000000000000
    [   13.280220] f800: ffffffc1366cf960 ffffffc1366cf960 ffffffc1366cf930 00000000ffffffd8
    [   13.288165] f820: ffffff8009931ac0 4554535953425553 4544006273753d4d 3831633d45434956
    [   13.296110] f840: ffff003832313a39 ffffff800845926c ffffffc1366cf880 0000000000040987
    [   13.304054] f860: 0000000000000000 ffffff8009c57000 0000000000000fff 000000013a5ff000
    [   13.311999] f880: 0000000000000000 ffffff800809797c ffffff80098febb1 6c2062756820746f
    [   13.319944] f8a0: 65776f702074736f 0000000005f5e0ff ffffff80098feb7f 00000000fffffffe
    [   13.327884] f8c0: 0000000000000030 ffffffffffffffff
    [   13.332835] [<ffffff80083c0df8>] addr_in_gen_pool+0x4/0x48
    [   13.338398] [<ffffff80086004d0>] xhci_mem_cleanup+0xc8/0x51c
    [   13.344137] [<ffffff80085f9250>] xhci_resume+0x308/0x65c
    [   13.349524] [<ffffff80085e3de8>] xhci_brcm_resume+0x84/0x8c
    [   13.355174] [<ffffff80084ad040>] platform_pm_resume+0x3c/0x64
    [   13.360997] [<ffffff80084b91b4>] dpm_run_callback+0x5c/0x15c
    [   13.366732] [<ffffff80084b96bc>] device_resume+0xc0/0x190
    [   13.372205] [<ffffff80084baa70>] dpm_resume+0x144/0x2cc
    [   13.377504] [<ffffff80084bafbc>] dpm_resume_end+0x20/0x34
    [   13.382980] [<ffffff80080e0d88>] suspend_devices_and_enter+0x104/0x704
    [   13.389585] [<ffffff80080e16a8>] pm_suspend+0x320/0x53c
    [   13.394881] [<ffffff80080dfd08>] state_store+0xbc/0xe0
    [   13.400094] [<ffffff80083a89d4>] kobj_attr_store+0x14/0x24
    [   13.405655] [<ffffff800822a614>] sysfs_kf_write+0x60/0x70
    [   13.411128] [<ffffff80082295d4>] kernfs_fop_write+0x130/0x194
    [   13.416954] [<ffffff80081b5d10>] __vfs_write+0x60/0x150
    [   13.422254] [<ffffff80081b6b20>] vfs_write+0xc8/0x164
    [   13.427376] [<ffffff80081b7dd8>] SyS_write+0x70/0xc8
    [   13.432412] [<ffffff8008083180>] el0_svc_naked+0x34/0x38
    [   13.437800] Code: 92800173 97f6fb9e 17fffff5 d1000442 (f8408c03)
    [   13.444033] ---[ end trace 2effe12f909ce205 ]---
    
    The call path leading to this problem is xhci_mem_cleanup() ->
    dma_free_coherent() -> dma_free_from_pool() -> addr_in_gen_pool. If the
    atomic_pool is NULL, we can't possibly have the address in the atomic
    pool anyway, so guard against that.
    
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index 7a723194ecbe..0207e3764d52 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -158,6 +158,9 @@ int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
 
 bool dma_in_atomic_pool(void *start, size_t size)
 {
+	if (unlikely(!atomic_pool))
+		return false;
+
 	return addr_in_gen_pool(atomic_pool, (unsigned long)start, size);
 }
 

commit 8270f3a11ceef64bdb0ab141180e8d2f17c619ec
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jan 4 18:31:48 2019 +0100

    dma-direct: fix DMA_ATTR_NO_KERNEL_MAPPING for remapped allocations
    
    We need to return a dma_addr_t even if we don't have a kernel mapping.
    Do so by consolidating the phys_to_dma call in a single place and jump
    to it from all the branches that return successfully.
    
    Fixes: bfd56cd60521 ("dma-mapping: support highmem in the generic remap allocator")
    Reported-by: Liviu Dudau <liviu@dudau.co.uk
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Liviu Dudau <liviu@dudau.co.uk>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index 18cc09fc27b9..7a723194ecbe 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -204,8 +204,7 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		ret = dma_alloc_from_pool(size, &page, flags);
 		if (!ret)
 			return NULL;
-		*dma_handle = phys_to_dma(dev, page_to_phys(page));
-		return ret;
+		goto done;
 	}
 
 	page = __dma_direct_alloc_pages(dev, size, dma_handle, flags, attrs);
@@ -215,8 +214,10 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 	/* remove any dirty cache lines on the kernel alias */
 	arch_dma_prep_coherent(page, size);
 
-	if (attrs & DMA_ATTR_NO_KERNEL_MAPPING)
-		return page; /* opaque cookie */
+	if (attrs & DMA_ATTR_NO_KERNEL_MAPPING) {
+		ret = page; /* opaque cookie */
+		goto done;
+	}
 
 	/* create a coherent mapping */
 	ret = dma_common_contiguous_remap(page, size, VM_USERMAP,
@@ -227,9 +228,9 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		return ret;
 	}
 
-	*dma_handle = phys_to_dma(dev, page_to_phys(page));
 	memset(ret, 0, size);
-
+done:
+	*dma_handle = phys_to_dma(dev, page_to_phys(page));
 	return ret;
 }
 

commit 20b105feda8d42360bd690b8fdf6b2f00ba4a993
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Dec 4 08:04:31 2018 -0800

    dma-mapping: remove a pointless memset in dma_atomic_pool_init
    
    We already zero the memory after allocating it from the pool that
    this function fills, and having the memset here in this form means
    we can't support CMA highmem allocations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reported-by: Russell King - ARM Linux <linux@armlinux.org.uk>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index 8a44317cfc1a..18cc09fc27b9 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -121,7 +121,6 @@ int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
 	if (!page)
 		goto out;
 
-	memset(page_address(page), 0, atomic_pool_size);
 	arch_dma_prep_coherent(page, atomic_pool_size);
 
 	atomic_pool = gen_pool_create(PAGE_SHIFT, -1);

commit a1da439cc0d929891f0f7372c9e03530c809068c
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Dec 5 11:14:01 2018 +0100

    dma-mapping: fix lack of DMA address assignment in generic remap allocator
    
    Commit bfd56cd60521 ("dma-mapping: support highmem in the generic remap
    allocator") replaced dma_direct_alloc_pages() with __dma_direct_alloc_pages(),
    which doesn't set dma_handle and zero allocated memory. Fix it by doing this
    directly in the caller function.
    
    Fixes: bfd56cd60521 ("dma-mapping: support highmem in the generic remap allocator")
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Thierry Reding <treding@nvidia.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index 68a64e3ff6a1..8a44317cfc1a 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -223,8 +223,14 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 	ret = dma_common_contiguous_remap(page, size, VM_USERMAP,
 			arch_dma_mmap_pgprot(dev, PAGE_KERNEL, attrs),
 			__builtin_return_address(0));
-	if (!ret)
+	if (!ret) {
 		__dma_direct_free_pages(dev, size, page);
+		return ret;
+	}
+
+	*dma_handle = phys_to_dma(dev, page_to_phys(page));
+	memset(ret, 0, size);
+
 	return ret;
 }
 

commit e440e26a0251b054243b3db6356f3869a9f9c2d1
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Nov 4 17:51:50 2018 +0100

    dma-remap: support DMA_ATTR_NO_KERNEL_MAPPING
    
    Do not waste vmalloc space on allocations that do not require a mapping
    into the kernel address space.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index dcc82dd668f8..68a64e3ff6a1 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -200,7 +200,8 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 
 	size = PAGE_ALIGN(size);
 
-	if (!gfpflags_allow_blocking(flags)) {
+	if (!gfpflags_allow_blocking(flags) &&
+	    !(attrs & DMA_ATTR_NO_KERNEL_MAPPING)) {
 		ret = dma_alloc_from_pool(size, &page, flags);
 		if (!ret)
 			return NULL;
@@ -215,6 +216,9 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 	/* remove any dirty cache lines on the kernel alias */
 	arch_dma_prep_coherent(page, size);
 
+	if (attrs & DMA_ATTR_NO_KERNEL_MAPPING)
+		return page; /* opaque cookie */
+
 	/* create a coherent mapping */
 	ret = dma_common_contiguous_remap(page, size, VM_USERMAP,
 			arch_dma_mmap_pgprot(dev, PAGE_KERNEL, attrs),
@@ -227,7 +231,10 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 void arch_dma_free(struct device *dev, size_t size, void *vaddr,
 		dma_addr_t dma_handle, unsigned long attrs)
 {
-	if (!dma_free_from_pool(vaddr, PAGE_ALIGN(size))) {
+	if (attrs & DMA_ATTR_NO_KERNEL_MAPPING) {
+		/* vaddr is a struct page cookie, not a kernel address */
+		__dma_direct_free_pages(dev, size, vaddr);
+	} else if (!dma_free_from_pool(vaddr, PAGE_ALIGN(size))) {
 		phys_addr_t phys = dma_to_phys(dev, dma_handle);
 		struct page *page = pfn_to_page(__phys_to_pfn(phys));
 

commit bfd56cd605219d90b210a5377fca31a644efe95c
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Nov 4 17:38:39 2018 +0100

    dma-mapping: support highmem in the generic remap allocator
    
    By using __dma_direct_alloc_pages we can deal entirely with struct page
    instead of having to derive a kernel virtual address.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index b32bb08f96ae..dcc82dd668f8 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -196,7 +196,7 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		gfp_t flags, unsigned long attrs)
 {
 	struct page *page = NULL;
-	void *ret, *kaddr;
+	void *ret;
 
 	size = PAGE_ALIGN(size);
 
@@ -208,10 +208,9 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 		return ret;
 	}
 
-	kaddr = dma_direct_alloc_pages(dev, size, dma_handle, flags, attrs);
-	if (!kaddr)
+	page = __dma_direct_alloc_pages(dev, size, dma_handle, flags, attrs);
+	if (!page)
 		return NULL;
-	page = virt_to_page(kaddr);
 
 	/* remove any dirty cache lines on the kernel alias */
 	arch_dma_prep_coherent(page, size);
@@ -221,7 +220,7 @@ void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
 			arch_dma_mmap_pgprot(dev, PAGE_KERNEL, attrs),
 			__builtin_return_address(0));
 	if (!ret)
-		dma_direct_free_pages(dev, size, kaddr, *dma_handle, attrs);
+		__dma_direct_free_pages(dev, size, page);
 	return ret;
 }
 
@@ -229,10 +228,11 @@ void arch_dma_free(struct device *dev, size_t size, void *vaddr,
 		dma_addr_t dma_handle, unsigned long attrs)
 {
 	if (!dma_free_from_pool(vaddr, PAGE_ALIGN(size))) {
-		void *kaddr = phys_to_virt(dma_to_phys(dev, dma_handle));
+		phys_addr_t phys = dma_to_phys(dev, dma_handle);
+		struct page *page = pfn_to_page(__phys_to_pfn(phys));
 
 		vunmap(vaddr);
-		dma_direct_free_pages(dev, size, kaddr, dma_handle, attrs);
+		__dma_direct_free_pages(dev, size, page);
 	}
 }
 

commit 0c3b3171ceccb8830c2bb5adff1b4e9b204c1450
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Nov 4 20:29:28 2018 +0100

    dma-mapping: move the arm64 noncoherent alloc/free support to common code
    
    The arm64 codebase to implement coherent dma allocation for architectures
    with non-coherent DMA is a good start for a generic implementation, given
    that is uses the generic remap helpers, provides the atomic pool for
    allocations that can't sleep and still is realtively simple and well
    tested.  Move it to kernel/dma and allow architectures to opt into it
    using a config symbol.  Architectures just need to provide a new
    arch_dma_prep_coherent helper to writeback an invalidate the caches
    for any memory that gets remapped for uncached access.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
index a15c393ea4e5..b32bb08f96ae 100644
--- a/kernel/dma/remap.c
+++ b/kernel/dma/remap.c
@@ -1,8 +1,13 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
+ * Copyright (C) 2012 ARM Ltd.
  * Copyright (c) 2014 The Linux Foundation
  */
-#include <linux/dma-mapping.h>
+#include <linux/dma-direct.h>
+#include <linux/dma-noncoherent.h>
+#include <linux/dma-contiguous.h>
+#include <linux/init.h>
+#include <linux/genalloc.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
 
@@ -86,3 +91,154 @@ void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags)
 	unmap_kernel_range((unsigned long)cpu_addr, PAGE_ALIGN(size));
 	vunmap(cpu_addr);
 }
+
+#ifdef CONFIG_DMA_DIRECT_REMAP
+static struct gen_pool *atomic_pool __ro_after_init;
+
+#define DEFAULT_DMA_COHERENT_POOL_SIZE  SZ_256K
+static size_t atomic_pool_size __initdata = DEFAULT_DMA_COHERENT_POOL_SIZE;
+
+static int __init early_coherent_pool(char *p)
+{
+	atomic_pool_size = memparse(p, &p);
+	return 0;
+}
+early_param("coherent_pool", early_coherent_pool);
+
+int __init dma_atomic_pool_init(gfp_t gfp, pgprot_t prot)
+{
+	unsigned int pool_size_order = get_order(atomic_pool_size);
+	unsigned long nr_pages = atomic_pool_size >> PAGE_SHIFT;
+	struct page *page;
+	void *addr;
+	int ret;
+
+	if (dev_get_cma_area(NULL))
+		page = dma_alloc_from_contiguous(NULL, nr_pages,
+						 pool_size_order, false);
+	else
+		page = alloc_pages(gfp, pool_size_order);
+	if (!page)
+		goto out;
+
+	memset(page_address(page), 0, atomic_pool_size);
+	arch_dma_prep_coherent(page, atomic_pool_size);
+
+	atomic_pool = gen_pool_create(PAGE_SHIFT, -1);
+	if (!atomic_pool)
+		goto free_page;
+
+	addr = dma_common_contiguous_remap(page, atomic_pool_size, VM_USERMAP,
+					   prot, __builtin_return_address(0));
+	if (!addr)
+		goto destroy_genpool;
+
+	ret = gen_pool_add_virt(atomic_pool, (unsigned long)addr,
+				page_to_phys(page), atomic_pool_size, -1);
+	if (ret)
+		goto remove_mapping;
+	gen_pool_set_algo(atomic_pool, gen_pool_first_fit_order_align, NULL);
+
+	pr_info("DMA: preallocated %zu KiB pool for atomic allocations\n",
+		atomic_pool_size / 1024);
+	return 0;
+
+remove_mapping:
+	dma_common_free_remap(addr, atomic_pool_size, VM_USERMAP);
+destroy_genpool:
+	gen_pool_destroy(atomic_pool);
+	atomic_pool = NULL;
+free_page:
+	if (!dma_release_from_contiguous(NULL, page, nr_pages))
+		__free_pages(page, pool_size_order);
+out:
+	pr_err("DMA: failed to allocate %zu KiB pool for atomic coherent allocation\n",
+		atomic_pool_size / 1024);
+	return -ENOMEM;
+}
+
+bool dma_in_atomic_pool(void *start, size_t size)
+{
+	return addr_in_gen_pool(atomic_pool, (unsigned long)start, size);
+}
+
+void *dma_alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)
+{
+	unsigned long val;
+	void *ptr = NULL;
+
+	if (!atomic_pool) {
+		WARN(1, "coherent pool not initialised!\n");
+		return NULL;
+	}
+
+	val = gen_pool_alloc(atomic_pool, size);
+	if (val) {
+		phys_addr_t phys = gen_pool_virt_to_phys(atomic_pool, val);
+
+		*ret_page = pfn_to_page(__phys_to_pfn(phys));
+		ptr = (void *)val;
+		memset(ptr, 0, size);
+	}
+
+	return ptr;
+}
+
+bool dma_free_from_pool(void *start, size_t size)
+{
+	if (!dma_in_atomic_pool(start, size))
+		return false;
+	gen_pool_free(atomic_pool, (unsigned long)start, size);
+	return true;
+}
+
+void *arch_dma_alloc(struct device *dev, size_t size, dma_addr_t *dma_handle,
+		gfp_t flags, unsigned long attrs)
+{
+	struct page *page = NULL;
+	void *ret, *kaddr;
+
+	size = PAGE_ALIGN(size);
+
+	if (!gfpflags_allow_blocking(flags)) {
+		ret = dma_alloc_from_pool(size, &page, flags);
+		if (!ret)
+			return NULL;
+		*dma_handle = phys_to_dma(dev, page_to_phys(page));
+		return ret;
+	}
+
+	kaddr = dma_direct_alloc_pages(dev, size, dma_handle, flags, attrs);
+	if (!kaddr)
+		return NULL;
+	page = virt_to_page(kaddr);
+
+	/* remove any dirty cache lines on the kernel alias */
+	arch_dma_prep_coherent(page, size);
+
+	/* create a coherent mapping */
+	ret = dma_common_contiguous_remap(page, size, VM_USERMAP,
+			arch_dma_mmap_pgprot(dev, PAGE_KERNEL, attrs),
+			__builtin_return_address(0));
+	if (!ret)
+		dma_direct_free_pages(dev, size, kaddr, *dma_handle, attrs);
+	return ret;
+}
+
+void arch_dma_free(struct device *dev, size_t size, void *vaddr,
+		dma_addr_t dma_handle, unsigned long attrs)
+{
+	if (!dma_free_from_pool(vaddr, PAGE_ALIGN(size))) {
+		void *kaddr = phys_to_virt(dma_to_phys(dev, dma_handle));
+
+		vunmap(vaddr);
+		dma_direct_free_pages(dev, size, kaddr, dma_handle, attrs);
+	}
+}
+
+long arch_dma_coherent_to_pfn(struct device *dev, void *cpu_addr,
+		dma_addr_t dma_addr)
+{
+	return __phys_to_pfn(dma_to_phys(dev, dma_addr));
+}
+#endif /* CONFIG_DMA_DIRECT_REMAP */

commit f0edfea8ef93ed6cc5f747c46c85c8e53e0798a0
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 24 10:31:08 2018 +0200

    dma-mapping: move the remap helpers to a separate file
    
    The dma remap code only makes sense for not cache coherent architectures
    (or possibly the corner case of highmem CMA allocations) and currently
    is only used by arm, arm64, csky and xtensa.  Split it out into a
    separate file with a separate Kconfig symbol, which gets the right
    copyright notice given that this code was written by Laura Abbott
    working for Code Aurora at that point.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Laura Abbott <labbott@redhat.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/kernel/dma/remap.c b/kernel/dma/remap.c
new file mode 100644
index 000000000000..a15c393ea4e5
--- /dev/null
+++ b/kernel/dma/remap.c
@@ -0,0 +1,88 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2014 The Linux Foundation
+ */
+#include <linux/dma-mapping.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+
+static struct vm_struct *__dma_common_pages_remap(struct page **pages,
+			size_t size, unsigned long vm_flags, pgprot_t prot,
+			const void *caller)
+{
+	struct vm_struct *area;
+
+	area = get_vm_area_caller(size, vm_flags, caller);
+	if (!area)
+		return NULL;
+
+	if (map_vm_area(area, prot, pages)) {
+		vunmap(area->addr);
+		return NULL;
+	}
+
+	return area;
+}
+
+/*
+ * Remaps an array of PAGE_SIZE pages into another vm_area.
+ * Cannot be used in non-sleeping contexts
+ */
+void *dma_common_pages_remap(struct page **pages, size_t size,
+			unsigned long vm_flags, pgprot_t prot,
+			const void *caller)
+{
+	struct vm_struct *area;
+
+	area = __dma_common_pages_remap(pages, size, vm_flags, prot, caller);
+	if (!area)
+		return NULL;
+
+	area->pages = pages;
+
+	return area->addr;
+}
+
+/*
+ * Remaps an allocated contiguous region into another vm_area.
+ * Cannot be used in non-sleeping contexts
+ */
+void *dma_common_contiguous_remap(struct page *page, size_t size,
+			unsigned long vm_flags,
+			pgprot_t prot, const void *caller)
+{
+	int i;
+	struct page **pages;
+	struct vm_struct *area;
+
+	pages = kmalloc(sizeof(struct page *) << get_order(size), GFP_KERNEL);
+	if (!pages)
+		return NULL;
+
+	for (i = 0; i < (size >> PAGE_SHIFT); i++)
+		pages[i] = nth_page(page, i);
+
+	area = __dma_common_pages_remap(pages, size, vm_flags, prot, caller);
+
+	kfree(pages);
+
+	if (!area)
+		return NULL;
+	return area->addr;
+}
+
+/*
+ * Unmaps a range previously mapped by dma_common_*_remap
+ */
+void dma_common_free_remap(void *cpu_addr, size_t size, unsigned long vm_flags)
+{
+	struct vm_struct *area = find_vm_area(cpu_addr);
+
+	if (!area || (area->flags & vm_flags) != vm_flags) {
+		WARN(1, "trying to free invalid coherent area: %p\n", cpu_addr);
+		return;
+	}
+
+	unmap_kernel_range((unsigned long)cpu_addr, PAGE_ALIGN(size));
+	vunmap(cpu_addr);
+}
