commit 298f3db6ee690259927b105d5ad1079563361323
Author: Peter Collingbourne <pcc@google.com>
Date:   Thu Apr 23 09:31:31 2020 -0700

    dma-contiguous: fix comment for dma_release_from_contiguous
    
    Commit 90ae409f9eb3 ("dma-direct: fix zone selection
    after an unaddressable CMA allocation") changed the logic in
    dma_release_from_contiguous to remove the normal pages fallback path,
    but did not update the comment. Fix that.
    
    Signed-off-by: Peter Collingbourne <pcc@google.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index 8bc6f2d670f9..15bc5026c485 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -222,8 +222,8 @@ bool dma_release_from_contiguous(struct device *dev, struct page *pages,
  * @gfp:   Allocation flags.
  *
  * This function allocates contiguous memory buffer for specified device. It
- * first tries to use device specific contiguous memory area if available or
- * the default global one, then tries a fallback allocation of normal pages.
+ * tries to use device specific contiguous memory area if available, or the
+ * default global one.
  *
  * Note that it byapss one-page size of allocations from the global area as
  * the addresses within one page are always contiguous, so there is no need

commit 8c8c5a4994a306c217fd061cbfc5903399fd4c1c
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Fri Jan 10 18:19:33 2020 +0100

    dma-contiguous: CMA: give precedence to cmdline
    
    Although the device tree might contain a reserved-memory DT node
    dedicated as the default CMA pool, users might want to change CMA's
    parameters using the kernel command line for debugging purposes and
    whatnot. Honor this by bypassing the reserved memory CMA setup, which
    will ultimately end up freeing the memblock and allow the command line
    CMA configuration routine to run.
    
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Reviewed-by: Phil Elwell <phil@raspberrypi.org>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index daa4e6eefdde..8bc6f2d670f9 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -302,9 +302,16 @@ static int __init rmem_cma_setup(struct reserved_mem *rmem)
 	phys_addr_t align = PAGE_SIZE << max(MAX_ORDER - 1, pageblock_order);
 	phys_addr_t mask = align - 1;
 	unsigned long node = rmem->fdt_node;
+	bool default_cma = of_get_flat_dt_prop(node, "linux,cma-default", NULL);
 	struct cma *cma;
 	int err;
 
+	if (size_cmdline != -1 && default_cma) {
+		pr_info("Reserved memory: bypass %s node, using cmdline CMA params instead\n",
+			rmem->name);
+		return -EBUSY;
+	}
+
 	if (!of_get_flat_dt_prop(node, "reusable", NULL) ||
 	    of_get_flat_dt_prop(node, "no-map", NULL))
 		return -EINVAL;
@@ -322,7 +329,7 @@ static int __init rmem_cma_setup(struct reserved_mem *rmem)
 	/* Architecture specific contiguous memory fixup. */
 	dma_contiguous_early_fixup(rmem->base, rmem->size);
 
-	if (of_get_flat_dt_prop(node, "linux,cma-default", NULL))
+	if (default_cma)
 		dma_contiguous_set_default(cma);
 
 	rmem->ops = &rmem_cma_ops;

commit ca66536845cd55c6a5fccd82694dcc87ed970780
Author: Shyam Saini <mayhs11saini@gmail.com>
Date:   Sun Oct 20 10:33:22 2019 +0530

    kernel: dma-contiguous: mark CMA parameters __initdata/__initconst
    
    These parameters are only referenced by __init routine calls during
    early boot so they should be marked as __initdata and __initconst
    accordingly.
    
    Signed-off-by: Shyam Saini <mayhs11saini@gmail.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index 69cfb4345388..daa4e6eefdde 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -42,10 +42,11 @@ struct cma *dma_contiguous_default_area;
  * Users, who want to set the size of global CMA area for their system
  * should use cma= kernel parameter.
  */
-static const phys_addr_t size_bytes = (phys_addr_t)CMA_SIZE_MBYTES * SZ_1M;
-static phys_addr_t size_cmdline = -1;
-static phys_addr_t base_cmdline;
-static phys_addr_t limit_cmdline;
+static const phys_addr_t size_bytes __initconst =
+	(phys_addr_t)CMA_SIZE_MBYTES * SZ_1M;
+static phys_addr_t  size_cmdline __initdata = -1;
+static phys_addr_t base_cmdline __initdata;
+static phys_addr_t limit_cmdline __initdata;
 
 static int __init early_cma(char *p)
 {

commit 90ae409f9eb3bcaf38688f9ec22375816053a08e
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Aug 20 11:45:49 2019 +0900

    dma-direct: fix zone selection after an unaddressable CMA allocation
    
    The new dma_alloc_contiguous hides if we allocate CMA or regular
    pages, and thus fails to retry a ZONE_NORMAL allocation if the CMA
    allocation succeeds but isn't addressable.  That means we either fail
    outright or dip into a small zone that might not succeed either.
    
    Thanks to Hillf Danton for debugging this issue.
    
    Fixes: b1d2dc009dec ("dma-contiguous: add dma_{alloc,free}_contiguous() helpers")
    Reported-by: Tobias Klausmann <tobias.johannes.klausmann@mni.thm.de>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Tobias Klausmann <tobias.johannes.klausmann@mni.thm.de>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index 2bd410f934b3..69cfb4345388 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -230,9 +230,7 @@ bool dma_release_from_contiguous(struct device *dev, struct page *pages,
  */
 struct page *dma_alloc_contiguous(struct device *dev, size_t size, gfp_t gfp)
 {
-	int node = dev ? dev_to_node(dev) : NUMA_NO_NODE;
-	size_t count = PAGE_ALIGN(size) >> PAGE_SHIFT;
-	size_t align = get_order(PAGE_ALIGN(size));
+	size_t count = size >> PAGE_SHIFT;
 	struct page *page = NULL;
 	struct cma *cma = NULL;
 
@@ -243,14 +241,12 @@ struct page *dma_alloc_contiguous(struct device *dev, size_t size, gfp_t gfp)
 
 	/* CMA can be used only in the context which permits sleeping */
 	if (cma && gfpflags_allow_blocking(gfp)) {
+		size_t align = get_order(size);
 		size_t cma_align = min_t(size_t, align, CONFIG_CMA_ALIGNMENT);
 
 		page = cma_alloc(cma, count, cma_align, gfp & __GFP_NOWARN);
 	}
 
-	/* Fallback allocation of normal pages */
-	if (!page)
-		page = alloc_pages_node(node, gfp, align);
 	return page;
 }
 

commit f46cc0152501e46d1b3aa5e7eade61145070eab0
Author: Nicolin Chen <nicoleotsuka@gmail.com>
Date:   Fri Jul 26 12:34:33 2019 -0700

    dma-contiguous: page-align the size in dma_free_contiguous()
    
    According to the original dma_direct_alloc_pages() code:
    {
            unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
    
            if (!dma_release_from_contiguous(dev, page, count))
                    __free_pages(page, get_order(size));
    }
    
    The count parameter for dma_release_from_contiguous() was page
    aligned before the right-shifting operation, while the new API
    dma_free_contiguous() forgets to have PAGE_ALIGN() at the size.
    
    So this patch simply adds it to prevent any corner case.
    
    Fixes: fdaeec198ada ("dma-contiguous: add dma_{alloc,free}_contiguous() helpers")
    Signed-off-by: Nicolin Chen <nicoleotsuka@gmail.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index ea8259f53eda..2bd410f934b3 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -267,7 +267,8 @@ struct page *dma_alloc_contiguous(struct device *dev, size_t size, gfp_t gfp)
  */
 void dma_free_contiguous(struct device *dev, struct page *page, size_t size)
 {
-	if (!cma_release(dev_get_cma_area(dev), page, size >> PAGE_SHIFT))
+	if (!cma_release(dev_get_cma_area(dev), page,
+			 PAGE_ALIGN(size) >> PAGE_SHIFT))
 		__free_pages(page, get_order(size));
 }
 

commit c6622a425acd1d2f3a443cd39b490a8777b622d7
Author: Nicolin Chen <nicoleotsuka@gmail.com>
Date:   Fri Jul 26 12:34:32 2019 -0700

    dma-contiguous: do not overwrite align in dma_alloc_contiguous()
    
    The dma_alloc_contiguous() limits align at CONFIG_CMA_ALIGNMENT for
    cma_alloc() however it does not restore it for the fallback routine.
    This will result in a size mismatch between the allocation and free
    when running into the fallback routines after cma_alloc() fails, if
    the align is larger than CONFIG_CMA_ALIGNMENT.
    
    This patch adds a cma_align to take care of cma_alloc() and prevent
    the align from being overwritten.
    
    Fixes: fdaeec198ada ("dma-contiguous: add dma_{alloc,free}_contiguous() helpers")
    Reported-by: Dafna Hirschfeld <dafna.hirschfeld@collabora.com>
    Signed-off-by: Nicolin Chen <nicoleotsuka@gmail.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index bfc0c17f2a3d..ea8259f53eda 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -243,8 +243,9 @@ struct page *dma_alloc_contiguous(struct device *dev, size_t size, gfp_t gfp)
 
 	/* CMA can be used only in the context which permits sleeping */
 	if (cma && gfpflags_allow_blocking(gfp)) {
-		align = min_t(size_t, align, CONFIG_CMA_ALIGNMENT);
-		page = cma_alloc(cma, count, align, gfp & __GFP_NOWARN);
+		size_t cma_align = min_t(size_t, align, CONFIG_CMA_ALIGNMENT);
+
+		page = cma_alloc(cma, count, cma_align, gfp & __GFP_NOWARN);
 	}
 
 	/* Fallback allocation of normal pages */

commit bd2e75633c8012fc8a7431c82fda66237133bf7e
Author: Nicolin Chen <nicoleotsuka@gmail.com>
Date:   Thu May 23 21:06:33 2019 -0700

    dma-contiguous: use fallback alloc_pages for single pages
    
    The addresses within a single page are always contiguous, so it's
    not so necessary to always allocate one single page from CMA area.
    Since the CMA area has a limited predefined size of space, it may
    run out of space in heavy use cases, where there might be quite a
    lot CMA pages being allocated for single pages.
    
    However, there is also a concern that a device might care where a
    page comes from -- it might expect the page from CMA area and act
    differently if the page doesn't.
    
    This patch tries to use the fallback alloc_pages path, instead of
    one-page size allocations from the global CMA area in case that a
    device does not have its own CMA area. This'd save resources from
    the CMA global area for more CMA allocations, and also reduce CMA
    fragmentations resulted from trivial allocations.
    
    Signed-off-by: Nicolin Chen <nicoleotsuka@gmail.com>
    Tested-by: dann frazier <dann.frazier@canonical.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index 637b120d647b..bfc0c17f2a3d 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -223,14 +223,23 @@ bool dma_release_from_contiguous(struct device *dev, struct page *pages,
  * This function allocates contiguous memory buffer for specified device. It
  * first tries to use device specific contiguous memory area if available or
  * the default global one, then tries a fallback allocation of normal pages.
+ *
+ * Note that it byapss one-page size of allocations from the global area as
+ * the addresses within one page are always contiguous, so there is no need
+ * to waste CMA pages for that kind; it also helps reduce fragmentations.
  */
 struct page *dma_alloc_contiguous(struct device *dev, size_t size, gfp_t gfp)
 {
 	int node = dev ? dev_to_node(dev) : NUMA_NO_NODE;
 	size_t count = PAGE_ALIGN(size) >> PAGE_SHIFT;
 	size_t align = get_order(PAGE_ALIGN(size));
-	struct cma *cma = dev_get_cma_area(dev);
 	struct page *page = NULL;
+	struct cma *cma = NULL;
+
+	if (dev && dev->cma_area)
+		cma = dev->cma_area;
+	else if (count > 1)
+		cma = dma_contiguous_default_area;
 
 	/* CMA can be used only in the context which permits sleeping */
 	if (cma && gfpflags_allow_blocking(gfp)) {

commit b1d2dc009dece4cd7e629419b52266ba51960a6b
Author: Nicolin Chen <nicoleotsuka@gmail.com>
Date:   Thu May 23 21:06:32 2019 -0700

    dma-contiguous: add dma_{alloc,free}_contiguous() helpers
    
    Both dma_alloc_from_contiguous() and dma_release_from_contiguous() are
    very simply implemented, but requiring callers to pass certain
    parameters like count and align, and taking a boolean parameter to check
    __GFP_NOWARN in the allocation flags. So every function call duplicates
    similar work:
    
            unsigned long order = get_order(size);
            size_t count = size >> PAGE_SHIFT;
    
            page = dma_alloc_from_contiguous(dev, count, order,
                            gfp & __GFP_NOWARN);
    
            [...]
    
            dma_release_from_contiguous(dev, page, size >> PAGE_SHIFT);
    
    Additionally, as CMA can be used only in the context which permits
    sleeping, most of callers do a gfpflags_allow_blocking() check and a
    corresponding fallback allocation of normal pages upon any false result:
    
            if (gfpflags_allow_blocking(flag))
                    page = dma_alloc_from_contiguous();
            if (!page)
                    page = alloc_pages();
    
            [...]
    
            if (!dma_release_from_contiguous(dev, page, count))
                    __free_pages(page, get_order(size));
    
    So this patch simplifies those function calls by abstracting these
    operations into the two new functions: dma_{alloc,free}_contiguous.
    
    As some callers of dma_{alloc,release}_from_contiguous() might be
    complicated, this patch just implements these two new functions to
    kernel/dma/direct.c only as an initial step.
    
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Nicolin Chen <nicoleotsuka@gmail.com>
    Tested-by: dann frazier <dann.frazier@canonical.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index b2a87905846d..637b120d647b 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -214,6 +214,53 @@ bool dma_release_from_contiguous(struct device *dev, struct page *pages,
 	return cma_release(dev_get_cma_area(dev), pages, count);
 }
 
+/**
+ * dma_alloc_contiguous() - allocate contiguous pages
+ * @dev:   Pointer to device for which the allocation is performed.
+ * @size:  Requested allocation size.
+ * @gfp:   Allocation flags.
+ *
+ * This function allocates contiguous memory buffer for specified device. It
+ * first tries to use device specific contiguous memory area if available or
+ * the default global one, then tries a fallback allocation of normal pages.
+ */
+struct page *dma_alloc_contiguous(struct device *dev, size_t size, gfp_t gfp)
+{
+	int node = dev ? dev_to_node(dev) : NUMA_NO_NODE;
+	size_t count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+	size_t align = get_order(PAGE_ALIGN(size));
+	struct cma *cma = dev_get_cma_area(dev);
+	struct page *page = NULL;
+
+	/* CMA can be used only in the context which permits sleeping */
+	if (cma && gfpflags_allow_blocking(gfp)) {
+		align = min_t(size_t, align, CONFIG_CMA_ALIGNMENT);
+		page = cma_alloc(cma, count, align, gfp & __GFP_NOWARN);
+	}
+
+	/* Fallback allocation of normal pages */
+	if (!page)
+		page = alloc_pages_node(node, gfp, align);
+	return page;
+}
+
+/**
+ * dma_free_contiguous() - release allocated pages
+ * @dev:   Pointer to device for which the pages were allocated.
+ * @page:  Pointer to the allocated pages.
+ * @size:  Size of allocated pages.
+ *
+ * This function releases memory allocated by dma_alloc_contiguous(). As the
+ * cma_release returns false when provided pages do not belong to contiguous
+ * area and true otherwise, this function then does a fallback __free_pages()
+ * upon a false-return.
+ */
+void dma_free_contiguous(struct device *dev, struct page *page, size_t size)
+{
+	if (!cma_release(dev_get_cma_area(dev), page, size >> PAGE_SHIFT))
+		__free_pages(page, get_order(size));
+}
+
 /*
  * Support for reserved memory regions defined in device tree
  */

commit a3ceed87b07769fb80ce9dc6b604e515dba14c4b
Author: He Zhe <zhe.he@windriver.com>
Date:   Mon Sep 17 11:24:20 2018 +0800

    dma-mapping: fix panic caused by passing empty cma command line argument
    
    early_cma does not check input argument before passing it to
    simple_strtoull. The argument would be a NULL pointer if "cma", without
    its value, is set in command line and thus causes the following panic.
    
    PANIC: early exception 0xe3 IP 10:ffffffffa3e9db8d error 0 cr2 0x0
    [    0.000000] CPU: 0 PID: 0 Comm: swapper Not tainted 4.19.0-rc3-yocto-standard+ #7
    [    0.000000] RIP: 0010:_parse_integer_fixup_radix+0xd/0x70
    ...
    [    0.000000] Call Trace:
    [    0.000000]  simple_strtoull+0x29/0x70
    [    0.000000]  memparse+0x26/0x90
    [    0.000000]  early_cma+0x17/0x6a
    [    0.000000]  do_early_param+0x57/0x8e
    [    0.000000]  parse_args+0x208/0x320
    [    0.000000]  ? rdinit_setup+0x30/0x30
    [    0.000000]  parse_early_options+0x29/0x2d
    [    0.000000]  ? rdinit_setup+0x30/0x30
    [    0.000000]  parse_early_param+0x36/0x4d
    [    0.000000]  setup_arch+0x336/0x99e
    [    0.000000]  start_kernel+0x6f/0x4e6
    [    0.000000]  x86_64_start_reservations+0x24/0x26
    [    0.000000]  x86_64_start_kernel+0x6f/0x72
    [    0.000000]  secondary_startup_64+0xa4/0xb0
    
    This patch adds a check to prevent the panic.
    
    Signed-off-by: He Zhe <zhe.he@windriver.com>
    Reviewed-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index 286d82329eb0..b2a87905846d 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -49,7 +49,11 @@ static phys_addr_t limit_cmdline;
 
 static int __init early_cma(char *p)
 {
-	pr_debug("%s(%s)\n", __func__, p);
+	if (!p) {
+		pr_err("Config string not provided\n");
+		return -EINVAL;
+	}
+
 	size_cmdline = memparse(p, &p);
 	if (*p != '@')
 		return 0;

commit d834c5ab83febf9624ad3b16c3c348aa1e02014c
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Aug 17 15:49:00 2018 -0700

    kernel/dma: remove unsupported gfp_mask parameter from dma_alloc_from_contiguous()
    
    The CMA memory allocator doesn't support standard gfp flags for memory
    allocation, so there is no point having it as a parameter for
    dma_alloc_from_contiguous() function.  Replace it by a boolean no_warn
    argument, which covers all the underlaying cma_alloc() function
    supports.
    
    This will help to avoid giving false feeling that this function supports
    standard gfp flags and callers can pass __GFP_ZERO to get zeroed buffer,
    what has already been an issue: see commit dd65a941f6ba ("arm64:
    dma-mapping: clear buffers allocated with FORCE_CONTIGUOUS flag").
    
    Link: http://lkml.kernel.org/r/20180709122020eucas1p21a71b092975cb4a3b9954ffc63f699d1~-sqUFoa-h2939329393eucas1p2Y@eucas1p2.samsung.com
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Michał Nazarewicz <mina86@mina86.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index 19ea5d70150c..286d82329eb0 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -178,7 +178,7 @@ int __init dma_contiguous_reserve_area(phys_addr_t size, phys_addr_t base,
  * @dev:   Pointer to device for which the allocation is performed.
  * @count: Requested number of pages.
  * @align: Requested alignment of pages (in PAGE_SIZE order).
- * @gfp_mask: GFP flags to use for this allocation.
+ * @no_warn: Avoid printing message about failed allocation.
  *
  * This function allocates memory buffer for specified device. It uses
  * device specific contiguous memory area if available or the default
@@ -186,13 +186,12 @@ int __init dma_contiguous_reserve_area(phys_addr_t size, phys_addr_t base,
  * function.
  */
 struct page *dma_alloc_from_contiguous(struct device *dev, size_t count,
-				       unsigned int align, gfp_t gfp_mask)
+				       unsigned int align, bool no_warn)
 {
 	if (align > CONFIG_CMA_ALIGNMENT)
 		align = CONFIG_CMA_ALIGNMENT;
 
-	return cma_alloc(dev_get_cma_area(dev), count, align,
-			 gfp_mask & __GFP_NOWARN);
+	return cma_alloc(dev_get_cma_area(dev), count, align, no_warn);
 }
 
 /**

commit 6518202970c1052148daaef9a8096711775e43a2
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Aug 17 15:48:57 2018 -0700

    mm/cma: remove unsupported gfp_mask parameter from cma_alloc()
    
    cma_alloc() doesn't really support gfp flags other than __GFP_NOWARN, so
    convert gfp_mask parameter to boolean no_warn parameter.
    
    This will help to avoid giving false feeling that this function supports
    standard gfp flags and callers can pass __GFP_ZERO to get zeroed buffer,
    what has already been an issue: see commit dd65a941f6ba ("arm64:
    dma-mapping: clear buffers allocated with FORCE_CONTIGUOUS flag").
    
    Link: http://lkml.kernel.org/r/20180709122019eucas1p2340da484acfcc932537e6014f4fd2c29~-sqTPJKij2939229392eucas1p2j@eucas1p2.samsung.com
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Michał Nazarewicz <mina86@mina86.com>
    Acked-by: Laura Abbott <labbott@redhat.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
index d987dcd1bd56..19ea5d70150c 100644
--- a/kernel/dma/contiguous.c
+++ b/kernel/dma/contiguous.c
@@ -191,7 +191,8 @@ struct page *dma_alloc_from_contiguous(struct device *dev, size_t count,
 	if (align > CONFIG_CMA_ALIGNMENT)
 		align = CONFIG_CMA_ALIGNMENT;
 
-	return cma_alloc(dev_get_cma_area(dev), count, align, gfp_mask);
+	return cma_alloc(dev_get_cma_area(dev), count, align,
+			 gfp_mask & __GFP_NOWARN);
 }
 
 /**

commit cf65a0f6f6ff7631ba0ac0513a14ca5b65320d80
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jun 12 19:01:45 2018 +0200

    dma-mapping: move all DMA mapping code to kernel/dma
    
    Currently the code is split over various files with dma- prefixes in the
    lib/ and drives/base directories, and the number of files keeps growing.
    Move them into a single directory to keep the code together and remove
    the file name prefixes.  To match the irq infrastructure this directory
    is placed under the kernel/ directory.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/dma/contiguous.c b/kernel/dma/contiguous.c
new file mode 100644
index 000000000000..d987dcd1bd56
--- /dev/null
+++ b/kernel/dma/contiguous.c
@@ -0,0 +1,278 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Contiguous Memory Allocator for DMA mapping framework
+ * Copyright (c) 2010-2011 by Samsung Electronics.
+ * Written by:
+ *	Marek Szyprowski <m.szyprowski@samsung.com>
+ *	Michal Nazarewicz <mina86@mina86.com>
+ */
+
+#define pr_fmt(fmt) "cma: " fmt
+
+#ifdef CONFIG_CMA_DEBUG
+#ifndef DEBUG
+#  define DEBUG
+#endif
+#endif
+
+#include <asm/page.h>
+#include <asm/dma-contiguous.h>
+
+#include <linux/memblock.h>
+#include <linux/err.h>
+#include <linux/sizes.h>
+#include <linux/dma-contiguous.h>
+#include <linux/cma.h>
+
+#ifdef CONFIG_CMA_SIZE_MBYTES
+#define CMA_SIZE_MBYTES CONFIG_CMA_SIZE_MBYTES
+#else
+#define CMA_SIZE_MBYTES 0
+#endif
+
+struct cma *dma_contiguous_default_area;
+
+/*
+ * Default global CMA area size can be defined in kernel's .config.
+ * This is useful mainly for distro maintainers to create a kernel
+ * that works correctly for most supported systems.
+ * The size can be set in bytes or as a percentage of the total memory
+ * in the system.
+ *
+ * Users, who want to set the size of global CMA area for their system
+ * should use cma= kernel parameter.
+ */
+static const phys_addr_t size_bytes = (phys_addr_t)CMA_SIZE_MBYTES * SZ_1M;
+static phys_addr_t size_cmdline = -1;
+static phys_addr_t base_cmdline;
+static phys_addr_t limit_cmdline;
+
+static int __init early_cma(char *p)
+{
+	pr_debug("%s(%s)\n", __func__, p);
+	size_cmdline = memparse(p, &p);
+	if (*p != '@')
+		return 0;
+	base_cmdline = memparse(p + 1, &p);
+	if (*p != '-') {
+		limit_cmdline = base_cmdline + size_cmdline;
+		return 0;
+	}
+	limit_cmdline = memparse(p + 1, &p);
+
+	return 0;
+}
+early_param("cma", early_cma);
+
+#ifdef CONFIG_CMA_SIZE_PERCENTAGE
+
+static phys_addr_t __init __maybe_unused cma_early_percent_memory(void)
+{
+	struct memblock_region *reg;
+	unsigned long total_pages = 0;
+
+	/*
+	 * We cannot use memblock_phys_mem_size() here, because
+	 * memblock_analyze() has not been called yet.
+	 */
+	for_each_memblock(memory, reg)
+		total_pages += memblock_region_memory_end_pfn(reg) -
+			       memblock_region_memory_base_pfn(reg);
+
+	return (total_pages * CONFIG_CMA_SIZE_PERCENTAGE / 100) << PAGE_SHIFT;
+}
+
+#else
+
+static inline __maybe_unused phys_addr_t cma_early_percent_memory(void)
+{
+	return 0;
+}
+
+#endif
+
+/**
+ * dma_contiguous_reserve() - reserve area(s) for contiguous memory handling
+ * @limit: End address of the reserved memory (optional, 0 for any).
+ *
+ * This function reserves memory from early allocator. It should be
+ * called by arch specific code once the early allocator (memblock or bootmem)
+ * has been activated and all other subsystems have already allocated/reserved
+ * memory.
+ */
+void __init dma_contiguous_reserve(phys_addr_t limit)
+{
+	phys_addr_t selected_size = 0;
+	phys_addr_t selected_base = 0;
+	phys_addr_t selected_limit = limit;
+	bool fixed = false;
+
+	pr_debug("%s(limit %08lx)\n", __func__, (unsigned long)limit);
+
+	if (size_cmdline != -1) {
+		selected_size = size_cmdline;
+		selected_base = base_cmdline;
+		selected_limit = min_not_zero(limit_cmdline, limit);
+		if (base_cmdline + size_cmdline == limit_cmdline)
+			fixed = true;
+	} else {
+#ifdef CONFIG_CMA_SIZE_SEL_MBYTES
+		selected_size = size_bytes;
+#elif defined(CONFIG_CMA_SIZE_SEL_PERCENTAGE)
+		selected_size = cma_early_percent_memory();
+#elif defined(CONFIG_CMA_SIZE_SEL_MIN)
+		selected_size = min(size_bytes, cma_early_percent_memory());
+#elif defined(CONFIG_CMA_SIZE_SEL_MAX)
+		selected_size = max(size_bytes, cma_early_percent_memory());
+#endif
+	}
+
+	if (selected_size && !dma_contiguous_default_area) {
+		pr_debug("%s: reserving %ld MiB for global area\n", __func__,
+			 (unsigned long)selected_size / SZ_1M);
+
+		dma_contiguous_reserve_area(selected_size, selected_base,
+					    selected_limit,
+					    &dma_contiguous_default_area,
+					    fixed);
+	}
+}
+
+/**
+ * dma_contiguous_reserve_area() - reserve custom contiguous area
+ * @size: Size of the reserved area (in bytes),
+ * @base: Base address of the reserved area optional, use 0 for any
+ * @limit: End address of the reserved memory (optional, 0 for any).
+ * @res_cma: Pointer to store the created cma region.
+ * @fixed: hint about where to place the reserved area
+ *
+ * This function reserves memory from early allocator. It should be
+ * called by arch specific code once the early allocator (memblock or bootmem)
+ * has been activated and all other subsystems have already allocated/reserved
+ * memory. This function allows to create custom reserved areas for specific
+ * devices.
+ *
+ * If @fixed is true, reserve contiguous area at exactly @base.  If false,
+ * reserve in range from @base to @limit.
+ */
+int __init dma_contiguous_reserve_area(phys_addr_t size, phys_addr_t base,
+				       phys_addr_t limit, struct cma **res_cma,
+				       bool fixed)
+{
+	int ret;
+
+	ret = cma_declare_contiguous(base, size, limit, 0, 0, fixed,
+					"reserved", res_cma);
+	if (ret)
+		return ret;
+
+	/* Architecture specific contiguous memory fixup. */
+	dma_contiguous_early_fixup(cma_get_base(*res_cma),
+				cma_get_size(*res_cma));
+
+	return 0;
+}
+
+/**
+ * dma_alloc_from_contiguous() - allocate pages from contiguous area
+ * @dev:   Pointer to device for which the allocation is performed.
+ * @count: Requested number of pages.
+ * @align: Requested alignment of pages (in PAGE_SIZE order).
+ * @gfp_mask: GFP flags to use for this allocation.
+ *
+ * This function allocates memory buffer for specified device. It uses
+ * device specific contiguous memory area if available or the default
+ * global one. Requires architecture specific dev_get_cma_area() helper
+ * function.
+ */
+struct page *dma_alloc_from_contiguous(struct device *dev, size_t count,
+				       unsigned int align, gfp_t gfp_mask)
+{
+	if (align > CONFIG_CMA_ALIGNMENT)
+		align = CONFIG_CMA_ALIGNMENT;
+
+	return cma_alloc(dev_get_cma_area(dev), count, align, gfp_mask);
+}
+
+/**
+ * dma_release_from_contiguous() - release allocated pages
+ * @dev:   Pointer to device for which the pages were allocated.
+ * @pages: Allocated pages.
+ * @count: Number of allocated pages.
+ *
+ * This function releases memory allocated by dma_alloc_from_contiguous().
+ * It returns false when provided pages do not belong to contiguous area and
+ * true otherwise.
+ */
+bool dma_release_from_contiguous(struct device *dev, struct page *pages,
+				 int count)
+{
+	return cma_release(dev_get_cma_area(dev), pages, count);
+}
+
+/*
+ * Support for reserved memory regions defined in device tree
+ */
+#ifdef CONFIG_OF_RESERVED_MEM
+#include <linux/of.h>
+#include <linux/of_fdt.h>
+#include <linux/of_reserved_mem.h>
+
+#undef pr_fmt
+#define pr_fmt(fmt) fmt
+
+static int rmem_cma_device_init(struct reserved_mem *rmem, struct device *dev)
+{
+	dev_set_cma_area(dev, rmem->priv);
+	return 0;
+}
+
+static void rmem_cma_device_release(struct reserved_mem *rmem,
+				    struct device *dev)
+{
+	dev_set_cma_area(dev, NULL);
+}
+
+static const struct reserved_mem_ops rmem_cma_ops = {
+	.device_init	= rmem_cma_device_init,
+	.device_release = rmem_cma_device_release,
+};
+
+static int __init rmem_cma_setup(struct reserved_mem *rmem)
+{
+	phys_addr_t align = PAGE_SIZE << max(MAX_ORDER - 1, pageblock_order);
+	phys_addr_t mask = align - 1;
+	unsigned long node = rmem->fdt_node;
+	struct cma *cma;
+	int err;
+
+	if (!of_get_flat_dt_prop(node, "reusable", NULL) ||
+	    of_get_flat_dt_prop(node, "no-map", NULL))
+		return -EINVAL;
+
+	if ((rmem->base & mask) || (rmem->size & mask)) {
+		pr_err("Reserved memory: incorrect alignment of CMA region\n");
+		return -EINVAL;
+	}
+
+	err = cma_init_reserved_mem(rmem->base, rmem->size, 0, rmem->name, &cma);
+	if (err) {
+		pr_err("Reserved memory: unable to setup CMA region\n");
+		return err;
+	}
+	/* Architecture specific contiguous memory fixup. */
+	dma_contiguous_early_fixup(rmem->base, rmem->size);
+
+	if (of_get_flat_dt_prop(node, "linux,cma-default", NULL))
+		dma_contiguous_set_default(cma);
+
+	rmem->ops = &rmem_cma_ops;
+	rmem->priv = cma;
+
+	pr_info("Reserved memory: created CMA memory pool at %pa, size %ld MiB\n",
+		&rmem->base, (unsigned long)rmem->size / SZ_1M);
+
+	return 0;
+}
+RESERVEDMEM_OF_DECLARE(cma, "shared-dma-pool", rmem_cma_setup);
+#endif
