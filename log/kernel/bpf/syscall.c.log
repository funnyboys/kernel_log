commit 5a764898afec0bc097003e8c3e727792289f76d6
Merge: 9321f1aaf63e 1195c7cebb95
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 10 18:16:22 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Pull networking fixes from David Miller:
    
     1) Restore previous behavior of CAP_SYS_ADMIN wrt loading networking
        BPF programs, from Maciej Żenczykowski.
    
     2) Fix dropped broadcasts in mac80211 code, from Seevalamuthu
        Mariappan.
    
     3) Slay memory leak in nl80211 bss color attribute parsing code, from
        Luca Coelho.
    
     4) Get route from skb properly in ip_route_use_hint(), from Miaohe Lin.
    
     5) Don't allow anything other than ARPHRD_ETHER in llc code, from Eric
        Dumazet.
    
     6) xsk code dips too deeply into DMA mapping implementation internals.
        Add dma_need_sync and use it. From Christoph Hellwig
    
     7) Enforce power-of-2 for BPF ringbuf sizes. From Andrii Nakryiko.
    
     8) Check for disallowed attributes when loading flow dissector BPF
        programs. From Lorenz Bauer.
    
     9) Correct packet injection to L3 tunnel devices via AF_PACKET, from
        Jason A. Donenfeld.
    
    10) Don't advertise checksum offload on ipa devices that don't support
        it. From Alex Elder.
    
    11) Resolve several issues in TCP MD5 signature support. Missing memory
        barriers, bogus options emitted when using syncookies, and failure
        to allow md5 key changes in established states. All from Eric
        Dumazet.
    
    12) Fix interface leak in hsr code, from Taehee Yoo.
    
    13) VF reset fixes in hns3 driver, from Huazhong Tan.
    
    14) Make loopback work again with ipv6 anycast, from David Ahern.
    
    15) Fix TX starvation under high load in fec driver, from Tobias
        Waldekranz.
    
    16) MLD2 payload lengths not checked properly in bridge multicast code,
        from Linus Lüssing.
    
    17) Packet scheduler code that wants to find the inner protocol
        currently only works for one level of VLAN encapsulation. Allow
        Q-in-Q situations to work properly here, from Toke
        Høiland-Jørgensen.
    
    18) Fix route leak in l2tp, from Xin Long.
    
    19) Resolve conflict between the sk->sk_user_data usage of bpf reuseport
        support and various protocols. From Martin KaFai Lau.
    
    20) Fix socket cgroup v2 reference counting in some situations, from
        Cong Wang.
    
    21) Cure memory leak in mlx5 connection tracking offload support, from
        Eli Britstein.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net: (146 commits)
      mlxsw: pci: Fix use-after-free in case of failed devlink reload
      mlxsw: spectrum_router: Remove inappropriate usage of WARN_ON()
      net: macb: fix call to pm_runtime in the suspend/resume functions
      net: macb: fix macb_suspend() by removing call to netif_carrier_off()
      net: macb: fix macb_get/set_wol() when moving to phylink
      net: macb: mark device wake capable when "magic-packet" property present
      net: macb: fix wakeup test in runtime suspend/resume routines
      bnxt_en: fix NULL dereference in case SR-IOV configuration fails
      libbpf: Fix libbpf hashmap on (I)LP32 architectures
      net/mlx5e: CT: Fix memory leak in cleanup
      net/mlx5e: Fix port buffers cell size value
      net/mlx5e: Fix 50G per lane indication
      net/mlx5e: Fix CPU mapping after function reload to avoid aRFS RX crash
      net/mlx5e: Fix VXLAN configuration restore after function reload
      net/mlx5e: Fix usage of rcu-protected pointer
      net/mxl5e: Verify that rpriv is not NULL
      net/mlx5: E-Switch, Fix vlan or qos setting in legacy mode
      net/mlx5: Fix eeprom support for SFP module
      cgroup: Fix sock_cgroup_data on big-endian.
      selftests: bpf: Fix detach from sockmap tests
      ...

commit 63960260457a02af2a6cb35d75e6bdb17299c882
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Jul 2 15:45:23 2020 -0700

    bpf: Check correct cred for CAP_SYSLOG in bpf_dump_raw_ok()
    
    When evaluating access control over kallsyms visibility, credentials at
    open() time need to be used, not the "current" creds (though in BPF's
    case, this has likely always been the same). Plumb access to associated
    file->f_cred down through bpf_dump_raw_ok() and its callers now that
    kallsysm_show_value() has been refactored to take struct cred.
    
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: bpf@vger.kernel.org
    Cc: stable@vger.kernel.org
    Fixes: 7105e828c087 ("bpf: allow for correlation of maps and helpers in dump")
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 8da159936bab..859053ddf05b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3139,7 +3139,8 @@ static const struct bpf_map *bpf_map_from_imm(const struct bpf_prog *prog,
 	return NULL;
 }
 
-static struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog)
+static struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog,
+					      const struct cred *f_cred)
 {
 	const struct bpf_map *map;
 	struct bpf_insn *insns;
@@ -3165,7 +3166,7 @@ static struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog)
 		    code == (BPF_JMP | BPF_CALL_ARGS)) {
 			if (code == (BPF_JMP | BPF_CALL_ARGS))
 				insns[i].code = BPF_JMP | BPF_CALL;
-			if (!bpf_dump_raw_ok())
+			if (!bpf_dump_raw_ok(f_cred))
 				insns[i].imm = 0;
 			continue;
 		}
@@ -3221,7 +3222,8 @@ static int set_info_rec_size(struct bpf_prog_info *info)
 	return 0;
 }
 
-static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
+static int bpf_prog_get_info_by_fd(struct file *file,
+				   struct bpf_prog *prog,
 				   const union bpf_attr *attr,
 				   union bpf_attr __user *uattr)
 {
@@ -3290,11 +3292,11 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		struct bpf_insn *insns_sanitized;
 		bool fault;
 
-		if (prog->blinded && !bpf_dump_raw_ok()) {
+		if (prog->blinded && !bpf_dump_raw_ok(file->f_cred)) {
 			info.xlated_prog_insns = 0;
 			goto done;
 		}
-		insns_sanitized = bpf_insn_prepare_dump(prog);
+		insns_sanitized = bpf_insn_prepare_dump(prog, file->f_cred);
 		if (!insns_sanitized)
 			return -ENOMEM;
 		uinsns = u64_to_user_ptr(info.xlated_prog_insns);
@@ -3328,7 +3330,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	}
 
 	if (info.jited_prog_len && ulen) {
-		if (bpf_dump_raw_ok()) {
+		if (bpf_dump_raw_ok(file->f_cred)) {
 			uinsns = u64_to_user_ptr(info.jited_prog_insns);
 			ulen = min_t(u32, info.jited_prog_len, ulen);
 
@@ -3363,7 +3365,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	ulen = info.nr_jited_ksyms;
 	info.nr_jited_ksyms = prog->aux->func_cnt ? : 1;
 	if (ulen) {
-		if (bpf_dump_raw_ok()) {
+		if (bpf_dump_raw_ok(file->f_cred)) {
 			unsigned long ksym_addr;
 			u64 __user *user_ksyms;
 			u32 i;
@@ -3394,7 +3396,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	ulen = info.nr_jited_func_lens;
 	info.nr_jited_func_lens = prog->aux->func_cnt ? : 1;
 	if (ulen) {
-		if (bpf_dump_raw_ok()) {
+		if (bpf_dump_raw_ok(file->f_cred)) {
 			u32 __user *user_lens;
 			u32 func_len, i;
 
@@ -3451,7 +3453,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	else
 		info.nr_jited_line_info = 0;
 	if (info.nr_jited_line_info && ulen) {
-		if (bpf_dump_raw_ok()) {
+		if (bpf_dump_raw_ok(file->f_cred)) {
 			__u64 __user *user_linfo;
 			u32 i;
 
@@ -3497,7 +3499,8 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	return 0;
 }
 
-static int bpf_map_get_info_by_fd(struct bpf_map *map,
+static int bpf_map_get_info_by_fd(struct file *file,
+				  struct bpf_map *map,
 				  const union bpf_attr *attr,
 				  union bpf_attr __user *uattr)
 {
@@ -3540,7 +3543,8 @@ static int bpf_map_get_info_by_fd(struct bpf_map *map,
 	return 0;
 }
 
-static int bpf_btf_get_info_by_fd(struct btf *btf,
+static int bpf_btf_get_info_by_fd(struct file *file,
+				  struct btf *btf,
 				  const union bpf_attr *attr,
 				  union bpf_attr __user *uattr)
 {
@@ -3555,7 +3559,8 @@ static int bpf_btf_get_info_by_fd(struct btf *btf,
 	return btf_get_info_by_fd(btf, attr, uattr);
 }
 
-static int bpf_link_get_info_by_fd(struct bpf_link *link,
+static int bpf_link_get_info_by_fd(struct file *file,
+				  struct bpf_link *link,
 				  const union bpf_attr *attr,
 				  union bpf_attr __user *uattr)
 {
@@ -3608,15 +3613,15 @@ static int bpf_obj_get_info_by_fd(const union bpf_attr *attr,
 		return -EBADFD;
 
 	if (f.file->f_op == &bpf_prog_fops)
-		err = bpf_prog_get_info_by_fd(f.file->private_data, attr,
+		err = bpf_prog_get_info_by_fd(f.file, f.file->private_data, attr,
 					      uattr);
 	else if (f.file->f_op == &bpf_map_fops)
-		err = bpf_map_get_info_by_fd(f.file->private_data, attr,
+		err = bpf_map_get_info_by_fd(f.file, f.file->private_data, attr,
 					     uattr);
 	else if (f.file->f_op == &btf_fops)
-		err = bpf_btf_get_info_by_fd(f.file->private_data, attr, uattr);
+		err = bpf_btf_get_info_by_fd(f.file, f.file->private_data, attr, uattr);
 	else if (f.file->f_op == &bpf_link_fops)
-		err = bpf_link_get_info_by_fd(f.file->private_data,
+		err = bpf_link_get_info_by_fd(f.file, f.file->private_data,
 					      attr, uattr);
 	else
 		err = -EINVAL;

commit bb0de3131f4c60a9bf976681e0fe4d1e55c7a821
Author: Lorenz Bauer <lmb@cloudflare.com>
Date:   Mon Jun 29 10:56:28 2020 +0100

    bpf: sockmap: Require attach_bpf_fd when detaching a program
    
    The sockmap code currently ignores the value of attach_bpf_fd when
    detaching a program. This is contrary to the usual behaviour of
    checking that attach_bpf_fd represents the currently attached
    program.
    
    Ensure that attach_bpf_fd is indeed the currently attached
    program. It turns out that all sockmap selftests already do this,
    which indicates that this is unlikely to cause breakage.
    
    Fixes: 604326b41a6f ("bpf, sockmap: convert to generic sk_msg interface")
    Signed-off-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200629095630.7933-5-lmb@cloudflare.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 28c6ef759037..a74fce8ce043 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2893,7 +2893,7 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	switch (ptype) {
 	case BPF_PROG_TYPE_SK_MSG:
 	case BPF_PROG_TYPE_SK_SKB:
-		return sock_map_get_from_fd(attr, NULL);
+		return sock_map_prog_detach(attr, ptype);
 	case BPF_PROG_TYPE_LIRC_MODE2:
 		return lirc_prog_detach(attr);
 	case BPF_PROG_TYPE_FLOW_DISSECTOR:

commit 4ac2add65974e4efafb8d4ccd8fc5660417ea312
Author: Lorenz Bauer <lmb@cloudflare.com>
Date:   Mon Jun 29 10:56:26 2020 +0100

    bpf: flow_dissector: Check value of unused flags to BPF_PROG_DETACH
    
    Using BPF_PROG_DETACH on a flow dissector program supports neither
    attach_flags nor attach_bpf_fd. Yet no value is enforced for them.
    
    Enforce that attach_flags are zero, and require the current program
    to be passed via attach_bpf_fd. This allows us to remove the check
    for CAP_SYS_ADMIN, since userspace can now no longer remove
    arbitrary flow dissector programs.
    
    Fixes: b27f7bb590ba ("flow_dissector: Move out netns_bpf prog callbacks")
    Signed-off-by: Lorenz Bauer <lmb@cloudflare.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200629095630.7933-3-lmb@cloudflare.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 7d946435587d..28c6ef759037 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2897,9 +2897,7 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_PROG_TYPE_LIRC_MODE2:
 		return lirc_prog_detach(attr);
 	case BPF_PROG_TYPE_FLOW_DISSECTOR:
-		if (!capable(CAP_NET_ADMIN))
-			return -EPERM;
-		return netns_bpf_prog_detach(attr);
+		return netns_bpf_prog_detach(attr, ptype);
 	case BPF_PROG_TYPE_CGROUP_DEVICE:
 	case BPF_PROG_TYPE_CGROUP_SKB:
 	case BPF_PROG_TYPE_CGROUP_SOCK:

commit b338cb921e6739ff59ce32f43342779fe5ffa732
Author: Maciej Żenczykowski <maze@google.com>
Date:   Sat Jun 20 14:26:16 2020 -0700

    bpf: Restore behaviour of CAP_SYS_ADMIN allowing the loading of networking bpf programs
    
    This is a fix for a regression in commit 2c78ee898d8f ("bpf: Implement CAP_BPF").
    Before the above commit it was possible to load network bpf programs
    with just the CAP_SYS_ADMIN privilege.
    
    The Android bpfloader happens to run in such a configuration (it has
    SYS_ADMIN but not NET_ADMIN) and creates maps and loads bpf programs
    for later use by Android's netd (which has NET_ADMIN but not SYS_ADMIN).
    
    Fixes: 2c78ee898d8f ("bpf: Implement CAP_BPF")
    Reported-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Maciej Żenczykowski <maze@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: John Stultz <john.stultz@linaro.org>
    Link: https://lore.kernel.org/bpf/20200620212616.93894-1-zenczykowski@gmail.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 8da159936bab..7d946435587d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2121,7 +2121,7 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	    !bpf_capable())
 		return -EPERM;
 
-	if (is_net_admin_prog_type(type) && !capable(CAP_NET_ADMIN))
+	if (is_net_admin_prog_type(type) && !capable(CAP_NET_ADMIN) && !capable(CAP_SYS_ADMIN))
 		return -EPERM;
 	if (is_perfmon_prog_type(type) && !perfmon_capable())
 		return -EPERM;

commit 96144c58abe7ff767e754b5b80995f7b8846d49b
Merge: f82e7b57b5fc bc139119a170
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 13 16:27:13 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Pull networking fixes from David Miller:
    
     1) Fix cfg80211 deadlock, from Johannes Berg.
    
     2) RXRPC fails to send norigications, from David Howells.
    
     3) MPTCP RM_ADDR parsing has an off by one pointer error, fix from
        Geliang Tang.
    
     4) Fix crash when using MSG_PEEK with sockmap, from Anny Hu.
    
     5) The ucc_geth driver needs __netdev_watchdog_up exported, from
        Valentin Longchamp.
    
     6) Fix hashtable memory leak in dccp, from Wang Hai.
    
     7) Fix how nexthops are marked as FDB nexthops, from David Ahern.
    
     8) Fix mptcp races between shutdown and recvmsg, from Paolo Abeni.
    
     9) Fix crashes in tipc_disc_rcv(), from Tuong Lien.
    
    10) Fix link speed reporting in iavf driver, from Brett Creeley.
    
    11) When a channel is used for XSK and then reused again later for XSK,
        we forget to clear out the relevant data structures in mlx5 which
        causes all kinds of problems. Fix from Maxim Mikityanskiy.
    
    12) Fix memory leak in genetlink, from Cong Wang.
    
    13) Disallow sockmap attachments to UDP sockets, it simply won't work.
        From Lorenz Bauer.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net: (83 commits)
      net: ethernet: ti: ale: fix allmulti for nu type ale
      net: ethernet: ti: am65-cpsw-nuss: fix ale parameters init
      net: atm: Remove the error message according to the atomic context
      bpf: Undo internal BPF_PROBE_MEM in BPF insns dump
      libbpf: Support pre-initializing .bss global variables
      tools/bpftool: Fix skeleton codegen
      bpf: Fix memlock accounting for sock_hash
      bpf: sockmap: Don't attach programs to UDP sockets
      bpf: tcp: Recv() should return 0 when the peer socket is closed
      ibmvnic: Flush existing work items before device removal
      genetlink: clean up family attributes allocations
      net: ipa: header pad field only valid for AP->modem endpoint
      net: ipa: program upper nibbles of sequencer type
      net: ipa: fix modem LAN RX endpoint id
      net: ipa: program metadata mask differently
      ionic: add pcie_print_link_status
      rxrpc: Fix race between incoming ACK parser and retransmitter
      net/mlx5: E-Switch, Fix some error pointer dereferences
      net/mlx5: Don't fail driver on failure to create debugfs
      net/mlx5e: CT: Fix ipv6 nat header rewrite actions
      ...

commit 29fcb05bbf1a7008900bb9bee347bdbfc7171036
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Fri Jun 12 17:21:15 2020 -0700

    bpf: Undo internal BPF_PROBE_MEM in BPF insns dump
    
    BPF_PROBE_MEM is kernel-internal implmementation details. When dumping BPF
    instructions to user-space, it needs to be replaced back with BPF_MEM mode.
    
    Fixes: 2a02759ef5f8 ("bpf: Add support for BTF pointers to interpreter")
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200613002115.1632142-1-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4d530b1d5683..e9a3ebc00e08 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3158,6 +3158,7 @@ static struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog)
 	struct bpf_insn *insns;
 	u32 off, type;
 	u64 imm;
+	u8 code;
 	int i;
 
 	insns = kmemdup(prog->insnsi, bpf_prog_insn_size(prog),
@@ -3166,21 +3167,27 @@ static struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog)
 		return insns;
 
 	for (i = 0; i < prog->len; i++) {
-		if (insns[i].code == (BPF_JMP | BPF_TAIL_CALL)) {
+		code = insns[i].code;
+
+		if (code == (BPF_JMP | BPF_TAIL_CALL)) {
 			insns[i].code = BPF_JMP | BPF_CALL;
 			insns[i].imm = BPF_FUNC_tail_call;
 			/* fall-through */
 		}
-		if (insns[i].code == (BPF_JMP | BPF_CALL) ||
-		    insns[i].code == (BPF_JMP | BPF_CALL_ARGS)) {
-			if (insns[i].code == (BPF_JMP | BPF_CALL_ARGS))
+		if (code == (BPF_JMP | BPF_CALL) ||
+		    code == (BPF_JMP | BPF_CALL_ARGS)) {
+			if (code == (BPF_JMP | BPF_CALL_ARGS))
 				insns[i].code = BPF_JMP | BPF_CALL;
 			if (!bpf_dump_raw_ok())
 				insns[i].imm = 0;
 			continue;
 		}
+		if (BPF_CLASS(code) == BPF_LDX && BPF_MODE(code) == BPF_PROBE_MEM) {
+			insns[i].code = BPF_LDX | BPF_SIZE(code) | BPF_MEM;
+			continue;
+		}
 
-		if (insns[i].code != (BPF_LD | BPF_IMM | BPF_DW))
+		if (code != (BPF_LD | BPF_IMM | BPF_DW))
 			continue;
 
 		imm = ((u64)insns[i + 1].imm << 32) | (u32)insns[i].imm;

commit 4382a79b2746faf9db98a34ae1a1cbd364473f75
Merge: 79ca035d2d94 b7e4b65f3fe9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 16:02:54 2020 -0700

    Merge branch 'uaccess.misc' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull misc uaccess updates from Al Viro:
     "Assorted uaccess patches for this cycle - the stuff that didn't fit
      into thematic series"
    
    * 'uaccess.misc' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      bpf: make bpf_check_uarg_tail_zero() use check_zeroed_user()
      x86: kvm_hv_set_msr(): use __put_user() instead of 32bit __clear_user()
      user_regset_copyout_zero(): use clear_user()
      TEST_ACCESS_OK _never_ had been checked anywhere
      x86: switch cp_stat64() to unsafe_put_user()
      binfmt_flat: don't use __put_user()
      binfmt_elf_fdpic: don't use __... uaccess primitives
      binfmt_elf: don't bother with __{put,copy_to}_user()
      pselect6() and friends: take handling the combined 6th/7th args into helper

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4d530b1d5683..c3ae2adaeccd 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -25,7 +25,7 @@
 #include <linux/nospec.h>
 #include <linux/audit.h>
 #include <uapi/linux/btf.h>
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 #include <linux/bpf_lsm.h>
 #include <linux/poll.h>
 #include <linux/bpf-netns.h>

commit cb8e59cc87201af93dfbb6c3dccc8fcad72a09c2
Merge: 2e63f6ce7ed2 065fcfd49763
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 16:27:18 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next
    
    Pull networking updates from David Miller:
    
     1) Allow setting bluetooth L2CAP modes via socket option, from Luiz
        Augusto von Dentz.
    
     2) Add GSO partial support to igc, from Sasha Neftin.
    
     3) Several cleanups and improvements to r8169 from Heiner Kallweit.
    
     4) Add IF_OPER_TESTING link state and use it when ethtool triggers a
        device self-test. From Andrew Lunn.
    
     5) Start moving away from custom driver versions, use the globally
        defined kernel version instead, from Leon Romanovsky.
    
     6) Support GRO vis gro_cells in DSA layer, from Alexander Lobakin.
    
     7) Allow hard IRQ deferral during NAPI, from Eric Dumazet.
    
     8) Add sriov and vf support to hinic, from Luo bin.
    
     9) Support Media Redundancy Protocol (MRP) in the bridging code, from
        Horatiu Vultur.
    
    10) Support netmap in the nft_nat code, from Pablo Neira Ayuso.
    
    11) Allow UDPv6 encapsulation of ESP in the ipsec code, from Sabrina
        Dubroca. Also add ipv6 support for espintcp.
    
    12) Lots of ReST conversions of the networking documentation, from Mauro
        Carvalho Chehab.
    
    13) Support configuration of ethtool rxnfc flows in bcmgenet driver,
        from Doug Berger.
    
    14) Allow to dump cgroup id and filter by it in inet_diag code, from
        Dmitry Yakunin.
    
    15) Add infrastructure to export netlink attribute policies to
        userspace, from Johannes Berg.
    
    16) Several optimizations to sch_fq scheduler, from Eric Dumazet.
    
    17) Fallback to the default qdisc if qdisc init fails because otherwise
        a packet scheduler init failure will make a device inoperative. From
        Jesper Dangaard Brouer.
    
    18) Several RISCV bpf jit optimizations, from Luke Nelson.
    
    19) Correct the return type of the ->ndo_start_xmit() method in several
        drivers, it's netdev_tx_t but many drivers were using
        'int'. From Yunjian Wang.
    
    20) Add an ethtool interface for PHY master/slave config, from Oleksij
        Rempel.
    
    21) Add BPF iterators, from Yonghang Song.
    
    22) Add cable test infrastructure, including ethool interfaces, from
        Andrew Lunn. Marvell PHY driver is the first to support this
        facility.
    
    23) Remove zero-length arrays all over, from Gustavo A. R. Silva.
    
    24) Calculate and maintain an explicit frame size in XDP, from Jesper
        Dangaard Brouer.
    
    25) Add CAP_BPF, from Alexei Starovoitov.
    
    26) Support terse dumps in the packet scheduler, from Vlad Buslov.
    
    27) Support XDP_TX bulking in dpaa2 driver, from Ioana Ciornei.
    
    28) Add devm_register_netdev(), from Bartosz Golaszewski.
    
    29) Minimize qdisc resets, from Cong Wang.
    
    30) Get rid of kernel_getsockopt and kernel_setsockopt in order to
        eliminate set_fs/get_fs calls. From Christoph Hellwig.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net-next: (2517 commits)
      selftests: net: ip_defrag: ignore EPERM
      net_failover: fixed rollback in net_failover_open()
      Revert "tipc: Fix potential tipc_aead refcnt leak in tipc_crypto_rcv"
      Revert "tipc: Fix potential tipc_node refcnt leak in tipc_rcv"
      vmxnet3: allow rx flow hash ops only when rss is enabled
      hinic: add set_channels ethtool_ops support
      selftests/bpf: Add a default $(CXX) value
      tools/bpf: Don't use $(COMPILE.c)
      bpf, selftests: Use bpf_probe_read_kernel
      s390/bpf: Use bcr 0,%0 as tail call nop filler
      s390/bpf: Maintain 8-byte stack alignment
      selftests/bpf: Fix verifier test
      selftests/bpf: Fix sample_cnt shared between two threads
      bpf, selftests: Adapt cls_redirect to call csum_level helper
      bpf: Add csum_level helper for fixing up csum levels
      bpf: Fix up bpf_skb_adjust_room helper's skb csum setting
      sfc: add missing annotation for efx_ef10_try_update_nic_stats_vf()
      crypto/chtls: IPv6 support for inline TLS
      Crypto/chcr: Fixes a coccinile check error
      Crypto/chcr: Fixes compilations warnings
      ...

commit b7e4b65f3fe92abbf4a1f57987a54c820969aebd
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri May 8 00:16:31 2020 -0400

    bpf: make bpf_check_uarg_tail_zero() use check_zeroed_user()
    
    ... rather than open-coding it, and badly, at that.
    
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 64783da34202..41ba746ecbc2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -67,32 +67,19 @@ int bpf_check_uarg_tail_zero(void __user *uaddr,
 			     size_t expected_size,
 			     size_t actual_size)
 {
-	unsigned char __user *addr;
-	unsigned char __user *end;
-	unsigned char val;
-	int err;
+	unsigned char __user *addr = uaddr + expected_size;
+	int res;
 
 	if (unlikely(actual_size > PAGE_SIZE))	/* silly large */
 		return -E2BIG;
 
-	if (unlikely(!access_ok(uaddr, actual_size)))
-		return -EFAULT;
-
 	if (actual_size <= expected_size)
 		return 0;
 
-	addr = uaddr + expected_size;
-	end  = uaddr + actual_size;
-
-	for (; addr < end; addr++) {
-		err = get_user(val, addr);
-		if (err)
-			return err;
-		if (val)
-			return -E2BIG;
-	}
-
-	return 0;
+	res = check_zeroed_user(addr, actual_size - expected_size);
+	if (res < 0)
+		return res;
+	return res ? 0 : -E2BIG;
 }
 
 const struct bpf_map_ops bpf_map_offload_ops = {

commit 041de93ff86fc500aa73e5360039c95f4d31b95f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:52:02 2020 -0700

    mm: remove vmalloc_user_node_flags
    
    Open code it in __bpf_map_area_alloc, which is the only caller.  Also
    clean up __bpf_map_area_alloc to have a single vmalloc call with slightly
    different flags instead of the current two different calls.
    
    For this to compile for the nommu case add a __vmalloc_node_range stub to
    nommu.c.
    
    [akpm@linux-foundation.org: fix nommu.c build]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Link: http://lkml.kernel.org/r/20200414131348.444715-27-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 9c1cf7a87fb3..42c7a42fc9c8 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -25,6 +25,7 @@
 #include <linux/nospec.h>
 #include <linux/audit.h>
 #include <uapi/linux/btf.h>
+#include <asm/pgtable.h>
 #include <linux/bpf_lsm.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
@@ -281,26 +282,29 @@ static void *__bpf_map_area_alloc(u64 size, int numa_node, bool mmapable)
 	 * __GFP_RETRY_MAYFAIL to avoid such situations.
 	 */
 
-	const gfp_t flags = __GFP_NOWARN | __GFP_ZERO;
+	const gfp_t gfp = __GFP_NOWARN | __GFP_ZERO;
+	unsigned int flags = 0;
+	unsigned long align = 1;
 	void *area;
 
 	if (size >= SIZE_MAX)
 		return NULL;
 
 	/* kmalloc()'ed memory can't be mmap()'ed */
-	if (!mmapable && size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER)) {
-		area = kmalloc_node(size, GFP_USER | __GFP_NORETRY | flags,
+	if (mmapable) {
+		BUG_ON(!PAGE_ALIGNED(size));
+		align = SHMLBA;
+		flags = VM_USERMAP;
+	} else if (size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER)) {
+		area = kmalloc_node(size, gfp | GFP_USER | __GFP_NORETRY,
 				    numa_node);
 		if (area != NULL)
 			return area;
 	}
-	if (mmapable) {
-		BUG_ON(!PAGE_ALIGNED(size));
-		return vmalloc_user_node_flags(size, numa_node, GFP_KERNEL |
-					       __GFP_RETRY_MAYFAIL | flags);
-	}
-	return __vmalloc_node(size, 1, GFP_KERNEL | __GFP_RETRY_MAYFAIL | flags,
-			      numa_node, __builtin_return_address(0));
+
+	return __vmalloc_node_range(size, align, VMALLOC_START, VMALLOC_END,
+			gfp | GFP_KERNEL | __GFP_RETRY_MAYFAIL, PAGE_KERNEL,
+			flags, numa_node, __builtin_return_address(0));
 }
 
 void *bpf_map_area_alloc(u64 size, int numa_node)

commit 2b9059489c839e67ca9254913325e18cea11a980
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:51:53 2020 -0700

    mm: remove __vmalloc_node_flags_caller
    
    Just use __vmalloc_node instead which gets and extra argument.  To be able
    to to use __vmalloc_node in all caller make it available outside of
    vmalloc and implement it in nommu.c.
    
    [akpm@linux-foundation.org: fix nommu build]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Link: http://lkml.kernel.org/r/20200414131348.444715-25-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4e6dee19a668..9c1cf7a87fb3 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -299,9 +299,8 @@ static void *__bpf_map_area_alloc(u64 size, int numa_node, bool mmapable)
 		return vmalloc_user_node_flags(size, numa_node, GFP_KERNEL |
 					       __GFP_RETRY_MAYFAIL | flags);
 	}
-	return __vmalloc_node_flags_caller(size, numa_node,
-					   GFP_KERNEL | __GFP_RETRY_MAYFAIL |
-					   flags, __builtin_return_address(0));
+	return __vmalloc_node(size, 1, GFP_KERNEL | __GFP_RETRY_MAYFAIL | flags,
+			      numa_node, __builtin_return_address(0));
 }
 
 void *bpf_map_area_alloc(u64 size, int numa_node)

commit 7f045a49fee04b5662cbdeaf0838f9322ae8c63a
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Sun May 31 10:28:38 2020 +0200

    bpf: Add link-based BPF program attachment to network namespace
    
    Extend bpf() syscall subcommands that operate on bpf_link, that is
    LINK_CREATE, LINK_UPDATE, OBJ_GET_INFO, to accept attach types tied to
    network namespaces (only flow dissector at the moment).
    
    Link-based and prog-based attachment can be used interchangeably, but only
    one can exist at a time. Attempts to attach a link when a prog is already
    attached directly, and the other way around, will be met with -EEXIST.
    Attempts to detach a program when link exists result in -EINVAL.
    
    Attachment of multiple links of same attach type to one netns is not
    supported with the intention to lift the restriction when a use-case
    presents itself. Because of that link create returns -E2BIG when trying to
    create another netns link, when one already exists.
    
    Link-based attachments to netns don't keep a netns alive by holding a ref
    to it. Instead links get auto-detached from netns when the latter is being
    destroyed, using a pernet pre_exit callback.
    
    When auto-detached, link lives in defunct state as long there are open FDs
    for it. -ENOLINK is returned if a user tries to update a defunct link.
    
    Because bpf_link to netns doesn't hold a ref to struct net, special care is
    taken when releasing, updating, or filling link info. The netns might be
    getting torn down when any of these link operations are in progress. That
    is why auto-detach and update/release/fill_info are synchronized by the
    same mutex. Also, link ops have to always check if auto-detach has not
    happened yet and if netns is still alive (refcnt > 0).
    
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200531082846.2117903-5-jakub@cloudflare.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c77ab9c76f7b..e14a842d7e0d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3887,6 +3887,9 @@ static int link_create(union bpf_attr *attr)
 	case BPF_PROG_TYPE_TRACING:
 		ret = tracing_bpf_link_attach(attr, prog);
 		break;
+	case BPF_PROG_TYPE_FLOW_DISSECTOR:
+		ret = netns_bpf_link_create(attr, prog);
+		break;
 	default:
 		ret = -EINVAL;
 	}

commit a3fd7ceee05431d2c51ed86c6cae015d236a51f0
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Sun May 31 10:28:36 2020 +0200

    net: Introduce netns_bpf for BPF programs attached to netns
    
    In order to:
    
     (1) attach more than one BPF program type to netns, or
     (2) support attaching BPF programs to netns with bpf_link, or
     (3) support multi-prog attach points for netns
    
    we will need to keep more state per netns than a single pointer like we
    have now for BPF flow dissector program.
    
    Prepare for the above by extracting netns_bpf that is part of struct net,
    for storing all state related to BPF programs attached to netns.
    
    Turn flow dissector callbacks for querying/attaching/detaching a program
    into generic ones that operate on netns_bpf. Next patch will move the
    generic callbacks into their own module.
    
    This is similar to how it is organized for cgroup with cgroup_bpf.
    
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Stanislav Fomichev <sdf@google.com>
    Link: https://lore.kernel.org/bpf/20200531082846.2117903-3-jakub@cloudflare.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index e83b0818b529..c77ab9c76f7b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -27,6 +27,7 @@
 #include <uapi/linux/btf.h>
 #include <linux/bpf_lsm.h>
 #include <linux/poll.h>
+#include <linux/bpf-netns.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
 			  (map)->map_type == BPF_MAP_TYPE_CGROUP_ARRAY || \
@@ -2868,7 +2869,7 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 		ret = lirc_prog_attach(attr, prog);
 		break;
 	case BPF_PROG_TYPE_FLOW_DISSECTOR:
-		ret = skb_flow_dissector_bpf_prog_attach(attr, prog);
+		ret = netns_bpf_prog_attach(attr, prog);
 		break;
 	case BPF_PROG_TYPE_CGROUP_DEVICE:
 	case BPF_PROG_TYPE_CGROUP_SKB:
@@ -2908,7 +2909,7 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_PROG_TYPE_FLOW_DISSECTOR:
 		if (!capable(CAP_NET_ADMIN))
 			return -EPERM;
-		return skb_flow_dissector_bpf_prog_detach(attr);
+		return netns_bpf_prog_detach(attr);
 	case BPF_PROG_TYPE_CGROUP_DEVICE:
 	case BPF_PROG_TYPE_CGROUP_SKB:
 	case BPF_PROG_TYPE_CGROUP_SOCK:
@@ -2961,7 +2962,7 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_LIRC_MODE2:
 		return lirc_prog_query(attr, uattr);
 	case BPF_FLOW_DISSECTOR:
-		return skb_flow_dissector_prog_query(attr, uattr);
+		return netns_bpf_prog_query(attr, uattr);
 	default:
 		return -EINVAL;
 	}

commit bb2359f4dbe98e8863b4e885fc09269ef4682ec3
Author: Denis Efremov <efremov@linux.com>
Date:   Mon Jun 1 19:28:14 2020 +0300

    bpf: Change kvfree to kfree in generic_map_lookup_batch()
    
    buf_prevkey in generic_map_lookup_batch() is allocated with
    kmalloc(). It's safe to free it with kfree().
    
    Fixes: cb4d03ab499d ("bpf: Add generic support for lookup batch op")
    Signed-off-by: Denis Efremov <efremov@linux.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20200601162814.17426-1-efremov@linux.com
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 9de3540fa90c..e83b0818b529 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1399,7 +1399,7 @@ int generic_map_lookup_batch(struct bpf_map *map,
 
 	buf = kmalloc(map->key_size + value_size, GFP_USER | __GFP_NOWARN);
 	if (!buf) {
-		kvfree(buf_prevkey);
+		kfree(buf_prevkey);
 		return -ENOMEM;
 	}
 

commit 457f44363a8894135c85b7a9afd2bd8196db24ab
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Fri May 29 00:54:20 2020 -0700

    bpf: Implement BPF ring buffer and verifier support for it
    
    This commit adds a new MPSC ring buffer implementation into BPF ecosystem,
    which allows multiple CPUs to submit data to a single shared ring buffer. On
    the consumption side, only single consumer is assumed.
    
    Motivation
    ----------
    There are two distinctive motivators for this work, which are not satisfied by
    existing perf buffer, which prompted creation of a new ring buffer
    implementation.
      - more efficient memory utilization by sharing ring buffer across CPUs;
      - preserving ordering of events that happen sequentially in time, even
      across multiple CPUs (e.g., fork/exec/exit events for a task).
    
    These two problems are independent, but perf buffer fails to satisfy both.
    Both are a result of a choice to have per-CPU perf ring buffer.  Both can be
    also solved by having an MPSC implementation of ring buffer. The ordering
    problem could technically be solved for perf buffer with some in-kernel
    counting, but given the first one requires an MPSC buffer, the same solution
    would solve the second problem automatically.
    
    Semantics and APIs
    ------------------
    Single ring buffer is presented to BPF programs as an instance of BPF map of
    type BPF_MAP_TYPE_RINGBUF. Two other alternatives considered, but ultimately
    rejected.
    
    One way would be to, similar to BPF_MAP_TYPE_PERF_EVENT_ARRAY, make
    BPF_MAP_TYPE_RINGBUF could represent an array of ring buffers, but not enforce
    "same CPU only" rule. This would be more familiar interface compatible with
    existing perf buffer use in BPF, but would fail if application needed more
    advanced logic to lookup ring buffer by arbitrary key. HASH_OF_MAPS addresses
    this with current approach. Additionally, given the performance of BPF
    ringbuf, many use cases would just opt into a simple single ring buffer shared
    among all CPUs, for which current approach would be an overkill.
    
    Another approach could introduce a new concept, alongside BPF map, to
    represent generic "container" object, which doesn't necessarily have key/value
    interface with lookup/update/delete operations. This approach would add a lot
    of extra infrastructure that has to be built for observability and verifier
    support. It would also add another concept that BPF developers would have to
    familiarize themselves with, new syntax in libbpf, etc. But then would really
    provide no additional benefits over the approach of using a map.
    BPF_MAP_TYPE_RINGBUF doesn't support lookup/update/delete operations, but so
    doesn't few other map types (e.g., queue and stack; array doesn't support
    delete, etc).
    
    The approach chosen has an advantage of re-using existing BPF map
    infrastructure (introspection APIs in kernel, libbpf support, etc), being
    familiar concept (no need to teach users a new type of object in BPF program),
    and utilizing existing tooling (bpftool). For common scenario of using
    a single ring buffer for all CPUs, it's as simple and straightforward, as
    would be with a dedicated "container" object. On the other hand, by being
    a map, it can be combined with ARRAY_OF_MAPS and HASH_OF_MAPS map-in-maps to
    implement a wide variety of topologies, from one ring buffer for each CPU
    (e.g., as a replacement for perf buffer use cases), to a complicated
    application hashing/sharding of ring buffers (e.g., having a small pool of
    ring buffers with hashed task's tgid being a look up key to preserve order,
    but reduce contention).
    
    Key and value sizes are enforced to be zero. max_entries is used to specify
    the size of ring buffer and has to be a power of 2 value.
    
    There are a bunch of similarities between perf buffer
    (BPF_MAP_TYPE_PERF_EVENT_ARRAY) and new BPF ring buffer semantics:
      - variable-length records;
      - if there is no more space left in ring buffer, reservation fails, no
        blocking;
      - memory-mappable data area for user-space applications for ease of
        consumption and high performance;
      - epoll notifications for new incoming data;
      - but still the ability to do busy polling for new data to achieve the
        lowest latency, if necessary.
    
    BPF ringbuf provides two sets of APIs to BPF programs:
      - bpf_ringbuf_output() allows to *copy* data from one place to a ring
        buffer, similarly to bpf_perf_event_output();
      - bpf_ringbuf_reserve()/bpf_ringbuf_commit()/bpf_ringbuf_discard() APIs
        split the whole process into two steps. First, a fixed amount of space is
        reserved. If successful, a pointer to a data inside ring buffer data area
        is returned, which BPF programs can use similarly to a data inside
        array/hash maps. Once ready, this piece of memory is either committed or
        discarded. Discard is similar to commit, but makes consumer ignore the
        record.
    
    bpf_ringbuf_output() has disadvantage of incurring extra memory copy, because
    record has to be prepared in some other place first. But it allows to submit
    records of the length that's not known to verifier beforehand. It also closely
    matches bpf_perf_event_output(), so will simplify migration significantly.
    
    bpf_ringbuf_reserve() avoids the extra copy of memory by providing a memory
    pointer directly to ring buffer memory. In a lot of cases records are larger
    than BPF stack space allows, so many programs have use extra per-CPU array as
    a temporary heap for preparing sample. bpf_ringbuf_reserve() avoid this needs
    completely. But in exchange, it only allows a known constant size of memory to
    be reserved, such that verifier can verify that BPF program can't access
    memory outside its reserved record space. bpf_ringbuf_output(), while slightly
    slower due to extra memory copy, covers some use cases that are not suitable
    for bpf_ringbuf_reserve().
    
    The difference between commit and discard is very small. Discard just marks
    a record as discarded, and such records are supposed to be ignored by consumer
    code. Discard is useful for some advanced use-cases, such as ensuring
    all-or-nothing multi-record submission, or emulating temporary malloc()/free()
    within single BPF program invocation.
    
    Each reserved record is tracked by verifier through existing
    reference-tracking logic, similar to socket ref-tracking. It is thus
    impossible to reserve a record, but forget to submit (or discard) it.
    
    bpf_ringbuf_query() helper allows to query various properties of ring buffer.
    Currently 4 are supported:
      - BPF_RB_AVAIL_DATA returns amount of unconsumed data in ring buffer;
      - BPF_RB_RING_SIZE returns the size of ring buffer;
      - BPF_RB_CONS_POS/BPF_RB_PROD_POS returns current logical possition of
        consumer/producer, respectively.
    Returned values are momentarily snapshots of ring buffer state and could be
    off by the time helper returns, so this should be used only for
    debugging/reporting reasons or for implementing various heuristics, that take
    into account highly-changeable nature of some of those characteristics.
    
    One such heuristic might involve more fine-grained control over poll/epoll
    notifications about new data availability in ring buffer. Together with
    BPF_RB_NO_WAKEUP/BPF_RB_FORCE_WAKEUP flags for output/commit/discard helpers,
    it allows BPF program a high degree of control and, e.g., more efficient
    batched notifications. Default self-balancing strategy, though, should be
    adequate for most applications and will work reliable and efficiently already.
    
    Design and implementation
    -------------------------
    This reserve/commit schema allows a natural way for multiple producers, either
    on different CPUs or even on the same CPU/in the same BPF program, to reserve
    independent records and work with them without blocking other producers. This
    means that if BPF program was interruped by another BPF program sharing the
    same ring buffer, they will both get a record reserved (provided there is
    enough space left) and can work with it and submit it independently. This
    applies to NMI context as well, except that due to using a spinlock during
    reservation, in NMI context, bpf_ringbuf_reserve() might fail to get a lock,
    in which case reservation will fail even if ring buffer is not full.
    
    The ring buffer itself internally is implemented as a power-of-2 sized
    circular buffer, with two logical and ever-increasing counters (which might
    wrap around on 32-bit architectures, that's not a problem):
      - consumer counter shows up to which logical position consumer consumed the
        data;
      - producer counter denotes amount of data reserved by all producers.
    
    Each time a record is reserved, producer that "owns" the record will
    successfully advance producer counter. At that point, data is still not yet
    ready to be consumed, though. Each record has 8 byte header, which contains
    the length of reserved record, as well as two extra bits: busy bit to denote
    that record is still being worked on, and discard bit, which might be set at
    commit time if record is discarded. In the latter case, consumer is supposed
    to skip the record and move on to the next one. Record header also encodes
    record's relative offset from the beginning of ring buffer data area (in
    pages). This allows bpf_ringbuf_commit()/bpf_ringbuf_discard() to accept only
    the pointer to the record itself, without requiring also the pointer to ring
    buffer itself. Ring buffer memory location will be restored from record
    metadata header. This significantly simplifies verifier, as well as improving
    API usability.
    
    Producer counter increments are serialized under spinlock, so there is
    a strict ordering between reservations. Commits, on the other hand, are
    completely lockless and independent. All records become available to consumer
    in the order of reservations, but only after all previous records where
    already committed. It is thus possible for slow producers to temporarily hold
    off submitted records, that were reserved later.
    
    Reservation/commit/consumer protocol is verified by litmus tests in
    Documentation/litmus-test/bpf-rb.
    
    One interesting implementation bit, that significantly simplifies (and thus
    speeds up as well) implementation of both producers and consumers is how data
    area is mapped twice contiguously back-to-back in the virtual memory. This
    allows to not take any special measures for samples that have to wrap around
    at the end of the circular buffer data area, because the next page after the
    last data page would be first data page again, and thus the sample will still
    appear completely contiguous in virtual memory. See comment and a simple ASCII
    diagram showing this visually in bpf_ringbuf_area_alloc().
    
    Another feature that distinguishes BPF ringbuf from perf ring buffer is
    a self-pacing notifications of new data being availability.
    bpf_ringbuf_commit() implementation will send a notification of new record
    being available after commit only if consumer has already caught up right up
    to the record being committed. If not, consumer still has to catch up and thus
    will see new data anyways without needing an extra poll notification.
    Benchmarks (see tools/testing/selftests/bpf/benchs/bench_ringbuf.c) show that
    this allows to achieve a very high throughput without having to resort to
    tricks like "notify only every Nth sample", which are necessary with perf
    buffer. For extreme cases, when BPF program wants more manual control of
    notifications, commit/discard/output helpers accept BPF_RB_NO_WAKEUP and
    BPF_RB_FORCE_WAKEUP flags, which give full control over notifications of data
    availability, but require extra caution and diligence in using this API.
    
    Comparison to alternatives
    --------------------------
    Before considering implementing BPF ring buffer from scratch existing
    alternatives in kernel were evaluated, but didn't seem to meet the needs. They
    largely fell into few categores:
      - per-CPU buffers (perf, ftrace, etc), which don't satisfy two motivations
        outlined above (ordering and memory consumption);
      - linked list-based implementations; while some were multi-producer designs,
        consuming these from user-space would be very complicated and most
        probably not performant; memory-mapping contiguous piece of memory is
        simpler and more performant for user-space consumers;
      - io_uring is SPSC, but also requires fixed-sized elements. Naively turning
        SPSC queue into MPSC w/ lock would have subpar performance compared to
        locked reserve + lockless commit, as with BPF ring buffer. Fixed sized
        elements would be too limiting for BPF programs, given existing BPF
        programs heavily rely on variable-sized perf buffer already;
      - specialized implementations (like a new printk ring buffer, [0]) with lots
        of printk-specific limitations and implications, that didn't seem to fit
        well for intended use with BPF programs.
    
      [0] https://lwn.net/Articles/779550/
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200529075424.3139988-2-andriin@fb.com
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2c969a9b90d3..9de3540fa90c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -26,6 +26,7 @@
 #include <linux/audit.h>
 #include <uapi/linux/btf.h>
 #include <linux/bpf_lsm.h>
+#include <linux/poll.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
 			  (map)->map_type == BPF_MAP_TYPE_CGROUP_ARRAY || \
@@ -662,6 +663,16 @@ static int bpf_map_mmap(struct file *filp, struct vm_area_struct *vma)
 	return err;
 }
 
+static __poll_t bpf_map_poll(struct file *filp, struct poll_table_struct *pts)
+{
+	struct bpf_map *map = filp->private_data;
+
+	if (map->ops->map_poll)
+		return map->ops->map_poll(map, filp, pts);
+
+	return EPOLLERR;
+}
+
 const struct file_operations bpf_map_fops = {
 #ifdef CONFIG_PROC_FS
 	.show_fdinfo	= bpf_map_show_fdinfo,
@@ -670,6 +681,7 @@ const struct file_operations bpf_map_fops = {
 	.read		= bpf_dummy_read,
 	.write		= bpf_dummy_write,
 	.mmap		= bpf_map_mmap,
+	.poll		= bpf_map_poll,
 };
 
 int bpf_map_new_fd(struct bpf_map *map, int flags)

commit 1ea0f9120c8ce105ca181b070561df5cbd6bc049
Author: Anton Protopopov <a.s.protopopov@gmail.com>
Date:   Wed May 27 18:56:59 2020 +0000

    bpf: Fix map permissions check
    
    The map_lookup_and_delete_elem() function should check for both FMODE_CAN_WRITE
    and FMODE_CAN_READ permissions because it returns a map element to user space.
    
    Fixes: bd513cd08f10 ("bpf: add MAP_LOOKUP_AND_DELETE_ELEM syscall")
    Signed-off-by: Anton Protopopov <a.s.protopopov@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200527185700.14658-5-a.s.protopopov@gmail.com
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index d13b804ff045..2c969a9b90d3 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1472,7 +1472,8 @@ static int map_lookup_and_delete_elem(union bpf_attr *attr)
 	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
-	if (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {
+	if (!(map_get_sys_perms(map, f) & FMODE_CAN_READ) ||
+	    !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {
 		err = -EPERM;
 		goto err_put;
 	}

commit fe537393b5795ecbe5746eec0e16124bc998a594
Author: Jakub Sitnicki <jakub@cloudflare.com>
Date:   Mon May 25 14:29:28 2020 +0200

    bpf: Fix returned error sign when link doesn't support updates
    
    System calls encode returned errors as negative values. Fix a typo that
    breaks this convention for bpf(LINK_UPDATE) when bpf_link doesn't support
    update operation.
    
    Fixes: f9d041271cf4 ("bpf: Refactor bpf_link update handling")
    Signed-off-by: Jakub Sitnicki <jakub@cloudflare.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200525122928.1164495-1-jakub@cloudflare.com
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index aaa29fb6f363..d13b804ff045 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3924,7 +3924,7 @@ static int link_update(union bpf_attr *attr)
 	if (link->ops->update_prog)
 		ret = link->ops->update_prog(link, new_prog, old_prog);
 	else
-		ret = EINVAL;
+		ret = -EINVAL;
 
 out_put_progs:
 	if (old_prog)

commit 13209a8f7304a34158f4366e8ea07a1965c05ac7
Merge: 316107119f47 98790bbac4db
Author: David S. Miller <davem@davemloft.net>
Date:   Sun May 24 13:47:27 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    The MSCC bug fix in 'net' had to be slightly adjusted because the
    register accesses are done slightly differently in net-next.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit dfeb376dd4cb2c5004aeb625e2475f58a5ff2ea7
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Mon May 18 22:38:24 2020 -0700

    bpf: Prevent mmap()'ing read-only maps as writable
    
    As discussed in [0], it's dangerous to allow mapping BPF map, that's meant to
    be frozen and is read-only on BPF program side, because that allows user-space
    to actually store a writable view to the page even after it is frozen. This is
    exacerbated by BPF verifier making a strong assumption that contents of such
    frozen map will remain unchanged. To prevent this, disallow mapping
    BPF_F_RDONLY_PROG mmap()'able BPF maps as writable, ever.
    
      [0] https://lore.kernel.org/bpf/CAEf4BzYGWYhXdp6BJ7_=9OQPJxQpgug080MMjdSB72i9R+5c6g@mail.gmail.com/
    
    Fixes: fc9702273e2e ("bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY")
    Suggested-by: Jann Horn <jannh@google.com>
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Reviewed-by: Jann Horn <jannh@google.com>
    Link: https://lore.kernel.org/bpf/20200519053824.1089415-1-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2843bbba9ca1..4e6dee19a668 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -623,9 +623,20 @@ static int bpf_map_mmap(struct file *filp, struct vm_area_struct *vma)
 
 	mutex_lock(&map->freeze_mutex);
 
-	if ((vma->vm_flags & VM_WRITE) && map->frozen) {
-		err = -EPERM;
-		goto out;
+	if (vma->vm_flags & VM_WRITE) {
+		if (map->frozen) {
+			err = -EPERM;
+			goto out;
+		}
+		/* map is meant to be read-only, so do not allow mapping as
+		 * writable, because it's possible to leak a writable page
+		 * reference and allows user-space to still modify it after
+		 * freezing, while verifier will assume contents do not change
+		 */
+		if (map->map_flags & BPF_F_RDONLY_PROG) {
+			err = -EACCES;
+			goto out;
+		}
 	}
 
 	/* set default open/close callbacks */

commit 1b66d253610c7f8f257103808a9460223a087469
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue May 19 00:45:45 2020 +0200

    bpf: Add get{peer, sock}name attach types for sock_addr
    
    As stated in 983695fa6765 ("bpf: fix unconnected udp hooks"), the objective
    for the existing cgroup connect/sendmsg/recvmsg/bind BPF hooks is to be
    transparent to applications. In Cilium we make use of these hooks [0] in
    order to enable E-W load balancing for existing Kubernetes service types
    for all Cilium managed nodes in the cluster. Those backends can be local
    or remote. The main advantage of this approach is that it operates as close
    as possible to the socket, and therefore allows to avoid packet-based NAT
    given in connect/sendmsg/recvmsg hooks we only need to xlate sock addresses.
    
    This also allows to expose NodePort services on loopback addresses in the
    host namespace, for example. As another advantage, this also efficiently
    blocks bind requests for applications in the host namespace for exposed
    ports. However, one missing item is that we also need to perform reverse
    xlation for inet{,6}_getname() hooks such that we can return the service
    IP/port tuple back to the application instead of the remote peer address.
    
    The vast majority of applications does not bother about getpeername(), but
    in a few occasions we've seen breakage when validating the peer's address
    since it returns unexpectedly the backend tuple instead of the service one.
    Therefore, this trivial patch allows to customise and adds a getpeername()
    as well as getsockname() BPF cgroup hook for both IPv4 and IPv6 in order
    to address this situation.
    
    Simple example:
    
      # ./cilium/cilium service list
      ID   Frontend     Service Type   Backend
      1    1.2.3.4:80   ClusterIP      1 => 10.0.0.10:80
    
    Before; curl's verbose output example, no getpeername() reverse xlation:
    
      # curl --verbose 1.2.3.4
      * Rebuilt URL to: 1.2.3.4/
      *   Trying 1.2.3.4...
      * TCP_NODELAY set
      * Connected to 1.2.3.4 (10.0.0.10) port 80 (#0)
      > GET / HTTP/1.1
      > Host: 1.2.3.4
      > User-Agent: curl/7.58.0
      > Accept: */*
      [...]
    
    After; with getpeername() reverse xlation:
    
      # curl --verbose 1.2.3.4
      * Rebuilt URL to: 1.2.3.4/
      *   Trying 1.2.3.4...
      * TCP_NODELAY set
      * Connected to 1.2.3.4 (1.2.3.4) port 80 (#0)
      > GET / HTTP/1.1
      >  Host: 1.2.3.4
      > User-Agent: curl/7.58.0
      > Accept: */*
      [...]
    
    Originally, I had both under a BPF_CGROUP_INET{4,6}_GETNAME type and exposed
    peer to the context similar as in inet{,6}_getname() fashion, but API-wise
    this is suboptimal as it always enforces programs having to test for ctx->peer
    which can easily be missed, hence BPF_CGROUP_INET{4,6}_GET{PEER,SOCK}NAME split.
    Similarly, the checked return code is on tnum_range(1, 1), but if a use case
    comes up in future, it can easily be changed to return an error code instead.
    Helper and ctx member access is the same as with connect/sendmsg/etc hooks.
    
      [0] https://github.com/cilium/cilium/blob/master/bpf/bpf_sock.c
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Andrey Ignatov <rdna@fb.com>
    Link: https://lore.kernel.org/bpf/61a479d759b2482ae3efb45546490bacd796a220.1589841594.git.daniel@iogearbox.net

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 57dfc98289d5..431241c74614 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1978,6 +1978,10 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 		case BPF_CGROUP_INET6_BIND:
 		case BPF_CGROUP_INET4_CONNECT:
 		case BPF_CGROUP_INET6_CONNECT:
+		case BPF_CGROUP_INET4_GETPEERNAME:
+		case BPF_CGROUP_INET6_GETPEERNAME:
+		case BPF_CGROUP_INET4_GETSOCKNAME:
+		case BPF_CGROUP_INET6_GETSOCKNAME:
 		case BPF_CGROUP_UDP4_SENDMSG:
 		case BPF_CGROUP_UDP6_SENDMSG:
 		case BPF_CGROUP_UDP4_RECVMSG:
@@ -2767,6 +2771,10 @@ attach_type_to_prog_type(enum bpf_attach_type attach_type)
 	case BPF_CGROUP_INET6_BIND:
 	case BPF_CGROUP_INET4_CONNECT:
 	case BPF_CGROUP_INET6_CONNECT:
+	case BPF_CGROUP_INET4_GETPEERNAME:
+	case BPF_CGROUP_INET6_GETPEERNAME:
+	case BPF_CGROUP_INET4_GETSOCKNAME:
+	case BPF_CGROUP_INET6_GETSOCKNAME:
 	case BPF_CGROUP_UDP4_SENDMSG:
 	case BPF_CGROUP_UDP6_SENDMSG:
 	case BPF_CGROUP_UDP4_RECVMSG:
@@ -2912,6 +2920,10 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_INET6_POST_BIND:
 	case BPF_CGROUP_INET4_CONNECT:
 	case BPF_CGROUP_INET6_CONNECT:
+	case BPF_CGROUP_INET4_GETPEERNAME:
+	case BPF_CGROUP_INET6_GETPEERNAME:
+	case BPF_CGROUP_INET4_GETSOCKNAME:
+	case BPF_CGROUP_INET6_GETSOCKNAME:
 	case BPF_CGROUP_UDP4_SENDMSG:
 	case BPF_CGROUP_UDP6_SENDMSG:
 	case BPF_CGROUP_UDP4_RECVMSG:

commit da07f52d3caf6c24c6dbffb5500f379d819e04bd
Merge: 93d43e58683e f85c1598ddfe
Author: David S. Miller <davem@davemloft.net>
Date:   Fri May 15 13:48:59 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Move the bpf verifier trace check into the new switch statement in
    HEAD.
    
    Resolve the overlapping changes in hinic, where bug fixes overlap
    the addition of VF support.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 2c78ee898d8f10ae6fb2fa23a3fbaec96b1b7366
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Wed May 13 16:03:54 2020 -0700

    bpf: Implement CAP_BPF
    
    Implement permissions as stated in uapi/linux/capability.h
    In order to do that the verifier allow_ptr_leaks flag is split
    into four flags and they are set as:
      env->allow_ptr_leaks = bpf_allow_ptr_leaks();
      env->bypass_spec_v1 = bpf_bypass_spec_v1();
      env->bypass_spec_v4 = bpf_bypass_spec_v4();
      env->bpf_capable = bpf_capable();
    
    The first three currently equivalent to perfmon_capable(), since leaking kernel
    pointers and reading kernel memory via side channel attacks is roughly
    equivalent to reading kernel memory with cap_perfmon.
    
    'bpf_capable' enables bounded loops, precision tracking, bpf to bpf calls and
    other verifier features. 'allow_ptr_leaks' enable ptr leaks, ptr conversions,
    subtraction of pointers. 'bypass_spec_v1' disables speculative analysis in the
    verifier, run time mitigations in bpf array, and enables indirect variable
    access in bpf programs. 'bypass_spec_v4' disables emission of sanitation code
    by the verifier.
    
    That means that the networking BPF program loaded with CAP_BPF + CAP_NET_ADMIN
    will have speculative checks done by the verifier and other spectre mitigation
    applied. Such networking BPF program will not be able to leak kernel pointers
    and will not be able to access arbitrary kernel memory.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200513230355.7858-3-alexei.starovoitov@gmail.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index de2a75500233..79bcd8d056d2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1534,7 +1534,7 @@ static int map_freeze(const union bpf_attr *attr)
 		err = -EBUSY;
 		goto err_put;
 	}
-	if (!capable(CAP_SYS_ADMIN)) {
+	if (!bpf_capable()) {
 		err = -EPERM;
 		goto err_put;
 	}
@@ -2009,6 +2009,55 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 	}
 }
 
+static bool is_net_admin_prog_type(enum bpf_prog_type prog_type)
+{
+	switch (prog_type) {
+	case BPF_PROG_TYPE_SCHED_CLS:
+	case BPF_PROG_TYPE_SCHED_ACT:
+	case BPF_PROG_TYPE_XDP:
+	case BPF_PROG_TYPE_LWT_IN:
+	case BPF_PROG_TYPE_LWT_OUT:
+	case BPF_PROG_TYPE_LWT_XMIT:
+	case BPF_PROG_TYPE_LWT_SEG6LOCAL:
+	case BPF_PROG_TYPE_SK_SKB:
+	case BPF_PROG_TYPE_SK_MSG:
+	case BPF_PROG_TYPE_LIRC_MODE2:
+	case BPF_PROG_TYPE_FLOW_DISSECTOR:
+	case BPF_PROG_TYPE_CGROUP_DEVICE:
+	case BPF_PROG_TYPE_CGROUP_SOCK:
+	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
+	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
+	case BPF_PROG_TYPE_CGROUP_SYSCTL:
+	case BPF_PROG_TYPE_SOCK_OPS:
+	case BPF_PROG_TYPE_EXT: /* extends any prog */
+		return true;
+	case BPF_PROG_TYPE_CGROUP_SKB:
+		/* always unpriv */
+	case BPF_PROG_TYPE_SK_REUSEPORT:
+		/* equivalent to SOCKET_FILTER. need CAP_BPF only */
+	default:
+		return false;
+	}
+}
+
+static bool is_perfmon_prog_type(enum bpf_prog_type prog_type)
+{
+	switch (prog_type) {
+	case BPF_PROG_TYPE_KPROBE:
+	case BPF_PROG_TYPE_TRACEPOINT:
+	case BPF_PROG_TYPE_PERF_EVENT:
+	case BPF_PROG_TYPE_RAW_TRACEPOINT:
+	case BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE:
+	case BPF_PROG_TYPE_TRACING:
+	case BPF_PROG_TYPE_LSM:
+	case BPF_PROG_TYPE_STRUCT_OPS: /* has access to struct sock */
+	case BPF_PROG_TYPE_EXT: /* extends any prog */
+		return true;
+	default:
+		return false;
+	}
+}
+
 /* last field in 'union bpf_attr' used by this command */
 #define	BPF_PROG_LOAD_LAST_FIELD attach_prog_fd
 
@@ -2031,7 +2080,7 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 
 	if (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) &&
 	    (attr->prog_flags & BPF_F_ANY_ALIGNMENT) &&
-	    !capable(CAP_SYS_ADMIN))
+	    !bpf_capable())
 		return -EPERM;
 
 	/* copy eBPF program license from user space */
@@ -2044,11 +2093,16 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	is_gpl = license_is_gpl_compatible(license);
 
 	if (attr->insn_cnt == 0 ||
-	    attr->insn_cnt > (capable(CAP_SYS_ADMIN) ? BPF_COMPLEXITY_LIMIT_INSNS : BPF_MAXINSNS))
+	    attr->insn_cnt > (bpf_capable() ? BPF_COMPLEXITY_LIMIT_INSNS : BPF_MAXINSNS))
 		return -E2BIG;
 	if (type != BPF_PROG_TYPE_SOCKET_FILTER &&
 	    type != BPF_PROG_TYPE_CGROUP_SKB &&
-	    !capable(CAP_SYS_ADMIN))
+	    !bpf_capable())
+		return -EPERM;
+
+	if (is_net_admin_prog_type(type) && !capable(CAP_NET_ADMIN))
+		return -EPERM;
+	if (is_perfmon_prog_type(type) && !perfmon_capable())
 		return -EPERM;
 
 	bpf_prog_load_fixup_attach_type(attr);
@@ -2682,6 +2736,11 @@ static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
 	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
 		return attach_type == prog->expected_attach_type ? 0 : -EINVAL;
 	case BPF_PROG_TYPE_CGROUP_SKB:
+		if (!capable(CAP_NET_ADMIN))
+			/* cg-skb progs can be loaded by unpriv user.
+			 * check permissions at attach time.
+			 */
+			return -EPERM;
 		return prog->enforce_expected_attach_type &&
 			prog->expected_attach_type != attach_type ?
 			-EINVAL : 0;
@@ -2747,9 +2806,6 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	struct bpf_prog *prog;
 	int ret;
 
-	if (!capable(CAP_NET_ADMIN))
-		return -EPERM;
-
 	if (CHECK_ATTR(BPF_PROG_ATTACH))
 		return -EINVAL;
 
@@ -2804,9 +2860,6 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 {
 	enum bpf_prog_type ptype;
 
-	if (!capable(CAP_NET_ADMIN))
-		return -EPERM;
-
 	if (CHECK_ATTR(BPF_PROG_DETACH))
 		return -EINVAL;
 
@@ -2819,6 +2872,8 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_PROG_TYPE_LIRC_MODE2:
 		return lirc_prog_detach(attr);
 	case BPF_PROG_TYPE_FLOW_DISSECTOR:
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
 		return skb_flow_dissector_bpf_prog_detach(attr);
 	case BPF_PROG_TYPE_CGROUP_DEVICE:
 	case BPF_PROG_TYPE_CGROUP_SKB:
@@ -2882,8 +2937,6 @@ static int bpf_prog_test_run(const union bpf_attr *attr,
 	struct bpf_prog *prog;
 	int ret = -ENOTSUPP;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
 	if (CHECK_ATTR(BPF_PROG_TEST_RUN))
 		return -EINVAL;
 
@@ -3184,7 +3237,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	info.run_time_ns = stats.nsecs;
 	info.run_cnt = stats.cnt;
 
-	if (!capable(CAP_SYS_ADMIN)) {
+	if (!bpf_capable()) {
 		info.jited_prog_len = 0;
 		info.xlated_prog_len = 0;
 		info.nr_jited_ksyms = 0;
@@ -3543,7 +3596,7 @@ static int bpf_btf_load(const union bpf_attr *attr)
 	if (CHECK_ATTR(BPF_BTF_LOAD))
 		return -EINVAL;
 
-	if (!capable(CAP_SYS_ADMIN))
+	if (!bpf_capable())
 		return -EPERM;
 
 	return btf_new_fd(attr);
@@ -3766,9 +3819,6 @@ static int link_create(union bpf_attr *attr)
 	struct bpf_prog *prog;
 	int ret;
 
-	if (!capable(CAP_NET_ADMIN))
-		return -EPERM;
-
 	if (CHECK_ATTR(BPF_LINK_CREATE))
 		return -EINVAL;
 
@@ -3817,9 +3867,6 @@ static int link_update(union bpf_attr *attr)
 	u32 flags;
 	int ret;
 
-	if (!capable(CAP_NET_ADMIN))
-		return -EPERM;
-
 	if (CHECK_ATTR(BPF_LINK_UPDATE))
 		return -EINVAL;
 
@@ -3988,7 +4035,7 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	union bpf_attr attr;
 	int err;
 
-	if (sysctl_unprivileged_bpf_disabled && !capable(CAP_SYS_ADMIN))
+	if (sysctl_unprivileged_bpf_disabled && !bpf_capable())
 		return -EPERM;
 
 	err = bpf_check_uarg_tail_zero(uattr, sizeof(attr), size);

commit 6086d29def80edd78f9832ea6eafa74e3818f6a7
Author: Yonghong Song <yhs@fb.com>
Date:   Sat May 9 10:59:09 2020 -0700

    bpf: Add bpf_map iterator
    
    Implement seq_file operations to traverse all bpf_maps.
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20200509175909.2476096-1-yhs@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index a293e88ee01a..de2a75500233 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2934,6 +2934,25 @@ static int bpf_obj_get_next_id(const union bpf_attr *attr,
 	return err;
 }
 
+struct bpf_map *bpf_map_get_curr_or_next(u32 *id)
+{
+	struct bpf_map *map;
+
+	spin_lock_bh(&map_idr_lock);
+again:
+	map = idr_get_next(&map_idr, id);
+	if (map) {
+		map = __bpf_map_inc_not_zero(map, false);
+		if (IS_ERR(map)) {
+			(*id)++;
+			goto again;
+		}
+	}
+	spin_unlock_bh(&map_idr_lock);
+
+	return map;
+}
+
 #define BPF_PROG_GET_FD_BY_ID_LAST_FIELD prog_id
 
 struct bpf_prog *bpf_prog_by_id(u32 id)

commit ac51d99bf81caac8d8881fe52098948110d0de68
Author: Yonghong Song <yhs@fb.com>
Date:   Sat May 9 10:59:05 2020 -0700

    bpf: Create anonymous bpf iterator
    
    A new bpf command BPF_ITER_CREATE is added.
    
    The anonymous bpf iterator is seq_file based.
    The seq_file private data are referenced by targets.
    The bpf_iter infrastructure allocated additional space
    at seq_file->private before the space used by targets
    to store some meta data, e.g.,
      prog:       prog to run
      session_id: an unique id for each opened seq_file
      seq_num:    how many times bpf programs are queried in this session
      done_stop:  an internal state to decide whether bpf program
                  should be called in seq_ops->stop() or not
    
    The seq_num will start from 0 for valid objects.
    The bpf program may see the same seq_num more than once if
     - seq_file buffer overflow happens and the same object
       is retried by bpf_seq_read(), or
     - the bpf program explicitly requests a retry of the
       same object
    
    Since module is not supported for bpf_iter, all target
    registeration happens at __init time, so there is no
    need to change bpf_iter_unreg_target() as it is used
    mostly in error path of the init function at which time
    no bpf iterators have been created yet.
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20200509175905.2475770-1-yhs@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 6ffe2d8fb6c7..a293e88ee01a 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3941,6 +3941,29 @@ static int bpf_enable_stats(union bpf_attr *attr)
 	return -EINVAL;
 }
 
+#define BPF_ITER_CREATE_LAST_FIELD iter_create.flags
+
+static int bpf_iter_create(union bpf_attr *attr)
+{
+	struct bpf_link *link;
+	int err;
+
+	if (CHECK_ATTR(BPF_ITER_CREATE))
+		return -EINVAL;
+
+	if (attr->iter_create.flags)
+		return -EINVAL;
+
+	link = bpf_link_get_from_fd(attr->iter_create.link_fd);
+	if (IS_ERR(link))
+		return PTR_ERR(link);
+
+	err = bpf_iter_new_fd(link);
+	bpf_link_put(link);
+
+	return err;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr;
@@ -4068,6 +4091,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_ENABLE_STATS:
 		err = bpf_enable_stats(&attr);
 		break;
+	case BPF_ITER_CREATE:
+		err = bpf_iter_create(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit de4e05cac46d206f9090051ef09930514bff73e4
Author: Yonghong Song <yhs@fb.com>
Date:   Sat May 9 10:59:01 2020 -0700

    bpf: Support bpf tracing/iter programs for BPF_LINK_CREATE
    
    Given a bpf program, the step to create an anonymous bpf iterator is:
      - create a bpf_iter_link, which combines bpf program and the target.
        In the future, there could be more information recorded in the link.
        A link_fd will be returned to the user space.
      - create an anonymous bpf iterator with the given link_fd.
    
    The bpf_iter_link can be pinned to bpffs mount file system to
    create a file based bpf iterator as well.
    
    The benefit to use of bpf_iter_link:
      - using bpf link simplifies design and implementation as bpf link
        is used for other tracing bpf programs.
      - for file based bpf iterator, bpf_iter_link provides a standard
        way to replace underlying bpf programs.
      - for both anonymous and free based iterators, bpf link query
        capability can be leveraged.
    
    The patch added support of tracing/iter programs for BPF_LINK_CREATE.
    A new link type BPF_LINK_TYPE_ITER is added to facilitate link
    querying. Currently, only prog_id is needed, so there is no
    additional in-kernel show_fdinfo() and fill_link_info() hook
    is needed for BPF_LINK_TYPE_ITER link.
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20200509175901.2475084-1-yhs@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index bb1ab7da6103..6ffe2d8fb6c7 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2729,6 +2729,8 @@ attach_type_to_prog_type(enum bpf_attach_type attach_type)
 	case BPF_CGROUP_GETSOCKOPT:
 	case BPF_CGROUP_SETSOCKOPT:
 		return BPF_PROG_TYPE_CGROUP_SOCKOPT;
+	case BPF_TRACE_ITER:
+		return BPF_PROG_TYPE_TRACING;
 	default:
 		return BPF_PROG_TYPE_UNSPEC;
 	}
@@ -3729,6 +3731,15 @@ static int bpf_map_do_batch(const union bpf_attr *attr,
 	return err;
 }
 
+static int tracing_bpf_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)
+{
+	if (attr->link_create.attach_type == BPF_TRACE_ITER &&
+	    prog->expected_attach_type == BPF_TRACE_ITER)
+		return bpf_iter_link_attach(attr, prog);
+
+	return -EINVAL;
+}
+
 #define BPF_LINK_CREATE_LAST_FIELD link_create.flags
 static int link_create(union bpf_attr *attr)
 {
@@ -3765,6 +3776,9 @@ static int link_create(union bpf_attr *attr)
 	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
 		ret = cgroup_bpf_link_attach(attr, prog);
 		break;
+	case BPF_PROG_TYPE_TRACING:
+		ret = tracing_bpf_link_attach(attr, prog);
+		break;
 	default:
 		ret = -EINVAL;
 	}

commit 138c67677ff5ac0bce7131033c39d52a81e87a60
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Fri May 1 11:56:22 2020 -0700

    bpf: Fix use-after-free of bpf_link when priming half-fails
    
    If bpf_link_prime() succeeds to allocate new anon file, but then fails to
    allocate ID for it, link priming is considered to be failed and user is
    supposed ot be able to directly kfree() bpf_link, because it was never exposed
    to user-space.
    
    But at that point file already keeps a pointer to bpf_link and will eventually
    call bpf_link_release(), so if bpf_link was kfree()'d by caller, that would
    lead to use-after-free.
    
    Fix this by first allocating ID and only then allocating file. Adding ID to
    link_idr is ok, because link at that point still doesn't have its ID set, so
    no user-space process can create a new FD for it.
    
    Fixes: a3b80e107894 ("bpf: Allocate ID for bpf_link")
    Reported-by: syzbot+39b64425f91b5aab714d@syzkaller.appspotmail.com
    Suggested-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20200501185622.3088964-1-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4f34eecec9ce..bb1ab7da6103 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2348,19 +2348,20 @@ int bpf_link_prime(struct bpf_link *link, struct bpf_link_primer *primer)
 	if (fd < 0)
 		return fd;
 
-	file = anon_inode_getfile("bpf_link", &bpf_link_fops, link, O_CLOEXEC);
-	if (IS_ERR(file)) {
-		put_unused_fd(fd);
-		return PTR_ERR(file);
-	}
 
 	id = bpf_link_alloc_id(link);
 	if (id < 0) {
 		put_unused_fd(fd);
-		fput(file);
 		return id;
 	}
 
+	file = anon_inode_getfile("bpf_link", &bpf_link_fops, link, O_CLOEXEC);
+	if (IS_ERR(file)) {
+		bpf_link_free_id(id);
+		put_unused_fd(fd);
+		return PTR_ERR(file);
+	}
+
 	primer->link = link;
 	primer->file = file;
 	primer->fd = fd;

commit d46edd671a147032e22cfeb271a5734703093649
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Apr 30 00:15:04 2020 -0700

    bpf: Sharing bpf runtime stats with BPF_ENABLE_STATS
    
    Currently, sysctl kernel.bpf_stats_enabled controls BPF runtime stats.
    Typical userspace tools use kernel.bpf_stats_enabled as follows:
    
      1. Enable kernel.bpf_stats_enabled;
      2. Check program run_time_ns;
      3. Sleep for the monitoring period;
      4. Check program run_time_ns again, calculate the difference;
      5. Disable kernel.bpf_stats_enabled.
    
    The problem with this approach is that only one userspace tool can toggle
    this sysctl. If multiple tools toggle the sysctl at the same time, the
    measurement may be inaccurate.
    
    To fix this problem while keep backward compatibility, introduce a new
    bpf command BPF_ENABLE_STATS. On success, this command enables stats and
    returns a valid fd. BPF_ENABLE_STATS takes argument "type". Currently,
    only one type, BPF_STATS_RUN_TIME, is supported. We can extend the
    command to support other types of stats in the future.
    
    With BPF_ENABLE_STATS, user space tool would have the following flow:
    
      1. Get a fd with BPF_ENABLE_STATS, and make sure it is valid;
      2. Check program run_time_ns;
      3. Sleep for the monitoring period;
      4. Check program run_time_ns again, calculate the difference;
      5. Close the fd.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200430071506.1408910-2-songliubraving@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c75b2dd2459c..4f34eecec9ce 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3872,6 +3872,60 @@ static int bpf_link_get_fd_by_id(const union bpf_attr *attr)
 	return fd;
 }
 
+DEFINE_MUTEX(bpf_stats_enabled_mutex);
+
+static int bpf_stats_release(struct inode *inode, struct file *file)
+{
+	mutex_lock(&bpf_stats_enabled_mutex);
+	static_key_slow_dec(&bpf_stats_enabled_key.key);
+	mutex_unlock(&bpf_stats_enabled_mutex);
+	return 0;
+}
+
+static const struct file_operations bpf_stats_fops = {
+	.release = bpf_stats_release,
+};
+
+static int bpf_enable_runtime_stats(void)
+{
+	int fd;
+
+	mutex_lock(&bpf_stats_enabled_mutex);
+
+	/* Set a very high limit to avoid overflow */
+	if (static_key_count(&bpf_stats_enabled_key.key) > INT_MAX / 2) {
+		mutex_unlock(&bpf_stats_enabled_mutex);
+		return -EBUSY;
+	}
+
+	fd = anon_inode_getfd("bpf-stats", &bpf_stats_fops, NULL, O_CLOEXEC);
+	if (fd >= 0)
+		static_key_slow_inc(&bpf_stats_enabled_key.key);
+
+	mutex_unlock(&bpf_stats_enabled_mutex);
+	return fd;
+}
+
+#define BPF_ENABLE_STATS_LAST_FIELD enable_stats.type
+
+static int bpf_enable_stats(union bpf_attr *attr)
+{
+
+	if (CHECK_ATTR(BPF_ENABLE_STATS))
+		return -EINVAL;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	switch (attr->enable_stats.type) {
+	case BPF_STATS_RUN_TIME:
+		return bpf_enable_runtime_stats();
+	default:
+		break;
+	}
+	return -EINVAL;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr;
@@ -3996,6 +4050,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 		err = bpf_obj_get_next_id(&attr, uattr,
 					  &link_idr, &link_idr_lock);
 		break;
+	case BPF_ENABLE_STATS:
+		err = bpf_enable_stats(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit 7f645462ca01d01abb94d75e6768c8b3ed3a188b
Author: Wei Yongjun <weiyongjun1@huawei.com>
Date:   Thu Apr 30 08:18:51 2020 +0000

    bpf: Fix error return code in map_lookup_and_delete_elem()
    
    Fix to return negative error code -EFAULT from the copy_to_user() error
    handling case instead of 0, as done elsewhere in this function.
    
    Fixes: bd513cd08f10 ("bpf: add MAP_LOOKUP_AND_DELETE_ELEM syscall")
    Signed-off-by: Wei Yongjun <weiyongjun1@huawei.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200430081851.166996-1-weiyongjun1@huawei.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 7626b8024471..2843bbba9ca1 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1485,8 +1485,10 @@ static int map_lookup_and_delete_elem(union bpf_attr *attr)
 	if (err)
 		goto free_value;
 
-	if (copy_to_user(uvalue, value, value_size) != 0)
+	if (copy_to_user(uvalue, value, value_size) != 0) {
+		err = -EFAULT;
 		goto free_value;
+	}
 
 	err = 0;
 

commit 449e14bfdb83bf772200840a7ac4dcc1d7cacf54
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed Apr 29 15:21:58 2020 +0200

    bpf: Fix unused variable warning
    
    Hiding the only using of bpf_link_type_strs[] in an #ifdef causes
    an unused-variable warning:
    
    kernel/bpf/syscall.c:2280:20: error: 'bpf_link_type_strs' defined but not used [-Werror=unused-variable]
     2280 | static const char *bpf_link_type_strs[] = {
    
    Move the definition into the same #ifdef.
    
    Fixes: f2e10bff16a0 ("bpf: Add support for BPF_OBJ_GET_INFO_BY_FD for bpf_link")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20200429132217.1294289-1-arnd@arndb.de

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index d23c04cbe14f..c75b2dd2459c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2271,6 +2271,7 @@ static int bpf_link_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+#ifdef CONFIG_PROC_FS
 #define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type)
 #define BPF_MAP_TYPE(_id, _ops)
 #define BPF_LINK_TYPE(_id, _name) [_id] = #_name,
@@ -2282,7 +2283,6 @@ static const char *bpf_link_type_strs[] = {
 #undef BPF_MAP_TYPE
 #undef BPF_LINK_TYPE
 
-#ifdef CONFIG_PROC_FS
 static void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp)
 {
 	const struct bpf_link *link = filp->private_data;

commit f2e10bff16a0fdd41ba278c84da9813700e356af
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Tue Apr 28 17:16:08 2020 -0700

    bpf: Add support for BPF_OBJ_GET_INFO_BY_FD for bpf_link
    
    Add ability to fetch bpf_link details through BPF_OBJ_GET_INFO_BY_FD command.
    Also enhance show_fdinfo to potentially include bpf_link type-specific
    information (similarly to obj_info).
    
    Also introduce enum bpf_link_type stored in bpf_link itself and expose it in
    UAPI. bpf_link_tracing also now will store and return bpf_attach_type.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200429001614.1544-5-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 1c213a730502..d23c04cbe14f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -51,9 +51,11 @@ static const struct bpf_map_ops * const bpf_map_types[] = {
 #define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type)
 #define BPF_MAP_TYPE(_id, _ops) \
 	[_id] = &_ops,
+#define BPF_LINK_TYPE(_id, _name)
 #include <linux/bpf_types.h>
 #undef BPF_PROG_TYPE
 #undef BPF_MAP_TYPE
+#undef BPF_LINK_TYPE
 };
 
 /*
@@ -1548,9 +1550,11 @@ static const struct bpf_prog_ops * const bpf_prog_types[] = {
 #define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type) \
 	[_id] = & _name ## _prog_ops,
 #define BPF_MAP_TYPE(_id, _ops)
+#define BPF_LINK_TYPE(_id, _name)
 #include <linux/bpf_types.h>
 #undef BPF_PROG_TYPE
 #undef BPF_MAP_TYPE
+#undef BPF_LINK_TYPE
 };
 
 static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
@@ -2183,10 +2187,11 @@ static int bpf_obj_get(const union bpf_attr *attr)
 				attr->file_flags);
 }
 
-void bpf_link_init(struct bpf_link *link,
+void bpf_link_init(struct bpf_link *link, enum bpf_link_type type,
 		   const struct bpf_link_ops *ops, struct bpf_prog *prog)
 {
 	atomic64_set(&link->refcnt, 1);
+	link->type = type;
 	link->id = 0;
 	link->ops = ops;
 	link->prog = prog;
@@ -2266,27 +2271,23 @@ static int bpf_link_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
-#ifdef CONFIG_PROC_FS
-static const struct bpf_link_ops bpf_raw_tp_lops;
-static const struct bpf_link_ops bpf_tracing_link_lops;
+#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type)
+#define BPF_MAP_TYPE(_id, _ops)
+#define BPF_LINK_TYPE(_id, _name) [_id] = #_name,
+static const char *bpf_link_type_strs[] = {
+	[BPF_LINK_TYPE_UNSPEC] = "<invalid>",
+#include <linux/bpf_types.h>
+};
+#undef BPF_PROG_TYPE
+#undef BPF_MAP_TYPE
+#undef BPF_LINK_TYPE
 
+#ifdef CONFIG_PROC_FS
 static void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp)
 {
 	const struct bpf_link *link = filp->private_data;
 	const struct bpf_prog *prog = link->prog;
 	char prog_tag[sizeof(prog->tag) * 2 + 1] = { };
-	const char *link_type;
-
-	if (link->ops == &bpf_raw_tp_lops)
-		link_type = "raw_tracepoint";
-	else if (link->ops == &bpf_tracing_link_lops)
-		link_type = "tracing";
-#ifdef CONFIG_CGROUP_BPF
-	else if (link->ops == &bpf_cgroup_link_lops)
-		link_type = "cgroup";
-#endif
-	else
-		link_type = "unknown";
 
 	bin2hex(prog_tag, prog->tag, sizeof(prog->tag));
 	seq_printf(m,
@@ -2294,10 +2295,12 @@ static void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp)
 		   "link_id:\t%u\n"
 		   "prog_tag:\t%s\n"
 		   "prog_id:\t%u\n",
-		   link_type,
+		   bpf_link_type_strs[link->type],
 		   link->id,
 		   prog_tag,
 		   prog->aux->id);
+	if (link->ops->show_fdinfo)
+		link->ops->show_fdinfo(link, m);
 }
 #endif
 
@@ -2403,6 +2406,7 @@ struct bpf_link *bpf_link_get_from_fd(u32 ufd)
 
 struct bpf_tracing_link {
 	struct bpf_link link;
+	enum bpf_attach_type attach_type;
 };
 
 static void bpf_tracing_link_release(struct bpf_link *link)
@@ -2418,9 +2422,33 @@ static void bpf_tracing_link_dealloc(struct bpf_link *link)
 	kfree(tr_link);
 }
 
+static void bpf_tracing_link_show_fdinfo(const struct bpf_link *link,
+					 struct seq_file *seq)
+{
+	struct bpf_tracing_link *tr_link =
+		container_of(link, struct bpf_tracing_link, link);
+
+	seq_printf(seq,
+		   "attach_type:\t%d\n",
+		   tr_link->attach_type);
+}
+
+static int bpf_tracing_link_fill_link_info(const struct bpf_link *link,
+					   struct bpf_link_info *info)
+{
+	struct bpf_tracing_link *tr_link =
+		container_of(link, struct bpf_tracing_link, link);
+
+	info->tracing.attach_type = tr_link->attach_type;
+
+	return 0;
+}
+
 static const struct bpf_link_ops bpf_tracing_link_lops = {
 	.release = bpf_tracing_link_release,
 	.dealloc = bpf_tracing_link_dealloc,
+	.show_fdinfo = bpf_tracing_link_show_fdinfo,
+	.fill_link_info = bpf_tracing_link_fill_link_info,
 };
 
 static int bpf_tracing_prog_attach(struct bpf_prog *prog)
@@ -2460,7 +2488,9 @@ static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 		err = -ENOMEM;
 		goto out_put_prog;
 	}
-	bpf_link_init(&link->link, &bpf_tracing_link_lops, prog);
+	bpf_link_init(&link->link, BPF_LINK_TYPE_TRACING,
+		      &bpf_tracing_link_lops, prog);
+	link->attach_type = prog->expected_attach_type;
 
 	err = bpf_link_prime(&link->link, &link_primer);
 	if (err) {
@@ -2502,9 +2532,56 @@ static void bpf_raw_tp_link_dealloc(struct bpf_link *link)
 	kfree(raw_tp);
 }
 
+static void bpf_raw_tp_link_show_fdinfo(const struct bpf_link *link,
+					struct seq_file *seq)
+{
+	struct bpf_raw_tp_link *raw_tp_link =
+		container_of(link, struct bpf_raw_tp_link, link);
+
+	seq_printf(seq,
+		   "tp_name:\t%s\n",
+		   raw_tp_link->btp->tp->name);
+}
+
+static int bpf_raw_tp_link_fill_link_info(const struct bpf_link *link,
+					  struct bpf_link_info *info)
+{
+	struct bpf_raw_tp_link *raw_tp_link =
+		container_of(link, struct bpf_raw_tp_link, link);
+	char __user *ubuf = u64_to_user_ptr(info->raw_tracepoint.tp_name);
+	const char *tp_name = raw_tp_link->btp->tp->name;
+	u32 ulen = info->raw_tracepoint.tp_name_len;
+	size_t tp_len = strlen(tp_name);
+
+	if (ulen && !ubuf)
+		return -EINVAL;
+
+	info->raw_tracepoint.tp_name_len = tp_len + 1;
+
+	if (!ubuf)
+		return 0;
+
+	if (ulen >= tp_len + 1) {
+		if (copy_to_user(ubuf, tp_name, tp_len + 1))
+			return -EFAULT;
+	} else {
+		char zero = '\0';
+
+		if (copy_to_user(ubuf, tp_name, ulen - 1))
+			return -EFAULT;
+		if (put_user(zero, ubuf + ulen - 1))
+			return -EFAULT;
+		return -ENOSPC;
+	}
+
+	return 0;
+}
+
 static const struct bpf_link_ops bpf_raw_tp_link_lops = {
 	.release = bpf_raw_tp_link_release,
 	.dealloc = bpf_raw_tp_link_dealloc,
+	.show_fdinfo = bpf_raw_tp_link_show_fdinfo,
+	.fill_link_info = bpf_raw_tp_link_fill_link_info,
 };
 
 #define BPF_RAW_TRACEPOINT_OPEN_LAST_FIELD raw_tracepoint.prog_fd
@@ -2570,7 +2647,8 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 		err = -ENOMEM;
 		goto out_put_btp;
 	}
-	bpf_link_init(&link->link, &bpf_raw_tp_link_lops, prog);
+	bpf_link_init(&link->link, BPF_LINK_TYPE_RAW_TRACEPOINT,
+		      &bpf_raw_tp_link_lops, prog);
 	link->btp = btp;
 
 	err = bpf_link_prime(&link->link, &link_primer);
@@ -3366,6 +3444,42 @@ static int bpf_btf_get_info_by_fd(struct btf *btf,
 	return btf_get_info_by_fd(btf, attr, uattr);
 }
 
+static int bpf_link_get_info_by_fd(struct bpf_link *link,
+				  const union bpf_attr *attr,
+				  union bpf_attr __user *uattr)
+{
+	struct bpf_link_info __user *uinfo = u64_to_user_ptr(attr->info.info);
+	struct bpf_link_info info;
+	u32 info_len = attr->info.info_len;
+	int err;
+
+	err = bpf_check_uarg_tail_zero(uinfo, sizeof(info), info_len);
+	if (err)
+		return err;
+	info_len = min_t(u32, sizeof(info), info_len);
+
+	memset(&info, 0, sizeof(info));
+	if (copy_from_user(&info, uinfo, info_len))
+		return -EFAULT;
+
+	info.type = link->type;
+	info.id = link->id;
+	info.prog_id = link->prog->aux->id;
+
+	if (link->ops->fill_link_info) {
+		err = link->ops->fill_link_info(link, &info);
+		if (err)
+			return err;
+	}
+
+	if (copy_to_user(uinfo, &info, info_len) ||
+	    put_user(info_len, &uattr->info.info_len))
+		return -EFAULT;
+
+	return 0;
+}
+
+
 #define BPF_OBJ_GET_INFO_BY_FD_LAST_FIELD info.info
 
 static int bpf_obj_get_info_by_fd(const union bpf_attr *attr,
@@ -3390,6 +3504,9 @@ static int bpf_obj_get_info_by_fd(const union bpf_attr *attr,
 					     uattr);
 	else if (f.file->f_op == &btf_fops)
 		err = bpf_btf_get_info_by_fd(f.file->private_data, attr, uattr);
+	else if (f.file->f_op == &bpf_link_fops)
+		err = bpf_link_get_info_by_fd(f.file->private_data,
+					      attr, uattr);
 	else
 		err = -EINVAL;
 

commit 2d602c8cf40d65d4a7ac34fe18648d8778e6e594
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Tue Apr 28 17:16:07 2020 -0700

    bpf: Support GET_FD_BY_ID and GET_NEXT_ID for bpf_link
    
    Add support to look up bpf_link by ID and iterate over all existing bpf_links
    in the system. GET_FD_BY_ID code handles not-yet-ready bpf_link by checking
    that its ID hasn't been set to non-zero value yet. Setting bpf_link's ID is
    done as the very last step in finalizing bpf_link, together with installing
    FD. This approach allows users of bpf_link in kernel code to not worry about
    races between user-space and kernel code that hasn't finished attaching and
    initializing bpf_link.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200429001614.1544-4-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5439e05e3d25..1c213a730502 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3713,6 +3713,48 @@ static int link_update(union bpf_attr *attr)
 	return ret;
 }
 
+static int bpf_link_inc_not_zero(struct bpf_link *link)
+{
+	return atomic64_fetch_add_unless(&link->refcnt, 1, 0) ? 0 : -ENOENT;
+}
+
+#define BPF_LINK_GET_FD_BY_ID_LAST_FIELD link_id
+
+static int bpf_link_get_fd_by_id(const union bpf_attr *attr)
+{
+	struct bpf_link *link;
+	u32 id = attr->link_id;
+	int fd, err;
+
+	if (CHECK_ATTR(BPF_LINK_GET_FD_BY_ID))
+		return -EINVAL;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	spin_lock_bh(&link_idr_lock);
+	link = idr_find(&link_idr, id);
+	/* before link is "settled", ID is 0, pretend it doesn't exist yet */
+	if (link) {
+		if (link->id)
+			err = bpf_link_inc_not_zero(link);
+		else
+			err = -EAGAIN;
+	} else {
+		err = -ENOENT;
+	}
+	spin_unlock_bh(&link_idr_lock);
+
+	if (err)
+		return err;
+
+	fd = bpf_link_new_fd(link);
+	if (fd < 0)
+		bpf_link_put(link);
+
+	return fd;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr;
@@ -3830,6 +3872,13 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_LINK_UPDATE:
 		err = link_update(&attr);
 		break;
+	case BPF_LINK_GET_FD_BY_ID:
+		err = bpf_link_get_fd_by_id(&attr);
+		break;
+	case BPF_LINK_GET_NEXT_ID:
+		err = bpf_obj_get_next_id(&attr, uattr,
+					  &link_idr, &link_idr_lock);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit a3b80e1078943dc12553166fb08e258463dec013
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Tue Apr 28 17:16:06 2020 -0700

    bpf: Allocate ID for bpf_link
    
    Generate ID for each bpf_link using IDR, similarly to bpf_map and bpf_prog.
    bpf_link creation, initialization, attachment, and exposing to user-space
    through FD and ID is a complicated multi-step process, abstract it away
    through bpf_link_primer and bpf_link_prime(), bpf_link_settle(), and
    bpf_link_cleanup() internal API. They guarantee that until bpf_link is
    properly attached, user-space won't be able to access partially-initialized
    bpf_link either from FD or ID. All this allows to simplify bpf_link attachment
    and error handling code.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200429001614.1544-3-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index f5358e1462eb..5439e05e3d25 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -42,6 +42,8 @@ static DEFINE_IDR(prog_idr);
 static DEFINE_SPINLOCK(prog_idr_lock);
 static DEFINE_IDR(map_idr);
 static DEFINE_SPINLOCK(map_idr_lock);
+static DEFINE_IDR(link_idr);
+static DEFINE_SPINLOCK(link_idr_lock);
 
 int sysctl_unprivileged_bpf_disabled __read_mostly;
 
@@ -2181,25 +2183,38 @@ static int bpf_obj_get(const union bpf_attr *attr)
 				attr->file_flags);
 }
 
-void bpf_link_init(struct bpf_link *link, const struct bpf_link_ops *ops,
-		   struct bpf_prog *prog)
+void bpf_link_init(struct bpf_link *link,
+		   const struct bpf_link_ops *ops, struct bpf_prog *prog)
 {
 	atomic64_set(&link->refcnt, 1);
+	link->id = 0;
 	link->ops = ops;
 	link->prog = prog;
 }
 
+static void bpf_link_free_id(int id)
+{
+	if (!id)
+		return;
+
+	spin_lock_bh(&link_idr_lock);
+	idr_remove(&link_idr, id);
+	spin_unlock_bh(&link_idr_lock);
+}
+
 /* Clean up bpf_link and corresponding anon_inode file and FD. After
  * anon_inode is created, bpf_link can't be just kfree()'d due to deferred
- * anon_inode's release() call. This helper manages marking bpf_link as
- * defunct, releases anon_inode file and puts reserved FD.
+ * anon_inode's release() call. This helper marksbpf_link as
+ * defunct, releases anon_inode file and puts reserved FD. bpf_prog's refcnt
+ * is not decremented, it's the responsibility of a calling code that failed
+ * to complete bpf_link initialization.
  */
-void bpf_link_cleanup(struct bpf_link *link, struct file *link_file,
-		      int link_fd)
+void bpf_link_cleanup(struct bpf_link_primer *primer)
 {
-	link->prog = NULL;
-	fput(link_file);
-	put_unused_fd(link_fd);
+	primer->link->prog = NULL;
+	bpf_link_free_id(primer->id);
+	fput(primer->file);
+	put_unused_fd(primer->fd);
 }
 
 void bpf_link_inc(struct bpf_link *link)
@@ -2210,6 +2225,7 @@ void bpf_link_inc(struct bpf_link *link)
 /* bpf_link_free is guaranteed to be called from process context */
 static void bpf_link_free(struct bpf_link *link)
 {
+	bpf_link_free_id(link->id);
 	if (link->prog) {
 		/* detach BPF program, clean up used resources */
 		link->ops->release(link);
@@ -2275,9 +2291,11 @@ static void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp)
 	bin2hex(prog_tag, prog->tag, sizeof(prog->tag));
 	seq_printf(m,
 		   "link_type:\t%s\n"
+		   "link_id:\t%u\n"
 		   "prog_tag:\t%s\n"
 		   "prog_id:\t%u\n",
 		   link_type,
+		   link->id,
 		   prog_tag,
 		   prog->aux->id);
 }
@@ -2292,36 +2310,76 @@ static const struct file_operations bpf_link_fops = {
 	.write		= bpf_dummy_write,
 };
 
-int bpf_link_new_fd(struct bpf_link *link)
+static int bpf_link_alloc_id(struct bpf_link *link)
 {
-	return anon_inode_getfd("bpf-link", &bpf_link_fops, link, O_CLOEXEC);
-}
+	int id;
+
+	idr_preload(GFP_KERNEL);
+	spin_lock_bh(&link_idr_lock);
+	id = idr_alloc_cyclic(&link_idr, link, 1, INT_MAX, GFP_ATOMIC);
+	spin_unlock_bh(&link_idr_lock);
+	idr_preload_end();
 
-/* Similar to bpf_link_new_fd, create anon_inode for given bpf_link, but
- * instead of immediately installing fd in fdtable, just reserve it and
- * return. Caller then need to either install it with fd_install(fd, file) or
- * release with put_unused_fd(fd).
- * This is useful for cases when bpf_link attachment/detachment are
- * complicated and expensive operations and should be delayed until all the fd
- * reservation and anon_inode creation succeeds.
+	return id;
+}
+
+/* Prepare bpf_link to be exposed to user-space by allocating anon_inode file,
+ * reserving unused FD and allocating ID from link_idr. This is to be paired
+ * with bpf_link_settle() to install FD and ID and expose bpf_link to
+ * user-space, if bpf_link is successfully attached. If not, bpf_link and
+ * pre-allocated resources are to be freed with bpf_cleanup() call. All the
+ * transient state is passed around in struct bpf_link_primer.
+ * This is preferred way to create and initialize bpf_link, especially when
+ * there are complicated and expensive operations inbetween creating bpf_link
+ * itself and attaching it to BPF hook. By using bpf_link_prime() and
+ * bpf_link_settle() kernel code using bpf_link doesn't have to perform
+ * expensive (and potentially failing) roll back operations in a rare case
+ * that file, FD, or ID can't be allocated.
  */
-struct file *bpf_link_new_file(struct bpf_link *link, int *reserved_fd)
+int bpf_link_prime(struct bpf_link *link, struct bpf_link_primer *primer)
 {
 	struct file *file;
-	int fd;
+	int fd, id;
 
 	fd = get_unused_fd_flags(O_CLOEXEC);
 	if (fd < 0)
-		return ERR_PTR(fd);
+		return fd;
 
 	file = anon_inode_getfile("bpf_link", &bpf_link_fops, link, O_CLOEXEC);
 	if (IS_ERR(file)) {
 		put_unused_fd(fd);
-		return file;
+		return PTR_ERR(file);
 	}
 
-	*reserved_fd = fd;
-	return file;
+	id = bpf_link_alloc_id(link);
+	if (id < 0) {
+		put_unused_fd(fd);
+		fput(file);
+		return id;
+	}
+
+	primer->link = link;
+	primer->file = file;
+	primer->fd = fd;
+	primer->id = id;
+	return 0;
+}
+
+int bpf_link_settle(struct bpf_link_primer *primer)
+{
+	/* make bpf_link fetchable by ID */
+	spin_lock_bh(&link_idr_lock);
+	primer->link->id = primer->id;
+	spin_unlock_bh(&link_idr_lock);
+	/* make bpf_link fetchable by FD */
+	fd_install(primer->fd, primer->file);
+	/* pass through installed FD */
+	return primer->fd;
+}
+
+int bpf_link_new_fd(struct bpf_link *link)
+{
+	return anon_inode_getfd("bpf-link", &bpf_link_fops, link, O_CLOEXEC);
 }
 
 struct bpf_link *bpf_link_get_from_fd(u32 ufd)
@@ -2367,9 +2425,9 @@ static const struct bpf_link_ops bpf_tracing_link_lops = {
 
 static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 {
+	struct bpf_link_primer link_primer;
 	struct bpf_tracing_link *link;
-	struct file *link_file;
-	int link_fd, err;
+	int err;
 
 	switch (prog->type) {
 	case BPF_PROG_TYPE_TRACING:
@@ -2404,22 +2462,19 @@ static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 	}
 	bpf_link_init(&link->link, &bpf_tracing_link_lops, prog);
 
-	link_file = bpf_link_new_file(&link->link, &link_fd);
-	if (IS_ERR(link_file)) {
+	err = bpf_link_prime(&link->link, &link_primer);
+	if (err) {
 		kfree(link);
-		err = PTR_ERR(link_file);
 		goto out_put_prog;
 	}
 
 	err = bpf_trampoline_link_prog(prog);
 	if (err) {
-		bpf_link_cleanup(&link->link, link_file, link_fd);
+		bpf_link_cleanup(&link_primer);
 		goto out_put_prog;
 	}
 
-	fd_install(link_fd, link_file);
-	return link_fd;
-
+	return bpf_link_settle(&link_primer);
 out_put_prog:
 	bpf_prog_put(prog);
 	return err;
@@ -2447,7 +2502,7 @@ static void bpf_raw_tp_link_dealloc(struct bpf_link *link)
 	kfree(raw_tp);
 }
 
-static const struct bpf_link_ops bpf_raw_tp_lops = {
+static const struct bpf_link_ops bpf_raw_tp_link_lops = {
 	.release = bpf_raw_tp_link_release,
 	.dealloc = bpf_raw_tp_link_dealloc,
 };
@@ -2456,13 +2511,13 @@ static const struct bpf_link_ops bpf_raw_tp_lops = {
 
 static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 {
+	struct bpf_link_primer link_primer;
 	struct bpf_raw_tp_link *link;
 	struct bpf_raw_event_map *btp;
-	struct file *link_file;
 	struct bpf_prog *prog;
 	const char *tp_name;
 	char buf[128];
-	int link_fd, err;
+	int err;
 
 	if (CHECK_ATTR(BPF_RAW_TRACEPOINT_OPEN))
 		return -EINVAL;
@@ -2515,24 +2570,22 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 		err = -ENOMEM;
 		goto out_put_btp;
 	}
-	bpf_link_init(&link->link, &bpf_raw_tp_lops, prog);
+	bpf_link_init(&link->link, &bpf_raw_tp_link_lops, prog);
 	link->btp = btp;
 
-	link_file = bpf_link_new_file(&link->link, &link_fd);
-	if (IS_ERR(link_file)) {
+	err = bpf_link_prime(&link->link, &link_primer);
+	if (err) {
 		kfree(link);
-		err = PTR_ERR(link_file);
 		goto out_put_btp;
 	}
 
 	err = bpf_probe_register(link->btp, prog);
 	if (err) {
-		bpf_link_cleanup(&link->link, link_file, link_fd);
+		bpf_link_cleanup(&link_primer);
 		goto out_put_btp;
 	}
 
-	fd_install(link_fd, link_file);
-	return link_fd;
+	return bpf_link_settle(&link_primer);
 
 out_put_btp:
 	bpf_put_raw_tracepoint(btp);
@@ -3464,7 +3517,7 @@ static int bpf_task_fd_query(const union bpf_attr *attr,
 	if (file->f_op == &bpf_link_fops) {
 		struct bpf_link *link = file->private_data;
 
-		if (link->ops == &bpf_raw_tp_lops) {
+		if (link->ops == &bpf_raw_tp_link_lops) {
 			struct bpf_raw_tp_link *raw_tp =
 				container_of(link, struct bpf_raw_tp_link, link);
 			struct bpf_raw_event_map *btp = raw_tp->btp;

commit f9d041271cf44ca02eed0cc82e1a6d8c814c53ed
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Tue Apr 28 17:16:05 2020 -0700

    bpf: Refactor bpf_link update handling
    
    Make bpf_link update support more generic by making it into another
    bpf_link_ops methods. This allows generic syscall handling code to be agnostic
    to various conditionally compiled features (e.g., the case of
    CONFIG_CGROUP_BPF). This also allows to keep link type-specific code to remain
    static within respective code base. Refactor existing bpf_cgroup_link code and
    take advantage of this.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200429001614.1544-2-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 7626b8024471..f5358e1462eb 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3645,13 +3645,10 @@ static int link_update(union bpf_attr *attr)
 		goto out_put_progs;
 	}
 
-#ifdef CONFIG_CGROUP_BPF
-	if (link->ops == &bpf_cgroup_link_lops) {
-		ret = cgroup_bpf_replace(link, old_prog, new_prog);
-		goto out_put_progs;
-	}
-#endif
-	ret = -EINVAL;
+	if (link->ops->update_prog)
+		ret = link->ops->update_prog(link, new_prog, old_prog);
+	else
+		ret = EINVAL;
 
 out_put_progs:
 	if (old_prog)

commit 6f302bfb221470e712ce3e5911fb83cdca174387
Author: Zou Wei <zou_wei@huawei.com>
Date:   Thu Apr 23 10:32:40 2020 +0800

    bpf: Make bpf_link_fops static
    
    Fix the following sparse warning:
    
    kernel/bpf/syscall.c:2289:30: warning: symbol 'bpf_link_fops' was not declared. Should it be static?
    
    Reported-by: Hulk Robot <hulkci@huawei.com>
    Signed-off-by: Zou Wei <zou_wei@huawei.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/1587609160-117806-1-git-send-email-zou_wei@huawei.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index bca58c235ac0..7626b8024471 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2283,7 +2283,7 @@ static void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp)
 }
 #endif
 
-const struct file_operations bpf_link_fops = {
+static const struct file_operations bpf_link_fops = {
 #ifdef CONFIG_PROC_FS
 	.show_fdinfo	= bpf_link_show_fdinfo,
 #endif

commit 4adb7a4a151c65ac7e9c3a1aa462c84190d48385
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Thu Apr 23 22:20:44 2020 -0700

    bpf: Fix leak in LINK_UPDATE and enforce empty old_prog_fd
    
    Fix bug of not putting bpf_link in LINK_UPDATE command.
    Also enforce zeroed old_prog_fd if no BPF_F_REPLACE flag is specified.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200424052045.4002963-1-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index d85f37239540..bca58c235ac0 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3628,8 +3628,10 @@ static int link_update(union bpf_attr *attr)
 		return PTR_ERR(link);
 
 	new_prog = bpf_prog_get(attr->link_update.new_prog_fd);
-	if (IS_ERR(new_prog))
-		return PTR_ERR(new_prog);
+	if (IS_ERR(new_prog)) {
+		ret = PTR_ERR(new_prog);
+		goto out_put_link;
+	}
 
 	if (flags & BPF_F_REPLACE) {
 		old_prog = bpf_prog_get(attr->link_update.old_prog_fd);
@@ -3638,6 +3640,9 @@ static int link_update(union bpf_attr *attr)
 			old_prog = NULL;
 			goto out_put_progs;
 		}
+	} else if (attr->link_update.old_prog_fd) {
+		ret = -EINVAL;
+		goto out_put_progs;
 	}
 
 #ifdef CONFIG_CGROUP_BPF
@@ -3653,6 +3658,8 @@ static int link_update(union bpf_attr *attr)
 		bpf_prog_put(old_prog);
 	if (ret)
 		bpf_prog_put(new_prog);
+out_put_link:
+	bpf_link_put(link);
 	return ret;
 }
 

commit 1f6cb19be2e231fe092f40decb71f066eba090d7
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Fri Apr 10 13:26:12 2020 -0700

    bpf: Prevent re-mmap()'ing BPF map as writable for initially r/o mapping
    
    VM_MAYWRITE flag during initial memory mapping determines if already mmap()'ed
    pages can be later remapped as writable ones through mprotect() call. To
    prevent user application to rewrite contents of memory-mapped as read-only and
    subsequently frozen BPF map, remove VM_MAYWRITE flag completely on initially
    read-only mapping.
    
    Alternatively, we could treat any memory-mapping on unfrozen map as writable
    and bump writecnt instead. But there is little legitimate reason to map
    BPF map as read-only and then re-mmap() it as writable through mprotect(),
    instead of just mmap()'ing it as read/write from the very beginning.
    
    Also, at the suggestion of Jann Horn, drop unnecessary refcounting in mmap
    operations. We can just rely on VMA holding reference to BPF map's file
    properly.
    
    Fixes: fc9702273e2e ("bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY")
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Jann Horn <jannh@google.com>
    Link: https://lore.kernel.org/bpf/20200410202613.3679837-1-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 64783da34202..d85f37239540 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -586,9 +586,7 @@ static void bpf_map_mmap_open(struct vm_area_struct *vma)
 {
 	struct bpf_map *map = vma->vm_file->private_data;
 
-	bpf_map_inc_with_uref(map);
-
-	if (vma->vm_flags & VM_WRITE) {
+	if (vma->vm_flags & VM_MAYWRITE) {
 		mutex_lock(&map->freeze_mutex);
 		map->writecnt++;
 		mutex_unlock(&map->freeze_mutex);
@@ -600,13 +598,11 @@ static void bpf_map_mmap_close(struct vm_area_struct *vma)
 {
 	struct bpf_map *map = vma->vm_file->private_data;
 
-	if (vma->vm_flags & VM_WRITE) {
+	if (vma->vm_flags & VM_MAYWRITE) {
 		mutex_lock(&map->freeze_mutex);
 		map->writecnt--;
 		mutex_unlock(&map->freeze_mutex);
 	}
-
-	bpf_map_put_with_uref(map);
 }
 
 static const struct vm_operations_struct bpf_map_default_vmops = {
@@ -635,14 +631,16 @@ static int bpf_map_mmap(struct file *filp, struct vm_area_struct *vma)
 	/* set default open/close callbacks */
 	vma->vm_ops = &bpf_map_default_vmops;
 	vma->vm_private_data = map;
+	vma->vm_flags &= ~VM_MAYEXEC;
+	if (!(vma->vm_flags & VM_WRITE))
+		/* disallow re-mapping with PROT_WRITE */
+		vma->vm_flags &= ~VM_MAYWRITE;
 
 	err = map->ops->map_mmap(map, vma);
 	if (err)
 		goto out;
 
-	bpf_map_inc_with_uref(map);
-
-	if (vma->vm_flags & VM_WRITE)
+	if (vma->vm_flags & VM_MAYWRITE)
 		map->writecnt++;
 out:
 	mutex_unlock(&map->freeze_mutex);

commit ed52f2c608c9451fa2bad298b2ab927416105d65
Merge: f87238d30c0d 8596a75f6c83
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Mar 30 19:52:37 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 0c991ebc8c69d29b7fc44db17075c5aa5253e2ab
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Sun Mar 29 19:59:59 2020 -0700

    bpf: Implement bpf_prog replacement for an active bpf_cgroup_link
    
    Add new operation (LINK_UPDATE), which allows to replace active bpf_prog from
    under given bpf_link. Currently this is only supported for bpf_cgroup_link,
    but will be extended to other kinds of bpf_links in follow-up patches.
    
    For bpf_cgroup_link, implemented functionality matches existing semantics for
    direct bpf_prog attachment (including BPF_F_REPLACE flag). User can either
    unconditionally set new bpf_prog regardless of which bpf_prog is currently
    active under given bpf_link, or, optionally, can specify expected active
    bpf_prog. If active bpf_prog doesn't match expected one, no changes are
    performed, old bpf_link stays intact and attached, operation returns
    a failure.
    
    cgroup_bpf_replace() operation is resolving race between auto-detachment and
    bpf_prog update in the same fashion as it's done for bpf_link detachment,
    except in this case update has no way of succeeding because of target cgroup
    marked as dying. So in this case error is returned.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200330030001.2312810-3-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 97d5c6fb63cd..e0a3b34d7039 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3596,6 +3596,58 @@ static int link_create(union bpf_attr *attr)
 	return ret;
 }
 
+#define BPF_LINK_UPDATE_LAST_FIELD link_update.old_prog_fd
+
+static int link_update(union bpf_attr *attr)
+{
+	struct bpf_prog *old_prog = NULL, *new_prog;
+	struct bpf_link *link;
+	u32 flags;
+	int ret;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (CHECK_ATTR(BPF_LINK_UPDATE))
+		return -EINVAL;
+
+	flags = attr->link_update.flags;
+	if (flags & ~BPF_F_REPLACE)
+		return -EINVAL;
+
+	link = bpf_link_get_from_fd(attr->link_update.link_fd);
+	if (IS_ERR(link))
+		return PTR_ERR(link);
+
+	new_prog = bpf_prog_get(attr->link_update.new_prog_fd);
+	if (IS_ERR(new_prog))
+		return PTR_ERR(new_prog);
+
+	if (flags & BPF_F_REPLACE) {
+		old_prog = bpf_prog_get(attr->link_update.old_prog_fd);
+		if (IS_ERR(old_prog)) {
+			ret = PTR_ERR(old_prog);
+			old_prog = NULL;
+			goto out_put_progs;
+		}
+	}
+
+#ifdef CONFIG_CGROUP_BPF
+	if (link->ops == &bpf_cgroup_link_lops) {
+		ret = cgroup_bpf_replace(link, old_prog, new_prog);
+		goto out_put_progs;
+	}
+#endif
+	ret = -EINVAL;
+
+out_put_progs:
+	if (old_prog)
+		bpf_prog_put(old_prog);
+	if (ret)
+		bpf_prog_put(new_prog);
+	return ret;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -3709,6 +3761,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_LINK_CREATE:
 		err = link_create(&attr);
 		break;
+	case BPF_LINK_UPDATE:
+		err = link_update(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit af6eea57437a830293eab56246b6025cc7d46ee7
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Sun Mar 29 19:59:58 2020 -0700

    bpf: Implement bpf_link-based cgroup BPF program attachment
    
    Implement new sub-command to attach cgroup BPF programs and return FD-based
    bpf_link back on success. bpf_link, once attached to cgroup, cannot be
    replaced, except by owner having its FD. Cgroup bpf_link supports only
    BPF_F_ALLOW_MULTI semantics. Both link-based and prog-based BPF_F_ALLOW_MULTI
    attachments can be freely intermixed.
    
    To prevent bpf_cgroup_link from keeping cgroup alive past the point when no
    BPF program can be executed, implement auto-detachment of link. When
    cgroup_bpf_release() is called, all attached bpf_links are forced to release
    cgroup refcounts, but they leave bpf_link otherwise active and allocated, as
    well as still owning underlying bpf_prog. This is because user-space might
    still have FDs open and active, so bpf_link as a user-referenced object can't
    be freed yet. Once last active FD is closed, bpf_link will be freed and
    underlying bpf_prog refcount will be dropped. But cgroup refcount won't be
    touched, because cgroup is released already.
    
    The inherent race between bpf_cgroup_link release (from closing last FD) and
    cgroup_bpf_release() is resolved by both operations taking cgroup_mutex. So
    the only additional check required is when bpf_cgroup_link attempts to detach
    itself from cgroup. At that time we need to check whether there is still
    cgroup associated with that link. And if not, exit with success, because
    bpf_cgroup_link was already successfully detached.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Roman Gushchin <guro@fb.com>
    Link: https://lore.kernel.org/bpf/20200330030001.2312810-2-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index a616b63f23b4..97d5c6fb63cd 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2175,13 +2175,6 @@ static int bpf_obj_get(const union bpf_attr *attr)
 				attr->file_flags);
 }
 
-struct bpf_link {
-	atomic64_t refcnt;
-	const struct bpf_link_ops *ops;
-	struct bpf_prog *prog;
-	struct work_struct work;
-};
-
 void bpf_link_init(struct bpf_link *link, const struct bpf_link_ops *ops,
 		   struct bpf_prog *prog)
 {
@@ -2195,8 +2188,8 @@ void bpf_link_init(struct bpf_link *link, const struct bpf_link_ops *ops,
  * anon_inode's release() call. This helper manages marking bpf_link as
  * defunct, releases anon_inode file and puts reserved FD.
  */
-static void bpf_link_cleanup(struct bpf_link *link, struct file *link_file,
-			     int link_fd)
+void bpf_link_cleanup(struct bpf_link *link, struct file *link_file,
+		      int link_fd)
 {
 	link->prog = NULL;
 	fput(link_file);
@@ -2266,6 +2259,10 @@ static void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp)
 		link_type = "raw_tracepoint";
 	else if (link->ops == &bpf_tracing_link_lops)
 		link_type = "tracing";
+#ifdef CONFIG_CGROUP_BPF
+	else if (link->ops == &bpf_cgroup_link_lops)
+		link_type = "cgroup";
+#endif
 	else
 		link_type = "unknown";
 
@@ -3553,6 +3550,52 @@ static int bpf_map_do_batch(const union bpf_attr *attr,
 	return err;
 }
 
+#define BPF_LINK_CREATE_LAST_FIELD link_create.flags
+static int link_create(union bpf_attr *attr)
+{
+	enum bpf_prog_type ptype;
+	struct bpf_prog *prog;
+	int ret;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (CHECK_ATTR(BPF_LINK_CREATE))
+		return -EINVAL;
+
+	ptype = attach_type_to_prog_type(attr->link_create.attach_type);
+	if (ptype == BPF_PROG_TYPE_UNSPEC)
+		return -EINVAL;
+
+	prog = bpf_prog_get_type(attr->link_create.prog_fd, ptype);
+	if (IS_ERR(prog))
+		return PTR_ERR(prog);
+
+	ret = bpf_prog_attach_check_attach_type(prog,
+						attr->link_create.attach_type);
+	if (ret)
+		goto err_out;
+
+	switch (ptype) {
+	case BPF_PROG_TYPE_CGROUP_SKB:
+	case BPF_PROG_TYPE_CGROUP_SOCK:
+	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
+	case BPF_PROG_TYPE_SOCK_OPS:
+	case BPF_PROG_TYPE_CGROUP_DEVICE:
+	case BPF_PROG_TYPE_CGROUP_SYSCTL:
+	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
+		ret = cgroup_bpf_link_attach(attr, prog);
+		break;
+	default:
+		ret = -EINVAL;
+	}
+
+err_out:
+	if (ret < 0)
+		bpf_prog_put(prog);
+	return ret;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -3663,6 +3706,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_MAP_DELETE_BATCH:
 		err = bpf_map_do_batch(&attr, uattr, BPF_MAP_DELETE_BATCH);
 		break;
+	case BPF_LINK_CREATE:
+		err = link_create(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit f0b5989745c3e0e92424d36869a97e4e8df7ab13
Merge: 1a147b74c2fd 7111951b8d49
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Mar 29 21:25:29 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Minor comment conflict in mac80211.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9e4e01dfd3254c7f04f24b7c6b29596bc12332f3
Author: KP Singh <kpsingh@google.com>
Date:   Sun Mar 29 01:43:52 2020 +0100

    bpf: lsm: Implement attach, detach and execution
    
    JITed BPF programs are dynamically attached to the LSM hooks
    using BPF trampolines. The trampoline prologue generates code to handle
    conversion of the signature of the hook to the appropriate BPF context.
    
    The allocated trampoline programs are attached to the nop functions
    initialized as LSM hooks.
    
    BPF_PROG_TYPE_LSM programs must have a GPL compatible license and
    and need CAP_SYS_ADMIN (required for loading eBPF programs).
    
    Upon attachment:
    
    * A BPF fexit trampoline is used for LSM hooks with a void return type.
    * A BPF fmod_ret trampoline is used for LSM hooks which return an
      int. The attached programs can override the return value of the
      bpf LSM hook to indicate a MAC Policy decision.
    
    Signed-off-by: KP Singh <kpsingh@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Reviewed-by: Brendan Jackman <jackmanb@google.com>
    Reviewed-by: Florent Revest <revest@google.com>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: James Morris <jamorris@linux.microsoft.com>
    Link: https://lore.kernel.org/bpf/20200329004356.27286-5-kpsingh@chromium.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b2584b25748c..a616b63f23b4 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -25,6 +25,7 @@
 #include <linux/nospec.h>
 #include <linux/audit.h>
 #include <uapi/linux/btf.h>
+#include <linux/bpf_lsm.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
 			  (map)->map_type == BPF_MAP_TYPE_CGROUP_ARRAY || \
@@ -1935,6 +1936,7 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 
 		switch (prog_type) {
 		case BPF_PROG_TYPE_TRACING:
+		case BPF_PROG_TYPE_LSM:
 		case BPF_PROG_TYPE_STRUCT_OPS:
 		case BPF_PROG_TYPE_EXT:
 			break;
@@ -2366,10 +2368,28 @@ static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 	struct file *link_file;
 	int link_fd, err;
 
-	if (prog->expected_attach_type != BPF_TRACE_FENTRY &&
-	    prog->expected_attach_type != BPF_TRACE_FEXIT &&
-	    prog->expected_attach_type != BPF_MODIFY_RETURN &&
-	    prog->type != BPF_PROG_TYPE_EXT) {
+	switch (prog->type) {
+	case BPF_PROG_TYPE_TRACING:
+		if (prog->expected_attach_type != BPF_TRACE_FENTRY &&
+		    prog->expected_attach_type != BPF_TRACE_FEXIT &&
+		    prog->expected_attach_type != BPF_MODIFY_RETURN) {
+			err = -EINVAL;
+			goto out_put_prog;
+		}
+		break;
+	case BPF_PROG_TYPE_EXT:
+		if (prog->expected_attach_type != 0) {
+			err = -EINVAL;
+			goto out_put_prog;
+		}
+		break;
+	case BPF_PROG_TYPE_LSM:
+		if (prog->expected_attach_type != BPF_LSM_MAC) {
+			err = -EINVAL;
+			goto out_put_prog;
+		}
+		break;
+	default:
 		err = -EINVAL;
 		goto out_put_prog;
 	}
@@ -2448,16 +2468,10 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 	if (IS_ERR(prog))
 		return PTR_ERR(prog);
 
-	if (prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT &&
-	    prog->type != BPF_PROG_TYPE_TRACING &&
-	    prog->type != BPF_PROG_TYPE_EXT &&
-	    prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE) {
-		err = -EINVAL;
-		goto out_put_prog;
-	}
-
-	if (prog->type == BPF_PROG_TYPE_TRACING ||
-	    prog->type == BPF_PROG_TYPE_EXT) {
+	switch (prog->type) {
+	case BPF_PROG_TYPE_TRACING:
+	case BPF_PROG_TYPE_EXT:
+	case BPF_PROG_TYPE_LSM:
 		if (attr->raw_tracepoint.name) {
 			/* The attach point for this category of programs
 			 * should be specified via btf_id during program load.
@@ -2465,11 +2479,14 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 			err = -EINVAL;
 			goto out_put_prog;
 		}
-		if (prog->expected_attach_type == BPF_TRACE_RAW_TP)
+		if (prog->type == BPF_PROG_TYPE_TRACING &&
+		    prog->expected_attach_type == BPF_TRACE_RAW_TP) {
 			tp_name = prog->aux->attach_func_name;
-		else
-			return bpf_tracing_prog_attach(prog);
-	} else {
+			break;
+		}
+		return bpf_tracing_prog_attach(prog);
+	case BPF_PROG_TYPE_RAW_TRACEPOINT:
+	case BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE:
 		if (strncpy_from_user(buf,
 				      u64_to_user_ptr(attr->raw_tracepoint.name),
 				      sizeof(buf) - 1) < 0) {
@@ -2478,6 +2495,10 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 		}
 		buf[sizeof(buf) - 1] = 0;
 		tp_name = buf;
+		break;
+	default:
+		err = -EINVAL;
+		goto out_put_prog;
 	}
 
 	btp = bpf_get_raw_tracepoint(tp_name);

commit f54a5bba120398e4d404e9553e6b92e6822eade0
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Thu Mar 26 11:16:13 2020 +0800

    bpf: Remove unused vairable 'bpf_xdp_link_lops'
    
    kernel/bpf/syscall.c:2263:34: warning: 'bpf_xdp_link_lops' defined but not used [-Wunused-const-variable=]
     static const struct bpf_link_ops bpf_xdp_link_lops;
                                      ^~~~~~~~~~~~~~~~~
    
    commit 70ed506c3bbc ("bpf: Introduce pinnable bpf_link abstraction")
    involded this unused variable, remove it.
    
    Reported-by: Hulk Robot <hulkci@huawei.com>
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20200326031613.19372-1-yuehaibing@huawei.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index fd4181939064..b2584b25748c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2252,7 +2252,6 @@ static int bpf_link_release(struct inode *inode, struct file *filp)
 #ifdef CONFIG_PROC_FS
 static const struct bpf_link_ops bpf_raw_tp_lops;
 static const struct bpf_link_ops bpf_tracing_link_lops;
-static const struct bpf_link_ops bpf_xdp_link_lops;
 
 static void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp)
 {

commit e28784e3781e19f546bd2c2cd7c1c4e7c54e7f73
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Tue Mar 24 23:57:42 2020 -0700

    bpf: Factor out attach_type to prog_type mapping for attach/detach
    
    Factor out logic mapping expected program attach type to program type and
    subsequent handling of program attach/detach. Also list out all supported
    cgroup BPF program types explicitly to prevent accidental bugs once more
    program types are added to a mapping. Do the same for prog_query API.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200325065746.640559-3-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 85567a6ea5f9..fd4181939064 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2535,36 +2535,18 @@ static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
 	}
 }
 
-#define BPF_PROG_ATTACH_LAST_FIELD replace_bpf_fd
-
-#define BPF_F_ATTACH_MASK \
-	(BPF_F_ALLOW_OVERRIDE | BPF_F_ALLOW_MULTI | BPF_F_REPLACE)
-
-static int bpf_prog_attach(const union bpf_attr *attr)
+static enum bpf_prog_type
+attach_type_to_prog_type(enum bpf_attach_type attach_type)
 {
-	enum bpf_prog_type ptype;
-	struct bpf_prog *prog;
-	int ret;
-
-	if (!capable(CAP_NET_ADMIN))
-		return -EPERM;
-
-	if (CHECK_ATTR(BPF_PROG_ATTACH))
-		return -EINVAL;
-
-	if (attr->attach_flags & ~BPF_F_ATTACH_MASK)
-		return -EINVAL;
-
-	switch (attr->attach_type) {
+	switch (attach_type) {
 	case BPF_CGROUP_INET_INGRESS:
 	case BPF_CGROUP_INET_EGRESS:
-		ptype = BPF_PROG_TYPE_CGROUP_SKB;
+		return BPF_PROG_TYPE_CGROUP_SKB;
 		break;
 	case BPF_CGROUP_INET_SOCK_CREATE:
 	case BPF_CGROUP_INET4_POST_BIND:
 	case BPF_CGROUP_INET6_POST_BIND:
-		ptype = BPF_PROG_TYPE_CGROUP_SOCK;
-		break;
+		return BPF_PROG_TYPE_CGROUP_SOCK;
 	case BPF_CGROUP_INET4_BIND:
 	case BPF_CGROUP_INET6_BIND:
 	case BPF_CGROUP_INET4_CONNECT:
@@ -2573,37 +2555,53 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_UDP6_SENDMSG:
 	case BPF_CGROUP_UDP4_RECVMSG:
 	case BPF_CGROUP_UDP6_RECVMSG:
-		ptype = BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
-		break;
+		return BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
 	case BPF_CGROUP_SOCK_OPS:
-		ptype = BPF_PROG_TYPE_SOCK_OPS;
-		break;
+		return BPF_PROG_TYPE_SOCK_OPS;
 	case BPF_CGROUP_DEVICE:
-		ptype = BPF_PROG_TYPE_CGROUP_DEVICE;
-		break;
+		return BPF_PROG_TYPE_CGROUP_DEVICE;
 	case BPF_SK_MSG_VERDICT:
-		ptype = BPF_PROG_TYPE_SK_MSG;
-		break;
+		return BPF_PROG_TYPE_SK_MSG;
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
-		ptype = BPF_PROG_TYPE_SK_SKB;
-		break;
+		return BPF_PROG_TYPE_SK_SKB;
 	case BPF_LIRC_MODE2:
-		ptype = BPF_PROG_TYPE_LIRC_MODE2;
-		break;
+		return BPF_PROG_TYPE_LIRC_MODE2;
 	case BPF_FLOW_DISSECTOR:
-		ptype = BPF_PROG_TYPE_FLOW_DISSECTOR;
-		break;
+		return BPF_PROG_TYPE_FLOW_DISSECTOR;
 	case BPF_CGROUP_SYSCTL:
-		ptype = BPF_PROG_TYPE_CGROUP_SYSCTL;
-		break;
+		return BPF_PROG_TYPE_CGROUP_SYSCTL;
 	case BPF_CGROUP_GETSOCKOPT:
 	case BPF_CGROUP_SETSOCKOPT:
-		ptype = BPF_PROG_TYPE_CGROUP_SOCKOPT;
-		break;
+		return BPF_PROG_TYPE_CGROUP_SOCKOPT;
 	default:
-		return -EINVAL;
+		return BPF_PROG_TYPE_UNSPEC;
 	}
+}
+
+#define BPF_PROG_ATTACH_LAST_FIELD replace_bpf_fd
+
+#define BPF_F_ATTACH_MASK \
+	(BPF_F_ALLOW_OVERRIDE | BPF_F_ALLOW_MULTI | BPF_F_REPLACE)
+
+static int bpf_prog_attach(const union bpf_attr *attr)
+{
+	enum bpf_prog_type ptype;
+	struct bpf_prog *prog;
+	int ret;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (CHECK_ATTR(BPF_PROG_ATTACH))
+		return -EINVAL;
+
+	if (attr->attach_flags & ~BPF_F_ATTACH_MASK)
+		return -EINVAL;
+
+	ptype = attach_type_to_prog_type(attr->attach_type);
+	if (ptype == BPF_PROG_TYPE_UNSPEC)
+		return -EINVAL;
 
 	prog = bpf_prog_get_type(attr->attach_bpf_fd, ptype);
 	if (IS_ERR(prog))
@@ -2625,8 +2623,17 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_PROG_TYPE_FLOW_DISSECTOR:
 		ret = skb_flow_dissector_bpf_prog_attach(attr, prog);
 		break;
-	default:
+	case BPF_PROG_TYPE_CGROUP_DEVICE:
+	case BPF_PROG_TYPE_CGROUP_SKB:
+	case BPF_PROG_TYPE_CGROUP_SOCK:
+	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
+	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
+	case BPF_PROG_TYPE_CGROUP_SYSCTL:
+	case BPF_PROG_TYPE_SOCK_OPS:
 		ret = cgroup_bpf_prog_attach(attr, ptype, prog);
+		break;
+	default:
+		ret = -EINVAL;
 	}
 
 	if (ret)
@@ -2646,53 +2653,27 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	if (CHECK_ATTR(BPF_PROG_DETACH))
 		return -EINVAL;
 
-	switch (attr->attach_type) {
-	case BPF_CGROUP_INET_INGRESS:
-	case BPF_CGROUP_INET_EGRESS:
-		ptype = BPF_PROG_TYPE_CGROUP_SKB;
-		break;
-	case BPF_CGROUP_INET_SOCK_CREATE:
-	case BPF_CGROUP_INET4_POST_BIND:
-	case BPF_CGROUP_INET6_POST_BIND:
-		ptype = BPF_PROG_TYPE_CGROUP_SOCK;
-		break;
-	case BPF_CGROUP_INET4_BIND:
-	case BPF_CGROUP_INET6_BIND:
-	case BPF_CGROUP_INET4_CONNECT:
-	case BPF_CGROUP_INET6_CONNECT:
-	case BPF_CGROUP_UDP4_SENDMSG:
-	case BPF_CGROUP_UDP6_SENDMSG:
-	case BPF_CGROUP_UDP4_RECVMSG:
-	case BPF_CGROUP_UDP6_RECVMSG:
-		ptype = BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
-		break;
-	case BPF_CGROUP_SOCK_OPS:
-		ptype = BPF_PROG_TYPE_SOCK_OPS;
-		break;
-	case BPF_CGROUP_DEVICE:
-		ptype = BPF_PROG_TYPE_CGROUP_DEVICE;
-		break;
-	case BPF_SK_MSG_VERDICT:
-		return sock_map_get_from_fd(attr, NULL);
-	case BPF_SK_SKB_STREAM_PARSER:
-	case BPF_SK_SKB_STREAM_VERDICT:
+	ptype = attach_type_to_prog_type(attr->attach_type);
+
+	switch (ptype) {
+	case BPF_PROG_TYPE_SK_MSG:
+	case BPF_PROG_TYPE_SK_SKB:
 		return sock_map_get_from_fd(attr, NULL);
-	case BPF_LIRC_MODE2:
+	case BPF_PROG_TYPE_LIRC_MODE2:
 		return lirc_prog_detach(attr);
-	case BPF_FLOW_DISSECTOR:
+	case BPF_PROG_TYPE_FLOW_DISSECTOR:
 		return skb_flow_dissector_bpf_prog_detach(attr);
-	case BPF_CGROUP_SYSCTL:
-		ptype = BPF_PROG_TYPE_CGROUP_SYSCTL;
-		break;
-	case BPF_CGROUP_GETSOCKOPT:
-	case BPF_CGROUP_SETSOCKOPT:
-		ptype = BPF_PROG_TYPE_CGROUP_SOCKOPT;
-		break;
+	case BPF_PROG_TYPE_CGROUP_DEVICE:
+	case BPF_PROG_TYPE_CGROUP_SKB:
+	case BPF_PROG_TYPE_CGROUP_SOCK:
+	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
+	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
+	case BPF_PROG_TYPE_CGROUP_SYSCTL:
+	case BPF_PROG_TYPE_SOCK_OPS:
+		return cgroup_bpf_prog_detach(attr, ptype);
 	default:
 		return -EINVAL;
 	}
-
-	return cgroup_bpf_prog_detach(attr, ptype);
 }
 
 #define BPF_PROG_QUERY_LAST_FIELD query.prog_cnt
@@ -2726,7 +2707,7 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_SYSCTL:
 	case BPF_CGROUP_GETSOCKOPT:
 	case BPF_CGROUP_SETSOCKOPT:
-		break;
+		return cgroup_bpf_prog_query(attr, uattr);
 	case BPF_LIRC_MODE2:
 		return lirc_prog_query(attr, uattr);
 	case BPF_FLOW_DISSECTOR:
@@ -2734,8 +2715,6 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	default:
 		return -EINVAL;
 	}
-
-	return cgroup_bpf_prog_query(attr, uattr);
 }
 
 #define BPF_PROG_TEST_RUN_LAST_FIELD test.ctx_out

commit 9fb16955fb661945ddffce4504dcffbe55cd518a
Merge: 1f074e677a34 1b649e0bcae7
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Mar 25 18:58:11 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Overlapping header include additions in macsec.c
    
    A bug fix in 'net' overlapping with the removal of 'version'
    string in ena_netdev.c
    
    Overlapping test additions in selftests Makefile
    
    Overlapping PCI ID table adjustments in iwlwifi driver.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 5c6f25887963f15492b604dd25cb149c501bbabf
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri Mar 20 17:22:58 2020 +0100

    bpf: Explicitly memset some bpf info structures declared on the stack
    
    Trying to initialize a structure with "= {};" will not always clean out
    all padding locations in a structure. So be explicit and call memset to
    initialize everything for a number of bpf information structures that
    are then copied from userspace, sometimes from smaller memory locations
    than the size of the structure.
    
    Reported-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200320162258.GA794295@kroah.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 3ac1629cf00e..966b7b34cde0 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2795,7 +2795,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 				   union bpf_attr __user *uattr)
 {
 	struct bpf_prog_info __user *uinfo = u64_to_user_ptr(attr->info.info);
-	struct bpf_prog_info info = {};
+	struct bpf_prog_info info;
 	u32 info_len = attr->info.info_len;
 	struct bpf_prog_stats stats;
 	char __user *uinsns;
@@ -2807,6 +2807,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		return err;
 	info_len = min_t(u32, sizeof(info), info_len);
 
+	memset(&info, 0, sizeof(info));
 	if (copy_from_user(&info, uinfo, info_len))
 		return -EFAULT;
 
@@ -3070,7 +3071,7 @@ static int bpf_map_get_info_by_fd(struct bpf_map *map,
 				  union bpf_attr __user *uattr)
 {
 	struct bpf_map_info __user *uinfo = u64_to_user_ptr(attr->info.info);
-	struct bpf_map_info info = {};
+	struct bpf_map_info info;
 	u32 info_len = attr->info.info_len;
 	int err;
 
@@ -3079,6 +3080,7 @@ static int bpf_map_get_info_by_fd(struct bpf_map *map,
 		return err;
 	info_len = min_t(u32, sizeof(info), info_len);
 
+	memset(&info, 0, sizeof(info));
 	info.type = map->map_type;
 	info.id = map->id;
 	info.key_size = map->key_size;

commit 8096f229421f7b22433775e928d506f0342e5907
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri Mar 20 10:48:13 2020 +0100

    bpf: Explicitly memset the bpf_attr structure
    
    For the bpf syscall, we are relying on the compiler to properly zero out
    the bpf_attr union that we copy userspace data into. Unfortunately that
    doesn't always work properly, padding and other oddities might not be
    correctly zeroed, and in some tests odd things have been found when the
    stack is pre-initialized to other values.
    
    Fix this by explicitly memsetting the structure to 0 before using it.
    
    Reported-by: Maciej Żenczykowski <maze@google.com>
    Reported-by: John Stultz <john.stultz@linaro.org>
    Reported-by: Alexander Potapenko <glider@google.com>
    Reported-by: Alistair Delva <adelva@google.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://android-review.googlesource.com/c/kernel/common/+/1235490
    Link: https://lore.kernel.org/bpf/20200320094813.GA421650@kroah.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2857b7dda382..3ac1629cf00e 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3362,7 +3362,7 @@ static int bpf_map_do_batch(const union bpf_attr *attr,
 
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
-	union bpf_attr attr = {};
+	union bpf_attr attr;
 	int err;
 
 	if (sysctl_unprivileged_bpf_disabled && !capable(CAP_SYS_ADMIN))
@@ -3374,6 +3374,7 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	size = min_t(u32, size, sizeof(attr));
 
 	/* copy attributes from user space, may be less than sizeof(bpf_attr) */
+	memset(&attr, 0, sizeof(attr));
 	if (copy_from_user(&attr, uattr, size) != 0)
 		return -EFAULT;
 

commit 8e7ae2518f5265f0ef09d561748098fde5a87ccd
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Mar 13 18:02:09 2020 -0700

    bpf: Sanitize the bpf_struct_ops tcp-cc name
    
    The bpf_struct_ops tcp-cc name should be sanitized in order to
    avoid problematic chars (e.g. whitespaces).
    
    This patch reuses the bpf_obj_name_cpy() for accepting the same set
    of characters in order to keep a consistent bpf programming experience.
    A "size" param is added.  Also, the strlen is returned on success so
    that the caller (like the bpf_tcp_ca here) can error out on empty name.
    The existing callers of the bpf_obj_name_cpy() only need to change the
    testing statement to "if (err < 0)".  For all these existing callers,
    the err will be overwritten later, so no extra change is needed
    for the new strlen return value.
    
    v3:
      - reverse xmas tree style
    v2:
      - Save the orig_src to avoid "end - size" (Andrii)
    
    Fixes: 0baf26b0fcd7 ("bpf: tcp: Support tcp_congestion_ops in bpf")
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20200314010209.1131542-1-kafai@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0c7fb0d4836d..2857b7dda382 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -696,14 +696,15 @@ int bpf_get_file_flag(int flags)
 		   offsetof(union bpf_attr, CMD##_LAST_FIELD) - \
 		   sizeof(attr->CMD##_LAST_FIELD)) != NULL
 
-/* dst and src must have at least BPF_OBJ_NAME_LEN number of bytes.
- * Return 0 on success and < 0 on error.
+/* dst and src must have at least "size" number of bytes.
+ * Return strlen on success and < 0 on error.
  */
-static int bpf_obj_name_cpy(char *dst, const char *src)
+int bpf_obj_name_cpy(char *dst, const char *src, unsigned int size)
 {
-	const char *end = src + BPF_OBJ_NAME_LEN;
+	const char *end = src + size;
+	const char *orig_src = src;
 
-	memset(dst, 0, BPF_OBJ_NAME_LEN);
+	memset(dst, 0, size);
 	/* Copy all isalnum(), '_' and '.' chars. */
 	while (src < end && *src) {
 		if (!isalnum(*src) &&
@@ -712,11 +713,11 @@ static int bpf_obj_name_cpy(char *dst, const char *src)
 		*dst++ = *src++;
 	}
 
-	/* No '\0' found in BPF_OBJ_NAME_LEN number of bytes */
+	/* No '\0' found in "size" number of bytes */
 	if (src == end)
 		return -EINVAL;
 
-	return 0;
+	return src - orig_src;
 }
 
 int map_check_no_btf(const struct bpf_map *map,
@@ -810,8 +811,9 @@ static int map_create(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
-	err = bpf_obj_name_cpy(map->name, attr->map_name);
-	if (err)
+	err = bpf_obj_name_cpy(map->name, attr->map_name,
+			       sizeof(attr->map_name));
+	if (err < 0)
 		goto free_map;
 
 	atomic64_set(&map->refcnt, 1);
@@ -2098,8 +2100,9 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 		goto free_prog;
 
 	prog->aux->load_time = ktime_get_boottime_ns();
-	err = bpf_obj_name_cpy(prog->aux->name, attr->prog_name);
-	if (err)
+	err = bpf_obj_name_cpy(prog->aux->name, attr->prog_name,
+			       sizeof(attr->prog_name));
+	if (err < 0)
 		goto free_prog;
 
 	/* run eBPF verifier */

commit 98868668367b24487c0b0b3298d7ca98409baf07
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Thu Mar 12 17:21:28 2020 -0700

    bpf: Abstract away entire bpf_link clean up procedure
    
    Instead of requiring users to do three steps for cleaning up bpf_link, its
    anon_inode file, and unused fd, abstract that away into bpf_link_cleanup()
    helper. bpf_link_defunct() is removed, as it shouldn't be needed as an
    individual operation anymore.
    
    v1->v2:
    - keep bpf_link_cleanup() static for now (Daniel).
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20200313002128.2028680-1-andriin@fb.com
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b2f73ecacced..85567a6ea5f9 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2188,9 +2188,17 @@ void bpf_link_init(struct bpf_link *link, const struct bpf_link_ops *ops,
 	link->prog = prog;
 }
 
-void bpf_link_defunct(struct bpf_link *link)
+/* Clean up bpf_link and corresponding anon_inode file and FD. After
+ * anon_inode is created, bpf_link can't be just kfree()'d due to deferred
+ * anon_inode's release() call. This helper manages marking bpf_link as
+ * defunct, releases anon_inode file and puts reserved FD.
+ */
+static void bpf_link_cleanup(struct bpf_link *link, struct file *link_file,
+			     int link_fd)
 {
 	link->prog = NULL;
+	fput(link_file);
+	put_unused_fd(link_fd);
 }
 
 void bpf_link_inc(struct bpf_link *link)
@@ -2383,9 +2391,7 @@ static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 
 	err = bpf_trampoline_link_prog(prog);
 	if (err) {
-		bpf_link_defunct(&link->link);
-		fput(link_file);
-		put_unused_fd(link_fd);
+		bpf_link_cleanup(&link->link, link_file, link_fd);
 		goto out_put_prog;
 	}
 
@@ -2498,9 +2504,7 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 
 	err = bpf_probe_register(link->btp, prog);
 	if (err) {
-		bpf_link_defunct(&link->link);
-		fput(link_file);
-		put_unused_fd(link_fd);
+		bpf_link_cleanup(&link->link, link_file, link_fd);
 		goto out_put_btp;
 	}
 

commit babf3164095b0670435910340c2a1eec37757b57
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Mon Mar 9 16:10:51 2020 -0700

    bpf: Add bpf_link_new_file that doesn't install FD
    
    Add bpf_link_new_file() API for cases when we need to ensure anon_inode is
    successfully created before we proceed with expensive BPF program attachment
    procedure, which will require equally (if not more so) expensive and
    potentially failing compensation detachment procedure just because anon_inode
    creation failed. This API allows to simplify code by ensuring first that
    anon_inode is created and after BPF program is attached proceed with
    fd_install() that can't fail.
    
    After anon_inode file is created, link can't be just kfree()'d anymore,
    because its destruction will be performed by deferred file_operations->release
    call. For this, bpf_link API required specifying two separate operations:
    release() and dealloc(), former performing detachment only, while the latter
    frees memory used by bpf_link itself. dealloc() needs to be specified, because
    struct bpf_link is frequently embedded into link type-specific container
    struct (e.g., struct bpf_raw_tp_link), so bpf_link itself doesn't know how to
    properly free the memory. In case when anon_inode file was successfully
    created, but subsequent BPF attachment failed, bpf_link needs to be marked as
    "defunct", so that file's release() callback will perform only memory
    deallocation, but no detachment.
    
    Convert raw tracepoint and tracing attachment to new API and eliminate
    detachment from error handling path.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Link: https://lore.kernel.org/bpf/20200309231051.1270337-1-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 7ce0815793dd..b2f73ecacced 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2188,6 +2188,11 @@ void bpf_link_init(struct bpf_link *link, const struct bpf_link_ops *ops,
 	link->prog = prog;
 }
 
+void bpf_link_defunct(struct bpf_link *link)
+{
+	link->prog = NULL;
+}
+
 void bpf_link_inc(struct bpf_link *link)
 {
 	atomic64_inc(&link->refcnt);
@@ -2196,14 +2201,13 @@ void bpf_link_inc(struct bpf_link *link)
 /* bpf_link_free is guaranteed to be called from process context */
 static void bpf_link_free(struct bpf_link *link)
 {
-	struct bpf_prog *prog;
-
-	/* remember prog locally, because release below will free link memory */
-	prog = link->prog;
-	/* extra clean up and kfree of container link struct */
-	link->ops->release(link);
-	/* no more accesing of link members after this point */
-	bpf_prog_put(prog);
+	if (link->prog) {
+		/* detach BPF program, clean up used resources */
+		link->ops->release(link);
+		bpf_prog_put(link->prog);
+	}
+	/* free bpf_link and its containing memory */
+	link->ops->dealloc(link);
 }
 
 static void bpf_link_put_deferred(struct work_struct *work)
@@ -2281,6 +2285,33 @@ int bpf_link_new_fd(struct bpf_link *link)
 	return anon_inode_getfd("bpf-link", &bpf_link_fops, link, O_CLOEXEC);
 }
 
+/* Similar to bpf_link_new_fd, create anon_inode for given bpf_link, but
+ * instead of immediately installing fd in fdtable, just reserve it and
+ * return. Caller then need to either install it with fd_install(fd, file) or
+ * release with put_unused_fd(fd).
+ * This is useful for cases when bpf_link attachment/detachment are
+ * complicated and expensive operations and should be delayed until all the fd
+ * reservation and anon_inode creation succeeds.
+ */
+struct file *bpf_link_new_file(struct bpf_link *link, int *reserved_fd)
+{
+	struct file *file;
+	int fd;
+
+	fd = get_unused_fd_flags(O_CLOEXEC);
+	if (fd < 0)
+		return ERR_PTR(fd);
+
+	file = anon_inode_getfile("bpf_link", &bpf_link_fops, link, O_CLOEXEC);
+	if (IS_ERR(file)) {
+		put_unused_fd(fd);
+		return file;
+	}
+
+	*reserved_fd = fd;
+	return file;
+}
+
 struct bpf_link *bpf_link_get_from_fd(u32 ufd)
 {
 	struct fd f = fdget(ufd);
@@ -2305,21 +2336,27 @@ struct bpf_tracing_link {
 };
 
 static void bpf_tracing_link_release(struct bpf_link *link)
+{
+	WARN_ON_ONCE(bpf_trampoline_unlink_prog(link->prog));
+}
+
+static void bpf_tracing_link_dealloc(struct bpf_link *link)
 {
 	struct bpf_tracing_link *tr_link =
 		container_of(link, struct bpf_tracing_link, link);
 
-	WARN_ON_ONCE(bpf_trampoline_unlink_prog(link->prog));
 	kfree(tr_link);
 }
 
 static const struct bpf_link_ops bpf_tracing_link_lops = {
 	.release = bpf_tracing_link_release,
+	.dealloc = bpf_tracing_link_dealloc,
 };
 
 static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 {
 	struct bpf_tracing_link *link;
+	struct file *link_file;
 	int link_fd, err;
 
 	if (prog->expected_attach_type != BPF_TRACE_FENTRY &&
@@ -2337,20 +2374,24 @@ static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 	}
 	bpf_link_init(&link->link, &bpf_tracing_link_lops, prog);
 
-	err = bpf_trampoline_link_prog(prog);
-	if (err)
-		goto out_free_link;
+	link_file = bpf_link_new_file(&link->link, &link_fd);
+	if (IS_ERR(link_file)) {
+		kfree(link);
+		err = PTR_ERR(link_file);
+		goto out_put_prog;
+	}
 
-	link_fd = bpf_link_new_fd(&link->link);
-	if (link_fd < 0) {
-		WARN_ON_ONCE(bpf_trampoline_unlink_prog(prog));
-		err = link_fd;
-		goto out_free_link;
+	err = bpf_trampoline_link_prog(prog);
+	if (err) {
+		bpf_link_defunct(&link->link);
+		fput(link_file);
+		put_unused_fd(link_fd);
+		goto out_put_prog;
 	}
+
+	fd_install(link_fd, link_file);
 	return link_fd;
 
-out_free_link:
-	kfree(link);
 out_put_prog:
 	bpf_prog_put(prog);
 	return err;
@@ -2368,19 +2409,28 @@ static void bpf_raw_tp_link_release(struct bpf_link *link)
 
 	bpf_probe_unregister(raw_tp->btp, raw_tp->link.prog);
 	bpf_put_raw_tracepoint(raw_tp->btp);
+}
+
+static void bpf_raw_tp_link_dealloc(struct bpf_link *link)
+{
+	struct bpf_raw_tp_link *raw_tp =
+		container_of(link, struct bpf_raw_tp_link, link);
+
 	kfree(raw_tp);
 }
 
 static const struct bpf_link_ops bpf_raw_tp_lops = {
 	.release = bpf_raw_tp_link_release,
+	.dealloc = bpf_raw_tp_link_dealloc,
 };
 
 #define BPF_RAW_TRACEPOINT_OPEN_LAST_FIELD raw_tracepoint.prog_fd
 
 static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 {
-	struct bpf_raw_tp_link *raw_tp;
+	struct bpf_raw_tp_link *link;
 	struct bpf_raw_event_map *btp;
+	struct file *link_file;
 	struct bpf_prog *prog;
 	const char *tp_name;
 	char buf[128];
@@ -2431,28 +2481,32 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 		goto out_put_prog;
 	}
 
-	raw_tp = kzalloc(sizeof(*raw_tp), GFP_USER);
-	if (!raw_tp) {
+	link = kzalloc(sizeof(*link), GFP_USER);
+	if (!link) {
 		err = -ENOMEM;
 		goto out_put_btp;
 	}
-	bpf_link_init(&raw_tp->link, &bpf_raw_tp_lops, prog);
-	raw_tp->btp = btp;
+	bpf_link_init(&link->link, &bpf_raw_tp_lops, prog);
+	link->btp = btp;
 
-	err = bpf_probe_register(raw_tp->btp, prog);
-	if (err)
-		goto out_free_tp;
+	link_file = bpf_link_new_file(&link->link, &link_fd);
+	if (IS_ERR(link_file)) {
+		kfree(link);
+		err = PTR_ERR(link_file);
+		goto out_put_btp;
+	}
 
-	link_fd = bpf_link_new_fd(&raw_tp->link);
-	if (link_fd < 0) {
-		bpf_probe_unregister(raw_tp->btp, prog);
-		err = link_fd;
-		goto out_free_tp;
+	err = bpf_probe_register(link->btp, prog);
+	if (err) {
+		bpf_link_defunct(&link->link);
+		fput(link_file);
+		put_unused_fd(link_fd);
+		goto out_put_btp;
 	}
+
+	fd_install(link_fd, link_file);
 	return link_fd;
 
-out_free_tp:
-	kfree(raw_tp);
 out_put_btp:
 	bpf_put_raw_tracepoint(btp);
 out_put_prog:

commit 849b4d94582a966ecb533448415462846da1f0fa
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Mar 4 17:34:54 2020 -0800

    bpf: Do not allow map_freeze in struct_ops map
    
    struct_ops map cannot support map_freeze.  Otherwise, a struct_ops
    cannot be unregistered from the subsystem.
    
    Fixes: 85d33df357b6 ("bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS")
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200305013454.535397-1-kafai@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index a91ad518c050..0c7fb0d4836d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1510,6 +1510,11 @@ static int map_freeze(const union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
+	if (map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
+		fdput(f);
+		return -ENOTSUPP;
+	}
+
 	mutex_lock(&map->freeze_mutex);
 
 	if (map->writecnt) {

commit ae24082331d9bbaae283aafbe930a8f0eb85605a
Author: KP Singh <kpsingh@google.com>
Date:   Wed Mar 4 20:18:49 2020 +0100

    bpf: Introduce BPF_MODIFY_RETURN
    
    When multiple programs are attached, each program receives the return
    value from the previous program on the stack and the last program
    provides the return value to the attached function.
    
    The fmod_ret bpf programs are run after the fentry programs and before
    the fexit programs. The original function is only called if all the
    fmod_ret programs return 0 to avoid any unintended side-effects. The
    success value, i.e. 0 is not currently configurable but can be made so
    where user-space can specify it at load time.
    
    For example:
    
    int func_to_be_attached(int a, int b)
    {  <--- do_fentry
    
    do_fmod_ret:
       <update ret by calling fmod_ret>
       if (ret != 0)
            goto do_fexit;
    
    original_function:
    
        <side_effects_happen_here>
    
    }  <--- do_fexit
    
    The fmod_ret program attached to this function can be defined as:
    
    SEC("fmod_ret/func_to_be_attached")
    int BPF_PROG(func_name, int a, int b, int ret)
    {
            // This will skip the original function logic.
            return 1;
    }
    
    The first fmod_ret program is passed 0 in its return argument.
    
    Signed-off-by: KP Singh <kpsingh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20200304191853.1529-4-kpsingh@chromium.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 13de65363ba2..7ce0815793dd 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2324,6 +2324,7 @@ static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 
 	if (prog->expected_attach_type != BPF_TRACE_FENTRY &&
 	    prog->expected_attach_type != BPF_TRACE_FEXIT &&
+	    prog->expected_attach_type != BPF_MODIFY_RETURN &&
 	    prog->type != BPF_PROG_TYPE_EXT) {
 		err = -EINVAL;
 		goto out_put_prog;

commit 70ed506c3bbcfa846d4636b23051ca79fa4781f7
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Mon Mar 2 20:31:57 2020 -0800

    bpf: Introduce pinnable bpf_link abstraction
    
    Introduce bpf_link abstraction, representing an attachment of BPF program to
    a BPF hook point (e.g., tracepoint, perf event, etc). bpf_link encapsulates
    ownership of attached BPF program, reference counting of a link itself, when
    reference from multiple anonymous inodes, as well as ensures that release
    callback will be called from a process context, so that users can safely take
    mutex locks and sleep.
    
    Additionally, with a new abstraction it's now possible to generalize pinning
    of a link object in BPF FS, allowing to explicitly prevent BPF program
    detachment on process exit by pinning it in a BPF FS and let it open from
    independent other process to keep working with it.
    
    Convert two existing bpf_link-like objects (raw tracepoint and tracing BPF
    program attachments) into utilizing bpf_link framework, making them pinnable
    in BPF FS. More FD-based bpf_links will be added in follow up patches.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200303043159.323675-2-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c536c65256ad..13de65363ba2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2173,24 +2173,154 @@ static int bpf_obj_get(const union bpf_attr *attr)
 				attr->file_flags);
 }
 
-static int bpf_tracing_prog_release(struct inode *inode, struct file *filp)
+struct bpf_link {
+	atomic64_t refcnt;
+	const struct bpf_link_ops *ops;
+	struct bpf_prog *prog;
+	struct work_struct work;
+};
+
+void bpf_link_init(struct bpf_link *link, const struct bpf_link_ops *ops,
+		   struct bpf_prog *prog)
 {
-	struct bpf_prog *prog = filp->private_data;
+	atomic64_set(&link->refcnt, 1);
+	link->ops = ops;
+	link->prog = prog;
+}
+
+void bpf_link_inc(struct bpf_link *link)
+{
+	atomic64_inc(&link->refcnt);
+}
+
+/* bpf_link_free is guaranteed to be called from process context */
+static void bpf_link_free(struct bpf_link *link)
+{
+	struct bpf_prog *prog;
 
-	WARN_ON_ONCE(bpf_trampoline_unlink_prog(prog));
+	/* remember prog locally, because release below will free link memory */
+	prog = link->prog;
+	/* extra clean up and kfree of container link struct */
+	link->ops->release(link);
+	/* no more accesing of link members after this point */
 	bpf_prog_put(prog);
+}
+
+static void bpf_link_put_deferred(struct work_struct *work)
+{
+	struct bpf_link *link = container_of(work, struct bpf_link, work);
+
+	bpf_link_free(link);
+}
+
+/* bpf_link_put can be called from atomic context, but ensures that resources
+ * are freed from process context
+ */
+void bpf_link_put(struct bpf_link *link)
+{
+	if (!atomic64_dec_and_test(&link->refcnt))
+		return;
+
+	if (in_atomic()) {
+		INIT_WORK(&link->work, bpf_link_put_deferred);
+		schedule_work(&link->work);
+	} else {
+		bpf_link_free(link);
+	}
+}
+
+static int bpf_link_release(struct inode *inode, struct file *filp)
+{
+	struct bpf_link *link = filp->private_data;
+
+	bpf_link_put(link);
 	return 0;
 }
 
-static const struct file_operations bpf_tracing_prog_fops = {
-	.release	= bpf_tracing_prog_release,
+#ifdef CONFIG_PROC_FS
+static const struct bpf_link_ops bpf_raw_tp_lops;
+static const struct bpf_link_ops bpf_tracing_link_lops;
+static const struct bpf_link_ops bpf_xdp_link_lops;
+
+static void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp)
+{
+	const struct bpf_link *link = filp->private_data;
+	const struct bpf_prog *prog = link->prog;
+	char prog_tag[sizeof(prog->tag) * 2 + 1] = { };
+	const char *link_type;
+
+	if (link->ops == &bpf_raw_tp_lops)
+		link_type = "raw_tracepoint";
+	else if (link->ops == &bpf_tracing_link_lops)
+		link_type = "tracing";
+	else
+		link_type = "unknown";
+
+	bin2hex(prog_tag, prog->tag, sizeof(prog->tag));
+	seq_printf(m,
+		   "link_type:\t%s\n"
+		   "prog_tag:\t%s\n"
+		   "prog_id:\t%u\n",
+		   link_type,
+		   prog_tag,
+		   prog->aux->id);
+}
+#endif
+
+const struct file_operations bpf_link_fops = {
+#ifdef CONFIG_PROC_FS
+	.show_fdinfo	= bpf_link_show_fdinfo,
+#endif
+	.release	= bpf_link_release,
 	.read		= bpf_dummy_read,
 	.write		= bpf_dummy_write,
 };
 
+int bpf_link_new_fd(struct bpf_link *link)
+{
+	return anon_inode_getfd("bpf-link", &bpf_link_fops, link, O_CLOEXEC);
+}
+
+struct bpf_link *bpf_link_get_from_fd(u32 ufd)
+{
+	struct fd f = fdget(ufd);
+	struct bpf_link *link;
+
+	if (!f.file)
+		return ERR_PTR(-EBADF);
+	if (f.file->f_op != &bpf_link_fops) {
+		fdput(f);
+		return ERR_PTR(-EINVAL);
+	}
+
+	link = f.file->private_data;
+	bpf_link_inc(link);
+	fdput(f);
+
+	return link;
+}
+
+struct bpf_tracing_link {
+	struct bpf_link link;
+};
+
+static void bpf_tracing_link_release(struct bpf_link *link)
+{
+	struct bpf_tracing_link *tr_link =
+		container_of(link, struct bpf_tracing_link, link);
+
+	WARN_ON_ONCE(bpf_trampoline_unlink_prog(link->prog));
+	kfree(tr_link);
+}
+
+static const struct bpf_link_ops bpf_tracing_link_lops = {
+	.release = bpf_tracing_link_release,
+};
+
 static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 {
-	int tr_fd, err;
+	struct bpf_tracing_link *link;
+	int link_fd, err;
 
 	if (prog->expected_attach_type != BPF_TRACE_FENTRY &&
 	    prog->expected_attach_type != BPF_TRACE_FEXIT &&
@@ -2199,58 +2329,61 @@ static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 		goto out_put_prog;
 	}
 
+	link = kzalloc(sizeof(*link), GFP_USER);
+	if (!link) {
+		err = -ENOMEM;
+		goto out_put_prog;
+	}
+	bpf_link_init(&link->link, &bpf_tracing_link_lops, prog);
+
 	err = bpf_trampoline_link_prog(prog);
 	if (err)
-		goto out_put_prog;
+		goto out_free_link;
 
-	tr_fd = anon_inode_getfd("bpf-tracing-prog", &bpf_tracing_prog_fops,
-				 prog, O_CLOEXEC);
-	if (tr_fd < 0) {
+	link_fd = bpf_link_new_fd(&link->link);
+	if (link_fd < 0) {
 		WARN_ON_ONCE(bpf_trampoline_unlink_prog(prog));
-		err = tr_fd;
-		goto out_put_prog;
+		err = link_fd;
+		goto out_free_link;
 	}
-	return tr_fd;
+	return link_fd;
 
+out_free_link:
+	kfree(link);
 out_put_prog:
 	bpf_prog_put(prog);
 	return err;
 }
 
-struct bpf_raw_tracepoint {
+struct bpf_raw_tp_link {
+	struct bpf_link link;
 	struct bpf_raw_event_map *btp;
-	struct bpf_prog *prog;
 };
 
-static int bpf_raw_tracepoint_release(struct inode *inode, struct file *filp)
+static void bpf_raw_tp_link_release(struct bpf_link *link)
 {
-	struct bpf_raw_tracepoint *raw_tp = filp->private_data;
+	struct bpf_raw_tp_link *raw_tp =
+		container_of(link, struct bpf_raw_tp_link, link);
 
-	if (raw_tp->prog) {
-		bpf_probe_unregister(raw_tp->btp, raw_tp->prog);
-		bpf_prog_put(raw_tp->prog);
-	}
+	bpf_probe_unregister(raw_tp->btp, raw_tp->link.prog);
 	bpf_put_raw_tracepoint(raw_tp->btp);
 	kfree(raw_tp);
-	return 0;
 }
 
-static const struct file_operations bpf_raw_tp_fops = {
-	.release	= bpf_raw_tracepoint_release,
-	.read		= bpf_dummy_read,
-	.write		= bpf_dummy_write,
+static const struct bpf_link_ops bpf_raw_tp_lops = {
+	.release = bpf_raw_tp_link_release,
 };
 
 #define BPF_RAW_TRACEPOINT_OPEN_LAST_FIELD raw_tracepoint.prog_fd
 
 static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 {
-	struct bpf_raw_tracepoint *raw_tp;
+	struct bpf_raw_tp_link *raw_tp;
 	struct bpf_raw_event_map *btp;
 	struct bpf_prog *prog;
 	const char *tp_name;
 	char buf[128];
-	int tp_fd, err;
+	int link_fd, err;
 
 	if (CHECK_ATTR(BPF_RAW_TRACEPOINT_OPEN))
 		return -EINVAL;
@@ -2302,21 +2435,20 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 		err = -ENOMEM;
 		goto out_put_btp;
 	}
+	bpf_link_init(&raw_tp->link, &bpf_raw_tp_lops, prog);
 	raw_tp->btp = btp;
-	raw_tp->prog = prog;
 
 	err = bpf_probe_register(raw_tp->btp, prog);
 	if (err)
 		goto out_free_tp;
 
-	tp_fd = anon_inode_getfd("bpf-raw-tracepoint", &bpf_raw_tp_fops, raw_tp,
-				 O_CLOEXEC);
-	if (tp_fd < 0) {
+	link_fd = bpf_link_new_fd(&raw_tp->link);
+	if (link_fd < 0) {
 		bpf_probe_unregister(raw_tp->btp, prog);
-		err = tp_fd;
+		err = link_fd;
 		goto out_free_tp;
 	}
-	return tp_fd;
+	return link_fd;
 
 out_free_tp:
 	kfree(raw_tp);
@@ -3266,15 +3398,21 @@ static int bpf_task_fd_query(const union bpf_attr *attr,
 	if (err)
 		goto out;
 
-	if (file->f_op == &bpf_raw_tp_fops) {
-		struct bpf_raw_tracepoint *raw_tp = file->private_data;
-		struct bpf_raw_event_map *btp = raw_tp->btp;
+	if (file->f_op == &bpf_link_fops) {
+		struct bpf_link *link = file->private_data;
 
-		err = bpf_task_fd_query_copy(attr, uattr,
-					     raw_tp->prog->aux->id,
-					     BPF_FD_TYPE_RAW_TRACEPOINT,
-					     btp->tp->name, 0, 0);
-		goto put_file;
+		if (link->ops == &bpf_raw_tp_lops) {
+			struct bpf_raw_tp_link *raw_tp =
+				container_of(link, struct bpf_raw_tp_link, link);
+			struct bpf_raw_event_map *btp = raw_tp->btp;
+
+			err = bpf_task_fd_query_copy(attr, uattr,
+						     raw_tp->link.prog->aux->id,
+						     BPF_FD_TYPE_RAW_TRACEPOINT,
+						     btp->tp->name, 0, 0);
+			goto put_file;
+		}
+		goto out_not_supp;
 	}
 
 	event = perf_get_event(file);
@@ -3294,6 +3432,7 @@ static int bpf_task_fd_query(const union bpf_attr *attr,
 		goto put_file;
 	}
 
+out_not_supp:
 	err = -ENOTSUPP;
 put_file:
 	fput(file);

commit 1ed4d92458a969e71e7914550b6f0c730c14d84e
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Tue Feb 25 15:04:21 2020 -0800

    bpf: INET_DIAG support in bpf_sk_storage
    
    This patch adds INET_DIAG support to bpf_sk_storage.
    
    1. Although this series adds bpf_sk_storage diag capability to inet sk,
       bpf_sk_storage is in general applicable to all fullsock.  Hence, the
       bpf_sk_storage logic will operate on SK_DIAG_* nlattr.  The caller
       will pass in its specific nesting nlattr (e.g. INET_DIAG_*) as
       the argument.
    
    2. The request will be like:
            INET_DIAG_REQ_SK_BPF_STORAGES (nla_nest) (defined in latter patch)
                    SK_DIAG_BPF_STORAGE_REQ_MAP_FD (nla_put_u32)
                    SK_DIAG_BPF_STORAGE_REQ_MAP_FD (nla_put_u32)
                    ......
    
       Considering there could have multiple bpf_sk_storages in a sk,
       instead of reusing INET_DIAG_INFO ("ss -i"),  the user can select
       some specific bpf_sk_storage to dump by specifying an array of
       SK_DIAG_BPF_STORAGE_REQ_MAP_FD.
    
       If no SK_DIAG_BPF_STORAGE_REQ_MAP_FD is specified (i.e. an empty
       INET_DIAG_REQ_SK_BPF_STORAGES), it will dump all bpf_sk_storages
       of a sk.
    
    3. The reply will be like:
            INET_DIAG_BPF_SK_STORAGES (nla_nest) (defined in latter patch)
                    SK_DIAG_BPF_STORAGE (nla_nest)
                            SK_DIAG_BPF_STORAGE_MAP_ID (nla_put_u32)
                            SK_DIAG_BPF_STORAGE_MAP_VALUE (nla_reserve_64bit)
                    SK_DIAG_BPF_STORAGE (nla_nest)
                            SK_DIAG_BPF_STORAGE_MAP_ID (nla_put_u32)
                            SK_DIAG_BPF_STORAGE_MAP_VALUE (nla_reserve_64bit)
                    ......
    
    4. Unlike other INET_DIAG info of a sk which is pretty static, the size
       required to dump the bpf_sk_storage(s) of a sk is dynamic as the
       system adding more bpf_sk_storage_map.  It is hard to set a static
       min_dump_alloc size.
    
       Hence, this series learns it at the runtime and adjust the
       cb->min_dump_alloc as it iterates all sk(s) of a system.  The
       "unsigned int *res_diag_size" in bpf_sk_storage_diag_put()
       is for this purpose.
    
       The next patch will update the cb->min_dump_alloc as it
       iterates the sk(s).
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20200225230421.1975729-1-kafai@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index a79743a89815..c536c65256ad 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -902,6 +902,21 @@ void bpf_map_inc_with_uref(struct bpf_map *map)
 }
 EXPORT_SYMBOL_GPL(bpf_map_inc_with_uref);
 
+struct bpf_map *bpf_map_get(u32 ufd)
+{
+	struct fd f = fdget(ufd);
+	struct bpf_map *map;
+
+	map = __bpf_map_get(f);
+	if (IS_ERR(map))
+		return map;
+
+	bpf_map_inc(map);
+	fdput(f);
+
+	return map;
+}
+
 struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 {
 	struct fd f = fdget(ufd);

commit b6e5dae15a61b0cc9219799926813baad0b58967
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 24 15:01:49 2020 +0100

    bpf: Replace open coded recursion prevention in sys_bpf()
    
    The required protection is that the caller cannot be migrated to a
    different CPU as these functions end up in places which take either a hash
    bucket lock or might trigger a kprobe inside the memory allocator. Both
    scenarios can lead to deadlocks. The deadlock prevention is per CPU by
    incrementing a per CPU variable which temporarily blocks the invocation of
    BPF programs from perf and kprobes.
    
    Replace the open coded preempt_[dis|en]able and __this_cpu_[inc|dec] pairs
    with the new helper functions. These functions are already prepared to make
    BPF work on PREEMPT_RT enabled kernels. No functional change for !RT
    kernels.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200224145644.317843926@linutronix.de

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index a91ad518c050..a79743a89815 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -171,11 +171,7 @@ static int bpf_map_update_value(struct bpf_map *map, struct fd f, void *key,
 						    flags);
 	}
 
-	/* must increment bpf_prog_active to avoid kprobe+bpf triggering from
-	 * inside bpf map update or delete otherwise deadlocks are possible
-	 */
-	preempt_disable();
-	__this_cpu_inc(bpf_prog_active);
+	bpf_disable_instrumentation();
 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
 		err = bpf_percpu_hash_update(map, key, value, flags);
@@ -206,8 +202,7 @@ static int bpf_map_update_value(struct bpf_map *map, struct fd f, void *key,
 		err = map->ops->map_update_elem(map, key, value, flags);
 		rcu_read_unlock();
 	}
-	__this_cpu_dec(bpf_prog_active);
-	preempt_enable();
+	bpf_enable_instrumentation();
 	maybe_wait_bpf_programs(map);
 
 	return err;
@@ -222,8 +217,7 @@ static int bpf_map_copy_value(struct bpf_map *map, void *key, void *value,
 	if (bpf_map_is_dev_bound(map))
 		return bpf_map_offload_lookup_elem(map, key, value);
 
-	preempt_disable();
-	this_cpu_inc(bpf_prog_active);
+	bpf_disable_instrumentation();
 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
 		err = bpf_percpu_hash_copy(map, key, value);
@@ -268,8 +262,7 @@ static int bpf_map_copy_value(struct bpf_map *map, void *key, void *value,
 		rcu_read_unlock();
 	}
 
-	this_cpu_dec(bpf_prog_active);
-	preempt_enable();
+	bpf_enable_instrumentation();
 	maybe_wait_bpf_programs(map);
 
 	return err;
@@ -1136,13 +1129,11 @@ static int map_delete_elem(union bpf_attr *attr)
 		goto out;
 	}
 
-	preempt_disable();
-	__this_cpu_inc(bpf_prog_active);
+	bpf_disable_instrumentation();
 	rcu_read_lock();
 	err = map->ops->map_delete_elem(map, key);
 	rcu_read_unlock();
-	__this_cpu_dec(bpf_prog_active);
-	preempt_enable();
+	bpf_enable_instrumentation();
 	maybe_wait_bpf_programs(map);
 out:
 	kfree(key);
@@ -1254,13 +1245,11 @@ int generic_map_delete_batch(struct bpf_map *map,
 			break;
 		}
 
-		preempt_disable();
-		__this_cpu_inc(bpf_prog_active);
+		bpf_disable_instrumentation();
 		rcu_read_lock();
 		err = map->ops->map_delete_elem(map, key);
 		rcu_read_unlock();
-		__this_cpu_dec(bpf_prog_active);
-		preempt_enable();
+		bpf_enable_instrumentation();
 		maybe_wait_bpf_programs(map);
 		if (err)
 			break;

commit be8704ff07d2374bcc5c675526f95e70c6459683
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Jan 20 16:53:46 2020 -0800

    bpf: Introduce dynamic program extensions
    
    Introduce dynamic program extensions. The users can load additional BPF
    functions and replace global functions in previously loaded BPF programs while
    these programs are executing.
    
    Global functions are verified individually by the verifier based on their types only.
    Hence the global function in the new program which types match older function can
    safely replace that corresponding function.
    
    This new function/program is called 'an extension' of old program. At load time
    the verifier uses (attach_prog_fd, attach_btf_id) pair to identify the function
    to be replaced. The BPF program type is derived from the target program into
    extension program. Technically bpf_verifier_ops is copied from target program.
    The BPF_PROG_TYPE_EXT program type is a placeholder. It has empty verifier_ops.
    The extension program can call the same bpf helper functions as target program.
    Single BPF_PROG_TYPE_EXT type is used to extend XDP, SKB and all other program
    types. The verifier allows only one level of replacement. Meaning that the
    extension program cannot recursively extend an extension. That also means that
    the maximum stack size is increasing from 512 to 1024 bytes and maximum
    function nesting level from 8 to 16. The programs don't always consume that
    much. The stack usage is determined by the number of on-stack variables used by
    the program. The verifier could have enforced 512 limit for combined original
    plus extension program, but it makes for difficult user experience. The main
    use case for extensions is to provide generic mechanism to plug external
    programs into policy program or function call chaining.
    
    BPF trampoline is used to track both fentry/fexit and program extensions
    because both are using the same nop slot at the beginning of every BPF
    function. Attaching fentry/fexit to a function that was replaced is not
    allowed. The opposite is true as well. Replacing a function that currently
    being analyzed with fentry/fexit is not allowed. The executable page allocated
    by BPF trampoline is not used by program extensions. This inefficiency will be
    optimized in future patches.
    
    Function by function verification of global function supports scalars and
    pointer to context only. Hence program extensions are supported for such class
    of global functions only. In the future the verifier will be extended with
    support to pointers to structures, arrays with sizes, etc.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Link: https://lore.kernel.org/bpf/20200121005348.2769920-2-ast@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 9a840c57f6df..a91ad518c050 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1932,13 +1932,15 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 		switch (prog_type) {
 		case BPF_PROG_TYPE_TRACING:
 		case BPF_PROG_TYPE_STRUCT_OPS:
+		case BPF_PROG_TYPE_EXT:
 			break;
 		default:
 			return -EINVAL;
 		}
 	}
 
-	if (prog_fd && prog_type != BPF_PROG_TYPE_TRACING)
+	if (prog_fd && prog_type != BPF_PROG_TYPE_TRACING &&
+	    prog_type != BPF_PROG_TYPE_EXT)
 		return -EINVAL;
 
 	switch (prog_type) {
@@ -1981,6 +1983,10 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 		default:
 			return -EINVAL;
 		}
+	case BPF_PROG_TYPE_EXT:
+		if (expected_attach_type)
+			return -EINVAL;
+		/* fallthrough */
 	default:
 		return 0;
 	}
@@ -2183,7 +2189,8 @@ static int bpf_tracing_prog_attach(struct bpf_prog *prog)
 	int tr_fd, err;
 
 	if (prog->expected_attach_type != BPF_TRACE_FENTRY &&
-	    prog->expected_attach_type != BPF_TRACE_FEXIT) {
+	    prog->expected_attach_type != BPF_TRACE_FEXIT &&
+	    prog->type != BPF_PROG_TYPE_EXT) {
 		err = -EINVAL;
 		goto out_put_prog;
 	}
@@ -2250,12 +2257,14 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 
 	if (prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT &&
 	    prog->type != BPF_PROG_TYPE_TRACING &&
+	    prog->type != BPF_PROG_TYPE_EXT &&
 	    prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE) {
 		err = -EINVAL;
 		goto out_put_prog;
 	}
 
-	if (prog->type == BPF_PROG_TYPE_TRACING) {
+	if (prog->type == BPF_PROG_TYPE_TRACING ||
+	    prog->type == BPF_PROG_TYPE_EXT) {
 		if (attr->raw_tracepoint.name) {
 			/* The attach point for this category of programs
 			 * should be specified via btf_id during program load.

commit 2e3a94aa2bfc6de95a0700f0a868c6f5db3a9592
Author: Brian Vazquez <brianvv@google.com>
Date:   Sun Jan 19 11:40:40 2020 -0800

    bpf: Fix memory leaks in generic update/delete batch ops
    
    Generic update/delete batch ops functions were using __bpf_copy_key
    without properly freeing the memory. Handle the memory allocation and
    copy_from_user separately.
    
    Fixes: aa2e93b8e58e ("bpf: Add generic support for update and delete batch ops")
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Brian Vazquez <brianvv@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200119194040.128369-1-brianvv@google.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c26a71460f02..9a840c57f6df 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1239,12 +1239,15 @@ int generic_map_delete_batch(struct bpf_map *map,
 	if (!max_count)
 		return 0;
 
+	key = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+	if (!key)
+		return -ENOMEM;
+
 	for (cp = 0; cp < max_count; cp++) {
-		key = __bpf_copy_key(keys + cp * map->key_size, map->key_size);
-		if (IS_ERR(key)) {
-			err = PTR_ERR(key);
+		err = -EFAULT;
+		if (copy_from_user(key, keys + cp * map->key_size,
+				   map->key_size))
 			break;
-		}
 
 		if (bpf_map_is_dev_bound(map)) {
 			err = bpf_map_offload_delete_elem(map, key);
@@ -1264,6 +1267,8 @@ int generic_map_delete_batch(struct bpf_map *map,
 	}
 	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
 		err = -EFAULT;
+
+	kfree(key);
 	return err;
 }
 
@@ -1294,18 +1299,21 @@ int generic_map_update_batch(struct bpf_map *map,
 	if (!max_count)
 		return 0;
 
+	key = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+	if (!key)
+		return -ENOMEM;
+
 	value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
-	if (!value)
+	if (!value) {
+		kfree(key);
 		return -ENOMEM;
+	}
 
 	for (cp = 0; cp < max_count; cp++) {
-		key = __bpf_copy_key(keys + cp * map->key_size, map->key_size);
-		if (IS_ERR(key)) {
-			err = PTR_ERR(key);
-			break;
-		}
 		err = -EFAULT;
-		if (copy_from_user(value, values + cp * value_size, value_size))
+		if (copy_from_user(key, keys + cp * map->key_size,
+		    map->key_size) ||
+		    copy_from_user(value, values + cp * value_size, value_size))
 			break;
 
 		err = bpf_map_update_value(map, f, key, value,

commit 81f2b572cf4fd5c4178fe0e2b5bb1ab096385fd8
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Thu Jan 16 22:53:00 2020 +0800

    bpf: Remove set but not used variable 'first_key'
    
    kernel/bpf/syscall.c: In function generic_map_lookup_batch:
    kernel/bpf/syscall.c:1339:7: warning: variable first_key set but not used [-Wunused-but-set-variable]
    
    It is never used, so remove it.
    
    Reported-by: Hulk Robot <hulkci@huawei.com>
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Brian Vazquez <brianvv@google.com>
    Link: https://lore.kernel.org/bpf/20200116145300.59056-1-yuehaibing@huawei.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0d94d361379c..c26a71460f02 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1336,7 +1336,6 @@ int generic_map_lookup_batch(struct bpf_map *map,
 	void *buf, *buf_prevkey, *prev_key, *key, *value;
 	int err, retry = MAP_LOOKUP_RETRIES;
 	u32 value_size, cp, max_count;
-	bool first_key = false;
 
 	if (attr->batch.elem_flags & ~BPF_F_LOCK)
 		return -EINVAL;
@@ -1365,7 +1364,6 @@ int generic_map_lookup_batch(struct bpf_map *map,
 	}
 
 	err = -EFAULT;
-	first_key = false;
 	prev_key = NULL;
 	if (ubatch && copy_from_user(buf_prevkey, ubatch, map->key_size))
 		goto free_buf;

commit 057996380a42bb64ccc04383cfa9c0ace4ea11f0
Author: Yonghong Song <yhs@fb.com>
Date:   Wed Jan 15 10:43:04 2020 -0800

    bpf: Add batch ops to all htab bpf map
    
    htab can't use generic batch support due some problematic behaviours
    inherent to the data structre, i.e. while iterating the bpf map  a
    concurrent program might delete the next entry that batch was about to
    use, in that case there's no easy solution to retrieve the next entry,
    the issue has been discussed multiple times (see [1] and [2]).
    
    The only way hmap can be traversed without the problem previously
    exposed is by making sure that the map is traversing entire buckets.
    This commit implements those strict requirements for hmap, the
    implementation follows the same interaction that generic support with
    some exceptions:
    
     - If keys/values buffer are not big enough to traverse a bucket,
       ENOSPC will be returned.
     - out_batch contains the value of the next bucket in the iteration, not
       the next key, but this is transparent for the user since the user
       should never use out_batch for other than bpf batch syscalls.
    
    This commits implements BPF_MAP_LOOKUP_BATCH and adds support for new
    command BPF_MAP_LOOKUP_AND_DELETE_BATCH. Note that for update/delete
    batch ops it is possible to use the generic implementations.
    
    [1] https://lore.kernel.org/bpf/20190724165803.87470-1-brianvv@google.com/
    [2] https://lore.kernel.org/bpf/20190906225434.3635421-1-yhs@fb.com/
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Brian Vazquez <brianvv@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200115184308.162644-6-brianvv@google.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ce8244d1ba99..0d94d361379c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -3310,7 +3310,8 @@ static int bpf_map_do_batch(const union bpf_attr *attr,
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
-	if (cmd == BPF_MAP_LOOKUP_BATCH &&
+	if ((cmd == BPF_MAP_LOOKUP_BATCH ||
+	     cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH) &&
 	    !(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {
 		err = -EPERM;
 		goto err_put;
@@ -3324,6 +3325,8 @@ static int bpf_map_do_batch(const union bpf_attr *attr,
 
 	if (cmd == BPF_MAP_LOOKUP_BATCH)
 		BPF_DO_BATCH(map->ops->map_lookup_batch);
+	else if (cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH)
+		BPF_DO_BATCH(map->ops->map_lookup_and_delete_batch);
 	else if (cmd == BPF_MAP_UPDATE_BATCH)
 		BPF_DO_BATCH(map->ops->map_update_batch);
 	else
@@ -3434,6 +3437,10 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_MAP_LOOKUP_BATCH:
 		err = bpf_map_do_batch(&attr, uattr, BPF_MAP_LOOKUP_BATCH);
 		break;
+	case BPF_MAP_LOOKUP_AND_DELETE_BATCH:
+		err = bpf_map_do_batch(&attr, uattr,
+				       BPF_MAP_LOOKUP_AND_DELETE_BATCH);
+		break;
 	case BPF_MAP_UPDATE_BATCH:
 		err = bpf_map_do_batch(&attr, uattr, BPF_MAP_UPDATE_BATCH);
 		break;

commit aa2e93b8e58e18442edfb2427446732415bc215e
Author: Brian Vazquez <brianvv@google.com>
Date:   Wed Jan 15 10:43:02 2020 -0800

    bpf: Add generic support for update and delete batch ops
    
    This commit adds generic support for update and delete batch ops that
    can be used for almost all the bpf maps. These commands share the same
    UAPI attr that lookup and lookup_and_delete batch ops use and the
    syscall commands are:
    
      BPF_MAP_UPDATE_BATCH
      BPF_MAP_DELETE_BATCH
    
    The main difference between update/delete and lookup batch ops is that
    for update/delete keys/values must be specified for userspace and
    because of that, neither in_batch nor out_batch are used.
    
    Suggested-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Brian Vazquez <brianvv@google.com>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200115184308.162644-4-brianvv@google.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index d604ddbb1afb..ce8244d1ba99 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1218,6 +1218,111 @@ static int map_get_next_key(union bpf_attr *attr)
 	return err;
 }
 
+int generic_map_delete_batch(struct bpf_map *map,
+			     const union bpf_attr *attr,
+			     union bpf_attr __user *uattr)
+{
+	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+	u32 cp, max_count;
+	int err = 0;
+	void *key;
+
+	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+		return -EINVAL;
+
+	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+	    !map_value_has_spin_lock(map)) {
+		return -EINVAL;
+	}
+
+	max_count = attr->batch.count;
+	if (!max_count)
+		return 0;
+
+	for (cp = 0; cp < max_count; cp++) {
+		key = __bpf_copy_key(keys + cp * map->key_size, map->key_size);
+		if (IS_ERR(key)) {
+			err = PTR_ERR(key);
+			break;
+		}
+
+		if (bpf_map_is_dev_bound(map)) {
+			err = bpf_map_offload_delete_elem(map, key);
+			break;
+		}
+
+		preempt_disable();
+		__this_cpu_inc(bpf_prog_active);
+		rcu_read_lock();
+		err = map->ops->map_delete_elem(map, key);
+		rcu_read_unlock();
+		__this_cpu_dec(bpf_prog_active);
+		preempt_enable();
+		maybe_wait_bpf_programs(map);
+		if (err)
+			break;
+	}
+	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
+		err = -EFAULT;
+	return err;
+}
+
+int generic_map_update_batch(struct bpf_map *map,
+			     const union bpf_attr *attr,
+			     union bpf_attr __user *uattr)
+{
+	void __user *values = u64_to_user_ptr(attr->batch.values);
+	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+	u32 value_size, cp, max_count;
+	int ufd = attr->map_fd;
+	void *key, *value;
+	struct fd f;
+	int err = 0;
+
+	f = fdget(ufd);
+	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+		return -EINVAL;
+
+	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+	    !map_value_has_spin_lock(map)) {
+		return -EINVAL;
+	}
+
+	value_size = bpf_map_value_size(map);
+
+	max_count = attr->batch.count;
+	if (!max_count)
+		return 0;
+
+	value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
+	if (!value)
+		return -ENOMEM;
+
+	for (cp = 0; cp < max_count; cp++) {
+		key = __bpf_copy_key(keys + cp * map->key_size, map->key_size);
+		if (IS_ERR(key)) {
+			err = PTR_ERR(key);
+			break;
+		}
+		err = -EFAULT;
+		if (copy_from_user(value, values + cp * value_size, value_size))
+			break;
+
+		err = bpf_map_update_value(map, f, key, value,
+					   attr->batch.elem_flags);
+
+		if (err)
+			break;
+	}
+
+	if (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))
+		err = -EFAULT;
+
+	kfree(value);
+	kfree(key);
+	return err;
+}
+
 #define MAP_LOOKUP_RETRIES 3
 
 int generic_map_lookup_batch(struct bpf_map *map,
@@ -3219,6 +3324,10 @@ static int bpf_map_do_batch(const union bpf_attr *attr,
 
 	if (cmd == BPF_MAP_LOOKUP_BATCH)
 		BPF_DO_BATCH(map->ops->map_lookup_batch);
+	else if (cmd == BPF_MAP_UPDATE_BATCH)
+		BPF_DO_BATCH(map->ops->map_update_batch);
+	else
+		BPF_DO_BATCH(map->ops->map_delete_batch);
 
 err_put:
 	fdput(f);
@@ -3325,6 +3434,12 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_MAP_LOOKUP_BATCH:
 		err = bpf_map_do_batch(&attr, uattr, BPF_MAP_LOOKUP_BATCH);
 		break;
+	case BPF_MAP_UPDATE_BATCH:
+		err = bpf_map_do_batch(&attr, uattr, BPF_MAP_UPDATE_BATCH);
+		break;
+	case BPF_MAP_DELETE_BATCH:
+		err = bpf_map_do_batch(&attr, uattr, BPF_MAP_DELETE_BATCH);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit cb4d03ab499d4c040f4ab6fd4389d2b49f42b5a5
Author: Brian Vazquez <brianvv@google.com>
Date:   Wed Jan 15 10:43:01 2020 -0800

    bpf: Add generic support for lookup batch op
    
    This commit introduces generic support for the bpf_map_lookup_batch.
    This implementation can be used by almost all the bpf maps since its core
    implementation is relying on the existing map_get_next_key and
    map_lookup_elem. The bpf syscall subcommand introduced is:
    
      BPF_MAP_LOOKUP_BATCH
    
    The UAPI attribute is:
    
      struct { /* struct used by BPF_MAP_*_BATCH commands */
             __aligned_u64   in_batch;       /* start batch,
                                              * NULL to start from beginning
                                              */
             __aligned_u64   out_batch;      /* output: next start batch */
             __aligned_u64   keys;
             __aligned_u64   values;
             __u32           count;          /* input/output:
                                              * input: # of key/value
                                              * elements
                                              * output: # of filled elements
                                              */
             __u32           map_fd;
             __u64           elem_flags;
             __u64           flags;
      } batch;
    
    in_batch/out_batch are opaque values use to communicate between
    user/kernel space, in_batch/out_batch must be of key_size length.
    
    To start iterating from the beginning in_batch must be null,
    count is the # of key/value elements to retrieve. Note that the 'keys'
    buffer must be a buffer of key_size * count size and the 'values' buffer
    must be value_size * count, where value_size must be aligned to 8 bytes
    by userspace if it's dealing with percpu maps. 'count' will contain the
    number of keys/values successfully retrieved. Note that 'count' is an
    input/output variable and it can contain a lower value after a call.
    
    If there's no more entries to retrieve, ENOENT will be returned. If error
    is ENOENT, count might be > 0 in case it copied some values but there were
    no more entries to retrieve.
    
    Note that if the return code is an error and not -EFAULT,
    count indicates the number of elements successfully processed.
    
    Suggested-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Brian Vazquez <brianvv@google.com>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200115184308.162644-3-brianvv@google.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 08b0b6e40454..d604ddbb1afb 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -219,10 +219,8 @@ static int bpf_map_copy_value(struct bpf_map *map, void *key, void *value,
 	void *ptr;
 	int err;
 
-	if (bpf_map_is_dev_bound(map)) {
-		err =  bpf_map_offload_lookup_elem(map, key, value);
-		return err;
-	}
+	if (bpf_map_is_dev_bound(map))
+		return bpf_map_offload_lookup_elem(map, key, value);
 
 	preempt_disable();
 	this_cpu_inc(bpf_prog_active);
@@ -1220,6 +1218,109 @@ static int map_get_next_key(union bpf_attr *attr)
 	return err;
 }
 
+#define MAP_LOOKUP_RETRIES 3
+
+int generic_map_lookup_batch(struct bpf_map *map,
+				    const union bpf_attr *attr,
+				    union bpf_attr __user *uattr)
+{
+	void __user *uobatch = u64_to_user_ptr(attr->batch.out_batch);
+	void __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);
+	void __user *values = u64_to_user_ptr(attr->batch.values);
+	void __user *keys = u64_to_user_ptr(attr->batch.keys);
+	void *buf, *buf_prevkey, *prev_key, *key, *value;
+	int err, retry = MAP_LOOKUP_RETRIES;
+	u32 value_size, cp, max_count;
+	bool first_key = false;
+
+	if (attr->batch.elem_flags & ~BPF_F_LOCK)
+		return -EINVAL;
+
+	if ((attr->batch.elem_flags & BPF_F_LOCK) &&
+	    !map_value_has_spin_lock(map))
+		return -EINVAL;
+
+	value_size = bpf_map_value_size(map);
+
+	max_count = attr->batch.count;
+	if (!max_count)
+		return 0;
+
+	if (put_user(0, &uattr->batch.count))
+		return -EFAULT;
+
+	buf_prevkey = kmalloc(map->key_size, GFP_USER | __GFP_NOWARN);
+	if (!buf_prevkey)
+		return -ENOMEM;
+
+	buf = kmalloc(map->key_size + value_size, GFP_USER | __GFP_NOWARN);
+	if (!buf) {
+		kvfree(buf_prevkey);
+		return -ENOMEM;
+	}
+
+	err = -EFAULT;
+	first_key = false;
+	prev_key = NULL;
+	if (ubatch && copy_from_user(buf_prevkey, ubatch, map->key_size))
+		goto free_buf;
+	key = buf;
+	value = key + map->key_size;
+	if (ubatch)
+		prev_key = buf_prevkey;
+
+	for (cp = 0; cp < max_count;) {
+		rcu_read_lock();
+		err = map->ops->map_get_next_key(map, prev_key, key);
+		rcu_read_unlock();
+		if (err)
+			break;
+		err = bpf_map_copy_value(map, key, value,
+					 attr->batch.elem_flags);
+
+		if (err == -ENOENT) {
+			if (retry) {
+				retry--;
+				continue;
+			}
+			err = -EINTR;
+			break;
+		}
+
+		if (err)
+			goto free_buf;
+
+		if (copy_to_user(keys + cp * map->key_size, key,
+				 map->key_size)) {
+			err = -EFAULT;
+			goto free_buf;
+		}
+		if (copy_to_user(values + cp * value_size, value, value_size)) {
+			err = -EFAULT;
+			goto free_buf;
+		}
+
+		if (!prev_key)
+			prev_key = buf_prevkey;
+
+		swap(prev_key, key);
+		retry = MAP_LOOKUP_RETRIES;
+		cp++;
+	}
+
+	if (err == -EFAULT)
+		goto free_buf;
+
+	if ((copy_to_user(&uattr->batch.count, &cp, sizeof(cp)) ||
+		    (cp && copy_to_user(uobatch, prev_key, map->key_size))))
+		err = -EFAULT;
+
+free_buf:
+	kfree(buf_prevkey);
+	kfree(buf);
+	return err;
+}
+
 #define BPF_MAP_LOOKUP_AND_DELETE_ELEM_LAST_FIELD value
 
 static int map_lookup_and_delete_elem(union bpf_attr *attr)
@@ -3076,6 +3177,54 @@ static int bpf_task_fd_query(const union bpf_attr *attr,
 	return err;
 }
 
+#define BPF_MAP_BATCH_LAST_FIELD batch.flags
+
+#define BPF_DO_BATCH(fn)			\
+	do {					\
+		if (!fn) {			\
+			err = -ENOTSUPP;	\
+			goto err_put;		\
+		}				\
+		err = fn(map, attr, uattr);	\
+	} while (0)
+
+static int bpf_map_do_batch(const union bpf_attr *attr,
+			    union bpf_attr __user *uattr,
+			    int cmd)
+{
+	struct bpf_map *map;
+	int err, ufd;
+	struct fd f;
+
+	if (CHECK_ATTR(BPF_MAP_BATCH))
+		return -EINVAL;
+
+	ufd = attr->batch.map_fd;
+	f = fdget(ufd);
+	map = __bpf_map_get(f);
+	if (IS_ERR(map))
+		return PTR_ERR(map);
+
+	if (cmd == BPF_MAP_LOOKUP_BATCH &&
+	    !(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {
+		err = -EPERM;
+		goto err_put;
+	}
+
+	if (cmd != BPF_MAP_LOOKUP_BATCH &&
+	    !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {
+		err = -EPERM;
+		goto err_put;
+	}
+
+	if (cmd == BPF_MAP_LOOKUP_BATCH)
+		BPF_DO_BATCH(map->ops->map_lookup_batch);
+
+err_put:
+	fdput(f);
+	return err;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -3173,6 +3322,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_MAP_LOOKUP_AND_DELETE_ELEM:
 		err = map_lookup_and_delete_elem(&attr);
 		break;
+	case BPF_MAP_LOOKUP_BATCH:
+		err = bpf_map_do_batch(&attr, uattr, BPF_MAP_LOOKUP_BATCH);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit 15c14a3dca421f086c187155afba3222b879472d
Author: Brian Vazquez <brianvv@google.com>
Date:   Wed Jan 15 10:43:00 2020 -0800

    bpf: Add bpf_map_{value_size, update_value, map_copy_value} functions
    
    This commit moves reusable code from map_lookup_elem and map_update_elem
    to avoid code duplication in kernel/bpf/syscall.c.
    
    Signed-off-by: Brian Vazquez <brianvv@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200115184308.162644-2-brianvv@google.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index f9db72a96ec0..08b0b6e40454 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -129,6 +129,154 @@ static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 	return map;
 }
 
+static u32 bpf_map_value_size(struct bpf_map *map)
+{
+	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY ||
+	    map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)
+		return round_up(map->value_size, 8) * num_possible_cpus();
+	else if (IS_FD_MAP(map))
+		return sizeof(u32);
+	else
+		return  map->value_size;
+}
+
+static void maybe_wait_bpf_programs(struct bpf_map *map)
+{
+	/* Wait for any running BPF programs to complete so that
+	 * userspace, when we return to it, knows that all programs
+	 * that could be running use the new map value.
+	 */
+	if (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS ||
+	    map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS)
+		synchronize_rcu();
+}
+
+static int bpf_map_update_value(struct bpf_map *map, struct fd f, void *key,
+				void *value, __u64 flags)
+{
+	int err;
+
+	/* Need to create a kthread, thus must support schedule */
+	if (bpf_map_is_dev_bound(map)) {
+		return bpf_map_offload_update_elem(map, key, value, flags);
+	} else if (map->map_type == BPF_MAP_TYPE_CPUMAP ||
+		   map->map_type == BPF_MAP_TYPE_SOCKHASH ||
+		   map->map_type == BPF_MAP_TYPE_SOCKMAP ||
+		   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
+		return map->ops->map_update_elem(map, key, value, flags);
+	} else if (IS_FD_PROG_ARRAY(map)) {
+		return bpf_fd_array_map_update_elem(map, f.file, key, value,
+						    flags);
+	}
+
+	/* must increment bpf_prog_active to avoid kprobe+bpf triggering from
+	 * inside bpf map update or delete otherwise deadlocks are possible
+	 */
+	preempt_disable();
+	__this_cpu_inc(bpf_prog_active);
+	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
+		err = bpf_percpu_hash_update(map, key, value, flags);
+	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
+		err = bpf_percpu_array_update(map, key, value, flags);
+	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {
+		err = bpf_percpu_cgroup_storage_update(map, key, value,
+						       flags);
+	} else if (IS_FD_ARRAY(map)) {
+		rcu_read_lock();
+		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
+						   flags);
+		rcu_read_unlock();
+	} else if (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {
+		rcu_read_lock();
+		err = bpf_fd_htab_map_update_elem(map, f.file, key, value,
+						  flags);
+		rcu_read_unlock();
+	} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {
+		/* rcu_read_lock() is not needed */
+		err = bpf_fd_reuseport_array_update_elem(map, key, value,
+							 flags);
+	} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||
+		   map->map_type == BPF_MAP_TYPE_STACK) {
+		err = map->ops->map_push_elem(map, value, flags);
+	} else {
+		rcu_read_lock();
+		err = map->ops->map_update_elem(map, key, value, flags);
+		rcu_read_unlock();
+	}
+	__this_cpu_dec(bpf_prog_active);
+	preempt_enable();
+	maybe_wait_bpf_programs(map);
+
+	return err;
+}
+
+static int bpf_map_copy_value(struct bpf_map *map, void *key, void *value,
+			      __u64 flags)
+{
+	void *ptr;
+	int err;
+
+	if (bpf_map_is_dev_bound(map)) {
+		err =  bpf_map_offload_lookup_elem(map, key, value);
+		return err;
+	}
+
+	preempt_disable();
+	this_cpu_inc(bpf_prog_active);
+	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
+		err = bpf_percpu_hash_copy(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
+		err = bpf_percpu_array_copy(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {
+		err = bpf_percpu_cgroup_storage_copy(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {
+		err = bpf_stackmap_copy(map, key, value);
+	} else if (IS_FD_ARRAY(map) || IS_FD_PROG_ARRAY(map)) {
+		err = bpf_fd_array_map_lookup_elem(map, key, value);
+	} else if (IS_FD_HASH(map)) {
+		err = bpf_fd_htab_map_lookup_elem(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {
+		err = bpf_fd_reuseport_array_lookup_elem(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||
+		   map->map_type == BPF_MAP_TYPE_STACK) {
+		err = map->ops->map_peek_elem(map, value);
+	} else if (map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
+		/* struct_ops map requires directly updating "value" */
+		err = bpf_struct_ops_map_sys_lookup_elem(map, key, value);
+	} else {
+		rcu_read_lock();
+		if (map->ops->map_lookup_elem_sys_only)
+			ptr = map->ops->map_lookup_elem_sys_only(map, key);
+		else
+			ptr = map->ops->map_lookup_elem(map, key);
+		if (IS_ERR(ptr)) {
+			err = PTR_ERR(ptr);
+		} else if (!ptr) {
+			err = -ENOENT;
+		} else {
+			err = 0;
+			if (flags & BPF_F_LOCK)
+				/* lock 'ptr' and copy everything but lock */
+				copy_map_value_locked(map, value, ptr, true);
+			else
+				copy_map_value(map, value, ptr);
+			/* mask lock, since value wasn't zero inited */
+			check_and_init_map_lock(map, value);
+		}
+		rcu_read_unlock();
+	}
+
+	this_cpu_dec(bpf_prog_active);
+	preempt_enable();
+	maybe_wait_bpf_programs(map);
+
+	return err;
+}
+
 static void *__bpf_map_area_alloc(u64 size, int numa_node, bool mmapable)
 {
 	/* We really just want to fail instead of triggering OOM killer
@@ -827,7 +975,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 	void __user *uvalue = u64_to_user_ptr(attr->value);
 	int ufd = attr->map_fd;
 	struct bpf_map *map;
-	void *key, *value, *ptr;
+	void *key, *value;
 	u32 value_size;
 	struct fd f;
 	int err;
@@ -859,75 +1007,14 @@ static int map_lookup_elem(union bpf_attr *attr)
 		goto err_put;
 	}
 
-	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
-	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
-	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY ||
-	    map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)
-		value_size = round_up(map->value_size, 8) * num_possible_cpus();
-	else if (IS_FD_MAP(map))
-		value_size = sizeof(u32);
-	else
-		value_size = map->value_size;
+	value_size = bpf_map_value_size(map);
 
 	err = -ENOMEM;
 	value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
 	if (!value)
 		goto free_key;
 
-	if (bpf_map_is_dev_bound(map)) {
-		err = bpf_map_offload_lookup_elem(map, key, value);
-		goto done;
-	}
-
-	preempt_disable();
-	this_cpu_inc(bpf_prog_active);
-	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
-	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
-		err = bpf_percpu_hash_copy(map, key, value);
-	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
-		err = bpf_percpu_array_copy(map, key, value);
-	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {
-		err = bpf_percpu_cgroup_storage_copy(map, key, value);
-	} else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {
-		err = bpf_stackmap_copy(map, key, value);
-	} else if (IS_FD_ARRAY(map) || IS_FD_PROG_ARRAY(map)) {
-		err = bpf_fd_array_map_lookup_elem(map, key, value);
-	} else if (IS_FD_HASH(map)) {
-		err = bpf_fd_htab_map_lookup_elem(map, key, value);
-	} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {
-		err = bpf_fd_reuseport_array_lookup_elem(map, key, value);
-	} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||
-		   map->map_type == BPF_MAP_TYPE_STACK) {
-		err = map->ops->map_peek_elem(map, value);
-	} else if (map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
-		/* struct_ops map requires directly updating "value" */
-		err = bpf_struct_ops_map_sys_lookup_elem(map, key, value);
-	} else {
-		rcu_read_lock();
-		if (map->ops->map_lookup_elem_sys_only)
-			ptr = map->ops->map_lookup_elem_sys_only(map, key);
-		else
-			ptr = map->ops->map_lookup_elem(map, key);
-		if (IS_ERR(ptr)) {
-			err = PTR_ERR(ptr);
-		} else if (!ptr) {
-			err = -ENOENT;
-		} else {
-			err = 0;
-			if (attr->flags & BPF_F_LOCK)
-				/* lock 'ptr' and copy everything but lock */
-				copy_map_value_locked(map, value, ptr, true);
-			else
-				copy_map_value(map, value, ptr);
-			/* mask lock, since value wasn't zero inited */
-			check_and_init_map_lock(map, value);
-		}
-		rcu_read_unlock();
-	}
-	this_cpu_dec(bpf_prog_active);
-	preempt_enable();
-
-done:
+	err = bpf_map_copy_value(map, key, value, attr->flags);
 	if (err)
 		goto free_value;
 
@@ -946,16 +1033,6 @@ static int map_lookup_elem(union bpf_attr *attr)
 	return err;
 }
 
-static void maybe_wait_bpf_programs(struct bpf_map *map)
-{
-	/* Wait for any running BPF programs to complete so that
-	 * userspace, when we return to it, knows that all programs
-	 * that could be running use the new map value.
-	 */
-	if (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS ||
-	    map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS)
-		synchronize_rcu();
-}
 
 #define BPF_MAP_UPDATE_ELEM_LAST_FIELD flags
 
@@ -1011,61 +1088,8 @@ static int map_update_elem(union bpf_attr *attr)
 	if (copy_from_user(value, uvalue, value_size) != 0)
 		goto free_value;
 
-	/* Need to create a kthread, thus must support schedule */
-	if (bpf_map_is_dev_bound(map)) {
-		err = bpf_map_offload_update_elem(map, key, value, attr->flags);
-		goto out;
-	} else if (map->map_type == BPF_MAP_TYPE_CPUMAP ||
-		   map->map_type == BPF_MAP_TYPE_SOCKHASH ||
-		   map->map_type == BPF_MAP_TYPE_SOCKMAP ||
-		   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
-		err = map->ops->map_update_elem(map, key, value, attr->flags);
-		goto out;
-	} else if (IS_FD_PROG_ARRAY(map)) {
-		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
-						   attr->flags);
-		goto out;
-	}
+	err = bpf_map_update_value(map, f, key, value, attr->flags);
 
-	/* must increment bpf_prog_active to avoid kprobe+bpf triggering from
-	 * inside bpf map update or delete otherwise deadlocks are possible
-	 */
-	preempt_disable();
-	__this_cpu_inc(bpf_prog_active);
-	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
-	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
-		err = bpf_percpu_hash_update(map, key, value, attr->flags);
-	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
-		err = bpf_percpu_array_update(map, key, value, attr->flags);
-	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {
-		err = bpf_percpu_cgroup_storage_update(map, key, value,
-						       attr->flags);
-	} else if (IS_FD_ARRAY(map)) {
-		rcu_read_lock();
-		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
-						   attr->flags);
-		rcu_read_unlock();
-	} else if (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {
-		rcu_read_lock();
-		err = bpf_fd_htab_map_update_elem(map, f.file, key, value,
-						  attr->flags);
-		rcu_read_unlock();
-	} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {
-		/* rcu_read_lock() is not needed */
-		err = bpf_fd_reuseport_array_update_elem(map, key, value,
-							 attr->flags);
-	} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||
-		   map->map_type == BPF_MAP_TYPE_STACK) {
-		err = map->ops->map_push_elem(map, value, attr->flags);
-	} else {
-		rcu_read_lock();
-		err = map->ops->map_update_elem(map, key, value, attr->flags);
-		rcu_read_unlock();
-	}
-	__this_cpu_dec(bpf_prog_active);
-	preempt_enable();
-	maybe_wait_bpf_programs(map);
-out:
 free_value:
 	kfree(value);
 free_key:

commit 85d33df357b634649ddbe0a20fd2d0fc5732c3cb
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Jan 8 16:35:05 2020 -0800

    bpf: Introduce BPF_MAP_TYPE_STRUCT_OPS
    
    The patch introduces BPF_MAP_TYPE_STRUCT_OPS.  The map value
    is a kernel struct with its func ptr implemented in bpf prog.
    This new map is the interface to register/unregister/introspect
    a bpf implemented kernel struct.
    
    The kernel struct is actually embedded inside another new struct
    (or called the "value" struct in the code).  For example,
    "struct tcp_congestion_ops" is embbeded in:
    struct bpf_struct_ops_tcp_congestion_ops {
            refcount_t refcnt;
            enum bpf_struct_ops_state state;
            struct tcp_congestion_ops data;  /* <-- kernel subsystem struct here */
    }
    The map value is "struct bpf_struct_ops_tcp_congestion_ops".
    The "bpftool map dump" will then be able to show the
    state ("inuse"/"tobefree") and the number of subsystem's refcnt (e.g.
    number of tcp_sock in the tcp_congestion_ops case).  This "value" struct
    is created automatically by a macro.  Having a separate "value" struct
    will also make extending "struct bpf_struct_ops_XYZ" easier (e.g. adding
    "void (*init)(void)" to "struct bpf_struct_ops_XYZ" to do some
    initialization works before registering the struct_ops to the kernel
    subsystem).  The libbpf will take care of finding and populating the
    "struct bpf_struct_ops_XYZ" from "struct XYZ".
    
    Register a struct_ops to a kernel subsystem:
    1. Load all needed BPF_PROG_TYPE_STRUCT_OPS prog(s)
    2. Create a BPF_MAP_TYPE_STRUCT_OPS with attr->btf_vmlinux_value_type_id
       set to the btf id "struct bpf_struct_ops_tcp_congestion_ops" of the
       running kernel.
       Instead of reusing the attr->btf_value_type_id,
       btf_vmlinux_value_type_id s added such that attr->btf_fd can still be
       used as the "user" btf which could store other useful sysadmin/debug
       info that may be introduced in the furture,
       e.g. creation-date/compiler-details/map-creator...etc.
    3. Create a "struct bpf_struct_ops_tcp_congestion_ops" object as described
       in the running kernel btf.  Populate the value of this object.
       The function ptr should be populated with the prog fds.
    4. Call BPF_MAP_UPDATE with the object created in (3) as
       the map value.  The key is always "0".
    
    During BPF_MAP_UPDATE, the code that saves the kernel-func-ptr's
    args as an array of u64 is generated.  BPF_MAP_UPDATE also allows
    the specific struct_ops to do some final checks in "st_ops->init_member()"
    (e.g. ensure all mandatory func ptrs are implemented).
    If everything looks good, it will register this kernel struct
    to the kernel subsystem.  The map will not allow further update
    from this point.
    
    Unregister a struct_ops from the kernel subsystem:
    BPF_MAP_DELETE with key "0".
    
    Introspect a struct_ops:
    BPF_MAP_LOOKUP_ELEM with key "0".  The map value returned will
    have the prog _id_ populated as the func ptr.
    
    The map value state (enum bpf_struct_ops_state) will transit from:
    INIT (map created) =>
    INUSE (map updated, i.e. reg) =>
    TOBEFREE (map value deleted, i.e. unreg)
    
    The kernel subsystem needs to call bpf_struct_ops_get() and
    bpf_struct_ops_put() to manage the "refcnt" in the
    "struct bpf_struct_ops_XYZ".  This patch uses a separate refcnt
    for the purose of tracking the subsystem usage.  Another approach
    is to reuse the map->refcnt and then "show" (i.e. during map_lookup)
    the subsystem's usage by doing map->refcnt - map->usercnt to filter out
    the map-fd/pinned-map usage.  However, that will also tie down the
    future semantics of map->refcnt and map->usercnt.
    
    The very first subsystem's refcnt (during reg()) holds one
    count to map->refcnt.  When the very last subsystem's refcnt
    is gone, it will also release the map->refcnt.  All bpf_prog will be
    freed when the map->refcnt reaches 0 (i.e. during map_free()).
    
    Here is how the bpftool map command will look like:
    [root@arch-fb-vm1 bpf]# bpftool map show
    6: struct_ops  name dctcp  flags 0x0
            key 4B  value 256B  max_entries 1  memlock 4096B
            btf_id 6
    [root@arch-fb-vm1 bpf]# bpftool map dump id 6
    [{
            "value": {
                "refcnt": {
                    "refs": {
                        "counter": 1
                    }
                },
                "state": 1,
                "data": {
                    "list": {
                        "next": 0,
                        "prev": 0
                    },
                    "key": 0,
                    "flags": 2,
                    "init": 24,
                    "release": 0,
                    "ssthresh": 25,
                    "cong_avoid": 30,
                    "set_state": 27,
                    "cwnd_event": 28,
                    "in_ack_event": 26,
                    "undo_cwnd": 29,
                    "pkts_acked": 0,
                    "min_tso_segs": 0,
                    "sndbuf_expand": 0,
                    "cong_control": 0,
                    "get_info": 0,
                    "name": [98,112,102,95,100,99,116,99,112,0,0,0,0,0,0,0
                    ],
                    "owner": 0
                }
            }
        }
    ]
    
    Misc Notes:
    * bpf_struct_ops_map_sys_lookup_elem() is added for syscall lookup.
      It does an inplace update on "*value" instead returning a pointer
      to syscall.c.  Otherwise, it needs a separate copy of "zero" value
      for the BPF_STRUCT_OPS_STATE_INIT to avoid races.
    
    * The bpf_struct_ops_map_delete_elem() is also called without
      preempt_disable() from map_delete_elem().  It is because
      the "->unreg()" may requires sleepable context, e.g.
      the "tcp_unregister_congestion_control()".
    
    * "const" is added to some of the existing "struct btf_func_model *"
      function arg to avoid a compiler warning caused by this patch.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200109003505.3855919-1-kafai@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 03a02ef4c496..f9db72a96ec0 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -628,7 +628,7 @@ static int map_check_btf(struct bpf_map *map, const struct btf *btf,
 	return ret;
 }
 
-#define BPF_MAP_CREATE_LAST_FIELD btf_value_type_id
+#define BPF_MAP_CREATE_LAST_FIELD btf_vmlinux_value_type_id
 /* called via syscall */
 static int map_create(union bpf_attr *attr)
 {
@@ -642,6 +642,14 @@ static int map_create(union bpf_attr *attr)
 	if (err)
 		return -EINVAL;
 
+	if (attr->btf_vmlinux_value_type_id) {
+		if (attr->map_type != BPF_MAP_TYPE_STRUCT_OPS ||
+		    attr->btf_key_type_id || attr->btf_value_type_id)
+			return -EINVAL;
+	} else if (attr->btf_key_type_id && !attr->btf_value_type_id) {
+		return -EINVAL;
+	}
+
 	f_flags = bpf_get_file_flag(attr->map_flags);
 	if (f_flags < 0)
 		return f_flags;
@@ -664,32 +672,35 @@ static int map_create(union bpf_attr *attr)
 	atomic64_set(&map->usercnt, 1);
 	mutex_init(&map->freeze_mutex);
 
-	if (attr->btf_key_type_id || attr->btf_value_type_id) {
+	map->spin_lock_off = -EINVAL;
+	if (attr->btf_key_type_id || attr->btf_value_type_id ||
+	    /* Even the map's value is a kernel's struct,
+	     * the bpf_prog.o must have BTF to begin with
+	     * to figure out the corresponding kernel's
+	     * counter part.  Thus, attr->btf_fd has
+	     * to be valid also.
+	     */
+	    attr->btf_vmlinux_value_type_id) {
 		struct btf *btf;
 
-		if (!attr->btf_value_type_id) {
-			err = -EINVAL;
-			goto free_map;
-		}
-
 		btf = btf_get_by_fd(attr->btf_fd);
 		if (IS_ERR(btf)) {
 			err = PTR_ERR(btf);
 			goto free_map;
 		}
+		map->btf = btf;
 
-		err = map_check_btf(map, btf, attr->btf_key_type_id,
-				    attr->btf_value_type_id);
-		if (err) {
-			btf_put(btf);
-			goto free_map;
+		if (attr->btf_value_type_id) {
+			err = map_check_btf(map, btf, attr->btf_key_type_id,
+					    attr->btf_value_type_id);
+			if (err)
+				goto free_map;
 		}
 
-		map->btf = btf;
 		map->btf_key_type_id = attr->btf_key_type_id;
 		map->btf_value_type_id = attr->btf_value_type_id;
-	} else {
-		map->spin_lock_off = -EINVAL;
+		map->btf_vmlinux_value_type_id =
+			attr->btf_vmlinux_value_type_id;
 	}
 
 	err = security_bpf_map_alloc(map);
@@ -888,6 +899,9 @@ static int map_lookup_elem(union bpf_attr *attr)
 	} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||
 		   map->map_type == BPF_MAP_TYPE_STACK) {
 		err = map->ops->map_peek_elem(map, value);
+	} else if (map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
+		/* struct_ops map requires directly updating "value" */
+		err = bpf_struct_ops_map_sys_lookup_elem(map, key, value);
 	} else {
 		rcu_read_lock();
 		if (map->ops->map_lookup_elem_sys_only)
@@ -1003,7 +1017,8 @@ static int map_update_elem(union bpf_attr *attr)
 		goto out;
 	} else if (map->map_type == BPF_MAP_TYPE_CPUMAP ||
 		   map->map_type == BPF_MAP_TYPE_SOCKHASH ||
-		   map->map_type == BPF_MAP_TYPE_SOCKMAP) {
+		   map->map_type == BPF_MAP_TYPE_SOCKMAP ||
+		   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
 		err = map->ops->map_update_elem(map, key, value, attr->flags);
 		goto out;
 	} else if (IS_FD_PROG_ARRAY(map)) {
@@ -1092,7 +1107,9 @@ static int map_delete_elem(union bpf_attr *attr)
 	if (bpf_map_is_dev_bound(map)) {
 		err = bpf_map_offload_delete_elem(map, key);
 		goto out;
-	} else if (IS_FD_PROG_ARRAY(map)) {
+	} else if (IS_FD_PROG_ARRAY(map) ||
+		   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {
+		/* These maps require sleepable context */
 		err = map->ops->map_delete_elem(map, key);
 		goto out;
 	}
@@ -2822,6 +2839,7 @@ static int bpf_map_get_info_by_fd(struct bpf_map *map,
 		info.btf_key_type_id = map->btf_key_type_id;
 		info.btf_value_type_id = map->btf_value_type_id;
 	}
+	info.btf_vmlinux_value_type_id = map->btf_vmlinux_value_type_id;
 
 	if (bpf_map_is_dev_bound(map)) {
 		err = bpf_map_offload_info_fill(&info, map);

commit 27ae7997a66174cb8afd6a75b3989f5e0c1b9e5a
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Jan 8 16:35:03 2020 -0800

    bpf: Introduce BPF_PROG_TYPE_STRUCT_OPS
    
    This patch allows the kernel's struct ops (i.e. func ptr) to be
    implemented in BPF.  The first use case in this series is the
    "struct tcp_congestion_ops" which will be introduced in a
    latter patch.
    
    This patch introduces a new prog type BPF_PROG_TYPE_STRUCT_OPS.
    The BPF_PROG_TYPE_STRUCT_OPS prog is verified against a particular
    func ptr of a kernel struct.  The attr->attach_btf_id is the btf id
    of a kernel struct.  The attr->expected_attach_type is the member
    "index" of that kernel struct.  The first member of a struct starts
    with member index 0.  That will avoid ambiguity when a kernel struct
    has multiple func ptrs with the same func signature.
    
    For example, a BPF_PROG_TYPE_STRUCT_OPS prog is written
    to implement the "init" func ptr of the "struct tcp_congestion_ops".
    The attr->attach_btf_id is the btf id of the "struct tcp_congestion_ops"
    of the _running_ kernel.  The attr->expected_attach_type is 3.
    
    The ctx of BPF_PROG_TYPE_STRUCT_OPS is an array of u64 args saved
    by arch_prepare_bpf_trampoline that will be done in the next
    patch when introducing BPF_MAP_TYPE_STRUCT_OPS.
    
    "struct bpf_struct_ops" is introduced as a common interface for the kernel
    struct that supports BPF_PROG_TYPE_STRUCT_OPS prog.  The supporting kernel
    struct will need to implement an instance of the "struct bpf_struct_ops".
    
    The supporting kernel struct also needs to implement a bpf_verifier_ops.
    During BPF_PROG_LOAD, bpf_struct_ops_find() will find the right
    bpf_verifier_ops by searching the attr->attach_btf_id.
    
    A new "btf_struct_access" is also added to the bpf_verifier_ops such
    that the supporting kernel struct can optionally provide its own specific
    check on accessing the func arg (e.g. provide limited write access).
    
    After btf_vmlinux is parsed, the new bpf_struct_ops_init() is called
    to initialize some values (e.g. the btf id of the supporting kernel
    struct) and it can only be done once the btf_vmlinux is available.
    
    The R0 checks at BPF_EXIT is excluded for the BPF_PROG_TYPE_STRUCT_OPS prog
    if the return type of the prog->aux->attach_func_proto is "void".
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20200109003503.3855825-1-kafai@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 81ee8595dfee..03a02ef4c496 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1672,17 +1672,22 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 			   enum bpf_attach_type expected_attach_type,
 			   u32 btf_id, u32 prog_fd)
 {
-	switch (prog_type) {
-	case BPF_PROG_TYPE_TRACING:
+	if (btf_id) {
 		if (btf_id > BTF_MAX_TYPE)
 			return -EINVAL;
-		break;
-	default:
-		if (btf_id || prog_fd)
+
+		switch (prog_type) {
+		case BPF_PROG_TYPE_TRACING:
+		case BPF_PROG_TYPE_STRUCT_OPS:
+			break;
+		default:
 			return -EINVAL;
-		break;
+		}
 	}
 
+	if (prog_fd && prog_type != BPF_PROG_TYPE_TRACING)
+		return -EINVAL;
+
 	switch (prog_type) {
 	case BPF_PROG_TYPE_CGROUP_SOCK:
 		switch (expected_attach_type) {

commit 7dd68b3279f1792103d12e69933db3128c6d416e
Author: Andrey Ignatov <rdna@fb.com>
Date:   Wed Dec 18 23:44:35 2019 -0800

    bpf: Support replacing cgroup-bpf program in MULTI mode
    
    The common use-case in production is to have multiple cgroup-bpf
    programs per attach type that cover multiple use-cases. Such programs
    are attached with BPF_F_ALLOW_MULTI and can be maintained by different
    people.
    
    Order of programs usually matters, for example imagine two egress
    programs: the first one drops packets and the second one counts packets.
    If they're swapped the result of counting program will be different.
    
    It brings operational challenges with updating cgroup-bpf program(s)
    attached with BPF_F_ALLOW_MULTI since there is no way to replace a
    program:
    
    * One way to update is to detach all programs first and then attach the
      new version(s) again in the right order. This introduces an
      interruption in the work a program is doing and may not be acceptable
      (e.g. if it's egress firewall);
    
    * Another way is attach the new version of a program first and only then
      detach the old version. This introduces the time interval when two
      versions of same program are working, what may not be acceptable if a
      program is not idempotent. It also imposes additional burden on
      program developers to make sure that two versions of their program can
      co-exist.
    
    Solve the problem by introducing a "replace" mode in BPF_PROG_ATTACH
    command for cgroup-bpf programs being attached with BPF_F_ALLOW_MULTI
    flag. This mode is enabled by newly introduced BPF_F_REPLACE attach flag
    and bpf_attr.replace_bpf_fd attribute to pass fd of the old program to
    replace
    
    That way user can replace any program among those attached with
    BPF_F_ALLOW_MULTI flag without the problems described above.
    
    Details of the new API:
    
    * If BPF_F_REPLACE is set but replace_bpf_fd doesn't have valid
      descriptor of BPF program, BPF_PROG_ATTACH will return corresponding
      error (EINVAL or EBADF).
    
    * If replace_bpf_fd has valid descriptor of BPF program but such a
      program is not attached to specified cgroup, BPF_PROG_ATTACH will
      return ENOENT.
    
    BPF_F_REPLACE is introduced to make the user intent clear, since
    replace_bpf_fd alone can't be used for this (its default value, 0, is a
    valid fd). BPF_F_REPLACE also makes it possible to extend the API in the
    future (e.g. add BPF_F_BEFORE and BPF_F_AFTER if needed).
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Andrii Narkyiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/30cd850044a0057bdfcaaf154b7d2f39850ba813.1576741281.git.rdna@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b08c362f4e02..81ee8595dfee 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2073,10 +2073,10 @@ static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
 	}
 }
 
-#define BPF_PROG_ATTACH_LAST_FIELD attach_flags
+#define BPF_PROG_ATTACH_LAST_FIELD replace_bpf_fd
 
 #define BPF_F_ATTACH_MASK \
-	(BPF_F_ALLOW_OVERRIDE | BPF_F_ALLOW_MULTI)
+	(BPF_F_ALLOW_OVERRIDE | BPF_F_ALLOW_MULTI | BPF_F_REPLACE)
 
 static int bpf_prog_attach(const union bpf_attr *attr)
 {

commit 7e6897f95935973c3253fd756135b5ea58043dc8
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Fri Dec 13 18:51:09 2019 +0100

    bpf, xdp: Start using the BPF dispatcher for XDP
    
    This commit adds a BPF dispatcher for XDP. The dispatcher is updated
    from the XDP control-path, dev_xdp_install(), and used when an XDP
    program is run via bpf_prog_run_xdp().
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20191213175112.30208-4-bjorn.topel@gmail.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 66b90eaf99fe..b08c362f4e02 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2338,17 +2338,12 @@ static int bpf_obj_get_next_id(const union bpf_attr *attr,
 
 #define BPF_PROG_GET_FD_BY_ID_LAST_FIELD prog_id
 
-static int bpf_prog_get_fd_by_id(const union bpf_attr *attr)
+struct bpf_prog *bpf_prog_by_id(u32 id)
 {
 	struct bpf_prog *prog;
-	u32 id = attr->prog_id;
-	int fd;
-
-	if (CHECK_ATTR(BPF_PROG_GET_FD_BY_ID))
-		return -EINVAL;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	if (!id)
+		return ERR_PTR(-ENOENT);
 
 	spin_lock_bh(&prog_idr_lock);
 	prog = idr_find(&prog_idr, id);
@@ -2357,7 +2352,22 @@ static int bpf_prog_get_fd_by_id(const union bpf_attr *attr)
 	else
 		prog = ERR_PTR(-ENOENT);
 	spin_unlock_bh(&prog_idr_lock);
+	return prog;
+}
+
+static int bpf_prog_get_fd_by_id(const union bpf_attr *attr)
+{
+	struct bpf_prog *prog;
+	u32 id = attr->prog_id;
+	int fd;
+
+	if (CHECK_ATTR(BPF_PROG_GET_FD_BY_ID))
+		return -EINVAL;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 
+	prog = bpf_prog_by_id(id);
 	if (IS_ERR(prog))
 		return PTR_ERR(prog);
 

commit bae141f54be83b06652c1d47e50e4e75ed4e9c7e
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Dec 6 22:49:34 2019 +0100

    bpf: Emit audit messages upon successful prog load and unload
    
    Allow for audit messages to be emitted upon BPF program load and
    unload for having a timeline of events. The load itself is in
    syscall context, so additional info about the process initiating
    the BPF prog creation can be logged and later directly correlated
    to the unload event.
    
    The only info really needed from BPF side is the globally unique
    prog ID where then audit user space tooling can query / dump all
    info needed about the specific BPF program right upon load event
    and enrich the record, thus these changes needed here can be kept
    small and non-intrusive to the core.
    
    Raw example output:
    
      # auditctl -D
      # auditctl -a always,exit -F arch=x86_64 -S bpf
      # ausearch --start recent -m 1334
      ...
      ----
      time->Wed Nov 27 16:04:13 2019
      type=PROCTITLE msg=audit(1574867053.120:84664): proctitle="./bpf"
      type=SYSCALL msg=audit(1574867053.120:84664): arch=c000003e syscall=321   \
        success=yes exit=3 a0=5 a1=7ffea484fbe0 a2=70 a3=0 items=0 ppid=7477    \
        pid=12698 auid=1001 uid=1001 gid=1001 euid=1001 suid=1001 fsuid=1001    \
        egid=1001 sgid=1001 fsgid=1001 tty=pts2 ses=4 comm="bpf"                \
        exe="/home/jolsa/auditd/audit-testsuite/tests/bpf/bpf"                  \
        subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 key=(null)
      type=UNKNOWN[1334] msg=audit(1574867053.120:84664): prog-id=76 op=LOAD
      ----
      time->Wed Nov 27 16:04:13 2019
      type=UNKNOWN[1334] msg=audit(1574867053.120:84665): prog-id=76 op=UNLOAD
      ...
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Co-developed-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Paul Moore <paul@paul-moore.com>
    Link: https://lore.kernel.org/bpf/20191206214934.11319-1-jolsa@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index e3461ec59570..66b90eaf99fe 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -23,6 +23,7 @@
 #include <linux/timekeeping.h>
 #include <linux/ctype.h>
 #include <linux/nospec.h>
+#include <linux/audit.h>
 #include <uapi/linux/btf.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
@@ -1306,6 +1307,36 @@ static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
 	return 0;
 }
 
+enum bpf_audit {
+	BPF_AUDIT_LOAD,
+	BPF_AUDIT_UNLOAD,
+	BPF_AUDIT_MAX,
+};
+
+static const char * const bpf_audit_str[BPF_AUDIT_MAX] = {
+	[BPF_AUDIT_LOAD]   = "LOAD",
+	[BPF_AUDIT_UNLOAD] = "UNLOAD",
+};
+
+static void bpf_audit_prog(const struct bpf_prog *prog, unsigned int op)
+{
+	struct audit_context *ctx = NULL;
+	struct audit_buffer *ab;
+
+	if (WARN_ON_ONCE(op >= BPF_AUDIT_MAX))
+		return;
+	if (audit_enabled == AUDIT_OFF)
+		return;
+	if (op == BPF_AUDIT_LOAD)
+		ctx = audit_context();
+	ab = audit_log_start(ctx, GFP_ATOMIC, AUDIT_BPF);
+	if (unlikely(!ab))
+		return;
+	audit_log_format(ab, "prog-id=%u op=%s",
+			 prog->aux->id, bpf_audit_str[op]);
+	audit_log_end(ab);
+}
+
 int __bpf_prog_charge(struct user_struct *user, u32 pages)
 {
 	unsigned long memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
@@ -1421,6 +1452,7 @@ static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 {
 	if (atomic64_dec_and_test(&prog->aux->refcnt)) {
 		perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_UNLOAD, 0);
+		bpf_audit_prog(prog, BPF_AUDIT_UNLOAD);
 		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog, do_idr_lock);
 		__bpf_prog_put_noref(prog, true);
@@ -1830,6 +1862,7 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	 */
 	bpf_prog_kallsyms_add(prog);
 	perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_LOAD, 0);
+	bpf_audit_prog(prog, BPF_AUDIT_LOAD);
 
 	err = bpf_prog_new_fd(prog);
 	if (err < 0)

commit da765a2f599304a81a25e77908d1790414ecdbb6
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Nov 22 21:07:58 2019 +0100

    bpf: Add poke dependency tracking for prog array maps
    
    This work adds program tracking to prog array maps. This is needed such
    that upon prog array updates/deletions we can fix up all programs which
    make use of this tail call map. We add ops->map_poke_{un,}track()
    helpers to maps to maintain the list of programs and ops->map_poke_run()
    for triggering the actual update.
    
    bpf_array_aux is extended to contain the list head and poke_mutex in
    order to serialize program patching during updates/deletions.
    bpf_free_used_maps() will untrack the program shortly before dropping
    the reference to the map. For clearing out the prog array once all urefs
    are dropped we need to use schedule_work() to have a sleepable context.
    
    The prog_array_map_poke_run() is triggered during updates/deletions and
    walks the maintained prog list. It checks in their poke_tabs whether the
    map and key is matching and runs the actual bpf_arch_text_poke() for
    patching in the nop or new jmp location. Depending on the type of update,
    we use one of BPF_MOD_{NOP_TO_JUMP,JUMP_TO_NOP,JUMP_TO_JUMP}.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/1fb364bb3c565b3e415d5ea348f036ff379e779d.1574452833.git.daniel@iogearbox.net

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b904d56ec686..e3461ec59570 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -25,12 +25,13 @@
 #include <linux/nospec.h>
 #include <uapi/linux/btf.h>
 
-#define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY || \
-			   (map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
-			   (map)->map_type == BPF_MAP_TYPE_CGROUP_ARRAY || \
-			   (map)->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS)
+#define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
+			  (map)->map_type == BPF_MAP_TYPE_CGROUP_ARRAY || \
+			  (map)->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS)
+#define IS_FD_PROG_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY)
 #define IS_FD_HASH(map) ((map)->map_type == BPF_MAP_TYPE_HASH_OF_MAPS)
-#define IS_FD_MAP(map) (IS_FD_ARRAY(map) || IS_FD_HASH(map))
+#define IS_FD_MAP(map) (IS_FD_ARRAY(map) || IS_FD_PROG_ARRAY(map) || \
+			IS_FD_HASH(map))
 
 #define BPF_OBJ_FLAG_MASK   (BPF_F_RDONLY | BPF_F_WRONLY)
 
@@ -877,7 +878,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 		err = bpf_percpu_cgroup_storage_copy(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {
 		err = bpf_stackmap_copy(map, key, value);
-	} else if (IS_FD_ARRAY(map)) {
+	} else if (IS_FD_ARRAY(map) || IS_FD_PROG_ARRAY(map)) {
 		err = bpf_fd_array_map_lookup_elem(map, key, value);
 	} else if (IS_FD_HASH(map)) {
 		err = bpf_fd_htab_map_lookup_elem(map, key, value);
@@ -1004,6 +1005,10 @@ static int map_update_elem(union bpf_attr *attr)
 		   map->map_type == BPF_MAP_TYPE_SOCKMAP) {
 		err = map->ops->map_update_elem(map, key, value, attr->flags);
 		goto out;
+	} else if (IS_FD_PROG_ARRAY(map)) {
+		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
+						   attr->flags);
+		goto out;
 	}
 
 	/* must increment bpf_prog_active to avoid kprobe+bpf triggering from
@@ -1086,6 +1091,9 @@ static int map_delete_elem(union bpf_attr *attr)
 	if (bpf_map_is_dev_bound(map)) {
 		err = bpf_map_offload_delete_elem(map, key);
 		goto out;
+	} else if (IS_FD_PROG_ARRAY(map)) {
+		err = map->ops->map_delete_elem(map, key);
+		goto out;
 	}
 
 	preempt_disable();

commit 2beee5f57441413b64a9c2bd657e17beabb98d1c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Nov 22 21:07:56 2019 +0100

    bpf: Move owner type, jited info into array auxiliary data
    
    We're going to extend this with further information which is only
    relevant for prog array at this point. Given this info is not used
    in critical path, move it into its own structure such that the main
    array map structure can be kept on diet.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/b9ddccdb0f6f7026489ee955f16c96381e1e7238.1574452833.git.daniel@iogearbox.net

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 373778da8489..b904d56ec686 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -389,13 +389,12 @@ static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 {
 	const struct bpf_map *map = filp->private_data;
 	const struct bpf_array *array;
-	u32 owner_prog_type = 0;
-	u32 owner_jited = 0;
+	u32 type = 0, jited = 0;
 
 	if (map->map_type == BPF_MAP_TYPE_PROG_ARRAY) {
 		array = container_of(map, struct bpf_array, map);
-		owner_prog_type = array->owner_prog_type;
-		owner_jited = array->owner_jited;
+		type  = array->aux->type;
+		jited = array->aux->jited;
 	}
 
 	seq_printf(m,
@@ -415,12 +414,9 @@ static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 		   map->memory.pages * 1ULL << PAGE_SHIFT,
 		   map->id,
 		   READ_ONCE(map->frozen));
-
-	if (owner_prog_type) {
-		seq_printf(m, "owner_prog_type:\t%u\n",
-			   owner_prog_type);
-		seq_printf(m, "owner_jited:\t%u\n",
-			   owner_jited);
+	if (type) {
+		seq_printf(m, "owner_prog_type:\t%u\n", type);
+		seq_printf(m, "owner_jited:\t%u\n", jited);
 	}
 }
 #endif

commit 6332be04c039a72fca32ed0a4265bac58d606bb6
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Nov 22 21:07:55 2019 +0100

    bpf: Move bpf_free_used_maps into sleepable section
    
    We later on are going to need a sleepable context as opposed to plain
    RCU callback in order to untrack programs we need to poke at runtime
    and tracking as well as image update is performed under mutex.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/09823b1d5262876e9b83a8e75df04cf0467357a4.1574452833.git.daniel@iogearbox.net

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4ae52eb05f41..373778da8489 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1302,25 +1302,6 @@ static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
 	return 0;
 }
 
-/* drop refcnt on maps used by eBPF program and free auxilary data */
-static void free_used_maps(struct bpf_prog_aux *aux)
-{
-	enum bpf_cgroup_storage_type stype;
-	int i;
-
-	for_each_cgroup_storage_type(stype) {
-		if (!aux->cgroup_storage[stype])
-			continue;
-		bpf_cgroup_storage_release(aux->prog,
-					   aux->cgroup_storage[stype]);
-	}
-
-	for (i = 0; i < aux->used_map_cnt; i++)
-		bpf_map_put(aux->used_maps[i]);
-
-	kfree(aux->used_maps);
-}
-
 int __bpf_prog_charge(struct user_struct *user, u32 pages)
 {
 	unsigned long memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
@@ -1415,7 +1396,6 @@ static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 
 	kvfree(aux->func_info);
 	kfree(aux->func_info_aux);
-	free_used_maps(aux);
 	bpf_prog_uncharge_memlock(aux->prog);
 	security_bpf_prog_free(aux);
 	bpf_prog_free(aux->prog);

commit 84bb46cd62283cc371769ec1f77ff7924099f584
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Sat Nov 23 09:54:58 2019 -0800

    Revert "bpf: Emit audit messages upon successful prog load and unload"
    
    This commit reverts commit 91e6015b082b ("bpf: Emit audit messages
    upon successful prog load and unload") and its follow up commit
    7599a896f2e4 ("audit: Move audit_log_task declaration under
    CONFIG_AUDITSYSCALL") as requested by Paul Moore. The change needs
    close review on linux-audit, tests etc.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b51ecb9644d0..4ae52eb05f41 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -23,7 +23,6 @@
 #include <linux/timekeeping.h>
 #include <linux/ctype.h>
 #include <linux/nospec.h>
-#include <linux/audit.h>
 #include <uapi/linux/btf.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY || \
@@ -1322,34 +1321,6 @@ static void free_used_maps(struct bpf_prog_aux *aux)
 	kfree(aux->used_maps);
 }
 
-enum bpf_event {
-	BPF_EVENT_LOAD,
-	BPF_EVENT_UNLOAD,
-};
-
-static const char * const bpf_event_audit_str[] = {
-	[BPF_EVENT_LOAD]   = "LOAD",
-	[BPF_EVENT_UNLOAD] = "UNLOAD",
-};
-
-static void bpf_audit_prog(const struct bpf_prog *prog, enum bpf_event event)
-{
-	bool has_task_context = event == BPF_EVENT_LOAD;
-	struct audit_buffer *ab;
-
-	if (audit_enabled == AUDIT_OFF)
-		return;
-	ab = audit_log_start(audit_context(), GFP_ATOMIC, AUDIT_BPF);
-	if (unlikely(!ab))
-		return;
-	if (has_task_context)
-		audit_log_task(ab);
-	audit_log_format(ab, "%sprog-id=%u event=%s",
-			 has_task_context ? " " : "",
-			 prog->aux->id, bpf_event_audit_str[event]);
-	audit_log_end(ab);
-}
-
 int __bpf_prog_charge(struct user_struct *user, u32 pages)
 {
 	unsigned long memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
@@ -1466,7 +1437,6 @@ static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 {
 	if (atomic64_dec_and_test(&prog->aux->refcnt)) {
 		perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_UNLOAD, 0);
-		bpf_audit_prog(prog, BPF_EVENT_UNLOAD);
 		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog, do_idr_lock);
 		__bpf_prog_put_noref(prog, true);
@@ -1876,7 +1846,6 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	 */
 	bpf_prog_kallsyms_add(prog);
 	perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_LOAD, 0);
-	bpf_audit_prog(prog, BPF_EVENT_LOAD);
 
 	err = bpf_prog_new_fd(prog);
 	if (err < 0)

commit 196e8ca74886c433dcfc64a809707074b936aaf5
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Nov 20 23:04:44 2019 +0100

    bpf: Switch bpf_map_{area_alloc,area_mmapable_alloc}() to u64 size
    
    Given we recently extended the original bpf_map_area_alloc() helper in
    commit fc9702273e2e ("bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY"),
    we need to apply the same logic as in ff1c08e1f74b ("bpf: Change size
    to u64 for bpf_map_{area_alloc, charge_init}()"). To avoid conflicts,
    extend it for bpf-next.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 17f4254495f2..b51ecb9644d0 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -128,7 +128,7 @@ static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 	return map;
 }
 
-static void *__bpf_map_area_alloc(size_t size, int numa_node, bool mmapable)
+static void *__bpf_map_area_alloc(u64 size, int numa_node, bool mmapable)
 {
 	/* We really just want to fail instead of triggering OOM killer
 	 * under memory pressure, therefore we set __GFP_NORETRY to kmalloc,
@@ -143,6 +143,9 @@ static void *__bpf_map_area_alloc(size_t size, int numa_node, bool mmapable)
 	const gfp_t flags = __GFP_NOWARN | __GFP_ZERO;
 	void *area;
 
+	if (size >= SIZE_MAX)
+		return NULL;
+
 	/* kmalloc()'ed memory can't be mmap()'ed */
 	if (!mmapable && size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER)) {
 		area = kmalloc_node(size, GFP_USER | __GFP_NORETRY | flags,
@@ -160,12 +163,12 @@ static void *__bpf_map_area_alloc(size_t size, int numa_node, bool mmapable)
 					   flags, __builtin_return_address(0));
 }
 
-void *bpf_map_area_alloc(size_t size, int numa_node)
+void *bpf_map_area_alloc(u64 size, int numa_node)
 {
 	return __bpf_map_area_alloc(size, numa_node, false);
 }
 
-void *bpf_map_area_mmapable_alloc(size_t size, int numa_node)
+void *bpf_map_area_mmapable_alloc(u64 size, int numa_node)
 {
 	return __bpf_map_area_alloc(size, numa_node, true);
 }
@@ -214,7 +217,7 @@ static void bpf_uncharge_memlock(struct user_struct *user, u32 pages)
 		atomic_long_sub(pages, &user->locked_vm);
 }
 
-int bpf_map_charge_init(struct bpf_map_memory *mem, size_t size)
+int bpf_map_charge_init(struct bpf_map_memory *mem, u64 size)
 {
 	u32 pages = round_up(size, PAGE_SIZE) >> PAGE_SHIFT;
 	struct user_struct *user;

commit 91e6015b082b08a74e5d9d326f651e5890a93519
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Nov 20 22:38:16 2019 +0100

    bpf: Emit audit messages upon successful prog load and unload
    
    Allow for audit messages to be emitted upon BPF program load and
    unload for having a timeline of events. The load itself is in
    syscall context, so additional info about the process initiating
    the BPF prog creation can be logged and later directly correlated
    to the unload event.
    
    The only info really needed from BPF side is the globally unique
    prog ID where then audit user space tooling can query / dump all
    info needed about the specific BPF program right upon load event
    and enrich the record, thus these changes needed here can be kept
    small and non-intrusive to the core.
    
    Raw example output:
    
      # auditctl -D
      # auditctl -a always,exit -F arch=x86_64 -S bpf
      # ausearch --start recent -m 1334
      [...]
      ----
      time->Wed Nov 20 12:45:51 2019
      type=PROCTITLE msg=audit(1574271951.590:8974): proctitle="./test_verifier"
      type=SYSCALL msg=audit(1574271951.590:8974): arch=c000003e syscall=321 success=yes exit=14 a0=5 a1=7ffe2d923e80 a2=78 a3=0 items=0 ppid=742 pid=949 auid=0 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts0 ses=2 comm="test_verifier" exe="/root/bpf-next/tools/testing/selftests/bpf/test_verifier" subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 key=(null)
      type=UNKNOWN[1334] msg=audit(1574271951.590:8974): auid=0 uid=0 gid=0 ses=2 subj=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 pid=949 comm="test_verifier" exe="/root/bpf-next/tools/testing/selftests/bpf/test_verifier" prog-id=3260 event=LOAD
      ----
      time->Wed Nov 20 12:45:51 2019
    type=UNKNOWN[1334] msg=audit(1574271951.590:8975): prog-id=3260 event=UNLOAD
      ----
      [...]
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20191120213816.8186-1-jolsa@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index bac3becf9f90..17f4254495f2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -23,6 +23,7 @@
 #include <linux/timekeeping.h>
 #include <linux/ctype.h>
 #include <linux/nospec.h>
+#include <linux/audit.h>
 #include <uapi/linux/btf.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY || \
@@ -1318,6 +1319,34 @@ static void free_used_maps(struct bpf_prog_aux *aux)
 	kfree(aux->used_maps);
 }
 
+enum bpf_event {
+	BPF_EVENT_LOAD,
+	BPF_EVENT_UNLOAD,
+};
+
+static const char * const bpf_event_audit_str[] = {
+	[BPF_EVENT_LOAD]   = "LOAD",
+	[BPF_EVENT_UNLOAD] = "UNLOAD",
+};
+
+static void bpf_audit_prog(const struct bpf_prog *prog, enum bpf_event event)
+{
+	bool has_task_context = event == BPF_EVENT_LOAD;
+	struct audit_buffer *ab;
+
+	if (audit_enabled == AUDIT_OFF)
+		return;
+	ab = audit_log_start(audit_context(), GFP_ATOMIC, AUDIT_BPF);
+	if (unlikely(!ab))
+		return;
+	if (has_task_context)
+		audit_log_task(ab);
+	audit_log_format(ab, "%sprog-id=%u event=%s",
+			 has_task_context ? " " : "",
+			 prog->aux->id, bpf_event_audit_str[event]);
+	audit_log_end(ab);
+}
+
 int __bpf_prog_charge(struct user_struct *user, u32 pages)
 {
 	unsigned long memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
@@ -1434,6 +1463,7 @@ static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 {
 	if (atomic64_dec_and_test(&prog->aux->refcnt)) {
 		perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_UNLOAD, 0);
+		bpf_audit_prog(prog, BPF_EVENT_UNLOAD);
 		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog, do_idr_lock);
 		__bpf_prog_put_noref(prog, true);
@@ -1843,6 +1873,7 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	 */
 	bpf_prog_kallsyms_add(prog);
 	perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_LOAD, 0);
+	bpf_audit_prog(prog, BPF_EVENT_LOAD);
 
 	err = bpf_prog_new_fd(prog);
 	if (err < 0)

commit fc9702273e2edb90400a34b3be76f7b08fa3344b
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Sun Nov 17 09:28:04 2019 -0800

    bpf: Add mmap() support for BPF_MAP_TYPE_ARRAY
    
    Add ability to memory-map contents of BPF array map. This is extremely useful
    for working with BPF global data from userspace programs. It allows to avoid
    typical bpf_map_{lookup,update}_elem operations, improving both performance
    and usability.
    
    There had to be special considerations for map freezing, to avoid having
    writable memory view into a frozen map. To solve this issue, map freezing and
    mmap-ing is happening under mutex now:
      - if map is already frozen, no writable mapping is allowed;
      - if map has writable memory mappings active (accounted in map->writecnt),
        map freezing will keep failing with -EBUSY;
      - once number of writable memory mappings drops to zero, map freezing can be
        performed again.
    
    Only non-per-CPU plain arrays are supported right now. Maps with spinlocks
    can't be memory mapped either.
    
    For BPF_F_MMAPABLE array, memory allocation has to be done through vmalloc()
    to be mmap()'able. We also need to make sure that array data memory is
    page-sized and page-aligned, so we over-allocate memory in such a way that
    struct bpf_array is at the end of a single page of memory with array->value
    being aligned with the start of the second page. On deallocation we need to
    accomodate this memory arrangement to free vmalloc()'ed memory correctly.
    
    One important consideration regarding how memory-mapping subsystem functions.
    Memory-mapping subsystem provides few optional callbacks, among them open()
    and close().  close() is called for each memory region that is unmapped, so
    that users can decrease their reference counters and free up resources, if
    necessary. open() is *almost* symmetrical: it's called for each memory region
    that is being mapped, **except** the very first one. So bpf_map_mmap does
    initial refcnt bump, while open() will do any extra ones after that. Thus
    number of close() calls is equal to number of open() calls plus one more.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Song Liu <songliubraving@fb.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Link: https://lore.kernel.org/bpf/20191117172806.2195367-4-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 52fe4bacb330..bac3becf9f90 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -127,7 +127,7 @@ static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 	return map;
 }
 
-void *bpf_map_area_alloc(size_t size, int numa_node)
+static void *__bpf_map_area_alloc(size_t size, int numa_node, bool mmapable)
 {
 	/* We really just want to fail instead of triggering OOM killer
 	 * under memory pressure, therefore we set __GFP_NORETRY to kmalloc,
@@ -142,18 +142,33 @@ void *bpf_map_area_alloc(size_t size, int numa_node)
 	const gfp_t flags = __GFP_NOWARN | __GFP_ZERO;
 	void *area;
 
-	if (size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER)) {
+	/* kmalloc()'ed memory can't be mmap()'ed */
+	if (!mmapable && size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER)) {
 		area = kmalloc_node(size, GFP_USER | __GFP_NORETRY | flags,
 				    numa_node);
 		if (area != NULL)
 			return area;
 	}
-
+	if (mmapable) {
+		BUG_ON(!PAGE_ALIGNED(size));
+		return vmalloc_user_node_flags(size, numa_node, GFP_KERNEL |
+					       __GFP_RETRY_MAYFAIL | flags);
+	}
 	return __vmalloc_node_flags_caller(size, numa_node,
 					   GFP_KERNEL | __GFP_RETRY_MAYFAIL |
 					   flags, __builtin_return_address(0));
 }
 
+void *bpf_map_area_alloc(size_t size, int numa_node)
+{
+	return __bpf_map_area_alloc(size, numa_node, false);
+}
+
+void *bpf_map_area_mmapable_alloc(size_t size, int numa_node)
+{
+	return __bpf_map_area_alloc(size, numa_node, true);
+}
+
 void bpf_map_area_free(void *area)
 {
 	kvfree(area);
@@ -425,6 +440,74 @@ static ssize_t bpf_dummy_write(struct file *filp, const char __user *buf,
 	return -EINVAL;
 }
 
+/* called for any extra memory-mapped regions (except initial) */
+static void bpf_map_mmap_open(struct vm_area_struct *vma)
+{
+	struct bpf_map *map = vma->vm_file->private_data;
+
+	bpf_map_inc_with_uref(map);
+
+	if (vma->vm_flags & VM_WRITE) {
+		mutex_lock(&map->freeze_mutex);
+		map->writecnt++;
+		mutex_unlock(&map->freeze_mutex);
+	}
+}
+
+/* called for all unmapped memory region (including initial) */
+static void bpf_map_mmap_close(struct vm_area_struct *vma)
+{
+	struct bpf_map *map = vma->vm_file->private_data;
+
+	if (vma->vm_flags & VM_WRITE) {
+		mutex_lock(&map->freeze_mutex);
+		map->writecnt--;
+		mutex_unlock(&map->freeze_mutex);
+	}
+
+	bpf_map_put_with_uref(map);
+}
+
+static const struct vm_operations_struct bpf_map_default_vmops = {
+	.open		= bpf_map_mmap_open,
+	.close		= bpf_map_mmap_close,
+};
+
+static int bpf_map_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct bpf_map *map = filp->private_data;
+	int err;
+
+	if (!map->ops->map_mmap || map_value_has_spin_lock(map))
+		return -ENOTSUPP;
+
+	if (!(vma->vm_flags & VM_SHARED))
+		return -EINVAL;
+
+	mutex_lock(&map->freeze_mutex);
+
+	if ((vma->vm_flags & VM_WRITE) && map->frozen) {
+		err = -EPERM;
+		goto out;
+	}
+
+	/* set default open/close callbacks */
+	vma->vm_ops = &bpf_map_default_vmops;
+	vma->vm_private_data = map;
+
+	err = map->ops->map_mmap(map, vma);
+	if (err)
+		goto out;
+
+	bpf_map_inc_with_uref(map);
+
+	if (vma->vm_flags & VM_WRITE)
+		map->writecnt++;
+out:
+	mutex_unlock(&map->freeze_mutex);
+	return err;
+}
+
 const struct file_operations bpf_map_fops = {
 #ifdef CONFIG_PROC_FS
 	.show_fdinfo	= bpf_map_show_fdinfo,
@@ -432,6 +515,7 @@ const struct file_operations bpf_map_fops = {
 	.release	= bpf_map_release,
 	.read		= bpf_dummy_read,
 	.write		= bpf_dummy_write,
+	.mmap		= bpf_map_mmap,
 };
 
 int bpf_map_new_fd(struct bpf_map *map, int flags)
@@ -577,6 +661,7 @@ static int map_create(union bpf_attr *attr)
 
 	atomic64_set(&map->refcnt, 1);
 	atomic64_set(&map->usercnt, 1);
+	mutex_init(&map->freeze_mutex);
 
 	if (attr->btf_key_type_id || attr->btf_value_type_id) {
 		struct btf *btf;
@@ -1163,6 +1248,13 @@ static int map_freeze(const union bpf_attr *attr)
 	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
+
+	mutex_lock(&map->freeze_mutex);
+
+	if (map->writecnt) {
+		err = -EBUSY;
+		goto err_put;
+	}
 	if (READ_ONCE(map->frozen)) {
 		err = -EBUSY;
 		goto err_put;
@@ -1174,6 +1266,7 @@ static int map_freeze(const union bpf_attr *attr)
 
 	WRITE_ONCE(map->frozen, true);
 err_put:
+	mutex_unlock(&map->freeze_mutex);
 	fdput(f);
 	return err;
 }

commit 85192dbf4de08795afe2b88e52a36fc6abfc3dba
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Sun Nov 17 09:28:03 2019 -0800

    bpf: Convert bpf_prog refcnt to atomic64_t
    
    Similarly to bpf_map's refcnt/usercnt, convert bpf_prog's refcnt to atomic64
    and remove artificial 32k limit. This allows to make bpf_prog's refcounting
    non-failing, simplifying logic of users of bpf_prog_add/bpf_prog_inc.
    
    Validated compilation by running allyesconfig kernel build.
    
    Suggested-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20191117172806.2195367-3-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 20030751b7a2..52fe4bacb330 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1339,7 +1339,7 @@ static void __bpf_prog_put_noref(struct bpf_prog *prog, bool deferred)
 
 static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 {
-	if (atomic_dec_and_test(&prog->aux->refcnt)) {
+	if (atomic64_dec_and_test(&prog->aux->refcnt)) {
 		perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_UNLOAD, 0);
 		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog, do_idr_lock);
@@ -1445,16 +1445,9 @@ static struct bpf_prog *____bpf_prog_get(struct fd f)
 	return f.file->private_data;
 }
 
-/* prog's refcnt limit */
-#define BPF_MAX_REFCNT 32768
-
-struct bpf_prog *bpf_prog_add(struct bpf_prog *prog, int i)
+void bpf_prog_add(struct bpf_prog *prog, int i)
 {
-	if (atomic_add_return(i, &prog->aux->refcnt) > BPF_MAX_REFCNT) {
-		atomic_sub(i, &prog->aux->refcnt);
-		return ERR_PTR(-EBUSY);
-	}
-	return prog;
+	atomic64_add(i, &prog->aux->refcnt);
 }
 EXPORT_SYMBOL_GPL(bpf_prog_add);
 
@@ -1465,13 +1458,13 @@ void bpf_prog_sub(struct bpf_prog *prog, int i)
 	 * path holds a reference to the program, thus atomic_sub() can
 	 * be safely used in such cases!
 	 */
-	WARN_ON(atomic_sub_return(i, &prog->aux->refcnt) == 0);
+	WARN_ON(atomic64_sub_return(i, &prog->aux->refcnt) == 0);
 }
 EXPORT_SYMBOL_GPL(bpf_prog_sub);
 
-struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
+void bpf_prog_inc(struct bpf_prog *prog)
 {
-	return bpf_prog_add(prog, 1);
+	atomic64_inc(&prog->aux->refcnt);
 }
 EXPORT_SYMBOL_GPL(bpf_prog_inc);
 
@@ -1480,12 +1473,7 @@ struct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)
 {
 	int refold;
 
-	refold = atomic_fetch_add_unless(&prog->aux->refcnt, 1, 0);
-
-	if (refold >= BPF_MAX_REFCNT) {
-		__bpf_prog_put(prog, false);
-		return ERR_PTR(-EBUSY);
-	}
+	refold = atomic64_fetch_add_unless(&prog->aux->refcnt, 1, 0);
 
 	if (!refold)
 		return ERR_PTR(-ENOENT);
@@ -1523,7 +1511,7 @@ static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *attach_type,
 		goto out;
 	}
 
-	prog = bpf_prog_inc(prog);
+	bpf_prog_inc(prog);
 out:
 	fdput(f);
 	return prog;
@@ -1714,7 +1702,7 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	prog->orig_prog = NULL;
 	prog->jited = 0;
 
-	atomic_set(&prog->aux->refcnt, 1);
+	atomic64_set(&prog->aux->refcnt, 1);
 	prog->gpl_compatible = is_gpl ? 1 : 0;
 
 	if (bpf_prog_is_dev_bound(prog->aux)) {

commit 1e0bd5a091e5d9e0f1d5b0e6329b87bb1792f784
Author: Andrii Nakryiko <andriin@fb.com>
Date:   Sun Nov 17 09:28:02 2019 -0800

    bpf: Switch bpf_map ref counter to atomic64_t so bpf_map_inc() never fails
    
    92117d8443bc ("bpf: fix refcnt overflow") turned refcounting of bpf_map into
    potentially failing operation, when refcount reaches BPF_MAX_REFCNT limit
    (32k). Due to using 32-bit counter, it's possible in practice to overflow
    refcounter and make it wrap around to 0, causing erroneous map free, while
    there are still references to it, causing use-after-free problems.
    
    But having a failing refcounting operations are problematic in some cases. One
    example is mmap() interface. After establishing initial memory-mapping, user
    is allowed to arbitrarily map/remap/unmap parts of mapped memory, arbitrarily
    splitting it into multiple non-contiguous regions. All this happening without
    any control from the users of mmap subsystem. Rather mmap subsystem sends
    notifications to original creator of memory mapping through open/close
    callbacks, which are optionally specified during initial memory mapping
    creation. These callbacks are used to maintain accurate refcount for bpf_map
    (see next patch in this series). The problem is that open() callback is not
    supposed to fail, because memory-mapped resource is set up and properly
    referenced. This is posing a problem for using memory-mapping with BPF maps.
    
    One solution to this is to maintain separate refcount for just memory-mappings
    and do single bpf_map_inc/bpf_map_put when it goes from/to zero, respectively.
    There are similar use cases in current work on tcp-bpf, necessitating extra
    counter as well. This seems like a rather unfortunate and ugly solution that
    doesn't scale well to various new use cases.
    
    Another approach to solve this is to use non-failing refcount_t type, which
    uses 32-bit counter internally, but, once reaching overflow state at UINT_MAX,
    stays there. This utlimately causes memory leak, but prevents use after free.
    
    But given refcounting is not the most performance-critical operation with BPF
    maps (it's not used from running BPF program code), we can also just switch to
    64-bit counter that can't overflow in practice, potentially disadvantaging
    32-bit platforms a tiny bit. This simplifies semantics and allows above
    described scenarios to not worry about failing refcount increment operation.
    
    In terms of struct bpf_map size, we are still good and use the same amount of
    space:
    
    BEFORE (3 cache lines, 8 bytes of padding at the end):
    struct bpf_map {
            const struct bpf_map_ops  * ops __attribute__((__aligned__(64))); /*     0     8 */
            struct bpf_map *           inner_map_meta;       /*     8     8 */
            void *                     security;             /*    16     8 */
            enum bpf_map_type  map_type;                     /*    24     4 */
            u32                        key_size;             /*    28     4 */
            u32                        value_size;           /*    32     4 */
            u32                        max_entries;          /*    36     4 */
            u32                        map_flags;            /*    40     4 */
            int                        spin_lock_off;        /*    44     4 */
            u32                        id;                   /*    48     4 */
            int                        numa_node;            /*    52     4 */
            u32                        btf_key_type_id;      /*    56     4 */
            u32                        btf_value_type_id;    /*    60     4 */
            /* --- cacheline 1 boundary (64 bytes) --- */
            struct btf *               btf;                  /*    64     8 */
            struct bpf_map_memory memory;                    /*    72    16 */
            bool                       unpriv_array;         /*    88     1 */
            bool                       frozen;               /*    89     1 */
    
            /* XXX 38 bytes hole, try to pack */
    
            /* --- cacheline 2 boundary (128 bytes) --- */
            atomic_t                   refcnt __attribute__((__aligned__(64))); /*   128     4 */
            atomic_t                   usercnt;              /*   132     4 */
            struct work_struct work;                         /*   136    32 */
            char                       name[16];             /*   168    16 */
    
            /* size: 192, cachelines: 3, members: 21 */
            /* sum members: 146, holes: 1, sum holes: 38 */
            /* padding: 8 */
            /* forced alignments: 2, forced holes: 1, sum forced holes: 38 */
    } __attribute__((__aligned__(64)));
    
    AFTER (same 3 cache lines, no extra padding now):
    struct bpf_map {
            const struct bpf_map_ops  * ops __attribute__((__aligned__(64))); /*     0     8 */
            struct bpf_map *           inner_map_meta;       /*     8     8 */
            void *                     security;             /*    16     8 */
            enum bpf_map_type  map_type;                     /*    24     4 */
            u32                        key_size;             /*    28     4 */
            u32                        value_size;           /*    32     4 */
            u32                        max_entries;          /*    36     4 */
            u32                        map_flags;            /*    40     4 */
            int                        spin_lock_off;        /*    44     4 */
            u32                        id;                   /*    48     4 */
            int                        numa_node;            /*    52     4 */
            u32                        btf_key_type_id;      /*    56     4 */
            u32                        btf_value_type_id;    /*    60     4 */
            /* --- cacheline 1 boundary (64 bytes) --- */
            struct btf *               btf;                  /*    64     8 */
            struct bpf_map_memory memory;                    /*    72    16 */
            bool                       unpriv_array;         /*    88     1 */
            bool                       frozen;               /*    89     1 */
    
            /* XXX 38 bytes hole, try to pack */
    
            /* --- cacheline 2 boundary (128 bytes) --- */
            atomic64_t                 refcnt __attribute__((__aligned__(64))); /*   128     8 */
            atomic64_t                 usercnt;              /*   136     8 */
            struct work_struct work;                         /*   144    32 */
            char                       name[16];             /*   176    16 */
    
            /* size: 192, cachelines: 3, members: 21 */
            /* sum members: 154, holes: 1, sum holes: 38 */
            /* forced alignments: 2, forced holes: 1, sum forced holes: 38 */
    } __attribute__((__aligned__(64)));
    
    This patch, while modifying all users of bpf_map_inc, also cleans up its
    interface to match bpf_map_put with separate operations for bpf_map_inc and
    bpf_map_inc_with_uref (to match bpf_map_put and bpf_map_put_with_uref,
    respectively). Also, given there are no users of bpf_map_inc_not_zero
    specifying uref=true, remove uref flag and default to uref=false internally.
    
    Signed-off-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20191117172806.2195367-2-andriin@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c88c815c2154..20030751b7a2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -311,7 +311,7 @@ static void bpf_map_free_deferred(struct work_struct *work)
 
 static void bpf_map_put_uref(struct bpf_map *map)
 {
-	if (atomic_dec_and_test(&map->usercnt)) {
+	if (atomic64_dec_and_test(&map->usercnt)) {
 		if (map->ops->map_release_uref)
 			map->ops->map_release_uref(map);
 	}
@@ -322,7 +322,7 @@ static void bpf_map_put_uref(struct bpf_map *map)
  */
 static void __bpf_map_put(struct bpf_map *map, bool do_idr_lock)
 {
-	if (atomic_dec_and_test(&map->refcnt)) {
+	if (atomic64_dec_and_test(&map->refcnt)) {
 		/* bpf_map_free_id() must be called first */
 		bpf_map_free_id(map, do_idr_lock);
 		btf_put(map->btf);
@@ -575,8 +575,8 @@ static int map_create(union bpf_attr *attr)
 	if (err)
 		goto free_map;
 
-	atomic_set(&map->refcnt, 1);
-	atomic_set(&map->usercnt, 1);
+	atomic64_set(&map->refcnt, 1);
+	atomic64_set(&map->usercnt, 1);
 
 	if (attr->btf_key_type_id || attr->btf_value_type_id) {
 		struct btf *btf;
@@ -653,21 +653,19 @@ struct bpf_map *__bpf_map_get(struct fd f)
 	return f.file->private_data;
 }
 
-/* prog's and map's refcnt limit */
-#define BPF_MAX_REFCNT 32768
-
-struct bpf_map *bpf_map_inc(struct bpf_map *map, bool uref)
+void bpf_map_inc(struct bpf_map *map)
 {
-	if (atomic_inc_return(&map->refcnt) > BPF_MAX_REFCNT) {
-		atomic_dec(&map->refcnt);
-		return ERR_PTR(-EBUSY);
-	}
-	if (uref)
-		atomic_inc(&map->usercnt);
-	return map;
+	atomic64_inc(&map->refcnt);
 }
 EXPORT_SYMBOL_GPL(bpf_map_inc);
 
+void bpf_map_inc_with_uref(struct bpf_map *map)
+{
+	atomic64_inc(&map->refcnt);
+	atomic64_inc(&map->usercnt);
+}
+EXPORT_SYMBOL_GPL(bpf_map_inc_with_uref);
+
 struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 {
 	struct fd f = fdget(ufd);
@@ -677,38 +675,30 @@ struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 	if (IS_ERR(map))
 		return map;
 
-	map = bpf_map_inc(map, true);
+	bpf_map_inc_with_uref(map);
 	fdput(f);
 
 	return map;
 }
 
 /* map_idr_lock should have been held */
-static struct bpf_map *__bpf_map_inc_not_zero(struct bpf_map *map,
-					      bool uref)
+static struct bpf_map *__bpf_map_inc_not_zero(struct bpf_map *map, bool uref)
 {
 	int refold;
 
-	refold = atomic_fetch_add_unless(&map->refcnt, 1, 0);
-
-	if (refold >= BPF_MAX_REFCNT) {
-		__bpf_map_put(map, false);
-		return ERR_PTR(-EBUSY);
-	}
-
+	refold = atomic64_fetch_add_unless(&map->refcnt, 1, 0);
 	if (!refold)
 		return ERR_PTR(-ENOENT);
-
 	if (uref)
-		atomic_inc(&map->usercnt);
+		atomic64_inc(&map->usercnt);
 
 	return map;
 }
 
-struct bpf_map *bpf_map_inc_not_zero(struct bpf_map *map, bool uref)
+struct bpf_map *bpf_map_inc_not_zero(struct bpf_map *map)
 {
 	spin_lock_bh(&map_idr_lock);
-	map = __bpf_map_inc_not_zero(map, uref);
+	map = __bpf_map_inc_not_zero(map, false);
 	spin_unlock_bh(&map_idr_lock);
 
 	return map;
@@ -1455,6 +1445,9 @@ static struct bpf_prog *____bpf_prog_get(struct fd f)
 	return f.file->private_data;
 }
 
+/* prog's refcnt limit */
+#define BPF_MAX_REFCNT 32768
+
 struct bpf_prog *bpf_prog_add(struct bpf_prog *prog, int i)
 {
 	if (atomic_add_return(i, &prog->aux->refcnt) > BPF_MAX_REFCNT) {

commit 5b92a28aae4dd0f88778d540ecfdcdaec5a41723
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Nov 14 10:57:17 2019 -0800

    bpf: Support attaching tracing BPF program to other BPF programs
    
    Allow FENTRY/FEXIT BPF programs to attach to other BPF programs of any type
    including their subprograms. This feature allows snooping on input and output
    packets in XDP, TC programs including their return values. In order to do that
    the verifier needs to track types not only of vmlinux, but types of other BPF
    programs as well. The verifier also needs to translate uapi/linux/bpf.h types
    used by networking programs into kernel internal BTF types used by FENTRY/FEXIT
    BPF programs. In some cases LLVM optimizations can remove arguments from BPF
    subprograms without adjusting BTF info that LLVM backend knows. When BTF info
    disagrees with actual types that the verifiers sees the BPF trampoline has to
    fallback to conservative and treat all arguments as u64. The FENTRY/FEXIT
    program can still attach to such subprograms, but it won't be able to recognize
    pointer types like 'struct sk_buff *' and it won't be able to pass them to
    bpf_skb_output() for dumping packets to user space. The FENTRY/FEXIT program
    would need to use bpf_probe_read_kernel() instead.
    
    The BPF_PROG_LOAD command is extended with attach_prog_fd field. When it's set
    to zero the attach_btf_id is one vmlinux BTF type ids. When attach_prog_fd
    points to previously loaded BPF program the attach_btf_id is BTF type id of
    main function or one of its subprograms.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20191114185720.1641606-18-ast@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 43ba647de720..c88c815c2154 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1577,7 +1577,7 @@ static void bpf_prog_load_fixup_attach_type(union bpf_attr *attr)
 static int
 bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 			   enum bpf_attach_type expected_attach_type,
-			   u32 btf_id)
+			   u32 btf_id, u32 prog_fd)
 {
 	switch (prog_type) {
 	case BPF_PROG_TYPE_TRACING:
@@ -1585,7 +1585,7 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 			return -EINVAL;
 		break;
 	default:
-		if (btf_id)
+		if (btf_id || prog_fd)
 			return -EINVAL;
 		break;
 	}
@@ -1636,7 +1636,7 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 }
 
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD attach_btf_id
+#define	BPF_PROG_LOAD_LAST_FIELD attach_prog_fd
 
 static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 {
@@ -1679,7 +1679,8 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 
 	bpf_prog_load_fixup_attach_type(attr);
 	if (bpf_prog_load_check_attach(type, attr->expected_attach_type,
-				       attr->attach_btf_id))
+				       attr->attach_btf_id,
+				       attr->attach_prog_fd))
 		return -EINVAL;
 
 	/* plain bpf_prog allocation */
@@ -1689,6 +1690,16 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 
 	prog->expected_attach_type = attr->expected_attach_type;
 	prog->aux->attach_btf_id = attr->attach_btf_id;
+	if (attr->attach_prog_fd) {
+		struct bpf_prog *tgt_prog;
+
+		tgt_prog = bpf_prog_get(attr->attach_prog_fd);
+		if (IS_ERR(tgt_prog)) {
+			err = PTR_ERR(tgt_prog);
+			goto free_prog_nouncharge;
+		}
+		prog->aux->linked_prog = tgt_prog;
+	}
 
 	prog->aux->offload_requested = !!attr->prog_ifindex;
 

commit 8c1b6e69dcc1e11bd24111e3734dd740aaf3fda1
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Nov 14 10:57:16 2019 -0800

    bpf: Compare BTF types of functions arguments with actual types
    
    Make the verifier check that BTF types of function arguments match actual types
    passed into top-level BPF program and into BPF-to-BPF calls. If types match
    such BPF programs and sub-programs will have full support of BPF trampoline. If
    types mismatch the trampoline has to be conservative. It has to save/restore
    five program arguments and assume 64-bit scalars.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Song Liu <songliubraving@fb.com>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Link: https://lore.kernel.org/bpf/20191114185720.1641606-17-ast@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 05a0ee75eca0..43ba647de720 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1328,6 +1328,7 @@ static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 	struct bpf_prog_aux *aux = container_of(rcu, struct bpf_prog_aux, rcu);
 
 	kvfree(aux->func_info);
+	kfree(aux->func_info_aux);
 	free_used_maps(aux);
 	bpf_prog_uncharge_memlock(aux->prog);
 	security_bpf_prog_free(aux);

commit 91cc1a99740e2ed1d903b5906afb470cc5a07379
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Nov 14 10:57:15 2019 -0800

    bpf: Annotate context types
    
    Annotate BPF program context types with program-side type and kernel-side type.
    This type information is used by the verifier. btf_get_prog_ctx_type() is
    used in the later patches to verify that BTF type of ctx in BPF program matches to
    kernel expected ctx type. For example, the XDP program type is:
    BPF_PROG_TYPE(BPF_PROG_TYPE_XDP, xdp, struct xdp_md, struct xdp_buff)
    That means that XDP program should be written as:
    int xdp_prog(struct xdp_md *ctx) { ... }
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20191114185720.1641606-16-ast@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index e2e37bea86bc..05a0ee75eca0 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -43,7 +43,7 @@ static DEFINE_SPINLOCK(map_idr_lock);
 int sysctl_unprivileged_bpf_disabled __read_mostly;
 
 static const struct bpf_map_ops * const bpf_map_types[] = {
-#define BPF_PROG_TYPE(_id, _ops)
+#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type)
 #define BPF_MAP_TYPE(_id, _ops) \
 	[_id] = &_ops,
 #include <linux/bpf_types.h>
@@ -1189,7 +1189,7 @@ static int map_freeze(const union bpf_attr *attr)
 }
 
 static const struct bpf_prog_ops * const bpf_prog_types[] = {
-#define BPF_PROG_TYPE(_id, _name) \
+#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type) \
 	[_id] = & _name ## _prog_ops,
 #define BPF_MAP_TYPE(_id, _ops)
 #include <linux/bpf_types.h>

commit fec56f5890d93fc2ed74166c397dc186b1c25951
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Nov 14 10:57:04 2019 -0800

    bpf: Introduce BPF trampoline
    
    Introduce BPF trampoline concept to allow kernel code to call into BPF programs
    with practically zero overhead.  The trampoline generation logic is
    architecture dependent.  It's converting native calling convention into BPF
    calling convention.  BPF ISA is 64-bit (even on 32-bit architectures). The
    registers R1 to R5 are used to pass arguments into BPF functions. The main BPF
    program accepts only single argument "ctx" in R1. Whereas CPU native calling
    convention is different. x86-64 is passing first 6 arguments in registers
    and the rest on the stack. x86-32 is passing first 3 arguments in registers.
    sparc64 is passing first 6 in registers. And so on.
    
    The trampolines between BPF and kernel already exist.  BPF_CALL_x macros in
    include/linux/filter.h statically compile trampolines from BPF into kernel
    helpers. They convert up to five u64 arguments into kernel C pointers and
    integers. On 64-bit architectures this BPF_to_kernel trampolines are nops. On
    32-bit architecture they're meaningful.
    
    The opposite job kernel_to_BPF trampolines is done by CAST_TO_U64 macros and
    __bpf_trace_##call() shim functions in include/trace/bpf_probe.h. They convert
    kernel function arguments into array of u64s that BPF program consumes via
    R1=ctx pointer.
    
    This patch set is doing the same job as __bpf_trace_##call() static
    trampolines, but dynamically for any kernel function. There are ~22k global
    kernel functions that are attachable via nop at function entry. The function
    arguments and types are described in BTF.  The job of btf_distill_func_proto()
    function is to extract useful information from BTF into "function model" that
    architecture dependent trampoline generators will use to generate assembly code
    to cast kernel function arguments into array of u64s.  For example the kernel
    function eth_type_trans has two pointers. They will be casted to u64 and stored
    into stack of generated trampoline. The pointer to that stack space will be
    passed into BPF program in R1. On x86-64 such generated trampoline will consume
    16 bytes of stack and two stores of %rdi and %rsi into stack. The verifier will
    make sure that only two u64 are accessed read-only by BPF program. The verifier
    will also recognize the precise type of the pointers being accessed and will
    not allow typecasting of the pointer to a different type within BPF program.
    
    The tracing use case in the datacenter demonstrated that certain key kernel
    functions have (like tcp_retransmit_skb) have 2 or more kprobes that are always
    active.  Other functions have both kprobe and kretprobe.  So it is essential to
    keep both kernel code and BPF programs executing at maximum speed. Hence
    generated BPF trampoline is re-generated every time new program is attached or
    detached to maintain maximum performance.
    
    To avoid the high cost of retpoline the attached BPF programs are called
    directly. __bpf_prog_enter/exit() are used to support per-program execution
    stats.  In the future this logic will be optimized further by adding support
    for bpf_stats_enabled_key inside generated assembly code. Introduction of
    preemptible and sleepable BPF programs will completely remove the need to call
    to __bpf_prog_enter/exit().
    
    Detach of a BPF program from the trampoline should not fail. To avoid memory
    allocation in detach path the half of the page is used as a reserve and flipped
    after each attach/detach. 2k bytes is enough to call 40+ BPF programs directly
    which is enough for BPF tracing use cases. This limit can be increased in the
    future.
    
    BPF_TRACE_FENTRY programs have access to raw kernel function arguments while
    BPF_TRACE_FEXIT programs have access to kernel return value as well. Often
    kprobe BPF program remembers function arguments in a map while kretprobe
    fetches arguments from a map and analyzes them together with return value.
    BPF_TRACE_FEXIT accelerates this typical use case.
    
    Recursion prevention for kprobe BPF programs is done via per-cpu
    bpf_prog_active counter. In practice that turned out to be a mistake. It
    caused programs to randomly skip execution. The tracing tools missed results
    they were looking for. Hence BPF trampoline doesn't provide builtin recursion
    prevention. It's a job of BPF program itself and will be addressed in the
    follow up patches.
    
    BPF trampoline is intended to be used beyond tracing and fentry/fexit use cases
    in the future. For example to remove retpoline cost from XDP programs.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Link: https://lore.kernel.org/bpf/20191114185720.1641606-5-ast@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 6d9ce95e5a8d..e2e37bea86bc 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1799,6 +1799,49 @@ static int bpf_obj_get(const union bpf_attr *attr)
 				attr->file_flags);
 }
 
+static int bpf_tracing_prog_release(struct inode *inode, struct file *filp)
+{
+	struct bpf_prog *prog = filp->private_data;
+
+	WARN_ON_ONCE(bpf_trampoline_unlink_prog(prog));
+	bpf_prog_put(prog);
+	return 0;
+}
+
+static const struct file_operations bpf_tracing_prog_fops = {
+	.release	= bpf_tracing_prog_release,
+	.read		= bpf_dummy_read,
+	.write		= bpf_dummy_write,
+};
+
+static int bpf_tracing_prog_attach(struct bpf_prog *prog)
+{
+	int tr_fd, err;
+
+	if (prog->expected_attach_type != BPF_TRACE_FENTRY &&
+	    prog->expected_attach_type != BPF_TRACE_FEXIT) {
+		err = -EINVAL;
+		goto out_put_prog;
+	}
+
+	err = bpf_trampoline_link_prog(prog);
+	if (err)
+		goto out_put_prog;
+
+	tr_fd = anon_inode_getfd("bpf-tracing-prog", &bpf_tracing_prog_fops,
+				 prog, O_CLOEXEC);
+	if (tr_fd < 0) {
+		WARN_ON_ONCE(bpf_trampoline_unlink_prog(prog));
+		err = tr_fd;
+		goto out_put_prog;
+	}
+	return tr_fd;
+
+out_put_prog:
+	bpf_prog_put(prog);
+	return err;
+}
+
 struct bpf_raw_tracepoint {
 	struct bpf_raw_event_map *btp;
 	struct bpf_prog *prog;
@@ -1850,14 +1893,16 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 
 	if (prog->type == BPF_PROG_TYPE_TRACING) {
 		if (attr->raw_tracepoint.name) {
-			/* raw_tp name should not be specified in raw_tp
-			 * programs that were verified via in-kernel BTF info
+			/* The attach point for this category of programs
+			 * should be specified via btf_id during program load.
 			 */
 			err = -EINVAL;
 			goto out_put_prog;
 		}
-		/* raw_tp name is taken from type name instead */
-		tp_name = prog->aux->attach_func_name;
+		if (prog->expected_attach_type == BPF_TRACE_RAW_TP)
+			tp_name = prog->aux->attach_func_name;
+		else
+			return bpf_tracing_prog_attach(prog);
 	} else {
 		if (strncpy_from_user(buf,
 				      u64_to_user_ptr(attr->raw_tracepoint.name),

commit ae8a76fb8b5d03fa2adc7249dc6131ba6a0c6119
Merge: d31e95585ca6 358fdb456288
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 2 15:27:42 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Alexei Starovoitov says:
    
    ====================
    pull-request: bpf-next 2019-11-02
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    We've added 30 non-merge commits during the last 7 day(s) which contain
    a total of 41 files changed, 1864 insertions(+), 474 deletions(-).
    
    The main changes are:
    
    1) Fix long standing user vs kernel access issue by introducing
       bpf_probe_read_user() and bpf_probe_read_kernel() helpers, from Daniel.
    
    2) Accelerated xskmap lookup, from Björn and Maciej.
    
    3) Support for automatic map pinning in libbpf, from Toke.
    
    4) Cleanup of BTF-enabled raw tracepoints, from Alexei.
    
    5) Various fixes to libbpf and selftests.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d31e95585ca697fb31440c6fe30113adc85ecfbd
Merge: c23fcbbc6aa4 1204c70d9dcb
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 2 13:12:51 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    The only slightly tricky merge conflict was the netdevsim because the
    mutex locking fix overlapped a lot of driver reload reorganization.
    
    The rest were (relatively) trivial in nature.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f1b9509c2fb0ef4db8d22dac9aef8e856a5d81f6
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Wed Oct 30 15:32:11 2019 -0700

    bpf: Replace prog_raw_tp+btf_id with prog_tracing
    
    The bpf program type raw_tp together with 'expected_attach_type'
    was the most appropriate api to indicate BTF-enabled raw_tp programs.
    But during development it became apparent that 'expected_attach_type'
    cannot be used and new 'attach_btf_id' field had to be introduced.
    Which means that the information is duplicated in two fields where
    one of them is ignored.
    Clean it up by introducing new program type where both
    'expected_attach_type' and 'attach_btf_id' fields have
    specific meaning.
    In the future 'expected_attach_type' will be extended
    with other attach points that have similar semantics to raw_tp.
    This patch is replacing BTF-enabled BPF_PROG_TYPE_RAW_TRACEPOINT with
    prog_type = BPF_RPOG_TYPE_TRACING
    expected_attach_type = BPF_TRACE_RAW_TP
    attach_btf_id = btf_id of raw tracepoint inside the kernel
    Future patches will add
    expected_attach_type = BPF_TRACE_FENTRY or BPF_TRACE_FEXIT
    where programs have the same input context and the same helpers,
    but different attach points.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20191030223212.953010-2-ast@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ff5225759553..985d01ced196 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1571,7 +1571,7 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 			   u32 btf_id)
 {
 	switch (prog_type) {
-	case BPF_PROG_TYPE_RAW_TRACEPOINT:
+	case BPF_PROG_TYPE_TRACING:
 		if (btf_id > BTF_MAX_TYPE)
 			return -EINVAL;
 		break;
@@ -1833,13 +1833,13 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 		return PTR_ERR(prog);
 
 	if (prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT &&
+	    prog->type != BPF_PROG_TYPE_TRACING &&
 	    prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE) {
 		err = -EINVAL;
 		goto out_put_prog;
 	}
 
-	if (prog->type == BPF_PROG_TYPE_RAW_TRACEPOINT &&
-	    prog->aux->attach_btf_id) {
+	if (prog->type == BPF_PROG_TYPE_TRACING) {
 		if (attr->raw_tracepoint.name) {
 			/* raw_tp name should not be specified in raw_tp
 			 * programs that were verified via in-kernel BTF info

commit 3820729160440158a014add69cc0d371061a96b2
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Thu Oct 24 17:18:11 2019 -0700

    bpf: Prepare btf_ctx_access for non raw_tp use case
    
    This patch makes a few changes to btf_ctx_access() to prepare
    it for non raw_tp use case where the attach_btf_id is not
    necessary a BTF_KIND_TYPEDEF.
    
    It moves the "btf_trace_" prefix check and typedef-follow logic to a new
    function "check_attach_btf_id()" which is called only once during
    bpf_check().  btf_ctx_access() only operates on a BTF_KIND_FUNC_PROTO
    type now. That should also be more efficient since it is done only
    one instead of every-time check_ctx_access() is called.
    
    "check_attach_btf_id()" needs to find the func_proto type from
    the attach_btf_id.  It needs to store the result into the
    newly added prog->aux->attach_func_proto.  func_proto
    btf type has no name, so a proper name should be stored into
    "attach_func_name" also.
    
    v2:
    - Move the "btf_trace_" check to an earlier verifier phase (Alexei)
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20191025001811.1718491-1-kafai@fb.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 16ea3c0db4f6..ff5225759553 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1848,9 +1848,7 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 			goto out_put_prog;
 		}
 		/* raw_tp name is taken from type name instead */
-		tp_name = kernel_type_name(prog->aux->attach_btf_id);
-		/* skip the prefix */
-		tp_name += sizeof("btf_trace_") - 1;
+		tp_name = prog->aux->attach_func_name;
 	} else {
 		if (strncpy_from_user(buf,
 				      u64_to_user_ptr(attr->raw_tracepoint.name),

commit 3b4d9eb2ee74dd5ea7fa36cffb0ca7f5bc4924da
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Oct 22 23:30:38 2019 +0200

    bpf: Fix use after free in bpf_get_prog_name
    
    There is one more problematic case I noticed while recently fixing BPF kallsyms
    handling in cd7455f1013e ("bpf: Fix use after free in subprog's jited symbol
    removal") and that is bpf_get_prog_name().
    
    If BTF has been attached to the prog, then we may be able to fetch the function
    signature type id in kallsyms through prog->aux->func_info[prog->aux->func_idx].type_id.
    However, while the BTF object itself is torn down via RCU callback, the prog's
    aux->func_info is immediately freed via kvfree(prog->aux->func_info) once the
    prog's refcount either hit zero or when subprograms were already exposed via
    kallsyms and we hit the error path added in 5482e9a93c83 ("bpf: Fix memleak in
    aux->func_info and aux->btf").
    
    This violates RCU as well since kallsyms could be walked in parallel where we
    could access aux->func_info. Hence, defer kvfree() to after RCU grace period.
    Looking at ba64e7d85252 ("bpf: btf: support proper non-jit func info") there
    is no reason/dependency where we couldn't defer the kvfree(aux->func_info) into
    the RCU callback.
    
    Fixes: 5482e9a93c83 ("bpf: Fix memleak in aux->func_info and aux->btf")
    Fixes: ba64e7d85252 ("bpf: btf: support proper non-jit func info")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Yonghong Song <yhs@fb.com>
    Cc: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/875f2906a7c1a0691f2d567b4d8e4ea2739b1e88.1571779205.git.daniel@iogearbox.net

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index bcfc362de4f2..0937719b87e2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1326,6 +1326,7 @@ static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 {
 	struct bpf_prog_aux *aux = container_of(rcu, struct bpf_prog_aux, rcu);
 
+	kvfree(aux->func_info);
 	free_used_maps(aux);
 	bpf_prog_uncharge_memlock(aux->prog);
 	security_bpf_prog_free(aux);
@@ -1336,7 +1337,6 @@ static void __bpf_prog_put_noref(struct bpf_prog *prog, bool deferred)
 {
 	bpf_prog_kallsyms_del_all(prog);
 	btf_put(prog->aux->btf);
-	kvfree(prog->aux->func_info);
 	bpf_prog_free_linfo(prog);
 
 	if (deferred)

commit cd7455f1013ef96d5cbf5c05d2b7c06f273810a6
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Oct 22 15:57:23 2019 +0200

    bpf: Fix use after free in subprog's jited symbol removal
    
    syzkaller managed to trigger the following crash:
    
      [...]
      BUG: unable to handle page fault for address: ffffc90001923030
      #PF: supervisor read access in kernel mode
      #PF: error_code(0x0000) - not-present page
      PGD aa551067 P4D aa551067 PUD aa552067 PMD a572b067 PTE 80000000a1173163
      Oops: 0000 [#1] PREEMPT SMP KASAN
      CPU: 0 PID: 7982 Comm: syz-executor912 Not tainted 5.4.0-rc3+ #0
      Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
      RIP: 0010:bpf_jit_binary_hdr include/linux/filter.h:787 [inline]
      RIP: 0010:bpf_get_prog_addr_region kernel/bpf/core.c:531 [inline]
      RIP: 0010:bpf_tree_comp kernel/bpf/core.c:600 [inline]
      RIP: 0010:__lt_find include/linux/rbtree_latch.h:115 [inline]
      RIP: 0010:latch_tree_find include/linux/rbtree_latch.h:208 [inline]
      RIP: 0010:bpf_prog_kallsyms_find kernel/bpf/core.c:674 [inline]
      RIP: 0010:is_bpf_text_address+0x184/0x3b0 kernel/bpf/core.c:709
      [...]
      Call Trace:
       kernel_text_address kernel/extable.c:147 [inline]
       __kernel_text_address+0x9a/0x110 kernel/extable.c:102
       unwind_get_return_address+0x4c/0x90 arch/x86/kernel/unwind_frame.c:19
       arch_stack_walk+0x98/0xe0 arch/x86/kernel/stacktrace.c:26
       stack_trace_save+0xb6/0x150 kernel/stacktrace.c:123
       save_stack mm/kasan/common.c:69 [inline]
       set_track mm/kasan/common.c:77 [inline]
       __kasan_kmalloc+0x11c/0x1b0 mm/kasan/common.c:510
       kasan_slab_alloc+0xf/0x20 mm/kasan/common.c:518
       slab_post_alloc_hook mm/slab.h:584 [inline]
       slab_alloc mm/slab.c:3319 [inline]
       kmem_cache_alloc+0x1f5/0x2e0 mm/slab.c:3483
       getname_flags+0xba/0x640 fs/namei.c:138
       getname+0x19/0x20 fs/namei.c:209
       do_sys_open+0x261/0x560 fs/open.c:1091
       __do_sys_open fs/open.c:1115 [inline]
       __se_sys_open fs/open.c:1110 [inline]
       __x64_sys_open+0x87/0x90 fs/open.c:1110
       do_syscall_64+0xf7/0x1c0 arch/x86/entry/common.c:290
       entry_SYSCALL_64_after_hwframe+0x49/0xbe
      [...]
    
    After further debugging it turns out that we walk kallsyms while in parallel
    we tear down a BPF program which contains subprograms that have been JITed
    though the program itself has not been fully exposed and is eventually bailing
    out with error.
    
    The bpf_prog_kallsyms_del_subprogs() in bpf_prog_load()'s error path removes
    the symbols, however, bpf_prog_free() tears down the JIT memory too early via
    scheduled work. Instead, it needs to properly respect RCU grace period as the
    kallsyms walk for BPF is under RCU.
    
    Fix it by refactoring __bpf_prog_put()'s tear down and reuse it in our error
    path where we defer final destruction when we have subprogs in the program.
    
    Fixes: 7d1982b4e335 ("bpf: fix panic in prog load calls cleanup")
    Fixes: 1c2a088a6626 ("bpf: x64: add JIT support for multi-function programs")
    Reported-by: syzbot+710043c5d1d5b5013bc7@syzkaller.appspotmail.com
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: syzbot+710043c5d1d5b5013bc7@syzkaller.appspotmail.com
    Link: https://lore.kernel.org/bpf/55f6367324c2d7e9583fa9ccf5385dcbba0d7a6e.1571752452.git.daniel@iogearbox.net

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 82eabd4e38ad..bcfc362de4f2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1332,18 +1332,26 @@ static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 	bpf_prog_free(aux->prog);
 }
 
+static void __bpf_prog_put_noref(struct bpf_prog *prog, bool deferred)
+{
+	bpf_prog_kallsyms_del_all(prog);
+	btf_put(prog->aux->btf);
+	kvfree(prog->aux->func_info);
+	bpf_prog_free_linfo(prog);
+
+	if (deferred)
+		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
+	else
+		__bpf_prog_put_rcu(&prog->aux->rcu);
+}
+
 static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
 		perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_UNLOAD, 0);
 		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog, do_idr_lock);
-		bpf_prog_kallsyms_del_all(prog);
-		btf_put(prog->aux->btf);
-		kvfree(prog->aux->func_info);
-		bpf_prog_free_linfo(prog);
-
-		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
+		__bpf_prog_put_noref(prog, true);
 	}
 }
 
@@ -1741,11 +1749,12 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	return err;
 
 free_used_maps:
-	bpf_prog_free_linfo(prog);
-	kvfree(prog->aux->func_info);
-	btf_put(prog->aux->btf);
-	bpf_prog_kallsyms_del_subprogs(prog);
-	free_used_maps(prog->aux);
+	/* In case we have subprogs, we need to wait for a grace
+	 * period before we can tear down JIT memory since symbols
+	 * are already exposed under kallsyms.
+	 */
+	__bpf_prog_put_noref(prog, prog->aux->func_cnt);
+	return err;
 free_prog:
 	bpf_prog_uncharge_memlock(prog);
 free_prog_sec:

commit c108e3c1bdbd0783d7c19ee80abb0591f79029e8
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Oct 17 23:09:33 2019 -0700

    bpf: Fix bpf_attr.attach_btf_id check
    
    Only raw_tracepoint program type can have bpf_attr.attach_btf_id >= 0.
    Make sure to reject other program types that accidentally set it to non-zero.
    
    Fixes: ccfe29eb29c2 ("bpf: Add attach_btf_id attribute to program load")
    Reported-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Yonghong Song <yhs@fb.com>
    Link: https://lore.kernel.org/bpf/20191018060933.2950231-1-ast@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 523e3ac15a08..16ea3c0db4f6 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1570,6 +1570,17 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 			   enum bpf_attach_type expected_attach_type,
 			   u32 btf_id)
 {
+	switch (prog_type) {
+	case BPF_PROG_TYPE_RAW_TRACEPOINT:
+		if (btf_id > BTF_MAX_TYPE)
+			return -EINVAL;
+		break;
+	default:
+		if (btf_id)
+			return -EINVAL;
+		break;
+	}
+
 	switch (prog_type) {
 	case BPF_PROG_TYPE_CGROUP_SOCK:
 		switch (expected_attach_type) {
@@ -1610,13 +1621,7 @@ bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
 		default:
 			return -EINVAL;
 		}
-	case BPF_PROG_TYPE_RAW_TRACEPOINT:
-		if (btf_id > BTF_MAX_TYPE)
-			return -EINVAL;
-		return 0;
 	default:
-		if (btf_id)
-			return -EINVAL;
 		return 0;
 	}
 }

commit ac4414b5ca47d16c8de3134cc1b868056c4a68ea
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Tue Oct 15 20:25:01 2019 -0700

    bpf: Attach raw_tp program with BTF via type name
    
    BTF type id specified at program load time has all
    necessary information to attach that program to raw tracepoint.
    Use kernel type name to find raw tracepoint.
    
    Add missing CHECK_ATTR() condition.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20191016032505.2089704-8-ast@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b56c482c9760..523e3ac15a08 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1816,17 +1816,52 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 	struct bpf_raw_tracepoint *raw_tp;
 	struct bpf_raw_event_map *btp;
 	struct bpf_prog *prog;
-	char tp_name[128];
+	const char *tp_name;
+	char buf[128];
 	int tp_fd, err;
 
-	if (strncpy_from_user(tp_name, u64_to_user_ptr(attr->raw_tracepoint.name),
-			      sizeof(tp_name) - 1) < 0)
-		return -EFAULT;
-	tp_name[sizeof(tp_name) - 1] = 0;
+	if (CHECK_ATTR(BPF_RAW_TRACEPOINT_OPEN))
+		return -EINVAL;
+
+	prog = bpf_prog_get(attr->raw_tracepoint.prog_fd);
+	if (IS_ERR(prog))
+		return PTR_ERR(prog);
+
+	if (prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT &&
+	    prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE) {
+		err = -EINVAL;
+		goto out_put_prog;
+	}
+
+	if (prog->type == BPF_PROG_TYPE_RAW_TRACEPOINT &&
+	    prog->aux->attach_btf_id) {
+		if (attr->raw_tracepoint.name) {
+			/* raw_tp name should not be specified in raw_tp
+			 * programs that were verified via in-kernel BTF info
+			 */
+			err = -EINVAL;
+			goto out_put_prog;
+		}
+		/* raw_tp name is taken from type name instead */
+		tp_name = kernel_type_name(prog->aux->attach_btf_id);
+		/* skip the prefix */
+		tp_name += sizeof("btf_trace_") - 1;
+	} else {
+		if (strncpy_from_user(buf,
+				      u64_to_user_ptr(attr->raw_tracepoint.name),
+				      sizeof(buf) - 1) < 0) {
+			err = -EFAULT;
+			goto out_put_prog;
+		}
+		buf[sizeof(buf) - 1] = 0;
+		tp_name = buf;
+	}
 
 	btp = bpf_get_raw_tracepoint(tp_name);
-	if (!btp)
-		return -ENOENT;
+	if (!btp) {
+		err = -ENOENT;
+		goto out_put_prog;
+	}
 
 	raw_tp = kzalloc(sizeof(*raw_tp), GFP_USER);
 	if (!raw_tp) {
@@ -1834,38 +1869,27 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 		goto out_put_btp;
 	}
 	raw_tp->btp = btp;
-
-	prog = bpf_prog_get(attr->raw_tracepoint.prog_fd);
-	if (IS_ERR(prog)) {
-		err = PTR_ERR(prog);
-		goto out_free_tp;
-	}
-	if (prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT &&
-	    prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE) {
-		err = -EINVAL;
-		goto out_put_prog;
-	}
+	raw_tp->prog = prog;
 
 	err = bpf_probe_register(raw_tp->btp, prog);
 	if (err)
-		goto out_put_prog;
+		goto out_free_tp;
 
-	raw_tp->prog = prog;
 	tp_fd = anon_inode_getfd("bpf-raw-tracepoint", &bpf_raw_tp_fops, raw_tp,
 				 O_CLOEXEC);
 	if (tp_fd < 0) {
 		bpf_probe_unregister(raw_tp->btp, prog);
 		err = tp_fd;
-		goto out_put_prog;
+		goto out_free_tp;
 	}
 	return tp_fd;
 
-out_put_prog:
-	bpf_prog_put(prog);
 out_free_tp:
 	kfree(raw_tp);
 out_put_btp:
 	bpf_put_raw_tracepoint(btp);
+out_put_prog:
+	bpf_prog_put(prog);
 	return err;
 }
 

commit ccfe29eb29c2edcea6552072ef00ff4117f53e83
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Tue Oct 15 20:24:58 2019 -0700

    bpf: Add attach_btf_id attribute to program load
    
    Add attach_btf_id attribute to prog_load command.
    It's similar to existing expected_attach_type attribute which is
    used in several cgroup based program types.
    Unfortunately expected_attach_type is ignored for
    tracing programs and cannot be reused for new purpose.
    Hence introduce attach_btf_id to verify bpf programs against
    given in-kernel BTF type id at load time.
    It is strictly checked to be valid for raw_tp programs only.
    In a later patches it will become:
    btf_id == 0 semantics of existing raw_tp progs.
    btd_id > 0 raw_tp with BTF and additional type safety.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Link: https://lore.kernel.org/bpf/20191016032505.2089704-5-ast@kernel.org

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 82eabd4e38ad..b56c482c9760 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -23,6 +23,7 @@
 #include <linux/timekeeping.h>
 #include <linux/ctype.h>
 #include <linux/nospec.h>
+#include <uapi/linux/btf.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY || \
 			   (map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
@@ -1565,8 +1566,9 @@ static void bpf_prog_load_fixup_attach_type(union bpf_attr *attr)
 }
 
 static int
-bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
-				enum bpf_attach_type expected_attach_type)
+bpf_prog_load_check_attach(enum bpf_prog_type prog_type,
+			   enum bpf_attach_type expected_attach_type,
+			   u32 btf_id)
 {
 	switch (prog_type) {
 	case BPF_PROG_TYPE_CGROUP_SOCK:
@@ -1608,13 +1610,19 @@ bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 		default:
 			return -EINVAL;
 		}
+	case BPF_PROG_TYPE_RAW_TRACEPOINT:
+		if (btf_id > BTF_MAX_TYPE)
+			return -EINVAL;
+		return 0;
 	default:
+		if (btf_id)
+			return -EINVAL;
 		return 0;
 	}
 }
 
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD line_info_cnt
+#define	BPF_PROG_LOAD_LAST_FIELD attach_btf_id
 
 static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 {
@@ -1656,7 +1664,8 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 		return -EPERM;
 
 	bpf_prog_load_fixup_attach_type(attr);
-	if (bpf_prog_load_check_attach_type(type, attr->expected_attach_type))
+	if (bpf_prog_load_check_attach(type, attr->expected_attach_type,
+				       attr->attach_btf_id))
 		return -EINVAL;
 
 	/* plain bpf_prog allocation */
@@ -1665,6 +1674,7 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 		return -ENOMEM;
 
 	prog->expected_attach_type = attr->expected_attach_type;
+	prog->aux->attach_btf_id = attr->attach_btf_id;
 
 	prog->aux->offload_requested = !!attr->prog_ifindex;
 

commit 1e46c09ec10049a9e366153b32e41cc557383fdb
Merge: f9bcfe214b00 593f191a8005
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Sep 6 16:49:17 2019 +0200

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    The following pull-request contains BPF updates for your *net-next* tree.
    
    The main changes are:
    
    1) Add the ability to use unaligned chunks in the AF_XDP umem. By
       relaxing where the chunks can be placed, it allows to use an
       arbitrary buffer size and place whenever there is a free
       address in the umem. Helps more seamless DPDK AF_XDP driver
       integration. Support for i40e, ixgbe and mlx5e, from Kevin and
       Maxim.
    
    2) Addition of a wakeup flag for AF_XDP tx and fill rings so the
       application can wake up the kernel for rx/tx processing which
       avoids busy-spinning of the latter, useful when app and driver
       is located on the same core. Support for i40e, ixgbe and mlx5e,
       from Magnus and Maxim.
    
    3) bpftool fixes for printf()-like functions so compiler can actually
       enforce checks, bpftool build system improvements for custom output
       directories, and addition of 'bpftool map freeze' command, from Quentin.
    
    4) Support attaching/detaching XDP programs from 'bpftool net' command,
       from Daniel.
    
    5) Automatic xskmap cleanup when AF_XDP socket is released, and several
       barrier/{read,write}_once fixes in AF_XDP code, from Björn.
    
    6) Relicense of bpf_helpers.h/bpf_endian.h for future libbpf
       inclusion as well as libbpf versioning improvements, from Andrii.
    
    7) Several new BPF kselftests for verifier precision tracking, from Alexei.
    
    8) Several BPF kselftest fixes wrt endianess to run on s390x, from Ilya.
    
    9) And more BPF kselftest improvements all over the place, from Stanislav.
    
    10) Add simple BPF map op cache for nfp driver to batch dumps, from Jakub.
    
    11) AF_XDP socket umem mapping improvements for 32bit archs, from Ivan.
    
    12) Add BPF-to-BPF call and BTF line info support for s390x JIT, from Yauheni.
    
    13) Small optimization in arm64 JIT to spare 1 insns for BPF_MOD, from Jerin.
    
    14) Fix an error check in bpf_tcp_gen_syncookie() helper, from Petar.
    
    15) Various minor fixes and cleanups, from Nathan, Masahiro, Masanari,
        Peter, Wei, Yue.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 10d274e880eb208ec6a76261a9f8f8155020f771
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Aug 22 22:52:12 2019 -0700

    bpf: introduce verifier internal test flag
    
    Introduce BPF_F_TEST_STATE_FREQ flag to stress test parentage chain
    and state pruning.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c0f62fd67c6b..ca60eafa6922 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1629,6 +1629,7 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 
 	if (attr->prog_flags & ~(BPF_F_STRICT_ALIGNMENT |
 				 BPF_F_ANY_ALIGNMENT |
+				 BPF_F_TEST_STATE_FREQ |
 				 BPF_F_TEST_RND_HI32))
 		return -EINVAL;
 

commit c751798aa224fadc5124b49eeb38fb468c0fa039
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Aug 23 22:14:23 2019 +0200

    bpf: fix use after free in prog symbol exposure
    
    syzkaller managed to trigger the warning in bpf_jit_free() which checks via
    bpf_prog_kallsyms_verify_off() for potentially unlinked JITed BPF progs
    in kallsyms, and subsequently trips over GPF when walking kallsyms entries:
    
      [...]
      8021q: adding VLAN 0 to HW filter on device batadv0
      8021q: adding VLAN 0 to HW filter on device batadv0
      WARNING: CPU: 0 PID: 9869 at kernel/bpf/core.c:810 bpf_jit_free+0x1e8/0x2a0
      Kernel panic - not syncing: panic_on_warn set ...
      CPU: 0 PID: 9869 Comm: kworker/0:7 Not tainted 5.0.0-rc8+ #1
      Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
      Workqueue: events bpf_prog_free_deferred
      Call Trace:
       __dump_stack lib/dump_stack.c:77 [inline]
       dump_stack+0x113/0x167 lib/dump_stack.c:113
       panic+0x212/0x40b kernel/panic.c:214
       __warn.cold.8+0x1b/0x38 kernel/panic.c:571
       report_bug+0x1a4/0x200 lib/bug.c:186
       fixup_bug arch/x86/kernel/traps.c:178 [inline]
       do_error_trap+0x11b/0x200 arch/x86/kernel/traps.c:271
       do_invalid_op+0x36/0x40 arch/x86/kernel/traps.c:290
       invalid_op+0x14/0x20 arch/x86/entry/entry_64.S:973
      RIP: 0010:bpf_jit_free+0x1e8/0x2a0
      Code: 02 4c 89 e2 83 e2 07 38 d0 7f 08 84 c0 0f 85 86 00 00 00 48 ba 00 02 00 00 00 00 ad de 0f b6 43 02 49 39 d6 0f 84 5f fe ff ff <0f> 0b e9 58 fe ff ff 48 b8 00 00 00 00 00 fc ff df 4c 89 e2 48 c1
      RSP: 0018:ffff888092f67cd8 EFLAGS: 00010202
      RAX: 0000000000000007 RBX: ffffc90001947000 RCX: ffffffff816e9d88
      RDX: dead000000000200 RSI: 0000000000000008 RDI: ffff88808769f7f0
      RBP: ffff888092f67d00 R08: fffffbfff1394059 R09: fffffbfff1394058
      R10: fffffbfff1394058 R11: ffffffff89ca02c7 R12: ffffc90001947002
      R13: ffffc90001947020 R14: ffffffff881eca80 R15: ffff88808769f7e8
      BUG: unable to handle kernel paging request at fffffbfff400d000
      #PF error: [normal kernel read fault]
      PGD 21ffee067 P4D 21ffee067 PUD 21ffed067 PMD 9f942067 PTE 0
      Oops: 0000 [#1] PREEMPT SMP KASAN
      CPU: 0 PID: 9869 Comm: kworker/0:7 Not tainted 5.0.0-rc8+ #1
      Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
      Workqueue: events bpf_prog_free_deferred
      RIP: 0010:bpf_get_prog_addr_region kernel/bpf/core.c:495 [inline]
      RIP: 0010:bpf_tree_comp kernel/bpf/core.c:558 [inline]
      RIP: 0010:__lt_find include/linux/rbtree_latch.h:115 [inline]
      RIP: 0010:latch_tree_find include/linux/rbtree_latch.h:208 [inline]
      RIP: 0010:bpf_prog_kallsyms_find+0x107/0x2e0 kernel/bpf/core.c:632
      Code: 00 f0 ff ff 44 38 c8 7f 08 84 c0 0f 85 fa 00 00 00 41 f6 45 02 01 75 02 0f 0b 48 39 da 0f 82 92 00 00 00 48 89 d8 48 c1 e8 03 <42> 0f b6 04 30 84 c0 74 08 3c 03 0f 8e 45 01 00 00 8b 03 48 c1 e0
      [...]
    
    Upon further debugging, it turns out that whenever we trigger this
    issue, the kallsyms removal in bpf_prog_ksym_node_del() was /skipped/
    but yet bpf_jit_free() reported that the entry is /in use/.
    
    Problem is that symbol exposure via bpf_prog_kallsyms_add() but also
    perf_event_bpf_event() were done /after/ bpf_prog_new_fd(). Once the
    fd is exposed to the public, a parallel close request came in right
    before we attempted to do the bpf_prog_kallsyms_add().
    
    Given at this time the prog reference count is one, we start to rip
    everything underneath us via bpf_prog_release() -> bpf_prog_put().
    The memory is eventually released via deferred free, so we're seeing
    that bpf_jit_free() has a kallsym entry because we added it from
    bpf_prog_load() but /after/ bpf_prog_put() from the remote CPU.
    
    Therefore, move both notifications /before/ we install the fd. The
    issue was never seen between bpf_prog_alloc_id() and bpf_prog_new_fd()
    because upon bpf_prog_get_fd_by_id() we'll take another reference to
    the BPF prog, so we're still holding the original reference from the
    bpf_prog_load().
    
    Fixes: 6ee52e2a3fe4 ("perf, bpf: Introduce PERF_RECORD_BPF_EVENT")
    Fixes: 74451e66d516 ("bpf: make jited programs visible in traces")
    Reported-by: syzbot+bd3bba6ff3fcea7a6ec6@syzkaller.appspotmail.com
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Song Liu <songliubraving@fb.com>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5d141f16f6fa..272071e9112f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1707,20 +1707,26 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	if (err)
 		goto free_used_maps;
 
-	err = bpf_prog_new_fd(prog);
-	if (err < 0) {
-		/* failed to allocate fd.
-		 * bpf_prog_put() is needed because the above
-		 * bpf_prog_alloc_id() has published the prog
-		 * to the userspace and the userspace may
-		 * have refcnt-ed it through BPF_PROG_GET_FD_BY_ID.
-		 */
-		bpf_prog_put(prog);
-		return err;
-	}
-
+	/* Upon success of bpf_prog_alloc_id(), the BPF prog is
+	 * effectively publicly exposed. However, retrieving via
+	 * bpf_prog_get_fd_by_id() will take another reference,
+	 * therefore it cannot be gone underneath us.
+	 *
+	 * Only for the time /after/ successful bpf_prog_new_fd()
+	 * and before returning to userspace, we might just hold
+	 * one reference and any parallel close on that fd could
+	 * rip everything out. Hence, below notifications must
+	 * happen before bpf_prog_new_fd().
+	 *
+	 * Also, any failure handling from this point onwards must
+	 * be using bpf_prog_put() given the program is exposed.
+	 */
 	bpf_prog_kallsyms_add(prog);
 	perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_LOAD, 0);
+
+	err = bpf_prog_new_fd(prog);
+	if (err < 0)
+		bpf_prog_put(prog);
 	return err;
 
 free_used_maps:

commit 1b9ed84ecf268904d89edf2908426a8eb3b5a4ba
Author: Quentin Monnet <quentin.monnet@netronome.com>
Date:   Tue Aug 20 10:31:50 2019 +0100

    bpf: add new BPF_BTF_GET_NEXT_ID syscall command
    
    Add a new command for the bpf() system call: BPF_BTF_GET_NEXT_ID is used
    to cycle through all BTF objects loaded on the system.
    
    The motivation is to be able to inspect (list) all BTF objects presents
    on the system.
    
    Signed-off-by: Quentin Monnet <quentin.monnet@netronome.com>
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index cf8052b016e7..c0f62fd67c6b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2884,6 +2884,10 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 		err = bpf_obj_get_next_id(&attr, uattr,
 					  &map_idr, &map_idr_lock);
 		break;
+	case BPF_BTF_GET_NEXT_ID:
+		err = bpf_obj_get_next_id(&attr, uattr,
+					  &btf_idr, &btf_idr_lock);
+		break;
 	case BPF_PROG_GET_FD_BY_ID:
 		err = bpf_prog_get_fd_by_id(&attr);
 		break;

commit b0e4701ce15d0381cdea0643c7f0a35dc529cec2
Author: Stanislav Fomichev <sdf@google.com>
Date:   Wed Aug 14 10:37:48 2019 -0700

    bpf: export bpf_map_inc_not_zero
    
    Rename existing bpf_map_inc_not_zero to __bpf_map_inc_not_zero to
    indicate that it's caller's responsibility to do proper locking.
    Create and export bpf_map_inc_not_zero wrapper that properly
    locks map_idr_lock. Will be used in the next commit to
    hold a map while cloning a socket.
    
    Cc: Martin KaFai Lau <kafai@fb.com>
    Cc: Yonghong Song <yhs@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5d141f16f6fa..cf8052b016e7 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -683,8 +683,8 @@ struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 }
 
 /* map_idr_lock should have been held */
-static struct bpf_map *bpf_map_inc_not_zero(struct bpf_map *map,
-					    bool uref)
+static struct bpf_map *__bpf_map_inc_not_zero(struct bpf_map *map,
+					      bool uref)
 {
 	int refold;
 
@@ -704,6 +704,16 @@ static struct bpf_map *bpf_map_inc_not_zero(struct bpf_map *map,
 	return map;
 }
 
+struct bpf_map *bpf_map_inc_not_zero(struct bpf_map *map, bool uref)
+{
+	spin_lock_bh(&map_idr_lock);
+	map = __bpf_map_inc_not_zero(map, uref);
+	spin_unlock_bh(&map_idr_lock);
+
+	return map;
+}
+EXPORT_SYMBOL_GPL(bpf_map_inc_not_zero);
+
 int __weak bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)
 {
 	return -ENOTSUPP;
@@ -2177,7 +2187,7 @@ static int bpf_map_get_fd_by_id(const union bpf_attr *attr)
 	spin_lock_bh(&map_idr_lock);
 	map = idr_find(&map_idr, id);
 	if (map)
-		map = bpf_map_inc_not_zero(map, true);
+		map = __bpf_map_inc_not_zero(map, true);
 	else
 		map = ERR_PTR(-ENOENT);
 	spin_unlock_bh(&map_idr_lock);

commit 237f83dfbe668443b5e31c3c7576125871cca674
Merge: 8f6ccf6159ae 1ff2f0fa450e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 11 10:55:49 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Some highlights from this development cycle:
    
       1) Big refactoring of ipv6 route and neigh handling to support
          nexthop objects configurable as units from userspace. From David
          Ahern.
    
       2) Convert explored_states in BPF verifier into a hash table,
          significantly decreased state held for programs with bpf2bpf
          calls, from Alexei Starovoitov.
    
       3) Implement bpf_send_signal() helper, from Yonghong Song.
    
       4) Various classifier enhancements to mvpp2 driver, from Maxime
          Chevallier.
    
       5) Add aRFS support to hns3 driver, from Jian Shen.
    
       6) Fix use after free in inet frags by allocating fqdirs dynamically
          and reworking how rhashtable dismantle occurs, from Eric Dumazet.
    
       7) Add act_ctinfo packet classifier action, from Kevin
          Darbyshire-Bryant.
    
       8) Add TFO key backup infrastructure, from Jason Baron.
    
       9) Remove several old and unused ISDN drivers, from Arnd Bergmann.
    
      10) Add devlink notifications for flash update status to mlxsw driver,
          from Jiri Pirko.
    
      11) Lots of kTLS offload infrastructure fixes, from Jakub Kicinski.
    
      12) Add support for mv88e6250 DSA chips, from Rasmus Villemoes.
    
      13) Various enhancements to ipv6 flow label handling, from Eric
          Dumazet and Willem de Bruijn.
    
      14) Support TLS offload in nfp driver, from Jakub Kicinski, Dirk van
          der Merwe, and others.
    
      15) Various improvements to axienet driver including converting it to
          phylink, from Robert Hancock.
    
      16) Add PTP support to sja1105 DSA driver, from Vladimir Oltean.
    
      17) Add mqprio qdisc offload support to dpaa2-eth, from Ioana
          Radulescu.
    
      18) Add devlink health reporting to mlx5, from Moshe Shemesh.
    
      19) Convert stmmac over to phylink, from Jose Abreu.
    
      20) Add PTP PHC (Physical Hardware Clock) support to mlxsw, from
          Shalom Toledo.
    
      21) Add nftables SYNPROXY support, from Fernando Fernandez Mancera.
    
      22) Convert tcp_fastopen over to use SipHash, from Ard Biesheuvel.
    
      23) Track spill/fill of constants in BPF verifier, from Alexei
          Starovoitov.
    
      24) Support bounded loops in BPF, from Alexei Starovoitov.
    
      25) Various page_pool API fixes and improvements, from Jesper Dangaard
          Brouer.
    
      26) Just like ipv4, support ref-countless ipv6 route handling. From
          Wei Wang.
    
      27) Support VLAN offloading in aquantia driver, from Igor Russkikh.
    
      28) Add AF_XDP zero-copy support to mlx5, from Maxim Mikityanskiy.
    
      29) Add flower GRE encap/decap support to nfp driver, from Pieter
          Jansen van Vuuren.
    
      30) Protect against stack overflow when using act_mirred, from John
          Hurley.
    
      31) Allow devmap map lookups from eBPF, from Toke Høiland-Jørgensen.
    
      32) Use page_pool API in netsec driver, Ilias Apalodimas.
    
      33) Add Google gve network driver, from Catherine Sullivan.
    
      34) More indirect call avoidance, from Paolo Abeni.
    
      35) Add kTLS TX HW offload support to mlx5, from Tariq Toukan.
    
      36) Add XDP_REDIRECT support to bnxt_en, from Andy Gospodarek.
    
      37) Add MPLS manipulation actions to TC, from John Hurley.
    
      38) Add sending a packet to connection tracking from TC actions, and
          then allow flower classifier matching on conntrack state. From
          Paul Blakey.
    
      39) Netfilter hw offload support, from Pablo Neira Ayuso"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (2080 commits)
      net/mlx5e: Return in default case statement in tx_post_resync_params
      mlx5: Return -EINVAL when WARN_ON_ONCE triggers in mlx5e_tls_resync().
      net: dsa: add support for BRIDGE_MROUTER attribute
      pkt_sched: Include const.h
      net: netsec: remove static declaration for netsec_set_tx_de()
      net: netsec: remove superfluous if statement
      netfilter: nf_tables: add hardware offload support
      net: flow_offload: rename tc_cls_flower_offload to flow_cls_offload
      net: flow_offload: add flow_block_cb_is_busy() and use it
      net: sched: remove tcf block API
      drivers: net: use flow block API
      net: sched: use flow block API
      net: flow_offload: add flow_block_cb_{priv, incref, decref}()
      net: flow_offload: add list handling functions
      net: flow_offload: add flow_block_cb_alloc() and flow_block_cb_free()
      net: flow_offload: rename TCF_BLOCK_BINDER_TYPE_* to FLOW_BLOCK_BINDER_TYPE_*
      net: flow_offload: rename TC_BLOCK_{UN}BIND to FLOW_BLOCK_{UN}BIND
      net: flow_offload: add flow_block_cb_setup_simple()
      net: hisilicon: Add an tx_desc to adapt HI13X1_GMAC
      net: hisilicon: Add an rx_desc to adapt HI13X1_GMAC
      ...

commit 341924049558e5f7c1a148a2c461a417933d35d9
Merge: 516337048fa4 4a8f81b8c053
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 3 10:50:21 2019 +0200

    Merge branch 'timers/vdso' into timers/core
    
    so the hyper-v clocksource update can be applied.

commit 0d01da6afc5402f60325c5da31b22f7d56689b49
Author: Stanislav Fomichev <sdf@google.com>
Date:   Thu Jun 27 13:38:47 2019 -0700

    bpf: implement getsockopt and setsockopt hooks
    
    Implement new BPF_PROG_TYPE_CGROUP_SOCKOPT program type and
    BPF_CGROUP_{G,S}ETSOCKOPT cgroup hooks.
    
    BPF_CGROUP_SETSOCKOPT can modify user setsockopt arguments before
    passing them down to the kernel or bypass kernel completely.
    BPF_CGROUP_GETSOCKOPT can can inspect/modify getsockopt arguments that
    kernel returns.
    Both hooks reuse existing PTR_TO_PACKET{,_END} infrastructure.
    
    The buffer memory is pre-allocated (because I don't think there is
    a precedent for working with __user memory from bpf). This might be
    slow to do for each {s,g}etsockopt call, that's why I've added
    __cgroup_bpf_prog_array_is_empty that exits early if there is nothing
    attached to a cgroup. Note, however, that there is a race between
    __cgroup_bpf_prog_array_is_empty and BPF_PROG_RUN_ARRAY where cgroup
    program layout might have changed; this should not be a problem
    because in general there is a race between multiple calls to
    {s,g}etsocktop and user adding/removing bpf progs from a cgroup.
    
    The return code of the BPF program is handled as follows:
    * 0: EPERM
    * 1: success, continue with next BPF program in the cgroup chain
    
    v9:
    * allow overwriting setsockopt arguments (Alexei Starovoitov):
      * use set_fs (same as kernel_setsockopt)
      * buffer is always kzalloc'd (no small on-stack buffer)
    
    v8:
    * use s32 for optlen (Andrii Nakryiko)
    
    v7:
    * return only 0 or 1 (Alexei Starovoitov)
    * always run all progs (Alexei Starovoitov)
    * use optval=0 as kernel bypass in setsockopt (Alexei Starovoitov)
      (decided to use optval=-1 instead, optval=0 might be a valid input)
    * call getsockopt hook after kernel handlers (Alexei Starovoitov)
    
    v6:
    * rework cgroup chaining; stop as soon as bpf program returns
      0 or 2; see patch with the documentation for the details
    * drop Andrii's and Martin's Acked-by (not sure they are comfortable
      with the new state of things)
    
    v5:
    * skip copy_to_user() and put_user() when ret == 0 (Martin Lau)
    
    v4:
    * don't export bpf_sk_fullsock helper (Martin Lau)
    * size != sizeof(__u64) for uapi pointers (Martin Lau)
    * offsetof instead of bpf_ctx_range when checking ctx access (Martin Lau)
    
    v3:
    * typos in BPF_PROG_CGROUP_SOCKOPT_RUN_ARRAY comments (Andrii Nakryiko)
    * reverse christmas tree in BPF_PROG_CGROUP_SOCKOPT_RUN_ARRAY (Andrii
      Nakryiko)
    * use __bpf_md_ptr instead of __u32 for optval{,_end} (Martin Lau)
    * use BPF_FIELD_SIZEOF() for consistency (Martin Lau)
    * new CG_SOCKOPT_ACCESS macro to wrap repeated parts
    
    v2:
    * moved bpf_sockopt_kern fields around to remove a hole (Martin Lau)
    * aligned bpf_sockopt_kern->buf to 8 bytes (Martin Lau)
    * bpf_prog_array_is_empty instead of bpf_prog_array_length (Martin Lau)
    * added [0,2] return code check to verifier (Martin Lau)
    * dropped unused buf[64] from the stack (Martin Lau)
    * use PTR_TO_SOCKET for bpf_sockopt->sk (Martin Lau)
    * dropped bpf_target_off from ctx rewrites (Martin Lau)
    * use return code for kernel bypass (Martin Lau & Andrii Nakryiko)
    
    Cc: Andrii Nakryiko <andriin@fb.com>
    Cc: Martin Lau <kafai@fb.com>
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 7713cf39795a..b0f545e07425 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1590,6 +1590,14 @@ bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 		default:
 			return -EINVAL;
 		}
+	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
+		switch (expected_attach_type) {
+		case BPF_CGROUP_SETSOCKOPT:
+		case BPF_CGROUP_GETSOCKOPT:
+			return 0;
+		default:
+			return -EINVAL;
+		}
 	default:
 		return 0;
 	}
@@ -1840,6 +1848,7 @@ static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
 	switch (prog->type) {
 	case BPF_PROG_TYPE_CGROUP_SOCK:
 	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
+	case BPF_PROG_TYPE_CGROUP_SOCKOPT:
 		return attach_type == prog->expected_attach_type ? 0 : -EINVAL;
 	case BPF_PROG_TYPE_CGROUP_SKB:
 		return prog->enforce_expected_attach_type &&
@@ -1912,6 +1921,10 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_SYSCTL:
 		ptype = BPF_PROG_TYPE_CGROUP_SYSCTL;
 		break;
+	case BPF_CGROUP_GETSOCKOPT:
+	case BPF_CGROUP_SETSOCKOPT:
+		ptype = BPF_PROG_TYPE_CGROUP_SOCKOPT;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -1995,6 +2008,10 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_CGROUP_SYSCTL:
 		ptype = BPF_PROG_TYPE_CGROUP_SYSCTL;
 		break;
+	case BPF_CGROUP_GETSOCKOPT:
+	case BPF_CGROUP_SETSOCKOPT:
+		ptype = BPF_PROG_TYPE_CGROUP_SOCKOPT;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -2031,6 +2048,8 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_SOCK_OPS:
 	case BPF_CGROUP_DEVICE:
 	case BPF_CGROUP_SYSCTL:
+	case BPF_CGROUP_GETSOCKOPT:
+	case BPF_CGROUP_SETSOCKOPT:
 		break;
 	case BPF_LIRC_MODE2:
 		return lirc_prog_query(attr, uattr);

commit 9285ec4c8b61d4930a575081abeba2cd4f449a74
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Fri Jun 21 22:32:48 2019 +0200

    timekeeping: Use proper clock specifier names in functions
    
    This makes boot uniformly boottime and tai uniformly clocktai, to
    address the remaining oversights.
    
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Link: https://lkml.kernel.org/r/20190621203249.3909-2-Jason@zx2c4.com

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ef63d26622f2..96c8928b468b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1666,7 +1666,7 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	if (err < 0)
 		goto free_prog;
 
-	prog->aux->load_time = ktime_get_boot_ns();
+	prog->aux->load_time = ktime_get_boottime_ns();
 	err = bpf_obj_name_cpy(prog->aux->name, attr->prog_name);
 	if (err)
 		goto free_prog;

commit 13091aa30535b719e269f20a7bc34002bf5afae5
Merge: f97252a8c33f 29f785ff76b6
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jun 17 19:48:13 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Honestly all the conflicts were simple overlapping changes,
    nothing really interesting to report.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit da0f382029868806e88c046eb2560fdee7a9457c
Merge: eb7c825bf747 4fddbf8a99ee
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 17 15:55:34 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Pull networking fixes from David Miller:
     "Lots of bug fixes here:
    
       1) Out of bounds access in __bpf_skc_lookup, from Lorenz Bauer.
    
       2) Fix rate reporting in cfg80211_calculate_bitrate_he(), from John
          Crispin.
    
       3) Use after free in psock backlog workqueue, from John Fastabend.
    
       4) Fix source port matching in fdb peer flow rule of mlx5, from Raed
          Salem.
    
       5) Use atomic_inc_not_zero() in fl6_sock_lookup(), from Eric Dumazet.
    
       6) Network header needs to be set for packet redirect in nfp, from
          John Hurley.
    
       7) Fix udp zerocopy refcnt, from Willem de Bruijn.
    
       8) Don't assume linear buffers in vxlan and geneve error handlers,
          from Stefano Brivio.
    
       9) Fix TOS matching in mlxsw, from Jiri Pirko.
    
      10) More SCTP cookie memory leak fixes, from Neil Horman.
    
      11) Fix VLAN filtering in rtl8366, from Linus Walluij.
    
      12) Various TCP SACK payload size and fragmentation memory limit fixes
          from Eric Dumazet.
    
      13) Use after free in pneigh_get_next(), also from Eric Dumazet.
    
      14) LAPB control block leak fix from Jeremy Sowden"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net: (145 commits)
      lapb: fixed leak of control-blocks.
      tipc: purge deferredq list for each grp member in tipc_group_delete
      ax25: fix inconsistent lock state in ax25_destroy_timer
      neigh: fix use-after-free read in pneigh_get_next
      tcp: fix compile error if !CONFIG_SYSCTL
      hv_sock: Suppress bogus "may be used uninitialized" warnings
      be2net: Fix number of Rx queues used for flow hashing
      net: handle 802.1P vlan 0 packets properly
      tcp: enforce tcp_min_snd_mss in tcp_mtu_probing()
      tcp: add tcp_min_snd_mss sysctl
      tcp: tcp_fragment() should apply sane memory limits
      tcp: limit payload size of sacked skbs
      Revert "net: phylink: set the autoneg state in phylink_phy_change"
      bpf: fix nested bpf tracepoints with per-cpu data
      bpf: Fix out of bounds memory access in bpf_sk_storage
      vsock/virtio: set SOCK_DONE on peer shutdown
      net: dsa: rtl8366: Fix up VLAN filtering
      net: phylink: set the autoneg state in phylink_phy_change
      net: add high_order_alloc_disable sysctl/static key
      tcp: add tcp_tx_skb_cache sysctl
      ...

commit 983695fa676568fc0fe5ddd995c7267aabc24632
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jun 7 01:48:57 2019 +0200

    bpf: fix unconnected udp hooks
    
    Intention of cgroup bind/connect/sendmsg BPF hooks is to act transparently
    to applications as also stated in original motivation in 7828f20e3779 ("Merge
    branch 'bpf-cgroup-bind-connect'"). When recently integrating the latter
    two hooks into Cilium to enable host based load-balancing with Kubernetes,
    I ran into the issue that pods couldn't start up as DNS got broken. Kubernetes
    typically sets up DNS as a service and is thus subject to load-balancing.
    
    Upon further debugging, it turns out that the cgroupv2 sendmsg BPF hooks API
    is currently insufficient and thus not usable as-is for standard applications
    shipped with most distros. To break down the issue we ran into with a simple
    example:
    
      # cat /etc/resolv.conf
      nameserver 147.75.207.207
      nameserver 147.75.207.208
    
    For the purpose of a simple test, we set up above IPs as service IPs and
    transparently redirect traffic to a different DNS backend server for that
    node:
    
      # cilium service list
      ID   Frontend            Backend
      1    147.75.207.207:53   1 => 8.8.8.8:53
      2    147.75.207.208:53   1 => 8.8.8.8:53
    
    The attached BPF program is basically selecting one of the backends if the
    service IP/port matches on the cgroup hook. DNS breaks here, because the
    hooks are not transparent enough to applications which have built-in msg_name
    address checks:
    
      # nslookup 1.1.1.1
      ;; reply from unexpected source: 8.8.8.8#53, expected 147.75.207.207#53
      ;; reply from unexpected source: 8.8.8.8#53, expected 147.75.207.208#53
      ;; reply from unexpected source: 8.8.8.8#53, expected 147.75.207.207#53
      [...]
      ;; connection timed out; no servers could be reached
    
      # dig 1.1.1.1
      ;; reply from unexpected source: 8.8.8.8#53, expected 147.75.207.207#53
      ;; reply from unexpected source: 8.8.8.8#53, expected 147.75.207.208#53
      ;; reply from unexpected source: 8.8.8.8#53, expected 147.75.207.207#53
      [...]
    
      ; <<>> DiG 9.11.3-1ubuntu1.7-Ubuntu <<>> 1.1.1.1
      ;; global options: +cmd
      ;; connection timed out; no servers could be reached
    
    For comparison, if none of the service IPs is used, and we tell nslookup
    to use 8.8.8.8 directly it works just fine, of course:
    
      # nslookup 1.1.1.1 8.8.8.8
      1.1.1.1.in-addr.arpa  name = one.one.one.one.
    
    In order to fix this and thus act more transparent to the application,
    this needs reverse translation on recvmsg() side. A minimal fix for this
    API is to add similar recvmsg() hooks behind the BPF cgroups static key
    such that the program can track state and replace the current sockaddr_in{,6}
    with the original service IP. From BPF side, this basically tracks the
    service tuple plus socket cookie in an LRU map where the reverse NAT can
    then be retrieved via map value as one example. Side-note: the BPF cgroups
    static key should be converted to a per-hook static key in future.
    
    Same example after this fix:
    
      # cilium service list
      ID   Frontend            Backend
      1    147.75.207.207:53   1 => 8.8.8.8:53
      2    147.75.207.208:53   1 => 8.8.8.8:53
    
    Lookups work fine now:
    
      # nslookup 1.1.1.1
      1.1.1.1.in-addr.arpa    name = one.one.one.one.
    
      Authoritative answers can be found from:
    
      # dig 1.1.1.1
    
      ; <<>> DiG 9.11.3-1ubuntu1.7-Ubuntu <<>> 1.1.1.1
      ;; global options: +cmd
      ;; Got answer:
      ;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 51550
      ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1
    
      ;; OPT PSEUDOSECTION:
      ; EDNS: version: 0, flags:; udp: 512
      ;; QUESTION SECTION:
      ;1.1.1.1.                       IN      A
    
      ;; AUTHORITY SECTION:
      .                       23426   IN      SOA     a.root-servers.net. nstld.verisign-grs.com. 2019052001 1800 900 604800 86400
    
      ;; Query time: 17 msec
      ;; SERVER: 147.75.207.207#53(147.75.207.207)
      ;; WHEN: Tue May 21 12:59:38 UTC 2019
      ;; MSG SIZE  rcvd: 111
    
    And from an actual packet level it shows that we're using the back end
    server when talking via 147.75.207.20{7,8} front end:
    
      # tcpdump -i any udp
      [...]
      12:59:52.698732 IP foo.42011 > google-public-dns-a.google.com.domain: 18803+ PTR? 1.1.1.1.in-addr.arpa. (38)
      12:59:52.698735 IP foo.42011 > google-public-dns-a.google.com.domain: 18803+ PTR? 1.1.1.1.in-addr.arpa. (38)
      12:59:52.701208 IP google-public-dns-a.google.com.domain > foo.42011: 18803 1/0/0 PTR one.one.one.one. (67)
      12:59:52.701208 IP google-public-dns-a.google.com.domain > foo.42011: 18803 1/0/0 PTR one.one.one.one. (67)
      [...]
    
    In order to be flexible and to have same semantics as in sendmsg BPF
    programs, we only allow return codes in [1,1] range. In the sendmsg case
    the program is called if msg->msg_name is present which can be the case
    in both, connected and unconnected UDP.
    
    The former only relies on the sockaddr_in{,6} passed via connect(2) if
    passed msg->msg_name was NULL. Therefore, on recvmsg side, we act in similar
    way to call into the BPF program whenever a non-NULL msg->msg_name was
    passed independent of sk->sk_state being TCP_ESTABLISHED or not. Note
    that for TCP case, the msg->msg_name is ignored in the regular recvmsg
    path and therefore not relevant.
    
    For the case of ip{,v6}_recv_error() paths, picked up via MSG_ERRQUEUE,
    the hook is not called. This is intentional as it aligns with the same
    semantics as in case of TCP cgroup BPF hooks right now. This might be
    better addressed in future through a different bpf_attach_type such
    that this case can be distinguished from the regular recvmsg paths,
    for example.
    
    Fixes: 1cedee13d25a ("bpf: Hooks for sys_sendmsg")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrey Ignatov <rdna@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Martynas Pumputis <m@lambda.lt>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index cb5440b02e82..e8ba3a153691 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1581,6 +1581,8 @@ bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 		case BPF_CGROUP_INET6_CONNECT:
 		case BPF_CGROUP_UDP4_SENDMSG:
 		case BPF_CGROUP_UDP6_SENDMSG:
+		case BPF_CGROUP_UDP4_RECVMSG:
+		case BPF_CGROUP_UDP6_RECVMSG:
 			return 0;
 		default:
 			return -EINVAL;
@@ -1875,6 +1877,8 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_INET6_CONNECT:
 	case BPF_CGROUP_UDP4_SENDMSG:
 	case BPF_CGROUP_UDP6_SENDMSG:
+	case BPF_CGROUP_UDP4_RECVMSG:
+	case BPF_CGROUP_UDP6_RECVMSG:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
 		break;
 	case BPF_CGROUP_SOCK_OPS:
@@ -1960,6 +1964,8 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_CGROUP_INET6_CONNECT:
 	case BPF_CGROUP_UDP4_SENDMSG:
 	case BPF_CGROUP_UDP6_SENDMSG:
+	case BPF_CGROUP_UDP4_RECVMSG:
+	case BPF_CGROUP_UDP6_RECVMSG:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
 		break;
 	case BPF_CGROUP_SOCK_OPS:
@@ -2011,6 +2017,8 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_INET6_CONNECT:
 	case BPF_CGROUP_UDP4_SENDMSG:
 	case BPF_CGROUP_UDP6_SENDMSG:
+	case BPF_CGROUP_UDP4_RECVMSG:
+	case BPF_CGROUP_UDP6_RECVMSG:
 	case BPF_CGROUP_SOCK_OPS:
 	case BPF_CGROUP_DEVICE:
 	case BPF_CGROUP_SYSCTL:

commit 5b497af42fab12cadc0e29bcb7052cf9963603f5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 29 07:18:09 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 295
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of version 2 of the gnu general public license as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 64 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190529141901.894819585@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index cb5440b02e82..ef63d26622f2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1,13 +1,5 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of version 2 of the GNU General Public
- * License as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
- * General Public License for more details.
  */
 #include <linux/bpf.h>
 #include <linux/bpf_trace.h>

commit c85d69135a9175c50a823d04d62d932312d037b3
Author: Roman Gushchin <guro@fb.com>
Date:   Wed May 29 18:03:59 2019 -0700

    bpf: move memory size checks to bpf_map_charge_init()
    
    Most bpf map types doing similar checks and bytes to pages
    conversion during memory allocation and charging.
    
    Let's unify these checks by moving them into bpf_map_charge_init().
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4a5ebad99154..4c53cbd3329d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -205,11 +205,16 @@ static void bpf_uncharge_memlock(struct user_struct *user, u32 pages)
 		atomic_long_sub(pages, &user->locked_vm);
 }
 
-int bpf_map_charge_init(struct bpf_map_memory *mem, u32 pages)
+int bpf_map_charge_init(struct bpf_map_memory *mem, size_t size)
 {
-	struct user_struct *user = get_current_user();
+	u32 pages = round_up(size, PAGE_SIZE) >> PAGE_SHIFT;
+	struct user_struct *user;
 	int ret;
 
+	if (size >= U32_MAX - PAGE_SIZE)
+		return -E2BIG;
+
+	user = get_current_user();
 	ret = bpf_charge_memlock(user, pages);
 	if (ret) {
 		free_uid(user);

commit b936ca643ade11f265fa10e5fb71c20d9c5243f1
Author: Roman Gushchin <guro@fb.com>
Date:   Wed May 29 18:03:58 2019 -0700

    bpf: rework memlock-based memory accounting for maps
    
    In order to unify the existing memlock charging code with the
    memcg-based memory accounting, which will be added later, let's
    rework the current scheme.
    
    Currently the following design is used:
      1) .alloc() callback optionally checks if the allocation will likely
         succeed using bpf_map_precharge_memlock()
      2) .alloc() performs actual allocations
      3) .alloc() callback calculates map cost and sets map.memory.pages
      4) map_create() calls bpf_map_init_memlock() which sets map.memory.user
         and performs actual charging; in case of failure the map is
         destroyed
      <map is in use>
      1) bpf_map_free_deferred() calls bpf_map_release_memlock(), which
         performs uncharge and releases the user
      2) .map_free() callback releases the memory
    
    The scheme can be simplified and made more robust:
      1) .alloc() calculates map cost and calls bpf_map_charge_init()
      2) bpf_map_charge_init() sets map.memory.user and performs actual
        charge
      3) .alloc() performs actual allocations
      <map is in use>
      1) .map_free() callback releases the memory
      2) bpf_map_charge_finish() performs uncharge and releases the user
    
    The new scheme also allows to reuse bpf_map_charge_init()/finish()
    functions for memcg-based accounting. Because charges are performed
    before actual allocations and uncharges after freeing the memory,
    no bogus memory pressure can be created.
    
    In cases when the map structure is not available (e.g. it's not
    created yet, or is already destroyed), on-stack bpf_map_memory
    structure is used. The charge can be transferred with the
    bpf_map_charge_move() function.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 8289a2ce14fc..4a5ebad99154 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -188,19 +188,6 @@ void bpf_map_init_from_attr(struct bpf_map *map, union bpf_attr *attr)
 	map->numa_node = bpf_map_attr_numa_node(attr);
 }
 
-int bpf_map_precharge_memlock(u32 pages)
-{
-	struct user_struct *user = get_current_user();
-	unsigned long memlock_limit, cur;
-
-	memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
-	cur = atomic_long_read(&user->locked_vm);
-	free_uid(user);
-	if (cur + pages > memlock_limit)
-		return -EPERM;
-	return 0;
-}
-
 static int bpf_charge_memlock(struct user_struct *user, u32 pages)
 {
 	unsigned long memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
@@ -214,29 +201,40 @@ static int bpf_charge_memlock(struct user_struct *user, u32 pages)
 
 static void bpf_uncharge_memlock(struct user_struct *user, u32 pages)
 {
-	atomic_long_sub(pages, &user->locked_vm);
+	if (user)
+		atomic_long_sub(pages, &user->locked_vm);
 }
 
-static int bpf_map_init_memlock(struct bpf_map *map)
+int bpf_map_charge_init(struct bpf_map_memory *mem, u32 pages)
 {
 	struct user_struct *user = get_current_user();
 	int ret;
 
-	ret = bpf_charge_memlock(user, map->memory.pages);
+	ret = bpf_charge_memlock(user, pages);
 	if (ret) {
 		free_uid(user);
 		return ret;
 	}
-	map->memory.user = user;
-	return ret;
+
+	mem->pages = pages;
+	mem->user = user;
+
+	return 0;
 }
 
-static void bpf_map_release_memlock(struct bpf_map *map)
+void bpf_map_charge_finish(struct bpf_map_memory *mem)
 {
-	struct user_struct *user = map->memory.user;
+	bpf_uncharge_memlock(mem->user, mem->pages);
+	free_uid(mem->user);
+}
 
-	bpf_uncharge_memlock(user, map->memory.pages);
-	free_uid(user);
+void bpf_map_charge_move(struct bpf_map_memory *dst,
+			 struct bpf_map_memory *src)
+{
+	*dst = *src;
+
+	/* Make sure src will not be used for the redundant uncharging. */
+	memset(src, 0, sizeof(struct bpf_map_memory));
 }
 
 int bpf_map_charge_memlock(struct bpf_map *map, u32 pages)
@@ -304,11 +302,13 @@ void bpf_map_free_id(struct bpf_map *map, bool do_idr_lock)
 static void bpf_map_free_deferred(struct work_struct *work)
 {
 	struct bpf_map *map = container_of(work, struct bpf_map, work);
+	struct bpf_map_memory mem;
 
-	bpf_map_release_memlock(map);
+	bpf_map_charge_move(&mem, &map->memory);
 	security_bpf_map_free(map);
 	/* implementation dependent freeing */
 	map->ops->map_free(map);
+	bpf_map_charge_finish(&mem);
 }
 
 static void bpf_map_put_uref(struct bpf_map *map)
@@ -550,6 +550,7 @@ static int map_check_btf(struct bpf_map *map, const struct btf *btf,
 static int map_create(union bpf_attr *attr)
 {
 	int numa_node = bpf_map_attr_numa_node(attr);
+	struct bpf_map_memory mem;
 	struct bpf_map *map;
 	int f_flags;
 	int err;
@@ -574,7 +575,7 @@ static int map_create(union bpf_attr *attr)
 
 	err = bpf_obj_name_cpy(map->name, attr->map_name);
 	if (err)
-		goto free_map_nouncharge;
+		goto free_map;
 
 	atomic_set(&map->refcnt, 1);
 	atomic_set(&map->usercnt, 1);
@@ -584,20 +585,20 @@ static int map_create(union bpf_attr *attr)
 
 		if (!attr->btf_value_type_id) {
 			err = -EINVAL;
-			goto free_map_nouncharge;
+			goto free_map;
 		}
 
 		btf = btf_get_by_fd(attr->btf_fd);
 		if (IS_ERR(btf)) {
 			err = PTR_ERR(btf);
-			goto free_map_nouncharge;
+			goto free_map;
 		}
 
 		err = map_check_btf(map, btf, attr->btf_key_type_id,
 				    attr->btf_value_type_id);
 		if (err) {
 			btf_put(btf);
-			goto free_map_nouncharge;
+			goto free_map;
 		}
 
 		map->btf = btf;
@@ -609,15 +610,11 @@ static int map_create(union bpf_attr *attr)
 
 	err = security_bpf_map_alloc(map);
 	if (err)
-		goto free_map_nouncharge;
-
-	err = bpf_map_init_memlock(map);
-	if (err)
-		goto free_map_sec;
+		goto free_map;
 
 	err = bpf_map_alloc_id(map);
 	if (err)
-		goto free_map;
+		goto free_map_sec;
 
 	err = bpf_map_new_fd(map, f_flags);
 	if (err < 0) {
@@ -633,13 +630,13 @@ static int map_create(union bpf_attr *attr)
 
 	return err;
 
-free_map:
-	bpf_map_release_memlock(map);
 free_map_sec:
 	security_bpf_map_free(map);
-free_map_nouncharge:
+free_map:
 	btf_put(map->btf);
+	bpf_map_charge_move(&mem, &map->memory);
 	map->ops->map_free(map);
+	bpf_map_charge_finish(&mem);
 	return err;
 }
 

commit 3539b96e041c06e4317082816d90ec09160aeb11
Author: Roman Gushchin <guro@fb.com>
Date:   Wed May 29 18:03:57 2019 -0700

    bpf: group memory related fields in struct bpf_map_memory
    
    Group "user" and "pages" fields of bpf_map into the bpf_map_memory
    structure. Later it can be extended with "memcg" and other related
    information.
    
    The main reason for a such change (beside cosmetics) is to pass
    bpf_map_memory structure to charging functions before the actual
    allocation of bpf_map.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 1539774d78c7..8289a2ce14fc 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -222,19 +222,20 @@ static int bpf_map_init_memlock(struct bpf_map *map)
 	struct user_struct *user = get_current_user();
 	int ret;
 
-	ret = bpf_charge_memlock(user, map->pages);
+	ret = bpf_charge_memlock(user, map->memory.pages);
 	if (ret) {
 		free_uid(user);
 		return ret;
 	}
-	map->user = user;
+	map->memory.user = user;
 	return ret;
 }
 
 static void bpf_map_release_memlock(struct bpf_map *map)
 {
-	struct user_struct *user = map->user;
-	bpf_uncharge_memlock(user, map->pages);
+	struct user_struct *user = map->memory.user;
+
+	bpf_uncharge_memlock(user, map->memory.pages);
 	free_uid(user);
 }
 
@@ -242,17 +243,17 @@ int bpf_map_charge_memlock(struct bpf_map *map, u32 pages)
 {
 	int ret;
 
-	ret = bpf_charge_memlock(map->user, pages);
+	ret = bpf_charge_memlock(map->memory.user, pages);
 	if (ret)
 		return ret;
-	map->pages += pages;
+	map->memory.pages += pages;
 	return ret;
 }
 
 void bpf_map_uncharge_memlock(struct bpf_map *map, u32 pages)
 {
-	bpf_uncharge_memlock(map->user, pages);
-	map->pages -= pages;
+	bpf_uncharge_memlock(map->memory.user, pages);
+	map->memory.pages -= pages;
 }
 
 static int bpf_map_alloc_id(struct bpf_map *map)
@@ -395,7 +396,7 @@ static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 		   map->value_size,
 		   map->max_entries,
 		   map->map_flags,
-		   map->pages * 1ULL << PAGE_SHIFT,
+		   map->memory.pages * 1ULL << PAGE_SHIFT,
 		   map->id,
 		   READ_ONCE(map->frozen));
 

commit 5cf1e91456301f8c4f6bbc63ff76cff12f92f31b
Author: brakmo <brakmo@fb.com>
Date:   Tue May 28 16:59:36 2019 -0700

    bpf: cgroup inet skb programs can return 0 to 3
    
    Allows cgroup inet skb programs to return values in the range [0, 3].
    The second bit is used to deterine if congestion occurred and higher
    level protocol should decrease rate. E.g. TCP would call tcp_enter_cwr()
    
    The bpf_prog must set expected_attach_type to BPF_CGROUP_INET_EGRESS
    at load time if it uses the new return values (i.e. 2 or 3).
    
    The expected_attach_type is currently not enforced for
    BPF_PROG_TYPE_CGROUP_SKB.  e.g Meaning the current bpf_prog with
    expected_attach_type setting to BPF_CGROUP_INET_EGRESS can attach to
    BPF_CGROUP_INET_INGRESS.  Blindly enforcing expected_attach_type will
    break backward compatibility.
    
    This patch adds a enforce_expected_attach_type bit to only
    enforce the expected_attach_type when it uses the new
    return value.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 3d546b6f4646..1539774d78c7 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1585,6 +1585,14 @@ bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 		default:
 			return -EINVAL;
 		}
+	case BPF_PROG_TYPE_CGROUP_SKB:
+		switch (expected_attach_type) {
+		case BPF_CGROUP_INET_INGRESS:
+		case BPF_CGROUP_INET_EGRESS:
+			return 0;
+		default:
+			return -EINVAL;
+		}
 	default:
 		return 0;
 	}
@@ -1836,6 +1844,10 @@ static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
 	case BPF_PROG_TYPE_CGROUP_SOCK:
 	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
 		return attach_type == prog->expected_attach_type ? 0 : -EINVAL;
+	case BPF_PROG_TYPE_CGROUP_SKB:
+		return prog->enforce_expected_attach_type &&
+			prog->expected_attach_type != attach_type ?
+			-EINVAL : 0;
 	default:
 		return 0;
 	}

commit c240eff63a1cf1c4edc768e0cfc374811c02f069
Author: Jiong Wang <jiong.wang@netronome.com>
Date:   Fri May 24 23:25:16 2019 +0100

    bpf: introduce new bpf prog load flags "BPF_F_TEST_RND_HI32"
    
    x86_64 and AArch64 perhaps are two arches that running bpf testsuite
    frequently, however the zero extension insertion pass is not enabled for
    them because of their hardware support.
    
    It is critical to guarantee the pass correction as it is supposed to be
    enabled at default for a couple of other arches, for example PowerPC,
    SPARC, arm, NFP etc. Therefore, it would be very useful if there is a way
    to test this pass on for example x86_64.
    
    The test methodology employed by this set is "poisoning" useless bits. High
    32-bit of a definition is randomized if it is identified as not used by any
    later insn. Such randomization is only enabled under testing mode which is
    gated by the new bpf prog load flags "BPF_F_TEST_RND_HI32".
    
    Suggested-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index cb5440b02e82..3d546b6f4646 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1604,7 +1604,9 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	if (CHECK_ATTR(BPF_PROG_LOAD))
 		return -EINVAL;
 
-	if (attr->prog_flags & ~(BPF_F_STRICT_ALIGNMENT | BPF_F_ANY_ALIGNMENT))
+	if (attr->prog_flags & ~(BPF_F_STRICT_ALIGNMENT |
+				 BPF_F_ANY_ALIGNMENT |
+				 BPF_F_TEST_RND_HI32))
 		return -EINVAL;
 
 	if (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) &&

commit c6110222c6f49ea68169f353565eb865488a8619
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue May 14 01:18:55 2019 +0200

    bpf: add map_lookup_elem_sys_only for lookups from syscall side
    
    Add a callback map_lookup_elem_sys_only() that map implementations
    could use over map_lookup_elem() from system call side in case the
    map implementation needs to handle the latter differently than from
    the BPF data path. If map_lookup_elem_sys_only() is set, this will
    be preferred pick for map lookups out of user space. This hook is
    used in a follow-up fix for LRU map, but once development window
    opens, we can convert other map types from map_lookup_elem() (here,
    the one called upon BPF_MAP_LOOKUP_ELEM cmd is meant) over to use
    the callback to simplify and clean up the latter.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ad3ccf82f31d..cb5440b02e82 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -808,7 +808,10 @@ static int map_lookup_elem(union bpf_attr *attr)
 		err = map->ops->map_peek_elem(map, value);
 	} else {
 		rcu_read_lock();
-		ptr = map->ops->map_lookup_elem(map, key);
+		if (map->ops->map_lookup_elem_sys_only)
+			ptr = map->ops->map_lookup_elem_sys_only(map, key);
+		else
+			ptr = map->ops->map_lookup_elem(map, key);
 		if (IS_ERR(ptr)) {
 			err = PTR_ERR(ptr);
 		} else if (!ptr) {

commit 6ac99e8f23d4b10258406ca0dd7bffca5f31da9d
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Apr 26 16:39:39 2019 -0700

    bpf: Introduce bpf sk local storage
    
    After allowing a bpf prog to
    - directly read the skb->sk ptr
    - get the fullsock bpf_sock by "bpf_sk_fullsock()"
    - get the bpf_tcp_sock by "bpf_tcp_sock()"
    - get the listener sock by "bpf_get_listener_sock()"
    - avoid duplicating the fields of "(bpf_)sock" and "(bpf_)tcp_sock"
      into different bpf running context.
    
    this patch is another effort to make bpf's network programming
    more intuitive to do (together with memory and performance benefit).
    
    When bpf prog needs to store data for a sk, the current practice is to
    define a map with the usual 4-tuples (src/dst ip/port) as the key.
    If multiple bpf progs require to store different sk data, multiple maps
    have to be defined.  Hence, wasting memory to store the duplicated
    keys (i.e. 4 tuples here) in each of the bpf map.
    [ The smallest key could be the sk pointer itself which requires
      some enhancement in the verifier and it is a separate topic. ]
    
    Also, the bpf prog needs to clean up the elem when sk is freed.
    Otherwise, the bpf map will become full and un-usable quickly.
    The sk-free tracking currently could be done during sk state
    transition (e.g. BPF_SOCK_OPS_STATE_CB).
    
    The size of the map needs to be predefined which then usually ended-up
    with an over-provisioned map in production.  Even the map was re-sizable,
    while the sk naturally come and go away already, this potential re-size
    operation is arguably redundant if the data can be directly connected
    to the sk itself instead of proxy-ing through a bpf map.
    
    This patch introduces sk->sk_bpf_storage to provide local storage space
    at sk for bpf prog to use.  The space will be allocated when the first bpf
    prog has created data for this particular sk.
    
    The design optimizes the bpf prog's lookup (and then optionally followed by
    an inline update).  bpf_spin_lock should be used if the inline update needs
    to be protected.
    
    BPF_MAP_TYPE_SK_STORAGE:
    -----------------------
    To define a bpf "sk-local-storage", a BPF_MAP_TYPE_SK_STORAGE map (new in
    this patch) needs to be created.  Multiple BPF_MAP_TYPE_SK_STORAGE maps can
    be created to fit different bpf progs' needs.  The map enforces
    BTF to allow printing the sk-local-storage during a system-wise
    sk dump (e.g. "ss -ta") in the future.
    
    The purpose of a BPF_MAP_TYPE_SK_STORAGE map is not for lookup/update/delete
    a "sk-local-storage" data from a particular sk.
    Think of the map as a meta-data (or "type") of a "sk-local-storage".  This
    particular "type" of "sk-local-storage" data can then be stored in any sk.
    
    The main purposes of this map are mostly:
    1. Define the size of a "sk-local-storage" type.
    2. Provide a similar syscall userspace API as the map (e.g. lookup/update,
       map-id, map-btf...etc.)
    3. Keep track of all sk's storages of this "type" and clean them up
       when the map is freed.
    
    sk->sk_bpf_storage:
    ------------------
    The main lookup/update/delete is done on sk->sk_bpf_storage (which
    is a "struct bpf_sk_storage").  When doing a lookup,
    the "map" pointer is now used as the "key" to search on the
    sk_storage->list.  The "map" pointer is actually serving
    as the "type" of the "sk-local-storage" that is being
    requested.
    
    To allow very fast lookup, it should be as fast as looking up an
    array at a stable-offset.  At the same time, it is not ideal to
    set a hard limit on the number of sk-local-storage "type" that the
    system can have.  Hence, this patch takes a cache approach.
    The last search result from sk_storage->list is cached in
    sk_storage->cache[] which is a stable sized array.  Each
    "sk-local-storage" type has a stable offset to the cache[] array.
    In the future, a map's flag could be introduced to do cache
    opt-out/enforcement if it became necessary.
    
    The cache size is 16 (i.e. 16 types of "sk-local-storage").
    Programs can share map.  On the program side, having a few bpf_progs
    running in the networking hotpath is already a lot.  The bpf_prog
    should have already consolidated the existing sock-key-ed map usage
    to minimize the map lookup penalty.  16 has enough runway to grow.
    
    All sk-local-storage data will be removed from sk->sk_bpf_storage
    during sk destruction.
    
    bpf_sk_storage_get() and bpf_sk_storage_delete():
    ------------------------------------------------
    Instead of using bpf_map_(lookup|update|delete)_elem(),
    the bpf prog needs to use the new helper bpf_sk_storage_get() and
    bpf_sk_storage_delete().  The verifier can then enforce the
    ARG_PTR_TO_SOCKET argument.  The bpf_sk_storage_get() also allows to
    "create" new elem if one does not exist in the sk.  It is done by
    the new BPF_SK_STORAGE_GET_F_CREATE flag.  An optional value can also be
    provided as the initial value during BPF_SK_STORAGE_GET_F_CREATE.
    The BPF_MAP_TYPE_SK_STORAGE also supports bpf_spin_lock.  Together,
    it has eliminated the potential use cases for an equivalent
    bpf_map_update_elem() API (for bpf_prog) in this patch.
    
    Misc notes:
    ----------
    1. map_get_next_key is not supported.  From the userspace syscall
       perspective,  the map has the socket fd as the key while the map
       can be shared by pinned-file or map-id.
    
       Since btf is enforced, the existing "ss" could be enhanced to pretty
       print the local-storage.
    
       Supporting a kernel defined btf with 4 tuples as the return key could
       be explored later also.
    
    2. The sk->sk_lock cannot be acquired.  Atomic operations is used instead.
       e.g. cmpxchg is done on the sk->sk_bpf_storage ptr.
       Please refer to the source code comments for the details in
       synchronization cases and considerations.
    
    3. The mem is charged to the sk->sk_omem_alloc as the sk filter does.
    
    Benchmark:
    ---------
    Here is the benchmark data collected by turning on
    the "kernel.bpf_stats_enabled" sysctl.
    Two bpf progs are tested:
    
    One bpf prog with the usual bpf hashmap (max_entries = 8192) with the
    sk ptr as the key. (verifier is modified to support sk ptr as the key
    That should have shortened the key lookup time.)
    
    Another bpf prog is with the new BPF_MAP_TYPE_SK_STORAGE.
    
    Both are storing a "u32 cnt", do a lookup on "egress_skb/cgroup" for
    each egress skb and then bump the cnt.  netperf is used to drive
    data with 4096 connected UDP sockets.
    
    BPF_MAP_TYPE_HASH with a modifier verifier (152ns per bpf run)
    27: cgroup_skb  name egress_sk_map  tag 74f56e832918070b run_time_ns 58280107540 run_cnt 381347633
        loaded_at 2019-04-15T13:46:39-0700  uid 0
        xlated 344B  jited 258B  memlock 4096B  map_ids 16
        btf_id 5
    
    BPF_MAP_TYPE_SK_STORAGE in this patch (66ns per bpf run)
    30: cgroup_skb  name egress_sk_stora  tag d4aa70984cc7bbf6 run_time_ns 25617093319 run_cnt 390989739
        loaded_at 2019-04-15T13:47:54-0700  uid 0
        xlated 168B  jited 156B  memlock 4096B  map_ids 17
        btf_id 6
    
    Here is a high-level picture on how are the objects organized:
    
           sk
        ┌──────┐
        │      │
        │      │
        │      │
        │*sk_bpf_storage─────▶ bpf_sk_storage
        └──────┘                 ┌───────┐
                     ┌───────────┤ list  │
                     │           │       │
                     │           │       │
                     │           │       │
                     │           └───────┘
                     │
                     │     elem
                     │  ┌────────┐
                     ├─▶│ snode  │
                     │  ├────────┤
                     │  │  data  │          bpf_map
                     │  ├────────┤        ┌─────────┐
                     │  │map_node│◀─┬─────┤  list   │
                     │  └────────┘  │     │         │
                     │              │     │         │
                     │     elem     │     │         │
                     │  ┌────────┐  │     └─────────┘
                     └─▶│ snode  │  │
                        ├────────┤  │
       bpf_map          │  data  │  │
     ┌─────────┐        ├────────┤  │
     │  list   ├───────▶│map_node│  │
     │         │        └────────┘  │
     │         │                    │
     │         │           elem     │
     └─────────┘        ┌────────┐  │
                     ┌─▶│ snode  │  │
                     │  ├────────┤  │
                     │  │  data  │  │
                     │  ├────────┤  │
                     │  │map_node│◀─┘
                     │  └────────┘
                     │
                     │
                     │          ┌───────┐
         sk          └──────────│ list  │
      ┌──────┐                  │       │
      │      │                  │       │
      │      │                  │       │
      │      │                  └───────┘
      │*sk_bpf_storage───────▶bpf_sk_storage
      └──────┘
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ae141e745f92..ad3ccf82f31d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -526,7 +526,8 @@ static int map_check_btf(struct bpf_map *map, const struct btf *btf,
 			return -EACCES;
 		if (map->map_type != BPF_MAP_TYPE_HASH &&
 		    map->map_type != BPF_MAP_TYPE_ARRAY &&
-		    map->map_type != BPF_MAP_TYPE_CGROUP_STORAGE)
+		    map->map_type != BPF_MAP_TYPE_CGROUP_STORAGE &&
+		    map->map_type != BPF_MAP_TYPE_SK_STORAGE)
 			return -ENOTSUPP;
 		if (map->spin_lock_off + sizeof(struct bpf_spin_lock) >
 		    map->value_size) {

commit 9df1c28bb75217b244257152ab7d788bb2a386d0
Author: Matt Mullins <mmullins@fb.com>
Date:   Fri Apr 26 11:49:47 2019 -0700

    bpf: add writable context for raw tracepoints
    
    This is an opt-in interface that allows a tracepoint to provide a safe
    buffer that can be written from a BPF_PROG_TYPE_RAW_TRACEPOINT program.
    The size of the buffer must be a compile-time constant, and is checked
    before allowing a BPF program to attach to a tracepoint that uses this
    feature.
    
    The pointer to this buffer will be the first argument of tracepoints
    that opt in; the pointer is valid and can be bpf_probe_read() by both
    BPF_PROG_TYPE_RAW_TRACEPOINT and BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE
    programs that attach to such a tracepoint, but the buffer to which it
    points may only be written by the latter.
    
    Signed-off-by: Matt Mullins <mmullins@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b0de49598341..ae141e745f92 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1789,12 +1789,16 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 	}
 	raw_tp->btp = btp;
 
-	prog = bpf_prog_get_type(attr->raw_tracepoint.prog_fd,
-				 BPF_PROG_TYPE_RAW_TRACEPOINT);
+	prog = bpf_prog_get(attr->raw_tracepoint.prog_fd);
 	if (IS_ERR(prog)) {
 		err = PTR_ERR(prog);
 		goto out_free_tp;
 	}
+	if (prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT &&
+	    prog->type != BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE) {
+		err = -EINVAL;
+		goto out_put_prog;
+	}
 
 	err = bpf_probe_register(raw_tp->btp, prog);
 	if (err)

commit 118c8e9ae629d35fa9b3d27a7b9d59298b1b4183
Author: Stanislav Fomichev <sdf@google.com>
Date:   Thu Apr 25 14:37:23 2019 -0700

    bpf: support BPF_PROG_QUERY for BPF_FLOW_DISSECTOR attach_type
    
    target_fd is target namespace. If there is a flow dissector BPF program
    attached to that namespace, its (single) id is returned.
    
    v5:
    * drop net ref right after rcu unlock (Daniel Borkmann)
    
    v4:
    * add missing put_net (Jann Horn)
    
    v3:
    * add missing inline to skb_flow_dissector_prog_query static def
      (kbuild test robot <lkp@intel.com>)
    
    v2:
    * don't sleep in rcu critical section (Jakub Kicinski)
    * check input prog_cnt (exit early)
    
    Cc: Jann Horn <jannh@google.com>
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 92c9b8a32b50..b0de49598341 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2009,6 +2009,8 @@ static int bpf_prog_query(const union bpf_attr *attr,
 		break;
 	case BPF_LIRC_MODE2:
 		return lirc_prog_query(attr, uattr);
+	case BPF_FLOW_DISSECTOR:
+		return skb_flow_dissector_prog_query(attr, uattr);
 	default:
 		return -EINVAL;
 	}

commit 7b146cebe30cb481b0f70d85779da938da818637
Author: Andrey Ignatov <rdna@fb.com>
Date:   Wed Feb 27 12:59:24 2019 -0800

    bpf: Sysctl hook
    
    Containerized applications may run as root and it may create problems
    for whole host. Specifically such applications may change a sysctl and
    affect applications in other containers.
    
    Furthermore in existing infrastructure it may not be possible to just
    completely disable writing to sysctl, instead such a process should be
    gradual with ability to log what sysctl are being changed by a
    container, investigate, limit the set of writable sysctl to currently
    used ones (so that new ones can not be changed) and eventually reduce
    this set to zero.
    
    The patch introduces new program type BPF_PROG_TYPE_CGROUP_SYSCTL and
    attach type BPF_CGROUP_SYSCTL to solve these problems on cgroup basis.
    
    New program type has access to following minimal context:
            struct bpf_sysctl {
                    __u32   write;
            };
    
    Where @write indicates whether sysctl is being read (= 0) or written (=
    1).
    
    Helpers to access sysctl name and value will be introduced separately.
    
    BPF_CGROUP_SYSCTL attach point is added to sysctl code right before
    passing control to ctl_table->proc_handler so that BPF program can
    either allow or deny access to sysctl.
    
    Suggested-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index d995eedfdd16..92c9b8a32b50 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1888,6 +1888,9 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_FLOW_DISSECTOR:
 		ptype = BPF_PROG_TYPE_FLOW_DISSECTOR;
 		break;
+	case BPF_CGROUP_SYSCTL:
+		ptype = BPF_PROG_TYPE_CGROUP_SYSCTL;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -1966,6 +1969,9 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 		return lirc_prog_detach(attr);
 	case BPF_FLOW_DISSECTOR:
 		return skb_flow_dissector_bpf_prog_detach(attr);
+	case BPF_CGROUP_SYSCTL:
+		ptype = BPF_PROG_TYPE_CGROUP_SYSCTL;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -1999,6 +2005,7 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_UDP6_SENDMSG:
 	case BPF_CGROUP_SOCK_OPS:
 	case BPF_CGROUP_DEVICE:
+	case BPF_CGROUP_SYSCTL:
 		break;
 	case BPF_LIRC_MODE2:
 		return lirc_prog_query(attr, uattr);

commit b0b9395d865e3060d97658fbc9ba3f77fecc8da1
Author: Stanislav Fomichev <sdf@google.com>
Date:   Tue Apr 9 11:49:09 2019 -0700

    bpf: support input __sk_buff context in BPF_PROG_TEST_RUN
    
    Add new set of arguments to bpf_attr for BPF_PROG_TEST_RUN:
    * ctx_in/ctx_size_in - input context
    * ctx_out/ctx_size_out - output context
    
    The intended use case is to pass some meta data to the test runs that
    operate on skb (this has being brought up on recent LPC).
    
    For programs that use bpf_prog_test_run_skb, support __sk_buff input and
    output. Initially, from input __sk_buff, copy _only_ cb and priority into
    skb, all other non-zero fields are prohibited (with EINVAL).
    If the user has set ctx_out/ctx_size_out, copy the potentially modified
    __sk_buff back to the userspace.
    
    We require all fields of input __sk_buff except the ones we explicitly
    support to be set to zero. The expectation is that in the future we might
    add support for more fields and we want to fail explicitly if the user
    runs the program on the kernel where we don't yet support them.
    
    The API is intentionally vague (i.e. we don't explicitly add __sk_buff
    to bpf_attr, but ctx_in) to potentially let other test_run types use
    this interface in the future (this can be xdp_md for xdp types for
    example).
    
    v4:
      * don't copy more than allowed in bpf_ctx_init [Martin]
    
    v3:
      * handle case where ctx_in is NULL, but ctx_out is not [Martin]
      * convert size==0 checks to ptr==NULL checks and add some extra ptr
        checks [Martin]
    
    v2:
      * Addressed comments from Martin Lau
    
    Signed-off-by: Stanislav Fomichev <sdf@google.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 438199e2eca4..d995eedfdd16 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2009,7 +2009,7 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	return cgroup_bpf_prog_query(attr, uattr);
 }
 
-#define BPF_PROG_TEST_RUN_LAST_FIELD test.duration
+#define BPF_PROG_TEST_RUN_LAST_FIELD test.ctx_out
 
 static int bpf_prog_test_run(const union bpf_attr *attr,
 			     union bpf_attr __user *uattr)
@@ -2022,6 +2022,14 @@ static int bpf_prog_test_run(const union bpf_attr *attr,
 	if (CHECK_ATTR(BPF_PROG_TEST_RUN))
 		return -EINVAL;
 
+	if ((attr->test.ctx_size_in && !attr->test.ctx_in) ||
+	    (!attr->test.ctx_size_in && attr->test.ctx_in))
+		return -EINVAL;
+
+	if ((attr->test.ctx_size_out && !attr->test.ctx_out) ||
+	    (!attr->test.ctx_size_out && attr->test.ctx_out))
+		return -EINVAL;
+
 	prog = bpf_prog_get(attr->test.prog_fd);
 	if (IS_ERR(prog))
 		return PTR_ERR(prog);

commit 2824ecb7010f6a20e9a4140512b798469ab066cc
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Apr 9 23:20:10 2019 +0200

    bpf: allow for key-less BTF in array map
    
    Given we'll be reusing BPF array maps for global data/bss/rodata
    sections, we need a way to associate BTF DataSec type as its map
    value type. In usual cases we have this ugly BPF_ANNOTATE_KV_PAIR()
    macro hack e.g. via 38d5d3b3d5db ("bpf: Introduce BPF_ANNOTATE_KV_PAIR")
    to get initial map to type association going. While more use cases
    for it are discouraged, this also won't work for global data since
    the use of array map is a BPF loader detail and therefore unknown
    at compilation time. For array maps with just a single entry we make
    an exception in terms of BTF in that key type is declared optional
    if value type is of DataSec type. The latter LLVM is guaranteed to
    emit and it also aligns with how we regard global data maps as just
    a plain buffer area reusing existing map facilities for allowing
    things like introspection with existing tools.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 198c9680bf0d..438199e2eca4 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -504,9 +504,16 @@ static int map_check_btf(struct bpf_map *map, const struct btf *btf,
 	u32 key_size, value_size;
 	int ret = 0;
 
-	key_type = btf_type_id_size(btf, &btf_key_id, &key_size);
-	if (!key_type || key_size != map->key_size)
-		return -EINVAL;
+	/* Some maps allow key to be unspecified. */
+	if (btf_key_id) {
+		key_type = btf_type_id_size(btf, &btf_key_id, &key_size);
+		if (!key_type || key_size != map->key_size)
+			return -EINVAL;
+	} else {
+		key_type = btf_type_by_id(btf, 0);
+		if (!map->ops->map_check_btf)
+			return -EINVAL;
+	}
 
 	value_type = btf_type_id_size(btf, &btf_value_id, &value_size);
 	if (!value_type || value_size != map->value_size)
@@ -573,7 +580,7 @@ static int map_create(union bpf_attr *attr)
 	if (attr->btf_key_type_id || attr->btf_value_type_id) {
 		struct btf *btf;
 
-		if (!attr->btf_key_type_id || !attr->btf_value_type_id) {
+		if (!attr->btf_value_type_id) {
 			err = -EINVAL;
 			goto free_map_nouncharge;
 		}

commit 3e0ddc4f3ff1436970e96e76f3df3c3b5f5173b6
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Apr 9 23:20:07 2019 +0200

    bpf: allow . char as part of the object name
    
    Trivial addition to allow '.' aside from '_' as "special" characters
    in the object name. Used to allow for substrings in maps from loader
    side such as ".bss", ".data", ".rodata", but could also be useful for
    other purposes.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b3ce516e5a20..198c9680bf0d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -474,10 +474,10 @@ static int bpf_obj_name_cpy(char *dst, const char *src)
 	const char *end = src + BPF_OBJ_NAME_LEN;
 
 	memset(dst, 0, BPF_OBJ_NAME_LEN);
-
-	/* Copy all isalnum() and '_' char */
+	/* Copy all isalnum(), '_' and '.' chars. */
 	while (src < end && *src) {
-		if (!isalnum(*src) && *src != '_')
+		if (!isalnum(*src) &&
+		    *src != '_' && *src != '.')
 			return -EINVAL;
 		*dst++ = *src++;
 	}

commit 87df15de441bd4add7876ef584da8cabdd9a042a
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Apr 9 23:20:06 2019 +0200

    bpf: add syscall side map freeze support
    
    This patch adds a new BPF_MAP_FREEZE command which allows to
    "freeze" the map globally as read-only / immutable from syscall
    side.
    
    Map permission handling has been refactored into map_get_sys_perms()
    and drops FMODE_CAN_WRITE in case of locked map. Main use case is
    to allow for setting up .rodata sections from the BPF ELF which
    are loaded into the kernel, meaning BPF loader first allocates
    map, sets up map value by copying .rodata section into it and once
    complete, it calls BPF_MAP_FREEZE on the map fd to prevent further
    modifications.
    
    Right now BPF_MAP_FREEZE only takes map fd as argument while remaining
    bpf_attr members are required to be zero. I didn't add write-only
    locking here as counterpart since I don't have a concrete use-case
    for it on my side, and I think it makes probably more sense to wait
    once there is actually one. In that case bpf_attr can be extended
    as usual with a flag field and/or others where flag 0 means that
    we lock the map read-only hence this doesn't prevent to add further
    extensions to BPF_MAP_FREEZE upon need.
    
    A map creation flag like BPF_F_WRONCE was not considered for couple
    of reasons: i) in case of a generic implementation, a map can consist
    of more than just one element, thus there could be multiple map
    updates needed to set the map into a state where it can then be
    made immutable, ii) WRONCE indicates exact one-time write before
    it is then set immutable. A generic implementation would set a bit
    atomically on map update entry (if unset), indicating that every
    subsequent update from then onwards will need to bail out there.
    However, map updates can fail, so upon failure that flag would need
    to be unset again and the update attempt would need to be repeated
    for it to be eventually made immutable. While this can be made
    race-free, this approach feels less clean and in combination with
    reason i), it's not generic enough. A dedicated BPF_MAP_FREEZE
    command directly sets the flag and caller has the guarantee that
    map is immutable from syscall side upon successful return for any
    future syscall invocations that would alter the map state, which
    is also more intuitive from an API point of view. A command name
    such as BPF_MAP_LOCK has been avoided as it's too close with BPF
    map spin locks (which already has BPF_F_LOCK flag). BPF_MAP_FREEZE
    is so far only enabled for privileged users.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0c9276b54c88..b3ce516e5a20 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -355,6 +355,18 @@ static int bpf_map_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+static fmode_t map_get_sys_perms(struct bpf_map *map, struct fd f)
+{
+	fmode_t mode = f.file->f_mode;
+
+	/* Our file permissions may have been overridden by global
+	 * map permissions facing syscall side.
+	 */
+	if (READ_ONCE(map->frozen))
+		mode &= ~FMODE_CAN_WRITE;
+	return mode;
+}
+
 #ifdef CONFIG_PROC_FS
 static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 {
@@ -376,14 +388,16 @@ static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 		   "max_entries:\t%u\n"
 		   "map_flags:\t%#x\n"
 		   "memlock:\t%llu\n"
-		   "map_id:\t%u\n",
+		   "map_id:\t%u\n"
+		   "frozen:\t%u\n",
 		   map->map_type,
 		   map->key_size,
 		   map->value_size,
 		   map->max_entries,
 		   map->map_flags,
 		   map->pages * 1ULL << PAGE_SHIFT,
-		   map->id);
+		   map->id,
+		   READ_ONCE(map->frozen));
 
 	if (owner_prog_type) {
 		seq_printf(m, "owner_prog_type:\t%u\n",
@@ -727,8 +741,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
-
-	if (!(f.file->f_mode & FMODE_CAN_READ)) {
+	if (!(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {
 		err = -EPERM;
 		goto err_put;
 	}
@@ -857,8 +870,7 @@ static int map_update_elem(union bpf_attr *attr)
 	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
-
-	if (!(f.file->f_mode & FMODE_CAN_WRITE)) {
+	if (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {
 		err = -EPERM;
 		goto err_put;
 	}
@@ -969,8 +981,7 @@ static int map_delete_elem(union bpf_attr *attr)
 	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
-
-	if (!(f.file->f_mode & FMODE_CAN_WRITE)) {
+	if (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {
 		err = -EPERM;
 		goto err_put;
 	}
@@ -1021,8 +1032,7 @@ static int map_get_next_key(union bpf_attr *attr)
 	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
-
-	if (!(f.file->f_mode & FMODE_CAN_READ)) {
+	if (!(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {
 		err = -EPERM;
 		goto err_put;
 	}
@@ -1089,8 +1099,7 @@ static int map_lookup_and_delete_elem(union bpf_attr *attr)
 	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
-
-	if (!(f.file->f_mode & FMODE_CAN_WRITE)) {
+	if (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {
 		err = -EPERM;
 		goto err_put;
 	}
@@ -1132,6 +1141,36 @@ static int map_lookup_and_delete_elem(union bpf_attr *attr)
 	return err;
 }
 
+#define BPF_MAP_FREEZE_LAST_FIELD map_fd
+
+static int map_freeze(const union bpf_attr *attr)
+{
+	int err = 0, ufd = attr->map_fd;
+	struct bpf_map *map;
+	struct fd f;
+
+	if (CHECK_ATTR(BPF_MAP_FREEZE))
+		return -EINVAL;
+
+	f = fdget(ufd);
+	map = __bpf_map_get(f);
+	if (IS_ERR(map))
+		return PTR_ERR(map);
+	if (READ_ONCE(map->frozen)) {
+		err = -EBUSY;
+		goto err_put;
+	}
+	if (!capable(CAP_SYS_ADMIN)) {
+		err = -EPERM;
+		goto err_put;
+	}
+
+	WRITE_ONCE(map->frozen, true);
+err_put:
+	fdput(f);
+	return err;
+}
+
 static const struct bpf_prog_ops * const bpf_prog_types[] = {
 #define BPF_PROG_TYPE(_id, _name) \
 	[_id] = & _name ## _prog_ops,
@@ -2735,6 +2774,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_MAP_GET_NEXT_KEY:
 		err = map_get_next_key(&attr);
 		break;
+	case BPF_MAP_FREEZE:
+		err = map_freeze(&attr);
+		break;
 	case BPF_PROG_LOAD:
 		err = bpf_prog_load(&attr, uattr);
 		break;

commit 591fe9888d7809d9ee5c828020b6c6ae27c37229
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Apr 9 23:20:05 2019 +0200

    bpf: add program side {rd, wr}only support for maps
    
    This work adds two new map creation flags BPF_F_RDONLY_PROG
    and BPF_F_WRONLY_PROG in order to allow for read-only or
    write-only BPF maps from a BPF program side.
    
    Today we have BPF_F_RDONLY and BPF_F_WRONLY, but this only
    applies to system call side, meaning the BPF program has full
    read/write access to the map as usual while bpf(2) calls with
    map fd can either only read or write into the map depending
    on the flags. BPF_F_RDONLY_PROG and BPF_F_WRONLY_PROG allows
    for the exact opposite such that verifier is going to reject
    program loads if write into a read-only map or a read into a
    write-only map is detected. For read-only map case also some
    helpers are forbidden for programs that would alter the map
    state such as map deletion, update, etc. As opposed to the two
    BPF_F_RDONLY / BPF_F_WRONLY flags, BPF_F_RDONLY_PROG as well
    as BPF_F_WRONLY_PROG really do correspond to the map lifetime.
    
    We've enabled this generic map extension to various non-special
    maps holding normal user data: array, hash, lru, lpm, local
    storage, queue and stack. Further generic map types could be
    followed up in future depending on use-case. Main use case
    here is to forbid writes into .rodata map values from verifier
    side.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 56b4b0e08b3b..0c9276b54c88 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -501,6 +501,8 @@ static int map_check_btf(struct bpf_map *map, const struct btf *btf,
 	map->spin_lock_off = btf_find_spin_lock(btf, value_type);
 
 	if (map_value_has_spin_lock(map)) {
+		if (map->map_flags & BPF_F_RDONLY_PROG)
+			return -EACCES;
 		if (map->map_type != BPF_MAP_TYPE_HASH &&
 		    map->map_type != BPF_MAP_TYPE_ARRAY &&
 		    map->map_type != BPF_MAP_TYPE_CGROUP_STORAGE)

commit be70bcd53de66e86f2726e576307cbdaebd3b1a5
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Apr 9 23:20:04 2019 +0200

    bpf: do not retain flags that are not tied to map lifetime
    
    Both BPF_F_WRONLY / BPF_F_RDONLY flags are tied to the map file
    descriptor, but not to the map object itself! Meaning, at map
    creation time BPF_F_RDONLY can be set to make the map read-only
    from syscall side, but this holds only for the returned fd, so
    any other fd either retrieved via bpf file system or via map id
    for the very same underlying map object can have read-write access
    instead.
    
    Given that, keeping the two flags around in the map_flags attribute
    and exposing them to user space upon map dump is misleading and
    may lead to false conclusions. Since these two flags are not
    tied to the map object lets also not store them as map property.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 828518bb947b..56b4b0e08b3b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -166,13 +166,25 @@ void bpf_map_area_free(void *area)
 	kvfree(area);
 }
 
+static u32 bpf_map_flags_retain_permanent(u32 flags)
+{
+	/* Some map creation flags are not tied to the map object but
+	 * rather to the map fd instead, so they have no meaning upon
+	 * map object inspection since multiple file descriptors with
+	 * different (access) properties can exist here. Thus, given
+	 * this has zero meaning for the map itself, lets clear these
+	 * from here.
+	 */
+	return flags & ~(BPF_F_RDONLY | BPF_F_WRONLY);
+}
+
 void bpf_map_init_from_attr(struct bpf_map *map, union bpf_attr *attr)
 {
 	map->map_type = attr->map_type;
 	map->key_size = attr->key_size;
 	map->value_size = attr->value_size;
 	map->max_entries = attr->max_entries;
-	map->map_flags = attr->map_flags;
+	map->map_flags = bpf_map_flags_retain_permanent(attr->map_flags);
 	map->numa_node = bpf_map_attr_numa_node(attr);
 }
 

commit d8eca5bbb2be9bc7546f9e733786fa2f1a594c67
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Apr 9 23:20:03 2019 +0200

    bpf: implement lookup-free direct value access for maps
    
    This generic extension to BPF maps allows for directly loading
    an address residing inside a BPF map value as a single BPF
    ldimm64 instruction!
    
    The idea is similar to what BPF_PSEUDO_MAP_FD does today, which
    is a special src_reg flag for ldimm64 instruction that indicates
    that inside the first part of the double insns's imm field is a
    file descriptor which the verifier then replaces as a full 64bit
    address of the map into both imm parts. For the newly added
    BPF_PSEUDO_MAP_VALUE src_reg flag, the idea is the following:
    the first part of the double insns's imm field is again a file
    descriptor corresponding to the map, and the second part of the
    imm field is an offset into the value. The verifier will then
    replace both imm parts with an address that points into the BPF
    map value at the given value offset for maps that support this
    operation. Currently supported is array map with single entry.
    It is possible to support more than just single map element by
    reusing both 16bit off fields of the insns as a map index, so
    full array map lookup could be expressed that way. It hasn't
    been implemented here due to lack of concrete use case, but
    could easily be done so in future in a compatible way, since
    both off fields right now have to be 0 and would correctly
    denote a map index 0.
    
    The BPF_PSEUDO_MAP_VALUE is a distinct flag as otherwise with
    BPF_PSEUDO_MAP_FD we could not differ offset 0 between load of
    map pointer versus load of map's value at offset 0, and changing
    BPF_PSEUDO_MAP_FD's encoding into off by one to differ between
    regular map pointer and map value pointer would add unnecessary
    complexity and increases barrier for debugability thus less
    suitable. Using the second part of the imm field as an offset
    into the value does /not/ come with limitations since maximum
    possible value size is in u32 universe anyway.
    
    This optimization allows for efficiently retrieving an address
    to a map value memory area without having to issue a helper call
    which needs to prepare registers according to calling convention,
    etc, without needing the extra NULL test, and without having to
    add the offset in an additional instruction to the value base
    pointer. The verifier then treats the destination register as
    PTR_TO_MAP_VALUE with constant reg->off from the user passed
    offset from the second imm field, and guarantees that this is
    within bounds of the map value. Any subsequent operations are
    normally treated as typical map value handling without anything
    extra needed from verification side.
    
    The two map operations for direct value access have been added to
    array map for now. In future other types could be supported as
    well depending on the use case. The main use case for this commit
    is to allow for BPF loader support for global variables that
    reside in .data/.rodata/.bss sections such that we can directly
    load the address of them with minimal additional infrastructure
    required. Loader support has been added in subsequent commits for
    libbpf library.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 1d65e56594db..828518bb947b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2072,13 +2072,26 @@ static int bpf_map_get_fd_by_id(const union bpf_attr *attr)
 }
 
 static const struct bpf_map *bpf_map_from_imm(const struct bpf_prog *prog,
-					      unsigned long addr)
+					      unsigned long addr, u32 *off,
+					      u32 *type)
 {
+	const struct bpf_map *map;
 	int i;
 
-	for (i = 0; i < prog->aux->used_map_cnt; i++)
-		if (prog->aux->used_maps[i] == (void *)addr)
-			return prog->aux->used_maps[i];
+	for (i = 0, *off = 0; i < prog->aux->used_map_cnt; i++) {
+		map = prog->aux->used_maps[i];
+		if (map == (void *)addr) {
+			*type = BPF_PSEUDO_MAP_FD;
+			return map;
+		}
+		if (!map->ops->map_direct_value_meta)
+			continue;
+		if (!map->ops->map_direct_value_meta(map, addr, off)) {
+			*type = BPF_PSEUDO_MAP_VALUE;
+			return map;
+		}
+	}
+
 	return NULL;
 }
 
@@ -2086,6 +2099,7 @@ static struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog)
 {
 	const struct bpf_map *map;
 	struct bpf_insn *insns;
+	u32 off, type;
 	u64 imm;
 	int i;
 
@@ -2113,11 +2127,11 @@ static struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog)
 			continue;
 
 		imm = ((u64)insns[i + 1].imm << 32) | (u32)insns[i].imm;
-		map = bpf_map_from_imm(prog, imm);
+		map = bpf_map_from_imm(prog, imm, &off, &type);
 		if (map) {
-			insns[i].src_reg = BPF_PSEUDO_MAP_FD;
+			insns[i].src_reg = type;
 			insns[i].imm = map->id;
-			insns[i + 1].imm = 0;
+			insns[i + 1].imm = off;
 			continue;
 		}
 	}

commit c04c0d2b968ac45d6ef020316808ef6c82325a82
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Apr 1 21:27:45 2019 -0700

    bpf: increase complexity limit and maximum program size
    
    Large verifier speed improvements allow to increase
    verifier complexity limit.
    Now regardless of the program composition and its size it takes
    little time for the verifier to hit insn_processed limit.
    On typical x86 machine non-debug kernel processes 1M instructions
    in 1/10 of a second.
    (before these speed improvements specially crafted programs
    could be hitting multi-second verification times)
    Full kasan kernel with debug takes ~1 second for the same 1M insns.
    Hence bump the BPF_COMPLEXITY_LIMIT_INSNS limit to 1M.
    Also increase the number of instructions per program
    from 4k to internal BPF_COMPLEXITY_LIMIT_INSNS limit.
    4k limit was confusing to users, since small programs with hundreds
    of insns could be hitting BPF_COMPLEXITY_LIMIT_INSNS limit.
    Sometimes adding more insns and bpf_trace_printk debug statements
    would make the verifier accept the program while removing
    code would make the verifier reject it.
    Some user space application started to add #define MAX_FOO to
    their programs and do:
      MAX_FOO=100;
    again:
      compile with MAX_FOO;
      try to load;
      if (fails_to_load) { reduce MAX_FOO; goto again; }
    to be able to fit maximum amount of processing into single program.
    Other users artificially split their single program into a set of programs
    and use all 32 iterations of tail_calls to increase compute limits.
    And the most advanced folks used unlimited tc-bpf filter list
    to execute many bpf programs.
    Essentially the users managed to workaround 4k insn limit.
    This patch removes the limit for root programs from uapi.
    BPF_COMPLEXITY_LIMIT_INSNS is the kernel internal limit
    and success to load the program no longer depends on program size,
    but on 'smartness' of the verifier only.
    The verifier will continue to get smarter with every kernel release.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index afca36f53c49..1d65e56594db 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1557,7 +1557,8 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	/* eBPF programs must be GPL compatible to use GPL-ed functions */
 	is_gpl = license_is_gpl_compatible(license);
 
-	if (attr->insn_cnt == 0 || attr->insn_cnt > BPF_MAXINSNS)
+	if (attr->insn_cnt == 0 ||
+	    attr->insn_cnt > (capable(CAP_SYS_ADMIN) ? BPF_COMPLEXITY_LIMIT_INSNS : BPF_MAXINSNS))
 		return -E2BIG;
 	if (type != BPF_PROG_TYPE_SOCKET_FILTER &&
 	    type != BPF_PROG_TYPE_CGROUP_SKB &&

commit f01a7dbe98ae4265023fa5d3af0f076f0b18a647
Author: Martynas Pumputis <m@lambda.lt>
Date:   Mon Mar 18 16:10:26 2019 +0100

    bpf: Try harder when allocating memory for large maps
    
    It has been observed that sometimes a higher order memory allocation
    for BPF maps fails when there is no obvious memory pressure in a system.
    E.g. the map (BPF_MAP_TYPE_LRU_HASH, key=38, value=56, max_elems=524288)
    could not be created due to vmalloc unable to allocate 75497472B,
    when the system's memory consumption (in MB) was the following:
    
        Total: 3942 Used: 837 (21.24%) Free: 138 Buffers: 239 Cached: 2727
    
    Later analysis [1] by Michal Hocko showed that the vmalloc was not trying
    to reclaim memory from the page cache and was failing prematurely due to
    __GFP_NORETRY.
    
    Considering dcda9b0471 ("mm, tree wide: replace __GFP_REPEAT by
    __GFP_RETRY_MAYFAIL with more useful semantic") and [1], we can replace
    __GFP_NORETRY with __GFP_RETRY_MAYFAIL, as it won't invoke OOM killer
    and will try harder to fulfil allocation requests.
    
    Unfortunately, replacing the body of the BPF map memory allocation
    function with the kvmalloc_node helper function is not an option at
    this point in time, given 1) kmalloc is non-optional for higher order
    allocations, and 2) passing __GFP_RETRY_MAYFAIL to the kmalloc would
    stress the slab allocator too much for large requests.
    
    The change has been tested with the workloads mentioned above and by
    observing oom_kill value from /proc/vmstat.
    
    [1]: https://lore.kernel.org/bpf/20190310071318.GW5232@dhcp22.suse.cz/
    
    Signed-off-by: Martynas Pumputis <m@lambda.lt>
    Acked-by: Yonghong Song <yhs@fb.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20190318153940.GL8924@dhcp22.suse.cz/

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 62f6bced3a3c..afca36f53c49 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -136,21 +136,29 @@ static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 
 void *bpf_map_area_alloc(size_t size, int numa_node)
 {
-	/* We definitely need __GFP_NORETRY, so OOM killer doesn't
-	 * trigger under memory pressure as we really just want to
-	 * fail instead.
+	/* We really just want to fail instead of triggering OOM killer
+	 * under memory pressure, therefore we set __GFP_NORETRY to kmalloc,
+	 * which is used for lower order allocation requests.
+	 *
+	 * It has been observed that higher order allocation requests done by
+	 * vmalloc with __GFP_NORETRY being set might fail due to not trying
+	 * to reclaim memory from the page cache, thus we set
+	 * __GFP_RETRY_MAYFAIL to avoid such situations.
 	 */
-	const gfp_t flags = __GFP_NOWARN | __GFP_NORETRY | __GFP_ZERO;
+
+	const gfp_t flags = __GFP_NOWARN | __GFP_ZERO;
 	void *area;
 
 	if (size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER)) {
-		area = kmalloc_node(size, GFP_USER | flags, numa_node);
+		area = kmalloc_node(size, GFP_USER | __GFP_NORETRY | flags,
+				    numa_node);
 		if (area != NULL)
 			return area;
 	}
 
-	return __vmalloc_node_flags_caller(size, numa_node, GFP_KERNEL | flags,
-					   __builtin_return_address(0));
+	return __vmalloc_node_flags_caller(size, numa_node,
+					   GFP_KERNEL | __GFP_RETRY_MAYFAIL |
+					   flags, __builtin_return_address(0));
 }
 
 void bpf_map_area_free(void *area)

commit 203b6609e0ede49eb0b97008b1150c69e9d2ffd3
Merge: 3478588b5136 c978b9460fe1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 07:59:36 2019 -0800

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf updates from Ingo Molnar:
     "Lots of tooling updates - too many to list, here's a few highlights:
    
       - Various subcommand updates to 'perf trace', 'perf report', 'perf
         record', 'perf annotate', 'perf script', 'perf test', etc.
    
       - CPU and NUMA topology and affinity handling improvements,
    
       - HW tracing and HW support updates:
          - Intel PT updates
          - ARM CoreSight updates
          - vendor HW event updates
    
       - BPF updates
    
       - Tons of infrastructure updates, both on the build system and the
         library support side
    
       - Documentation updates.
    
       - ... and lots of other changes, see the changelog for details.
    
      Kernel side updates:
    
       - Tighten up kprobes blacklist handling, reduce the number of places
         where developers can install a kprobe and hang/crash the system.
    
       - Fix/enhance vma address filter handling.
    
       - Various PMU driver updates, small fixes and additions.
    
       - refcount_t conversions
    
       - BPF updates
    
       - error code propagation enhancements
    
       - misc other changes"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (238 commits)
      perf script python: Add Python3 support to syscall-counts-by-pid.py
      perf script python: Add Python3 support to syscall-counts.py
      perf script python: Add Python3 support to stat-cpi.py
      perf script python: Add Python3 support to stackcollapse.py
      perf script python: Add Python3 support to sctop.py
      perf script python: Add Python3 support to powerpc-hcalls.py
      perf script python: Add Python3 support to net_dropmonitor.py
      perf script python: Add Python3 support to mem-phys-addr.py
      perf script python: Add Python3 support to failed-syscalls-by-pid.py
      perf script python: Add Python3 support to netdev-times.py
      perf tools: Add perf_exe() helper to find perf binary
      perf script: Handle missing fields with -F +..
      perf data: Add perf_data__open_dir_data function
      perf data: Add perf_data__(create_dir|close_dir) functions
      perf data: Fail check_backup in case of error
      perf data: Make check_backup work over directories
      perf tools: Add rm_rf_perf_data function
      perf tools: Add pattern name checking to rm_rf
      perf tools: Add depth checking to rm_rf
      perf data: Add global path holder
      ...

commit f7fb7c1a1c8f86005d34f28278524213c521f761
Merge: 8c4238df4d0c 87dab7c3d54c
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Mar 4 10:14:31 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2019-03-04
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    The main changes are:
    
    1) Add AF_XDP support to libbpf. Rationale is to facilitate writing
       AF_XDP applications by offering higher-level APIs that hide many
       of the details of the AF_XDP uapi. Sample programs are converted
       over to this new interface as well, from Magnus.
    
    2) Introduce a new cant_sleep() macro for annotation of functions
       that cannot sleep and use it in BPF_PROG_RUN() to assert that
       BPF programs run under preemption disabled context, from Peter.
    
    3) Introduce per BPF prog stats in order to monitor the usage
       of BPF; this is controlled by kernel.bpf_stats_enabled sysctl
       knob where monitoring tools can make use of this to efficiently
       determine the average cost of programs, from Alexei.
    
    4) Split up BPF selftest's test_progs similarly as we already
       did with test_verifier. This allows to further reduce merge
       conflicts in future and to get more structure into our
       quickly growing BPF selftest suite, from Stanislav.
    
    5) Fix a bug in BTF's dedup algorithm which can cause an infinite
       loop in some circumstances; also various BPF doc fixes and
       improvements, from Andrii.
    
    6) Various BPF sample cleanups and migration to libbpf in order
       to further isolate the old sample loader code (so we can get
       rid of it at some point), from Jakub.
    
    7) Add a new BPF helper for BPF cgroup skb progs that allows
       to set ECN CE code point and a Host Bandwidth Manager (HBM)
       sample program for limiting the bandwidth used by v2 cgroups,
       from Lawrence.
    
    8) Enable write access to skb->queue_mapping from tc BPF egress
       programs in order to let BPF pick TX queue, from Jesper.
    
    9) Fix a bug in BPF spinlock handling for map-in-map which did
       not propagate spin_lock_off to the meta map, from Yonghong.
    
    10) Fix a bug in the new per-CPU BPF prog counters to properly
        initialize stats for each CPU, from Eric.
    
    11) Add various BPF helper prototypes to selftest's bpf_helpers.h,
        from Willem.
    
    12) Fix various BPF samples bugs in XDP and tracing progs,
        from Toke, Daniel and Yonghong.
    
    13) Silence preemption splat in test_bpf after BPF_PROG_RUN()
        enforces it now everywhere, from Anders.
    
    14) Fix a signedness bug in libbpf's btf_dedup_ref_type() to
        get error handling working, from Dan.
    
    15) Fix bpftool documentation and auto-completion with regards
        to stream_{verdict,parser} attach types, from Alban.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9eb359140cd307f8a14f61c19b155ffca5291057
Merge: cf29576fee60 07f12b26e21a
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Mar 2 12:54:35 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit 352d20d611414715353ee65fc206ee57ab1a6984
Author: Peng Sun <sironhide0null@gmail.com>
Date:   Wed Feb 27 22:36:25 2019 +0800

    bpf: drop refcount if bpf_map_new_fd() fails in map_create()
    
    In bpf/syscall.c, map_create() first set map->usercnt to 1, a file
    descriptor is supposed to return to userspace. When bpf_map_new_fd()
    fails, drop the refcount.
    
    Fixes: bd5f5f4ecb78 ("bpf: Add BPF_MAP_GET_FD_BY_ID")
    Signed-off-by: Peng Sun <sironhide0null@gmail.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index f98328abe8fa..84470d1480aa 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -559,12 +559,12 @@ static int map_create(union bpf_attr *attr)
 	err = bpf_map_new_fd(map, f_flags);
 	if (err < 0) {
 		/* failed to allocate fd.
-		 * bpf_map_put() is needed because the above
+		 * bpf_map_put_with_uref() is needed because the above
 		 * bpf_map_alloc_id() has published the map
 		 * to the userspace and the userspace may
 		 * have refcnt-ed it through BPF_MAP_GET_FD_BY_ID.
 		 */
-		bpf_map_put(map);
+		bpf_map_put_with_uref(map);
 		return err;
 	}
 

commit 9ed8f1a6e7670aadd5aef30456a90b456ed1b185
Merge: 43f4e6279f05 7d762d69145a
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 28 08:27:17 2019 +0100

    Merge branch 'linus' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 5f8f8b93aeb8371c54af08bece2bd04bc2d48707
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Feb 25 14:28:40 2019 -0800

    bpf: expose program stats via bpf_prog_info
    
    Return bpf program run_time_ns and run_cnt via bpf_prog_info
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Andrii Nakryiko <andriin@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 31cf66fc3f5c..174581dfe225 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2152,6 +2152,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	struct bpf_prog_info __user *uinfo = u64_to_user_ptr(attr->info.info);
 	struct bpf_prog_info info = {};
 	u32 info_len = attr->info.info_len;
+	struct bpf_prog_stats stats;
 	char __user *uinsns;
 	u32 ulen;
 	int err;
@@ -2191,6 +2192,10 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	if (err)
 		return err;
 
+	bpf_prog_get_stats(prog, &stats);
+	info.run_time_ns = stats.nsecs;
+	info.run_cnt = stats.cnt;
+
 	if (!capable(CAP_SYS_ADMIN)) {
 		info.jited_prog_len = 0;
 		info.xlated_prog_len = 0;

commit 492ecee892c2a4ba6a14903d5d586ff750b7e805
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Mon Feb 25 14:28:39 2019 -0800

    bpf: enable program stats
    
    JITed BPF programs are indistinguishable from kernel functions, but unlike
    kernel code BPF code can be changed often.
    Typical approach of "perf record" + "perf report" profiling and tuning of
    kernel code works just as well for BPF programs, but kernel code doesn't
    need to be monitored whereas BPF programs do.
    Users load and run large amount of BPF programs.
    These BPF stats allow tools monitor the usage of BPF on the server.
    The monitoring tools will turn sysctl kernel.bpf_stats_enabled
    on and off for few seconds to sample average cost of the programs.
    Aggregated data over hours and days will provide an insight into cost of BPF
    and alarms can trigger in case given program suddenly gets more expensive.
    
    The cost of two sched_clock() per program invocation adds ~20 nsec.
    Fast BPF progs (like selftests/bpf/progs/test_pkt_access.c) will slow down
    from ~10 nsec to ~30 nsec.
    static_key minimizes the cost of the stats collection.
    There is no measurable difference before/after this patch
    with kernel.bpf_stats_enabled=0
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ec7c552af76b..31cf66fc3f5c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1283,24 +1283,54 @@ static int bpf_prog_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+static void bpf_prog_get_stats(const struct bpf_prog *prog,
+			       struct bpf_prog_stats *stats)
+{
+	u64 nsecs = 0, cnt = 0;
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		const struct bpf_prog_stats *st;
+		unsigned int start;
+		u64 tnsecs, tcnt;
+
+		st = per_cpu_ptr(prog->aux->stats, cpu);
+		do {
+			start = u64_stats_fetch_begin_irq(&st->syncp);
+			tnsecs = st->nsecs;
+			tcnt = st->cnt;
+		} while (u64_stats_fetch_retry_irq(&st->syncp, start));
+		nsecs += tnsecs;
+		cnt += tcnt;
+	}
+	stats->nsecs = nsecs;
+	stats->cnt = cnt;
+}
+
 #ifdef CONFIG_PROC_FS
 static void bpf_prog_show_fdinfo(struct seq_file *m, struct file *filp)
 {
 	const struct bpf_prog *prog = filp->private_data;
 	char prog_tag[sizeof(prog->tag) * 2 + 1] = { };
+	struct bpf_prog_stats stats;
 
+	bpf_prog_get_stats(prog, &stats);
 	bin2hex(prog_tag, prog->tag, sizeof(prog->tag));
 	seq_printf(m,
 		   "prog_type:\t%u\n"
 		   "prog_jited:\t%u\n"
 		   "prog_tag:\t%s\n"
 		   "memlock:\t%llu\n"
-		   "prog_id:\t%u\n",
+		   "prog_id:\t%u\n"
+		   "run_time_ns:\t%llu\n"
+		   "run_cnt:\t%llu\n",
 		   prog->type,
 		   prog->jited,
 		   prog_tag,
 		   prog->pages * 1ULL << PAGE_SHIFT,
-		   prog->aux->id);
+		   prog->aux->id,
+		   stats.nsecs,
+		   stats.cnt);
 }
 #endif
 

commit 781e62823cb81b972dc8652c1827205cda2ac9ac
Author: Peng Sun <sironhide0null@gmail.com>
Date:   Tue Feb 26 22:15:37 2019 +0800

    bpf: decrease usercnt if bpf_map_new_fd() fails in bpf_map_get_fd_by_id()
    
    In bpf/syscall.c, bpf_map_get_fd_by_id() use bpf_map_inc_not_zero()
    to increase the refcount, both map->refcnt and map->usercnt. Then, if
    bpf_map_new_fd() fails, should handle map->usercnt too.
    
    Fixes: bd5f5f4ecb78 ("bpf: Add BPF_MAP_GET_FD_BY_ID")
    Signed-off-by: Peng Sun <sironhide0null@gmail.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 8577bb7f8be6..f98328abe8fa 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1986,7 +1986,7 @@ static int bpf_map_get_fd_by_id(const union bpf_attr *attr)
 
 	fd = bpf_map_new_fd(map, f_flags);
 	if (fd < 0)
-		bpf_map_put(map);
+		bpf_map_put_with_uref(map);
 
 	return fd;
 }

commit a655fe9f194842693258f43b5382855db1c2f654
Merge: 7499a288bf1a 27b4ad621e88
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Feb 8 15:00:17 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    An ipvlan bug fix in 'net' conflicted with the abstraction away
    of the IPV6 specific support in 'net-next'.
    
    Similarly, a bug fix for mlx5 in 'net' conflicted with the flow
    action conversion in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 96049f3afd50fe8db69fa0068cdca822e747b1e4
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Jan 31 15:40:09 2019 -0800

    bpf: introduce BPF_F_LOCK flag
    
    Introduce BPF_F_LOCK flag for map_lookup and map_update syscall commands
    and for map_update() helper function.
    In all these cases take a lock of existing element (which was provided
    in BTF description) before copying (in or out) the rest of map value.
    
    Implementation details that are part of uapi:
    
    Array:
    The array map takes the element lock for lookup/update.
    
    Hash:
    hash map also takes the lock for lookup/update and tries to avoid the bucket lock.
    If old element exists it takes the element lock and updates the element in place.
    If element doesn't exist it allocates new one and inserts into hash table
    while holding the bucket lock.
    In rare case the hashmap has to take both the bucket lock and the element lock
    to update old value in place.
    
    Cgroup local storage:
    It is similar to array. update in place and lookup are done with lock taken.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b29e6dc44650..0834958f1dc4 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -682,7 +682,7 @@ static void *__bpf_copy_key(void __user *ukey, u64 key_size)
 }
 
 /* last field in 'union bpf_attr' used by this command */
-#define BPF_MAP_LOOKUP_ELEM_LAST_FIELD value
+#define BPF_MAP_LOOKUP_ELEM_LAST_FIELD flags
 
 static int map_lookup_elem(union bpf_attr *attr)
 {
@@ -698,6 +698,9 @@ static int map_lookup_elem(union bpf_attr *attr)
 	if (CHECK_ATTR(BPF_MAP_LOOKUP_ELEM))
 		return -EINVAL;
 
+	if (attr->flags & ~BPF_F_LOCK)
+		return -EINVAL;
+
 	f = fdget(ufd);
 	map = __bpf_map_get(f);
 	if (IS_ERR(map))
@@ -708,6 +711,12 @@ static int map_lookup_elem(union bpf_attr *attr)
 		goto err_put;
 	}
 
+	if ((attr->flags & BPF_F_LOCK) &&
+	    !map_value_has_spin_lock(map)) {
+		err = -EINVAL;
+		goto err_put;
+	}
+
 	key = __bpf_copy_key(ukey, map->key_size);
 	if (IS_ERR(key)) {
 		err = PTR_ERR(key);
@@ -758,7 +767,13 @@ static int map_lookup_elem(union bpf_attr *attr)
 			err = -ENOENT;
 		} else {
 			err = 0;
-			copy_map_value(map, value, ptr);
+			if (attr->flags & BPF_F_LOCK)
+				/* lock 'ptr' and copy everything but lock */
+				copy_map_value_locked(map, value, ptr, true);
+			else
+				copy_map_value(map, value, ptr);
+			/* mask lock, since value wasn't zero inited */
+			check_and_init_map_lock(map, value);
 		}
 		rcu_read_unlock();
 	}
@@ -818,6 +833,12 @@ static int map_update_elem(union bpf_attr *attr)
 		goto err_put;
 	}
 
+	if ((attr->flags & BPF_F_LOCK) &&
+	    !map_value_has_spin_lock(map)) {
+		err = -EINVAL;
+		goto err_put;
+	}
+
 	key = __bpf_copy_key(ukey, map->key_size);
 	if (IS_ERR(key)) {
 		err = PTR_ERR(key);

commit e16d2f1ab96849b4b65e64b82550a7ecdbf405eb
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Jan 31 15:40:05 2019 -0800

    bpf: add support for bpf_spin_lock to cgroup local storage
    
    Allow 'struct bpf_spin_lock' to reside inside cgroup local storage.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ebf0a673cb83..b29e6dc44650 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -482,7 +482,8 @@ static int map_check_btf(struct bpf_map *map, const struct btf *btf,
 
 	if (map_value_has_spin_lock(map)) {
 		if (map->map_type != BPF_MAP_TYPE_HASH &&
-		    map->map_type != BPF_MAP_TYPE_ARRAY)
+		    map->map_type != BPF_MAP_TYPE_ARRAY &&
+		    map->map_type != BPF_MAP_TYPE_CGROUP_STORAGE)
 			return -ENOTSUPP;
 		if (map->spin_lock_off + sizeof(struct bpf_spin_lock) >
 		    map->value_size) {

commit d83525ca62cf8ebe3271d14c36fb900c294274a2
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Thu Jan 31 15:40:04 2019 -0800

    bpf: introduce bpf_spin_lock
    
    Introduce 'struct bpf_spin_lock' and bpf_spin_lock/unlock() helpers to let
    bpf program serialize access to other variables.
    
    Example:
    struct hash_elem {
        int cnt;
        struct bpf_spin_lock lock;
    };
    struct hash_elem * val = bpf_map_lookup_elem(&hash_map, &key);
    if (val) {
        bpf_spin_lock(&val->lock);
        val->cnt++;
        bpf_spin_unlock(&val->lock);
    }
    
    Restrictions and safety checks:
    - bpf_spin_lock is only allowed inside HASH and ARRAY maps.
    - BTF description of the map is mandatory for safety analysis.
    - bpf program can take one bpf_spin_lock at a time, since two or more can
      cause dead locks.
    - only one 'struct bpf_spin_lock' is allowed per map element.
      It drastically simplifies implementation yet allows bpf program to use
      any number of bpf_spin_locks.
    - when bpf_spin_lock is taken the calls (either bpf2bpf or helpers) are not allowed.
    - bpf program must bpf_spin_unlock() before return.
    - bpf program can access 'struct bpf_spin_lock' only via
      bpf_spin_lock()/bpf_spin_unlock() helpers.
    - load/store into 'struct bpf_spin_lock lock;' field is not allowed.
    - to use bpf_spin_lock() helper the BTF description of map value must be
      a struct and have 'struct bpf_spin_lock anyname;' field at the top level.
      Nested lock inside another struct is not allowed.
    - syscall map_lookup doesn't copy bpf_spin_lock field to user space.
    - syscall map_update and program map_update do not update bpf_spin_lock field.
    - bpf_spin_lock cannot be on the stack or inside networking packet.
      bpf_spin_lock can only be inside HASH or ARRAY map value.
    - bpf_spin_lock is available to root only and to all program types.
    - bpf_spin_lock is not allowed in inner maps of map-in-map.
    - ld_abs is not allowed inside spin_lock-ed region.
    - tracing progs and socket filter progs cannot use bpf_spin_lock due to
      insufficient preemption checks
    
    Implementation details:
    - cgroup-bpf class of programs can nest with xdp/tc programs.
      Hence bpf_spin_lock is equivalent to spin_lock_irqsave.
      Other solutions to avoid nested bpf_spin_lock are possible.
      Like making sure that all networking progs run with softirq disabled.
      spin_lock_irqsave is the simplest and doesn't add overhead to the
      programs that don't use it.
    - arch_spinlock_t is used when its implemented as queued_spin_lock
    - archs can force their own arch_spinlock_t
    - on architectures where queued_spin_lock is not available and
      sizeof(arch_spinlock_t) != sizeof(__u32) trivial lock is used.
    - presence of bpf_spin_lock inside map value could have been indicated via
      extra flag during map_create, but specifying it via BTF is cleaner.
      It provides introspection for map key/value and reduces user mistakes.
    
    Next steps:
    - allow bpf_spin_lock in other map types (like cgroup local storage)
    - introduce BPF_F_LOCK flag for bpf_map_update() syscall and helper
      to request kernel to grab bpf_spin_lock before rewriting the value.
      That will serialize access to map elements.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b155cd17c1bd..ebf0a673cb83 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -463,7 +463,7 @@ int map_check_no_btf(const struct bpf_map *map,
 	return -ENOTSUPP;
 }
 
-static int map_check_btf(const struct bpf_map *map, const struct btf *btf,
+static int map_check_btf(struct bpf_map *map, const struct btf *btf,
 			 u32 btf_key_id, u32 btf_value_id)
 {
 	const struct btf_type *key_type, *value_type;
@@ -478,6 +478,21 @@ static int map_check_btf(const struct bpf_map *map, const struct btf *btf,
 	if (!value_type || value_size != map->value_size)
 		return -EINVAL;
 
+	map->spin_lock_off = btf_find_spin_lock(btf, value_type);
+
+	if (map_value_has_spin_lock(map)) {
+		if (map->map_type != BPF_MAP_TYPE_HASH &&
+		    map->map_type != BPF_MAP_TYPE_ARRAY)
+			return -ENOTSUPP;
+		if (map->spin_lock_off + sizeof(struct bpf_spin_lock) >
+		    map->value_size) {
+			WARN_ONCE(1,
+				  "verifier bug spin_lock_off %d value_size %d\n",
+				  map->spin_lock_off, map->value_size);
+			return -EFAULT;
+		}
+	}
+
 	if (map->ops->map_check_btf)
 		ret = map->ops->map_check_btf(map, btf, key_type, value_type);
 
@@ -542,6 +557,8 @@ static int map_create(union bpf_attr *attr)
 		map->btf = btf;
 		map->btf_key_type_id = attr->btf_key_type_id;
 		map->btf_value_type_id = attr->btf_value_type_id;
+	} else {
+		map->spin_lock_off = -EINVAL;
 	}
 
 	err = security_bpf_map_alloc(map);
@@ -740,7 +757,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 			err = -ENOENT;
 		} else {
 			err = 0;
-			memcpy(value, ptr, value_size);
+			copy_map_value(map, value, ptr);
 		}
 		rcu_read_unlock();
 	}

commit 7c4cd051add3d00bbff008a133c936c515eaa8fe
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Jan 30 18:12:45 2019 -0800

    bpf: Fix syscall's stackmap lookup potential deadlock
    
    The map_lookup_elem used to not acquiring spinlock
    in order to optimize the reader.
    
    It was true until commit 557c0c6e7df8 ("bpf: convert stackmap to pre-allocation")
    The syscall's map_lookup_elem(stackmap) calls bpf_stackmap_copy().
    bpf_stackmap_copy() may find the elem no longer needed after the copy is done.
    If that is the case, pcpu_freelist_push() saves this elem for reuse later.
    This push requires a spinlock.
    
    If a tracing bpf_prog got run in the middle of the syscall's
    map_lookup_elem(stackmap) and this tracing bpf_prog is calling
    bpf_get_stackid(stackmap) which also requires the same pcpu_freelist's
    spinlock, it may end up with a dead lock situation as reported by
    Eric Dumazet in https://patchwork.ozlabs.org/patch/1030266/
    
    The situation is the same as the syscall's map_update_elem() which
    needs to acquire the pcpu_freelist's spinlock and could race
    with tracing bpf_prog.  Hence, this patch fixes it by protecting
    bpf_stackmap_copy() with this_cpu_inc(bpf_prog_active)
    to prevent tracing bpf_prog from running.
    
    A later syscall's map_lookup_elem commit f1a2e44a3aec ("bpf: add queue and stack maps")
    also acquires a spinlock and races with tracing bpf_prog similarly.
    Hence, this patch is forward looking and protects the majority
    of the map lookups.  bpf_map_offload_lookup_elem() is the exception
    since it is for network bpf_prog only (i.e. never called by tracing
    bpf_prog).
    
    Fixes: 557c0c6e7df8 ("bpf: convert stackmap to pre-allocation")
    Reported-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b155cd17c1bd..8577bb7f8be6 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -713,8 +713,13 @@ static int map_lookup_elem(union bpf_attr *attr)
 
 	if (bpf_map_is_dev_bound(map)) {
 		err = bpf_map_offload_lookup_elem(map, key, value);
-	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
-		   map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
+		goto done;
+	}
+
+	preempt_disable();
+	this_cpu_inc(bpf_prog_active);
+	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
 		err = bpf_percpu_hash_copy(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
 		err = bpf_percpu_array_copy(map, key, value);
@@ -744,7 +749,10 @@ static int map_lookup_elem(union bpf_attr *attr)
 		}
 		rcu_read_unlock();
 	}
+	this_cpu_dec(bpf_prog_active);
+	preempt_enable();
 
+done:
 	if (err)
 		goto free_value;
 

commit 6ee52e2a3fe4ea35520720736e6791df1fb67106
Author: Song Liu <songliubraving@fb.com>
Date:   Thu Jan 17 08:15:15 2019 -0800

    perf, bpf: Introduce PERF_RECORD_BPF_EVENT
    
    For better performance analysis of BPF programs, this patch introduces
    PERF_RECORD_BPF_EVENT, a new perf_event_type that exposes BPF program
    load/unload information to user space.
    
    Each BPF program may contain up to BPF_MAX_SUBPROGS (256) sub programs.
    The following example shows kernel symbols for a BPF program with 7 sub
    programs:
    
        ffffffffa0257cf9 t bpf_prog_b07ccb89267cf242_F
        ffffffffa02592e1 t bpf_prog_2dcecc18072623fc_F
        ffffffffa025b0e9 t bpf_prog_bb7a405ebaec5d5c_F
        ffffffffa025dd2c t bpf_prog_a7540d4a39ec1fc7_F
        ffffffffa025fcca t bpf_prog_05762d4ade0e3737_F
        ffffffffa026108f t bpf_prog_db4bd11e35df90d4_F
        ffffffffa0263f00 t bpf_prog_89d64e4abf0f0126_F
        ffffffffa0257cf9 t bpf_prog_ae31629322c4b018__dummy_tracepoi
    
    When a bpf program is loaded, PERF_RECORD_KSYMBOL is generated for each
    of these sub programs. Therefore, PERF_RECORD_BPF_EVENT is not needed
    for simple profiling.
    
    For annotation, user space need to listen to PERF_RECORD_BPF_EVENT and
    gather more information about these (sub) programs via sys_bpf.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Reviewed-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradeaed.org>
    Tested-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: kernel-team@fb.com
    Cc: netdev@vger.kernel.org
    Link: http://lkml.kernel.org/r/20190117161521.1341602-4-songliubraving@fb.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b155cd17c1bd..30ebd085790b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1211,6 +1211,7 @@ static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
+		perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_UNLOAD, 0);
 		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog, do_idr_lock);
 		bpf_prog_kallsyms_del_all(prog);
@@ -1554,6 +1555,7 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	}
 
 	bpf_prog_kallsyms_add(prog);
+	perf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_LOAD, 0);
 	return err;
 
 free_used_maps:

commit 96d4f267e40f9509e8a66e2b39e8b95655617693
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 3 18:57:57 2019 -0800

    Remove 'type' argument from access_ok() function
    
    Nobody has actually used the type (VERIFY_READ vs VERIFY_WRITE) argument
    of the user address range verification function since we got rid of the
    old racy i386-only code to walk page tables by hand.
    
    It existed because the original 80386 would not honor the write protect
    bit when in kernel mode, so you had to do COW by hand before doing any
    user access.  But we haven't supported that in a long time, and these
    days the 'type' argument is a purely historical artifact.
    
    A discussion about extending 'user_access_begin()' to do the range
    checking resulted this patch, because there is no way we're going to
    move the old VERIFY_xyz interface to that model.  And it's best done at
    the end of the merge window when I've done most of my merges, so let's
    just get this done once and for all.
    
    This patch was mostly done with a sed-script, with manual fix-ups for
    the cases that weren't of the trivial 'access_ok(VERIFY_xyz' form.
    
    There were a couple of notable cases:
    
     - csky still had the old "verify_area()" name as an alias.
    
     - the iter_iov code had magical hardcoded knowledge of the actual
       values of VERIFY_{READ,WRITE} (not that they mattered, since nothing
       really used it)
    
     - microblaze used the type argument for a debug printout
    
    but other than those oddities this should be a total no-op patch.
    
    I tried to fix up all architectures, did fairly extensive grepping for
    access_ok() uses, and the changes are trivial, but I may have missed
    something.  Any missed conversion should be trivially fixable, though.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0607db304def..b155cd17c1bd 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -79,7 +79,7 @@ int bpf_check_uarg_tail_zero(void __user *uaddr,
 	if (unlikely(actual_size > PAGE_SIZE))	/* silly large */
 		return -E2BIG;
 
-	if (unlikely(!access_ok(VERIFY_READ, uaddr, actual_size)))
+	if (unlikely(!access_ok(uaddr, actual_size)))
 		return -EFAULT;
 
 	if (actual_size <= expected_size)

commit a38d1107f937ca95dcf820161ef44ea683d6a0b1
Author: Matt Mullins <mmullins@fb.com>
Date:   Wed Dec 12 16:42:37 2018 -0800

    bpf: support raw tracepoints in modules
    
    Distributions build drivers as modules, including network and filesystem
    drivers which export numerous tracepoints.  This enables
    bpf(BPF_RAW_TRACEPOINT_OPEN) to attach to those tracepoints.
    
    Signed-off-by: Matt Mullins <mmullins@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5db31067d85e..0607db304def 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1604,6 +1604,7 @@ static int bpf_raw_tracepoint_release(struct inode *inode, struct file *filp)
 		bpf_probe_unregister(raw_tp->btp, raw_tp->prog);
 		bpf_prog_put(raw_tp->prog);
 	}
+	bpf_put_raw_tracepoint(raw_tp->btp);
 	kfree(raw_tp);
 	return 0;
 }
@@ -1629,13 +1630,15 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 		return -EFAULT;
 	tp_name[sizeof(tp_name) - 1] = 0;
 
-	btp = bpf_find_raw_tracepoint(tp_name);
+	btp = bpf_get_raw_tracepoint(tp_name);
 	if (!btp)
 		return -ENOENT;
 
 	raw_tp = kzalloc(sizeof(*raw_tp), GFP_USER);
-	if (!raw_tp)
-		return -ENOMEM;
+	if (!raw_tp) {
+		err = -ENOMEM;
+		goto out_put_btp;
+	}
 	raw_tp->btp = btp;
 
 	prog = bpf_prog_get_type(attr->raw_tracepoint.prog_fd,
@@ -1663,6 +1666,8 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 	bpf_prog_put(prog);
 out_free_tp:
 	kfree(raw_tp);
+out_put_btp:
+	bpf_put_raw_tracepoint(btp);
 	return err;
 }
 

commit 6c4fc209fcf9d27efbaa48368773e4d2bfbd59aa
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Dec 16 00:49:47 2018 +0100

    bpf: remove useless version check for prog load
    
    Existing libraries and tracing frameworks work around this kernel
    version check by automatically deriving the kernel version from
    uname(3) or similar such that the user does not need to do it
    manually; these workarounds also make the version check useless
    at the same time.
    
    Moreover, most other BPF tracing types enabling bpf_probe_read()-like
    functionality have /not/ adapted this check, and in general these
    days it is well understood anyway that all the tracing programs are
    not stable with regards to future kernels as kernel internal data
    structures are subject to change from release to release.
    
    Back at last netconf we discussed [0] and agreed to remove this
    check from bpf_prog_load() and instead document it here in the uapi
    header that there is no such guarantee for stable API for these
    programs.
    
      [0] http://vger.kernel.org/netconf2018_files/DanielBorkmann_netconf2018.pdf
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 6ae062f1cf20..5db31067d85e 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1473,11 +1473,6 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 
 	if (attr->insn_cnt == 0 || attr->insn_cnt > BPF_MAXINSNS)
 		return -E2BIG;
-
-	if (type == BPF_PROG_TYPE_KPROBE &&
-	    attr->kern_version != LINUX_VERSION_CODE)
-		return -EINVAL;
-
 	if (type != BPF_PROG_TYPE_SOCKET_FILTER &&
 	    type != BPF_PROG_TYPE_CGROUP_SKB &&
 	    !capable(CAP_SYS_ADMIN))

commit 9f8c1c5712954f9d8877ac55b18adbdf03e51e1f
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Dec 12 10:45:38 2018 +0100

    bpf: remove obsolete prog->aux sanitation in bpf_insn_prepare_dump
    
    This logic is not needed anymore since we got rid of the verifier
    rewrite that was using prog->aux address in f6069b9aa993 ("bpf:
    fix redirect to map under tail calls").
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 7f1410d6fbe9..6ae062f1cf20 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2032,13 +2032,6 @@ static struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog)
 			insns[i + 1].imm = 0;
 			continue;
 		}
-
-		if (!bpf_dump_raw_ok() &&
-		    imm == (unsigned long)prog->aux) {
-			insns[i].imm = 0;
-			insns[i + 1].imm = 0;
-			continue;
-		}
 	}
 
 	return insns;

commit c872bdb38febb4c31ece3599c52cf1f833b89f4e
Author: Song Liu <songliubraving@fb.com>
Date:   Wed Dec 12 09:37:46 2018 -0800

    bpf: include sub program tags in bpf_prog_info
    
    Changes v2 -> v3:
    1. remove check for bpf_dump_raw_ok().
    
    Changes v1 -> v2:
    1. Fix error path as Martin suggested.
    
    This patch adds nr_prog_tags and prog_tags to bpf_prog_info. This is a
    reliable way for user space to get tags of all sub programs. Before this
    patch, user space need to find sub program tags via kallsyms.
    
    This feature will be used in BPF introspection, where user space queries
    information about BPF programs via sys_bpf.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b7c585838c72..7f1410d6fbe9 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2315,6 +2315,28 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		}
 	}
 
+	ulen = info.nr_prog_tags;
+	info.nr_prog_tags = prog->aux->func_cnt ? : 1;
+	if (ulen) {
+		__u8 __user (*user_prog_tags)[BPF_TAG_SIZE];
+		u32 i;
+
+		user_prog_tags = u64_to_user_ptr(info.prog_tags);
+		ulen = min_t(u32, info.nr_prog_tags, ulen);
+		if (prog->aux->func_cnt) {
+			for (i = 0; i < ulen; i++) {
+				if (copy_to_user(user_prog_tags[i],
+						 prog->aux->func[i]->tag,
+						 BPF_TAG_SIZE))
+					return -EFAULT;
+			}
+		} else {
+			if (copy_to_user(user_prog_tags[0],
+					 prog->tag, BPF_TAG_SIZE))
+				return -EFAULT;
+		}
+	}
+
 done:
 	if (copy_to_user(uinfo, &info, info_len) ||
 	    put_user(info_len, &uattr->info.info_len))

commit 9e794163a69c103633fefb10a3879408d4e4e2c8
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Dec 12 10:18:21 2018 -0800

    bpf: Remove bpf_dump_raw_ok() check for func_info and line_info
    
    The func_info and line_info have the bpf insn offset but
    they do not contain kernel address.  They will still be useful
    for the userspace tool to annotate the xlated insn.
    
    This patch removes the bpf_dump_raw_ok() guard for the
    func_info and line_info during bpf_prog_get_info_by_fd().
    
    The guard stays for jited_line_info which contains the kernel
    address.
    
    Although this bpf_dump_raw_ok() guard behavior has started since
    the earlier func_info patch series, I marked the Fixes tag to the
    latest line_info patch series which contains both func_info and
    line_info and this patch is fixing for both of them.
    
    Fixes: c454a46b5efd ("bpf: Add bpf_line_info support")
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 70fb11106fc2..b7c585838c72 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2272,33 +2272,25 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	ulen = info.nr_func_info;
 	info.nr_func_info = prog->aux->func_info_cnt;
 	if (info.nr_func_info && ulen) {
-		if (bpf_dump_raw_ok()) {
-			char __user *user_finfo;
+		char __user *user_finfo;
 
-			user_finfo = u64_to_user_ptr(info.func_info);
-			ulen = min_t(u32, info.nr_func_info, ulen);
-			if (copy_to_user(user_finfo, prog->aux->func_info,
-					 info.func_info_rec_size * ulen))
-				return -EFAULT;
-		} else {
-			info.func_info = 0;
-		}
+		user_finfo = u64_to_user_ptr(info.func_info);
+		ulen = min_t(u32, info.nr_func_info, ulen);
+		if (copy_to_user(user_finfo, prog->aux->func_info,
+				 info.func_info_rec_size * ulen))
+			return -EFAULT;
 	}
 
 	ulen = info.nr_line_info;
 	info.nr_line_info = prog->aux->nr_linfo;
 	if (info.nr_line_info && ulen) {
-		if (bpf_dump_raw_ok()) {
-			__u8 __user *user_linfo;
+		__u8 __user *user_linfo;
 
-			user_linfo = u64_to_user_ptr(info.line_info);
-			ulen = min_t(u32, info.nr_line_info, ulen);
-			if (copy_to_user(user_linfo, prog->aux->linfo,
-					 info.line_info_rec_size * ulen))
-				return -EFAULT;
-		} else {
-			info.line_info = 0;
-		}
+		user_linfo = u64_to_user_ptr(info.line_info);
+		ulen = min_t(u32, info.nr_line_info, ulen);
+		if (copy_to_user(user_linfo, prog->aux->linfo,
+				 info.line_info_rec_size * ulen))
+			return -EFAULT;
 	}
 
 	ulen = info.nr_jited_line_info;

commit 1b2b234b1318afb3775d4c6624fd5a96558f19df
Author: Roman Gushchin <guroan@gmail.com>
Date:   Mon Dec 10 15:43:00 2018 -0800

    bpf: pass struct btf pointer to the map_check_btf() callback
    
    If key_type or value_type are of non-trivial data types
    (e.g. structure or typedef), it's not possible to check them without
    the additional information, which can't be obtained without a pointer
    to the btf structure.
    
    So, let's pass btf pointer to the map_check_btf() callbacks.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5745c7837621..70fb11106fc2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -456,6 +456,7 @@ static int bpf_obj_name_cpy(char *dst, const char *src)
 }
 
 int map_check_no_btf(const struct bpf_map *map,
+		     const struct btf *btf,
 		     const struct btf_type *key_type,
 		     const struct btf_type *value_type)
 {
@@ -478,7 +479,7 @@ static int map_check_btf(const struct bpf_map *map, const struct btf *btf,
 		return -EINVAL;
 
 	if (map->ops->map_check_btf)
-		ret = map->ops->map_check_btf(map, key_type, value_type);
+		ret = map->ops->map_check_btf(map, btf, key_type, value_type);
 
 	return ret;
 }

commit 11d8b82d2222cade12caad2c125f23023777dcbc
Author: Yonghong Song <yhs@fb.com>
Date:   Mon Dec 10 14:14:08 2018 -0800

    bpf: rename *_info_cnt to nr_*_info in bpf_prog_info
    
    In uapi bpf.h, currently we have the following fields in
    the struct bpf_prog_info:
            __u32 func_info_cnt;
            __u32 line_info_cnt;
            __u32 jited_line_info_cnt;
    The above field names "func_info_cnt" and "line_info_cnt"
    also appear in union bpf_attr for program loading.
    
    The original intention is to keep the names the same
    between bpf_prog_info and bpf_attr
    so it will imply what we returned to user space will be
    the same as what the user space passed to the kernel.
    
    Such a naming convention in bpf_prog_info is not consistent
    with other fields like:
            __u32 nr_jited_ksyms;
            __u32 nr_jited_func_lens;
    
    This patch made this adjustment so in bpf_prog_info
    newly introduced *_info_cnt becomes nr_*_info.
    
    Acked-by: Song Liu <songliubraving@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index a99a23bf5910..5745c7837621 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2055,15 +2055,15 @@ static int set_info_rec_size(struct bpf_prog_info *info)
 	 * _rec_size back to the info.
 	 */
 
-	if ((info->func_info_cnt || info->func_info_rec_size) &&
+	if ((info->nr_func_info || info->func_info_rec_size) &&
 	    info->func_info_rec_size != sizeof(struct bpf_func_info))
 		return -EINVAL;
 
-	if ((info->line_info_cnt || info->line_info_rec_size) &&
+	if ((info->nr_line_info || info->line_info_rec_size) &&
 	    info->line_info_rec_size != sizeof(struct bpf_line_info))
 		return -EINVAL;
 
-	if ((info->jited_line_info_cnt || info->jited_line_info_rec_size) &&
+	if ((info->nr_jited_line_info || info->jited_line_info_rec_size) &&
 	    info->jited_line_info_rec_size != sizeof(__u64))
 		return -EINVAL;
 
@@ -2125,9 +2125,9 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		info.xlated_prog_len = 0;
 		info.nr_jited_ksyms = 0;
 		info.nr_jited_func_lens = 0;
-		info.func_info_cnt = 0;
-		info.line_info_cnt = 0;
-		info.jited_line_info_cnt = 0;
+		info.nr_func_info = 0;
+		info.nr_line_info = 0;
+		info.nr_jited_line_info = 0;
 		goto done;
 	}
 
@@ -2268,14 +2268,14 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	if (prog->aux->btf)
 		info.btf_id = btf_id(prog->aux->btf);
 
-	ulen = info.func_info_cnt;
-	info.func_info_cnt = prog->aux->func_info_cnt;
-	if (info.func_info_cnt && ulen) {
+	ulen = info.nr_func_info;
+	info.nr_func_info = prog->aux->func_info_cnt;
+	if (info.nr_func_info && ulen) {
 		if (bpf_dump_raw_ok()) {
 			char __user *user_finfo;
 
 			user_finfo = u64_to_user_ptr(info.func_info);
-			ulen = min_t(u32, info.func_info_cnt, ulen);
+			ulen = min_t(u32, info.nr_func_info, ulen);
 			if (copy_to_user(user_finfo, prog->aux->func_info,
 					 info.func_info_rec_size * ulen))
 				return -EFAULT;
@@ -2284,14 +2284,14 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		}
 	}
 
-	ulen = info.line_info_cnt;
-	info.line_info_cnt = prog->aux->nr_linfo;
-	if (info.line_info_cnt && ulen) {
+	ulen = info.nr_line_info;
+	info.nr_line_info = prog->aux->nr_linfo;
+	if (info.nr_line_info && ulen) {
 		if (bpf_dump_raw_ok()) {
 			__u8 __user *user_linfo;
 
 			user_linfo = u64_to_user_ptr(info.line_info);
-			ulen = min_t(u32, info.line_info_cnt, ulen);
+			ulen = min_t(u32, info.nr_line_info, ulen);
 			if (copy_to_user(user_linfo, prog->aux->linfo,
 					 info.line_info_rec_size * ulen))
 				return -EFAULT;
@@ -2300,18 +2300,18 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		}
 	}
 
-	ulen = info.jited_line_info_cnt;
+	ulen = info.nr_jited_line_info;
 	if (prog->aux->jited_linfo)
-		info.jited_line_info_cnt = prog->aux->nr_linfo;
+		info.nr_jited_line_info = prog->aux->nr_linfo;
 	else
-		info.jited_line_info_cnt = 0;
-	if (info.jited_line_info_cnt && ulen) {
+		info.nr_jited_line_info = 0;
+	if (info.nr_jited_line_info && ulen) {
 		if (bpf_dump_raw_ok()) {
 			__u64 __user *user_linfo;
 			u32 i;
 
 			user_linfo = u64_to_user_ptr(info.jited_line_info);
-			ulen = min_t(u32, info.jited_line_info_cnt, ulen);
+			ulen = min_t(u32, info.nr_jited_line_info, ulen);
 			for (i = 0; i < ulen; i++) {
 				if (put_user((__u64)(long)prog->aux->jited_linfo[i],
 					     &user_linfo[i]))

commit 7a5725ddc6e18aab61f647c0996d3b7eb03ff5cb
Author: Song Liu <songliubraving@fb.com>
Date:   Mon Dec 10 11:17:50 2018 -0800

    bpf: clean up bpf_prog_get_info_by_fd()
    
    info.nr_jited_ksyms and info.nr_jited_func_lens cannot be 0 in these two
    statements, so we don't need to check them.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 19c88cff7880..a99a23bf5910 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2209,7 +2209,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 
 	ulen = info.nr_jited_ksyms;
 	info.nr_jited_ksyms = prog->aux->func_cnt ? : 1;
-	if (info.nr_jited_ksyms && ulen) {
+	if (ulen) {
 		if (bpf_dump_raw_ok()) {
 			unsigned long ksym_addr;
 			u64 __user *user_ksyms;
@@ -2240,7 +2240,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 
 	ulen = info.nr_jited_func_lens;
 	info.nr_jited_func_lens = prog->aux->func_cnt ? : 1;
-	if (info.nr_jited_func_lens && ulen) {
+	if (ulen) {
 		if (bpf_dump_raw_ok()) {
 			u32 __user *user_lens;
 			u32 func_len, i;

commit c454a46b5efd8eff8880e88ece2976e60a26bf35
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Dec 7 16:42:25 2018 -0800

    bpf: Add bpf_line_info support
    
    This patch adds bpf_line_info support.
    
    It accepts an array of bpf_line_info objects during BPF_PROG_LOAD.
    The "line_info", "line_info_cnt" and "line_info_rec_size" are added
    to the "union bpf_attr".  The "line_info_rec_size" makes
    bpf_line_info extensible in the future.
    
    The new "check_btf_line()" ensures the userspace line_info is valid
    for the kernel to use.
    
    When the verifier is translating/patching the bpf_prog (through
    "bpf_patch_insn_single()"), the line_infos' insn_off is also
    adjusted by the newly added "bpf_adj_linfo()".
    
    If the bpf_prog is jited, this patch also provides the jited addrs (in
    aux->jited_linfo) for the corresponding line_info.insn_off.
    "bpf_prog_fill_jited_linfo()" is added to fill the aux->jited_linfo.
    It is currently called by the x86 jit.  Other jits can also use
    "bpf_prog_fill_jited_linfo()" and it will be done in the followup patches.
    In the future, if it deemed necessary, a particular jit could also provide
    its own "bpf_prog_fill_jited_linfo()" implementation.
    
    A few "*line_info*" fields are added to the bpf_prog_info such
    that the user can get the xlated line_info back (i.e. the line_info
    with its insn_off reflecting the translated prog).  The jited_line_info
    is available if the prog is jited.  It is an array of __u64.
    If the prog is not jited, jited_line_info_cnt is 0.
    
    The verifier's verbose log with line_info will be done in
    a follow up patch.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index aa05aa38f4a8..19c88cff7880 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1215,6 +1215,7 @@ static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 		bpf_prog_kallsyms_del_all(prog);
 		btf_put(prog->aux->btf);
 		kvfree(prog->aux->func_info);
+		bpf_prog_free_linfo(prog);
 
 		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
 	}
@@ -1439,7 +1440,7 @@ bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 }
 
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD func_info_cnt
+#define	BPF_PROG_LOAD_LAST_FIELD line_info_cnt
 
 static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 {
@@ -1560,6 +1561,7 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	return err;
 
 free_used_maps:
+	bpf_prog_free_linfo(prog);
 	kvfree(prog->aux->func_info);
 	btf_put(prog->aux->btf);
 	bpf_prog_kallsyms_del_subprogs(prog);
@@ -2041,6 +2043,37 @@ static struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog)
 	return insns;
 }
 
+static int set_info_rec_size(struct bpf_prog_info *info)
+{
+	/*
+	 * Ensure info.*_rec_size is the same as kernel expected size
+	 *
+	 * or
+	 *
+	 * Only allow zero *_rec_size if both _rec_size and _cnt are
+	 * zero.  In this case, the kernel will set the expected
+	 * _rec_size back to the info.
+	 */
+
+	if ((info->func_info_cnt || info->func_info_rec_size) &&
+	    info->func_info_rec_size != sizeof(struct bpf_func_info))
+		return -EINVAL;
+
+	if ((info->line_info_cnt || info->line_info_rec_size) &&
+	    info->line_info_rec_size != sizeof(struct bpf_line_info))
+		return -EINVAL;
+
+	if ((info->jited_line_info_cnt || info->jited_line_info_rec_size) &&
+	    info->jited_line_info_rec_size != sizeof(__u64))
+		return -EINVAL;
+
+	info->func_info_rec_size = sizeof(struct bpf_func_info);
+	info->line_info_rec_size = sizeof(struct bpf_line_info);
+	info->jited_line_info_rec_size = sizeof(__u64);
+
+	return 0;
+}
+
 static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 				   const union bpf_attr *attr,
 				   union bpf_attr __user *uattr)
@@ -2083,11 +2116,9 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 				return -EFAULT;
 	}
 
-	if ((info.func_info_cnt || info.func_info_rec_size) &&
-	    info.func_info_rec_size != sizeof(struct bpf_func_info))
-		return -EINVAL;
-
-	info.func_info_rec_size = sizeof(struct bpf_func_info);
+	err = set_info_rec_size(&info);
+	if (err)
+		return err;
 
 	if (!capable(CAP_SYS_ADMIN)) {
 		info.jited_prog_len = 0;
@@ -2095,6 +2126,8 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		info.nr_jited_ksyms = 0;
 		info.nr_jited_func_lens = 0;
 		info.func_info_cnt = 0;
+		info.line_info_cnt = 0;
+		info.jited_line_info_cnt = 0;
 		goto done;
 	}
 
@@ -2251,6 +2284,44 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		}
 	}
 
+	ulen = info.line_info_cnt;
+	info.line_info_cnt = prog->aux->nr_linfo;
+	if (info.line_info_cnt && ulen) {
+		if (bpf_dump_raw_ok()) {
+			__u8 __user *user_linfo;
+
+			user_linfo = u64_to_user_ptr(info.line_info);
+			ulen = min_t(u32, info.line_info_cnt, ulen);
+			if (copy_to_user(user_linfo, prog->aux->linfo,
+					 info.line_info_rec_size * ulen))
+				return -EFAULT;
+		} else {
+			info.line_info = 0;
+		}
+	}
+
+	ulen = info.jited_line_info_cnt;
+	if (prog->aux->jited_linfo)
+		info.jited_line_info_cnt = prog->aux->nr_linfo;
+	else
+		info.jited_line_info_cnt = 0;
+	if (info.jited_line_info_cnt && ulen) {
+		if (bpf_dump_raw_ok()) {
+			__u64 __user *user_linfo;
+			u32 i;
+
+			user_linfo = u64_to_user_ptr(info.jited_line_info);
+			ulen = min_t(u32, info.jited_line_info_cnt, ulen);
+			for (i = 0; i < ulen; i++) {
+				if (put_user((__u64)(long)prog->aux->jited_linfo[i],
+					     &user_linfo[i]))
+					return -EFAULT;
+			}
+		} else {
+			info.jited_line_info = 0;
+		}
+	}
+
 done:
 	if (copy_to_user(uinfo, &info, info_len) ||
 	    put_user(info_len, &uattr->info.info_len))

commit 7337224fc150b3b762190425399ac0e8dee380d1
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Dec 5 17:35:43 2018 -0800

    bpf: Improve the info.func_info and info.func_info_rec_size behavior
    
    1) When bpf_dump_raw_ok() == false and the kernel can provide >=1
       func_info to the userspace, the current behavior is setting
       the info.func_info_cnt to 0 instead of setting info.func_info
       to 0.
    
       It is different from the behavior in jited_func_lens/nr_jited_func_lens,
       jited_ksyms/nr_jited_ksyms...etc.
    
       This patch fixes it. (i.e. set func_info to 0 instead of
       func_info_cnt to 0 when bpf_dump_raw_ok() == false).
    
    2) When the userspace passed in info.func_info_cnt == 0, the kernel
       will set the expected func_info size back to the
       info.func_info_rec_size.  It is a way for the userspace to learn
       the kernel expected func_info_rec_size introduced in
       commit 838e96904ff3 ("bpf: Introduce bpf_func_info").
    
       An exception is the kernel expected size is not set when
       func_info is not available for a bpf_prog.  This makes the
       returned info.func_info_rec_size has different values
       depending on the returned value of info.func_info_cnt.
    
       This patch sets the kernel expected size to info.func_info_rec_size
       independent of the info.func_info_cnt.
    
    3) The current logic only rejects invalid func_info_rec_size if
       func_info_cnt is non zero.  This patch also rejects invalid
       nonzero info.func_info_rec_size and not equal to the kernel
       expected size.
    
    4) Set info.btf_id as long as prog->aux->btf != NULL.  That will
       setup the later copy_to_user() codes look the same as others
       which then easier to understand and maintain.
    
       prog->aux->btf is not NULL only if prog->aux->func_info_cnt > 0.
    
       Breaking up info.btf_id from prog->aux->func_info_cnt is needed
       for the later line info patch anyway.
    
       A similar change is made to bpf_get_prog_name().
    
    Fixes: 838e96904ff3 ("bpf: Introduce bpf_func_info")
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4445d0d084d8..aa05aa38f4a8 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2083,6 +2083,12 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 				return -EFAULT;
 	}
 
+	if ((info.func_info_cnt || info.func_info_rec_size) &&
+	    info.func_info_rec_size != sizeof(struct bpf_func_info))
+		return -EINVAL;
+
+	info.func_info_rec_size = sizeof(struct bpf_func_info);
+
 	if (!capable(CAP_SYS_ADMIN)) {
 		info.jited_prog_len = 0;
 		info.xlated_prog_len = 0;
@@ -2226,35 +2232,23 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		}
 	}
 
-	if (prog->aux->btf) {
-		u32 krec_size = sizeof(struct bpf_func_info);
-		u32 ucnt, urec_size;
-
+	if (prog->aux->btf)
 		info.btf_id = btf_id(prog->aux->btf);
 
-		ucnt = info.func_info_cnt;
-		info.func_info_cnt = prog->aux->func_info_cnt;
-		urec_size = info.func_info_rec_size;
-		info.func_info_rec_size = krec_size;
-		if (ucnt) {
-			/* expect passed-in urec_size is what the kernel expects */
-			if (urec_size != info.func_info_rec_size)
-				return -EINVAL;
-
-			if (bpf_dump_raw_ok()) {
-				char __user *user_finfo;
-
-				user_finfo = u64_to_user_ptr(info.func_info);
-				ucnt = min_t(u32, info.func_info_cnt, ucnt);
-				if (copy_to_user(user_finfo, prog->aux->func_info,
-						 krec_size * ucnt))
-					return -EFAULT;
-			} else {
-				info.func_info_cnt = 0;
-			}
+	ulen = info.func_info_cnt;
+	info.func_info_cnt = prog->aux->func_info_cnt;
+	if (info.func_info_cnt && ulen) {
+		if (bpf_dump_raw_ok()) {
+			char __user *user_finfo;
+
+			user_finfo = u64_to_user_ptr(info.func_info);
+			ulen = min_t(u32, info.func_info_cnt, ulen);
+			if (copy_to_user(user_finfo, prog->aux->func_info,
+					 info.func_info_rec_size * ulen))
+				return -EFAULT;
+		} else {
+			info.func_info = 0;
 		}
-	} else {
-		info.func_info_cnt = 0;
 	}
 
 done:

commit 5482e9a93c83839f94e75db712e6837e6a39962c
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Sat Dec 1 17:08:44 2018 -0800

    bpf: Fix memleak in aux->func_info and aux->btf
    
    The aux->func_info and aux->btf are leaked in the error out cases
    during bpf_prog_load().  This patch fixes it.
    
    Fixes: ba64e7d85252 ("bpf: btf: support proper non-jit func info")
    Cc: Yonghong Song <yhs@fb.com>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index f9554d9a14e1..4445d0d084d8 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1560,6 +1560,8 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	return err;
 
 free_used_maps:
+	kvfree(prog->aux->func_info);
+	btf_put(prog->aux->btf);
 	bpf_prog_kallsyms_del_subprogs(prog);
 	free_used_maps(prog->aux);
 free_prog:

commit e9ee9efc0d176512cdce9d27ff8549d7ffa2bfcd
Author: David Miller <davem@davemloft.net>
Date:   Fri Nov 30 21:08:14 2018 -0800

    bpf: Add BPF_F_ANY_ALIGNMENT.
    
    Often we want to write tests cases that check things like bad context
    offset accesses.  And one way to do this is to use an odd offset on,
    for example, a 32-bit load.
    
    This unfortunately triggers the alignment checks first on platforms
    that do not set CONFIG_EFFICIENT_UNALIGNED_ACCESS.  So the test
    case see the alignment failure rather than what it was testing for.
    
    It is often not completely possible to respect the original intention
    of the test, or even test the same exact thing, while solving the
    alignment issue.
    
    Another option could have been to check the alignment after the
    context and other validations are performed by the verifier, but
    that is a non-trivial change to the verifier.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 85cbeec06e50..f9554d9a14e1 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1452,9 +1452,14 @@ static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 	if (CHECK_ATTR(BPF_PROG_LOAD))
 		return -EINVAL;
 
-	if (attr->prog_flags & ~BPF_F_STRICT_ALIGNMENT)
+	if (attr->prog_flags & ~(BPF_F_STRICT_ALIGNMENT | BPF_F_ANY_ALIGNMENT))
 		return -EINVAL;
 
+	if (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) &&
+	    (attr->prog_flags & BPF_F_ANY_ALIGNMENT) &&
+	    !capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
 	/* copy eBPF program license from user space */
 	if (strncpy_from_user(license, u64_to_user_ptr(attr->license),
 			      sizeof(license) - 1) < 0)

commit ba64e7d8525236aa56ab58ba3a3a71615c4ee289
Author: Yonghong Song <yhs@fb.com>
Date:   Sat Nov 24 23:20:44 2018 -0800

    bpf: btf: support proper non-jit func info
    
    Commit 838e96904ff3 ("bpf: Introduce bpf_func_info")
    added bpf func info support. The userspace is able
    to get better ksym's for bpf programs with jit, and
    is able to print out func prototypes.
    
    For a program containing func-to-func calls, the existing
    implementation returns user specified number of function
    calls and BTF types if jit is enabled. If the jit is not
    enabled, it only returns the type for the main function.
    
    This is undesirable. Interpreter may still be used
    and we should keep feature identical regardless of
    whether jit is enabled or not.
    This patch fixed this discrepancy.
    
    Fixes: 838e96904ff3 ("bpf: Introduce bpf_func_info")
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 998377808102..85cbeec06e50 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1214,6 +1214,7 @@ static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 		bpf_prog_free_id(prog, do_idr_lock);
 		bpf_prog_kallsyms_del_all(prog);
 		btf_put(prog->aux->btf);
+		kvfree(prog->aux->func_info);
 
 		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
 	}
@@ -2219,46 +2220,28 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	}
 
 	if (prog->aux->btf) {
+		u32 krec_size = sizeof(struct bpf_func_info);
 		u32 ucnt, urec_size;
 
 		info.btf_id = btf_id(prog->aux->btf);
 
 		ucnt = info.func_info_cnt;
-		info.func_info_cnt = prog->aux->func_cnt ? : 1;
+		info.func_info_cnt = prog->aux->func_info_cnt;
 		urec_size = info.func_info_rec_size;
-		info.func_info_rec_size = sizeof(struct bpf_func_info);
+		info.func_info_rec_size = krec_size;
 		if (ucnt) {
 			/* expect passed-in urec_size is what the kernel expects */
 			if (urec_size != info.func_info_rec_size)
 				return -EINVAL;
 
 			if (bpf_dump_raw_ok()) {
-				struct bpf_func_info kern_finfo;
 				char __user *user_finfo;
-				u32 i, insn_offset;
 
 				user_finfo = u64_to_user_ptr(info.func_info);
-				if (prog->aux->func_cnt) {
-					ucnt = min_t(u32, info.func_info_cnt, ucnt);
-					insn_offset = 0;
-					for (i = 0; i < ucnt; i++) {
-						kern_finfo.insn_offset = insn_offset;
-						kern_finfo.type_id = prog->aux->func[i]->aux->type_id;
-						if (copy_to_user(user_finfo, &kern_finfo,
-								 sizeof(kern_finfo)))
-							return -EFAULT;
-
-						/* func[i]->len holds the prog len */
-						insn_offset += prog->aux->func[i]->len;
-						user_finfo += urec_size;
-					}
-				} else {
-					kern_finfo.insn_offset = 0;
-					kern_finfo.type_id = prog->aux->type_id;
-					if (copy_to_user(user_finfo, &kern_finfo,
-							 sizeof(kern_finfo)))
-						return -EFAULT;
-				}
+				ucnt = min_t(u32, info.func_info_cnt, ucnt);
+				if (copy_to_user(user_finfo, prog->aux->func_info,
+						 krec_size * ucnt))
+					return -EFAULT;
 			} else {
 				info.func_info_cnt = 0;
 			}

commit 838e96904ff3fc6c30e5ebbc611474669856e3c0
Author: Yonghong Song <yhs@fb.com>
Date:   Mon Nov 19 15:29:11 2018 -0800

    bpf: Introduce bpf_func_info
    
    This patch added interface to load a program with the following
    additional information:
       . prog_btf_fd
       . func_info, func_info_rec_size and func_info_cnt
    where func_info will provide function range and type_id
    corresponding to each function.
    
    The func_info_rec_size is introduced in the UAPI to specify
    struct bpf_func_info size passed from user space. This
    intends to make bpf_func_info structure growable in the future.
    If the kernel gets a different bpf_func_info size from userspace,
    it will try to handle user request with part of bpf_func_info
    it can understand. In this patch, kernel can understand
      struct bpf_func_info {
           __u32   insn_offset;
           __u32   type_id;
      };
    If user passed a bpf func_info record size of 16 bytes, the
    kernel can still handle part of records with the above definition.
    
    If verifier agrees with function range provided by the user,
    the bpf_prog ksym for each function will use the func name
    provided in the type_id, which is supposed to provide better
    encoding as it is not limited by 16 bytes program name
    limitation and this is better for bpf program which contains
    multiple subprograms.
    
    The bpf_prog_info interface is also extended to
    return btf_id, func_info, func_info_rec_size and func_info_cnt
    to userspace, so userspace can print out the function prototype
    for each xlated function. The insn_offset in the returned
    func_info corresponds to the insn offset for xlated functions.
    With other jit related fields in bpf_prog_info, userspace can also
    print out function prototypes for each jited function.
    
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index cf5040fd5434..998377808102 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1213,6 +1213,7 @@ static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog, do_idr_lock);
 		bpf_prog_kallsyms_del_all(prog);
+		btf_put(prog->aux->btf);
 
 		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
 	}
@@ -1437,9 +1438,9 @@ bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 }
 
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD expected_attach_type
+#define	BPF_PROG_LOAD_LAST_FIELD func_info_cnt
 
-static int bpf_prog_load(union bpf_attr *attr)
+static int bpf_prog_load(union bpf_attr *attr, union bpf_attr __user *uattr)
 {
 	enum bpf_prog_type type = attr->prog_type;
 	struct bpf_prog *prog;
@@ -1525,7 +1526,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 		goto free_prog;
 
 	/* run eBPF verifier */
-	err = bpf_check(&prog, attr);
+	err = bpf_check(&prog, attr, uattr);
 	if (err < 0)
 		goto free_used_maps;
 
@@ -2079,6 +2080,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		info.xlated_prog_len = 0;
 		info.nr_jited_ksyms = 0;
 		info.nr_jited_func_lens = 0;
+		info.func_info_cnt = 0;
 		goto done;
 	}
 
@@ -2216,6 +2218,55 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		}
 	}
 
+	if (prog->aux->btf) {
+		u32 ucnt, urec_size;
+
+		info.btf_id = btf_id(prog->aux->btf);
+
+		ucnt = info.func_info_cnt;
+		info.func_info_cnt = prog->aux->func_cnt ? : 1;
+		urec_size = info.func_info_rec_size;
+		info.func_info_rec_size = sizeof(struct bpf_func_info);
+		if (ucnt) {
+			/* expect passed-in urec_size is what the kernel expects */
+			if (urec_size != info.func_info_rec_size)
+				return -EINVAL;
+
+			if (bpf_dump_raw_ok()) {
+				struct bpf_func_info kern_finfo;
+				char __user *user_finfo;
+				u32 i, insn_offset;
+
+				user_finfo = u64_to_user_ptr(info.func_info);
+				if (prog->aux->func_cnt) {
+					ucnt = min_t(u32, info.func_info_cnt, ucnt);
+					insn_offset = 0;
+					for (i = 0; i < ucnt; i++) {
+						kern_finfo.insn_offset = insn_offset;
+						kern_finfo.type_id = prog->aux->func[i]->aux->type_id;
+						if (copy_to_user(user_finfo, &kern_finfo,
+								 sizeof(kern_finfo)))
+							return -EFAULT;
+
+						/* func[i]->len holds the prog len */
+						insn_offset += prog->aux->func[i]->len;
+						user_finfo += urec_size;
+					}
+				} else {
+					kern_finfo.insn_offset = 0;
+					kern_finfo.type_id = prog->aux->type_id;
+					if (copy_to_user(user_finfo, &kern_finfo,
+							 sizeof(kern_finfo)))
+						return -EFAULT;
+				}
+			} else {
+				info.func_info_cnt = 0;
+			}
+		}
+	} else {
+		info.func_info_cnt = 0;
+	}
+
 done:
 	if (copy_to_user(uinfo, &info, info_len) ||
 	    put_user(info_len, &uattr->info.info_len))
@@ -2501,7 +2552,7 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 		err = map_get_next_key(&attr);
 		break;
 	case BPF_PROG_LOAD:
-		err = bpf_prog_load(&attr);
+		err = bpf_prog_load(&attr, uattr);
 		break;
 	case BPF_OBJ_PIN:
 		err = bpf_obj_pin(&attr);

commit 28c2fae726bf5003cd209b0d5910a642af98316f
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Nov 2 11:35:46 2018 +0100

    bpf: fix bpf_prog_get_info_by_fd to return 0 func_lens for unpriv
    
    While dbecd7388476 ("bpf: get kernel symbol addresses via syscall")
    zeroed info.nr_jited_ksyms in bpf_prog_get_info_by_fd() for queries
    from unprivileged users, commit 815581c11cc2 ("bpf: get JITed image
    lengths of functions via syscall") forgot about doing so and therefore
    returns the #elems of the user set up buffer which is incorrect. It
    also needs to indicate a info.nr_jited_func_lens of zero.
    
    Fixes: 815581c11cc2 ("bpf: get JITed image lengths of functions via syscall")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Sandipan Das <sandipan@linux.vnet.ibm.com>
    Cc: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 9418174c276c..cf5040fd5434 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2078,6 +2078,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		info.jited_prog_len = 0;
 		info.xlated_prog_len = 0;
 		info.nr_jited_ksyms = 0;
+		info.nr_jited_func_lens = 0;
 		goto done;
 	}
 

commit ff1889fc531f582f902175c0acc80321af540b24
Author: Song Liu <songliubraving@fb.com>
Date:   Fri Nov 2 10:16:17 2018 -0700

    bpf: show main program address and length in bpf_prog_info
    
    Currently, when there is no subprog (prog->aux->func_cnt == 0),
    bpf_prog_info does not return any jited_ksyms or jited_func_lens. This
    patch adds main program address (prog->bpf_func) and main program
    length (prog->jited_len) to bpf_prog_info.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 34a9eef5992c..9418174c276c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2158,11 +2158,11 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	}
 
 	ulen = info.nr_jited_ksyms;
-	info.nr_jited_ksyms = prog->aux->func_cnt;
+	info.nr_jited_ksyms = prog->aux->func_cnt ? : 1;
 	if (info.nr_jited_ksyms && ulen) {
 		if (bpf_dump_raw_ok()) {
+			unsigned long ksym_addr;
 			u64 __user *user_ksyms;
-			ulong ksym_addr;
 			u32 i;
 
 			/* copy the address of the kernel symbol
@@ -2170,9 +2170,17 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 			 */
 			ulen = min_t(u32, info.nr_jited_ksyms, ulen);
 			user_ksyms = u64_to_user_ptr(info.jited_ksyms);
-			for (i = 0; i < ulen; i++) {
-				ksym_addr = (ulong) prog->aux->func[i]->bpf_func;
-				if (put_user((u64) ksym_addr, &user_ksyms[i]))
+			if (prog->aux->func_cnt) {
+				for (i = 0; i < ulen; i++) {
+					ksym_addr = (unsigned long)
+						prog->aux->func[i]->bpf_func;
+					if (put_user((u64) ksym_addr,
+						     &user_ksyms[i]))
+						return -EFAULT;
+				}
+			} else {
+				ksym_addr = (unsigned long) prog->bpf_func;
+				if (put_user((u64) ksym_addr, &user_ksyms[0]))
 					return -EFAULT;
 			}
 		} else {
@@ -2181,7 +2189,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	}
 
 	ulen = info.nr_jited_func_lens;
-	info.nr_jited_func_lens = prog->aux->func_cnt;
+	info.nr_jited_func_lens = prog->aux->func_cnt ? : 1;
 	if (info.nr_jited_func_lens && ulen) {
 		if (bpf_dump_raw_ok()) {
 			u32 __user *user_lens;
@@ -2190,9 +2198,16 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 			/* copy the JITed image lengths for each function */
 			ulen = min_t(u32, info.nr_jited_func_lens, ulen);
 			user_lens = u64_to_user_ptr(info.jited_func_lens);
-			for (i = 0; i < ulen; i++) {
-				func_len = prog->aux->func[i]->jited_len;
-				if (put_user(func_len, &user_lens[i]))
+			if (prog->aux->func_cnt) {
+				for (i = 0; i < ulen; i++) {
+					func_len =
+						prog->aux->func[i]->jited_len;
+					if (put_user(func_len, &user_lens[i]))
+						return -EFAULT;
+				}
+			} else {
+				func_len = prog->jited_len;
+				if (put_user(func_len, &user_lens[0]))
 					return -EFAULT;
 			}
 		} else {

commit de57e99ceb65d0d7775cc14a8ba5931d7de1d708
Author: Song Liu <songliubraving@fb.com>
Date:   Fri Nov 2 10:16:16 2018 -0700

    bpf: show real jited address in bpf_prog_info->jited_ksyms
    
    Currently, jited_ksyms in bpf_prog_info shows page addresses of jited
    bpf program. The main reason here is to not expose randomized start
    address. However, this is not ideal for detailed profiling (find hot
    instructions from stack traces). This patch replaces the page address
    with real prog start address.
    
    This change is OK because bpf_prog_get_info_by_fd() is only available
    to root.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ccb93277aae2..34a9eef5992c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2172,7 +2172,6 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 			user_ksyms = u64_to_user_ptr(info.jited_ksyms);
 			for (i = 0; i < ulen; i++) {
 				ksym_addr = (ulong) prog->aux->func[i]->bpf_func;
-				ksym_addr &= PAGE_MASK;
 				if (put_user((u64) ksym_addr, &user_ksyms[i]))
 					return -EFAULT;
 			}

commit 540fefc08f75aedb517acbf525d393b8efddabd9
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Fri Oct 19 13:52:38 2018 -0700

    bpf: remove unused variable
    
    fix the following warning
    ../kernel/bpf/syscall.c: In function ‘map_lookup_and_delete_elem’:
    ../kernel/bpf/syscall.c:1010:22: warning: unused variable ‘ptr’ [-Wunused-variable]
      void *key, *value, *ptr;
                          ^~~
    
    Fixes: bd513cd08f10 ("bpf: add MAP_LOOKUP_AND_DELETE_ELEM syscall")
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 49ae64a26562..ccb93277aae2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1007,7 +1007,7 @@ static int map_lookup_and_delete_elem(union bpf_attr *attr)
 	void __user *uvalue = u64_to_user_ptr(attr->value);
 	int ufd = attr->map_fd;
 	struct bpf_map *map;
-	void *key, *value, *ptr;
+	void *key, *value;
 	u32 value_size;
 	struct fd f;
 	int err;

commit bd513cd08f10cbe28856f99ae951e86e86803861
Author: Mauricio Vasquez B <mauricio.vasquez@polito.it>
Date:   Thu Oct 18 15:16:30 2018 +0200

    bpf: add MAP_LOOKUP_AND_DELETE_ELEM syscall
    
    The previous patch implemented a bpf queue/stack maps that
    provided the peek/pop/push functions.  There is not a direct
    relationship between those functions and the current maps
    syscalls, hence a new MAP_LOOKUP_AND_DELETE_ELEM syscall is added,
    this is mapped to the pop operation in the queue/stack maps
    and it is still to implement in other kind of maps.
    
    Signed-off-by: Mauricio Vasquez B <mauricio.vasquez@polito.it>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 1617407f9ee5..49ae64a26562 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -999,6 +999,69 @@ static int map_get_next_key(union bpf_attr *attr)
 	return err;
 }
 
+#define BPF_MAP_LOOKUP_AND_DELETE_ELEM_LAST_FIELD value
+
+static int map_lookup_and_delete_elem(union bpf_attr *attr)
+{
+	void __user *ukey = u64_to_user_ptr(attr->key);
+	void __user *uvalue = u64_to_user_ptr(attr->value);
+	int ufd = attr->map_fd;
+	struct bpf_map *map;
+	void *key, *value, *ptr;
+	u32 value_size;
+	struct fd f;
+	int err;
+
+	if (CHECK_ATTR(BPF_MAP_LOOKUP_AND_DELETE_ELEM))
+		return -EINVAL;
+
+	f = fdget(ufd);
+	map = __bpf_map_get(f);
+	if (IS_ERR(map))
+		return PTR_ERR(map);
+
+	if (!(f.file->f_mode & FMODE_CAN_WRITE)) {
+		err = -EPERM;
+		goto err_put;
+	}
+
+	key = __bpf_copy_key(ukey, map->key_size);
+	if (IS_ERR(key)) {
+		err = PTR_ERR(key);
+		goto err_put;
+	}
+
+	value_size = map->value_size;
+
+	err = -ENOMEM;
+	value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
+	if (!value)
+		goto free_key;
+
+	if (map->map_type == BPF_MAP_TYPE_QUEUE ||
+	    map->map_type == BPF_MAP_TYPE_STACK) {
+		err = map->ops->map_pop_elem(map, value);
+	} else {
+		err = -ENOTSUPP;
+	}
+
+	if (err)
+		goto free_value;
+
+	if (copy_to_user(uvalue, value, value_size) != 0)
+		goto free_value;
+
+	err = 0;
+
+free_value:
+	kfree(value);
+free_key:
+	kfree(key);
+err_put:
+	fdput(f);
+	return err;
+}
+
 static const struct bpf_prog_ops * const bpf_prog_types[] = {
 #define BPF_PROG_TYPE(_id, _name) \
 	[_id] = & _name ## _prog_ops,
@@ -2472,6 +2535,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_TASK_FD_QUERY:
 		err = bpf_task_fd_query(&attr, uattr);
 		break;
+	case BPF_MAP_LOOKUP_AND_DELETE_ELEM:
+		err = map_lookup_and_delete_elem(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit f1a2e44a3aeccb3ff18d3ccc0b0203e70b95bd92
Author: Mauricio Vasquez B <mauricio.vasquez@polito.it>
Date:   Thu Oct 18 15:16:25 2018 +0200

    bpf: add queue and stack maps
    
    Queue/stack maps implement a FIFO/LIFO data storage for ebpf programs.
    These maps support peek, pop and push operations that are exposed to eBPF
    programs through the new bpf_map[peek/pop/push] helpers.  Those operations
    are exposed to userspace applications through the already existing
    syscalls in the following way:
    
    BPF_MAP_LOOKUP_ELEM            -> peek
    BPF_MAP_LOOKUP_AND_DELETE_ELEM -> pop
    BPF_MAP_UPDATE_ELEM            -> push
    
    Queue/stack maps are implemented using a buffer, tail and head indexes,
    hence BPF_F_NO_PREALLOC is not supported.
    
    As opposite to other maps, queue and stack do not use RCU for protecting
    maps values, the bpf_map[peek/pop] have a ARG_PTR_TO_UNINIT_MAP_VALUE
    argument that is a pointer to a memory zone where to save the value of a
    map.  Basically the same as ARG_PTR_TO_UNINIT_MEM, but the size has not
    be passed as an extra argument.
    
    Our main motivation for implementing queue/stack maps was to keep track
    of a pool of elements, like network ports in a SNAT, however we forsee
    other use cases, like for exampling saving last N kernel events in a map
    and then analysing from userspace.
    
    Signed-off-by: Mauricio Vasquez B <mauricio.vasquez@polito.it>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 78d9dd95e25f..1617407f9ee5 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -727,6 +727,9 @@ static int map_lookup_elem(union bpf_attr *attr)
 		err = bpf_fd_htab_map_lookup_elem(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {
 		err = bpf_fd_reuseport_array_lookup_elem(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||
+		   map->map_type == BPF_MAP_TYPE_STACK) {
+		err = map->ops->map_peek_elem(map, value);
 	} else {
 		rcu_read_lock();
 		ptr = map->ops->map_lookup_elem(map, key);
@@ -857,6 +860,9 @@ static int map_update_elem(union bpf_attr *attr)
 		/* rcu_read_lock() is not needed */
 		err = bpf_fd_reuseport_array_update_elem(map, key, value,
 							 attr->flags);
+	} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||
+		   map->map_type == BPF_MAP_TYPE_STACK) {
+		err = map->ops->map_push_elem(map, value, attr->flags);
 	} else {
 		rcu_read_lock();
 		err = map->ops->map_update_elem(map, key, value, attr->flags);

commit c9d29f4658a5a6d2c2ba2afeb20ff763fc6286f9
Author: Mauricio Vasquez B <mauricio.vasquez@polito.it>
Date:   Thu Oct 18 15:16:14 2018 +0200

    bpf/syscall: allow key to be null in map functions
    
    This commit adds the required logic to allow key being NULL
    in case the key_size of the map is 0.
    
    A new __bpf_copy_key function helper only copies the key from
    userpsace when key_size != 0, otherwise it enforces that key must be
    null.
    
    Signed-off-by: Mauricio Vasquez B <mauricio.vasquez@polito.it>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index f4ecd6ed2252..78d9dd95e25f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -651,6 +651,17 @@ int __weak bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)
 	return -ENOTSUPP;
 }
 
+static void *__bpf_copy_key(void __user *ukey, u64 key_size)
+{
+	if (key_size)
+		return memdup_user(ukey, key_size);
+
+	if (ukey)
+		return ERR_PTR(-EINVAL);
+
+	return NULL;
+}
+
 /* last field in 'union bpf_attr' used by this command */
 #define BPF_MAP_LOOKUP_ELEM_LAST_FIELD value
 
@@ -678,7 +689,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 		goto err_put;
 	}
 
-	key = memdup_user(ukey, map->key_size);
+	key = __bpf_copy_key(ukey, map->key_size);
 	if (IS_ERR(key)) {
 		err = PTR_ERR(key);
 		goto err_put;
@@ -785,7 +796,7 @@ static int map_update_elem(union bpf_attr *attr)
 		goto err_put;
 	}
 
-	key = memdup_user(ukey, map->key_size);
+	key = __bpf_copy_key(ukey, map->key_size);
 	if (IS_ERR(key)) {
 		err = PTR_ERR(key);
 		goto err_put;
@@ -888,7 +899,7 @@ static int map_delete_elem(union bpf_attr *attr)
 		goto err_put;
 	}
 
-	key = memdup_user(ukey, map->key_size);
+	key = __bpf_copy_key(ukey, map->key_size);
 	if (IS_ERR(key)) {
 		err = PTR_ERR(key);
 		goto err_put;
@@ -941,7 +952,7 @@ static int map_get_next_key(union bpf_attr *attr)
 	}
 
 	if (ukey) {
-		key = memdup_user(ukey, map->key_size);
+		key = __bpf_copy_key(ukey, map->key_size);
 		if (IS_ERR(key)) {
 			err = PTR_ERR(key);
 			goto err_put;

commit 604326b41a6fb9b4a78b6179335decee0365cd8c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Oct 13 02:45:58 2018 +0200

    bpf, sockmap: convert to generic sk_msg interface
    
    Add a generic sk_msg layer, and convert current sockmap and later
    kTLS over to make use of it. While sk_buff handles network packet
    representation from netdevice up to socket, sk_msg handles data
    representation from application to socket layer.
    
    This means that sk_msg framework spans across ULP users in the
    kernel, and enables features such as introspection or filtering
    of data with the help of BPF programs that operate on this data
    structure.
    
    Latter becomes in particular useful for kTLS where data encryption
    is deferred into the kernel, and as such enabling the kernel to
    perform L7 introspection and policy based on BPF for TLS connections
    where the record is being encrypted after BPF has run and came to
    a verdict. In order to get there, first step is to transform open
    coding of scatter-gather list handling into a common core framework
    that subsystems can use.
    
    The code itself has been split and refactored into three bigger
    pieces: i) the generic sk_msg API which deals with managing the
    scatter gather ring, providing helpers for walking and mangling,
    transferring application data from user space into it, and preparing
    it for BPF pre/post-processing, ii) the plain sock map itself
    where sockets can be attached to or detached from; these bits
    are independent of i) which can now be used also without sock
    map, and iii) the integration with plain TCP as one protocol
    to be used for processing L7 application data (later this could
    e.g. also be extended to other protocols like UDP). The semantics
    are the same with the old sock map code and therefore no change
    of user facing behavior or APIs. While pursuing this work it
    also helped finding a number of bugs in the old sockmap code
    that we've fixed already in earlier commits. The test_sockmap
    kselftest suite passes through fine as well.
    
    Joint work with John.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 53968f82b919..f4ecd6ed2252 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1664,7 +1664,7 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	switch (ptype) {
 	case BPF_PROG_TYPE_SK_SKB:
 	case BPF_PROG_TYPE_SK_MSG:
-		ret = sockmap_get_from_fd(attr, ptype, prog);
+		ret = sock_map_get_from_fd(attr, prog);
 		break;
 	case BPF_PROG_TYPE_LIRC_MODE2:
 		ret = lirc_prog_attach(attr, prog);
@@ -1718,10 +1718,10 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 		ptype = BPF_PROG_TYPE_CGROUP_DEVICE;
 		break;
 	case BPF_SK_MSG_VERDICT:
-		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_MSG, NULL);
+		return sock_map_get_from_fd(attr, NULL);
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
-		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_SKB, NULL);
+		return sock_map_get_from_fd(attr, NULL);
 	case BPF_LIRC_MODE2:
 		return lirc_prog_detach(attr);
 	case BPF_FLOW_DISSECTOR:

commit 1ae80cf31938c8f77c37a29bbe29e7f1cd492be8
Author: Daniel Colascione <dancol@google.com>
Date:   Fri Oct 12 03:54:27 2018 -0700

    bpf: wait for running BPF programs when updating map-in-map
    
    The map-in-map frequently serves as a mechanism for atomic
    snapshotting of state that a BPF program might record.  The current
    implementation is dangerous to use in this way, however, since
    userspace has no way of knowing when all programs that might have
    retrieved the "old" value of the map may have completed.
    
    This change ensures that map update operations on map-in-map map types
    always wait for all references to the old map to drop before returning
    to userspace.
    
    Signed-off-by: Daniel Colascione <dancol@google.com>
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4f416234251f..53968f82b919 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -748,6 +748,17 @@ static int map_lookup_elem(union bpf_attr *attr)
 	return err;
 }
 
+static void maybe_wait_bpf_programs(struct bpf_map *map)
+{
+	/* Wait for any running BPF programs to complete so that
+	 * userspace, when we return to it, knows that all programs
+	 * that could be running use the new map value.
+	 */
+	if (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS ||
+	    map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS)
+		synchronize_rcu();
+}
+
 #define BPF_MAP_UPDATE_ELEM_LAST_FIELD flags
 
 static int map_update_elem(union bpf_attr *attr)
@@ -842,6 +853,7 @@ static int map_update_elem(union bpf_attr *attr)
 	}
 	__this_cpu_dec(bpf_prog_active);
 	preempt_enable();
+	maybe_wait_bpf_programs(map);
 out:
 free_value:
 	kfree(value);
@@ -894,6 +906,7 @@ static int map_delete_elem(union bpf_attr *attr)
 	rcu_read_unlock();
 	__this_cpu_dec(bpf_prog_active);
 	preempt_enable();
+	maybe_wait_bpf_programs(map);
 out:
 	kfree(key);
 err_put:

commit 509db2833e0ddac7faf6e7d2dd6e7f85c98fbee0
Author: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
Date:   Tue Oct 9 10:04:49 2018 +0900

    bpf: error handling when map_lookup_elem isn't supported
    
    The error value returned by map_lookup_elem doesn't differentiate
    whether lookup was failed because of invalid key or lookup is not
    supported.
    
    Lets add handling for -EOPNOTSUPP return value of map_lookup_elem()
    method of map, with expectation from map's implementation that it
    should return -EOPNOTSUPP if lookup is not supported.
    
    The errno for bpf syscall for BPF_MAP_LOOKUP_ELEM command will be set
    to EOPNOTSUPP if map lookup is not supported.
    
    Signed-off-by: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5742df21598c..4f416234251f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -719,10 +719,15 @@ static int map_lookup_elem(union bpf_attr *attr)
 	} else {
 		rcu_read_lock();
 		ptr = map->ops->map_lookup_elem(map, key);
-		if (ptr)
+		if (IS_ERR(ptr)) {
+			err = PTR_ERR(ptr);
+		} else if (!ptr) {
+			err = -ENOENT;
+		} else {
+			err = 0;
 			memcpy(value, ptr, value_size);
+		}
 		rcu_read_unlock();
-		err = ptr ? 0 : -ENOENT;
 	}
 
 	if (err)

commit b741f1630346defcbc8cc60f1a2bdae8b3b0036f
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Sep 28 14:45:43 2018 +0000

    bpf: introduce per-cpu cgroup local storage
    
    This commit introduced per-cpu cgroup local storage.
    
    Per-cpu cgroup local storage is very similar to simple cgroup storage
    (let's call it shared), except all the data is per-cpu.
    
    The main goal of per-cpu variant is to implement super fast
    counters (e.g. packet counters), which don't require neither
    lookups, neither atomic operations.
    
    >From userspace's point of view, accessing a per-cpu cgroup storage
    is similar to other per-cpu map types (e.g. per-cpu hashmaps and
    arrays).
    
    Writing to a per-cpu cgroup storage is not atomic, but is performed
    by copying longs, so some minimal atomicity is here, exactly
    as with other per-cpu maps.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 8c91d2b41b1e..5742df21598c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -686,7 +686,8 @@ static int map_lookup_elem(union bpf_attr *attr)
 
 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
-	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
+	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY ||
+	    map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)
 		value_size = round_up(map->value_size, 8) * num_possible_cpus();
 	else if (IS_FD_MAP(map))
 		value_size = sizeof(u32);
@@ -705,6 +706,8 @@ static int map_lookup_elem(union bpf_attr *attr)
 		err = bpf_percpu_hash_copy(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
 		err = bpf_percpu_array_copy(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {
+		err = bpf_percpu_cgroup_storage_copy(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {
 		err = bpf_stackmap_copy(map, key, value);
 	} else if (IS_FD_ARRAY(map)) {
@@ -774,7 +777,8 @@ static int map_update_elem(union bpf_attr *attr)
 
 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
-	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
+	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY ||
+	    map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)
 		value_size = round_up(map->value_size, 8) * num_possible_cpus();
 	else
 		value_size = map->value_size;
@@ -809,6 +813,9 @@ static int map_update_elem(union bpf_attr *attr)
 		err = bpf_percpu_hash_update(map, key, value, attr->flags);
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
 		err = bpf_percpu_array_update(map, key, value, attr->flags);
+	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {
+		err = bpf_percpu_cgroup_storage_update(map, key, value,
+						       attr->flags);
 	} else if (IS_FD_ARRAY(map)) {
 		rcu_read_lock();
 		err = bpf_fd_array_map_update_elem(map, f.file, key, value,

commit 8bad74f9840f87661f20ced3dc80c84ab4fd55a1
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Sep 28 14:45:36 2018 +0000

    bpf: extend cgroup bpf core to allow multiple cgroup storage types
    
    In order to introduce per-cpu cgroup storage, let's generalize
    bpf cgroup core to support multiple cgroup storage types.
    Potentially, per-node cgroup storage can be added later.
    
    This commit is mostly a formal change that replaces
    cgroup_storage pointer with a array of cgroup_storage pointers.
    It doesn't actually introduce a new storage type,
    it will be done later.
    
    Each bpf program is now able to have one cgroup storage of each type.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b3c2d09bcf7a..8c91d2b41b1e 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -988,10 +988,15 @@ static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
 /* drop refcnt on maps used by eBPF program and free auxilary data */
 static void free_used_maps(struct bpf_prog_aux *aux)
 {
+	enum bpf_cgroup_storage_type stype;
 	int i;
 
-	if (aux->cgroup_storage)
-		bpf_cgroup_storage_release(aux->prog, aux->cgroup_storage);
+	for_each_cgroup_storage_type(stype) {
+		if (!aux->cgroup_storage[stype])
+			continue;
+		bpf_cgroup_storage_release(aux->prog,
+					   aux->cgroup_storage[stype]);
+	}
 
 	for (i = 0; i < aux->used_map_cnt; i++)
 		bpf_map_put(aux->used_maps[i]);

commit d58e468b1112dcd1d5193c0a89ff9f98b5a3e8b9
Author: Petar Penkov <ppenkov@google.com>
Date:   Fri Sep 14 07:46:18 2018 -0700

    flow_dissector: implements flow dissector BPF hook
    
    Adds a hook for programs of type BPF_PROG_TYPE_FLOW_DISSECTOR and
    attach type BPF_FLOW_DISSECTOR that is executed in the flow dissector
    path. The BPF program is per-network namespace.
    
    Signed-off-by: Petar Penkov <ppenkov@google.com>
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 3c9636f03bb2..b3c2d09bcf7a 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1615,6 +1615,9 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_LIRC_MODE2:
 		ptype = BPF_PROG_TYPE_LIRC_MODE2;
 		break;
+	case BPF_FLOW_DISSECTOR:
+		ptype = BPF_PROG_TYPE_FLOW_DISSECTOR;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -1636,6 +1639,9 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_PROG_TYPE_LIRC_MODE2:
 		ret = lirc_prog_attach(attr, prog);
 		break;
+	case BPF_PROG_TYPE_FLOW_DISSECTOR:
+		ret = skb_flow_dissector_bpf_prog_attach(attr, prog);
+		break;
 	default:
 		ret = cgroup_bpf_prog_attach(attr, ptype, prog);
 	}
@@ -1688,6 +1694,8 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_SKB, NULL);
 	case BPF_LIRC_MODE2:
 		return lirc_prog_detach(attr);
+	case BPF_FLOW_DISSECTOR:
+		return skb_flow_dissector_bpf_prog_detach(attr);
 	default:
 		return -EINVAL;
 	}

commit efbaec89c642cd1d4977fc6df9923697e1598d4e
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Tue Aug 28 07:42:32 2018 +0000

    bpf: remove duplicated include from syscall.c
    
    Remove duplicated include.
    
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 8339d81cba1d..3c9636f03bb2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -30,7 +30,6 @@
 #include <linux/cred.h>
 #include <linux/timekeeping.h>
 #include <linux/ctype.h>
-#include <linux/btf.h>
 #include <linux/nospec.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY || \

commit 9a76aba02a37718242d7cdc294f0a3901928aa57
Merge: 0a957467c5fd 26a1ccc6c117
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 15 15:04:25 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Highlights:
    
       - Gustavo A. R. Silva keeps working on the implicit switch fallthru
         changes.
    
       - Support 802.11ax High-Efficiency wireless in cfg80211 et al, From
         Luca Coelho.
    
       - Re-enable ASPM in r8169, from Kai-Heng Feng.
    
       - Add virtual XFRM interfaces, which avoids all of the limitations of
         existing IPSEC tunnels. From Steffen Klassert.
    
       - Convert GRO over to use a hash table, so that when we have many
         flows active we don't traverse a long list during accumluation.
    
       - Many new self tests for routing, TC, tunnels, etc. Too many
         contributors to mention them all, but I'm really happy to keep
         seeing this stuff.
    
       - Hardware timestamping support for dpaa_eth/fsl-fman from Yangbo Lu.
    
       - Lots of cleanups and fixes in L2TP code from Guillaume Nault.
    
       - Add IPSEC offload support to netdevsim, from Shannon Nelson.
    
       - Add support for slotting with non-uniform distribution to netem
         packet scheduler, from Yousuk Seung.
    
       - Add UDP GSO support to mlx5e, from Boris Pismenny.
    
       - Support offloading of Team LAG in NFP, from John Hurley.
    
       - Allow to configure TX queue selection based upon RX queue, from
         Amritha Nambiar.
    
       - Support ethtool ring size configuration in aquantia, from Anton
         Mikaev.
    
       - Support DSCP and flowlabel per-transport in SCTP, from Xin Long.
    
       - Support list based batching and stack traversal of SKBs, this is
         very exciting work. From Edward Cree.
    
       - Busyloop optimizations in vhost_net, from Toshiaki Makita.
    
       - Introduce the ETF qdisc, which allows time based transmissions. IGB
         can offload this in hardware. From Vinicius Costa Gomes.
    
       - Add parameter support to devlink, from Moshe Shemesh.
    
       - Several multiplication and division optimizations for BPF JIT in
         nfp driver, from Jiong Wang.
    
       - Lots of prepatory work to make more of the packet scheduler layer
         lockless, when possible, from Vlad Buslov.
    
       - Add ACK filter and NAT awareness to sch_cake packet scheduler, from
         Toke Høiland-Jørgensen.
    
       - Support regions and region snapshots in devlink, from Alex Vesker.
    
       - Allow to attach XDP programs to both HW and SW at the same time on
         a given device, with initial support in nfp. From Jakub Kicinski.
    
       - Add TLS RX offload and support in mlx5, from Ilya Lesokhin.
    
       - Use PHYLIB in r8169 driver, from Heiner Kallweit.
    
       - All sorts of changes to support Spectrum 2 in mlxsw driver, from
         Ido Schimmel.
    
       - PTP support in mv88e6xxx DSA driver, from Andrew Lunn.
    
       - Make TCP_USER_TIMEOUT socket option more accurate, from Jon
         Maxwell.
    
       - Support for templates in packet scheduler classifier, from Jiri
         Pirko.
    
       - IPV6 support in RDS, from Ka-Cheong Poon.
    
       - Native tproxy support in nf_tables, from Máté Eckl.
    
       - Maintain IP fragment queue in an rbtree, but optimize properly for
         in-order frags. From Peter Oskolkov.
    
       - Improvde handling of ACKs on hole repairs, from Yuchung Cheng"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1996 commits)
      bpf: test: fix spelling mistake "REUSEEPORT" -> "REUSEPORT"
      hv/netvsc: Fix NULL dereference at single queue mode fallback
      net: filter: mark expected switch fall-through
      xen-netfront: fix warn message as irq device name has '/'
      cxgb4: Add new T5 PCI device ids 0x50af and 0x50b0
      net: dsa: mv88e6xxx: missing unlock on error path
      rds: fix building with IPV6=m
      inet/connection_sock: prefer _THIS_IP_ to current_text_addr
      net: dsa: mv88e6xxx: bitwise vs logical bug
      net: sock_diag: Fix spectre v1 gadget in __sock_diag_cmd()
      ieee802154: hwsim: using right kind of iteration
      net: hns3: Add vlan filter setting by ethtool command -K
      net: hns3: Set tx ring' tc info when netdev is up
      net: hns3: Remove tx ring BD len register in hns3_enet
      net: hns3: Fix desc num set to default when setting channel
      net: hns3: Fix for phy link issue when using marvell phy driver
      net: hns3: Fix for information of phydev lost problem when down/up
      net: hns3: Fix for command format parsing error in hclge_is_all_function_id_zero
      net: hns3: Add support for serdes loopback selftest
      bnxt_en: take coredump_record structure off stack
      ...

commit de5d1b39ea0b38a9f4dfb08966042b7b91e2df30
Merge: 1c594774283a fd2efaa4eb53
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 13 12:23:39 2018 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking/atomics update from Thomas Gleixner:
     "The locking, atomics and memory model brains delivered:
    
       - A larger update to the atomics code which reworks the ordering
         barriers, consolidates the atomic primitives, provides the new
         atomic64_fetch_add_unless() primitive and cleans up the include
         hell.
    
       - Simplify cmpxchg() instrumentation and add instrumentation for
         xchg() and cmpxchg_double().
    
       - Updates to the memory model and documentation"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (48 commits)
      locking/atomics: Rework ordering barriers
      locking/atomics: Instrument cmpxchg_double*()
      locking/atomics: Instrument xchg()
      locking/atomics: Simplify cmpxchg() instrumentation
      locking/atomics/x86: Reduce arch_cmpxchg64*() instrumentation
      tools/memory-model: Rename litmus tests to comply to norm7
      tools/memory-model/Documentation: Fix typo, smb->smp
      sched/Documentation: Update wake_up() & co. memory-barrier guarantees
      locking/spinlock, sched/core: Clarify requirements for smp_mb__after_spinlock()
      sched/core: Use smp_mb() in wake_woken_function()
      tools/memory-model: Add informal LKMM documentation to MAINTAINERS
      locking/atomics/Documentation: Describe atomic_set() as a write operation
      tools/memory-model: Make scripts executable
      tools/memory-model: Remove ACCESS_ONCE() from model
      tools/memory-model: Remove ACCESS_ONCE() from recipes
      locking/memory-barriers.txt/kokr: Update Korean translation to fix broken DMA vs. MMIO ordering example
      MAINTAINERS: Add Daniel Lustig as an LKMM reviewer
      tools/memory-model: Fix ISA2+pooncelock+pooncelock+pombonce name
      tools/memory-model: Add litmus test for full multicopy atomicity
      locking/refcount: Always allow checked forms
      ...

commit e8d2bec0457962e8f348a9a3627b398f7fe5c5fc
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Aug 12 01:59:17 2018 +0200

    bpf: decouple btf from seq bpf fs dump and enable more maps
    
    Commit a26ca7c982cb ("bpf: btf: Add pretty print support to
    the basic arraymap") and 699c86d6ec21 ("bpf: btf: add pretty
    print for hash/lru_hash maps") enabled support for BTF and
    dumping via BPF fs for array and hash/lru map. However, both
    can be decoupled from each other such that regular BPF maps
    can be supported for attaching BTF key/value information,
    while not all maps necessarily need to dump via map_seq_show_elem()
    callback.
    
    The basic sanity check which is a prerequisite for all maps
    is that key/value size has to match in any case, and some maps
    can have extra checks via map_check_btf() callback, e.g.
    probing certain types or indicating no support in general. With
    that we can also enable retrieving BTF info for per-cpu map
    types and lpm.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Yonghong Song <yhs@fb.com>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 57f4d076141b..43727ed0d94a 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -103,6 +103,7 @@ int bpf_check_uarg_tail_zero(void __user *uaddr,
 const struct bpf_map_ops bpf_map_offload_ops = {
 	.map_alloc = bpf_map_offload_map_alloc,
 	.map_free = bpf_map_offload_map_free,
+	.map_check_btf = map_check_no_btf,
 };
 
 static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
@@ -455,6 +456,34 @@ static int bpf_obj_name_cpy(char *dst, const char *src)
 	return 0;
 }
 
+int map_check_no_btf(const struct bpf_map *map,
+		     const struct btf_type *key_type,
+		     const struct btf_type *value_type)
+{
+	return -ENOTSUPP;
+}
+
+static int map_check_btf(const struct bpf_map *map, const struct btf *btf,
+			 u32 btf_key_id, u32 btf_value_id)
+{
+	const struct btf_type *key_type, *value_type;
+	u32 key_size, value_size;
+	int ret = 0;
+
+	key_type = btf_type_id_size(btf, &btf_key_id, &key_size);
+	if (!key_type || key_size != map->key_size)
+		return -EINVAL;
+
+	value_type = btf_type_id_size(btf, &btf_value_id, &value_size);
+	if (!value_type || value_size != map->value_size)
+		return -EINVAL;
+
+	if (map->ops->map_check_btf)
+		ret = map->ops->map_check_btf(map, key_type, value_type);
+
+	return ret;
+}
+
 #define BPF_MAP_CREATE_LAST_FIELD btf_value_type_id
 /* called via syscall */
 static int map_create(union bpf_attr *attr)
@@ -489,8 +518,7 @@ static int map_create(union bpf_attr *attr)
 	atomic_set(&map->refcnt, 1);
 	atomic_set(&map->usercnt, 1);
 
-	if (bpf_map_support_seq_show(map) &&
-	    (attr->btf_key_type_id || attr->btf_value_type_id)) {
+	if (attr->btf_key_type_id || attr->btf_value_type_id) {
 		struct btf *btf;
 
 		if (!attr->btf_key_type_id || !attr->btf_value_type_id) {
@@ -504,8 +532,8 @@ static int map_create(union bpf_attr *attr)
 			goto free_map_nouncharge;
 		}
 
-		err = map->ops->map_check_btf(map, btf, attr->btf_key_type_id,
-					      attr->btf_value_type_id);
+		err = map_check_btf(map, btf, attr->btf_key_type_id,
+				    attr->btf_value_type_id);
 		if (err) {
 			btf_put(btf);
 			goto free_map_nouncharge;

commit 5dc4c4b7d4e8115e7cde96a030f98cb3ab2e458c
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Aug 8 01:01:24 2018 -0700

    bpf: Introduce BPF_MAP_TYPE_REUSEPORT_SOCKARRAY
    
    This patch introduces a new map type BPF_MAP_TYPE_REUSEPORT_SOCKARRAY.
    
    To unleash the full potential of a bpf prog, it is essential for the
    userspace to be capable of directly setting up a bpf map which can then
    be consumed by the bpf prog to make decision.  In this case, decide which
    SO_REUSEPORT sk to serve the incoming request.
    
    By adding BPF_MAP_TYPE_REUSEPORT_SOCKARRAY, the userspace has total control
    and visibility on where a SO_REUSEPORT sk should be located in a bpf map.
    The later patch will introduce BPF_PROG_TYPE_SK_REUSEPORT such that
    the bpf prog can directly select a sk from the bpf map.  That will
    raise the programmability of the bpf prog attached to a reuseport
    group (a group of sk serving the same IP:PORT).
    
    For example, in UDP, the bpf prog can peek into the payload (e.g.
    through the "data" pointer introduced in the later patch) to learn
    the application level's connection information and then decide which sk
    to pick from a bpf map.  The userspace can tightly couple the sk's location
    in a bpf map with the application logic in generating the UDP payload's
    connection information.  This connection info contact/API stays within the
    userspace.
    
    Also, when used with map-in-map, the userspace can switch the
    old-server-process's inner map to a new-server-process's inner map
    in one call "bpf_map_update_elem(outer_map, &index, &new_reuseport_array)".
    The bpf prog will then direct incoming requests to the new process instead
    of the old process.  The old process can finish draining the pending
    requests (e.g. by "accept()") before closing the old-fds.  [Note that
    deleting a fd from a bpf map does not necessary mean the fd is closed]
    
    During map_update_elem(),
    Only SO_REUSEPORT sk (i.e. which has already been added
    to a reuse->socks[]) can be used.  That means a SO_REUSEPORT sk that is
    "bind()" for UDP or "bind()+listen()" for TCP.  These conditions are
    ensured in "reuseport_array_update_check()".
    
    A SO_REUSEPORT sk can only be added once to a map (i.e. the
    same sk cannot be added twice even to the same map).  SO_REUSEPORT
    already allows another sk to be created for the same IP:PORT.
    There is no need to re-create a similar usage in the BPF side.
    
    When a SO_REUSEPORT is deleted from the "reuse->socks[]" (e.g. "close()"),
    it will notify the bpf map to remove it from the map also.  It is
    done through "bpf_sk_reuseport_detach()" and it will only be called
    if >=1 of the "reuse->sock[]" has ever been added to a bpf map.
    
    The map_update()/map_delete() has to be in-sync with the
    "reuse->socks[]".  Hence, the same "reuseport_lock" used
    by "reuse->socks[]" has to be used here also. Care has
    been taken to ensure the lock is only acquired when the
    adding sk passes some strict tests. and
    freeing the map does not require the reuseport_lock.
    
    The reuseport_array will also support lookup from the syscall
    side.  It will return a sock_gen_cookie().  The sock_gen_cookie()
    is on-demand (i.e. a sk's cookie is not generated until the very
    first map_lookup_elem()).
    
    The lookup cookie is 64bits but it goes against the logical userspace
    expectation on 32bits sizeof(fd) (and as other fd based bpf maps do also).
    It may catch user in surprise if we enforce value_size=8 while
    userspace still pass a 32bits fd during update.  Supporting different
    value_size between lookup and update seems unintuitive also.
    
    We also need to consider what if other existing fd based maps want
    to return 64bits value from syscall's lookup in the future.
    Hence, reuseport_array supports both value_size 4 and 8, and
    assuming user will usually use value_size=4.  The syscall's lookup
    will return ENOSPC on value_size=4.  It will will only
    return 64bits value from sock_gen_cookie() when user consciously
    choose value_size=8 (as a signal that lookup is desired) which then
    requires a 64bits value in both lookup and update.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5af4e9e2722d..57f4d076141b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -684,6 +684,8 @@ static int map_lookup_elem(union bpf_attr *attr)
 		err = bpf_fd_array_map_lookup_elem(map, key, value);
 	} else if (IS_FD_HASH(map)) {
 		err = bpf_fd_htab_map_lookup_elem(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {
+		err = bpf_fd_reuseport_array_lookup_elem(map, key, value);
 	} else {
 		rcu_read_lock();
 		ptr = map->ops->map_lookup_elem(map, key);
@@ -790,6 +792,10 @@ static int map_update_elem(union bpf_attr *attr)
 		err = bpf_fd_htab_map_update_elem(map, f.file, key, value,
 						  attr->flags);
 		rcu_read_unlock();
+	} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {
+		/* rcu_read_lock() is not needed */
+		err = bpf_fd_reuseport_array_update_elem(map, key, value,
+							 attr->flags);
 	} else {
 		rcu_read_lock();
 		err = map->ops->map_update_elem(map, key, value, attr->flags);

commit de9cbbaadba5adf88a19e46df61f7054000838f6
Author: Roman Gushchin <guro@fb.com>
Date:   Thu Aug 2 14:27:18 2018 -0700

    bpf: introduce cgroup storage maps
    
    This commit introduces BPF_MAP_TYPE_CGROUP_STORAGE maps:
    a special type of maps which are implementing the cgroup storage.
    
    >From the userspace point of view it's almost a generic
    hash map with the (cgroup inode id, attachment type) pair
    used as a key.
    
    The only difference is that some operations are restricted:
      1) a user can't create new entries,
      2) a user can't remove existing entries.
    
    The lookup from userspace is o(log(n)).
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 7958252a4d29..5af4e9e2722d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -957,6 +957,9 @@ static void free_used_maps(struct bpf_prog_aux *aux)
 {
 	int i;
 
+	if (aux->cgroup_storage)
+		bpf_cgroup_storage_release(aux->prog, aux->cgroup_storage);
+
 	for (i = 0; i < aux->used_map_cnt; i++)
 		bpf_map_put(aux->used_maps[i]);
 

commit 0a4c58f5702858822621fa1177c7d3475f181ccb
Author: Roman Gushchin <guro@fb.com>
Date:   Thu Aug 2 14:27:17 2018 -0700

    bpf: add ability to charge bpf maps memory dynamically
    
    This commits extends existing bpf maps memory charging API
    to support dynamic charging/uncharging.
    
    This is required to account memory used by maps,
    if all entries are created dynamically after
    the map initialization.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index a31a1ba0f8ea..7958252a4d29 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -181,32 +181,60 @@ int bpf_map_precharge_memlock(u32 pages)
 	return 0;
 }
 
-static int bpf_map_charge_memlock(struct bpf_map *map)
+static int bpf_charge_memlock(struct user_struct *user, u32 pages)
 {
-	struct user_struct *user = get_current_user();
-	unsigned long memlock_limit;
+	unsigned long memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
 
-	memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+	if (atomic_long_add_return(pages, &user->locked_vm) > memlock_limit) {
+		atomic_long_sub(pages, &user->locked_vm);
+		return -EPERM;
+	}
+	return 0;
+}
 
-	atomic_long_add(map->pages, &user->locked_vm);
+static void bpf_uncharge_memlock(struct user_struct *user, u32 pages)
+{
+	atomic_long_sub(pages, &user->locked_vm);
+}
+
+static int bpf_map_init_memlock(struct bpf_map *map)
+{
+	struct user_struct *user = get_current_user();
+	int ret;
 
-	if (atomic_long_read(&user->locked_vm) > memlock_limit) {
-		atomic_long_sub(map->pages, &user->locked_vm);
+	ret = bpf_charge_memlock(user, map->pages);
+	if (ret) {
 		free_uid(user);
-		return -EPERM;
+		return ret;
 	}
 	map->user = user;
-	return 0;
+	return ret;
 }
 
-static void bpf_map_uncharge_memlock(struct bpf_map *map)
+static void bpf_map_release_memlock(struct bpf_map *map)
 {
 	struct user_struct *user = map->user;
-
-	atomic_long_sub(map->pages, &user->locked_vm);
+	bpf_uncharge_memlock(user, map->pages);
 	free_uid(user);
 }
 
+int bpf_map_charge_memlock(struct bpf_map *map, u32 pages)
+{
+	int ret;
+
+	ret = bpf_charge_memlock(map->user, pages);
+	if (ret)
+		return ret;
+	map->pages += pages;
+	return ret;
+}
+
+void bpf_map_uncharge_memlock(struct bpf_map *map, u32 pages)
+{
+	bpf_uncharge_memlock(map->user, pages);
+	map->pages -= pages;
+}
+
 static int bpf_map_alloc_id(struct bpf_map *map)
 {
 	int id;
@@ -256,7 +284,7 @@ static void bpf_map_free_deferred(struct work_struct *work)
 {
 	struct bpf_map *map = container_of(work, struct bpf_map, work);
 
-	bpf_map_uncharge_memlock(map);
+	bpf_map_release_memlock(map);
 	security_bpf_map_free(map);
 	/* implementation dependent freeing */
 	map->ops->map_free(map);
@@ -492,7 +520,7 @@ static int map_create(union bpf_attr *attr)
 	if (err)
 		goto free_map_nouncharge;
 
-	err = bpf_map_charge_memlock(map);
+	err = bpf_map_init_memlock(map);
 	if (err)
 		goto free_map_sec;
 
@@ -515,7 +543,7 @@ static int map_create(union bpf_attr *attr)
 	return err;
 
 free_map:
-	bpf_map_uncharge_memlock(map);
+	bpf_map_release_memlock(map);
 free_map_sec:
 	security_bpf_map_free(map);
 free_map_nouncharge:

commit 52b544bd386688177c41d53e748111c29d0ccc98
Merge: afed7bcf9487 9d3cce1e8b85
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Jul 17 09:27:43 2018 +0200

    Merge tag 'v4.18-rc5' into locking/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 99ba2b5aba24e022683a7db63204f9e306fe7ab9
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Thu Jul 5 08:50:04 2018 -0700

    bpf: sockhash, disallow bpf_tcp_close and update in parallel
    
    After latest lock updates there is no longer anything preventing a
    close and recvmsg call running in parallel. Additionally, we can
    race update with close if we close a socket and simultaneously update
    if via the BPF userspace API (note the cgroup ops are already run
    with sock_lock held).
    
    To resolve this take sock_lock in close and update paths.
    
    Reported-by: syzbot+b680e42077a0d7c9a0c4@syzkaller.appspotmail.com
    Fixes: e9db4ef6bf4c ("bpf: sockhash fix omitted bucket lock in sock_close")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index d10ecd78105f..a31a1ba0f8ea 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -735,7 +735,9 @@ static int map_update_elem(union bpf_attr *attr)
 	if (bpf_map_is_dev_bound(map)) {
 		err = bpf_map_offload_update_elem(map, key, value, attr->flags);
 		goto out;
-	} else if (map->map_type == BPF_MAP_TYPE_CPUMAP) {
+	} else if (map->map_type == BPF_MAP_TYPE_CPUMAP ||
+		   map->map_type == BPF_MAP_TYPE_SOCKHASH ||
+		   map->map_type == BPF_MAP_TYPE_SOCKMAP) {
 		err = map->ops->map_update_elem(map, key, value, attr->flags);
 		goto out;
 	}

commit fdb5c4531c1e0e50e609df83f736b6f3a02896e2
Author: Sean Young <sean@mess.org>
Date:   Tue Jun 19 00:04:24 2018 +0100

    bpf: fix attach type BPF_LIRC_MODE2 dependency wrt CONFIG_CGROUP_BPF
    
    If the kernel is compiled with CONFIG_CGROUP_BPF not enabled, it is not
    possible to attach, detach or query IR BPF programs to /dev/lircN devices,
    making them impossible to use. For embedded devices, it should be possible
    to use IR decoding without cgroups or CONFIG_CGROUP_BPF enabled.
    
    This change requires some refactoring, since bpf_prog_{attach,detach,query}
    functions are now always compiled, but their code paths for cgroups need
    moving out. Rather than a #ifdef CONFIG_CGROUP_BPF in kernel/bpf/syscall.c,
    moving them to kernel/bpf/cgroup.c and kernel/bpf/sockmap.c does not
    require #ifdefs since that is already conditionally compiled.
    
    Fixes: f4364dcfc86d ("media: rc: introduce BPF_PROG_LIRC_MODE2")
    Signed-off-by: Sean Young <sean@mess.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 35dc466641f2..d10ecd78105f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1483,8 +1483,6 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 	return err;
 }
 
-#ifdef CONFIG_CGROUP_BPF
-
 static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
 					     enum bpf_attach_type attach_type)
 {
@@ -1499,40 +1497,6 @@ static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
 
 #define BPF_PROG_ATTACH_LAST_FIELD attach_flags
 
-static int sockmap_get_from_fd(const union bpf_attr *attr,
-			       int type, bool attach)
-{
-	struct bpf_prog *prog = NULL;
-	int ufd = attr->target_fd;
-	struct bpf_map *map;
-	struct fd f;
-	int err;
-
-	f = fdget(ufd);
-	map = __bpf_map_get(f);
-	if (IS_ERR(map))
-		return PTR_ERR(map);
-
-	if (attach) {
-		prog = bpf_prog_get_type(attr->attach_bpf_fd, type);
-		if (IS_ERR(prog)) {
-			fdput(f);
-			return PTR_ERR(prog);
-		}
-	}
-
-	err = sock_map_prog(map, prog, attr->attach_type);
-	if (err) {
-		fdput(f);
-		if (prog)
-			bpf_prog_put(prog);
-		return err;
-	}
-
-	fdput(f);
-	return 0;
-}
-
 #define BPF_F_ATTACH_MASK \
 	(BPF_F_ALLOW_OVERRIDE | BPF_F_ALLOW_MULTI)
 
@@ -1540,7 +1504,6 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 {
 	enum bpf_prog_type ptype;
 	struct bpf_prog *prog;
-	struct cgroup *cgrp;
 	int ret;
 
 	if (!capable(CAP_NET_ADMIN))
@@ -1577,12 +1540,15 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 		ptype = BPF_PROG_TYPE_CGROUP_DEVICE;
 		break;
 	case BPF_SK_MSG_VERDICT:
-		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_MSG, true);
+		ptype = BPF_PROG_TYPE_SK_MSG;
+		break;
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
-		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_SKB, true);
+		ptype = BPF_PROG_TYPE_SK_SKB;
+		break;
 	case BPF_LIRC_MODE2:
-		return lirc_prog_attach(attr);
+		ptype = BPF_PROG_TYPE_LIRC_MODE2;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -1596,18 +1562,20 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 		return -EINVAL;
 	}
 
-	cgrp = cgroup_get_from_fd(attr->target_fd);
-	if (IS_ERR(cgrp)) {
-		bpf_prog_put(prog);
-		return PTR_ERR(cgrp);
+	switch (ptype) {
+	case BPF_PROG_TYPE_SK_SKB:
+	case BPF_PROG_TYPE_SK_MSG:
+		ret = sockmap_get_from_fd(attr, ptype, prog);
+		break;
+	case BPF_PROG_TYPE_LIRC_MODE2:
+		ret = lirc_prog_attach(attr, prog);
+		break;
+	default:
+		ret = cgroup_bpf_prog_attach(attr, ptype, prog);
 	}
 
-	ret = cgroup_bpf_attach(cgrp, prog, attr->attach_type,
-				attr->attach_flags);
 	if (ret)
 		bpf_prog_put(prog);
-	cgroup_put(cgrp);
-
 	return ret;
 }
 
@@ -1616,9 +1584,6 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 static int bpf_prog_detach(const union bpf_attr *attr)
 {
 	enum bpf_prog_type ptype;
-	struct bpf_prog *prog;
-	struct cgroup *cgrp;
-	int ret;
 
 	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
@@ -1651,29 +1616,17 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 		ptype = BPF_PROG_TYPE_CGROUP_DEVICE;
 		break;
 	case BPF_SK_MSG_VERDICT:
-		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_MSG, false);
+		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_MSG, NULL);
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
-		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_SKB, false);
+		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_SKB, NULL);
 	case BPF_LIRC_MODE2:
 		return lirc_prog_detach(attr);
 	default:
 		return -EINVAL;
 	}
 
-	cgrp = cgroup_get_from_fd(attr->target_fd);
-	if (IS_ERR(cgrp))
-		return PTR_ERR(cgrp);
-
-	prog = bpf_prog_get_type(attr->attach_bpf_fd, ptype);
-	if (IS_ERR(prog))
-		prog = NULL;
-
-	ret = cgroup_bpf_detach(cgrp, prog, attr->attach_type, 0);
-	if (prog)
-		bpf_prog_put(prog);
-	cgroup_put(cgrp);
-	return ret;
+	return cgroup_bpf_prog_detach(attr, ptype);
 }
 
 #define BPF_PROG_QUERY_LAST_FIELD query.prog_cnt
@@ -1681,9 +1634,6 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 static int bpf_prog_query(const union bpf_attr *attr,
 			  union bpf_attr __user *uattr)
 {
-	struct cgroup *cgrp;
-	int ret;
-
 	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
 	if (CHECK_ATTR(BPF_PROG_QUERY))
@@ -1711,14 +1661,9 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	default:
 		return -EINVAL;
 	}
-	cgrp = cgroup_get_from_fd(attr->query.target_fd);
-	if (IS_ERR(cgrp))
-		return PTR_ERR(cgrp);
-	ret = cgroup_bpf_query(cgrp, attr, uattr);
-	cgroup_put(cgrp);
-	return ret;
+
+	return cgroup_bpf_prog_query(attr, uattr);
 }
-#endif /* CONFIG_CGROUP_BPF */
 
 #define BPF_PROG_TEST_RUN_LAST_FIELD test.duration
 
@@ -2365,7 +2310,6 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_OBJ_GET:
 		err = bpf_obj_get(&attr);
 		break;
-#ifdef CONFIG_CGROUP_BPF
 	case BPF_PROG_ATTACH:
 		err = bpf_prog_attach(&attr);
 		break;
@@ -2375,7 +2319,6 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_PROG_QUERY:
 		err = bpf_prog_query(&attr, uattr);
 		break;
-#endif
 	case BPF_PROG_TEST_RUN:
 		err = bpf_prog_test_run(&attr, uattr);
 		break;

commit bfc18e389c7a09fbbbed6bf4032396685b14246e
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jun 21 13:13:04 2018 +0100

    atomics/treewide: Rename __atomic_add_unless() => atomic_fetch_add_unless()
    
    While __atomic_add_unless() was originally intended as a building-block
    for atomic_add_unless(), it's now used in a number of places around the
    kernel. It's the only common atomic operation named __atomic*(), rather
    than atomic_*(), and for consistency it would be better named
    atomic_fetch_add_unless().
    
    This lack of consistency is slightly confusing, and gets in the way of
    scripting atomics. Given that, let's clean things up and promote it to
    an official part of the atomics API, in the form of
    atomic_fetch_add_unless().
    
    This patch converts definitions and invocations over to the new name,
    including the instrumented version, using the following script:
    
      ----
      git grep -w __atomic_add_unless | while read line; do
      sed -i '{s/\<__atomic_add_unless\>/atomic_fetch_add_unless/}' "${line%%:*}";
      done
      git grep -w __arch_atomic_add_unless | while read line; do
      sed -i '{s/\<__arch_atomic_add_unless\>/arch_atomic_fetch_add_unless/}' "${line%%:*}";
      done
      ----
    
    Note that we do not have atomic{64,_long}_fetch_add_unless(), which will
    be introduced by later patches.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Palmer Dabbelt <palmer@sifive.com>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/lkml/20180621121321.4761-2-mark.rutland@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 35dc466641f2..f12db70d3bf3 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -575,7 +575,7 @@ static struct bpf_map *bpf_map_inc_not_zero(struct bpf_map *map,
 {
 	int refold;
 
-	refold = __atomic_add_unless(&map->refcnt, 1, 0);
+	refold = atomic_fetch_add_unless(&map->refcnt, 1, 0);
 
 	if (refold >= BPF_MAX_REFCNT) {
 		__bpf_map_put(map, false);
@@ -1142,7 +1142,7 @@ struct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)
 {
 	int refold;
 
-	refold = __atomic_add_unless(&prog->aux->refcnt, 1, 0);
+	refold = atomic_fetch_add_unless(&prog->aux->refcnt, 1, 0);
 
 	if (refold >= BPF_MAX_REFCNT) {
 		__bpf_prog_put(prog, false);

commit 9facc336876f7ecf9edba4c67b90426fde4ec898
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jun 15 02:30:48 2018 +0200

    bpf: reject any prog that failed read-only lock
    
    We currently lock any JITed image as read-only via bpf_jit_binary_lock_ro()
    as well as the BPF image as read-only through bpf_prog_lock_ro(). In
    the case any of these would fail we throw a WARN_ON_ONCE() in order to
    yell loudly to the log. Perhaps, to some extend, this may be comparable
    to an allocation where __GFP_NOWARN is explicitly not set.
    
    Added via 65869a47f348 ("bpf: improve read-only handling"), this behavior
    is slightly different compared to any of the other in-kernel set_memory_ro()
    users who do not check the return code of set_memory_ro() and friends /at
    all/ (e.g. in the case of module_enable_ro() / module_disable_ro()). Given
    in BPF this is mandatory hardening step, we want to know whether there
    are any issues that would leave both BPF data writable. So it happens
    that syzkaller enabled fault injection and it triggered memory allocation
    failure deep inside x86's change_page_attr_set_clr() which was triggered
    from set_memory_ro().
    
    Now, there are two options: i) leaving everything as is, and ii) reworking
    the image locking code in order to have a final checkpoint out of the
    central bpf_prog_select_runtime() which probes whether any of the calls
    during prog setup weren't successful, and then bailing out with an error.
    Option ii) is a better approach since this additional paranoia avoids
    altogether leaving any potential W+X pages from BPF side in the system.
    Therefore, lets be strict about it, and reject programs in such unlikely
    occasion. While testing I noticed also that one bpf_prog_lock_ro()
    call was missing on the outer dummy prog in case of calls, e.g. in the
    destructor we call bpf_prog_free_deferred() on the main prog where we
    try to bpf_prog_unlock_free() the program, and since we go via
    bpf_prog_select_runtime() do that as well.
    
    Reported-by: syzbot+3b889862e65a98317058@syzkaller.appspotmail.com
    Reported-by: syzbot+9e762b52dd17e616a7a5@syzkaller.appspotmail.com
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0f62692fe635..35dc466641f2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1353,9 +1353,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (err < 0)
 		goto free_used_maps;
 
-	/* eBPF program is ready to be JITed */
-	if (!prog->bpf_func)
-		prog = bpf_prog_select_runtime(prog, &err);
+	prog = bpf_prog_select_runtime(prog, &err);
 	if (err < 0)
 		goto free_used_maps;
 

commit 7d1982b4e335c1b184406b7566f6041bfe313c35
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jun 15 02:30:47 2018 +0200

    bpf: fix panic in prog load calls cleanup
    
    While testing I found that when hitting error path in bpf_prog_load()
    where we jump to free_used_maps and prog contained BPF to BPF calls
    that were JITed earlier, then we never clean up the bpf_prog_kallsyms_add()
    done under jit_subprogs(). Add proper API to make BPF kallsyms deletion
    more clear and fix that.
    
    Fixes: 1c2a088a6626 ("bpf: x64: add JIT support for multi-function programs")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0fa20624707f..0f62692fe635 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1034,14 +1034,9 @@ static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
-		int i;
-
 		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog, do_idr_lock);
-
-		for (i = 0; i < prog->aux->func_cnt; i++)
-			bpf_prog_kallsyms_del(prog->aux->func[i]);
-		bpf_prog_kallsyms_del(prog);
+		bpf_prog_kallsyms_del_all(prog);
 
 		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
 	}
@@ -1384,6 +1379,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 	return err;
 
 free_used_maps:
+	bpf_prog_kallsyms_del_subprogs(prog);
 	free_used_maps(prog->aux);
 free_prog:
 	bpf_prog_uncharge_memlock(prog);

commit 4316b40914ecde3738968225af56e650e8b61938
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Jun 2 23:06:34 2018 +0200

    bpf: show prog and map id in fdinfo
    
    Its trivial and straight forward to expose it for scripts that can
    then use it along with bpftool in order to inspect an individual
    application's used maps and progs. Right now we dump some basic
    information in the fdinfo file but with the help of the map/prog
    id full introspection becomes possible now.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 7365d79ae00d..0fa20624707f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -327,13 +327,15 @@ static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 		   "value_size:\t%u\n"
 		   "max_entries:\t%u\n"
 		   "map_flags:\t%#x\n"
-		   "memlock:\t%llu\n",
+		   "memlock:\t%llu\n"
+		   "map_id:\t%u\n",
 		   map->map_type,
 		   map->key_size,
 		   map->value_size,
 		   map->max_entries,
 		   map->map_flags,
-		   map->pages * 1ULL << PAGE_SHIFT);
+		   map->pages * 1ULL << PAGE_SHIFT,
+		   map->id);
 
 	if (owner_prog_type) {
 		seq_printf(m, "owner_prog_type:\t%u\n",
@@ -1070,11 +1072,13 @@ static void bpf_prog_show_fdinfo(struct seq_file *m, struct file *filp)
 		   "prog_type:\t%u\n"
 		   "prog_jited:\t%u\n"
 		   "prog_tag:\t%s\n"
-		   "memlock:\t%llu\n",
+		   "memlock:\t%llu\n"
+		   "prog_id:\t%u\n",
 		   prog->type,
 		   prog->jited,
 		   prog_tag,
-		   prog->pages * 1ULL << PAGE_SHIFT);
+		   prog->pages * 1ULL << PAGE_SHIFT,
+		   prog->aux->id);
 }
 #endif
 

commit f4364dcfc86df7c1ca47b256eaf6b6d0cdd0d936
Author: Sean Young <sean@mess.org>
Date:   Sun May 27 12:24:09 2018 +0100

    media: rc: introduce BPF_PROG_LIRC_MODE2
    
    Add support for BPF_PROG_LIRC_MODE2. This type of BPF program can call
    rc_keydown() to reported decoded IR scancodes, or rc_repeat() to report
    that the last key should be repeated.
    
    The bpf program can be attached to using the bpf(BPF_PROG_ATTACH) syscall;
    the target_fd must be the /dev/lircN device.
    
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Sean Young <sean@mess.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index e254526d6744..7365d79ae00d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -11,6 +11,7 @@
  */
 #include <linux/bpf.h>
 #include <linux/bpf_trace.h>
+#include <linux/bpf_lirc.h>
 #include <linux/btf.h>
 #include <linux/syscalls.h>
 #include <linux/slab.h>
@@ -1582,6 +1583,8 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
 		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_SKB, true);
+	case BPF_LIRC_MODE2:
+		return lirc_prog_attach(attr);
 	default:
 		return -EINVAL;
 	}
@@ -1654,6 +1657,8 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
 		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_SKB, false);
+	case BPF_LIRC_MODE2:
+		return lirc_prog_detach(attr);
 	default:
 		return -EINVAL;
 	}
@@ -1703,6 +1708,8 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_SOCK_OPS:
 	case BPF_CGROUP_DEVICE:
 		break;
+	case BPF_LIRC_MODE2:
+		return lirc_prog_query(attr, uattr);
 	default:
 		return -EINVAL;
 	}

commit 1cedee13d25ab118d325f95588c1a084e9317229
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri May 25 08:55:23 2018 -0700

    bpf: Hooks for sys_sendmsg
    
    In addition to already existing BPF hooks for sys_bind and sys_connect,
    the patch provides new hooks for sys_sendmsg.
    
    It leverages existing BPF program type `BPF_PROG_TYPE_CGROUP_SOCK_ADDR`
    that provides access to socket itlself (properties like family, type,
    protocol) and user-passed `struct sockaddr *` so that BPF program can
    override destination IP and port for system calls such as sendto(2) or
    sendmsg(2) and/or assign source IP to the socket.
    
    The hooks are implemented as two new attach types:
    `BPF_CGROUP_UDP4_SENDMSG` and `BPF_CGROUP_UDP6_SENDMSG` for UDPv4 and
    UDPv6 correspondingly.
    
    UDPv4 and UDPv6 separate attach types for same reason as sys_bind and
    sys_connect hooks, i.e. to prevent reading from / writing to e.g.
    user_ip6 fields when user passes sockaddr_in since it'd be out-of-bound.
    
    The difference with already existing hooks is sys_sendmsg are
    implemented only for unconnected UDP.
    
    For TCP it doesn't make sense to change user-provided `struct sockaddr *`
    at sendto(2)/sendmsg(2) time since socket either was already connected
    and has source/destination set or wasn't connected and call to
    sendto(2)/sendmsg(2) would lead to ENOTCONN anyway.
    
    Connected UDP is already handled by sys_connect hooks that can override
    source/destination at connect time and use fast-path later, i.e. these
    hooks don't affect UDP fast-path.
    
    Rewriting source IP is implemented differently than that in sys_connect
    hooks. When sys_sendmsg is used with unconnected UDP it doesn't work to
    just bind socket to desired local IP address since source IP can be set
    on per-packet basis by using ancillary data (cmsg(3)). So no matter if
    socket is bound or not, source IP has to be rewritten on every call to
    sys_sendmsg.
    
    To do so two new fields are added to UAPI `struct bpf_sock_addr`;
    * `msg_src_ip4` to set source IPv4 for UDPv4;
    * `msg_src_ip6` to set source IPv6 for UDPv6.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 388d4feda348..e254526d6744 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1249,6 +1249,8 @@ bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 		case BPF_CGROUP_INET6_BIND:
 		case BPF_CGROUP_INET4_CONNECT:
 		case BPF_CGROUP_INET6_CONNECT:
+		case BPF_CGROUP_UDP4_SENDMSG:
+		case BPF_CGROUP_UDP6_SENDMSG:
 			return 0;
 		default:
 			return -EINVAL;
@@ -1565,6 +1567,8 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_INET6_BIND:
 	case BPF_CGROUP_INET4_CONNECT:
 	case BPF_CGROUP_INET6_CONNECT:
+	case BPF_CGROUP_UDP4_SENDMSG:
+	case BPF_CGROUP_UDP6_SENDMSG:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
 		break;
 	case BPF_CGROUP_SOCK_OPS:
@@ -1635,6 +1639,8 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_CGROUP_INET6_BIND:
 	case BPF_CGROUP_INET4_CONNECT:
 	case BPF_CGROUP_INET6_CONNECT:
+	case BPF_CGROUP_UDP4_SENDMSG:
+	case BPF_CGROUP_UDP6_SENDMSG:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
 		break;
 	case BPF_CGROUP_SOCK_OPS:
@@ -1692,6 +1698,8 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_INET6_POST_BIND:
 	case BPF_CGROUP_INET4_CONNECT:
 	case BPF_CGROUP_INET6_CONNECT:
+	case BPF_CGROUP_UDP4_SENDMSG:
+	case BPF_CGROUP_UDP6_SENDMSG:
 	case BPF_CGROUP_SOCK_OPS:
 	case BPF_CGROUP_DEVICE:
 		break;

commit 41bdc4b40ed6fb26c6acc655ed9a243a348709c9
Author: Yonghong Song <yhs@fb.com>
Date:   Thu May 24 11:21:09 2018 -0700

    bpf: introduce bpf subcommand BPF_TASK_FD_QUERY
    
    Currently, suppose a userspace application has loaded a bpf program
    and attached it to a tracepoint/kprobe/uprobe, and a bpf
    introspection tool, e.g., bpftool, wants to show which bpf program
    is attached to which tracepoint/kprobe/uprobe. Such attachment
    information will be really useful to understand the overall bpf
    deployment in the system.
    
    There is a name field (16 bytes) for each program, which could
    be used to encode the attachment point. There are some drawbacks
    for this approaches. First, bpftool user (e.g., an admin) may not
    really understand the association between the name and the
    attachment point. Second, if one program is attached to multiple
    places, encoding a proper name which can imply all these
    attachments becomes difficult.
    
    This patch introduces a new bpf subcommand BPF_TASK_FD_QUERY.
    Given a pid and fd, if the <pid, fd> is associated with a
    tracepoint/kprobe/uprobe perf event, BPF_TASK_FD_QUERY will return
       . prog_id
       . tracepoint name, or
       . k[ret]probe funcname + offset or kernel addr, or
       . u[ret]probe filename + offset
    to the userspace.
    The user can use "bpftool prog" to find more information about
    bpf program itself with prog_id.
    
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 788456c18617..388d4feda348 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -18,7 +18,9 @@
 #include <linux/vmalloc.h>
 #include <linux/mmzone.h>
 #include <linux/anon_inodes.h>
+#include <linux/fdtable.h>
 #include <linux/file.h>
+#include <linux/fs.h>
 #include <linux/license.h>
 #include <linux/filter.h>
 #include <linux/version.h>
@@ -2178,6 +2180,132 @@ static int bpf_btf_get_fd_by_id(const union bpf_attr *attr)
 	return btf_get_fd_by_id(attr->btf_id);
 }
 
+static int bpf_task_fd_query_copy(const union bpf_attr *attr,
+				    union bpf_attr __user *uattr,
+				    u32 prog_id, u32 fd_type,
+				    const char *buf, u64 probe_offset,
+				    u64 probe_addr)
+{
+	char __user *ubuf = u64_to_user_ptr(attr->task_fd_query.buf);
+	u32 len = buf ? strlen(buf) : 0, input_len;
+	int err = 0;
+
+	if (put_user(len, &uattr->task_fd_query.buf_len))
+		return -EFAULT;
+	input_len = attr->task_fd_query.buf_len;
+	if (input_len && ubuf) {
+		if (!len) {
+			/* nothing to copy, just make ubuf NULL terminated */
+			char zero = '\0';
+
+			if (put_user(zero, ubuf))
+				return -EFAULT;
+		} else if (input_len >= len + 1) {
+			/* ubuf can hold the string with NULL terminator */
+			if (copy_to_user(ubuf, buf, len + 1))
+				return -EFAULT;
+		} else {
+			/* ubuf cannot hold the string with NULL terminator,
+			 * do a partial copy with NULL terminator.
+			 */
+			char zero = '\0';
+
+			err = -ENOSPC;
+			if (copy_to_user(ubuf, buf, input_len - 1))
+				return -EFAULT;
+			if (put_user(zero, ubuf + input_len - 1))
+				return -EFAULT;
+		}
+	}
+
+	if (put_user(prog_id, &uattr->task_fd_query.prog_id) ||
+	    put_user(fd_type, &uattr->task_fd_query.fd_type) ||
+	    put_user(probe_offset, &uattr->task_fd_query.probe_offset) ||
+	    put_user(probe_addr, &uattr->task_fd_query.probe_addr))
+		return -EFAULT;
+
+	return err;
+}
+
+#define BPF_TASK_FD_QUERY_LAST_FIELD task_fd_query.probe_addr
+
+static int bpf_task_fd_query(const union bpf_attr *attr,
+			     union bpf_attr __user *uattr)
+{
+	pid_t pid = attr->task_fd_query.pid;
+	u32 fd = attr->task_fd_query.fd;
+	const struct perf_event *event;
+	struct files_struct *files;
+	struct task_struct *task;
+	struct file *file;
+	int err;
+
+	if (CHECK_ATTR(BPF_TASK_FD_QUERY))
+		return -EINVAL;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (attr->task_fd_query.flags != 0)
+		return -EINVAL;
+
+	task = get_pid_task(find_vpid(pid), PIDTYPE_PID);
+	if (!task)
+		return -ENOENT;
+
+	files = get_files_struct(task);
+	put_task_struct(task);
+	if (!files)
+		return -ENOENT;
+
+	err = 0;
+	spin_lock(&files->file_lock);
+	file = fcheck_files(files, fd);
+	if (!file)
+		err = -EBADF;
+	else
+		get_file(file);
+	spin_unlock(&files->file_lock);
+	put_files_struct(files);
+
+	if (err)
+		goto out;
+
+	if (file->f_op == &bpf_raw_tp_fops) {
+		struct bpf_raw_tracepoint *raw_tp = file->private_data;
+		struct bpf_raw_event_map *btp = raw_tp->btp;
+
+		err = bpf_task_fd_query_copy(attr, uattr,
+					     raw_tp->prog->aux->id,
+					     BPF_FD_TYPE_RAW_TRACEPOINT,
+					     btp->tp->name, 0, 0);
+		goto put_file;
+	}
+
+	event = perf_get_event(file);
+	if (!IS_ERR(event)) {
+		u64 probe_offset, probe_addr;
+		u32 prog_id, fd_type;
+		const char *buf;
+
+		err = bpf_get_perf_event_info(event, &prog_id, &fd_type,
+					      &buf, &probe_offset,
+					      &probe_addr);
+		if (!err)
+			err = bpf_task_fd_query_copy(attr, uattr, prog_id,
+						     fd_type, buf,
+						     probe_offset,
+						     probe_addr);
+		goto put_file;
+	}
+
+	err = -ENOTSUPP;
+put_file:
+	fput(file);
+out:
+	return err;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -2264,6 +2392,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_BTF_GET_FD_BY_ID:
 		err = bpf_btf_get_fd_by_id(&attr);
 		break;
+	case BPF_TASK_FD_QUERY:
+		err = bpf_task_fd_query(&attr, uattr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit 815581c11cc29f74af252b6306ea1ec94160231a
Author: Sandipan Das <sandipan@linux.vnet.ibm.com>
Date:   Thu May 24 12:26:52 2018 +0530

    bpf: get JITed image lengths of functions via syscall
    
    This adds new two new fields to struct bpf_prog_info. For
    multi-function programs, these fields can be used to pass
    a list of the JITed image lengths of each function for a
    given program to userspace using the bpf system call with
    the BPF_OBJ_GET_INFO_BY_FD command.
    
    This can be used by userspace applications like bpftool
    to split up the contiguous JITed dump, also obtained via
    the system call, into more relatable chunks corresponding
    to each function.
    
    Signed-off-by: Sandipan Das <sandipan@linux.vnet.ibm.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c8e987a612b5..788456c18617 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2037,6 +2037,26 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		}
 	}
 
+	ulen = info.nr_jited_func_lens;
+	info.nr_jited_func_lens = prog->aux->func_cnt;
+	if (info.nr_jited_func_lens && ulen) {
+		if (bpf_dump_raw_ok()) {
+			u32 __user *user_lens;
+			u32 func_len, i;
+
+			/* copy the JITed image lengths for each function */
+			ulen = min_t(u32, info.nr_jited_func_lens, ulen);
+			user_lens = u64_to_user_ptr(info.jited_func_lens);
+			for (i = 0; i < ulen; i++) {
+				func_len = prog->aux->func[i]->jited_len;
+				if (put_user(func_len, &user_lens[i]))
+					return -EFAULT;
+			}
+		} else {
+			info.jited_func_lens = 0;
+		}
+	}
+
 done:
 	if (copy_to_user(uinfo, &info, info_len) ||
 	    put_user(info_len, &uattr->info.info_len))

commit 4d56a76ead2fcd856e677cdc9445ad331a409b8c
Author: Sandipan Das <sandipan@linux.vnet.ibm.com>
Date:   Thu May 24 12:26:51 2018 +0530

    bpf: fix multi-function JITed dump obtained via syscall
    
    Currently, for multi-function programs, we cannot get the JITed
    instructions using the bpf system call's BPF_OBJ_GET_INFO_BY_FD
    command. Because of this, userspace tools such as bpftool fail
    to identify a multi-function program as being JITed or not.
    
    With the JIT enabled and the test program running, this can be
    verified as follows:
    
      # cat /proc/sys/net/core/bpf_jit_enable
      1
    
    Before applying this patch:
    
      # bpftool prog list
      1: kprobe  name foo  tag b811aab41a39ad3d  gpl
              loaded_at 2018-05-16T11:43:38+0530  uid 0
              xlated 216B  not jited  memlock 65536B
      ...
    
      # bpftool prog dump jited id 1
      no instructions returned
    
    After applying this patch:
    
      # bpftool prog list
      1: kprobe  name foo  tag b811aab41a39ad3d  gpl
              loaded_at 2018-05-16T12:13:01+0530  uid 0
              xlated 216B  jited 308B  memlock 65536B
      ...
    
      # bpftool prog dump jited id 1
         0:   nop
         4:   nop
         8:   mflr    r0
         c:   std     r0,16(r1)
        10:   stdu    r1,-112(r1)
        14:   std     r31,104(r1)
        18:   addi    r31,r1,48
        1c:   li      r3,10
      ...
    
    Signed-off-by: Sandipan Das <sandipan@linux.vnet.ibm.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 068a4fc79ddb..c8e987a612b5 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1970,13 +1970,44 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	 * for offload.
 	 */
 	ulen = info.jited_prog_len;
-	info.jited_prog_len = prog->jited_len;
+	if (prog->aux->func_cnt) {
+		u32 i;
+
+		info.jited_prog_len = 0;
+		for (i = 0; i < prog->aux->func_cnt; i++)
+			info.jited_prog_len += prog->aux->func[i]->jited_len;
+	} else {
+		info.jited_prog_len = prog->jited_len;
+	}
+
 	if (info.jited_prog_len && ulen) {
 		if (bpf_dump_raw_ok()) {
 			uinsns = u64_to_user_ptr(info.jited_prog_insns);
 			ulen = min_t(u32, info.jited_prog_len, ulen);
-			if (copy_to_user(uinsns, prog->bpf_func, ulen))
-				return -EFAULT;
+
+			/* for multi-function programs, copy the JITed
+			 * instructions for all the functions
+			 */
+			if (prog->aux->func_cnt) {
+				u32 len, free, i;
+				u8 *img;
+
+				free = ulen;
+				for (i = 0; i < prog->aux->func_cnt; i++) {
+					len = prog->aux->func[i]->jited_len;
+					len = min_t(u32, len, free);
+					img = (u8 *) prog->aux->func[i]->bpf_func;
+					if (copy_to_user(uinsns, img, len))
+						return -EFAULT;
+					uinsns += len;
+					free -= len;
+					if (!free)
+						break;
+				}
+			} else {
+				if (copy_to_user(uinsns, prog->bpf_func, ulen))
+					return -EFAULT;
+			}
 		} else {
 			info.jited_prog_insns = 0;
 		}

commit dbecd7388476aedeb66389febea84d5450d28773
Author: Sandipan Das <sandipan@linux.vnet.ibm.com>
Date:   Thu May 24 12:26:48 2018 +0530

    bpf: get kernel symbol addresses via syscall
    
    This adds new two new fields to struct bpf_prog_info. For
    multi-function programs, these fields can be used to pass
    a list of kernel symbol addresses for all functions in a
    given program to userspace using the bpf system call with
    the BPF_OBJ_GET_INFO_BY_FD command.
    
    When bpf_jit_kallsyms is enabled, we can get the address
    of the corresponding kernel symbol for a callee function
    and resolve the symbol's name. The address is determined
    by adding the value of the call instruction's imm field
    to __bpf_call_base. This offset gets assigned to the imm
    field by the verifier.
    
    For some architectures, such as powerpc64, the imm field
    is not large enough to hold this offset.
    
    We resolve this by:
    
    [1] Assigning the subprog id to the imm field of a call
        instruction in the verifier instead of the offset of
        the callee's symbol's address from __bpf_call_base.
    
    [2] Determining the address of a callee's corresponding
        symbol by using the imm field as an index for the
        list of kernel symbol addresses now available from
        the program info.
    
    Suggested-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Sandipan Das <sandipan@linux.vnet.ibm.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0b4c94551001..068a4fc79ddb 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1933,6 +1933,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	if (!capable(CAP_SYS_ADMIN)) {
 		info.jited_prog_len = 0;
 		info.xlated_prog_len = 0;
+		info.nr_jited_ksyms = 0;
 		goto done;
 	}
 
@@ -1981,6 +1982,30 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		}
 	}
 
+	ulen = info.nr_jited_ksyms;
+	info.nr_jited_ksyms = prog->aux->func_cnt;
+	if (info.nr_jited_ksyms && ulen) {
+		if (bpf_dump_raw_ok()) {
+			u64 __user *user_ksyms;
+			ulong ksym_addr;
+			u32 i;
+
+			/* copy the address of the kernel symbol
+			 * corresponding to each function
+			 */
+			ulen = min_t(u32, info.nr_jited_ksyms, ulen);
+			user_ksyms = u64_to_user_ptr(info.jited_ksyms);
+			for (i = 0; i < ulen; i++) {
+				ksym_addr = (ulong) prog->aux->func[i]->bpf_func;
+				ksym_addr &= PAGE_MASK;
+				if (put_user((u64) ksym_addr, &user_ksyms[i]))
+					return -EFAULT;
+			}
+		} else {
+			info.jited_ksyms = 0;
+		}
+	}
+
 done:
 	if (copy_to_user(uinfo, &info, info_len) ||
 	    put_user(info_len, &uattr->info.info_len))

commit 9b2cf328b2eccf761537a06bef914d2a0700fba7
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Tue May 22 14:57:21 2018 -0700

    bpf: btf: Rename btf_key_id and btf_value_id in bpf_map_info
    
    In "struct bpf_map_info", the name "btf_id", "btf_key_id" and "btf_value_id"
    could cause confusion because the "id" of "btf_id" means the BPF obj id
    given to the BTF object while
    "btf_key_id" and "btf_value_id" means the BTF type id within
    that BTF object.
    
    To make it clear, btf_key_id and btf_value_id are
    renamed to btf_key_type_id and btf_value_type_id.
    
    Suggested-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2b29ef84ded3..0b4c94551001 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -422,7 +422,7 @@ static int bpf_obj_name_cpy(char *dst, const char *src)
 	return 0;
 }
 
-#define BPF_MAP_CREATE_LAST_FIELD btf_value_id
+#define BPF_MAP_CREATE_LAST_FIELD btf_value_type_id
 /* called via syscall */
 static int map_create(union bpf_attr *attr)
 {
@@ -457,10 +457,10 @@ static int map_create(union bpf_attr *attr)
 	atomic_set(&map->usercnt, 1);
 
 	if (bpf_map_support_seq_show(map) &&
-	    (attr->btf_key_id || attr->btf_value_id)) {
+	    (attr->btf_key_type_id || attr->btf_value_type_id)) {
 		struct btf *btf;
 
-		if (!attr->btf_key_id || !attr->btf_value_id) {
+		if (!attr->btf_key_type_id || !attr->btf_value_type_id) {
 			err = -EINVAL;
 			goto free_map_nouncharge;
 		}
@@ -471,16 +471,16 @@ static int map_create(union bpf_attr *attr)
 			goto free_map_nouncharge;
 		}
 
-		err = map->ops->map_check_btf(map, btf, attr->btf_key_id,
-					      attr->btf_value_id);
+		err = map->ops->map_check_btf(map, btf, attr->btf_key_type_id,
+					      attr->btf_value_type_id);
 		if (err) {
 			btf_put(btf);
 			goto free_map_nouncharge;
 		}
 
 		map->btf = btf;
-		map->btf_key_id = attr->btf_key_id;
-		map->btf_value_id = attr->btf_value_id;
+		map->btf_key_type_id = attr->btf_key_type_id;
+		map->btf_value_type_id = attr->btf_value_type_id;
 	}
 
 	err = security_bpf_map_alloc(map);
@@ -2013,8 +2013,8 @@ static int bpf_map_get_info_by_fd(struct bpf_map *map,
 
 	if (map->btf) {
 		info.btf_id = btf_id(map->btf);
-		info.btf_key_id = map->btf_key_id;
-		info.btf_value_id = map->btf_value_id;
+		info.btf_key_type_id = map->btf_key_type_id;
+		info.btf_value_type_id = map->btf_value_type_id;
 	}
 
 	if (bpf_map_is_dev_bound(map)) {

commit dcab51f19b291d5ee23724c51b0a3a597c16451a
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Tue May 22 15:03:31 2018 -0700

    bpf: Expose check_uarg_tail_zero()
    
    This patch exposes check_uarg_tail_zero() which will
    be reused by a later BTF patch.  Its name is changed to
    bpf_check_uarg_tail_zero().
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Yonghong Song <yhs@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index bfcde949c7f8..2b29ef84ded3 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -65,9 +65,9 @@ static const struct bpf_map_ops * const bpf_map_types[] = {
  * copy_from_user() call. However, this is not a concern since this function is
  * meant to be a future-proofing of bits.
  */
-static int check_uarg_tail_zero(void __user *uaddr,
-				size_t expected_size,
-				size_t actual_size)
+int bpf_check_uarg_tail_zero(void __user *uaddr,
+			     size_t expected_size,
+			     size_t actual_size)
 {
 	unsigned char __user *addr;
 	unsigned char __user *end;
@@ -1899,7 +1899,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	u32 ulen;
 	int err;
 
-	err = check_uarg_tail_zero(uinfo, sizeof(info), info_len);
+	err = bpf_check_uarg_tail_zero(uinfo, sizeof(info), info_len);
 	if (err)
 		return err;
 	info_len = min_t(u32, sizeof(info), info_len);
@@ -1998,7 +1998,7 @@ static int bpf_map_get_info_by_fd(struct bpf_map *map,
 	u32 info_len = attr->info.info_len;
 	int err;
 
-	err = check_uarg_tail_zero(uinfo, sizeof(info), info_len);
+	err = bpf_check_uarg_tail_zero(uinfo, sizeof(info), info_len);
 	if (err)
 		return err;
 	info_len = min_t(u32, sizeof(info), info_len);
@@ -2038,7 +2038,7 @@ static int bpf_btf_get_info_by_fd(struct btf *btf,
 	u32 info_len = attr->info.info_len;
 	int err;
 
-	err = check_uarg_tail_zero(uinfo, sizeof(*uinfo), info_len);
+	err = bpf_check_uarg_tail_zero(uinfo, sizeof(*uinfo), info_len);
 	if (err)
 		return err;
 
@@ -2110,7 +2110,7 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	if (sysctl_unprivileged_bpf_disabled && !capable(CAP_SYS_ADMIN))
 		return -EPERM;
 
-	err = check_uarg_tail_zero(uattr, sizeof(attr), size);
+	err = bpf_check_uarg_tail_zero(uattr, sizeof(attr), size);
 	if (err)
 		return err;
 	size = min_t(u32, size, sizeof(attr));

commit b9f672af148bf7a08a6031743156faffd58dbc7e
Merge: 8e725f7caafb e23afe5e7cba
Author: David S. Miller <davem@davemloft.net>
Date:   Wed May 16 22:47:11 2018 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2018-05-17
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    The main changes are:
    
    1) Provide a new BPF helper for doing a FIB and neighbor lookup
       in the kernel tables from an XDP or tc BPF program. The helper
       provides a fast-path for forwarding packets. The API supports
       IPv4, IPv6 and MPLS protocols, but currently IPv4 and IPv6 are
       implemented in this initial work, from David (Ahern).
    
    2) Just a tiny diff but huge feature enabled for nfp driver by
       extending the BPF offload beyond a pure host processing offload.
       Offloaded XDP programs are allowed to set the RX queue index and
       thus opening the door for defining a fully programmable RSS/n-tuple
       filter replacement. Once BPF decided on a queue already, the device
       data-path will skip the conventional RSS processing completely,
       from Jakub.
    
    3) The original sockmap implementation was array based similar to
       devmap. However unlike devmap where an ifindex has a 1:1 mapping
       into the map there are use cases with sockets that need to be
       referenced using longer keys. Hence, sockhash map is added reusing
       as much of the sockmap code as possible, from John.
    
    4) Introduce BTF ID. The ID is allocatd through an IDR similar as
       with BPF maps and progs. It also makes BTF accessible to user
       space via BPF_BTF_GET_FD_BY_ID and adds exposure of the BTF data
       through BPF_OBJ_GET_INFO_BY_FD, from Martin.
    
    5) Enable BPF stackmap with build_id also in NMI context. Due to the
       up_read() of current->mm->mmap_sem build_id cannot be parsed.
       This work defers the up_read() via a per-cpu irq_work so that
       at least limited support can be enabled, from Song.
    
    6) Various BPF JIT follow-up cleanups and fixups after the LD_ABS/LD_IND
       JIT conversion as well as implementation of an optimized 32/64 bit
       immediate load in the arm64 JIT that allows to reduce the number of
       emitted instructions; in case of tested real-world programs they
       were shrinking by three percent, from Daniel.
    
    7) Add ifindex parameter to the libbpf loader in order to enable
       BPF offload support. Right now only iproute2 can load offloaded
       BPF and this will also enable libbpf for direct integration into
       other applications, from David (Beckett).
    
    8) Convert the plain text documentation under Documentation/bpf/ into
       RST format since this is the appropriate standard the kernel is
       moving to for all documentation. Also add an overview README.rst,
       from Jesper.
    
    9) Add __printf verification attribute to the bpf_verifier_vlog()
       helper. Though it uses va_list we can still allow gcc to check
       the format string, from Mathieu.
    
    10) Fix a bash reference in the BPF selftest's Makefile. The '|& ...'
        is a bash 4.0+ feature which is not guaranteed to be available
        when calling out to shell, therefore use a more portable variant,
        from Joe.
    
    11) Fix a 64 bit division in xdp_umem_reg() by using div_u64()
        instead of relying on the gcc built-in, from Björn.
    
    12) Fix a sock hashmap kmalloc warning reported by syzbot when an
        overly large key size is used in hashmap then causing overflows
        in htab->elem_size. Reject bogus attr->key_size early in the
        sock_hash_alloc(), from Yonghong.
    
    13) Ensure in BPF selftests when urandom_read is being linked that
        --build-id is always enabled so that test_stacktrace_build_id[_nmi]
        won't be failing, from Alexei.
    
    14) Add bitsperlong.h as well as errno.h uapi headers into the tools
        header infrastructure which point to one of the arch specific
        uapi headers. This was needed in order to fix a build error on
        some systems for the BPF selftests, from Sirio.
    
    15) Allow for short options to be used in the xdp_monitor BPF sample
        code. And also a bpf.h tools uapi header sync in order to fix a
        selftest build failure. Both from Prashant.
    
    16) More formally clarify the meaning of ID in the direct packet access
        section of the BPF documentation, from Wang.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b2d6cee117f708d493c020f9f355297321507be7
Merge: b753a9faaf9a 4bc871984f7c
Author: David S. Miller <davem@davemloft.net>
Date:   Fri May 11 20:53:22 2018 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    The bpf syscall and selftests conflicts were trivial
    overlapping changes.
    
    The r8169 change involved moving the added mdelay from 'net' into a
    different function.
    
    A TLS close bug fix overlapped with the splitting of the TLS state
    into separate TX and RX parts.  I just expanded the tests in the bug
    fix from "ctx->conf == X" into "ctx->tx_conf == X && ctx->rx_conf
    == X".
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 62dab84c81a487d946a5fc37c6df541dd95cca38
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri May 4 14:49:52 2018 -0700

    bpf: btf: Add struct bpf_btf_info
    
    During BPF_OBJ_GET_INFO_BY_FD on a btf_fd, the current bpf_attr's
    info.info is directly filled with the BTF binary data.  It is
    not extensible.  In this case, we want to add BTF ID.
    
    This patch adds "struct bpf_btf_info" which has the BTF ID as
    one of its member.  The BTF binary data itself is exposed through
    the "btf" and "btf_size" members.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 31c4092da277..e2aeb5e89f44 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2021,6 +2021,21 @@ static int bpf_map_get_info_by_fd(struct bpf_map *map,
 	return 0;
 }
 
+static int bpf_btf_get_info_by_fd(struct btf *btf,
+				  const union bpf_attr *attr,
+				  union bpf_attr __user *uattr)
+{
+	struct bpf_btf_info __user *uinfo = u64_to_user_ptr(attr->info.info);
+	u32 info_len = attr->info.info_len;
+	int err;
+
+	err = check_uarg_tail_zero(uinfo, sizeof(*uinfo), info_len);
+	if (err)
+		return err;
+
+	return btf_get_info_by_fd(btf, attr, uattr);
+}
+
 #define BPF_OBJ_GET_INFO_BY_FD_LAST_FIELD info.info
 
 static int bpf_obj_get_info_by_fd(const union bpf_attr *attr,
@@ -2044,7 +2059,7 @@ static int bpf_obj_get_info_by_fd(const union bpf_attr *attr,
 		err = bpf_map_get_info_by_fd(f.file->private_data, attr,
 					     uattr);
 	else if (f.file->f_op == &btf_fops)
-		err = btf_get_info_by_fd(f.file->private_data, attr, uattr);
+		err = bpf_btf_get_info_by_fd(f.file->private_data, attr, uattr);
 	else
 		err = -EINVAL;
 

commit 78958fca7ead2f81b60a6827881c4866d1ed0c52
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri May 4 14:49:51 2018 -0700

    bpf: btf: Introduce BTF ID
    
    This patch gives an ID to each loaded BTF.  The ID is allocated by
    the idr like the existing prog-id and map-id.
    
    The bpf_put(map->btf) is moved to __bpf_map_put() so that the
    userspace can stop seeing the BTF ID ASAP when the last BTF
    refcnt is gone.
    
    It also makes BTF accessible from userspace through the
    1. new BPF_BTF_GET_FD_BY_ID command.  It is limited to CAP_SYS_ADMIN
       which is inline with the BPF_BTF_LOAD cmd and the existing
       BPF_[MAP|PROG]_GET_FD_BY_ID cmd.
    2. new btf_id (and btf_key_id + btf_value_id) in "struct bpf_map_info"
    
    Once the BTF ID handler is accessible from userspace, freeing a BTF
    object has to go through a rcu period.  The BPF_BTF_GET_FD_BY_ID cmd
    can then be done under a rcu_read_lock() instead of taking
    spin_lock.
    [Note: A similar rcu usage can be done to the existing
           bpf_prog_get_fd_by_id() in a follow up patch]
    
    When processing the BPF_BTF_GET_FD_BY_ID cmd,
    refcount_inc_not_zero() is needed because the BTF object
    could be already in the rcu dead row .  btf_get() is
    removed since its usage is currently limited to btf.c
    alone.  refcount_inc() is used directly instead.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 9b87198deea2..31c4092da277 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -252,7 +252,6 @@ static void bpf_map_free_deferred(struct work_struct *work)
 
 	bpf_map_uncharge_memlock(map);
 	security_bpf_map_free(map);
-	btf_put(map->btf);
 	/* implementation dependent freeing */
 	map->ops->map_free(map);
 }
@@ -273,6 +272,7 @@ static void __bpf_map_put(struct bpf_map *map, bool do_idr_lock)
 	if (atomic_dec_and_test(&map->refcnt)) {
 		/* bpf_map_free_id() must be called first */
 		bpf_map_free_id(map, do_idr_lock);
+		btf_put(map->btf);
 		INIT_WORK(&map->work, bpf_map_free_deferred);
 		schedule_work(&map->work);
 	}
@@ -2002,6 +2002,12 @@ static int bpf_map_get_info_by_fd(struct bpf_map *map,
 	info.map_flags = map->map_flags;
 	memcpy(info.name, map->name, sizeof(map->name));
 
+	if (map->btf) {
+		info.btf_id = btf_id(map->btf);
+		info.btf_key_id = map->btf_key_id;
+		info.btf_value_id = map->btf_value_id;
+	}
+
 	if (bpf_map_is_dev_bound(map)) {
 		err = bpf_map_offload_info_fill(&info, map);
 		if (err)
@@ -2059,6 +2065,19 @@ static int bpf_btf_load(const union bpf_attr *attr)
 	return btf_new_fd(attr);
 }
 
+#define BPF_BTF_GET_FD_BY_ID_LAST_FIELD btf_id
+
+static int bpf_btf_get_fd_by_id(const union bpf_attr *attr)
+{
+	if (CHECK_ATTR(BPF_BTF_GET_FD_BY_ID))
+		return -EINVAL;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	return btf_get_fd_by_id(attr->btf_id);
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -2142,6 +2161,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_BTF_LOAD:
 		err = bpf_btf_load(&attr);
 		break;
+	case BPF_BTF_GET_FD_BY_ID:
+		err = bpf_btf_get_fd_by_id(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit 630a4d3874a06aa9f9481cbfc688981aad7a834c
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu May 3 18:37:09 2018 -0700

    nfp: bpf: record offload neutral maps in the driver
    
    For asynchronous events originating from the device, like perf event
    output, we need to be able to make sure that objects being referred
    to by the FW message are valid on the host.  FW events can get queued
    and reordered.  Even if we had a FW message "barrier" we should still
    protect ourselves from bogus FW output.
    
    Add a reverse-mapping hash table and record in it all raw map pointers
    FW may refer to.  Only record neutral maps, i.e. perf event arrays.
    These are currently the only objects FW can refer to.  Use RCU protection
    on the read side, update side is under RTNL.
    
    Since program vs map destruction order is slightly painful for offload
    simply take an extra reference on all the recorded maps to make sure
    they don't disappear.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 263e13ede029..9b87198deea2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -282,6 +282,7 @@ void bpf_map_put(struct bpf_map *map)
 {
 	__bpf_map_put(map, true);
 }
+EXPORT_SYMBOL_GPL(bpf_map_put);
 
 void bpf_map_put_with_uref(struct bpf_map *map)
 {
@@ -543,6 +544,7 @@ struct bpf_map *bpf_map_inc(struct bpf_map *map, bool uref)
 		atomic_inc(&map->usercnt);
 	return map;
 }
+EXPORT_SYMBOL_GPL(bpf_map_inc);
 
 struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 {

commit d0f1a451e33d9ca834422622da30aa68daade56b
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri May 4 02:13:57 2018 +0200

    bpf: use array_index_nospec in find_prog_type
    
    Commit 9ef09e35e521 ("bpf: fix possible spectre-v1 in find_and_alloc_map()")
    converted find_and_alloc_map() over to use array_index_nospec() to sanitize
    map type that user space passes on map creation, and this patch does an
    analogous conversion for progs in find_prog_type() as it's also passed from
    user space when loading progs as attr->prog_type.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 8f434485abd2..016ef9025827 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -874,11 +874,17 @@ static const struct bpf_prog_ops * const bpf_prog_types[] = {
 
 static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
 {
-	if (type >= ARRAY_SIZE(bpf_prog_types) || !bpf_prog_types[type])
+	const struct bpf_prog_ops *ops;
+
+	if (type >= ARRAY_SIZE(bpf_prog_types))
+		return -EINVAL;
+	type = array_index_nospec(type, ARRAY_SIZE(bpf_prog_types));
+	ops = bpf_prog_types[type];
+	if (!ops)
 		return -EINVAL;
 
 	if (!bpf_prog_is_dev_bound(prog->aux))
-		prog->aux->ops = bpf_prog_types[type];
+		prog->aux->ops = ops;
 	else
 		prog->aux->ops = &bpf_offload_prog_ops;
 	prog->type = type;

commit 9ef09e35e521bf0df5325cc9cffa726a8f5f3c1b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu May 3 17:04:59 2018 +0100

    bpf: fix possible spectre-v1 in find_and_alloc_map()
    
    It's possible for userspace to control attr->map_type. Sanitize it when
    using it as an array index to prevent an out-of-bounds value being used
    under speculation.
    
    Found by smatch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: netdev@vger.kernel.org
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ebfe9f29dae8..8f434485abd2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -26,6 +26,7 @@
 #include <linux/cred.h>
 #include <linux/timekeeping.h>
 #include <linux/ctype.h>
+#include <linux/nospec.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY || \
 			   (map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
@@ -102,12 +103,14 @@ const struct bpf_map_ops bpf_map_offload_ops = {
 static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 {
 	const struct bpf_map_ops *ops;
+	u32 type = attr->map_type;
 	struct bpf_map *map;
 	int err;
 
-	if (attr->map_type >= ARRAY_SIZE(bpf_map_types))
+	if (type >= ARRAY_SIZE(bpf_map_types))
 		return ERR_PTR(-EINVAL);
-	ops = bpf_map_types[attr->map_type];
+	type = array_index_nospec(type, ARRAY_SIZE(bpf_map_types));
+	ops = bpf_map_types[type];
 	if (!ops)
 		return ERR_PTR(-EINVAL);
 
@@ -122,7 +125,7 @@ static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return map;
 	map->ops = ops;
-	map->map_type = attr->map_type;
+	map->map_type = type;
 	return map;
 }
 

commit 4d220ed0f8140c478ab7b0a14d96821da639b646
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Sat Apr 28 19:56:37 2018 -0700

    bpf: remove tracepoints from bpf core
    
    tracepoints to bpf core were added as a way to provide introspection
    to bpf programs and maps, but after some time it became clear that
    this approach is inadequate, so prog_id, map_id and corresponding
    get_next_id, get_fd_by_id, get_info_by_fd, prog_query APIs were
    introduced and fully adopted by bpftool and other applications.
    The tracepoints in bpf core started to rot and causing syzbot warnings:
    WARNING: CPU: 0 PID: 3008 at kernel/trace/trace_event_perf.c:274
    Kernel panic - not syncing: panic_on_warn set ...
    perf_trace_bpf_map_keyval+0x260/0xbd0 include/trace/events/bpf.h:228
    trace_bpf_map_update_elem include/trace/events/bpf.h:274 [inline]
    map_update_elem kernel/bpf/syscall.c:597 [inline]
    SYSC_bpf kernel/bpf/syscall.c:1478 [inline]
    Hence this patch deletes tracepoints in bpf core.
    
    Reported-by: Eric Biggers <ebiggers3@gmail.com>
    Reported-by: syzbot <bot+a9dbb3c3e64b62536a4bc5ee7bbd4ca627566188@syzkaller.appspotmail.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0bd2944eafb9..263e13ede029 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -503,7 +503,6 @@ static int map_create(union bpf_attr *attr)
 		return err;
 	}
 
-	trace_bpf_map_create(map, err);
 	return err;
 
 free_map:
@@ -663,7 +662,6 @@ static int map_lookup_elem(union bpf_attr *attr)
 	if (copy_to_user(uvalue, value, value_size) != 0)
 		goto free_value;
 
-	trace_bpf_map_lookup_elem(map, ufd, key, value);
 	err = 0;
 
 free_value:
@@ -760,8 +758,6 @@ static int map_update_elem(union bpf_attr *attr)
 	__this_cpu_dec(bpf_prog_active);
 	preempt_enable();
 out:
-	if (!err)
-		trace_bpf_map_update_elem(map, ufd, key, value);
 free_value:
 	kfree(value);
 free_key:
@@ -814,8 +810,6 @@ static int map_delete_elem(union bpf_attr *attr)
 	__this_cpu_dec(bpf_prog_active);
 	preempt_enable();
 out:
-	if (!err)
-		trace_bpf_map_delete_elem(map, ufd, key);
 	kfree(key);
 err_put:
 	fdput(f);
@@ -879,7 +873,6 @@ static int map_get_next_key(union bpf_attr *attr)
 	if (copy_to_user(unext_key, next_key, map->key_size) != 0)
 		goto free_next_key;
 
-	trace_bpf_map_next_key(map, ufd, key, next_key);
 	err = 0;
 
 free_next_key:
@@ -1027,7 +1020,6 @@ static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
 		int i;
 
-		trace_bpf_prog_put_rcu(prog);
 		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog, do_idr_lock);
 
@@ -1194,11 +1186,7 @@ struct bpf_prog *bpf_prog_get(u32 ufd)
 struct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,
 				       bool attach_drv)
 {
-	struct bpf_prog *prog = __bpf_prog_get(ufd, &type, attach_drv);
-
-	if (!IS_ERR(prog))
-		trace_bpf_prog_get_type(prog);
-	return prog;
+	return __bpf_prog_get(ufd, &type, attach_drv);
 }
 EXPORT_SYMBOL_GPL(bpf_prog_get_type_dev);
 
@@ -1373,7 +1361,6 @@ static int bpf_prog_load(union bpf_attr *attr)
 	}
 
 	bpf_prog_kallsyms_add(prog);
-	trace_bpf_prog_load(prog, err);
 	return err;
 
 free_used_maps:

commit 79741a38b4a2538a68342c45b813ecb9dd648ee8
Merge: cb586c63e3fc c0885f61bbb6
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Apr 26 21:19:50 2018 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2018-04-27
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    The main changes are:
    
    1) Add extensive BPF helper description into include/uapi/linux/bpf.h
       and a new script bpf_helpers_doc.py which allows for generating a
       man page out of it. Thus, every helper in BPF now comes with proper
       function signature, detailed description and return code explanation,
       from Quentin.
    
    2) Migrate the BPF collect metadata tunnel tests from BPF samples over
       to the BPF selftests and further extend them with v6 vxlan, geneve
       and ipip tests, simplify the ipip tests, improve documentation and
       convert to bpf_ntoh*() / bpf_hton*() api, from William.
    
    3) Currently, helpers that expect ARG_PTR_TO_MAP_{KEY,VALUE} can only
       access stack and packet memory. Extend this to allow such helpers
       to also use map values, which enabled use cases where value from
       a first lookup can be directly used as a key for a second lookup,
       from Paul.
    
    4) Add a new helper bpf_skb_get_xfrm_state() for tc BPF programs in
       order to retrieve XFRM state information containing SPI, peer
       address and reqid values, from Eyal.
    
    5) Various optimizations in nfp driver's BPF JIT in order to turn ADD
       and SUB instructions with negative immediate into the opposite
       operation with a positive immediate such that nfp can better fit
       small immediates into instructions. Savings in instruction count
       up to 4% have been observed, from Jakub.
    
    6) Add the BPF prog's gpl_compatible flag to struct bpf_prog_info
       and add support for dumping this through bpftool, from Jiri.
    
    7) Move the BPF sockmap samples over into BPF selftests instead since
       sockmap was rather a series of tests than sample anyway and this way
       this can be run from automated bots, from John.
    
    8) Follow-up fix for bpf_adjust_tail() helper in order to make it work
       with generic XDP, from Nikita.
    
    9) Some follow-up cleanups to BTF, namely, removing unused defines from
       BTF uapi header and renaming 'name' struct btf_* members into name_off
       to make it more clear they are offsets into string section, from Martin.
    
    10) Remove test_sock_addr from TEST_GEN_PROGS in BPF selftests since
        not run directly but invoked from test_sock_addr.sh, from Yonghong.
    
    11) Remove redundant ret assignment in sample BPF loader, from Wang.
    
    12) Add couple of missing files to BPF selftest's gitignore, from Anders.
    
    There are two trivial merge conflicts while pulling:
    
      1) Remove samples/sockmap/Makefile since all sockmap tests have been
         moved to selftests.
      2) Add both hunks from tools/testing/selftests/bpf/.gitignore to the
         file since git should ignore all of them.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b85fab0e67b162014cd328cb4e2a8e8ae382cb8a
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Wed Apr 25 19:41:06 2018 +0200

    bpf: Add gpl_compatible flag to struct bpf_prog_info
    
    Adding gpl_compatible flag to struct bpf_prog_info
    so it can be dumped via bpf_prog_get_info_by_fd and
    displayed via bpftool progs dump.
    
    Alexei noticed 4-byte hole in struct bpf_prog_info,
    so we put the u32 flags field in there, and we can
    keep adding bit fields in there without breaking
    user space.
    
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index fe23dc5a3ec4..7bb4ff1c770a 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1914,6 +1914,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	info.load_time = prog->aux->load_time;
 	info.created_by_uid = from_kuid_munged(current_user_ns(),
 					       prog->aux->user->uid);
+	info.gpl_compatible = prog->gpl_compatible;
 
 	memcpy(info.tag, prog->tag, sizeof(prog->tag));
 	memcpy(info.name, prog->aux->name, sizeof(prog->aux->name));

commit a9537c937c1790b62de5e2789e67c0bf439b11ea
Merge: e9350d443584 25eb0ea7174c
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Apr 25 23:04:22 2018 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Merging net into net-next to help the bpf folks avoid
    some really ugly merge conflicts.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ba6b8de423f8d0dee48d6030288ed81c03ddf9f0
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Apr 23 15:39:23 2018 -0700

    bpf: sockmap, map_release does not hold refcnt for pinned maps
    
    Relying on map_release hook to decrement the reference counts when a
    map is removed only works if the map is not being pinned. In the
    pinned case the ref is decremented immediately and the BPF programs
    released. After this BPF programs may not be in-use which is not
    what the user would expect.
    
    This patch moves the release logic into bpf_map_put_uref() and brings
    sockmap in-line with how a similar case is handled in prog array maps.
    
    Fixes: 3d9e952697de ("bpf: sockmap, fix leaking maps with attached but not detached progs")
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4ca46df19c9a..ebfe9f29dae8 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -257,8 +257,8 @@ static void bpf_map_free_deferred(struct work_struct *work)
 static void bpf_map_put_uref(struct bpf_map *map)
 {
 	if (atomic_dec_and_test(&map->usercnt)) {
-		if (map->map_type == BPF_MAP_TYPE_PROG_ARRAY)
-			bpf_fd_array_map_clear(map);
+		if (map->ops->map_release_uref)
+			map->ops->map_release_uref(map);
 	}
 }
 

commit a26ca7c982cb576749cbdd01e8ecde4bf010d60a
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Apr 18 15:56:03 2018 -0700

    bpf: btf: Add pretty print support to the basic arraymap
    
    This patch adds pretty print support to the basic arraymap.
    Support for other bpf maps can be added later.
    
    This patch adds new attrs to the BPF_MAP_CREATE command to allow
    specifying the btf_fd, btf_key_id and btf_value_id.  The
    BPF_MAP_CREATE can then associate the btf to the map if
    the creating map supports BTF.
    
    A BTF supported map needs to implement two new map ops,
    map_seq_show_elem() and map_check_btf().  This patch has
    implemented these new map ops for the basic arraymap.
    
    It also adds file_operations, bpffs_map_fops, to the pinned
    map such that the pinned map can be opened and read.
    After that, the user has an intuitive way to do
    "cat bpffs/pathto/a-pinned-map" instead of getting
    an error.
    
    bpffs_map_fops should not be extended further to support
    other operations.  Other operations (e.g. write/key-lookup...)
    should be realized by the userspace tools (e.g. bpftool) through
    the BPF_OBJ_GET_INFO_BY_FD, map's lookup/update interface...etc.
    Follow up patches will allow the userspace to obtain
    the BTF from a map-fd.
    
    Here is a sample output when reading a pinned arraymap
    with the following map's value:
    
    struct map_value {
            int count_a;
            int count_b;
    };
    
    cat /sys/fs/bpf/pinned_array_map:
    
    0: {1,2}
    1: {3,4}
    2: {5,6}
    ...
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0a4924a0a8da..fe23dc5a3ec4 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -27,6 +27,7 @@
 #include <linux/cred.h>
 #include <linux/timekeeping.h>
 #include <linux/ctype.h>
+#include <linux/btf.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY || \
 			   (map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
@@ -251,6 +252,7 @@ static void bpf_map_free_deferred(struct work_struct *work)
 
 	bpf_map_uncharge_memlock(map);
 	security_bpf_map_free(map);
+	btf_put(map->btf);
 	/* implementation dependent freeing */
 	map->ops->map_free(map);
 }
@@ -416,7 +418,7 @@ static int bpf_obj_name_cpy(char *dst, const char *src)
 	return 0;
 }
 
-#define BPF_MAP_CREATE_LAST_FIELD map_ifindex
+#define BPF_MAP_CREATE_LAST_FIELD btf_value_id
 /* called via syscall */
 static int map_create(union bpf_attr *attr)
 {
@@ -450,6 +452,33 @@ static int map_create(union bpf_attr *attr)
 	atomic_set(&map->refcnt, 1);
 	atomic_set(&map->usercnt, 1);
 
+	if (bpf_map_support_seq_show(map) &&
+	    (attr->btf_key_id || attr->btf_value_id)) {
+		struct btf *btf;
+
+		if (!attr->btf_key_id || !attr->btf_value_id) {
+			err = -EINVAL;
+			goto free_map_nouncharge;
+		}
+
+		btf = btf_get_by_fd(attr->btf_fd);
+		if (IS_ERR(btf)) {
+			err = PTR_ERR(btf);
+			goto free_map_nouncharge;
+		}
+
+		err = map->ops->map_check_btf(map, btf, attr->btf_key_id,
+					      attr->btf_value_id);
+		if (err) {
+			btf_put(btf);
+			goto free_map_nouncharge;
+		}
+
+		map->btf = btf;
+		map->btf_key_id = attr->btf_key_id;
+		map->btf_value_id = attr->btf_value_id;
+	}
+
 	err = security_bpf_map_alloc(map);
 	if (err)
 		goto free_map_nouncharge;
@@ -482,6 +511,7 @@ static int map_create(union bpf_attr *attr)
 free_map_sec:
 	security_bpf_map_free(map);
 free_map_nouncharge:
+	btf_put(map->btf);
 	map->ops->map_free(map);
 	return err;
 }

commit 60197cfb6e11ffc03aa0ed23765b2f7e70b2e2d4
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Apr 18 15:56:02 2018 -0700

    bpf: btf: Add BPF_OBJ_GET_INFO_BY_FD support to BTF fd
    
    This patch adds BPF_OBJ_GET_INFO_BY_FD support to BTF fd.
    The original BTF data, which was used to create the BTF fd during
    the earlier BPF_BTF_LOAD call, will be returned.
    
    The userspace is expected to allocate buffer
    to info.info and the buffer size is set to info.info_len before
    calling BPF_OBJ_GET_INFO_BY_FD.
    
    The original BTF data is copied to the userspace buffer (info.info).
    Only upto the user's specified info.info_len will be copied.
    
    The original BTF data size is set to info.info_len.  The userspace
    needs to check if it is bigger than its allocated buffer size.
    If it is, the userspace should realloc with the kernel-returned
    info.info_len and call the BPF_OBJ_GET_INFO_BY_FD again.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index cd8ebadc66eb..0a4924a0a8da 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2017,6 +2017,8 @@ static int bpf_obj_get_info_by_fd(const union bpf_attr *attr,
 	else if (f.file->f_op == &bpf_map_fops)
 		err = bpf_map_get_info_by_fd(f.file->private_data, attr,
 					     uattr);
+	else if (f.file->f_op == &btf_fops)
+		err = btf_get_info_by_fd(f.file->private_data, attr, uattr);
 	else
 		err = -EINVAL;
 

commit f56a653c1fd13a197076dec4461c656fd2adec73
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Apr 18 15:56:01 2018 -0700

    bpf: btf: Add BPF_BTF_LOAD command
    
    This patch adds a BPF_BTF_LOAD command which
    1) loads and verifies the BTF (implemented in earlier patches)
    2) returns a BTF fd to userspace.  In the next patch, the
       BTF fd can be specified during BPF_MAP_CREATE.
    
    It currently limits to CAP_SYS_ADMIN.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4ca46df19c9a..cd8ebadc66eb 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -11,6 +11,7 @@
  */
 #include <linux/bpf.h>
 #include <linux/bpf_trace.h>
+#include <linux/btf.h>
 #include <linux/syscalls.h>
 #include <linux/slab.h>
 #include <linux/sched/signal.h>
@@ -2023,6 +2024,19 @@ static int bpf_obj_get_info_by_fd(const union bpf_attr *attr,
 	return err;
 }
 
+#define BPF_BTF_LOAD_LAST_FIELD btf_log_level
+
+static int bpf_btf_load(const union bpf_attr *attr)
+{
+	if (CHECK_ATTR(BPF_BTF_LOAD))
+		return -EINVAL;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	return btf_new_fd(attr);
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -2103,6 +2117,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_RAW_TRACEPOINT_OPEN:
 		err = bpf_raw_tracepoint_open(&attr);
 		break;
+	case BPF_BTF_LOAD:
+		err = bpf_btf_load(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit 33491588c1fb2c76ed114a211ad0ee76c16b5a0c
Author: Anders Roxell <anders.roxell@linaro.org>
Date:   Tue Apr 3 14:09:47 2018 +0200

    kernel/bpf/syscall: fix warning defined but not used
    
    There will be a build warning -Wunused-function if CONFIG_CGROUP_BPF
    isn't defined, since the only user is inside #ifdef CONFIG_CGROUP_BPF:
    kernel/bpf/syscall.c:1229:12: warning: ‘bpf_prog_attach_check_attach_type’
        defined but not used [-Wunused-function]
     static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
    Current code moves function bpf_prog_attach_check_attach_type inside
    ifdef CONFIG_CGROUP_BPF.
    
    Fixes: 5e43f899b03a ("bpf: Check attach type at prog load time")
    Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0244973ee544..4ca46df19c9a 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1226,18 +1226,6 @@ bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 	}
 }
 
-static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
-					     enum bpf_attach_type attach_type)
-{
-	switch (prog->type) {
-	case BPF_PROG_TYPE_CGROUP_SOCK:
-	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
-		return attach_type == prog->expected_attach_type ? 0 : -EINVAL;
-	default:
-		return 0;
-	}
-}
-
 /* last field in 'union bpf_attr' used by this command */
 #define	BPF_PROG_LOAD_LAST_FIELD expected_attach_type
 
@@ -1465,6 +1453,18 @@ static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
 
 #ifdef CONFIG_CGROUP_BPF
 
+static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
+					     enum bpf_attach_type attach_type)
+{
+	switch (prog->type) {
+	case BPF_PROG_TYPE_CGROUP_SOCK:
+	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
+		return attach_type == prog->expected_attach_type ? 0 : -EINVAL;
+	default:
+		return 0;
+	}
+}
+
 #define BPF_PROG_ATTACH_LAST_FIELD attach_flags
 
 static int sockmap_get_from_fd(const union bpf_attr *attr,

commit aac3fc320d9404f2665a8b1249dc3170d5fa3caf
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri Mar 30 15:08:07 2018 -0700

    bpf: Post-hooks for sys_bind
    
    "Post-hooks" are hooks that are called right before returning from
    sys_bind. At this time IP and port are already allocated and no further
    changes to `struct sock` can happen before returning from sys_bind but
    BPF program has a chance to inspect the socket and change sys_bind
    result.
    
    Specifically it can e.g. inspect what port was allocated and if it
    doesn't satisfy some policy, BPF program can force sys_bind to fail and
    return EPERM to user.
    
    Another example of usage is recording the IP:port pair to some map to
    use it in later calls to sys_connect. E.g. if some TCP server inside
    cgroup was bound to some IP:port_n, it can be recorded to a map. And
    later when some TCP client inside same cgroup is trying to connect to
    127.0.0.1:port_n, BPF hook for sys_connect can override the destination
    and connect application to IP:port_n instead of 127.0.0.1:port_n. That
    helps forcing all applications inside a cgroup to use desired IP and not
    break those applications if they e.g. use localhost to communicate
    between each other.
    
    == Implementation details ==
    
    Post-hooks are implemented as two new attach types
    `BPF_CGROUP_INET4_POST_BIND` and `BPF_CGROUP_INET6_POST_BIND` for
    existing prog type `BPF_PROG_TYPE_CGROUP_SOCK`.
    
    Separate attach types for IPv4 and IPv6 are introduced to avoid access
    to IPv6 field in `struct sock` from `inet_bind()` and to IPv4 field from
    `inet6_bind()` since those fields might not make sense in such cases.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index cf1b29bc0ab8..0244973ee544 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1171,11 +1171,46 @@ struct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,
 }
 EXPORT_SYMBOL_GPL(bpf_prog_get_type_dev);
 
+/* Initially all BPF programs could be loaded w/o specifying
+ * expected_attach_type. Later for some of them specifying expected_attach_type
+ * at load time became required so that program could be validated properly.
+ * Programs of types that are allowed to be loaded both w/ and w/o (for
+ * backward compatibility) expected_attach_type, should have the default attach
+ * type assigned to expected_attach_type for the latter case, so that it can be
+ * validated later at attach time.
+ *
+ * bpf_prog_load_fixup_attach_type() sets expected_attach_type in @attr if
+ * prog type requires it but has some attach types that have to be backward
+ * compatible.
+ */
+static void bpf_prog_load_fixup_attach_type(union bpf_attr *attr)
+{
+	switch (attr->prog_type) {
+	case BPF_PROG_TYPE_CGROUP_SOCK:
+		/* Unfortunately BPF_ATTACH_TYPE_UNSPEC enumeration doesn't
+		 * exist so checking for non-zero is the way to go here.
+		 */
+		if (!attr->expected_attach_type)
+			attr->expected_attach_type =
+				BPF_CGROUP_INET_SOCK_CREATE;
+		break;
+	}
+}
+
 static int
 bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 				enum bpf_attach_type expected_attach_type)
 {
 	switch (prog_type) {
+	case BPF_PROG_TYPE_CGROUP_SOCK:
+		switch (expected_attach_type) {
+		case BPF_CGROUP_INET_SOCK_CREATE:
+		case BPF_CGROUP_INET4_POST_BIND:
+		case BPF_CGROUP_INET6_POST_BIND:
+			return 0;
+		default:
+			return -EINVAL;
+		}
 	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
 		switch (expected_attach_type) {
 		case BPF_CGROUP_INET4_BIND:
@@ -1195,6 +1230,7 @@ static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
 					     enum bpf_attach_type attach_type)
 {
 	switch (prog->type) {
+	case BPF_PROG_TYPE_CGROUP_SOCK:
 	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
 		return attach_type == prog->expected_attach_type ? 0 : -EINVAL;
 	default:
@@ -1240,6 +1276,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 	    !capable(CAP_SYS_ADMIN))
 		return -EPERM;
 
+	bpf_prog_load_fixup_attach_type(attr);
 	if (bpf_prog_load_check_attach_type(type, attr->expected_attach_type))
 		return -EINVAL;
 
@@ -1489,6 +1526,8 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 		ptype = BPF_PROG_TYPE_CGROUP_SKB;
 		break;
 	case BPF_CGROUP_INET_SOCK_CREATE:
+	case BPF_CGROUP_INET4_POST_BIND:
+	case BPF_CGROUP_INET6_POST_BIND:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK;
 		break;
 	case BPF_CGROUP_INET4_BIND:
@@ -1557,6 +1596,8 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 		ptype = BPF_PROG_TYPE_CGROUP_SKB;
 		break;
 	case BPF_CGROUP_INET_SOCK_CREATE:
+	case BPF_CGROUP_INET4_POST_BIND:
+	case BPF_CGROUP_INET6_POST_BIND:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK;
 		break;
 	case BPF_CGROUP_INET4_BIND:
@@ -1616,6 +1657,8 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_INET_SOCK_CREATE:
 	case BPF_CGROUP_INET4_BIND:
 	case BPF_CGROUP_INET6_BIND:
+	case BPF_CGROUP_INET4_POST_BIND:
+	case BPF_CGROUP_INET6_POST_BIND:
 	case BPF_CGROUP_INET4_CONNECT:
 	case BPF_CGROUP_INET6_CONNECT:
 	case BPF_CGROUP_SOCK_OPS:

commit d74bad4e74ee373787a9ae24197c17b7cdc428d5
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri Mar 30 15:08:05 2018 -0700

    bpf: Hooks for sys_connect
    
    == The problem ==
    
    See description of the problem in the initial patch of this patch set.
    
    == The solution ==
    
    The patch provides much more reliable in-kernel solution for the 2nd
    part of the problem: making outgoing connecttion from desired IP.
    
    It adds new attach types `BPF_CGROUP_INET4_CONNECT` and
    `BPF_CGROUP_INET6_CONNECT` for program type
    `BPF_PROG_TYPE_CGROUP_SOCK_ADDR` that can be used to override both
    source and destination of a connection at connect(2) time.
    
    Local end of connection can be bound to desired IP using newly
    introduced BPF-helper `bpf_bind()`. It allows to bind to only IP though,
    and doesn't support binding to port, i.e. leverages
    `IP_BIND_ADDRESS_NO_PORT` socket option. There are two reasons for this:
    * looking for a free port is expensive and can affect performance
      significantly;
    * there is no use-case for port.
    
    As for remote end (`struct sockaddr *` passed by user), both parts of it
    can be overridden, remote IP and remote port. It's useful if an
    application inside cgroup wants to connect to another application inside
    same cgroup or to itself, but knows nothing about IP assigned to the
    cgroup.
    
    Support is added for IPv4 and IPv6, for TCP and UDP.
    
    IPv4 and IPv6 have separate attach types for same reason as sys_bind
    hooks, i.e. to prevent reading from / writing to e.g. user_ip6 fields
    when user passes sockaddr_in since it'd be out-of-bound.
    
    == Implementation notes ==
    
    The patch introduces new field in `struct proto`: `pre_connect` that is
    a pointer to a function with same signature as `connect` but is called
    before it. The reason is in some cases BPF hooks should be called way
    before control is passed to `sk->sk_prot->connect`. Specifically
    `inet_dgram_connect` autobinds socket before calling
    `sk->sk_prot->connect` and there is no way to call `bpf_bind()` from
    hooks from e.g. `ip4_datagram_connect` or `ip6_datagram_connect` since
    it'd cause double-bind. On the other hand `proto.pre_connect` provides a
    flexible way to add BPF hooks for connect only for necessary `proto` and
    call them at desired time before `connect`. Since `bpf_bind()` is
    allowed to bind only to IP and autobind in `inet_dgram_connect` binds
    only port there is no chance of double-bind.
    
    bpf_bind() sets `force_bind_address_no_port` to bind to only IP despite
    of value of `bind_address_no_port` socket field.
    
    bpf_bind() sets `with_lock` to `false` when calling to __inet_bind()
    and __inet6_bind() since all call-sites, where bpf_bind() is called,
    already hold socket lock.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2cad66a4cacb..cf1b29bc0ab8 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1180,6 +1180,8 @@ bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 		switch (expected_attach_type) {
 		case BPF_CGROUP_INET4_BIND:
 		case BPF_CGROUP_INET6_BIND:
+		case BPF_CGROUP_INET4_CONNECT:
+		case BPF_CGROUP_INET6_CONNECT:
 			return 0;
 		default:
 			return -EINVAL;
@@ -1491,6 +1493,8 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 		break;
 	case BPF_CGROUP_INET4_BIND:
 	case BPF_CGROUP_INET6_BIND:
+	case BPF_CGROUP_INET4_CONNECT:
+	case BPF_CGROUP_INET6_CONNECT:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
 		break;
 	case BPF_CGROUP_SOCK_OPS:
@@ -1557,6 +1561,8 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 		break;
 	case BPF_CGROUP_INET4_BIND:
 	case BPF_CGROUP_INET6_BIND:
+	case BPF_CGROUP_INET4_CONNECT:
+	case BPF_CGROUP_INET6_CONNECT:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
 		break;
 	case BPF_CGROUP_SOCK_OPS:
@@ -1610,6 +1616,8 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_INET_SOCK_CREATE:
 	case BPF_CGROUP_INET4_BIND:
 	case BPF_CGROUP_INET6_BIND:
+	case BPF_CGROUP_INET4_CONNECT:
+	case BPF_CGROUP_INET6_CONNECT:
 	case BPF_CGROUP_SOCK_OPS:
 	case BPF_CGROUP_DEVICE:
 		break;

commit 4fbac77d2d092b475dda9eea66da674369665427
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri Mar 30 15:08:02 2018 -0700

    bpf: Hooks for sys_bind
    
    == The problem ==
    
    There is a use-case when all processes inside a cgroup should use one
    single IP address on a host that has multiple IP configured.  Those
    processes should use the IP for both ingress and egress, for TCP and UDP
    traffic. So TCP/UDP servers should be bound to that IP to accept
    incoming connections on it, and TCP/UDP clients should make outgoing
    connections from that IP. It should not require changing application
    code since it's often not possible.
    
    Currently it's solved by intercepting glibc wrappers around syscalls
    such as `bind(2)` and `connect(2)`. It's done by a shared library that
    is preloaded for every process in a cgroup so that whenever TCP/UDP
    server calls `bind(2)`, the library replaces IP in sockaddr before
    passing arguments to syscall. When application calls `connect(2)` the
    library transparently binds the local end of connection to that IP
    (`bind(2)` with `IP_BIND_ADDRESS_NO_PORT` to avoid performance penalty).
    
    Shared library approach is fragile though, e.g.:
    * some applications clear env vars (incl. `LD_PRELOAD`);
    * `/etc/ld.so.preload` doesn't help since some applications are linked
      with option `-z nodefaultlib`;
    * other applications don't use glibc and there is nothing to intercept.
    
    == The solution ==
    
    The patch provides much more reliable in-kernel solution for the 1st
    part of the problem: binding TCP/UDP servers on desired IP. It does not
    depend on application environment and implementation details (whether
    glibc is used or not).
    
    It adds new eBPF program type `BPF_PROG_TYPE_CGROUP_SOCK_ADDR` and
    attach types `BPF_CGROUP_INET4_BIND` and `BPF_CGROUP_INET6_BIND`
    (similar to already existing `BPF_CGROUP_INET_SOCK_CREATE`).
    
    The new program type is intended to be used with sockets (`struct sock`)
    in a cgroup and provided by user `struct sockaddr`. Pointers to both of
    them are parts of the context passed to programs of newly added types.
    
    The new attach types provides hooks in `bind(2)` system call for both
    IPv4 and IPv6 so that one can write a program to override IP addresses
    and ports user program tries to bind to and apply such a program for
    whole cgroup.
    
    == Implementation notes ==
    
    [1]
    Separate attach types for `AF_INET` and `AF_INET6` are added
    intentionally to prevent reading/writing to offsets that don't make
    sense for corresponding socket family. E.g. if user passes `sockaddr_in`
    it doesn't make sense to read from / write to `user_ip6[]` context
    fields.
    
    [2]
    The write access to `struct bpf_sock_addr_kern` is implemented using
    special field as an additional "register".
    
    There are just two registers in `sock_addr_convert_ctx_access`: `src`
    with value to write and `dst` with pointer to context that can't be
    changed not to break later instructions. But the fields, allowed to
    write to, are not available directly and to access them address of
    corresponding pointer has to be loaded first. To get additional register
    the 1st not used by `src` and `dst` one is taken, its content is saved
    to `bpf_sock_addr_kern.tmp_reg`, then the register is used to load
    address of pointer field, and finally the register's content is restored
    from the temporary field after writing `src` value.
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 9d3b572d4dec..2cad66a4cacb 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1175,19 +1175,29 @@ static int
 bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
 				enum bpf_attach_type expected_attach_type)
 {
-	/* There are currently no prog types that require specifying
-	 * attach_type at load time.
-	 */
-	return 0;
+	switch (prog_type) {
+	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
+		switch (expected_attach_type) {
+		case BPF_CGROUP_INET4_BIND:
+		case BPF_CGROUP_INET6_BIND:
+			return 0;
+		default:
+			return -EINVAL;
+		}
+	default:
+		return 0;
+	}
 }
 
 static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
 					     enum bpf_attach_type attach_type)
 {
-	/* There are currently no prog types that require specifying
-	 * attach_type at load time.
-	 */
-	return 0;
+	switch (prog->type) {
+	case BPF_PROG_TYPE_CGROUP_SOCK_ADDR:
+		return attach_type == prog->expected_attach_type ? 0 : -EINVAL;
+	default:
+		return 0;
+	}
 }
 
 /* last field in 'union bpf_attr' used by this command */
@@ -1479,6 +1489,10 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_INET_SOCK_CREATE:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK;
 		break;
+	case BPF_CGROUP_INET4_BIND:
+	case BPF_CGROUP_INET6_BIND:
+		ptype = BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
+		break;
 	case BPF_CGROUP_SOCK_OPS:
 		ptype = BPF_PROG_TYPE_SOCK_OPS;
 		break;
@@ -1541,6 +1555,10 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_CGROUP_INET_SOCK_CREATE:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK;
 		break;
+	case BPF_CGROUP_INET4_BIND:
+	case BPF_CGROUP_INET6_BIND:
+		ptype = BPF_PROG_TYPE_CGROUP_SOCK_ADDR;
+		break;
 	case BPF_CGROUP_SOCK_OPS:
 		ptype = BPF_PROG_TYPE_SOCK_OPS;
 		break;
@@ -1590,6 +1608,8 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_INET_INGRESS:
 	case BPF_CGROUP_INET_EGRESS:
 	case BPF_CGROUP_INET_SOCK_CREATE:
+	case BPF_CGROUP_INET4_BIND:
+	case BPF_CGROUP_INET6_BIND:
 	case BPF_CGROUP_SOCK_OPS:
 	case BPF_CGROUP_DEVICE:
 		break;

commit 5e43f899b03a3492ce5fc44e8900becb04dae9c0
Author: Andrey Ignatov <rdna@fb.com>
Date:   Fri Mar 30 15:08:00 2018 -0700

    bpf: Check attach type at prog load time
    
    == The problem ==
    
    There are use-cases when a program of some type can be attached to
    multiple attach points and those attach points must have different
    permissions to access context or to call helpers.
    
    E.g. context structure may have fields for both IPv4 and IPv6 but it
    doesn't make sense to read from / write to IPv6 field when attach point
    is somewhere in IPv4 stack.
    
    Same applies to BPF-helpers: it may make sense to call some helper from
    some attach point, but not from other for same prog type.
    
    == The solution ==
    
    Introduce `expected_attach_type` field in in `struct bpf_attr` for
    `BPF_PROG_LOAD` command. If scenario described in "The problem" section
    is the case for some prog type, the field will be checked twice:
    
    1) At load time prog type is checked to see if attach type for it must
       be known to validate program permissions correctly. Prog will be
       rejected with EINVAL if it's the case and `expected_attach_type` is
       not specified or has invalid value.
    
    2) At attach time `attach_type` is compared with `expected_attach_type`,
       if prog type requires to have one, and, if they differ, attach will
       be rejected with EINVAL.
    
    The `expected_attach_type` is now available as part of `struct bpf_prog`
    in both `bpf_verifier_ops->is_valid_access()` and
    `bpf_verifier_ops->get_func_proto()` () and can be used to check context
    accesses and calls to helpers correspondingly.
    
    Initially the idea was discussed by Alexei Starovoitov <ast@fb.com> and
    Daniel Borkmann <daniel@iogearbox.net> here:
    https://marc.info/?l=linux-netdev&m=152107378717201&w=2
    
    Signed-off-by: Andrey Ignatov <rdna@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 95ca2523fa6e..9d3b572d4dec 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1171,8 +1171,27 @@ struct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,
 }
 EXPORT_SYMBOL_GPL(bpf_prog_get_type_dev);
 
+static int
+bpf_prog_load_check_attach_type(enum bpf_prog_type prog_type,
+				enum bpf_attach_type expected_attach_type)
+{
+	/* There are currently no prog types that require specifying
+	 * attach_type at load time.
+	 */
+	return 0;
+}
+
+static int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,
+					     enum bpf_attach_type attach_type)
+{
+	/* There are currently no prog types that require specifying
+	 * attach_type at load time.
+	 */
+	return 0;
+}
+
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD prog_ifindex
+#define	BPF_PROG_LOAD_LAST_FIELD expected_attach_type
 
 static int bpf_prog_load(union bpf_attr *attr)
 {
@@ -1209,11 +1228,16 @@ static int bpf_prog_load(union bpf_attr *attr)
 	    !capable(CAP_SYS_ADMIN))
 		return -EPERM;
 
+	if (bpf_prog_load_check_attach_type(type, attr->expected_attach_type))
+		return -EINVAL;
+
 	/* plain bpf_prog allocation */
 	prog = bpf_prog_alloc(bpf_prog_size(attr->insn_cnt), GFP_USER);
 	if (!prog)
 		return -ENOMEM;
 
+	prog->expected_attach_type = attr->expected_attach_type;
+
 	prog->aux->offload_requested = !!attr->prog_ifindex;
 
 	err = security_bpf_prog_alloc(prog->aux);
@@ -1474,6 +1498,11 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	if (IS_ERR(prog))
 		return PTR_ERR(prog);
 
+	if (bpf_prog_attach_check_attach_type(prog, attr->attach_type)) {
+		bpf_prog_put(prog);
+		return -EINVAL;
+	}
+
 	cgrp = cgroup_get_from_fd(attr->target_fd);
 	if (IS_ERR(cgrp)) {
 		bpf_prog_put(prog);

commit c4f6699dfcb8558d138fe838f741b2c10f416cf9
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Wed Mar 28 12:05:37 2018 -0700

    bpf: introduce BPF_RAW_TRACEPOINT
    
    Introduce BPF_PROG_TYPE_RAW_TRACEPOINT bpf program type to access
    kernel internal arguments of the tracepoints in their raw form.
    
    >From bpf program point of view the access to the arguments look like:
    struct bpf_raw_tracepoint_args {
           __u64 args[0];
    };
    
    int bpf_prog(struct bpf_raw_tracepoint_args *ctx)
    {
      // program can read args[N] where N depends on tracepoint
      // and statically verified at program load+attach time
    }
    
    kprobe+bpf infrastructure allows programs access function arguments.
    This feature allows programs access raw tracepoint arguments.
    
    Similar to proposed 'dynamic ftrace events' there are no abi guarantees
    to what the tracepoints arguments are and what their meaning is.
    The program needs to type cast args properly and use bpf_probe_read()
    helper to access struct fields when argument is a pointer.
    
    For every tracepoint __bpf_trace_##call function is prepared.
    In assembler it looks like:
    (gdb) disassemble __bpf_trace_xdp_exception
    Dump of assembler code for function __bpf_trace_xdp_exception:
       0xffffffff81132080 <+0>:     mov    %ecx,%ecx
       0xffffffff81132082 <+2>:     jmpq   0xffffffff811231f0 <bpf_trace_run3>
    
    where
    
    TRACE_EVENT(xdp_exception,
            TP_PROTO(const struct net_device *dev,
                     const struct bpf_prog *xdp, u32 act),
    
    The above assembler snippet is casting 32-bit 'act' field into 'u64'
    to pass into bpf_trace_run3(), while 'dev' and 'xdp' args are passed as-is.
    All of ~500 of __bpf_trace_*() functions are only 5-10 byte long
    and in total this approach adds 7k bytes to .text.
    
    This approach gives the lowest possible overhead
    while calling trace_xdp_exception() from kernel C code and
    transitioning into bpf land.
    Since tracepoint+bpf are used at speeds of 1M+ events per second
    this is valuable optimization.
    
    The new BPF_RAW_TRACEPOINT_OPEN sys_bpf command is introduced
    that returns anon_inode FD of 'bpf-raw-tracepoint' object.
    
    The user space looks like:
    // load bpf prog with BPF_PROG_TYPE_RAW_TRACEPOINT type
    prog_fd = bpf_prog_load(...);
    // receive anon_inode fd for given bpf_raw_tracepoint with prog attached
    raw_tp_fd = bpf_raw_tracepoint_open("xdp_exception", prog_fd);
    
    Ctrl-C of tracing daemon or cmdline tool that uses this feature
    will automatically detach bpf program, unload it and
    unregister tracepoint probe.
    
    On the kernel side the __bpf_raw_tp_map section of pointers to
    tracepoint definition and to __bpf_trace_*() probe function is used
    to find a tracepoint with "xdp_exception" name and
    corresponding __bpf_trace_xdp_exception() probe function
    which are passed to tracepoint_probe_register() to connect probe
    with tracepoint.
    
    Addition of bpf_raw_tracepoint doesn't interfere with ftrace and perf
    tracepoint mechanisms. perf_event_open() can be used in parallel
    on the same tracepoint.
    Multiple bpf_raw_tracepoint_open("xdp_exception", prog_fd) are permitted.
    Each with its own bpf program. The kernel will execute
    all tracepoint probes and all attached bpf programs.
    
    In the future bpf_raw_tracepoints can be extended with
    query/introspection logic.
    
    __bpf_raw_tp_map section logic was contributed by Steven Rostedt
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 77d45bd9f507..95ca2523fa6e 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1315,6 +1315,81 @@ static int bpf_obj_get(const union bpf_attr *attr)
 				attr->file_flags);
 }
 
+struct bpf_raw_tracepoint {
+	struct bpf_raw_event_map *btp;
+	struct bpf_prog *prog;
+};
+
+static int bpf_raw_tracepoint_release(struct inode *inode, struct file *filp)
+{
+	struct bpf_raw_tracepoint *raw_tp = filp->private_data;
+
+	if (raw_tp->prog) {
+		bpf_probe_unregister(raw_tp->btp, raw_tp->prog);
+		bpf_prog_put(raw_tp->prog);
+	}
+	kfree(raw_tp);
+	return 0;
+}
+
+static const struct file_operations bpf_raw_tp_fops = {
+	.release	= bpf_raw_tracepoint_release,
+	.read		= bpf_dummy_read,
+	.write		= bpf_dummy_write,
+};
+
+#define BPF_RAW_TRACEPOINT_OPEN_LAST_FIELD raw_tracepoint.prog_fd
+
+static int bpf_raw_tracepoint_open(const union bpf_attr *attr)
+{
+	struct bpf_raw_tracepoint *raw_tp;
+	struct bpf_raw_event_map *btp;
+	struct bpf_prog *prog;
+	char tp_name[128];
+	int tp_fd, err;
+
+	if (strncpy_from_user(tp_name, u64_to_user_ptr(attr->raw_tracepoint.name),
+			      sizeof(tp_name) - 1) < 0)
+		return -EFAULT;
+	tp_name[sizeof(tp_name) - 1] = 0;
+
+	btp = bpf_find_raw_tracepoint(tp_name);
+	if (!btp)
+		return -ENOENT;
+
+	raw_tp = kzalloc(sizeof(*raw_tp), GFP_USER);
+	if (!raw_tp)
+		return -ENOMEM;
+	raw_tp->btp = btp;
+
+	prog = bpf_prog_get_type(attr->raw_tracepoint.prog_fd,
+				 BPF_PROG_TYPE_RAW_TRACEPOINT);
+	if (IS_ERR(prog)) {
+		err = PTR_ERR(prog);
+		goto out_free_tp;
+	}
+
+	err = bpf_probe_register(raw_tp->btp, prog);
+	if (err)
+		goto out_put_prog;
+
+	raw_tp->prog = prog;
+	tp_fd = anon_inode_getfd("bpf-raw-tracepoint", &bpf_raw_tp_fops, raw_tp,
+				 O_CLOEXEC);
+	if (tp_fd < 0) {
+		bpf_probe_unregister(raw_tp->btp, prog);
+		err = tp_fd;
+		goto out_put_prog;
+	}
+	return tp_fd;
+
+out_put_prog:
+	bpf_prog_put(prog);
+out_free_tp:
+	kfree(raw_tp);
+	return err;
+}
+
 #ifdef CONFIG_CGROUP_BPF
 
 #define BPF_PROG_ATTACH_LAST_FIELD attach_flags
@@ -1925,6 +2000,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_OBJ_GET_INFO_BY_FD:
 		err = bpf_obj_get_info_by_fd(&attr, uattr);
 		break;
+	case BPF_RAW_TRACEPOINT_OPEN:
+		err = bpf_raw_tracepoint_open(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit b76354cdfefb68bc017065850a34d486887a4f7b
Author: Shaohua Li <shli@fb.com>
Date:   Tue Mar 27 11:53:21 2018 -0700

    bpf: follow idr code convention
    
    Generally we do a preload before doing idr allocation. This also help
    improve the allocation success rate in memory pressure.
    
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Shaohua Li <shli@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index dd172ee16716..77d45bd9f507 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -203,11 +203,13 @@ static int bpf_map_alloc_id(struct bpf_map *map)
 {
 	int id;
 
+	idr_preload(GFP_KERNEL);
 	spin_lock_bh(&map_idr_lock);
 	id = idr_alloc_cyclic(&map_idr, map, 1, INT_MAX, GFP_ATOMIC);
 	if (id > 0)
 		map->id = id;
 	spin_unlock_bh(&map_idr_lock);
+	idr_preload_end();
 
 	if (WARN_ON_ONCE(!id))
 		return -ENOSPC;
@@ -940,11 +942,13 @@ static int bpf_prog_alloc_id(struct bpf_prog *prog)
 {
 	int id;
 
+	idr_preload(GFP_KERNEL);
 	spin_lock_bh(&prog_idr_lock);
 	id = idr_alloc_cyclic(&prog_idr, prog, 1, INT_MAX, GFP_ATOMIC);
 	if (id > 0)
 		prog->aux->id = id;
 	spin_unlock_bh(&prog_idr_lock);
+	idr_preload_end();
 
 	/* id is in [1, INT_MAX) */
 	if (WARN_ON_ONCE(!id))

commit 03fe2debbb2771fb90881e4ce8109b09cf772a5c
Merge: 6686c459e144 f36b7534b833
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Mar 23 11:24:57 2018 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Fun set of conflict resolutions here...
    
    For the mac80211 stuff, these were fortunately just parallel
    adds.  Trivially resolved.
    
    In drivers/net/phy/phy.c we had a bug fix in 'net' that moved the
    function phy_disable_interrupts() earlier in the file, whilst in
    'net-next' the phy_error() call from this function was removed.
    
    In net/ipv4/xfrm4_policy.c, David Ahern's changes to remove the
    'rt_table_id' member of rtable collided with a bug fix in 'net' that
    added a new struct member "rt_mtu_locked" which needs to be copied
    over here.
    
    The mlxsw driver conflict consisted of net-next separating
    the span code and definitions into separate files, whilst
    a 'net' bug fix made some changes to that moved code.
    
    The mlx5 infiniband conflict resolution was quite non-trivial,
    the RDMA tree's merge commit was used as a guide here, and
    here are their notes:
    
    ====================
    
        Due to bug fixes found by the syzkaller bot and taken into the for-rc
        branch after development for the 4.17 merge window had already started
        being taken into the for-next branch, there were fairly non-trivial
        merge issues that would need to be resolved between the for-rc branch
        and the for-next branch.  This merge resolves those conflicts and
        provides a unified base upon which ongoing development for 4.17 can
        be based.
    
        Conflicts:
                drivers/infiniband/hw/mlx5/main.c - Commit 42cea83f9524
                (IB/mlx5: Fix cleanup order on unload) added to for-rc and
                commit b5ca15ad7e61 (IB/mlx5: Add proper representors support)
                add as part of the devel cycle both needed to modify the
                init/de-init functions used by mlx5.  To support the new
                representors, the new functions added by the cleanup patch
                needed to be made non-static, and the init/de-init list
                added by the representors patch needed to be modified to
                match the init/de-init list changes made by the cleanup
                patch.
        Updates:
                drivers/infiniband/hw/mlx5/mlx5_ib.h - Update function
                prototypes added by representors patch to reflect new function
                names as changed by cleanup patch
                drivers/infiniband/hw/mlx5/ib_rep.c - Update init/de-init
                stage list to match new order from cleanup patch
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 0fa4fe85f4724fff89b09741c437cbee9cf8b008
Author: Chenbo Feng <fengc@google.com>
Date:   Mon Mar 19 17:57:27 2018 -0700

    bpf: skip unnecessary capability check
    
    The current check statement in BPF syscall will do a capability check
    for CAP_SYS_ADMIN before checking sysctl_unprivileged_bpf_disabled. This
    code path will trigger unnecessary security hooks on capability checking
    and cause false alarms on unprivileged process trying to get CAP_SYS_ADMIN
    access. This can be resolved by simply switch the order of the statement
    and CAP_SYS_ADMIN is not required anyway if unprivileged bpf syscall is
    allowed.
    
    Signed-off-by: Chenbo Feng <fengc@google.com>
    Acked-by: Lorenzo Colitti <lorenzo@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index e24aa3241387..43f95d190eea 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1845,7 +1845,7 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	union bpf_attr attr = {};
 	int err;
 
-	if (!capable(CAP_SYS_ADMIN) && sysctl_unprivileged_bpf_disabled)
+	if (sysctl_unprivileged_bpf_disabled && !capable(CAP_SYS_ADMIN))
 		return -EPERM;
 
 	err = check_uarg_tail_zero(uattr, sizeof(attr), size);

commit 4f738adba30a7cfc006f605707e7aee847ffefa0
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Sun Mar 18 12:57:10 2018 -0700

    bpf: create tcp_bpf_ulp allowing BPF to monitor socket TX/RX data
    
    This implements a BPF ULP layer to allow policy enforcement and
    monitoring at the socket layer. In order to support this a new
    program type BPF_PROG_TYPE_SK_MSG is used to run the policy at
    the sendmsg/sendpage hook. To attach the policy to sockets a
    sockmap is used with a new program attach type BPF_SK_MSG_VERDICT.
    
    Similar to previous sockmap usages when a sock is added to a
    sockmap, via a map update, if the map contains a BPF_SK_MSG_VERDICT
    program type attached then the BPF ULP layer is created on the
    socket and the attached BPF_PROG_TYPE_SK_MSG program is run for
    every msg in sendmsg case and page/offset in sendpage case.
    
    BPF_PROG_TYPE_SK_MSG Semantics/API:
    
    BPF_PROG_TYPE_SK_MSG supports only two return codes SK_PASS and
    SK_DROP. Returning SK_DROP free's the copied data in the sendmsg
    case and in the sendpage case leaves the data untouched. Both cases
    return -EACESS to the user. Returning SK_PASS will allow the msg to
    be sent.
    
    In the sendmsg case data is copied into kernel space buffers before
    running the BPF program. The kernel space buffers are stored in a
    scatterlist object where each element is a kernel memory buffer.
    Some effort is made to coalesce data from the sendmsg call here.
    For example a sendmsg call with many one byte iov entries will
    likely be pushed into a single entry. The BPF program is run with
    data pointers (start/end) pointing to the first sg element.
    
    In the sendpage case data is not copied. We opt not to copy the
    data by default here, because the BPF infrastructure does not
    know what bytes will be needed nor when they will be needed. So
    copying all bytes may be wasteful. Because of this the initial
    start/end data pointers are (0,0). Meaning no data can be read or
    written. This avoids reading data that may be modified by the
    user. A new helper is added later in this series if reading and
    writing the data is needed. The helper call will do a copy by
    default so that the page is exclusively owned by the BPF call.
    
    The verdict from the BPF_PROG_TYPE_SK_MSG applies to the entire msg
    in the sendmsg() case and the entire page/offset in the sendpage case.
    This avoids ambiguity on how to handle mixed return codes in the
    sendmsg case. Again a helper is added later in the series if
    a verdict needs to apply to multiple system calls and/or only
    a subpart of the currently being processed message.
    
    The helper msg_redirect_map() can be used to select the socket to
    send the data on. This is used similar to existing redirect use
    cases. This allows policy to redirect msgs.
    
    Pseudo code simple example:
    
    The basic logic to attach a program to a socket is as follows,
    
      // load the programs
      bpf_prog_load(SOCKMAP_TCP_MSG_PROG, BPF_PROG_TYPE_SK_MSG,
                    &obj, &msg_prog);
    
      // lookup the sockmap
      bpf_map_msg = bpf_object__find_map_by_name(obj, "my_sock_map");
    
      // get fd for sockmap
      map_fd_msg = bpf_map__fd(bpf_map_msg);
    
      // attach program to sockmap
      bpf_prog_attach(msg_prog, map_fd_msg, BPF_SK_MSG_VERDICT, 0);
    
    Adding sockets to the map is done in the normal way,
    
      // Add a socket 'fd' to sockmap at location 'i'
      bpf_map_update_elem(map_fd_msg, &i, fd, BPF_ANY);
    
    After the above any socket attached to "my_sock_map", in this case
    'fd', will run the BPF msg verdict program (msg_prog) on every
    sendmsg and sendpage system call.
    
    For a complete example see BPF selftests or sockmap samples.
    
    Implementation notes:
    
    It seemed the simplest, to me at least, to use a refcnt to ensure
    psock is not lost across the sendmsg copy into the sg, the bpf program
    running on the data in sg_data, and the final pass to the TCP stack.
    Some performance testing may show a better method to do this and avoid
    the refcnt cost, but for now use the simpler method.
    
    Another item that will come after basic support is in place is
    supporting MSG_MORE flag. At the moment we call sendpages even if
    the MSG_MORE flag is set. An enhancement would be to collect the
    pages into a larger scatterlist and pass down the stack. Notice that
    bpf_tcp_sendmsg() could support this with some additional state saved
    across sendmsg calls. I built the code to support this without having
    to do refactoring work. Other features TBD include ZEROCOPY and the
    TCP_RECV_QUEUE/TCP_NO_QUEUE support. This will follow initial series
    shortly.
    
    Future work could improve size limits on the scatterlist rings used
    here. Currently, we use MAX_SKB_FRAGS simply because this was being
    used already in the TLS case. Future work could extend the kernel sk
    APIs to tune this depending on workload. This is a trade-off
    between memory usage and throughput performance.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index e24aa3241387..3aeb4ea2a93a 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1315,7 +1315,8 @@ static int bpf_obj_get(const union bpf_attr *attr)
 
 #define BPF_PROG_ATTACH_LAST_FIELD attach_flags
 
-static int sockmap_get_from_fd(const union bpf_attr *attr, bool attach)
+static int sockmap_get_from_fd(const union bpf_attr *attr,
+			       int type, bool attach)
 {
 	struct bpf_prog *prog = NULL;
 	int ufd = attr->target_fd;
@@ -1329,8 +1330,7 @@ static int sockmap_get_from_fd(const union bpf_attr *attr, bool attach)
 		return PTR_ERR(map);
 
 	if (attach) {
-		prog = bpf_prog_get_type(attr->attach_bpf_fd,
-					 BPF_PROG_TYPE_SK_SKB);
+		prog = bpf_prog_get_type(attr->attach_bpf_fd, type);
 		if (IS_ERR(prog)) {
 			fdput(f);
 			return PTR_ERR(prog);
@@ -1382,9 +1382,11 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_DEVICE:
 		ptype = BPF_PROG_TYPE_CGROUP_DEVICE;
 		break;
+	case BPF_SK_MSG_VERDICT:
+		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_MSG, true);
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
-		return sockmap_get_from_fd(attr, true);
+		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_SKB, true);
 	default:
 		return -EINVAL;
 	}
@@ -1437,9 +1439,11 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_CGROUP_DEVICE:
 		ptype = BPF_PROG_TYPE_CGROUP_DEVICE;
 		break;
+	case BPF_SK_MSG_VERDICT:
+		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_MSG, false);
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
-		return sockmap_get_from_fd(attr, false);
+		return sockmap_get_from_fd(attr, BPF_PROG_TYPE_SK_SKB, false);
 	default:
 		return -EINVAL;
 	}

commit 9c147b56fc7165856da9c510463fafc2f0d58d5f
Author: Mickaël Salaün <mic@digikod.net>
Date:   Fri Jan 26 00:54:02 2018 +0100

    bpf: Use the IS_FD_ARRAY() macro in map_update_elem()
    
    Make the code more readable.
    
    Signed-off-by: Mickaël Salaün <mic@digikod.net>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5bdb0cc84ad2..e24aa3241387 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -709,10 +709,7 @@ static int map_update_elem(union bpf_attr *attr)
 		err = bpf_percpu_hash_update(map, key, value, attr->flags);
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
 		err = bpf_percpu_array_update(map, key, value, attr->flags);
-	} else if (map->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY ||
-		   map->map_type == BPF_MAP_TYPE_PROG_ARRAY ||
-		   map->map_type == BPF_MAP_TYPE_CGROUP_ARRAY ||
-		   map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS) {
+	} else if (IS_FD_ARRAY(map)) {
 		rcu_read_lock();
 		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
 						   attr->flags);

commit 52775b33bb5072fbc07b02c0cf4fe8da1f7ee7cd
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Jan 17 19:13:28 2018 -0800

    bpf: offload: report device information about offloaded maps
    
    Tell user space about device on which the map was created.
    Unfortunate reality of user ABI makes sharing this code
    with program offload difficult but the information is the
    same.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 97a825ffc763..5bdb0cc84ad2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1801,6 +1801,12 @@ static int bpf_map_get_info_by_fd(struct bpf_map *map,
 	info.map_flags = map->map_flags;
 	memcpy(info.name, map->name, sizeof(map->name));
 
+	if (bpf_map_is_dev_bound(map)) {
+		err = bpf_map_offload_info_fill(&info, map);
+		if (err)
+			return err;
+	}
+
 	if (copy_to_user(uinfo, &info, info_len) ||
 	    put_user(info_len, &uattr->info.info_len))
 		return -EFAULT;

commit 61f3c964dfd287b05d7ac6660a4f4ddfef84786c
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Wed Jan 17 16:52:02 2018 -0800

    bpf: allow socket_filter programs to use bpf_prog_test_run
    
    in order to improve test coverage allow socket_filter program type
    to be run via bpf_prog_test_run command.
    Since such programs can be loaded by non-root tighten
    permissions for bpf_prog_test_run to be root only
    to avoid surprises.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c28524483bf4..97a825ffc763 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1504,6 +1504,8 @@ static int bpf_prog_test_run(const union bpf_attr *attr,
 	struct bpf_prog *prog;
 	int ret = -ENOTSUPP;
 
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 	if (CHECK_ATTR(BPF_PROG_TEST_RUN))
 		return -EINVAL;
 

commit fcfb126defda3cee3f1d9460dbe9a2ccac4fbd21
Author: Jiong Wang <jiong.wang@netronome.com>
Date:   Tue Jan 16 16:05:19 2018 -0800

    bpf: add new jited info fields in bpf_dev_offload and bpf_prog_info
    
    For host JIT, there are "jited_len"/"bpf_func" fields in struct bpf_prog
    used by all host JIT targets to get jited image and it's length. While for
    offload, targets are likely to have different offload mechanisms that these
    info are kept in device private data fields.
    
    Therefore, BPF_OBJ_GET_INFO_BY_FD syscall needs an unified way to get JIT
    length and contents info for offload targets.
    
    One way is to introduce new callback to parse device private data then fill
    those fields in bpf_prog_info. This might be a little heavy, the other way
    is to add generic fields which will be initialized by all offload targets.
    
    This patch follow the second approach to introduce two new fields in
    struct bpf_dev_offload and teach bpf_prog_get_info_by_fd about them to fill
    correct jited_prog_len and jited_prog_insns in bpf_prog_info.
    
    Reviewed-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Jiong Wang <jiong.wang@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c691b9e972e3..c28524483bf4 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1724,19 +1724,6 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		goto done;
 	}
 
-	ulen = info.jited_prog_len;
-	info.jited_prog_len = prog->jited_len;
-	if (info.jited_prog_len && ulen) {
-		if (bpf_dump_raw_ok()) {
-			uinsns = u64_to_user_ptr(info.jited_prog_insns);
-			ulen = min_t(u32, info.jited_prog_len, ulen);
-			if (copy_to_user(uinsns, prog->bpf_func, ulen))
-				return -EFAULT;
-		} else {
-			info.jited_prog_insns = 0;
-		}
-	}
-
 	ulen = info.xlated_prog_len;
 	info.xlated_prog_len = bpf_prog_insn_size(prog);
 	if (info.xlated_prog_len && ulen) {
@@ -1762,6 +1749,24 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 		err = bpf_prog_offload_info_fill(&info, prog);
 		if (err)
 			return err;
+		goto done;
+	}
+
+	/* NOTE: the following code is supposed to be skipped for offload.
+	 * bpf_prog_offload_info_fill() is the place to fill similar fields
+	 * for offload.
+	 */
+	ulen = info.jited_prog_len;
+	info.jited_prog_len = prog->jited_len;
+	if (info.jited_prog_len && ulen) {
+		if (bpf_dump_raw_ok()) {
+			uinsns = u64_to_user_ptr(info.jited_prog_insns);
+			ulen = min_t(u32, info.jited_prog_len, ulen);
+			if (copy_to_user(uinsns, prog->bpf_func, ulen))
+				return -EFAULT;
+		} else {
+			info.jited_prog_insns = 0;
+		}
 	}
 
 done:

commit a38845729ea3985db5d2544ec3ef3dc8f6313a27
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu Jan 11 20:29:09 2018 -0800

    bpf: offload: add map offload infrastructure
    
    BPF map offload follow similar path to program offload.  At creation
    time users may specify ifindex of the device on which they want to
    create the map.  Map will be validated by the kernel's
    .map_alloc_check callback and device driver will be called for the
    actual allocation.  Map will have an empty set of operations
    associated with it (save for alloc and free callbacks).  The real
    device callbacks are kept in map->offload->dev_ops because they
    have slightly different signatures.  Map operations are called in
    process context so the driver may communicate with HW freely,
    msleep(), wait() etc.
    
    Map alloc and free callbacks are muxed via existing .ndo_bpf, and
    are always called with rtnl lock held.  Maps and programs are
    guaranteed to be destroyed before .ndo_uninit (i.e. before
    unregister_netdev() returns).  Map callbacks are invoked with
    bpf_devs_lock *read* locked, drivers must take care of exclusive
    locking if necessary.
    
    All offload-specific branches are marked with unlikely() (through
    bpf_map_is_dev_bound()), given that branch penalty will be
    negligible compared to IO anyway, and we don't want to penalize
    SW path unnecessarily.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index a3f726bb42ea..c691b9e972e3 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -94,6 +94,11 @@ static int check_uarg_tail_zero(void __user *uaddr,
 	return 0;
 }
 
+const struct bpf_map_ops bpf_map_offload_ops = {
+	.map_alloc = bpf_map_offload_map_alloc,
+	.map_free = bpf_map_offload_map_free,
+};
+
 static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 {
 	const struct bpf_map_ops *ops;
@@ -111,6 +116,8 @@ static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 		if (err)
 			return ERR_PTR(err);
 	}
+	if (attr->map_ifindex)
+		ops = &bpf_map_offload_ops;
 	map = ops->map_alloc(attr);
 	if (IS_ERR(map))
 		return map;
@@ -208,16 +215,25 @@ static int bpf_map_alloc_id(struct bpf_map *map)
 	return id > 0 ? 0 : id;
 }
 
-static void bpf_map_free_id(struct bpf_map *map, bool do_idr_lock)
+void bpf_map_free_id(struct bpf_map *map, bool do_idr_lock)
 {
 	unsigned long flags;
 
+	/* Offloaded maps are removed from the IDR store when their device
+	 * disappears - even if someone holds an fd to them they are unusable,
+	 * the memory is gone, all ops will fail; they are simply waiting for
+	 * refcnt to drop to be freed.
+	 */
+	if (!map->id)
+		return;
+
 	if (do_idr_lock)
 		spin_lock_irqsave(&map_idr_lock, flags);
 	else
 		__acquire(&map_idr_lock);
 
 	idr_remove(&map_idr, map->id);
+	map->id = 0;
 
 	if (do_idr_lock)
 		spin_unlock_irqrestore(&map_idr_lock, flags);
@@ -397,7 +413,7 @@ static int bpf_obj_name_cpy(char *dst, const char *src)
 	return 0;
 }
 
-#define BPF_MAP_CREATE_LAST_FIELD map_name
+#define BPF_MAP_CREATE_LAST_FIELD map_ifindex
 /* called via syscall */
 static int map_create(union bpf_attr *attr)
 {
@@ -585,8 +601,10 @@ static int map_lookup_elem(union bpf_attr *attr)
 	if (!value)
 		goto free_key;
 
-	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
-	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
+	if (bpf_map_is_dev_bound(map)) {
+		err = bpf_map_offload_lookup_elem(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+		   map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
 		err = bpf_percpu_hash_copy(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
 		err = bpf_percpu_array_copy(map, key, value);
@@ -673,7 +691,10 @@ static int map_update_elem(union bpf_attr *attr)
 		goto free_value;
 
 	/* Need to create a kthread, thus must support schedule */
-	if (map->map_type == BPF_MAP_TYPE_CPUMAP) {
+	if (bpf_map_is_dev_bound(map)) {
+		err = bpf_map_offload_update_elem(map, key, value, attr->flags);
+		goto out;
+	} else if (map->map_type == BPF_MAP_TYPE_CPUMAP) {
 		err = map->ops->map_update_elem(map, key, value, attr->flags);
 		goto out;
 	}
@@ -750,6 +771,11 @@ static int map_delete_elem(union bpf_attr *attr)
 		goto err_put;
 	}
 
+	if (bpf_map_is_dev_bound(map)) {
+		err = bpf_map_offload_delete_elem(map, key);
+		goto out;
+	}
+
 	preempt_disable();
 	__this_cpu_inc(bpf_prog_active);
 	rcu_read_lock();
@@ -757,7 +783,7 @@ static int map_delete_elem(union bpf_attr *attr)
 	rcu_read_unlock();
 	__this_cpu_dec(bpf_prog_active);
 	preempt_enable();
-
+out:
 	if (!err)
 		trace_bpf_map_delete_elem(map, ufd, key);
 	kfree(key);
@@ -807,9 +833,15 @@ static int map_get_next_key(union bpf_attr *attr)
 	if (!next_key)
 		goto free_key;
 
+	if (bpf_map_is_dev_bound(map)) {
+		err = bpf_map_offload_get_next_key(map, key, next_key);
+		goto out;
+	}
+
 	rcu_read_lock();
 	err = map->ops->map_get_next_key(map, key, next_key);
 	rcu_read_unlock();
+out:
 	if (err)
 		goto free_next_key;
 

commit bd475643d74e8ed78bfd36d941053b0e45974e8e
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu Jan 11 20:29:06 2018 -0800

    bpf: add helper for copying attrs to struct bpf_map
    
    All map types reimplement the field-by-field copy of union bpf_attr
    members into struct bpf_map.  Add a helper to perform this operation.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c0ac03a04880..a3f726bb42ea 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -143,6 +143,16 @@ void bpf_map_area_free(void *area)
 	kvfree(area);
 }
 
+void bpf_map_init_from_attr(struct bpf_map *map, union bpf_attr *attr)
+{
+	map->map_type = attr->map_type;
+	map->key_size = attr->key_size;
+	map->value_size = attr->value_size;
+	map->max_entries = attr->max_entries;
+	map->map_flags = attr->map_flags;
+	map->numa_node = bpf_map_attr_numa_node(attr);
+}
+
 int bpf_map_precharge_memlock(u32 pages)
 {
 	struct user_struct *user = get_current_user();

commit 1110f3a9bcf394c06b81a98206aee9b6860653c8
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Thu Jan 11 20:29:03 2018 -0800

    bpf: add map_alloc_check callback
    
    .map_alloc callbacks contain a number of checks validating user-
    -provided map attributes against constraints of a particular map
    type.  For offloaded maps we will need to check map attributes
    without actually allocating any memory on the host.  Add a new
    callback for validating attributes before any memory is allocated.
    This callback can be selectively implemented by map types for
    sharing code with offloads, or simply to separate the logical
    steps of validation and allocation.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2bac0dc8baba..c0ac03a04880 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -96,16 +96,25 @@ static int check_uarg_tail_zero(void __user *uaddr,
 
 static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 {
+	const struct bpf_map_ops *ops;
 	struct bpf_map *map;
+	int err;
 
-	if (attr->map_type >= ARRAY_SIZE(bpf_map_types) ||
-	    !bpf_map_types[attr->map_type])
+	if (attr->map_type >= ARRAY_SIZE(bpf_map_types))
+		return ERR_PTR(-EINVAL);
+	ops = bpf_map_types[attr->map_type];
+	if (!ops)
 		return ERR_PTR(-EINVAL);
 
-	map = bpf_map_types[attr->map_type]->map_alloc(attr);
+	if (ops->map_alloc_check) {
+		err = ops->map_alloc_check(attr);
+		if (err)
+			return ERR_PTR(err);
+	}
+	map = ops->map_alloc(attr);
 	if (IS_ERR(map))
 		return map;
-	map->ops = bpf_map_types[attr->map_type];
+	map->ops = ops;
 	map->map_type = attr->map_type;
 	return map;
 }

commit a0ce093180f2bbb832b3f5583adc640ad67ea568
Merge: f4803f1b73f8 ef7f8cec80a0
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jan 9 10:37:00 2018 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit 040ee69226f8a96b7943645d68f41d5d44b5ff7d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Dec 2 20:20:38 2017 -0500

    fix "netfilter: xt_bpf: Fix XT_BPF_MODE_FD_PINNED mode of 'xt_bpf_info_v1'"
    
    Descriptor table is a shared object; it's not a place where you can
    stick temporary references to files, especially when we don't need
    an opened file at all.
    
    Cc: stable@vger.kernel.org # v4.14
    Fixes: 98589a0998b8 ("netfilter: xt_bpf: Fix XT_BPF_MODE_FD_PINNED mode of 'xt_bpf_info_v1'")
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2c4cfeaa8d5e..5cb783fc8224 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1057,7 +1057,7 @@ struct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)
 }
 EXPORT_SYMBOL_GPL(bpf_prog_inc_not_zero);
 
-static bool bpf_prog_get_ok(struct bpf_prog *prog,
+bool bpf_prog_get_ok(struct bpf_prog *prog,
 			    enum bpf_prog_type *attach_type, bool attach_drv)
 {
 	/* not an attachment, just a refcount inc, always allow */

commit 675fc275a3a2d905535207237402c6d8dcb5fa4b
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Dec 27 18:39:09 2017 -0800

    bpf: offload: report device information for offloaded programs
    
    Report to the user ifindex and namespace information of offloaded
    programs.  If device has disappeared return -ENODEV.  Specify the
    namespace using dev/inode combination.
    
    CC: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index e02dafa6f402..ebf0fb23e237 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1707,6 +1707,12 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 			return -EFAULT;
 	}
 
+	if (bpf_prog_is_dev_bound(prog->aux)) {
+		err = bpf_prog_offload_info_fill(&info, prog);
+		if (err)
+			return err;
+	}
+
 done:
 	if (copy_to_user(uinfo, &info, info_len) ||
 	    put_user(info_len, &uattr->info.info_len))

commit ad8ad79f4f6078f456792f7f8d344da2be9bc74f
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Dec 27 18:39:07 2017 -0800

    bpf: offload: free program id when device disappears
    
    Bound programs are quite useless after their device disappears.
    They are simply waiting for reference count to go to zero,
    don't list them in BPF_PROG_GET_NEXT_ID by freeing their ID
    early.
    
    Note that orphaned offload programs will return -ENODEV on
    BPF_OBJ_GET_INFO_BY_FD so user will never see ID 0.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index e0afc2e39fd5..e02dafa6f402 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -905,9 +905,13 @@ static int bpf_prog_alloc_id(struct bpf_prog *prog)
 	return id > 0 ? 0 : id;
 }
 
-static void bpf_prog_free_id(struct bpf_prog *prog, bool do_idr_lock)
+void bpf_prog_free_id(struct bpf_prog *prog, bool do_idr_lock)
 {
-	/* cBPF to eBPF migrations are currently not in the idr store. */
+	/* cBPF to eBPF migrations are currently not in the idr store.
+	 * Offloaded programs are removed from the store when their device
+	 * disappears - even if someone grabs an fd to them they are unusable,
+	 * simply waiting for refcnt to drop to be freed.
+	 */
 	if (!prog->aux->id)
 		return;
 
@@ -917,6 +921,7 @@ static void bpf_prog_free_id(struct bpf_prog *prog, bool do_idr_lock)
 		__acquire(&prog_idr_lock);
 
 	idr_remove(&prog_idr, prog->aux->id);
+	prog->aux->id = 0;
 
 	if (do_idr_lock)
 		spin_unlock_bh(&prog_idr_lock);

commit 9a18eedb145d080d542766af1d7513ebfccd1604
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Wed Dec 27 18:39:04 2017 -0800

    bpf: offload: don't use prog->aux->offload as boolean
    
    We currently use aux->offload to indicate that program is bound
    to a specific device.  This forces us to keep the offload structure
    around even after the device is gone.  Add a bool member to
    struct bpf_prog_aux to indicate if offload was requested.
    
    Suggested-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 007802c5ca7d..e0afc2e39fd5 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1157,6 +1157,8 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (!prog)
 		return -ENOMEM;
 
+	prog->aux->offload_requested = !!attr->prog_ifindex;
+
 	err = security_bpf_prog_alloc(prog->aux);
 	if (err)
 		goto free_prog_nouncharge;
@@ -1178,7 +1180,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 	atomic_set(&prog->aux->refcnt, 1);
 	prog->gpl_compatible = is_gpl ? 1 : 0;
 
-	if (attr->prog_ifindex) {
+	if (bpf_prog_is_dev_bound(prog->aux)) {
 		err = bpf_prog_offload_init(prog, attr);
 		if (err)
 			goto free_prog;

commit 7105e828c087de970fcb5a9509db51bfe6bd7894
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Dec 20 13:42:57 2017 +0100

    bpf: allow for correlation of maps and helpers in dump
    
    Currently a dump of an xlated prog (post verifier stage) doesn't
    correlate used helpers as well as maps. The prog info lists
    involved map ids, however there's no correlation of where in the
    program they are used as of today. Likewise, bpftool does not
    correlate helper calls with the target functions.
    
    The latter can be done w/o any kernel changes through kallsyms,
    and also has the advantage that this works with inlined helpers
    and BPF calls.
    
    Example, via interpreter:
    
      # tc filter show dev foo ingress
      filter protocol all pref 49152 bpf chain 0
      filter protocol all pref 49152 bpf chain 0 handle 0x1 foo.o:[ingress] \
                          direct-action not_in_hw id 1 tag c74773051b364165   <-- prog id:1
    
      * Output before patch (calls/maps remain unclear):
    
      # bpftool prog dump xlated id 1             <-- dump prog id:1
       0: (b7) r1 = 2
       1: (63) *(u32 *)(r10 -4) = r1
       2: (bf) r2 = r10
       3: (07) r2 += -4
       4: (18) r1 = 0xffff95c47a8d4800
       6: (85) call unknown#73040
       7: (15) if r0 == 0x0 goto pc+18
       8: (bf) r2 = r10
       9: (07) r2 += -4
      10: (bf) r1 = r0
      11: (85) call unknown#73040
      12: (15) if r0 == 0x0 goto pc+23
      [...]
    
      * Output after patch:
    
      # bpftool prog dump xlated id 1
       0: (b7) r1 = 2
       1: (63) *(u32 *)(r10 -4) = r1
       2: (bf) r2 = r10
       3: (07) r2 += -4
       4: (18) r1 = map[id:2]                     <-- map id:2
       6: (85) call bpf_map_lookup_elem#73424     <-- helper call
       7: (15) if r0 == 0x0 goto pc+18
       8: (bf) r2 = r10
       9: (07) r2 += -4
      10: (bf) r1 = r0
      11: (85) call bpf_map_lookup_elem#73424
      12: (15) if r0 == 0x0 goto pc+23
      [...]
    
      # bpftool map show id 2                     <-- show/dump/etc map id:2
      2: hash_of_maps  flags 0x0
            key 4B  value 4B  max_entries 3  memlock 4096B
    
    Example, JITed, same prog:
    
      # tc filter show dev foo ingress
      filter protocol all pref 49152 bpf chain 0
      filter protocol all pref 49152 bpf chain 0 handle 0x1 foo.o:[ingress] \
                      direct-action not_in_hw id 3 tag c74773051b364165 jited
    
      # bpftool prog show id 3
      3: sched_cls  tag c74773051b364165
            loaded_at Dec 19/13:48  uid 0
            xlated 384B  jited 257B  memlock 4096B  map_ids 2
    
      # bpftool prog dump xlated id 3
       0: (b7) r1 = 2
       1: (63) *(u32 *)(r10 -4) = r1
       2: (bf) r2 = r10
       3: (07) r2 += -4
       4: (18) r1 = map[id:2]                      <-- map id:2
       6: (85) call __htab_map_lookup_elem#77408   <-+ inlined rewrite
       7: (15) if r0 == 0x0 goto pc+2                |
       8: (07) r0 += 56                              |
       9: (79) r0 = *(u64 *)(r0 +0)                <-+
      10: (15) if r0 == 0x0 goto pc+24
      11: (bf) r2 = r10
      12: (07) r2 += -4
      [...]
    
    Example, same prog, but kallsyms disabled (in that case we are
    also not allowed to pass any relative offsets, etc, so prog
    becomes pointer sanitized on dump):
    
      # sysctl kernel.kptr_restrict=2
      kernel.kptr_restrict = 2
    
      # bpftool prog dump xlated id 3
       0: (b7) r1 = 2
       1: (63) *(u32 *)(r10 -4) = r1
       2: (bf) r2 = r10
       3: (07) r2 += -4
       4: (18) r1 = map[id:2]
       6: (85) call bpf_unspec#0
       7: (15) if r0 == 0x0 goto pc+2
      [...]
    
    Example, BPF calls via interpreter:
    
      # bpftool prog dump xlated id 1
       0: (85) call pc+2#__bpf_prog_run_args32
       1: (b7) r0 = 1
       2: (95) exit
       3: (b7) r0 = 2
       4: (95) exit
    
    Example, BPF calls via JIT:
    
      # sysctl net.core.bpf_jit_enable=1
      net.core.bpf_jit_enable = 1
      # sysctl net.core.bpf_jit_kallsyms=1
      net.core.bpf_jit_kallsyms = 1
    
      # bpftool prog dump xlated id 1
       0: (85) call pc+2#bpf_prog_3b185187f1855c4c_F
       1: (b7) r0 = 1
       2: (95) exit
       3: (b7) r0 = 2
       4: (95) exit
    
    And finally, an example for tail calls that is now working
    as well wrt correlation:
    
      # bpftool prog dump xlated id 2
      [...]
      10: (b7) r2 = 8
      11: (85) call bpf_trace_printk#-41312
      12: (bf) r1 = r6
      13: (18) r2 = map[id:1]
      15: (b7) r3 = 0
      16: (85) call bpf_tail_call#12
      17: (b7) r1 = 42
      18: (6b) *(u16 *)(r6 +46) = r1
      19: (b7) r0 = 0
      20: (95) exit
    
      # bpftool map show id 1
      1: prog_array  flags 0x0
            key 4B  value 4B  max_entries 1  memlock 4096B
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 30e728dcd35d..007802c5ca7d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1558,6 +1558,67 @@ static int bpf_map_get_fd_by_id(const union bpf_attr *attr)
 	return fd;
 }
 
+static const struct bpf_map *bpf_map_from_imm(const struct bpf_prog *prog,
+					      unsigned long addr)
+{
+	int i;
+
+	for (i = 0; i < prog->aux->used_map_cnt; i++)
+		if (prog->aux->used_maps[i] == (void *)addr)
+			return prog->aux->used_maps[i];
+	return NULL;
+}
+
+static struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog)
+{
+	const struct bpf_map *map;
+	struct bpf_insn *insns;
+	u64 imm;
+	int i;
+
+	insns = kmemdup(prog->insnsi, bpf_prog_insn_size(prog),
+			GFP_USER);
+	if (!insns)
+		return insns;
+
+	for (i = 0; i < prog->len; i++) {
+		if (insns[i].code == (BPF_JMP | BPF_TAIL_CALL)) {
+			insns[i].code = BPF_JMP | BPF_CALL;
+			insns[i].imm = BPF_FUNC_tail_call;
+			/* fall-through */
+		}
+		if (insns[i].code == (BPF_JMP | BPF_CALL) ||
+		    insns[i].code == (BPF_JMP | BPF_CALL_ARGS)) {
+			if (insns[i].code == (BPF_JMP | BPF_CALL_ARGS))
+				insns[i].code = BPF_JMP | BPF_CALL;
+			if (!bpf_dump_raw_ok())
+				insns[i].imm = 0;
+			continue;
+		}
+
+		if (insns[i].code != (BPF_LD | BPF_IMM | BPF_DW))
+			continue;
+
+		imm = ((u64)insns[i + 1].imm << 32) | (u32)insns[i].imm;
+		map = bpf_map_from_imm(prog, imm);
+		if (map) {
+			insns[i].src_reg = BPF_PSEUDO_MAP_FD;
+			insns[i].imm = map->id;
+			insns[i + 1].imm = 0;
+			continue;
+		}
+
+		if (!bpf_dump_raw_ok() &&
+		    imm == (unsigned long)prog->aux) {
+			insns[i].imm = 0;
+			insns[i + 1].imm = 0;
+			continue;
+		}
+	}
+
+	return insns;
+}
+
 static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 				   const union bpf_attr *attr,
 				   union bpf_attr __user *uattr)
@@ -1608,18 +1669,34 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	ulen = info.jited_prog_len;
 	info.jited_prog_len = prog->jited_len;
 	if (info.jited_prog_len && ulen) {
-		uinsns = u64_to_user_ptr(info.jited_prog_insns);
-		ulen = min_t(u32, info.jited_prog_len, ulen);
-		if (copy_to_user(uinsns, prog->bpf_func, ulen))
-			return -EFAULT;
+		if (bpf_dump_raw_ok()) {
+			uinsns = u64_to_user_ptr(info.jited_prog_insns);
+			ulen = min_t(u32, info.jited_prog_len, ulen);
+			if (copy_to_user(uinsns, prog->bpf_func, ulen))
+				return -EFAULT;
+		} else {
+			info.jited_prog_insns = 0;
+		}
 	}
 
 	ulen = info.xlated_prog_len;
 	info.xlated_prog_len = bpf_prog_insn_size(prog);
 	if (info.xlated_prog_len && ulen) {
+		struct bpf_insn *insns_sanitized;
+		bool fault;
+
+		if (prog->blinded && !bpf_dump_raw_ok()) {
+			info.xlated_prog_insns = 0;
+			goto done;
+		}
+		insns_sanitized = bpf_insn_prepare_dump(prog);
+		if (!insns_sanitized)
+			return -ENOMEM;
 		uinsns = u64_to_user_ptr(info.xlated_prog_insns);
 		ulen = min_t(u32, info.xlated_prog_len, ulen);
-		if (copy_to_user(uinsns, prog->insnsi, ulen))
+		fault = copy_to_user(uinsns, insns_sanitized, ulen);
+		kfree(insns_sanitized);
+		if (fault)
 			return -EFAULT;
 	}
 

commit 4f74d80971bce93d9e608c40324d662c70eb4664
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Dec 20 13:42:56 2017 +0100

    bpf: fix kallsyms handling for subprogs
    
    Right now kallsyms handling is not working with JITed subprogs.
    The reason is that when in 1c2a088a6626 ("bpf: x64: add JIT support
    for multi-function programs") in jit_subprogs() they are passed
    to bpf_prog_kallsyms_add(), then their prog type is 0, which BPF
    core will think it's a cBPF program as only cBPF programs have a
    0 type. Thus, they need to inherit the type from the main prog.
    
    Once that is fixed, they are indeed added to the BPF kallsyms
    infra, but their tag is 0. Therefore, since intention is to add
    them as bpf_prog_F_<tag>, we need to pass them to bpf_prog_calc_tag()
    first. And once this is resolved, there is a use-after-free on
    prog cleanup: we remove the kallsyms entry from the main prog,
    later walk all subprogs and call bpf_jit_free() on them. However,
    the kallsyms linkage was never released on them. Thus, do that
    for all subprogs right in __bpf_prog_put() when refcount hits 0.
    
    Fixes: 1c2a088a6626 ("bpf: x64: add JIT support for multi-function programs")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index e2e1c78ce1dc..30e728dcd35d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -937,10 +937,16 @@ static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
+		int i;
+
 		trace_bpf_prog_put_rcu(prog);
 		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog, do_idr_lock);
+
+		for (i = 0; i < prog->aux->func_cnt; i++)
+			bpf_prog_kallsyms_del(prog->aux->func[i]);
 		bpf_prog_kallsyms_del(prog);
+
 		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
 	}
 }

commit 1c2a088a6626d4f51d2f2c97b0cbedbfbf3637f6
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Dec 14 17:55:15 2017 -0800

    bpf: x64: add JIT support for multi-function programs
    
    Typical JIT does several passes over bpf instructions to
    compute total size and relative offsets of jumps and calls.
    With multitple bpf functions calling each other all relative calls
    will have invalid offsets intially therefore we need to additional
    last pass over the program to emit calls with correct offsets.
    For example in case of three bpf functions:
    main:
      call foo
      call bpf_map_lookup
      exit
    foo:
      call bar
      exit
    bar:
      exit
    
    We will call bpf_int_jit_compile() indepedently for main(), foo() and bar()
    x64 JIT typically does 4-5 passes to converge.
    After these initial passes the image for these 3 functions
    will be good except call targets, since start addresses of
    foo() and bar() are unknown when we were JITing main()
    (note that call bpf_map_lookup will be resolved properly
    during initial passes).
    Once start addresses of 3 functions are known we patch
    call_insn->imm to point to right functions and call
    bpf_int_jit_compile() again which needs only one pass.
    Additional safety checks are done to make sure this
    last pass doesn't produce image that is larger or smaller
    than previous pass.
    
    When constant blinding is on it's applied to all functions
    at the first pass, since doing it once again at the last
    pass can change size of the JITed code.
    
    Tested on x64 and arm64 hw with JIT on/off, blinding on/off.
    x64 jits bpf-to-bpf calls correctly while arm64 falls back to interpreter.
    All other JITs that support normal BPF_CALL will behave the same way
    since bpf-to-bpf call is equivalent to bpf-to-kernel call from
    JITs point of view.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2c4cfeaa8d5e..e2e1c78ce1dc 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1194,7 +1194,8 @@ static int bpf_prog_load(union bpf_attr *attr)
 		goto free_used_maps;
 
 	/* eBPF program is ready to be JITed */
-	prog = bpf_prog_select_runtime(prog, &err);
+	if (!prog->bpf_func)
+		prog = bpf_prog_select_runtime(prog, &err);
 	if (err < 0)
 		goto free_used_maps;
 

commit 1ee640095f049e7ac4ec36b985abada497b98cc2
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Nov 20 15:21:59 2017 -0800

    bpf: revert report offload info to user space
    
    This reverts commit bd601b6ada11 ("bpf: report offload info to user
    space").  The ifindex by itself is not sufficient, we should provide
    information on which network namespace this ifindex belongs to.
    After considering some options we concluded that it's best to just
    remove this API for now, and rework it in -next.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 41509cf825d8..2c4cfeaa8d5e 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1616,11 +1616,6 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 			return -EFAULT;
 	}
 
-	if (bpf_prog_is_dev_bound(prog->aux)) {
-		info.status |= BPF_PROG_STATUS_DEV_BOUND;
-		info.ifindex = bpf_prog_offload_ifindex(prog);
-	}
-
 done:
 	if (copy_to_user(uinfo, &info, info_len) ||
 	    put_user(info_len, &uattr->info.info_len))

commit 479321e9c31a6c05426790b11888427400f75ac8
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Nov 20 15:21:56 2017 -0800

    bpf: turn bpf_prog_get_type() into a wrapper
    
    bpf_prog_get_type() is identical to bpf_prog_get_type_dev(),
    with false passed as attach_drv.  Instead of keeping it as
    an exported symbol turn it into static inline wrapper.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 38da55905ab0..41509cf825d8 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1097,16 +1097,6 @@ struct bpf_prog *bpf_prog_get(u32 ufd)
 	return __bpf_prog_get(ufd, NULL, false);
 }
 
-struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
-{
-	struct bpf_prog *prog = __bpf_prog_get(ufd, &type, false);
-
-	if (!IS_ERR(prog))
-		trace_bpf_prog_get_type(prog);
-	return prog;
-}
-EXPORT_SYMBOL_GPL(bpf_prog_get_type);
-
 struct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,
 				       bool attach_drv)
 {

commit 288b3de55aace830f13280985ec9e6bcbff33b1b
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Nov 20 15:21:54 2017 -0800

    bpf: offload: move offload device validation out to the drivers
    
    With TC shared block changes we can't depend on correct netdev
    pointer being available in cls_bpf.  Move the device validation
    to the driver.  Core will only make sure that offloaded programs
    are always attached in the driver (or in HW by the driver).  We
    trust that drivers which implement offload callbacks will perform
    necessary checks.
    
    Moving the checks to the driver is generally a useful thing,
    in practice the check should be against a switchdev instance,
    not a netdev, given that most ASICs will probably allow using
    the same program on many ports.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 8e9d065bb7cd..38da55905ab0 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1057,22 +1057,23 @@ struct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)
 }
 EXPORT_SYMBOL_GPL(bpf_prog_inc_not_zero);
 
-static bool bpf_prog_can_attach(struct bpf_prog *prog,
-				enum bpf_prog_type *attach_type,
-				struct net_device *netdev)
+static bool bpf_prog_get_ok(struct bpf_prog *prog,
+			    enum bpf_prog_type *attach_type, bool attach_drv)
 {
-	struct bpf_dev_offload *offload = prog->aux->offload;
+	/* not an attachment, just a refcount inc, always allow */
+	if (!attach_type)
+		return true;
 
 	if (prog->type != *attach_type)
 		return false;
-	if (offload && offload->netdev != netdev)
+	if (bpf_prog_is_dev_bound(prog->aux) && !attach_drv)
 		return false;
 
 	return true;
 }
 
 static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *attach_type,
-				       struct net_device *netdev)
+				       bool attach_drv)
 {
 	struct fd f = fdget(ufd);
 	struct bpf_prog *prog;
@@ -1080,7 +1081,7 @@ static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *attach_type,
 	prog = ____bpf_prog_get(f);
 	if (IS_ERR(prog))
 		return prog;
-	if (attach_type && !bpf_prog_can_attach(prog, attach_type, netdev)) {
+	if (!bpf_prog_get_ok(prog, attach_type, attach_drv)) {
 		prog = ERR_PTR(-EINVAL);
 		goto out;
 	}
@@ -1093,12 +1094,12 @@ static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *attach_type,
 
 struct bpf_prog *bpf_prog_get(u32 ufd)
 {
-	return __bpf_prog_get(ufd, NULL, NULL);
+	return __bpf_prog_get(ufd, NULL, false);
 }
 
 struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
 {
-	struct bpf_prog *prog = __bpf_prog_get(ufd, &type, NULL);
+	struct bpf_prog *prog = __bpf_prog_get(ufd, &type, false);
 
 	if (!IS_ERR(prog))
 		trace_bpf_prog_get_type(prog);
@@ -1107,9 +1108,9 @@ struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
 EXPORT_SYMBOL_GPL(bpf_prog_get_type);
 
 struct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,
-				       struct net_device *netdev)
+				       bool attach_drv)
 {
-	struct bpf_prog *prog = __bpf_prog_get(ufd, &type, netdev);
+	struct bpf_prog *prog = __bpf_prog_get(ufd, &type, attach_drv);
 
 	if (!IS_ERR(prog))
 		trace_bpf_prog_get_type(prog);

commit 1f6f4cb7ba219b00a3fa9afe8049fa16444d8b52
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Nov 20 15:21:53 2017 -0800

    bpf: offload: rename the ifindex field
    
    bpf_target_prog seems long and clunky, rename it to prog_ifindex.
    We don't want to call this field just ifindex, because maps
    may need a similar field in the future and bpf_attr members for
    programs and maps are unnamed.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 09badc37e864..8e9d065bb7cd 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1118,7 +1118,7 @@ struct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,
 EXPORT_SYMBOL_GPL(bpf_prog_get_type_dev);
 
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD prog_target_ifindex
+#define	BPF_PROG_LOAD_LAST_FIELD prog_ifindex
 
 static int bpf_prog_load(union bpf_attr *attr)
 {
@@ -1181,7 +1181,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 	atomic_set(&prog->aux->refcnt, 1);
 	prog->gpl_compatible = is_gpl ? 1 : 0;
 
-	if (attr->prog_target_ifindex) {
+	if (attr->prog_ifindex) {
 		err = bpf_prog_offload_init(prog, attr);
 		if (err)
 			goto free_prog;

commit ebc614f687369f9df99828572b1d85a7c2de3d92
Author: Roman Gushchin <guro@fb.com>
Date:   Sun Nov 5 08:15:32 2017 -0500

    bpf, cgroup: implement eBPF-based device controller for cgroup v2
    
    Cgroup v2 lacks the device controller, provided by cgroup v1.
    This patch adds a new eBPF program type, which in combination
    of previously added ability to attach multiple eBPF programs
    to a cgroup, will provide a similar functionality, but with some
    additional flexibility.
    
    This patch introduces a BPF_PROG_TYPE_CGROUP_DEVICE program type.
    A program takes major and minor device numbers, device type
    (block/character) and access type (mknod/read/write) as parameters
    and returns an integer which defines if the operation should be
    allowed or terminated with -EPERM.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 416d70cdfc76..09badc37e864 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1326,6 +1326,9 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_SOCK_OPS:
 		ptype = BPF_PROG_TYPE_SOCK_OPS;
 		break;
+	case BPF_CGROUP_DEVICE:
+		ptype = BPF_PROG_TYPE_CGROUP_DEVICE;
+		break;
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
 		return sockmap_get_from_fd(attr, true);
@@ -1378,6 +1381,9 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_CGROUP_SOCK_OPS:
 		ptype = BPF_PROG_TYPE_SOCK_OPS;
 		break;
+	case BPF_CGROUP_DEVICE:
+		ptype = BPF_PROG_TYPE_CGROUP_DEVICE;
+		break;
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
 		return sockmap_get_from_fd(attr, false);
@@ -1420,6 +1426,7 @@ static int bpf_prog_query(const union bpf_attr *attr,
 	case BPF_CGROUP_INET_EGRESS:
 	case BPF_CGROUP_INET_SOCK_CREATE:
 	case BPF_CGROUP_SOCK_OPS:
+	case BPF_CGROUP_DEVICE:
 		break;
 	default:
 		return -EINVAL;

commit 6c8dfe21c435cf2953e3cee43e12180cbc4f0820
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Nov 3 13:56:21 2017 -0700

    cls_bpf: allow attaching programs loaded for specific device
    
    If TC program is loaded with skip_sw flag, we should allow
    the device-specific programs to be accepted.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 68f9123acd39..416d70cdfc76 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1115,6 +1115,7 @@ struct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,
 		trace_bpf_prog_get_type(prog);
 	return prog;
 }
+EXPORT_SYMBOL_GPL(bpf_prog_get_type_dev);
 
 /* last field in 'union bpf_attr' used by this command */
 #define	BPF_PROG_LOAD_LAST_FIELD prog_target_ifindex

commit 248f346ffe9508dee0039db4ac839cb31ba3bdec
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Nov 3 13:56:20 2017 -0700

    xdp: allow attaching programs loaded for specific device
    
    Pass the netdev pointer to bpf_prog_get_type().  This way
    BPF code can decide whether the device matches what the
    code was loaded/translated for.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 3217c20ea91b..68f9123acd39 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1057,7 +1057,22 @@ struct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)
 }
 EXPORT_SYMBOL_GPL(bpf_prog_inc_not_zero);
 
-static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *attach_type)
+static bool bpf_prog_can_attach(struct bpf_prog *prog,
+				enum bpf_prog_type *attach_type,
+				struct net_device *netdev)
+{
+	struct bpf_dev_offload *offload = prog->aux->offload;
+
+	if (prog->type != *attach_type)
+		return false;
+	if (offload && offload->netdev != netdev)
+		return false;
+
+	return true;
+}
+
+static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *attach_type,
+				       struct net_device *netdev)
 {
 	struct fd f = fdget(ufd);
 	struct bpf_prog *prog;
@@ -1065,7 +1080,7 @@ static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *attach_type)
 	prog = ____bpf_prog_get(f);
 	if (IS_ERR(prog))
 		return prog;
-	if (attach_type && (prog->type != *attach_type || prog->aux->offload)) {
+	if (attach_type && !bpf_prog_can_attach(prog, attach_type, netdev)) {
 		prog = ERR_PTR(-EINVAL);
 		goto out;
 	}
@@ -1078,12 +1093,12 @@ static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *attach_type)
 
 struct bpf_prog *bpf_prog_get(u32 ufd)
 {
-	return __bpf_prog_get(ufd, NULL);
+	return __bpf_prog_get(ufd, NULL, NULL);
 }
 
 struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
 {
-	struct bpf_prog *prog = __bpf_prog_get(ufd, &type);
+	struct bpf_prog *prog = __bpf_prog_get(ufd, &type, NULL);
 
 	if (!IS_ERR(prog))
 		trace_bpf_prog_get_type(prog);
@@ -1091,6 +1106,16 @@ struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
 }
 EXPORT_SYMBOL_GPL(bpf_prog_get_type);
 
+struct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,
+				       struct net_device *netdev)
+{
+	struct bpf_prog *prog = __bpf_prog_get(ufd, &type, netdev);
+
+	if (!IS_ERR(prog))
+		trace_bpf_prog_get_type(prog);
+	return prog;
+}
+
 /* last field in 'union bpf_attr' used by this command */
 #define	BPF_PROG_LOAD_LAST_FIELD prog_target_ifindex
 

commit bd601b6ada11fdfb9e277f24ad2eb54bc599156b
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Nov 3 13:56:18 2017 -0700

    bpf: report offload info to user space
    
    Extend struct bpf_prog_info to contain information about program
    being bound to a device.  Since the netdev may get destroyed while
    program still exists we need a flag to indicate the program is
    loaded for a device, even if the device is gone.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 1574b9f0f24e..3217c20ea91b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1592,6 +1592,11 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 			return -EFAULT;
 	}
 
+	if (bpf_prog_is_dev_bound(prog->aux)) {
+		info.status |= BPF_PROG_STATUS_DEV_BOUND;
+		info.ifindex = bpf_prog_offload_ifindex(prog);
+	}
+
 done:
 	if (copy_to_user(uinfo, &info, info_len) ||
 	    put_user(info_len, &uattr->info.info_len))

commit ab3f0063c48c26c927851b6767824e35a716d878
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Nov 3 13:56:17 2017 -0700

    bpf: offload: add infrastructure for loading programs for a specific netdev
    
    The fact that we don't know which device the program is going
    to be used on is quite limiting in current eBPF infrastructure.
    We have to reverse or limit the changes which kernel makes to
    the loaded bytecode if we want it to be offloaded to a networking
    device.  We also have to invent new APIs for debugging and
    troubleshooting support.
    
    Make it possible to load programs for a specific netdev.  This
    helps us to bring the debug information closer to the core
    eBPF infrastructure (e.g. we will be able to reuse the verifer
    log in device JIT).  It allows device JITs to perform translation
    on the original bytecode.
    
    __bpf_prog_get() when called to get a reference for an attachment
    point will now refuse to give it if program has a device assigned.
    Following patches will add a version of that function which passes
    the expected netdev in. @type argument in __bpf_prog_get() is
    renamed to attach_type to make it clearer that it's only set on
    attachment.
    
    All calls to ndo_bpf are protected by rtnl, only verifier callbacks
    are not.  We need a wait queue to make sure netdev doesn't get
    destroyed while verifier is still running and calling its driver.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Simon Horman <simon.horman@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 323be2473c4b..1574b9f0f24e 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -824,7 +824,10 @@ static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
 	if (type >= ARRAY_SIZE(bpf_prog_types) || !bpf_prog_types[type])
 		return -EINVAL;
 
-	prog->aux->ops = bpf_prog_types[type];
+	if (!bpf_prog_is_dev_bound(prog->aux))
+		prog->aux->ops = bpf_prog_types[type];
+	else
+		prog->aux->ops = &bpf_offload_prog_ops;
 	prog->type = type;
 	return 0;
 }
@@ -1054,7 +1057,7 @@ struct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)
 }
 EXPORT_SYMBOL_GPL(bpf_prog_inc_not_zero);
 
-static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *type)
+static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *attach_type)
 {
 	struct fd f = fdget(ufd);
 	struct bpf_prog *prog;
@@ -1062,7 +1065,7 @@ static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *type)
 	prog = ____bpf_prog_get(f);
 	if (IS_ERR(prog))
 		return prog;
-	if (type && prog->type != *type) {
+	if (attach_type && (prog->type != *attach_type || prog->aux->offload)) {
 		prog = ERR_PTR(-EINVAL);
 		goto out;
 	}
@@ -1089,7 +1092,7 @@ struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
 EXPORT_SYMBOL_GPL(bpf_prog_get_type);
 
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD prog_name
+#define	BPF_PROG_LOAD_LAST_FIELD prog_target_ifindex
 
 static int bpf_prog_load(union bpf_attr *attr)
 {
@@ -1152,6 +1155,12 @@ static int bpf_prog_load(union bpf_attr *attr)
 	atomic_set(&prog->aux->refcnt, 1);
 	prog->gpl_compatible = is_gpl ? 1 : 0;
 
+	if (attr->prog_target_ifindex) {
+		err = bpf_prog_offload_init(prog, attr);
+		if (err)
+			goto free_prog;
+	}
+
 	/* find program type: socket_filter vs tracing_filter */
 	err = find_prog_type(type, prog);
 	if (err < 0)

commit f66e448cfda021b0bcd884f26709796fe19c7cc1
Author: Chenbo Feng <fengc@google.com>
Date:   Wed Oct 18 13:00:26 2017 -0700

    selinux: bpf: Add addtional check for bpf object file receive
    
    Introduce a bpf object related check when sending and receiving files
    through unix domain socket as well as binder. It checks if the receiving
    process have privilege to read/write the bpf map or use the bpf program.
    This check is necessary because the bpf maps and programs are using a
    anonymous inode as their shared inode so the normal way of checking the
    files and sockets when passing between processes cannot work properly on
    eBPF object. This check only works when the BPF_SYSCALL is configured.
    
    Signed-off-by: Chenbo Feng <fengc@google.com>
    Acked-by: Stephen Smalley <sds@tycho.nsa.gov>
    Reviewed-by: James Morris <james.l.morris@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5cb56d06b48d..323be2473c4b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -315,7 +315,7 @@ static ssize_t bpf_dummy_write(struct file *filp, const char __user *buf,
 	return -EINVAL;
 }
 
-static const struct file_operations bpf_map_fops = {
+const struct file_operations bpf_map_fops = {
 #ifdef CONFIG_PROC_FS
 	.show_fdinfo	= bpf_map_show_fdinfo,
 #endif
@@ -975,7 +975,7 @@ static void bpf_prog_show_fdinfo(struct seq_file *m, struct file *filp)
 }
 #endif
 
-static const struct file_operations bpf_prog_fops = {
+const struct file_operations bpf_prog_fops = {
 #ifdef CONFIG_PROC_FS
 	.show_fdinfo	= bpf_prog_show_fdinfo,
 #endif

commit afdb09c720b62b8090584c11151d856df330e57d
Author: Chenbo Feng <fengc@google.com>
Date:   Wed Oct 18 13:00:24 2017 -0700

    security: bpf: Add LSM hooks for bpf object related syscall
    
    Introduce several LSM hooks for the syscalls that will allow the
    userspace to access to eBPF object such as eBPF programs and eBPF maps.
    The security check is aimed to enforce a per object security protection
    for eBPF object so only processes with the right priviliges can
    read/write to a specific map or use a specific eBPF program. Besides
    that, a general security hook is added before the multiplexer of bpf
    syscall to check the cmd and the attribute used for the command. The
    actual security module can decide which command need to be checked and
    how the cmd should be checked.
    
    Signed-off-by: Chenbo Feng <fengc@google.com>
    Acked-by: James Morris <james.l.morris@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 676a06e6b322..5cb56d06b48d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -212,6 +212,7 @@ static void bpf_map_free_deferred(struct work_struct *work)
 	struct bpf_map *map = container_of(work, struct bpf_map, work);
 
 	bpf_map_uncharge_memlock(map);
+	security_bpf_map_free(map);
 	/* implementation dependent freeing */
 	map->ops->map_free(map);
 }
@@ -325,6 +326,12 @@ static const struct file_operations bpf_map_fops = {
 
 int bpf_map_new_fd(struct bpf_map *map, int flags)
 {
+	int ret;
+
+	ret = security_bpf_map(map, OPEN_FMODE(flags));
+	if (ret < 0)
+		return ret;
+
 	return anon_inode_getfd("bpf-map", &bpf_map_fops, map,
 				flags | O_CLOEXEC);
 }
@@ -405,10 +412,14 @@ static int map_create(union bpf_attr *attr)
 	atomic_set(&map->refcnt, 1);
 	atomic_set(&map->usercnt, 1);
 
-	err = bpf_map_charge_memlock(map);
+	err = security_bpf_map_alloc(map);
 	if (err)
 		goto free_map_nouncharge;
 
+	err = bpf_map_charge_memlock(map);
+	if (err)
+		goto free_map_sec;
+
 	err = bpf_map_alloc_id(map);
 	if (err)
 		goto free_map;
@@ -430,6 +441,8 @@ static int map_create(union bpf_attr *attr)
 
 free_map:
 	bpf_map_uncharge_memlock(map);
+free_map_sec:
+	security_bpf_map_free(map);
 free_map_nouncharge:
 	map->ops->map_free(map);
 	return err;
@@ -914,6 +927,7 @@ static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 
 	free_used_maps(aux);
 	bpf_prog_uncharge_memlock(aux->prog);
+	security_bpf_prog_free(aux);
 	bpf_prog_free(aux->prog);
 }
 
@@ -972,6 +986,12 @@ static const struct file_operations bpf_prog_fops = {
 
 int bpf_prog_new_fd(struct bpf_prog *prog)
 {
+	int ret;
+
+	ret = security_bpf_prog(prog);
+	if (ret < 0)
+		return ret;
+
 	return anon_inode_getfd("bpf-prog", &bpf_prog_fops, prog,
 				O_RDWR | O_CLOEXEC);
 }
@@ -1111,10 +1131,14 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (!prog)
 		return -ENOMEM;
 
-	err = bpf_prog_charge_memlock(prog);
+	err = security_bpf_prog_alloc(prog->aux);
 	if (err)
 		goto free_prog_nouncharge;
 
+	err = bpf_prog_charge_memlock(prog);
+	if (err)
+		goto free_prog_sec;
+
 	prog->len = attr->insn_cnt;
 
 	err = -EFAULT;
@@ -1172,6 +1196,8 @@ static int bpf_prog_load(union bpf_attr *attr)
 	free_used_maps(prog->aux);
 free_prog:
 	bpf_prog_uncharge_memlock(prog);
+free_prog_sec:
+	security_bpf_prog_free(prog->aux);
 free_prog_nouncharge:
 	bpf_prog_free(prog);
 	return err;
@@ -1640,6 +1666,10 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	if (copy_from_user(&attr, uattr, size) != 0)
 		return -EFAULT;
 
+	err = security_bpf(cmd, &attr, size);
+	if (err < 0)
+		return err;
+
 	switch (cmd) {
 	case BPF_MAP_CREATE:
 		err = map_create(&attr);

commit 6e71b04a82248ccf13a94b85cbc674a9fefe53f5
Author: Chenbo Feng <fengc@google.com>
Date:   Wed Oct 18 13:00:22 2017 -0700

    bpf: Add file mode configuration into bpf maps
    
    Introduce the map read/write flags to the eBPF syscalls that returns the
    map fd. The flags is used to set up the file mode when construct a new
    file descriptor for bpf maps. To not break the backward capability, the
    f_flags is set to O_RDWR if the flag passed by syscall is 0. Otherwise
    it should be O_RDONLY or O_WRONLY. When the userspace want to modify or
    read the map content, it will check the file mode to see if it is
    allowed to make the change.
    
    Signed-off-by: Chenbo Feng <fengc@google.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0e893cac6795..676a06e6b322 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -34,6 +34,8 @@
 #define IS_FD_HASH(map) ((map)->map_type == BPF_MAP_TYPE_HASH_OF_MAPS)
 #define IS_FD_MAP(map) (IS_FD_ARRAY(map) || IS_FD_HASH(map))
 
+#define BPF_OBJ_FLAG_MASK   (BPF_F_RDONLY | BPF_F_WRONLY)
+
 DEFINE_PER_CPU(int, bpf_prog_active);
 static DEFINE_IDR(prog_idr);
 static DEFINE_SPINLOCK(prog_idr_lock);
@@ -294,17 +296,48 @@ static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 }
 #endif
 
+static ssize_t bpf_dummy_read(struct file *filp, char __user *buf, size_t siz,
+			      loff_t *ppos)
+{
+	/* We need this handler such that alloc_file() enables
+	 * f_mode with FMODE_CAN_READ.
+	 */
+	return -EINVAL;
+}
+
+static ssize_t bpf_dummy_write(struct file *filp, const char __user *buf,
+			       size_t siz, loff_t *ppos)
+{
+	/* We need this handler such that alloc_file() enables
+	 * f_mode with FMODE_CAN_WRITE.
+	 */
+	return -EINVAL;
+}
+
 static const struct file_operations bpf_map_fops = {
 #ifdef CONFIG_PROC_FS
 	.show_fdinfo	= bpf_map_show_fdinfo,
 #endif
 	.release	= bpf_map_release,
+	.read		= bpf_dummy_read,
+	.write		= bpf_dummy_write,
 };
 
-int bpf_map_new_fd(struct bpf_map *map)
+int bpf_map_new_fd(struct bpf_map *map, int flags)
 {
 	return anon_inode_getfd("bpf-map", &bpf_map_fops, map,
-				O_RDWR | O_CLOEXEC);
+				flags | O_CLOEXEC);
+}
+
+int bpf_get_file_flag(int flags)
+{
+	if ((flags & BPF_F_RDONLY) && (flags & BPF_F_WRONLY))
+		return -EINVAL;
+	if (flags & BPF_F_RDONLY)
+		return O_RDONLY;
+	if (flags & BPF_F_WRONLY)
+		return O_WRONLY;
+	return O_RDWR;
 }
 
 /* helper macro to check that unused fields 'union bpf_attr' are zero */
@@ -344,12 +377,17 @@ static int map_create(union bpf_attr *attr)
 {
 	int numa_node = bpf_map_attr_numa_node(attr);
 	struct bpf_map *map;
+	int f_flags;
 	int err;
 
 	err = CHECK_ATTR(BPF_MAP_CREATE);
 	if (err)
 		return -EINVAL;
 
+	f_flags = bpf_get_file_flag(attr->map_flags);
+	if (f_flags < 0)
+		return f_flags;
+
 	if (numa_node != NUMA_NO_NODE &&
 	    ((unsigned int)numa_node >= nr_node_ids ||
 	     !node_online(numa_node)))
@@ -375,7 +413,7 @@ static int map_create(union bpf_attr *attr)
 	if (err)
 		goto free_map;
 
-	err = bpf_map_new_fd(map);
+	err = bpf_map_new_fd(map, f_flags);
 	if (err < 0) {
 		/* failed to allocate fd.
 		 * bpf_map_put() is needed because the above
@@ -490,6 +528,11 @@ static int map_lookup_elem(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
+	if (!(f.file->f_mode & FMODE_CAN_READ)) {
+		err = -EPERM;
+		goto err_put;
+	}
+
 	key = memdup_user(ukey, map->key_size);
 	if (IS_ERR(key)) {
 		err = PTR_ERR(key);
@@ -570,6 +613,11 @@ static int map_update_elem(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
+	if (!(f.file->f_mode & FMODE_CAN_WRITE)) {
+		err = -EPERM;
+		goto err_put;
+	}
+
 	key = memdup_user(ukey, map->key_size);
 	if (IS_ERR(key)) {
 		err = PTR_ERR(key);
@@ -659,6 +707,11 @@ static int map_delete_elem(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
+	if (!(f.file->f_mode & FMODE_CAN_WRITE)) {
+		err = -EPERM;
+		goto err_put;
+	}
+
 	key = memdup_user(ukey, map->key_size);
 	if (IS_ERR(key)) {
 		err = PTR_ERR(key);
@@ -702,6 +755,11 @@ static int map_get_next_key(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
+	if (!(f.file->f_mode & FMODE_CAN_READ)) {
+		err = -EPERM;
+		goto err_put;
+	}
+
 	if (ukey) {
 		key = memdup_user(ukey, map->key_size);
 		if (IS_ERR(key)) {
@@ -908,6 +966,8 @@ static const struct file_operations bpf_prog_fops = {
 	.show_fdinfo	= bpf_prog_show_fdinfo,
 #endif
 	.release	= bpf_prog_release,
+	.read		= bpf_dummy_read,
+	.write		= bpf_dummy_write,
 };
 
 int bpf_prog_new_fd(struct bpf_prog *prog)
@@ -1117,11 +1177,11 @@ static int bpf_prog_load(union bpf_attr *attr)
 	return err;
 }
 
-#define BPF_OBJ_LAST_FIELD bpf_fd
+#define BPF_OBJ_LAST_FIELD file_flags
 
 static int bpf_obj_pin(const union bpf_attr *attr)
 {
-	if (CHECK_ATTR(BPF_OBJ))
+	if (CHECK_ATTR(BPF_OBJ) || attr->file_flags != 0)
 		return -EINVAL;
 
 	return bpf_obj_pin_user(attr->bpf_fd, u64_to_user_ptr(attr->pathname));
@@ -1129,10 +1189,12 @@ static int bpf_obj_pin(const union bpf_attr *attr)
 
 static int bpf_obj_get(const union bpf_attr *attr)
 {
-	if (CHECK_ATTR(BPF_OBJ) || attr->bpf_fd != 0)
+	if (CHECK_ATTR(BPF_OBJ) || attr->bpf_fd != 0 ||
+	    attr->file_flags & ~BPF_OBJ_FLAG_MASK)
 		return -EINVAL;
 
-	return bpf_obj_get_user(u64_to_user_ptr(attr->pathname));
+	return bpf_obj_get_user(u64_to_user_ptr(attr->pathname),
+				attr->file_flags);
 }
 
 #ifdef CONFIG_CGROUP_BPF
@@ -1392,20 +1454,26 @@ static int bpf_prog_get_fd_by_id(const union bpf_attr *attr)
 	return fd;
 }
 
-#define BPF_MAP_GET_FD_BY_ID_LAST_FIELD map_id
+#define BPF_MAP_GET_FD_BY_ID_LAST_FIELD open_flags
 
 static int bpf_map_get_fd_by_id(const union bpf_attr *attr)
 {
 	struct bpf_map *map;
 	u32 id = attr->map_id;
+	int f_flags;
 	int fd;
 
-	if (CHECK_ATTR(BPF_MAP_GET_FD_BY_ID))
+	if (CHECK_ATTR(BPF_MAP_GET_FD_BY_ID) ||
+	    attr->open_flags & ~BPF_OBJ_FLAG_MASK)
 		return -EINVAL;
 
 	if (!capable(CAP_SYS_ADMIN))
 		return -EPERM;
 
+	f_flags = bpf_get_file_flag(attr->open_flags);
+	if (f_flags < 0)
+		return f_flags;
+
 	spin_lock_bh(&map_idr_lock);
 	map = idr_find(&map_idr, id);
 	if (map)
@@ -1417,7 +1485,7 @@ static int bpf_map_get_fd_by_id(const union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
-	fd = bpf_map_new_fd(map);
+	fd = bpf_map_new_fd(map, f_flags);
 	if (fd < 0)
 		bpf_map_put(map);
 

commit 00176a34d9e27ab1e77db75fe13abc005cffe0ca
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Oct 16 16:40:54 2017 -0700

    bpf: remove the verifier ops from program structure
    
    Since the verifier ops don't have to be associated with
    the program for its entire lifetime we can move it to
    verifier's struct bpf_verifier_env.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 444902b5a30d..0e893cac6795 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -748,22 +748,12 @@ static const struct bpf_prog_ops * const bpf_prog_types[] = {
 #undef BPF_MAP_TYPE
 };
 
-static const struct bpf_verifier_ops * const bpf_verifier_ops[] = {
-#define BPF_PROG_TYPE(_id, _name) \
-	[_id] = & _name ## _verifier_ops,
-#define BPF_MAP_TYPE(_id, _ops)
-#include <linux/bpf_types.h>
-#undef BPF_PROG_TYPE
-#undef BPF_MAP_TYPE
-};
-
 static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
 {
 	if (type >= ARRAY_SIZE(bpf_prog_types) || !bpf_prog_types[type])
 		return -EINVAL;
 
 	prog->aux->ops = bpf_prog_types[type];
-	prog->aux->vops = bpf_verifier_ops[type];
 	prog->type = type;
 	return 0;
 }

commit 7de16e3a35578f4f5accc6f5f23970310483d0a2
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Oct 16 16:40:53 2017 -0700

    bpf: split verifier and program ops
    
    struct bpf_verifier_ops contains both verifier ops and operations
    used later during program's lifetime (test_run).  Split the runtime
    ops into a different structure.
    
    BPF_PROG_TYPE() will now append ## _prog_ops or ## _verifier_ops
    to the names.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 54fba06942f5..444902b5a30d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -739,9 +739,18 @@ static int map_get_next_key(union bpf_attr *attr)
 	return err;
 }
 
-static const struct bpf_verifier_ops * const bpf_prog_types[] = {
-#define BPF_PROG_TYPE(_id, _ops) \
-	[_id] = &_ops,
+static const struct bpf_prog_ops * const bpf_prog_types[] = {
+#define BPF_PROG_TYPE(_id, _name) \
+	[_id] = & _name ## _prog_ops,
+#define BPF_MAP_TYPE(_id, _ops)
+#include <linux/bpf_types.h>
+#undef BPF_PROG_TYPE
+#undef BPF_MAP_TYPE
+};
+
+static const struct bpf_verifier_ops * const bpf_verifier_ops[] = {
+#define BPF_PROG_TYPE(_id, _name) \
+	[_id] = & _name ## _verifier_ops,
 #define BPF_MAP_TYPE(_id, _ops)
 #include <linux/bpf_types.h>
 #undef BPF_PROG_TYPE
@@ -754,6 +763,7 @@ static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
 		return -EINVAL;
 
 	prog->aux->ops = bpf_prog_types[type];
+	prog->aux->vops = bpf_verifier_ops[type];
 	prog->type = type;
 	return 0;
 }

commit 6710e1126934d8b4372b4d2f9ae1646cd3f151bf
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Mon Oct 16 12:19:28 2017 +0200

    bpf: introduce new bpf cpu map type BPF_MAP_TYPE_CPUMAP
    
    The 'cpumap' is primarily used as a backend map for XDP BPF helper
    call bpf_redirect_map() and XDP_REDIRECT action, like 'devmap'.
    
    This patch implement the main part of the map.  It is not connected to
    the XDP redirect system yet, and no SKB allocation are done yet.
    
    The main concern in this patch is to ensure the datapath can run
    without any locking.  This adds complexity to the setup and tear-down
    procedure, which assumptions are extra carefully documented in the
    code comments.
    
    V2:
     - make sure array isn't larger than NR_CPUS
     - make sure CPUs added is a valid possible CPU
    
    V3: fix nitpicks from Jakub Kicinski <kubakici@wp.pl>
    
    V5:
     - Restrict map allocation to root / CAP_SYS_ADMIN
     - WARN_ON_ONCE if queue is not empty on tear-down
     - Return -EPERM on memlock limit instead of -ENOMEM
     - Error code in __cpu_map_entry_alloc() also handle ptr_ring_cleanup()
     - Moved cpu_map_enqueue() to next patch
    
    V6: all notice by Daniel Borkmann
     - Fix err return code in cpu_map_alloc() introduced in V5
     - Move cpu_possible() check after max_entries boundary check
     - Forbid usage initially in check_map_func_compatibility()
    
    V7:
     - Fix alloc error path spotted by Daniel Borkmann
     - Did stress test adding+removing CPUs from the map concurrently
     - Fixed refcnt issue on cpu_map_entry, kthread started too soon
     - Make sure packets are flushed during tear-down, involved use of
       rcu_barrier() and kthread_run only exit after queue is empty
     - Fix alloc error path in __cpu_map_entry_alloc() for ptr_ring
    
    V8:
     - Nitpicking comments and gramma by Edward Cree
     - Fix missing semi-colon introduced in V7 due to rebasing
     - Move struct bpf_cpu_map_entry members cpu+map_id to tracepoint patch
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index d124e702e040..54fba06942f5 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -592,6 +592,12 @@ static int map_update_elem(union bpf_attr *attr)
 	if (copy_from_user(value, uvalue, value_size) != 0)
 		goto free_value;
 
+	/* Need to create a kthread, thus must support schedule */
+	if (map->map_type == BPF_MAP_TYPE_CPUMAP) {
+		err = map->ops->map_update_elem(map, key, value, attr->flags);
+		goto out;
+	}
+
 	/* must increment bpf_prog_active to avoid kprobe+bpf triggering from
 	 * inside bpf map update or delete otherwise deadlocks are possible
 	 */
@@ -622,7 +628,7 @@ static int map_update_elem(union bpf_attr *attr)
 	}
 	__this_cpu_dec(bpf_prog_active);
 	preempt_enable();
-
+out:
 	if (!err)
 		trace_bpf_map_update_elem(map, ufd, key, value);
 free_value:

commit 473d97343f94ff20f5196078314e4dd83156d3a2
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Thu Oct 5 21:52:11 2017 -0700

    bpf: Change bpf_obj_name_cpy() to better ensure map's name is init by 0
    
    During get_info_by_fd, the prog/map name is memcpy-ed.  It depends
    on the prog->aux->name and map->name to be zero initialized.
    
    bpf_prog_aux is easy to guarantee that aux->name is zero init.
    
    The name in bpf_map may be harder to be guaranteed in the future when
    new map type is added.
    
    Hence, this patch makes bpf_obj_name_cpy() to always zero init
    the prog/map name.
    
    Suggested-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0048cb24ba7b..d124e702e040 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -322,6 +322,8 @@ static int bpf_obj_name_cpy(char *dst, const char *src)
 {
 	const char *end = src + BPF_OBJ_NAME_LEN;
 
+	memset(dst, 0, BPF_OBJ_NAME_LEN);
+
 	/* Copy all isalnum() and '_' char */
 	while (src < end && *src) {
 		if (!isalnum(*src) && *src != '_')
@@ -333,9 +335,6 @@ static int bpf_obj_name_cpy(char *dst, const char *src)
 	if (src == end)
 		return -EINVAL;
 
-	/* '\0' terminates dst */
-	*dst = 0;
-
 	return 0;
 }
 

commit 468e2f64d220fe2dc11caa2bcb9b3a1e50fc7321
Author: Alexei Starovoitov <ast@fb.com>
Date:   Mon Oct 2 22:50:22 2017 -0700

    bpf: introduce BPF_PROG_QUERY command
    
    introduce BPF_PROG_QUERY command to retrieve a set of either
    attached programs to given cgroup or a set of effective programs
    that will execute for events within a cgroup
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    for cgroup bits
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 51bee695d32c..0048cb24ba7b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1272,6 +1272,37 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	return ret;
 }
 
+#define BPF_PROG_QUERY_LAST_FIELD query.prog_cnt
+
+static int bpf_prog_query(const union bpf_attr *attr,
+			  union bpf_attr __user *uattr)
+{
+	struct cgroup *cgrp;
+	int ret;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+	if (CHECK_ATTR(BPF_PROG_QUERY))
+		return -EINVAL;
+	if (attr->query.query_flags & ~BPF_F_QUERY_EFFECTIVE)
+		return -EINVAL;
+
+	switch (attr->query.attach_type) {
+	case BPF_CGROUP_INET_INGRESS:
+	case BPF_CGROUP_INET_EGRESS:
+	case BPF_CGROUP_INET_SOCK_CREATE:
+	case BPF_CGROUP_SOCK_OPS:
+		break;
+	default:
+		return -EINVAL;
+	}
+	cgrp = cgroup_get_from_fd(attr->query.target_fd);
+	if (IS_ERR(cgrp))
+		return PTR_ERR(cgrp);
+	ret = cgroup_bpf_query(cgrp, attr, uattr);
+	cgroup_put(cgrp);
+	return ret;
+}
 #endif /* CONFIG_CGROUP_BPF */
 
 #define BPF_PROG_TEST_RUN_LAST_FIELD test.duration
@@ -1568,6 +1599,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_PROG_DETACH:
 		err = bpf_prog_detach(&attr);
 		break;
+	case BPF_PROG_QUERY:
+		err = bpf_prog_query(&attr, uattr);
+		break;
 #endif
 	case BPF_PROG_TEST_RUN:
 		err = bpf_prog_test_run(&attr, uattr);

commit 324bda9e6c5add86ba2e1066476481c48132aca0
Author: Alexei Starovoitov <ast@fb.com>
Date:   Mon Oct 2 22:50:21 2017 -0700

    bpf: multi program support for cgroup+bpf
    
    introduce BPF_F_ALLOW_MULTI flag that can be used to attach multiple
    bpf programs to a cgroup.
    
    The difference between three possible flags for BPF_PROG_ATTACH command:
    - NONE(default): No further bpf programs allowed in the subtree.
    - BPF_F_ALLOW_OVERRIDE: If a sub-cgroup installs some bpf program,
      the program in this cgroup yields to sub-cgroup program.
    - BPF_F_ALLOW_MULTI: If a sub-cgroup installs some bpf program,
      that cgroup program gets run in addition to the program in this cgroup.
    
    NONE and BPF_F_ALLOW_OVERRIDE existed before. This patch doesn't
    change their behavior. It only clarifies the semantics in relation
    to new flag.
    
    Only one program is allowed to be attached to a cgroup with
    NONE or BPF_F_ALLOW_OVERRIDE flag.
    Multiple programs are allowed to be attached to a cgroup with
    BPF_F_ALLOW_MULTI flag. They are executed in FIFO order
    (those that were attached first, run first)
    The programs of sub-cgroup are executed first, then programs of
    this cgroup and then programs of parent cgroup.
    All eligible programs are executed regardless of return code from
    earlier programs.
    
    To allow efficient execution of multiple programs attached to a cgroup
    and to avoid penalizing cgroups without any programs attached
    introduce 'struct bpf_prog_array' which is RCU protected array
    of pointers to bpf programs.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    for cgroup bits
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b927da66f653..51bee695d32c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1168,6 +1168,9 @@ static int sockmap_get_from_fd(const union bpf_attr *attr, bool attach)
 	return 0;
 }
 
+#define BPF_F_ATTACH_MASK \
+	(BPF_F_ALLOW_OVERRIDE | BPF_F_ALLOW_MULTI)
+
 static int bpf_prog_attach(const union bpf_attr *attr)
 {
 	enum bpf_prog_type ptype;
@@ -1181,7 +1184,7 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	if (CHECK_ATTR(BPF_PROG_ATTACH))
 		return -EINVAL;
 
-	if (attr->attach_flags & ~BPF_F_ALLOW_OVERRIDE)
+	if (attr->attach_flags & ~BPF_F_ATTACH_MASK)
 		return -EINVAL;
 
 	switch (attr->attach_type) {
@@ -1212,8 +1215,8 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 		return PTR_ERR(cgrp);
 	}
 
-	ret = cgroup_bpf_update(cgrp, prog, attr->attach_type,
-				attr->attach_flags & BPF_F_ALLOW_OVERRIDE);
+	ret = cgroup_bpf_attach(cgrp, prog, attr->attach_type,
+				attr->attach_flags);
 	if (ret)
 		bpf_prog_put(prog);
 	cgroup_put(cgrp);
@@ -1225,6 +1228,8 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 
 static int bpf_prog_detach(const union bpf_attr *attr)
 {
+	enum bpf_prog_type ptype;
+	struct bpf_prog *prog;
 	struct cgroup *cgrp;
 	int ret;
 
@@ -1237,23 +1242,33 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	switch (attr->attach_type) {
 	case BPF_CGROUP_INET_INGRESS:
 	case BPF_CGROUP_INET_EGRESS:
+		ptype = BPF_PROG_TYPE_CGROUP_SKB;
+		break;
 	case BPF_CGROUP_INET_SOCK_CREATE:
+		ptype = BPF_PROG_TYPE_CGROUP_SOCK;
+		break;
 	case BPF_CGROUP_SOCK_OPS:
-		cgrp = cgroup_get_from_fd(attr->target_fd);
-		if (IS_ERR(cgrp))
-			return PTR_ERR(cgrp);
-
-		ret = cgroup_bpf_update(cgrp, NULL, attr->attach_type, false);
-		cgroup_put(cgrp);
+		ptype = BPF_PROG_TYPE_SOCK_OPS;
 		break;
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
-		ret = sockmap_get_from_fd(attr, false);
-		break;
+		return sockmap_get_from_fd(attr, false);
 	default:
 		return -EINVAL;
 	}
 
+	cgrp = cgroup_get_from_fd(attr->target_fd);
+	if (IS_ERR(cgrp))
+		return PTR_ERR(cgrp);
+
+	prog = bpf_prog_get_type(attr->attach_bpf_fd, ptype);
+	if (IS_ERR(prog))
+		prog = NULL;
+
+	ret = cgroup_bpf_detach(cgrp, prog, attr->attach_type, 0);
+	if (prog)
+		bpf_prog_put(prog);
+	cgroup_put(cgrp);
 	return ret;
 }
 

commit 721e08dad17e226ef68819d0a23dc53c25fe8ea5
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Sep 29 10:52:17 2017 -0700

    bpf: Fix compiler warning on info.map_ids for 32bit platform
    
    This patch uses u64_to_user_ptr() to cast info.map_ids to a userspace ptr.
    It also tags the user_map_ids with '__user' for sparse check.
    
    Fixes: cb4d2b3f03d8 ("bpf: Add name, load_time, uid and map_ids to bpf_prog_info")
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 11a7f82a55d1..b927da66f653 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1405,7 +1405,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	info.nr_map_ids = prog->aux->used_map_cnt;
 	ulen = min_t(u32, info.nr_map_ids, ulen);
 	if (ulen) {
-		u32 *user_map_ids = (u32 *)info.map_ids;
+		u32 __user *user_map_ids = u64_to_user_ptr(info.map_ids);
 		u32 i;
 
 		for (i = 0; i < ulen; i++)

commit ad5b177bd73f5107d97c36f56395c4281fb6f089
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Sep 27 14:37:53 2017 -0700

    bpf: Add map_name to bpf_map_info
    
    This patch allows userspace to specify a name for a map
    during BPF_MAP_CREATE.
    
    The map's name can later be exported to user space
    via BPF_OBJ_GET_INFO_BY_FD.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 45970df3f820..11a7f82a55d1 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -339,7 +339,7 @@ static int bpf_obj_name_cpy(char *dst, const char *src)
 	return 0;
 }
 
-#define BPF_MAP_CREATE_LAST_FIELD numa_node
+#define BPF_MAP_CREATE_LAST_FIELD map_name
 /* called via syscall */
 static int map_create(union bpf_attr *attr)
 {
@@ -361,6 +361,10 @@ static int map_create(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
+	err = bpf_obj_name_cpy(map->name, attr->map_name);
+	if (err)
+		goto free_map_nouncharge;
+
 	atomic_set(&map->refcnt, 1);
 	atomic_set(&map->usercnt, 1);
 
@@ -1462,6 +1466,7 @@ static int bpf_map_get_info_by_fd(struct bpf_map *map,
 	info.value_size = map->value_size;
 	info.max_entries = map->max_entries;
 	info.map_flags = map->map_flags;
+	memcpy(info.name, map->name, sizeof(map->name));
 
 	if (copy_to_user(uinfo, &info, info_len) ||
 	    put_user(info_len, &uattr->info.info_len))

commit cb4d2b3f03d8eed90be3a194e5b54b734ec4bbe9
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Sep 27 14:37:52 2017 -0700

    bpf: Add name, load_time, uid and map_ids to bpf_prog_info
    
    The patch adds name and load_time to struct bpf_prog_aux.  They
    are also exported to bpf_prog_info.
    
    The bpf_prog's name is passed by userspace during BPF_PROG_LOAD.
    The kernel only stores the first (BPF_PROG_NAME_LEN - 1) bytes
    and the name stored in the kernel is always \0 terminated.
    
    The kernel will reject name that contains characters other than
    isalnum() and '_'.  It will also reject name that is not null
    terminated.
    
    The existing 'user->uid' of the bpf_prog_aux is also exported to
    the bpf_prog_info as created_by_uid.
    
    The existing 'used_maps' of the bpf_prog_aux is exported to
    the newly added members 'nr_map_ids' and 'map_ids' of
    the bpf_prog_info.  On the input, nr_map_ids tells how
    big the userspace's map_ids buffer is.  On the output,
    nr_map_ids tells the exact user_map_cnt and it will only
    copy up to the userspace's map_ids buffer is allowed.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 25d074920a00..45970df3f820 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -23,6 +23,9 @@
 #include <linux/version.h>
 #include <linux/kernel.h>
 #include <linux/idr.h>
+#include <linux/cred.h>
+#include <linux/timekeeping.h>
+#include <linux/ctype.h>
 
 #define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY || \
 			   (map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
@@ -312,6 +315,30 @@ int bpf_map_new_fd(struct bpf_map *map)
 		   offsetof(union bpf_attr, CMD##_LAST_FIELD) - \
 		   sizeof(attr->CMD##_LAST_FIELD)) != NULL
 
+/* dst and src must have at least BPF_OBJ_NAME_LEN number of bytes.
+ * Return 0 on success and < 0 on error.
+ */
+static int bpf_obj_name_cpy(char *dst, const char *src)
+{
+	const char *end = src + BPF_OBJ_NAME_LEN;
+
+	/* Copy all isalnum() and '_' char */
+	while (src < end && *src) {
+		if (!isalnum(*src) && *src != '_')
+			return -EINVAL;
+		*dst++ = *src++;
+	}
+
+	/* No '\0' found in BPF_OBJ_NAME_LEN number of bytes */
+	if (src == end)
+		return -EINVAL;
+
+	/* '\0' terminates dst */
+	*dst = 0;
+
+	return 0;
+}
+
 #define BPF_MAP_CREATE_LAST_FIELD numa_node
 /* called via syscall */
 static int map_create(union bpf_attr *attr)
@@ -973,7 +1000,7 @@ struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
 EXPORT_SYMBOL_GPL(bpf_prog_get_type);
 
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD prog_flags
+#define	BPF_PROG_LOAD_LAST_FIELD prog_name
 
 static int bpf_prog_load(union bpf_attr *attr)
 {
@@ -1037,6 +1064,11 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (err < 0)
 		goto free_prog;
 
+	prog->aux->load_time = ktime_get_boot_ns();
+	err = bpf_obj_name_cpy(prog->aux->name, attr->prog_name);
+	if (err)
+		goto free_prog;
+
 	/* run eBPF verifier */
 	err = bpf_check(&prog, attr);
 	if (err < 0)
@@ -1358,8 +1390,25 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 
 	info.type = prog->type;
 	info.id = prog->aux->id;
+	info.load_time = prog->aux->load_time;
+	info.created_by_uid = from_kuid_munged(current_user_ns(),
+					       prog->aux->user->uid);
 
 	memcpy(info.tag, prog->tag, sizeof(prog->tag));
+	memcpy(info.name, prog->aux->name, sizeof(prog->aux->name));
+
+	ulen = info.nr_map_ids;
+	info.nr_map_ids = prog->aux->used_map_cnt;
+	ulen = min_t(u32, info.nr_map_ids, ulen);
+	if (ulen) {
+		u32 *user_map_ids = (u32 *)info.map_ids;
+		u32 i;
+
+		for (i = 0; i < ulen; i++)
+			if (put_user(prog->aux->used_maps[i]->id,
+				     &user_map_ids[i]))
+				return -EFAULT;
+	}
 
 	if (!capable(CAP_SYS_ADMIN)) {
 		info.jited_prog_len = 0;

commit 930651a75bf1ba6893a8b8475270664ebdb6cf4a
Author: Eric Dumazet <edumazet@google.com>
Date:   Tue Sep 19 09:15:59 2017 -0700

    bpf: do not disable/enable BH in bpf_map_free_id()
    
    syzkaller reported following splat [1]
    
    Since hard irq are disabled by the caller, bpf_map_free_id()
    should not try to enable/disable BH.
    
    Another solution would be to change htab_map_delete_elem() to
    defer the free_htab_elem() call after
    raw_spin_unlock_irqrestore(&b->lock, flags), but this might be not
    enough to cover other code paths.
    
    [1]
    WARNING: CPU: 1 PID: 8052 at kernel/softirq.c:161 __local_bh_enable_ip
    +0x1e/0x160 kernel/softirq.c:161
    Kernel panic - not syncing: panic_on_warn set ...
    
    CPU: 1 PID: 8052 Comm: syz-executor1 Not tainted 4.13.0-next-20170915+
    #23
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS
    Google 01/01/2011
    Call Trace:
     __dump_stack lib/dump_stack.c:16 [inline]
     dump_stack+0x194/0x257 lib/dump_stack.c:52
     panic+0x1e4/0x417 kernel/panic.c:181
     __warn+0x1c4/0x1d9 kernel/panic.c:542
     report_bug+0x211/0x2d0 lib/bug.c:183
     fixup_bug+0x40/0x90 arch/x86/kernel/traps.c:178
     do_trap_no_signal arch/x86/kernel/traps.c:212 [inline]
     do_trap+0x260/0x390 arch/x86/kernel/traps.c:261
     do_error_trap+0x120/0x390 arch/x86/kernel/traps.c:298
     do_invalid_op+0x1b/0x20 arch/x86/kernel/traps.c:311
     invalid_op+0x18/0x20 arch/x86/entry/entry_64.S:905
    RIP: 0010:__local_bh_enable_ip+0x1e/0x160 kernel/softirq.c:161
    RSP: 0018:ffff8801cdcd7748 EFLAGS: 00010046
    RAX: 0000000000000082 RBX: 0000000000000201 RCX: 0000000000000000
    RDX: 1ffffffff0b5933c RSI: 0000000000000201 RDI: ffffffff85ac99e0
    RBP: ffff8801cdcd7758 R08: ffffffff85b87158 R09: 1ffff10039b9aec6
    R10: ffff8801c99f24c0 R11: 0000000000000002 R12: ffffffff817b0b47
    R13: dffffc0000000000 R14: ffff8801cdcd77e8 R15: 0000000000000001
     __raw_spin_unlock_bh include/linux/spinlock_api_smp.h:176 [inline]
     _raw_spin_unlock_bh+0x30/0x40 kernel/locking/spinlock.c:207
     spin_unlock_bh include/linux/spinlock.h:361 [inline]
     bpf_map_free_id kernel/bpf/syscall.c:197 [inline]
     __bpf_map_put+0x267/0x320 kernel/bpf/syscall.c:227
     bpf_map_put+0x1a/0x20 kernel/bpf/syscall.c:235
     bpf_map_fd_put_ptr+0x15/0x20 kernel/bpf/map_in_map.c:96
     free_htab_elem+0xc3/0x1b0 kernel/bpf/hashtab.c:658
     htab_map_delete_elem+0x74d/0x970 kernel/bpf/hashtab.c:1063
     map_delete_elem kernel/bpf/syscall.c:633 [inline]
     SYSC_bpf kernel/bpf/syscall.c:1479 [inline]
     SyS_bpf+0x2188/0x46a0 kernel/bpf/syscall.c:1451
     entry_SYSCALL_64_fastpath+0x1f/0xbe
    
    Fixes: f3f1c054c288 ("bpf: Introduce bpf_map ID")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index cb17e1cd1d43..25d074920a00 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -186,15 +186,17 @@ static int bpf_map_alloc_id(struct bpf_map *map)
 
 static void bpf_map_free_id(struct bpf_map *map, bool do_idr_lock)
 {
+	unsigned long flags;
+
 	if (do_idr_lock)
-		spin_lock_bh(&map_idr_lock);
+		spin_lock_irqsave(&map_idr_lock, flags);
 	else
 		__acquire(&map_idr_lock);
 
 	idr_remove(&map_idr, map->id);
 
 	if (do_idr_lock)
-		spin_unlock_bh(&map_idr_lock);
+		spin_unlock_irqrestore(&map_idr_lock, flags);
 	else
 		__release(&map_idr_lock);
 }

commit 5a67da2a71c64daeb456f6f3e87b5c7cecdc5ffa
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Fri Sep 8 14:00:49 2017 -0700

    bpf: add support for sockmap detach programs
    
    The bpf map sockmap supports adding programs via attach commands. This
    patch adds the detach command to keep the API symmetric and allow
    users to remove previously added programs. Otherwise the user would
    have to delete the map and re-add it to get in this state.
    
    This also adds a series of additional tests to capture detach operation
    and also attaching/detaching invalid prog types.
    
    API note: socks will run (or not run) programs depending on the state
    of the map at the time the sock is added. We do not for example walk
    the map and remove programs from previously attached socks.
    
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 70ad8e220343..cb17e1cd1d43 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1096,10 +1096,10 @@ static int bpf_obj_get(const union bpf_attr *attr)
 
 #define BPF_PROG_ATTACH_LAST_FIELD attach_flags
 
-static int sockmap_get_from_fd(const union bpf_attr *attr)
+static int sockmap_get_from_fd(const union bpf_attr *attr, bool attach)
 {
+	struct bpf_prog *prog = NULL;
 	int ufd = attr->target_fd;
-	struct bpf_prog *prog;
 	struct bpf_map *map;
 	struct fd f;
 	int err;
@@ -1109,16 +1109,20 @@ static int sockmap_get_from_fd(const union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
-	prog = bpf_prog_get_type(attr->attach_bpf_fd, BPF_PROG_TYPE_SK_SKB);
-	if (IS_ERR(prog)) {
-		fdput(f);
-		return PTR_ERR(prog);
+	if (attach) {
+		prog = bpf_prog_get_type(attr->attach_bpf_fd,
+					 BPF_PROG_TYPE_SK_SKB);
+		if (IS_ERR(prog)) {
+			fdput(f);
+			return PTR_ERR(prog);
+		}
 	}
 
-	err = sock_map_attach_prog(map, prog, attr->attach_type);
+	err = sock_map_prog(map, prog, attr->attach_type);
 	if (err) {
 		fdput(f);
-		bpf_prog_put(prog);
+		if (prog)
+			bpf_prog_put(prog);
 		return err;
 	}
 
@@ -1155,7 +1159,7 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 		break;
 	case BPF_SK_SKB_STREAM_PARSER:
 	case BPF_SK_SKB_STREAM_VERDICT:
-		return sockmap_get_from_fd(attr);
+		return sockmap_get_from_fd(attr, true);
 	default:
 		return -EINVAL;
 	}
@@ -1204,7 +1208,10 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 		ret = cgroup_bpf_update(cgrp, NULL, attr->attach_type, false);
 		cgroup_put(cgrp);
 		break;
-
+	case BPF_SK_SKB_STREAM_PARSER:
+	case BPF_SK_SKB_STREAM_VERDICT:
+		ret = sockmap_get_from_fd(attr, false);
+		break;
 	default:
 		return -EINVAL;
 	}

commit 96e5ae4e76f1ea950d493f510399b49308bea731
Author: Eric Dumazet <edumazet@google.com>
Date:   Mon Sep 4 22:41:02 2017 -0700

    bpf: fix numa_node validation
    
    syzkaller reported crashes in bpf map creation or map update [1]
    
    Problem is that nr_node_ids is a signed integer,
    NUMA_NO_NODE is also an integer, so it is very tempting
    to declare numa_node as a signed integer.
    
    This means the typical test to validate a user provided value :
    
            if (numa_node != NUMA_NO_NODE &&
                (numa_node >= nr_node_ids ||
                 !node_online(numa_node)))
    
    must be written :
    
            if (numa_node != NUMA_NO_NODE &&
                ((unsigned int)numa_node >= nr_node_ids ||
                 !node_online(numa_node)))
    
    [1]
    kernel BUG at mm/slab.c:3256!
    invalid opcode: 0000 [#1] SMP KASAN
    Dumping ftrace buffer:
       (ftrace buffer empty)
    Modules linked in:
    CPU: 0 PID: 2946 Comm: syzkaller916108 Not tainted 4.13.0-rc7+ #35
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    task: ffff8801d2bc60c0 task.stack: ffff8801c0c90000
    RIP: 0010:____cache_alloc_node+0x1d4/0x1e0 mm/slab.c:3292
    RSP: 0018:ffff8801c0c97638 EFLAGS: 00010096
    RAX: ffffffffffff8b7b RBX: 0000000001080220 RCX: 0000000000000000
    RDX: 00000000ffff8b7b RSI: 0000000001080220 RDI: ffff8801dac00040
    RBP: ffff8801c0c976c0 R08: 0000000000000000 R09: 0000000000000000
    R10: ffff8801c0c97620 R11: 0000000000000001 R12: ffff8801dac00040
    R13: ffff8801dac00040 R14: 0000000000000000 R15: 00000000ffff8b7b
    FS:  0000000002119940(0000) GS:ffff8801db200000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000020001fec CR3: 00000001d2980000 CR4: 00000000001406f0
    Call Trace:
     __do_kmalloc_node mm/slab.c:3688 [inline]
     __kmalloc_node+0x33/0x70 mm/slab.c:3696
     kmalloc_node include/linux/slab.h:535 [inline]
     alloc_htab_elem+0x2a8/0x480 kernel/bpf/hashtab.c:740
     htab_map_update_elem+0x740/0xb80 kernel/bpf/hashtab.c:820
     map_update_elem kernel/bpf/syscall.c:587 [inline]
     SYSC_bpf kernel/bpf/syscall.c:1468 [inline]
     SyS_bpf+0x20c5/0x4c40 kernel/bpf/syscall.c:1443
     entry_SYSCALL_64_fastpath+0x1f/0xbe
    RIP: 0033:0x440409
    RSP: 002b:00007ffd1f1792b8 EFLAGS: 00000246 ORIG_RAX: 0000000000000141
    RAX: ffffffffffffffda RBX: 00000000004002c8 RCX: 0000000000440409
    RDX: 0000000000000020 RSI: 0000000020006000 RDI: 0000000000000002
    RBP: 0000000000000086 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000246 R12: 0000000000401d70
    R13: 0000000000401e00 R14: 0000000000000000 R15: 0000000000000000
    Code: 83 c2 01 89 50 18 4c 03 70 08 e8 38 f4 ff ff 4d 85 f6 0f 85 3e ff ff ff 44 89 fe 4c 89 ef e8 94 fb ff ff 49 89 c6 e9 2b ff ff ff <0f> 0b 0f 0b 0f 0b 66 0f 1f 44 00 00 55 48 89 e5 41 57 41 56 41
    RIP: ____cache_alloc_node+0x1d4/0x1e0 mm/slab.c:3292 RSP: ffff8801c0c97638
    ---[ end trace d745f355da2e33ce ]---
    Kernel panic - not syncing: Fatal exception
    
    Fixes: 96eabe7a40aa ("bpf: Allow selecting numa node during map creation")
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Martin KaFai Lau <kafai@fb.com>
    Cc: Alexei Starovoitov <ast@fb.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 021a05d9d800..70ad8e220343 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -323,7 +323,8 @@ static int map_create(union bpf_attr *attr)
 		return -EINVAL;
 
 	if (numa_node != NUMA_NO_NODE &&
-	    (numa_node >= nr_node_ids || !node_online(numa_node)))
+	    ((unsigned int)numa_node >= nr_node_ids ||
+	     !node_online(numa_node)))
 		return -EINVAL;
 
 	/* find map type and init map: hashtable vs rbtree vs bloom vs ... */

commit 464bc0fd6273d518aee79fbd37211dd9bc35d863
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Mon Aug 28 07:10:04 2017 -0700

    bpf: convert sockmap field attach_bpf_fd2 to type
    
    In the initial sockmap API we provided strparser and verdict programs
    using a single attach command by extending the attach API with a the
    attach_bpf_fd2 field.
    
    However, if we add other programs in the future we will be adding a
    field for every new possible type, attach_bpf_fd(3,4,..). This
    seems a bit clumsy for an API. So lets push the programs using two
    new type fields.
    
       BPF_SK_SKB_STREAM_PARSER
       BPF_SK_SKB_STREAM_VERDICT
    
    This has the advantage of having a readable name and can easily be
    extended in the future.
    
    Updates to samples and sockmap included here also generalize tests
    slightly to support upcoming patch for multiple map support.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Fixes: 174a79ff9515 ("bpf: sockmap with sk redirect support")
    Suggested-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 9378f3ba2cbf..021a05d9d800 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1093,12 +1093,12 @@ static int bpf_obj_get(const union bpf_attr *attr)
 
 #ifdef CONFIG_CGROUP_BPF
 
-#define BPF_PROG_ATTACH_LAST_FIELD attach_bpf_fd2
+#define BPF_PROG_ATTACH_LAST_FIELD attach_flags
 
-static int sockmap_get_from_fd(const union bpf_attr *attr, int ptype)
+static int sockmap_get_from_fd(const union bpf_attr *attr)
 {
-	struct bpf_prog *prog1, *prog2;
 	int ufd = attr->target_fd;
+	struct bpf_prog *prog;
 	struct bpf_map *map;
 	struct fd f;
 	int err;
@@ -1108,29 +1108,16 @@ static int sockmap_get_from_fd(const union bpf_attr *attr, int ptype)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
-	if (!map->ops->map_attach) {
-		fdput(f);
-		return -EOPNOTSUPP;
-	}
-
-	prog1 = bpf_prog_get_type(attr->attach_bpf_fd, ptype);
-	if (IS_ERR(prog1)) {
+	prog = bpf_prog_get_type(attr->attach_bpf_fd, BPF_PROG_TYPE_SK_SKB);
+	if (IS_ERR(prog)) {
 		fdput(f);
-		return PTR_ERR(prog1);
-	}
-
-	prog2 = bpf_prog_get_type(attr->attach_bpf_fd2, ptype);
-	if (IS_ERR(prog2)) {
-		fdput(f);
-		bpf_prog_put(prog1);
-		return PTR_ERR(prog2);
+		return PTR_ERR(prog);
 	}
 
-	err = map->ops->map_attach(map, prog1, prog2);
+	err = sock_map_attach_prog(map, prog, attr->attach_type);
 	if (err) {
 		fdput(f);
-		bpf_prog_put(prog1);
-		bpf_prog_put(prog2);
+		bpf_prog_put(prog);
 		return err;
 	}
 
@@ -1165,16 +1152,13 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_SOCK_OPS:
 		ptype = BPF_PROG_TYPE_SOCK_OPS;
 		break;
-	case BPF_CGROUP_SMAP_INGRESS:
-		ptype = BPF_PROG_TYPE_SK_SKB;
-		break;
+	case BPF_SK_SKB_STREAM_PARSER:
+	case BPF_SK_SKB_STREAM_VERDICT:
+		return sockmap_get_from_fd(attr);
 	default:
 		return -EINVAL;
 	}
 
-	if (attr->attach_type == BPF_CGROUP_SMAP_INGRESS)
-		return sockmap_get_from_fd(attr, ptype);
-
 	prog = bpf_prog_get_type(attr->attach_bpf_fd, ptype);
 	if (IS_ERR(prog))
 		return PTR_ERR(prog);

commit 96eabe7a40aa17e613cf3db2c742ee8b1fc764d0
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Aug 18 11:28:00 2017 -0700

    bpf: Allow selecting numa node during map creation
    
    The current map creation API does not allow to provide the numa-node
    preference.  The memory usually comes from where the map-creation-process
    is running.  The performance is not ideal if the bpf_prog is known to
    always run in a numa node different from the map-creation-process.
    
    One of the use case is sharding on CPU to different LRU maps (i.e.
    an array of LRU maps).  Here is the test result of map_perf_test on
    the INNER_LRU_HASH_PREALLOC test if we force the lru map used by
    CPU0 to be allocated from a remote numa node:
    
    [ The machine has 20 cores. CPU0-9 at node 0. CPU10-19 at node 1 ]
    
    ># taskset -c 10 ./map_perf_test 512 8 1260000 8000000
    5:inner_lru_hash_map_perf pre-alloc 1628380 events per sec
    4:inner_lru_hash_map_perf pre-alloc 1626396 events per sec
    3:inner_lru_hash_map_perf pre-alloc 1626144 events per sec
    6:inner_lru_hash_map_perf pre-alloc 1621657 events per sec
    2:inner_lru_hash_map_perf pre-alloc 1621534 events per sec
    1:inner_lru_hash_map_perf pre-alloc 1620292 events per sec
    7:inner_lru_hash_map_perf pre-alloc 1613305 events per sec
    0:inner_lru_hash_map_perf pre-alloc 1239150 events per sec  #<<<
    
    After specifying numa node:
    ># taskset -c 10 ./map_perf_test 512 8 1260000 8000000
    5:inner_lru_hash_map_perf pre-alloc 1629627 events per sec
    3:inner_lru_hash_map_perf pre-alloc 1628057 events per sec
    1:inner_lru_hash_map_perf pre-alloc 1623054 events per sec
    6:inner_lru_hash_map_perf pre-alloc 1616033 events per sec
    2:inner_lru_hash_map_perf pre-alloc 1614630 events per sec
    4:inner_lru_hash_map_perf pre-alloc 1612651 events per sec
    7:inner_lru_hash_map_perf pre-alloc 1609337 events per sec
    0:inner_lru_hash_map_perf pre-alloc 1619340 events per sec #<<<
    
    This patch adds one field, numa_node, to the bpf_attr.  Since numa node 0
    is a valid node, a new flag BPF_F_NUMA_NODE is also added.  The numa_node
    field is honored if and only if the BPF_F_NUMA_NODE flag is set.
    
    Numa node selection is not supported for percpu map.
    
    This patch does not change all the kmalloc.  F.e.
    'htab = kzalloc()' is not changed since the object
    is small enough to stay in the cache.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b8cb1b3c9bfb..9378f3ba2cbf 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -105,7 +105,7 @@ static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 	return map;
 }
 
-void *bpf_map_area_alloc(size_t size)
+void *bpf_map_area_alloc(size_t size, int numa_node)
 {
 	/* We definitely need __GFP_NORETRY, so OOM killer doesn't
 	 * trigger under memory pressure as we really just want to
@@ -115,12 +115,13 @@ void *bpf_map_area_alloc(size_t size)
 	void *area;
 
 	if (size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER)) {
-		area = kmalloc(size, GFP_USER | flags);
+		area = kmalloc_node(size, GFP_USER | flags, numa_node);
 		if (area != NULL)
 			return area;
 	}
 
-	return __vmalloc(size, GFP_KERNEL | flags, PAGE_KERNEL);
+	return __vmalloc_node_flags_caller(size, numa_node, GFP_KERNEL | flags,
+					   __builtin_return_address(0));
 }
 
 void bpf_map_area_free(void *area)
@@ -309,10 +310,11 @@ int bpf_map_new_fd(struct bpf_map *map)
 		   offsetof(union bpf_attr, CMD##_LAST_FIELD) - \
 		   sizeof(attr->CMD##_LAST_FIELD)) != NULL
 
-#define BPF_MAP_CREATE_LAST_FIELD inner_map_fd
+#define BPF_MAP_CREATE_LAST_FIELD numa_node
 /* called via syscall */
 static int map_create(union bpf_attr *attr)
 {
+	int numa_node = bpf_map_attr_numa_node(attr);
 	struct bpf_map *map;
 	int err;
 
@@ -320,6 +322,10 @@ static int map_create(union bpf_attr *attr)
 	if (err)
 		return -EINVAL;
 
+	if (numa_node != NUMA_NO_NODE &&
+	    (numa_node >= nr_node_ids || !node_online(numa_node)))
+		return -EINVAL;
+
 	/* find map type and init map: hashtable vs rbtree vs bloom vs ... */
 	map = find_and_alloc_map(attr);
 	if (IS_ERR(map))

commit ae2b27b859a144f503d382580320873c0beb09c7
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Fri Aug 18 10:27:02 2017 +0300

    bpf: fix a return in sockmap_get_from_fd()
    
    "map" is a valid pointer.  We wanted to return "err" instead.  Also
    let's return a zero literal at the end.
    
    Fixes: 174a79ff9515 ("bpf: sockmap with sk redirect support")
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index d2f2bdf71ffa..b8cb1b3c9bfb 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1125,11 +1125,11 @@ static int sockmap_get_from_fd(const union bpf_attr *attr, int ptype)
 		fdput(f);
 		bpf_prog_put(prog1);
 		bpf_prog_put(prog2);
-		return PTR_ERR(map);
+		return err;
 	}
 
 	fdput(f);
-	return err;
+	return 0;
 }
 
 static int bpf_prog_attach(const union bpf_attr *attr)

commit 174a79ff9515f400b9a6115643dafd62a635b7e6
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Tue Aug 15 22:32:47 2017 -0700

    bpf: sockmap with sk redirect support
    
    Recently we added a new map type called dev map used to forward XDP
    packets between ports (6093ec2dc313). This patches introduces a
    similar notion for sockets.
    
    A sockmap allows users to add participating sockets to a map. When
    sockets are added to the map enough context is stored with the
    map entry to use the entry with a new helper
    
      bpf_sk_redirect_map(map, key, flags)
    
    This helper (analogous to bpf_redirect_map in XDP) is given the map
    and an entry in the map. When called from a sockmap program, discussed
    below, the skb will be sent on the socket using skb_send_sock().
    
    With the above we need a bpf program to call the helper from that will
    then implement the send logic. The initial site implemented in this
    series is the recv_sock hook. For this to work we implemented a map
    attach command to add attributes to a map. In sockmap we add two
    programs a parse program and a verdict program. The parse program
    uses strparser to build messages and pass them to the verdict program.
    The parse programs use the normal strparser semantics. The verdict
    program is of type SK_SKB.
    
    The verdict program returns a verdict SK_DROP, or  SK_REDIRECT for
    now. Additional actions may be added later. When SK_REDIRECT is
    returned, expected when bpf program uses bpf_sk_redirect_map(), the
    sockmap logic will consult per cpu variables set by the helper routine
    and pull the sock entry out of the sock map. This pattern follows the
    existing redirect logic in cls and xdp programs.
    
    This gives the flow,
    
     recv_sock -> str_parser (parse_prog) -> verdict_prog -> skb_send_sock
                                                         \
                                                          -> kfree_skb
    
    As an example use case a message based load balancer may use specific
    logic in the verdict program to select the sock to send on.
    
    Sample programs are provided in future patches that hopefully illustrate
    the user interfaces. Also selftests are in follow-on patches.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 17e29f596de1..d2f2bdf71ffa 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1087,7 +1087,50 @@ static int bpf_obj_get(const union bpf_attr *attr)
 
 #ifdef CONFIG_CGROUP_BPF
 
-#define BPF_PROG_ATTACH_LAST_FIELD attach_flags
+#define BPF_PROG_ATTACH_LAST_FIELD attach_bpf_fd2
+
+static int sockmap_get_from_fd(const union bpf_attr *attr, int ptype)
+{
+	struct bpf_prog *prog1, *prog2;
+	int ufd = attr->target_fd;
+	struct bpf_map *map;
+	struct fd f;
+	int err;
+
+	f = fdget(ufd);
+	map = __bpf_map_get(f);
+	if (IS_ERR(map))
+		return PTR_ERR(map);
+
+	if (!map->ops->map_attach) {
+		fdput(f);
+		return -EOPNOTSUPP;
+	}
+
+	prog1 = bpf_prog_get_type(attr->attach_bpf_fd, ptype);
+	if (IS_ERR(prog1)) {
+		fdput(f);
+		return PTR_ERR(prog1);
+	}
+
+	prog2 = bpf_prog_get_type(attr->attach_bpf_fd2, ptype);
+	if (IS_ERR(prog2)) {
+		fdput(f);
+		bpf_prog_put(prog1);
+		return PTR_ERR(prog2);
+	}
+
+	err = map->ops->map_attach(map, prog1, prog2);
+	if (err) {
+		fdput(f);
+		bpf_prog_put(prog1);
+		bpf_prog_put(prog2);
+		return PTR_ERR(map);
+	}
+
+	fdput(f);
+	return err;
+}
 
 static int bpf_prog_attach(const union bpf_attr *attr)
 {
@@ -1116,10 +1159,16 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_SOCK_OPS:
 		ptype = BPF_PROG_TYPE_SOCK_OPS;
 		break;
+	case BPF_CGROUP_SMAP_INGRESS:
+		ptype = BPF_PROG_TYPE_SK_SKB;
+		break;
 	default:
 		return -EINVAL;
 	}
 
+	if (attr->attach_type == BPF_CGROUP_SMAP_INGRESS)
+		return sockmap_get_from_fd(attr, ptype);
+
 	prog = bpf_prog_get_type(attr->attach_bpf_fd, ptype);
 	if (IS_ERR(prog))
 		return PTR_ERR(prog);

commit a6f6df69c48b86cd84f36c70593eb4968fceb34a
Author: John Fastabend <john.fastabend@gmail.com>
Date:   Tue Aug 15 22:32:22 2017 -0700

    bpf: export bpf_prog_inc_not_zero
    
    bpf_prog_inc_not_zero will be used by upcoming sockmap patches this
    patch simply exports it so we can pull it in.
    
    Signed-off-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index fbe09a0cccf4..17e29f596de1 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -911,7 +911,7 @@ struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
 EXPORT_SYMBOL_GPL(bpf_prog_inc);
 
 /* prog_idr_lock should have been held */
-static struct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)
+struct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)
 {
 	int refold;
 
@@ -927,6 +927,7 @@ static struct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)
 
 	return prog;
 }
+EXPORT_SYMBOL_GPL(bpf_prog_inc_not_zero);
 
 static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *type)
 {

commit 752ba56fb130b27c32c2ae6c82c8ef246b22106c
Author: Mickaël Salaün <mic@digikod.net>
Date:   Mon Aug 7 20:45:20 2017 +0200

    bpf: Extend check_uarg_tail_zero() checks
    
    The function check_uarg_tail_zero() was created from bpf(2) for
    BPF_OBJ_GET_INFO_BY_FD without taking the access_ok() nor the PAGE_SIZE
    checks. Make this checks more generally available while unlikely to be
    triggered, extend the memory range check and add an explanation
    including why the ToCToU should not be a security concern.
    
    Signed-off-by: Mickaël Salaün <mic@digikod.net>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Martin KaFai Lau <kafai@fb.com>
    Link: https://lkml.kernel.org/r/CAGXu5j+vRGFvJZmjtAcT8Hi8B+Wz0e1b6VKYZHfQP_=DXzC4CQ@mail.gmail.com
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c653ee0bd162..fbe09a0cccf4 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -48,6 +48,15 @@ static const struct bpf_map_ops * const bpf_map_types[] = {
 #undef BPF_MAP_TYPE
 };
 
+/*
+ * If we're handed a bigger struct than we know of, ensure all the unknown bits
+ * are 0 - i.e. new user-space does not rely on any kernel feature extensions
+ * we don't know about yet.
+ *
+ * There is a ToCToU between this function call and the following
+ * copy_from_user() call. However, this is not a concern since this function is
+ * meant to be a future-proofing of bits.
+ */
 static int check_uarg_tail_zero(void __user *uaddr,
 				size_t expected_size,
 				size_t actual_size)
@@ -57,6 +66,12 @@ static int check_uarg_tail_zero(void __user *uaddr,
 	unsigned char val;
 	int err;
 
+	if (unlikely(actual_size > PAGE_SIZE))	/* silly large */
+		return -E2BIG;
+
+	if (unlikely(!access_ok(VERIFY_READ, uaddr, actual_size)))
+		return -EFAULT;
+
 	if (actual_size <= expected_size)
 		return 0;
 
@@ -1393,17 +1408,6 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	if (!capable(CAP_SYS_ADMIN) && sysctl_unprivileged_bpf_disabled)
 		return -EPERM;
 
-	if (!access_ok(VERIFY_READ, uattr, 1))
-		return -EFAULT;
-
-	if (size > PAGE_SIZE)	/* silly large */
-		return -E2BIG;
-
-	/* If we're handed a bigger struct than we know of,
-	 * ensure all the unknown bits are 0 - i.e. new
-	 * user-space does not rely on any kernel feature
-	 * extensions we dont know about yet.
-	 */
 	err = check_uarg_tail_zero(uattr, sizeof(attr), size);
 	if (err)
 		return err;

commit 58291a7465f6b88248c9f34807c16705bd5698f8
Author: Mickaël Salaün <mic@digikod.net>
Date:   Mon Aug 7 20:45:19 2017 +0200

    bpf: Move check_uarg_tail_zero() upward
    
    The function check_uarg_tail_zero() may be useful for other part of the
    code in the syscall.c file. Move this function at the beginning of the
    file.
    
    Signed-off-by: Mickaël Salaün <mic@digikod.net>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 6c772adabad2..c653ee0bd162 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -48,6 +48,32 @@ static const struct bpf_map_ops * const bpf_map_types[] = {
 #undef BPF_MAP_TYPE
 };
 
+static int check_uarg_tail_zero(void __user *uaddr,
+				size_t expected_size,
+				size_t actual_size)
+{
+	unsigned char __user *addr;
+	unsigned char __user *end;
+	unsigned char val;
+	int err;
+
+	if (actual_size <= expected_size)
+		return 0;
+
+	addr = uaddr + expected_size;
+	end  = uaddr + actual_size;
+
+	for (; addr < end; addr++) {
+		err = get_user(val, addr);
+		if (err)
+			return err;
+		if (val)
+			return -E2BIG;
+	}
+
+	return 0;
+}
+
 static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 {
 	struct bpf_map *map;
@@ -1246,32 +1272,6 @@ static int bpf_map_get_fd_by_id(const union bpf_attr *attr)
 	return fd;
 }
 
-static int check_uarg_tail_zero(void __user *uaddr,
-				size_t expected_size,
-				size_t actual_size)
-{
-	unsigned char __user *addr;
-	unsigned char __user *end;
-	unsigned char val;
-	int err;
-
-	if (actual_size <= expected_size)
-		return 0;
-
-	addr = uaddr + expected_size;
-	end  = uaddr + actual_size;
-
-	for (; addr < end; addr++) {
-		err = get_user(val, addr);
-		if (err)
-			return err;
-		if (val)
-			return -E2BIG;
-	}
-
-	return 0;
-}
-
 static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 				   const union bpf_attr *attr,
 				   union bpf_attr __user *uattr)

commit 9975a54b3c9ecf029cbf5dd7a8c9701b1d74029e
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jul 28 17:05:25 2017 +0200

    bpf: fix bpf_prog_get_info_by_fd to dump correct xlated_prog_len
    
    bpf_prog_size(prog->len) is not the correct length we want to dump
    back to user space. The code in bpf_prog_get_info_by_fd() uses this
    to copy prog->insnsi to user space, but bpf_prog_size(prog->len) also
    includes the size of struct bpf_prog itself plus program instructions
    and is usually used either in context of accounting or for bpf_prog_alloc()
    et al, thus we copy out of bounds in bpf_prog_get_info_by_fd()
    potentially. Use the correct bpf_prog_insn_size() instead.
    
    Fixes: 1e2709769086 ("bpf: Add BPF_OBJ_GET_INFO_BY_FD")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 84bb39975ad4..6c772adabad2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1312,7 +1312,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	}
 
 	ulen = info.xlated_prog_len;
-	info.xlated_prog_len = bpf_prog_size(prog->len);
+	info.xlated_prog_len = bpf_prog_insn_size(prog);
 	if (info.xlated_prog_len && ulen) {
 		uinsns = u64_to_user_ptr(info.xlated_prog_insns);
 		ulen = min_t(u32, info.xlated_prog_len, ulen);

commit 89b096898a8450b0a5b97d521e000ae9f94f81f9
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jul 27 21:02:46 2017 +0200

    bpf: don't indicate success when copy_from_user fails
    
    err in bpf_prog_get_info_by_fd() still holds 0 at that time from prior
    check_uarg_tail_zero() check. Explicitly return -EFAULT instead, so
    user space can be notified of buggy behavior.
    
    Fixes: 1e2709769086 ("bpf: Add BPF_OBJ_GET_INFO_BY_FD")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 045646da97cc..84bb39975ad4 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1289,7 +1289,7 @@ static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
 	info_len = min_t(u32, sizeof(info), info_len);
 
 	if (copy_from_user(&info, uinfo, info_len))
-		return err;
+		return -EFAULT;
 
 	info.type = prog->type;
 	info.id = prog->aux->id;

commit 7114f51fcb979f167ab5f625ac74059dcb1afc28
Merge: ea3b25e1320d e4448ed87ccd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 5 16:05:24 2017 -0700

    Merge branch 'work.memdup_user' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull memdup_user() conversions from Al Viro:
     "A fairly self-contained series - hunting down open-coded memdup_user()
      and memdup_user_nul() instances"
    
    * 'work.memdup_user' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      bpf: don't open-code memdup_user()
      kimage_file_prepare_segments(): don't open-code memdup_user()
      ethtool: don't open-code memdup_user()
      do_ip_setsockopt(): don't open-code memdup_user()
      do_ipv6_setsockopt(): don't open-code memdup_user()
      irda: don't open-code memdup_user()
      xfrm_user_policy(): don't open-code memdup_user()
      ima_write_policy(): don't open-code memdup_user_nul()
      sel_write_validatetrans(): don't open-code memdup_user_nul()

commit 9780c0ab1a4e64ef6998c4d83f9df5be806a02dc
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Jul 2 02:13:28 2017 +0200

    bpf: export whether tail call has jited owner
    
    We do export through fdinfo already whether a prog is JITed or not,
    given a program load can fail in case of either prog or tail call map
    has JITed property, but neither both are JITed or not JITed, we can
    facilitate error reporting in loaders like iproute2 through exporting
    owner_jited of tail call map. We already do export owner_prog_type
    through this facility, so parser can pick up both for comparison.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index d4d47de75bba..18980472f5b0 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -216,10 +216,12 @@ static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 	const struct bpf_map *map = filp->private_data;
 	const struct bpf_array *array;
 	u32 owner_prog_type = 0;
+	u32 owner_jited = 0;
 
 	if (map->map_type == BPF_MAP_TYPE_PROG_ARRAY) {
 		array = container_of(map, struct bpf_array, map);
 		owner_prog_type = array->owner_prog_type;
+		owner_jited = array->owner_jited;
 	}
 
 	seq_printf(m,
@@ -236,9 +238,12 @@ static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 		   map->map_flags,
 		   map->pages * 1ULL << PAGE_SHIFT);
 
-	if (owner_prog_type)
+	if (owner_prog_type) {
 		seq_printf(m, "owner_prog_type:\t%u\n",
 			   owner_prog_type);
+		seq_printf(m, "owner_jited:\t%u\n",
+			   owner_jited);
+	}
 }
 #endif
 

commit 40304b2a1567fecc321f640ee4239556dd0f3ee0
Author: Lawrence Brakmo <brakmo@fb.com>
Date:   Fri Jun 30 20:02:40 2017 -0700

    bpf: BPF support for sock_ops
    
    Created a new BPF program type, BPF_PROG_TYPE_SOCK_OPS, and a corresponding
    struct that allows BPF programs of this type to access some of the
    socket's fields (such as IP addresses, ports, etc.). It uses the
    existing bpf cgroups infrastructure so the programs can be attached per
    cgroup with full inheritance support. The program will be called at
    appropriate times to set relevant connections parameters such as buffer
    sizes, SYN and SYN-ACK RTOs, etc., based on connection information such
    as IP addresses, port numbers, etc.
    
    Alghough there are already 3 mechanisms to set parameters (sysctls,
    route metrics and setsockopts), this new mechanism provides some
    distinct advantages. Unlike sysctls, it can set parameters per
    connection. In contrast to route metrics, it can also use port numbers
    and information provided by a user level program. In addition, it could
    set parameters probabilistically for evaluation purposes (i.e. do
    something different on 10% of the flows and compare results with the
    other 90% of the flows). Also, in cases where IPv6 addresses contain
    geographic information, the rules to make changes based on the distance
    (or RTT) between the hosts are much easier than route metric rules and
    can be global. Finally, unlike setsockopt, it oes not require
    application changes and it can be updated easily at any time.
    
    Although the bpf cgroup framework already contains a sock related
    program type (BPF_PROG_TYPE_CGROUP_SOCK), I created the new type
    (BPF_PROG_TYPE_SOCK_OPS) beccause the existing type expects to be called
    only once during the connections's lifetime. In contrast, the new
    program type will be called multiple times from different places in the
    network stack code.  For example, before sending SYN and SYN-ACKs to set
    an appropriate timeout, when the connection is established to set
    congestion control, etc. As a result it has "op" field to specify the
    type of operation requested.
    
    The purpose of this new program type is to simplify setting connection
    parameters, such as buffer sizes, TCP's SYN RTO, etc. For example, it is
    easy to use facebook's internal IPv6 addresses to determine if both hosts
    of a connection are in the same datacenter. Therefore, it is easy to
    write a BPF program to choose a small SYN RTO value when both hosts are
    in the same datacenter.
    
    This patch only contains the framework to support the new BPF program
    type, following patches add the functionality to set various connection
    parameters.
    
    This patch defines a new BPF program type: BPF_PROG_TYPE_SOCKET_OPS
    and a new bpf syscall command to load a new program of this type:
    BPF_PROG_LOAD_SOCKET_OPS.
    
    Two new corresponding structs (one for the kernel one for the user/BPF
    program):
    
    /* kernel version */
    struct bpf_sock_ops_kern {
            struct sock *sk;
            __u32  op;
            union {
                    __u32 reply;
                    __u32 replylong[4];
            };
    };
    
    /* user version
     * Some fields are in network byte order reflecting the sock struct
     * Use the bpf_ntohl helper macro in samples/bpf/bpf_endian.h to
     * convert them to host byte order.
     */
    struct bpf_sock_ops {
            __u32 op;
            union {
                    __u32 reply;
                    __u32 replylong[4];
            };
            __u32 family;
            __u32 remote_ip4;     /* In network byte order */
            __u32 local_ip4;      /* In network byte order */
            __u32 remote_ip6[4];  /* In network byte order */
            __u32 local_ip6[4];   /* In network byte order */
            __u32 remote_port;    /* In network byte order */
            __u32 local_port;     /* In host byte horder */
    };
    
    Currently there are two types of ops. The first type expects the BPF
    program to return a value which is then used by the caller (or a
    negative value to indicate the operation is not supported). The second
    type expects state changes to be done by the BPF program, for example
    through a setsockopt BPF helper function, and they ignore the return
    value.
    
    The reply fields of the bpf_sockt_ops struct are there in case a bpf
    program needs to return a value larger than an integer.
    
    Signed-off-by: Lawrence Brakmo <brakmo@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4409ccca8831..d4d47de75bba 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1079,6 +1079,9 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_INET_SOCK_CREATE:
 		ptype = BPF_PROG_TYPE_CGROUP_SOCK;
 		break;
+	case BPF_CGROUP_SOCK_OPS:
+		ptype = BPF_PROG_TYPE_SOCK_OPS;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -1119,6 +1122,7 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_CGROUP_INET_INGRESS:
 	case BPF_CGROUP_INET_EGRESS:
 	case BPF_CGROUP_INET_SOCK_CREATE:
+	case BPF_CGROUP_SOCK_OPS:
 		cgrp = cgroup_get_from_fd(attr->target_fd);
 		if (IS_ERR(cgrp))
 			return PTR_ERR(cgrp);
@@ -1133,6 +1137,7 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 
 	return ret;
 }
+
 #endif /* CONFIG_CGROUP_BPF */
 
 #define BPF_PROG_TEST_RUN_LAST_FIELD test.duration

commit e4448ed87ccdbacb74871736f63220642242b32f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat May 13 18:43:00 2017 -0400

    bpf: don't open-code memdup_user()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index fd2411fd6914..4b8b10bddfde 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -322,14 +322,11 @@ static int map_lookup_elem(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
-	err = -ENOMEM;
-	key = kmalloc(map->key_size, GFP_USER);
-	if (!key)
+	key = memdup_user(ukey, map->key_size);
+	if (IS_ERR(key)) {
+		err = PTR_ERR(key);
 		goto err_put;
-
-	err = -EFAULT;
-	if (copy_from_user(key, ukey, map->key_size) != 0)
-		goto free_key;
+	}
 
 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
@@ -402,14 +399,11 @@ static int map_update_elem(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
-	err = -ENOMEM;
-	key = kmalloc(map->key_size, GFP_USER);
-	if (!key)
+	key = memdup_user(ukey, map->key_size);
+	if (IS_ERR(key)) {
+		err = PTR_ERR(key);
 		goto err_put;
-
-	err = -EFAULT;
-	if (copy_from_user(key, ukey, map->key_size) != 0)
-		goto free_key;
+	}
 
 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
@@ -488,14 +482,11 @@ static int map_delete_elem(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
-	err = -ENOMEM;
-	key = kmalloc(map->key_size, GFP_USER);
-	if (!key)
+	key = memdup_user(ukey, map->key_size);
+	if (IS_ERR(key)) {
+		err = PTR_ERR(key);
 		goto err_put;
-
-	err = -EFAULT;
-	if (copy_from_user(key, ukey, map->key_size) != 0)
-		goto free_key;
+	}
 
 	preempt_disable();
 	__this_cpu_inc(bpf_prog_active);
@@ -507,7 +498,6 @@ static int map_delete_elem(union bpf_attr *attr)
 
 	if (!err)
 		trace_bpf_map_delete_elem(map, ufd, key);
-free_key:
 	kfree(key);
 err_put:
 	fdput(f);
@@ -536,14 +526,11 @@ static int map_get_next_key(union bpf_attr *attr)
 		return PTR_ERR(map);
 
 	if (ukey) {
-		err = -ENOMEM;
-		key = kmalloc(map->key_size, GFP_USER);
-		if (!key)
+		key = memdup_user(ukey, map->key_size);
+		if (IS_ERR(key)) {
+			err = PTR_ERR(key);
 			goto err_put;
-
-		err = -EFAULT;
-		if (copy_from_user(key, ukey, map->key_size) != 0)
-			goto free_key;
+		}
 	} else {
 		key = NULL;
 	}

commit 14dc6f04f49dc12614d7e90928b495b8d73cd471
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Tue Jun 27 23:08:34 2017 -0700

    bpf: Add syscall lookup support for fd array and htab
    
    This patch allows userspace to do BPF_MAP_LOOKUP_ELEM on
    BPF_MAP_TYPE_PROG_ARRAY,
    BPF_MAP_TYPE_ARRAY_OF_MAPS and
    BPF_MAP_TYPE_HASH_OF_MAPS.
    
    The lookup returns a prog-id or map-id to the userspace.
    The userspace can then use the BPF_PROG_GET_FD_BY_ID
    or BPF_MAP_GET_FD_BY_ID to get a fd.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 8942c820d620..4409ccca8831 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -24,6 +24,13 @@
 #include <linux/kernel.h>
 #include <linux/idr.h>
 
+#define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY || \
+			   (map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \
+			   (map)->map_type == BPF_MAP_TYPE_CGROUP_ARRAY || \
+			   (map)->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS)
+#define IS_FD_HASH(map) ((map)->map_type == BPF_MAP_TYPE_HASH_OF_MAPS)
+#define IS_FD_MAP(map) (IS_FD_ARRAY(map) || IS_FD_HASH(map))
+
 DEFINE_PER_CPU(int, bpf_prog_active);
 static DEFINE_IDR(prog_idr);
 static DEFINE_SPINLOCK(prog_idr_lock);
@@ -411,6 +418,8 @@ static int map_lookup_elem(union bpf_attr *attr)
 	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
 	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
 		value_size = round_up(map->value_size, 8) * num_possible_cpus();
+	else if (IS_FD_MAP(map))
+		value_size = sizeof(u32);
 	else
 		value_size = map->value_size;
 
@@ -426,9 +435,10 @@ static int map_lookup_elem(union bpf_attr *attr)
 		err = bpf_percpu_array_copy(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {
 		err = bpf_stackmap_copy(map, key, value);
-	} else if (map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS ||
-		   map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {
-		err = -ENOTSUPP;
+	} else if (IS_FD_ARRAY(map)) {
+		err = bpf_fd_array_map_lookup_elem(map, key, value);
+	} else if (IS_FD_HASH(map)) {
+		err = bpf_fd_htab_map_lookup_elem(map, key, value);
 	} else {
 		rcu_read_lock();
 		ptr = map->ops->map_lookup_elem(map, key);

commit 1e270976908686ec25fb91b8a34145be54137976
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Jun 5 12:15:52 2017 -0700

    bpf: Add BPF_OBJ_GET_INFO_BY_FD
    
    A single BPF_OBJ_GET_INFO_BY_FD cmd is used to obtain the info
    for both bpf_prog and bpf_map.  The kernel can figure out the
    fd is associated with a bpf_prog or bpf_map.
    
    The suggested struct bpf_prog_info and struct bpf_map_info are
    not meant to be a complete list and it is not the goal of this patch.
    New fields can be added in the future patch.
    
    The focus of this patch is to create the interface,
    BPF_OBJ_GET_INFO_BY_FD cmd for exposing the bpf_prog's and
    bpf_map's info.
    
    The obj's info, which will be extended (and get bigger) over time, is
    separated from the bpf_attr to avoid bloating the bpf_attr.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 1802bb9c47d9..8942c820d620 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -1239,6 +1239,145 @@ static int bpf_map_get_fd_by_id(const union bpf_attr *attr)
 	return fd;
 }
 
+static int check_uarg_tail_zero(void __user *uaddr,
+				size_t expected_size,
+				size_t actual_size)
+{
+	unsigned char __user *addr;
+	unsigned char __user *end;
+	unsigned char val;
+	int err;
+
+	if (actual_size <= expected_size)
+		return 0;
+
+	addr = uaddr + expected_size;
+	end  = uaddr + actual_size;
+
+	for (; addr < end; addr++) {
+		err = get_user(val, addr);
+		if (err)
+			return err;
+		if (val)
+			return -E2BIG;
+	}
+
+	return 0;
+}
+
+static int bpf_prog_get_info_by_fd(struct bpf_prog *prog,
+				   const union bpf_attr *attr,
+				   union bpf_attr __user *uattr)
+{
+	struct bpf_prog_info __user *uinfo = u64_to_user_ptr(attr->info.info);
+	struct bpf_prog_info info = {};
+	u32 info_len = attr->info.info_len;
+	char __user *uinsns;
+	u32 ulen;
+	int err;
+
+	err = check_uarg_tail_zero(uinfo, sizeof(info), info_len);
+	if (err)
+		return err;
+	info_len = min_t(u32, sizeof(info), info_len);
+
+	if (copy_from_user(&info, uinfo, info_len))
+		return err;
+
+	info.type = prog->type;
+	info.id = prog->aux->id;
+
+	memcpy(info.tag, prog->tag, sizeof(prog->tag));
+
+	if (!capable(CAP_SYS_ADMIN)) {
+		info.jited_prog_len = 0;
+		info.xlated_prog_len = 0;
+		goto done;
+	}
+
+	ulen = info.jited_prog_len;
+	info.jited_prog_len = prog->jited_len;
+	if (info.jited_prog_len && ulen) {
+		uinsns = u64_to_user_ptr(info.jited_prog_insns);
+		ulen = min_t(u32, info.jited_prog_len, ulen);
+		if (copy_to_user(uinsns, prog->bpf_func, ulen))
+			return -EFAULT;
+	}
+
+	ulen = info.xlated_prog_len;
+	info.xlated_prog_len = bpf_prog_size(prog->len);
+	if (info.xlated_prog_len && ulen) {
+		uinsns = u64_to_user_ptr(info.xlated_prog_insns);
+		ulen = min_t(u32, info.xlated_prog_len, ulen);
+		if (copy_to_user(uinsns, prog->insnsi, ulen))
+			return -EFAULT;
+	}
+
+done:
+	if (copy_to_user(uinfo, &info, info_len) ||
+	    put_user(info_len, &uattr->info.info_len))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int bpf_map_get_info_by_fd(struct bpf_map *map,
+				  const union bpf_attr *attr,
+				  union bpf_attr __user *uattr)
+{
+	struct bpf_map_info __user *uinfo = u64_to_user_ptr(attr->info.info);
+	struct bpf_map_info info = {};
+	u32 info_len = attr->info.info_len;
+	int err;
+
+	err = check_uarg_tail_zero(uinfo, sizeof(info), info_len);
+	if (err)
+		return err;
+	info_len = min_t(u32, sizeof(info), info_len);
+
+	info.type = map->map_type;
+	info.id = map->id;
+	info.key_size = map->key_size;
+	info.value_size = map->value_size;
+	info.max_entries = map->max_entries;
+	info.map_flags = map->map_flags;
+
+	if (copy_to_user(uinfo, &info, info_len) ||
+	    put_user(info_len, &uattr->info.info_len))
+		return -EFAULT;
+
+	return 0;
+}
+
+#define BPF_OBJ_GET_INFO_BY_FD_LAST_FIELD info.info
+
+static int bpf_obj_get_info_by_fd(const union bpf_attr *attr,
+				  union bpf_attr __user *uattr)
+{
+	int ufd = attr->info.bpf_fd;
+	struct fd f;
+	int err;
+
+	if (CHECK_ATTR(BPF_OBJ_GET_INFO_BY_FD))
+		return -EINVAL;
+
+	f = fdget(ufd);
+	if (!f.file)
+		return -EBADFD;
+
+	if (f.file->f_op == &bpf_prog_fops)
+		err = bpf_prog_get_info_by_fd(f.file->private_data, attr,
+					      uattr);
+	else if (f.file->f_op == &bpf_map_fops)
+		err = bpf_map_get_info_by_fd(f.file->private_data, attr,
+					     uattr);
+	else
+		err = -EINVAL;
+
+	fdput(f);
+	return err;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -1258,23 +1397,10 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	 * user-space does not rely on any kernel feature
 	 * extensions we dont know about yet.
 	 */
-	if (size > sizeof(attr)) {
-		unsigned char __user *addr;
-		unsigned char __user *end;
-		unsigned char val;
-
-		addr = (void __user *)uattr + sizeof(attr);
-		end  = (void __user *)uattr + size;
-
-		for (; addr < end; addr++) {
-			err = get_user(val, addr);
-			if (err)
-				return err;
-			if (val)
-				return -E2BIG;
-		}
-		size = sizeof(attr);
-	}
+	err = check_uarg_tail_zero(uattr, sizeof(attr), size);
+	if (err)
+		return err;
+	size = min_t(u32, size, sizeof(attr));
 
 	/* copy attributes from user space, may be less than sizeof(bpf_attr) */
 	if (copy_from_user(&attr, uattr, size) != 0)
@@ -1330,6 +1456,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_MAP_GET_FD_BY_ID:
 		err = bpf_map_get_fd_by_id(&attr);
 		break;
+	case BPF_OBJ_GET_INFO_BY_FD:
+		err = bpf_obj_get_info_by_fd(&attr, uattr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit bd5f5f4ecb78e2698dad655645b6d6a2f7012a8c
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Jun 5 12:15:50 2017 -0700

    bpf: Add BPF_MAP_GET_FD_BY_ID
    
    Add BPF_MAP_GET_FD_BY_ID command to allow user to get a fd
    from a bpf_map's ID.
    
    bpf_map_inc_not_zero() is added and is called with map_idr_lock
    held.
    
    __bpf_map_put() is also added which has the 'bool do_idr_lock'
    param to decide if the map_idr_lock should be acquired when
    freeing the map->id.
    
    In the error path of bpf_map_inc_not_zero(), it may have to
    call __bpf_map_put(map, false) which does not need
    to take the map_idr_lock when freeing the map->id.
    
    It is currently limited to CAP_SYS_ADMIN which we can
    consider to lift it in followup patches.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index dc6253bb8ebb..1802bb9c47d9 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -135,11 +135,19 @@ static int bpf_map_alloc_id(struct bpf_map *map)
 	return id > 0 ? 0 : id;
 }
 
-static void bpf_map_free_id(struct bpf_map *map)
+static void bpf_map_free_id(struct bpf_map *map, bool do_idr_lock)
 {
-	spin_lock_bh(&map_idr_lock);
+	if (do_idr_lock)
+		spin_lock_bh(&map_idr_lock);
+	else
+		__acquire(&map_idr_lock);
+
 	idr_remove(&map_idr, map->id);
-	spin_unlock_bh(&map_idr_lock);
+
+	if (do_idr_lock)
+		spin_unlock_bh(&map_idr_lock);
+	else
+		__release(&map_idr_lock);
 }
 
 /* called from workqueue */
@@ -163,16 +171,21 @@ static void bpf_map_put_uref(struct bpf_map *map)
 /* decrement map refcnt and schedule it for freeing via workqueue
  * (unrelying map implementation ops->map_free() might sleep)
  */
-void bpf_map_put(struct bpf_map *map)
+static void __bpf_map_put(struct bpf_map *map, bool do_idr_lock)
 {
 	if (atomic_dec_and_test(&map->refcnt)) {
 		/* bpf_map_free_id() must be called first */
-		bpf_map_free_id(map);
+		bpf_map_free_id(map, do_idr_lock);
 		INIT_WORK(&map->work, bpf_map_free_deferred);
 		schedule_work(&map->work);
 	}
 }
 
+void bpf_map_put(struct bpf_map *map)
+{
+	__bpf_map_put(map, true);
+}
+
 void bpf_map_put_with_uref(struct bpf_map *map)
 {
 	bpf_map_put_uref(map);
@@ -271,15 +284,20 @@ static int map_create(union bpf_attr *attr)
 		goto free_map;
 
 	err = bpf_map_new_fd(map);
-	if (err < 0)
-		/* failed to allocate fd */
-		goto free_id;
+	if (err < 0) {
+		/* failed to allocate fd.
+		 * bpf_map_put() is needed because the above
+		 * bpf_map_alloc_id() has published the map
+		 * to the userspace and the userspace may
+		 * have refcnt-ed it through BPF_MAP_GET_FD_BY_ID.
+		 */
+		bpf_map_put(map);
+		return err;
+	}
 
 	trace_bpf_map_create(map, err);
 	return err;
 
-free_id:
-	bpf_map_free_id(map);
 free_map:
 	bpf_map_uncharge_memlock(map);
 free_map_nouncharge:
@@ -331,6 +349,28 @@ struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 	return map;
 }
 
+/* map_idr_lock should have been held */
+static struct bpf_map *bpf_map_inc_not_zero(struct bpf_map *map,
+					    bool uref)
+{
+	int refold;
+
+	refold = __atomic_add_unless(&map->refcnt, 1, 0);
+
+	if (refold >= BPF_MAX_REFCNT) {
+		__bpf_map_put(map, false);
+		return ERR_PTR(-EBUSY);
+	}
+
+	if (!refold)
+		return ERR_PTR(-ENOENT);
+
+	if (uref)
+		atomic_inc(&map->usercnt);
+
+	return map;
+}
+
 int __weak bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)
 {
 	return -ENOTSUPP;
@@ -1167,6 +1207,38 @@ static int bpf_prog_get_fd_by_id(const union bpf_attr *attr)
 	return fd;
 }
 
+#define BPF_MAP_GET_FD_BY_ID_LAST_FIELD map_id
+
+static int bpf_map_get_fd_by_id(const union bpf_attr *attr)
+{
+	struct bpf_map *map;
+	u32 id = attr->map_id;
+	int fd;
+
+	if (CHECK_ATTR(BPF_MAP_GET_FD_BY_ID))
+		return -EINVAL;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	spin_lock_bh(&map_idr_lock);
+	map = idr_find(&map_idr, id);
+	if (map)
+		map = bpf_map_inc_not_zero(map, true);
+	else
+		map = ERR_PTR(-ENOENT);
+	spin_unlock_bh(&map_idr_lock);
+
+	if (IS_ERR(map))
+		return PTR_ERR(map);
+
+	fd = bpf_map_new_fd(map);
+	if (fd < 0)
+		bpf_map_put(map);
+
+	return fd;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -1255,6 +1327,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_PROG_GET_FD_BY_ID:
 		err = bpf_prog_get_fd_by_id(&attr);
 		break;
+	case BPF_MAP_GET_FD_BY_ID:
+		err = bpf_map_get_fd_by_id(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit b16d9aa4c2b90af8d2c3201e245150f8c430c3bc
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Jun 5 12:15:49 2017 -0700

    bpf: Add BPF_PROG_GET_FD_BY_ID
    
    Add BPF_PROG_GET_FD_BY_ID command to allow user to get a fd
    from a bpf_prog's ID.
    
    bpf_prog_inc_not_zero() is added and is called with prog_idr_lock
    held.
    
    __bpf_prog_put() is also added which has the 'bool do_idr_lock'
    param to decide if the prog_idr_lock should be acquired when
    freeing the prog->id.
    
    In the error path of bpf_prog_inc_not_zero(), it may have to
    call __bpf_prog_put(map, false) which does not need
    to take the prog_idr_lock when freeing the prog->id.
    
    It is currently limited to CAP_SYS_ADMIN which we can
    consider to lift it in followup patches.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2405feedb8c1..dc6253bb8ebb 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -703,15 +703,23 @@ static int bpf_prog_alloc_id(struct bpf_prog *prog)
 	return id > 0 ? 0 : id;
 }
 
-static void bpf_prog_free_id(struct bpf_prog *prog)
+static void bpf_prog_free_id(struct bpf_prog *prog, bool do_idr_lock)
 {
 	/* cBPF to eBPF migrations are currently not in the idr store. */
 	if (!prog->aux->id)
 		return;
 
-	spin_lock_bh(&prog_idr_lock);
+	if (do_idr_lock)
+		spin_lock_bh(&prog_idr_lock);
+	else
+		__acquire(&prog_idr_lock);
+
 	idr_remove(&prog_idr, prog->aux->id);
-	spin_unlock_bh(&prog_idr_lock);
+
+	if (do_idr_lock)
+		spin_unlock_bh(&prog_idr_lock);
+	else
+		__release(&prog_idr_lock);
 }
 
 static void __bpf_prog_put_rcu(struct rcu_head *rcu)
@@ -723,16 +731,21 @@ static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 	bpf_prog_free(aux->prog);
 }
 
-void bpf_prog_put(struct bpf_prog *prog)
+static void __bpf_prog_put(struct bpf_prog *prog, bool do_idr_lock)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
 		trace_bpf_prog_put_rcu(prog);
 		/* bpf_prog_free_id() must be called first */
-		bpf_prog_free_id(prog);
+		bpf_prog_free_id(prog, do_idr_lock);
 		bpf_prog_kallsyms_del(prog);
 		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
 	}
 }
+
+void bpf_prog_put(struct bpf_prog *prog)
+{
+	__bpf_prog_put(prog, true);
+}
 EXPORT_SYMBOL_GPL(bpf_prog_put);
 
 static int bpf_prog_release(struct inode *inode, struct file *filp)
@@ -814,6 +827,24 @@ struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
 }
 EXPORT_SYMBOL_GPL(bpf_prog_inc);
 
+/* prog_idr_lock should have been held */
+static struct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)
+{
+	int refold;
+
+	refold = __atomic_add_unless(&prog->aux->refcnt, 1, 0);
+
+	if (refold >= BPF_MAX_REFCNT) {
+		__bpf_prog_put(prog, false);
+		return ERR_PTR(-EBUSY);
+	}
+
+	if (!refold)
+		return ERR_PTR(-ENOENT);
+
+	return prog;
+}
+
 static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *type)
 {
 	struct fd f = fdget(ufd);
@@ -928,16 +959,21 @@ static int bpf_prog_load(union bpf_attr *attr)
 		goto free_used_maps;
 
 	err = bpf_prog_new_fd(prog);
-	if (err < 0)
-		/* failed to allocate fd */
-		goto free_id;
+	if (err < 0) {
+		/* failed to allocate fd.
+		 * bpf_prog_put() is needed because the above
+		 * bpf_prog_alloc_id() has published the prog
+		 * to the userspace and the userspace may
+		 * have refcnt-ed it through BPF_PROG_GET_FD_BY_ID.
+		 */
+		bpf_prog_put(prog);
+		return err;
+	}
 
 	bpf_prog_kallsyms_add(prog);
 	trace_bpf_prog_load(prog, err);
 	return err;
 
-free_id:
-	bpf_prog_free_id(prog);
 free_used_maps:
 	free_used_maps(prog->aux);
 free_prog:
@@ -1099,6 +1135,38 @@ static int bpf_obj_get_next_id(const union bpf_attr *attr,
 	return err;
 }
 
+#define BPF_PROG_GET_FD_BY_ID_LAST_FIELD prog_id
+
+static int bpf_prog_get_fd_by_id(const union bpf_attr *attr)
+{
+	struct bpf_prog *prog;
+	u32 id = attr->prog_id;
+	int fd;
+
+	if (CHECK_ATTR(BPF_PROG_GET_FD_BY_ID))
+		return -EINVAL;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	spin_lock_bh(&prog_idr_lock);
+	prog = idr_find(&prog_idr, id);
+	if (prog)
+		prog = bpf_prog_inc_not_zero(prog);
+	else
+		prog = ERR_PTR(-ENOENT);
+	spin_unlock_bh(&prog_idr_lock);
+
+	if (IS_ERR(prog))
+		return PTR_ERR(prog);
+
+	fd = bpf_prog_new_fd(prog);
+	if (fd < 0)
+		bpf_prog_put(prog);
+
+	return fd;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -1184,6 +1252,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 		err = bpf_obj_get_next_id(&attr, uattr,
 					  &map_idr, &map_idr_lock);
 		break;
+	case BPF_PROG_GET_FD_BY_ID:
+		err = bpf_prog_get_fd_by_id(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit 34ad5580f8f9c86cb273ebea25c149613cd1667e
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Jun 5 12:15:48 2017 -0700

    bpf: Add BPF_(PROG|MAP)_GET_NEXT_ID command
    
    This patch adds BPF_PROG_GET_NEXT_ID and BPF_MAP_GET_NEXT_ID
    to allow userspace to iterate all bpf_prog IDs and bpf_map IDs.
    
    The API is trying to be consistent with the existing
    BPF_MAP_GET_NEXT_KEY.
    
    It is currently limited to CAP_SYS_ADMIN which we can
    consider to lift it in followup patches.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4c3075b5d840..2405feedb8c1 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -166,6 +166,7 @@ static void bpf_map_put_uref(struct bpf_map *map)
 void bpf_map_put(struct bpf_map *map)
 {
 	if (atomic_dec_and_test(&map->refcnt)) {
+		/* bpf_map_free_id() must be called first */
 		bpf_map_free_id(map);
 		INIT_WORK(&map->work, bpf_map_free_deferred);
 		schedule_work(&map->work);
@@ -726,6 +727,7 @@ void bpf_prog_put(struct bpf_prog *prog)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
 		trace_bpf_prog_put_rcu(prog);
+		/* bpf_prog_free_id() must be called first */
 		bpf_prog_free_id(prog);
 		bpf_prog_kallsyms_del(prog);
 		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
@@ -1069,6 +1071,34 @@ static int bpf_prog_test_run(const union bpf_attr *attr,
 	return ret;
 }
 
+#define BPF_OBJ_GET_NEXT_ID_LAST_FIELD next_id
+
+static int bpf_obj_get_next_id(const union bpf_attr *attr,
+			       union bpf_attr __user *uattr,
+			       struct idr *idr,
+			       spinlock_t *lock)
+{
+	u32 next_id = attr->start_id;
+	int err = 0;
+
+	if (CHECK_ATTR(BPF_OBJ_GET_NEXT_ID) || next_id >= INT_MAX)
+		return -EINVAL;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	next_id++;
+	spin_lock_bh(lock);
+	if (!idr_get_next(idr, &next_id))
+		err = -ENOENT;
+	spin_unlock_bh(lock);
+
+	if (!err)
+		err = put_user(next_id, &uattr->next_id);
+
+	return err;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -1146,6 +1176,14 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_PROG_TEST_RUN:
 		err = bpf_prog_test_run(&attr, uattr);
 		break;
+	case BPF_PROG_GET_NEXT_ID:
+		err = bpf_obj_get_next_id(&attr, uattr,
+					  &prog_idr, &prog_idr_lock);
+		break;
+	case BPF_MAP_GET_NEXT_ID:
+		err = bpf_obj_get_next_id(&attr, uattr,
+					  &map_idr, &map_idr_lock);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit f3f1c054c288bb6e503005e6d73611151ed20e91
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Jun 5 12:15:47 2017 -0700

    bpf: Introduce bpf_map ID
    
    This patch generates an unique ID for each created bpf_map.
    The approach is similar to the earlier patch for bpf_prog ID.
    
    It is worth to note that the bpf_map's ID and bpf_prog's ID
    are in two independent ID spaces and both have the same valid range:
    [1, INT_MAX).
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2a1b32b470f1..4c3075b5d840 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -27,6 +27,8 @@
 DEFINE_PER_CPU(int, bpf_prog_active);
 static DEFINE_IDR(prog_idr);
 static DEFINE_SPINLOCK(prog_idr_lock);
+static DEFINE_IDR(map_idr);
+static DEFINE_SPINLOCK(map_idr_lock);
 
 int sysctl_unprivileged_bpf_disabled __read_mostly;
 
@@ -117,6 +119,29 @@ static void bpf_map_uncharge_memlock(struct bpf_map *map)
 	free_uid(user);
 }
 
+static int bpf_map_alloc_id(struct bpf_map *map)
+{
+	int id;
+
+	spin_lock_bh(&map_idr_lock);
+	id = idr_alloc_cyclic(&map_idr, map, 1, INT_MAX, GFP_ATOMIC);
+	if (id > 0)
+		map->id = id;
+	spin_unlock_bh(&map_idr_lock);
+
+	if (WARN_ON_ONCE(!id))
+		return -ENOSPC;
+
+	return id > 0 ? 0 : id;
+}
+
+static void bpf_map_free_id(struct bpf_map *map)
+{
+	spin_lock_bh(&map_idr_lock);
+	idr_remove(&map_idr, map->id);
+	spin_unlock_bh(&map_idr_lock);
+}
+
 /* called from workqueue */
 static void bpf_map_free_deferred(struct work_struct *work)
 {
@@ -141,6 +166,7 @@ static void bpf_map_put_uref(struct bpf_map *map)
 void bpf_map_put(struct bpf_map *map)
 {
 	if (atomic_dec_and_test(&map->refcnt)) {
+		bpf_map_free_id(map);
 		INIT_WORK(&map->work, bpf_map_free_deferred);
 		schedule_work(&map->work);
 	}
@@ -239,14 +265,20 @@ static int map_create(union bpf_attr *attr)
 	if (err)
 		goto free_map_nouncharge;
 
+	err = bpf_map_alloc_id(map);
+	if (err)
+		goto free_map;
+
 	err = bpf_map_new_fd(map);
 	if (err < 0)
 		/* failed to allocate fd */
-		goto free_map;
+		goto free_id;
 
 	trace_bpf_map_create(map, err);
 	return err;
 
+free_id:
+	bpf_map_free_id(map);
 free_map:
 	bpf_map_uncharge_memlock(map);
 free_map_nouncharge:

commit dc4bb0e2356149aee4cdae061936f3bbdd45595c
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Mon Jun 5 12:15:46 2017 -0700

    bpf: Introduce bpf_prog ID
    
    This patch generates an unique ID for each BPF_PROG_LOAD-ed prog.
    It is worth to note that each BPF_PROG_LOAD-ed prog will have
    a different ID even they have the same bpf instructions.
    
    The ID is generated by the existing idr_alloc_cyclic().
    The ID is ranged from [1, INT_MAX).  It is allocated in cyclic manner,
    so an ID will get reused every 2 billion BPF_PROG_LOAD.
    
    The bpf_prog_alloc_id() is done after bpf_prog_select_runtime()
    because the jit process may have allocated a new prog.  Hence,
    we need to ensure the value of pointer 'prog' will not be changed
    any more before storing the prog to the prog_idr.
    
    After bpf_prog_select_runtime(), the prog is read-only.  Hence,
    the id is stored in 'struct bpf_prog_aux'.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 59da103adb85..2a1b32b470f1 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -22,8 +22,11 @@
 #include <linux/filter.h>
 #include <linux/version.h>
 #include <linux/kernel.h>
+#include <linux/idr.h>
 
 DEFINE_PER_CPU(int, bpf_prog_active);
+static DEFINE_IDR(prog_idr);
+static DEFINE_SPINLOCK(prog_idr_lock);
 
 int sysctl_unprivileged_bpf_disabled __read_mostly;
 
@@ -650,6 +653,34 @@ static void bpf_prog_uncharge_memlock(struct bpf_prog *prog)
 	free_uid(user);
 }
 
+static int bpf_prog_alloc_id(struct bpf_prog *prog)
+{
+	int id;
+
+	spin_lock_bh(&prog_idr_lock);
+	id = idr_alloc_cyclic(&prog_idr, prog, 1, INT_MAX, GFP_ATOMIC);
+	if (id > 0)
+		prog->aux->id = id;
+	spin_unlock_bh(&prog_idr_lock);
+
+	/* id is in [1, INT_MAX) */
+	if (WARN_ON_ONCE(!id))
+		return -ENOSPC;
+
+	return id > 0 ? 0 : id;
+}
+
+static void bpf_prog_free_id(struct bpf_prog *prog)
+{
+	/* cBPF to eBPF migrations are currently not in the idr store. */
+	if (!prog->aux->id)
+		return;
+
+	spin_lock_bh(&prog_idr_lock);
+	idr_remove(&prog_idr, prog->aux->id);
+	spin_unlock_bh(&prog_idr_lock);
+}
+
 static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 {
 	struct bpf_prog_aux *aux = container_of(rcu, struct bpf_prog_aux, rcu);
@@ -663,6 +694,7 @@ void bpf_prog_put(struct bpf_prog *prog)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
 		trace_bpf_prog_put_rcu(prog);
+		bpf_prog_free_id(prog);
 		bpf_prog_kallsyms_del(prog);
 		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
 	}
@@ -857,15 +889,21 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (err < 0)
 		goto free_used_maps;
 
+	err = bpf_prog_alloc_id(prog);
+	if (err)
+		goto free_used_maps;
+
 	err = bpf_prog_new_fd(prog);
 	if (err < 0)
 		/* failed to allocate fd */
-		goto free_used_maps;
+		goto free_id;
 
 	bpf_prog_kallsyms_add(prog);
 	trace_bpf_prog_load(prog, err);
 	return err;
 
+free_id:
+	bpf_prog_free_id(prog);
 free_used_maps:
 	free_used_maps(prog->aux);
 free_prog:

commit 80b7d81912d807f161d55e9c2c9cc81061666f83
Author: Chenbo Feng <fengc@google.com>
Date:   Wed May 31 18:16:00 2017 -0700

    bpf: Remove the capability check for cgroup skb eBPF program
    
    Currently loading a cgroup skb eBPF program require a CAP_SYS_ADMIN
    capability while attaching the program to a cgroup only requires the
    user have CAP_NET_ADMIN privilege. We can escape the capability
    check when load the program just like socket filter program to make
    the capability requirement consistent.
    
    Change since v1:
    Change the code style in order to be compliant with checkpatch.pl
    preference
    
    Signed-off-by: Chenbo Feng <fengc@google.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 265a0d854e33..59da103adb85 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -815,7 +815,9 @@ static int bpf_prog_load(union bpf_attr *attr)
 	    attr->kern_version != LINUX_VERSION_CODE)
 		return -EINVAL;
 
-	if (type != BPF_PROG_TYPE_SOCKET_FILTER && !capable(CAP_SYS_ADMIN))
+	if (type != BPF_PROG_TYPE_SOCKET_FILTER &&
+	    type != BPF_PROG_TYPE_CGROUP_SKB &&
+	    !capable(CAP_SYS_ADMIN))
 		return -EPERM;
 
 	/* plain bpf_prog allocation */

commit e07b98d9bffe410019dfcf62c3428d4a96c56a2c
Author: David S. Miller <davem@davemloft.net>
Date:   Wed May 10 11:38:07 2017 -0700

    bpf: Add strict alignment flag for BPF_PROG_LOAD.
    
    Add a new field, "prog_flags", and an initial flag value
    BPF_F_STRICT_ALIGNMENT.
    
    When set, the verifier will enforce strict pointer alignment
    regardless of the setting of CONFIG_EFFICIENT_UNALIGNED_ACCESS.
    
    The verifier, in this mode, will also use a fixed value of "2" in
    place of NET_IP_ALIGN.
    
    This facilitates test cases that will exercise and validate this part
    of the verifier even when run on architectures where alignment doesn't
    matter.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index fd2411fd6914..265a0d854e33 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -783,7 +783,7 @@ struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
 EXPORT_SYMBOL_GPL(bpf_prog_get_type);
 
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD kern_version
+#define	BPF_PROG_LOAD_LAST_FIELD prog_flags
 
 static int bpf_prog_load(union bpf_attr *attr)
 {
@@ -796,6 +796,9 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (CHECK_ATTR(BPF_PROG_LOAD))
 		return -EINVAL;
 
+	if (attr->prog_flags & ~BPF_F_STRICT_ALIGNMENT)
+		return -EINVAL;
+
 	/* copy eBPF program license from user space */
 	if (strncpy_from_user(license, u64_to_user_ptr(attr->license),
 			      sizeof(license) - 1) < 0)

commit 19809c2da28aee5860ad9a2eff760730a0710df0
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 8 15:57:44 2017 -0700

    mm, vmalloc: use __GFP_HIGHMEM implicitly
    
    __vmalloc* allows users to provide gfp flags for the underlying
    allocation.  This API is quite popular
    
      $ git grep "=[[:space:]]__vmalloc\|return[[:space:]]*__vmalloc" | wc -l
      77
    
    The only problem is that many people are not aware that they really want
    to give __GFP_HIGHMEM along with other flags because there is really no
    reason to consume precious lowmemory on CONFIG_HIGHMEM systems for pages
    which are mapped to the kernel vmalloc space.  About half of users don't
    use this flag, though.  This signals that we make the API unnecessarily
    too complex.
    
    This patch simply uses __GFP_HIGHMEM implicitly when allocating pages to
    be mapped to the vmalloc space.  Current users which add __GFP_HIGHMEM
    are simplified and drop the flag.
    
    Link: http://lkml.kernel.org/r/20170307141020.29107-1-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Matthew Wilcox <mawilcox@microsoft.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Cristopher Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 13642c73dca0..fd2411fd6914 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -67,8 +67,7 @@ void *bpf_map_area_alloc(size_t size)
 			return area;
 	}
 
-	return __vmalloc(size, GFP_KERNEL | __GFP_HIGHMEM | flags,
-			 PAGE_KERNEL);
+	return __vmalloc(size, GFP_KERNEL | flags, PAGE_KERNEL);
 }
 
 void bpf_map_area_free(void *area)

commit 8fe45924387be6b5c1be59a7eb330790c61d5d10
Author: Teng Qin <qinteng@fb.com>
Date:   Mon Apr 24 19:00:37 2017 -0700

    bpf: map_get_next_key to return first key on NULL
    
    When iterating through a map, we need to find a key that does not exist
    in the map so map_get_next_key will give us the first key of the map.
    This often requires a lot of guessing in production systems.
    
    This patch makes map_get_next_key return the first key when the key
    pointer in the parameter is NULL.
    
    Signed-off-by: Teng Qin <qinteng@fb.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b89288e2b589..13642c73dca0 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -536,14 +536,18 @@ static int map_get_next_key(union bpf_attr *attr)
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
-	err = -ENOMEM;
-	key = kmalloc(map->key_size, GFP_USER);
-	if (!key)
-		goto err_put;
-
-	err = -EFAULT;
-	if (copy_from_user(key, ukey, map->key_size) != 0)
-		goto free_key;
+	if (ukey) {
+		err = -ENOMEM;
+		key = kmalloc(map->key_size, GFP_USER);
+		if (!key)
+			goto err_put;
+
+		err = -EFAULT;
+		if (copy_from_user(key, ukey, map->key_size) != 0)
+			goto free_key;
+	} else {
+		key = NULL;
+	}
 
 	err = -ENOMEM;
 	next_key = kmalloc(map->key_size, GFP_USER);

commit 40077e0cf62206ac3c315b6991d8dcddb3703286
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Tue Apr 11 15:34:58 2017 +0200

    bpf: remove struct bpf_map_type_list
    
    There's no need to have struct bpf_map_type_list since
    it just contains a list_head, the type, and the ops
    pointer. Since the types are densely packed and not
    actually dynamically registered, it's much easier and
    smaller to have an array of type->ops pointer. Also
    initialize this array statically to remove code needed
    to initialize it.
    
    In order to save duplicating the list, move it to the
    types header file added by the previous patch and
    include it in the same fashion.
    
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ea55691cbf5e..b89288e2b589 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -27,30 +27,29 @@ DEFINE_PER_CPU(int, bpf_prog_active);
 
 int sysctl_unprivileged_bpf_disabled __read_mostly;
 
-static LIST_HEAD(bpf_map_types);
+static const struct bpf_map_ops * const bpf_map_types[] = {
+#define BPF_PROG_TYPE(_id, _ops)
+#define BPF_MAP_TYPE(_id, _ops) \
+	[_id] = &_ops,
+#include <linux/bpf_types.h>
+#undef BPF_PROG_TYPE
+#undef BPF_MAP_TYPE
+};
 
 static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
 {
-	struct bpf_map_type_list *tl;
 	struct bpf_map *map;
 
-	list_for_each_entry(tl, &bpf_map_types, list_node) {
-		if (tl->type == attr->map_type) {
-			map = tl->ops->map_alloc(attr);
-			if (IS_ERR(map))
-				return map;
-			map->ops = tl->ops;
-			map->map_type = attr->map_type;
-			return map;
-		}
-	}
-	return ERR_PTR(-EINVAL);
-}
+	if (attr->map_type >= ARRAY_SIZE(bpf_map_types) ||
+	    !bpf_map_types[attr->map_type])
+		return ERR_PTR(-EINVAL);
 
-/* boot time registration of different map implementations */
-void bpf_register_map_type(struct bpf_map_type_list *tl)
-{
-	list_add(&tl->list_node, &bpf_map_types);
+	map = bpf_map_types[attr->map_type]->map_alloc(attr);
+	if (IS_ERR(map))
+		return map;
+	map->ops = bpf_map_types[attr->map_type];
+	map->map_type = attr->map_type;
+	return map;
 }
 
 void *bpf_map_area_alloc(size_t size)
@@ -576,8 +575,10 @@ static int map_get_next_key(union bpf_attr *attr)
 static const struct bpf_verifier_ops * const bpf_prog_types[] = {
 #define BPF_PROG_TYPE(_id, _ops) \
 	[_id] = &_ops,
+#define BPF_MAP_TYPE(_id, _ops)
 #include <linux/bpf_types.h>
 #undef BPF_PROG_TYPE
+#undef BPF_MAP_TYPE
 };
 
 static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)

commit be9370a7d8614d1fa54649c75de14458e79b91ec
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Tue Apr 11 15:34:57 2017 +0200

    bpf: remove struct bpf_prog_type_list
    
    There's no need to have struct bpf_prog_type_list since
    it just contains a list_head, the type, and the ops
    pointer. Since the types are densely packed and not
    actually dynamically registered, it's much easier and
    smaller to have an array of type->ops pointer. Also
    initialize this array statically to remove code needed
    to initialize it.
    
    In order to save duplicating the list, move it to a new
    header file and include it in the places needing it.
    
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ab0cf4c43690..ea55691cbf5e 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -573,26 +573,21 @@ static int map_get_next_key(union bpf_attr *attr)
 	return err;
 }
 
-static LIST_HEAD(bpf_prog_types);
+static const struct bpf_verifier_ops * const bpf_prog_types[] = {
+#define BPF_PROG_TYPE(_id, _ops) \
+	[_id] = &_ops,
+#include <linux/bpf_types.h>
+#undef BPF_PROG_TYPE
+};
 
 static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
 {
-	struct bpf_prog_type_list *tl;
-
-	list_for_each_entry(tl, &bpf_prog_types, list_node) {
-		if (tl->type == type) {
-			prog->aux->ops = tl->ops;
-			prog->type = type;
-			return 0;
-		}
-	}
-
-	return -EINVAL;
-}
+	if (type >= ARRAY_SIZE(bpf_prog_types) || !bpf_prog_types[type])
+		return -EINVAL;
 
-void bpf_register_prog_type(struct bpf_prog_type_list *tl)
-{
-	list_add(&tl->list_node, &bpf_prog_types);
+	prog->aux->ops = bpf_prog_types[type];
+	prog->type = type;
+	return 0;
 }
 
 /* drop refcnt on maps used by eBPF program and free auxilary data */

commit 1cf1cae963c2e6032aebe1637e995bc2f5d330f4
Author: Alexei Starovoitov <ast@fb.com>
Date:   Thu Mar 30 21:45:38 2017 -0700

    bpf: introduce BPF_PROG_TEST_RUN command
    
    development and testing of networking bpf programs is quite cumbersome.
    Despite availability of user space bpf interpreters the kernel is
    the ultimate authority and execution environment.
    Current test frameworks for TC include creation of netns, veth,
    qdiscs and use of various packet generators just to test functionality
    of a bpf program. XDP testing is even more complicated, since
    qemu needs to be started with gro/gso disabled and precise queue
    configuration, transferring of xdp program from host into guest,
    attaching to virtio/eth0 and generating traffic from the host
    while capturing the results from the guest.
    
    Moreover analyzing performance bottlenecks in XDP program is
    impossible in virtio environment, since cost of running the program
    is tiny comparing to the overhead of virtio packet processing,
    so performance testing can only be done on physical nic
    with another server generating traffic.
    
    Furthermore ongoing changes to user space control plane of production
    applications cannot be run on the test servers leaving bpf programs
    stubbed out for testing.
    
    Last but not least, the upstream llvm changes are validated by the bpf
    backend testsuite which has no ability to test the code generated.
    
    To improve this situation introduce BPF_PROG_TEST_RUN command
    to test and performance benchmark bpf programs.
    
    Joint work with Daniel Borkmann.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Martin KaFai Lau <kafai@fb.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c35ebfe6d84d..ab0cf4c43690 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -973,6 +973,28 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 }
 #endif /* CONFIG_CGROUP_BPF */
 
+#define BPF_PROG_TEST_RUN_LAST_FIELD test.duration
+
+static int bpf_prog_test_run(const union bpf_attr *attr,
+			     union bpf_attr __user *uattr)
+{
+	struct bpf_prog *prog;
+	int ret = -ENOTSUPP;
+
+	if (CHECK_ATTR(BPF_PROG_TEST_RUN))
+		return -EINVAL;
+
+	prog = bpf_prog_get(attr->test.prog_fd);
+	if (IS_ERR(prog))
+		return PTR_ERR(prog);
+
+	if (prog->aux->ops->test_run)
+		ret = prog->aux->ops->test_run(prog, attr, uattr);
+
+	bpf_prog_put(prog);
+	return ret;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -1039,7 +1061,6 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_OBJ_GET:
 		err = bpf_obj_get(&attr);
 		break;
-
 #ifdef CONFIG_CGROUP_BPF
 	case BPF_PROG_ATTACH:
 		err = bpf_prog_attach(&attr);
@@ -1048,7 +1069,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 		err = bpf_prog_detach(&attr);
 		break;
 #endif
-
+	case BPF_PROG_TEST_RUN:
+		err = bpf_prog_test_run(&attr, uattr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit bcc6b1b7ebf857a9fe56202e2be3361131588c15
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Mar 22 10:00:34 2017 -0700

    bpf: Add hash of maps support
    
    This patch adds hash of maps support (hashmap->bpf_map).
    BPF_MAP_TYPE_HASH_OF_MAPS is added.
    
    A map-in-map contains a pointer to another map and lets call
    this pointer 'inner_map_ptr'.
    
    Notes on deleting inner_map_ptr from a hash map:
    
    1. For BPF_F_NO_PREALLOC map-in-map, when deleting
       an inner_map_ptr, the htab_elem itself will go through
       a rcu grace period and the inner_map_ptr resides
       in the htab_elem.
    
    2. For pre-allocated htab_elem (!BPF_F_NO_PREALLOC),
       when deleting an inner_map_ptr, the htab_elem may
       get reused immediately.  This situation is similar
       to the existing prealloc-ated use cases.
    
       However, the bpf_map_fd_put_ptr() calls bpf_map_put() which calls
       inner_map->ops->map_free(inner_map) which will go
       through a rcu grace period (i.e. all bpf_map's map_free
       currently goes through a rcu grace period).  Hence,
       the inner_map_ptr is still safe for the rcu reader side.
    
    This patch also includes BPF_MAP_TYPE_HASH_OF_MAPS to the
    check_map_prealloc() in the verifier.  preallocation is a
    must for BPF_PROG_TYPE_PERF_EVENT.  Hence, even we don't expect
    heavy updates to map-in-map, enforcing BPF_F_NO_PREALLOC for map-in-map
    is impossible without disallowing BPF_PROG_TYPE_PERF_EVENT from using
    map-in-map first.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 6e24fdf1f373..c35ebfe6d84d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -352,7 +352,8 @@ static int map_lookup_elem(union bpf_attr *attr)
 		err = bpf_percpu_array_copy(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {
 		err = bpf_stackmap_copy(map, key, value);
-	} else if (map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS) {
+	} else if (map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS ||
+		   map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {
 		err = -ENOTSUPP;
 	} else {
 		rcu_read_lock();
@@ -446,6 +447,11 @@ static int map_update_elem(union bpf_attr *attr)
 		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
 						   attr->flags);
 		rcu_read_unlock();
+	} else if (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {
+		rcu_read_lock();
+		err = bpf_fd_htab_map_update_elem(map, f.file, key, value,
+						  attr->flags);
+		rcu_read_unlock();
 	} else {
 		rcu_read_lock();
 		err = map->ops->map_update_elem(map, key, value, attr->flags);

commit 56f668dfe00dcf086734f1c42ea999398fad6572
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Mar 22 10:00:33 2017 -0700

    bpf: Add array of maps support
    
    This patch adds a few helper funcs to enable map-in-map
    support (i.e. outer_map->inner_map).  The first outer_map type
    BPF_MAP_TYPE_ARRAY_OF_MAPS is also added in this patch.
    The next patch will introduce a hash of maps type.
    
    Any bpf map type can be acted as an inner_map.  The exception
    is BPF_MAP_TYPE_PROG_ARRAY because the extra level of
    indirection makes it harder to verify the owner_prog_type
    and owner_jited.
    
    Multi-level map-in-map is not supported (i.e. map->map is ok
    but not map->map->map).
    
    When adding an inner_map to an outer_map, it currently checks the
    map_type, key_size, value_size, map_flags, max_entries and ops.
    The verifier also uses those map's properties to do static analysis.
    map_flags is needed because we need to ensure BPF_PROG_TYPE_PERF_EVENT
    is using a preallocated hashtab for the inner_hash also.  ops and
    max_entries are needed to generate inlined map-lookup instructions.
    For simplicity reason, a simple '==' test is used for both map_flags
    and max_entries.  The equality of ops is implied by the equality of
    map_type.
    
    During outer_map creation time, an inner_map_fd is needed to create an
    outer_map.  However, the inner_map_fd's life time does not depend on the
    outer_map.  The inner_map_fd is merely used to initialize
    the inner_map_meta of the outer_map.
    
    Also, for the outer_map:
    
    * It allows element update and delete from syscall
    * It allows element lookup from bpf_prog
    
    The above is similar to the current fd_array pattern.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 48c914b983bd..6e24fdf1f373 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -215,7 +215,7 @@ int bpf_map_new_fd(struct bpf_map *map)
 		   offsetof(union bpf_attr, CMD##_LAST_FIELD) - \
 		   sizeof(attr->CMD##_LAST_FIELD)) != NULL
 
-#define BPF_MAP_CREATE_LAST_FIELD map_flags
+#define BPF_MAP_CREATE_LAST_FIELD inner_map_fd
 /* called via syscall */
 static int map_create(union bpf_attr *attr)
 {
@@ -352,6 +352,8 @@ static int map_lookup_elem(union bpf_attr *attr)
 		err = bpf_percpu_array_copy(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {
 		err = bpf_stackmap_copy(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS) {
+		err = -ENOTSUPP;
 	} else {
 		rcu_read_lock();
 		ptr = map->ops->map_lookup_elem(map, key);
@@ -438,7 +440,8 @@ static int map_update_elem(union bpf_attr *attr)
 		err = bpf_percpu_array_update(map, key, value, attr->flags);
 	} else if (map->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY ||
 		   map->map_type == BPF_MAP_TYPE_PROG_ARRAY ||
-		   map->map_type == BPF_MAP_TYPE_CGROUP_ARRAY) {
+		   map->map_type == BPF_MAP_TYPE_CGROUP_ARRAY ||
+		   map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS) {
 		rcu_read_lock();
 		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
 						   attr->flags);

commit e245c5c6a5656e4d61aa7bb08e9694fd6e5b2b9d
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Mar 15 18:26:39 2017 -0700

    bpf: move fixup_bpf_calls() function
    
    no functional change.
    move fixup_bpf_calls() to verifier.c
    it's being refactored in the next patch
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 7af0dcc5d755..48c914b983bd 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -586,59 +586,6 @@ void bpf_register_prog_type(struct bpf_prog_type_list *tl)
 	list_add(&tl->list_node, &bpf_prog_types);
 }
 
-/* fixup insn->imm field of bpf_call instructions:
- * if (insn->imm == BPF_FUNC_map_lookup_elem)
- *      insn->imm = bpf_map_lookup_elem - __bpf_call_base;
- * else if (insn->imm == BPF_FUNC_map_update_elem)
- *      insn->imm = bpf_map_update_elem - __bpf_call_base;
- * else ...
- *
- * this function is called after eBPF program passed verification
- */
-static void fixup_bpf_calls(struct bpf_prog *prog)
-{
-	const struct bpf_func_proto *fn;
-	int i;
-
-	for (i = 0; i < prog->len; i++) {
-		struct bpf_insn *insn = &prog->insnsi[i];
-
-		if (insn->code == (BPF_JMP | BPF_CALL)) {
-			/* we reach here when program has bpf_call instructions
-			 * and it passed bpf_check(), means that
-			 * ops->get_func_proto must have been supplied, check it
-			 */
-			BUG_ON(!prog->aux->ops->get_func_proto);
-
-			if (insn->imm == BPF_FUNC_get_route_realm)
-				prog->dst_needed = 1;
-			if (insn->imm == BPF_FUNC_get_prandom_u32)
-				bpf_user_rnd_init_once();
-			if (insn->imm == BPF_FUNC_xdp_adjust_head)
-				prog->xdp_adjust_head = 1;
-			if (insn->imm == BPF_FUNC_tail_call) {
-				/* mark bpf_tail_call as different opcode
-				 * to avoid conditional branch in
-				 * interpeter for every normal call
-				 * and to prevent accidental JITing by
-				 * JIT compiler that doesn't support
-				 * bpf_tail_call yet
-				 */
-				insn->imm = 0;
-				insn->code |= BPF_X;
-				continue;
-			}
-
-			fn = prog->aux->ops->get_func_proto(insn->imm);
-			/* all functions that have prototype and verifier allowed
-			 * programs to call them, must be real in-kernel functions
-			 */
-			BUG_ON(!fn->func);
-			insn->imm = fn->func - __bpf_call_base;
-		}
-	}
-}
-
 /* drop refcnt on maps used by eBPF program and free auxilary data */
 static void free_used_maps(struct bpf_prog_aux *aux)
 {
@@ -892,9 +839,6 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (err < 0)
 		goto free_used_maps;
 
-	/* fixup BPF_CALL->imm field */
-	fixup_bpf_calls(prog);
-
 	/* eBPF program is ready to be JITed */
 	prog = bpf_prog_select_runtime(prog, &err);
 	if (err < 0)

commit 3f07c0144132e4f59d88055ac8ff3e691a5fa2b8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/signal.h>
    
    We are going to split <linux/sched/signal.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/signal.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 461eb1e66a0f..7af0dcc5d755 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -13,6 +13,7 @@
 #include <linux/bpf_trace.h>
 #include <linux/syscalls.h>
 #include <linux/slab.h>
+#include <linux/sched/signal.h>
 #include <linux/vmalloc.h>
 #include <linux/mmzone.h>
 #include <linux/anon_inodes.h>

commit 74451e66d516c55e309e8d89a4a1e7596e46aacd
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Feb 16 22:24:50 2017 +0100

    bpf: make jited programs visible in traces
    
    Long standing issue with JITed programs is that stack traces from
    function tracing check whether a given address is kernel code
    through {__,}kernel_text_address(), which checks for code in core
    kernel, modules and dynamically allocated ftrace trampolines. But
    what is still missing is BPF JITed programs (interpreted programs
    are not an issue as __bpf_prog_run() will be attributed to them),
    thus when a stack trace is triggered, the code walking the stack
    won't see any of the JITed ones. The same for address correlation
    done from user space via reading /proc/kallsyms. This is read by
    tools like perf, but the latter is also useful for permanent live
    tracing with eBPF itself in combination with stack maps when other
    eBPF types are part of the callchain. See offwaketime example on
    dumping stack from a map.
    
    This work tries to tackle that issue by making the addresses and
    symbols known to the kernel. The lookup from *kernel_text_address()
    is implemented through a latched RB tree that can be read under
    RCU in fast-path that is also shared for symbol/size/offset lookup
    for a specific given address in kallsyms. The slow-path iteration
    through all symbols in the seq file done via RCU list, which holds
    a tiny fraction of all exported ksyms, usually below 0.1 percent.
    Function symbols are exported as bpf_prog_<tag>, in order to aide
    debugging and attribution. This facility is currently enabled for
    root-only when bpf_jit_kallsyms is set to 1, and disabled if hardening
    is active in any mode. The rationale behind this is that still a lot
    of systems ship with world read permissions on kallsyms thus addresses
    should not get suddenly exposed for them. If that situation gets
    much better in future, we always have the option to change the
    default on this. Likewise, unprivileged programs are not allowed
    to add entries there either, but that is less of a concern as most
    such programs types relevant in this context are for root-only anyway.
    If enabled, call graphs and stack traces will then show a correct
    attribution; one example is illustrated below, where the trace is
    now visible in tooling such as perf script --kallsyms=/proc/kallsyms
    and friends.
    
    Before:
    
      7fff8166889d bpf_clone_redirect+0x80007f0020ed (/lib/modules/4.9.0-rc8+/build/vmlinux)
             f5d80 __sendmsg_nocancel+0xffff006451f1a007 (/usr/lib64/libc-2.18.so)
    
    After:
    
      7fff816688b7 bpf_clone_redirect+0x80007f002107 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fffa0575728 bpf_prog_33c45a467c9e061a+0x8000600020fb (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fffa07ef1fc cls_bpf_classify+0x8000600020dc (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff81678b68 tc_classify+0x80007f002078 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8164d40b __netif_receive_skb_core+0x80007f0025fb (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8164d718 __netif_receive_skb+0x80007f002018 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8164e565 process_backlog+0x80007f002095 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8164dc71 net_rx_action+0x80007f002231 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff81767461 __softirqentry_text_start+0x80007f0020d1 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff817658ac do_softirq_own_stack+0x80007f00201c (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff810a2c20 do_softirq+0x80007f002050 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff810a2cb5 __local_bh_enable_ip+0x80007f002085 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8168d452 ip_finish_output2+0x80007f002152 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8168ea3d ip_finish_output+0x80007f00217d (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff8168f2af ip_output+0x80007f00203f (/lib/modules/4.9.0-rc8+/build/vmlinux)
      [...]
      7fff81005854 do_syscall_64+0x80007f002054 (/lib/modules/4.9.0-rc8+/build/vmlinux)
      7fff817649eb return_from_SYSCALL_64+0x80007f002000 (/lib/modules/4.9.0-rc8+/build/vmlinux)
             f5d80 __sendmsg_nocancel+0xffff01c484812007 (/usr/lib64/libc-2.18.so)
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index f74ca17af64a..461eb1e66a0f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -707,6 +707,7 @@ void bpf_prog_put(struct bpf_prog *prog)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
 		trace_bpf_prog_put_rcu(prog);
+		bpf_prog_kallsyms_del(prog);
 		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
 	}
 }
@@ -903,6 +904,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 		/* failed to allocate fd */
 		goto free_used_maps;
 
+	bpf_prog_kallsyms_add(prog);
 	trace_bpf_prog_load(prog, err);
 	return err;
 

commit 3f64116a838e6c3468f9d5eed7f1f87cf3a2c3eb
Merge: f3caf8618bce 4695daefba8d
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Feb 16 19:34:01 2017 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit 7f677633379b4abb3281cdbe7e7006f049305c03
Author: Alexei Starovoitov <ast@fb.com>
Date:   Fri Feb 10 20:28:24 2017 -0800

    bpf: introduce BPF_F_ALLOW_OVERRIDE flag
    
    If BPF_F_ALLOW_OVERRIDE flag is used in BPF_PROG_ATTACH command
    to the given cgroup the descendent cgroup will be able to override
    effective bpf program that was inherited from this cgroup.
    By default it's not passed, therefore override is disallowed.
    
    Examples:
    1.
    prog X attached to /A with default
    prog Y fails to attach to /A/B and /A/B/C
    Everything under /A runs prog X
    
    2.
    prog X attached to /A with allow_override.
    prog Y fails to attach to /A/B with default (non-override)
    prog M attached to /A/B with allow_override.
    Everything under /A/B runs prog M only.
    
    3.
    prog X attached to /A with allow_override.
    prog Y fails to attach to /A with default.
    The user has to detach first to switch the mode.
    
    In the future this behavior may be extended with a chain of
    non-overridable programs.
    
    Also fix the bug where detach from cgroup where nothing is attached
    was not throwing error. Return ENOENT in such case.
    
    Add several testcases and adjust libbpf.
    
    Fixes: 3007098494be ("cgroup: add support for eBPF programs")
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Tejun Heo <tj@kernel.org>
    Acked-by: Daniel Mack <daniel@zonque.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 19b6129eab23..bbb016adbaeb 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -920,13 +920,14 @@ static int bpf_obj_get(const union bpf_attr *attr)
 
 #ifdef CONFIG_CGROUP_BPF
 
-#define BPF_PROG_ATTACH_LAST_FIELD attach_type
+#define BPF_PROG_ATTACH_LAST_FIELD attach_flags
 
 static int bpf_prog_attach(const union bpf_attr *attr)
 {
+	enum bpf_prog_type ptype;
 	struct bpf_prog *prog;
 	struct cgroup *cgrp;
-	enum bpf_prog_type ptype;
+	int ret;
 
 	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
@@ -934,6 +935,9 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	if (CHECK_ATTR(BPF_PROG_ATTACH))
 		return -EINVAL;
 
+	if (attr->attach_flags & ~BPF_F_ALLOW_OVERRIDE)
+		return -EINVAL;
+
 	switch (attr->attach_type) {
 	case BPF_CGROUP_INET_INGRESS:
 	case BPF_CGROUP_INET_EGRESS:
@@ -956,10 +960,13 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 		return PTR_ERR(cgrp);
 	}
 
-	cgroup_bpf_update(cgrp, prog, attr->attach_type);
+	ret = cgroup_bpf_update(cgrp, prog, attr->attach_type,
+				attr->attach_flags & BPF_F_ALLOW_OVERRIDE);
+	if (ret)
+		bpf_prog_put(prog);
 	cgroup_put(cgrp);
 
-	return 0;
+	return ret;
 }
 
 #define BPF_PROG_DETACH_LAST_FIELD attach_type
@@ -967,6 +974,7 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 static int bpf_prog_detach(const union bpf_attr *attr)
 {
 	struct cgroup *cgrp;
+	int ret;
 
 	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
@@ -982,7 +990,7 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 		if (IS_ERR(cgrp))
 			return PTR_ERR(cgrp);
 
-		cgroup_bpf_update(cgrp, NULL, attr->attach_type);
+		ret = cgroup_bpf_update(cgrp, NULL, attr->attach_type, false);
 		cgroup_put(cgrp);
 		break;
 
@@ -990,7 +998,7 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 		return -EINVAL;
 	}
 
-	return 0;
+	return ret;
 }
 #endif /* CONFIG_CGROUP_BPF */
 

commit 4e8f2fc1a55d543717efb70e170b09e773d0542b
Merge: 158f323b9868 1b1bc42c1692
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Jan 28 10:33:06 2017 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Two trivial overlapping changes conflicts in MPLS and mlx5.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a67edbf4fb6deadcfe57a04a134abed4a5ba3bb5
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Jan 25 02:28:18 2017 +0100

    bpf: add initial bpf tracepoints
    
    This work adds a number of tracepoints to paths that are either
    considered slow-path or exception-like states, where monitoring or
    inspecting them would be desirable.
    
    For bpf(2) syscall, tracepoints have been placed for main commands
    when they succeed. In XDP case, tracepoint is for exceptions, that
    is, f.e. on abnormal BPF program exit such as unknown or XDP_ABORTED
    return code, or when error occurs during XDP_TX action and the packet
    could not be forwarded.
    
    Both have been split into separate event headers, and can be further
    extended. Worst case, if they unexpectedly should get into our way in
    future, they can also removed [1]. Of course, these tracepoints (like
    any other) can be analyzed by eBPF itself, etc. Example output:
    
      # ./perf record -a -e bpf:* sleep 10
      # ./perf script
      sock_example  6197 [005]   283.980322:      bpf:bpf_map_create: map type=ARRAY ufd=4 key=4 val=8 max=256 flags=0
      sock_example  6197 [005]   283.980721:       bpf:bpf_prog_load: prog=a5ea8fa30ea6849c type=SOCKET_FILTER ufd=5
      sock_example  6197 [005]   283.988423:   bpf:bpf_prog_get_type: prog=a5ea8fa30ea6849c type=SOCKET_FILTER
      sock_example  6197 [005]   283.988443: bpf:bpf_map_lookup_elem: map type=ARRAY ufd=4 key=[06 00 00 00] val=[00 00 00 00 00 00 00 00]
      [...]
      sock_example  6197 [005]   288.990868: bpf:bpf_map_lookup_elem: map type=ARRAY ufd=4 key=[01 00 00 00] val=[14 00 00 00 00 00 00 00]
           swapper     0 [005]   289.338243:    bpf:bpf_prog_put_rcu: prog=a5ea8fa30ea6849c type=SOCKET_FILTER
    
      [1] https://lwn.net/Articles/705270/
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 1d6b29e4e2c3..05ad086ab71d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -10,6 +10,7 @@
  * General Public License for more details.
  */
 #include <linux/bpf.h>
+#include <linux/bpf_trace.h>
 #include <linux/syscalls.h>
 #include <linux/slab.h>
 #include <linux/anon_inodes.h>
@@ -215,6 +216,7 @@ static int map_create(union bpf_attr *attr)
 		/* failed to allocate fd */
 		goto free_map;
 
+	trace_bpf_map_create(map, err);
 	return err;
 
 free_map:
@@ -339,6 +341,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 	if (copy_to_user(uvalue, value, value_size) != 0)
 		goto free_value;
 
+	trace_bpf_map_lookup_elem(map, ufd, key, value);
 	err = 0;
 
 free_value:
@@ -421,6 +424,8 @@ static int map_update_elem(union bpf_attr *attr)
 	__this_cpu_dec(bpf_prog_active);
 	preempt_enable();
 
+	if (!err)
+		trace_bpf_map_update_elem(map, ufd, key, value);
 free_value:
 	kfree(value);
 free_key:
@@ -466,6 +471,8 @@ static int map_delete_elem(union bpf_attr *attr)
 	__this_cpu_dec(bpf_prog_active);
 	preempt_enable();
 
+	if (!err)
+		trace_bpf_map_delete_elem(map, ufd, key);
 free_key:
 	kfree(key);
 err_put:
@@ -518,6 +525,7 @@ static int map_get_next_key(union bpf_attr *attr)
 	if (copy_to_user(unext_key, next_key, map->key_size) != 0)
 		goto free_next_key;
 
+	trace_bpf_map_next_key(map, ufd, key, next_key);
 	err = 0;
 
 free_next_key:
@@ -671,8 +679,10 @@ static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 
 void bpf_prog_put(struct bpf_prog *prog)
 {
-	if (atomic_dec_and_test(&prog->aux->refcnt))
+	if (atomic_dec_and_test(&prog->aux->refcnt)) {
+		trace_bpf_prog_put_rcu(prog);
 		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
+	}
 }
 EXPORT_SYMBOL_GPL(bpf_prog_put);
 
@@ -781,7 +791,11 @@ struct bpf_prog *bpf_prog_get(u32 ufd)
 
 struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
 {
-	return __bpf_prog_get(ufd, &type);
+	struct bpf_prog *prog = __bpf_prog_get(ufd, &type);
+
+	if (!IS_ERR(prog))
+		trace_bpf_prog_get_type(prog);
+	return prog;
 }
 EXPORT_SYMBOL_GPL(bpf_prog_get_type);
 
@@ -863,6 +877,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 		/* failed to allocate fd */
 		goto free_used_maps;
 
+	trace_bpf_prog_load(prog, err);
 	return err;
 
 free_used_maps:

commit d407bd25a204bd66b7346dde24bd3d37ef0e0b05
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Jan 18 15:14:17 2017 +0100

    bpf: don't trigger OOM killer under pressure with map alloc
    
    This patch adds two helpers, bpf_map_area_alloc() and bpf_map_area_free(),
    that are to be used for map allocations. Using kmalloc() for very large
    allocations can cause excessive work within the page allocator, so i) fall
    back earlier to vmalloc() when the attempt is considered costly anyway,
    and even more importantly ii) don't trigger OOM killer with any of the
    allocators.
    
    Since this is based on a user space request, for example, when creating
    maps with element pre-allocation, we really want such requests to fail
    instead of killing other user space processes.
    
    Also, don't spam the kernel log with warnings should any of the allocations
    fail under pressure. Given that, we can make backend selection in
    bpf_map_area_alloc() generic, and convert all maps over to use this API
    for spots with potentially large allocation requests.
    
    Note, replacing the one kmalloc_array() is fine as overflow checks happen
    earlier in htab_map_alloc(), since it must also protect the multiplication
    for vmalloc() should kmalloc_array() fail.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 1d6b29e4e2c3..19b6129eab23 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -12,6 +12,8 @@
 #include <linux/bpf.h>
 #include <linux/syscalls.h>
 #include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/mmzone.h>
 #include <linux/anon_inodes.h>
 #include <linux/file.h>
 #include <linux/license.h>
@@ -49,6 +51,30 @@ void bpf_register_map_type(struct bpf_map_type_list *tl)
 	list_add(&tl->list_node, &bpf_map_types);
 }
 
+void *bpf_map_area_alloc(size_t size)
+{
+	/* We definitely need __GFP_NORETRY, so OOM killer doesn't
+	 * trigger under memory pressure as we really just want to
+	 * fail instead.
+	 */
+	const gfp_t flags = __GFP_NOWARN | __GFP_NORETRY | __GFP_ZERO;
+	void *area;
+
+	if (size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER)) {
+		area = kmalloc(size, GFP_USER | flags);
+		if (area != NULL)
+			return area;
+	}
+
+	return __vmalloc(size, GFP_KERNEL | __GFP_HIGHMEM | flags,
+			 PAGE_KERNEL);
+}
+
+void bpf_map_area_free(void *area)
+{
+	kvfree(area);
+}
+
 int bpf_map_precharge_memlock(u32 pages)
 {
 	struct user_struct *user = get_current_user();

commit f1f7714ea51c56b7163fb1a5acf39c6a204dd758
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jan 13 23:38:15 2017 +0100

    bpf: rework prog_digest into prog_tag
    
    Commit 7bd509e311f4 ("bpf: add prog_digest and expose it via
    fdinfo/netlink") was recently discussed, partially due to
    admittedly suboptimal name of "prog_digest" in combination
    with sha1 hash usage, thus inevitably and rightfully concerns
    about its security in terms of collision resistance were
    raised with regards to use-cases.
    
    The intended use cases are for debugging resp. introspection
    only for providing a stable "tag" over the instruction sequence
    that both kernel and user space can calculate independently.
    It's not usable at all for making a security relevant decision.
    So collisions where two different instruction sequences generate
    the same tag can happen, but ideally at a rather low rate. The
    "tag" will be dumped in hex and is short enough to introspect
    in tracepoints or kallsyms output along with other data such
    as stack trace, etc. Thus, this patch performs a rename into
    prog_tag and truncates the tag to a short output (64 bits) to
    make it obvious it's not collision-free.
    
    Should in future a hash or facility be needed with a security
    relevant focus, then we can think about requirements, constraints,
    etc that would fit to that situation. For now, rework the exposed
    parts for the current use cases as long as nothing has been
    released yet. Tested on x86_64 and s390x.
    
    Fixes: 7bd509e311f4 ("bpf: add prog_digest and expose it via fdinfo/netlink")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index e89acea22ecf..1d6b29e4e2c3 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -688,17 +688,17 @@ static int bpf_prog_release(struct inode *inode, struct file *filp)
 static void bpf_prog_show_fdinfo(struct seq_file *m, struct file *filp)
 {
 	const struct bpf_prog *prog = filp->private_data;
-	char prog_digest[sizeof(prog->digest) * 2 + 1] = { };
+	char prog_tag[sizeof(prog->tag) * 2 + 1] = { };
 
-	bin2hex(prog_digest, prog->digest, sizeof(prog->digest));
+	bin2hex(prog_tag, prog->tag, sizeof(prog->tag));
 	seq_printf(m,
 		   "prog_type:\t%u\n"
 		   "prog_jited:\t%u\n"
-		   "prog_digest:\t%s\n"
+		   "prog_tag:\t%s\n"
 		   "memlock:\t%llu\n",
 		   prog->type,
 		   prog->jited,
-		   prog_digest,
+		   prog_tag,
 		   prog->pages * 1ULL << PAGE_SHIFT);
 }
 #endif

commit 5ccb071e97fbd9ffe623a0d3977cc6d013bee93c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Dec 18 01:52:58 2016 +0100

    bpf: fix overflow in prog accounting
    
    Commit aaac3ba95e4c ("bpf: charge user for creation of BPF maps and
    programs") made a wrong assumption of charging against prog->pages.
    Unlike map->pages, prog->pages are still subject to change when we
    need to expand the program through bpf_prog_realloc().
    
    This can for example happen during verification stage when we need to
    expand and rewrite parts of the program. Should the required space
    cross a page boundary, then prog->pages is not the same anymore as
    its original value that we used to bpf_prog_charge_memlock() on. Thus,
    we'll hit a wrap-around during bpf_prog_uncharge_memlock() when prog
    is freed eventually. I noticed this that despite having unlimited
    memlock, programs suddenly refused to load with EPERM error due to
    insufficient memlock.
    
    There are two ways to fix this issue. One would be to add a cached
    variable to struct bpf_prog that takes a snapshot of prog->pages at the
    time of charging. The other approach is to also account for resizes. I
    chose to go with the latter for a couple of reasons: i) We want accounting
    rather to be more accurate instead of further fooling limits, ii) adding
    yet another page counter on struct bpf_prog would also be a waste just
    for this purpose. We also do want to charge as early as possible to
    avoid going into the verifier just to find out later on that we crossed
    limits. The only place that needs to be fixed is bpf_prog_realloc(),
    since only here we expand the program, so we try to account for the
    needed delta and should we fail, call-sites check for outcome anyway.
    On cBPF to eBPF migrations, we don't grab a reference to the user as
    they are charged differently. With that in place, my test case worked
    fine.
    
    Fixes: aaac3ba95e4c ("bpf: charge user for creation of BPF maps and programs")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 35d674c1f12e..e89acea22ecf 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -615,19 +615,39 @@ static void free_used_maps(struct bpf_prog_aux *aux)
 	kfree(aux->used_maps);
 }
 
+int __bpf_prog_charge(struct user_struct *user, u32 pages)
+{
+	unsigned long memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+	unsigned long user_bufs;
+
+	if (user) {
+		user_bufs = atomic_long_add_return(pages, &user->locked_vm);
+		if (user_bufs > memlock_limit) {
+			atomic_long_sub(pages, &user->locked_vm);
+			return -EPERM;
+		}
+	}
+
+	return 0;
+}
+
+void __bpf_prog_uncharge(struct user_struct *user, u32 pages)
+{
+	if (user)
+		atomic_long_sub(pages, &user->locked_vm);
+}
+
 static int bpf_prog_charge_memlock(struct bpf_prog *prog)
 {
 	struct user_struct *user = get_current_user();
-	unsigned long memlock_limit;
-
-	memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+	int ret;
 
-	atomic_long_add(prog->pages, &user->locked_vm);
-	if (atomic_long_read(&user->locked_vm) > memlock_limit) {
-		atomic_long_sub(prog->pages, &user->locked_vm);
+	ret = __bpf_prog_charge(user, prog->pages);
+	if (ret) {
 		free_uid(user);
-		return -EPERM;
+		return ret;
 	}
+
 	prog->aux->user = user;
 	return 0;
 }
@@ -636,7 +656,7 @@ static void bpf_prog_uncharge_memlock(struct bpf_prog *prog)
 {
 	struct user_struct *user = prog->aux->user;
 
-	atomic_long_sub(prog->pages, &user->locked_vm);
+	__bpf_prog_uncharge(user, prog->pages);
 	free_uid(user);
 }
 

commit aafe6ae9cee32df85eb5e8bb6dd1d918e6807b09
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Dec 18 01:52:57 2016 +0100

    bpf: dynamically allocate digest scratch buffer
    
    Geert rightfully complained that 7bd509e311f4 ("bpf: add prog_digest
    and expose it via fdinfo/netlink") added a too large allocation of
    variable 'raw' from bss section, and should instead be done dynamically:
    
      # ./scripts/bloat-o-meter kernel/bpf/core.o.1 kernel/bpf/core.o.2
      add/remove: 3/0 grow/shrink: 0/0 up/down: 33291/0 (33291)
      function                                     old     new   delta
      raw                                            -   32832  +32832
      [...]
    
    Since this is only relevant during program creation path, which can be
    considered slow-path anyway, lets allocate that dynamically and be not
    implicitly dependent on verifier mutex. Move bpf_prog_calc_digest() at
    the beginning of replace_map_fd_with_map_ptr() and also error handling
    stays straight forward.
    
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4819ec9d95f6..35d674c1f12e 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -811,7 +811,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 
 	err = -EFAULT;
 	if (copy_from_user(prog->insns, u64_to_user_ptr(attr->insns),
-			   prog->len * sizeof(struct bpf_insn)) != 0)
+			   bpf_prog_insn_size(prog)) != 0)
 		goto free_prog;
 
 	prog->orig_prog = NULL;

commit 17bedab2723145d17b14084430743549e6943d03
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Wed Dec 7 15:53:11 2016 -0800

    bpf: xdp: Allow head adjustment in XDP prog
    
    This patch allows XDP prog to extend/remove the packet
    data at the head (like adding or removing header).  It is
    done by adding a new XDP helper bpf_xdp_adjust_head().
    
    It also renames bpf_helper_changes_skb_data() to
    bpf_helper_changes_pkt_data() to better reflect
    that XDP prog does not work on skb.
    
    This patch adds one "xdp_adjust_head" bit to bpf_prog for the
    XDP-capable driver to check if the XDP prog requires
    bpf_xdp_adjust_head() support.  The driver can then decide
    to error out during XDP_SETUP_PROG.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: John Fastabend <john.r.fastabend@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 88f609f1c0c3..4819ec9d95f6 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -579,6 +579,8 @@ static void fixup_bpf_calls(struct bpf_prog *prog)
 				prog->dst_needed = 1;
 			if (insn->imm == BPF_FUNC_get_prandom_u32)
 				bpf_user_rnd_init_once();
+			if (insn->imm == BPF_FUNC_xdp_adjust_head)
+				prog->xdp_adjust_head = 1;
 			if (insn->imm == BPF_FUNC_tail_call) {
 				/* mark bpf_tail_call as different opcode
 				 * to avoid conditional branch in

commit ef0915cacd04c9e35be5f9d62a4e4b5b4b9bcfd1
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Dec 7 01:15:44 2016 +0100

    bpf: fix loading of BPF_MAXINSNS sized programs
    
    General assumption is that single program can hold up to BPF_MAXINSNS,
    that is, 4096 number of instructions. It is the case with cBPF and
    that limit was carried over to eBPF. When recently testing digest, I
    noticed that it's actually not possible to feed 4096 instructions
    via bpf(2).
    
    The check for > BPF_MAXINSNS was added back then to bpf_check() in
    cbd357008604 ("bpf: verifier (add ability to receive verification log)").
    However, 09756af46893 ("bpf: expand BPF syscall with program load/unload")
    added yet another check that comes before that into bpf_prog_load(),
    but this time bails out already in case of >= BPF_MAXINSNS.
    
    Fix it up and perform the check early in bpf_prog_load(), so we can drop
    the second one in bpf_check(). It makes sense, because also a 0 insn
    program is useless and we don't want to waste any resources doing work
    up to bpf_check() point. The existing bpf(2) man page documents E2BIG
    as the official error for such cases, so just stick with it as well.
    
    Fixes: 09756af46893 ("bpf: expand BPF syscall with program load/unload")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c0d2b423ce93..88f609f1c0c3 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -786,8 +786,8 @@ static int bpf_prog_load(union bpf_attr *attr)
 	/* eBPF programs must be GPL compatible to use GPL-ed functions */
 	is_gpl = license_is_gpl_compatible(license);
 
-	if (attr->insn_cnt >= BPF_MAXINSNS)
-		return -EINVAL;
+	if (attr->insn_cnt == 0 || attr->insn_cnt > BPF_MAXINSNS)
+		return -E2BIG;
 
 	if (type == BPF_PROG_TYPE_KPROBE &&
 	    attr->kern_version != LINUX_VERSION_CODE)

commit 7bd509e311f408f7a5132fcdde2069af65fa05ae
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Dec 4 23:19:41 2016 +0100

    bpf: add prog_digest and expose it via fdinfo/netlink
    
    When loading a BPF program via bpf(2), calculate the digest over
    the program's instruction stream and store it in struct bpf_prog's
    digest member. This is done at a point in time before any instructions
    are rewritten by the verifier. Any unstable map file descriptor
    number part of the imm field will be zeroed for the hash.
    
    fdinfo example output for progs:
    
      # cat /proc/1590/fdinfo/5
      pos:          0
      flags:        02000002
      mnt_id:       11
      prog_type:    1
      prog_jited:   1
      prog_digest:  b27e8b06da22707513aa97363dfb11c7c3675d28
      memlock:      4096
    
    When programs are pinned and retrieved by an ELF loader, the loader
    can check the program's digest through fdinfo and compare it against
    one that was generated over the ELF file's program section to see
    if the program needs to be reloaded. Furthermore, this can also be
    exposed through other means such as netlink in case of a tc cls/act
    dump (or xdp in future), but also through tracepoints or other
    facilities to identify the program. Other than that, the digest can
    also serve as a base name for the work in progress kallsyms support
    of programs. The digest doesn't depend/select the crypto layer, since
    we need to keep dependencies to a minimum. iproute2 will get support
    for this facility.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 85af86c496cd..c0d2b423ce93 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -662,8 +662,30 @@ static int bpf_prog_release(struct inode *inode, struct file *filp)
 	return 0;
 }
 
+#ifdef CONFIG_PROC_FS
+static void bpf_prog_show_fdinfo(struct seq_file *m, struct file *filp)
+{
+	const struct bpf_prog *prog = filp->private_data;
+	char prog_digest[sizeof(prog->digest) * 2 + 1] = { };
+
+	bin2hex(prog_digest, prog->digest, sizeof(prog->digest));
+	seq_printf(m,
+		   "prog_type:\t%u\n"
+		   "prog_jited:\t%u\n"
+		   "prog_digest:\t%s\n"
+		   "memlock:\t%llu\n",
+		   prog->type,
+		   prog->jited,
+		   prog_digest,
+		   prog->pages * 1ULL << PAGE_SHIFT);
+}
+#endif
+
 static const struct file_operations bpf_prog_fops = {
-        .release = bpf_prog_release,
+#ifdef CONFIG_PROC_FS
+	.show_fdinfo	= bpf_prog_show_fdinfo,
+#endif
+	.release	= bpf_prog_release,
 };
 
 int bpf_prog_new_fd(struct bpf_prog *prog)

commit 61023658760032e97869b07d54be9681d2529e77
Author: David Ahern <dsa@cumulusnetworks.com>
Date:   Thu Dec 1 08:48:04 2016 -0800

    bpf: Add new cgroup attach type to enable sock modifications
    
    Add new cgroup based program type, BPF_PROG_TYPE_CGROUP_SOCK. Similar to
    BPF_PROG_TYPE_CGROUP_SKB programs can be attached to a cgroup and run
    any time a process in the cgroup opens an AF_INET or AF_INET6 socket.
    Currently only sk_bound_dev_if is exported to userspace for modification
    by a bpf program.
    
    This allows a cgroup to be configured such that AF_INET{6} sockets opened
    by processes are automatically bound to a specific device. In turn, this
    enables the running of programs that do not support SO_BINDTODEVICE in a
    specific VRF context / L3 domain.
    
    Signed-off-by: David Ahern <dsa@cumulusnetworks.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5518a6839ab1..85af86c496cd 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -869,7 +869,9 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_CGROUP_INET_EGRESS:
 		ptype = BPF_PROG_TYPE_CGROUP_SKB;
 		break;
-
+	case BPF_CGROUP_INET_SOCK_CREATE:
+		ptype = BPF_PROG_TYPE_CGROUP_SOCK;
+		break;
 	default:
 		return -EINVAL;
 	}
@@ -905,6 +907,7 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	switch (attr->attach_type) {
 	case BPF_CGROUP_INET_INGRESS:
 	case BPF_CGROUP_INET_EGRESS:
+	case BPF_CGROUP_INET_SOCK_CREATE:
 		cgrp = cgroup_get_from_fd(attr->target_fd);
 		if (IS_ERR(cgrp))
 			return PTR_ERR(cgrp);

commit b2cd12574aa3e1625f471ff57cde7f628a18a46b
Author: David Ahern <dsa@cumulusnetworks.com>
Date:   Thu Dec 1 08:48:03 2016 -0800

    bpf: Refactor cgroups code in prep for new type
    
    Code move and rename only; no functional change intended.
    
    Signed-off-by: David Ahern <dsa@cumulusnetworks.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4caa18e6860a..5518a6839ab1 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -856,6 +856,7 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 {
 	struct bpf_prog *prog;
 	struct cgroup *cgrp;
+	enum bpf_prog_type ptype;
 
 	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
@@ -866,25 +867,26 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	switch (attr->attach_type) {
 	case BPF_CGROUP_INET_INGRESS:
 	case BPF_CGROUP_INET_EGRESS:
-		prog = bpf_prog_get_type(attr->attach_bpf_fd,
-					 BPF_PROG_TYPE_CGROUP_SKB);
-		if (IS_ERR(prog))
-			return PTR_ERR(prog);
-
-		cgrp = cgroup_get_from_fd(attr->target_fd);
-		if (IS_ERR(cgrp)) {
-			bpf_prog_put(prog);
-			return PTR_ERR(cgrp);
-		}
-
-		cgroup_bpf_update(cgrp, prog, attr->attach_type);
-		cgroup_put(cgrp);
+		ptype = BPF_PROG_TYPE_CGROUP_SKB;
 		break;
 
 	default:
 		return -EINVAL;
 	}
 
+	prog = bpf_prog_get_type(attr->attach_bpf_fd, ptype);
+	if (IS_ERR(prog))
+		return PTR_ERR(prog);
+
+	cgrp = cgroup_get_from_fd(attr->target_fd);
+	if (IS_ERR(cgrp)) {
+		bpf_prog_put(prog);
+		return PTR_ERR(cgrp);
+	}
+
+	cgroup_bpf_update(cgrp, prog, attr->attach_type);
+	cgroup_put(cgrp);
+
 	return 0;
 }
 

commit 21116b7068b9b66ac16b2fe3675469f459968c3f
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Nov 26 01:28:07 2016 +0100

    bpf: add owner_prog_type and accounted mem to array map's fdinfo
    
    Allow for checking the owner_prog_type of a program array map. In some
    cases bpf(2) can return -EINVAL /after/ the verifier passed and did all
    the rewrites of the bpf program.
    
    The reason that lets us fail at this late stage is that program array
    maps are incompatible. Allow users to inspect this earlier after they
    got the map fd through BPF_OBJ_GET command. tc will get support for this.
    
    Also, display how much we charged the map with regards to RLIMIT_MEMLOCK.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 1090d16a31c1..4caa18e6860a 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -138,18 +138,31 @@ static int bpf_map_release(struct inode *inode, struct file *filp)
 static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 {
 	const struct bpf_map *map = filp->private_data;
+	const struct bpf_array *array;
+	u32 owner_prog_type = 0;
+
+	if (map->map_type == BPF_MAP_TYPE_PROG_ARRAY) {
+		array = container_of(map, struct bpf_array, map);
+		owner_prog_type = array->owner_prog_type;
+	}
 
 	seq_printf(m,
 		   "map_type:\t%u\n"
 		   "key_size:\t%u\n"
 		   "value_size:\t%u\n"
 		   "max_entries:\t%u\n"
-		   "map_flags:\t%#x\n",
+		   "map_flags:\t%#x\n"
+		   "memlock:\t%llu\n",
 		   map->map_type,
 		   map->key_size,
 		   map->value_size,
 		   map->max_entries,
-		   map->map_flags);
+		   map->map_flags,
+		   map->pages * 1ULL << PAGE_SHIFT);
+
+	if (owner_prog_type)
+		seq_printf(m, "owner_prog_type:\t%u\n",
+			   owner_prog_type);
 }
 #endif
 

commit f4324551489e8781d838f941b7aee4208e52e8bf
Author: Daniel Mack <daniel@zonque.org>
Date:   Wed Nov 23 16:52:27 2016 +0100

    bpf: add BPF_PROG_ATTACH and BPF_PROG_DETACH commands
    
    Extend the bpf(2) syscall by two new commands, BPF_PROG_ATTACH and
    BPF_PROG_DETACH which allow attaching and detaching eBPF programs
    to a target.
    
    On the API level, the target could be anything that has an fd in
    userspace, hence the name of the field in union bpf_attr is called
    'target_fd'.
    
    When called with BPF_ATTACH_TYPE_CGROUP_INET_{E,IN}GRESS, the target is
    expected to be a valid file descriptor of a cgroup v2 directory which
    has the bpf controller enabled. These are the only use-cases
    implemented by this patch at this point, but more can be added.
    
    If a program of the given type already exists in the given cgroup,
    the program is swapped automically, so userspace does not have to drop
    an existing program first before installing a new one, which would
    otherwise leave a gap in which no program is attached.
    
    For more information on the propagation logic to subcgroups, please
    refer to the bpf cgroup controller implementation.
    
    The API is guarded by CAP_NET_ADMIN.
    
    Signed-off-by: Daniel Mack <daniel@zonque.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index eb15498b8d55..1090d16a31c1 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -835,6 +835,77 @@ static int bpf_obj_get(const union bpf_attr *attr)
 	return bpf_obj_get_user(u64_to_user_ptr(attr->pathname));
 }
 
+#ifdef CONFIG_CGROUP_BPF
+
+#define BPF_PROG_ATTACH_LAST_FIELD attach_type
+
+static int bpf_prog_attach(const union bpf_attr *attr)
+{
+	struct bpf_prog *prog;
+	struct cgroup *cgrp;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (CHECK_ATTR(BPF_PROG_ATTACH))
+		return -EINVAL;
+
+	switch (attr->attach_type) {
+	case BPF_CGROUP_INET_INGRESS:
+	case BPF_CGROUP_INET_EGRESS:
+		prog = bpf_prog_get_type(attr->attach_bpf_fd,
+					 BPF_PROG_TYPE_CGROUP_SKB);
+		if (IS_ERR(prog))
+			return PTR_ERR(prog);
+
+		cgrp = cgroup_get_from_fd(attr->target_fd);
+		if (IS_ERR(cgrp)) {
+			bpf_prog_put(prog);
+			return PTR_ERR(cgrp);
+		}
+
+		cgroup_bpf_update(cgrp, prog, attr->attach_type);
+		cgroup_put(cgrp);
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+#define BPF_PROG_DETACH_LAST_FIELD attach_type
+
+static int bpf_prog_detach(const union bpf_attr *attr)
+{
+	struct cgroup *cgrp;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (CHECK_ATTR(BPF_PROG_DETACH))
+		return -EINVAL;
+
+	switch (attr->attach_type) {
+	case BPF_CGROUP_INET_INGRESS:
+	case BPF_CGROUP_INET_EGRESS:
+		cgrp = cgroup_get_from_fd(attr->target_fd);
+		if (IS_ERR(cgrp))
+			return PTR_ERR(cgrp);
+
+		cgroup_bpf_update(cgrp, NULL, attr->attach_type);
+		cgroup_put(cgrp);
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+#endif /* CONFIG_CGROUP_BPF */
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -901,6 +972,16 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_OBJ_GET:
 		err = bpf_obj_get(&attr);
 		break;
+
+#ifdef CONFIG_CGROUP_BPF
+	case BPF_PROG_ATTACH:
+		err = bpf_prog_attach(&attr);
+		break;
+	case BPF_PROG_DETACH:
+		err = bpf_prog_detach(&attr);
+		break;
+#endif
+
 	default:
 		err = -EINVAL;
 		break;

commit 97bc402db7821259f6a722cb38e060aa9b35b6e8
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sat Nov 19 01:45:00 2016 +0100

    bpf, mlx5: fix mlx5e_create_rq taking reference on prog
    
    In mlx5e_create_rq(), when creating a new queue, we call bpf_prog_add() but
    without checking the return value. bpf_prog_add() can fail since 92117d8443bc
    ("bpf: fix refcnt overflow"), so we really must check it. Take the reference
    right when we assign it to the rq from priv->xdp_prog, and just drop the
    reference on error path. Destruction in mlx5e_destroy_rq() looks good, though.
    
    Fixes: 86994156c736 ("net/mlx5e: XDP fast RX drop bpf programs support")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ce1b7de7d72c..eb15498b8d55 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -696,6 +696,7 @@ struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
 {
 	return bpf_prog_add(prog, 1);
 }
+EXPORT_SYMBOL_GPL(bpf_prog_inc);
 
 static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *type)
 {

commit 8f8449384ec364ba2a654f11f94e754e4ff719e0
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Nov 11 10:55:10 2016 -0800

    bpf: Add BPF_MAP_TYPE_LRU_PERCPU_HASH
    
    Provide a LRU version of the existing BPF_MAP_TYPE_PERCPU_HASH
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 233e3ac836a6..ce1b7de7d72c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -292,6 +292,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 		goto free_key;
 
 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
 	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
 		value_size = round_up(map->value_size, 8) * num_possible_cpus();
 	else
@@ -302,7 +303,8 @@ static int map_lookup_elem(union bpf_attr *attr)
 	if (!value)
 		goto free_key;
 
-	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH) {
+	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
 		err = bpf_percpu_hash_copy(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
 		err = bpf_percpu_array_copy(map, key, value);
@@ -366,6 +368,7 @@ static int map_update_elem(union bpf_attr *attr)
 		goto free_key;
 
 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||
 	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
 		value_size = round_up(map->value_size, 8) * num_possible_cpus();
 	else
@@ -385,7 +388,8 @@ static int map_update_elem(union bpf_attr *attr)
 	 */
 	preempt_disable();
 	__this_cpu_inc(bpf_prog_active);
-	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH) {
+	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {
 		err = bpf_percpu_hash_update(map, key, value, attr->flags);
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
 		err = bpf_percpu_array_update(map, key, value, attr->flags);

commit bb598c1b8c9bf56981927dcb8c0dc34b8ff95342
Merge: eb2ca35f1814 e76d21c40bd6
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Nov 15 10:54:36 2016 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Several cases of bug fixes in 'net' overlapping other changes in
    'net-next-.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 535e7b4b5ef220be374b895684f274872aebd0f8
Author: Mickaël Salaün <mic@digikod.net>
Date:   Sun Nov 13 19:44:03 2016 +0100

    bpf: Use u64_to_user_ptr()
    
    Replace the custom u64_to_ptr() function with the u64_to_user_ptr()
    macro.
    
    Signed-off-by: Mickaël Salaün <mic@digikod.net>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 23eb2050f15e..cdc06546401b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -17,6 +17,7 @@
 #include <linux/license.h>
 #include <linux/filter.h>
 #include <linux/version.h>
+#include <linux/kernel.h>
 
 DEFINE_PER_CPU(int, bpf_prog_active);
 
@@ -252,12 +253,6 @@ struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 	return map;
 }
 
-/* helper to convert user pointers passed inside __aligned_u64 fields */
-static void __user *u64_to_ptr(__u64 val)
-{
-	return (void __user *) (unsigned long) val;
-}
-
 int __weak bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)
 {
 	return -ENOTSUPP;
@@ -268,8 +263,8 @@ int __weak bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)
 
 static int map_lookup_elem(union bpf_attr *attr)
 {
-	void __user *ukey = u64_to_ptr(attr->key);
-	void __user *uvalue = u64_to_ptr(attr->value);
+	void __user *ukey = u64_to_user_ptr(attr->key);
+	void __user *uvalue = u64_to_user_ptr(attr->value);
 	int ufd = attr->map_fd;
 	struct bpf_map *map;
 	void *key, *value, *ptr;
@@ -342,8 +337,8 @@ static int map_lookup_elem(union bpf_attr *attr)
 
 static int map_update_elem(union bpf_attr *attr)
 {
-	void __user *ukey = u64_to_ptr(attr->key);
-	void __user *uvalue = u64_to_ptr(attr->value);
+	void __user *ukey = u64_to_user_ptr(attr->key);
+	void __user *uvalue = u64_to_user_ptr(attr->value);
 	int ufd = attr->map_fd;
 	struct bpf_map *map;
 	void *key, *value;
@@ -420,7 +415,7 @@ static int map_update_elem(union bpf_attr *attr)
 
 static int map_delete_elem(union bpf_attr *attr)
 {
-	void __user *ukey = u64_to_ptr(attr->key);
+	void __user *ukey = u64_to_user_ptr(attr->key);
 	int ufd = attr->map_fd;
 	struct bpf_map *map;
 	struct fd f;
@@ -464,8 +459,8 @@ static int map_delete_elem(union bpf_attr *attr)
 
 static int map_get_next_key(union bpf_attr *attr)
 {
-	void __user *ukey = u64_to_ptr(attr->key);
-	void __user *unext_key = u64_to_ptr(attr->next_key);
+	void __user *ukey = u64_to_user_ptr(attr->key);
+	void __user *unext_key = u64_to_user_ptr(attr->next_key);
 	int ufd = attr->map_fd;
 	struct bpf_map *map;
 	void *key, *next_key;
@@ -741,7 +736,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 		return -EINVAL;
 
 	/* copy eBPF program license from user space */
-	if (strncpy_from_user(license, u64_to_ptr(attr->license),
+	if (strncpy_from_user(license, u64_to_user_ptr(attr->license),
 			      sizeof(license) - 1) < 0)
 		return -EFAULT;
 	license[sizeof(license) - 1] = 0;
@@ -771,7 +766,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 	prog->len = attr->insn_cnt;
 
 	err = -EFAULT;
-	if (copy_from_user(prog->insns, u64_to_ptr(attr->insns),
+	if (copy_from_user(prog->insns, u64_to_user_ptr(attr->insns),
 			   prog->len * sizeof(struct bpf_insn)) != 0)
 		goto free_prog;
 
@@ -822,7 +817,7 @@ static int bpf_obj_pin(const union bpf_attr *attr)
 	if (CHECK_ATTR(BPF_OBJ))
 		return -EINVAL;
 
-	return bpf_obj_pin_user(attr->bpf_fd, u64_to_ptr(attr->pathname));
+	return bpf_obj_pin_user(attr->bpf_fd, u64_to_user_ptr(attr->pathname));
 }
 
 static int bpf_obj_get(const union bpf_attr *attr)
@@ -830,7 +825,7 @@ static int bpf_obj_get(const union bpf_attr *attr)
 	if (CHECK_ATTR(BPF_OBJ) || attr->bpf_fd != 0)
 		return -EINVAL;
 
-	return bpf_obj_get_user(u64_to_ptr(attr->pathname));
+	return bpf_obj_get_user(u64_to_user_ptr(attr->pathname));
 }
 
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)

commit c540594f864bb4645573c2c0a304919fabb3d7ea
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Nov 9 22:02:34 2016 +0100

    bpf, mlx4: fix prog refcount in mlx4_en_try_alloc_resources error path
    
    Commit 67f8b1dcb9ee ("net/mlx4_en: Refactor the XDP forwarding rings
    scheme") added a bug in that the prog's reference count is not dropped
    in the error path when mlx4_en_try_alloc_resources() is failing from
    mlx4_xdp_set().
    
    We previously took bpf_prog_add(prog, priv->rx_ring_num - 1), that we
    need to release again. Earlier in the call path, dev_change_xdp_fd()
    itself holds a reference to the prog as well (hence the '- 1' in the
    bpf_prog_add()), so a simple atomic_sub() is safe to use here. When
    an error is propagated, then bpf_prog_put() is called eventually from
    dev_change_xdp_fd()
    
    Fixes: 67f8b1dcb9ee ("net/mlx4_en: Refactor the XDP forwarding rings scheme")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 228f962447a5..23eb2050f15e 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -680,6 +680,17 @@ struct bpf_prog *bpf_prog_add(struct bpf_prog *prog, int i)
 }
 EXPORT_SYMBOL_GPL(bpf_prog_add);
 
+void bpf_prog_sub(struct bpf_prog *prog, int i)
+{
+	/* Only to be used for undoing previous bpf_prog_add() in some
+	 * error path. We still know that another entity in our call
+	 * path holds a reference to the program, thus atomic_sub() can
+	 * be safely used in such cases!
+	 */
+	WARN_ON(atomic_sub_return(i, &prog->aux->refcnt) == 0);
+}
+EXPORT_SYMBOL_GPL(bpf_prog_sub);
+
 struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
 {
 	return bpf_prog_add(prog, 1);

commit 20b2b24f91f70e7d3f0918c077546cb21bd73a87
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Nov 4 00:56:31 2016 +0100

    bpf: fix map not being uncharged during map creation failure
    
    In map_create(), we first find and create the map, then once that
    suceeded, we charge it to the user's RLIMIT_MEMLOCK, and then fetch
    a new anon fd through anon_inode_getfd(). The problem is, once the
    latter fails f.e. due to RLIMIT_NOFILE limit, then we only destruct
    the map via map->ops->map_free(), but without uncharging the previously
    locked memory first. That means that the user_struct allocation is
    leaked as well as the accounted RLIMIT_MEMLOCK memory not released.
    Make the label names in the fix consistent with bpf_prog_load().
    
    Fixes: aaac3ba95e4c ("bpf: charge user for creation of BPF maps and programs")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 228f962447a5..237f3d6a7ddc 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -194,7 +194,7 @@ static int map_create(union bpf_attr *attr)
 
 	err = bpf_map_charge_memlock(map);
 	if (err)
-		goto free_map;
+		goto free_map_nouncharge;
 
 	err = bpf_map_new_fd(map);
 	if (err < 0)
@@ -204,6 +204,8 @@ static int map_create(union bpf_attr *attr)
 	return err;
 
 free_map:
+	bpf_map_uncharge_memlock(map);
+free_map_nouncharge:
 	map->ops->map_free(map);
 	return err;
 }

commit 59d3656d5bf504f771fc44fdbc7a9a8590795f22
Author: Brenden Blanco <bblanco@plumgrid.com>
Date:   Tue Jul 19 12:16:46 2016 -0700

    bpf: add bpf_prog_add api for bulk prog refcnt
    
    A subsystem may need to store many copies of a bpf program, each
    deserving its own reference. Rather than requiring the caller to loop
    one by one (with possible mid-loop failure), add a bulk bpf_prog_add
    api.
    
    Signed-off-by: Brenden Blanco <bblanco@plumgrid.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 96d938a22050..228f962447a5 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -670,14 +670,20 @@ static struct bpf_prog *____bpf_prog_get(struct fd f)
 	return f.file->private_data;
 }
 
-struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
+struct bpf_prog *bpf_prog_add(struct bpf_prog *prog, int i)
 {
-	if (atomic_inc_return(&prog->aux->refcnt) > BPF_MAX_REFCNT) {
-		atomic_dec(&prog->aux->refcnt);
+	if (atomic_add_return(i, &prog->aux->refcnt) > BPF_MAX_REFCNT) {
+		atomic_sub(i, &prog->aux->refcnt);
 		return ERR_PTR(-EBUSY);
 	}
 	return prog;
 }
+EXPORT_SYMBOL_GPL(bpf_prog_add);
+
+struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
+{
+	return bpf_prog_add(prog, 1);
+}
 
 static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *type)
 {

commit 4ed8ec521ed57c4e207ad464ca0388776de74d4b
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Thu Jun 30 10:28:43 2016 -0700

    cgroup: bpf: Add BPF_MAP_TYPE_CGROUP_ARRAY
    
    Add a BPF_MAP_TYPE_CGROUP_ARRAY and its bpf_map_ops's implementations.
    To update an element, the caller is expected to obtain a cgroup2 backed
    fd by open(cgroup2_dir) and then update the array with that fd.
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Cc: Alexei Starovoitov <ast@fb.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Tejun Heo <tj@kernel.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 22863d9872b1..96d938a22050 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -393,7 +393,8 @@ static int map_update_elem(union bpf_attr *attr)
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
 		err = bpf_percpu_array_update(map, key, value, attr->flags);
 	} else if (map->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY ||
-		   map->map_type == BPF_MAP_TYPE_PROG_ARRAY) {
+		   map->map_type == BPF_MAP_TYPE_PROG_ARRAY ||
+		   map->map_type == BPF_MAP_TYPE_CGROUP_ARRAY) {
 		rcu_read_lock();
 		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
 						   attr->flags);

commit 113214be7f6c98dd6d0435e4765aea8dea91662c
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jun 30 17:24:44 2016 +0200

    bpf: refactor bpf_prog_get and type check into helper
    
    Since bpf_prog_get() and program type check is used in a couple of places,
    refactor this into a small helper function that we can make use of. Since
    the non RO prog->aux part is not used in performance critical paths and a
    program destruction via RCU is rather very unlikley when doing the put, we
    shouldn't have an issue just doing the bpf_prog_get() + prog->type != type
    check, but actually not taking the ref at all (due to being in fdget() /
    fdput() section of the bpf fd) is even cleaner and makes the diff smaller
    as well, so just go for that. Callsites are changed to make use of the new
    helper where possible.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index f6806a1d7ed9..22863d9872b1 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -657,7 +657,7 @@ int bpf_prog_new_fd(struct bpf_prog *prog)
 				O_RDWR | O_CLOEXEC);
 }
 
-static struct bpf_prog *__bpf_prog_get(struct fd f)
+static struct bpf_prog *____bpf_prog_get(struct fd f)
 {
 	if (!f.file)
 		return ERR_PTR(-EBADF);
@@ -678,24 +678,35 @@ struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
 	return prog;
 }
 
-/* called by sockets/tracing/seccomp before attaching program to an event
- * pairs with bpf_prog_put()
- */
-struct bpf_prog *bpf_prog_get(u32 ufd)
+static struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *type)
 {
 	struct fd f = fdget(ufd);
 	struct bpf_prog *prog;
 
-	prog = __bpf_prog_get(f);
+	prog = ____bpf_prog_get(f);
 	if (IS_ERR(prog))
 		return prog;
+	if (type && prog->type != *type) {
+		prog = ERR_PTR(-EINVAL);
+		goto out;
+	}
 
 	prog = bpf_prog_inc(prog);
+out:
 	fdput(f);
-
 	return prog;
 }
-EXPORT_SYMBOL_GPL(bpf_prog_get);
+
+struct bpf_prog *bpf_prog_get(u32 ufd)
+{
+	return __bpf_prog_get(ufd, NULL);
+}
+
+struct bpf_prog *bpf_prog_get_type(u32 ufd, enum bpf_prog_type type)
+{
+	return __bpf_prog_get(ufd, &type);
+}
+EXPORT_SYMBOL_GPL(bpf_prog_get_type);
 
 /* last field in 'union bpf_attr' used by this command */
 #define	BPF_PROG_LOAD_LAST_FIELD kern_version

commit 1aacde3d22c42281236155c1ef6d7a5aa32a826b
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Jun 30 17:24:43 2016 +0200

    bpf: generally move prog destruction to RCU deferral
    
    Jann Horn reported following analysis that could potentially result
    in a very hard to trigger (if not impossible) UAF race, to quote his
    event timeline:
    
     - Set up a process with threads T1, T2 and T3
     - Let T1 set up a socket filter F1 that invokes another filter F2
       through a BPF map [tail call]
     - Let T1 trigger the socket filter via a unix domain socket write,
       don't wait for completion
     - Let T2 call PERF_EVENT_IOC_SET_BPF with F2, don't wait for completion
     - Now T2 should be behind bpf_prog_get(), but before bpf_prog_put()
     - Let T3 close the file descriptor for F2, dropping the reference
       count of F2 to 2
     - At this point, T1 should have looked up F2 from the map, but not
       finished executing it
     - Let T3 remove F2 from the BPF map, dropping the reference count of
       F2 to 1
     - Now T2 should call bpf_prog_put() (wrong BPF program type), dropping
       the reference count of F2 to 0 and scheduling bpf_prog_free_deferred()
       via schedule_work()
     - At this point, the BPF program could be freed
     - BPF execution is still running in a freed BPF program
    
    While at PERF_EVENT_IOC_SET_BPF time it's only guaranteed that the perf
    event fd we're doing the syscall on doesn't disappear from underneath us
    for whole syscall time, it may not be the case for the bpf fd used as
    an argument only after we did the put. It needs to be a valid fd pointing
    to a BPF program at the time of the call to make the bpf_prog_get() and
    while T2 gets preempted, F2 must have dropped reference to 1 on the other
    CPU. The fput() from the close() in T3 should also add additionally delay
    to the reference drop via exit_task_work() when bpf_prog_release() gets
    called as well as scheduling bpf_prog_free_deferred().
    
    That said, it makes nevertheless sense to move the BPF prog destruction
    generally after RCU grace period to guarantee that such scenario above,
    but also others as recently fixed in ceb56070359b ("bpf, perf: delay release
    of BPF prog after grace period") with regards to tail calls won't happen.
    Integrating bpf_prog_free_deferred() directly into the RCU callback is
    not allowed since the invocation might happen from either softirq or
    process context, so we're not permitted to block. Reviewing all bpf_prog_put()
    invocations from eBPF side (note, cBPF -> eBPF progs don't use this for
    their destruction) with call_rcu() look good to me.
    
    Since we don't know whether at the time of attaching the program, we're
    already part of a tail call map, we need to use RCU variant. However, due
    to this, there won't be severely more stress on the RCU callback queue:
    situations with above bpf_prog_get() and bpf_prog_put() combo in practice
    normally won't lead to releases, but even if they would, enough effort/
    cycles have to be put into loading a BPF program into the kernel already.
    
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c23a4e9311b3..f6806a1d7ed9 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -623,7 +623,7 @@ static void bpf_prog_uncharge_memlock(struct bpf_prog *prog)
 	free_uid(user);
 }
 
-static void __prog_put_common(struct rcu_head *rcu)
+static void __bpf_prog_put_rcu(struct rcu_head *rcu)
 {
 	struct bpf_prog_aux *aux = container_of(rcu, struct bpf_prog_aux, rcu);
 
@@ -632,17 +632,10 @@ static void __prog_put_common(struct rcu_head *rcu)
 	bpf_prog_free(aux->prog);
 }
 
-/* version of bpf_prog_put() that is called after a grace period */
-void bpf_prog_put_rcu(struct bpf_prog *prog)
-{
-	if (atomic_dec_and_test(&prog->aux->refcnt))
-		call_rcu(&prog->aux->rcu, __prog_put_common);
-}
-
 void bpf_prog_put(struct bpf_prog *prog)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt))
-		__prog_put_common(&prog->aux->rcu);
+		call_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);
 }
 EXPORT_SYMBOL_GPL(bpf_prog_put);
 
@@ -650,7 +643,7 @@ static int bpf_prog_release(struct inode *inode, struct file *filp)
 {
 	struct bpf_prog *prog = filp->private_data;
 
-	bpf_prog_put_rcu(prog);
+	bpf_prog_put(prog);
 	return 0;
 }
 

commit d056a788765e67773124f520159185bc89f5d1ad
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Jun 15 22:47:13 2016 +0200

    bpf, maps: extend map_fd_get_ptr arguments
    
    This patch extends map_fd_get_ptr() callback that is used by fd array
    maps, so that struct file pointer from the related map can be passed
    in. It's safe to remove map_update_elem() callback for the two maps since
    this is only allowed from syscall side, but not from eBPF programs for these
    two map types. Like in per-cpu map case, bpf_fd_array_map_update_elem()
    needs to be called directly here due to the extra argument.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index fc3adcd064b1..c23a4e9311b3 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -392,6 +392,12 @@ static int map_update_elem(union bpf_attr *attr)
 		err = bpf_percpu_hash_update(map, key, value, attr->flags);
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
 		err = bpf_percpu_array_update(map, key, value, attr->flags);
+	} else if (map->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY ||
+		   map->map_type == BPF_MAP_TYPE_PROG_ARRAY) {
+		rcu_read_lock();
+		err = bpf_fd_array_map_update_elem(map, f.file, key, value,
+						   attr->flags);
+		rcu_read_unlock();
 	} else {
 		rcu_read_lock();
 		err = map->ops->map_update_elem(map, key, value, attr->flags);

commit 61d1b6a42fec61c5065f54cc62cef02b483c69fb
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Jun 15 22:47:12 2016 +0200

    bpf, maps: add release callback
    
    Add a release callback for maps that is invoked when the last
    reference to its struct file is gone and the struct file about
    to be released by vfs. The handler will be used by fd array maps.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 46ecce4b79ed..fc3adcd064b1 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -124,7 +124,12 @@ void bpf_map_put_with_uref(struct bpf_map *map)
 
 static int bpf_map_release(struct inode *inode, struct file *filp)
 {
-	bpf_map_put_with_uref(filp->private_data);
+	struct bpf_map *map = filp->private_data;
+
+	if (map->ops->map_release)
+		map->ops->map_release(map, filp);
+
+	bpf_map_put_with_uref(map);
 	return 0;
 }
 

commit d1c55ab5e41fcd72cb0a8bef86d3f652ad9ad9f5
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri May 13 19:08:31 2016 +0200

    bpf: prepare bpf_int_jit_compile/bpf_prog_select_runtime apis
    
    Since the blinding is strictly only called from inside eBPF JITs,
    we need to change signatures for bpf_int_jit_compile() and
    bpf_prog_select_runtime() first in order to prepare that the
    eBPF program we're dealing with can change underneath. Hence,
    for call sites, we need to return the latest prog. No functional
    change in this patch.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index cf5e9f7ad13a..46ecce4b79ed 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -762,7 +762,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 	fixup_bpf_calls(prog);
 
 	/* eBPF program is ready to be JITed */
-	err = bpf_prog_select_runtime(prog);
+	prog = bpf_prog_select_runtime(prog, &err);
 	if (err < 0)
 		goto free_used_maps;
 

commit 92117d8443bc5afacc8d5ba82e541946310f106e
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Apr 27 18:56:20 2016 -0700

    bpf: fix refcnt overflow
    
    On a system with >32Gbyte of phyiscal memory and infinite RLIMIT_MEMLOCK,
    the malicious application may overflow 32-bit bpf program refcnt.
    It's also possible to overflow map refcnt on 1Tb system.
    Impose 32k hard limit which means that the same bpf program or
    map cannot be shared by more than 32k processes.
    
    Fixes: 1be7f75d1668 ("bpf: enable non-root eBPF programs")
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index adc5e4bd74f8..cf5e9f7ad13a 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -218,11 +218,18 @@ struct bpf_map *__bpf_map_get(struct fd f)
 	return f.file->private_data;
 }
 
-void bpf_map_inc(struct bpf_map *map, bool uref)
+/* prog's and map's refcnt limit */
+#define BPF_MAX_REFCNT 32768
+
+struct bpf_map *bpf_map_inc(struct bpf_map *map, bool uref)
 {
-	atomic_inc(&map->refcnt);
+	if (atomic_inc_return(&map->refcnt) > BPF_MAX_REFCNT) {
+		atomic_dec(&map->refcnt);
+		return ERR_PTR(-EBUSY);
+	}
 	if (uref)
 		atomic_inc(&map->usercnt);
+	return map;
 }
 
 struct bpf_map *bpf_map_get_with_uref(u32 ufd)
@@ -234,7 +241,7 @@ struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 	if (IS_ERR(map))
 		return map;
 
-	bpf_map_inc(map, true);
+	map = bpf_map_inc(map, true);
 	fdput(f);
 
 	return map;
@@ -658,6 +665,15 @@ static struct bpf_prog *__bpf_prog_get(struct fd f)
 	return f.file->private_data;
 }
 
+struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
+{
+	if (atomic_inc_return(&prog->aux->refcnt) > BPF_MAX_REFCNT) {
+		atomic_dec(&prog->aux->refcnt);
+		return ERR_PTR(-EBUSY);
+	}
+	return prog;
+}
+
 /* called by sockets/tracing/seccomp before attaching program to an event
  * pairs with bpf_prog_put()
  */
@@ -670,7 +686,7 @@ struct bpf_prog *bpf_prog_get(u32 ufd)
 	if (IS_ERR(prog))
 		return prog;
 
-	atomic_inc(&prog->aux->refcnt);
+	prog = bpf_prog_inc(prog);
 	fdput(f);
 
 	return prog;

commit 322cea2f41adb62c975f46a3242f4e3b43226fa1
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Mar 25 00:30:25 2016 +0100

    bpf: add missing map_flags to bpf_map_show_fdinfo
    
    Add map_flags attribute to bpf_map_show_fdinfo(), so that tools like
    tc can check for them when loading objects from a pinned entry, e.g.
    if user intent wrt allocation (BPF_F_NO_PREALLOC) is different to the
    pinned object, it can bail out. Follow-up to 6c9059817432 ("bpf:
    pre-allocate hash map elements"), so that tc can still support this
    with v4.6.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2a2efe1bc76c..adc5e4bd74f8 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -137,11 +137,13 @@ static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
 		   "map_type:\t%u\n"
 		   "key_size:\t%u\n"
 		   "value_size:\t%u\n"
-		   "max_entries:\t%u\n",
+		   "max_entries:\t%u\n"
+		   "map_flags:\t%#x\n",
 		   map->map_type,
 		   map->key_size,
 		   map->value_size,
-		   map->max_entries);
+		   map->max_entries,
+		   map->map_flags);
 }
 #endif
 

commit b8cdc05173f05d212627b7aba7ec47fa334a79f2
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Mar 9 18:56:49 2016 -0800

    bpf: bpf_stackmap_copy depends on CONFIG_PERF_EVENTS
    
    0-day bot reported build error:
    kernel/built-in.o: In function `map_lookup_elem':
    >> kernel/bpf/.tmp_syscall.o:(.text+0x329b3c): undefined reference to `bpf_stackmap_copy'
    when CONFIG_BPF_SYSCALL is set and CONFIG_PERF_EVENTS is not.
    Add weak definition to resolve it.
    This code path in map_lookup_elem() is never taken
    when CONFIG_PERF_EVENTS is not set.
    
    Fixes: 557c0c6e7df8 ("bpf: convert stackmap to pre-allocation")
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2978d0d08869..2a2efe1bc76c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -244,6 +244,11 @@ static void __user *u64_to_ptr(__u64 val)
 	return (void __user *) (unsigned long) val;
 }
 
+int __weak bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)
+{
+	return -ENOTSUPP;
+}
+
 /* last field in 'union bpf_attr' used by this command */
 #define BPF_MAP_LOOKUP_ELEM_LAST_FIELD value
 

commit 557c0c6e7df8e14a46bd7560d193fa5bbc00a858
Author: Alexei Starovoitov <ast@fb.com>
Date:   Mon Mar 7 21:57:17 2016 -0800

    bpf: convert stackmap to pre-allocation
    
    It was observed that calling bpf_get_stackid() from a kprobe inside
    slub or from spin_unlock causes similar deadlock as with hashmap,
    therefore convert stackmap to use pre-allocated memory.
    
    The call_rcu is no longer feasible mechanism, since delayed freeing
    causes bpf_get_stackid() to fail unpredictably when number of actual
    stacks is significantly less than user requested max_entries.
    Since elements are no longer freed into slub, we can push elements into
    freelist immediately and let them be recycled.
    However the very unlikley race between user space map_lookup() and
    program-side recycling is possible:
         cpu0                          cpu1
         ----                          ----
    user does lookup(stackidX)
    starts copying ips into buffer
                                       delete(stackidX)
                                       calls bpf_get_stackid()
                                       which recyles the element and
                                       overwrites with new stack trace
    
    To avoid user space seeing a partial stack trace consisting of two
    merged stack traces, do bucket = xchg(, NULL); copy; xchg(,bucket);
    to preserve consistent stack trace delivery to user space.
    Now we can move memset(,0) of left-over element value from critical
    path of bpf_get_stackid() into slow-path of user space lookup.
    Also disallow lookup() from bpf program, since it's useless and
    program shouldn't be messing with collected stack trace.
    
    Note that similar race between user space lookup and kernel side updates
    is also present in hashmap, but it's not a new race. bpf programs were
    always allowed to modify hash and array map elements while user space
    is copying them.
    
    Fixes: d5a3b1f69186 ("bpf: introduce BPF_MAP_TYPE_STACK_TRACE")
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index cbd94b2144ff..2978d0d08869 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -290,6 +290,8 @@ static int map_lookup_elem(union bpf_attr *attr)
 		err = bpf_percpu_hash_copy(map, key, value);
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
 		err = bpf_percpu_array_copy(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {
+		err = bpf_stackmap_copy(map, key, value);
 	} else {
 		rcu_read_lock();
 		ptr = map->ops->map_lookup_elem(map, key);

commit 6c90598174322b8888029e40dd84a4eb01f56afe
Author: Alexei Starovoitov <ast@fb.com>
Date:   Mon Mar 7 21:57:15 2016 -0800

    bpf: pre-allocate hash map elements
    
    If kprobe is placed on spin_unlock then calling kmalloc/kfree from
    bpf programs is not safe, since the following dead lock is possible:
    kfree->spin_lock(kmem_cache_node->lock)...spin_unlock->kprobe->
    bpf_prog->map_update->kmalloc->spin_lock(of the same kmem_cache_node->lock)
    and deadlocks.
    
    The following solutions were considered and some implemented, but
    eventually discarded
    - kmem_cache_create for every map
    - add recursion check to slow-path of slub
    - use reserved memory in bpf_map_update for in_irq or in preempt_disabled
    - kmalloc via irq_work
    
    At the end pre-allocation of all map elements turned out to be the simplest
    solution and since the user is charged upfront for all the memory, such
    pre-allocation doesn't affect the user space visible behavior.
    
    Since it's impossible to tell whether kprobe is triggered in a safe
    location from kmalloc point of view, use pre-allocation by default
    and introduce new BPF_F_NO_PREALLOC flag.
    
    While testing of per-cpu hash maps it was discovered
    that alloc_percpu(GFP_ATOMIC) has odd corner cases and often
    fails to allocate memory even when 90% of it is free.
    The pre-allocation of per-cpu hash elements solves this problem as well.
    
    Turned out that bpf_map_update() quickly followed by
    bpf_map_lookup()+bpf_map_delete() is very common pattern used
    in many of iovisor/bcc/tools, so there is additional benefit of
    pre-allocation, since such use cases are must faster.
    
    Since all hash map elements are now pre-allocated we can remove
    atomic increment of htab->count and save few more cycles.
    
    Also add bpf_map_precharge_memlock() to check rlimit_memlock early to avoid
    large malloc/free done by users who don't have sufficient limits.
    
    Pre-allocation is done with vmalloc and alloc/free is done
    via percpu_freelist. Here are performance numbers for different
    pre-allocation algorithms that were implemented, but discarded
    in favor of percpu_freelist:
    
    1 cpu:
    pcpu_ida        2.1M
    pcpu_ida nolock 2.3M
    bt              2.4M
    kmalloc         1.8M
    hlist+spinlock  2.3M
    pcpu_freelist   2.6M
    
    4 cpu:
    pcpu_ida        1.5M
    pcpu_ida nolock 1.8M
    bt w/smp_align  1.7M
    bt no/smp_align 1.1M
    kmalloc         0.7M
    hlist+spinlock  0.2M
    pcpu_freelist   2.0M
    
    8 cpu:
    pcpu_ida        0.7M
    bt w/smp_align  0.8M
    kmalloc         0.4M
    pcpu_freelist   1.5M
    
    32 cpu:
    kmalloc         0.13M
    pcpu_freelist   0.49M
    
    pcpu_ida nolock is a modified percpu_ida algorithm without
    percpu_ida_cpu locks and without cross-cpu tag stealing.
    It's faster than existing percpu_ida, but not as fast as pcpu_freelist.
    
    bt is a variant of block/blk-mq-tag.c simlified and customized
    for bpf use case. bt w/smp_align is using cache line for every 'long'
    (similar to blk-mq-tag). bt no/smp_align allocates 'long'
    bitmasks continuously to save memory. It's comparable to percpu_ida
    and in some cases faster, but slower than percpu_freelist
    
    hlist+spinlock is the simplest free list with single spinlock.
    As expeceted it has very bad scaling in SMP.
    
    kmalloc is existing implementation which is still available via
    BPF_F_NO_PREALLOC flag. It's significantly slower in single cpu and
    in 8 cpu setup it's 3 times slower than pre-allocation with pcpu_freelist,
    but saves memory, so in cases where map->max_entries can be large
    and number of map update/delete per second is low, it may make
    sense to use it.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index dc99f6a000f5..cbd94b2144ff 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -48,6 +48,19 @@ void bpf_register_map_type(struct bpf_map_type_list *tl)
 	list_add(&tl->list_node, &bpf_map_types);
 }
 
+int bpf_map_precharge_memlock(u32 pages)
+{
+	struct user_struct *user = get_current_user();
+	unsigned long memlock_limit, cur;
+
+	memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+	cur = atomic_long_read(&user->locked_vm);
+	free_uid(user);
+	if (cur + pages > memlock_limit)
+		return -EPERM;
+	return 0;
+}
+
 static int bpf_map_charge_memlock(struct bpf_map *map)
 {
 	struct user_struct *user = get_current_user();
@@ -153,7 +166,7 @@ int bpf_map_new_fd(struct bpf_map *map)
 		   offsetof(union bpf_attr, CMD##_LAST_FIELD) - \
 		   sizeof(attr->CMD##_LAST_FIELD)) != NULL
 
-#define BPF_MAP_CREATE_LAST_FIELD max_entries
+#define BPF_MAP_CREATE_LAST_FIELD map_flags
 /* called via syscall */
 static int map_create(union bpf_attr *attr)
 {

commit b121d1e74d1f24654bdc3165d3db1ca149501356
Author: Alexei Starovoitov <ast@fb.com>
Date:   Mon Mar 7 21:57:13 2016 -0800

    bpf: prevent kprobe+bpf deadlocks
    
    if kprobe is placed within update or delete hash map helpers
    that hold bucket spin lock and triggered bpf program is trying to
    grab the spinlock for the same bucket on the same cpu, it will
    deadlock.
    Fix it by extending existing recursion prevention mechanism.
    
    Note, map_lookup and other tracing helpers don't have this problem,
    since they don't hold any locks and don't modify global data.
    bpf_trace_printk has its own recursive check and ok as well.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c95a753c2007..dc99f6a000f5 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -18,6 +18,8 @@
 #include <linux/filter.h>
 #include <linux/version.h>
 
+DEFINE_PER_CPU(int, bpf_prog_active);
+
 int sysctl_unprivileged_bpf_disabled __read_mostly;
 
 static LIST_HEAD(bpf_map_types);
@@ -347,6 +349,11 @@ static int map_update_elem(union bpf_attr *attr)
 	if (copy_from_user(value, uvalue, value_size) != 0)
 		goto free_value;
 
+	/* must increment bpf_prog_active to avoid kprobe+bpf triggering from
+	 * inside bpf map update or delete otherwise deadlocks are possible
+	 */
+	preempt_disable();
+	__this_cpu_inc(bpf_prog_active);
 	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH) {
 		err = bpf_percpu_hash_update(map, key, value, attr->flags);
 	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
@@ -356,6 +363,8 @@ static int map_update_elem(union bpf_attr *attr)
 		err = map->ops->map_update_elem(map, key, value, attr->flags);
 		rcu_read_unlock();
 	}
+	__this_cpu_dec(bpf_prog_active);
+	preempt_enable();
 
 free_value:
 	kfree(value);
@@ -394,9 +403,13 @@ static int map_delete_elem(union bpf_attr *attr)
 	if (copy_from_user(key, ukey, map->key_size) != 0)
 		goto free_key;
 
+	preempt_disable();
+	__this_cpu_inc(bpf_prog_active);
 	rcu_read_lock();
 	err = map->ops->map_delete_elem(map, key);
 	rcu_read_unlock();
+	__this_cpu_dec(bpf_prog_active);
+	preempt_enable();
 
 free_key:
 	kfree(key);

commit 15a07b33814d14ca817887dbea8530728dc0fbe4
Author: Alexei Starovoitov <ast@fb.com>
Date:   Mon Feb 1 22:39:55 2016 -0800

    bpf: add lookup/update support for per-cpu hash and array maps
    
    The functions bpf_map_lookup_elem(map, key, value) and
    bpf_map_update_elem(map, key, value, flags) need to get/set
    values from all-cpus for per-cpu hash and array maps,
    so that user space can aggregate/update them as necessary.
    
    Example of single counter aggregation in user space:
      unsigned int nr_cpus = sysconf(_SC_NPROCESSORS_CONF);
      long values[nr_cpus];
      long value = 0;
    
      bpf_lookup_elem(fd, key, values);
      for (i = 0; i < nr_cpus; i++)
        value += values[i];
    
    The user space must provide round_up(value_size, 8) * nr_cpus
    array to get/set values, since kernel will use 'long' copy
    of per-cpu values to try to copy good counters atomically.
    It's a best-effort, since bpf programs and user space are racing
    to access the same memory.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 637397059f76..c95a753c2007 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -239,6 +239,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 	int ufd = attr->map_fd;
 	struct bpf_map *map;
 	void *key, *value, *ptr;
+	u32 value_size;
 	struct fd f;
 	int err;
 
@@ -259,23 +260,35 @@ static int map_lookup_elem(union bpf_attr *attr)
 	if (copy_from_user(key, ukey, map->key_size) != 0)
 		goto free_key;
 
+	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
+		value_size = round_up(map->value_size, 8) * num_possible_cpus();
+	else
+		value_size = map->value_size;
+
 	err = -ENOMEM;
-	value = kmalloc(map->value_size, GFP_USER | __GFP_NOWARN);
+	value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
 	if (!value)
 		goto free_key;
 
-	rcu_read_lock();
-	ptr = map->ops->map_lookup_elem(map, key);
-	if (ptr)
-		memcpy(value, ptr, map->value_size);
-	rcu_read_unlock();
+	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH) {
+		err = bpf_percpu_hash_copy(map, key, value);
+	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
+		err = bpf_percpu_array_copy(map, key, value);
+	} else {
+		rcu_read_lock();
+		ptr = map->ops->map_lookup_elem(map, key);
+		if (ptr)
+			memcpy(value, ptr, value_size);
+		rcu_read_unlock();
+		err = ptr ? 0 : -ENOENT;
+	}
 
-	err = -ENOENT;
-	if (!ptr)
+	if (err)
 		goto free_value;
 
 	err = -EFAULT;
-	if (copy_to_user(uvalue, value, map->value_size) != 0)
+	if (copy_to_user(uvalue, value, value_size) != 0)
 		goto free_value;
 
 	err = 0;
@@ -298,6 +311,7 @@ static int map_update_elem(union bpf_attr *attr)
 	int ufd = attr->map_fd;
 	struct bpf_map *map;
 	void *key, *value;
+	u32 value_size;
 	struct fd f;
 	int err;
 
@@ -318,21 +332,30 @@ static int map_update_elem(union bpf_attr *attr)
 	if (copy_from_user(key, ukey, map->key_size) != 0)
 		goto free_key;
 
+	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||
+	    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY)
+		value_size = round_up(map->value_size, 8) * num_possible_cpus();
+	else
+		value_size = map->value_size;
+
 	err = -ENOMEM;
-	value = kmalloc(map->value_size, GFP_USER | __GFP_NOWARN);
+	value = kmalloc(value_size, GFP_USER | __GFP_NOWARN);
 	if (!value)
 		goto free_key;
 
 	err = -EFAULT;
-	if (copy_from_user(value, uvalue, map->value_size) != 0)
+	if (copy_from_user(value, uvalue, value_size) != 0)
 		goto free_value;
 
-	/* eBPF program that use maps are running under rcu_read_lock(),
-	 * therefore all map accessors rely on this fact, so do the same here
-	 */
-	rcu_read_lock();
-	err = map->ops->map_update_elem(map, key, value, attr->flags);
-	rcu_read_unlock();
+	if (map->map_type == BPF_MAP_TYPE_PERCPU_HASH) {
+		err = bpf_percpu_hash_update(map, key, value, attr->flags);
+	} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {
+		err = bpf_percpu_array_update(map, key, value, attr->flags);
+	} else {
+		rcu_read_lock();
+		err = map->ops->map_update_elem(map, key, value, attr->flags);
+		rcu_read_unlock();
+	}
 
 free_value:
 	kfree(value);

commit f188b951f33a0464338f94f928338f84fc0e4392
Merge: 6b20da4d8f3f 071f5d105a0a
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Dec 3 21:03:21 2015 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/renesas/ravb_main.c
            kernel/bpf/syscall.c
            net/ipv4/ipmr.c
    
    All three conflicts were cases of overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 01b3f52157ff5a47d6d8d796f396a4b34a53c61d
Author: Alexei Starovoitov <ast@kernel.org>
Date:   Sun Nov 29 16:59:35 2015 -0800

    bpf: fix allocation warnings in bpf maps and integer overflow
    
    For large map->value_size the user space can trigger memory allocation warnings like:
    WARNING: CPU: 2 PID: 11122 at mm/page_alloc.c:2989
    __alloc_pages_nodemask+0x695/0x14e0()
    Call Trace:
     [<     inline     >] __dump_stack lib/dump_stack.c:15
     [<ffffffff82743b56>] dump_stack+0x68/0x92 lib/dump_stack.c:50
     [<ffffffff81244ec9>] warn_slowpath_common+0xd9/0x140 kernel/panic.c:460
     [<ffffffff812450f9>] warn_slowpath_null+0x29/0x30 kernel/panic.c:493
     [<     inline     >] __alloc_pages_slowpath mm/page_alloc.c:2989
     [<ffffffff81554e95>] __alloc_pages_nodemask+0x695/0x14e0 mm/page_alloc.c:3235
     [<ffffffff816188fe>] alloc_pages_current+0xee/0x340 mm/mempolicy.c:2055
     [<     inline     >] alloc_pages include/linux/gfp.h:451
     [<ffffffff81550706>] alloc_kmem_pages+0x16/0xf0 mm/page_alloc.c:3414
     [<ffffffff815a1c89>] kmalloc_order+0x19/0x60 mm/slab_common.c:1007
     [<ffffffff815a1cef>] kmalloc_order_trace+0x1f/0xa0 mm/slab_common.c:1018
     [<     inline     >] kmalloc_large include/linux/slab.h:390
     [<ffffffff81627784>] __kmalloc+0x234/0x250 mm/slub.c:3525
     [<     inline     >] kmalloc include/linux/slab.h:463
     [<     inline     >] map_update_elem kernel/bpf/syscall.c:288
     [<     inline     >] SYSC_bpf kernel/bpf/syscall.c:744
    
    To avoid never succeeding kmalloc with order >= MAX_ORDER check that
    elem->value_size and computed elem_size are within limits for both hash and
    array type maps.
    Also add __GFP_NOWARN to kmalloc(value_size | elem_size) to avoid OOM warnings.
    Note kmalloc(key_size) is highly unlikely to trigger OOM, since key_size <= 512,
    so keep those kmalloc-s as-is.
    
    Large value_size can cause integer overflows in elem_size and map.pages
    formulas, so check for that as well.
    
    Fixes: aaac3ba95e4c ("bpf: charge user for creation of BPF maps and programs")
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 4a8f3c1d7da6..3b39550d8485 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -240,7 +240,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 		goto free_key;
 
 	err = -ENOMEM;
-	value = kmalloc(map->value_size, GFP_USER);
+	value = kmalloc(map->value_size, GFP_USER | __GFP_NOWARN);
 	if (!value)
 		goto free_key;
 
@@ -299,7 +299,7 @@ static int map_update_elem(union bpf_attr *attr)
 		goto free_key;
 
 	err = -ENOMEM;
-	value = kmalloc(map->value_size, GFP_USER);
+	value = kmalloc(map->value_size, GFP_USER | __GFP_NOWARN);
 	if (!value)
 		goto free_key;
 

commit c9da161c6517ba12154059d3b965c2cbaf16f90f
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Nov 24 21:28:15 2015 +0100

    bpf: fix clearing on persistent program array maps
    
    Currently, when having map file descriptors pointing to program arrays,
    there's still the issue that we unconditionally flush program array
    contents via bpf_fd_array_map_clear() in bpf_map_release(). This happens
    when such a file descriptor is released and is independent of the map's
    refcount.
    
    Having this flush independent of the refcount is for a reason: there
    can be arbitrary complex dependency chains among tail calls, also circular
    ones (direct or indirect, nesting limit determined during runtime), and
    we need to make sure that the map drops all references to eBPF programs
    it holds, so that the map's refcount can eventually drop to zero and
    initiate its freeing. Btw, a walk of the whole dependency graph would
    not be possible for various reasons, one being complexity and another
    one inconsistency, i.e. new programs can be added to parts of the graph
    at any time, so there's no guaranteed consistent state for the time of
    such a walk.
    
    Now, the program array pinning itself works, but the issue is that each
    derived file descriptor on close would nevertheless call unconditionally
    into bpf_fd_array_map_clear(). Instead, keep track of users and postpone
    this flush until the last reference to a user is dropped. As this only
    concerns a subset of references (f.e. a prog array could hold a program
    that itself has reference on the prog array holding it, etc), we need to
    track them separately.
    
    Short analysis on the refcounting: on map creation time usercnt will be
    one, so there's no change in behaviour for bpf_map_release(), if unpinned.
    If we already fail in map_create(), we are immediately freed, and no
    file descriptor has been made public yet. In bpf_obj_pin_user(), we need
    to probe for a possible map in bpf_fd_probe_obj() already with a usercnt
    reference, so before we drop the reference on the fd with fdput().
    Therefore, if actual pinning fails, we need to drop that reference again
    in bpf_any_put(), otherwise we keep holding it. When last reference
    drops on the inode, the bpf_any_put() in bpf_evict_inode() will take
    care of dropping the usercnt again. In the bpf_obj_get_user() case, the
    bpf_any_get() will grab a reference on the usercnt, still at a time when
    we have the reference on the path. Should we later on fail to grab a new
    file descriptor, bpf_any_put() will drop it, otherwise we hold it until
    bpf_map_release() time.
    
    Joint work with Alexei.
    
    Fixes: b2197755b263 ("bpf: add support for persistent maps/progs")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0d3313d02a7e..4a8f3c1d7da6 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -82,6 +82,14 @@ static void bpf_map_free_deferred(struct work_struct *work)
 	map->ops->map_free(map);
 }
 
+static void bpf_map_put_uref(struct bpf_map *map)
+{
+	if (atomic_dec_and_test(&map->usercnt)) {
+		if (map->map_type == BPF_MAP_TYPE_PROG_ARRAY)
+			bpf_fd_array_map_clear(map);
+	}
+}
+
 /* decrement map refcnt and schedule it for freeing via workqueue
  * (unrelying map implementation ops->map_free() might sleep)
  */
@@ -93,17 +101,15 @@ void bpf_map_put(struct bpf_map *map)
 	}
 }
 
-static int bpf_map_release(struct inode *inode, struct file *filp)
+void bpf_map_put_with_uref(struct bpf_map *map)
 {
-	struct bpf_map *map = filp->private_data;
-
-	if (map->map_type == BPF_MAP_TYPE_PROG_ARRAY)
-		/* prog_array stores refcnt-ed bpf_prog pointers
-		 * release them all when user space closes prog_array_fd
-		 */
-		bpf_fd_array_map_clear(map);
-
+	bpf_map_put_uref(map);
 	bpf_map_put(map);
+}
+
+static int bpf_map_release(struct inode *inode, struct file *filp)
+{
+	bpf_map_put_with_uref(filp->private_data);
 	return 0;
 }
 
@@ -142,6 +148,7 @@ static int map_create(union bpf_attr *attr)
 		return PTR_ERR(map);
 
 	atomic_set(&map->refcnt, 1);
+	atomic_set(&map->usercnt, 1);
 
 	err = bpf_map_charge_memlock(map);
 	if (err)
@@ -174,7 +181,14 @@ struct bpf_map *__bpf_map_get(struct fd f)
 	return f.file->private_data;
 }
 
-struct bpf_map *bpf_map_get(u32 ufd)
+void bpf_map_inc(struct bpf_map *map, bool uref)
+{
+	atomic_inc(&map->refcnt);
+	if (uref)
+		atomic_inc(&map->usercnt);
+}
+
+struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 {
 	struct fd f = fdget(ufd);
 	struct bpf_map *map;
@@ -183,7 +197,7 @@ struct bpf_map *bpf_map_get(u32 ufd)
 	if (IS_ERR(map))
 		return map;
 
-	atomic_inc(&map->refcnt);
+	bpf_map_inc(map, true);
 	fdput(f);
 
 	return map;

commit f99bf205dab026ef434520198af2fcb7dae0efdb
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Nov 19 11:56:22 2015 +0100

    bpf: add show_fdinfo handler for maps
    
    Add a handler for show_fdinfo() to be used by the anon-inodes
    backend for eBPF maps, and dump the map specification there. Not
    only useful for admins, but also it provides a minimal way to
    compare specs from ELF vs pinned object.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0d3313d02a7e..6d1407bc1531 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -93,6 +93,23 @@ void bpf_map_put(struct bpf_map *map)
 	}
 }
 
+#ifdef CONFIG_PROC_FS
+static void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)
+{
+	const struct bpf_map *map = filp->private_data;
+
+	seq_printf(m,
+		   "map_type:\t%u\n"
+		   "key_size:\t%u\n"
+		   "value_size:\t%u\n"
+		   "max_entries:\t%u\n",
+		   map->map_type,
+		   map->key_size,
+		   map->value_size,
+		   map->max_entries);
+}
+#endif
+
 static int bpf_map_release(struct inode *inode, struct file *filp)
 {
 	struct bpf_map *map = filp->private_data;
@@ -108,7 +125,10 @@ static int bpf_map_release(struct inode *inode, struct file *filp)
 }
 
 static const struct file_operations bpf_map_fops = {
-	.release = bpf_map_release,
+#ifdef CONFIG_PROC_FS
+	.show_fdinfo	= bpf_map_show_fdinfo,
+#endif
+	.release	= bpf_map_release,
 };
 
 int bpf_map_new_fd(struct bpf_map *map)

commit b2197755b2633e164a439682fb05a9b5ea48f706
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Oct 29 14:58:09 2015 +0100

    bpf: add support for persistent maps/progs
    
    This work adds support for "persistent" eBPF maps/programs. The term
    "persistent" is to be understood that maps/programs have a facility
    that lets them survive process termination. This is desired by various
    eBPF subsystem users.
    
    Just to name one example: tc classifier/action. Whenever tc parses
    the ELF object, extracts and loads maps/progs into the kernel, these
    file descriptors will be out of reach after the tc instance exits.
    So a subsequent tc invocation won't be able to access/relocate on this
    resource, and therefore maps cannot easily be shared, f.e. between the
    ingress and egress networking data path.
    
    The current workaround is that Unix domain sockets (UDS) need to be
    instrumented in order to pass the created eBPF map/program file
    descriptors to a third party management daemon through UDS' socket
    passing facility. This makes it a bit complicated to deploy shared
    eBPF maps or programs (programs f.e. for tail calls) among various
    processes.
    
    We've been brainstorming on how we could tackle this issue and various
    approches have been tried out so far, which can be read up further in
    the below reference.
    
    The architecture we eventually ended up with is a minimal file system
    that can hold map/prog objects. The file system is a per mount namespace
    singleton, and the default mount point is /sys/fs/bpf/. Any subsequent
    mounts within a given namespace will point to the same instance. The
    file system allows for creating a user-defined directory structure.
    The objects for maps/progs are created/fetched through bpf(2) with
    two new commands (BPF_OBJ_PIN/BPF_OBJ_GET). I.e. a bpf file descriptor
    along with a pathname is being passed to bpf(2) that in turn creates
    (we call it eBPF object pinning) the file system nodes. Only the pathname
    is being passed to bpf(2) for getting a new BPF file descriptor to an
    existing node. The user can use that to access maps and progs later on,
    through bpf(2). Removal of file system nodes is being managed through
    normal VFS functions such as unlink(2), etc. The file system code is
    kept to a very minimum and can be further extended later on.
    
    The next step I'm working on is to add dump eBPF map/prog commands
    to bpf(2), so that a specification from a given file descriptor can
    be retrieved. This can be used by things like CRIU but also applications
    can inspect the meta data after calling BPF_OBJ_GET.
    
    Big thanks also to Alexei and Hannes who significantly contributed
    in the design discussion that eventually let us end up with this
    architecture here.
    
    Reference: https://lkml.org/lkml/2015/10/15/925
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index d7783cb04d86..0d3313d02a7e 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -111,7 +111,7 @@ static const struct file_operations bpf_map_fops = {
 	.release = bpf_map_release,
 };
 
-static int bpf_map_new_fd(struct bpf_map *map)
+int bpf_map_new_fd(struct bpf_map *map)
 {
 	return anon_inode_getfd("bpf-map", &bpf_map_fops, map,
 				O_RDWR | O_CLOEXEC);
@@ -174,7 +174,7 @@ struct bpf_map *__bpf_map_get(struct fd f)
 	return f.file->private_data;
 }
 
-static struct bpf_map *bpf_map_get(u32 ufd)
+struct bpf_map *bpf_map_get(u32 ufd)
 {
 	struct fd f = fdget(ufd);
 	struct bpf_map *map;
@@ -548,7 +548,7 @@ static const struct file_operations bpf_prog_fops = {
         .release = bpf_prog_release,
 };
 
-static int bpf_prog_new_fd(struct bpf_prog *prog)
+int bpf_prog_new_fd(struct bpf_prog *prog)
 {
 	return anon_inode_getfd("bpf-prog", &bpf_prog_fops, prog,
 				O_RDWR | O_CLOEXEC);
@@ -674,6 +674,24 @@ static int bpf_prog_load(union bpf_attr *attr)
 	return err;
 }
 
+#define BPF_OBJ_LAST_FIELD bpf_fd
+
+static int bpf_obj_pin(const union bpf_attr *attr)
+{
+	if (CHECK_ATTR(BPF_OBJ))
+		return -EINVAL;
+
+	return bpf_obj_pin_user(attr->bpf_fd, u64_to_ptr(attr->pathname));
+}
+
+static int bpf_obj_get(const union bpf_attr *attr)
+{
+	if (CHECK_ATTR(BPF_OBJ) || attr->bpf_fd != 0)
+		return -EINVAL;
+
+	return bpf_obj_get_user(u64_to_ptr(attr->pathname));
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -734,6 +752,12 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_PROG_LOAD:
 		err = bpf_prog_load(&attr);
 		break;
+	case BPF_OBJ_PIN:
+		err = bpf_obj_pin(&attr);
+		break;
+	case BPF_OBJ_GET:
+		err = bpf_obj_get(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit e9d8afa90b789b07d414637ab557d169d6b2b84e
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Oct 29 14:58:08 2015 +0100

    bpf: consolidate bpf_prog_put{, _rcu} dismantle paths
    
    We currently have duplicated cleanup code in bpf_prog_put() and
    bpf_prog_put_rcu() cleanup paths. Back then we decided that it was
    not worth it to make it a common helper called by both, but with
    the recent addition of resource charging, we could have avoided
    the fix in commit ac00737f4e81 ("bpf: Need to call bpf_prog_uncharge_memlock
    from bpf_prog_put") if we would have had only a single, common path.
    We can simplify it further by assigning aux->prog only once during
    allocation time.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 3fff82ca68fa..d7783cb04d86 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -513,7 +513,7 @@ static void bpf_prog_uncharge_memlock(struct bpf_prog *prog)
 	free_uid(user);
 }
 
-static void __prog_put_rcu(struct rcu_head *rcu)
+static void __prog_put_common(struct rcu_head *rcu)
 {
 	struct bpf_prog_aux *aux = container_of(rcu, struct bpf_prog_aux, rcu);
 
@@ -525,19 +525,14 @@ static void __prog_put_rcu(struct rcu_head *rcu)
 /* version of bpf_prog_put() that is called after a grace period */
 void bpf_prog_put_rcu(struct bpf_prog *prog)
 {
-	if (atomic_dec_and_test(&prog->aux->refcnt)) {
-		prog->aux->prog = prog;
-		call_rcu(&prog->aux->rcu, __prog_put_rcu);
-	}
+	if (atomic_dec_and_test(&prog->aux->refcnt))
+		call_rcu(&prog->aux->rcu, __prog_put_common);
 }
 
 void bpf_prog_put(struct bpf_prog *prog)
 {
-	if (atomic_dec_and_test(&prog->aux->refcnt)) {
-		free_used_maps(prog->aux);
-		bpf_prog_uncharge_memlock(prog);
-		bpf_prog_free(prog);
-	}
+	if (atomic_dec_and_test(&prog->aux->refcnt))
+		__prog_put_common(&prog->aux->rcu);
 }
 EXPORT_SYMBOL_GPL(bpf_prog_put);
 

commit c210129760a010b555372ef74f4e1a46d4eb8a22
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Oct 29 14:58:07 2015 +0100

    bpf: align and clean bpf_{map,prog}_get helpers
    
    Add a bpf_map_get() function that we're going to use later on and
    align/clean the remaining helpers a bit so that we have them a bit
    more consistent:
    
      - __bpf_map_get() and __bpf_prog_get() that both work on the fd
        struct, check whether the descriptor is eBPF and return the
        pointer to the map/prog stored in the private data.
    
        Also, we can return f.file->private_data directly, the function
        signature is enough of a documentation already.
    
      - bpf_map_get() and bpf_prog_get() that both work on u32 user fd,
        call their respective __bpf_map_get()/__bpf_prog_get() variants,
        and take a reference.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2b89ef0a9757..3fff82ca68fa 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -162,19 +162,29 @@ static int map_create(union bpf_attr *attr)
 /* if error is returned, fd is released.
  * On success caller should complete fd access with matching fdput()
  */
-struct bpf_map *bpf_map_get(struct fd f)
+struct bpf_map *__bpf_map_get(struct fd f)
 {
-	struct bpf_map *map;
-
 	if (!f.file)
 		return ERR_PTR(-EBADF);
-
 	if (f.file->f_op != &bpf_map_fops) {
 		fdput(f);
 		return ERR_PTR(-EINVAL);
 	}
 
-	map = f.file->private_data;
+	return f.file->private_data;
+}
+
+static struct bpf_map *bpf_map_get(u32 ufd)
+{
+	struct fd f = fdget(ufd);
+	struct bpf_map *map;
+
+	map = __bpf_map_get(f);
+	if (IS_ERR(map))
+		return map;
+
+	atomic_inc(&map->refcnt);
+	fdput(f);
 
 	return map;
 }
@@ -202,7 +212,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 		return -EINVAL;
 
 	f = fdget(ufd);
-	map = bpf_map_get(f);
+	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
@@ -261,7 +271,7 @@ static int map_update_elem(union bpf_attr *attr)
 		return -EINVAL;
 
 	f = fdget(ufd);
-	map = bpf_map_get(f);
+	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
@@ -314,7 +324,7 @@ static int map_delete_elem(union bpf_attr *attr)
 		return -EINVAL;
 
 	f = fdget(ufd);
-	map = bpf_map_get(f);
+	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
@@ -355,7 +365,7 @@ static int map_get_next_key(union bpf_attr *attr)
 		return -EINVAL;
 
 	f = fdget(ufd);
-	map = bpf_map_get(f);
+	map = __bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
 
@@ -549,21 +559,16 @@ static int bpf_prog_new_fd(struct bpf_prog *prog)
 				O_RDWR | O_CLOEXEC);
 }
 
-static struct bpf_prog *get_prog(struct fd f)
+static struct bpf_prog *__bpf_prog_get(struct fd f)
 {
-	struct bpf_prog *prog;
-
 	if (!f.file)
 		return ERR_PTR(-EBADF);
-
 	if (f.file->f_op != &bpf_prog_fops) {
 		fdput(f);
 		return ERR_PTR(-EINVAL);
 	}
 
-	prog = f.file->private_data;
-
-	return prog;
+	return f.file->private_data;
 }
 
 /* called by sockets/tracing/seccomp before attaching program to an event
@@ -574,13 +579,13 @@ struct bpf_prog *bpf_prog_get(u32 ufd)
 	struct fd f = fdget(ufd);
 	struct bpf_prog *prog;
 
-	prog = get_prog(f);
-
+	prog = __bpf_prog_get(f);
 	if (IS_ERR(prog))
 		return prog;
 
 	atomic_inc(&prog->aux->refcnt);
 	fdput(f);
+
 	return prog;
 }
 EXPORT_SYMBOL_GPL(bpf_prog_get);

commit aa79781b65b9cf79807ade78f2703f5e9402c336
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Oct 29 14:58:06 2015 +0100

    bpf: abstract anon_inode_getfd invocations
    
    Since we're going to use anon_inode_getfd() invocations in more than just
    the current places, make a helper function for both, so that we only need
    to pass a map/prog pointer to the helper itself in order to get a fd. The
    new helpers are called bpf_map_new_fd() and bpf_prog_new_fd().
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 687dd6ca574d..2b89ef0a9757 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -111,6 +111,12 @@ static const struct file_operations bpf_map_fops = {
 	.release = bpf_map_release,
 };
 
+static int bpf_map_new_fd(struct bpf_map *map)
+{
+	return anon_inode_getfd("bpf-map", &bpf_map_fops, map,
+				O_RDWR | O_CLOEXEC);
+}
+
 /* helper macro to check that unused fields 'union bpf_attr' are zero */
 #define CHECK_ATTR(CMD) \
 	memchr_inv((void *) &attr->CMD##_LAST_FIELD + \
@@ -141,8 +147,7 @@ static int map_create(union bpf_attr *attr)
 	if (err)
 		goto free_map;
 
-	err = anon_inode_getfd("bpf-map", &bpf_map_fops, map, O_RDWR | O_CLOEXEC);
-
+	err = bpf_map_new_fd(map);
 	if (err < 0)
 		/* failed to allocate fd */
 		goto free_map;
@@ -538,6 +543,12 @@ static const struct file_operations bpf_prog_fops = {
         .release = bpf_prog_release,
 };
 
+static int bpf_prog_new_fd(struct bpf_prog *prog)
+{
+	return anon_inode_getfd("bpf-prog", &bpf_prog_fops, prog,
+				O_RDWR | O_CLOEXEC);
+}
+
 static struct bpf_prog *get_prog(struct fd f)
 {
 	struct bpf_prog *prog;
@@ -647,7 +658,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (err < 0)
 		goto free_used_maps;
 
-	err = anon_inode_getfd("bpf-prog", &bpf_prog_fops, prog, O_RDWR | O_CLOEXEC);
+	err = bpf_prog_new_fd(prog);
 	if (err < 0)
 		/* failed to allocate fd */
 		goto free_used_maps;

commit ac00737f4e8198f8ff5007c70af4dfe4fd47ea94
Author: Tom Herbert <tom@herbertland.com>
Date:   Wed Oct 14 14:40:44 2015 -0700

    bpf: Need to call bpf_prog_uncharge_memlock from bpf_prog_put
    
    Currently, is only called from __prog_put_rcu in the bpf_prog_release
    path. Need this to call this from bpf_prog_put also to get correct
    accounting.
    
    Fixes: aaac3ba95e4c8b49 ("bpf: charge user for creation of BPF maps and programs")
    Signed-off-by: Tom Herbert <tom@herbertland.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index f640e5f7afbd..687dd6ca574d 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -520,6 +520,7 @@ void bpf_prog_put(struct bpf_prog *prog)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
 		free_used_maps(prog->aux);
+		bpf_prog_uncharge_memlock(prog);
 		bpf_prog_free(prog);
 	}
 }

commit aaac3ba95e4c8b496d22f68bd1bc01cfbf525eca
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Oct 7 22:23:22 2015 -0700

    bpf: charge user for creation of BPF maps and programs
    
    since eBPF programs and maps use kernel memory consider it 'locked' memory
    from user accounting point of view and charge it against RLIMIT_MEMLOCK limit.
    This limit is typically set to 64Kbytes by distros, so almost all
    bpf+tracing programs would need to increase it, since they use maps,
    but kernel charges maximum map size upfront.
    For example the hash map of 1024 elements will be charged as 64Kbyte.
    It's inconvenient for current users and changes current behavior for root,
    but probably worth doing to be consistent root vs non-root.
    
    Similar accounting logic is done by mmap of perf_event.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 83697bc8e574..f640e5f7afbd 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -46,11 +46,38 @@ void bpf_register_map_type(struct bpf_map_type_list *tl)
 	list_add(&tl->list_node, &bpf_map_types);
 }
 
+static int bpf_map_charge_memlock(struct bpf_map *map)
+{
+	struct user_struct *user = get_current_user();
+	unsigned long memlock_limit;
+
+	memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+
+	atomic_long_add(map->pages, &user->locked_vm);
+
+	if (atomic_long_read(&user->locked_vm) > memlock_limit) {
+		atomic_long_sub(map->pages, &user->locked_vm);
+		free_uid(user);
+		return -EPERM;
+	}
+	map->user = user;
+	return 0;
+}
+
+static void bpf_map_uncharge_memlock(struct bpf_map *map)
+{
+	struct user_struct *user = map->user;
+
+	atomic_long_sub(map->pages, &user->locked_vm);
+	free_uid(user);
+}
+
 /* called from workqueue */
 static void bpf_map_free_deferred(struct work_struct *work)
 {
 	struct bpf_map *map = container_of(work, struct bpf_map, work);
 
+	bpf_map_uncharge_memlock(map);
 	/* implementation dependent freeing */
 	map->ops->map_free(map);
 }
@@ -110,6 +137,10 @@ static int map_create(union bpf_attr *attr)
 
 	atomic_set(&map->refcnt, 1);
 
+	err = bpf_map_charge_memlock(map);
+	if (err)
+		goto free_map;
+
 	err = anon_inode_getfd("bpf-map", &bpf_map_fops, map, O_RDWR | O_CLOEXEC);
 
 	if (err < 0)
@@ -442,11 +473,37 @@ static void free_used_maps(struct bpf_prog_aux *aux)
 	kfree(aux->used_maps);
 }
 
+static int bpf_prog_charge_memlock(struct bpf_prog *prog)
+{
+	struct user_struct *user = get_current_user();
+	unsigned long memlock_limit;
+
+	memlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+
+	atomic_long_add(prog->pages, &user->locked_vm);
+	if (atomic_long_read(&user->locked_vm) > memlock_limit) {
+		atomic_long_sub(prog->pages, &user->locked_vm);
+		free_uid(user);
+		return -EPERM;
+	}
+	prog->aux->user = user;
+	return 0;
+}
+
+static void bpf_prog_uncharge_memlock(struct bpf_prog *prog)
+{
+	struct user_struct *user = prog->aux->user;
+
+	atomic_long_sub(prog->pages, &user->locked_vm);
+	free_uid(user);
+}
+
 static void __prog_put_rcu(struct rcu_head *rcu)
 {
 	struct bpf_prog_aux *aux = container_of(rcu, struct bpf_prog_aux, rcu);
 
 	free_used_maps(aux);
+	bpf_prog_uncharge_memlock(aux->prog);
 	bpf_prog_free(aux->prog);
 }
 
@@ -554,6 +611,10 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (!prog)
 		return -ENOMEM;
 
+	err = bpf_prog_charge_memlock(prog);
+	if (err)
+		goto free_prog_nouncharge;
+
 	prog->len = attr->insn_cnt;
 
 	err = -EFAULT;
@@ -595,6 +656,8 @@ static int bpf_prog_load(union bpf_attr *attr)
 free_used_maps:
 	free_used_maps(prog->aux);
 free_prog:
+	bpf_prog_uncharge_memlock(prog);
+free_prog_nouncharge:
 	bpf_prog_free(prog);
 	return err;
 }

commit 1be7f75d1668d6296b80bf35dcf6762393530afc
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Oct 7 22:23:21 2015 -0700

    bpf: enable non-root eBPF programs
    
    In order to let unprivileged users load and execute eBPF programs
    teach verifier to prevent pointer leaks.
    Verifier will prevent
    - any arithmetic on pointers
      (except R10+Imm which is used to compute stack addresses)
    - comparison of pointers
      (except if (map_value_ptr == 0) ... )
    - passing pointers to helper functions
    - indirectly passing pointers in stack to helper functions
    - returning pointer from bpf program
    - storing pointers into ctx or maps
    
    Spill/fill of pointers into stack is allowed, but mangling
    of pointers stored in the stack or reading them byte by byte is not.
    
    Within bpf programs the pointers do exist, since programs need to
    be able to access maps, pass skb pointer to LD_ABS insns, etc
    but programs cannot pass such pointer values to the outside
    or obfuscate them.
    
    Only allow BPF_PROG_TYPE_SOCKET_FILTER unprivileged programs,
    so that socket filters (tcpdump), af_packet (quic acceleration)
    and future kcm can use it.
    tracing and tc cls/act program types still require root permissions,
    since tracing actually needs to be able to see all kernel pointers
    and tc is for root only.
    
    For example, the following unprivileged socket filter program is allowed:
    int bpf_prog1(struct __sk_buff *skb)
    {
      u32 index = load_byte(skb, ETH_HLEN + offsetof(struct iphdr, protocol));
      u64 *value = bpf_map_lookup_elem(&my_map, &index);
    
      if (value)
            *value += skb->len;
      return 0;
    }
    
    but the following program is not:
    int bpf_prog1(struct __sk_buff *skb)
    {
      u32 index = load_byte(skb, ETH_HLEN + offsetof(struct iphdr, protocol));
      u64 *value = bpf_map_lookup_elem(&my_map, &index);
    
      if (value)
            *value += (u64) skb;
      return 0;
    }
    since it would leak the kernel address into the map.
    
    Unprivileged socket filter bpf programs have access to the
    following helper functions:
    - map lookup/update/delete (but they cannot store kernel pointers into them)
    - get_random (it's already exposed to unprivileged user space)
    - get_smp_processor_id
    - tail_call into another socket filter program
    - ktime_get_ns
    
    The feature is controlled by sysctl kernel.unprivileged_bpf_disabled.
    This toggle defaults to off (0), but can be set true (1).  Once true,
    bpf programs and maps cannot be accessed from unprivileged process,
    and the toggle cannot be set back to false.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c868cafbc00c..83697bc8e574 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -18,6 +18,8 @@
 #include <linux/filter.h>
 #include <linux/version.h>
 
+int sysctl_unprivileged_bpf_disabled __read_mostly;
+
 static LIST_HEAD(bpf_map_types);
 
 static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
@@ -544,6 +546,9 @@ static int bpf_prog_load(union bpf_attr *attr)
 	    attr->kern_version != LINUX_VERSION_CODE)
 		return -EINVAL;
 
+	if (type != BPF_PROG_TYPE_SOCKET_FILTER && !capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
 	/* plain bpf_prog allocation */
 	prog = bpf_prog_alloc(bpf_prog_size(attr->insn_cnt), GFP_USER);
 	if (!prog)
@@ -599,11 +604,7 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	union bpf_attr attr = {};
 	int err;
 
-	/* the syscall is limited to root temporarily. This restriction will be
-	 * lifted when security audit is clean. Note that eBPF+tracing must have
-	 * this restriction, since it may pass kernel data to user space
-	 */
-	if (!capable(CAP_SYS_ADMIN))
+	if (!capable(CAP_SYS_ADMIN) && sysctl_unprivileged_bpf_disabled)
 		return -EPERM;
 
 	if (!access_ok(VERIFY_READ, uattr, 1))

commit 3ad0040573b0c00f88488bc31958acd07a55ee2e
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Thu Oct 8 01:20:39 2015 +0200

    bpf: split state from prandom_u32() and consolidate {c, e}BPF prngs
    
    While recently arguing on a seccomp discussion that raw prandom_u32()
    access shouldn't be exposed to unpriviledged user space, I forgot the
    fact that SKF_AD_RANDOM extension actually already does it for some time
    in cBPF via commit 4cd3675ebf74 ("filter: added BPF random opcode").
    
    Since prandom_u32() is being used in a lot of critical networking code,
    lets be more conservative and split their states. Furthermore, consolidate
    eBPF and cBPF prandom handlers to use the new internal PRNG. For eBPF,
    bpf_get_prandom_u32() was only accessible for priviledged users, but
    should that change one day, we also don't want to leak raw sequences
    through things like eBPF maps.
    
    One thought was also to have own per bpf_prog states, but due to ABI
    reasons this is not easily possible, i.e. the program code currently
    cannot access bpf_prog itself, and copying the rnd_state to/from the
    stack scratch space whenever a program uses the prng seems not really
    worth the trouble and seems too hacky. If needed, taus113 could in such
    cases be implemented within eBPF using a map entry to keep the state
    space, or get_random_bytes() could become a second helper in cases where
    performance would not be critical.
    
    Both sides can trigger a one-time late init via prandom_init_once() on
    the shared state. Performance-wise, there should even be a tiny gain
    as bpf_user_rnd_u32() saves one function call. The PRNG needs to live
    inside the BPF core since kernels could have a NET-less config as well.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Cc: Chema Gonzalez <chema@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 5f35f420c12f..c868cafbc00c 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -404,6 +404,8 @@ static void fixup_bpf_calls(struct bpf_prog *prog)
 
 			if (insn->imm == BPF_FUNC_get_route_realm)
 				prog->dst_needed = 1;
+			if (insn->imm == BPF_FUNC_get_prandom_u32)
+				bpf_user_rnd_init_once();
 			if (insn->imm == BPF_FUNC_tail_call) {
 				/* mark bpf_tail_call as different opcode
 				 * to avoid conditional branch in

commit c46646d0484f5d08e2bede9b45034ba5b8b489cc
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Sep 30 01:41:51 2015 +0200

    sched, bpf: add helper for retrieving routing realms
    
    Using routing realms as part of the classifier is quite useful, it
    can be viewed as a tag for one or multiple routing entries (think of
    an analogy to net_cls cgroup for processes), set by user space routing
    daemons or via iproute2 as an indicator for traffic classifiers and
    later on processed in the eBPF program.
    
    Unlike actions, the classifier can inspect device flags and enable
    netif_keep_dst() if necessary. tc actions don't have that possibility,
    but in case people know what they are doing, it can be used from there
    as well (e.g. via devs that must keep dsts by design anyway).
    
    If a realm is set, the handler returns the non-zero realm. User space
    can set the full 32bit realm for the dst.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 2190ab14b763..5f35f420c12f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -402,6 +402,8 @@ static void fixup_bpf_calls(struct bpf_prog *prog)
 			 */
 			BUG_ON(!prog->aux->ops->get_func_proto);
 
+			if (insn->imm == BPF_FUNC_get_route_realm)
+				prog->dst_needed = 1;
 			if (insn->imm == BPF_FUNC_tail_call) {
 				/* mark bpf_tail_call as different opcode
 				 * to avoid conditional branch in

commit a91263d520246b63c63e75ddfb072ee6a853fe15
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Wed Sep 30 01:41:50 2015 +0200

    ebpf: migrate bpf_prog's flags to bitfield
    
    As we need to add further flags to the bpf_prog structure, lets migrate
    both bools to a bitfield representation. The size of the base structure
    (excluding insns) remains unchanged at 40 bytes.
    
    Add also tags for the kmemchecker, so that it doesn't throw false
    positives. Even in case gcc would generate suboptimal code, it's not
    being accessed in performance critical paths.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 35bac8e8b071..2190ab14b763 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -553,10 +553,10 @@ static int bpf_prog_load(union bpf_attr *attr)
 		goto free_prog;
 
 	prog->orig_prog = NULL;
-	prog->jited = false;
+	prog->jited = 0;
 
 	atomic_set(&prog->aux->refcnt, 1);
-	prog->gpl_compatible = is_gpl;
+	prog->gpl_compatible = is_gpl ? 1 : 0;
 
 	/* find program type: socket_filter vs tracing_filter */
 	err = find_prog_type(type, prog);

commit 592867bfabe2fcb449393ba7eb0de4f972a08c63
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Tue Sep 8 18:00:09 2015 +0200

    ebpf: fix fd refcount leaks related to maps in bpf syscall
    
    We may already have gotten a proper fd struct through fdget(), so
    whenever we return at the end of an map operation, we need to call
    fdput(). However, each map operation from syscall side first probes
    CHECK_ATTR() to verify that unused fields in the bpf_attr union are
    zero.
    
    In case of malformed input, we return with error, but the lookup to
    the map_fd was already performed at that time, so that we return
    without an corresponding fdput(). Fix it by performing an fdget()
    only right before bpf_map_get(). The fdget() invocation on maps in
    the verifier is not affected.
    
    Fixes: db20fd2b0108 ("bpf: add lookup/update/delete/iterate methods to BPF maps")
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index dc9b464fefa9..35bac8e8b071 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -155,14 +155,15 @@ static int map_lookup_elem(union bpf_attr *attr)
 	void __user *ukey = u64_to_ptr(attr->key);
 	void __user *uvalue = u64_to_ptr(attr->value);
 	int ufd = attr->map_fd;
-	struct fd f = fdget(ufd);
 	struct bpf_map *map;
 	void *key, *value, *ptr;
+	struct fd f;
 	int err;
 
 	if (CHECK_ATTR(BPF_MAP_LOOKUP_ELEM))
 		return -EINVAL;
 
+	f = fdget(ufd);
 	map = bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
@@ -213,14 +214,15 @@ static int map_update_elem(union bpf_attr *attr)
 	void __user *ukey = u64_to_ptr(attr->key);
 	void __user *uvalue = u64_to_ptr(attr->value);
 	int ufd = attr->map_fd;
-	struct fd f = fdget(ufd);
 	struct bpf_map *map;
 	void *key, *value;
+	struct fd f;
 	int err;
 
 	if (CHECK_ATTR(BPF_MAP_UPDATE_ELEM))
 		return -EINVAL;
 
+	f = fdget(ufd);
 	map = bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
@@ -265,14 +267,15 @@ static int map_delete_elem(union bpf_attr *attr)
 {
 	void __user *ukey = u64_to_ptr(attr->key);
 	int ufd = attr->map_fd;
-	struct fd f = fdget(ufd);
 	struct bpf_map *map;
+	struct fd f;
 	void *key;
 	int err;
 
 	if (CHECK_ATTR(BPF_MAP_DELETE_ELEM))
 		return -EINVAL;
 
+	f = fdget(ufd);
 	map = bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);
@@ -305,14 +308,15 @@ static int map_get_next_key(union bpf_attr *attr)
 	void __user *ukey = u64_to_ptr(attr->key);
 	void __user *unext_key = u64_to_ptr(attr->next_key);
 	int ufd = attr->map_fd;
-	struct fd f = fdget(ufd);
 	struct bpf_map *map;
 	void *key, *next_key;
+	struct fd f;
 	int err;
 
 	if (CHECK_ATTR(BPF_MAP_GET_NEXT_KEY))
 		return -EINVAL;
 
+	f = fdget(ufd);
 	map = bpf_map_get(f);
 	if (IS_ERR(map))
 		return PTR_ERR(map);

commit 2a36f0b92eb638dd023870574eb471b1c56be9ad
Author: Wang Nan <wangnan0@huawei.com>
Date:   Thu Aug 6 07:02:33 2015 +0000

    bpf: Make the bpf_prog_array_map more generic
    
    All the map backends are of generic nature. In order to avoid
    adding much special code into the eBPF core, rewrite part of
    the bpf_prog_array map code and make it more generic. So the
    new perf_event_array map type can reuse most of code with
    bpf_prog_array map and add fewer lines of special code.
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Signed-off-by: Kaixu Xia <xiakaixu@huawei.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index a1b14d197a4f..dc9b464fefa9 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -72,7 +72,7 @@ static int bpf_map_release(struct inode *inode, struct file *filp)
 		/* prog_array stores refcnt-ed bpf_prog pointers
 		 * release them all when user space closes prog_array_fd
 		 */
-		bpf_prog_array_map_clear(map);
+		bpf_fd_array_map_clear(map);
 
 	bpf_map_put(map);
 	return 0;

commit abf2e7d6e2e315b32ee00067a69aaad2cf4e1b3f
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Thu May 28 19:26:02 2015 -0700

    bpf: add missing rcu protection when releasing programs from prog_array
    
    Normally the program attachment place (like sockets, qdiscs) takes
    care of rcu protection and calls bpf_prog_put() after a grace period.
    The programs stored inside prog_array may not be attached anywhere,
    so prog_array needs to take care of preserving rcu protection.
    Otherwise bpf_tail_call() will race with bpf_prog_put().
    To solve that introduce bpf_prog_put_rcu() helper function and use
    it in 3 places where unattached program can decrement refcnt:
    closing program fd, deleting/replacing program in prog_array.
    
    Fixes: 04fd61ab36ec ("bpf: allow bpf programs to tail-call other bpf programs")
    Reported-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 98a69bd83069..a1b14d197a4f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -432,6 +432,23 @@ static void free_used_maps(struct bpf_prog_aux *aux)
 	kfree(aux->used_maps);
 }
 
+static void __prog_put_rcu(struct rcu_head *rcu)
+{
+	struct bpf_prog_aux *aux = container_of(rcu, struct bpf_prog_aux, rcu);
+
+	free_used_maps(aux);
+	bpf_prog_free(aux->prog);
+}
+
+/* version of bpf_prog_put() that is called after a grace period */
+void bpf_prog_put_rcu(struct bpf_prog *prog)
+{
+	if (atomic_dec_and_test(&prog->aux->refcnt)) {
+		prog->aux->prog = prog;
+		call_rcu(&prog->aux->rcu, __prog_put_rcu);
+	}
+}
+
 void bpf_prog_put(struct bpf_prog *prog)
 {
 	if (atomic_dec_and_test(&prog->aux->refcnt)) {
@@ -445,7 +462,7 @@ static int bpf_prog_release(struct inode *inode, struct file *filp)
 {
 	struct bpf_prog *prog = filp->private_data;
 
-	bpf_prog_put(prog);
+	bpf_prog_put_rcu(prog);
 	return 0;
 }
 

commit 04fd61ab36ec065e194ab5e74ae34a5240d992bb
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Tue May 19 16:59:03 2015 -0700

    bpf: allow bpf programs to tail-call other bpf programs
    
    introduce bpf_tail_call(ctx, &jmp_table, index) helper function
    which can be used from BPF programs like:
    int bpf_prog(struct pt_regs *ctx)
    {
      ...
      bpf_tail_call(ctx, &jmp_table, index);
      ...
    }
    that is roughly equivalent to:
    int bpf_prog(struct pt_regs *ctx)
    {
      ...
      if (jmp_table[index])
        return (*jmp_table[index])(ctx);
      ...
    }
    The important detail that it's not a normal call, but a tail call.
    The kernel stack is precious, so this helper reuses the current
    stack frame and jumps into another BPF program without adding
    extra call frame.
    It's trivially done in interpreter and a bit trickier in JITs.
    In case of x64 JIT the bigger part of generated assembler prologue
    is common for all programs, so it is simply skipped while jumping.
    Other JITs can do similar prologue-skipping optimization or
    do stack unwind before jumping into the next program.
    
    bpf_tail_call() arguments:
    ctx - context pointer
    jmp_table - one of BPF_MAP_TYPE_PROG_ARRAY maps used as the jump table
    index - index in the jump table
    
    Since all BPF programs are idenitified by file descriptor, user space
    need to populate the jmp_table with FDs of other BPF programs.
    If jmp_table[index] is empty the bpf_tail_call() doesn't jump anywhere
    and program execution continues as normal.
    
    New BPF_MAP_TYPE_PROG_ARRAY map type is introduced so that user space can
    populate this jmp_table array with FDs of other bpf programs.
    Programs can share the same jmp_table array or use multiple jmp_tables.
    
    The chain of tail calls can form unpredictable dynamic loops therefore
    tail_call_cnt is used to limit the number of calls and currently is set to 32.
    
    Use cases:
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    
    ==========
    - simplify complex programs by splitting them into a sequence of small programs
    
    - dispatch routine
      For tracing and future seccomp the program may be triggered on all system
      calls, but processing of syscall arguments will be different. It's more
      efficient to implement them as:
      int syscall_entry(struct seccomp_data *ctx)
      {
         bpf_tail_call(ctx, &syscall_jmp_table, ctx->nr /* syscall number */);
         ... default: process unknown syscall ...
      }
      int sys_write_event(struct seccomp_data *ctx) {...}
      int sys_read_event(struct seccomp_data *ctx) {...}
      syscall_jmp_table[__NR_write] = sys_write_event;
      syscall_jmp_table[__NR_read] = sys_read_event;
    
      For networking the program may call into different parsers depending on
      packet format, like:
      int packet_parser(struct __sk_buff *skb)
      {
         ... parse L2, L3 here ...
         __u8 ipproto = load_byte(skb, ... offsetof(struct iphdr, protocol));
         bpf_tail_call(skb, &ipproto_jmp_table, ipproto);
         ... default: process unknown protocol ...
      }
      int parse_tcp(struct __sk_buff *skb) {...}
      int parse_udp(struct __sk_buff *skb) {...}
      ipproto_jmp_table[IPPROTO_TCP] = parse_tcp;
      ipproto_jmp_table[IPPROTO_UDP] = parse_udp;
    
    - for TC use case, bpf_tail_call() allows to implement reclassify-like logic
    
    - bpf_map_update_elem/delete calls into BPF_MAP_TYPE_PROG_ARRAY jump table
      are atomic, so user space can build chains of BPF programs on the fly
    
    Implementation details:
    =======================
    - high performance of bpf_tail_call() is the goal.
      It could have been implemented without JIT changes as a wrapper on top of
      BPF_PROG_RUN() macro, but with two downsides:
      . all programs would have to pay performance penalty for this feature and
        tail call itself would be slower, since mandatory stack unwind, return,
        stack allocate would be done for every tailcall.
      . tailcall would be limited to programs running preempt_disabled, since
        generic 'void *ctx' doesn't have room for 'tail_call_cnt' and it would
        need to be either global per_cpu variable accessed by helper and by wrapper
        or global variable protected by locks.
    
      In this implementation x64 JIT bypasses stack unwind and jumps into the
      callee program after prologue.
    
    - bpf_prog_array_compatible() ensures that prog_type of callee and caller
      are the same and JITed/non-JITed flag is the same, since calling JITed
      program from non-JITed is invalid, since stack frames are different.
      Similarly calling kprobe type program from socket type program is invalid.
    
    - jump table is implemented as BPF_MAP_TYPE_PROG_ARRAY to reuse 'map'
      abstraction, its user space API and all of verifier logic.
      It's in the existing arraymap.c file, since several functions are
      shared with regular array map.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 3bae6c591914..98a69bd83069 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -68,6 +68,12 @@ static int bpf_map_release(struct inode *inode, struct file *filp)
 {
 	struct bpf_map *map = filp->private_data;
 
+	if (map->map_type == BPF_MAP_TYPE_PROG_ARRAY)
+		/* prog_array stores refcnt-ed bpf_prog pointers
+		 * release them all when user space closes prog_array_fd
+		 */
+		bpf_prog_array_map_clear(map);
+
 	bpf_map_put(map);
 	return 0;
 }
@@ -392,6 +398,19 @@ static void fixup_bpf_calls(struct bpf_prog *prog)
 			 */
 			BUG_ON(!prog->aux->ops->get_func_proto);
 
+			if (insn->imm == BPF_FUNC_tail_call) {
+				/* mark bpf_tail_call as different opcode
+				 * to avoid conditional branch in
+				 * interpeter for every normal call
+				 * and to prevent accidental JITing by
+				 * JIT compiler that doesn't support
+				 * bpf_tail_call yet
+				 */
+				insn->imm = 0;
+				insn->code |= BPF_X;
+				continue;
+			}
+
 			fn = prog->aux->ops->get_func_proto(insn->imm);
 			/* all functions that have prototype and verifier allowed
 			 * programs to call them, must be real in-kernel functions
@@ -532,7 +551,9 @@ static int bpf_prog_load(union bpf_attr *attr)
 	fixup_bpf_calls(prog);
 
 	/* eBPF program is ready to be JITed */
-	bpf_prog_select_runtime(prog);
+	err = bpf_prog_select_runtime(prog);
+	if (err < 0)
+		goto free_used_maps;
 
 	err = anon_inode_getfd("bpf-prog", &bpf_prog_fops, prog, O_RDWR | O_CLOEXEC);
 	if (err < 0)

commit 6c373ca89399c5a3f7ef210ad8f63dc3437da345
Merge: bb0fd7ab0986 9f9151412dd7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 15 09:00:47 2015 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Add BQL support to via-rhine, from Tino Reichardt.
    
     2) Integrate SWITCHDEV layer support into the DSA layer, so DSA drivers
        can support hw switch offloading.  From Floria Fainelli.
    
     3) Allow 'ip address' commands to initiate multicast group join/leave,
        from Madhu Challa.
    
     4) Many ipv4 FIB lookup optimizations from Alexander Duyck.
    
     5) Support EBPF in cls_bpf classifier and act_bpf action, from Daniel
        Borkmann.
    
     6) Remove the ugly compat support in ARP for ugly layers like ax25,
        rose, etc.  And use this to clean up the neigh layer, then use it to
        implement MPLS support.  All from Eric Biederman.
    
     7) Support L3 forwarding offloading in switches, from Scott Feldman.
    
     8) Collapse the LOCAL and MAIN ipv4 FIB tables when possible, to speed
        up route lookups even further.  From Alexander Duyck.
    
     9) Many improvements and bug fixes to the rhashtable implementation,
        from Herbert Xu and Thomas Graf.  In particular, in the case where
        an rhashtable user bulk adds a large number of items into an empty
        table, we expand the table much more sanely.
    
    10) Don't make the tcp_metrics hash table per-namespace, from Eric
        Biederman.
    
    11) Extend EBPF to access SKB fields, from Alexei Starovoitov.
    
    12) Split out new connection request sockets so that they can be
        established in the main hash table.  Much less false sharing since
        hash lookups go direct to the request sockets instead of having to
        go first to the listener then to the request socks hashed
        underneath.  From Eric Dumazet.
    
    13) Add async I/O support for crytpo AF_ALG sockets, from Tadeusz Struk.
    
    14) Support stable privacy address generation for RFC7217 in IPV6.  From
        Hannes Frederic Sowa.
    
    15) Hash network namespace into IP frag IDs, also from Hannes Frederic
        Sowa.
    
    16) Convert PTP get/set methods to use 64-bit time, from Richard
        Cochran.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1816 commits)
      fm10k: Bump driver version to 0.15.2
      fm10k: corrected VF multicast update
      fm10k: mbx_update_max_size does not drop all oversized messages
      fm10k: reset head instead of calling update_max_size
      fm10k: renamed mbx_tx_dropped to mbx_tx_oversized
      fm10k: update xcast mode before synchronizing multicast addresses
      fm10k: start service timer on probe
      fm10k: fix function header comment
      fm10k: comment next_vf_mbx flow
      fm10k: don't handle mailbox events in iov_event path and always process mailbox
      fm10k: use separate workqueue for fm10k driver
      fm10k: Set PF queues to unlimited bandwidth during virtualization
      fm10k: expose tx_timeout_count as an ethtool stat
      fm10k: only increment tx_timeout_count in Tx hang path
      fm10k: remove extraneous "Reset interface" message
      fm10k: separate PF only stats so that VF does not display them
      fm10k: use hw->mac.max_queues for stats
      fm10k: only show actual queues, not the maximum in hardware
      fm10k: allow creation of VLAN on default vid
      fm10k: fix unused warnings
      ...

commit 2541517c32be2531e0da59dfd7efc1ce844644f5
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Mar 25 12:49:20 2015 -0700

    tracing, perf: Implement BPF programs attached to kprobes
    
    BPF programs, attached to kprobes, provide a safe way to execute
    user-defined BPF byte-code programs without being able to crash or
    hang the kernel in any way. The BPF engine makes sure that such
    programs have a finite execution time and that they cannot break
    out of their sandbox.
    
    The user interface is to attach to a kprobe via the perf syscall:
    
            struct perf_event_attr attr = {
                    .type   = PERF_TYPE_TRACEPOINT,
                    .config = event_id,
                    ...
            };
    
            event_fd = perf_event_open(&attr,...);
            ioctl(event_fd, PERF_EVENT_IOC_SET_BPF, prog_fd);
    
    'prog_fd' is a file descriptor associated with BPF program
    previously loaded.
    
    'event_id' is an ID of the kprobe created.
    
    Closing 'event_fd':
    
            close(event_fd);
    
    ... automatically detaches BPF program from it.
    
    BPF programs can call in-kernel helper functions to:
    
      - lookup/update/delete elements in maps
    
      - probe_read - wraper of probe_kernel_read() used to access any
        kernel data structures
    
    BPF programs receive 'struct pt_regs *' as an input ('struct pt_regs' is
    architecture dependent) and return 0 to ignore the event and 1 to store
    kprobe event into the ring buffer.
    
    Note, kprobes are a fundamentally _not_ a stable kernel ABI,
    so BPF programs attached to kprobes must be recompiled for
    every kernel version and user must supply correct LINUX_VERSION_CODE
    in attr.kern_version during bpf_prog_load() call.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1427312966-8434-4-git-send-email-ast@plumgrid.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 536edc2be307..504c10b990ef 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -16,6 +16,7 @@
 #include <linux/file.h>
 #include <linux/license.h>
 #include <linux/filter.h>
+#include <linux/version.h>
 
 static LIST_HEAD(bpf_map_types);
 
@@ -467,7 +468,7 @@ struct bpf_prog *bpf_prog_get(u32 ufd)
 }
 
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD log_buf
+#define	BPF_PROG_LOAD_LAST_FIELD kern_version
 
 static int bpf_prog_load(union bpf_attr *attr)
 {
@@ -492,6 +493,10 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (attr->insn_cnt >= BPF_MAXINSNS)
 		return -EINVAL;
 
+	if (type == BPF_PROG_TYPE_KPROBE &&
+	    attr->kern_version != LINUX_VERSION_CODE)
+		return -EINVAL;
+
 	/* plain bpf_prog allocation */
 	prog = bpf_prog_alloc(bpf_prog_size(attr->insn_cnt), GFP_USER);
 	if (!prog)

commit 9bac3d6d548e5cc925570b263f35b70a00a00ffd
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Mar 13 11:57:42 2015 -0700

    bpf: allow extended BPF programs access skb fields
    
    introduce user accessible mirror of in-kernel 'struct sk_buff':
    struct __sk_buff {
        __u32 len;
        __u32 pkt_type;
        __u32 mark;
        __u32 queue_mapping;
    };
    
    bpf programs can do:
    
    int bpf_prog(struct __sk_buff *skb)
    {
        __u32 var = skb->pkt_type;
    
    which will be compiled to bpf assembler as:
    
    dst_reg = *(u32 *)(src_reg + 4) // 4 == offsetof(struct __sk_buff, pkt_type)
    
    bpf verifier will check validity of access and will convert it to:
    
    dst_reg = *(u8 *)(src_reg + offsetof(struct sk_buff, __pkt_type_offset))
    dst_reg &= 7
    
    since skb->pkt_type is a bitfield.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 669719ccc9ee..ea75c654af1b 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -519,7 +519,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 		goto free_prog;
 
 	/* run eBPF verifier */
-	err = bpf_check(prog, attr);
+	err = bpf_check(&prog, attr);
 	if (err < 0)
 		goto free_used_maps;
 

commit e2e9b6541dd4b31848079da80fe2253daaafb549
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Mar 1 12:31:48 2015 +0100

    cls_bpf: add initial eBPF support for programmable classifiers
    
    This work extends the "classic" BPF programmable tc classifier by
    extending its scope also to native eBPF code!
    
    This allows for user space to implement own custom, 'safe' C like
    classifiers (or whatever other frontend language LLVM et al may
    provide in future), that can then be compiled with the LLVM eBPF
    backend to an eBPF elf file. The result of this can be loaded into
    the kernel via iproute2's tc. In the kernel, they can be JITed on
    major archs and thus run in native performance.
    
    Simple, minimal toy example to demonstrate the workflow:
    
      #include <linux/ip.h>
      #include <linux/if_ether.h>
      #include <linux/bpf.h>
    
      #include "tc_bpf_api.h"
    
      __section("classify")
      int cls_main(struct sk_buff *skb)
      {
        return (0x800 << 16) | load_byte(skb, ETH_HLEN + __builtin_offsetof(struct iphdr, tos));
      }
    
      char __license[] __section("license") = "GPL";
    
    The classifier can then be compiled into eBPF opcodes and loaded
    via tc, for example:
    
      clang -O2 -emit-llvm -c cls.c -o - | llc -march=bpf -filetype=obj -o cls.o
      tc filter add dev em1 parent 1: bpf cls.o [...]
    
    As it has been demonstrated, the scope can even reach up to a fully
    fledged flow dissector (similarly as in samples/bpf/sockex2_kern.c).
    
    For tc, maps are allowed to be used, but from kernel context only,
    in other words, eBPF code can keep state across filter invocations.
    In future, we perhaps may reattach from a different application to
    those maps e.g., to read out collected statistics/state.
    
    Similarly as in socket filters, we may extend functionality for eBPF
    classifiers over time depending on the use cases. For that purpose,
    cls_bpf programs are using BPF_PROG_TYPE_SCHED_CLS program type, so
    we can allow additional functions/accessors (e.g. an ABI compatible
    offset translation to skb fields/metadata). For an initial cls_bpf
    support, we allow the same set of helper functions as eBPF socket
    filters, but we could diverge at some point in time w/o problem.
    
    I was wondering whether cls_bpf and act_bpf could share C programs,
    I can imagine that at some point, we introduce i) further common
    handlers for both (or even beyond their scope), and/or if truly needed
    ii) some restricted function space for each of them. Both can be
    abstracted easily through struct bpf_verifier_ops in future.
    
    The context of cls_bpf versus act_bpf is slightly different though:
    a cls_bpf program will return a specific classid whereas act_bpf a
    drop/non-drop return code, latter may also in future mangle skbs.
    That said, we can surely have a "classify" and "action" section in
    a single object file, or considered mentioned constraint add a
    possibility of a shared section.
    
    The workflow for getting native eBPF running from tc [1] is as
    follows: for f_bpf, I've added a slightly modified ELF parser code
    from Alexei's kernel sample, which reads out the LLVM compiled
    object, sets up maps (and dynamically fixes up map fds) if any, and
    loads the eBPF instructions all centrally through the bpf syscall.
    
    The resulting fd from the loaded program itself is being passed down
    to cls_bpf, which looks up struct bpf_prog from the fd store, and
    holds reference, so that it stays available also after tc program
    lifetime. On tc filter destruction, it will then drop its reference.
    
    Moreover, I've also added the optional possibility to annotate an
    eBPF filter with a name (e.g. path to object file, or something
    else if preferred) so that when tc dumps currently installed filters,
    some more context can be given to an admin for a given instance (as
    opposed to just the file descriptor number).
    
    Last but not least, bpf_prog_get() and bpf_prog_put() needed to be
    exported, so that eBPF can be used from cls_bpf built as a module.
    Thanks to 60a3b2253c41 ("net: bpf: make eBPF interpreter images
    read-only") I think this is of no concern since anything wanting to
    alter eBPF opcode after verification stage would crash the kernel.
    
      [1] http://git.breakpoint.cc/cgit/dborkman/iproute2.git/log/?h=ebpf
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Jamal Hadi Salim <jhs@mojatatu.com>
    Cc: Jiri Pirko <jiri@resnulli.us>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0d69449acbd0..669719ccc9ee 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -419,6 +419,7 @@ void bpf_prog_put(struct bpf_prog *prog)
 		bpf_prog_free(prog);
 	}
 }
+EXPORT_SYMBOL_GPL(bpf_prog_put);
 
 static int bpf_prog_release(struct inode *inode, struct file *filp)
 {
@@ -466,6 +467,7 @@ struct bpf_prog *bpf_prog_get(u32 ufd)
 	fdput(f);
 	return prog;
 }
+EXPORT_SYMBOL_GPL(bpf_prog_get);
 
 /* last field in 'union bpf_attr' used by this command */
 #define	BPF_PROG_LOAD_LAST_FIELD log_buf

commit 24701ecea76b0b93bd9667486934ec310825f558
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Sun Mar 1 12:31:47 2015 +0100

    ebpf: move read-only fields to bpf_prog and shrink bpf_prog_aux
    
    is_gpl_compatible and prog_type should be moved directly into bpf_prog
    as they stay immutable during bpf_prog's lifetime, are core attributes
    and they can be locked as read-only later on via bpf_prog_select_runtime().
    
    With a bit of rearranging, this also allows us to shrink bpf_prog_aux
    to exactly 1 cacheline.
    
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 536edc2be307..0d69449acbd0 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -354,10 +354,11 @@ static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
 	list_for_each_entry(tl, &bpf_prog_types, list_node) {
 		if (tl->type == type) {
 			prog->aux->ops = tl->ops;
-			prog->aux->prog_type = type;
+			prog->type = type;
 			return 0;
 		}
 	}
+
 	return -EINVAL;
 }
 
@@ -508,7 +509,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 	prog->jited = false;
 
 	atomic_set(&prog->aux->refcnt, 1);
-	prog->aux->is_gpl_compatible = is_gpl;
+	prog->gpl_compatible = is_gpl;
 
 	/* find program type: socket_filter vs tracing_filter */
 	err = find_prog_type(type, prog);
@@ -517,7 +518,6 @@ static int bpf_prog_load(union bpf_attr *attr)
 
 	/* run eBPF verifier */
 	err = bpf_check(prog, attr);
-
 	if (err < 0)
 		goto free_used_maps;
 
@@ -528,7 +528,6 @@ static int bpf_prog_load(union bpf_attr *attr)
 	bpf_prog_select_runtime(prog);
 
 	err = anon_inode_getfd("bpf-prog", &bpf_prog_fops, prog, O_RDWR | O_CLOEXEC);
-
 	if (err < 0)
 		/* failed to allocate fd */
 		goto free_used_maps;

commit 8ebe667c41e054384df19f2f382bc415badfaee1
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Thu Jan 22 17:11:08 2015 -0800

    bpf: rcu lock must not be held when calling copy_to_user()
    
    BUG: sleeping function called from invalid context at mm/memory.c:3732
    in_atomic(): 0, irqs_disabled(): 0, pid: 671, name: test_maps
    1 lock held by test_maps/671:
     #0:  (rcu_read_lock){......}, at: [<0000000000264190>] map_lookup_elem+0xe8/0x260
    Call Trace:
    ([<0000000000115b7e>] show_trace+0x12e/0x150)
     [<0000000000115c40>] show_stack+0xa0/0x100
     [<00000000009b163c>] dump_stack+0x74/0xc8
     [<000000000017424a>] ___might_sleep+0x23a/0x248
     [<00000000002b58e8>] might_fault+0x70/0xe8
     [<0000000000264230>] map_lookup_elem+0x188/0x260
     [<0000000000264716>] SyS_bpf+0x20e/0x840
    
    Fix it by allocating temporary buffer to store map element value.
    
    Fixes: db20fd2b0108 ("bpf: add lookup/update/delete/iterate methods to BPF maps")
    Reported-by: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Acked-by: Daniel Borkmann <dborkman@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 088ac0b1b106..536edc2be307 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -150,7 +150,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 	int ufd = attr->map_fd;
 	struct fd f = fdget(ufd);
 	struct bpf_map *map;
-	void *key, *value;
+	void *key, *value, *ptr;
 	int err;
 
 	if (CHECK_ATTR(BPF_MAP_LOOKUP_ELEM))
@@ -169,20 +169,29 @@ static int map_lookup_elem(union bpf_attr *attr)
 	if (copy_from_user(key, ukey, map->key_size) != 0)
 		goto free_key;
 
-	err = -ENOENT;
-	rcu_read_lock();
-	value = map->ops->map_lookup_elem(map, key);
+	err = -ENOMEM;
+	value = kmalloc(map->value_size, GFP_USER);
 	if (!value)
-		goto err_unlock;
+		goto free_key;
+
+	rcu_read_lock();
+	ptr = map->ops->map_lookup_elem(map, key);
+	if (ptr)
+		memcpy(value, ptr, map->value_size);
+	rcu_read_unlock();
+
+	err = -ENOENT;
+	if (!ptr)
+		goto free_value;
 
 	err = -EFAULT;
 	if (copy_to_user(uvalue, value, map->value_size) != 0)
-		goto err_unlock;
+		goto free_value;
 
 	err = 0;
 
-err_unlock:
-	rcu_read_unlock();
+free_value:
+	kfree(value);
 free_key:
 	kfree(key);
 err_put:

commit a1854d6ac0008518bfc45e791172ad250999c2a2
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Thu Nov 13 17:36:47 2014 -0800

    bpf: fix BPF_MAP_LOOKUP_ELEM command return code
    
    fix errno of BPF_MAP_LOOKUP_ELEM command as bpf manpage
    described it in commit b4fc1a460f30("Merge branch 'bpf-next'"):
    -----
    BPF_MAP_LOOKUP_ELEM
        int bpf_lookup_elem(int fd, void *key, void *value)
        {
            union bpf_attr attr = {
                .map_fd = fd,
                .key = ptr_to_u64(key),
                .value = ptr_to_u64(value),
            };
    
            return bpf(BPF_MAP_LOOKUP_ELEM, &attr, sizeof(attr));
        }
        bpf() syscall looks up an element with given key in  a  map  fd.
        If  element  is found it returns zero and stores element's value
        into value.  If element is not found  it  returns  -1  and  sets
        errno to ENOENT.
    
    and further down in manpage:
    
       ENOENT For BPF_MAP_LOOKUP_ELEM or BPF_MAP_DELETE_ELEM,  indicates  that
              element with given key was not found.
    -----
    
    In general all BPF commands return ENOENT when map element is not found
    (including BPF_MAP_GET_NEXT_KEY and BPF_MAP_UPDATE_ELEM with
     flags == BPF_MAP_UPDATE_ONLY)
    
    Subsequent patch adds a testsuite to check return values for all of
    these combinations.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index c0d03bf317a2..088ac0b1b106 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -169,7 +169,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 	if (copy_from_user(key, ukey, map->key_size) != 0)
 		goto free_key;
 
-	err = -ESRCH;
+	err = -ENOENT;
 	rcu_read_lock();
 	value = map->ops->map_lookup_elem(map, key);
 	if (!value)

commit 3274f52073d88b62f3c5ace82ae9d48546232e72
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Thu Nov 13 17:36:44 2014 -0800

    bpf: add 'flags' attribute to BPF_MAP_UPDATE_ELEM command
    
    the current meaning of BPF_MAP_UPDATE_ELEM syscall command is:
    either update existing map element or create a new one.
    Initially the plan was to add a new command to handle the case of
    'create new element if it didn't exist', but 'flags' style looks
    cleaner and overall diff is much smaller (more code reused), so add 'flags'
    attribute to BPF_MAP_UPDATE_ELEM command with the following meaning:
     #define BPF_ANY        0 /* create new element or update existing */
     #define BPF_NOEXIST    1 /* create new element if it didn't exist */
     #define BPF_EXIST      2 /* update existing element */
    
    bpf_update_elem(fd, key, value, BPF_NOEXIST) call can fail with EEXIST
    if element already exists.
    
    bpf_update_elem(fd, key, value, BPF_EXIST) can fail with ENOENT
    if element doesn't exist.
    
    Userspace will call it as:
    int bpf_update_elem(int fd, void *key, void *value, __u64 flags)
    {
        union bpf_attr attr = {
            .map_fd = fd,
            .key = ptr_to_u64(key),
            .value = ptr_to_u64(value),
            .flags = flags;
        };
    
        return bpf(BPF_MAP_UPDATE_ELEM, &attr, sizeof(attr));
    }
    
    First two bits of 'flags' are used to encode style of bpf_update_elem() command.
    Bits 2-63 are reserved for future use.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index ba61c8c16032..c0d03bf317a2 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -190,7 +190,7 @@ static int map_lookup_elem(union bpf_attr *attr)
 	return err;
 }
 
-#define BPF_MAP_UPDATE_ELEM_LAST_FIELD value
+#define BPF_MAP_UPDATE_ELEM_LAST_FIELD flags
 
 static int map_update_elem(union bpf_attr *attr)
 {
@@ -231,7 +231,7 @@ static int map_update_elem(union bpf_attr *attr)
 	 * therefore all map accessors rely on this fact, so do the same here
 	 */
 	rcu_read_lock();
-	err = map->ops->map_update_elem(map, key, value);
+	err = map->ops->map_update_elem(map, key, value, attr->flags);
 	rcu_read_unlock();
 
 free_value:

commit cbd357008604925355ae7b54a09137dabb81b580
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Sep 26 00:17:03 2014 -0700

    bpf: verifier (add ability to receive verification log)
    
    add optional attributes for BPF_PROG_LOAD syscall:
    union bpf_attr {
        struct {
            ...
            __u32         log_level; /* verbosity level of eBPF verifier */
            __u32         log_size;  /* size of user buffer */
            __aligned_u64 log_buf;   /* user supplied 'char *buffer' */
        };
    };
    
    when log_level > 0 the verifier will return its verification log in the user
    supplied buffer 'log_buf' which can be used by program author to analyze why
    verifier rejected given program.
    
    'Understanding eBPF verifier messages' section of Documentation/networking/filter.txt
    provides several examples of these messages, like the program:
    
      BPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),
      BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),
      BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),
      BPF_LD_MAP_FD(BPF_REG_1, 0),
      BPF_CALL_FUNC(BPF_FUNC_map_lookup_elem),
      BPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, 1),
      BPF_ST_MEM(BPF_DW, BPF_REG_0, 4, 0),
      BPF_EXIT_INSN(),
    
    will be rejected with the following multi-line message in log_buf:
    
      0: (7a) *(u64 *)(r10 -8) = 0
      1: (bf) r2 = r10
      2: (07) r2 += -8
      3: (b7) r1 = 0
      4: (85) call 1
      5: (15) if r0 == 0x0 goto pc+1
       R0=map_ptr R10=fp
      6: (7a) *(u64 *)(r0 +4) = 0
      misaligned access off 4 size 8
    
    The format of the output can change at any time as verifier evolves.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 74b3628c5fdb..ba61c8c16032 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -458,7 +458,7 @@ struct bpf_prog *bpf_prog_get(u32 ufd)
 }
 
 /* last field in 'union bpf_attr' used by this command */
-#define	BPF_PROG_LOAD_LAST_FIELD license
+#define	BPF_PROG_LOAD_LAST_FIELD log_buf
 
 static int bpf_prog_load(union bpf_attr *attr)
 {

commit 51580e798cb61b0fc63fa3aa6c5c975375aa0550
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Sep 26 00:17:02 2014 -0700

    bpf: verifier (add docs)
    
    this patch adds all of eBPF verfier documentation and empty bpf_check()
    
    The end goal for the verifier is to statically check safety of the program.
    
    Verifier will catch:
    - loops
    - out of range jumps
    - unreachable instructions
    - invalid instructions
    - uninitialized register access
    - uninitialized stack access
    - misaligned stack access
    - out of range stack access
    - invalid calling convention
    
    More details in Documentation/networking/filter.txt
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index b513659d120f..74b3628c5fdb 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -507,7 +507,7 @@ static int bpf_prog_load(union bpf_attr *attr)
 		goto free_prog;
 
 	/* run eBPF verifier */
-	/* err = bpf_check(prog, tb); */
+	err = bpf_check(prog, attr);
 
 	if (err < 0)
 		goto free_used_maps;

commit 0a542a86d73b1577e7d4f55fc95dcffd3fe62643
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Sep 26 00:17:01 2014 -0700

    bpf: handle pseudo BPF_CALL insn
    
    in native eBPF programs userspace is using pseudo BPF_CALL instructions
    which encode one of 'enum bpf_func_id' inside insn->imm field.
    Verifier checks that program using correct function arguments to given func_id.
    If all checks passed, kernel needs to fixup BPF_CALL->imm fields by
    replacing func_id with in-kernel function pointer.
    eBPF interpreter just calls the function.
    
    In-kernel eBPF users continue to use generic BPF_CALL.
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0afb4eaa1887..b513659d120f 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -357,6 +357,40 @@ void bpf_register_prog_type(struct bpf_prog_type_list *tl)
 	list_add(&tl->list_node, &bpf_prog_types);
 }
 
+/* fixup insn->imm field of bpf_call instructions:
+ * if (insn->imm == BPF_FUNC_map_lookup_elem)
+ *      insn->imm = bpf_map_lookup_elem - __bpf_call_base;
+ * else if (insn->imm == BPF_FUNC_map_update_elem)
+ *      insn->imm = bpf_map_update_elem - __bpf_call_base;
+ * else ...
+ *
+ * this function is called after eBPF program passed verification
+ */
+static void fixup_bpf_calls(struct bpf_prog *prog)
+{
+	const struct bpf_func_proto *fn;
+	int i;
+
+	for (i = 0; i < prog->len; i++) {
+		struct bpf_insn *insn = &prog->insnsi[i];
+
+		if (insn->code == (BPF_JMP | BPF_CALL)) {
+			/* we reach here when program has bpf_call instructions
+			 * and it passed bpf_check(), means that
+			 * ops->get_func_proto must have been supplied, check it
+			 */
+			BUG_ON(!prog->aux->ops->get_func_proto);
+
+			fn = prog->aux->ops->get_func_proto(insn->imm);
+			/* all functions that have prototype and verifier allowed
+			 * programs to call them, must be real in-kernel functions
+			 */
+			BUG_ON(!fn->func);
+			insn->imm = fn->func - __bpf_call_base;
+		}
+	}
+}
+
 /* drop refcnt on maps used by eBPF program and free auxilary data */
 static void free_used_maps(struct bpf_prog_aux *aux)
 {
@@ -478,6 +512,9 @@ static int bpf_prog_load(union bpf_attr *attr)
 	if (err < 0)
 		goto free_used_maps;
 
+	/* fixup BPF_CALL->imm field */
+	fixup_bpf_calls(prog);
+
 	/* eBPF program is ready to be JITed */
 	bpf_prog_select_runtime(prog);
 

commit 09756af46893c18839062976c3252e93a1beeba7
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Sep 26 00:17:00 2014 -0700

    bpf: expand BPF syscall with program load/unload
    
    eBPF programs are similar to kernel modules. They are loaded by the user
    process and automatically unloaded when process exits. Each eBPF program is
    a safe run-to-completion set of instructions. eBPF verifier statically
    determines that the program terminates and is safe to execute.
    
    The following syscall wrapper can be used to load the program:
    int bpf_prog_load(enum bpf_prog_type prog_type,
                      const struct bpf_insn *insns, int insn_cnt,
                      const char *license)
    {
        union bpf_attr attr = {
            .prog_type = prog_type,
            .insns = ptr_to_u64(insns),
            .insn_cnt = insn_cnt,
            .license = ptr_to_u64(license),
        };
    
        return bpf(BPF_PROG_LOAD, &attr, sizeof(attr));
    }
    where 'insns' is an array of eBPF instructions and 'license' is a string
    that must be GPL compatible to call helper functions marked gpl_only
    
    Upon succesful load the syscall returns prog_fd.
    Use close(prog_fd) to unload the program.
    
    User space tests and examples follow in the later patches
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index f94349ecaf61..0afb4eaa1887 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -14,6 +14,8 @@
 #include <linux/slab.h>
 #include <linux/anon_inodes.h>
 #include <linux/file.h>
+#include <linux/license.h>
+#include <linux/filter.h>
 
 static LIST_HEAD(bpf_map_types);
 
@@ -334,6 +336,166 @@ static int map_get_next_key(union bpf_attr *attr)
 	return err;
 }
 
+static LIST_HEAD(bpf_prog_types);
+
+static int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)
+{
+	struct bpf_prog_type_list *tl;
+
+	list_for_each_entry(tl, &bpf_prog_types, list_node) {
+		if (tl->type == type) {
+			prog->aux->ops = tl->ops;
+			prog->aux->prog_type = type;
+			return 0;
+		}
+	}
+	return -EINVAL;
+}
+
+void bpf_register_prog_type(struct bpf_prog_type_list *tl)
+{
+	list_add(&tl->list_node, &bpf_prog_types);
+}
+
+/* drop refcnt on maps used by eBPF program and free auxilary data */
+static void free_used_maps(struct bpf_prog_aux *aux)
+{
+	int i;
+
+	for (i = 0; i < aux->used_map_cnt; i++)
+		bpf_map_put(aux->used_maps[i]);
+
+	kfree(aux->used_maps);
+}
+
+void bpf_prog_put(struct bpf_prog *prog)
+{
+	if (atomic_dec_and_test(&prog->aux->refcnt)) {
+		free_used_maps(prog->aux);
+		bpf_prog_free(prog);
+	}
+}
+
+static int bpf_prog_release(struct inode *inode, struct file *filp)
+{
+	struct bpf_prog *prog = filp->private_data;
+
+	bpf_prog_put(prog);
+	return 0;
+}
+
+static const struct file_operations bpf_prog_fops = {
+        .release = bpf_prog_release,
+};
+
+static struct bpf_prog *get_prog(struct fd f)
+{
+	struct bpf_prog *prog;
+
+	if (!f.file)
+		return ERR_PTR(-EBADF);
+
+	if (f.file->f_op != &bpf_prog_fops) {
+		fdput(f);
+		return ERR_PTR(-EINVAL);
+	}
+
+	prog = f.file->private_data;
+
+	return prog;
+}
+
+/* called by sockets/tracing/seccomp before attaching program to an event
+ * pairs with bpf_prog_put()
+ */
+struct bpf_prog *bpf_prog_get(u32 ufd)
+{
+	struct fd f = fdget(ufd);
+	struct bpf_prog *prog;
+
+	prog = get_prog(f);
+
+	if (IS_ERR(prog))
+		return prog;
+
+	atomic_inc(&prog->aux->refcnt);
+	fdput(f);
+	return prog;
+}
+
+/* last field in 'union bpf_attr' used by this command */
+#define	BPF_PROG_LOAD_LAST_FIELD license
+
+static int bpf_prog_load(union bpf_attr *attr)
+{
+	enum bpf_prog_type type = attr->prog_type;
+	struct bpf_prog *prog;
+	int err;
+	char license[128];
+	bool is_gpl;
+
+	if (CHECK_ATTR(BPF_PROG_LOAD))
+		return -EINVAL;
+
+	/* copy eBPF program license from user space */
+	if (strncpy_from_user(license, u64_to_ptr(attr->license),
+			      sizeof(license) - 1) < 0)
+		return -EFAULT;
+	license[sizeof(license) - 1] = 0;
+
+	/* eBPF programs must be GPL compatible to use GPL-ed functions */
+	is_gpl = license_is_gpl_compatible(license);
+
+	if (attr->insn_cnt >= BPF_MAXINSNS)
+		return -EINVAL;
+
+	/* plain bpf_prog allocation */
+	prog = bpf_prog_alloc(bpf_prog_size(attr->insn_cnt), GFP_USER);
+	if (!prog)
+		return -ENOMEM;
+
+	prog->len = attr->insn_cnt;
+
+	err = -EFAULT;
+	if (copy_from_user(prog->insns, u64_to_ptr(attr->insns),
+			   prog->len * sizeof(struct bpf_insn)) != 0)
+		goto free_prog;
+
+	prog->orig_prog = NULL;
+	prog->jited = false;
+
+	atomic_set(&prog->aux->refcnt, 1);
+	prog->aux->is_gpl_compatible = is_gpl;
+
+	/* find program type: socket_filter vs tracing_filter */
+	err = find_prog_type(type, prog);
+	if (err < 0)
+		goto free_prog;
+
+	/* run eBPF verifier */
+	/* err = bpf_check(prog, tb); */
+
+	if (err < 0)
+		goto free_used_maps;
+
+	/* eBPF program is ready to be JITed */
+	bpf_prog_select_runtime(prog);
+
+	err = anon_inode_getfd("bpf-prog", &bpf_prog_fops, prog, O_RDWR | O_CLOEXEC);
+
+	if (err < 0)
+		/* failed to allocate fd */
+		goto free_used_maps;
+
+	return err;
+
+free_used_maps:
+	free_used_maps(prog->aux);
+free_prog:
+	bpf_prog_free(prog);
+	return err;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -395,6 +557,9 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_MAP_GET_NEXT_KEY:
 		err = map_get_next_key(&attr);
 		break;
+	case BPF_PROG_LOAD:
+		err = bpf_prog_load(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit db20fd2b01087bdfbe30bce314a198eefedcc42e
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Sep 26 00:16:59 2014 -0700

    bpf: add lookup/update/delete/iterate methods to BPF maps
    
    'maps' is a generic storage of different types for sharing data between kernel
    and userspace.
    
    The maps are accessed from user space via BPF syscall, which has commands:
    
    - create a map with given type and attributes
      fd = bpf(BPF_MAP_CREATE, union bpf_attr *attr, u32 size)
      returns fd or negative error
    
    - lookup key in a given map referenced by fd
      err = bpf(BPF_MAP_LOOKUP_ELEM, union bpf_attr *attr, u32 size)
      using attr->map_fd, attr->key, attr->value
      returns zero and stores found elem into value or negative error
    
    - create or update key/value pair in a given map
      err = bpf(BPF_MAP_UPDATE_ELEM, union bpf_attr *attr, u32 size)
      using attr->map_fd, attr->key, attr->value
      returns zero or negative error
    
    - find and delete element by key in a given map
      err = bpf(BPF_MAP_DELETE_ELEM, union bpf_attr *attr, u32 size)
      using attr->map_fd, attr->key
    
    - iterate map elements (based on input key return next_key)
      err = bpf(BPF_MAP_GET_NEXT_KEY, union bpf_attr *attr, u32 size)
      using attr->map_fd, attr->key, attr->next_key
    
    - close(fd) deletes the map
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 428a0e23adc0..f94349ecaf61 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -13,6 +13,7 @@
 #include <linux/syscalls.h>
 #include <linux/slab.h>
 #include <linux/anon_inodes.h>
+#include <linux/file.h>
 
 static LIST_HEAD(bpf_map_types);
 
@@ -111,6 +112,228 @@ static int map_create(union bpf_attr *attr)
 	return err;
 }
 
+/* if error is returned, fd is released.
+ * On success caller should complete fd access with matching fdput()
+ */
+struct bpf_map *bpf_map_get(struct fd f)
+{
+	struct bpf_map *map;
+
+	if (!f.file)
+		return ERR_PTR(-EBADF);
+
+	if (f.file->f_op != &bpf_map_fops) {
+		fdput(f);
+		return ERR_PTR(-EINVAL);
+	}
+
+	map = f.file->private_data;
+
+	return map;
+}
+
+/* helper to convert user pointers passed inside __aligned_u64 fields */
+static void __user *u64_to_ptr(__u64 val)
+{
+	return (void __user *) (unsigned long) val;
+}
+
+/* last field in 'union bpf_attr' used by this command */
+#define BPF_MAP_LOOKUP_ELEM_LAST_FIELD value
+
+static int map_lookup_elem(union bpf_attr *attr)
+{
+	void __user *ukey = u64_to_ptr(attr->key);
+	void __user *uvalue = u64_to_ptr(attr->value);
+	int ufd = attr->map_fd;
+	struct fd f = fdget(ufd);
+	struct bpf_map *map;
+	void *key, *value;
+	int err;
+
+	if (CHECK_ATTR(BPF_MAP_LOOKUP_ELEM))
+		return -EINVAL;
+
+	map = bpf_map_get(f);
+	if (IS_ERR(map))
+		return PTR_ERR(map);
+
+	err = -ENOMEM;
+	key = kmalloc(map->key_size, GFP_USER);
+	if (!key)
+		goto err_put;
+
+	err = -EFAULT;
+	if (copy_from_user(key, ukey, map->key_size) != 0)
+		goto free_key;
+
+	err = -ESRCH;
+	rcu_read_lock();
+	value = map->ops->map_lookup_elem(map, key);
+	if (!value)
+		goto err_unlock;
+
+	err = -EFAULT;
+	if (copy_to_user(uvalue, value, map->value_size) != 0)
+		goto err_unlock;
+
+	err = 0;
+
+err_unlock:
+	rcu_read_unlock();
+free_key:
+	kfree(key);
+err_put:
+	fdput(f);
+	return err;
+}
+
+#define BPF_MAP_UPDATE_ELEM_LAST_FIELD value
+
+static int map_update_elem(union bpf_attr *attr)
+{
+	void __user *ukey = u64_to_ptr(attr->key);
+	void __user *uvalue = u64_to_ptr(attr->value);
+	int ufd = attr->map_fd;
+	struct fd f = fdget(ufd);
+	struct bpf_map *map;
+	void *key, *value;
+	int err;
+
+	if (CHECK_ATTR(BPF_MAP_UPDATE_ELEM))
+		return -EINVAL;
+
+	map = bpf_map_get(f);
+	if (IS_ERR(map))
+		return PTR_ERR(map);
+
+	err = -ENOMEM;
+	key = kmalloc(map->key_size, GFP_USER);
+	if (!key)
+		goto err_put;
+
+	err = -EFAULT;
+	if (copy_from_user(key, ukey, map->key_size) != 0)
+		goto free_key;
+
+	err = -ENOMEM;
+	value = kmalloc(map->value_size, GFP_USER);
+	if (!value)
+		goto free_key;
+
+	err = -EFAULT;
+	if (copy_from_user(value, uvalue, map->value_size) != 0)
+		goto free_value;
+
+	/* eBPF program that use maps are running under rcu_read_lock(),
+	 * therefore all map accessors rely on this fact, so do the same here
+	 */
+	rcu_read_lock();
+	err = map->ops->map_update_elem(map, key, value);
+	rcu_read_unlock();
+
+free_value:
+	kfree(value);
+free_key:
+	kfree(key);
+err_put:
+	fdput(f);
+	return err;
+}
+
+#define BPF_MAP_DELETE_ELEM_LAST_FIELD key
+
+static int map_delete_elem(union bpf_attr *attr)
+{
+	void __user *ukey = u64_to_ptr(attr->key);
+	int ufd = attr->map_fd;
+	struct fd f = fdget(ufd);
+	struct bpf_map *map;
+	void *key;
+	int err;
+
+	if (CHECK_ATTR(BPF_MAP_DELETE_ELEM))
+		return -EINVAL;
+
+	map = bpf_map_get(f);
+	if (IS_ERR(map))
+		return PTR_ERR(map);
+
+	err = -ENOMEM;
+	key = kmalloc(map->key_size, GFP_USER);
+	if (!key)
+		goto err_put;
+
+	err = -EFAULT;
+	if (copy_from_user(key, ukey, map->key_size) != 0)
+		goto free_key;
+
+	rcu_read_lock();
+	err = map->ops->map_delete_elem(map, key);
+	rcu_read_unlock();
+
+free_key:
+	kfree(key);
+err_put:
+	fdput(f);
+	return err;
+}
+
+/* last field in 'union bpf_attr' used by this command */
+#define BPF_MAP_GET_NEXT_KEY_LAST_FIELD next_key
+
+static int map_get_next_key(union bpf_attr *attr)
+{
+	void __user *ukey = u64_to_ptr(attr->key);
+	void __user *unext_key = u64_to_ptr(attr->next_key);
+	int ufd = attr->map_fd;
+	struct fd f = fdget(ufd);
+	struct bpf_map *map;
+	void *key, *next_key;
+	int err;
+
+	if (CHECK_ATTR(BPF_MAP_GET_NEXT_KEY))
+		return -EINVAL;
+
+	map = bpf_map_get(f);
+	if (IS_ERR(map))
+		return PTR_ERR(map);
+
+	err = -ENOMEM;
+	key = kmalloc(map->key_size, GFP_USER);
+	if (!key)
+		goto err_put;
+
+	err = -EFAULT;
+	if (copy_from_user(key, ukey, map->key_size) != 0)
+		goto free_key;
+
+	err = -ENOMEM;
+	next_key = kmalloc(map->key_size, GFP_USER);
+	if (!next_key)
+		goto free_key;
+
+	rcu_read_lock();
+	err = map->ops->map_get_next_key(map, key, next_key);
+	rcu_read_unlock();
+	if (err)
+		goto free_next_key;
+
+	err = -EFAULT;
+	if (copy_to_user(unext_key, next_key, map->key_size) != 0)
+		goto free_next_key;
+
+	err = 0;
+
+free_next_key:
+	kfree(next_key);
+free_key:
+	kfree(key);
+err_put:
+	fdput(f);
+	return err;
+}
+
 SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
 {
 	union bpf_attr attr = {};
@@ -160,6 +383,18 @@ SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, siz
 	case BPF_MAP_CREATE:
 		err = map_create(&attr);
 		break;
+	case BPF_MAP_LOOKUP_ELEM:
+		err = map_lookup_elem(&attr);
+		break;
+	case BPF_MAP_UPDATE_ELEM:
+		err = map_update_elem(&attr);
+		break;
+	case BPF_MAP_DELETE_ELEM:
+		err = map_delete_elem(&attr);
+		break;
+	case BPF_MAP_GET_NEXT_KEY:
+		err = map_get_next_key(&attr);
+		break;
 	default:
 		err = -EINVAL;
 		break;

commit 99c55f7d47c0dc6fc64729f37bf435abf43f4c60
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Fri Sep 26 00:16:57 2014 -0700

    bpf: introduce BPF syscall and maps
    
    BPF syscall is a multiplexor for a range of different operations on eBPF.
    This patch introduces syscall with single command to create a map.
    Next patch adds commands to access maps.
    
    'maps' is a generic storage of different types for sharing data between kernel
    and userspace.
    
    Userspace example:
    /* this syscall wrapper creates a map with given type and attributes
     * and returns map_fd on success.
     * use close(map_fd) to delete the map
     */
    int bpf_create_map(enum bpf_map_type map_type, int key_size,
                       int value_size, int max_entries)
    {
        union bpf_attr attr = {
            .map_type = map_type,
            .key_size = key_size,
            .value_size = value_size,
            .max_entries = max_entries
        };
    
        return bpf(BPF_MAP_CREATE, &attr, sizeof(attr));
    }
    
    'union bpf_attr' is backwards compatible with future extensions.
    
    More details in Documentation/networking/filter.txt and in manpage
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
new file mode 100644
index 000000000000..428a0e23adc0
--- /dev/null
+++ b/kernel/bpf/syscall.c
@@ -0,0 +1,169 @@
+/* Copyright (c) 2011-2014 PLUMgrid, http://plumgrid.com
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ */
+#include <linux/bpf.h>
+#include <linux/syscalls.h>
+#include <linux/slab.h>
+#include <linux/anon_inodes.h>
+
+static LIST_HEAD(bpf_map_types);
+
+static struct bpf_map *find_and_alloc_map(union bpf_attr *attr)
+{
+	struct bpf_map_type_list *tl;
+	struct bpf_map *map;
+
+	list_for_each_entry(tl, &bpf_map_types, list_node) {
+		if (tl->type == attr->map_type) {
+			map = tl->ops->map_alloc(attr);
+			if (IS_ERR(map))
+				return map;
+			map->ops = tl->ops;
+			map->map_type = attr->map_type;
+			return map;
+		}
+	}
+	return ERR_PTR(-EINVAL);
+}
+
+/* boot time registration of different map implementations */
+void bpf_register_map_type(struct bpf_map_type_list *tl)
+{
+	list_add(&tl->list_node, &bpf_map_types);
+}
+
+/* called from workqueue */
+static void bpf_map_free_deferred(struct work_struct *work)
+{
+	struct bpf_map *map = container_of(work, struct bpf_map, work);
+
+	/* implementation dependent freeing */
+	map->ops->map_free(map);
+}
+
+/* decrement map refcnt and schedule it for freeing via workqueue
+ * (unrelying map implementation ops->map_free() might sleep)
+ */
+void bpf_map_put(struct bpf_map *map)
+{
+	if (atomic_dec_and_test(&map->refcnt)) {
+		INIT_WORK(&map->work, bpf_map_free_deferred);
+		schedule_work(&map->work);
+	}
+}
+
+static int bpf_map_release(struct inode *inode, struct file *filp)
+{
+	struct bpf_map *map = filp->private_data;
+
+	bpf_map_put(map);
+	return 0;
+}
+
+static const struct file_operations bpf_map_fops = {
+	.release = bpf_map_release,
+};
+
+/* helper macro to check that unused fields 'union bpf_attr' are zero */
+#define CHECK_ATTR(CMD) \
+	memchr_inv((void *) &attr->CMD##_LAST_FIELD + \
+		   sizeof(attr->CMD##_LAST_FIELD), 0, \
+		   sizeof(*attr) - \
+		   offsetof(union bpf_attr, CMD##_LAST_FIELD) - \
+		   sizeof(attr->CMD##_LAST_FIELD)) != NULL
+
+#define BPF_MAP_CREATE_LAST_FIELD max_entries
+/* called via syscall */
+static int map_create(union bpf_attr *attr)
+{
+	struct bpf_map *map;
+	int err;
+
+	err = CHECK_ATTR(BPF_MAP_CREATE);
+	if (err)
+		return -EINVAL;
+
+	/* find map type and init map: hashtable vs rbtree vs bloom vs ... */
+	map = find_and_alloc_map(attr);
+	if (IS_ERR(map))
+		return PTR_ERR(map);
+
+	atomic_set(&map->refcnt, 1);
+
+	err = anon_inode_getfd("bpf-map", &bpf_map_fops, map, O_RDWR | O_CLOEXEC);
+
+	if (err < 0)
+		/* failed to allocate fd */
+		goto free_map;
+
+	return err;
+
+free_map:
+	map->ops->map_free(map);
+	return err;
+}
+
+SYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)
+{
+	union bpf_attr attr = {};
+	int err;
+
+	/* the syscall is limited to root temporarily. This restriction will be
+	 * lifted when security audit is clean. Note that eBPF+tracing must have
+	 * this restriction, since it may pass kernel data to user space
+	 */
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (!access_ok(VERIFY_READ, uattr, 1))
+		return -EFAULT;
+
+	if (size > PAGE_SIZE)	/* silly large */
+		return -E2BIG;
+
+	/* If we're handed a bigger struct than we know of,
+	 * ensure all the unknown bits are 0 - i.e. new
+	 * user-space does not rely on any kernel feature
+	 * extensions we dont know about yet.
+	 */
+	if (size > sizeof(attr)) {
+		unsigned char __user *addr;
+		unsigned char __user *end;
+		unsigned char val;
+
+		addr = (void __user *)uattr + sizeof(attr);
+		end  = (void __user *)uattr + size;
+
+		for (; addr < end; addr++) {
+			err = get_user(val, addr);
+			if (err)
+				return err;
+			if (val)
+				return -E2BIG;
+		}
+		size = sizeof(attr);
+	}
+
+	/* copy attributes from user space, may be less than sizeof(bpf_attr) */
+	if (copy_from_user(&attr, uattr, size) != 0)
+		return -EFAULT;
+
+	switch (cmd) {
+	case BPF_MAP_CREATE:
+		err = map_create(&attr);
+		break;
+	default:
+		err = -EINVAL;
+		break;
+	}
+
+	return err;
+}
