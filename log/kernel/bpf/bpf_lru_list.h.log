commit 0ac16296ffc638f5163f9aeeeb1fe447268e449f
Author: Qiujun Huang <hqjagain@gmail.com>
Date:   Fri Apr 3 16:07:34 2020 +0800

    bpf: Fix a typo "inacitve" -> "inactive"
    
    There is a typo in struct bpf_lru_list's next_inactive_rotation
    description, thus fix s/inacitve/inactive/.
    
    Signed-off-by: Qiujun Huang <hqjagain@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/1585901254-30377-1-git-send-email-hqjagain@gmail.com

diff --git a/kernel/bpf/bpf_lru_list.h b/kernel/bpf/bpf_lru_list.h
index f02504640e18..6b12f06ee18c 100644
--- a/kernel/bpf/bpf_lru_list.h
+++ b/kernel/bpf/bpf_lru_list.h
@@ -30,7 +30,7 @@ struct bpf_lru_node {
 struct bpf_lru_list {
 	struct list_head lists[NR_BPF_LRU_LIST_T];
 	unsigned int counts[NR_BPF_LRU_LIST_COUNT];
-	/* The next inacitve list rotation starts from here */
+	/* The next inactive list rotation starts from here */
 	struct list_head *next_inactive_rotation;
 
 	raw_spinlock_t lock ____cacheline_aligned_in_smp;

commit 25763b3c864cf517d686661012d184ee47a49b4c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 28 10:10:09 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 206
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of version 2 of the gnu general public license as
      published by the free software foundation
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 107 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190528171438.615055994@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/bpf/bpf_lru_list.h b/kernel/bpf/bpf_lru_list.h
index 7d4f89b7cb84..f02504640e18 100644
--- a/kernel/bpf/bpf_lru_list.h
+++ b/kernel/bpf/bpf_lru_list.h
@@ -1,8 +1,5 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /* Copyright (c) 2016 Facebook
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of version 2 of the GNU General Public
- * License as published by the Free Software Foundation.
  */
 #ifndef __BPF_LRU_LIST_H_
 #define __BPF_LRU_LIST_H_

commit bb9b9f8802212d98e70c63045b1734162945eaa5
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Thu Aug 31 23:27:13 2017 -0700

    bpf: Only set node->ref = 1 if it has not been set
    
    This patch writes 'node->ref = 1' only if node->ref is 0.
    The number of lookups/s for a ~1M entries LRU map increased by
    ~30% (260097 to 343313).
    
    Other writes on 'node->ref = 0' is not changed.  In those cases, the
    same cache line has to be changed anyway.
    
    First column: Size of the LRU hash
    Second column: Number of lookups/s
    
    Before:
    > echo "$((2**20+1)): $(./map_perf_test 1024 1 $((2**20+1)) 10000000 | awk '{print $3}')"
    1048577: 260097
    
    After:
    > echo "$((2**20+1)): $(./map_perf_test 1024 1 $((2**20+1)) 10000000 | awk '{print $3}')"
    1048577: 343313
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/bpf_lru_list.h b/kernel/bpf/bpf_lru_list.h
index 5c35a98d02bf..7d4f89b7cb84 100644
--- a/kernel/bpf/bpf_lru_list.h
+++ b/kernel/bpf/bpf_lru_list.h
@@ -69,7 +69,8 @@ static inline void bpf_lru_node_set_ref(struct bpf_lru_node *node)
 	/* ref is an approximation on access frequency.  It does not
 	 * have to be very accurate.  Hence, no protection is used.
 	 */
-	node->ref = 1;
+	if (!node->ref)
+		node->ref = 1;
 }
 
 int bpf_lru_init(struct bpf_lru *lru, bool percpu, u32 hash_offset,

commit 961578b63474d13ad0e2f615fcc2901c5197dda6
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Nov 11 10:55:07 2016 -0800

    bpf: Add percpu LRU list
    
    Instead of having a common LRU list, this patch allows a
    percpu LRU list which can be selected by specifying a map
    attribute.  The map attribute will be added in the later
    patch.
    
    While the common use case for LRU is #reads >> #updates,
    percpu LRU list allows bpf prog to absorb unusual #updates
    under pathological case (e.g. external traffic facing machine which
    could be under attack).
    
    Each percpu LRU is isolated from each other.  The LRU nodes (including
    free nodes) cannot be moved across different LRU Lists.
    
    Here are the update performance comparison between
    common LRU list and percpu LRU list (the test code is
    at the last patch):
    
    [root@kerneltest003.31.prn1 ~]# for i in 1 4 8; do echo -n "$i cpus: "; \
    ./map_perf_test 16 $i | awk '{r += $3}END{print r " updates"}'; done
     1 cpus: 2934082 updates
     4 cpus: 7391434 updates
     8 cpus: 6500576 updates
    
    [root@kerneltest003.31.prn1 ~]# for i in 1 4 8; do echo -n "$i cpus: "; \
    ./map_perf_test 32 $i | awk '{r += $3}END{printr " updates"}'; done
      1 cpus: 2896553 updates
      4 cpus: 9766395 updates
      8 cpus: 17460553 updates
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/bpf_lru_list.h b/kernel/bpf/bpf_lru_list.h
index aaa2445ed1ca..5c35a98d02bf 100644
--- a/kernel/bpf/bpf_lru_list.h
+++ b/kernel/bpf/bpf_lru_list.h
@@ -53,11 +53,15 @@ struct bpf_common_lru {
 typedef bool (*del_from_htab_func)(void *arg, struct bpf_lru_node *node);
 
 struct bpf_lru {
-	struct bpf_common_lru common_lru;
+	union {
+		struct bpf_common_lru common_lru;
+		struct bpf_lru_list __percpu *percpu_lru;
+	};
 	del_from_htab_func del_from_htab;
 	void *del_arg;
 	unsigned int hash_offset;
 	unsigned int nr_scans;
+	bool percpu;
 };
 
 static inline void bpf_lru_node_set_ref(struct bpf_lru_node *node)
@@ -68,7 +72,7 @@ static inline void bpf_lru_node_set_ref(struct bpf_lru_node *node)
 	node->ref = 1;
 }
 
-int bpf_lru_init(struct bpf_lru *lru, u32 hash_offset,
+int bpf_lru_init(struct bpf_lru *lru, bool percpu, u32 hash_offset,
 		 del_from_htab_func del_from_htab, void *delete_arg);
 void bpf_lru_populate(struct bpf_lru *lru, void *buf, u32 node_offset,
 		      u32 elem_size, u32 nr_elems);

commit 3a08c2fd763450a927d1130de078d6f9e74944fb
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Fri Nov 11 10:55:06 2016 -0800

    bpf: LRU List
    
    Introduce bpf_lru_list which will provide LRU capability to
    the bpf_htab in the later patch.
    
    * General Thoughts:
    1. Target use case.  Read is more often than update.
       (i.e. bpf_lookup_elem() is more often than bpf_update_elem()).
       If bpf_prog does a bpf_lookup_elem() first and then an in-place
       update, it still counts as a read operation to the LRU list concern.
    2. It may be useful to think of it as a LRU cache
    3. Optimize the read case
       3.1 No lock in read case
       3.2 The LRU maintenance is only done during bpf_update_elem()
    4. If there is a percpu LRU list, it will lose the system-wise LRU
       property.  A completely isolated percpu LRU list has the best
       performance but the memory utilization is not ideal considering
       the work load may be imbalance.
    5. Hence, this patch starts the LRU implementation with a global LRU
       list with batched operations before accessing the global LRU list.
       As a LRU cache, #read >> #update/#insert operations, it will work well.
    6. There is a local list (for each cpu) which is named
       'struct bpf_lru_locallist'.  This local list is not used to sort
       the LRU property.  Instead, the local list is to batch enough
       operations before acquiring the lock of the global LRU list.  More
       details on this later.
    7. In the later patch, it allows a percpu LRU list by specifying a
       map-attribute for scalability reason and for use cases that need to
       prepare for the worst (and pathological) case like DoS attack.
       The percpu LRU list is completely isolated from each other and the
       LRU nodes (including free nodes) cannot be moved across the list.  The
       following description is for the global LRU list but mostly applicable
       to the percpu LRU list also.
    
    * Global LRU List:
    1. It has three sub-lists: active-list, inactive-list and free-list.
    2. The two list idea, active and inactive, is borrowed from the
       page cache.
    3. All nodes are pre-allocated and all sit at the free-list (of the
       global LRU list) at the beginning.  The pre-allocation reasoning
       is similar to the existing BPF_MAP_TYPE_HASH.  However,
       opting-out prealloc (BPF_F_NO_PREALLOC) is not supported in
       the LRU map.
    
    * Active/Inactive List (of the global LRU list):
    1. The active list, as its name says it, maintains the active set of
       the nodes.  We can think of it as the working set or more frequently
       accessed nodes.  The access frequency is approximated by a ref-bit.
       The ref-bit is set during the bpf_lookup_elem().
    2. The inactive list, as its name also says it, maintains a less
       active set of nodes.  They are the candidates to be removed
       from the bpf_htab when we are running out of free nodes.
    3. The ordering of these two lists is acting as a rough clock.
       The tail of the inactive list is the older nodes and
       should be released first if the bpf_htab needs free element.
    
    * Rotating the Active/Inactive List (of the global LRU list):
    1. It is the basic operation to maintain the LRU property of
       the global list.
    2. The active list is only rotated when the inactive list is running
       low.  This idea is similar to the current page cache.
       Inactive running low is currently defined as
       "# of inactive < # of active".
    3. The active list rotation always starts from the tail.  It moves
       node without ref-bit set to the head of the inactive list.
       It moves node with ref-bit set back to the head of the active
       list and then clears its ref-bit.
    4. The inactive rotation is pretty simply.
       It walks the inactive list and moves the nodes back to the head of
       active list if its ref-bit is set. The ref-bit is cleared after moving
       to the active list.
       If the node does not have ref-bit set, it just leave it as it is
       because it is already in the inactive list.
    
    * Shrinking the Inactive List (of the global LRU list):
    1. Shrinking is the operation to get free nodes when the bpf_htab is
       full.
    2. It usually only shrinks the inactive list to get free nodes.
    3. During shrinking, it will walk the inactive list from the tail,
       delete the nodes without ref-bit set from bpf_htab.
    4. If no free node found after step (3), it will forcefully get
       one node from the tail of inactive or active list.  Forcefully is
       in the sense that it ignores the ref-bit.
    
    * Local List:
    1. Each CPU has a 'struct bpf_lru_locallist'.  The purpose is to
       batch enough operations before acquiring the lock of the
       global LRU.
    2. A local list has two sub-lists, free-list and pending-list.
    3. During bpf_update_elem(), it will try to get from the free-list
       of (the current CPU local list).
    4. If the local free-list is empty, it will acquire from the
       global LRU list.  The global LRU list can either satisfy it
       by its global free-list or by shrinking the global inactive
       list.  Since we have acquired the global LRU list lock,
       it will try to get at most LOCAL_FREE_TARGET elements
       to the local free list.
    5. When a new element is added to the bpf_htab, it will
       first sit at the pending-list (of the local list) first.
       The pending-list will be flushed to the global LRU list
       when it needs to acquire free nodes from the global list
       next time.
    
    * Lock Consideration:
    The LRU list has a lock (lru_lock).  Each bucket of htab has a
    lock (buck_lock).  If both locks need to be acquired together,
    the lock order is always lru_lock -> buck_lock and this only
    happens in the bpf_lru_list.c logic.
    
    In hashtab.c, both locks are not acquired together (i.e. one
    lock is always released first before acquiring another lock).
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/bpf/bpf_lru_list.h b/kernel/bpf/bpf_lru_list.h
new file mode 100644
index 000000000000..aaa2445ed1ca
--- /dev/null
+++ b/kernel/bpf/bpf_lru_list.h
@@ -0,0 +1,80 @@
+/* Copyright (c) 2016 Facebook
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ */
+#ifndef __BPF_LRU_LIST_H_
+#define __BPF_LRU_LIST_H_
+
+#include <linux/list.h>
+#include <linux/spinlock_types.h>
+
+#define NR_BPF_LRU_LIST_T	(3)
+#define NR_BPF_LRU_LIST_COUNT	(2)
+#define NR_BPF_LRU_LOCAL_LIST_T (2)
+#define BPF_LOCAL_LIST_T_OFFSET NR_BPF_LRU_LIST_T
+
+enum bpf_lru_list_type {
+	BPF_LRU_LIST_T_ACTIVE,
+	BPF_LRU_LIST_T_INACTIVE,
+	BPF_LRU_LIST_T_FREE,
+	BPF_LRU_LOCAL_LIST_T_FREE,
+	BPF_LRU_LOCAL_LIST_T_PENDING,
+};
+
+struct bpf_lru_node {
+	struct list_head list;
+	u16 cpu;
+	u8 type;
+	u8 ref;
+};
+
+struct bpf_lru_list {
+	struct list_head lists[NR_BPF_LRU_LIST_T];
+	unsigned int counts[NR_BPF_LRU_LIST_COUNT];
+	/* The next inacitve list rotation starts from here */
+	struct list_head *next_inactive_rotation;
+
+	raw_spinlock_t lock ____cacheline_aligned_in_smp;
+};
+
+struct bpf_lru_locallist {
+	struct list_head lists[NR_BPF_LRU_LOCAL_LIST_T];
+	u16 next_steal;
+	raw_spinlock_t lock;
+};
+
+struct bpf_common_lru {
+	struct bpf_lru_list lru_list;
+	struct bpf_lru_locallist __percpu *local_list;
+};
+
+typedef bool (*del_from_htab_func)(void *arg, struct bpf_lru_node *node);
+
+struct bpf_lru {
+	struct bpf_common_lru common_lru;
+	del_from_htab_func del_from_htab;
+	void *del_arg;
+	unsigned int hash_offset;
+	unsigned int nr_scans;
+};
+
+static inline void bpf_lru_node_set_ref(struct bpf_lru_node *node)
+{
+	/* ref is an approximation on access frequency.  It does not
+	 * have to be very accurate.  Hence, no protection is used.
+	 */
+	node->ref = 1;
+}
+
+int bpf_lru_init(struct bpf_lru *lru, u32 hash_offset,
+		 del_from_htab_func del_from_htab, void *delete_arg);
+void bpf_lru_populate(struct bpf_lru *lru, void *buf, u32 node_offset,
+		      u32 elem_size, u32 nr_elems);
+void bpf_lru_destroy(struct bpf_lru *lru);
+struct bpf_lru_node *bpf_lru_pop_free(struct bpf_lru *lru, u32 hash);
+void bpf_lru_push_free(struct bpf_lru *lru, struct bpf_lru_node *node);
+void bpf_lru_promote(struct bpf_lru *lru, struct bpf_lru_node *node);
+
+#endif
