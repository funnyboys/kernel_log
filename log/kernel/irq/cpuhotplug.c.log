commit 11ea68f553e244851d15793a7fa33a97c46d8271
Author: Ming Lei <ming.lei@redhat.com>
Date:   Mon Jan 20 17:16:25 2020 +0800

    genirq, sched/isolation: Isolate from handling managed interrupts
    
    The affinity of managed interrupts is completely handled in the kernel and
    cannot be changed via the /proc/irq/* interfaces from user space. As the
    kernel tries to spread out interrupts evenly accross CPUs on x86 to prevent
    vector exhaustion, it can happen that a managed interrupt whose affinity
    mask contains both isolated and housekeeping CPUs is routed to an isolated
    CPU. As a consequence IO submitted on a housekeeping CPU causes interrupts
    on the isolated CPU.
    
    Add a new sub-parameter 'managed_irq' for 'isolcpus' and the corresponding
    logic in the interrupt affinity selection code.
    
    The subparameter indicates to the interrupt affinity selection logic that
    it should try to avoid the above scenario.
    
    This isolation is best effort and only effective if the automatically
    assigned interrupt mask of a device queue contains isolated and
    housekeeping CPUs. If housekeeping CPUs are online then such interrupts are
    directed to the housekeeping CPU so that IO submitted on the housekeeping
    CPU cannot disturb the isolated CPU.
    
    If a queue's affinity mask contains only isolated CPUs then this parameter
    has no effect on the interrupt routing decision, though interrupts are only
    happening when tasks running on those isolated CPUs submit IO. IO submitted
    on housekeeping CPUs has no influence on those queues.
    
    If the affinity mask contains both housekeeping and isolated CPUs, but none
    of the contained housekeeping CPUs is online, then the interrupt is also
    routed to an isolated CPU. Interrupts are only delivered when one of the
    isolated CPUs in the affinity mask submits IO. If one of the contained
    housekeeping CPUs comes online, the CPU hotplug logic migrates the
    interrupt automatically back to the upcoming housekeeping CPU. Depending on
    the type of interrupt controller, this can require that at least one
    interrupt is delivered to the isolated CPU in order to complete the
    migration.
    
    [ tglx: Removed unused parameter, added and edited comments/documentation
            and rephrased the changelog so it contains more details. ]
    
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20200120091625.17912-1-ming.lei@redhat.com

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 6c7ca2e983a5..02236b13b359 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -12,6 +12,7 @@
 #include <linux/interrupt.h>
 #include <linux/ratelimit.h>
 #include <linux/irq.h>
+#include <linux/sched/isolation.h>
 
 #include "internals.h"
 
@@ -171,6 +172,20 @@ void irq_migrate_all_off_this_cpu(void)
 	}
 }
 
+static bool hk_should_isolate(struct irq_data *data, unsigned int cpu)
+{
+	const struct cpumask *hk_mask;
+
+	if (!housekeeping_enabled(HK_FLAG_MANAGED_IRQ))
+		return false;
+
+	hk_mask = housekeeping_cpumask(HK_FLAG_MANAGED_IRQ);
+	if (cpumask_subset(irq_data_get_effective_affinity_mask(data), hk_mask))
+		return false;
+
+	return cpumask_test_cpu(cpu, hk_mask);
+}
+
 static void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)
 {
 	struct irq_data *data = irq_desc_get_irq_data(desc);
@@ -188,9 +203,11 @@ static void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)
 	/*
 	 * If the interrupt can only be directed to a single target
 	 * CPU then it is already assigned to a CPU in the affinity
-	 * mask. No point in trying to move it around.
+	 * mask. No point in trying to move it around unless the
+	 * isolation mechanism requests to move it to an upcoming
+	 * housekeeping CPU.
 	 */
-	if (!irqd_is_single_target(data))
+	if (!irqd_is_single_target(data) || hk_should_isolate(data, cpu))
 		irq_set_affinity_locked(data, affinity, false);
 }
 

commit 4001d8e8762f57d418b66e4e668601791900a1dd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jun 28 13:11:49 2019 +0200

    genirq: Delay deactivation in free_irq()
    
    When interrupts are shutdown, they are immediately deactivated in the
    irqdomain hierarchy. While this looks obviously correct there is a subtle
    issue:
    
    There might be an interrupt in flight when free_irq() is invoking the
    shutdown. This is properly handled at the irq descriptor / primary handler
    level, but the deactivation might completely disable resources which are
    required to acknowledge the interrupt.
    
    Split the shutdown code and deactivate the interrupt after synchronization
    in free_irq(). Fixup all other usage sites where this is not an issue to
    invoke the combined shutdown_and_deactivate() function instead.
    
    This still might be an issue if the interrupt in flight servicing is
    delayed on a remote CPU beyond the invocation of synchronize_irq(), but
    that cannot be handled at that level and needs to be handled in the
    synchronize_irq() context.
    
    Fixes: f8264e34965a ("irqdomain: Introduce new interfaces to support hierarchy irqdomains")
    Reported-by: Robert Hodaszi <Robert.Hodaszi@digi.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Link: https://lkml.kernel.org/r/20190628111440.098196390@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 5b1072e394b2..6c7ca2e983a5 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -116,7 +116,7 @@ static bool migrate_one_irq(struct irq_desc *desc)
 		 */
 		if (irqd_affinity_is_managed(d)) {
 			irqd_set_managed_shutdown(d);
-			irq_shutdown(desc);
+			irq_shutdown_and_deactivate(desc);
 			return false;
 		}
 		affinity = cpu_online_mask;

commit 52a65ff5603e685e9b19c2e108b3f0826dc7a86b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 14 22:15:19 2018 +0100

    genirq: Add missing SPDX identifiers
    
    Add SPDX identifiers to files
    
     - which contain an explicit license boiler plate or reference
    
     - which do not contain a license reference and were not updated in the
       initial SPDX conversion because the license was deduced by the scanners
       via EXPORT_SYMBOL_GPL as GPL2.0 only.
    
    [ tglx: Moved adding identifiers from the patch which removes the
            references/boilerplate ]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Link: https://lkml.kernel.org/r/20180314212030.668321222@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 9eb09aef0313..5b1072e394b2 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Generic cpu hotunplug interrupt migration code copied from the
  * arch/arm implementation

commit 60b09c51bb4fb46e2331fdbb39f91520f31d35f7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Oct 9 12:47:24 2017 +0200

    genirq/cpuhotplug: Add sanity check for effective affinity mask
    
    The effective affinity mask handling has no safety net when the mask is not
    updated by the interrupt chip or the mask contains offline CPUs.
    
    If that happens the CPU unplug code fails to migrate interrupts.
    
    Add sanity checks and emit a warning when the mask contains only offline
    CPUs.
    
    Fixes: 415fcf1a2293 ("genirq/cpuhotplug: Use effective affinity mask")
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1710042208400.2406@nanos

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 638eb9c83d9f..9eb09aef0313 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -18,8 +18,34 @@
 static inline bool irq_needs_fixup(struct irq_data *d)
 {
 	const struct cpumask *m = irq_data_get_effective_affinity_mask(d);
+	unsigned int cpu = smp_processor_id();
 
-	return cpumask_test_cpu(smp_processor_id(), m);
+#ifdef CONFIG_GENERIC_IRQ_EFFECTIVE_AFF_MASK
+	/*
+	 * The cpumask_empty() check is a workaround for interrupt chips,
+	 * which do not implement effective affinity, but the architecture has
+	 * enabled the config switch. Use the general affinity mask instead.
+	 */
+	if (cpumask_empty(m))
+		m = irq_data_get_affinity_mask(d);
+
+	/*
+	 * Sanity check. If the mask is not empty when excluding the outgoing
+	 * CPU then it must contain at least one online CPU. The outgoing CPU
+	 * has been removed from the online mask already.
+	 */
+	if (cpumask_any_but(m, cpu) < nr_cpu_ids &&
+	    cpumask_any_and(m, cpu_online_mask) >= nr_cpu_ids) {
+		/*
+		 * If this happens then there was a missed IRQ fixup at some
+		 * point. Warn about it and enforce fixup.
+		 */
+		pr_warn("Eff. affinity %*pbl of IRQ %u contains only offline CPUs after offlining CPU %u\n",
+			cpumask_pr_args(m), d->irq, cpu);
+		return true;
+	}
+#endif
+	return cpumask_test_cpu(cpu, m);
 }
 
 static bool migrate_one_irq(struct irq_desc *desc)

commit 8397913303abc9333f376a518a8368fa22ca5e6e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 27 12:21:11 2017 +0200

    genirq/cpuhotplug: Revert "Set force affinity flag on hotplug migration"
    
    That commit was part of the changes moving x86 to the generic CPU hotplug
    interrupt migration code. The force flag was required on x86 before the
    hierarchical irqdomain rework, but invoking set_affinity() with force=true
    stayed and had no side effects.
    
    At some point in the past, the force flag got repurposed to support the
    exynos timer interrupt affinity setting to a not yet online CPU, so the
    interrupt controller callback does not verify the supplied affinity mask
    against cpu_online_mask.
    
    Setting the flag in the CPU hotplug code causes the cpu online masking to
    be blocked on these irq controllers and results in potentially affining an
    interrupt to the CPU which is unplugged, i.e. instead of moving it away,
    it's just reassigned to it.
    
    As the force flags is not longer needed on x86, it's safe to revert that
    patch so the ARM irqchips which use the force flag work again.
    
    Add comments to that effect, so this won't happen again.
    
    Note: The online mask handling should be done in the generic code and the
    force flag and the masking in the irq chips removed all together, but
    that's not a change possible for 4.13.
    
    Fixes: 77f85e66aa8b ("genirq/cpuhotplug: Set force affinity flag on hotplug migration")
    Reported-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: LAK <linux-arm-kernel@lists.infradead.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1707271217590.3109@nanos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index aee8f7ec40af..638eb9c83d9f 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -95,8 +95,13 @@ static bool migrate_one_irq(struct irq_desc *desc)
 		affinity = cpu_online_mask;
 		brokeaff = true;
 	}
-
-	err = irq_do_set_affinity(d, affinity, true);
+	/*
+	 * Do not set the force argument of irq_do_set_affinity() as this
+	 * disables the masking of offline CPUs from the supplied affinity
+	 * mask and therefore might keep/reassign the irq to the outgoing
+	 * CPU.
+	 */
+	err = irq_do_set_affinity(d, affinity, false);
 	if (err) {
 		pr_warn_ratelimited("IRQ%u: set affinity failed(%d).\n",
 				    d->irq, err);

commit 8f31a9845db348f5781df47ce04c79e4cfe90016
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:53 2017 +0200

    genirq/cpuhotplug: Avoid irq affinity setting for single targets
    
    Avoid trying to add a newly online CPU to the effective affinity mask of an
    started up interrupt. That interrupt will either stay on the already online
    CPU or move around for no value.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: http://lkml.kernel.org/r/20170619235447.431321047@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index b7964e72ded7..aee8f7ec40af 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -148,9 +148,17 @@ static void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)
 	    !irq_data_get_irq_chip(data) || !cpumask_test_cpu(cpu, affinity))
 		return;
 
-	if (irqd_is_managed_and_shutdown(data))
+	if (irqd_is_managed_and_shutdown(data)) {
 		irq_startup(desc, IRQ_RESEND, IRQ_START_COND);
-	else
+		return;
+	}
+
+	/*
+	 * If the interrupt can only be directed to a single target
+	 * CPU then it is already assigned to a CPU in the affinity
+	 * mask. No point in trying to move it around.
+	 */
+	if (!irqd_is_single_target(data))
 		irq_set_affinity_locked(data, affinity, false);
 }
 

commit c5cb83bb337c25caae995d992d1cdf9b317f83de
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:51 2017 +0200

    genirq/cpuhotplug: Handle managed IRQs on CPU hotplug
    
    If a CPU goes offline, interrupts affine to the CPU are moved away. If the
    outgoing CPU is the last CPU in the affinity mask the migration code breaks
    the affinity and sets it it all online cpus.
    
    This is a problem for affinity managed interrupts as CPU hotplug is often
    used for power management purposes. If the affinity is broken, the
    interrupt is not longer affine to the CPUs to which it was allocated.
    
    The affinity spreading allows to lay out multi queue devices in a way that
    they are assigned to a single CPU or a group of CPUs. If the last CPU goes
    offline, then the queue is not longer used, so the interrupt can be
    shutdown gracefully and parked until one of the assigned CPUs comes online
    again.
    
    Add a graceful shutdown mechanism into the irq affinity breaking code path,
    mark the irq as MANAGED_SHUTDOWN and leave the affinity mask unmodified.
    
    In the online path, scan the active interrupts for managed interrupts and
    if the interrupt is functional and the newly online CPU is part of the
    affinity mask, restart the interrupt if it is marked MANAGED_SHUTDOWN or if
    the interrupts is started up, try to add the CPU back to the effective
    affinity mask.
    
    Originally-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20170619235447.273417334@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 0b093db3336b..b7964e72ded7 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -83,6 +83,15 @@ static bool migrate_one_irq(struct irq_desc *desc)
 		chip->irq_mask(d);
 
 	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
+		/*
+		 * If the interrupt is managed, then shut it down and leave
+		 * the affinity untouched.
+		 */
+		if (irqd_affinity_is_managed(d)) {
+			irqd_set_managed_shutdown(d);
+			irq_shutdown(desc);
+			return false;
+		}
 		affinity = cpu_online_mask;
 		brokeaff = true;
 	}
@@ -129,3 +138,39 @@ void irq_migrate_all_off_this_cpu(void)
 		}
 	}
 }
+
+static void irq_restore_affinity_of_irq(struct irq_desc *desc, unsigned int cpu)
+{
+	struct irq_data *data = irq_desc_get_irq_data(desc);
+	const struct cpumask *affinity = irq_data_get_affinity_mask(data);
+
+	if (!irqd_affinity_is_managed(data) || !desc->action ||
+	    !irq_data_get_irq_chip(data) || !cpumask_test_cpu(cpu, affinity))
+		return;
+
+	if (irqd_is_managed_and_shutdown(data))
+		irq_startup(desc, IRQ_RESEND, IRQ_START_COND);
+	else
+		irq_set_affinity_locked(data, affinity, false);
+}
+
+/**
+ * irq_affinity_online_cpu - Restore affinity for managed interrupts
+ * @cpu:	Upcoming CPU for which interrupts should be restored
+ */
+int irq_affinity_online_cpu(unsigned int cpu)
+{
+	struct irq_desc *desc;
+	unsigned int irq;
+
+	irq_lock_sparse();
+	for_each_active_irq(irq) {
+		desc = irq_to_desc(irq);
+		raw_spin_lock_irq(&desc->lock);
+		irq_restore_affinity_of_irq(desc, cpu);
+		raw_spin_unlock_irq(&desc->lock);
+	}
+	irq_unlock_sparse();
+
+	return 0;
+}

commit 415fcf1a2293046e0c1f4ab8558a87bad66652b1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:39 2017 +0200

    genirq/cpuhotplug: Use effective affinity mask
    
    If the architecture supports the effective affinity mask, migrating
    interrupts away which are not targeted by the effective mask is
    pointless.
    
    They can stay in the user or system supplied affinity mask, but won't be
    targetted at any given point as the affinity setter functions need to
    validate against the online cpu mask anyway.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: http://lkml.kernel.org/r/20170619235446.328488490@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index e09cb91a7c8b..0b093db3336b 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -14,6 +14,14 @@
 
 #include "internals.h"
 
+/* For !GENERIC_IRQ_EFFECTIVE_AFF_MASK this looks at general affinity mask */
+static inline bool irq_needs_fixup(struct irq_data *d)
+{
+	const struct cpumask *m = irq_data_get_effective_affinity_mask(d);
+
+	return cpumask_test_cpu(smp_processor_id(), m);
+}
+
 static bool migrate_one_irq(struct irq_desc *desc)
 {
 	struct irq_data *d = irq_desc_get_irq_data(desc);
@@ -42,9 +50,7 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	 * Note: Do not check desc->action as this might be a chained
 	 * interrupt.
 	 */
-	affinity = irq_data_get_affinity_mask(d);
-	if (irqd_is_per_cpu(d) || !irqd_is_started(d) ||
-	    !cpumask_test_cpu(smp_processor_id(), affinity)) {
+	if (irqd_is_per_cpu(d) || !irqd_is_started(d) || !irq_needs_fixup(d)) {
 		/*
 		 * If an irq move is pending, abort it if the dying CPU is
 		 * the sole target.
@@ -69,6 +75,8 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	 */
 	if (irq_fixup_move_pending(desc, true))
 		affinity = irq_desc_get_pending_mask(desc);
+	else
+		affinity = irq_data_get_affinity_mask(d);
 
 	/* Mask the chip for interrupts which cannot move in process context */
 	if (maskchip && chip->irq_mask)

commit 77f85e66aa8be563ae5804eebf74a78ec6ef5555
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:31 2017 +0200

    genirq/cpuhotplug: Set force affinity flag on hotplug migration
    
    Set the force migration flag when migrating interrupts away from an
    outgoing CPU.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: http://lkml.kernel.org/r/20170619235445.681874648@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 6f46587a9ce5..e09cb91a7c8b 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -79,7 +79,7 @@ static bool migrate_one_irq(struct irq_desc *desc)
 		brokeaff = true;
 	}
 
-	err = irq_do_set_affinity(d, affinity, false);
+	err = irq_do_set_affinity(d, affinity, true);
 	if (err) {
 		pr_warn_ratelimited("IRQ%u: set affinity failed(%d).\n",
 				    d->irq, err);

commit 47a06d3a783217acae02976f15ca07ddc1ac024f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:30 2017 +0200

    genirq/cpuhotplug: Add support for conditional masking
    
    Interrupts which cannot be migrated in process context, need to be masked
    before the affinity is changed forcefully.
    
    Add support for that. Will be compiled out for architectures which do not
    have this x86 specific issue.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: http://lkml.kernel.org/r/20170619235445.604565591@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 4be4bd669d81..6f46587a9ce5 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -18,6 +18,7 @@ static bool migrate_one_irq(struct irq_desc *desc)
 {
 	struct irq_data *d = irq_desc_get_irq_data(desc);
 	struct irq_chip *chip = irq_data_get_irq_chip(d);
+	bool maskchip = !irq_can_move_pcntxt(d) && !irqd_irq_masked(d);
 	const struct cpumask *affinity;
 	bool brokeaff = false;
 	int err;
@@ -69,6 +70,10 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	if (irq_fixup_move_pending(desc, true))
 		affinity = irq_desc_get_pending_mask(desc);
 
+	/* Mask the chip for interrupts which cannot move in process context */
+	if (maskchip && chip->irq_mask)
+		chip->irq_mask(d);
+
 	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
 		affinity = cpu_online_mask;
 		brokeaff = true;
@@ -78,8 +83,12 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	if (err) {
 		pr_warn_ratelimited("IRQ%u: set affinity failed(%d).\n",
 				    d->irq, err);
-		return false;
+		brokeaff = false;
 	}
+
+	if (maskchip && chip->irq_unmask)
+		chip->irq_unmask(d);
+
 	return brokeaff;
 }
 

commit f0383c24b4855f6a4b5a358c7b2d2c16e0437e9b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:29 2017 +0200

    genirq/cpuhotplug: Add support for cleaning up move in progress
    
    In order to move x86 to the generic hotplug migration code, add support for
    cleaning up move in progress bits.
    
    On architectures which have this x86 specific (mis)feature not enabled,
    this is optimized out by the compiler.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: http://lkml.kernel.org/r/20170619235445.525817311@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 09b20e127aee..4be4bd669d81 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -18,7 +18,7 @@ static bool migrate_one_irq(struct irq_desc *desc)
 {
 	struct irq_data *d = irq_desc_get_irq_data(desc);
 	struct irq_chip *chip = irq_data_get_irq_chip(d);
-	const struct cpumask *affinity = d->common->affinity;
+	const struct cpumask *affinity;
 	bool brokeaff = false;
 	int err;
 
@@ -41,9 +41,33 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	 * Note: Do not check desc->action as this might be a chained
 	 * interrupt.
 	 */
+	affinity = irq_data_get_affinity_mask(d);
 	if (irqd_is_per_cpu(d) || !irqd_is_started(d) ||
-	    !cpumask_test_cpu(smp_processor_id(), affinity))
+	    !cpumask_test_cpu(smp_processor_id(), affinity)) {
+		/*
+		 * If an irq move is pending, abort it if the dying CPU is
+		 * the sole target.
+		 */
+		irq_fixup_move_pending(desc, false);
 		return false;
+	}
+
+	/*
+	 * Complete an eventually pending irq move cleanup. If this
+	 * interrupt was moved in hard irq context, then the vectors need
+	 * to be cleaned up. It can't wait until this interrupt actually
+	 * happens and this CPU was involved.
+	 */
+	irq_force_complete_move(desc);
+
+	/*
+	 * If there is a setaffinity pending, then try to reuse the pending
+	 * mask, so the last change of the affinity does not get lost. If
+	 * there is no move pending or the pending mask does not contain
+	 * any online CPU, use the current affinity mask.
+	 */
+	if (irq_fixup_move_pending(desc, true))
+		affinity = irq_desc_get_pending_mask(desc);
 
 	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
 		affinity = cpu_online_mask;

commit 91f26cb4cd3c22bd656ab46c49329aacaaab5504
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:28 2017 +0200

    genirq/cpuhotplug: Do not migrated shutdown irqs
    
    Interrupts, which are shut down are tried to be migrated as well. That's
    pointless because the interrupt cannot fire and the next startup will move
    it to the proper place anyway.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: http://lkml.kernel.org/r/20170619235445.447550992@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 41fe1e04d5d9..09b20e127aee 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -33,10 +33,15 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	}
 
 	/*
-	 * If this is a per-CPU interrupt, or the affinity does not
-	 * include this CPU, then we have nothing to do.
+	 * No move required, if:
+	 * - Interrupt is per cpu
+	 * - Interrupt is not started
+	 * - Affinity mask does not include this CPU.
+	 *
+	 * Note: Do not check desc->action as this might be a chained
+	 * interrupt.
 	 */
-	if (irqd_is_per_cpu(d) ||
+	if (irqd_is_per_cpu(d) || !irqd_is_started(d) ||
 	    !cpumask_test_cpu(smp_processor_id(), affinity))
 		return false;
 

commit e8a7035039306c90bcc99129ffc18e0be052bbb9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:27 2017 +0200

    genirq/cpuhotplug: Reorder check logic
    
    Move the checks for a valid irq chip and the irq_set_affinity() callback
    right in front of the whole migration logic. No point in doing a gazillion
    of other things when the interrupt cannot be migrated at all.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: http://lkml.kernel.org/r/20170619235445.354181630@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 9c5521b247d5..41fe1e04d5d9 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -17,9 +17,20 @@
 static bool migrate_one_irq(struct irq_desc *desc)
 {
 	struct irq_data *d = irq_desc_get_irq_data(desc);
+	struct irq_chip *chip = irq_data_get_irq_chip(d);
 	const struct cpumask *affinity = d->common->affinity;
-	struct irq_chip *c;
-	bool ret = false;
+	bool brokeaff = false;
+	int err;
+
+	/*
+	 * IRQ chip might be already torn down, but the irq descriptor is
+	 * still in the radix tree. Also if the chip has no affinity setter,
+	 * nothing can be done here.
+	 */
+	if (!chip || !chip->irq_set_affinity) {
+		pr_debug("IRQ %u: Unable to migrate away\n", d->irq);
+		return false;
+	}
 
 	/*
 	 * If this is a per-CPU interrupt, or the affinity does not
@@ -31,23 +42,16 @@ static bool migrate_one_irq(struct irq_desc *desc)
 
 	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
 		affinity = cpu_online_mask;
-		ret = true;
+		brokeaff = true;
 	}
 
-	c = irq_data_get_irq_chip(d);
-	if (!c->irq_set_affinity) {
-		pr_debug("IRQ%u: unable to set affinity\n", d->irq);
-		ret = false;
-	} else {
-		int r = irq_do_set_affinity(d, affinity, false);
-		if (r) {
-			pr_warn_ratelimited("IRQ%u: set affinity failed(%d).\n",
-					    d->irq, r);
-			ret = false;
-		}
+	err = irq_do_set_affinity(d, affinity, false);
+	if (err) {
+		pr_warn_ratelimited("IRQ%u: set affinity failed(%d).\n",
+				    d->irq, err);
+		return false;
 	}
-
-	return ret;
+	return brokeaff;
 }
 
 /**

commit 735c09524d3e7c92315e8e2699a1b9acb4fb415c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:26 2017 +0200

    genirq/cpuhotplug: Dont claim success on error
    
    In case the affinity of an interrupt was broken, a printk is emitted.
    
    But if the affinity cannot be set at all due to a missing
    irq_set_affinity() callback or due to a failing callback, the message is
    still printed preceeded by a warning/error.
    
    That makes no sense whatsoever.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: http://lkml.kernel.org/r/20170619235445.274852976@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 705139831590..9c5521b247d5 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -37,11 +37,14 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	c = irq_data_get_irq_chip(d);
 	if (!c->irq_set_affinity) {
 		pr_debug("IRQ%u: unable to set affinity\n", d->irq);
+		ret = false;
 	} else {
 		int r = irq_do_set_affinity(d, affinity, false);
-		if (r)
+		if (r) {
 			pr_warn_ratelimited("IRQ%u: set affinity failed(%d).\n",
 					    d->irq, r);
+			ret = false;
+		}
 	}
 
 	return ret;

commit 0dd945ff4647a1f29c6ae8f4f9a69c8f37c994cf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 20 01:37:25 2017 +0200

    genirq/cpuhotplug: Remove irq disabling logic
    
    This is called from stop_machine() with interrupts disabled. No point in
    disabling them some more.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Keith Busch <keith.busch@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Link: http://lkml.kernel.org/r/20170619235445.198042748@linutronix.de

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 011f8c4c63da..705139831590 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -59,11 +59,8 @@ static bool migrate_one_irq(struct irq_desc *desc)
  */
 void irq_migrate_all_off_this_cpu(void)
 {
-	unsigned int irq;
 	struct irq_desc *desc;
-	unsigned long flags;
-
-	local_irq_save(flags);
+	unsigned int irq;
 
 	for_each_active_irq(irq) {
 		bool affinity_broken;
@@ -73,10 +70,9 @@ void irq_migrate_all_off_this_cpu(void)
 		affinity_broken = migrate_one_irq(desc);
 		raw_spin_unlock(&desc->lock);
 
-		if (affinity_broken)
-			pr_warn_ratelimited("IRQ%u no longer affine to CPU%u\n",
+		if (affinity_broken) {
+			pr_warn_ratelimited("IRQ %u: no longer affine to CPU%u\n",
 					    irq, smp_processor_id());
+		}
 	}
-
-	local_irq_restore(flags);
 }

commit 58c9c87ca9dca99f269267f5aadcd051fef1637b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Oct 22 14:34:57 2015 +0200

    genirq: Make the cpuhotplug migration code less noisy
    
    The original arm code has a pr_debug() statement for the case where
    the irq chip has no set_affinity() callback. That's sufficient for
    debugging and we really don't want to spam dmesg with useless warnings
    for the normal case.
    
    Fixes: f1e0bb0ad473: "genirq: Introduce generic irq migration for cpu hotunplug"
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Requested-by: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yang Yingliang <yangyingliang@huawei.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Hanjun Guo <hanjun.guo@linaro.org>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
index 80f4f4e56fed..011f8c4c63da 100644
--- a/kernel/irq/cpuhotplug.c
+++ b/kernel/irq/cpuhotplug.c
@@ -36,7 +36,7 @@ static bool migrate_one_irq(struct irq_desc *desc)
 
 	c = irq_data_get_irq_chip(d);
 	if (!c->irq_set_affinity) {
-		pr_warn_ratelimited("IRQ%u: unable to set affinity\n", d->irq);
+		pr_debug("IRQ%u: unable to set affinity\n", d->irq);
 	} else {
 		int r = irq_do_set_affinity(d, affinity, false);
 		if (r)

commit f1e0bb0ad473a32d1b7e6d285ae9f7e47710bb5e
Author: Yang Yingliang <yangyingliang@huawei.com>
Date:   Thu Sep 24 17:32:13 2015 +0800

    genirq: Introduce generic irq migration for cpu hotunplug
    
    ARM and ARM64 have almost identical code for migrating interrupts on
    cpu hotunplug. Provide a generic version which can be used by both.
    
    The new code addresses a shortcoming in the ARM[64] variants which
    fails to update the affinity change in some cases. The solution for
    this is to use the core function irq_do_set_affinity() instead of open
    coding it.
    
    [ tglx: Added copyright notice and license boilerplate. Rewrote
            subject and changelog. ]
    
    Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
    Acked-by: Russell King - ARM Linux <linux@arm.linux.org.uk>
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Hanjun Guo <hanjun.guo@linaro.org>
    Cc: <linux-arm-kernel@lists.infradead.org>
    Link: http://lkml.kernel.org/r/1443087135-17044-2-git-send-email-yangyingliang@huawei.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/irq/cpuhotplug.c b/kernel/irq/cpuhotplug.c
new file mode 100644
index 000000000000..80f4f4e56fed
--- /dev/null
+++ b/kernel/irq/cpuhotplug.c
@@ -0,0 +1,82 @@
+/*
+ * Generic cpu hotunplug interrupt migration code copied from the
+ * arch/arm implementation
+ *
+ * Copyright (C) Russell King
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/interrupt.h>
+#include <linux/ratelimit.h>
+#include <linux/irq.h>
+
+#include "internals.h"
+
+static bool migrate_one_irq(struct irq_desc *desc)
+{
+	struct irq_data *d = irq_desc_get_irq_data(desc);
+	const struct cpumask *affinity = d->common->affinity;
+	struct irq_chip *c;
+	bool ret = false;
+
+	/*
+	 * If this is a per-CPU interrupt, or the affinity does not
+	 * include this CPU, then we have nothing to do.
+	 */
+	if (irqd_is_per_cpu(d) ||
+	    !cpumask_test_cpu(smp_processor_id(), affinity))
+		return false;
+
+	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
+		affinity = cpu_online_mask;
+		ret = true;
+	}
+
+	c = irq_data_get_irq_chip(d);
+	if (!c->irq_set_affinity) {
+		pr_warn_ratelimited("IRQ%u: unable to set affinity\n", d->irq);
+	} else {
+		int r = irq_do_set_affinity(d, affinity, false);
+		if (r)
+			pr_warn_ratelimited("IRQ%u: set affinity failed(%d).\n",
+					    d->irq, r);
+	}
+
+	return ret;
+}
+
+/**
+ * irq_migrate_all_off_this_cpu - Migrate irqs away from offline cpu
+ *
+ * The current CPU has been marked offline.  Migrate IRQs off this CPU.
+ * If the affinity settings do not allow other CPUs, force them onto any
+ * available CPU.
+ *
+ * Note: we must iterate over all IRQs, whether they have an attached
+ * action structure or not, as we need to get chained interrupts too.
+ */
+void irq_migrate_all_off_this_cpu(void)
+{
+	unsigned int irq;
+	struct irq_desc *desc;
+	unsigned long flags;
+
+	local_irq_save(flags);
+
+	for_each_active_irq(irq) {
+		bool affinity_broken;
+
+		desc = irq_to_desc(irq);
+		raw_spin_lock(&desc->lock);
+		affinity_broken = migrate_one_irq(desc);
+		raw_spin_unlock(&desc->lock);
+
+		if (affinity_broken)
+			pr_warn_ratelimited("IRQ%u no longer affine to CPU%u\n",
+					    irq, smp_processor_id());
+	}
+
+	local_irq_restore(flags);
+}
