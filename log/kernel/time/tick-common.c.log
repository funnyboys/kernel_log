commit e5d4d1756b07d9490a0269a9e68c1e05ee1feb9b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 21 12:25:58 2020 +0100

    timekeeping: Split jiffies seqlock
    
    seqlock consists of a sequence counter and a spinlock_t which is used to
    serialize the writers. spinlock_t is substituted by a "sleeping" spinlock
    on PREEMPT_RT enabled kernels which breaks the usage in the timekeeping
    code as the writers are executed in hard interrupt and therefore
    non-preemptible context even on PREEMPT_RT.
    
    The spinlock in seqlock cannot be unconditionally replaced by a
    raw_spinlock_t as many seqlock users have nesting spinlock sections or
    other code which is not suitable to run in truly atomic context on RT.
    
    Instead of providing a raw_seqlock API for a single use case, open code the
    seqlock for the jiffies use case and implement it with a raw_spinlock_t and
    a sequence counter.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200321113242.120587764@linutronix.de

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 7e5d3524e924..6c9c342dd0e5 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -84,13 +84,15 @@ int tick_is_oneshot_available(void)
 static void tick_periodic(int cpu)
 {
 	if (tick_do_timer_cpu == cpu) {
-		write_seqlock(&jiffies_lock);
+		raw_spin_lock(&jiffies_lock);
+		write_seqcount_begin(&jiffies_seq);
 
 		/* Keep track of the next tick event */
 		tick_next_period = ktime_add(tick_next_period, tick_period);
 
 		do_timer(1);
-		write_sequnlock(&jiffies_lock);
+		write_seqcount_end(&jiffies_seq);
+		raw_spin_unlock(&jiffies_lock);
 		update_wall_time();
 	}
 
@@ -162,9 +164,9 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 		ktime_t next;
 
 		do {
-			seq = read_seqbegin(&jiffies_lock);
+			seq = read_seqcount_begin(&jiffies_seq);
 			next = tick_next_period;
-		} while (read_seqretry(&jiffies_lock, seq));
+		} while (read_seqcount_retry(&jiffies_seq, seq));
 
 		clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);
 

commit 5167c506d62dd9ffab73eba23c79b0a8845c9fe1
Author: Chunyan Zhang <zhang.lyra@gmail.com>
Date:   Fri Jan 10 16:39:02 2020 +0800

    tick/common: Touch watchdog in tick_unfreeze() on all CPUs
    
    Suspend to IDLE invokes tick_unfreeze() on resume. tick_unfreeze() on the
    first resuming CPU resumes timekeeping, which also has the side effect of
    resetting the softlockup watchdog on this CPU.
    
    But on the secondary CPUs the watchdog is not reset in the resume /
    unfreeze() path, which can result in false softlockup warnings on those
    CPUs depending on the time spent in suspend.
    
    Prevent this by clearing the softlock watchdog in the unfreeze path also
    on the secondary resuming CPUs.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Chunyan Zhang <chunyan.zhang@unisoc.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/r/20200110083902.27276-1-chunyan.zhang@unisoc.com

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 59225b484e4e..7e5d3524e924 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -11,6 +11,7 @@
 #include <linux/err.h>
 #include <linux/hrtimer.h>
 #include <linux/interrupt.h>
+#include <linux/nmi.h>
 #include <linux/percpu.h>
 #include <linux/profile.h>
 #include <linux/sched.h>
@@ -558,6 +559,7 @@ void tick_unfreeze(void)
 		trace_suspend_resume(TPS("timekeeping_freeze"),
 				     smp_processor_id(), false);
 	} else {
+		touch_softlockup_watchdog();
 		tick_resume_local();
 	}
 

commit a0e928ed7c603a47dca8643e58db224a799ff2c5
Merge: 5a2bf1abbf96 13e792a19d4e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 14:50:46 2019 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Ingo Molnar:
     "This cycle had the following changes:
    
       - Timer tracing improvements (Anna-Maria Gleixner)
    
       - Continued tasklet reduction work: remove the hrtimer_tasklet
         (Thomas Gleixner)
    
       - Fix CPU hotplug remove race in the tick-broadcast mask handling
         code (Thomas Gleixner)
    
       - Force upper bound for setting CLOCK_REALTIME, to fix ABI
         inconsistencies with handling values that are close to the maximum
         supported and the vagueness of when uptime related wraparound might
         occur. Make the consistent maximum the year 2232 across all
         relevant ABIs and APIs. (Thomas Gleixner)
    
       - various cleanups and smaller fixes"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      tick: Fix typos in comments
      tick/broadcast: Fix warning about undefined tick_broadcast_oneshot_offline()
      timekeeping: Force upper bound for setting CLOCK_REALTIME
      timer/trace: Improve timer tracing
      timer/trace: Replace deprecated vsprintf pointer extension %pf by %ps
      timer: Move trace point to get proper index
      tick/sched: Update tick_sched struct documentation
      tick: Remove outgoing CPU from broadcast masks
      timekeeping: Consistently use unsigned int for seqcount snapshot
      softirq: Remove tasklet_hrtimer
      xfrm: Replace hrtimer tasklet with softirq hrtimer
      mac80211_hwsim: Replace hrtimer tasklet with softirq hrtimer

commit 08ae95f4fd3b38b257f5dc7e6507e071c27ba0d5
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Apr 11 13:34:48 2019 +1000

    nohz_full: Allow the boot CPU to be nohz_full
    
    Allow the boot CPU/CPU0 to be nohz_full. Have the boot CPU take the
    do_timer duty during boot until a housekeeping CPU can take over.
    
    This is supported when CONFIG_PM_SLEEP_SMP is not configured, or when
    it is configured and the arch allows suspend on non-zero CPUs.
    
    nohz_full has been trialed at a large supercomputer site and found to
    significantly reduce jitter. In order to deploy it in production, they
    need CPU0 to be nohz_full because their job control system requires
    the application CPUs to start from 0, and the housekeeping CPUs are
    placed higher. An equivalent job scheduling that uses CPU0 for
    housekeeping could be achieved by modifying their system, but it is
    preferable if nohz_full can support their environment without
    modification.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: https://lkml.kernel.org/r/20190411033448.20842-6-npiggin@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index df401463a191..e49e8091f9ac 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -46,6 +46,14 @@ ktime_t tick_period;
  *    procedure also covers cpu hotplug.
  */
 int tick_do_timer_cpu __read_mostly = TICK_DO_TIMER_BOOT;
+#ifdef CONFIG_NO_HZ_FULL
+/*
+ * tick_do_timer_boot_cpu indicates the boot CPU temporarily owns
+ * tick_do_timer_cpu and it should be taken over by an eligible secondary
+ * when one comes online.
+ */
+static int tick_do_timer_boot_cpu __read_mostly = -1;
+#endif
 
 /*
  * Debugging: see timer_list.c
@@ -167,6 +175,26 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 	}
 }
 
+#ifdef CONFIG_NO_HZ_FULL
+static void giveup_do_timer(void *info)
+{
+	int cpu = *(unsigned int *)info;
+
+	WARN_ON(tick_do_timer_cpu != smp_processor_id());
+
+	tick_do_timer_cpu = cpu;
+}
+
+static void tick_take_do_timer_from_boot(void)
+{
+	int cpu = smp_processor_id();
+	int from = tick_do_timer_boot_cpu;
+
+	if (from >= 0 && from != cpu)
+		smp_call_function_single(from, giveup_do_timer, &cpu, 1);
+}
+#endif
+
 /*
  * Setup the tick device
  */
@@ -186,12 +214,26 @@ static void tick_setup_device(struct tick_device *td,
 		 * this cpu:
 		 */
 		if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
-			if (!tick_nohz_full_cpu(cpu))
-				tick_do_timer_cpu = cpu;
-			else
-				tick_do_timer_cpu = TICK_DO_TIMER_NONE;
+			tick_do_timer_cpu = cpu;
+
 			tick_next_period = ktime_get();
 			tick_period = NSEC_PER_SEC / HZ;
+#ifdef CONFIG_NO_HZ_FULL
+			/*
+			 * The boot CPU may be nohz_full, in which case set
+			 * tick_do_timer_boot_cpu so the first housekeeping
+			 * secondary that comes up will take do_timer from
+			 * us.
+			 */
+			if (tick_nohz_full_cpu(cpu))
+				tick_do_timer_boot_cpu = cpu;
+
+		} else if (tick_do_timer_boot_cpu != -1 &&
+						!tick_nohz_full_cpu(cpu)) {
+			tick_take_do_timer_from_boot();
+			tick_do_timer_boot_cpu = -1;
+			WARN_ON(tick_do_timer_cpu != cpu);
+#endif
 		}
 
 		/*

commit 3f2552f7e9c5abef2775c53f7af66532f8bf65bc
Author: Chang-An Chen <chang-an.chen@mediatek.com>
Date:   Fri Mar 29 10:59:09 2019 +0800

    timers/sched_clock: Prevent generic sched_clock wrap caused by tick_freeze()
    
    tick_freeze() introduced by suspend-to-idle in commit 124cf9117c5f ("PM /
    sleep: Make it possible to quiesce timers during suspend-to-idle") uses
    timekeeping_suspend() instead of syscore_suspend() during
    suspend-to-idle. As a consequence generic sched_clock will keep going
    because sched_clock_suspend() and sched_clock_resume() are not invoked
    during suspend-to-idle which can result in a generic sched_clock wrap.
    
    On a ARM system with suspend-to-idle enabled, sched_clock is registered
    as "56 bits at 13MHz, resolution 76ns, wraps every 4398046511101ns", which
    means the real wrapping duration is 8796093022202ns.
    
    [  134.551779] suspend-to-idle suspend (timekeeping_suspend())
    [ 1204.912239] suspend-to-idle resume (timekeeping_resume())
    ......
    [ 1206.912239] suspend-to-idle suspend (timekeeping_suspend())
    [ 5880.502807] suspend-to-idle resume (timekeeping_resume())
    ......
    [ 6000.403724] suspend-to-idle suspend (timekeeping_suspend())
    [ 8035.753167] suspend-to-idle resume  (timekeeping_resume())
    ......
    [ 8795.786684] (2)[321:charger_thread]......
    [ 8795.788387] (2)[321:charger_thread]......
    [    0.057226] (0)[0:swapper/0]......
    [    0.061447] (2)[0:swapper/2]......
    
    sched_clock was not stopped during suspend-to-idle, and sched_clock_poll
    hrtimer was not expired because timekeeping_suspend() was invoked during
    suspend-to-idle. It makes sched_clock wrap at kernel time 8796s.
    
    To prevent this, invoke sched_clock_suspend() and sched_clock_resume() in
    tick_freeze() together with timekeeping_suspend() and timekeeping_resume().
    
    Fixes: 124cf9117c5f (PM / sleep: Make it possible to quiesce timers during suspend-to-idle)
    Signed-off-by: Chang-An Chen <chang-an.chen@mediatek.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Matthias Brugger <matthias.bgg@gmail.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Corey Minyard <cminyard@mvista.com>
    Cc: <linux-mediatek@lists.infradead.org>
    Cc: <linux-arm-kernel@lists.infradead.org>
    Cc: Stanley Chu <stanley.chu@mediatek.com>
    Cc: <kuohong.wang@mediatek.com>
    Cc: <freddy.hsin@mediatek.com>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/1553828349-8914-1-git-send-email-chang-an.chen@mediatek.com

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 529143b4c8d2..df401463a191 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -487,6 +487,7 @@ void tick_freeze(void)
 		trace_suspend_resume(TPS("timekeeping_freeze"),
 				     smp_processor_id(), true);
 		system_state = SYSTEM_SUSPEND;
+		sched_clock_suspend();
 		timekeeping_suspend();
 	} else {
 		tick_suspend_local();
@@ -510,6 +511,7 @@ void tick_unfreeze(void)
 
 	if (tick_freeze_depth == num_online_cpus()) {
 		timekeeping_resume();
+		sched_clock_resume();
 		system_state = SYSTEM_RUNNING;
 		trace_suspend_resume(TPS("timekeeping_freeze"),
 				     smp_processor_id(), false);

commit e1e41b6ce5f9c1a80bf4f2404ec5ab11c6c5a2ad
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Mon Mar 18 20:55:56 2019 +0100

    timekeeping: Consistently use unsigned int for seqcount snapshot
    
    The timekeeping code uses a random mix of "unsigned long" and "unsigned
    int" for the seqcount snapshots (ratio 14:12). Since the seqlock.h API is
    entirely based on unsigned int, use that throughout.
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Stephen Boyd <sboyd@kernel.org>
    Link: https://lkml.kernel.org/r/20190318195557.20773-1-linux@rasmusvillemoes.dk

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 529143b4c8d2..561641b2153f 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -149,7 +149,7 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 	    !tick_broadcast_oneshot_active()) {
 		clockevents_switch_state(dev, CLOCK_EVT_STATE_PERIODIC);
 	} else {
-		unsigned long seq;
+		unsigned int seq;
 		ktime_t next;
 
 		do {

commit f49c174b5f431db9fa17315269e288d4548b651c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 31 19:21:10 2018 +0100

    hrtimers/tick/clockevents: Remove sloppy license references
    
    "For licencing details see kernel-base/COPYING" and similar license
    references have no value over the SPDX identifier. Remove them.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Acked-by: Corey Minyard <cminyard@mvista.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Peter Anvin <hpa@zytor.com>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: David Riley <davidriley@chromium.org>
    Cc: Colin Cross <ccross@android.com>
    Cc: Mark Brown <broonie@kernel.org>
    Link: https://lkml.kernel.org/r/20181031182252.963632760@linutronix.de

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 455b8d65a2b7..529143b4c8d2 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -6,9 +6,6 @@
  * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>
  * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar
  * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner
- *
- * This code is licenced under the GPL version 2. For details see
- * kernel-base/COPYING.
  */
 #include <linux/cpu.h>
 #include <linux/err.h>

commit 35728b8209ee7d25b6241a56304ee926469bd154
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 31 19:21:09 2018 +0100

    time: Add SPDX license identifiers
    
    Update the time(r) core files files with the correct SPDX license
    identifier based on the license text in the file itself. The SPDX
    identifier is a legally binding shorthand, which can be used instead of the
    full boiler plate text.
    
    This work is based on a script and data from Philippe Ombredanne, Kate
    Stewart and myself. The data has been created with two independent license
    scanners and manual inspection.
    
    The following files do not contain any direct license information and have
    been omitted from the big initial SPDX changes:
    
      timeconst.bc: The .bc files were not touched
      time.c, timer.c, timekeeping.c: Licence was deduced from EXPORT_SYMBOL_GPL
    
    As those files do not contain direct license references they fall under the
    project license, i.e. GPL V2 only.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Acked-by: Corey Minyard <cminyard@mvista.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: David Riley <davidriley@chromium.org>
    Cc: Colin Cross <ccross@android.com>
    Cc: Mark Brown <broonie@kernel.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: https://lkml.kernel.org/r/20181031182252.879109557@linutronix.de

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 7b5008039c2d..455b8d65a2b7 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains the base functions to manage periodic tick
  * related events.

commit 58c5fc2b96e4ae65068d815a1c3ca81da92fa1c9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 31 19:21:08 2018 +0100

    time: Remove useless filenames in top level comments
    
    Remove the pointless filenames in the top level comments. They have no
    value at all and just occupy space. While at it tidy up some of the
    comments and remove a stale one.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Acked-by: Corey Minyard <cminyard@mvista.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Peter Anvin <hpa@zytor.com>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: David Riley <davidriley@chromium.org>
    Cc: Colin Cross <ccross@android.com>
    Cc: Mark Brown <broonie@kernel.org>
    Link: https://lkml.kernel.org/r/20181031182252.794898238@linutronix.de

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 14de3727b18e..7b5008039c2d 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -1,6 +1,4 @@
 /*
- * linux/kernel/time/tick-common.c
- *
  * This file contains the base functions to manage periodic tick
  * related events.
  *

commit 5b5ccbc2b041f98f26b984e013d303b7f9e6fb8e
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Mon Jul 9 16:45:35 2018 +0100

    Revert "tick: Prefer a lower rating device only if it's CPU local device"
    
    This reverts commit 1332a90558013ae4242e3dd7934bdcdeafb06c0d.
    
    The original issue was not because of incorrect checking of cpumask for
    both new and old tick device. It was incorrectly analysed was due to the
    misunderstanding of the comment and misinterpretation of the return value
    from tick_check_preferred. The main issue is with the clockevent driver
    that sets the cpumask to cpu_all_mask instead of cpu_possible_mask.
    
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Kevin Hilman <khilman@baylibre.com>
    Tested-by: Martin Blumenstingl <martin.blumenstingl@googlemail.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Link: https://lkml.kernel.org/r/1531151136-18297-1-git-send-email-sudeep.holla@arm.com

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index b7005dd21ec1..14de3727b18e 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -277,8 +277,7 @@ static bool tick_check_preferred(struct clock_event_device *curdev,
 	 */
 	return !curdev ||
 		newdev->rating > curdev->rating ||
-	       (!cpumask_equal(curdev->cpumask, newdev->cpumask) &&
-	        !tick_check_percpu(curdev, newdev, smp_processor_id()));
+	       !cpumask_equal(curdev->cpumask, newdev->cpumask);
 }
 
 /*

commit 3c89adb0d11117f64d5b501730be7fb2bf53a479
Merge: 11e7c2188061 a24e16b1310c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 5 09:38:39 2018 -0700

    Merge tag 'pm-4.18-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "These include a significant update of the generic power domains
      (genpd) and Operating Performance Points (OPP) frameworks, mostly
      related to the introduction of power domain performance levels,
      cpufreq updates (new driver for Qualcomm Kryo processors, updates of
      the existing drivers, some core fixes, schedutil governor
      improvements), PCI power management fixes, ACPI workaround for
      EC-based wakeup events handling on resume from suspend-to-idle, and
      major updates of the turbostat and pm-graph utilities.
    
      Specifics:
    
       - Introduce power domain performance levels into the the generic
         power domains (genpd) and Operating Performance Points (OPP)
         frameworks (Viresh Kumar, Rajendra Nayak, Dan Carpenter).
    
       - Fix two issues in the runtime PM framework related to the
         initialization and removal of devices using device links (Ulf
         Hansson).
    
       - Clean up the initialization of drivers for devices in PM domains
         (Ulf Hansson, Geert Uytterhoeven).
    
       - Fix a cpufreq core issue related to the policy sysfs interface
         causing CPU online to fail for CPUs sharing one cpufreq policy in
         some situations (Tao Wang).
    
       - Make it possible to use platform-specific suspend/resume hooks in
         the cpufreq-dt driver and make the Armada 37xx DVFS use that
         feature (Viresh Kumar, Miquel Raynal).
    
       - Optimize policy transition notifications in cpufreq (Viresh Kumar).
    
       - Improve the iowait boost mechanism in the schedutil cpufreq
         governor (Patrick Bellasi).
    
       - Improve the handling of deferred frequency updates in the schedutil
         cpufreq governor (Joel Fernandes, Dietmar Eggemann, Rafael Wysocki,
         Viresh Kumar).
    
       - Add a new cpufreq driver for Qualcomm Kryo (Ilia Lin).
    
       - Fix and clean up some cpufreq drivers (Colin Ian King, Dmitry
         Osipenko, Doug Smythies, Luc Van Oostenryck, Simon Horman, Viresh
         Kumar).
    
       - Fix the handling of PCI devices with the DPM_SMART_SUSPEND flag set
         and update stale comments in the PCI core PM code (Rafael Wysocki).
    
       - Work around an issue related to the handling of EC-based wakeup
         events in the ACPI PM core during resume from suspend-to-idle if
         the EC has been put into the low-power mode (Rafael Wysocki).
    
       - Improve the handling of wakeup source objects in the PM core (Doug
         Berger, Mahendran Ganesh, Rafael Wysocki).
    
       - Update the driver core to prevent deferred probe from breaking
         suspend/resume ordering (Feng Kan).
    
       - Clean up the PM core somewhat (Bjorn Helgaas, Ulf Hansson, Rafael
         Wysocki).
    
       - Make the core suspend/resume code and cpufreq support the RT patch
         (Sebastian Andrzej Siewior, Thomas Gleixner).
    
       - Consolidate the PM QoS handling in cpuidle governors (Rafael
         Wysocki).
    
       - Fix a possible crash in the hibernation core (Tetsuo Handa).
    
       - Update the rockchip-io Adaptive Voltage Scaling (AVS) driver (David
         Wu).
    
       - Update the turbostat utility (fixes, cleanups, new CPU IDs, new
         command line options, built-in "Low Power Idle" counters support,
         new POLL and POLL% columns) and add an entry for it to MAINTAINERS
         (Len Brown, Artem Bityutskiy, Chen Yu, Laura Abbott, Matt Turner,
         Prarit Bhargava, Srinivas Pandruvada).
    
       - Update the pm-graph to version 5.1 (Todd Brandt).
    
       - Update the intel_pstate_tracer utility (Doug Smythies)"
    
    * tag 'pm-4.18-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (128 commits)
      tools/power turbostat: update version number
      tools/power turbostat: Add Node in output
      tools/power turbostat: add node information into turbostat calculations
      tools/power turbostat: remove num_ from cpu_topology struct
      tools/power turbostat: rename num_cores_per_pkg to num_cores_per_node
      tools/power turbostat: track thread ID in cpu_topology
      tools/power turbostat: Calculate additional node information for a package
      tools/power turbostat: Fix node and siblings lookup data
      tools/power turbostat: set max_num_cpus equal to the cpumask length
      tools/power turbostat: if --num_iterations, print for specific number of iterations
      tools/power turbostat: Add Cannon Lake support
      tools/power turbostat: delete duplicate #defines
      x86: msr-index.h: Correct SNB_C1/C3_AUTO_UNDEMOTE defines
      tools/power turbostat: Correct SNB_C1/C3_AUTO_UNDEMOTE defines
      tools/power turbostat: add POLL and POLL% column
      tools/power turbostat: Fix --hide Pk%pc10
      tools/power turbostat: Build-in "Low Power Idle" counters support
      tools/power turbostat: Don't make man pages executable
      tools/power turbostat: remove blank lines
      tools/power turbostat: a small C-states dump readability immprovement
      ...

commit c1a957d17086d20d52d7f9c8dffaeac2ee09d6f9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 25 17:54:41 2018 +0200

    PM / suspend: Prevent might sleep splats
    
    timekeeping suspend/resume calls read_persistent_clock() which takes
    rtc_lock. That results in might sleep warnings because at that point
    we run with interrupts disabled.
    
    We cannot convert rtc_lock to a raw spinlock as that would trigger
    other might sleep warnings.
    
    As a workaround we disable the might sleep warnings by setting
    system_state to SYSTEM_SUSPEND before calling sysdev_suspend() and
    restoring it to SYSTEM_RUNNING afer sysdev_resume(). There is no lock
    contention because hibernate / suspend to RAM is single-CPU at this
    point.
    
    In s2idle's case the system_state is set to SYSTEM_SUSPEND before
    timekeeping_suspend() which is invoked by the last CPU. In the resume
    case it set back to SYSTEM_RUNNING after timekeeping_resume() which is
    invoked by the first CPU in the resume case. The other CPUs will block
    on tick_freeze_lock.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [bigeasy: cover s2idle in tick_freeze() / tick_unfreeze()]
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 49edc1c4f3e6..14de3727b18e 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -490,6 +490,7 @@ void tick_freeze(void)
 	if (tick_freeze_depth == num_online_cpus()) {
 		trace_suspend_resume(TPS("timekeeping_freeze"),
 				     smp_processor_id(), true);
+		system_state = SYSTEM_SUSPEND;
 		timekeeping_suspend();
 	} else {
 		tick_suspend_local();
@@ -513,6 +514,7 @@ void tick_unfreeze(void)
 
 	if (tick_freeze_depth == num_online_cpus()) {
 		timekeeping_resume();
+		system_state = SYSTEM_RUNNING;
 		trace_suspend_resume(TPS("timekeeping_freeze"),
 				     smp_processor_id(), false);
 	} else {

commit 1332a90558013ae4242e3dd7934bdcdeafb06c0d
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Wed May 9 17:02:08 2018 +0100

    tick: Prefer a lower rating device only if it's CPU local device
    
    Checking the equality of cpumask for both new and old tick device doesn't
    ensure that it's CPU local device. This will cause issue if a low rating
    clockevent tick device is registered first followed by the registration
    of higher rating clockevent tick device.
    
    In such case, clockevents_released list will never get emptied as both
    the devices get selected as preferred one and we will loop forever in
    clockevents_notify_released.
    
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: https://lkml.kernel.org/r/1525881728-4858-1-git-send-email-sudeep.holla@arm.com

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 49edc1c4f3e6..78e598334007 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -277,7 +277,8 @@ static bool tick_check_preferred(struct clock_event_device *curdev,
 	 */
 	return !curdev ||
 		newdev->rating > curdev->rating ||
-	       !cpumask_equal(curdev->cpumask, newdev->cpumask);
+	       (!cpumask_equal(curdev->cpumask, newdev->cpumask) &&
+	        !tick_check_percpu(curdev, newdev, smp_processor_id()));
 }
 
 /*

commit a3ed0e4393d6885b4af7ce84b437dc696490a530
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 25 15:33:38 2018 +0200

    Revert: Unify CLOCK_MONOTONIC and CLOCK_BOOTTIME
    
    Revert commits
    
    92af4dcb4e1c ("tracing: Unify the "boot" and "mono" tracing clocks")
    127bfa5f4342 ("hrtimer: Unify MONOTONIC and BOOTTIME clock behavior")
    7250a4047aa6 ("posix-timers: Unify MONOTONIC and BOOTTIME clock behavior")
    d6c7270e913d ("timekeeping: Remove boot time specific code")
    f2d6fdbfd238 ("Input: Evdev - unify MONOTONIC and BOOTTIME clock behavior")
    d6ed449afdb3 ("timekeeping: Make the MONOTONIC clock behave like the BOOTTIME clock")
    72199320d49d ("timekeeping: Add the new CLOCK_MONOTONIC_ACTIVE clock")
    
    As stated in the pull request for the unification of CLOCK_MONOTONIC and
    CLOCK_BOOTTIME, it was clear that we might have to revert the change.
    
    As reported by several folks systemd and other applications rely on the
    documented behaviour of CLOCK_MONOTONIC on Linux and break with the above
    changes. After resume daemons time out and other timeout related issues are
    observed. Rafael compiled this list:
    
    * systemd kills daemons on resume, after >WatchdogSec seconds
      of suspending (Genki Sky).  [Verified that that's because systemd uses
      CLOCK_MONOTONIC and expects it to not include the suspend time.]
    
    * systemd-journald misbehaves after resume:
      systemd-journald[7266]: File /var/log/journal/016627c3c4784cd4812d4b7e96a34226/system.journal
    corrupted or uncleanly shut down, renaming and replacing.
      (Mike Galbraith).
    
    * NetworkManager reports "networking disabled" and networking is broken
      after resume 50% of the time (Pavel).  [May be because of systemd.]
    
    * MATE desktop dims the display and starts the screensaver right after
      system resume (Pavel).
    
    * Full system hang during resume (me).  [May be due to systemd or NM or both.]
    
    That happens on debian and open suse systems.
    
    It's sad, that these problems were neither catched in -next nor by those
    folks who expressed interest in this change.
    
    Reported-by: Rafael J. Wysocki <rjw@rjwysocki.net>
    Reported-by: Genki Sky <sky@genki.is>,
    Reported-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Kevin Easton <kevin@guarana.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Salyzyn <salyzyn@android.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 099572ca4a8f..49edc1c4f3e6 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -419,19 +419,6 @@ void tick_suspend_local(void)
 	clockevents_shutdown(td->evtdev);
 }
 
-static void tick_forward_next_period(void)
-{
-	ktime_t delta, now = ktime_get();
-	u64 n;
-
-	delta = ktime_sub(now, tick_next_period);
-	n = ktime_divns(delta, tick_period);
-	tick_next_period += n * tick_period;
-	if (tick_next_period < now)
-		tick_next_period += tick_period;
-	tick_sched_forward_next_period();
-}
-
 /**
  * tick_resume_local - Resume the local tick device
  *
@@ -444,8 +431,6 @@ void tick_resume_local(void)
 	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 	bool broadcast = tick_resume_check_broadcast();
 
-	tick_forward_next_period();
-
 	clockevents_tick_resume(td->evtdev);
 	if (!broadcast) {
 		if (td->mode == TICKDEV_MODE_PERIODIC)

commit d6ed449afdb38f89a7b38ec50e367559e1b8f71f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 1 17:33:33 2018 +0100

    timekeeping: Make the MONOTONIC clock behave like the BOOTTIME clock
    
    The MONOTONIC clock is not fast forwarded by the time spent in suspend on
    resume. This is only done for the BOOTTIME clock. The reason why the
    MONOTONIC clock is not forwarded is historical: the original Linux
    implementation was using jiffies as a base for the MONOTONIC clock and
    jiffies have never been advanced after resume.
    
    At some point when timekeeping was unified in the core code, the
    MONONOTIC clock was advanced after resume which also advanced jiffies causing
    interesting side effects. As a consequence the the MONOTONIC clock forwarding
    was disabled again and the BOOTTIME clock was introduced, which allows to read
    time since boot.
    
    Back then it was not possible to completely distangle the MONOTONIC clock and
    jiffies because there were still interfaces which exposed the MONOTONIC clock
    behaviour based on the timer wheel and therefore jiffies.
    
    As of today none of the MONOTONIC clock facilities depends on jiffies
    anymore so the forwarding can be done seperately. This is achieved by
    forwarding the variables which are used for the jiffies update after resume
    before the tick is restarted,
    
    In timekeeping resume, the change is rather simple. Instead of updating the
    offset between the MONOTONIC clock and the REALTIME/BOOTTIME clocks, advance the
    time keeper base for the MONOTONIC and the MONOTONIC_RAW clocks by the time
    spent in suspend.
    
    The MONOTONIC clock is now the same as the BOOTTIME clock and the offset between
    the REALTIME and the MONOTONIC clocks is the same as before suspend.
    
    There might be side effects in applications, which rely on the
    (unfortunately) well documented behaviour of the MONOTONIC clock, but the
    downsides of the existing behaviour are probably worse.
    
    There is one obvious issue. Up to now it was possible to retrieve the time
    spent in suspend by observing the delta between the MONOTONIC clock and the
    BOOTTIME clock. This is not longer available, but the previously introduced
    mechanism to read the active non-suspended monotonic time can mitigate that
    in a detectable fashion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Kevin Easton <kevin@guarana.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Salyzyn <salyzyn@android.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20180301165150.062975504@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 49edc1c4f3e6..099572ca4a8f 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -419,6 +419,19 @@ void tick_suspend_local(void)
 	clockevents_shutdown(td->evtdev);
 }
 
+static void tick_forward_next_period(void)
+{
+	ktime_t delta, now = ktime_get();
+	u64 n;
+
+	delta = ktime_sub(now, tick_next_period);
+	n = ktime_divns(delta, tick_period);
+	tick_next_period += n * tick_period;
+	if (tick_next_period < now)
+		tick_next_period += tick_period;
+	tick_sched_forward_next_period();
+}
+
 /**
  * tick_resume_local - Resume the local tick device
  *
@@ -431,6 +444,8 @@ void tick_resume_local(void)
 	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 	bool broadcast = tick_resume_check_broadcast();
 
+	tick_forward_next_period();
+
 	clockevents_tick_resume(td->evtdev);
 	if (!broadcast) {
 		if (td->mode == TICKDEV_MODE_PERIODIC)

commit 8b0e195314fabd58a331c4f7b6db75a1565535d7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Dec 25 12:30:41 2016 +0100

    ktime: Cleanup ktime_set() usage
    
    ktime_set(S,N) was required for the timespec storage type and is still
    useful for situations where a Seconds and Nanoseconds part of a time value
    needs to be converted. For anything where the Seconds argument is 0, this
    is pointless and can be replaced with a simple assignment.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 4fcd99e12aa0..49edc1c4f3e6 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -178,8 +178,8 @@ static void tick_setup_device(struct tick_device *td,
 			      struct clock_event_device *newdev, int cpu,
 			      const struct cpumask *cpumask)
 {
-	ktime_t next_event;
 	void (*handler)(struct clock_event_device *) = NULL;
+	ktime_t next_event = 0;
 
 	/*
 	 * First device setup ?
@@ -195,7 +195,7 @@ static void tick_setup_device(struct tick_device *td,
 			else
 				tick_do_timer_cpu = TICK_DO_TIMER_NONE;
 			tick_next_period = ktime_get();
-			tick_period = ktime_set(0, NSEC_PER_SEC / HZ);
+			tick_period = NSEC_PER_SEC / HZ;
 		}
 
 		/*

commit eef7635a22f6b144206b5ca2f1398f637acffc4d
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Fri Sep 11 09:34:26 2015 +0530

    clockevents: Remove unused set_mode() callback
    
    All users are migrated to the per-state callbacks, get rid of the
    unused interface and the core support code.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linaro-kernel@lists.linaro.org
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/fd60de14cf6d125489c031207567bb255ad946f6.1441943991.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index d11c55b6ab7d..4fcd99e12aa0 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -398,7 +398,6 @@ void tick_shutdown(unsigned int cpu)
 		 * the set mode function!
 		 */
 		clockevent_set_state(dev, CLOCK_EVT_STATE_DETACHED);
-		dev->mode = CLOCK_EVT_MODE_UNUSED;
 		clockevents_exchange_device(dev, NULL);
 		dev->event_handler = clockevents_handle_noop;
 		td->evtdev = NULL;

commit d74892c5b291c0010295d26d6b1e11cd70451722
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Wed Jul 29 15:14:17 2015 -0400

    clockevents: Drop redundant cpumask check in tick_check_new_device()
    
    The same check is performed by tick_check_percpu().
    
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Link: http://lkml.kernel.org/r/20150729151417.069d1bb0@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index f8bf47571dda..d11c55b6ab7d 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -304,9 +304,6 @@ void tick_check_new_device(struct clock_event_device *newdev)
 	int cpu;
 
 	cpu = smp_processor_id();
-	if (!cpumask_test_cpu(cpu, newdev->cpumask))
-		goto out_bc;
-
 	td = &per_cpu(tick_cpu_device, cpu);
 	curdev = td->evtdev;
 

commit 0f44705175347ec96935d60b765b5d14ecc763bb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 14 12:01:04 2015 +0200

    tick: Move the export of tick_broadcast_oneshot_control to the proper place
    
    tick_broadcast_oneshot_control got moved from tick-broadcast to
    tick-common, but the export stayed in the old place. Fix it up.
    
    Fixes: f32dd1170511 'tick/broadcast: Make idle check independent from mode and config'
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 55e13efff1ab..f8bf47571dda 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -363,6 +363,7 @@ int tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 
 	return __tick_broadcast_oneshot_control(state);
 }
+EXPORT_SYMBOL_GPL(tick_broadcast_oneshot_control);
 
 #ifdef CONFIG_HOTPLUG_CPU
 /*

commit f32dd117051185da6e923b35491a44d7debeeea5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 16:29:38 2015 +0200

    tick/broadcast: Make idle check independent from mode and config
    
    Currently the broadcast busy check, which prevents the idle code from
    going into deep idle, works only in one shot mode.
    
    If NOHZ and HIGHRES are off (config or command line) there is no
    sanity check at all, so under certain conditions cpus are allowed to
    go into deep idle, where the local timer stops, and are not woken up
    again because there is no broadcast timer installed or a hrtimer based
    broadcast device is not evaluated.
    
    Move tick_broadcast_oneshot_control() into the common code and provide
    proper subfunctions for the various config combinations.
    
    The common check in tick_broadcast_oneshot_control() is for the C3STOP
    misfeature flag of the local clock event device. If its not set, idle
    can proceed. If set, further checks are necessary.
    
    Provide checks for the trivial cases:
    
     - If broadcast is disabled in the config, then return busy
    
     - If oneshot mode (NOHZ/HIGHES) is disabled in the config, return
       busy if the broadcast device is hrtimer based.
    
     - If oneshot mode is enabled in the config call the original
       tick_broadcast_oneshot_control() function. That function needs
       extra checks which will be implemented in seperate patches.
    
    [ Split out from a larger combo patch ]
    
    Reported-and-tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <Catalin.Marinas@arm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1507070929360.3916@nanos

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 76446cb5dfe1..55e13efff1ab 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -343,6 +343,27 @@ void tick_check_new_device(struct clock_event_device *newdev)
 	tick_install_broadcast_device(newdev);
 }
 
+/**
+ * tick_broadcast_oneshot_control - Enter/exit broadcast oneshot mode
+ * @state:	The target state (enter/exit)
+ *
+ * The system enters/leaves a state, where affected devices might stop
+ * Returns 0 on success, -EBUSY if the cpu is used to broadcast wakeups.
+ *
+ * Called with interrupts disabled, so clockevents_lock is not
+ * required here because the local clock event device cannot go away
+ * under us.
+ */
+int tick_broadcast_oneshot_control(enum tick_broadcast_state state)
+{
+	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
+
+	if (!(td->evtdev->features & CLOCK_EVT_FEAT_C3STOP))
+		return 0;
+
+	return __tick_broadcast_oneshot_control(state);
+}
+
 #ifdef CONFIG_HOTPLUG_CPU
 /*
  * Transfer the do_timer job away from a dying cpu.

commit 43c9fad942b5afb9e03801c0721d83160fa5b0dd
Merge: cb8a4deaf9b2 d461003574eb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 23 14:18:07 2015 -0700

    Merge tag 'pm+acpi-4.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management and ACPI updates from Rafael Wysocki:
     "The rework of backlight interface selection API from Hans de Goede
      stands out from the number of commits and the number of affected
      places perspective.  The cpufreq core fixes from Viresh Kumar are
      quite significant too as far as the number of commits goes and because
      they should reduce CPU online/offline overhead quite a bit in the
      majority of cases.
    
      From the new featues point of view, the ACPICA update (to upstream
      revision 20150515) adding support for new ACPI 6 material to ACPICA is
      the one that matters the most as some new significant features will be
      based on it going forward.  Also included is an update of the ACPI
      device power management core to follow ACPI 6 (which in turn reflects
      the Windows' device PM implementation), a PM core extension to support
      wakeup interrupts in a more generic way and support for the ACPI _CCA
      device configuration object.
    
      The rest is mostly fixes and cleanups all over and some documentation
      updates, including new DT bindings for Operating Performance Points.
    
      There is one fix for a regression introduced in the 4.1 cycle, but it
      adds quite a number of lines of code, it wasn't really ready before
      Thursday and you were on vacation, so I refrained from pushing it on
      the last minute for 4.1.
    
      Specifics:
    
       - ACPICA update to upstream revision 20150515 including basic support
         for ACPI 6 features: new ACPI tables introduced by ACPI 6 (STAO,
         XENV, WPBT, NFIT, IORT), changes related to the other tables (DTRM,
         FADT, LPIT, MADT), new predefined names (_BTH, _CR3, _DSD, _LPI,
         _MTL, _PRR, _RDI, _RST, _TFP, _TSN), fixes and cleanups (Bob Moore,
         Lv Zheng).
    
       - ACPI device power management core code update to follow ACPI 6
         which reflects the ACPI device power management implementation in
         Windows (Rafael J Wysocki).
    
       - rework of the backlight interface selection logic to reduce the
         number of kernel command line options and improve the handling of
         DMI quirks that may be involved in that and to make the code
         generally more straightforward (Hans de Goede).
    
       - fixes for the ACPI Embedded Controller (EC) driver related to the
         handling of EC transactions (Lv Zheng).
    
       - fix for a regression related to the ACPI resources management and
         resulting from a recent change of ACPI initialization code ordering
         (Rafael J Wysocki).
    
       - fix for a system initialization regression related to ACPI
         introduced during the 3.14 cycle and caused by running the code
         that switches the platform over to the ACPI mode too early in the
         initialization sequence (Rafael J Wysocki).
    
       - support for the ACPI _CCA device configuration object related to
         DMA cache coherence (Suravee Suthikulpanit).
    
       - ACPI/APEI fixes and cleanups (Jiri Kosina, Borislav Petkov).
    
       - ACPI battery driver cleanups (Luis Henriques, Mathias Krause).
    
       - ACPI processor driver cleanups (Hanjun Guo).
    
       - cleanups and documentation update related to the ACPI device
         properties interface based on _DSD (Rafael J Wysocki).
    
       - ACPI device power management fixes (Rafael J Wysocki).
    
       - assorted cleanups related to ACPI (Dominik Brodowski, Fabian
         Frederick, Lorenzo Pieralisi, Mathias Krause, Rafael J Wysocki).
    
       - fix for a long-standing issue causing General Protection Faults to
         be generated occasionally on return to user space after resume from
         ACPI-based suspend-to-RAM on 32-bit x86 (Ingo Molnar).
    
       - fix to make the suspend core code return -EBUSY consistently in all
         cases when system suspend is aborted due to wakeup detection (Ruchi
         Kandoi).
    
       - support for automated device wakeup IRQ handling allowing drivers
         to make their PM support more starightforward (Tony Lindgren).
    
       - new tracepoints for suspend-to-idle tracing and rework of the
         prepare/complete callbacks tracing in the PM core (Todd E Brandt,
         Rafael J Wysocki).
    
       - wakeup sources framework enhancements (Jin Qian).
    
       - new macro for noirq system PM callbacks (Grygorii Strashko).
    
       - assorted cleanups related to system suspend (Rafael J Wysocki).
    
       - cpuidle core cleanups to make the code more efficient (Rafael J
         Wysocki).
    
       - powernv/pseries cpuidle driver update (Shilpasri G Bhat).
    
       - cpufreq core fixes related to CPU online/offline that should reduce
         the overhead of these operations quite a bit, unless the CPU in
         question is physically going away (Viresh Kumar, Saravana Kannan).
    
       - serialization of cpufreq governor callbacks to avoid race
         conditions in some cases (Viresh Kumar).
    
       - intel_pstate driver fixes and cleanups (Doug Smythies, Prarit
         Bhargava, Joe Konno).
    
       - cpufreq driver (arm_big_little, cpufreq-dt, qoriq) updates (Sudeep
         Holla, Felipe Balbi, Tang Yuantian).
    
       - assorted cleanups in cpufreq drivers and core (Shailendra Verma,
         Fabian Frederick, Wang Long).
    
       - new Device Tree bindings for representing Operating Performance
         Points (Viresh Kumar).
    
       - updates for the common clock operations support code in the PM core
         (Rajendra Nayak, Geert Uytterhoeven).
    
       - PM domains core code update (Geert Uytterhoeven).
    
       - Intel Knights Landing support for the RAPL (Running Average Power
         Limit) power capping driver (Dasaratharaman Chandramouli).
    
       - fixes related to the floor frequency setting on Atom SoCs in the
         RAPL power capping driver (Ajay Thomas).
    
       - runtime PM framework documentation update (Ben Dooks).
    
       - cpupower tool fix (Herton R Krzesinski)"
    
    * tag 'pm+acpi-4.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (194 commits)
      cpuidle: powernv/pseries: Auto-promotion of snooze to deeper idle state
      x86: Load __USER_DS into DS/ES after resume
      PM / OPP: Add binding for 'opp-suspend'
      PM / OPP: Allow multiple OPP tables to be passed via DT
      PM / OPP: Add new bindings to address shortcomings of existing bindings
      ACPI: Constify ACPI device IDs in documentation
      ACPI / enumeration: Document the rules regarding the PRP0001 device ID
      ACPI / video: Make acpi_video_unregister_backlight() private
      acpi-video-detect: Remove old API
      toshiba-acpi: Port to new backlight interface selection API
      thinkpad-acpi: Port to new backlight interface selection API
      sony-laptop: Port to new backlight interface selection API
      samsung-laptop: Port to new backlight interface selection API
      msi-wmi: Port to new backlight interface selection API
      msi-laptop: Port to new backlight interface selection API
      intel-oaktrail: Port to new backlight interface selection API
      ideapad-laptop: Port to new backlight interface selection API
      fujitsu-laptop: Port to new backlight interface selection API
      eeepc-laptop: Port to new backlight interface selection API
      dell-wmi: Port to new backlight interface selection API
      ...

commit 051ebd101b05c09d9b5b673e19fb0586e9bfec56
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 2 14:13:46 2015 +0200

    clockevents: Use set/get state helper functions
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 311e2e133517..17f144450050 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -377,7 +377,7 @@ void tick_shutdown(unsigned int cpu)
 		 * Prevent that the clock events layer tries to call
 		 * the set mode function!
 		 */
-		dev->state = CLOCK_EVT_STATE_DETACHED;
+		clockevent_set_state(dev, CLOCK_EVT_STATE_DETACHED);
 		dev->mode = CLOCK_EVT_MODE_UNUSED;
 		clockevents_exchange_device(dev, NULL);
 		dev->event_handler = clockevents_handle_noop;

commit d7eb231c71420bc34ac3d35403115600f920cfc2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 2 14:08:46 2015 +0200

    clockevents: Provide functions to set and get the state
    
    We want to rename dev->state, so provide proper get and set
    functions. Rename clockevents_set_state() to
    clockevents_switch_state() to avoid confusion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index cf881c62c3c5..311e2e133517 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -150,7 +150,7 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 
 	if ((dev->features & CLOCK_EVT_FEAT_PERIODIC) &&
 	    !tick_broadcast_oneshot_active()) {
-		clockevents_set_state(dev, CLOCK_EVT_STATE_PERIODIC);
+		clockevents_switch_state(dev, CLOCK_EVT_STATE_PERIODIC);
 	} else {
 		unsigned long seq;
 		ktime_t next;
@@ -160,7 +160,7 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 			next = tick_next_period;
 		} while (read_seqretry(&jiffies_lock, seq));
 
-		clockevents_set_state(dev, CLOCK_EVT_STATE_ONESHOT);
+		clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);
 
 		for (;;) {
 			if (!clockevents_program_event(dev, next, false))

commit 472c4a9437d3c6a0b1e59df7c5aa14075946aa70
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu May 21 13:33:46 2015 +0530

    clockevents: Use helpers to check the state of a clockevent device
    
    Use accessor functions to check the state of clockevent devices in
    core code.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: linaro-kernel@lists.linaro.org
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/fa2b9869fd17f210eaa156ec2b594efd0230b6c7.1432192527.git.viresh.kumar@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index ea5f9eae8f74..cf881c62c3c5 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -112,7 +112,7 @@ void tick_handle_periodic(struct clock_event_device *dev)
 		return;
 #endif
 
-	if (dev->state != CLOCK_EVT_STATE_ONESHOT)
+	if (!clockevent_state_oneshot(dev))
 		return;
 	for (;;) {
 		/*

commit 87e9b9f1d86c2ee9a10c2a4186a72d0af4cc963e
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat May 16 01:38:15 2015 +0200

    PM / sleep: Make suspend-to-idle-specific code depend on CONFIG_SUSPEND
    
    Since idle_should_freeze() is defined to always return 'false'
    for CONFIG_SUSPEND unset, all of the code depending on it in
    cpuidle_idle_call() is not necessary in that case.
    
    Make that code depend on CONFIG_SUSPEND too to avoid building it
    when it is not going to be used.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 80c043052487..51508465153c 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -441,6 +441,7 @@ void tick_resume(void)
 	tick_resume_local();
 }
 
+#ifdef CONFIG_SUSPEND
 static DEFINE_RAW_SPINLOCK(tick_freeze_lock);
 static unsigned int tick_freeze_depth;
 
@@ -494,6 +495,7 @@ void tick_unfreeze(void)
 
 	raw_spin_unlock(&tick_freeze_lock);
 }
+#endif /* CONFIG_SUSPEND */
 
 /**
  * tick_init - initialize the tick control

commit 75e0678e7095c486a1b39ea560f8eb51a2714d6d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sun May 10 01:23:35 2015 +0200

    PM / tick: Add tracepoints for suspend-to-idle diagnostics
    
    Add suspend/resume tracepoints to tick_freeze() and tick_unfreeze()
    to catch when timekeeping is suspended and resumed during suspend-to-idle
    so as to be able to check whether or not we enter the "frozen" state
    and to measure the time spent in it.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 3ae6afa1eb98..80c043052487 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -19,6 +19,7 @@
 #include <linux/profile.h>
 #include <linux/sched.h>
 #include <linux/module.h>
+#include <trace/events/power.h>
 
 #include <asm/irq_regs.h>
 
@@ -457,10 +458,13 @@ void tick_freeze(void)
 	raw_spin_lock(&tick_freeze_lock);
 
 	tick_freeze_depth++;
-	if (tick_freeze_depth == num_online_cpus())
+	if (tick_freeze_depth == num_online_cpus()) {
+		trace_suspend_resume(TPS("timekeeping_freeze"),
+				     smp_processor_id(), true);
 		timekeeping_suspend();
-	else
+	} else {
 		tick_suspend_local();
+	}
 
 	raw_spin_unlock(&tick_freeze_lock);
 }
@@ -478,10 +482,13 @@ void tick_unfreeze(void)
 {
 	raw_spin_lock(&tick_freeze_lock);
 
-	if (tick_freeze_depth == num_online_cpus())
+	if (tick_freeze_depth == num_online_cpus()) {
 		timekeeping_resume();
-	else
+		trace_suspend_resume(TPS("timekeeping_freeze"),
+				     smp_processor_id(), false);
+	} else {
 		tick_resume_local();
+	}
 
 	tick_freeze_depth--;
 

commit c6eb3f70d4482806dc2d3e1e3c7736f497b1d418
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Apr 14 21:08:51 2015 +0000

    hrtimer: Get rid of hrtimer softirq
    
    hrtimer softirq is a leftover from the initial implementation and
    serves only the purpose to handle the enqueueing of already expired
    timers in the high resolution timer mode. We discussed whether we
    change the return value and force all start sites to handle that the
    timer is already expired, but that would be a Herculean task and I'm
    not sure whether its a good idea to enforce that handling on
    everyone.
    
    A simpler solution is to enforce a timer interrupt instead of raising
    and scheduling a softirq. Just use the existing infrastructure to do
    so and remove all the softirq leftovers.
    
    The HRTIMER softirq enum is now unused, but kept around because trace
    parsers rely on the existing numbering.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20150414203501.840834708@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 3ae6afa1eb98..ea5f9eae8f74 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -102,6 +102,16 @@ void tick_handle_periodic(struct clock_event_device *dev)
 
 	tick_periodic(cpu);
 
+#if defined(CONFIG_HIGH_RES_TIMERS) || defined(CONFIG_NO_HZ_COMMON)
+	/*
+	 * The cpu might have transitioned to HIGHRES or NOHZ mode via
+	 * update_process_times() -> run_local_timers() ->
+	 * hrtimer_run_queues().
+	 */
+	if (dev->event_handler != tick_handle_periodic)
+		return;
+#endif
+
 	if (dev->state != CLOCK_EVT_STATE_ONESHOT)
 		return;
 	for (;;) {

commit def747087e83aa5f6a71582cfa71e18341988688
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Apr 3 15:31:32 2015 +0200

    timers/PM: Drop unnecessary braces from tick_freeze()
    
    Some braces in tick_freeze() are not necessary, so drop them.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: peterz@infradead.org
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1534128.H5hN3KBFB4@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index ad66a51ca4fa..3ae6afa1eb98 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -457,11 +457,10 @@ void tick_freeze(void)
 	raw_spin_lock(&tick_freeze_lock);
 
 	tick_freeze_depth++;
-	if (tick_freeze_depth == num_online_cpus()) {
+	if (tick_freeze_depth == num_online_cpus())
 		timekeeping_suspend();
-	} else {
+	else
 		tick_suspend_local();
-	}
 
 	raw_spin_unlock(&tick_freeze_lock);
 }

commit 422fe7502e3f16dc1c680f22d31f59f022edc10d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Apr 3 15:21:51 2015 +0200

    timers/PM: Fix up tick_unfreeze()
    
    A recent conflict resolution has left tick_resume() in
    tick_unfreeze() which leads to an unbalanced execution of
    tick_resume_broadcast() every time that function runs.
    
    Fix that by replacing the tick_resume() in tick_unfreeze()
    with tick_resume_local() as appropriate.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: boris.ostrovsky@oracle.com
    Cc: david.vrabel@citrix.com
    Cc: konrad.wilk@oracle.com
    Cc: peterz@infradead.org
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/8099075.V0LvN3pQAV@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index fac3e98fec49..ad66a51ca4fa 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -482,7 +482,7 @@ void tick_unfreeze(void)
 	if (tick_freeze_depth == num_online_cpus())
 		timekeeping_resume();
 	else
-		tick_resume();
+		tick_resume_local();
 
 	tick_freeze_depth--;
 

commit a49b116dcb1265f238f3169507424257b0519069
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:38:05 2015 +0200

    clockevents: Cleanup dead cpu explicitely
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call. We are way better off to have explicit
    calls instead of this monstrosity.
    
    Split out the cleanup function for a dead cpu and invoke it
    directly from the cpu down code. Make it conditional on
    CPU_HOTPLUG as well.
    
    Temporary change, will be refined in the future.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Rebased, added clockevents_notify() removal ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1735025.raBZdQHM3m@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 055c868f3ec9..fac3e98fec49 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -348,7 +348,6 @@ void tick_handover_do_timer(void)
 			TICK_DO_TIMER_NONE;
 	}
 }
-#endif
 
 /*
  * Shutdown an event device on a given cpu:
@@ -357,9 +356,9 @@ void tick_handover_do_timer(void)
  * access the hardware device itself.
  * We just set the mode and remove it from the lists.
  */
-void tick_shutdown(unsigned int *cpup)
+void tick_shutdown(unsigned int cpu)
 {
-	struct tick_device *td = &per_cpu(tick_cpu_device, *cpup);
+	struct tick_device *td = &per_cpu(tick_cpu_device, cpu);
 	struct clock_event_device *dev = td->evtdev;
 
 	td->mode = TICKDEV_MODE_PERIODIC;
@@ -375,6 +374,7 @@ void tick_shutdown(unsigned int *cpup)
 		td->evtdev = NULL;
 	}
 }
+#endif
 
 /**
  * tick_suspend_local - Suspend the local tick device

commit 52c063d1adbc16c76e70fffa20727fcd4e9343b3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:37:24 2015 +0200

    clockevents: Make tick handover explicit
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call. We are way better off to have explicit
    calls instead of this monstrosity.
    
    Split out the tick_handover call and invoke it explicitely from
    the hotplug code. Temporary solution will be cleaned up in later
    patches.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Rebase ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1658173.RkEEILFiQZ@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index e28ba5c044c5..055c868f3ec9 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -332,20 +332,23 @@ void tick_check_new_device(struct clock_event_device *newdev)
 	tick_install_broadcast_device(newdev);
 }
 
+#ifdef CONFIG_HOTPLUG_CPU
 /*
  * Transfer the do_timer job away from a dying cpu.
  *
- * Called with interrupts disabled.
+ * Called with interrupts disabled. Not locking required. If
+ * tick_do_timer_cpu is owned by this cpu, nothing can change it.
  */
-void tick_handover_do_timer(int *cpup)
+void tick_handover_do_timer(void)
 {
-	if (*cpup == tick_do_timer_cpu) {
+	if (tick_do_timer_cpu == smp_processor_id()) {
 		int cpu = cpumask_first(cpu_online_mask);
 
 		tick_do_timer_cpu = (cpu < nr_cpu_ids) ? cpu :
 			TICK_DO_TIMER_NONE;
 	}
 }
+#endif
 
 /*
  * Shutdown an event device on a given cpu:

commit 7270d11c56f594af4d166b2988421cd8ed933dc1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 13:11:52 2015 +0100

    arm/bL_switcher: Kill tick suspend hackery
    
    Use the new tick_suspend/resume_local() and get rid of the
    homebrewn implementation of these in the ARM bL switcher.  The
    check for the cpumask is completely pointless.  There is no harm
    to suspend a per cpu tick device unconditionally.  If that's a
    real issue then we fix it proper at the core level and not with
    some completely undocumented hacks in some random core code.
    
    Move the tick internals to the core code, now that this nuisance
    is gone.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ rjw: Rebase, changelog ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Link: http://lkml.kernel.org/r/1655112.Ws17YsMfN7@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index da796d65d1fb..e28ba5c044c5 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -380,7 +380,7 @@ void tick_shutdown(unsigned int *cpup)
  *
  * No locks required. Nothing can change the per cpu device.
  */
-static void tick_suspend_local(void)
+void tick_suspend_local(void)
 {
 	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 

commit f46481d0a7cb942b84145acb80ad43bdb1ff8eb4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 13:11:04 2015 +0100

    tick/xen: Provide and use tick_suspend_local() and tick_resume_local()
    
    Xen calls on every cpu into tick_resume() which is just wrong.
    tick_resume() is for the syscore global suspend/resume
    invocation. What XEN really wants is a per cpu local resume
    function.
    
    Provide a tick_resume_local() function and use it in XEN.
    
    Also provide a complementary tick_suspend_local() and modify
    tick_unfreeze() and tick_freeze(), respectively, to use the
    new local tick resume/suspend functions.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Combined two patches, rebased, modified subject/changelog. ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1698741.eezk9tnXtG@vostro.rjw.lan
    [ Merged to latest timers/core. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 1a60c2ae96a8..da796d65d1fb 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -374,40 +374,32 @@ void tick_shutdown(unsigned int *cpup)
 }
 
 /**
- * tick_suspend - Suspend the tick and the broadcast device
+ * tick_suspend_local - Suspend the local tick device
  *
- * Called from syscore_suspend() via timekeeping_suspend with only one
- * CPU online and interrupts disabled or from tick_unfreeze() under
- * tick_freeze_lock.
+ * Called from the local cpu for freeze with interrupts disabled.
  *
  * No locks required. Nothing can change the per cpu device.
  */
-void tick_suspend(void)
+static void tick_suspend_local(void)
 {
 	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 
 	clockevents_shutdown(td->evtdev);
-	tick_suspend_broadcast();
 }
 
 /**
- * tick_resume - Resume the tick and the broadcast device
+ * tick_resume_local - Resume the local tick device
  *
- * Called from syscore_resume() via timekeeping_resume with only one
- * CPU online and interrupts disabled or from tick_unfreeze() under
- * tick_freeze_lock.
+ * Called from the local CPU for unfreeze or XEN resume magic.
  *
  * No locks required. Nothing can change the per cpu device.
  */
-void tick_resume(void)
+void tick_resume_local(void)
 {
-	struct tick_device *td;
-	int broadcast;
+	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
+	bool broadcast = tick_resume_check_broadcast();
 
-	broadcast = tick_resume_broadcast();
-	td = this_cpu_ptr(&tick_cpu_device);
 	clockevents_tick_resume(td->evtdev);
-
 	if (!broadcast) {
 		if (td->mode == TICKDEV_MODE_PERIODIC)
 			tick_setup_periodic(td->evtdev, 0);
@@ -416,6 +408,35 @@ void tick_resume(void)
 	}
 }
 
+/**
+ * tick_suspend - Suspend the tick and the broadcast device
+ *
+ * Called from syscore_suspend() via timekeeping_suspend with only one
+ * CPU online and interrupts disabled or from tick_unfreeze() under
+ * tick_freeze_lock.
+ *
+ * No locks required. Nothing can change the per cpu device.
+ */
+void tick_suspend(void)
+{
+	tick_suspend_local();
+	tick_suspend_broadcast();
+}
+
+/**
+ * tick_resume - Resume the tick and the broadcast device
+ *
+ * Called from syscore_resume() via timekeeping_resume with only one
+ * CPU online and interrupts disabled.
+ *
+ * No locks required. Nothing can change the per cpu device.
+ */
+void tick_resume(void)
+{
+	tick_resume_broadcast();
+	tick_resume_local();
+}
+
 static DEFINE_RAW_SPINLOCK(tick_freeze_lock);
 static unsigned int tick_freeze_depth;
 
@@ -436,7 +457,7 @@ void tick_freeze(void)
 	if (tick_freeze_depth == num_online_cpus()) {
 		timekeeping_suspend();
 	} else {
-		tick_suspend();
+		tick_suspend_local();
 	}
 
 	raw_spin_unlock(&tick_freeze_lock);

commit 4ffee521f36390c7720d493591b764ca35c8030b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 13:09:16 2015 +0100

    clockevents: Make suspend/resume calls explicit
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call.
    
    We are way better off to have explicit calls instead of this
    monstrosity. Split out the suspend/resume() calls and invoke
    them directly from the call sites.
    
    No locking required at this point because these calls happen
    with interrupts disabled and a single cpu online.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Rebased on top of 4.0-rc5. ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/713674030.jVm1qaHuPf@vostro.rjw.lan
    [ Rebased on top of latest timers/core. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index a5b877130ae9..1a60c2ae96a8 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -373,18 +373,39 @@ void tick_shutdown(unsigned int *cpup)
 	}
 }
 
+/**
+ * tick_suspend - Suspend the tick and the broadcast device
+ *
+ * Called from syscore_suspend() via timekeeping_suspend with only one
+ * CPU online and interrupts disabled or from tick_unfreeze() under
+ * tick_freeze_lock.
+ *
+ * No locks required. Nothing can change the per cpu device.
+ */
 void tick_suspend(void)
 {
 	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 
 	clockevents_shutdown(td->evtdev);
+	tick_suspend_broadcast();
 }
 
+/**
+ * tick_resume - Resume the tick and the broadcast device
+ *
+ * Called from syscore_resume() via timekeeping_resume with only one
+ * CPU online and interrupts disabled or from tick_unfreeze() under
+ * tick_freeze_lock.
+ *
+ * No locks required. Nothing can change the per cpu device.
+ */
 void tick_resume(void)
 {
-	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
-	int broadcast = tick_resume_broadcast();
+	struct tick_device *td;
+	int broadcast;
 
+	broadcast = tick_resume_broadcast();
+	td = this_cpu_ptr(&tick_cpu_device);
 	clockevents_tick_resume(td->evtdev);
 
 	if (!broadcast) {
@@ -416,7 +437,6 @@ void tick_freeze(void)
 		timekeeping_suspend();
 	} else {
 		tick_suspend();
-		tick_suspend_broadcast();
 	}
 
 	raw_spin_unlock(&tick_freeze_lock);

commit 77e32c89a7117614ab3d66d20c1088de721abfaa
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Fri Feb 27 17:21:33 2015 +0530

    clockevents: Manage device's state separately for the core
    
    'enum clock_event_mode' is used for two purposes today:
    
     - to pass mode to the driver of clockevent device::set_mode().
    
     - for managing state of the device for clockevents core.
    
    For supporting new modes/states we have moved away from the
    legacy set_mode() callback to new per-mode/state callbacks. New
    modes/states shouldn't be exposed to the legacy (now OBSOLOTE)
    callbacks and so we shouldn't add new states to 'enum
    clock_event_mode'.
    
    Lets have separate enums for the two use cases mentioned above.
    Keep using the earlier enum for legacy set_mode() callback and
    mark it OBSOLETE. And add another enum to clearly specify the
    possible states of a clockevent device.
    
    This also renames the newly added per-mode callbacks to reflect
    state changes.
    
    We haven't got rid of 'mode' member of 'struct
    clock_event_device' as it is used by some of the clockevent
    drivers and it would automatically die down once we migrate
    those drivers to the new interface. It ('mode') is only updated
    now for the drivers using the legacy interface.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: linaro-kernel@lists.linaro.org
    Cc: linaro-networking@linaro.org
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/b6b0143a8a57bd58352ad35e08c25424c879c0cb.1425037853.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 5c50664c21d7..a5b877130ae9 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -102,7 +102,7 @@ void tick_handle_periodic(struct clock_event_device *dev)
 
 	tick_periodic(cpu);
 
-	if (dev->mode != CLOCK_EVT_MODE_ONESHOT)
+	if (dev->state != CLOCK_EVT_STATE_ONESHOT)
 		return;
 	for (;;) {
 		/*
@@ -140,7 +140,7 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 
 	if ((dev->features & CLOCK_EVT_FEAT_PERIODIC) &&
 	    !tick_broadcast_oneshot_active()) {
-		clockevents_set_mode(dev, CLOCK_EVT_MODE_PERIODIC);
+		clockevents_set_state(dev, CLOCK_EVT_STATE_PERIODIC);
 	} else {
 		unsigned long seq;
 		ktime_t next;
@@ -150,7 +150,7 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 			next = tick_next_period;
 		} while (read_seqretry(&jiffies_lock, seq));
 
-		clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
+		clockevents_set_state(dev, CLOCK_EVT_STATE_ONESHOT);
 
 		for (;;) {
 			if (!clockevents_program_event(dev, next, false))
@@ -365,6 +365,7 @@ void tick_shutdown(unsigned int *cpup)
 		 * Prevent that the clock events layer tries to call
 		 * the set mode function!
 		 */
+		dev->state = CLOCK_EVT_STATE_DETACHED;
 		dev->mode = CLOCK_EVT_MODE_UNUSED;
 		clockevents_exchange_device(dev, NULL);
 		dev->event_handler = clockevents_handle_noop;

commit 554ef3876c6acdff1331feab10275e9e9e0adb84
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Fri Feb 27 17:21:32 2015 +0530

    clockevents: Handle tick device's resume separately
    
    Upcoming patch will redefine possible states of a clockevent
    device. The RESUME mode is a special case only for tick's
    clockevent devices. In future it can be replaced by ->resume()
    callback already available for clockevent devices.
    
    Lets handle it separately so that clockevents_set_mode() only
    handles states valid across all devices. This also renames
    set_mode_resume() to tick_resume() to make it more explicit.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: linaro-kernel@lists.linaro.org
    Cc: linaro-networking@linaro.org
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/c1b0112410870f49e7bf06958e1483eac6c15e20.1425037853.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index f7c515595b42..5c50664c21d7 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -384,7 +384,7 @@ void tick_resume(void)
 	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 	int broadcast = tick_resume_broadcast();
 
-	clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_RESUME);
+	clockevents_tick_resume(td->evtdev);
 
 	if (!broadcast) {
 		if (td->mode == TICKDEV_MODE_PERIODIC)

commit 124cf9117c5f93cc5b324530b7e105b09c729d5d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Feb 13 23:50:43 2015 +0100

    PM / sleep: Make it possible to quiesce timers during suspend-to-idle
    
    The efficiency of suspend-to-idle depends on being able to keep CPUs
    in the deepest available idle states for as much time as possible.
    Ideally, they should only be brought out of idle by system wakeup
    interrupts.
    
    However, timer interrupts occurring periodically prevent that from
    happening and it is not practical to chase all of the "misbehaving"
    timers in a whack-a-mole fashion.  A much more effective approach is
    to suspend the local ticks for all CPUs and the entire timekeeping
    along the lines of what is done during full suspend, which also
    helps to keep suspend-to-idle and full suspend reasonably similar.
    
    The idea is to suspend the local tick on each CPU executing
    cpuidle_enter_freeze() and to make the last of them suspend the
    entire timekeeping.  That should prevent timer interrupts from
    triggering until an IO interrupt wakes up one of the CPUs.  It
    needs to be done with interrupts disabled on all of the CPUs,
    though, because otherwise the suspended clocksource might be
    accessed by an interrupt handler which might lead to fatal
    consequences.
    
    Unfortunately, the existing ->enter callbacks provided by cpuidle
    drivers generally cannot be used for implementing that, because some
    of them re-enable interrupts temporarily and some idle entry methods
    cause interrupts to be re-enabled automatically on exit.  Also some
    of these callbacks manipulate local clock event devices of the CPUs
    which really shouldn't be done after suspending their ticks.
    
    To overcome that difficulty, introduce a new cpuidle state callback,
    ->enter_freeze, that will be guaranteed (1) to keep interrupts
    disabled all the time (and return with interrupts disabled) and (2)
    not to touch the CPU timer devices.  Modify cpuidle_enter_freeze() to
    look for the deepest available idle state with ->enter_freeze present
    and to make the CPU execute that callback with suspended tick (and the
    last of the online CPUs to execute it with suspended timekeeping).
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 7efeedf53ebd..f7c515595b42 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -394,6 +394,56 @@ void tick_resume(void)
 	}
 }
 
+static DEFINE_RAW_SPINLOCK(tick_freeze_lock);
+static unsigned int tick_freeze_depth;
+
+/**
+ * tick_freeze - Suspend the local tick and (possibly) timekeeping.
+ *
+ * Check if this is the last online CPU executing the function and if so,
+ * suspend timekeeping.  Otherwise suspend the local tick.
+ *
+ * Call with interrupts disabled.  Must be balanced with %tick_unfreeze().
+ * Interrupts must not be enabled before the subsequent %tick_unfreeze().
+ */
+void tick_freeze(void)
+{
+	raw_spin_lock(&tick_freeze_lock);
+
+	tick_freeze_depth++;
+	if (tick_freeze_depth == num_online_cpus()) {
+		timekeeping_suspend();
+	} else {
+		tick_suspend();
+		tick_suspend_broadcast();
+	}
+
+	raw_spin_unlock(&tick_freeze_lock);
+}
+
+/**
+ * tick_unfreeze - Resume the local tick and (possibly) timekeeping.
+ *
+ * Check if this is the first CPU executing the function and if so, resume
+ * timekeeping.  Otherwise resume the local tick.
+ *
+ * Call with interrupts disabled.  Must be balanced with %tick_freeze().
+ * Interrupts must not be enabled after the preceding %tick_freeze().
+ */
+void tick_unfreeze(void)
+{
+	raw_spin_lock(&tick_freeze_lock);
+
+	if (tick_freeze_depth == num_online_cpus())
+		timekeeping_resume();
+	else
+		tick_resume();
+
+	tick_freeze_depth--;
+
+	raw_spin_unlock(&tick_freeze_lock);
+}
+
 /**
  * tick_init - initialize the tick control
  */

commit 0429fbc0bdc297d64188483ba029a23773ae07b0
Merge: 6929c358972f 513d1a2884a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 15 07:48:18 2014 +0200

    Merge branch 'for-3.18-consistent-ops' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    Pull percpu consistent-ops changes from Tejun Heo:
     "Way back, before the current percpu allocator was implemented, static
      and dynamic percpu memory areas were allocated and handled separately
      and had their own accessors.  The distinction has been gone for many
      years now; however, the now duplicate two sets of accessors remained
      with the pointer based ones - this_cpu_*() - evolving various other
      operations over time.  During the process, we also accumulated other
      inconsistent operations.
    
      This pull request contains Christoph's patches to clean up the
      duplicate accessor situation.  __get_cpu_var() uses are replaced with
      with this_cpu_ptr() and __this_cpu_ptr() with raw_cpu_ptr().
    
      Unfortunately, the former sometimes is tricky thanks to C being a bit
      messy with the distinction between lvalues and pointers, which led to
      a rather ugly solution for cpumask_var_t involving the introduction of
      this_cpu_cpumask_var_ptr().
    
      This converts most of the uses but not all.  Christoph will follow up
      with the remaining conversions in this merge window and hopefully
      remove the obsolete accessors"
    
    * 'for-3.18-consistent-ops' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (38 commits)
      irqchip: Properly fetch the per cpu offset
      percpu: Resolve ambiguities in __get_cpu_var/cpumask_var_t -fix
      ia64: sn_nodepda cannot be assigned to after this_cpu conversion. Use __this_cpu_write.
      percpu: Resolve ambiguities in __get_cpu_var/cpumask_var_t
      Revert "powerpc: Replace __get_cpu_var uses"
      percpu: Remove __this_cpu_ptr
      clocksource: Replace __this_cpu_ptr with raw_cpu_ptr
      sparc: Replace __get_cpu_var uses
      avr32: Replace __get_cpu_var with __this_cpu_write
      blackfin: Replace __get_cpu_var uses
      tile: Use this_cpu_ptr() for hardware counters
      tile: Replace __get_cpu_var uses
      powerpc: Replace __get_cpu_var uses
      alpha: Replace __get_cpu_var
      ia64: Replace __get_cpu_var uses
      s390: cio driver &__get_cpu_var replacements
      s390: Replace __get_cpu_var uses
      mips: Replace __get_cpu_var uses
      MIPS: Replace __get_cpu_var uses in FPU emulator.
      arm: Replace __this_cpu_ptr with raw_cpu_ptr
      ...

commit a80e49e2cc3145af014a8ae44f575829cc236192
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Aug 16 17:47:18 2014 +0200

    nohz: Move nohz full init call to tick init
    
    This way we unbloat a bit main.c and more importantly we initialize
    nohz full after init_IRQ(). This dependency will be needed in further
    patches because nohz full needs irq work to raise its own IRQ.
    Information about the support for this ability on ARM64 is obtained on
    init_IRQ() which initialize the pointer to __smp_call_function.
    
    Since tick_init() is called right after init_IRQ(), this is a good place
    to call tick_nohz_init() and prepare for that dependency.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 0a0608edeb26..052b4b53c3d6 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -400,4 +400,5 @@ void tick_resume(void)
 void __init tick_init(void)
 {
 	tick_broadcast_init();
+	tick_nohz_init();
 }

commit 22127e93c587afa01e4f7225d2d1cf1d26ae7dfe
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Aug 17 12:30:25 2014 -0500

    time: Replace __get_cpu_var uses
    
    Convert uses of __get_cpu_var for creating a address from a percpu
    offset to this_cpu_ptr.
    
    The two cases where get_cpu_var is used to actually access a percpu
    variable are changed to use this_cpu_read/raw_cpu_read.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 0a0608edeb26..decfb5f6edb0 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -224,7 +224,7 @@ static void tick_setup_device(struct tick_device *td,
 
 void tick_install_replacement(struct clock_event_device *newdev)
 {
-	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
+	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 	int cpu = smp_processor_id();
 
 	clockevents_exchange_device(td->evtdev, newdev);
@@ -374,14 +374,14 @@ void tick_shutdown(unsigned int *cpup)
 
 void tick_suspend(void)
 {
-	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
+	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 
 	clockevents_shutdown(td->evtdev);
 }
 
 void tick_resume(void)
 {
-	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
+	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 	int broadcast = tick_resume_broadcast();
 
 	clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_RESUME);

commit 521c42990e9d561ed5ed9f501f07639d0512b3c9
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Apr 15 10:54:37 2014 +0530

    tick-common: Fix wrong check in tick_check_replacement()
    
    tick_check_replacement() returns if a replacement of clock_event_device is
    possible or not. It does this as the first check:
    
            if (tick_check_percpu(curdev, newdev, smp_processor_id()))
                    return false;
    
    Thats wrong. tick_check_percpu() returns true when the device is
    useable. Check for false instead.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: <stable@vger.kernel.org> # v3.11+
    Cc: linaro-kernel@lists.linaro.org
    Cc: fweisbec@gmail.com
    Cc: Arvind.Chauhan@arm.com
    Cc: linaro-networking@linaro.org
    Link: http://lkml.kernel.org/r/486a02efe0246635aaba786e24b42d316438bf3b.1397537987.git.viresh.kumar@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 015661279b68..0a0608edeb26 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -276,7 +276,7 @@ static bool tick_check_preferred(struct clock_event_device *curdev,
 bool tick_check_replacement(struct clock_event_device *curdev,
 			    struct clock_event_device *newdev)
 {
-	if (tick_check_percpu(curdev, newdev, smp_processor_id()))
+	if (!tick_check_percpu(curdev, newdev, smp_processor_id()))
 		return false;
 
 	return tick_check_preferred(curdev, newdev);

commit b97f0291a2504291aef850077f98cab68a5a2f33
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Mar 25 13:56:23 2014 +0530

    tick: Remove code duplication in tick_handle_periodic()
    
    tick_handle_periodic() is calling ktime_add() at two places, first before the
    infinite loop and then at the end of infinite loop. We can rearrange code a bit
    to fix code duplication here.
    
    It looks quite simple and shouldn't break anything, I guess :)
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: linaro-kernel@lists.linaro.org
    Cc: fweisbec@gmail.com
    Link: http://lkml.kernel.org/r/be3481e8f3f71df694a4b43623254fc93ca51b59.1395735873.git.viresh.kumar@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 0fec63414fb6..015661279b68 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -98,18 +98,19 @@ static void tick_periodic(int cpu)
 void tick_handle_periodic(struct clock_event_device *dev)
 {
 	int cpu = smp_processor_id();
-	ktime_t next;
+	ktime_t next = dev->next_event;
 
 	tick_periodic(cpu);
 
 	if (dev->mode != CLOCK_EVT_MODE_ONESHOT)
 		return;
-	/*
-	 * Setup the next period for devices, which do not have
-	 * periodic mode:
-	 */
-	next = ktime_add(dev->next_event, tick_period);
 	for (;;) {
+		/*
+		 * Setup the next period for devices, which do not have
+		 * periodic mode:
+		 */
+		next = ktime_add(next, tick_period);
+
 		if (!clockevents_program_event(dev, next, false))
 			return;
 		/*
@@ -123,7 +124,6 @@ void tick_handle_periodic(struct clock_event_device *dev)
 		 */
 		if (timekeeping_valid_for_hres())
 			tick_periodic(cpu);
-		next = ktime_add(next, tick_period);
 	}
 }
 

commit cacb3c76c2012ade52124e8c6fdc5cb125625772
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Mar 25 16:09:18 2014 +0530

    tick: Fix spelling mistake in tick_handle_periodic()
    
    One of the comments in tick_handle_periodic() had 'when' instead of 'which' (My
    guess :)). Fix it.
    
    Also fix spelling mistake in 'Possible'.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: linaro-kernel@lists.linaro.org
    Cc: skarafotis@gmail.com
    Link: http://lkml.kernel.org/r/2b29ca4230c163e44179941d7c7a16c1474385c2.1395743878.git.viresh.kumar@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 20b2fe37d105..0fec63414fb6 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -118,7 +118,7 @@ void tick_handle_periodic(struct clock_event_device *dev)
 		 * to be sure we're using a real hardware clocksource.
 		 * Otherwise we could get trapped in an infinite
 		 * loop, as the tick_periodic() increments jiffies,
-		 * when then will increment time, posibly causing
+		 * which then will increment time, possibly causing
 		 * the loop to trigger again and again.
 		 */
 		if (timekeeping_valid_for_hres())

commit d05d24a984f8e14086771a158083dbe6facb769e
Merge: dba861461f88 38aef31ce777
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Jan 12 14:13:31 2014 +0100

    Merge branch 'fortglx/3.14/time' of git://git.linaro.org/people/john.stultz/linux into timers/core
    
    Pull timekeeping updates from John Stultz.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 47a1b796306356f358e515149d86baf0cc6bf007
Author: John Stultz <john.stultz@linaro.org>
Date:   Thu Dec 12 13:10:55 2013 -0800

    tick/timekeeping: Call update_wall_time outside the jiffies lock
    
    Since the xtime lock was split into the timekeeping lock and
    the jiffies lock, we no longer need to call update_wall_time()
    while holding the jiffies lock.
    
    Thus, this patch splits update_wall_time() out from do_timer().
    
    This allows us to get away from calling clock_was_set_delayed()
    in update_wall_time() and instead use the standard clock_was_set()
    call that previously would deadlock, as it causes the jiffies lock
    to be acquired.
    
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 64522ecdfe0e..91c5f27e82a3 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -70,6 +70,7 @@ static void tick_periodic(int cpu)
 
 		do_timer(1);
 		write_sequnlock(&jiffies_lock);
+		update_wall_time();
 	}
 
 	update_process_times(user_mode(get_irq_regs()));

commit 050ded1bbaea3331745cf2782315f5bc2582d083
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Fri Nov 15 14:15:33 2013 -0800

    tick: Document tick_do_timer_cpu
    
    Taken straight from a tglx email ;)
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 64522ecdfe0e..162b03ab0ad2 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -33,6 +33,21 @@ DEFINE_PER_CPU(struct tick_device, tick_cpu_device);
  */
 ktime_t tick_next_period;
 ktime_t tick_period;
+
+/*
+ * tick_do_timer_cpu is a timer core internal variable which holds the CPU NR
+ * which is responsible for calling do_timer(), i.e. the timekeeping stuff. This
+ * variable has two functions:
+ *
+ * 1) Prevent a thundering herd issue of a gazillion of CPUs trying to grab the
+ *    timekeeping lock all at once. Only the CPU which is assigned to do the
+ *    update is handling it.
+ *
+ * 2) Hand off the duty in the NOHZ idle case by setting the value to
+ *    TICK_DO_TIMER_NONE, i.e. a non existing CPU. So the next cpu which looks
+ *    at it will take over and keep the time keeping alive.  The handover
+ *    procedure also covers cpu hotplug.
+ */
 int tick_do_timer_cpu __read_mostly = TICK_DO_TIMER_BOOT;
 
 /*

commit 07bd1172902e782f288e4d44b1fde7dec0f08b6f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 1 22:14:10 2013 +0200

    tick: Sanitize broadcast control logic
    
    The recent implementation of a generic dummy timer resulted in a
    different registration order of per cpu local timers which made the
    broadcast control logic go belly up.
    
    If the dummy timer is the first clock event device which is registered
    for a CPU, then it is installed, the broadcast timer is initialized
    and the CPU is marked as broadcast target.
    
    If a real clock event device is installed after that, we can fail to
    take the CPU out of the broadcast mask. In the worst case we end up
    with two periodic timer events firing for the same CPU. One from the
    per cpu hardware device and one from the broadcast.
    
    Now the problem is that we have no way to distinguish whether the
    system is in a state which makes broadcasting necessary or the
    broadcast bit was set due to the nonfunctional dummy timer
    installment.
    
    To solve this we need to keep track of the system state seperately and
    provide a more detailed decision logic whether we keep the CPU in
    broadcast mode or not.
    
    The old decision logic only clears the broadcast mode, if the newly
    installed clock event device is not affected by power states.
    
    The new logic clears the broadcast mode if one of the following is
    true:
    
      - The new device is not affected by power states.
    
      - The system is not in a power state affected mode
    
      - The system has switched to oneshot mode. The oneshot broadcast is
        controlled from the deep idle state. The CPU is not in idle at
        this point, so it's safe to remove it from the mask.
    
    If we clear the broadcast bit for the CPU when a new device is
    installed, we also shutdown the broadcast device when this was the
    last CPU in the broadcast mask.
    
    If the broadcast bit is kept, then we leave the new device in shutdown
    state and rely on the broadcast to deliver the timer interrupts via
    the broadcast ipis.
    
    Reported-and-tested-by: Stehle Vincent-B46079 <B46079@freescale.com>
    Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
    Cc: John Stultz <john.stultz@linaro.org>,
    Cc: Mark Rutland <mark.rutland@arm.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1307012153060.4013@ionos.tec.linutronix.de
    Cc: stable@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index edd45f64162f..64522ecdfe0e 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -194,7 +194,8 @@ static void tick_setup_device(struct tick_device *td,
 	 * When global broadcasting is active, check if the current
 	 * device is registered as a placeholder for broadcast mode.
 	 * This allows us to handle this x86 misfeature in a generic
-	 * way.
+	 * way. This function also returns !=0 when we keep the
+	 * current active broadcast state for this CPU.
 	 */
 	if (tick_device_uses_broadcast(newdev, cpu))
 		return;

commit 70e5975d3a04be5479a28eec4a2fb10f98ad2785
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Thu Jun 13 11:39:50 2013 -0700

    clockevents: Prefer CPU local devices over global devices
    
    On an SMP system with only one global clockevent and a dummy
    clockevent per CPU we run into problems. We want the dummy
    clockevents to be registered as the per CPU tick devices, but
    we can only achieve that if we register the dummy clockevents
    before the global clockevent or if we artificially inflate the
    rating of the dummy clockevents to be higher than the rating
    of the global clockevent. Failure to do so leads to boot
    hangs when the dummy timers are registered on all other CPUs
    besides the CPU that accepted the global clockevent as its tick
    device and there is no broadcast timer to poke the dummy
    devices.
    
    If we're registering multiple clockevents and one clockevent is
    global and the other is local to a particular CPU we should
    choose to use the local clockevent regardless of the rating of
    the device. This way, if the clockevent is a dummy it will take
    the tick device duty as long as there isn't a higher rated tick
    device and any global clockevent will be bumped out into
    broadcast mode, fixing the problem described above.
    
    Reported-and-tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Tested-by: soren.brinkmann@xilinx.com
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: John Stultz <john.stultz@linaro.org>
    Link: http://lkml.kernel.org/r/20130613183950.GA32061@codeaurora.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 5edfb4806032..edd45f64162f 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -243,8 +243,13 @@ static bool tick_check_preferred(struct clock_event_device *curdev,
 			return false;
 	}
 
-	/* Use the higher rated one */
-	return !curdev || newdev->rating > curdev->rating;
+	/*
+	 * Use the higher rated one, but prefer a CPU local device with a lower
+	 * rating than a non-CPU local device
+	 */
+	return !curdev ||
+		newdev->rating > curdev->rating ||
+	       !cpumask_equal(curdev->cpumask, newdev->cpumask);
 }
 
 /*

commit 03e13cf5ee60584fe0c831682c67212effb7fca4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 20:31:50 2013 +0000

    clockevents: Implement unbind functionality
    
    Provide a sysfs interface to allow unbinding of clockevent
    devices. The device is unbound if it is unused or if there is a
    replacement device available. Unbinding of broadcast devices is not
    supported as we don't want to foster that nonsense. If no replacement
    device is available the unbind returns -EBUSY. Unbind is available
    from the kernel and through sysfs, which is necessary to drop the
    module refcount.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130425143436.499216659@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index c34021650348..5edfb4806032 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -205,6 +205,17 @@ static void tick_setup_device(struct tick_device *td,
 		tick_setup_oneshot(newdev, handler, next_event);
 }
 
+void tick_install_replacement(struct clock_event_device *newdev)
+{
+	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
+	int cpu = smp_processor_id();
+
+	clockevents_exchange_device(td->evtdev, newdev);
+	tick_setup_device(td, newdev, cpu, cpumask_of(cpu));
+	if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
+		tick_oneshot_notify();
+}
+
 static bool tick_check_percpu(struct clock_event_device *curdev,
 			      struct clock_event_device *newdev, int cpu)
 {
@@ -236,6 +247,19 @@ static bool tick_check_preferred(struct clock_event_device *curdev,
 	return !curdev || newdev->rating > curdev->rating;
 }
 
+/*
+ * Check whether the new device is a better fit than curdev. curdev
+ * can be NULL !
+ */
+bool tick_check_replacement(struct clock_event_device *curdev,
+			    struct clock_event_device *newdev)
+{
+	if (tick_check_percpu(curdev, newdev, smp_processor_id()))
+		return false;
+
+	return tick_check_preferred(curdev, newdev);
+}
+
 /*
  * Check, if the new registered device should be used. Called with
  * clockevents_lock held and interrupts disabled.

commit 45cb8e01b2ecef1c2afb18333e95793fa1a90281
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 20:31:50 2013 +0000

    clockevents: Split out selection logic
    
    Split out the clockevent device selection logic. Preparatory patch to
    allow unbinding active clockevent devices.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130425143436.431796247@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 433a1e11d13b..c34021650348 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -205,6 +205,37 @@ static void tick_setup_device(struct tick_device *td,
 		tick_setup_oneshot(newdev, handler, next_event);
 }
 
+static bool tick_check_percpu(struct clock_event_device *curdev,
+			      struct clock_event_device *newdev, int cpu)
+{
+	if (!cpumask_test_cpu(cpu, newdev->cpumask))
+		return false;
+	if (cpumask_equal(newdev->cpumask, cpumask_of(cpu)))
+		return true;
+	/* Check if irq affinity can be set */
+	if (newdev->irq >= 0 && !irq_can_set_affinity(newdev->irq))
+		return false;
+	/* Prefer an existing cpu local device */
+	if (curdev && cpumask_equal(curdev->cpumask, cpumask_of(cpu)))
+		return false;
+	return true;
+}
+
+static bool tick_check_preferred(struct clock_event_device *curdev,
+				 struct clock_event_device *newdev)
+{
+	/* Prefer oneshot capable device */
+	if (!(newdev->features & CLOCK_EVT_FEAT_ONESHOT)) {
+		if (curdev && (curdev->features & CLOCK_EVT_FEAT_ONESHOT))
+			return false;
+		if (tick_oneshot_mode_active())
+			return false;
+	}
+
+	/* Use the higher rated one */
+	return !curdev || newdev->rating > curdev->rating;
+}
+
 /*
  * Check, if the new registered device should be used. Called with
  * clockevents_lock held and interrupts disabled.
@@ -223,40 +254,12 @@ void tick_check_new_device(struct clock_event_device *newdev)
 	curdev = td->evtdev;
 
 	/* cpu local device ? */
-	if (!cpumask_equal(newdev->cpumask, cpumask_of(cpu))) {
-
-		/*
-		 * If the cpu affinity of the device interrupt can not
-		 * be set, ignore it.
-		 */
-		if (!irq_can_set_affinity(newdev->irq))
-			goto out_bc;
-
-		/*
-		 * If we have a cpu local device already, do not replace it
-		 * by a non cpu local device
-		 */
-		if (curdev && cpumask_equal(curdev->cpumask, cpumask_of(cpu)))
-			goto out_bc;
-	}
+	if (!tick_check_percpu(curdev, newdev, cpu))
+		goto out_bc;
 
-	/*
-	 * If we have an active device, then check the rating and the oneshot
-	 * feature.
-	 */
-	if (curdev) {
-		/*
-		 * Prefer one shot capable devices !
-		 */
-		if ((curdev->features & CLOCK_EVT_FEAT_ONESHOT) &&
-		    !(newdev->features & CLOCK_EVT_FEAT_ONESHOT))
-			goto out_bc;
-		/*
-		 * Check the rating
-		 */
-		if (curdev->rating >= newdev->rating)
-			goto out_bc;
-	}
+	/* Preference decision */
+	if (!tick_check_preferred(curdev, newdev))
+		goto out_bc;
 
 	if (!try_module_get(newdev->owner))
 		return;

commit ccf33d6880f39a35158fff66db13000ae4943fac
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 20:31:49 2013 +0000

    clockevents: Add module refcount
    
    We want to be able to remove clockevent modules as well. Add a
    refcount so we don't remove a module with an active clock event
    device.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130425143436.307435149@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 84c7cfca4d7d..433a1e11d13b 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -18,6 +18,7 @@
 #include <linux/percpu.h>
 #include <linux/profile.h>
 #include <linux/sched.h>
+#include <linux/module.h>
 
 #include <asm/irq_regs.h>
 
@@ -257,6 +258,9 @@ void tick_check_new_device(struct clock_event_device *newdev)
 			goto out_bc;
 	}
 
+	if (!try_module_get(newdev->owner))
+		return;
+
 	/*
 	 * Replace the eventually existing device by the new
 	 * device. If the current device is the broadcast device, do

commit 8c53daf63f56791ed47fc585206ef3049489612f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 20:31:48 2013 +0000

    clockevents: Move the tick_notify() switch case to clockevents_notify()
    
    No need to call another function and have duplicated cases.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130425143436.235746557@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 170a4bdfa99e..84c7cfca4d7d 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -284,7 +284,7 @@ void tick_check_new_device(struct clock_event_device *newdev)
  *
  * Called with interrupts disabled.
  */
-static void tick_handover_do_timer(int *cpup)
+void tick_handover_do_timer(int *cpup)
 {
 	if (*cpup == tick_do_timer_cpu) {
 		int cpu = cpumask_first(cpu_online_mask);
@@ -301,7 +301,7 @@ static void tick_handover_do_timer(int *cpup)
  * access the hardware device itself.
  * We just set the mode and remove it from the lists.
  */
-static void tick_shutdown(unsigned int *cpup)
+void tick_shutdown(unsigned int *cpup)
 {
 	struct tick_device *td = &per_cpu(tick_cpu_device, *cpup);
 	struct clock_event_device *dev = td->evtdev;
@@ -319,14 +319,14 @@ static void tick_shutdown(unsigned int *cpup)
 	}
 }
 
-static void tick_suspend(void)
+void tick_suspend(void)
 {
 	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
 
 	clockevents_shutdown(td->evtdev);
 }
 
-static void tick_resume(void)
+void tick_resume(void)
 {
 	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
 	int broadcast = tick_resume_broadcast();
@@ -341,48 +341,6 @@ static void tick_resume(void)
 	}
 }
 
-/*
- * Called with clockevents_lock held and interrupts disabled
- */
-void tick_notify(unsigned long reason, void *dev)
-{
-	switch (reason) {
-
-	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
-	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
-	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
-		tick_broadcast_on_off(reason, dev);
-		break;
-
-	case CLOCK_EVT_NOTIFY_BROADCAST_ENTER:
-	case CLOCK_EVT_NOTIFY_BROADCAST_EXIT:
-		tick_broadcast_oneshot_control(reason);
-		break;
-
-	case CLOCK_EVT_NOTIFY_CPU_DYING:
-		tick_handover_do_timer(dev);
-		break;
-
-	case CLOCK_EVT_NOTIFY_CPU_DEAD:
-		tick_shutdown_broadcast_oneshot(dev);
-		tick_shutdown_broadcast(dev);
-		tick_shutdown(dev);
-		break;
-
-	case CLOCK_EVT_NOTIFY_SUSPEND:
-		tick_suspend();
-		tick_suspend_broadcast();
-		break;
-
-	case CLOCK_EVT_NOTIFY_RESUME:
-		tick_resume();
-		break;
-
-	default:
-		break;
-	}
-}
-
 /**
  * tick_init - initialize the tick control
  */

commit 7126cac426137633e470167524e7bcb590fd49b3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 20:31:48 2013 +0000

    clockevents: Simplify locking
    
    Now that the notifier chain is gone there are no other users and it's
    pointless to nest tick_device_lock inside of clockevents_lock because
    there is no other use case.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130425143436.162888472@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index dbf4e18d5101..170a4bdfa99e 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -33,7 +33,6 @@ DEFINE_PER_CPU(struct tick_device, tick_cpu_device);
 ktime_t tick_next_period;
 ktime_t tick_period;
 int tick_do_timer_cpu __read_mostly = TICK_DO_TIMER_BOOT;
-static DEFINE_RAW_SPINLOCK(tick_device_lock);
 
 /*
  * Debugging: see timer_list.c
@@ -206,16 +205,14 @@ static void tick_setup_device(struct tick_device *td,
 }
 
 /*
- * Check, if the new registered device should be used.
+ * Check, if the new registered device should be used. Called with
+ * clockevents_lock held and interrupts disabled.
  */
 void tick_check_new_device(struct clock_event_device *newdev)
 {
 	struct clock_event_device *curdev;
 	struct tick_device *td;
 	int cpu;
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&tick_device_lock, flags);
 
 	cpu = smp_processor_id();
 	if (!cpumask_test_cpu(cpu, newdev->cpumask))
@@ -273,8 +270,6 @@ void tick_check_new_device(struct clock_event_device *newdev)
 	tick_setup_device(td, newdev, cpu, cpumask_of(cpu));
 	if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
 		tick_oneshot_notify();
-
-	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 	return;
 
 out_bc:
@@ -282,7 +277,6 @@ void tick_check_new_device(struct clock_event_device *newdev)
 	 * Can the new device be used as a broadcast device ?
 	 */
 	tick_install_broadcast_device(newdev);
-	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
 /*
@@ -311,9 +305,7 @@ static void tick_shutdown(unsigned int *cpup)
 {
 	struct tick_device *td = &per_cpu(tick_cpu_device, *cpup);
 	struct clock_event_device *dev = td->evtdev;
-	unsigned long flags;
 
-	raw_spin_lock_irqsave(&tick_device_lock, flags);
 	td->mode = TICKDEV_MODE_PERIODIC;
 	if (dev) {
 		/*
@@ -325,26 +317,20 @@ static void tick_shutdown(unsigned int *cpup)
 		dev->event_handler = clockevents_handle_noop;
 		td->evtdev = NULL;
 	}
-	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
 static void tick_suspend(void)
 {
 	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
-	unsigned long flags;
 
-	raw_spin_lock_irqsave(&tick_device_lock, flags);
 	clockevents_shutdown(td->evtdev);
-	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
 static void tick_resume(void)
 {
 	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
-	unsigned long flags;
 	int broadcast = tick_resume_broadcast();
 
-	raw_spin_lock_irqsave(&tick_device_lock, flags);
 	clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_RESUME);
 
 	if (!broadcast) {
@@ -353,9 +339,11 @@ static void tick_resume(void)
 		else
 			tick_resume_oneshot();
 	}
-	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
+/*
+ * Called with clockevents_lock held and interrupts disabled
+ */
 void tick_notify(unsigned long reason, void *dev)
 {
 	switch (reason) {

commit 7172a286ced0c1f4f239a0fa09db54ed37d3ead2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 20:31:47 2013 +0000

    clockevents: Get rid of the notifier chain
    
    7+ years and still a single user. Kill it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130425143436.098520211@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 5d3fb100bc06..dbf4e18d5101 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -208,11 +208,11 @@ static void tick_setup_device(struct tick_device *td,
 /*
  * Check, if the new registered device should be used.
  */
-static int tick_check_new_device(struct clock_event_device *newdev)
+void tick_check_new_device(struct clock_event_device *newdev)
 {
 	struct clock_event_device *curdev;
 	struct tick_device *td;
-	int cpu, ret = NOTIFY_OK;
+	int cpu;
 	unsigned long flags;
 
 	raw_spin_lock_irqsave(&tick_device_lock, flags);
@@ -275,18 +275,14 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 		tick_oneshot_notify();
 
 	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
-	return NOTIFY_STOP;
+	return;
 
 out_bc:
 	/*
 	 * Can the new device be used as a broadcast device ?
 	 */
-	if (tick_check_broadcast_device(newdev))
-		ret = NOTIFY_STOP;
-
+	tick_install_broadcast_device(newdev);
 	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
-
-	return ret;
 }
 
 /*
@@ -360,17 +356,10 @@ static void tick_resume(void)
 	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
-/*
- * Notification about clock event devices
- */
-static int tick_notify(struct notifier_block *nb, unsigned long reason,
-			       void *dev)
+void tick_notify(unsigned long reason, void *dev)
 {
 	switch (reason) {
 
-	case CLOCK_EVT_NOTIFY_ADD:
-		return tick_check_new_device(dev);
-
 	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
 	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
 	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
@@ -404,21 +393,12 @@ static int tick_notify(struct notifier_block *nb, unsigned long reason,
 	default:
 		break;
 	}
-
-	return NOTIFY_OK;
 }
 
-static struct notifier_block tick_notifier = {
-	.notifier_call = tick_notify,
-};
-
 /**
  * tick_init - initialize the tick control
- *
- * Register the notifier with the clockevents framework
  */
 void __init tick_init(void)
 {
-	clockevents_register_notifier(&tick_notifier);
 	tick_broadcast_init();
 }

commit 534c97b0950b1967bca1c753aeaed32f5db40264
Merge: 64049d1973c1 265f22a975c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 5 13:23:27 2013 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull 'full dynticks' support from Ingo Molnar:
     "This tree from Frederic Weisbecker adds a new, (exciting! :-) core
      kernel feature to the timer and scheduler subsystems: 'full dynticks',
      or CONFIG_NO_HZ_FULL=y.
    
      This feature extends the nohz variable-size timer tick feature from
      idle to busy CPUs (running at most one task) as well, potentially
      reducing the number of timer interrupts significantly.
    
      This feature got motivated by real-time folks and the -rt tree, but
      the general utility and motivation of full-dynticks runs wider than
      that:
    
       - HPC workloads get faster: CPUs running a single task should be able
         to utilize a maximum amount of CPU power.  A periodic timer tick at
         HZ=1000 can cause a constant overhead of up to 1.0%.  This feature
         removes that overhead - and speeds up the system by 0.5%-1.0% on
         typical distro configs even on modern systems.
    
       - Real-time workload latency reduction: CPUs running critical tasks
         should experience as little jitter as possible.  The last remaining
         source of kernel-related jitter was the periodic timer tick.
    
       - A single task executing on a CPU is a pretty common situation,
         especially with an increasing number of cores/CPUs, so this feature
         helps desktop and mobile workloads as well.
    
      The cost of the feature is mainly related to increased timer
      reprogramming overhead when a CPU switches its tick period, and thus
      slightly longer to-idle and from-idle latency.
    
      Configuration-wise a third mode of operation is added to the existing
      two NOHZ kconfig modes:
    
       - CONFIG_HZ_PERIODIC: [formerly !CONFIG_NO_HZ], now explicitly named
         as a config option.  This is the traditional Linux periodic tick
         design: there's a HZ tick going on all the time, regardless of
         whether a CPU is idle or not.
    
       - CONFIG_NO_HZ_IDLE: [formerly CONFIG_NO_HZ=y], this turns off the
         periodic tick when a CPU enters idle mode.
    
       - CONFIG_NO_HZ_FULL: this new mode, in addition to turning off the
         tick when a CPU is idle, also slows the tick down to 1 Hz (one
         timer interrupt per second) when only a single task is running on a
         CPU.
    
      The .config behavior is compatible: existing !CONFIG_NO_HZ and
      CONFIG_NO_HZ=y settings get translated to the new values, without the
      user having to configure anything.  CONFIG_NO_HZ_FULL is turned off by
      default.
    
      This feature is based on a lot of infrastructure work that has been
      steadily going upstream in the last 2-3 cycles: related RCU support
      and non-periodic cputime support in particular is upstream already.
    
      This tree adds the final pieces and activates the feature.  The pull
      request is marked RFC because:
    
       - it's marked 64-bit only at the moment - the 32-bit support patch is
         small but did not get ready in time.
    
       - it has a number of fresh commits that came in after the merge
         window.  The overwhelming majority of commits are from before the
         merge window, but still some aspects of the tree are fresh and so I
         marked it RFC.
    
       - it's a pretty wide-reaching feature with lots of effects - and
         while the components have been in testing for some time, the full
         combination is still not very widely used.  That it's default-off
         should reduce its regression abilities and obviously there are no
         known regressions with CONFIG_NO_HZ_FULL=y enabled either.
    
       - the feature is not completely idempotent: there is no 100%
         equivalent replacement for a periodic scheduler/timer tick.  In
         particular there's ongoing work to map out and reduce its effects
         on scheduler load-balancing and statistics.  This should not impact
         correctness though, there are no known regressions related to this
         feature at this point.
    
       - it's a pretty ambitious feature that with time will likely be
         enabled by most Linux distros, and we'd like you to make input on
         its design/implementation, if you dislike some aspect we missed.
         Without flaming us to crisp! :-)
    
      Future plans:
    
       - there's ongoing work to reduce 1Hz to 0Hz, to essentially shut off
         the periodic tick altogether when there's a single busy task on a
         CPU.  We'd first like 1 Hz to be exposed more widely before we go
         for the 0 Hz target though.
    
       - once we reach 0 Hz we can remove the periodic tick assumption from
         nr_running>=2 as well, by essentially interrupting busy tasks only
         as frequently as the sched_latency constraints require us to do -
         once every 4-40 msecs, depending on nr_running.
    
      I am personally leaning towards biting the bullet and doing this in
      v3.10, like the -rt tree this effort has been going on for too long -
      but the final word is up to you as usual.
    
      More technical details can be found in Documentation/timers/NO_HZ.txt"
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (39 commits)
      sched: Keep at least 1 tick per second for active dynticks tasks
      rcu: Fix full dynticks' dependency on wide RCU nocb mode
      nohz: Protect smp_processor_id() in tick_nohz_task_switch()
      nohz_full: Add documentation.
      cputime_nsecs: use math64.h for nsec resolution conversion helpers
      nohz: Select VIRT_CPU_ACCOUNTING_GEN from full dynticks config
      nohz: Reduce overhead under high-freq idling patterns
      nohz: Remove full dynticks' superfluous dependency on RCU tree
      nohz: Fix unavailable tick_stop tracepoint in dynticks idle
      nohz: Add basic tracing
      nohz: Select wide RCU nocb for full dynticks
      nohz: Disable the tick when irq resume in full dynticks CPU
      nohz: Re-evaluate the tick for the new task after a context switch
      nohz: Prepare to stop the tick on irq exit
      nohz: Implement full dynticks kick
      nohz: Re-evaluate the tick from the scheduler IPI
      sched: New helper to prevent from stopping the tick in full dynticks
      sched: Kick full dynticks CPU that have more than one task enqueued.
      perf: New helper to prevent full dynticks CPUs from stopping tick
      perf: Kick full dynticks CPU if events rotation is needed
      ...

commit 6f7a05d7018de222e40ca003721037a530979974
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 11:45:53 2013 +0200

    clockevents: Set dummy handler on CPU_DEAD shutdown
    
    Vitaliy reported that a per cpu HPET timer interrupt crashes the
    system during hibernation. What happens is that the per cpu HPET timer
    gets shut down when the nonboot cpus are stopped. When the nonboot
    cpus are onlined again the HPET code sets up the MSI interrupt which
    fires before the clock event device is registered. The event handler
    is still set to hrtimer_interrupt, which then crashes the machine due
    to highres mode not being active.
    
    See http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=700333
    
    There is no real good way to avoid that in the HPET code. The HPET
    code alrady has a mechanism to detect spurious interrupts when event
    handler == NULL for a similar reason.
    
    We can handle that in the clockevent/tick layer and replace the
    previous functional handler with a dummy handler like we do in
    tick_setup_new_device().
    
    The original clockevents code did this in clockevents_exchange_device(),
    but that got removed by commit 7c1e76897 (clockevents: prevent
    clockevent event_handler ending up handler_noop) which forgot to fix
    it up in tick_shutdown(). Same issue with the broadcast device.
    
    Reported-by: Vitaliy Fillipov <vitalif@yourcmc.ru>
    Cc: Ben Hutchings <ben@decadent.org.uk>
    Cc: stable@vger.kernel.org
    Cc: 700333@bugs.debian.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 74413e396acc..6176a3e45709 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -323,6 +323,7 @@ static void tick_shutdown(unsigned int *cpup)
 		 */
 		dev->mode = CLOCK_EVT_MODE_UNUSED;
 		clockevents_exchange_device(dev, NULL);
+		dev->event_handler = clockevents_handle_noop;
 		td->evtdev = NULL;
 	}
 	raw_spin_unlock_irqrestore(&tick_device_lock, flags);

commit c5bfece2d6129131b4ade985e63bc35ddf5868d4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Apr 12 16:45:34 2013 +0200

    nohz: Switch from "extended nohz" to "full nohz" based naming
    
    "Extended nohz" was used as a naming base for the full dynticks
    API and Kconfig symbols. It reflects the fact the system tries
    to stop the tick in more places than just idle.
    
    But that "extended" name is a bit opaque and vague. Rename it to
    "full" makes it clearer what the system tries to do under this
    config: try to shutdown the tick anytime it can. The various
    constraints that prevent that to happen shouldn't be considered
    as fundamental properties of this feature but rather technical
    issues that may be solved in the future.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index b7dc0cbdb59b..83f2bd967161 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -163,7 +163,7 @@ static void tick_setup_device(struct tick_device *td,
 		 * this cpu:
 		 */
 		if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
-			if (!tick_nohz_extended_cpu(cpu))
+			if (!tick_nohz_full_cpu(cpu))
 				tick_do_timer_cpu = cpu;
 			else
 				tick_do_timer_cpu = TICK_DO_TIMER_NONE;

commit a382bf934449ddeb625167537ae81daa0211b477
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Dec 18 18:24:35 2012 +0100

    nohz: Assign timekeeping duty to a CPU outside the full dynticks range
    
    This way the full nohz CPUs can safely run with the tick
    stopped with a guarantee that somebody else is taking
    care of the jiffies and GTOD progression.
    
    Once the duty is attributed to a CPU, it won't change. Also that
    CPU can't enter into dyntick idle mode or be hot unplugged.
    
    This may later be improved from a power consumption POV. At
    least we should be able to share the duty amongst all CPUs
    outside the full dynticks range. Then the duty could even be
    shared with full dynticks CPUs when those can't stop their
    tick for any reason.
    
    But let's start with that very simple approach first.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [fix have_nohz_full_mask offcase]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index b1600a6973f4..b7dc0cbdb59b 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -163,7 +163,10 @@ static void tick_setup_device(struct tick_device *td,
 		 * this cpu:
 		 */
 		if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
-			tick_do_timer_cpu = cpu;
+			if (!tick_nohz_extended_cpu(cpu))
+				tick_do_timer_cpu = cpu;
+			else
+				tick_do_timer_cpu = TICK_DO_TIMER_NONE;
 			tick_next_period = ktime_get();
 			tick_period = ktime_set(0, NSEC_PER_SEC / HZ);
 		}

commit b352bc1cbc29134a356b5c16ee2281807a7b984e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 5 14:25:32 2013 +0100

    tick: Convert broadcast cpu bitmaps to cpumask_var_t
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20130306111537.366394000@linutronix.de
    Cc: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index b1600a6973f4..74413e396acc 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -416,4 +416,5 @@ static struct notifier_block tick_notifier = {
 void __init tick_init(void)
 {
 	clockevents_register_notifier(&tick_notifier);
+	tick_broadcast_init();
 }

commit d6ad418763888f617ac5b4849823e4cd670df1dd
Author: John Stultz <john.stultz@linaro.org>
Date:   Tue Feb 28 16:50:11 2012 -0800

    time: Kill xtime_lock, replacing it with jiffies_lock
    
    Now that timekeeping is protected by its own locks, rename
    the xtime_lock to jifffies_lock to better describe what it
    protects.
    
    CC: Thomas Gleixner <tglx@linutronix.de>
    CC: Eric Dumazet <eric.dumazet@gmail.com>
    CC: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index da6c9ecad4e4..b1600a6973f4 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -63,13 +63,13 @@ int tick_is_oneshot_available(void)
 static void tick_periodic(int cpu)
 {
 	if (tick_do_timer_cpu == cpu) {
-		write_seqlock(&xtime_lock);
+		write_seqlock(&jiffies_lock);
 
 		/* Keep track of the next tick event */
 		tick_next_period = ktime_add(tick_next_period, tick_period);
 
 		do_timer(1);
-		write_sequnlock(&xtime_lock);
+		write_sequnlock(&jiffies_lock);
 	}
 
 	update_process_times(user_mode(get_irq_regs()));
@@ -130,9 +130,9 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 		ktime_t next;
 
 		do {
-			seq = read_seqbegin(&xtime_lock);
+			seq = read_seqbegin(&jiffies_lock);
 			next = tick_next_period;
-		} while (read_seqretry(&xtime_lock, seq));
+		} while (read_seqretry(&jiffies_lock, seq));
 
 		clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
 

commit d1748302f70be7469809809283fe164156a34231
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Aug 23 15:29:42 2011 +0200

    clockevents: Make minimum delay adjustments configurable
    
    The automatic increase of the min_delta_ns of a clockevents device
    should be done in the clockevents code as the minimum delay is an
    attribute of the clockevents device.
    
    In addition not all architectures want the automatic adjustment, on a
    massively virtualized system it can happen that the programming of a
    clock event fails several times in a row because the virtual cpu has
    been rescheduled quickly enough. In that case the minimum delay will
    erroneously be increased with no way back. The new config symbol
    GENERIC_CLOCKEVENTS_MIN_ADJUST is used to enable the automatic
    adjustment. The config option is selected only for x86.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: john stultz <johnstul@us.ibm.com>
    Link: http://lkml.kernel.org/r/20110823133142.494157493@de.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 119528de8235..da6c9ecad4e4 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -94,7 +94,7 @@ void tick_handle_periodic(struct clock_event_device *dev)
 	 */
 	next = ktime_add(dev->next_event, tick_period);
 	for (;;) {
-		if (!clockevents_program_event(dev, next, ktime_get()))
+		if (!clockevents_program_event(dev, next, false))
 			return;
 		/*
 		 * Have to be careful here. If we're in oneshot mode,
@@ -137,7 +137,7 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 		clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
 
 		for (;;) {
-			if (!clockevents_program_event(dev, next, ktime_get()))
+			if (!clockevents_program_event(dev, next, false))
 				return;
 			next = ktime_add(next, tick_period);
 		}

commit 420c1c572d4ceaa2f37b6311b7017ac6cf049fe2
Merge: 9620639b7ea3 6e6823d17b15
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 18:53:35 2011 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (62 commits)
      posix-clocks: Check write permissions in posix syscalls
      hrtimer: Remove empty hrtimer_init_hres_timer()
      hrtimer: Update hrtimer->state documentation
      hrtimer: Update base[CLOCK_BOOTTIME].offset correctly
      timers: Export CLOCK_BOOTTIME via the posix timers interface
      timers: Add CLOCK_BOOTTIME hrtimer base
      time: Extend get_xtime_and_monotonic_offset() to also return sleep
      time: Introduce get_monotonic_boottime and ktime_get_boottime
      hrtimers: extend hrtimer base code to handle more then 2 clockids
      ntp: Remove redundant and incorrect parameter check
      mn10300: Switch do_timer() to xtimer_update()
      posix clocks: Introduce dynamic clocks
      posix-timers: Cleanup namespace
      posix-timers: Add support for fd based clocks
      x86: Add clock_adjtime for x86
      posix-timers: Introduce a syscall for clock tuning.
      time: Splitout compat timex accessors
      ntp: Add ADJ_SETOFFSET mode bit
      time: Introduce timekeeping_inject_offset
      posix-timer: Update comment
      ...
    
    Fix up new system-call-related conflicts in
            arch/x86/ia32/ia32entry.S
            arch/x86/include/asm/unistd_32.h
            arch/x86/include/asm/unistd_64.h
            arch/x86/kernel/syscall_table_32.S
    (name_to_handle_at()/open_by_handle_at() vs clock_adjtime()), and some
    due to movement of get_jiffies_64() in:
            kernel/time.c

commit 3a142a0672b48a853f00af61f184c7341ac9c99d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 25 22:34:23 2011 +0100

    clockevents: Prevent oneshot mode when broadcast device is periodic
    
    When the per cpu timer is marked CLOCK_EVT_FEAT_C3STOP, then we only
    can switch into oneshot mode, when the backup broadcast device
    supports oneshot mode as well. Otherwise we would try to switch the
    broadcast device into an unsupported mode unconditionally. This went
    unnoticed so far as the current available broadcast devices support
    oneshot mode. Seth unearthed this problem while debugging and working
    around an hpet related BIOS wreckage.
    
    Add the necessary check to tick_is_oneshot_available().
    
    Reported-and-tested-by: Seth Forshee <seth.forshee@canonical.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <alpine.LFD.2.00.1102252231200.2701@localhost6.localdomain6>
    Cc: stable@kernel.org # .21 ->

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 051bc80a0c43..ed228ef6f6b8 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -51,7 +51,11 @@ int tick_is_oneshot_available(void)
 {
 	struct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);
 
-	return dev && (dev->features & CLOCK_EVT_FEAT_ONESHOT);
+	if (!dev || !(dev->features & CLOCK_EVT_FEAT_ONESHOT))
+		return 0;
+	if (!(dev->features & CLOCK_EVT_FEAT_C3STOP))
+		return 1;
+	return tick_broadcast_oneshot_available();
 }
 
 /*

commit e2830b5c1b2b2217894370a3b95af87d4a958401
Author: Torben Hohn <torbenh@gmx.de>
Date:   Thu Jan 27 16:00:32 2011 +0100

    time: Make do_timer() and xtime_lock local to kernel/time/
    
    All callers of do_timer() are converted to xtime_update(). The only
    users of xtime_lock are in kernel/time/. Make both local to
    kernel/time/ and remove them from the global header files.
    
    [ tglx: Reuse tick-internal.h instead of creating another local header
            file. Massaged changelog ]
    
    Signed-off-by: Torben Hohn <torbenh@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: johnstul@us.ibm.com
    Cc: yong.zhang0@gmail.com
    Cc: hch@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 051bc80a0c43..0e98fac3d479 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -18,7 +18,6 @@
 #include <linux/percpu.h>
 #include <linux/profile.h>
 #include <linux/sched.h>
-#include <linux/tick.h>
 
 #include <asm/irq_regs.h>
 

commit 909ea96468096b07fbb41aaf69be060d92bd9271
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Dec 8 16:22:55 2010 +0100

    core: Replace __get_cpu_var with __this_cpu_read if not used for an address.
    
    __get_cpu_var() can be replaced with this_cpu_read and will then use a
    single read instruction with implied address calculation to access the
    correct per cpu instance.
    
    However, the address of a per cpu variable passed to __this_cpu_read()
    cannot be determined (since it's an implied address conversion through
    segment prefixes).  Therefore apply this only to uses of __get_cpu_var
    where the address of the variable is not used.
    
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index b6b898d2eeef..051bc80a0c43 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -49,7 +49,7 @@ struct tick_device *tick_get_device(int cpu)
  */
 int tick_is_oneshot_available(void)
 {
-	struct clock_event_device *dev = __get_cpu_var(tick_cpu_device).evtdev;
+	struct clock_event_device *dev = __this_cpu_read(tick_cpu_device.evtdev);
 
 	return dev && (dev->features & CLOCK_EVT_FEAT_ONESHOT);
 }

commit b5f91da0a6973bb6f9ff3b91b0e92c0773a458f3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 8 12:40:31 2009 +0100

    clockevents: Convert to raw_spinlock
    
    Convert locks which cannot be sleeping locks in preempt-rt to
    raw_spinlocks.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index af39cf1cfa50..b6b898d2eeef 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -34,7 +34,7 @@ DEFINE_PER_CPU(struct tick_device, tick_cpu_device);
 ktime_t tick_next_period;
 ktime_t tick_period;
 int tick_do_timer_cpu __read_mostly = TICK_DO_TIMER_BOOT;
-static DEFINE_SPINLOCK(tick_device_lock);
+static DEFINE_RAW_SPINLOCK(tick_device_lock);
 
 /*
  * Debugging: see timer_list.c
@@ -209,7 +209,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	int cpu, ret = NOTIFY_OK;
 	unsigned long flags;
 
-	spin_lock_irqsave(&tick_device_lock, flags);
+	raw_spin_lock_irqsave(&tick_device_lock, flags);
 
 	cpu = smp_processor_id();
 	if (!cpumask_test_cpu(cpu, newdev->cpumask))
@@ -268,7 +268,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
 		tick_oneshot_notify();
 
-	spin_unlock_irqrestore(&tick_device_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 	return NOTIFY_STOP;
 
 out_bc:
@@ -278,7 +278,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	if (tick_check_broadcast_device(newdev))
 		ret = NOTIFY_STOP;
 
-	spin_unlock_irqrestore(&tick_device_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 
 	return ret;
 }
@@ -311,7 +311,7 @@ static void tick_shutdown(unsigned int *cpup)
 	struct clock_event_device *dev = td->evtdev;
 	unsigned long flags;
 
-	spin_lock_irqsave(&tick_device_lock, flags);
+	raw_spin_lock_irqsave(&tick_device_lock, flags);
 	td->mode = TICKDEV_MODE_PERIODIC;
 	if (dev) {
 		/*
@@ -322,7 +322,7 @@ static void tick_shutdown(unsigned int *cpup)
 		clockevents_exchange_device(dev, NULL);
 		td->evtdev = NULL;
 	}
-	spin_unlock_irqrestore(&tick_device_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
 static void tick_suspend(void)
@@ -330,9 +330,9 @@ static void tick_suspend(void)
 	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
 	unsigned long flags;
 
-	spin_lock_irqsave(&tick_device_lock, flags);
+	raw_spin_lock_irqsave(&tick_device_lock, flags);
 	clockevents_shutdown(td->evtdev);
-	spin_unlock_irqrestore(&tick_device_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
 static void tick_resume(void)
@@ -341,7 +341,7 @@ static void tick_resume(void)
 	unsigned long flags;
 	int broadcast = tick_resume_broadcast();
 
-	spin_lock_irqsave(&tick_device_lock, flags);
+	raw_spin_lock_irqsave(&tick_device_lock, flags);
 	clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_RESUME);
 
 	if (!broadcast) {
@@ -350,7 +350,7 @@ static void tick_resume(void)
 		else
 			tick_resume_oneshot();
 	}
-	spin_unlock_irqrestore(&tick_device_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
 /*

commit d192c47f25daa98996c7eae543d8a27e41247ec2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 8 12:49:26 2009 +0100

    clockevents: Make tick_device_lock static
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 83c4417b6a3c..af39cf1cfa50 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -34,7 +34,7 @@ DEFINE_PER_CPU(struct tick_device, tick_cpu_device);
 ktime_t tick_next_period;
 ktime_t tick_period;
 int tick_do_timer_cpu __read_mostly = TICK_DO_TIMER_BOOT;
-DEFINE_SPINLOCK(tick_device_lock);
+static DEFINE_SPINLOCK(tick_device_lock);
 
 /*
  * Debugging: see timer_list.c

commit 74a03b69d1b5ce00a568e142ca97e76b7f5239c6
Author: john stultz <johnstul@us.ibm.com>
Date:   Fri May 1 13:10:25 2009 -0700

    clockevents: prevent endless loop in tick_handle_periodic()
    
    tick_handle_periodic() can lock up hard when a one shot clock event
    device is used in combination with jiffies clocksource.
    
    Avoid an endless loop issue by requiring that a highres valid
    clocksource be installed before we call tick_periodic() in a loop when
    using ONESHOT mode. The result is we will only increment jiffies once
    per interrupt until a continuous hardware clocksource is available.
    
    Without this, we can run into a endless loop, where each cycle through
    the loop, jiffies is updated which increments time by tick_period or
    more (due to clock steering), which can cause the event programming to
    think the next event was before the newly incremented time and fail
    causing tick_periodic() to be called again and the whole process loops
    forever.
    
    [ Impact: prevent hard lock up ]
    
    Signed-off-by: John Stultz <johnstul@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@kernel.org

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 21a5ca849514..83c4417b6a3c 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -93,7 +93,17 @@ void tick_handle_periodic(struct clock_event_device *dev)
 	for (;;) {
 		if (!clockevents_program_event(dev, next, ktime_get()))
 			return;
-		tick_periodic(cpu);
+		/*
+		 * Have to be careful here. If we're in oneshot mode,
+		 * before we call tick_periodic() in a loop, we need
+		 * to be sure we're using a real hardware clocksource.
+		 * Otherwise we could get trapped in an infinite
+		 * loop, as the tick_periodic() increments jiffies,
+		 * when then will increment time, posibly causing
+		 * the loop to trigger again and again.
+		 */
+		if (timekeeping_valid_for_hres())
+			tick_periodic(cpu);
 		next = ktime_add(next, tick_period);
 	}
 }

commit 94df7de0289bc2df3d6e85cd2ece52bf42682f45
Author: Sebastien Dugue <sebastien.dugue@bull.net>
Date:   Mon Dec 1 14:09:07 2008 +0100

    hrtimers: allow the hot-unplugging of all cpus
    
    Impact: fix CPU hotplug hang on Power6 testbox
    
    On architectures that support offlining all cpus (at least powerpc/pseries),
    hot-unpluging the tick_do_timer_cpu can result in a system hang.
    
    This comes from the fact that if the cpu going down happens to be the
    cpu doing the tick, then as the tick_do_timer_cpu handover happens after the
    cpu is dead (via the CPU_DEAD notification), we're left without ticks,
    jiffies are frozen and any task relying on timers (msleep, ...) is stuck.
    That's particularly the case for the cpu looping in __cpu_die() waiting
    for the dying cpu to be dead.
    
    This patch addresses this by having the tick_do_timer_cpu handover happen
    earlier during the CPU_DYING notification. For this, a new clockevent
    notification type is introduced (CLOCK_EVT_NOTIFY_CPU_DYING) which is triggered
    in hrtimer_cpu_notify().
    
    Signed-off-by: Sebastien Dugue <sebastien.dugue@bull.net>
    Cc: <stable@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 63e05d423a09..21a5ca849514 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -273,6 +273,21 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	return ret;
 }
 
+/*
+ * Transfer the do_timer job away from a dying cpu.
+ *
+ * Called with interrupts disabled.
+ */
+static void tick_handover_do_timer(int *cpup)
+{
+	if (*cpup == tick_do_timer_cpu) {
+		int cpu = cpumask_first(cpu_online_mask);
+
+		tick_do_timer_cpu = (cpu < nr_cpu_ids) ? cpu :
+			TICK_DO_TIMER_NONE;
+	}
+}
+
 /*
  * Shutdown an event device on a given cpu:
  *
@@ -297,13 +312,6 @@ static void tick_shutdown(unsigned int *cpup)
 		clockevents_exchange_device(dev, NULL);
 		td->evtdev = NULL;
 	}
-	/* Transfer the do_timer job away from this cpu */
-	if (*cpup == tick_do_timer_cpu) {
-		int cpu = cpumask_first(cpu_online_mask);
-
-		tick_do_timer_cpu = (cpu < nr_cpu_ids) ? cpu :
-			TICK_DO_TIMER_NONE;
-	}
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
@@ -357,6 +365,10 @@ static int tick_notify(struct notifier_block *nb, unsigned long reason,
 		tick_broadcast_oneshot_control(reason);
 		break;
 
+	case CLOCK_EVT_NOTIFY_CPU_DYING:
+		tick_handover_do_timer(dev);
+		break;
+
 	case CLOCK_EVT_NOTIFY_CPU_DEAD:
 		tick_shutdown_broadcast_oneshot(dev);
 		tick_shutdown_broadcast(dev);

commit 6b954823c24f04ed026a8517f6bab5abda279db8
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Jan 1 10:12:25 2009 +1030

    cpumask: convert kernel time functions
    
    Impact: Use new APIs
    
    Convert kernel/time functions to use struct cpumask *.
    
    Note the ugly bitmap declarations in tick-broadcast.c.  These should
    be cpumask_var_t, but there was no obvious initialization function to
    put the alloc_cpumask_var() calls in.  This was safe.
    
    (Eventually 'struct cpumask' will be undefined for CONFIG_CPUMASK_OFFSTACK,
    so we use a bitmap here to show we really mean it).
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index f8372be74122..63e05d423a09 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -254,7 +254,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 		curdev = NULL;
 	}
 	clockevents_exchange_device(curdev, newdev);
-	tick_setup_device(td, newdev, cpu, &cpumask_of_cpu(cpu));
+	tick_setup_device(td, newdev, cpu, cpumask_of(cpu));
 	if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
 		tick_oneshot_notify();
 
@@ -299,9 +299,9 @@ static void tick_shutdown(unsigned int *cpup)
 	}
 	/* Transfer the do_timer job away from this cpu */
 	if (*cpup == tick_do_timer_cpu) {
-		int cpu = first_cpu(cpu_online_map);
+		int cpu = cpumask_first(cpu_online_mask);
 
-		tick_do_timer_cpu = (cpu != NR_CPUS) ? cpu :
+		tick_do_timer_cpu = (cpu < nr_cpu_ids) ? cpu :
 			TICK_DO_TIMER_NONE;
 	}
 	spin_unlock_irqrestore(&tick_device_lock, flags);

commit 320ab2b0b1e08e3805a3e1084a2f0eb1938d5d67
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Dec 13 21:20:26 2008 +1030

    cpumask: convert struct clock_event_device to cpumask pointers.
    
    Impact: change calling convention of existing clock_event APIs
    
    struct clock_event_timer's cpumask field gets changed to take pointer,
    as does the ->broadcast function.
    
    Another single-patch change.  For safety, we BUG_ON() in
    clockevents_register_device() if it's not set.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index ab65d217583f..f8372be74122 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -171,7 +171,7 @@ static void tick_setup_device(struct tick_device *td,
 	 * When the device is not per cpu, pin the interrupt to the
 	 * current cpu:
 	 */
-	if (!cpumask_equal(&newdev->cpumask, cpumask))
+	if (!cpumask_equal(newdev->cpumask, cpumask))
 		irq_set_affinity(newdev->irq, cpumask);
 
 	/*
@@ -202,14 +202,14 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	spin_lock_irqsave(&tick_device_lock, flags);
 
 	cpu = smp_processor_id();
-	if (!cpu_isset(cpu, newdev->cpumask))
+	if (!cpumask_test_cpu(cpu, newdev->cpumask))
 		goto out_bc;
 
 	td = &per_cpu(tick_cpu_device, cpu);
 	curdev = td->evtdev;
 
 	/* cpu local device ? */
-	if (!cpus_equal(newdev->cpumask, cpumask_of_cpu(cpu))) {
+	if (!cpumask_equal(newdev->cpumask, cpumask_of(cpu))) {
 
 		/*
 		 * If the cpu affinity of the device interrupt can not
@@ -222,7 +222,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 		 * If we have a cpu local device already, do not replace it
 		 * by a non cpu local device
 		 */
-		if (curdev && cpus_equal(curdev->cpumask, cpumask_of_cpu(cpu)))
+		if (curdev && cpumask_equal(curdev->cpumask, cpumask_of(cpu)))
 			goto out_bc;
 	}
 

commit 0de26520c7cabf36e1de090ea8092f011a6106ce
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Dec 13 21:20:26 2008 +1030

    cpumask: make irq_set_affinity() take a const struct cpumask
    
    Impact: change existing irq_chip API
    
    Not much point with gentle transition here: the struct irq_chip's
    setaffinity method signature needs to change.
    
    Fortunately, not widely used code, but hits a few architectures.
    
    Note: In irq_select_affinity() I save a temporary in by mangling
    irq_desc[irq].affinity directly.  Ingo, does this break anything?
    
    (Folded in fix from KOSAKI Motohiro)
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Reviewed-by: Grant Grundler <grundler@parisc-linux.org>
    Acked-by: Ingo Molnar <mingo@redhat.com>
    Cc: ralf@linux-mips.org
    Cc: grundler@parisc-linux.org
    Cc: jeremy@xensource.com
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index df12434b43ca..ab65d217583f 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -136,7 +136,7 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
  */
 static void tick_setup_device(struct tick_device *td,
 			      struct clock_event_device *newdev, int cpu,
-			      const cpumask_t *cpumask)
+			      const struct cpumask *cpumask)
 {
 	ktime_t next_event;
 	void (*handler)(struct clock_event_device *) = NULL;
@@ -171,8 +171,8 @@ static void tick_setup_device(struct tick_device *td,
 	 * When the device is not per cpu, pin the interrupt to the
 	 * current cpu:
 	 */
-	if (!cpus_equal(newdev->cpumask, *cpumask))
-		irq_set_affinity(newdev->irq, *cpumask);
+	if (!cpumask_equal(&newdev->cpumask, cpumask))
+		irq_set_affinity(newdev->irq, cpumask);
 
 	/*
 	 * When global broadcasting is active, check if the current

commit 27ce4cb4a0c7cf59b9a9952266883862f2e4c99f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 22 19:04:02 2008 +0200

    clockevents: prevent mode mismatch on cpu online
    
    Impact: timer hang on CPU online observed on AMD C1E systems
    
    When a CPU is brought online then the broadcast machinery can
    be in the one shot state already. Check this and setup the timer
    device of the new CPU in one shot mode so the broadcast code
    can pick up the next_event value correctly.
    
    Another AMD C1E oddity, as we switch to broadcast immediately and
    not after the full bring up via the ACPI cpu idle code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index b523d095decf..df12434b43ca 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -109,7 +109,8 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 	if (!tick_device_is_functional(dev))
 		return;
 
-	if (dev->features & CLOCK_EVT_FEAT_PERIODIC) {
+	if ((dev->features & CLOCK_EVT_FEAT_PERIODIC) &&
+	    !tick_broadcast_oneshot_active()) {
 		clockevents_set_mode(dev, CLOCK_EVT_MODE_PERIODIC);
 	} else {
 		unsigned long seq;

commit 6441402b1f173fa38e561d3cee7c01c32e5281ad
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 22 18:46:37 2008 +0200

    clockevents: prevent cpu online to interfere with nohz
    
    Impact: rare hang which can be triggered on CPU online.
    
    tick_do_timer_cpu keeps track of the CPU which updates jiffies
    via do_timer. The value -1 is used to signal, that currently no
    CPU is doing this. There are two cases, where the variable can
    have this state:
    
     boot:
        necessary for systems where the boot cpu id can be != 0
    
     nohz long idle sleep:
        When the CPU which did the jiffies update last goes into
        a long idle sleep it drops the update jiffies duty so
        another CPU which is not idle can pick it up and keep
        jiffies going.
    
    Using the same value for both situations is wrong, as the CPU online
    code can see the -1 state when the timer of the newly onlined CPU is
    setup. The setup for a newly onlined CPU goes through periodic mode
    and can pick up the do_timer duty without being aware of the nohz /
    highres mode of the already running system.
    
    Use two separate states and make them constants to avoid magic
    numbers confusion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 019315ebf9de..b523d095decf 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -33,7 +33,7 @@ DEFINE_PER_CPU(struct tick_device, tick_cpu_device);
  */
 ktime_t tick_next_period;
 ktime_t tick_period;
-int tick_do_timer_cpu __read_mostly = -1;
+int tick_do_timer_cpu __read_mostly = TICK_DO_TIMER_BOOT;
 DEFINE_SPINLOCK(tick_device_lock);
 
 /*
@@ -148,7 +148,7 @@ static void tick_setup_device(struct tick_device *td,
 		 * If no cpu took the do_timer update, assign it to
 		 * this cpu:
 		 */
-		if (tick_do_timer_cpu == -1) {
+		if (tick_do_timer_cpu == TICK_DO_TIMER_BOOT) {
 			tick_do_timer_cpu = cpu;
 			tick_next_period = ktime_get();
 			tick_period = ktime_set(0, NSEC_PER_SEC / HZ);
@@ -300,7 +300,8 @@ static void tick_shutdown(unsigned int *cpup)
 	if (*cpup == tick_do_timer_cpu) {
 		int cpu = first_cpu(cpu_online_map);
 
-		tick_do_timer_cpu = (cpu != NR_CPUS) ? cpu : -1;
+		tick_do_timer_cpu = (cpu != NR_CPUS) ? cpu :
+			TICK_DO_TIMER_NONE;
 	}
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 }

commit 2344abbcbdb82140050e8be29d3d55e4f6fe860b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 16 11:32:50 2008 -0700

    clockevents: make device shutdown robust
    
    The device shut down does not cleanup the next_event variable of the
    clock event device. So when the device is reactivated the possible
    stale next_event value can prevent the device to be reprogrammed as it
    claims to wait on a event already.
    
    This is the root cause of the resurfacing suspend/resume problem,
    where systems need key press to come back to life.
    
    Fix this by setting next_event to KTIME_MAX when the device is shut
    down. Use a separate function for shutdown which takes care of that
    and only keep the direct set mode call in the broadcast code, where we
    can not touch the next_event value.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index c4777193d567..019315ebf9de 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -249,7 +249,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	 * not give it back to the clockevents layer !
 	 */
 	if (tick_is_broadcast_device(curdev)) {
-		clockevents_set_mode(curdev, CLOCK_EVT_MODE_SHUTDOWN);
+		clockevents_shutdown(curdev);
 		curdev = NULL;
 	}
 	clockevents_exchange_device(curdev, newdev);
@@ -311,7 +311,7 @@ static void tick_suspend(void)
 	unsigned long flags;
 
 	spin_lock_irqsave(&tick_device_lock, flags);
-	clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_SHUTDOWN);
+	clockevents_shutdown(td->evtdev);
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 

commit 7c1e76897492d92b6a1c2d6892494d39ded9680c
Author: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Wed Sep 3 21:36:50 2008 +0000

    clockevents: prevent clockevent event_handler ending up handler_noop
    
    There is a ordering related problem with clockevents code, due to which
    clockevents_register_device() called after tickless/highres switch
    will not work. The new clockevent ends up with clockevents_handle_noop as
    event handler, resulting in no timer activity.
    
    The problematic path seems to be
    
    * old device already has hrtimer_interrupt as the event_handler
    * new clockevent device registers with a higher rating
    * tick_check_new_device() is called
      * clockevents_exchange_device() gets called
        * old->event_handler is set to clockevents_handle_noop
      * tick_setup_device() is called for the new device
        * which sets new->event_handler using the old->event_handler which is noop.
    
    Change the ordering so that new device inherits the proper handler.
    
    This does not have any issue in normal case as most likely all the clockevent
    devices are setup before the highres switch. But, can potentially be affecting
    some corner case where HPET force detect happens after the highres switch.
    This was a problem with HPET in MSI mode code that we have been experimenting
    with.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Signed-off-by: Shaohua Li <shaohua.li@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 80c4336f4188..c4777193d567 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -161,6 +161,7 @@ static void tick_setup_device(struct tick_device *td,
 	} else {
 		handler = td->evtdev->event_handler;
 		next_event = td->evtdev->next_event;
+		td->evtdev->event_handler = clockevents_handle_noop;
 	}
 
 	td->evtdev = newdev;

commit 0bc3cc03fa6e1c20aecb5a33356bcaae410640b9
Author: Mike Travis <travis@sgi.com>
Date:   Thu Jul 24 18:21:31 2008 -0700

    cpumask: change cpumask_of_cpu_ptr to use new cpumask_of_cpu
    
      * Replace previous instances of the cpumask_of_cpu_ptr* macros
        with a the new (lvalue capable) generic cpumask_of_cpu().
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index bf43284d6855..80c4336f4188 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -196,12 +196,10 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	struct tick_device *td;
 	int cpu, ret = NOTIFY_OK;
 	unsigned long flags;
-	cpumask_of_cpu_ptr_declare(cpumask);
 
 	spin_lock_irqsave(&tick_device_lock, flags);
 
 	cpu = smp_processor_id();
-	cpumask_of_cpu_ptr_next(cpumask, cpu);
 	if (!cpu_isset(cpu, newdev->cpumask))
 		goto out_bc;
 
@@ -209,7 +207,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	curdev = td->evtdev;
 
 	/* cpu local device ? */
-	if (!cpus_equal(newdev->cpumask, *cpumask)) {
+	if (!cpus_equal(newdev->cpumask, cpumask_of_cpu(cpu))) {
 
 		/*
 		 * If the cpu affinity of the device interrupt can not
@@ -222,7 +220,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 		 * If we have a cpu local device already, do not replace it
 		 * by a non cpu local device
 		 */
-		if (curdev && cpus_equal(curdev->cpumask, *cpumask))
+		if (curdev && cpus_equal(curdev->cpumask, cpumask_of_cpu(cpu)))
 			goto out_bc;
 	}
 
@@ -254,7 +252,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 		curdev = NULL;
 	}
 	clockevents_exchange_device(curdev, newdev);
-	tick_setup_device(td, newdev, cpu, cpumask);
+	tick_setup_device(td, newdev, cpu, &cpumask_of_cpu(cpu));
 	if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
 		tick_oneshot_notify();
 

commit c18a41fbbc500ac0307ffd2b0ae73c2af9d0b0ab
Author: Mike Travis <travis@sgi.com>
Date:   Tue Jul 15 14:14:34 2008 -0700

    cpumask: Optimize cpumask_of_cpu in kernel/time/tick-common.c
    
      * Optimize various places where a pointer to the cpumask_of_cpu value
        will result in reducing stack pressure.
    
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 4f3886562b8c..bf43284d6855 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -135,7 +135,7 @@ void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
  */
 static void tick_setup_device(struct tick_device *td,
 			      struct clock_event_device *newdev, int cpu,
-			      cpumask_t cpumask)
+			      const cpumask_t *cpumask)
 {
 	ktime_t next_event;
 	void (*handler)(struct clock_event_device *) = NULL;
@@ -169,8 +169,8 @@ static void tick_setup_device(struct tick_device *td,
 	 * When the device is not per cpu, pin the interrupt to the
 	 * current cpu:
 	 */
-	if (!cpus_equal(newdev->cpumask, cpumask))
-		irq_set_affinity(newdev->irq, cpumask);
+	if (!cpus_equal(newdev->cpumask, *cpumask))
+		irq_set_affinity(newdev->irq, *cpumask);
 
 	/*
 	 * When global broadcasting is active, check if the current
@@ -196,20 +196,20 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	struct tick_device *td;
 	int cpu, ret = NOTIFY_OK;
 	unsigned long flags;
-	cpumask_t cpumask;
+	cpumask_of_cpu_ptr_declare(cpumask);
 
 	spin_lock_irqsave(&tick_device_lock, flags);
 
 	cpu = smp_processor_id();
+	cpumask_of_cpu_ptr_next(cpumask, cpu);
 	if (!cpu_isset(cpu, newdev->cpumask))
 		goto out_bc;
 
 	td = &per_cpu(tick_cpu_device, cpu);
 	curdev = td->evtdev;
-	cpumask = cpumask_of_cpu(cpu);
 
 	/* cpu local device ? */
-	if (!cpus_equal(newdev->cpumask, cpumask)) {
+	if (!cpus_equal(newdev->cpumask, *cpumask)) {
 
 		/*
 		 * If the cpu affinity of the device interrupt can not
@@ -222,7 +222,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 		 * If we have a cpu local device already, do not replace it
 		 * by a non cpu local device
 		 */
-		if (curdev && cpus_equal(curdev->cpumask, cpumask))
+		if (curdev && cpus_equal(curdev->cpumask, *cpumask))
 			goto out_bc;
 	}
 

commit d7b906897e9caae452947e33674df0a2d6f7e10f
Author: Russell King <rmk+lkml@arm.linux.org.uk>
Date:   Thu Apr 17 07:46:24 2008 +0200

    [S390] genirq/clockevents: move irq affinity prototypes/inlines to interrupt.h
    
    > Generic code is not supposed to include irq.h. Replace this include
    > by linux/hardirq.h instead and add/replace an include of linux/irq.h
    > in asm header files where necessary.
    > This change should only matter for architectures that make use of
    > GENERIC_CLOCKEVENTS.
    > Architectures in question are mips, x86, arm, sh, powerpc, uml and sparc64.
    >
    > I did some cross compile tests for mips, x86_64, arm, powerpc and sparc64.
    > This patch fixes also build breakages caused by the include replacement in
    > tick-common.h.
    
    I generally dislike adding optional linux/* includes in asm/* includes -
    I'm nervous about this causing include loops.
    
    However, there's a separate point to be discussed here.
    
    That is, what interfaces are expected of every architecture in the kernel.
    If generic code wants to be able to set the affinity of interrupts, then
    that needs to become part of the interfaces listed in linux/interrupt.h
    rather than linux/irq.h.
    
    So what I suggest is this approach instead (against Linus' tree of a
    couple of days ago) - we move irq_set_affinity() and irq_can_set_affinity()
    to linux/interrupt.h, change the linux/irq.h includes to linux/interrupt.h
    and include asm/irq_regs.h where needed (asm/irq_regs.h is supposed to be
    rarely used include since not much touches the stacked parent context
    registers.)
    
    Build tested on ARM PXA family kernels and ARM's Realview platform
    kernels which both use genirq.
    
    [ tglx@linutronix.de: add GENERIC_HARDIRQ dependencies ]
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 1bea399a9ef0..4f3886562b8c 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -14,12 +14,14 @@
 #include <linux/cpu.h>
 #include <linux/err.h>
 #include <linux/hrtimer.h>
-#include <linux/irq.h>
+#include <linux/interrupt.h>
 #include <linux/percpu.h>
 #include <linux/profile.h>
 #include <linux/sched.h>
 #include <linux/tick.h>
 
+#include <asm/irq_regs.h>
+
 #include "tick-internal.h"
 
 /*

commit 1595f452f3d8daa066bfd3ba4120754bed3329e1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Oct 14 22:57:45 2007 +0200

    clockevents: introduce force broadcast notifier
    
    The 64bit SMP bootup is slightly different to the 32bit one. It enables
    the boot CPU local APIC timer before all CPUs are brought up. Some AMD C1E
    systems have the C1E feature flag only set in the secondary CPU. Due to
    the early enable of the boot CPU local APIC timer the APIC timer is
    registered as a fully functional device. When we detect the wreckage during
    the bringup of the secondary CPU, we need to force the boot CPU into
    broadcast mode.
    
    Add a new notifier reason and implement the force broadcast in the clock
    events layer.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 3f3ae3907830..1bea399a9ef0 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -345,6 +345,7 @@ static int tick_notify(struct notifier_block *nb, unsigned long reason,
 
 	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
 	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
+	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
 		tick_broadcast_on_off(reason, dev);
 		break;
 

commit 4a93232dab0a07074bcc5291a0f1f39919916f31
Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Fri Oct 12 23:04:23 2007 +0200

    clock events: allow replacement of broadcast timer
    
    Change the broadcast timer, if a timer with higher rating becomes available.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Greg KH <greg@kroah.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 77a21abc8716..3f3ae3907830 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -200,7 +200,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 
 	cpu = smp_processor_id();
 	if (!cpu_isset(cpu, newdev->cpumask))
-		goto out;
+		goto out_bc;
 
 	td = &per_cpu(tick_cpu_device, cpu);
 	curdev = td->evtdev;
@@ -265,7 +265,7 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	 */
 	if (tick_check_broadcast_device(newdev))
 		ret = NOTIFY_STOP;
-out:
+
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 
 	return ret;

commit 18de5bc4c1f1f1fa5e14f354a7603bd6e9d4e3b6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 21 04:37:34 2007 -0700

    clockevents: fix resume logic
    
    We need to make sure, that the clockevent devices are resumed, before
    the tick is resumed. The current resume logic does not guarantee this.
    
    Add CLOCK_EVT_MODE_RESUME and call the set mode functions of the clock
    event devices before resuming the tick / oneshot functionality.
    
    Fixup the existing users.
    
    Thanks to Nigel Cunningham for tracking down a long standing thinko,
    which affected the jinxed VAIO.
    
    [akpm@linux-foundation.org: xen build fix]
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index a96ec9ab3454..77a21abc8716 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -318,12 +318,17 @@ static void tick_resume(void)
 {
 	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
 	unsigned long flags;
+	int broadcast = tick_resume_broadcast();
 
 	spin_lock_irqsave(&tick_device_lock, flags);
-	if (td->mode == TICKDEV_MODE_PERIODIC)
-		tick_setup_periodic(td->evtdev, 0);
-	else
-		tick_resume_oneshot();
+	clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_RESUME);
+
+	if (!broadcast) {
+		if (td->mode == TICKDEV_MODE_PERIODIC)
+			tick_setup_periodic(td->evtdev, 0);
+		else
+			tick_resume_oneshot();
+	}
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
@@ -360,8 +365,7 @@ static int tick_notify(struct notifier_block *nb, unsigned long reason,
 		break;
 
 	case CLOCK_EVT_NOTIFY_RESUME:
-		if (!tick_resume_broadcast())
-			tick_resume();
+		tick_resume();
 		break;
 
 	default:

commit d3ed782458f315c30ea679b919a2cc59f2b82565
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 8 00:30:03 2007 -0700

    highres/dyntick: prevent xtime lock contention
    
    While the !highres/!dyntick code assigns the duty of the do_timer() call to
    one specific CPU, this was dropped in the highres/dyntick part during
    development.
    
    Steven Rostedt discovered the xtime lock contention on highres/dyntick due
    to several CPUs trying to update jiffies.
    
    Add the single CPU assignement back.  In the dyntick case this needs to be
    handled carefully, as the CPU which has the do_timer() duty must drop the
    assignement and let it be grabbed by another CPU, which is active.
    Otherwise the do_timer() calls would not happen during the long sleep.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Mark Lord <mlord@pobox.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index bfda3f7f0716..a96ec9ab3454 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -31,7 +31,7 @@ DEFINE_PER_CPU(struct tick_device, tick_cpu_device);
  */
 ktime_t tick_next_period;
 ktime_t tick_period;
-static int tick_do_timer_cpu = -1;
+int tick_do_timer_cpu __read_mostly = -1;
 DEFINE_SPINLOCK(tick_device_lock);
 
 /*
@@ -295,6 +295,12 @@ static void tick_shutdown(unsigned int *cpup)
 		clockevents_exchange_device(dev, NULL);
 		td->evtdev = NULL;
 	}
+	/* Transfer the do_timer job away from this cpu */
+	if (*cpup == tick_do_timer_cpu) {
+		int cpu = first_cpu(cpu_online_map);
+
+		tick_do_timer_cpu = (cpu != NR_CPUS) ? cpu : -1;
+	}
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 

commit cd05a1f818073a623455a58e756c5b419fc98db9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 17 00:25:52 2007 +0100

    [PATCH] clockevents: Fix suspend/resume to disk hangs
    
    I finally found a dual core box, which survives suspend/resume without
    crashing in the middle of nowhere. Sigh, I never figured out from the
    code and the bug reports what's going on.
    
    The observed hangs are caused by a stale state transition of the clock
    event devices, which keeps the RCU synchronization away from completion,
    when the non boot CPU is brought back up.
    
    The suspend/resume in oneshot mode needs the similar care as the
    periodic mode during suspend to RAM. My assumption that the state
    transitions during the different shutdown/bringups of s2disk would go
    through the periodic boot phase and then switch over to highres resp.
    nohz mode were simply wrong.
    
    Add the appropriate suspend / resume handling for the non periodic
    modes.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 43ba1bdec14c..bfda3f7f0716 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -298,18 +298,17 @@ static void tick_shutdown(unsigned int *cpup)
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
-static void tick_suspend_periodic(void)
+static void tick_suspend(void)
 {
 	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
 	unsigned long flags;
 
 	spin_lock_irqsave(&tick_device_lock, flags);
-	if (td->mode == TICKDEV_MODE_PERIODIC)
-		clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_SHUTDOWN);
+	clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_SHUTDOWN);
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
-static void tick_resume_periodic(void)
+static void tick_resume(void)
 {
 	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
 	unsigned long flags;
@@ -317,6 +316,8 @@ static void tick_resume_periodic(void)
 	spin_lock_irqsave(&tick_device_lock, flags);
 	if (td->mode == TICKDEV_MODE_PERIODIC)
 		tick_setup_periodic(td->evtdev, 0);
+	else
+		tick_resume_oneshot();
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
@@ -348,13 +349,13 @@ static int tick_notify(struct notifier_block *nb, unsigned long reason,
 		break;
 
 	case CLOCK_EVT_NOTIFY_SUSPEND:
-		tick_suspend_periodic();
+		tick_suspend();
 		tick_suspend_broadcast();
 		break;
 
 	case CLOCK_EVT_NOTIFY_RESUME:
 		if (!tick_resume_broadcast())
-			tick_resume_periodic();
+			tick_resume();
 		break;
 
 	default:

commit 6321dd60c76b2e12383bc06046288b15397ed3a0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 6 08:25:42 2007 +0100

    [PATCH] Save/restore periodic tick information over suspend/resume
    
    The programming of periodic tick devices needs to be saved/restored
    across suspend/resume - otherwise we might end up with a system coming
    up that relies on getting a PIT (or HPET) interrupt, while those devices
    default to 'no interrupts' after powerup. (To confuse things it worked
    to a certain degree on some systems because the lapic gets initialized
    as a side-effect of SMP bootup.)
    
    This suspend / resume thing was dropped unintentionally during the
    last-minute -mm code reshuffling.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 0986a2bfab49..43ba1bdec14c 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -298,6 +298,28 @@ static void tick_shutdown(unsigned int *cpup)
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 }
 
+static void tick_suspend_periodic(void)
+{
+	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
+	unsigned long flags;
+
+	spin_lock_irqsave(&tick_device_lock, flags);
+	if (td->mode == TICKDEV_MODE_PERIODIC)
+		clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_SHUTDOWN);
+	spin_unlock_irqrestore(&tick_device_lock, flags);
+}
+
+static void tick_resume_periodic(void)
+{
+	struct tick_device *td = &__get_cpu_var(tick_cpu_device);
+	unsigned long flags;
+
+	spin_lock_irqsave(&tick_device_lock, flags);
+	if (td->mode == TICKDEV_MODE_PERIODIC)
+		tick_setup_periodic(td->evtdev, 0);
+	spin_unlock_irqrestore(&tick_device_lock, flags);
+}
+
 /*
  * Notification about clock event devices
  */
@@ -325,6 +347,16 @@ static int tick_notify(struct notifier_block *nb, unsigned long reason,
 		tick_shutdown(dev);
 		break;
 
+	case CLOCK_EVT_NOTIFY_SUSPEND:
+		tick_suspend_periodic();
+		tick_suspend_broadcast();
+		break;
+
+	case CLOCK_EVT_NOTIFY_RESUME:
+		if (!tick_resume_broadcast())
+			tick_resume_periodic();
+		break;
+
 	default:
 		break;
 	}

commit 3494c16676a21e7e53e21b08a0a469a38df6dcfb
Author: David S. Miller <davem@sunset.davemloft.net>
Date:   Sat Feb 24 22:11:42 2007 -0800

    [TICK] tick-common: Fix one-shot handling in tick_handle_periodic().
    
    When clockevents_program_event() is given an expire time in the
    past, it does not update dev->next_event, so this looping code
    would loop forever once the first in-the-past expiration time
    was used.
    
    Keep advancing "next" locally to fix this bug.
    
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 4500e347f1bb..0986a2bfab49 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -77,6 +77,7 @@ static void tick_periodic(int cpu)
 void tick_handle_periodic(struct clock_event_device *dev)
 {
 	int cpu = smp_processor_id();
+	ktime_t next;
 
 	tick_periodic(cpu);
 
@@ -86,12 +87,12 @@ void tick_handle_periodic(struct clock_event_device *dev)
 	 * Setup the next period for devices, which do not have
 	 * periodic mode:
 	 */
+	next = ktime_add(dev->next_event, tick_period);
 	for (;;) {
-		ktime_t next = ktime_add(dev->next_event, tick_period);
-
 		if (!clockevents_program_event(dev, next, ktime_get()))
 			return;
 		tick_periodic(cpu);
+		next = ktime_add(next, tick_period);
 	}
 }
 

commit 289f480af87e45f7a6de6ba9b4c061c2e259fe98
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 16 01:28:15 2007 -0800

    [PATCH] Add debugging feature /proc/timer_list
    
    add /proc/timer_list, which prints all currently pending (high-res) timers,
    all clock-event sources and their parameters in a human-readable form.
    
    Sample output:
    
    Timer List Version: v0.1
    HRTIMER_MAX_CLOCK_BASES: 2
    now at 4246046273872 nsecs
    
    cpu: 0
     clock 0:
      .index:      0
      .resolution: 1 nsecs
      .get_time:   ktime_get_real
      .offset:     1273998312645738432 nsecs
    active timers:
     clock 1:
      .index:      1
      .resolution: 1 nsecs
      .get_time:   ktime_get
      .offset:     0 nsecs
    active timers:
     #0: <f5a90ec8>, hrtimer_sched_tick, hrtimer_stop_sched_tick, swapper/0
     # expires at 4246432689566 nsecs [in 386415694 nsecs]
     #1: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, pcscd/2050
     # expires at 4247018194689 nsecs [in 971920817 nsecs]
     #2: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, irqbalance/1909
     # expires at 4247351358392 nsecs [in 1305084520 nsecs]
     #3: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, crond/2157
     # expires at 4249097614968 nsecs [in 3051341096 nsecs]
     #4: <f5a90ec8>, it_real_fn, do_setitimer, syslogd/1888
     # expires at 4251329900926 nsecs [in 5283627054 nsecs]
      .expires_next   : 4246432689566 nsecs
      .hres_active    : 1
      .check_clocks   : 0
      .nr_events      : 31306
      .idle_tick      : 4246020791890 nsecs
      .tick_stopped   : 1
      .idle_jiffies   : 986504
      .idle_calls     : 40700
      .idle_sleeps    : 36014
      .idle_entrytime : 4246019418883 nsecs
      .idle_sleeptime : 4178181972709 nsecs
    
    cpu: 1
     clock 0:
      .index:      0
      .resolution: 1 nsecs
      .get_time:   ktime_get_real
      .offset:     1273998312645738432 nsecs
    active timers:
     clock 1:
      .index:      1
      .resolution: 1 nsecs
      .get_time:   ktime_get
      .offset:     0 nsecs
    active timers:
     #0: <f5a90ec8>, hrtimer_sched_tick, hrtimer_restart_sched_tick, swapper/0
     # expires at 4246050084568 nsecs [in 3810696 nsecs]
     #1: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, atd/2227
     # expires at 4261010635003 nsecs [in 14964361131 nsecs]
     #2: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, smartd/2332
     # expires at 5469485798970 nsecs [in 1223439525098 nsecs]
      .expires_next   : 4246050084568 nsecs
      .hres_active    : 1
      .check_clocks   : 0
      .nr_events      : 24043
      .idle_tick      : 4246046084568 nsecs
      .tick_stopped   : 0
      .idle_jiffies   : 986510
      .idle_calls     : 26360
      .idle_sleeps    : 22551
      .idle_entrytime : 4246043874339 nsecs
      .idle_sleeptime : 4170763761184 nsecs
    
    tick_broadcast_mask: 00000003
    event_broadcast_mask: 00000001
    
    CPU#0's local event device:
    
    Clock Event Device: lapic
     capabilities:   0000000e
     max_delta_ns:   807385544
     min_delta_ns:   1443
     mult:           44624025
     shift:          32
     set_next_event: lapic_next_event
     set_mode:       lapic_timer_setup
     event_handler:  hrtimer_interrupt
      .installed:  1
      .expires:    4246432689566 nsecs
    
    CPU#1's local event device:
    
    Clock Event Device: lapic
     capabilities:   0000000e
     max_delta_ns:   807385544
     min_delta_ns:   1443
     mult:           44624025
     shift:          32
     set_next_event: lapic_next_event
     set_mode:       lapic_timer_setup
     event_handler:  hrtimer_interrupt
      .installed:  1
      .expires:    4246050084568 nsecs
    
    Clock Event Device: hpet
     capabilities:   00000007
     max_delta_ns:   2147483647
     min_delta_ns:   3352
     mult:           61496110
     shift:          32
     set_next_event: hpet_next_event
     set_mode:       hpet_set_mode
     event_handler:  handle_nextevt_broadcast
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index c35d449be031..4500e347f1bb 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -34,6 +34,14 @@ ktime_t tick_period;
 static int tick_do_timer_cpu = -1;
 DEFINE_SPINLOCK(tick_device_lock);
 
+/*
+ * Debugging: see timer_list.c
+ */
+struct tick_device *tick_get_device(int cpu)
+{
+	return &per_cpu(tick_cpu_device, cpu);
+}
+
 /**
  * tick_is_oneshot_available - check for a oneshot capable event device
  */

commit 79bf2bb335b85db25d27421c798595a2fa2a0e82
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 16 01:28:03 2007 -0800

    [PATCH] tick-management: dyntick / highres functionality
    
    With Ingo Molnar <mingo@elte.hu>
    
    Add functions to provide dynamic ticks and high resolution timers.  The code
    which keeps track of jiffies and handles the long idle periods is shared
    between tick based and high resolution timer based dynticks.  The dyntick
    functionality can be disabled on the kernel commandline.  Provide also the
    infrastructure to support high resolution timers.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 48167a6ae55c..c35d449be031 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -34,6 +34,16 @@ ktime_t tick_period;
 static int tick_do_timer_cpu = -1;
 DEFINE_SPINLOCK(tick_device_lock);
 
+/**
+ * tick_is_oneshot_available - check for a oneshot capable event device
+ */
+int tick_is_oneshot_available(void)
+{
+	struct clock_event_device *dev = __get_cpu_var(tick_cpu_device).evtdev;
+
+	return dev && (dev->features & CLOCK_EVT_FEAT_ONESHOT);
+}
+
 /*
  * Periodic tick
  */
@@ -162,6 +172,8 @@ static void tick_setup_device(struct tick_device *td,
 
 	if (td->mode == TICKDEV_MODE_PERIODIC)
 		tick_setup_periodic(newdev, 0);
+	else
+		tick_setup_oneshot(newdev, handler, next_event);
 }
 
 /*
@@ -208,6 +220,12 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	 * feature.
 	 */
 	if (curdev) {
+		/*
+		 * Prefer one shot capable devices !
+		 */
+		if ((curdev->features & CLOCK_EVT_FEAT_ONESHOT) &&
+		    !(newdev->features & CLOCK_EVT_FEAT_ONESHOT))
+			goto out_bc;
 		/*
 		 * Check the rating
 		 */
@@ -226,6 +244,8 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 	}
 	clockevents_exchange_device(curdev, newdev);
 	tick_setup_device(td, newdev, cpu, cpumask);
+	if (newdev->features & CLOCK_EVT_FEAT_ONESHOT)
+		tick_oneshot_notify();
 
 	spin_unlock_irqrestore(&tick_device_lock, flags);
 	return NOTIFY_STOP;
@@ -285,7 +305,13 @@ static int tick_notify(struct notifier_block *nb, unsigned long reason,
 		tick_broadcast_on_off(reason, dev);
 		break;
 
+	case CLOCK_EVT_NOTIFY_BROADCAST_ENTER:
+	case CLOCK_EVT_NOTIFY_BROADCAST_EXIT:
+		tick_broadcast_oneshot_control(reason);
+		break;
+
 	case CLOCK_EVT_NOTIFY_CPU_DEAD:
+		tick_shutdown_broadcast_oneshot(dev);
 		tick_shutdown_broadcast(dev);
 		tick_shutdown(dev);
 		break;

commit f8381cba04ba8173fd5a2b8e5cd8b3290ee13a98
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 16 01:28:02 2007 -0800

    [PATCH] tick-management: broadcast functionality
    
    With Ingo Molnar <mingo@elte.hu>
    
    Add broadcast functionality, so per cpu clock event devices can be registered
    as dummy devices or switched from/to broadcast on demand.  The broadcast
    function distributes the events via the broadcast function of the clock event
    device.  This is primarily designed to replace the switch apic timer to / from
    IPI in power states, where the apic stops.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
index 46e4381c26ea..48167a6ae55c 100644
--- a/kernel/time/tick-common.c
+++ b/kernel/time/tick-common.c
@@ -20,17 +20,19 @@
 #include <linux/sched.h>
 #include <linux/tick.h>
 
+#include "tick-internal.h"
+
 /*
  * Tick devices
  */
-static DEFINE_PER_CPU(struct tick_device, tick_cpu_device);
+DEFINE_PER_CPU(struct tick_device, tick_cpu_device);
 /*
  * Tick next event: keeps track of the tick time
  */
-static ktime_t tick_next_period;
-static ktime_t tick_period;
+ktime_t tick_next_period;
+ktime_t tick_period;
 static int tick_do_timer_cpu = -1;
-static DEFINE_SPINLOCK(tick_device_lock);
+DEFINE_SPINLOCK(tick_device_lock);
 
 /*
  * Periodic tick
@@ -78,9 +80,13 @@ void tick_handle_periodic(struct clock_event_device *dev)
 /*
  * Setup the device for a periodic tick
  */
-void tick_setup_periodic(struct clock_event_device *dev)
+void tick_setup_periodic(struct clock_event_device *dev, int broadcast)
 {
-	dev->event_handler = tick_handle_periodic;
+	tick_set_periodic_handler(dev, broadcast);
+
+	/* Broadcast setup ? */
+	if (!tick_device_is_functional(dev))
+		return;
 
 	if (dev->features & CLOCK_EVT_FEAT_PERIODIC) {
 		clockevents_set_mode(dev, CLOCK_EVT_MODE_PERIODIC);
@@ -145,6 +151,15 @@ static void tick_setup_device(struct tick_device *td,
 	if (!cpus_equal(newdev->cpumask, cpumask))
 		irq_set_affinity(newdev->irq, cpumask);
 
+	/*
+	 * When global broadcasting is active, check if the current
+	 * device is registered as a placeholder for broadcast mode.
+	 * This allows us to handle this x86 misfeature in a generic
+	 * way.
+	 */
+	if (tick_device_uses_broadcast(newdev, cpu))
+		return;
+
 	if (td->mode == TICKDEV_MODE_PERIODIC)
 		tick_setup_periodic(newdev, 0);
 }
@@ -197,19 +212,33 @@ static int tick_check_new_device(struct clock_event_device *newdev)
 		 * Check the rating
 		 */
 		if (curdev->rating >= newdev->rating)
-			goto out;
+			goto out_bc;
 	}
 
 	/*
 	 * Replace the eventually existing device by the new
-	 * device.
+	 * device. If the current device is the broadcast device, do
+	 * not give it back to the clockevents layer !
 	 */
+	if (tick_is_broadcast_device(curdev)) {
+		clockevents_set_mode(curdev, CLOCK_EVT_MODE_SHUTDOWN);
+		curdev = NULL;
+	}
 	clockevents_exchange_device(curdev, newdev);
 	tick_setup_device(td, newdev, cpu, cpumask);
-	ret = NOTIFY_STOP;
 
+	spin_unlock_irqrestore(&tick_device_lock, flags);
+	return NOTIFY_STOP;
+
+out_bc:
+	/*
+	 * Can the new device be used as a broadcast device ?
+	 */
+	if (tick_check_broadcast_device(newdev))
+		ret = NOTIFY_STOP;
 out:
 	spin_unlock_irqrestore(&tick_device_lock, flags);
+
 	return ret;
 }
 
@@ -251,7 +280,13 @@ static int tick_notify(struct notifier_block *nb, unsigned long reason,
 	case CLOCK_EVT_NOTIFY_ADD:
 		return tick_check_new_device(dev);
 
+	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
+	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
+		tick_broadcast_on_off(reason, dev);
+		break;
+
 	case CLOCK_EVT_NOTIFY_CPU_DEAD:
+		tick_shutdown_broadcast(dev);
 		tick_shutdown(dev);
 		break;
 

commit 906568c9c668ff994f4078932ec6ae1e3950d1af
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 16 01:28:01 2007 -0800

    [PATCH] tick-management: core functionality
    
    With Ingo Molnar <mingo@elte.hu>
    
    The tick-management code is the first user of the clockevents layer.  It takes
    clock event devices from the clock events core and uses them to provide the
    periodic tick.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-common.c b/kernel/time/tick-common.c
new file mode 100644
index 000000000000..46e4381c26ea
--- /dev/null
+++ b/kernel/time/tick-common.c
@@ -0,0 +1,277 @@
+/*
+ * linux/kernel/time/tick-common.c
+ *
+ * This file contains the base functions to manage periodic tick
+ * related events.
+ *
+ * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>
+ * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar
+ * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner
+ *
+ * This code is licenced under the GPL version 2. For details see
+ * kernel-base/COPYING.
+ */
+#include <linux/cpu.h>
+#include <linux/err.h>
+#include <linux/hrtimer.h>
+#include <linux/irq.h>
+#include <linux/percpu.h>
+#include <linux/profile.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+
+/*
+ * Tick devices
+ */
+static DEFINE_PER_CPU(struct tick_device, tick_cpu_device);
+/*
+ * Tick next event: keeps track of the tick time
+ */
+static ktime_t tick_next_period;
+static ktime_t tick_period;
+static int tick_do_timer_cpu = -1;
+static DEFINE_SPINLOCK(tick_device_lock);
+
+/*
+ * Periodic tick
+ */
+static void tick_periodic(int cpu)
+{
+	if (tick_do_timer_cpu == cpu) {
+		write_seqlock(&xtime_lock);
+
+		/* Keep track of the next tick event */
+		tick_next_period = ktime_add(tick_next_period, tick_period);
+
+		do_timer(1);
+		write_sequnlock(&xtime_lock);
+	}
+
+	update_process_times(user_mode(get_irq_regs()));
+	profile_tick(CPU_PROFILING);
+}
+
+/*
+ * Event handler for periodic ticks
+ */
+void tick_handle_periodic(struct clock_event_device *dev)
+{
+	int cpu = smp_processor_id();
+
+	tick_periodic(cpu);
+
+	if (dev->mode != CLOCK_EVT_MODE_ONESHOT)
+		return;
+	/*
+	 * Setup the next period for devices, which do not have
+	 * periodic mode:
+	 */
+	for (;;) {
+		ktime_t next = ktime_add(dev->next_event, tick_period);
+
+		if (!clockevents_program_event(dev, next, ktime_get()))
+			return;
+		tick_periodic(cpu);
+	}
+}
+
+/*
+ * Setup the device for a periodic tick
+ */
+void tick_setup_periodic(struct clock_event_device *dev)
+{
+	dev->event_handler = tick_handle_periodic;
+
+	if (dev->features & CLOCK_EVT_FEAT_PERIODIC) {
+		clockevents_set_mode(dev, CLOCK_EVT_MODE_PERIODIC);
+	} else {
+		unsigned long seq;
+		ktime_t next;
+
+		do {
+			seq = read_seqbegin(&xtime_lock);
+			next = tick_next_period;
+		} while (read_seqretry(&xtime_lock, seq));
+
+		clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
+
+		for (;;) {
+			if (!clockevents_program_event(dev, next, ktime_get()))
+				return;
+			next = ktime_add(next, tick_period);
+		}
+	}
+}
+
+/*
+ * Setup the tick device
+ */
+static void tick_setup_device(struct tick_device *td,
+			      struct clock_event_device *newdev, int cpu,
+			      cpumask_t cpumask)
+{
+	ktime_t next_event;
+	void (*handler)(struct clock_event_device *) = NULL;
+
+	/*
+	 * First device setup ?
+	 */
+	if (!td->evtdev) {
+		/*
+		 * If no cpu took the do_timer update, assign it to
+		 * this cpu:
+		 */
+		if (tick_do_timer_cpu == -1) {
+			tick_do_timer_cpu = cpu;
+			tick_next_period = ktime_get();
+			tick_period = ktime_set(0, NSEC_PER_SEC / HZ);
+		}
+
+		/*
+		 * Startup in periodic mode first.
+		 */
+		td->mode = TICKDEV_MODE_PERIODIC;
+	} else {
+		handler = td->evtdev->event_handler;
+		next_event = td->evtdev->next_event;
+	}
+
+	td->evtdev = newdev;
+
+	/*
+	 * When the device is not per cpu, pin the interrupt to the
+	 * current cpu:
+	 */
+	if (!cpus_equal(newdev->cpumask, cpumask))
+		irq_set_affinity(newdev->irq, cpumask);
+
+	if (td->mode == TICKDEV_MODE_PERIODIC)
+		tick_setup_periodic(newdev, 0);
+}
+
+/*
+ * Check, if the new registered device should be used.
+ */
+static int tick_check_new_device(struct clock_event_device *newdev)
+{
+	struct clock_event_device *curdev;
+	struct tick_device *td;
+	int cpu, ret = NOTIFY_OK;
+	unsigned long flags;
+	cpumask_t cpumask;
+
+	spin_lock_irqsave(&tick_device_lock, flags);
+
+	cpu = smp_processor_id();
+	if (!cpu_isset(cpu, newdev->cpumask))
+		goto out;
+
+	td = &per_cpu(tick_cpu_device, cpu);
+	curdev = td->evtdev;
+	cpumask = cpumask_of_cpu(cpu);
+
+	/* cpu local device ? */
+	if (!cpus_equal(newdev->cpumask, cpumask)) {
+
+		/*
+		 * If the cpu affinity of the device interrupt can not
+		 * be set, ignore it.
+		 */
+		if (!irq_can_set_affinity(newdev->irq))
+			goto out_bc;
+
+		/*
+		 * If we have a cpu local device already, do not replace it
+		 * by a non cpu local device
+		 */
+		if (curdev && cpus_equal(curdev->cpumask, cpumask))
+			goto out_bc;
+	}
+
+	/*
+	 * If we have an active device, then check the rating and the oneshot
+	 * feature.
+	 */
+	if (curdev) {
+		/*
+		 * Check the rating
+		 */
+		if (curdev->rating >= newdev->rating)
+			goto out;
+	}
+
+	/*
+	 * Replace the eventually existing device by the new
+	 * device.
+	 */
+	clockevents_exchange_device(curdev, newdev);
+	tick_setup_device(td, newdev, cpu, cpumask);
+	ret = NOTIFY_STOP;
+
+out:
+	spin_unlock_irqrestore(&tick_device_lock, flags);
+	return ret;
+}
+
+/*
+ * Shutdown an event device on a given cpu:
+ *
+ * This is called on a life CPU, when a CPU is dead. So we cannot
+ * access the hardware device itself.
+ * We just set the mode and remove it from the lists.
+ */
+static void tick_shutdown(unsigned int *cpup)
+{
+	struct tick_device *td = &per_cpu(tick_cpu_device, *cpup);
+	struct clock_event_device *dev = td->evtdev;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tick_device_lock, flags);
+	td->mode = TICKDEV_MODE_PERIODIC;
+	if (dev) {
+		/*
+		 * Prevent that the clock events layer tries to call
+		 * the set mode function!
+		 */
+		dev->mode = CLOCK_EVT_MODE_UNUSED;
+		clockevents_exchange_device(dev, NULL);
+		td->evtdev = NULL;
+	}
+	spin_unlock_irqrestore(&tick_device_lock, flags);
+}
+
+/*
+ * Notification about clock event devices
+ */
+static int tick_notify(struct notifier_block *nb, unsigned long reason,
+			       void *dev)
+{
+	switch (reason) {
+
+	case CLOCK_EVT_NOTIFY_ADD:
+		return tick_check_new_device(dev);
+
+	case CLOCK_EVT_NOTIFY_CPU_DEAD:
+		tick_shutdown(dev);
+		break;
+
+	default:
+		break;
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block tick_notifier = {
+	.notifier_call = tick_notify,
+};
+
+/**
+ * tick_init - initialize the tick control
+ *
+ * Register the notifier with the clockevents framework
+ */
+void __init tick_init(void)
+{
+	clockevents_register_notifier(&tick_notifier);
+}
