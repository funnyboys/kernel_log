commit 13e792a19d4e3a1c64e94197ba357685fd584ded
Author: Laurent Gauthier <laurent.gauthier@soccasys.com>
Date:   Sat Jan 5 00:07:45 2019 +0100

    tick: Fix typos in comments
    
    Signed-off-by: Laurent Gauthier <laurent.gauthier@soccasys.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 7541cbca695e..e51778c312f1 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -809,13 +809,13 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 			 * either the CPU handling the broadcast
 			 * interrupt or we got woken by something else.
 			 *
-			 * We are not longer in the broadcast mask, so
+			 * We are no longer in the broadcast mask, so
 			 * if the cpu local expiry time is already
 			 * reached, we would reprogram the cpu local
 			 * timer with an already expired event.
 			 *
 			 * This can lead to a ping-pong when we return
-			 * to idle and therefor rearm the broadcast
+			 * to idle and therefore rearm the broadcast
 			 * timer before the cpu local timer was able
 			 * to fire. This happens because the forced
 			 * reprogramming makes sure that the event

commit aba0954327c831f593702e3a81ef3ad4bec7a838
Author: Borislav Petkov <bp@suse.de>
Date:   Fri Mar 29 11:28:52 2019 +0100

    tick/broadcast: Fix warning about undefined tick_broadcast_oneshot_offline()
    
    Randconfig builds with
    
      CONFIG_TICK_ONESHOT=y
      CONFIG_HOTPLUG_CPU=n
    
    trigger
    
      kernel/time/tick-broadcast.c:39:13: warning: ‘tick_broadcast_oneshot_offline’ \
              declared ‘static’ but never defined [-Wunused-function]
    
    due to that function's definition missing.
    
    Move the CONFIG_HOTPLUG_CPU ifdeffery around its declaration too.
    
    Fixes: 1b72d4323798 ("tick: Remove outgoing CPU from broadcast masks")
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Mukesh Ojha <mojha@codeaurora.org>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: x86@kernel.org
    Link: https://lkml.kernel.org/r/20190329110508.6621-1-bp@alien8.de

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 0283523de045..7541cbca695e 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -36,12 +36,16 @@ static __cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
 static void tick_broadcast_setup_oneshot(struct clock_event_device *bc);
 static void tick_broadcast_clear_oneshot(int cpu);
 static void tick_resume_broadcast_oneshot(struct clock_event_device *bc);
+# ifdef CONFIG_HOTPLUG_CPU
 static void tick_broadcast_oneshot_offline(unsigned int cpu);
+# endif
 #else
 static inline void tick_broadcast_setup_oneshot(struct clock_event_device *bc) { BUG(); }
 static inline void tick_broadcast_clear_oneshot(int cpu) { }
 static inline void tick_resume_broadcast_oneshot(struct clock_event_device *bc) { }
+# ifdef CONFIG_HOTPLUG_CPU
 static inline void tick_broadcast_oneshot_offline(unsigned int cpu) { }
+# endif
 #endif
 
 /*

commit 1b72d43237980eab9b6ae6bb8181e51c840377e6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 16:39:20 2019 +0100

    tick: Remove outgoing CPU from broadcast masks
    
    Valentin reported that unplugging a CPU occasionally results in a warning
    in the tick broadcast code which is triggered when an offline CPU is in the
    broadcast mask.
    
    This happens because the outgoing CPU is not removing itself from the
    broadcast masks, especially not from the broadcast_force_mask. The removal
    happens on the control CPU after the outgoing CPU is dead. It's a long
    standing issue, but the warning is harmless.
    
    Rework the hotplug mechanism so that the outgoing CPU removes itself from
    the broadcast masks after disabling interrupts and removing itself from the
    online mask.
    
    Reported-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.21.1903211540180.1784@nanos.tec.linutronix.de

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index ee834d4fb814..0283523de045 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -36,10 +36,12 @@ static __cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
 static void tick_broadcast_setup_oneshot(struct clock_event_device *bc);
 static void tick_broadcast_clear_oneshot(int cpu);
 static void tick_resume_broadcast_oneshot(struct clock_event_device *bc);
+static void tick_broadcast_oneshot_offline(unsigned int cpu);
 #else
 static inline void tick_broadcast_setup_oneshot(struct clock_event_device *bc) { BUG(); }
 static inline void tick_broadcast_clear_oneshot(int cpu) { }
 static inline void tick_resume_broadcast_oneshot(struct clock_event_device *bc) { }
+static inline void tick_broadcast_oneshot_offline(unsigned int cpu) { }
 #endif
 
 /*
@@ -433,27 +435,29 @@ void tick_set_periodic_handler(struct clock_event_device *dev, int broadcast)
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
-/*
- * Remove a CPU from broadcasting
- */
-void tick_shutdown_broadcast(unsigned int cpu)
+static void tick_shutdown_broadcast(void)
 {
-	struct clock_event_device *bc;
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
-
-	bc = tick_broadcast_device.evtdev;
-	cpumask_clear_cpu(cpu, tick_broadcast_mask);
-	cpumask_clear_cpu(cpu, tick_broadcast_on);
+	struct clock_event_device *bc = tick_broadcast_device.evtdev;
 
 	if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC) {
 		if (bc && cpumask_empty(tick_broadcast_mask))
 			clockevents_shutdown(bc);
 	}
+}
 
-	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+/*
+ * Remove a CPU from broadcasting
+ */
+void tick_broadcast_offline(unsigned int cpu)
+{
+	raw_spin_lock(&tick_broadcast_lock);
+	cpumask_clear_cpu(cpu, tick_broadcast_mask);
+	cpumask_clear_cpu(cpu, tick_broadcast_on);
+	tick_broadcast_oneshot_offline(cpu);
+	tick_shutdown_broadcast();
+	raw_spin_unlock(&tick_broadcast_lock);
 }
+
 #endif
 
 void tick_suspend_broadcast(void)
@@ -950,14 +954,10 @@ void hotplug_cpu__broadcast_tick_pull(int deadcpu)
 }
 
 /*
- * Remove a dead CPU from broadcasting
+ * Remove a dying CPU from broadcasting
  */
-void tick_shutdown_broadcast_oneshot(unsigned int cpu)
+static void tick_broadcast_oneshot_offline(unsigned int cpu)
 {
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
-
 	/*
 	 * Clear the broadcast masks for the dead cpu, but do not stop
 	 * the broadcast device!
@@ -965,8 +965,6 @@ void tick_shutdown_broadcast_oneshot(unsigned int cpu)
 	cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
 	cpumask_clear_cpu(cpu, tick_broadcast_pending_mask);
 	cpumask_clear_cpu(cpu, tick_broadcast_force_mask);
-
-	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 #endif
 

commit 75b710af7139768fd4ba2d4e05335d2344796279
Author: Gustavo A. R. Silva <gustavo@embeddedor.com>
Date:   Wed Jan 23 02:14:13 2019 -0600

    timers: Mark expected switch fall-throughs
    
    In preparation to enabling -Wimplicit-fallthrough, mark switch cases
    where fall through is indeed expected.
    
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Stephen Boyd <sboyd@kernel.org>
    Link: https://lkml.kernel.org/r/20190123081413.GA3949@embeddedor

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 803fa67aace9..ee834d4fb814 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -375,6 +375,7 @@ void tick_broadcast_control(enum tick_broadcast_mode mode)
 	switch (mode) {
 	case TICK_BROADCAST_FORCE:
 		tick_broadcast_forced = 1;
+		/* fall through */
 	case TICK_BROADCAST_ON:
 		cpumask_set_cpu(cpu, tick_broadcast_on);
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_mask)) {

commit f49c174b5f431db9fa17315269e288d4548b651c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 31 19:21:10 2018 +0100

    hrtimers/tick/clockevents: Remove sloppy license references
    
    "For licencing details see kernel-base/COPYING" and similar license
    references have no value over the SPDX identifier. Remove them.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Acked-by: Corey Minyard <cminyard@mvista.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Peter Anvin <hpa@zytor.com>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: David Riley <davidriley@chromium.org>
    Cc: Colin Cross <ccross@android.com>
    Cc: Mark Brown <broonie@kernel.org>
    Link: https://lkml.kernel.org/r/20181031182252.963632760@linutronix.de

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index f4725f53d852..803fa67aace9 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -6,9 +6,6 @@
  * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>
  * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar
  * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner
- *
- * This code is licenced under the GPL version 2. For details see
- * kernel-base/COPYING.
  */
 #include <linux/cpu.h>
 #include <linux/err.h>

commit 35728b8209ee7d25b6241a56304ee926469bd154
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 31 19:21:09 2018 +0100

    time: Add SPDX license identifiers
    
    Update the time(r) core files files with the correct SPDX license
    identifier based on the license text in the file itself. The SPDX
    identifier is a legally binding shorthand, which can be used instead of the
    full boiler plate text.
    
    This work is based on a script and data from Philippe Ombredanne, Kate
    Stewart and myself. The data has been created with two independent license
    scanners and manual inspection.
    
    The following files do not contain any direct license information and have
    been omitted from the big initial SPDX changes:
    
      timeconst.bc: The .bc files were not touched
      time.c, timer.c, timekeeping.c: Licence was deduced from EXPORT_SYMBOL_GPL
    
    As those files do not contain direct license references they fall under the
    project license, i.e. GPL V2 only.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Acked-by: Corey Minyard <cminyard@mvista.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: David Riley <davidriley@chromium.org>
    Cc: Colin Cross <ccross@android.com>
    Cc: Mark Brown <broonie@kernel.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: https://lkml.kernel.org/r/20181031182252.879109557@linutronix.de

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 4f5abde2dfa7..f4725f53d852 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains functions which emulate a local clock-event
  * device via a broadcast event source.

commit 58c5fc2b96e4ae65068d815a1c3ca81da92fa1c9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 31 19:21:08 2018 +0100

    time: Remove useless filenames in top level comments
    
    Remove the pointless filenames in the top level comments. They have no
    value at all and just occupy space. While at it tidy up some of the
    comments and remove a stale one.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Kees Cook <keescook@chromium.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Acked-by: Corey Minyard <cminyard@mvista.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Philippe Ombredanne <pombredanne@nexb.com>
    Cc: Peter Anvin <hpa@zytor.com>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: David Riley <davidriley@chromium.org>
    Cc: Colin Cross <ccross@android.com>
    Cc: Mark Brown <broonie@kernel.org>
    Link: https://lkml.kernel.org/r/20181031182252.794898238@linutronix.de

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index be0aac2b4300..4f5abde2dfa7 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -1,6 +1,4 @@
 /*
- * linux/kernel/time/tick-broadcast.c
- *
  * This file contains functions which emulate a local clock-event
  * device via a broadcast event source.
  *

commit dc6253108f0fbff4a634055d5b8a91958ec2af81
Author: Peng Hao <peng.hao2@zte.com.cn>
Date:   Fri Sep 28 12:03:28 2018 -0400

    tick/broadcast: Remove redundant check
    
    tick_device_is_functional() is called early in tick_broadcast_control(), so
    no need to call it again later.
    
    Signed-off-by: Peng Hao <peng.hao2@zte.com.cn>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: fweisbec@gmail.com
    Link: https://lkml.kernel.org/r/1538150608-2599-1-git-send-email-penghao122@sina.com.cn

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index aa2094d5dd27..be0aac2b4300 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -400,8 +400,6 @@ void tick_broadcast_control(enum tick_broadcast_mode mode)
 		if (tick_broadcast_forced)
 			break;
 		cpumask_clear_cpu(cpu, tick_broadcast_on);
-		if (!tick_device_is_functional(dev))
-			break;
 		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_mask)) {
 			if (tick_broadcast_device.mode ==
 			    TICKDEV_MODE_PERIODIC)

commit 5596fe34495cf0f645f417eb928ef224df3e3cb4
Author: Dexuan Cui <decui@microsoft.com>
Date:   Tue May 15 19:52:50 2018 +0000

    tick/broadcast: Use for_each_cpu() specially on UP kernels
    
    for_each_cpu() unintuitively reports CPU0 as set independent of the actual
    cpumask content on UP kernels. This causes an unexpected PIT interrupt
    storm on a UP kernel running in an SMP virtual machine on Hyper-V, and as
    a result, the virtual machine can suffer from a strange random delay of 1~20
    minutes during boot-up, and sometimes it can hang forever.
    
    Protect if by checking whether the cpumask is empty before entering the
    for_each_cpu() loop.
    
    [ tglx: Use !IS_ENABLED(CONFIG_SMP) instead of #ifdeffery ]
    
    Signed-off-by: Dexuan Cui <decui@microsoft.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Josh Poulson <jopoulso@microsoft.com>
    Cc: "Michael Kelley (EOSG)" <Michael.H.Kelley@microsoft.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: stable@vger.kernel.org
    Cc: Rakib Mullick <rakib.mullick@gmail.com>
    Cc: Jork Loeser <Jork.Loeser@microsoft.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: KY Srinivasan <kys@microsoft.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Link: https://lkml.kernel.org/r/KL1P15301MB000678289FE55BA365B3279ABF990@KL1P15301MB0006.APCP153.PROD.OUTLOOK.COM
    Link: https://lkml.kernel.org/r/KL1P15301MB0006FA63BC22BEB64902EAA0BF930@KL1P15301MB0006.APCP153.PROD.OUTLOOK.COM

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index b398c2ea69b2..aa2094d5dd27 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -612,6 +612,14 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	now = ktime_get();
 	/* Find all expired events */
 	for_each_cpu(cpu, tick_broadcast_oneshot_mask) {
+		/*
+		 * Required for !SMP because for_each_cpu() reports
+		 * unconditionally CPU0 as set on UP kernels.
+		 */
+		if (!IS_ENABLED(CONFIG_SMP) &&
+		    cpumask_empty(tick_broadcast_oneshot_mask))
+			break;
+
 		td = &per_cpu(tick_cpu_device, cpu);
 		if (td->evtdev->next_event <= now) {
 			cpumask_set_cpu(cpu, tmpmask);

commit 94114c367553f3301747e47f6947cabde947575f
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Wed Jun 7 23:36:03 2017 -0700

    tick/broadcast: Make tick_broadcast_setup_oneshot() static
    
    This function isn't used outside of tick-broadcast.c, so let's
    mark it static.
    
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Link: http://lkml.kernel.org/r/20170608063603.13276-1-sboyd@codeaurora.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 987e496bb51a..b398c2ea69b2 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -37,9 +37,11 @@ static int tick_broadcast_forced;
 static __cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
 
 #ifdef CONFIG_TICK_ONESHOT
+static void tick_broadcast_setup_oneshot(struct clock_event_device *bc);
 static void tick_broadcast_clear_oneshot(int cpu);
 static void tick_resume_broadcast_oneshot(struct clock_event_device *bc);
 #else
+static inline void tick_broadcast_setup_oneshot(struct clock_event_device *bc) { BUG(); }
 static inline void tick_broadcast_clear_oneshot(int cpu) { }
 static inline void tick_resume_broadcast_oneshot(struct clock_event_device *bc) { }
 #endif
@@ -867,7 +869,7 @@ static void tick_broadcast_init_next_event(struct cpumask *mask,
 /**
  * tick_broadcast_setup_oneshot - setup the broadcast device
  */
-void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
+static void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 {
 	int cpu = smp_processor_id();
 

commit 20dcfe1b7df4072a3c13bdb7506f7138125d0099
Merge: c9b9f207b904 336a9cde10d6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 10:06:32 2017 -0800

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "Nothing exciting, just the usual pile of fixes, updates and cleanups:
    
       - A bunch of clocksource driver updates
    
       - Removal of CONFIG_TIMER_STATS and the related /proc file
    
       - More posix timer slim down work
    
       - A scalability enhancement in the tick broadcast code
    
       - Math cleanups"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      hrtimer: Catch invalid clockids again
      math64, tile: Fix build failure
      clocksource/drivers/arm_arch_timer:: Mark cyclecounter __ro_after_init
      timerfd: Protect the might cancel mechanism proper
      timer_list: Remove useless cast when printing
      time: Remove CONFIG_TIMER_STATS
      clocksource/drivers/arm_arch_timer: Work around Hisilicon erratum 161010101
      clocksource/drivers/arm_arch_timer: Introduce generic errata handling infrastructure
      clocksource/drivers/arm_arch_timer: Remove fsl-a008585 parameter
      clocksource/drivers/arm_arch_timer: Add dt binding for hisilicon-161010101 erratum
      clocksource/drivers/ostm: Add renesas-ostm timer driver
      clocksource/drivers/ostm: Document renesas-ostm timer DT bindings
      clocksource/drivers/tcb_clksrc: Use 32 bit tcb as sched_clock
      clocksource/drivers/gemini: Add driver for the Cortina Gemini
      clocksource: add DT bindings for Cortina Gemini
      clockevents: Add a clkevt-of mechanism like clksrc-of
      tick/broadcast: Reduce lock cacheline contention
      timers: Omit POSIX timer stuff from task_struct when disabled
      x86/timer: Make delay() work during early bootup
      delay: Add explanation of udelay() inaccuracy
      ...

commit 202461e2f3c15dbfb05825d29ace0d20cdf55fa4
Author: Mike Galbraith <efault@gmx.de>
Date:   Mon Feb 13 03:31:55 2017 +0100

    tick/broadcast: Prevent deadlock on tick_broadcast_lock
    
    tick_broadcast_lock is taken from interrupt context, but the following call
    chain takes the lock without disabling interrupts:
    
    [   12.703736]  _raw_spin_lock+0x3b/0x50
    [   12.703738]  tick_broadcast_control+0x5a/0x1a0
    [   12.703742]  intel_idle_cpu_online+0x22/0x100
    [   12.703744]  cpuhp_invoke_callback+0x245/0x9d0
    [   12.703752]  cpuhp_thread_fun+0x52/0x110
    [   12.703754]  smpboot_thread_fn+0x276/0x320
    
    So the following deadlock can happen:
    
       lock(tick_broadcast_lock);
       <Interrupt>
          lock(tick_broadcast_lock);
    
    intel_idle_cpu_online() is the only place which violates the calling
    convention of tick_broadcast_control(). This was caused by the removal of
    the smp function call in course of the cpu hotplug rework.
    
    Instead of slapping local_irq_disable/enable() at the call site, we can
    relax the calling convention and handle it in the core code, which makes
    the whole machinery more robust.
    
    Fixes: 29d7bbada98e ("intel_idle: Remove superfluous SMP fuction call")
    Reported-by: Gabriel C <nix.or.die@gmail.com>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Cc: Ruslan Ruslichenko <rruslich@cisco.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: lwn@lwn.net
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: stable <stable@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1486953115.5912.4.camel@gmx.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 3109204c87cc..17ac99b60ee5 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -347,17 +347,16 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
  *
  * Called when the system enters a state where affected tick devices
  * might stop. Note: TICK_BROADCAST_FORCE cannot be undone.
- *
- * Called with interrupts disabled, so clockevents_lock is not
- * required here because the local clock event device cannot go away
- * under us.
  */
 void tick_broadcast_control(enum tick_broadcast_mode mode)
 {
 	struct clock_event_device *bc, *dev;
 	struct tick_device *td;
 	int cpu, bc_stopped;
+	unsigned long flags;
 
+	/* Protects also the local clockevent device. */
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 	td = this_cpu_ptr(&tick_cpu_device);
 	dev = td->evtdev;
 
@@ -365,12 +364,11 @@ void tick_broadcast_control(enum tick_broadcast_mode mode)
 	 * Is the device not affected by the powerstate ?
 	 */
 	if (!dev || !(dev->features & CLOCK_EVT_FEAT_C3STOP))
-		return;
+		goto out;
 
 	if (!tick_device_is_functional(dev))
-		return;
+		goto out;
 
-	raw_spin_lock(&tick_broadcast_lock);
 	cpu = smp_processor_id();
 	bc = tick_broadcast_device.evtdev;
 	bc_stopped = cpumask_empty(tick_broadcast_mask);
@@ -420,7 +418,8 @@ void tick_broadcast_control(enum tick_broadcast_mode mode)
 				tick_broadcast_setup_oneshot(bc);
 		}
 	}
-	raw_spin_unlock(&tick_broadcast_lock);
+out:
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 EXPORT_SYMBOL_GPL(tick_broadcast_control);
 

commit 668802c25729a8e3423015c33c05f1c3be3858e9
Author: Waiman Long <longman@redhat.com>
Date:   Mon Jan 30 12:57:43 2017 -0500

    tick/broadcast: Reduce lock cacheline contention
    
    It was observed that on an Intel x86 system without the ARAT (Always
    running APIC timer) feature and with fairly large number of CPUs as
    well as CPUs coming in and out of intel_idle frequently, the lock
    contention on the tick_broadcast_lock can become significant.
    
    To reduce contention, the lock is put into its own cacheline and all
    the cpumask_var_t variables are put into the __read_mostly section.
    
    Running the SP benchmark of the NAS Parallel Benchmarks on a 4-socket
    16-core 32-thread Nehalam system, the performance number improved
    from 3353.94 Mop/s to 3469.31 Mop/s when this patch was applied on
    a 4.9.6 kernel.  This is a 3.4% improvement.
    
    Signed-off-by: Waiman Long <longman@redhat.com>
    Cc: "Peter Zijlstra (Intel)" <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1485799063-20857-1-git-send-email-longman@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 3109204c87cc..244c93544150 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -29,12 +29,13 @@
  */
 
 static struct tick_device tick_broadcast_device;
-static cpumask_var_t tick_broadcast_mask;
-static cpumask_var_t tick_broadcast_on;
-static cpumask_var_t tmpmask;
-static DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
+static cpumask_var_t tick_broadcast_mask __cpumask_var_read_mostly;
+static cpumask_var_t tick_broadcast_on __cpumask_var_read_mostly;
+static cpumask_var_t tmpmask __cpumask_var_read_mostly;
 static int tick_broadcast_forced;
 
+static __cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
+
 #ifdef CONFIG_TICK_ONESHOT
 static void tick_broadcast_clear_oneshot(int cpu);
 static void tick_resume_broadcast_oneshot(struct clock_event_device *bc);
@@ -517,9 +518,9 @@ void tick_resume_broadcast(void)
 
 #ifdef CONFIG_TICK_ONESHOT
 
-static cpumask_var_t tick_broadcast_oneshot_mask;
-static cpumask_var_t tick_broadcast_pending_mask;
-static cpumask_var_t tick_broadcast_force_mask;
+static cpumask_var_t tick_broadcast_oneshot_mask __cpumask_var_read_mostly;
+static cpumask_var_t tick_broadcast_pending_mask __cpumask_var_read_mostly;
+static cpumask_var_t tick_broadcast_force_mask __cpumask_var_read_mostly;
 
 /*
  * Exposed for debugging: see timer_list.c

commit 2456e855354415bfaeb7badaa14e11b3e02c8466
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Dec 25 11:38:40 2016 +0100

    ktime: Get rid of the union
    
    ktime is a union because the initial implementation stored the time in
    scalar nanoseconds on 64 bit machine and in a endianess optimized timespec
    variant for 32bit machines. The Y2038 cleanup removed the timespec variant
    and switched everything to scalar nanoseconds. The union remained, but
    become completely pointless.
    
    Get rid of the union and just keep ktime_t as simple typedef of type s64.
    
    The conversion was done with coccinelle and some manual mopping up.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index d2a20e83ebae..3109204c87cc 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -604,14 +604,14 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	bool bc_local;
 
 	raw_spin_lock(&tick_broadcast_lock);
-	dev->next_event.tv64 = KTIME_MAX;
-	next_event.tv64 = KTIME_MAX;
+	dev->next_event = KTIME_MAX;
+	next_event = KTIME_MAX;
 	cpumask_clear(tmpmask);
 	now = ktime_get();
 	/* Find all expired events */
 	for_each_cpu(cpu, tick_broadcast_oneshot_mask) {
 		td = &per_cpu(tick_cpu_device, cpu);
-		if (td->evtdev->next_event.tv64 <= now.tv64) {
+		if (td->evtdev->next_event <= now) {
 			cpumask_set_cpu(cpu, tmpmask);
 			/*
 			 * Mark the remote cpu in the pending mask, so
@@ -619,8 +619,8 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 			 * timer in tick_broadcast_oneshot_control().
 			 */
 			cpumask_set_cpu(cpu, tick_broadcast_pending_mask);
-		} else if (td->evtdev->next_event.tv64 < next_event.tv64) {
-			next_event.tv64 = td->evtdev->next_event.tv64;
+		} else if (td->evtdev->next_event < next_event) {
+			next_event = td->evtdev->next_event;
 			next_cpu = cpu;
 		}
 	}
@@ -657,7 +657,7 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	 * - There are pending events on sleeping CPUs which were not
 	 * in the event mask
 	 */
-	if (next_event.tv64 != KTIME_MAX)
+	if (next_event != KTIME_MAX)
 		tick_broadcast_set_event(dev, next_cpu, next_event);
 
 	raw_spin_unlock(&tick_broadcast_lock);
@@ -672,7 +672,7 @@ static int broadcast_needs_cpu(struct clock_event_device *bc, int cpu)
 {
 	if (!(bc->features & CLOCK_EVT_FEAT_HRTIMER))
 		return 0;
-	if (bc->next_event.tv64 == KTIME_MAX)
+	if (bc->next_event == KTIME_MAX)
 		return 0;
 	return bc->bound_on == cpu ? -EBUSY : 0;
 }
@@ -688,7 +688,7 @@ static void broadcast_shutdown_local(struct clock_event_device *bc,
 	if (bc->features & CLOCK_EVT_FEAT_HRTIMER) {
 		if (broadcast_needs_cpu(bc, smp_processor_id()))
 			return;
-		if (dev->next_event.tv64 < bc->next_event.tv64)
+		if (dev->next_event < bc->next_event)
 			return;
 	}
 	clockevents_switch_state(dev, CLOCK_EVT_STATE_SHUTDOWN);
@@ -754,7 +754,7 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 			 */
 			if (cpumask_test_cpu(cpu, tick_broadcast_force_mask)) {
 				ret = -EBUSY;
-			} else if (dev->next_event.tv64 < bc->next_event.tv64) {
+			} else if (dev->next_event < bc->next_event) {
 				tick_broadcast_set_event(bc, cpu, dev->next_event);
 				/*
 				 * In case of hrtimer broadcasts the
@@ -789,7 +789,7 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 			/*
 			 * Bail out if there is no next event.
 			 */
-			if (dev->next_event.tv64 == KTIME_MAX)
+			if (dev->next_event == KTIME_MAX)
 				goto out;
 			/*
 			 * If the pending bit is not set, then we are
@@ -824,7 +824,7 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 			 * nohz fixups.
 			 */
 			now = ktime_get();
-			if (dev->next_event.tv64 <= now.tv64) {
+			if (dev->next_event <= now) {
 				cpumask_set_cpu(cpu, tick_broadcast_force_mask);
 				goto out;
 			}
@@ -897,7 +897,7 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 						       tick_next_period);
 			tick_broadcast_set_event(bc, cpu, tick_next_period);
 		} else
-			bc->next_event.tv64 = KTIME_MAX;
+			bc->next_event = KTIME_MAX;
 	} else {
 		/*
 		 * The first cpu which switches to oneshot mode sets

commit c1a9eeb938b5433947e5ea22f89baff3182e7075
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 15 12:10:37 2016 +0100

    tick/broadcast: Prevent NULL pointer dereference
    
    When a disfunctional timer, e.g. dummy timer, is installed, the tick core
    tries to setup the broadcast timer.
    
    If no broadcast device is installed, the kernel crashes with a NULL pointer
    dereference in tick_broadcast_setup_oneshot() because the function has no
    sanity check.
    
    Reported-by: Mason <slash.tmp@free.fr>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: Richard Cochran <rcochran@linutronix.de>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>,
    Cc: Sebastian Frias <sf84@laposte.net>
    Cc: Thibaud Cornic <thibaud_cornic@sigmadesigns.com>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Link: http://lkml.kernel.org/r/1147ef90-7877-e4d2-bb2b-5c4fa8d3144b@free.fr

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index f6aae7977824..d2a20e83ebae 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -871,6 +871,9 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 {
 	int cpu = smp_processor_id();
 
+	if (!bc)
+		return;
+
 	/* Set it up only once ! */
 	if (bc->event_handler != tick_handle_oneshot_broadcast) {
 		int was_periodic = clockevent_state_periodic(bc);

commit 0f44705175347ec96935d60b765b5d14ecc763bb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 14 12:01:04 2015 +0200

    tick: Move the export of tick_broadcast_oneshot_control to the proper place
    
    tick_broadcast_oneshot_control got moved from tick-broadcast to
    tick-common, but the export stayed in the old place. Fix it up.
    
    Fixes: f32dd1170511 'tick/broadcast: Make idle check independent from mode and config'
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 52b9e199b5ac..f6aae7977824 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -839,7 +839,6 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 	raw_spin_unlock(&tick_broadcast_lock);
 	return ret;
 }
-EXPORT_SYMBOL_GPL(tick_broadcast_oneshot_control);
 
 /*
  * Reset the one shot broadcast for a cpu

commit c4d029f2d43b39de7b9299e8b58102a442ba86f8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 11 14:26:34 2015 +0200

    tick/broadcast: Prevent NULL pointer dereference
    
    Dan reported that the recent changes to the broadcast code introduced
    a potential NULL dereference.
    
    Add the proper check.
    
    Fixes: e0454311903d "tick/broadcast: Sanity check the shutdown of the local clock_event"
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index ee3cf942d6eb..52b9e199b5ac 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -409,14 +409,16 @@ void tick_broadcast_control(enum tick_broadcast_mode mode)
 		break;
 	}
 
-	if (cpumask_empty(tick_broadcast_mask)) {
-		if (!bc_stopped)
-			clockevents_shutdown(bc);
-	} else if (bc_stopped) {
-		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
-			tick_broadcast_start_periodic(bc);
-		else
-			tick_broadcast_setup_oneshot(bc);
+	if (bc) {
+		if (cpumask_empty(tick_broadcast_mask)) {
+			if (!bc_stopped)
+				clockevents_shutdown(bc);
+		} else if (bc_stopped) {
+			if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
+				tick_broadcast_start_periodic(bc);
+			else
+				tick_broadcast_setup_oneshot(bc);
+		}
 	}
 	raw_spin_unlock(&tick_broadcast_lock);
 }

commit c4288334818c81c946acb23d2319881f58c3d497
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Jul 5 20:53:17 2015 +0000

    tick/broadcast: Handle spurious interrupts gracefully
    
    Andriy reported that on a virtual machine the warning about negative
    expiry time in the clock events programming code triggered:
    
    hpet: hpet0 irq 40 for MSI
    hpet: hpet1 irq 41 for MSI
    Switching to clocksource hpet
    WARNING: at kernel/time/clockevents.c:239
    
    [<ffffffff810ce6eb>] clockevents_program_event+0xdb/0xf0
    [<ffffffff810cf211>] tick_handle_periodic_broadcast+0x41/0x50
    [<ffffffff81016525>] timer_interrupt+0x15/0x20
    
    When the second hpet is installed as a per cpu timer the broadcast
    event is not longer required and stopped, which sets the next_evt of
    the broadcast device to KTIME_MAX.
    
    If after that a spurious interrupt happens on the broadcast device,
    then the current code blindly handles it and tries to reprogram the
    broadcast device afterwards, which adds the period to
    next_evt. KTIME_MAX + period results in a negative expiry value
    causing the WARN_ON in the clockevents code to trigger.
    
    Add a proper check for the state of the broadcast device into the
    interrupt handler and return if the interrupt is spurious.
    
    [ Folded in pointer fix from Sudeep ]
    
    Reported-by: Andriy Gapon <avg@FreeBSD.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Sudeep Holla <sudeep.holla@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/20150705205221.802094647@linutronix.de

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index c8d731ac9563..ee3cf942d6eb 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -316,6 +316,13 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 	bool bc_local;
 
 	raw_spin_lock(&tick_broadcast_lock);
+
+	/* Handle spurious interrupts gracefully */
+	if (clockevent_state_shutdown(tick_broadcast_device.evtdev)) {
+		raw_spin_unlock(&tick_broadcast_lock);
+		return;
+	}
+
 	bc_local = tick_do_periodic_broadcast();
 
 	if (clockevent_state_oneshot(dev)) {

commit d5113e13a550bc9c2b53cc9944b8a06453c4a0a1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 16:43:04 2015 +0200

    tick/broadcast: Check for hrtimer broadcast active early
    
    If the current cpu is the one which has the hrtimer based broadcast
    queued then we better return busy immediately instead of going through
    loops and hoops to figure that out.
    
    [ Split out from a larger combo patch ]
    
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <Catalin.Marinas@arm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1507070929360.3916@nanos

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 70b47bc928b9..c8d731ac9563 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -705,6 +705,17 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 	cpu = smp_processor_id();
 
 	if (state == TICK_BROADCAST_ENTER) {
+		/*
+		 * If the current CPU owns the hrtimer broadcast
+		 * mechanism, it cannot go deep idle and we do not add
+		 * the CPU to the broadcast mask. We don't have to go
+		 * through the EXIT path as the local timer is not
+		 * shutdown.
+		 */
+		ret = broadcast_needs_cpu(bc, cpu);
+		if (ret)
+			goto out;
+
 		/*
 		 * If the broadcast device is in periodic mode, we
 		 * return.
@@ -718,7 +729,10 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			WARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));
+
+			/* Conditionally shut down the local timer. */
 			broadcast_shutdown_local(bc, dev);
+
 			/*
 			 * We only reprogram the broadcast timer if we
 			 * did not mark ourself in the force mask and
@@ -733,18 +747,20 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 				ret = -EBUSY;
 			} else if (dev->next_event.tv64 < bc->next_event.tv64) {
 				tick_broadcast_set_event(bc, cpu, dev->next_event);
+				/*
+				 * In case of hrtimer broadcasts the
+				 * programming might have moved the
+				 * timer to this cpu. If yes, remove
+				 * us from the broadcast mask and
+				 * return busy.
+				 */
+				ret = broadcast_needs_cpu(bc, cpu);
+				if (ret) {
+					cpumask_clear_cpu(cpu,
+						tick_broadcast_oneshot_mask);
+				}
 			}
 		}
-		/*
-		 * If the current CPU owns the hrtimer broadcast
-		 * mechanism, it cannot go deep idle and we remove the
-		 * CPU from the broadcast mask. We don't have to go
-		 * through the EXIT path as the local timer is not
-		 * shutdown.
-		 */
-		ret = broadcast_needs_cpu(bc, cpu);
-		if (ret)
-			cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
 	} else {
 		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);

commit 0cc5281aa592d0020868f6ccaed359b4ad7b2684
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 16:45:15 2015 +0200

    tick/broadcast: Return busy when IPI is pending
    
    Tell the idle code not to go deep if the broadcast IPI is about to
    arrive.
    
    [ Split out from a larger combo patch ]
    
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <Catalin.Marinas@arm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1507070929360.3916@nanos

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index da7b40fdf6d0..70b47bc928b9 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -725,11 +725,15 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 			 * if the cpu local event is earlier than the
 			 * broadcast event. If the current CPU is in
 			 * the force mask, then we are going to be
-			 * woken by the IPI right away.
+			 * woken by the IPI right away; we return
+			 * busy, so the CPU does not try to go deep
+			 * idle.
 			 */
-			if (!cpumask_test_cpu(cpu, tick_broadcast_force_mask) &&
-			    dev->next_event.tv64 < bc->next_event.tv64)
+			if (cpumask_test_cpu(cpu, tick_broadcast_force_mask)) {
+				ret = -EBUSY;
+			} else if (dev->next_event.tv64 < bc->next_event.tv64) {
 				tick_broadcast_set_event(bc, cpu, dev->next_event);
+			}
 		}
 		/*
 		 * If the current CPU owns the hrtimer broadcast

commit d33257264b0267a8fd20f6717abbb484c9e21130
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 17:45:22 2015 +0200

    tick/broadcast: Return busy if periodic mode and hrtimer broadcast
    
    If the system is in periodic mode and the broadcast device is hrtimer
    based, return busy as we have no proper handling for this.
    
    [ Split out from a larger combo patch ]
    
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <Catalin.Marinas@arm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1507070929360.3916@nanos

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 83aa92e87a85..da7b40fdf6d0 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -709,8 +709,12 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 		 * If the broadcast device is in periodic mode, we
 		 * return.
 		 */
-		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
+		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC) {
+			/* If it is a hrtimer based broadcast, return busy */
+			if (bc->features & CLOCK_EVT_FEAT_HRTIMER)
+				ret = -EBUSY;
 			goto out;
+		}
 
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			WARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));

commit e3ac79e087ffe8a1f953ed44a74acf7616cb0b25
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 16:38:11 2015 +0200

    tick/broadcast: Move the check for periodic mode inside state handling
    
    We need to check more than the periodic mode for proper operation in
    all runtime combinations. To avoid code duplication move the check
    into the enter state handling.
    
    No functional change.
    
    [ Split out from a larger combo patch ]
    
    Reported-and-tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <Catalin.Marinas@arm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1507070929360.3916@nanos

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index fad3f789beec..83aa92e87a85 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -688,7 +688,6 @@ static void broadcast_shutdown_local(struct clock_event_device *bc,
 int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 {
 	struct clock_event_device *bc, *dev;
-	struct tick_device *td;
 	int cpu, ret = 0;
 	ktime_t now;
 
@@ -699,25 +698,20 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 	if (!tick_broadcast_device.evtdev)
 		return -EBUSY;
 
-	/*
-	 * Periodic mode does not care about the enter/exit of power
-	 * states
-	 */
-	if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
-		return 0;
-
-	/*
-	 * We are called with preemtion disabled from the depth of the
-	 * idle code, so we can't be moved away.
-	 */
-	td = this_cpu_ptr(&tick_cpu_device);
-	dev = td->evtdev;
+	dev = this_cpu_ptr(&tick_cpu_device)->evtdev;
 
 	raw_spin_lock(&tick_broadcast_lock);
 	bc = tick_broadcast_device.evtdev;
 	cpu = smp_processor_id();
 
 	if (state == TICK_BROADCAST_ENTER) {
+		/*
+		 * If the broadcast device is in periodic mode, we
+		 * return.
+		 */
+		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
+			goto out;
+
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			WARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));
 			broadcast_shutdown_local(bc, dev);

commit b78f3f3c898c824bf56ab55cfa59fc72be49c349
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 16:34:32 2015 +0200

    tick/broadcast: Prevent deep idle if no broadcast device available
    
    Add a check for a installed broadcast device to the oneshot control
    function and return busy if not.
    
    [ Split out from a larger combo patch ]
    
    Reported-and-tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <Catalin.Marinas@arm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1507070929360.3916@nanos

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index ef77b16ad5df..fad3f789beec 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -692,6 +692,13 @@ int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 	int cpu, ret = 0;
 	ktime_t now;
 
+	/*
+	 * If there is no broadcast device, tell the caller not to go
+	 * into deep idle.
+	 */
+	if (!tick_broadcast_device.evtdev)
+		return -EBUSY;
+
 	/*
 	 * Periodic mode does not care about the enter/exit of power
 	 * states

commit f32dd117051185da6e923b35491a44d7debeeea5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 16:29:38 2015 +0200

    tick/broadcast: Make idle check independent from mode and config
    
    Currently the broadcast busy check, which prevents the idle code from
    going into deep idle, works only in one shot mode.
    
    If NOHZ and HIGHRES are off (config or command line) there is no
    sanity check at all, so under certain conditions cpus are allowed to
    go into deep idle, where the local timer stops, and are not woken up
    again because there is no broadcast timer installed or a hrtimer based
    broadcast device is not evaluated.
    
    Move tick_broadcast_oneshot_control() into the common code and provide
    proper subfunctions for the various config combinations.
    
    The common check in tick_broadcast_oneshot_control() is for the C3STOP
    misfeature flag of the local clock event device. If its not set, idle
    can proceed. If set, further checks are necessary.
    
    Provide checks for the trivial cases:
    
     - If broadcast is disabled in the config, then return busy
    
     - If oneshot mode (NOHZ/HIGHES) is disabled in the config, return
       busy if the broadcast device is hrtimer based.
    
     - If oneshot mode is enabled in the config call the original
       tick_broadcast_oneshot_control() function. That function needs
       extra checks which will be implemented in seperate patches.
    
    [ Split out from a larger combo patch ]
    
    Reported-and-tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <Catalin.Marinas@arm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1507070929360.3916@nanos

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 9877d0b0aefc..ef77b16ad5df 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -685,18 +685,7 @@ static void broadcast_shutdown_local(struct clock_event_device *bc,
 	clockevents_switch_state(dev, CLOCK_EVT_STATE_SHUTDOWN);
 }
 
-/**
- * tick_broadcast_oneshot_control - Enter/exit broadcast oneshot mode
- * @state:	The target state (enter/exit)
- *
- * The system enters/leaves a state, where affected devices might stop
- * Returns 0 on success, -EBUSY if the cpu is used to broadcast wakeups.
- *
- * Called with interrupts disabled, so clockevents_lock is not
- * required here because the local clock event device cannot go away
- * under us.
- */
-int tick_broadcast_oneshot_control(enum tick_broadcast_state state)
+int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 {
 	struct clock_event_device *bc, *dev;
 	struct tick_device *td;
@@ -717,9 +706,6 @@ int tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 	td = this_cpu_ptr(&tick_cpu_device);
 	dev = td->evtdev;
 
-	if (!(dev->features & CLOCK_EVT_FEAT_C3STOP))
-		return 0;
-
 	raw_spin_lock(&tick_broadcast_lock);
 	bc = tick_broadcast_device.evtdev;
 	cpu = smp_processor_id();
@@ -961,6 +947,16 @@ bool tick_broadcast_oneshot_available(void)
 	return bc ? bc->features & CLOCK_EVT_FEAT_ONESHOT : false;
 }
 
+#else
+int __tick_broadcast_oneshot_control(enum tick_broadcast_state state)
+{
+	struct clock_event_device *bc = tick_broadcast_device.evtdev;
+
+	if (!bc || (bc->features & CLOCK_EVT_FEAT_HRTIMER))
+		return -EBUSY;
+
+	return 0;
+}
 #endif
 
 void __init tick_broadcast_init(void)

commit e0454311903d3fd0f12a86c9e65d7b271c2bb05d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 14:07:27 2015 +0200

    tick/broadcast: Sanity check the shutdown of the local clock_event
    
    The broadcast code shuts down the local clock event unconditionally
    even if no broadcast device is installed or if the broadcast device is
    hrtimer based.
    
    Add proper sanity checks.
    
    [ Split out from a larger combo patch ]
    
    Reported-and-tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <Catalin.Marinas@arm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1507070929360.3916@nanos

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index a76204089f78..9877d0b0aefc 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -159,7 +159,7 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 {
 	struct clock_event_device *bc = tick_broadcast_device.evtdev;
 	unsigned long flags;
-	int ret;
+	int ret = 0;
 
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
@@ -221,13 +221,14 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 			 * If we kept the cpu in the broadcast mask,
 			 * tell the caller to leave the per cpu device
 			 * in shutdown state. The periodic interrupt
-			 * is delivered by the broadcast device.
+			 * is delivered by the broadcast device, if
+			 * the broadcast device exists and is not
+			 * hrtimer based.
 			 */
-			ret = cpumask_test_cpu(cpu, tick_broadcast_mask);
+			if (bc && !(bc->features & CLOCK_EVT_FEAT_HRTIMER))
+				ret = cpumask_test_cpu(cpu, tick_broadcast_mask);
 			break;
 		default:
-			/* Nothing to do */
-			ret = 0;
 			break;
 		}
 	}
@@ -373,8 +374,16 @@ void tick_broadcast_control(enum tick_broadcast_mode mode)
 	case TICK_BROADCAST_ON:
 		cpumask_set_cpu(cpu, tick_broadcast_on);
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_mask)) {
-			if (tick_broadcast_device.mode ==
-			    TICKDEV_MODE_PERIODIC)
+			/*
+			 * Only shutdown the cpu local device, if:
+			 *
+			 * - the broadcast device exists
+			 * - the broadcast device is not a hrtimer based one
+			 * - the broadcast device is in periodic mode to
+			 *   avoid a hickup during switch to oneshot mode
+			 */
+			if (bc && !(bc->features & CLOCK_EVT_FEAT_HRTIMER) &&
+			    tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
 				clockevents_shutdown(dev);
 		}
 		break;

commit 8eb231261fdd20768db23863d00ef277de4b0543
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 7 14:11:00 2015 +0200

    tick/broadcast: Prevent hrtimer recursion
    
    The hrtimer based broadcast vehicle can cause a hrtimer recursion
    which went unnoticed until we changed the hrtimer expiry code to keep
    track of the currently running timer.
    
    local_timer_interrupt()
      local_handler()
        hrtimer_interrupt()
          expire_hrtimers()
            broadcast_hrtimer()
              send_ipis()
              local_handler()
                hrtimer_interrupt()
                 ....
    
    Solution is simple: Prevent the local handler call from the broadcast
    code when the broadcast 'device' is hrtimer based.
    
    [ Split out from a larger combo patch ]
    
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Cc: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
    Cc: Catalin Marinas <Catalin.Marinas@arm.com>
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1507070929360.3916@nanos

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index d39f32cdd1b5..a76204089f78 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -265,8 +265,22 @@ static bool tick_do_broadcast(struct cpumask *mask)
 	 * Check, if the current cpu is in the mask
 	 */
 	if (cpumask_test_cpu(cpu, mask)) {
+		struct clock_event_device *bc = tick_broadcast_device.evtdev;
+
 		cpumask_clear_cpu(cpu, mask);
-		local = true;
+		/*
+		 * We only run the local handler, if the broadcast
+		 * device is not hrtimer based. Otherwise we run into
+		 * a hrtimer recursion.
+		 *
+		 * local timer_interrupt()
+		 *   local_handler()
+		 *     expire_hrtimers()
+		 *       bc_handler()
+		 *         local_handler()
+		 *	     expire_hrtimers()
+		 */
+		local = !(bc->features & CLOCK_EVT_FEAT_HRTIMER);
 	}
 
 	if (!cpumask_empty(mask)) {

commit d7eb231c71420bc34ac3d35403115600f920cfc2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 2 14:08:46 2015 +0200

    clockevents: Provide functions to set and get the state
    
    We want to rename dev->state, so provide proper get and set
    functions. Rename clockevents_set_state() to
    clockevents_switch_state() to avoid confusion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 132f819fdcdf..d39f32cdd1b5 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -529,7 +529,7 @@ static void tick_broadcast_set_event(struct clock_event_device *bc, int cpu,
 				     ktime_t expires)
 {
 	if (!clockevent_state_oneshot(bc))
-		clockevents_set_state(bc, CLOCK_EVT_STATE_ONESHOT);
+		clockevents_switch_state(bc, CLOCK_EVT_STATE_ONESHOT);
 
 	clockevents_program_event(bc, expires, 1);
 	tick_broadcast_set_affinity(bc, cpumask_of(cpu));
@@ -537,7 +537,7 @@ static void tick_broadcast_set_event(struct clock_event_device *bc, int cpu,
 
 static void tick_resume_broadcast_oneshot(struct clock_event_device *bc)
 {
-	clockevents_set_state(bc, CLOCK_EVT_STATE_ONESHOT);
+	clockevents_switch_state(bc, CLOCK_EVT_STATE_ONESHOT);
 }
 
 /*
@@ -555,7 +555,7 @@ void tick_check_oneshot_broadcast_this_cpu(void)
 		 * switched over, leave the device alone.
 		 */
 		if (td->mode == TICKDEV_MODE_ONESHOT) {
-			clockevents_set_state(td->evtdev,
+			clockevents_switch_state(td->evtdev,
 					      CLOCK_EVT_STATE_ONESHOT);
 		}
 	}
@@ -659,7 +659,7 @@ static void broadcast_shutdown_local(struct clock_event_device *bc,
 		if (dev->next_event.tv64 < bc->next_event.tv64)
 			return;
 	}
-	clockevents_set_state(dev, CLOCK_EVT_STATE_SHUTDOWN);
+	clockevents_switch_state(dev, CLOCK_EVT_STATE_SHUTDOWN);
 }
 
 /**
@@ -729,7 +729,7 @@ int tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 			cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
 	} else {
 		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {
-			clockevents_set_state(dev, CLOCK_EVT_STATE_ONESHOT);
+			clockevents_switch_state(dev, CLOCK_EVT_STATE_ONESHOT);
 			/*
 			 * The cpu which was handling the broadcast
 			 * timer marked this cpu in the broadcast
@@ -847,7 +847,7 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 			   tick_broadcast_oneshot_mask, tmpmask);
 
 		if (was_periodic && !cpumask_empty(tmpmask)) {
-			clockevents_set_state(bc, CLOCK_EVT_STATE_ONESHOT);
+			clockevents_switch_state(bc, CLOCK_EVT_STATE_ONESHOT);
 			tick_broadcast_init_next_event(tmpmask,
 						       tick_next_period);
 			tick_broadcast_set_event(bc, cpu, tick_next_period);

commit 472c4a9437d3c6a0b1e59df7c5aa14075946aa70
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Thu May 21 13:33:46 2015 +0530

    clockevents: Use helpers to check the state of a clockevent device
    
    Use accessor functions to check the state of clockevent devices in
    core code.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: linaro-kernel@lists.linaro.org
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/fa2b9869fd17f210eaa156ec2b594efd0230b6c7.1432192527.git.viresh.kumar@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 12fcc55d607a..132f819fdcdf 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -303,7 +303,7 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 	raw_spin_lock(&tick_broadcast_lock);
 	bc_local = tick_do_periodic_broadcast();
 
-	if (dev->state == CLOCK_EVT_STATE_ONESHOT) {
+	if (clockevent_state_oneshot(dev)) {
 		ktime_t next = ktime_add(dev->next_event, tick_period);
 
 		clockevents_program_event(dev, next, true);
@@ -528,7 +528,7 @@ static void tick_broadcast_set_affinity(struct clock_event_device *bc,
 static void tick_broadcast_set_event(struct clock_event_device *bc, int cpu,
 				     ktime_t expires)
 {
-	if (bc->state != CLOCK_EVT_STATE_ONESHOT)
+	if (!clockevent_state_oneshot(bc))
 		clockevents_set_state(bc, CLOCK_EVT_STATE_ONESHOT);
 
 	clockevents_program_event(bc, expires, 1);
@@ -831,7 +831,7 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 
 	/* Set it up only once ! */
 	if (bc->event_handler != tick_handle_oneshot_broadcast) {
-		int was_periodic = bc->state == CLOCK_EVT_STATE_PERIODIC;
+		int was_periodic = clockevent_state_periodic(bc);
 
 		bc->event_handler = tick_handle_oneshot_broadcast;
 

commit 298dbd1c5cd66f0ac85981b83b7d519a5d88d1b8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 5 09:44:24 2015 +0200

    tick: broadcast: Simplify oneshot logic and shorten lock region
    
    Simplify the oneshot logic by avoiding the reprogramming loops. That
    also allows to call the cpu local handler outside of the
    broadcast_lock held region.
    
    Tested-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 5d9e4aab9797..12fcc55d607a 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -525,18 +525,14 @@ static void tick_broadcast_set_affinity(struct clock_event_device *bc,
 	irq_set_affinity(bc->irq, bc->cpumask);
 }
 
-static int tick_broadcast_set_event(struct clock_event_device *bc, int cpu,
-				    ktime_t expires, int force)
+static void tick_broadcast_set_event(struct clock_event_device *bc, int cpu,
+				     ktime_t expires)
 {
-	int ret;
-
 	if (bc->state != CLOCK_EVT_STATE_ONESHOT)
 		clockevents_set_state(bc, CLOCK_EVT_STATE_ONESHOT);
 
-	ret = clockevents_program_event(bc, expires, force);
-	if (!ret)
-		tick_broadcast_set_affinity(bc, cpumask_of(cpu));
-	return ret;
+	clockevents_program_event(bc, expires, 1);
+	tick_broadcast_set_affinity(bc, cpumask_of(cpu));
 }
 
 static void tick_resume_broadcast_oneshot(struct clock_event_device *bc)
@@ -573,9 +569,9 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	struct tick_device *td;
 	ktime_t now, next_event;
 	int cpu, next_cpu = 0;
+	bool bc_local;
 
 	raw_spin_lock(&tick_broadcast_lock);
-again:
 	dev->next_event.tv64 = KTIME_MAX;
 	next_event.tv64 = KTIME_MAX;
 	cpumask_clear(tmpmask);
@@ -615,13 +611,9 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 		cpumask_and(tmpmask, tmpmask, cpu_online_mask);
 
 	/*
-	 * Wakeup the cpus which have an expired event and handle the
-	 * broadcast event of the local cpu.
+	 * Wakeup the cpus which have an expired event.
 	 */
-	if (tick_do_broadcast(tmpmask)) {
-		td = this_cpu_ptr(&tick_cpu_device);
-		td->evtdev->event_handler(td->evtdev);
-	}
+	bc_local = tick_do_broadcast(tmpmask);
 
 	/*
 	 * Two reasons for reprogram:
@@ -633,15 +625,15 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	 * - There are pending events on sleeping CPUs which were not
 	 * in the event mask
 	 */
-	if (next_event.tv64 != KTIME_MAX) {
-		/*
-		 * Rearm the broadcast device. If event expired,
-		 * repeat the above
-		 */
-		if (tick_broadcast_set_event(dev, next_cpu, next_event, 0))
-			goto again;
-	}
+	if (next_event.tv64 != KTIME_MAX)
+		tick_broadcast_set_event(dev, next_cpu, next_event);
+
 	raw_spin_unlock(&tick_broadcast_lock);
+
+	if (bc_local) {
+		td = this_cpu_ptr(&tick_cpu_device);
+		td->evtdev->event_handler(td->evtdev);
+	}
 }
 
 static int broadcast_needs_cpu(struct clock_event_device *bc, int cpu)
@@ -723,7 +715,7 @@ int tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 			 */
 			if (!cpumask_test_cpu(cpu, tick_broadcast_force_mask) &&
 			    dev->next_event.tv64 < bc->next_event.tv64)
-				tick_broadcast_set_event(bc, cpu, dev->next_event, 1);
+				tick_broadcast_set_event(bc, cpu, dev->next_event);
 		}
 		/*
 		 * If the current CPU owns the hrtimer broadcast
@@ -858,7 +850,7 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 			clockevents_set_state(bc, CLOCK_EVT_STATE_ONESHOT);
 			tick_broadcast_init_next_event(tmpmask,
 						       tick_next_period);
-			tick_broadcast_set_event(bc, cpu, tick_next_period, 1);
+			tick_broadcast_set_event(bc, cpu, tick_next_period);
 		} else
 			bc->next_event.tv64 = KTIME_MAX;
 	} else {

commit 2951d5c031a3aaefa31b688fbf229e75692f4786
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 5 10:00:13 2015 +0200

    tick: broadcast: Prevent livelock from event handler
    
    With the removal of the hrtimer softirq the switch to highres/nohz
    mode happens in the tick interrupt. That leads to a livelock when the
    per cpu event handler is directly called from the broadcast handler
    under broadcast lock because broadcast lock needs to be taken for the
    highres/nohz switch as well.
    
    Solve this by calling the cpu local handler outside the broadcast_lock
    held region.
    
    Fixes: c6eb3f70d448 "hrtimer: Get rid of hrtimer softirq"
    Reported-and-tested-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 7e8ca4f448a8..5d9e4aab9797 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -255,18 +255,18 @@ int tick_receive_broadcast(void)
 /*
  * Broadcast the event to the cpus, which are set in the mask (mangled).
  */
-static void tick_do_broadcast(struct cpumask *mask)
+static bool tick_do_broadcast(struct cpumask *mask)
 {
 	int cpu = smp_processor_id();
 	struct tick_device *td;
+	bool local = false;
 
 	/*
 	 * Check, if the current cpu is in the mask
 	 */
 	if (cpumask_test_cpu(cpu, mask)) {
 		cpumask_clear_cpu(cpu, mask);
-		td = &per_cpu(tick_cpu_device, cpu);
-		td->evtdev->event_handler(td->evtdev);
+		local = true;
 	}
 
 	if (!cpumask_empty(mask)) {
@@ -279,16 +279,17 @@ static void tick_do_broadcast(struct cpumask *mask)
 		td = &per_cpu(tick_cpu_device, cpumask_first(mask));
 		td->evtdev->broadcast(mask);
 	}
+	return local;
 }
 
 /*
  * Periodic broadcast:
  * - invoke the broadcast handlers
  */
-static void tick_do_periodic_broadcast(void)
+static bool tick_do_periodic_broadcast(void)
 {
 	cpumask_and(tmpmask, cpu_online_mask, tick_broadcast_mask);
-	tick_do_broadcast(tmpmask);
+	return tick_do_broadcast(tmpmask);
 }
 
 /*
@@ -296,34 +297,26 @@ static void tick_do_periodic_broadcast(void)
  */
 static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 {
-	ktime_t next;
+	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
+	bool bc_local;
 
 	raw_spin_lock(&tick_broadcast_lock);
+	bc_local = tick_do_periodic_broadcast();
 
-	tick_do_periodic_broadcast();
+	if (dev->state == CLOCK_EVT_STATE_ONESHOT) {
+		ktime_t next = ktime_add(dev->next_event, tick_period);
 
-	/*
-	 * The device is in periodic mode. No reprogramming necessary:
-	 */
-	if (dev->state == CLOCK_EVT_STATE_PERIODIC)
-		goto unlock;
+		clockevents_program_event(dev, next, true);
+	}
+	raw_spin_unlock(&tick_broadcast_lock);
 
 	/*
-	 * Setup the next period for devices, which do not have
-	 * periodic mode. We read dev->next_event first and add to it
-	 * when the event already expired. clockevents_program_event()
-	 * sets dev->next_event only when the event is really
-	 * programmed to the device.
+	 * We run the handler of the local cpu after dropping
+	 * tick_broadcast_lock because the handler might deadlock when
+	 * trying to switch to oneshot mode.
 	 */
-	for (next = dev->next_event; ;) {
-		next = ktime_add(next, tick_period);
-
-		if (!clockevents_program_event(dev, next, false))
-			goto unlock;
-		tick_do_periodic_broadcast();
-	}
-unlock:
-	raw_spin_unlock(&tick_broadcast_lock);
+	if (bc_local)
+		td->evtdev->event_handler(td->evtdev);
 }
 
 /**
@@ -622,9 +615,13 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 		cpumask_and(tmpmask, tmpmask, cpu_online_mask);
 
 	/*
-	 * Wakeup the cpus which have an expired event.
+	 * Wakeup the cpus which have an expired event and handle the
+	 * broadcast event of the local cpu.
 	 */
-	tick_do_broadcast(tmpmask);
+	if (tick_do_broadcast(tmpmask)) {
+		td = this_cpu_ptr(&tick_cpu_device);
+		td->evtdev->event_handler(td->evtdev);
+	}
 
 	/*
 	 * Two reasons for reprogram:

commit a49b116dcb1265f238f3169507424257b0519069
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:38:05 2015 +0200

    clockevents: Cleanup dead cpu explicitely
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call. We are way better off to have explicit
    calls instead of this monstrosity.
    
    Split out the cleanup function for a dead cpu and invoke it
    directly from the cpu down code. Make it conditional on
    CPU_HOTPLUG as well.
    
    Temporary change, will be refined in the future.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Rebased, added clockevents_notify() removal ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1735025.raBZdQHM3m@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 55e43f20987a..7e8ca4f448a8 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -410,14 +410,14 @@ void tick_set_periodic_handler(struct clock_event_device *dev, int broadcast)
 		dev->event_handler = tick_handle_periodic_broadcast;
 }
 
+#ifdef CONFIG_HOTPLUG_CPU
 /*
  * Remove a CPU from broadcasting
  */
-void tick_shutdown_broadcast(unsigned int *cpup)
+void tick_shutdown_broadcast(unsigned int cpu)
 {
 	struct clock_event_device *bc;
 	unsigned long flags;
-	unsigned int cpu = *cpup;
 
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
@@ -432,6 +432,7 @@ void tick_shutdown_broadcast(unsigned int *cpup)
 
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
+#endif
 
 void tick_suspend_broadcast(void)
 {
@@ -672,21 +673,6 @@ static void broadcast_shutdown_local(struct clock_event_device *bc,
 	clockevents_set_state(dev, CLOCK_EVT_STATE_SHUTDOWN);
 }
 
-void hotplug_cpu__broadcast_tick_pull(int deadcpu)
-{
-	struct clock_event_device *bc;
-	unsigned long flags;
-
-	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
-	bc = tick_broadcast_device.evtdev;
-
-	if (bc && broadcast_needs_cpu(bc, deadcpu)) {
-		/* This moves the broadcast assignment to this CPU: */
-		clockevents_program_event(bc, bc->next_event, 1);
-	}
-	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
-}
-
 /**
  * tick_broadcast_oneshot_control - Enter/exit broadcast oneshot mode
  * @state:	The target state (enter/exit)
@@ -908,14 +894,28 @@ void tick_broadcast_switch_to_oneshot(void)
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
+#ifdef CONFIG_HOTPLUG_CPU
+void hotplug_cpu__broadcast_tick_pull(int deadcpu)
+{
+	struct clock_event_device *bc;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
+	bc = tick_broadcast_device.evtdev;
+
+	if (bc && broadcast_needs_cpu(bc, deadcpu)) {
+		/* This moves the broadcast assignment to this CPU: */
+		clockevents_program_event(bc, bc->next_event, 1);
+	}
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+}
 
 /*
  * Remove a dead CPU from broadcasting
  */
-void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
+void tick_shutdown_broadcast_oneshot(unsigned int cpu)
 {
 	unsigned long flags;
-	unsigned int cpu = *cpup;
 
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
@@ -929,6 +929,7 @@ void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
 
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
+#endif
 
 /*
  * Check, whether the broadcast device is in one shot mode

commit 1fe5d5c3c9ba0c4ade18e3325cba0ffe35127941
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:05:15 2015 +0200

    clockevents: Provide explicit broadcast oneshot control functions
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call. We are way better off to have explicit
    calls instead of this monstrosity.
    
    Split out the broadcast oneshot control into a separate function
    and provide inline helpers. Switch clockevents_notify() over.
    This will go away once all callers are converted.
    
    This also gets rid of the nested locking of clockevents_lock and
    broadcast_lock. The broadcast oneshot control functions do not
    require clockevents_lock. Only the managing functions
    (setup/shutdown/suspend/resume of the broadcast device require
    clockevents_lock.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Alexandre Courbot <gnurou@gmail.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephen Warren <swarren@wwwdotorg.org>
    Cc: Thierry Reding <thierry.reding@gmail.com>
    Cc: Tony Lindgren <tony@atomide.com>
    Link: http://lkml.kernel.org/r/13000649.8qZuEDV0OA@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 1a0bee04ef8c..55e43f20987a 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -687,18 +687,23 @@ void hotplug_cpu__broadcast_tick_pull(int deadcpu)
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
-/*
- * Powerstate information: The system enters/leaves a state, where
- * affected devices might stop
+/**
+ * tick_broadcast_oneshot_control - Enter/exit broadcast oneshot mode
+ * @state:	The target state (enter/exit)
+ *
+ * The system enters/leaves a state, where affected devices might stop
  * Returns 0 on success, -EBUSY if the cpu is used to broadcast wakeups.
+ *
+ * Called with interrupts disabled, so clockevents_lock is not
+ * required here because the local clock event device cannot go away
+ * under us.
  */
-int tick_broadcast_oneshot_control(unsigned long reason)
+int tick_broadcast_oneshot_control(enum tick_broadcast_state state)
 {
 	struct clock_event_device *bc, *dev;
 	struct tick_device *td;
-	unsigned long flags;
-	ktime_t now;
 	int cpu, ret = 0;
+	ktime_t now;
 
 	/*
 	 * Periodic mode does not care about the enter/exit of power
@@ -711,17 +716,17 @@ int tick_broadcast_oneshot_control(unsigned long reason)
 	 * We are called with preemtion disabled from the depth of the
 	 * idle code, so we can't be moved away.
 	 */
-	cpu = smp_processor_id();
-	td = &per_cpu(tick_cpu_device, cpu);
+	td = this_cpu_ptr(&tick_cpu_device);
 	dev = td->evtdev;
 
 	if (!(dev->features & CLOCK_EVT_FEAT_C3STOP))
 		return 0;
 
+	raw_spin_lock(&tick_broadcast_lock);
 	bc = tick_broadcast_device.evtdev;
+	cpu = smp_processor_id();
 
-	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
-	if (reason == CLOCK_EVT_NOTIFY_BROADCAST_ENTER) {
+	if (state == TICK_BROADCAST_ENTER) {
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			WARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));
 			broadcast_shutdown_local(bc, dev);
@@ -813,9 +818,10 @@ int tick_broadcast_oneshot_control(unsigned long reason)
 		}
 	}
 out:
-	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	raw_spin_unlock(&tick_broadcast_lock);
 	return ret;
 }
+EXPORT_SYMBOL_GPL(tick_broadcast_oneshot_control);
 
 /*
  * Reset the one shot broadcast for a cpu

commit 592a438ff3fea61d303c5784c209b3f1fd3e16df
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:01:10 2015 +0200

    clockevents: Provide explicit broadcast control functions
    
    clockevents_notify() is a leftover from the early design of the
    clockevents facility. It's really not a notification mechanism,
    it's a multiplex call. We are way better off to have explicit
    calls instead of this monstrosity.
    
    Split out the broadcast control into a separate function and
    provide inline helpers. Switch clockevents_notify() over. This
    will go away once all callers are converted.
    
    This also gets rid of the nested locking of clockevents_lock and
    broadcast_lock. The broadcast control functions do not require
    clockevents_lock. Only the managing functions
    (setup/shutdown/suspend/resume of the broadcast device require
    clockevents_lock.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tony Lindgren <tony@atomide.com>
    Link: http://lkml.kernel.org/r/8086559.ttsuS0n1Xr@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index f5e0fd5652dc..1a0bee04ef8c 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -33,7 +33,7 @@ static cpumask_var_t tick_broadcast_mask;
 static cpumask_var_t tick_broadcast_on;
 static cpumask_var_t tmpmask;
 static DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
-static int tick_broadcast_force;
+static int tick_broadcast_forced;
 
 #ifdef CONFIG_TICK_ONESHOT
 static void tick_broadcast_clear_oneshot(int cpu);
@@ -326,49 +326,54 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 	raw_spin_unlock(&tick_broadcast_lock);
 }
 
-/*
- * Powerstate information: The system enters/leaves a state, where
- * affected devices might stop
+/**
+ * tick_broadcast_control - Enable/disable or force broadcast mode
+ * @mode:	The selected broadcast mode
+ *
+ * Called when the system enters a state where affected tick devices
+ * might stop. Note: TICK_BROADCAST_FORCE cannot be undone.
+ *
+ * Called with interrupts disabled, so clockevents_lock is not
+ * required here because the local clock event device cannot go away
+ * under us.
  */
-static void tick_do_broadcast_on_off(unsigned long *reason)
+void tick_broadcast_control(enum tick_broadcast_mode mode)
 {
 	struct clock_event_device *bc, *dev;
 	struct tick_device *td;
-	unsigned long flags;
 	int cpu, bc_stopped;
 
-	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
-
-	cpu = smp_processor_id();
-	td = &per_cpu(tick_cpu_device, cpu);
+	td = this_cpu_ptr(&tick_cpu_device);
 	dev = td->evtdev;
-	bc = tick_broadcast_device.evtdev;
 
 	/*
 	 * Is the device not affected by the powerstate ?
 	 */
 	if (!dev || !(dev->features & CLOCK_EVT_FEAT_C3STOP))
-		goto out;
+		return;
 
 	if (!tick_device_is_functional(dev))
-		goto out;
+		return;
 
+	raw_spin_lock(&tick_broadcast_lock);
+	cpu = smp_processor_id();
+	bc = tick_broadcast_device.evtdev;
 	bc_stopped = cpumask_empty(tick_broadcast_mask);
 
-	switch (*reason) {
-	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
-	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
+	switch (mode) {
+	case TICK_BROADCAST_FORCE:
+		tick_broadcast_forced = 1;
+	case TICK_BROADCAST_ON:
 		cpumask_set_cpu(cpu, tick_broadcast_on);
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_mask)) {
 			if (tick_broadcast_device.mode ==
 			    TICKDEV_MODE_PERIODIC)
 				clockevents_shutdown(dev);
 		}
-		if (*reason == CLOCK_EVT_NOTIFY_BROADCAST_FORCE)
-			tick_broadcast_force = 1;
 		break;
-	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
-		if (tick_broadcast_force)
+
+	case TICK_BROADCAST_OFF:
+		if (tick_broadcast_forced)
 			break;
 		cpumask_clear_cpu(cpu, tick_broadcast_on);
 		if (!tick_device_is_functional(dev))
@@ -390,22 +395,9 @@ static void tick_do_broadcast_on_off(unsigned long *reason)
 		else
 			tick_broadcast_setup_oneshot(bc);
 	}
-out:
-	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
-}
-
-/*
- * Powerstate information: The system enters/leaves a state, where
- * affected devices might stop.
- */
-void tick_broadcast_on_off(unsigned long reason, int *oncpu)
-{
-	if (!cpumask_test_cpu(*oncpu, cpu_online_mask))
-		printk(KERN_ERR "tick-broadcast: ignoring broadcast for "
-		       "offline CPU #%d\n", *oncpu);
-	else
-		tick_do_broadcast_on_off(&reason);
+	raw_spin_unlock(&tick_broadcast_lock);
 }
+EXPORT_SYMBOL_GPL(tick_broadcast_control);
 
 /*
  * Set the periodic handler depending on broadcast on/off

commit 345527b1edce8df719e0884500c76832a18211c3
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Mon Mar 30 14:59:19 2015 +0530

    clockevents: Fix cpu_down() race for hrtimer based broadcasting
    
    It was found when doing a hotplug stress test on POWER, that the
    machine either hit softlockups or rcu_sched stall warnings.  The
    issue was traced to commit:
    
      7cba160ad789 ("powernv/cpuidle: Redesign idle states management")
    
    which exposed the cpu_down() race with hrtimer based broadcast mode:
    
      5d1638acb9f6 ("tick: Introduce hrtimer based broadcast")
    
    The race is the following:
    
    Assume CPU1 is the CPU which holds the hrtimer broadcasting duty
    before it is taken down.
    
            CPU0                                    CPU1
    
            cpu_down()                              take_cpu_down()
                                                    disable_interrupts()
    
            cpu_die()
    
            while (CPU1 != CPU_DEAD) {
                    msleep(100);
                    switch_to_idle();
                    stop_cpu_timer();
                    schedule_broadcast();
            }
    
            tick_cleanup_cpu_dead()
                    take_over_broadcast()
    
    So after CPU1 disabled interrupts it cannot handle the broadcast
    hrtimer anymore, so CPU0 will be stuck forever.
    
    Fix this by explicitly taking over broadcast duty before cpu_die().
    
    This is a temporary workaround. What we really want is a callback
    in the clockevent device which allows us to do that from the dying
    CPU by pushing the hrtimer onto a different cpu. That might involve
    an IPI and is definitely more complex than this immediate fix.
    
    Changelog was picked up from:
    
        https://lkml.org/lkml/2015/2/16/213
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: mpe@ellerman.id.au
    Cc: nicolas.pitre@linaro.org
    Cc: peterz@infradead.org
    Cc: rjw@rjwysocki.net
    Fixes: http://linuxppc.10917.n7.nabble.com/offlining-cpus-breakage-td88619.html
    Link: http://lkml.kernel.org/r/20150330092410.24979.59887.stgit@preeti.in.ibm.com
    [ Merged it to the latest timer tree, renamed the callback, tidied up the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 19cfb381faa9..f5e0fd5652dc 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -680,14 +680,19 @@ static void broadcast_shutdown_local(struct clock_event_device *bc,
 	clockevents_set_state(dev, CLOCK_EVT_STATE_SHUTDOWN);
 }
 
-static void broadcast_move_bc(int deadcpu)
+void hotplug_cpu__broadcast_tick_pull(int deadcpu)
 {
-	struct clock_event_device *bc = tick_broadcast_device.evtdev;
+	struct clock_event_device *bc;
+	unsigned long flags;
 
-	if (!bc || !broadcast_needs_cpu(bc, deadcpu))
-		return;
-	/* This moves the broadcast assignment to this cpu */
-	clockevents_program_event(bc, bc->next_event, 1);
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
+	bc = tick_broadcast_device.evtdev;
+
+	if (bc && broadcast_needs_cpu(bc, deadcpu)) {
+		/* This moves the broadcast assignment to this CPU: */
+		clockevents_program_event(bc, bc->next_event, 1);
+	}
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
 /*
@@ -924,8 +929,6 @@ void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
 	cpumask_clear_cpu(cpu, tick_broadcast_pending_mask);
 	cpumask_clear_cpu(cpu, tick_broadcast_force_mask);
 
-	broadcast_move_bc(cpu);
-
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 

commit f46481d0a7cb942b84145acb80ad43bdb1ff8eb4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 13:11:04 2015 +0100

    tick/xen: Provide and use tick_suspend_local() and tick_resume_local()
    
    Xen calls on every cpu into tick_resume() which is just wrong.
    tick_resume() is for the syscore global suspend/resume
    invocation. What XEN really wants is a per cpu local resume
    function.
    
    Provide a tick_resume_local() function and use it in XEN.
    
    Also provide a complementary tick_suspend_local() and modify
    tick_unfreeze() and tick_freeze(), respectively, to use the
    new local tick resume/suspend functions.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    [ Combined two patches, rebased, modified subject/changelog. ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1698741.eezk9tnXtG@vostro.rjw.lan
    [ Merged to latest timers/core. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 60e6c23ce1c7..19cfb381faa9 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -455,11 +455,26 @@ void tick_suspend_broadcast(void)
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
-int tick_resume_broadcast(void)
+/*
+ * This is called from tick_resume_local() on a resuming CPU. That's
+ * called from the core resume function, tick_unfreeze() and the magic XEN
+ * resume hackery.
+ *
+ * In none of these cases the broadcast device mode can change and the
+ * bit of the resuming CPU in the broadcast mask is safe as well.
+ */
+bool tick_resume_check_broadcast(void)
+{
+	if (tick_broadcast_device.mode == TICKDEV_MODE_ONESHOT)
+		return false;
+	else
+		return cpumask_test_cpu(smp_processor_id(), tick_broadcast_mask);
+}
+
+void tick_resume_broadcast(void)
 {
 	struct clock_event_device *bc;
 	unsigned long flags;
-	int broadcast = 0;
 
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
@@ -472,8 +487,6 @@ int tick_resume_broadcast(void)
 		case TICKDEV_MODE_PERIODIC:
 			if (!cpumask_empty(tick_broadcast_mask))
 				tick_broadcast_start_periodic(bc);
-			broadcast = cpumask_test_cpu(smp_processor_id(),
-						     tick_broadcast_mask);
 			break;
 		case TICKDEV_MODE_ONESHOT:
 			if (!cpumask_empty(tick_broadcast_mask))
@@ -482,11 +495,8 @@ int tick_resume_broadcast(void)
 		}
 	}
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
-
-	return broadcast;
 }
 
-
 #ifdef CONFIG_TICK_ONESHOT
 
 static cpumask_var_t tick_broadcast_oneshot_mask;

commit 080873ce2d1abd8c0a2b8c87bfa0762546a6b713
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 25 13:09:55 2015 +0100

    tick: Make tick_resume_broadcast_oneshot() static
    
    Solely used in tick-broadcast.c and the return value is
    hardcoded 0. Make it static and void.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1689058.QkHYDJSRKu@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index f0f8ee9dbc28..60e6c23ce1c7 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -37,8 +37,10 @@ static int tick_broadcast_force;
 
 #ifdef CONFIG_TICK_ONESHOT
 static void tick_broadcast_clear_oneshot(int cpu);
+static void tick_resume_broadcast_oneshot(struct clock_event_device *bc);
 #else
 static inline void tick_broadcast_clear_oneshot(int cpu) { }
+static inline void tick_resume_broadcast_oneshot(struct clock_event_device *bc) { }
 #endif
 
 /*
@@ -475,7 +477,7 @@ int tick_resume_broadcast(void)
 			break;
 		case TICKDEV_MODE_ONESHOT:
 			if (!cpumask_empty(tick_broadcast_mask))
-				broadcast = tick_resume_broadcast_oneshot(bc);
+				tick_resume_broadcast_oneshot(bc);
 			break;
 		}
 	}
@@ -541,10 +543,9 @@ static int tick_broadcast_set_event(struct clock_event_device *bc, int cpu,
 	return ret;
 }
 
-int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
+static void tick_resume_broadcast_oneshot(struct clock_event_device *bc)
 {
 	clockevents_set_state(bc, CLOCK_EVT_STATE_ONESHOT);
-	return 0;
 }
 
 /*

commit 77e32c89a7117614ab3d66d20c1088de721abfaa
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Fri Feb 27 17:21:33 2015 +0530

    clockevents: Manage device's state separately for the core
    
    'enum clock_event_mode' is used for two purposes today:
    
     - to pass mode to the driver of clockevent device::set_mode().
    
     - for managing state of the device for clockevents core.
    
    For supporting new modes/states we have moved away from the
    legacy set_mode() callback to new per-mode/state callbacks. New
    modes/states shouldn't be exposed to the legacy (now OBSOLOTE)
    callbacks and so we shouldn't add new states to 'enum
    clock_event_mode'.
    
    Lets have separate enums for the two use cases mentioned above.
    Keep using the earlier enum for legacy set_mode() callback and
    mark it OBSOLETE. And add another enum to clearly specify the
    possible states of a clockevent device.
    
    This also renames the newly added per-mode callbacks to reflect
    state changes.
    
    We haven't got rid of 'mode' member of 'struct
    clock_event_device' as it is used by some of the clockevent
    drivers and it would automatically die down once we migrate
    those drivers to the new interface. It ('mode') is only updated
    now for the drivers using the legacy interface.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: linaro-kernel@lists.linaro.org
    Cc: linaro-networking@linaro.org
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/b6b0143a8a57bd58352ad35e08c25424c879c0cb.1425037853.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 542d5bb5c13d..f0f8ee9dbc28 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -303,7 +303,7 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 	/*
 	 * The device is in periodic mode. No reprogramming necessary:
 	 */
-	if (dev->mode == CLOCK_EVT_MODE_PERIODIC)
+	if (dev->state == CLOCK_EVT_STATE_PERIODIC)
 		goto unlock;
 
 	/*
@@ -532,8 +532,8 @@ static int tick_broadcast_set_event(struct clock_event_device *bc, int cpu,
 {
 	int ret;
 
-	if (bc->mode != CLOCK_EVT_MODE_ONESHOT)
-		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
+	if (bc->state != CLOCK_EVT_STATE_ONESHOT)
+		clockevents_set_state(bc, CLOCK_EVT_STATE_ONESHOT);
 
 	ret = clockevents_program_event(bc, expires, force);
 	if (!ret)
@@ -543,7 +543,7 @@ static int tick_broadcast_set_event(struct clock_event_device *bc, int cpu,
 
 int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
 {
-	clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
+	clockevents_set_state(bc, CLOCK_EVT_STATE_ONESHOT);
 	return 0;
 }
 
@@ -562,8 +562,8 @@ void tick_check_oneshot_broadcast_this_cpu(void)
 		 * switched over, leave the device alone.
 		 */
 		if (td->mode == TICKDEV_MODE_ONESHOT) {
-			clockevents_set_mode(td->evtdev,
-					     CLOCK_EVT_MODE_ONESHOT);
+			clockevents_set_state(td->evtdev,
+					      CLOCK_EVT_STATE_ONESHOT);
 		}
 	}
 }
@@ -666,7 +666,7 @@ static void broadcast_shutdown_local(struct clock_event_device *bc,
 		if (dev->next_event.tv64 < bc->next_event.tv64)
 			return;
 	}
-	clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
+	clockevents_set_state(dev, CLOCK_EVT_STATE_SHUTDOWN);
 }
 
 static void broadcast_move_bc(int deadcpu)
@@ -741,7 +741,7 @@ int tick_broadcast_oneshot_control(unsigned long reason)
 			cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
 	} else {
 		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {
-			clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
+			clockevents_set_state(dev, CLOCK_EVT_STATE_ONESHOT);
 			/*
 			 * The cpu which was handling the broadcast
 			 * timer marked this cpu in the broadcast
@@ -842,7 +842,7 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 
 	/* Set it up only once ! */
 	if (bc->event_handler != tick_handle_oneshot_broadcast) {
-		int was_periodic = bc->mode == CLOCK_EVT_MODE_PERIODIC;
+		int was_periodic = bc->state == CLOCK_EVT_STATE_PERIODIC;
 
 		bc->event_handler = tick_handle_oneshot_broadcast;
 
@@ -858,7 +858,7 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 			   tick_broadcast_oneshot_mask, tmpmask);
 
 		if (was_periodic && !cpumask_empty(tmpmask)) {
-			clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
+			clockevents_set_state(bc, CLOCK_EVT_STATE_ONESHOT);
 			tick_broadcast_init_next_event(tmpmask,
 						       tick_next_period);
 			tick_broadcast_set_event(bc, cpu, tick_next_period, 1);

commit 554ef3876c6acdff1331feab10275e9e9e0adb84
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Fri Feb 27 17:21:32 2015 +0530

    clockevents: Handle tick device's resume separately
    
    Upcoming patch will redefine possible states of a clockevent
    device. The RESUME mode is a special case only for tick's
    clockevent devices. In future it can be replaced by ->resume()
    callback already available for clockevent devices.
    
    Lets handle it separately so that clockevents_set_mode() only
    handles states valid across all devices. This also renames
    set_mode_resume() to tick_resume() to make it more explicit.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: linaro-kernel@lists.linaro.org
    Cc: linaro-networking@linaro.org
    Cc: linux-arm-kernel@lists.infradead.org
    Link: http://lkml.kernel.org/r/c1b0112410870f49e7bf06958e1483eac6c15e20.1425037853.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 066f0ec05e48..542d5bb5c13d 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -464,7 +464,7 @@ int tick_resume_broadcast(void)
 	bc = tick_broadcast_device.evtdev;
 
 	if (bc) {
-		clockevents_set_mode(bc, CLOCK_EVT_MODE_RESUME);
+		clockevents_tick_resume(bc);
 
 		switch (tick_broadcast_device.mode) {
 		case TICKDEV_MODE_PERIODIC:

commit 22127e93c587afa01e4f7225d2d1cf1d26ae7dfe
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Aug 17 12:30:25 2014 -0500

    time: Replace __get_cpu_var uses
    
    Convert uses of __get_cpu_var for creating a address from a percpu
    offset to this_cpu_ptr.
    
    The two cases where get_cpu_var is used to actually access a percpu
    variable are changed to use this_cpu_read/raw_cpu_read.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 64c5990fd500..066f0ec05e48 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -554,7 +554,7 @@ int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
 void tick_check_oneshot_broadcast_this_cpu(void)
 {
 	if (cpumask_test_cpu(smp_processor_id(), tick_broadcast_oneshot_mask)) {
-		struct tick_device *td = &__get_cpu_var(tick_cpu_device);
+		struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
 
 		/*
 		 * We might be in the middle of switching over from

commit 1ead65812486cda65093683a99b8907a7242fa93
Merge: b6d739e95812 b97f0291a250
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 1 11:00:07 2014 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer changes from Thomas Gleixner:
     "This assorted collection provides:
    
       - A new timer based timer broadcast feature for systems which do not
         provide a global accessible timer device.  That allows those
         systems to put CPUs into deep idle states where the per cpu timer
         device stops.
    
       - A few NOHZ_FULL related improvements to the timer wheel
    
       - The usual updates to timer devices found in ARM SoCs
    
       - Small improvements and updates all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      tick: Remove code duplication in tick_handle_periodic()
      tick: Fix spelling mistake in tick_handle_periodic()
      x86: hpet: Use proper destructor for delayed work
      workqueue: Provide destroy_delayed_work_on_stack()
      clocksource: CMT, MTU2, TMU and STI should depend on GENERIC_CLOCKEVENTS
      timer: Remove code redundancy while calling get_nohz_timer_target()
      hrtimer: Rearrange comments in the order struct members are declared
      timer: Use variable head instead of &work_list in __run_timers()
      clocksource: exynos_mct: silence a static checker warning
      arm: zynq: Add support for cpufreq
      arm: zynq: Don't use arm_global_timer with cpufreq
      clocksource/cadence_ttc: Overhaul clocksource frequency adjustment
      clocksource/cadence_ttc: Call clockevents_update_freq() with IRQs enabled
      clocksource: Add Kconfig entries for CMT, MTU2, TMU and STI
      sh: Remove Kconfig entries for TMU, CMT and MTU2
      ARM: shmobile: Remove CMT, TMU and STI Kconfig entries
      clocksource: armada-370-xp: Use atomic access for shared registers
      clocksource: orion: Use atomic access for shared registers
      clocksource: timer-keystone: Delete unnecessary variable
      clocksource: timer-keystone: introduce clocksource driver for Keystone
      ...

commit dd5fd9b91a77b4c9c28b7ef9c181b1a875820d0a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Feb 11 14:35:40 2014 +0100

    tick: Clear broadcast pending bit when switching to oneshot
    
    AMD systems which use the C1E workaround in the amd_e400_idle routine
    trigger the WARN_ON_ONCE in the broadcast code when onlining a CPU.
    
    The reason is that the idle routine of those AMD systems switches the
    cpu into forced broadcast mode early on before the newly brought up
    CPU can switch over to high resolution / NOHZ mode. The timer related
    CPU1 bringup looks like this:
    
      clockevent_register_device(local_apic);
      tick_setup(local_apic);
      ...
      idle()
            tick_broadcast_on_off(FORCE);
            tick_broadcast_oneshot_control(ENTER)
              cpumask_set(cpu, broadcast_oneshot_mask);
            halt();
    
    Now the broadcast interrupt on CPU0 sets CPU1 in the
    broadcast_pending_mask and wakes CPU1. So CPU1 continues:
    
            local_apic_timer_interrupt()
               tick_handle_periodic();
               softirq()
                 tick_init_highres();
                   cpumask_clr(cpu, broadcast_oneshot_mask);
    
            tick_broadcast_oneshot_control(ENTER)
               WARN_ON(cpumask_test(cpu, broadcast_pending_mask);
    
    So while we remove CPU1 from the broadcast_oneshot_mask when we switch
    over to highres mode, we do not clear the pending bit, which then
    triggers the warning when we go back to idle.
    
    The reason why this is only visible on C1E affected AMD systems is
    that the other machines enter the deep sleep states via
    acpi_idle/intel_idle and exit the broadcast mode before executing the
    remote triggered local_apic_timer_interrupt. So the pending bit is
    already cleared when the switch over to highres mode is clearing the
    oneshot mask.
    
    The solution is simple: Clear the pending bit together with the mask
    bit when we switch over to highres mode.
    
    Stanislaw came up independently with the same patch by enforcing the
    C1E workaround and debugging the fallout. I picked mine, because mine
    has a changelog :)
    
    Reported-by: poma <pomidorabelisima@gmail.com>
    Debugged-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Olaf Hering <olaf@aepfle.de>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Justin M. Forbes <jforbes@redhat.com>
    Cc: Josh Boyer <jwboyer@redhat.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1402111434180.21991@ionos.tec.linutronix.de
    Cc: stable@vger.kernel.org # 3.10+
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 43780ab5e279..98977a57ac72 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -756,6 +756,7 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 static void tick_broadcast_clear_oneshot(int cpu)
 {
 	cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
+	cpumask_clear_cpu(cpu, tick_broadcast_pending_mask);
 }
 
 static void tick_broadcast_init_next_event(struct cpumask *mask,

commit 5d1638acb9f62fa7eb0c07cb85318bbe1f13b227
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Fri Feb 7 13:36:32 2014 +0530

    tick: Introduce hrtimer based broadcast
    
    On some architectures, in certain CPU deep idle states the local timers stop.
    An external clock device is used to wakeup these CPUs. The kernel support for the
    wakeup of these CPUs is provided by the tick broadcast framework by using the
    external clock device as the wakeup source.
    
    However not all implementations of architectures provide such an external
    clock device. This patch includes support in the broadcast framework to handle
    the wakeup of the CPUs in deep idle states on such systems by queuing a hrtimer
    on one of the CPUs, which is meant to handle the wakeup of CPUs in deep idle states.
    
    This patchset introduces a pseudo clock device which can be registered by the
    archs as tick_broadcast_device in the absence of a real external clock
    device. Once registered, the broadcast framework will work as is for these
    architectures as long as the archs take care of the BROADCAST_ENTER
    notification failing for one of the CPUs. This CPU is made the stand by CPU to
    handle wakeup of the CPUs in deep idle and it *must not enter deep idle states*.
    
    The CPU with the earliest wakeup is chosen to be this CPU. Hence this way the
    stand by CPU dynamically moves around and so does the hrtimer which is queued
    to trigger at the next earliest wakeup time. This is consistent with the case where
    an external clock device is present. The smp affinity of this clock device is
    set to the CPU with the earliest wakeup. This patchset handles the hotplug of
    the stand by CPU as well by moving the hrtimer on to the CPU handling the CPU_DEAD
    notification.
    
    Originally-from: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: deepthi@linux.vnet.ibm.com
    Cc: paulmck@linux.vnet.ibm.com
    Cc: fweisbec@gmail.com
    Cc: paulus@samba.org
    Cc: srivatsa.bhat@linux.vnet.ibm.com
    Cc: svaidy@linux.vnet.ibm.com
    Cc: peterz@infradead.org
    Cc: benh@kernel.crashing.org
    Cc: rafael.j.wysocki@intel.com
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/20140207080632.17187.80532.stgit@preeti.in.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 84c8fd91d744..63c7b2d9ed8e 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -643,6 +643,42 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	raw_spin_unlock(&tick_broadcast_lock);
 }
 
+static int broadcast_needs_cpu(struct clock_event_device *bc, int cpu)
+{
+	if (!(bc->features & CLOCK_EVT_FEAT_HRTIMER))
+		return 0;
+	if (bc->next_event.tv64 == KTIME_MAX)
+		return 0;
+	return bc->bound_on == cpu ? -EBUSY : 0;
+}
+
+static void broadcast_shutdown_local(struct clock_event_device *bc,
+				     struct clock_event_device *dev)
+{
+	/*
+	 * For hrtimer based broadcasting we cannot shutdown the cpu
+	 * local device if our own event is the first one to expire or
+	 * if we own the broadcast timer.
+	 */
+	if (bc->features & CLOCK_EVT_FEAT_HRTIMER) {
+		if (broadcast_needs_cpu(bc, smp_processor_id()))
+			return;
+		if (dev->next_event.tv64 < bc->next_event.tv64)
+			return;
+	}
+	clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
+}
+
+static void broadcast_move_bc(int deadcpu)
+{
+	struct clock_event_device *bc = tick_broadcast_device.evtdev;
+
+	if (!bc || !broadcast_needs_cpu(bc, deadcpu))
+		return;
+	/* This moves the broadcast assignment to this cpu */
+	clockevents_program_event(bc, bc->next_event, 1);
+}
+
 /*
  * Powerstate information: The system enters/leaves a state, where
  * affected devices might stop
@@ -661,7 +697,7 @@ int tick_broadcast_oneshot_control(unsigned long reason)
 	 * states
 	 */
 	if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
-		return;
+		return 0;
 
 	/*
 	 * We are called with preemtion disabled from the depth of the
@@ -672,7 +708,7 @@ int tick_broadcast_oneshot_control(unsigned long reason)
 	dev = td->evtdev;
 
 	if (!(dev->features & CLOCK_EVT_FEAT_C3STOP))
-		return;
+		return 0;
 
 	bc = tick_broadcast_device.evtdev;
 
@@ -680,7 +716,7 @@ int tick_broadcast_oneshot_control(unsigned long reason)
 	if (reason == CLOCK_EVT_NOTIFY_BROADCAST_ENTER) {
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			WARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));
-			clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
+			broadcast_shutdown_local(bc, dev);
 			/*
 			 * We only reprogram the broadcast timer if we
 			 * did not mark ourself in the force mask and
@@ -693,6 +729,16 @@ int tick_broadcast_oneshot_control(unsigned long reason)
 			    dev->next_event.tv64 < bc->next_event.tv64)
 				tick_broadcast_set_event(bc, cpu, dev->next_event, 1);
 		}
+		/*
+		 * If the current CPU owns the hrtimer broadcast
+		 * mechanism, it cannot go deep idle and we remove the
+		 * CPU from the broadcast mask. We don't have to go
+		 * through the EXIT path as the local timer is not
+		 * shutdown.
+		 */
+		ret = broadcast_needs_cpu(bc, cpu);
+		if (ret)
+			cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
 	} else {
 		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
@@ -866,6 +912,8 @@ void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
 	cpumask_clear_cpu(cpu, tick_broadcast_pending_mask);
 	cpumask_clear_cpu(cpu, tick_broadcast_force_mask);
 
+	broadcast_move_bc(cpu);
+
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 

commit da7e6f45c34d39186b72328bacc4dd86bff60e0a
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Fri Feb 7 13:36:06 2014 +0530

    time: Change the return type of clockevents_notify() to integer
    
    The broadcast framework can potentially be made use of by archs which do not have an
    external clock device as well. Then, it is required that one of the CPUs need
    to handle the broadcasting of wakeup IPIs to the CPUs in deep idle. As a
    result its local timers should remain functional all the time. For such
    a CPU, the BROADCAST_ENTER notification has to fail indicating that its clock
    device cannot be shutdown. To make way for this support, change the return
    type of tick_broadcast_oneshot_control() and hence clockevents_notify() to
    indicate such scenarios.
    
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: deepthi@linux.vnet.ibm.com
    Cc: paulmck@linux.vnet.ibm.com
    Cc: fweisbec@gmail.com
    Cc: paulus@samba.org
    Cc: srivatsa.bhat@linux.vnet.ibm.com
    Cc: svaidy@linux.vnet.ibm.com
    Cc: peterz@infradead.org
    Cc: benh@kernel.crashing.org
    Cc: rafael.j.wysocki@intel.com
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/20140207080606.17187.78306.stgit@preeti.in.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 003e6c3663b1..84c8fd91d744 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -646,14 +646,15 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 /*
  * Powerstate information: The system enters/leaves a state, where
  * affected devices might stop
+ * Returns 0 on success, -EBUSY if the cpu is used to broadcast wakeups.
  */
-void tick_broadcast_oneshot_control(unsigned long reason)
+int tick_broadcast_oneshot_control(unsigned long reason)
 {
 	struct clock_event_device *bc, *dev;
 	struct tick_device *td;
 	unsigned long flags;
 	ktime_t now;
-	int cpu;
+	int cpu, ret = 0;
 
 	/*
 	 * Periodic mode does not care about the enter/exit of power
@@ -759,6 +760,7 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 	}
 out:
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	return ret;
 }
 
 /*

commit 627ee7947e2e83ba565c31c5c9373d6e364b1ecd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Feb 3 14:34:31 2014 -0800

    clockevents: Serialize calls to clockevents_update_freq() in the core
    
    We can identify the broadcast device in the core and serialize all
    callers including interrupts on a different CPU against the update.
    Also, disabling interrupts is moved into the core allowing callers to
    leave interrutps enabled when calling clockevents_update_freq().
    
    Signed-off-by: Soren Brinkmann <soren.brinkmann@xilinx.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Soeren Brinkmann <soren.brinkmann@xilinx.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Michal Simek <michal.simek@xilinx.com>
    Link: http://lkml.kernel.org/r/1391466877-28908-2-git-send-email-soren.brinkmann@xilinx.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 43780ab5e279..003e6c3663b1 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -120,6 +120,19 @@ int tick_is_broadcast_device(struct clock_event_device *dev)
 	return (dev && tick_broadcast_device.evtdev == dev);
 }
 
+int tick_broadcast_update_freq(struct clock_event_device *dev, u32 freq)
+{
+	int ret = -ENODEV;
+
+	if (tick_is_broadcast_device(dev)) {
+		raw_spin_lock(&tick_broadcast_lock);
+		ret = __clockevents_update_freq(dev, freq);
+		raw_spin_unlock(&tick_broadcast_lock);
+	}
+	return ret;
+}
+
+
 static void err_broadcast(const struct cpumask *mask)
 {
 	pr_crit_once("Failed to broadcast timer tick. Some CPUs may be unresponsive.\n");
@@ -272,12 +285,8 @@ static void tick_do_broadcast(struct cpumask *mask)
  */
 static void tick_do_periodic_broadcast(void)
 {
-	raw_spin_lock(&tick_broadcast_lock);
-
 	cpumask_and(tmpmask, cpu_online_mask, tick_broadcast_mask);
 	tick_do_broadcast(tmpmask);
-
-	raw_spin_unlock(&tick_broadcast_lock);
 }
 
 /*
@@ -287,13 +296,15 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 {
 	ktime_t next;
 
+	raw_spin_lock(&tick_broadcast_lock);
+
 	tick_do_periodic_broadcast();
 
 	/*
 	 * The device is in periodic mode. No reprogramming necessary:
 	 */
 	if (dev->mode == CLOCK_EVT_MODE_PERIODIC)
-		return;
+		goto unlock;
 
 	/*
 	 * Setup the next period for devices, which do not have
@@ -306,9 +317,11 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 		next = ktime_add(next, tick_period);
 
 		if (!clockevents_program_event(dev, next, false))
-			return;
+			goto unlock;
 		tick_do_periodic_broadcast();
 	}
+unlock:
+	raw_spin_unlock(&tick_broadcast_lock);
 }
 
 /*

commit e8fcaa5c54e3b0371230e5d43a6f650c667da9c5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Aug 7 22:28:01 2013 +0200

    nohz: Convert a few places to use local per cpu accesses
    
    A few functions use remote per CPU access APIs when they
    deal with local values.
    
    Just do the right conversion to improve performance, code
    readability and debug checks.
    
    While at it, lets extend some of these function names with *_this_cpu()
    suffix in order to display their purpose more clearly.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 9532690daaa9..43780ab5e279 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -538,10 +538,10 @@ int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
  * Called from irq_enter() when idle was interrupted to reenable the
  * per cpu device.
  */
-void tick_check_oneshot_broadcast(int cpu)
+void tick_check_oneshot_broadcast_this_cpu(void)
 {
-	if (cpumask_test_cpu(cpu, tick_broadcast_oneshot_mask)) {
-		struct tick_device *td = &per_cpu(tick_cpu_device, cpu);
+	if (cpumask_test_cpu(smp_processor_id(), tick_broadcast_oneshot_mask)) {
+		struct tick_device *td = &__get_cpu_var(tick_cpu_device);
 
 		/*
 		 * We might be in the middle of switching over from

commit 245a34962661cd2ce7b4dd6c4aa65d870a589c50
Author: Soren Brinkmann <soren.brinkmann@xilinx.com>
Date:   Wed Sep 18 11:48:37 2013 -0700

    tick: broadcast: Deny per-cpu clockevents from being broadcast sources
    
    On most ARM systems the per-cpu clockevents are truly per-cpu in
    the sense that they can't be controlled on any other CPU besides
    the CPU that they interrupt. If one of these clockevents were to
    become a broadcast source we will run into a lot of trouble
    because the broadcast source is enabled on the first CPU to go
    into deep idle (if that CPU suffers from FEAT_C3_STOP) and that
    could be a different CPU than what the clockevent is interrupting
    (or even worse the CPU that the clockevent interrupts could be
    offline).
    
    Theoretically it's possible to support per-cpu clockevents as the
    broadcast source but so far we haven't needed this and supporting
    it is rather complicated. Let's just deny the possibility for now
    until this becomes a reality (let's hope it never does!).
    
    Signed-off-by: Soren Brinkmann <soren.brinkmann@xilinx.com>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Michal Simek <michal.simek@xilinx.com>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 218bcb565fed..9532690daaa9 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -70,6 +70,7 @@ static bool tick_check_broadcast_device(struct clock_event_device *curdev,
 					struct clock_event_device *newdev)
 {
 	if ((newdev->features & CLOCK_EVT_FEAT_DUMMY) ||
+	    (newdev->features & CLOCK_EVT_FEAT_PERCPU) ||
 	    (newdev->features & CLOCK_EVT_FEAT_C3STOP))
 		return false;
 

commit a272dcca1802a7e265a56e60b0d0a6715b0a8ac2
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Thu Jul 11 07:00:59 2013 -0700

    tick: broadcast: Check broadcast mode on CPU hotplug
    
    On ARM systems the dummy clockevent is registered with the cpu
    hotplug notifier chain before any other per-cpu clockevent. This
    has the side-effect of causing the dummy clockevent to be
    registered first in every hotplug sequence. Because the dummy is
    first, we'll try to turn the broadcast source on but the code in
    tick_device_uses_broadcast() assumes the broadcast source is in
    periodic mode and calls tick_broadcast_start_periodic()
    unconditionally.
    
    On boot this isn't a problem because we typically haven't
    switched into oneshot mode yet (if at all). During hotplug, if
    the broadcast source isn't in periodic mode we'll replace the
    broadcast oneshot handler with the broadcast periodic handler and
    start emulating oneshot mode when we shouldn't. Due to the way
    the broadcast oneshot handler programs the next_event it's
    possible for it to contain KTIME_MAX and cause us to hang the
    system when the periodic handler tries to program the next tick.
    Fix this by using the appropriate function to start the broadcast
    source.
    
    Reported-by: Stephen Warren <swarren@nvidia.com>
    Tested-by: Stephen Warren <swarren@nvidia.com>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Mark Rutland <Mark.Rutland@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: ARM kernel mailing list <linux-arm-kernel@lists.infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Joseph Lo <josephl@nvidia.com>
    Link: http://lkml.kernel.org/r/20130711140059.GA27430@codeaurora.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 6d3f91631de6..218bcb565fed 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -157,7 +157,10 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 		dev->event_handler = tick_handle_periodic;
 		tick_device_setup_broadcast_func(dev);
 		cpumask_set_cpu(cpu, tick_broadcast_mask);
-		tick_broadcast_start_periodic(bc);
+		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
+			tick_broadcast_start_periodic(bc);
+		else
+			tick_broadcast_setup_oneshot(bc);
 		ret = 1;
 	} else {
 		/*

commit 2b0f89317e99735bbf32eaede81f707f98ab1b5e
Merge: 07bd1172902e fa18f7bde3ad
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jul 4 23:11:22 2013 +0200

    Merge branch 'timers/posix-cpu-timers-for-tglx' of
    git://git.kernel.org/pub/scm/linux/kernel/git/frederic/linux-dynticks into timers/core
    
    Frederic sayed: "Most of these patches have been hanging around for
    several month now, in -mmotm for a significant chunk. They already
    missed a few releases."
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 07bd1172902e782f288e4d44b1fde7dec0f08b6f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 1 22:14:10 2013 +0200

    tick: Sanitize broadcast control logic
    
    The recent implementation of a generic dummy timer resulted in a
    different registration order of per cpu local timers which made the
    broadcast control logic go belly up.
    
    If the dummy timer is the first clock event device which is registered
    for a CPU, then it is installed, the broadcast timer is initialized
    and the CPU is marked as broadcast target.
    
    If a real clock event device is installed after that, we can fail to
    take the CPU out of the broadcast mask. In the worst case we end up
    with two periodic timer events firing for the same CPU. One from the
    per cpu hardware device and one from the broadcast.
    
    Now the problem is that we have no way to distinguish whether the
    system is in a state which makes broadcasting necessary or the
    broadcast bit was set due to the nonfunctional dummy timer
    installment.
    
    To solve this we need to keep track of the system state seperately and
    provide a more detailed decision logic whether we keep the CPU in
    broadcast mode or not.
    
    The old decision logic only clears the broadcast mode, if the newly
    installed clock event device is not affected by power states.
    
    The new logic clears the broadcast mode if one of the following is
    true:
    
      - The new device is not affected by power states.
    
      - The system is not in a power state affected mode
    
      - The system has switched to oneshot mode. The oneshot broadcast is
        controlled from the deep idle state. The CPU is not in idle at
        this point, so it's safe to remove it from the mask.
    
    If we clear the broadcast bit for the CPU when a new device is
    installed, we also shutdown the broadcast device when this was the
    last CPU in the broadcast mask.
    
    If the broadcast bit is kept, then we leave the new device in shutdown
    state and rely on the broadcast to deliver the timer interrupts via
    the broadcast ipis.
    
    Reported-and-tested-by: Stehle Vincent-B46079 <B46079@freescale.com>
    Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
    Cc: John Stultz <john.stultz@linaro.org>,
    Cc: Mark Rutland <mark.rutland@arm.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1307012153060.4013@ionos.tec.linutronix.de
    Cc: stable@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 248f80dba746..4430fa695b48 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -30,6 +30,7 @@
 
 static struct tick_device tick_broadcast_device;
 static cpumask_var_t tick_broadcast_mask;
+static cpumask_var_t tick_broadcast_on;
 static cpumask_var_t tmpmask;
 static DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
 static int tick_broadcast_force;
@@ -140,8 +141,9 @@ static void tick_device_setup_broadcast_func(struct clock_event_device *dev)
  */
 int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 {
+	struct clock_event_device *bc = tick_broadcast_device.evtdev;
 	unsigned long flags;
-	int ret = 0;
+	int ret;
 
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
@@ -155,20 +157,59 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 		dev->event_handler = tick_handle_periodic;
 		tick_device_setup_broadcast_func(dev);
 		cpumask_set_cpu(cpu, tick_broadcast_mask);
-		tick_broadcast_start_periodic(tick_broadcast_device.evtdev);
+		tick_broadcast_start_periodic(bc);
 		ret = 1;
 	} else {
 		/*
-		 * When the new device is not affected by the stop
-		 * feature and the cpu is marked in the broadcast mask
-		 * then clear the broadcast bit.
+		 * Clear the broadcast bit for this cpu if the
+		 * device is not power state affected.
 		 */
-		if (!(dev->features & CLOCK_EVT_FEAT_C3STOP)) {
-			int cpu = smp_processor_id();
+		if (!(dev->features & CLOCK_EVT_FEAT_C3STOP))
 			cpumask_clear_cpu(cpu, tick_broadcast_mask);
-			tick_broadcast_clear_oneshot(cpu);
-		} else {
+		else
 			tick_device_setup_broadcast_func(dev);
+
+		/*
+		 * Clear the broadcast bit if the CPU is not in
+		 * periodic broadcast on state.
+		 */
+		if (!cpumask_test_cpu(cpu, tick_broadcast_on))
+			cpumask_clear_cpu(cpu, tick_broadcast_mask);
+
+		switch (tick_broadcast_device.mode) {
+		case TICKDEV_MODE_ONESHOT:
+			/*
+			 * If the system is in oneshot mode we can
+			 * unconditionally clear the oneshot mask bit,
+			 * because the CPU is running and therefore
+			 * not in an idle state which causes the power
+			 * state affected device to stop. Let the
+			 * caller initialize the device.
+			 */
+			tick_broadcast_clear_oneshot(cpu);
+			ret = 0;
+			break;
+
+		case TICKDEV_MODE_PERIODIC:
+			/*
+			 * If the system is in periodic mode, check
+			 * whether the broadcast device can be
+			 * switched off now.
+			 */
+			if (cpumask_empty(tick_broadcast_mask) && bc)
+				clockevents_shutdown(bc);
+			/*
+			 * If we kept the cpu in the broadcast mask,
+			 * tell the caller to leave the per cpu device
+			 * in shutdown state. The periodic interrupt
+			 * is delivered by the broadcast device.
+			 */
+			ret = cpumask_test_cpu(cpu, tick_broadcast_mask);
+			break;
+		default:
+			/* Nothing to do */
+			ret = 0;
+			break;
 		}
 	}
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
@@ -298,6 +339,7 @@ static void tick_do_broadcast_on_off(unsigned long *reason)
 	switch (*reason) {
 	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
 	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
+		cpumask_set_cpu(cpu, tick_broadcast_on);
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_mask)) {
 			if (tick_broadcast_device.mode ==
 			    TICKDEV_MODE_PERIODIC)
@@ -307,8 +349,12 @@ static void tick_do_broadcast_on_off(unsigned long *reason)
 			tick_broadcast_force = 1;
 		break;
 	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
-		if (!tick_broadcast_force &&
-		    cpumask_test_and_clear_cpu(cpu, tick_broadcast_mask)) {
+		if (tick_broadcast_force)
+			break;
+		cpumask_clear_cpu(cpu, tick_broadcast_on);
+		if (!tick_device_is_functional(dev))
+			break;
+		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_mask)) {
 			if (tick_broadcast_device.mode ==
 			    TICKDEV_MODE_PERIODIC)
 				tick_setup_periodic(dev, 0);
@@ -366,6 +412,7 @@ void tick_shutdown_broadcast(unsigned int *cpup)
 
 	bc = tick_broadcast_device.evtdev;
 	cpumask_clear_cpu(cpu, tick_broadcast_mask);
+	cpumask_clear_cpu(cpu, tick_broadcast_on);
 
 	if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC) {
 		if (bc && cpumask_empty(tick_broadcast_mask))
@@ -821,6 +868,7 @@ bool tick_broadcast_oneshot_available(void)
 void __init tick_broadcast_init(void)
 {
 	zalloc_cpumask_var(&tick_broadcast_mask, GFP_NOWAIT);
+	zalloc_cpumask_var(&tick_broadcast_on, GFP_NOWAIT);
 	zalloc_cpumask_var(&tmpmask, GFP_NOWAIT);
 #ifdef CONFIG_TICK_ONESHOT
 	zalloc_cpumask_var(&tick_broadcast_oneshot_mask, GFP_NOWAIT);

commit 1f73a9806bdd07a5106409bbcab3884078bd34fe
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jul 1 22:14:10 2013 +0200

    tick: Prevent uncontrolled switch to oneshot mode
    
    When the system switches from periodic to oneshot mode, the broadcast
    logic causes a possibility that a CPU which has not yet switched to
    oneshot mode puts its own clock event device into oneshot mode without
    updating the state and the timer handler.
    
    CPU0                            CPU1
                                    per cpu tickdev is in periodic mode
                                    and switched to broadcast
    
    Switch to oneshot mode
     tick_broadcast_switch_to_oneshot()
      cpumask_copy(tick_oneshot_broacast_mask,
                   tick_broadcast_mask);
    
      broadcast device mode = oneshot
    
                                    Timer interrupt
    
                                    irq_enter()
                                     tick_check_oneshot_broadcast()
                                      dev->set_mode(ONESHOT);
    
                                    tick_handle_periodic()
                                     if (dev->mode == ONESHOT)
                                       dev->next_event += period;
                                       FAIL.
    
    We fail, because dev->next_event contains KTIME_MAX, if the device was
    in periodic mode before the uncontrolled switch to oneshot happened.
    
    We must copy the broadcast bits over to the oneshot mask, because
    otherwise a CPU which relies on the broadcast would not been woken up
    anymore after the broadcast device switched to oneshot mode.
    
    So we need to verify in tick_check_oneshot_broadcast() whether the CPU
    has already switched to oneshot mode. If not, leave the device
    untouched and let the CPU switch controlled into oneshot mode.
    
    This is a long standing bug, which was never noticed, because the main
    user of the broadcast x86 cannot run into that scenario, AFAICT. The
    nonarchitected timer mess of ARM creates a gazillion of differently
    broken abominations which trigger the shortcomings of that broadcast
    code, which better had never been necessary in the first place.
    
    Reported-and-tested-by: Stehle Vincent-B46079 <B46079@freescale.com>
    Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
    Cc: John Stultz <john.stultz@linaro.org>,
    Cc: Mark Rutland <mark.rutland@arm.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1307012153060.4013@ionos.tec.linutronix.de
    Cc: stable@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 4790037163f6..248f80dba746 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -492,7 +492,15 @@ void tick_check_oneshot_broadcast(int cpu)
 	if (cpumask_test_cpu(cpu, tick_broadcast_oneshot_mask)) {
 		struct tick_device *td = &per_cpu(tick_cpu_device, cpu);
 
-		clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_ONESHOT);
+		/*
+		 * We might be in the middle of switching over from
+		 * periodic to oneshot. If the CPU has not yet
+		 * switched over, leave the device alone.
+		 */
+		if (td->mode == TICKDEV_MODE_ONESHOT) {
+			clockevents_set_mode(td->evtdev,
+					     CLOCK_EVT_MODE_ONESHOT);
+		}
 	}
 }
 

commit c9b5a266b103af873abb9ac03bc3d067702c8f4b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 26 12:17:32 2013 +0200

    tick: Make oneshot broadcast robust vs. CPU offlining
    
    In periodic mode we remove offline cpus from the broadcast propagation
    mask. In oneshot mode we fail to do so. This was not a problem so far,
    but the recent changes to the broadcast propagation introduced a
    constellation which can result in a NULL pointer dereference.
    
    What happens is:
    
    CPU0                    CPU1
                            idle()
                              arch_idle()
                                tick_broadcast_oneshot_control(OFF);
                                  set cpu1 in tick_broadcast_force_mask
                              if (cpu_offline())
                                 arch_cpu_dead()
    
    cpu_dead_cleanup(cpu1)
     cpu1 tickdevice pointer = NULL
    
    broadcast interrupt
      dereference cpu1 tickdevice pointer -> OOPS
    
    We dereference the pointer because cpu1 is still set in
    tick_broadcast_force_mask and tick_do_broadcast() expects a valid
    cpumask and therefor lacks any further checks.
    
    Remove the cpu from the tick_broadcast_force_mask before we set the
    tick device pointer to NULL. Also add a sanity check to the oneshot
    broadcast function, so we can detect such issues w/o crashing the
    machine.
    
    Reported-by: Prarit Bhargava <prarit@redhat.com>
    Cc: athorlton@sgi.com
    Cc: CAI Qian <caiqian@redhat.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.02.1306261303260.4013@ionos.tec.linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index d067c01586f5..4790037163f6 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -532,6 +532,13 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	cpumask_or(tmpmask, tmpmask, tick_broadcast_force_mask);
 	cpumask_clear(tick_broadcast_force_mask);
 
+	/*
+	 * Sanity check. Catch the case where we try to broadcast to
+	 * offline cpus.
+	 */
+	if (WARN_ON_ONCE(!cpumask_subset(tmpmask, cpu_online_mask)))
+		cpumask_and(tmpmask, tmpmask, cpu_online_mask);
+
 	/*
 	 * Wakeup the cpus which have an expired event.
 	 */
@@ -773,10 +780,12 @@ void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	/*
-	 * Clear the broadcast mask flag for the dead cpu, but do not
-	 * stop the broadcast device!
+	 * Clear the broadcast masks for the dead cpu, but do not stop
+	 * the broadcast device!
 	 */
 	cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
+	cpumask_clear_cpu(cpu, tick_broadcast_pending_mask);
+	cpumask_clear_cpu(cpu, tick_broadcast_force_mask);
 
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }

commit ea8deb8dfa6b0e8d1b3d1051585706739b46656c
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Mon Jun 17 18:15:35 2013 +0200

    tick: Fix tick_broadcast_pending_mask not cleared
    
    The recent modification in the cpuidle framework consolidated the
    timer broadcast code across the different drivers by setting a new
    flag in the idle state. It tells the cpuidle core code to enter/exit
    the broadcast mode for the cpu when entering a deep idle state. The
    broadcast timer enter/exit is no longer handled by the back-end
    driver.
    
    This change made the local interrupt to be enabled *before* calling
    CLOCK_EVENT_NOTIFY_EXIT.
    
    On a tegra114, a four cores system, when the flag has been introduced
    in the driver, the following warning appeared:
    
    WARNING: at kernel/time/tick-broadcast.c:578 tick_broadcast_oneshot_control
    CPU: 2 PID: 0 Comm: swapper/2 Not tainted 3.10.0-rc3-next-20130529+ #15
    [<c00667f8>] (tick_broadcast_oneshot_control+0x1a4/0x1d0) from [<c0065cd0>] (tick_notify+0x240/0x40c)
    [<c0065cd0>] (tick_notify+0x240/0x40c) from [<c0044724>] (notifier_call_chain+0x44/0x84)
    [<c0044724>] (notifier_call_chain+0x44/0x84) from [<c0044828>] (raw_notifier_call_chain+0x18/0x20)
    [<c0044828>] (raw_notifier_call_chain+0x18/0x20) from [<c00650cc>] (clockevents_notify+0x28/0x170)
    [<c00650cc>] (clockevents_notify+0x28/0x170) from [<c033f1f0>] (cpuidle_idle_call+0x11c/0x168)
    [<c033f1f0>] (cpuidle_idle_call+0x11c/0x168) from [<c000ea94>] (arch_cpu_idle+0x8/0x38)
    [<c000ea94>] (arch_cpu_idle+0x8/0x38) from [<c005ea80>] (cpu_startup_entry+0x60/0x134)
    [<c005ea80>] (cpu_startup_entry+0x60/0x134) from [<804fe9a4>] (0x804fe9a4)
    
    I don't have the hardware, so I wasn't able to reproduce the warning
    but after looking a while at the code, I deduced the following:
    
     1. the CPU2 enters a deep idle state and sets the broadcast timer
    
     2. the timer expires, the tick_handle_oneshot_broadcast function is
        called, setting the tick_broadcast_pending_mask and waking up the
        idle cpu CPU2
    
     3. the CPU2 exits idle handles the interrupt and then invokes
        tick_broadcast_oneshot_control with CLOCK_EVENT_NOTIFY_EXIT which
        runs the following code:
    
        [...]
        if (dev->next_event.tv64 == KTIME_MAX)
                goto out;
    
        if (cpumask_test_and_clear_cpu(cpu,
                                     tick_broadcast_pending_mask))
                goto out;
        [...]
    
        So if there is no next event scheduled for CPU2, we fulfil the
        first condition and jump out without clearing the
        tick_broadcast_pending_mask.
    
     4. CPU2 goes to deep idle again and calls
        tick_broadcast_oneshot_control with CLOCK_NOTIFY_EVENT_ENTER but
        with the tick_broadcast_pending_mask set for CPU2, triggering the
        warning.
    
    The issue only surfaced due to the modifications of the cpuidle
    framework, which resulted in interrupts being enabled before the call
    to the clockevents code. If the call happens before interrupts have
    been enabled, the warning cannot trigger, because there is still the
    event pending which caused the broadcast timer expiry.
    
    Move the check for the next event below the check for the pending bit,
    so the pending bit gets cleared whether an event is scheduled on the
    cpu or not.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Reported-and-tested-by: Joseph Lo <josephl@nvidia.com>
    Cc: Stephen Warren <swarren@nvidia.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linaro-kernel@lists.linaro.org
    Link: http://lkml.kernel.org/r/1371485735-31249-1-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index b4c245580b79..20d6fba70652 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -599,8 +599,6 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 	} else {
 		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
-			if (dev->next_event.tv64 == KTIME_MAX)
-				goto out;
 			/*
 			 * The cpu which was handling the broadcast
 			 * timer marked this cpu in the broadcast
@@ -614,6 +612,11 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 				       tick_broadcast_pending_mask))
 				goto out;
 
+			/*
+			 * Bail out if there is no next event.
+			 */
+			if (dev->next_event.tv64 == KTIME_MAX)
+				goto out;
 			/*
 			 * If the pending bit is not set, then we are
 			 * either the CPU handling the broadcast

commit f5d00c1f9adb350c24c5301600f7bf2da99b66de
Author: Jiri Bohac <jbohac@suse.cz>
Date:   Tue May 28 15:29:03 2013 +0200

    tick: Remove useless timekeeping duty attribution to broadcast source
    
    Since 7300711e ("clockevents: broadcast fixup possible waiters"),
    the timekeeping duty is assigned to the CPU that handles the tick
    broadcast clock device by the time it is set in one shot mode.
    
    This is an issue in full dynticks mode where the timekeeping duty
    must stay handled by the boot CPU for now. Otherwise it prevents
    secondary CPUs from offlining and this breaks
    suspend/shutdown/reboot/...
    
    As it appears there is no reason for this timekeeping duty to be
    moved to the broadcast CPU, besides nothing prevent it from being
    later re-assigned to another target, let's simply remove it.
    
    Signed-off-by: Jiri Bohac <jbohac@suse.cz>
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 0c739423b0f9..b4c245580b79 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -698,10 +698,6 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 
 		bc->event_handler = tick_handle_oneshot_broadcast;
 
-		/* Take the do_timer update */
-		if (!tick_nohz_full_cpu(cpu))
-			tick_do_timer_cpu = cpu;
-
 		/*
 		 * We must be careful here. There might be other CPUs
 		 * waiting for periodic broadcast. We need to set the

commit 2938d2757fc99c26aa678ce4eba910c4a77c3a55
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 28 09:33:01 2013 +0200

    tick: Cure broadcast false positive pending bit warning
    
    commit 26517f3e (tick: Avoid programming the local cpu timer if
    broadcast pending) added a warning if the cpu enters broadcast mode
    again while the pending bit is still set. Meelis reported that the
    warning triggers. There are two corner cases which have been not
    considered:
    
    1) cpuidle calls clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER)
       twice. That can result in the following scenario
    
       CPU0                    CPU1
                               cpuidle_idle_call()
                                 clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER)
                                   set cpu in tick_broadcast_oneshot_mask
    
       broadcast interrupt
         event expired for cpu1
         set pending bit
    
                                 acpi_idle_enter_simple()
                                   clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER)
                                     WARN_ON(pending bit)
    
      Move the WARN_ON into the section where we enter broadcast mode so
      it wont provide false positives on the second call.
    
    2) safe_halt() enables interrupts, so a broadcast interrupt can be
       delivered befor the broadcast mode is disabled. That sets the
       pending bit for the CPU which receives the broadcast
       interrupt. Though the interrupt is delivered right away from the
       broadcast handler and leaves the pending bit stale.
    
       Clear the pending bit for the current cpu in the broadcast handler.
    
    Reported-and-tested-by: Meelis Roos <mroos@linux.ee>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1305271841130.4220@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 24938d577669..0c739423b0f9 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -511,6 +511,12 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 		}
 	}
 
+	/*
+	 * Remove the current cpu from the pending mask. The event is
+	 * delivered immediately in tick_do_broadcast() !
+	 */
+	cpumask_clear_cpu(smp_processor_id(), tick_broadcast_pending_mask);
+
 	/* Take care of enforced broadcast requests */
 	cpumask_or(tmpmask, tmpmask, tick_broadcast_force_mask);
 	cpumask_clear(tick_broadcast_force_mask);
@@ -575,8 +581,8 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 	if (reason == CLOCK_EVT_NOTIFY_BROADCAST_ENTER) {
-		WARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
+			WARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
 			/*
 			 * We only reprogram the broadcast timer if we

commit 45cb8e01b2ecef1c2afb18333e95793fa1a90281
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 20:31:50 2013 +0000

    clockevents: Split out selection logic
    
    Split out the clockevent device selection logic. Preparatory patch to
    allow unbinding active clockevent devices.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130425143436.431796247@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 0e374cd2e0ef..d067c01586f5 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -65,19 +65,34 @@ static void tick_broadcast_start_periodic(struct clock_event_device *bc)
 /*
  * Check, if the device can be utilized as broadcast device:
  */
+static bool tick_check_broadcast_device(struct clock_event_device *curdev,
+					struct clock_event_device *newdev)
+{
+	if ((newdev->features & CLOCK_EVT_FEAT_DUMMY) ||
+	    (newdev->features & CLOCK_EVT_FEAT_C3STOP))
+		return false;
+
+	if (tick_broadcast_device.mode == TICKDEV_MODE_ONESHOT &&
+	    !(newdev->features & CLOCK_EVT_FEAT_ONESHOT))
+		return false;
+
+	return !curdev || newdev->rating > curdev->rating;
+}
+
+/*
+ * Conditionally install/replace broadcast device
+ */
 void tick_install_broadcast_device(struct clock_event_device *dev)
 {
 	struct clock_event_device *cur = tick_broadcast_device.evtdev;
 
-	if ((dev->features & CLOCK_EVT_FEAT_DUMMY) ||
-	    (tick_broadcast_device.evtdev &&
-	     tick_broadcast_device.evtdev->rating >= dev->rating) ||
-	     (dev->features & CLOCK_EVT_FEAT_C3STOP))
+	if (!tick_check_broadcast_device(cur, dev))
 		return;
+
 	if (!try_module_get(dev->owner))
 		return;
 
-	clockevents_exchange_device(tick_broadcast_device.evtdev, dev);
+	clockevents_exchange_device(cur, dev);
 	if (cur)
 		cur->event_handler = clockevents_handle_noop;
 	tick_broadcast_device.evtdev = dev;

commit ccf33d6880f39a35158fff66db13000ae4943fac
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 20:31:49 2013 +0000

    clockevents: Add module refcount
    
    We want to be able to remove clockevent modules as well. Add a
    refcount so we don't remove a module with an active clock event
    device.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130425143436.307435149@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 3500caaa0bfd..0e374cd2e0ef 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -19,6 +19,7 @@
 #include <linux/profile.h>
 #include <linux/sched.h>
 #include <linux/smp.h>
+#include <linux/module.h>
 
 #include "tick-internal.h"
 
@@ -73,6 +74,8 @@ void tick_install_broadcast_device(struct clock_event_device *dev)
 	     tick_broadcast_device.evtdev->rating >= dev->rating) ||
 	     (dev->features & CLOCK_EVT_FEAT_C3STOP))
 		return;
+	if (!try_module_get(dev->owner))
+		return;
 
 	clockevents_exchange_device(tick_broadcast_device.evtdev, dev);
 	if (cur)

commit 7172a286ced0c1f4f239a0fa09db54ed37d3ead2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 20:31:47 2013 +0000

    clockevents: Get rid of the notifier chain
    
    7+ years and still a single user. Kill it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Link: http://lkml.kernel.org/r/20130425143436.098520211@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 24938d577669..3500caaa0bfd 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -64,7 +64,7 @@ static void tick_broadcast_start_periodic(struct clock_event_device *bc)
 /*
  * Check, if the device can be utilized as broadcast device:
  */
-int tick_check_broadcast_device(struct clock_event_device *dev)
+void tick_install_broadcast_device(struct clock_event_device *dev)
 {
 	struct clock_event_device *cur = tick_broadcast_device.evtdev;
 
@@ -72,7 +72,7 @@ int tick_check_broadcast_device(struct clock_event_device *dev)
 	    (tick_broadcast_device.evtdev &&
 	     tick_broadcast_device.evtdev->rating >= dev->rating) ||
 	     (dev->features & CLOCK_EVT_FEAT_C3STOP))
-		return 0;
+		return;
 
 	clockevents_exchange_device(tick_broadcast_device.evtdev, dev);
 	if (cur)
@@ -90,7 +90,6 @@ int tick_check_broadcast_device(struct clock_event_device *dev)
 	 */
 	if (dev->features & CLOCK_EVT_FEAT_ONESHOT)
 		tick_clock_notify();
-	return 1;
 }
 
 /*

commit cc51bf6e6d8b03bd459818492e0bc3bef09dcd74
Merge: 37cae5e24981 b4f711ee03d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 15 14:05:17 2013 -0700

    Merge branch 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer fixes from Thomas Gleixner:
    
     - Cure for not using zalloc in the first place, which leads to random
       crashes with CPUMASK_OFF_STACK.
    
     - Revert a user space visible change which broke udev
    
     - Add a missing cpu_online early return introduced by the new full
       dyntick conversions
    
     - Plug a long standing race in the timer wheel cpu hotplug code.
       Sigh...
    
     - Cleanup NOHZ per cpu data on cpu down to prevent stale data on cpu
       up.
    
    * 'timers-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      time: Revert ALWAYS_USE_PERSISTENT_CLOCK compile time optimizaitons
      timer: Don't reinitialize the cpu base lock during CPU_UP_PREPARE
      tick: Don't invoke tick_nohz_stop_sched_tick() if the cpu is offline
      tick: Cleanup NOHZ per cpu data on cpu down
      tick: Use zalloc_cpumask_var for allocating offstack cpumasks

commit 534c97b0950b1967bca1c753aeaed32f5db40264
Merge: 64049d1973c1 265f22a975c1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 5 13:23:27 2013 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull 'full dynticks' support from Ingo Molnar:
     "This tree from Frederic Weisbecker adds a new, (exciting! :-) core
      kernel feature to the timer and scheduler subsystems: 'full dynticks',
      or CONFIG_NO_HZ_FULL=y.
    
      This feature extends the nohz variable-size timer tick feature from
      idle to busy CPUs (running at most one task) as well, potentially
      reducing the number of timer interrupts significantly.
    
      This feature got motivated by real-time folks and the -rt tree, but
      the general utility and motivation of full-dynticks runs wider than
      that:
    
       - HPC workloads get faster: CPUs running a single task should be able
         to utilize a maximum amount of CPU power.  A periodic timer tick at
         HZ=1000 can cause a constant overhead of up to 1.0%.  This feature
         removes that overhead - and speeds up the system by 0.5%-1.0% on
         typical distro configs even on modern systems.
    
       - Real-time workload latency reduction: CPUs running critical tasks
         should experience as little jitter as possible.  The last remaining
         source of kernel-related jitter was the periodic timer tick.
    
       - A single task executing on a CPU is a pretty common situation,
         especially with an increasing number of cores/CPUs, so this feature
         helps desktop and mobile workloads as well.
    
      The cost of the feature is mainly related to increased timer
      reprogramming overhead when a CPU switches its tick period, and thus
      slightly longer to-idle and from-idle latency.
    
      Configuration-wise a third mode of operation is added to the existing
      two NOHZ kconfig modes:
    
       - CONFIG_HZ_PERIODIC: [formerly !CONFIG_NO_HZ], now explicitly named
         as a config option.  This is the traditional Linux periodic tick
         design: there's a HZ tick going on all the time, regardless of
         whether a CPU is idle or not.
    
       - CONFIG_NO_HZ_IDLE: [formerly CONFIG_NO_HZ=y], this turns off the
         periodic tick when a CPU enters idle mode.
    
       - CONFIG_NO_HZ_FULL: this new mode, in addition to turning off the
         tick when a CPU is idle, also slows the tick down to 1 Hz (one
         timer interrupt per second) when only a single task is running on a
         CPU.
    
      The .config behavior is compatible: existing !CONFIG_NO_HZ and
      CONFIG_NO_HZ=y settings get translated to the new values, without the
      user having to configure anything.  CONFIG_NO_HZ_FULL is turned off by
      default.
    
      This feature is based on a lot of infrastructure work that has been
      steadily going upstream in the last 2-3 cycles: related RCU support
      and non-periodic cputime support in particular is upstream already.
    
      This tree adds the final pieces and activates the feature.  The pull
      request is marked RFC because:
    
       - it's marked 64-bit only at the moment - the 32-bit support patch is
         small but did not get ready in time.
    
       - it has a number of fresh commits that came in after the merge
         window.  The overwhelming majority of commits are from before the
         merge window, but still some aspects of the tree are fresh and so I
         marked it RFC.
    
       - it's a pretty wide-reaching feature with lots of effects - and
         while the components have been in testing for some time, the full
         combination is still not very widely used.  That it's default-off
         should reduce its regression abilities and obviously there are no
         known regressions with CONFIG_NO_HZ_FULL=y enabled either.
    
       - the feature is not completely idempotent: there is no 100%
         equivalent replacement for a periodic scheduler/timer tick.  In
         particular there's ongoing work to map out and reduce its effects
         on scheduler load-balancing and statistics.  This should not impact
         correctness though, there are no known regressions related to this
         feature at this point.
    
       - it's a pretty ambitious feature that with time will likely be
         enabled by most Linux distros, and we'd like you to make input on
         its design/implementation, if you dislike some aspect we missed.
         Without flaming us to crisp! :-)
    
      Future plans:
    
       - there's ongoing work to reduce 1Hz to 0Hz, to essentially shut off
         the periodic tick altogether when there's a single busy task on a
         CPU.  We'd first like 1 Hz to be exposed more widely before we go
         for the 0 Hz target though.
    
       - once we reach 0 Hz we can remove the periodic tick assumption from
         nr_running>=2 as well, by essentially interrupting busy tasks only
         as frequently as the sched_latency constraints require us to do -
         once every 4-40 msecs, depending on nr_running.
    
      I am personally leaning towards biting the bullet and doing this in
      v3.10, like the -rt tree this effort has been going on for too long -
      but the final word is up to you as usual.
    
      More technical details can be found in Documentation/timers/NO_HZ.txt"
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (39 commits)
      sched: Keep at least 1 tick per second for active dynticks tasks
      rcu: Fix full dynticks' dependency on wide RCU nocb mode
      nohz: Protect smp_processor_id() in tick_nohz_task_switch()
      nohz_full: Add documentation.
      cputime_nsecs: use math64.h for nsec resolution conversion helpers
      nohz: Select VIRT_CPU_ACCOUNTING_GEN from full dynticks config
      nohz: Reduce overhead under high-freq idling patterns
      nohz: Remove full dynticks' superfluous dependency on RCU tree
      nohz: Fix unavailable tick_stop tracepoint in dynticks idle
      nohz: Add basic tracing
      nohz: Select wide RCU nocb for full dynticks
      nohz: Disable the tick when irq resume in full dynticks CPU
      nohz: Re-evaluate the tick for the new task after a context switch
      nohz: Prepare to stop the tick on irq exit
      nohz: Implement full dynticks kick
      nohz: Re-evaluate the tick from the scheduler IPI
      sched: New helper to prevent from stopping the tick in full dynticks
      sched: Kick full dynticks CPU that have more than one task enqueued.
      perf: New helper to prevent full dynticks CPUs from stopping tick
      perf: Kick full dynticks CPU if events rotation is needed
      ...

commit fbd44a607a1a5019bc32c3615cead8c5ee8f89c9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 3 20:22:36 2013 +0200

    tick: Use zalloc_cpumask_var for allocating offstack cpumasks
    
    commit b352bc1cbc (tick: Convert broadcast cpu bitmaps to
    cpumask_var_t) broke CONFIG_CPUMASK_OFFSTACK in a very subtle way.
    
    Instead of allocating the cpumasks with zalloc_cpumask_var it uses
    alloc_cpumask_var, so we can get random data there, which of course
    confuses the logic completely and causes random failures.
    
    Reported-and-tested-by: Dave Jones <davej@redhat.com>
    Reported-and-tested-by: Yinghai Lu <yinghai@kernel.org>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1305032015060.2990@ionos
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 61d00a8cdf2f..d70cdc42c829 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -785,11 +785,11 @@ bool tick_broadcast_oneshot_available(void)
 
 void __init tick_broadcast_init(void)
 {
-	alloc_cpumask_var(&tick_broadcast_mask, GFP_NOWAIT);
-	alloc_cpumask_var(&tmpmask, GFP_NOWAIT);
+	zalloc_cpumask_var(&tick_broadcast_mask, GFP_NOWAIT);
+	zalloc_cpumask_var(&tmpmask, GFP_NOWAIT);
 #ifdef CONFIG_TICK_ONESHOT
-	alloc_cpumask_var(&tick_broadcast_oneshot_mask, GFP_NOWAIT);
-	alloc_cpumask_var(&tick_broadcast_pending_mask, GFP_NOWAIT);
-	alloc_cpumask_var(&tick_broadcast_force_mask, GFP_NOWAIT);
+	zalloc_cpumask_var(&tick_broadcast_oneshot_mask, GFP_NOWAIT);
+	zalloc_cpumask_var(&tick_broadcast_pending_mask, GFP_NOWAIT);
+	zalloc_cpumask_var(&tick_broadcast_force_mask, GFP_NOWAIT);
 #endif
 }

commit c032862fba51a3ca504752d3a25186b324c5ce83
Merge: fda76e074c77 8700c95adb03
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 2 17:37:49 2013 +0200

    Merge commit '8700c95adb03' into timers/nohz
    
    The full dynticks tree needs the latest RCU and sched
    upstream updates in order to fix some dependencies.
    
    Merge a common upstream merge point that has these
    updates.
    
    Conflicts:
            include/linux/perf_event.h
            kernel/rcutree.h
            kernel/rcutree_plugin.h
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit 6f7a05d7018de222e40ca003721037a530979974
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 11:45:53 2013 +0200

    clockevents: Set dummy handler on CPU_DEAD shutdown
    
    Vitaliy reported that a per cpu HPET timer interrupt crashes the
    system during hibernation. What happens is that the per cpu HPET timer
    gets shut down when the nonboot cpus are stopped. When the nonboot
    cpus are onlined again the HPET code sets up the MSI interrupt which
    fires before the clock event device is registered. The event handler
    is still set to hrtimer_interrupt, which then crashes the machine due
    to highres mode not being active.
    
    See http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=700333
    
    There is no real good way to avoid that in the HPET code. The HPET
    code alrady has a mechanism to detect spurious interrupts when event
    handler == NULL for a similar reason.
    
    We can handle that in the clockevent/tick layer and replace the
    previous functional handler with a dummy handler like we do in
    tick_setup_new_device().
    
    The original clockevents code did this in clockevents_exchange_device(),
    but that got removed by commit 7c1e76897 (clockevents: prevent
    clockevent event_handler ending up handler_noop) which forgot to fix
    it up in tick_shutdown(). Same issue with the broadcast device.
    
    Reported-by: Vitaliy Fillipov <vitalif@yourcmc.ru>
    Cc: Ben Hutchings <ben@decadent.org.uk>
    Cc: stable@vger.kernel.org
    Cc: 700333@bugs.debian.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 6e23fde83dbe..61d00a8cdf2f 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -66,6 +66,8 @@ static void tick_broadcast_start_periodic(struct clock_event_device *bc)
  */
 int tick_check_broadcast_device(struct clock_event_device *dev)
 {
+	struct clock_event_device *cur = tick_broadcast_device.evtdev;
+
 	if ((dev->features & CLOCK_EVT_FEAT_DUMMY) ||
 	    (tick_broadcast_device.evtdev &&
 	     tick_broadcast_device.evtdev->rating >= dev->rating) ||
@@ -73,6 +75,8 @@ int tick_check_broadcast_device(struct clock_event_device *dev)
 		return 0;
 
 	clockevents_exchange_device(tick_broadcast_device.evtdev, dev);
+	if (cur)
+		cur->event_handler = clockevents_handle_noop;
 	tick_broadcast_device.evtdev = dev;
 	if (!cpumask_empty(tick_broadcast_mask))
 		tick_broadcast_start_periodic(dev);

commit 6402c7dc2a19c19bd8cdc7d80878b850da418942
Merge: 77c675ba1883 60d509fa6a9c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 24 20:33:46 2013 +0200

    Merge branch 'linus' into timers/core
    Reason: Get upstream fixes before adding conflicting code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit c038c1c44179c80da6201f91ef354e48d5689617
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Wed Apr 17 10:26:06 2013 -0700

    clockevents: Switch into oneshot mode even if broadcast registered late
    
    tick_oneshot_notify() is used to notify a particular CPU to try
    to switch into oneshot mode after a oneshot capable tick device
    is registered and tick_clock_notify() is used to notify all CPUs
    to try to switch into oneshot mode after a high res clocksource
    is registered. There is one caveat; if the tick devices suffer
    from FEAT_C3_STOP we don't try to switch into oneshot mode unless
    we have a oneshot capable broadcast device already registered.
    
    If the broadcast device is registered after the tick devices that
    have FEAT_C3_STOP we'll never try to switch into oneshot mode
    again, causing us to be stuck in periodic mode forever. Avoid
    this scenario by calling tick_clock_notify() after we register
    the broadcast device so that we try to switch into oneshot mode
    on all CPUs one more time.
    
    [ tglx: Adopted to timers/core and added a comment ]
    
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Link: http://lkml.kernel.org/r/1366219566-29783-1-git-send-email-sboyd@codeaurora.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index d76d816afc5d..f8d2109ef0a2 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -75,6 +75,16 @@ int tick_check_broadcast_device(struct clock_event_device *dev)
 	tick_broadcast_device.evtdev = dev;
 	if (!cpumask_empty(tick_broadcast_mask))
 		tick_broadcast_start_periodic(dev);
+	/*
+	 * Inform all cpus about this. We might be in a situation
+	 * where we did not switch to oneshot mode because the per cpu
+	 * devices are affected by CLOCK_EVT_FEAT_C3STOP and the lack
+	 * of a oneshot capable broadcast device. Without that
+	 * notification the systems stays stuck in periodic mode
+	 * forever.
+	 */
+	if (dev->features & CLOCK_EVT_FEAT_ONESHOT)
+		tick_clock_notify();
 	return 1;
 }
 

commit c5bfece2d6129131b4ade985e63bc35ddf5868d4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Apr 12 16:45:34 2013 +0200

    nohz: Switch from "extended nohz" to "full nohz" based naming
    
    "Extended nohz" was used as a naming base for the full dynticks
    API and Kconfig symbols. It reflects the fact the system tries
    to stop the tick in more places than just idle.
    
    But that "extended" name is a bit opaque and vague. Rename it to
    "full" makes it clearer what the system tries to do under this
    config: try to shutdown the tick anytime it can. The various
    constraints that prevent that to happen shouldn't be considered
    as fundamental properties of this feature but rather technical
    issues that may be solved in the future.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 8a6875cc1879..a3a3123f6272 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -573,7 +573,7 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 		bc->event_handler = tick_handle_oneshot_broadcast;
 
 		/* Take the do_timer update */
-		if (!tick_nohz_extended_cpu(cpu))
+		if (!tick_nohz_full_cpu(cpu))
 			tick_do_timer_cpu = cpu;
 
 		/*

commit a382bf934449ddeb625167537ae81daa0211b477
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Dec 18 18:24:35 2012 +0100

    nohz: Assign timekeeping duty to a CPU outside the full dynticks range
    
    This way the full nohz CPUs can safely run with the tick
    stopped with a guarantee that somebody else is taking
    care of the jiffies and GTOD progression.
    
    Once the duty is attributed to a CPU, it won't change. Also that
    CPU can't enter into dyntick idle mode or be hot unplugged.
    
    This may later be improved from a power consumption POV. At
    least we should be able to share the duty amongst all CPUs
    outside the full dynticks range. Then the duty could even be
    shared with full dynticks CPUs when those can't stop their
    tick for any reason.
    
    But let's start with that very simple approach first.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [fix have_nohz_full_mask offcase]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 2fb8cb88df8d..8a6875cc1879 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -573,7 +573,8 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 		bc->event_handler = tick_handle_oneshot_broadcast;
 
 		/* Take the do_timer update */
-		tick_do_timer_cpu = cpu;
+		if (!tick_nohz_extended_cpu(cpu))
+			tick_do_timer_cpu = cpu;
 
 		/*
 		 * We must be careful here. There might be other CPUs

commit eaa907c546f76222227dfc41784b22588af1e3d7
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 6 11:18:36 2013 +0000

    tick: Provide a check for a forced broadcast pending
    
    On the CPU which gets woken along with the target CPU of the broadcast
    the following happens:
    
      deep_idle()
                            <-- spurious wakeup
      broadcast_exit()
        set forced bit
    
      enable interrupts
    
                            <-- Nothing happens
    
      disable interrupts
    
      broadcast_enter()
                            <-- Here we observe the forced bit is set
      deep_idle()
    
    Now after that the target CPU of the broadcast runs the broadcast
    handler and finds the other CPU in both the broadcast and the forced
    mask, sends the IPI and stuff gets back to normal.
    
    So it's not actually harmful, just more evidence for the theory, that
    hardware designers have access to very special drug supplies.
    
    Now there is no point in going back to deep idle just to wake up again
    right away via an IPI. Provide a check which allows the idle code to
    avoid the deep idle transition.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: LAK <linux-arm-kernel@lists.infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Arjan van de Veen <arjan@infradead.org>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Cc: Jason Liu <liu.h.jason@gmail.com>
    Link: http://lkml.kernel.org/r/20130306111537.565418308@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 2100aad6b5f2..d76d816afc5d 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -403,6 +403,18 @@ struct cpumask *tick_get_broadcast_oneshot_mask(void)
 	return tick_broadcast_oneshot_mask;
 }
 
+/*
+ * Called before going idle with interrupts disabled. Checks whether a
+ * broadcast event from the other core is about to happen. We detected
+ * that in tick_broadcast_oneshot_control(). The callsite can use this
+ * to avoid a deep idle transition as we are about to get the
+ * broadcast IPI right away.
+ */
+int tick_check_broadcast_expired(void)
+{
+	return cpumask_test_cpu(smp_processor_id(), tick_broadcast_force_mask);
+}
+
 /*
  * Set broadcast interrupt affinity
  */

commit 989dcb645ca715129c5a2b39102c8334a20d9615
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 6 11:18:35 2013 +0000

    tick: Handle broadcast wakeup of multiple cpus
    
    Some brilliant hardware implementations wake multiple cores when the
    broadcast timer fires. This leads to the following interesting
    problem:
    
    CPU0                            CPU1
    wakeup from idle                wakeup from idle
    
    leave broadcast mode            leave broadcast mode
     restart per cpu timer           restart per cpu timer
                                    go back to idle
    handle broadcast
     (empty mask)
                                    enter broadcast mode
                                    programm broadcast device
    enter broadcast mode
    programm broadcast device
    
    So what happens is that due to the forced reprogramming of the cpu
    local timer, we need to set a event in the future. Now if we manage to
    go back to idle before the timer fires, we switch off the timer and
    arm the broadcast device with an already expired time (covered by
    forced mode). So in the worst case we repeat the above ping pong
    forever.
    
    Unfortunately we have no information about what caused the wakeup, but
    we can check current time against the expiry time of the local cpu. If
    the local event is already in the past, we know that the broadcast
    timer is about to fire and send an IPI. So we mark ourself as an IPI
    target even if we left broadcast mode and avoid the reprogramming of
    the local cpu timer.
    
    This still leaves the possibility that a CPU which is not handling the
    broadcast interrupt is going to reach idle again before the IPI
    arrives. This can't be solved in the core code and will be handled in
    follow up patches.
    
    Reported-by: Jason Liu <liu.h.jason@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: LAK <linux-arm-kernel@lists.infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Arjan van de Veen <arjan@infradead.org>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Link: http://lkml.kernel.org/r/20130306111537.492045206@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 005c0ca81a32..2100aad6b5f2 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -393,6 +393,7 @@ int tick_resume_broadcast(void)
 
 static cpumask_var_t tick_broadcast_oneshot_mask;
 static cpumask_var_t tick_broadcast_pending_mask;
+static cpumask_var_t tick_broadcast_force_mask;
 
 /*
  * Exposed for debugging: see timer_list.c
@@ -483,6 +484,10 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 		}
 	}
 
+	/* Take care of enforced broadcast requests */
+	cpumask_or(tmpmask, tmpmask, tick_broadcast_force_mask);
+	cpumask_clear(tick_broadcast_force_mask);
+
 	/*
 	 * Wakeup the cpus which have an expired event.
 	 */
@@ -518,6 +523,7 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 	struct clock_event_device *bc, *dev;
 	struct tick_device *td;
 	unsigned long flags;
+	ktime_t now;
 	int cpu;
 
 	/*
@@ -545,7 +551,16 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 		WARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
-			if (dev->next_event.tv64 < bc->next_event.tv64)
+			/*
+			 * We only reprogram the broadcast timer if we
+			 * did not mark ourself in the force mask and
+			 * if the cpu local event is earlier than the
+			 * broadcast event. If the current CPU is in
+			 * the force mask, then we are going to be
+			 * woken by the IPI right away.
+			 */
+			if (!cpumask_test_cpu(cpu, tick_broadcast_force_mask) &&
+			    dev->next_event.tv64 < bc->next_event.tv64)
 				tick_broadcast_set_event(bc, cpu, dev->next_event, 1);
 		}
 	} else {
@@ -566,6 +581,47 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 				       tick_broadcast_pending_mask))
 				goto out;
 
+			/*
+			 * If the pending bit is not set, then we are
+			 * either the CPU handling the broadcast
+			 * interrupt or we got woken by something else.
+			 *
+			 * We are not longer in the broadcast mask, so
+			 * if the cpu local expiry time is already
+			 * reached, we would reprogram the cpu local
+			 * timer with an already expired event.
+			 *
+			 * This can lead to a ping-pong when we return
+			 * to idle and therefor rearm the broadcast
+			 * timer before the cpu local timer was able
+			 * to fire. This happens because the forced
+			 * reprogramming makes sure that the event
+			 * will happen in the future and depending on
+			 * the min_delta setting this might be far
+			 * enough out that the ping-pong starts.
+			 *
+			 * If the cpu local next_event has expired
+			 * then we know that the broadcast timer
+			 * next_event has expired as well and
+			 * broadcast is about to be handled. So we
+			 * avoid reprogramming and enforce that the
+			 * broadcast handler, which did not run yet,
+			 * will invoke the cpu local handler.
+			 *
+			 * We cannot call the handler directly from
+			 * here, because we might be in a NOHZ phase
+			 * and we did not go through the irq_enter()
+			 * nohz fixups.
+			 */
+			now = ktime_get();
+			if (dev->next_event.tv64 <= now.tv64) {
+				cpumask_set_cpu(cpu, tick_broadcast_force_mask);
+				goto out;
+			}
+			/*
+			 * We got woken by something else. Reprogram
+			 * the cpu local timer device.
+			 */
 			tick_program_event(dev->next_event, 1);
 		}
 	}
@@ -707,5 +763,6 @@ void __init tick_broadcast_init(void)
 #ifdef CONFIG_TICK_ONESHOT
 	alloc_cpumask_var(&tick_broadcast_oneshot_mask, GFP_NOWAIT);
 	alloc_cpumask_var(&tick_broadcast_pending_mask, GFP_NOWAIT);
+	alloc_cpumask_var(&tick_broadcast_force_mask, GFP_NOWAIT);
 #endif
 }

commit 26517f3e99248668315aee9460dcea21628cdd7f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 6 11:18:35 2013 +0000

    tick: Avoid programming the local cpu timer if broadcast pending
    
    If the local cpu timer stops in deep idle, we arm the broadcast device
    and get woken by an IPI. Now when we return from deep idle we reenable
    the local cpu timer unconditionally before handling the IPI. But
    that's a pointless exercise: the timer is already expired and the IPI
    is on the way. And it's an expensive exercise as we use the forced
    reprogramming mode so that we do not lose a timer event. This forced
    reprogramming will loop at least once in the retry.
    
    To avoid this reprogramming, we mark the cpu in a pending bit mask
    before we send the IPI. Now when the IPI target cpu wakes up, it will
    see the pending bit set and skip the reprogramming. The reprogramming
    of the cpu local timer will happen in the IPI handler which runs the
    cpu local timer interrupt function.
    
    Reported-by: Jason Liu <liu.h.jason@gmail.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: LAK <linux-arm-kernel@lists.infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Arjan van de Veen <arjan@infradead.org>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Link: http://lkml.kernel.org/r/20130306111537.431082074@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 380910db7157..005c0ca81a32 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -392,6 +392,7 @@ int tick_resume_broadcast(void)
 #ifdef CONFIG_TICK_ONESHOT
 
 static cpumask_var_t tick_broadcast_oneshot_mask;
+static cpumask_var_t tick_broadcast_pending_mask;
 
 /*
  * Exposed for debugging: see timer_list.c
@@ -470,6 +471,12 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 		td = &per_cpu(tick_cpu_device, cpu);
 		if (td->evtdev->next_event.tv64 <= now.tv64) {
 			cpumask_set_cpu(cpu, tmpmask);
+			/*
+			 * Mark the remote cpu in the pending mask, so
+			 * it can avoid reprogramming the cpu local
+			 * timer in tick_broadcast_oneshot_control().
+			 */
+			cpumask_set_cpu(cpu, tick_broadcast_pending_mask);
 		} else if (td->evtdev->next_event.tv64 < next_event.tv64) {
 			next_event.tv64 = td->evtdev->next_event.tv64;
 			next_cpu = cpu;
@@ -535,6 +542,7 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 	if (reason == CLOCK_EVT_NOTIFY_BROADCAST_ENTER) {
+		WARN_ON_ONCE(cpumask_test_cpu(cpu, tick_broadcast_pending_mask));
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
 			if (dev->next_event.tv64 < bc->next_event.tv64)
@@ -543,10 +551,25 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 	} else {
 		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
-			if (dev->next_event.tv64 != KTIME_MAX)
-				tick_program_event(dev->next_event, 1);
+			if (dev->next_event.tv64 == KTIME_MAX)
+				goto out;
+			/*
+			 * The cpu which was handling the broadcast
+			 * timer marked this cpu in the broadcast
+			 * pending mask and fired the broadcast
+			 * IPI. So we are going to handle the expired
+			 * event anyway via the broadcast IPI
+			 * handler. No need to reprogram the timer
+			 * with an already expired event.
+			 */
+			if (cpumask_test_and_clear_cpu(cpu,
+				       tick_broadcast_pending_mask))
+				goto out;
+
+			tick_program_event(dev->next_event, 1);
 		}
 	}
+out:
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
@@ -683,5 +706,6 @@ void __init tick_broadcast_init(void)
 	alloc_cpumask_var(&tmpmask, GFP_NOWAIT);
 #ifdef CONFIG_TICK_ONESHOT
 	alloc_cpumask_var(&tick_broadcast_oneshot_mask, GFP_NOWAIT);
+	alloc_cpumask_var(&tick_broadcast_pending_mask, GFP_NOWAIT);
 #endif
 }

commit a7dc19b8652c862d5b7c4d2339bd3c428bd29c4a
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Mar 7 15:09:24 2013 +0000

    clockevents: Don't allow dummy broadcast timers
    
    Currently tick_check_broadcast_device doesn't reject clock_event_devices
    with CLOCK_EVT_FEAT_DUMMY, and may select them in preference to real
    hardware if they have a higher rating value. In this situation, the
    dummy timer is responsible for broadcasting to itself, and the core
    clockevents code may attempt to call non-existent callbacks for
    programming the dummy, eventually leading to a panic.
    
    This patch makes tick_check_broadcast_device always reject dummy timers,
    preventing this problem.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Jon Medhurst (Tixy) <tixy@linaro.org>
    Cc: stable@vger.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 2fb8cb88df8d..7f32fe0e52cd 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -67,7 +67,8 @@ static void tick_broadcast_start_periodic(struct clock_event_device *bc)
  */
 int tick_check_broadcast_device(struct clock_event_device *dev)
 {
-	if ((tick_broadcast_device.evtdev &&
+	if ((dev->features & CLOCK_EVT_FEAT_DUMMY) ||
+	    (tick_broadcast_device.evtdev &&
 	     tick_broadcast_device.evtdev->rating >= dev->rating) ||
 	     (dev->features & CLOCK_EVT_FEAT_C3STOP))
 		return 0;

commit d2348fb6fdc6d671ad45b62db237f76c8c115603
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Sat Mar 2 11:10:11 2013 +0100

    tick: Dynamically set broadcast irq affinity
    
    When a cpu goes to a deep idle state where its local timer is
    shutdown, it notifies the time frame work to use the broadcast timer
    instead.  Unfortunately, the broadcast device could wake up any CPU,
    including an idle one which is not concerned by the wake up at all. So
    in the worst case an idle CPU will wake up to send an IPI to the CPU
    whose timer expired.
    
    Provide an opt-in feature CLOCK_EVT_FEAT_DYNIRQ which tells the core
    that is should set the interrupt affinity of the broadcast interrupt
    to the cpu which has the earliest expiry time. This avoids unnecessary
    spurious wakeups and IPIs.
    
    [ tglx: Adopted to cpumask rework, silenced an uninitialized warning,
      massaged changelog ]
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: viresh.kumar@linaro.org
    Cc: jacob.jun.pan@linux.intel.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: santosh.shilimkar@ti.com
    Cc: linaro-kernel@lists.linaro.org
    Cc: patches@linaro.org
    Cc: rickard.andersson@stericsson.com
    Cc: vincent.guittot@linaro.org
    Cc: linus.walleij@stericsson.com
    Cc: john.stultz@linaro.org
    Link: http://lkml.kernel.org/r/1362219013-18173-3-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 70dd98ce18d7..380910db7157 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -401,13 +401,34 @@ struct cpumask *tick_get_broadcast_oneshot_mask(void)
 	return tick_broadcast_oneshot_mask;
 }
 
-static int tick_broadcast_set_event(struct clock_event_device *bc,
+/*
+ * Set broadcast interrupt affinity
+ */
+static void tick_broadcast_set_affinity(struct clock_event_device *bc,
+					const struct cpumask *cpumask)
+{
+	if (!(bc->features & CLOCK_EVT_FEAT_DYNIRQ))
+		return;
+
+	if (cpumask_equal(bc->cpumask, cpumask))
+		return;
+
+	bc->cpumask = cpumask;
+	irq_set_affinity(bc->irq, bc->cpumask);
+}
+
+static int tick_broadcast_set_event(struct clock_event_device *bc, int cpu,
 				    ktime_t expires, int force)
 {
+	int ret;
+
 	if (bc->mode != CLOCK_EVT_MODE_ONESHOT)
 		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
 
-	return clockevents_program_event(bc, expires, force);
+	ret = clockevents_program_event(bc, expires, force);
+	if (!ret)
+		tick_broadcast_set_affinity(bc, cpumask_of(cpu));
+	return ret;
 }
 
 int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
@@ -436,7 +457,7 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 {
 	struct tick_device *td;
 	ktime_t now, next_event;
-	int cpu;
+	int cpu, next_cpu = 0;
 
 	raw_spin_lock(&tick_broadcast_lock);
 again:
@@ -447,10 +468,12 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	/* Find all expired events */
 	for_each_cpu(cpu, tick_broadcast_oneshot_mask) {
 		td = &per_cpu(tick_cpu_device, cpu);
-		if (td->evtdev->next_event.tv64 <= now.tv64)
+		if (td->evtdev->next_event.tv64 <= now.tv64) {
 			cpumask_set_cpu(cpu, tmpmask);
-		else if (td->evtdev->next_event.tv64 < next_event.tv64)
+		} else if (td->evtdev->next_event.tv64 < next_event.tv64) {
 			next_event.tv64 = td->evtdev->next_event.tv64;
+			next_cpu = cpu;
+		}
 	}
 
 	/*
@@ -473,7 +496,7 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 		 * Rearm the broadcast device. If event expired,
 		 * repeat the above
 		 */
-		if (tick_broadcast_set_event(dev, next_event, 0))
+		if (tick_broadcast_set_event(dev, next_cpu, next_event, 0))
 			goto again;
 	}
 	raw_spin_unlock(&tick_broadcast_lock);
@@ -515,7 +538,7 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
 			if (dev->next_event.tv64 < bc->next_event.tv64)
-				tick_broadcast_set_event(bc, dev->next_event, 1);
+				tick_broadcast_set_event(bc, cpu, dev->next_event, 1);
 		}
 	} else {
 		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {
@@ -581,7 +604,7 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 			clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
 			tick_broadcast_init_next_event(tmpmask,
 						       tick_next_period);
-			tick_broadcast_set_event(bc, tick_next_period, 1);
+			tick_broadcast_set_event(bc, cpu, tick_next_period, 1);
 		} else
 			bc->next_event.tv64 = KTIME_MAX;
 	} else {

commit f9ae39d04ccdec8d8ecf532191b7056c279a22c0
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Sat Mar 2 11:10:10 2013 +0100

    tick: Pass broadcast device to tick_broadcast_set_event()
    
    Pass the broadcast timer to tick_broadcast_set_event() instead of
    reevaluating tick_broadcast_device.evtdev.
    
    [ tglx: Massaged changelog ]
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: viresh.kumar@linaro.org
    Cc: jacob.jun.pan@linux.intel.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: santosh.shilimkar@ti.com
    Cc: linaro-kernel@lists.linaro.org
    Cc: patches@linaro.org
    Cc: rickard.andersson@stericsson.com
    Cc: vincent.guittot@linaro.org
    Cc: linus.walleij@stericsson.com
    Cc: john.stultz@linaro.org
    Link: http://lkml.kernel.org/r/1362219013-18173-2-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 35b887517766..70dd98ce18d7 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -401,10 +401,9 @@ struct cpumask *tick_get_broadcast_oneshot_mask(void)
 	return tick_broadcast_oneshot_mask;
 }
 
-static int tick_broadcast_set_event(ktime_t expires, int force)
+static int tick_broadcast_set_event(struct clock_event_device *bc,
+				    ktime_t expires, int force)
 {
-	struct clock_event_device *bc = tick_broadcast_device.evtdev;
-
 	if (bc->mode != CLOCK_EVT_MODE_ONESHOT)
 		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
 
@@ -474,7 +473,7 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 		 * Rearm the broadcast device. If event expired,
 		 * repeat the above
 		 */
-		if (tick_broadcast_set_event(next_event, 0))
+		if (tick_broadcast_set_event(dev, next_event, 0))
 			goto again;
 	}
 	raw_spin_unlock(&tick_broadcast_lock);
@@ -516,7 +515,7 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
 			if (dev->next_event.tv64 < bc->next_event.tv64)
-				tick_broadcast_set_event(dev->next_event, 1);
+				tick_broadcast_set_event(bc, dev->next_event, 1);
 		}
 	} else {
 		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {
@@ -582,7 +581,7 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 			clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
 			tick_broadcast_init_next_event(tmpmask,
 						       tick_next_period);
-			tick_broadcast_set_event(tick_next_period, 1);
+			tick_broadcast_set_event(bc, tick_next_period, 1);
 		} else
 			bc->next_event.tv64 = KTIME_MAX;
 	} else {

commit b352bc1cbc29134a356b5c16ee2281807a7b984e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 5 14:25:32 2013 +0100

    tick: Convert broadcast cpu bitmaps to cpumask_var_t
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20130306111537.366394000@linutronix.de
    Cc: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 2fb8cb88df8d..35b887517766 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -28,9 +28,8 @@
  */
 
 static struct tick_device tick_broadcast_device;
-/* FIXME: Use cpumask_var_t. */
-static DECLARE_BITMAP(tick_broadcast_mask, NR_CPUS);
-static DECLARE_BITMAP(tmpmask, NR_CPUS);
+static cpumask_var_t tick_broadcast_mask;
+static cpumask_var_t tmpmask;
 static DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
 static int tick_broadcast_force;
 
@@ -50,7 +49,7 @@ struct tick_device *tick_get_broadcast_device(void)
 
 struct cpumask *tick_get_broadcast_mask(void)
 {
-	return to_cpumask(tick_broadcast_mask);
+	return tick_broadcast_mask;
 }
 
 /*
@@ -74,7 +73,7 @@ int tick_check_broadcast_device(struct clock_event_device *dev)
 
 	clockevents_exchange_device(tick_broadcast_device.evtdev, dev);
 	tick_broadcast_device.evtdev = dev;
-	if (!cpumask_empty(tick_get_broadcast_mask()))
+	if (!cpumask_empty(tick_broadcast_mask))
 		tick_broadcast_start_periodic(dev);
 	return 1;
 }
@@ -123,7 +122,7 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 	if (!tick_device_is_functional(dev)) {
 		dev->event_handler = tick_handle_periodic;
 		tick_device_setup_broadcast_func(dev);
-		cpumask_set_cpu(cpu, tick_get_broadcast_mask());
+		cpumask_set_cpu(cpu, tick_broadcast_mask);
 		tick_broadcast_start_periodic(tick_broadcast_device.evtdev);
 		ret = 1;
 	} else {
@@ -134,7 +133,7 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 		 */
 		if (!(dev->features & CLOCK_EVT_FEAT_C3STOP)) {
 			int cpu = smp_processor_id();
-			cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
+			cpumask_clear_cpu(cpu, tick_broadcast_mask);
 			tick_broadcast_clear_oneshot(cpu);
 		} else {
 			tick_device_setup_broadcast_func(dev);
@@ -198,9 +197,8 @@ static void tick_do_periodic_broadcast(void)
 {
 	raw_spin_lock(&tick_broadcast_lock);
 
-	cpumask_and(to_cpumask(tmpmask),
-		    cpu_online_mask, tick_get_broadcast_mask());
-	tick_do_broadcast(to_cpumask(tmpmask));
+	cpumask_and(tmpmask, cpu_online_mask, tick_broadcast_mask);
+	tick_do_broadcast(tmpmask);
 
 	raw_spin_unlock(&tick_broadcast_lock);
 }
@@ -263,13 +261,12 @@ static void tick_do_broadcast_on_off(unsigned long *reason)
 	if (!tick_device_is_functional(dev))
 		goto out;
 
-	bc_stopped = cpumask_empty(tick_get_broadcast_mask());
+	bc_stopped = cpumask_empty(tick_broadcast_mask);
 
 	switch (*reason) {
 	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
 	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
-		if (!cpumask_test_cpu(cpu, tick_get_broadcast_mask())) {
-			cpumask_set_cpu(cpu, tick_get_broadcast_mask());
+		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_mask)) {
 			if (tick_broadcast_device.mode ==
 			    TICKDEV_MODE_PERIODIC)
 				clockevents_shutdown(dev);
@@ -279,8 +276,7 @@ static void tick_do_broadcast_on_off(unsigned long *reason)
 		break;
 	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
 		if (!tick_broadcast_force &&
-		    cpumask_test_cpu(cpu, tick_get_broadcast_mask())) {
-			cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
+		    cpumask_test_and_clear_cpu(cpu, tick_broadcast_mask)) {
 			if (tick_broadcast_device.mode ==
 			    TICKDEV_MODE_PERIODIC)
 				tick_setup_periodic(dev, 0);
@@ -288,7 +284,7 @@ static void tick_do_broadcast_on_off(unsigned long *reason)
 		break;
 	}
 
-	if (cpumask_empty(tick_get_broadcast_mask())) {
+	if (cpumask_empty(tick_broadcast_mask)) {
 		if (!bc_stopped)
 			clockevents_shutdown(bc);
 	} else if (bc_stopped) {
@@ -337,10 +333,10 @@ void tick_shutdown_broadcast(unsigned int *cpup)
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	bc = tick_broadcast_device.evtdev;
-	cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
+	cpumask_clear_cpu(cpu, tick_broadcast_mask);
 
 	if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC) {
-		if (bc && cpumask_empty(tick_get_broadcast_mask()))
+		if (bc && cpumask_empty(tick_broadcast_mask))
 			clockevents_shutdown(bc);
 	}
 
@@ -376,13 +372,13 @@ int tick_resume_broadcast(void)
 
 		switch (tick_broadcast_device.mode) {
 		case TICKDEV_MODE_PERIODIC:
-			if (!cpumask_empty(tick_get_broadcast_mask()))
+			if (!cpumask_empty(tick_broadcast_mask))
 				tick_broadcast_start_periodic(bc);
 			broadcast = cpumask_test_cpu(smp_processor_id(),
-						     tick_get_broadcast_mask());
+						     tick_broadcast_mask);
 			break;
 		case TICKDEV_MODE_ONESHOT:
-			if (!cpumask_empty(tick_get_broadcast_mask()))
+			if (!cpumask_empty(tick_broadcast_mask))
 				broadcast = tick_resume_broadcast_oneshot(bc);
 			break;
 		}
@@ -395,15 +391,14 @@ int tick_resume_broadcast(void)
 
 #ifdef CONFIG_TICK_ONESHOT
 
-/* FIXME: use cpumask_var_t. */
-static DECLARE_BITMAP(tick_broadcast_oneshot_mask, NR_CPUS);
+static cpumask_var_t tick_broadcast_oneshot_mask;
 
 /*
  * Exposed for debugging: see timer_list.c
  */
 struct cpumask *tick_get_broadcast_oneshot_mask(void)
 {
-	return to_cpumask(tick_broadcast_oneshot_mask);
+	return tick_broadcast_oneshot_mask;
 }
 
 static int tick_broadcast_set_event(ktime_t expires, int force)
@@ -428,7 +423,7 @@ int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
  */
 void tick_check_oneshot_broadcast(int cpu)
 {
-	if (cpumask_test_cpu(cpu, to_cpumask(tick_broadcast_oneshot_mask))) {
+	if (cpumask_test_cpu(cpu, tick_broadcast_oneshot_mask)) {
 		struct tick_device *td = &per_cpu(tick_cpu_device, cpu);
 
 		clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_ONESHOT);
@@ -448,13 +443,13 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 again:
 	dev->next_event.tv64 = KTIME_MAX;
 	next_event.tv64 = KTIME_MAX;
-	cpumask_clear(to_cpumask(tmpmask));
+	cpumask_clear(tmpmask);
 	now = ktime_get();
 	/* Find all expired events */
-	for_each_cpu(cpu, tick_get_broadcast_oneshot_mask()) {
+	for_each_cpu(cpu, tick_broadcast_oneshot_mask) {
 		td = &per_cpu(tick_cpu_device, cpu);
 		if (td->evtdev->next_event.tv64 <= now.tv64)
-			cpumask_set_cpu(cpu, to_cpumask(tmpmask));
+			cpumask_set_cpu(cpu, tmpmask);
 		else if (td->evtdev->next_event.tv64 < next_event.tv64)
 			next_event.tv64 = td->evtdev->next_event.tv64;
 	}
@@ -462,7 +457,7 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	/*
 	 * Wakeup the cpus which have an expired event.
 	 */
-	tick_do_broadcast(to_cpumask(tmpmask));
+	tick_do_broadcast(tmpmask);
 
 	/*
 	 * Two reasons for reprogram:
@@ -518,16 +513,13 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 	if (reason == CLOCK_EVT_NOTIFY_BROADCAST_ENTER) {
-		if (!cpumask_test_cpu(cpu, tick_get_broadcast_oneshot_mask())) {
-			cpumask_set_cpu(cpu, tick_get_broadcast_oneshot_mask());
+		if (!cpumask_test_and_set_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
 			if (dev->next_event.tv64 < bc->next_event.tv64)
 				tick_broadcast_set_event(dev->next_event, 1);
 		}
 	} else {
-		if (cpumask_test_cpu(cpu, tick_get_broadcast_oneshot_mask())) {
-			cpumask_clear_cpu(cpu,
-					  tick_get_broadcast_oneshot_mask());
+		if (cpumask_test_and_clear_cpu(cpu, tick_broadcast_oneshot_mask)) {
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
 			if (dev->next_event.tv64 != KTIME_MAX)
 				tick_program_event(dev->next_event, 1);
@@ -543,7 +535,7 @@ void tick_broadcast_oneshot_control(unsigned long reason)
  */
 static void tick_broadcast_clear_oneshot(int cpu)
 {
-	cpumask_clear_cpu(cpu, tick_get_broadcast_oneshot_mask());
+	cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
 }
 
 static void tick_broadcast_init_next_event(struct cpumask *mask,
@@ -581,15 +573,14 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 		 * oneshot_mask bits for those and program the
 		 * broadcast device to fire.
 		 */
-		cpumask_copy(to_cpumask(tmpmask), tick_get_broadcast_mask());
-		cpumask_clear_cpu(cpu, to_cpumask(tmpmask));
-		cpumask_or(tick_get_broadcast_oneshot_mask(),
-			   tick_get_broadcast_oneshot_mask(),
-			   to_cpumask(tmpmask));
+		cpumask_copy(tmpmask, tick_broadcast_mask);
+		cpumask_clear_cpu(cpu, tmpmask);
+		cpumask_or(tick_broadcast_oneshot_mask,
+			   tick_broadcast_oneshot_mask, tmpmask);
 
-		if (was_periodic && !cpumask_empty(to_cpumask(tmpmask))) {
+		if (was_periodic && !cpumask_empty(tmpmask)) {
 			clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
-			tick_broadcast_init_next_event(to_cpumask(tmpmask),
+			tick_broadcast_init_next_event(tmpmask,
 						       tick_next_period);
 			tick_broadcast_set_event(tick_next_period, 1);
 		} else
@@ -639,7 +630,7 @@ void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
 	 * Clear the broadcast mask flag for the dead cpu, but do not
 	 * stop the broadcast device!
 	 */
-	cpumask_clear_cpu(cpu, tick_get_broadcast_oneshot_mask());
+	cpumask_clear_cpu(cpu, tick_broadcast_oneshot_mask);
 
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
@@ -663,3 +654,12 @@ bool tick_broadcast_oneshot_available(void)
 }
 
 #endif
+
+void __init tick_broadcast_init(void)
+{
+	alloc_cpumask_var(&tick_broadcast_mask, GFP_NOWAIT);
+	alloc_cpumask_var(&tmpmask, GFP_NOWAIT);
+#ifdef CONFIG_TICK_ONESHOT
+	alloc_cpumask_var(&tick_broadcast_oneshot_mask, GFP_NOWAIT);
+#endif
+}

commit 5d1d9a29bc0772abee765f09513779a2ef0ebbfd
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Feb 8 15:24:07 2013 +0000

    clockevents: Fix generic broadcast for FEAT_C3STOP
    
    Commit 12ad100046: "clockevents: Add generic timer broadcast function"
    made tick_device_uses_broadcast set up the generic broadcast function
    for dummy devices (where !tick_device_is_functional(dev)), but neglected
    to set up the broadcast function for devices that stop in low power
    states (with the CLOCK_EVT_FEAT_C3STOP flag).
    
    When these devices enter low power states they will not have the generic
    broadcast function assigned, and will bring down the system when an
    attempt is made to broadcast to them.
    
    This patch ensures that the broadcast function is also assigned for
    devices which require broadcast in low power states.
    
    Reported-by: Stephen Warren <swarren@nvidia.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Stephen Warren <swarren@nvidia.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: nico@linaro.org
    Cc: Marc.Zyngier@arm.com
    Cc: Will.Deacon@arm.com
    Cc: santosh.shilimkar@ti.com
    Cc: john.stultz@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index f726537d24eb..2fb8cb88df8d 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -92,6 +92,17 @@ static void err_broadcast(const struct cpumask *mask)
 	pr_crit_once("Failed to broadcast timer tick. Some CPUs may be unresponsive.\n");
 }
 
+static void tick_device_setup_broadcast_func(struct clock_event_device *dev)
+{
+	if (!dev->broadcast)
+		dev->broadcast = tick_broadcast;
+	if (!dev->broadcast) {
+		pr_warn_once("%s depends on broadcast, but no broadcast function available\n",
+			     dev->name);
+		dev->broadcast = err_broadcast;
+	}
+}
+
 /*
  * Check, if the device is disfunctional and a place holder, which
  * needs to be handled by the broadcast device.
@@ -111,13 +122,7 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 	 */
 	if (!tick_device_is_functional(dev)) {
 		dev->event_handler = tick_handle_periodic;
-		if (!dev->broadcast)
-			dev->broadcast = tick_broadcast;
-		if (!dev->broadcast) {
-			pr_warn_once("%s depends on broadcast, but no broadcast function available\n",
-				     dev->name);
-			dev->broadcast = err_broadcast;
-		}
+		tick_device_setup_broadcast_func(dev);
 		cpumask_set_cpu(cpu, tick_get_broadcast_mask());
 		tick_broadcast_start_periodic(tick_broadcast_device.evtdev);
 		ret = 1;
@@ -129,9 +134,10 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 		 */
 		if (!(dev->features & CLOCK_EVT_FEAT_C3STOP)) {
 			int cpu = smp_processor_id();
-
 			cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
 			tick_broadcast_clear_oneshot(cpu);
+		} else {
+			tick_device_setup_broadcast_func(dev);
 		}
 	}
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);

commit 12ad10004645d38356b14d1fbba379c523a61916
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jan 14 17:05:22 2013 +0000

    clockevents: Add generic timer broadcast function
    
    Currently, the timer broadcast mechanism is defined by a function
    pointer on struct clock_event_device. As the fundamental mechanism for
    broadcast is architecture-specific, this means that clock_event_device
    drivers cannot be shared across multiple architectures.
    
    This patch adds an (optional) architecture-specific function for timer
    tick broadcast, allowing drivers which may require broadcast
    functionality to be shared across multiple architectures.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: nico@linaro.org
    Cc: Will.Deacon@arm.com
    Cc: Marc.Zyngier@arm.com
    Cc: john.stultz@linaro.org
    Link: http://lkml.kernel.org/r/1358183124-28461-3-git-send-email-mark.rutland@arm.com
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 7cc81c57eb31..f726537d24eb 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -18,6 +18,7 @@
 #include <linux/percpu.h>
 #include <linux/profile.h>
 #include <linux/sched.h>
+#include <linux/smp.h>
 
 #include "tick-internal.h"
 
@@ -86,6 +87,11 @@ int tick_is_broadcast_device(struct clock_event_device *dev)
 	return (dev && tick_broadcast_device.evtdev == dev);
 }
 
+static void err_broadcast(const struct cpumask *mask)
+{
+	pr_crit_once("Failed to broadcast timer tick. Some CPUs may be unresponsive.\n");
+}
+
 /*
  * Check, if the device is disfunctional and a place holder, which
  * needs to be handled by the broadcast device.
@@ -105,6 +111,13 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 	 */
 	if (!tick_device_is_functional(dev)) {
 		dev->event_handler = tick_handle_periodic;
+		if (!dev->broadcast)
+			dev->broadcast = tick_broadcast;
+		if (!dev->broadcast) {
+			pr_warn_once("%s depends on broadcast, but no broadcast function available\n",
+				     dev->name);
+			dev->broadcast = err_broadcast;
+		}
 		cpumask_set_cpu(cpu, tick_get_broadcast_mask());
 		tick_broadcast_start_periodic(tick_broadcast_device.evtdev);
 		ret = 1;

commit 12572dbb53638c6e454ef831c8fee7de3df24389
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jan 14 17:05:21 2013 +0000

    clockevents: Add generic timer broadcast receiver
    
    Currently the broadcast mechanism used for timers is abstracted by a
    function pointer on struct clock_event_device. As the fundamental
    mechanism for broadcast is architecture-specific, this ties each
    clock_event_device driver to a single architecture, even where the
    driver is otherwise generic.
    
    This patch adds a standard path for the receipt of timer broadcasts, so
    drivers and/or architecture backends need not manage redundant lists of
    timers for the purpose of routing broadcast timer ticks.
    
    [tglx: Made the implementation depend on the config switch as well ]
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: nico@linaro.org
    Cc: Will.Deacon@arm.com
    Cc: Marc.Zyngier@arm.com
    Cc: john.stultz@linaro.org
    Link: http://lkml.kernel.org/r/1358183124-28461-2-git-send-email-mark.rutland@arm.com
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index f113755695e2..7cc81c57eb31 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -125,6 +125,23 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 	return ret;
 }
 
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
+int tick_receive_broadcast(void)
+{
+	struct tick_device *td = this_cpu_ptr(&tick_cpu_device);
+	struct clock_event_device *evt = td->evtdev;
+
+	if (!evt)
+		return -ENODEV;
+
+	if (!evt->event_handler)
+		return -EINVAL;
+
+	evt->event_handler(evt);
+	return 0;
+}
+#endif
+
 /*
  * Broadcast the event to the cpus, which are set in the mask (mangled).
  */

commit a6371f80230eaaafd7eef7efeedaa9509bdc982d
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed Apr 18 19:27:39 2012 -0700

    tick: Fix the spurious broadcast timer ticks after resume
    
    During resume, tick_resume_broadcast() programs the broadcast timer in
    oneshot mode unconditionally. On the platforms where broadcast timer
    is not really required, this will generate spurious broadcast timer
    ticks upon resume. For example, on the always running apic timer
    platforms with HPET, I see spurious hpet tick once every ~5minutes
    (which is the 32-bit hpet counter wraparound time).
    
    Similar to boot time, during resume make the oneshot mode setting of
    the broadcast clock event device conditional on the state of active
    broadcast users.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: svenjoac@gmx.de
    Cc: torvalds@linux-foundation.org
    Cc: rjw@sisk.pl
    Link: http://lkml.kernel.org/r/1334802459.28674.209.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 029531f3818c..f113755695e2 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -346,7 +346,8 @@ int tick_resume_broadcast(void)
 						     tick_get_broadcast_mask());
 			break;
 		case TICKDEV_MODE_ONESHOT:
-			broadcast = tick_resume_broadcast_oneshot(bc);
+			if (!cpumask_empty(tick_get_broadcast_mask()))
+				broadcast = tick_resume_broadcast_oneshot(bc);
 			break;
 		}
 	}

commit b9a6a23566960d0dd3f51e2e68b472cd61911078
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 18 17:31:58 2012 +0200

    tick: Ensure that the broadcast device is initialized
    
    Santosh found another trap when we avoid to initialize the broadcast
    device in the switch_to_oneshot code. The broadcast device might be
    still in SHUTDOWN state when we actually need to use it. That
    obviously breaks, as set_next_event() is called on a shutdown
    device. This did not break on x86, but Suresh analyzed it:
    
    From the review, most likely on Sven's system we are force enabling
    the hpet using the pci quirk's method very late. And in this case,
    hpet_clockevent (which will be global_clock_event) handler can be
    null, specifically as this platform might not be using deeper c-states
    and using the reliable APIC timer.
    
    Prior to commit 'fa4da365bc7772c', that handler will be set to
    'tick_handle_oneshot_broadcast' when we switch the broadcast timer to
    oneshot mode, even though we don't use it. Post commit
    'fa4da365bc7772c', we stopped switching the broadcast mode to oneshot
    as this is not really needed and his platform's global_clock_event's
    handler will remain null. While on my SNB laptop, same is set to
    'clockevents_handle_noop' because hpet gets enabled very early. (noop
    handler on my platform set when the early enabled hpet timer gets
    replaced by the lapic timer).
    
    But the commit 'fa4da365bc7772c' tracked the broadcast timer mode in
    the SW as oneshot, even though it didn't touch the HW timer. During
    resume however, tick_resume_broadcast() saw the SW broadcast mode as
    oneshot and actually programmed the broadcast device also into oneshot
    mode. So this triggered the null pointer de-reference after the hpet
    wraps around and depending on what the hpet counter is set to. On the
    normal platforms where hpet gets enabled early we should be seeing a
    spurious interrupt (in my SNB laptop I see one spurious interrupt
    after around 5 minutes ;) which is 32-bit hpet counter wraparound
    time), but that's a separate issue.
    
    Enforce the mode setting when trying to set an event.
    
    Reported-and-tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: torvalds@linux-foundation.org
    Cc: svenjoac@gmx.de
    Cc: rjw@sisk.pl
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1204181723350.2542@ionos

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 119aca5c6845..029531f3818c 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -373,6 +373,9 @@ static int tick_broadcast_set_event(ktime_t expires, int force)
 {
 	struct clock_event_device *bc = tick_broadcast_device.evtdev;
 
+	if (bc->mode != CLOCK_EVT_MODE_ONESHOT)
+		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
+
 	return clockevents_program_event(bc, expires, force);
 }
 

commit b435092f70ec5ebbfb6d075d5bf3c631b49a51de
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 18 12:08:23 2012 +0200

    tick: Fix oneshot broadcast setup really
    
    Sven Joachim reported, that suspend/resume on rc3 trips over a NULL
    pointer dereference. Linus spotted the clockevent handler being NULL.
    
    commit fa4da365b(clockevents: tTack broadcast device mode change in
    tick_broadcast_switch_to_oneshot()) tried to fix a problem with the
    broadcast device setup, which was introduced in commit 77b0d60c5(
    clockevents: Leave the broadcast device in shutdown mode when not
    needed).
    
    The initial commit avoided to set up the broadcast device when no
    broadcast request bits were set, but that left the broadcast device
    disfunctional. In consequence deep idle states which need the
    broadcast device were not woken up.
    
    commit fa4da365b tried to fix that by initializing the state of the
    broadcast facility, but that missed the fact, that nothing initializes
    the event handler and some other state of the underlying clock event
    device.
    
    The fix is to revert both commits and make only the mode setting of
    the clock event device conditional on the state of active broadcast
    users.
    
    That initializes everything except the low level device mode, but this
    happens when the broadcast functionality is invoked by deep idle.
    
    Reported-and-tested-by: Sven Joachim <svenjoac@gmx.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.02.1204181205540.2542@ionos

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index bf57abdc7bd0..119aca5c6845 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -531,7 +531,6 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 		int was_periodic = bc->mode == CLOCK_EVT_MODE_PERIODIC;
 
 		bc->event_handler = tick_handle_oneshot_broadcast;
-		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
 
 		/* Take the do_timer update */
 		tick_do_timer_cpu = cpu;
@@ -549,6 +548,7 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 			   to_cpumask(tmpmask));
 
 		if (was_periodic && !cpumask_empty(to_cpumask(tmpmask))) {
+			clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
 			tick_broadcast_init_next_event(to_cpumask(tmpmask),
 						       tick_next_period);
 			tick_broadcast_set_event(tick_next_period, 1);
@@ -577,15 +577,10 @@ void tick_broadcast_switch_to_oneshot(void)
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	tick_broadcast_device.mode = TICKDEV_MODE_ONESHOT;
-
-	if (cpumask_empty(tick_get_broadcast_mask()))
-		goto end;
-
 	bc = tick_broadcast_device.evtdev;
 	if (bc)
 		tick_broadcast_setup_oneshot(bc);
 
-end:
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 

commit fa4da365bc7772c2cd6d5405bdf151612455f957
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Apr 9 15:41:44 2012 -0700

    clockevents: tTack broadcast device mode change in tick_broadcast_switch_to_oneshot()
    
    In the commit 77b0d60c5adf39c74039e2142a1d3cd1e4d53799,
    "clockevents: Leave the broadcast device in shutdown mode when not needed",
    we were bailing out too quickly in tick_broadcast_switch_to_oneshot(),
    with out tracking the broadcast device mode change to 'TICKDEV_MODE_ONESHOT'.
    
    This breaks the platforms which need broadcast device oneshot services during
    deep idle states. tick_broadcast_oneshot_control() thinks that it is
    in periodic mode and fails to take proper decisions based on the
    CLOCK_EVT_NOTIFY_BROADCAST_[ENTER, EXIT] notifications during deep
    idle entry/exit.
    
    Fix this by tracking the broadcast device mode as 'TICKDEV_MODE_ONESHOT',
    before leaving the broadcast HW device in shutdown mode if there are no active
    requests for the moment.
    
    Reported-and-tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: johnstul@us.ibm.com
    Link: http://lkml.kernel.org/r/1334011304.12400.81.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index e883f57a3cd3..bf57abdc7bd0 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -575,10 +575,12 @@ void tick_broadcast_switch_to_oneshot(void)
 	unsigned long flags;
 
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
+
+	tick_broadcast_device.mode = TICKDEV_MODE_ONESHOT;
+
 	if (cpumask_empty(tick_get_broadcast_mask()))
 		goto end;
 
-	tick_broadcast_device.mode = TICKDEV_MODE_ONESHOT;
 	bc = tick_broadcast_device.evtdev;
 	if (bc)
 		tick_broadcast_setup_oneshot(bc);

commit 77b0d60c5adf39c74039e2142a1d3cd1e4d53799
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Fri Nov 4 17:18:21 2011 -0700

    clockevents: Leave the broadcast device in shutdown mode when not needed
    
    Platforms with Always Running APIC Timer doesn't use the broadcast timer
    but the kernel is leaving the broadcast timer (HPET in this case)
    in oneshot mode.
    
    On these platforms, before the switch to oneshot mode, broadcast device is
    actually in shutdown mode. Code checks for empty tick_broadcast_mask and
    avoids going into the periodic mode.
    
    During switch to oneshot mode, add the same tick_broadcast_mask checks in the
    tick_broadcast_switch_to_oneshot() and avoid the broadcast device going into
    the oneshot mode.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: venki@google.com
    Link: http://lkml.kernel.org/r/1320452301.15071.16.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index fd4a7b1625a2..e883f57a3cd3 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -575,11 +575,15 @@ void tick_broadcast_switch_to_oneshot(void)
 	unsigned long flags;
 
 	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
+	if (cpumask_empty(tick_get_broadcast_mask()))
+		goto end;
 
 	tick_broadcast_device.mode = TICKDEV_MODE_ONESHOT;
 	bc = tick_broadcast_device.evtdev;
 	if (bc)
 		tick_broadcast_setup_oneshot(bc);
+
+end:
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 

commit c1be84309c58b1e7c6d626e28fba41a22b364c3d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Dec 2 12:34:16 2011 +0100

    tick-broadcast: Stop active broadcast device when replacing it
    
    When a better rated broadcast device is installed, then the current
    active device is not disabled, which results in two running broadcast
    devices.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index f954282d9a82..fd4a7b1625a2 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -71,7 +71,7 @@ int tick_check_broadcast_device(struct clock_event_device *dev)
 	     (dev->features & CLOCK_EVT_FEAT_C3STOP))
 		return 0;
 
-	clockevents_exchange_device(NULL, dev);
+	clockevents_exchange_device(tick_broadcast_device.evtdev, dev);
 	tick_broadcast_device.evtdev = dev;
 	if (!cpumask_empty(tick_get_broadcast_mask()))
 		tick_broadcast_start_periodic(dev);

commit d1748302f70be7469809809283fe164156a34231
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Aug 23 15:29:42 2011 +0200

    clockevents: Make minimum delay adjustments configurable
    
    The automatic increase of the min_delta_ns of a clockevents device
    should be done in the clockevents code as the minimum delay is an
    attribute of the clockevents device.
    
    In addition not all architectures want the automatic adjustment, on a
    massively virtualized system it can happen that the programming of a
    clock event fails several times in a row because the virtual cpu has
    been rescheduled quickly enough. In that case the minimum delay will
    erroneously be increased with no way back. The new config symbol
    GENERIC_CLOCKEVENTS_MIN_ADJUST is used to enable the automatic
    adjustment. The config option is selected only for x86.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: john stultz <johnstul@us.ibm.com>
    Link: http://lkml.kernel.org/r/20110823133142.494157493@de.ibm.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index c7218d132738..f954282d9a82 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -194,7 +194,7 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 	for (next = dev->next_event; ;) {
 		next = ktime_add(next, tick_period);
 
-		if (!clockevents_program_event(dev, next, ktime_get()))
+		if (!clockevents_program_event(dev, next, false))
 			return;
 		tick_do_periodic_broadcast();
 	}
@@ -373,7 +373,7 @@ static int tick_broadcast_set_event(ktime_t expires, int force)
 {
 	struct clock_event_device *bc = tick_broadcast_device.evtdev;
 
-	return tick_dev_program_event(bc, expires, force);
+	return clockevents_program_event(bc, expires, force);
 }
 
 int tick_resume_broadcast_oneshot(struct clock_event_device *bc)

commit 250f972d85effad5b6e10da4bbd877e6a4b503b6
Merge: 7372b0b122af bbe7b8bef48c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 20 20:06:24 2011 +0200

    Merge branch 'timers/urgent' into timers/core
    
    Reason: Get upstream fixes and kfree_rcu which is necessary for a
    follow up patch.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 07f4beb0b5bbfaf36a64aa00d59e670ec578a95a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 16 11:07:48 2011 +0200

    tick: Clear broadcast active bit when switching to oneshot
    
    The first cpu which switches from periodic to oneshot mode switches
    also the broadcast device into oneshot mode. The broadcast device
    serves as a backup for per cpu timers which stop in deeper
    C-states. To avoid starvation of the cpus which might be in idle and
    depend on broadcast mode it marks the other cpus as broadcast active
    and sets the brodcast expiry value of those cpus to the next tick.
    
    The oneshot mode broadcast bit for the other cpus is sticky and gets
    only cleared when those cpus exit idle. If a cpu was not idle while
    the bit got set in consequence the bit prevents that the broadcast
    device is armed on behalf of that cpu when it enters idle for the
    first time after it switched to oneshot mode.
    
    In most cases that goes unnoticed as one of the other cpus has usually
    a timer pending which keeps the broadcast device armed with a short
    timeout. Now if the only cpu which has a short timer active has the
    bit set then the broadcast device will not be armed on behalf of that
    cpu and will fire way after the expected timer expiry. In the case of
    Christians bug report it took ~145 seconds which is about half of the
    wrap around time of HPET (the limit for that device) due to the fact
    that all other cpus had no timers armed which expired before the 145
    seconds timeframe.
    
    The solution is simply to clear the broadcast active bit
    unconditionally when a cpu switches to oneshot mode after the first
    cpu switched the broadcast device over. It's not idle at that point
    otherwise it would not be executing that code.
    
    [ I fundamentally hate that broadcast crap. Why the heck thought some
      folks that when going into deep idle it's a brilliant concept to
      switch off the last device which brings the cpu back from that
      state? ]
    
    Thanks to Christian for providing all the valuable debug information!
    
    Reported-and-tested-by: Christian Hoffmann <email@christianhoffmann.info>
    Cc: John Stultz <johnstul@us.ibm.com>
    Link: http://lkml.kernel.org/r/%3Calpine.LFD.2.02.1105161105170.3078%40ionos%3E
    Cc: stable@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index da800ffa810c..723c7637e55a 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -522,10 +522,11 @@ static void tick_broadcast_init_next_event(struct cpumask *mask,
  */
 void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 {
+	int cpu = smp_processor_id();
+
 	/* Set it up only once ! */
 	if (bc->event_handler != tick_handle_oneshot_broadcast) {
 		int was_periodic = bc->mode == CLOCK_EVT_MODE_PERIODIC;
-		int cpu = smp_processor_id();
 
 		bc->event_handler = tick_handle_oneshot_broadcast;
 		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
@@ -551,6 +552,15 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 			tick_broadcast_set_event(tick_next_period, 1);
 		} else
 			bc->next_event.tv64 = KTIME_MAX;
+	} else {
+		/*
+		 * The first cpu which switches to oneshot mode sets
+		 * the bit for all other cpus which are in the general
+		 * (periodic) broadcast mask. So the bit is set and
+		 * would prevent the first broadcast enter after this
+		 * to program the bc device.
+		 */
+		tick_broadcast_clear_oneshot(cpu);
 	}
 }
 

commit 7372b0b122af0f6675f3ab65bfd91c8a438e0480
Author: Andi Kleen <ak@linux.intel.com>
Date:   Wed May 4 15:09:27 2011 -0700

    clockevents: Move C3 stop test outside lock
    
    Avoid taking broadcast_lock in the idle path for systems where the
    timer doesn't stop in C3.
    
    [ tglx: Removed the stale label and added comment ]
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Kleikamp <dkleikamp@gmail.com>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: lenb@kernel.org
    Cc: paulmck@us.ibm.com
    Link: http://lkml.kernel.org/r/%3C20110504234806.GF2925%40one.firstfloor.org%3E
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index da800ffa810c..827e0f862da4 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -456,23 +456,27 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 	unsigned long flags;
 	int cpu;
 
-	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
-
 	/*
 	 * Periodic mode does not care about the enter/exit of power
 	 * states
 	 */
 	if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
-		goto out;
+		return;
 
-	bc = tick_broadcast_device.evtdev;
+	/*
+	 * We are called with preemtion disabled from the depth of the
+	 * idle code, so we can't be moved away.
+	 */
 	cpu = smp_processor_id();
 	td = &per_cpu(tick_cpu_device, cpu);
 	dev = td->evtdev;
 
 	if (!(dev->features & CLOCK_EVT_FEAT_C3STOP))
-		goto out;
+		return;
+
+	bc = tick_broadcast_device.evtdev;
 
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 	if (reason == CLOCK_EVT_NOTIFY_BROADCAST_ENTER) {
 		if (!cpumask_test_cpu(cpu, tick_get_broadcast_oneshot_mask())) {
 			cpumask_set_cpu(cpu, tick_get_broadcast_oneshot_mask());
@@ -489,8 +493,6 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 				tick_program_event(dev->next_event, 1);
 		}
 	}
-
-out:
 	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 

commit 420c1c572d4ceaa2f37b6311b7017ac6cf049fe2
Merge: 9620639b7ea3 6e6823d17b15
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 18:53:35 2011 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (62 commits)
      posix-clocks: Check write permissions in posix syscalls
      hrtimer: Remove empty hrtimer_init_hres_timer()
      hrtimer: Update hrtimer->state documentation
      hrtimer: Update base[CLOCK_BOOTTIME].offset correctly
      timers: Export CLOCK_BOOTTIME via the posix timers interface
      timers: Add CLOCK_BOOTTIME hrtimer base
      time: Extend get_xtime_and_monotonic_offset() to also return sleep
      time: Introduce get_monotonic_boottime and ktime_get_boottime
      hrtimers: extend hrtimer base code to handle more then 2 clockids
      ntp: Remove redundant and incorrect parameter check
      mn10300: Switch do_timer() to xtimer_update()
      posix clocks: Introduce dynamic clocks
      posix-timers: Cleanup namespace
      posix-timers: Add support for fd based clocks
      x86: Add clock_adjtime for x86
      posix-timers: Introduce a syscall for clock tuning.
      time: Splitout compat timex accessors
      ntp: Add ADJ_SETOFFSET mode bit
      time: Introduce timekeeping_inject_offset
      posix-timer: Update comment
      ...
    
    Fix up new system-call-related conflicts in
            arch/x86/ia32/ia32entry.S
            arch/x86/include/asm/unistd_32.h
            arch/x86/include/asm/unistd_64.h
            arch/x86/kernel/syscall_table_32.S
    (name_to_handle_at()/open_by_handle_at() vs clock_adjtime()), and some
    due to movement of get_jiffies_64() in:
            kernel/time.c

commit 3a142a0672b48a853f00af61f184c7341ac9c99d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 25 22:34:23 2011 +0100

    clockevents: Prevent oneshot mode when broadcast device is periodic
    
    When the per cpu timer is marked CLOCK_EVT_FEAT_C3STOP, then we only
    can switch into oneshot mode, when the backup broadcast device
    supports oneshot mode as well. Otherwise we would try to switch the
    broadcast device into an unsupported mode unconditionally. This went
    unnoticed so far as the current available broadcast devices support
    oneshot mode. Seth unearthed this problem while debugging and working
    around an hpet related BIOS wreckage.
    
    Add the necessary check to tick_is_oneshot_available().
    
    Reported-and-tested-by: Seth Forshee <seth.forshee@canonical.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <alpine.LFD.2.00.1102252231200.2701@localhost6.localdomain6>
    Cc: stable@kernel.org # .21 ->

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 48b2761b5668..a3b5aff62606 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -600,4 +600,14 @@ int tick_broadcast_oneshot_active(void)
 	return tick_broadcast_device.mode == TICKDEV_MODE_ONESHOT;
 }
 
+/*
+ * Check whether the broadcast device supports oneshot.
+ */
+bool tick_broadcast_oneshot_available(void)
+{
+	struct clock_event_device *bc = tick_broadcast_device.evtdev;
+
+	return bc ? bc->features & CLOCK_EVT_FEAT_ONESHOT : false;
+}
+
 #endif

commit e2830b5c1b2b2217894370a3b95af87d4a958401
Author: Torben Hohn <torbenh@gmx.de>
Date:   Thu Jan 27 16:00:32 2011 +0100

    time: Make do_timer() and xtime_lock local to kernel/time/
    
    All callers of do_timer() are converted to xtime_update(). The only
    users of xtime_lock are in kernel/time/. Make both local to
    kernel/time/ and remove them from the global header files.
    
    [ tglx: Reuse tick-internal.h instead of creating another local header
            file. Massaged changelog ]
    
    Signed-off-by: Torben Hohn <torbenh@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: johnstul@us.ibm.com
    Cc: yong.zhang0@gmail.com
    Cc: hch@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 48b2761b5668..92ef9a54f0a4 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -18,7 +18,6 @@
 #include <linux/percpu.h>
 #include <linux/profile.h>
 #include <linux/sched.h>
-#include <linux/tick.h>
 
 #include "tick-internal.h"
 

commit 698f93159a735bd29a8767c9f60d9b2d75870f8e
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Fri Jul 2 20:41:51 2010 +0200

    fix comment/printk typos concerning "already"
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index b3bafd5fc66d..48b2761b5668 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -188,7 +188,7 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 	/*
 	 * Setup the next period for devices, which do not have
 	 * periodic mode. We read dev->next_event first and add to it
-	 * when the event alrady expired. clockevents_program_event()
+	 * when the event already expired. clockevents_program_event()
 	 * sets dev->next_event only when the event is really
 	 * programmed to the device.
 	 */

commit b5f91da0a6973bb6f9ff3b91b0e92c0773a458f3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 8 12:40:31 2009 +0100

    clockevents: Convert to raw_spinlock
    
    Convert locks which cannot be sleeping locks in preempt-rt to
    raw_spinlocks.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index c2ec25087a35..b3bafd5fc66d 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -31,7 +31,7 @@ static struct tick_device tick_broadcast_device;
 /* FIXME: Use cpumask_var_t. */
 static DECLARE_BITMAP(tick_broadcast_mask, NR_CPUS);
 static DECLARE_BITMAP(tmpmask, NR_CPUS);
-static DEFINE_SPINLOCK(tick_broadcast_lock);
+static DEFINE_RAW_SPINLOCK(tick_broadcast_lock);
 static int tick_broadcast_force;
 
 #ifdef CONFIG_TICK_ONESHOT
@@ -96,7 +96,7 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 	unsigned long flags;
 	int ret = 0;
 
-	spin_lock_irqsave(&tick_broadcast_lock, flags);
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	/*
 	 * Devices might be registered with both periodic and oneshot
@@ -122,7 +122,7 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 			tick_broadcast_clear_oneshot(cpu);
 		}
 	}
-	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 	return ret;
 }
 
@@ -161,13 +161,13 @@ static void tick_do_broadcast(struct cpumask *mask)
  */
 static void tick_do_periodic_broadcast(void)
 {
-	spin_lock(&tick_broadcast_lock);
+	raw_spin_lock(&tick_broadcast_lock);
 
 	cpumask_and(to_cpumask(tmpmask),
 		    cpu_online_mask, tick_get_broadcast_mask());
 	tick_do_broadcast(to_cpumask(tmpmask));
 
-	spin_unlock(&tick_broadcast_lock);
+	raw_spin_unlock(&tick_broadcast_lock);
 }
 
 /*
@@ -212,7 +212,7 @@ static void tick_do_broadcast_on_off(unsigned long *reason)
 	unsigned long flags;
 	int cpu, bc_stopped;
 
-	spin_lock_irqsave(&tick_broadcast_lock, flags);
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	cpu = smp_processor_id();
 	td = &per_cpu(tick_cpu_device, cpu);
@@ -263,7 +263,7 @@ static void tick_do_broadcast_on_off(unsigned long *reason)
 			tick_broadcast_setup_oneshot(bc);
 	}
 out:
-	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
 /*
@@ -299,7 +299,7 @@ void tick_shutdown_broadcast(unsigned int *cpup)
 	unsigned long flags;
 	unsigned int cpu = *cpup;
 
-	spin_lock_irqsave(&tick_broadcast_lock, flags);
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	bc = tick_broadcast_device.evtdev;
 	cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
@@ -309,7 +309,7 @@ void tick_shutdown_broadcast(unsigned int *cpup)
 			clockevents_shutdown(bc);
 	}
 
-	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
 void tick_suspend_broadcast(void)
@@ -317,13 +317,13 @@ void tick_suspend_broadcast(void)
 	struct clock_event_device *bc;
 	unsigned long flags;
 
-	spin_lock_irqsave(&tick_broadcast_lock, flags);
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	bc = tick_broadcast_device.evtdev;
 	if (bc)
 		clockevents_shutdown(bc);
 
-	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
 int tick_resume_broadcast(void)
@@ -332,7 +332,7 @@ int tick_resume_broadcast(void)
 	unsigned long flags;
 	int broadcast = 0;
 
-	spin_lock_irqsave(&tick_broadcast_lock, flags);
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	bc = tick_broadcast_device.evtdev;
 
@@ -351,7 +351,7 @@ int tick_resume_broadcast(void)
 			break;
 		}
 	}
-	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 
 	return broadcast;
 }
@@ -405,7 +405,7 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	ktime_t now, next_event;
 	int cpu;
 
-	spin_lock(&tick_broadcast_lock);
+	raw_spin_lock(&tick_broadcast_lock);
 again:
 	dev->next_event.tv64 = KTIME_MAX;
 	next_event.tv64 = KTIME_MAX;
@@ -443,7 +443,7 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 		if (tick_broadcast_set_event(next_event, 0))
 			goto again;
 	}
-	spin_unlock(&tick_broadcast_lock);
+	raw_spin_unlock(&tick_broadcast_lock);
 }
 
 /*
@@ -457,7 +457,7 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 	unsigned long flags;
 	int cpu;
 
-	spin_lock_irqsave(&tick_broadcast_lock, flags);
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	/*
 	 * Periodic mode does not care about the enter/exit of power
@@ -492,7 +492,7 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 	}
 
 out:
-	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
 /*
@@ -563,13 +563,13 @@ void tick_broadcast_switch_to_oneshot(void)
 	struct clock_event_device *bc;
 	unsigned long flags;
 
-	spin_lock_irqsave(&tick_broadcast_lock, flags);
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	tick_broadcast_device.mode = TICKDEV_MODE_ONESHOT;
 	bc = tick_broadcast_device.evtdev;
 	if (bc)
 		tick_broadcast_setup_oneshot(bc);
-	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
 
@@ -581,7 +581,7 @@ void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
 	unsigned long flags;
 	unsigned int cpu = *cpup;
 
-	spin_lock_irqsave(&tick_broadcast_lock, flags);
+	raw_spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	/*
 	 * Clear the broadcast mask flag for the dead cpu, but do not
@@ -589,7 +589,7 @@ void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
 	 */
 	cpumask_clear_cpu(cpu, tick_get_broadcast_oneshot_mask());
 
-	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	raw_spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
 /*

commit f833bab87fca5c3ce13778421b1365845843b976
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Aug 17 14:34:59 2009 -0700

    clockevent: Prevent dead lock on clockevents_lock
    
    Currently clockevents_notify() is called with interrupts enabled at
    some places and interrupts disabled at some other places.
    
    This results in a deadlock in this scenario.
    
    cpu A holds clockevents_lock in clockevents_notify() with irqs enabled
    cpu B waits for clockevents_lock in clockevents_notify() with irqs disabled
    cpu C doing set_mtrr() which will try to rendezvous of all the cpus.
    
    This will result in C and A come to the rendezvous point and waiting
    for B. B is stuck forever waiting for the spinlock and thus not
    reaching the rendezvous point.
    
    Fix the clockevents code so that clockevents_lock is taken with
    interrupts disabled and thus avoid the above deadlock.
    
    Also call lapic_timer_propagate_broadcast() on the destination cpu so
    that we avoid calling smp_call_function() in the clockevents notifier
    chain.
    
    This issue left us wondering if we need to change the MTRR rendezvous
    logic to use stop machine logic (instead of smp_call_function) or add
    a check in spinlock debug code to see if there are other spinlocks
    which gets taken under both interrupts enabled/disabled conditions.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: "Pallipadi Venkatesh" <venkatesh.pallipadi@intel.com>
    Cc: "Brown Len" <len.brown@intel.com>
    LKML-Reference: <1250544899.2709.210.camel@sbs-t61.sc.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 877dbedc3118..c2ec25087a35 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -205,11 +205,11 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
  * Powerstate information: The system enters/leaves a state, where
  * affected devices might stop
  */
-static void tick_do_broadcast_on_off(void *why)
+static void tick_do_broadcast_on_off(unsigned long *reason)
 {
 	struct clock_event_device *bc, *dev;
 	struct tick_device *td;
-	unsigned long flags, *reason = why;
+	unsigned long flags;
 	int cpu, bc_stopped;
 
 	spin_lock_irqsave(&tick_broadcast_lock, flags);
@@ -276,8 +276,7 @@ void tick_broadcast_on_off(unsigned long reason, int *oncpu)
 		printk(KERN_ERR "tick-broadcast: ignoring broadcast for "
 		       "offline CPU #%d\n", *oncpu);
 	else
-		smp_call_function_single(*oncpu, tick_do_broadcast_on_off,
-					 &reason, 1);
+		tick_do_broadcast_on_off(&reason);
 }
 
 /*

commit a52f5c5620673c292cb159205bf0e1eb5af1985b
Author: Dmitri Vorobiev <dmitri.vorobiev@movial.com>
Date:   Fri May 1 13:10:21 2009 -0700

    clockevents: tick_broadcast_device can become static
    
    The variable tick_broadcast_device is not used outside of the
    file where it is defined, so let's make it static.
    
    Signed-off-by: Dmitri Vorobiev <dmitri.vorobiev@movial.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 118a3b3b3f9a..877dbedc3118 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -27,7 +27,7 @@
  * timer stops in C3 state.
  */
 
-struct tick_device tick_broadcast_device;
+static struct tick_device tick_broadcast_device;
 /* FIXME: Use cpumask_var_t. */
 static DECLARE_BITMAP(tick_broadcast_mask, NR_CPUS);
 static DECLARE_BITMAP(tmpmask, NR_CPUS);

commit 5db0e1e9e0f30f160b832a0b5cd1131954bf4f6e
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Jan 1 10:12:29 2009 +1030

    cpumask: replace for_each_cpu_mask_nr with for_each_cpu in kernel/time/
    
    Impact: cleanup
    
    Simple replacement, now the _nr is redundant.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Cc: Ingo Molnar <mingo@redhat.com>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 356fac57a182..118a3b3b3f9a 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -512,7 +512,7 @@ static void tick_broadcast_init_next_event(struct cpumask *mask,
 	struct tick_device *td;
 	int cpu;
 
-	for_each_cpu_mask_nr(cpu, *mask) {
+	for_each_cpu(cpu, mask) {
 		td = &per_cpu(tick_cpu_device, cpu);
 		if (td->evtdev)
 			td->evtdev->next_event = expires;

commit 6b954823c24f04ed026a8517f6bab5abda279db8
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Jan 1 10:12:25 2009 +1030

    cpumask: convert kernel time functions
    
    Impact: Use new APIs
    
    Convert kernel/time functions to use struct cpumask *.
    
    Note the ugly bitmap declarations in tick-broadcast.c.  These should
    be cpumask_var_t, but there was no obvious initialization function to
    put the alloc_cpumask_var() calls in.  This was safe.
    
    (Eventually 'struct cpumask' will be undefined for CONFIG_CPUMASK_OFFSTACK,
    so we use a bitmap here to show we really mean it).
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 9590af2327be..356fac57a182 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -28,7 +28,9 @@
  */
 
 struct tick_device tick_broadcast_device;
-static cpumask_t tick_broadcast_mask;
+/* FIXME: Use cpumask_var_t. */
+static DECLARE_BITMAP(tick_broadcast_mask, NR_CPUS);
+static DECLARE_BITMAP(tmpmask, NR_CPUS);
 static DEFINE_SPINLOCK(tick_broadcast_lock);
 static int tick_broadcast_force;
 
@@ -46,9 +48,9 @@ struct tick_device *tick_get_broadcast_device(void)
 	return &tick_broadcast_device;
 }
 
-cpumask_t *tick_get_broadcast_mask(void)
+struct cpumask *tick_get_broadcast_mask(void)
 {
-	return &tick_broadcast_mask;
+	return to_cpumask(tick_broadcast_mask);
 }
 
 /*
@@ -72,7 +74,7 @@ int tick_check_broadcast_device(struct clock_event_device *dev)
 
 	clockevents_exchange_device(NULL, dev);
 	tick_broadcast_device.evtdev = dev;
-	if (!cpus_empty(tick_broadcast_mask))
+	if (!cpumask_empty(tick_get_broadcast_mask()))
 		tick_broadcast_start_periodic(dev);
 	return 1;
 }
@@ -104,7 +106,7 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 	 */
 	if (!tick_device_is_functional(dev)) {
 		dev->event_handler = tick_handle_periodic;
-		cpu_set(cpu, tick_broadcast_mask);
+		cpumask_set_cpu(cpu, tick_get_broadcast_mask());
 		tick_broadcast_start_periodic(tick_broadcast_device.evtdev);
 		ret = 1;
 	} else {
@@ -116,7 +118,7 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 		if (!(dev->features & CLOCK_EVT_FEAT_C3STOP)) {
 			int cpu = smp_processor_id();
 
-			cpu_clear(cpu, tick_broadcast_mask);
+			cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
 			tick_broadcast_clear_oneshot(cpu);
 		}
 	}
@@ -125,9 +127,9 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 }
 
 /*
- * Broadcast the event to the cpus, which are set in the mask
+ * Broadcast the event to the cpus, which are set in the mask (mangled).
  */
-static void tick_do_broadcast(cpumask_t mask)
+static void tick_do_broadcast(struct cpumask *mask)
 {
 	int cpu = smp_processor_id();
 	struct tick_device *td;
@@ -135,22 +137,21 @@ static void tick_do_broadcast(cpumask_t mask)
 	/*
 	 * Check, if the current cpu is in the mask
 	 */
-	if (cpu_isset(cpu, mask)) {
-		cpu_clear(cpu, mask);
+	if (cpumask_test_cpu(cpu, mask)) {
+		cpumask_clear_cpu(cpu, mask);
 		td = &per_cpu(tick_cpu_device, cpu);
 		td->evtdev->event_handler(td->evtdev);
 	}
 
-	if (!cpus_empty(mask)) {
+	if (!cpumask_empty(mask)) {
 		/*
 		 * It might be necessary to actually check whether the devices
 		 * have different broadcast functions. For now, just use the
 		 * one of the first device. This works as long as we have this
 		 * misfeature only on x86 (lapic)
 		 */
-		cpu = first_cpu(mask);
-		td = &per_cpu(tick_cpu_device, cpu);
-		td->evtdev->broadcast(&mask);
+		td = &per_cpu(tick_cpu_device, cpumask_first(mask));
+		td->evtdev->broadcast(mask);
 	}
 }
 
@@ -160,12 +161,11 @@ static void tick_do_broadcast(cpumask_t mask)
  */
 static void tick_do_periodic_broadcast(void)
 {
-	cpumask_t mask;
-
 	spin_lock(&tick_broadcast_lock);
 
-	cpus_and(mask, cpu_online_map, tick_broadcast_mask);
-	tick_do_broadcast(mask);
+	cpumask_and(to_cpumask(tmpmask),
+		    cpu_online_mask, tick_get_broadcast_mask());
+	tick_do_broadcast(to_cpumask(tmpmask));
 
 	spin_unlock(&tick_broadcast_lock);
 }
@@ -228,13 +228,13 @@ static void tick_do_broadcast_on_off(void *why)
 	if (!tick_device_is_functional(dev))
 		goto out;
 
-	bc_stopped = cpus_empty(tick_broadcast_mask);
+	bc_stopped = cpumask_empty(tick_get_broadcast_mask());
 
 	switch (*reason) {
 	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
 	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
-		if (!cpu_isset(cpu, tick_broadcast_mask)) {
-			cpu_set(cpu, tick_broadcast_mask);
+		if (!cpumask_test_cpu(cpu, tick_get_broadcast_mask())) {
+			cpumask_set_cpu(cpu, tick_get_broadcast_mask());
 			if (tick_broadcast_device.mode ==
 			    TICKDEV_MODE_PERIODIC)
 				clockevents_shutdown(dev);
@@ -244,8 +244,8 @@ static void tick_do_broadcast_on_off(void *why)
 		break;
 	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
 		if (!tick_broadcast_force &&
-		    cpu_isset(cpu, tick_broadcast_mask)) {
-			cpu_clear(cpu, tick_broadcast_mask);
+		    cpumask_test_cpu(cpu, tick_get_broadcast_mask())) {
+			cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
 			if (tick_broadcast_device.mode ==
 			    TICKDEV_MODE_PERIODIC)
 				tick_setup_periodic(dev, 0);
@@ -253,7 +253,7 @@ static void tick_do_broadcast_on_off(void *why)
 		break;
 	}
 
-	if (cpus_empty(tick_broadcast_mask)) {
+	if (cpumask_empty(tick_get_broadcast_mask())) {
 		if (!bc_stopped)
 			clockevents_shutdown(bc);
 	} else if (bc_stopped) {
@@ -272,7 +272,7 @@ static void tick_do_broadcast_on_off(void *why)
  */
 void tick_broadcast_on_off(unsigned long reason, int *oncpu)
 {
-	if (!cpu_isset(*oncpu, cpu_online_map))
+	if (!cpumask_test_cpu(*oncpu, cpu_online_mask))
 		printk(KERN_ERR "tick-broadcast: ignoring broadcast for "
 		       "offline CPU #%d\n", *oncpu);
 	else
@@ -303,10 +303,10 @@ void tick_shutdown_broadcast(unsigned int *cpup)
 	spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	bc = tick_broadcast_device.evtdev;
-	cpu_clear(cpu, tick_broadcast_mask);
+	cpumask_clear_cpu(cpu, tick_get_broadcast_mask());
 
 	if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC) {
-		if (bc && cpus_empty(tick_broadcast_mask))
+		if (bc && cpumask_empty(tick_get_broadcast_mask()))
 			clockevents_shutdown(bc);
 	}
 
@@ -342,10 +342,10 @@ int tick_resume_broadcast(void)
 
 		switch (tick_broadcast_device.mode) {
 		case TICKDEV_MODE_PERIODIC:
-			if(!cpus_empty(tick_broadcast_mask))
+			if (!cpumask_empty(tick_get_broadcast_mask()))
 				tick_broadcast_start_periodic(bc);
-			broadcast = cpu_isset(smp_processor_id(),
-					      tick_broadcast_mask);
+			broadcast = cpumask_test_cpu(smp_processor_id(),
+						     tick_get_broadcast_mask());
 			break;
 		case TICKDEV_MODE_ONESHOT:
 			broadcast = tick_resume_broadcast_oneshot(bc);
@@ -360,14 +360,15 @@ int tick_resume_broadcast(void)
 
 #ifdef CONFIG_TICK_ONESHOT
 
-static cpumask_t tick_broadcast_oneshot_mask;
+/* FIXME: use cpumask_var_t. */
+static DECLARE_BITMAP(tick_broadcast_oneshot_mask, NR_CPUS);
 
 /*
- * Debugging: see timer_list.c
+ * Exposed for debugging: see timer_list.c
  */
-cpumask_t *tick_get_broadcast_oneshot_mask(void)
+struct cpumask *tick_get_broadcast_oneshot_mask(void)
 {
-	return &tick_broadcast_oneshot_mask;
+	return to_cpumask(tick_broadcast_oneshot_mask);
 }
 
 static int tick_broadcast_set_event(ktime_t expires, int force)
@@ -389,7 +390,7 @@ int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
  */
 void tick_check_oneshot_broadcast(int cpu)
 {
-	if (cpu_isset(cpu, tick_broadcast_oneshot_mask)) {
+	if (cpumask_test_cpu(cpu, to_cpumask(tick_broadcast_oneshot_mask))) {
 		struct tick_device *td = &per_cpu(tick_cpu_device, cpu);
 
 		clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_ONESHOT);
@@ -402,7 +403,6 @@ void tick_check_oneshot_broadcast(int cpu)
 static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 {
 	struct tick_device *td;
-	cpumask_t mask;
 	ktime_t now, next_event;
 	int cpu;
 
@@ -410,13 +410,13 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 again:
 	dev->next_event.tv64 = KTIME_MAX;
 	next_event.tv64 = KTIME_MAX;
-	mask = CPU_MASK_NONE;
+	cpumask_clear(to_cpumask(tmpmask));
 	now = ktime_get();
 	/* Find all expired events */
-	for_each_cpu_mask_nr(cpu, tick_broadcast_oneshot_mask) {
+	for_each_cpu(cpu, tick_get_broadcast_oneshot_mask()) {
 		td = &per_cpu(tick_cpu_device, cpu);
 		if (td->evtdev->next_event.tv64 <= now.tv64)
-			cpu_set(cpu, mask);
+			cpumask_set_cpu(cpu, to_cpumask(tmpmask));
 		else if (td->evtdev->next_event.tv64 < next_event.tv64)
 			next_event.tv64 = td->evtdev->next_event.tv64;
 	}
@@ -424,7 +424,7 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	/*
 	 * Wakeup the cpus which have an expired event.
 	 */
-	tick_do_broadcast(mask);
+	tick_do_broadcast(to_cpumask(tmpmask));
 
 	/*
 	 * Two reasons for reprogram:
@@ -476,15 +476,16 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 		goto out;
 
 	if (reason == CLOCK_EVT_NOTIFY_BROADCAST_ENTER) {
-		if (!cpu_isset(cpu, tick_broadcast_oneshot_mask)) {
-			cpu_set(cpu, tick_broadcast_oneshot_mask);
+		if (!cpumask_test_cpu(cpu, tick_get_broadcast_oneshot_mask())) {
+			cpumask_set_cpu(cpu, tick_get_broadcast_oneshot_mask());
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
 			if (dev->next_event.tv64 < bc->next_event.tv64)
 				tick_broadcast_set_event(dev->next_event, 1);
 		}
 	} else {
-		if (cpu_isset(cpu, tick_broadcast_oneshot_mask)) {
-			cpu_clear(cpu, tick_broadcast_oneshot_mask);
+		if (cpumask_test_cpu(cpu, tick_get_broadcast_oneshot_mask())) {
+			cpumask_clear_cpu(cpu,
+					  tick_get_broadcast_oneshot_mask());
 			clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
 			if (dev->next_event.tv64 != KTIME_MAX)
 				tick_program_event(dev->next_event, 1);
@@ -502,10 +503,11 @@ void tick_broadcast_oneshot_control(unsigned long reason)
  */
 static void tick_broadcast_clear_oneshot(int cpu)
 {
-	cpu_clear(cpu, tick_broadcast_oneshot_mask);
+	cpumask_clear_cpu(cpu, tick_get_broadcast_oneshot_mask());
 }
 
-static void tick_broadcast_init_next_event(cpumask_t *mask, ktime_t expires)
+static void tick_broadcast_init_next_event(struct cpumask *mask,
+					   ktime_t expires)
 {
 	struct tick_device *td;
 	int cpu;
@@ -526,7 +528,6 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 	if (bc->event_handler != tick_handle_oneshot_broadcast) {
 		int was_periodic = bc->mode == CLOCK_EVT_MODE_PERIODIC;
 		int cpu = smp_processor_id();
-		cpumask_t mask;
 
 		bc->event_handler = tick_handle_oneshot_broadcast;
 		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
@@ -540,13 +541,15 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 		 * oneshot_mask bits for those and program the
 		 * broadcast device to fire.
 		 */
-		mask = tick_broadcast_mask;
-		cpu_clear(cpu, mask);
-		cpus_or(tick_broadcast_oneshot_mask,
-			tick_broadcast_oneshot_mask, mask);
-
-		if (was_periodic && !cpus_empty(mask)) {
-			tick_broadcast_init_next_event(&mask, tick_next_period);
+		cpumask_copy(to_cpumask(tmpmask), tick_get_broadcast_mask());
+		cpumask_clear_cpu(cpu, to_cpumask(tmpmask));
+		cpumask_or(tick_get_broadcast_oneshot_mask(),
+			   tick_get_broadcast_oneshot_mask(),
+			   to_cpumask(tmpmask));
+
+		if (was_periodic && !cpumask_empty(to_cpumask(tmpmask))) {
+			tick_broadcast_init_next_event(to_cpumask(tmpmask),
+						       tick_next_period);
 			tick_broadcast_set_event(tick_next_period, 1);
 		} else
 			bc->next_event.tv64 = KTIME_MAX;
@@ -585,7 +588,7 @@ void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
 	 * Clear the broadcast mask flag for the dead cpu, but do not
 	 * stop the broadcast device!
 	 */
-	cpu_clear(cpu, tick_broadcast_oneshot_mask);
+	cpumask_clear_cpu(cpu, tick_get_broadcast_oneshot_mask());
 
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }

commit 320ab2b0b1e08e3805a3e1084a2f0eb1938d5d67
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Dec 13 21:20:26 2008 +1030

    cpumask: convert struct clock_event_device to cpumask pointers.
    
    Impact: change calling convention of existing clock_event APIs
    
    struct clock_event_timer's cpumask field gets changed to take pointer,
    as does the ->broadcast function.
    
    Another single-patch change.  For safety, we BUG_ON() in
    clockevents_register_device() if it's not set.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index f98a1b7b16e9..9590af2327be 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -150,7 +150,7 @@ static void tick_do_broadcast(cpumask_t mask)
 		 */
 		cpu = first_cpu(mask);
 		td = &per_cpu(tick_cpu_device, cpu);
-		td->evtdev->broadcast(mask);
+		td->evtdev->broadcast(&mask);
 	}
 }
 

commit fb02fbc14d17837b4b7b02dbb36142c16a7bf208
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Oct 17 10:01:23 2008 +0200

    NOHZ: restart tick device from irq_enter()
    
    We did not restart the tick device from irq_enter() to avoid double
    reprogramming and extra events in the return immediate to idle case.
    
    But long lasting softirqs can lead to a situation where jiffies become
    stale:
    
    idle()
      tick stopped (reprogrammed to next pending timer)
      halt()
       interrupt
         jiffies updated from irq_enter()
         interrupt handler
         softirq function 1 runs 20ms
         softirq function 2 arms a 10ms timer with a stale jiffies value
         jiffies updated from irq_exit()
         timer wheel has now an already expired timer
         (the one added in function 2)
         timer fires and timer softirq runs
    
    This was discovered when debugging a timer problem which happend only
    when the ath5k driver is active. The debugging proved that there is a
    softirq function running for more than 20ms, which is a bug by itself.
    
    To solve this we restart the tick timer right from irq_enter(), but do
    not go through the other functions which are necessary to return from
    idle when need_resched() is set.
    
    Reported-by: Elias Oltmanns <eo@nebensachen.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Elias Oltmanns <eo@nebensachen.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index cb01cd8f919b..f98a1b7b16e9 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -383,6 +383,19 @@ int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
 	return 0;
 }
 
+/*
+ * Called from irq_enter() when idle was interrupted to reenable the
+ * per cpu device.
+ */
+void tick_check_oneshot_broadcast(int cpu)
+{
+	if (cpu_isset(cpu, tick_broadcast_oneshot_mask)) {
+		struct tick_device *td = &per_cpu(tick_cpu_device, cpu);
+
+		clockevents_set_mode(td->evtdev, CLOCK_EVT_MODE_ONESHOT);
+	}
+}
+
 /*
  * Handle oneshot mode broadcasting
  */

commit 07454bfff151d2465ada809bbaddf3548cc1097c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Oct 4 10:51:07 2008 +0200

    clockevents: check broadcast tick device not the clock events device
    
    Impact: jiffies increment too fast.
    
    Hugh Dickins noted that with NOHZ=n and HIGHRES=n jiffies get
    incremented too fast. The reason is a wrong check in the broadcast
    enter/exit code, which keeps the local apic timer in periodic mode
    when the switch happens.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index bd7034542399..cb01cd8f919b 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -235,7 +235,8 @@ static void tick_do_broadcast_on_off(void *why)
 	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
 		if (!cpu_isset(cpu, tick_broadcast_mask)) {
 			cpu_set(cpu, tick_broadcast_mask);
-			if (bc->mode == TICKDEV_MODE_PERIODIC)
+			if (tick_broadcast_device.mode ==
+			    TICKDEV_MODE_PERIODIC)
 				clockevents_shutdown(dev);
 		}
 		if (*reason == CLOCK_EVT_NOTIFY_BROADCAST_FORCE)
@@ -245,7 +246,8 @@ static void tick_do_broadcast_on_off(void *why)
 		if (!tick_broadcast_force &&
 		    cpu_isset(cpu, tick_broadcast_mask)) {
 			cpu_clear(cpu, tick_broadcast_mask);
-			if (bc->mode == TICKDEV_MODE_PERIODIC)
+			if (tick_broadcast_device.mode ==
+			    TICKDEV_MODE_PERIODIC)
 				tick_setup_periodic(dev, 0);
 		}
 		break;

commit 27ce4cb4a0c7cf59b9a9952266883862f2e4c99f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 22 19:04:02 2008 +0200

    clockevents: prevent mode mismatch on cpu online
    
    Impact: timer hang on CPU online observed on AMD C1E systems
    
    When a CPU is brought online then the broadcast machinery can
    be in the one shot state already. Check this and setup the timer
    device of the new CPU in one shot mode so the broadcast code
    can pick up the next_event value correctly.
    
    Another AMD C1E oddity, as we switch to broadcast immediately and
    not after the full bring up via the ACPI cpu idle code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index e2b66d1c8ca5..bd7034542399 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -575,4 +575,12 @@ void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
+/*
+ * Check, whether the broadcast device is in one shot mode
+ */
+int tick_broadcast_oneshot_active(void)
+{
+	return tick_broadcast_device.mode == TICKDEV_MODE_ONESHOT;
+}
+
 #endif

commit 302745699c1b675b5d2a1af87271de10e4d96b6a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 22 19:02:25 2008 +0200

    clockevents: check broadcast device not tick device
    
    Impact: Possible hang on CPU online observed on AMD C1E machines.
    
    The broadcast setup code looks at the mode of the tick device to
    determine whether it needs to be shut down or setup. This is wrong
    when the broadcast mode is set to one shot already. This can happen
    when a CPU is brought online as it goes through the periodic setup
    first.
    
    The problem went unnoticed as sane systems do not call into that code
    before the switch to one shot for the clock event device happens.
    The AMD C1E idle routine switches over immediately and thereby shuts
    down the just setup device before the first interrupt happens.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index f1f3eee28113..e2b66d1c8ca5 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -235,7 +235,7 @@ static void tick_do_broadcast_on_off(void *why)
 	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
 		if (!cpu_isset(cpu, tick_broadcast_mask)) {
 			cpu_set(cpu, tick_broadcast_mask);
-			if (td->mode == TICKDEV_MODE_PERIODIC)
+			if (bc->mode == TICKDEV_MODE_PERIODIC)
 				clockevents_shutdown(dev);
 		}
 		if (*reason == CLOCK_EVT_NOTIFY_BROADCAST_FORCE)
@@ -245,7 +245,7 @@ static void tick_do_broadcast_on_off(void *why)
 		if (!tick_broadcast_force &&
 		    cpu_isset(cpu, tick_broadcast_mask)) {
 			cpu_clear(cpu, tick_broadcast_mask);
-			if (td->mode == TICKDEV_MODE_PERIODIC)
+			if (bc->mode == TICKDEV_MODE_PERIODIC)
 				tick_setup_periodic(dev, 0);
 		}
 		break;

commit 2344abbcbdb82140050e8be29d3d55e4f6fe860b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 16 11:32:50 2008 -0700

    clockevents: make device shutdown robust
    
    The device shut down does not cleanup the next_event variable of the
    clock event device. So when the device is reactivated the possible
    stale next_event value can prevent the device to be reprogrammed as it
    claims to wait on a event already.
    
    This is the root cause of the resurfacing suspend/resume problem,
    where systems need key press to come back to life.
    
    Fix this by setting next_event to KTIME_MAX when the device is shut
    down. Use a separate function for shutdown which takes care of that
    and only keep the direct set mode call in the broadcast code, where we
    can not touch the next_event value.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 2f5a38294bf9..f1f3eee28113 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -236,8 +236,7 @@ static void tick_do_broadcast_on_off(void *why)
 		if (!cpu_isset(cpu, tick_broadcast_mask)) {
 			cpu_set(cpu, tick_broadcast_mask);
 			if (td->mode == TICKDEV_MODE_PERIODIC)
-				clockevents_set_mode(dev,
-						     CLOCK_EVT_MODE_SHUTDOWN);
+				clockevents_shutdown(dev);
 		}
 		if (*reason == CLOCK_EVT_NOTIFY_BROADCAST_FORCE)
 			tick_broadcast_force = 1;
@@ -254,7 +253,7 @@ static void tick_do_broadcast_on_off(void *why)
 
 	if (cpus_empty(tick_broadcast_mask)) {
 		if (!bc_stopped)
-			clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
+			clockevents_shutdown(bc);
 	} else if (bc_stopped) {
 		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
 			tick_broadcast_start_periodic(bc);
@@ -306,7 +305,7 @@ void tick_shutdown_broadcast(unsigned int *cpup)
 
 	if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC) {
 		if (bc && cpus_empty(tick_broadcast_mask))
-			clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
+			clockevents_shutdown(bc);
 	}
 
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
@@ -321,7 +320,7 @@ void tick_suspend_broadcast(void)
 
 	bc = tick_broadcast_device.evtdev;
 	if (bc)
-		clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
+		clockevents_shutdown(bc);
 
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }

commit 7300711e8c6824fcfbd42a126980ff50439d8dd0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Sep 6 03:01:45 2008 +0200

    clockevents: broadcast fixup possible waiters
    
    Until the C1E patches arrived there where no users of periodic broadcast
    before switching to oneshot mode. Now we need to trigger a possible
    waiter for a periodic broadcast when switching to oneshot mode.
    Otherwise we can starve them for ever.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 2bc1f046151c..2f5a38294bf9 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -491,6 +491,18 @@ static void tick_broadcast_clear_oneshot(int cpu)
 	cpu_clear(cpu, tick_broadcast_oneshot_mask);
 }
 
+static void tick_broadcast_init_next_event(cpumask_t *mask, ktime_t expires)
+{
+	struct tick_device *td;
+	int cpu;
+
+	for_each_cpu_mask_nr(cpu, *mask) {
+		td = &per_cpu(tick_cpu_device, cpu);
+		if (td->evtdev)
+			td->evtdev->next_event = expires;
+	}
+}
+
 /**
  * tick_broadcast_setup_oneshot - setup the broadcast device
  */
@@ -498,9 +510,32 @@ void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 {
 	/* Set it up only once ! */
 	if (bc->event_handler != tick_handle_oneshot_broadcast) {
+		int was_periodic = bc->mode == CLOCK_EVT_MODE_PERIODIC;
+		int cpu = smp_processor_id();
+		cpumask_t mask;
+
 		bc->event_handler = tick_handle_oneshot_broadcast;
 		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
-		bc->next_event.tv64 = KTIME_MAX;
+
+		/* Take the do_timer update */
+		tick_do_timer_cpu = cpu;
+
+		/*
+		 * We must be careful here. There might be other CPUs
+		 * waiting for periodic broadcast. We need to set the
+		 * oneshot_mask bits for those and program the
+		 * broadcast device to fire.
+		 */
+		mask = tick_broadcast_mask;
+		cpu_clear(cpu, mask);
+		cpus_or(tick_broadcast_oneshot_mask,
+			tick_broadcast_oneshot_mask, mask);
+
+		if (was_periodic && !cpus_empty(mask)) {
+			tick_broadcast_init_next_event(&mask, tick_next_period);
+			tick_broadcast_set_event(tick_next_period, 1);
+		} else
+			bc->next_event.tv64 = KTIME_MAX;
 	}
 }
 

commit 1fb9b7d29d8e85ba3196eaa7ab871bf76fc98d36
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 3 21:37:14 2008 +0000

    clockevents: prevent endless loop lockup
    
    The C1E/HPET bug reports on AMDX2/RS690 systems where tracked down to a
    too small value of the HPET minumum delta for programming an event.
    
    The clockevents code needs to enforce an interrupt event on the clock event
    device in some cases. The enforcement code was stupid and naive, as it just
    added the minimum delta to the current time and tried to reprogram the device.
    When the minimum delta is too small, then this loops forever.
    
    Add a sanity check. Allow reprogramming to fail 3 times, then print a warning
    and double the minimum delta value to make sure, that this does not happen again.
    Use the same function for both tick-oneshot and tick-broadcast code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 5744f40b2697..2bc1f046151c 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -372,16 +372,8 @@ cpumask_t *tick_get_broadcast_oneshot_mask(void)
 static int tick_broadcast_set_event(ktime_t expires, int force)
 {
 	struct clock_event_device *bc = tick_broadcast_device.evtdev;
-	ktime_t now = ktime_get();
-	int res;
-
-	for(;;) {
-		res = clockevents_program_event(bc, expires, now);
-		if (!res || !force)
-			return res;
-		now = ktime_get();
-		expires = ktime_add(now, ktime_set(0, bc->min_delta_ns));
-	}
+
+	return tick_dev_program_event(bc, expires, force);
 }
 
 int tick_resume_broadcast_oneshot(struct clock_event_device *bc)

commit 9c17bcda991000351cb2373f78be7e4b1c44caa3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 3 21:37:08 2008 +0000

    clockevents: prevent multiple init/shutdown
    
    While chasing the C1E/HPET bugreports I went through the clock events
    code inch by inch and found that the broadcast device can be initialized
    and shutdown multiple times. Multiple shutdowns are not critical, but
    useless waste of time. Multiple initializations are simply broken. Another
    CPU might have the device in use already after the first initialization and
    the second init could just render it unusable again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 3044a88357fa..5744f40b2697 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -210,7 +210,7 @@ static void tick_do_broadcast_on_off(void *why)
 	struct clock_event_device *bc, *dev;
 	struct tick_device *td;
 	unsigned long flags, *reason = why;
-	int cpu;
+	int cpu, bc_stopped;
 
 	spin_lock_irqsave(&tick_broadcast_lock, flags);
 
@@ -228,6 +228,8 @@ static void tick_do_broadcast_on_off(void *why)
 	if (!tick_device_is_functional(dev))
 		goto out;
 
+	bc_stopped = cpus_empty(tick_broadcast_mask);
+
 	switch (*reason) {
 	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
 	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
@@ -250,9 +252,10 @@ static void tick_do_broadcast_on_off(void *why)
 		break;
 	}
 
-	if (cpus_empty(tick_broadcast_mask))
-		clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
-	else {
+	if (cpus_empty(tick_broadcast_mask)) {
+		if (!bc_stopped)
+			clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
+	} else if (bc_stopped) {
 		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
 			tick_broadcast_start_periodic(bc);
 		else
@@ -501,9 +504,12 @@ static void tick_broadcast_clear_oneshot(int cpu)
  */
 void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 {
-	bc->event_handler = tick_handle_oneshot_broadcast;
-	clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
-	bc->next_event.tv64 = KTIME_MAX;
+	/* Set it up only once ! */
+	if (bc->event_handler != tick_handle_oneshot_broadcast) {
+		bc->event_handler = tick_handle_oneshot_broadcast;
+		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
+		bc->next_event.tv64 = KTIME_MAX;
+	}
 }
 
 /*

commit d4496b39559c6d43f83e4c08b899984f8b8089b5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Sep 3 21:36:57 2008 +0000

    clockevents: prevent endless loop in periodic broadcast handler
    
    The reprogramming of the periodic broadcast handler was broken,
    when the first programming returned -ETIME. The clockevents code
    stores the new expiry value in the clock events device next_event field
    only when the programming time has not been elapsed yet. The loop in
    question calculates the new expiry value from the next_event value
    and therefor never increases.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 31463d370b94..3044a88357fa 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -175,6 +175,8 @@ static void tick_do_periodic_broadcast(void)
  */
 static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 {
+	ktime_t next;
+
 	tick_do_periodic_broadcast();
 
 	/*
@@ -185,10 +187,13 @@ static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 
 	/*
 	 * Setup the next period for devices, which do not have
-	 * periodic mode:
+	 * periodic mode. We read dev->next_event first and add to it
+	 * when the event alrady expired. clockevents_program_event()
+	 * sets dev->next_event only when the event is really
+	 * programmed to the device.
 	 */
-	for (;;) {
-		ktime_t next = ktime_add(dev->next_event, tick_period);
+	for (next = dev->next_event; ;) {
+		next = ktime_add(next, tick_period);
 
 		if (!clockevents_program_event(dev, next, ktime_get()))
 			return;

commit 82638844d9a8581bbf33201cc209a14876eca167
Merge: 9982fbface82 63cf13b77ab7
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jul 16 00:29:07 2008 +0200

    Merge branch 'linus' into cpus4096
    
    Conflicts:
    
            arch/x86/xen/smp.c
            kernel/sched_rt.c
            net/iucv/iucv.c
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 1a781a777b2f6ac46523fe92396215762ced624d
Merge: b9d2252c1e44 42a2f217a5e3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jul 15 21:55:59 2008 +0200

    Merge branch 'generic-ipi' into generic-ipi-for-linus
    
    Conflicts:
    
            arch/powerpc/Kconfig
            arch/s390/kernel/time.c
            arch/x86/kernel/apic_32.c
            arch/x86/kernel/cpu/perfctr-watchdog.c
            arch/x86/kernel/i8259_64.c
            arch/x86/kernel/ldt.c
            arch/x86/kernel/nmi_64.c
            arch/x86/kernel/smpboot.c
            arch/x86/xen/smp.c
            include/asm-x86/hw_irq_32.h
            include/asm-x86/hw_irq_64.h
            include/asm-x86/mach-default/irq_vectors.h
            include/asm-x86/mach-voyager/irq_vectors.h
            include/asm-x86/smp.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit aa276e1cafb3ce9d01d1e837bcd67e92616013ac
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 9 19:15:00 2008 +0200

    x86, clockevents: add C1E aware idle function
    
    C1E on AMD machines is like C3 but without control from the OS. Up to
    now we disabled the local apic timer for those machines as it stops
    when the CPU goes into C1E. This excludes those machines from high
    resolution timers / dynamic ticks, which hurts especially X2 based
    laptops.
    
    The current boot time C1E detection has another, more serious flaw
    as well: some BIOSes do not enable C1E until the ACPI processor module
    is loaded. This causes systems to stop working after that point.
    
    To work nicely with C1E enabled machines we use a separate idle
    function, which checks on idle entry whether C1E was enabled in the
    Interrupt Pending Message MSR. This allows us to do timer broadcasting
    for C1E and covers the late enablement of C1E as well.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 57a1f02e5ec0..67f80c261709 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -30,6 +30,7 @@
 struct tick_device tick_broadcast_device;
 static cpumask_t tick_broadcast_mask;
 static DEFINE_SPINLOCK(tick_broadcast_lock);
+static int tick_broadcast_force;
 
 #ifdef CONFIG_TICK_ONESHOT
 static void tick_broadcast_clear_oneshot(int cpu);
@@ -232,10 +233,11 @@ static void tick_do_broadcast_on_off(void *why)
 						     CLOCK_EVT_MODE_SHUTDOWN);
 		}
 		if (*reason == CLOCK_EVT_NOTIFY_BROADCAST_FORCE)
-			dev->features |= CLOCK_EVT_FEAT_DUMMY;
+			tick_broadcast_force = 1;
 		break;
 	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
-		if (cpu_isset(cpu, tick_broadcast_mask)) {
+		if (!tick_broadcast_force &&
+		    cpu_isset(cpu, tick_broadcast_mask)) {
 			cpu_clear(cpu, tick_broadcast_mask);
 			if (td->mode == TICKDEV_MODE_PERIODIC)
 				tick_setup_periodic(dev, 0);

commit 8691e5a8f691cc2a4fda0651e8d307aaba0e7d68
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jun 6 11:18:06 2008 +0200

    smp_call_function: get rid of the unused nonatomic/retry argument
    
    It's never used and the comments refer to nonatomic and retry
    interchangably. So get rid of it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 57a1f02e5ec0..75e718539dcb 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -266,7 +266,7 @@ void tick_broadcast_on_off(unsigned long reason, int *oncpu)
 		       "offline CPU #%d\n", *oncpu);
 	else
 		smp_call_function_single(*oncpu, tick_do_broadcast_on_off,
-					 &reason, 1, 1);
+					 &reason, 1);
 }
 
 /*

commit cad0e458d17c643c20c1d38f45a1d26125e6a622
Author: Mike Travis <travis@sgi.com>
Date:   Mon May 12 21:21:13 2008 +0200

    clocksource/events: use performance variant for_each_cpu_mask_nr
    
    Change references from for_each_cpu_mask to for_each_cpu_mask_nr
    where appropriate
    
    Reviewed-by: Paul Jackson <pj@sgi.com>
    Reviewed-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 57a1f02e5ec0..2d0a96346259 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -397,8 +397,7 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 	mask = CPU_MASK_NONE;
 	now = ktime_get();
 	/* Find all expired events */
-	for (cpu = first_cpu(tick_broadcast_oneshot_mask); cpu != NR_CPUS;
-	     cpu = next_cpu(cpu, tick_broadcast_oneshot_mask)) {
+	for_each_cpu_mask_nr(cpu, tick_broadcast_oneshot_mask) {
 		td = &per_cpu(tick_cpu_device, cpu);
 		if (td->evtdev->next_event.tv64 <= now.tv64)
 			cpu_set(cpu, mask);

commit 833df317f9ada91488898b005f4641bb674a3bae
Author: Glauber Costa <gcosta@redhat.com>
Date:   Fri Apr 18 13:38:58 2008 -0700

    clockevents: fix typo in tick-broadcast.c
    
    braodcast -> broadcast
    
    Signed-off-by: Glauber Costa <gcosta@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index fdfa0c745bb6..57a1f02e5ec0 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -262,7 +262,7 @@ static void tick_do_broadcast_on_off(void *why)
 void tick_broadcast_on_off(unsigned long reason, int *oncpu)
 {
 	if (!cpu_isset(*oncpu, cpu_online_map))
-		printk(KERN_ERR "tick-braodcast: ignoring broadcast for "
+		printk(KERN_ERR "tick-broadcast: ignoring broadcast for "
 		       "offline CPU #%d\n", *oncpu);
 	else
 		smp_call_function_single(*oncpu, tick_do_broadcast_on_off,

commit d7b906897e9caae452947e33674df0a2d6f7e10f
Author: Russell King <rmk+lkml@arm.linux.org.uk>
Date:   Thu Apr 17 07:46:24 2008 +0200

    [S390] genirq/clockevents: move irq affinity prototypes/inlines to interrupt.h
    
    > Generic code is not supposed to include irq.h. Replace this include
    > by linux/hardirq.h instead and add/replace an include of linux/irq.h
    > in asm header files where necessary.
    > This change should only matter for architectures that make use of
    > GENERIC_CLOCKEVENTS.
    > Architectures in question are mips, x86, arm, sh, powerpc, uml and sparc64.
    >
    > I did some cross compile tests for mips, x86_64, arm, powerpc and sparc64.
    > This patch fixes also build breakages caused by the include replacement in
    > tick-common.h.
    
    I generally dislike adding optional linux/* includes in asm/* includes -
    I'm nervous about this causing include loops.
    
    However, there's a separate point to be discussed here.
    
    That is, what interfaces are expected of every architecture in the kernel.
    If generic code wants to be able to set the affinity of interrupts, then
    that needs to become part of the interfaces listed in linux/interrupt.h
    rather than linux/irq.h.
    
    So what I suggest is this approach instead (against Linus' tree of a
    couple of days ago) - we move irq_set_affinity() and irq_can_set_affinity()
    to linux/interrupt.h, change the linux/irq.h includes to linux/interrupt.h
    and include asm/irq_regs.h where needed (asm/irq_regs.h is supposed to be
    rarely used include since not much touches the stacked parent context
    registers.)
    
    Build tested on ARM PXA family kernels and ARM's Realview platform
    kernels which both use genirq.
    
    [ tglx@linutronix.de: add GENERIC_HARDIRQ dependencies ]
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index e1bd50cbbf5d..fdfa0c745bb6 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -14,7 +14,7 @@
 #include <linux/cpu.h>
 #include <linux/err.h>
 #include <linux/hrtimer.h>
-#include <linux/irq.h>
+#include <linux/interrupt.h>
 #include <linux/percpu.h>
 #include <linux/profile.h>
 #include <linux/sched.h>

commit 186e3cb8a465bac010ee3b020768d2fa2b505aef
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jan 30 13:30:01 2008 +0100

    timer: clean up tick-broadcast.c
    
    clean up tick-broadcast.c
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 5b86698faa0b..e1bd50cbbf5d 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -126,9 +126,9 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 /*
  * Broadcast the event to the cpus, which are set in the mask
  */
-int tick_do_broadcast(cpumask_t mask)
+static void tick_do_broadcast(cpumask_t mask)
 {
-	int ret = 0, cpu = smp_processor_id();
+	int cpu = smp_processor_id();
 	struct tick_device *td;
 
 	/*
@@ -138,7 +138,6 @@ int tick_do_broadcast(cpumask_t mask)
 		cpu_clear(cpu, mask);
 		td = &per_cpu(tick_cpu_device, cpu);
 		td->evtdev->event_handler(td->evtdev);
-		ret = 1;
 	}
 
 	if (!cpus_empty(mask)) {
@@ -151,9 +150,7 @@ int tick_do_broadcast(cpumask_t mask)
 		cpu = first_cpu(mask);
 		td = &per_cpu(tick_cpu_device, cpu);
 		td->evtdev->broadcast(mask);
-		ret = 1;
 	}
-	return ret;
 }
 
 /*

commit cdc6f27d9e3c2f7ca1a3e19c6eabb1ad6a2add5d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Dec 18 18:05:58 2007 +0100

    clockevents: fix reprogramming decision in oneshot broadcast
    
    Resolve the following regression of a choppy, almost unusable laptop:
    
     http://lkml.org/lkml/2007/12/7/299
     http://bugzilla.kernel.org/show_bug.cgi?id=9525
    
    A previous version of the code did the reprogramming of the broadcast
    device in the return from idle code. This was removed, but the logic in
    tick_handle_oneshot_broadcast() was kept the same.
    
    When a broadcast interrupt happens we signal the expiry to all CPUs
    which have an expired event. If none of the CPUs has an expired event,
    which can happen in dyntick mode, then we reprogram the broadcast
    device. We do not reprogram otherwise, but this is only correct if all
    CPUs, which are in the idle broadcast state have been woken up.
    
    The code ignores, that there might be pending not yet expired events on
    other CPUs, which are in the idle broadcast state. So the delivery of
    those events can be delayed for quite a time.
    
    Change the tick_handle_oneshot_broadcast() function to check for CPUs,
    which are in broadcast state and are not woken up by the current event,
    and enforce the rearming of the broadcast device for those CPUs.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index aa82d7bf478a..5b86698faa0b 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -383,33 +383,6 @@ int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
 	return 0;
 }
 
-/*
- * Reprogram the broadcast device:
- *
- * Called with tick_broadcast_lock held and interrupts disabled.
- */
-static int tick_broadcast_reprogram(void)
-{
-	ktime_t expires = { .tv64 = KTIME_MAX };
-	struct tick_device *td;
-	int cpu;
-
-	/*
-	 * Find the event which expires next:
-	 */
-	for (cpu = first_cpu(tick_broadcast_oneshot_mask); cpu != NR_CPUS;
-	     cpu = next_cpu(cpu, tick_broadcast_oneshot_mask)) {
-		td = &per_cpu(tick_cpu_device, cpu);
-		if (td->evtdev->next_event.tv64 < expires.tv64)
-			expires = td->evtdev->next_event;
-	}
-
-	if (expires.tv64 == KTIME_MAX)
-		return 0;
-
-	return tick_broadcast_set_event(expires, 0);
-}
-
 /*
  * Handle oneshot mode broadcasting
  */
@@ -417,12 +390,13 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 {
 	struct tick_device *td;
 	cpumask_t mask;
-	ktime_t now;
+	ktime_t now, next_event;
 	int cpu;
 
 	spin_lock(&tick_broadcast_lock);
 again:
 	dev->next_event.tv64 = KTIME_MAX;
+	next_event.tv64 = KTIME_MAX;
 	mask = CPU_MASK_NONE;
 	now = ktime_get();
 	/* Find all expired events */
@@ -431,19 +405,31 @@ static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
 		td = &per_cpu(tick_cpu_device, cpu);
 		if (td->evtdev->next_event.tv64 <= now.tv64)
 			cpu_set(cpu, mask);
+		else if (td->evtdev->next_event.tv64 < next_event.tv64)
+			next_event.tv64 = td->evtdev->next_event.tv64;
 	}
 
 	/*
-	 * Wakeup the cpus which have an expired event. The broadcast
-	 * device is reprogrammed in the return from idle code.
+	 * Wakeup the cpus which have an expired event.
+	 */
+	tick_do_broadcast(mask);
+
+	/*
+	 * Two reasons for reprogram:
+	 *
+	 * - The global event did not expire any CPU local
+	 * events. This happens in dyntick mode, as the maximum PIT
+	 * delta is quite small.
+	 *
+	 * - There are pending events on sleeping CPUs which were not
+	 * in the event mask
 	 */
-	if (!tick_do_broadcast(mask)) {
+	if (next_event.tv64 != KTIME_MAX) {
 		/*
-		 * The global event did not expire any CPU local
-		 * events. This happens in dyntick mode, as the
-		 * maximum PIT delta is quite small.
+		 * Rearm the broadcast device. If event expired,
+		 * repeat the above
 		 */
-		if (tick_broadcast_reprogram())
+		if (tick_broadcast_set_event(next_event, 0))
 			goto again;
 	}
 	spin_unlock(&tick_broadcast_lock);

commit 8dce39c231af554932f8ab0d671e077ab6db9e46
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Nov 5 14:51:10 2007 -0800

    time: fix inconsistent function names in comments
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 8cfb8b2ce773..aa82d7bf478a 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -508,7 +508,7 @@ static void tick_broadcast_clear_oneshot(int cpu)
 }
 
 /**
- * tick_broadcast_setup_highres - setup the broadcast device for highres
+ * tick_broadcast_setup_oneshot - setup the broadcast device
  */
 void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 {

commit 3dfbc88464934fb6924a388bc03961d44f387926
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 17 18:04:32 2007 +0200

    x86: C1E late detection fix. Really switch off lapic timer
    
    Doh, I completely missed that devices marked DUMMY are not running
    the set_mode function. So we force broadcasting, but we keep the
    local APIC timer running.
    
    Let the clock event layer mark the device _after_ switching it off.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index fab9dd8bbd6b..8cfb8b2ce773 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -222,20 +222,8 @@ static void tick_do_broadcast_on_off(void *why)
 	if (!dev || !(dev->features & CLOCK_EVT_FEAT_C3STOP))
 		goto out;
 
-	/*
-	 * Defect device ?
-	 */
-	if (!tick_device_is_functional(dev)) {
-		/*
-		 * AMD C1E wreckage fixup:
-		 *
-		 * Device was registered functional in the first
-		 * place. Now the secondary CPU detected the C1E
-		 * misfeature and notifies us to fix it up
-		 */
-		if (*reason != CLOCK_EVT_NOTIFY_BROADCAST_FORCE)
-			goto out;
-	}
+	if (!tick_device_is_functional(dev))
+		goto out;
 
 	switch (*reason) {
 	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
@@ -246,6 +234,8 @@ static void tick_do_broadcast_on_off(void *why)
 				clockevents_set_mode(dev,
 						     CLOCK_EVT_MODE_SHUTDOWN);
 		}
+		if (*reason == CLOCK_EVT_NOTIFY_BROADCAST_FORCE)
+			dev->features |= CLOCK_EVT_FEAT_DUMMY;
 		break;
 	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
 		if (cpu_isset(cpu, tick_broadcast_mask)) {

commit bf020cb7b3918e186309db21d75cb91ebafc9d6f
Author: Avi Kivity <avi@qumranet.com>
Date:   Tue Oct 16 23:26:24 2007 -0700

    time: simplify smp_call_function_single() call sequence
    
    smp_call_function_single() now knows how to call the function on the
    current cpu.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index fc3fc79b3d59..fab9dd8bbd6b 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -274,21 +274,12 @@ static void tick_do_broadcast_on_off(void *why)
  */
 void tick_broadcast_on_off(unsigned long reason, int *oncpu)
 {
-	int cpu = get_cpu();
-
-	if (!cpu_isset(*oncpu, cpu_online_map)) {
+	if (!cpu_isset(*oncpu, cpu_online_map))
 		printk(KERN_ERR "tick-braodcast: ignoring broadcast for "
 		       "offline CPU #%d\n", *oncpu);
-	} else {
-
-		if (cpu == *oncpu)
-			tick_do_broadcast_on_off(&reason);
-		else
-			smp_call_function_single(*oncpu,
-						 tick_do_broadcast_on_off,
-						 &reason, 1, 1);
-	}
-	put_cpu();
+	else
+		smp_call_function_single(*oncpu, tick_do_broadcast_on_off,
+					 &reason, 1, 1);
 }
 
 /*

commit 1595f452f3d8daa066bfd3ba4120754bed3329e1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Oct 14 22:57:45 2007 +0200

    clockevents: introduce force broadcast notifier
    
    The 64bit SMP bootup is slightly different to the 32bit one. It enables
    the boot CPU local APIC timer before all CPUs are brought up. Some AMD C1E
    systems have the C1E feature flag only set in the secondary CPU. Due to
    the early enable of the boot CPU local APIC timer the APIC timer is
    registered as a fully functional device. When we detect the wreckage during
    the bringup of the secondary CPU, we need to force the boot CPU into
    broadcast mode.
    
    Add a new notifier reason and implement the force broadcast in the clock
    events layer.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 298bc7c6f09f..fc3fc79b3d59 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -217,26 +217,43 @@ static void tick_do_broadcast_on_off(void *why)
 	bc = tick_broadcast_device.evtdev;
 
 	/*
-	 * Is the device in broadcast mode forever or is it not
-	 * affected by the powerstate ?
+	 * Is the device not affected by the powerstate ?
 	 */
-	if (!dev || !tick_device_is_functional(dev) ||
-	    !(dev->features & CLOCK_EVT_FEAT_C3STOP))
+	if (!dev || !(dev->features & CLOCK_EVT_FEAT_C3STOP))
 		goto out;
 
-	if (*reason == CLOCK_EVT_NOTIFY_BROADCAST_ON) {
+	/*
+	 * Defect device ?
+	 */
+	if (!tick_device_is_functional(dev)) {
+		/*
+		 * AMD C1E wreckage fixup:
+		 *
+		 * Device was registered functional in the first
+		 * place. Now the secondary CPU detected the C1E
+		 * misfeature and notifies us to fix it up
+		 */
+		if (*reason != CLOCK_EVT_NOTIFY_BROADCAST_FORCE)
+			goto out;
+	}
+
+	switch (*reason) {
+	case CLOCK_EVT_NOTIFY_BROADCAST_ON:
+	case CLOCK_EVT_NOTIFY_BROADCAST_FORCE:
 		if (!cpu_isset(cpu, tick_broadcast_mask)) {
 			cpu_set(cpu, tick_broadcast_mask);
 			if (td->mode == TICKDEV_MODE_PERIODIC)
 				clockevents_set_mode(dev,
 						     CLOCK_EVT_MODE_SHUTDOWN);
 		}
-	} else {
+		break;
+	case CLOCK_EVT_NOTIFY_BROADCAST_OFF:
 		if (cpu_isset(cpu, tick_broadcast_mask)) {
 			cpu_clear(cpu, tick_broadcast_mask);
 			if (td->mode == TICKDEV_MODE_PERIODIC)
 				tick_setup_periodic(dev, 0);
 		}
+		break;
 	}
 
 	if (cpus_empty(tick_broadcast_mask))

commit 4a93232dab0a07074bcc5291a0f1f39919916f31
Author: Venki Pallipadi <venkatesh.pallipadi@intel.com>
Date:   Fri Oct 12 23:04:23 2007 +0200

    clock events: allow replacement of broadcast timer
    
    Change the broadcast timer, if a timer with higher rating becomes available.
    
    Signed-off-by: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Greg KH <greg@kroah.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index acf15b49e55b..298bc7c6f09f 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -64,8 +64,9 @@ static void tick_broadcast_start_periodic(struct clock_event_device *bc)
  */
 int tick_check_broadcast_device(struct clock_event_device *dev)
 {
-	if (tick_broadcast_device.evtdev ||
-	    (dev->features & CLOCK_EVT_FEAT_C3STOP))
+	if ((tick_broadcast_device.evtdev &&
+	     tick_broadcast_device.evtdev->rating >= dev->rating) ||
+	     (dev->features & CLOCK_EVT_FEAT_C3STOP))
 		return 0;
 
 	clockevents_exchange_device(NULL, dev);
@@ -513,11 +514,9 @@ static void tick_broadcast_clear_oneshot(int cpu)
  */
 void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
 {
-	if (bc->mode != CLOCK_EVT_MODE_ONESHOT) {
-		bc->event_handler = tick_handle_oneshot_broadcast;
-		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
-		bc->next_event.tv64 = KTIME_MAX;
-	}
+	bc->event_handler = tick_handle_oneshot_broadcast;
+	clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
+	bc->next_event.tv64 = KTIME_MAX;
 }
 
 /*

commit c8a1d398de70a7774359b4720c392891cdd485f9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Oct 12 23:04:06 2007 +0200

    clockevents: fix periodic broadcast for oneshot devices
    
    The next_event member of the clock event device is used to keep track
    of the next periodic event. For one shot only devices it is wrong to
    clear the variable, as the next event will be based on it.
    
    Pointed out by Ralf Baechle
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 0962e0577660..acf15b49e55b 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -176,8 +176,6 @@ static void tick_do_periodic_broadcast(void)
  */
 static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
 {
-	dev->next_event.tv64 = KTIME_MAX;
-
 	tick_do_periodic_broadcast();
 
 	/*

commit b7e113dc9d52c4a37d2da6fafe77959f3a28eccf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Sep 22 22:29:06 2007 +0000

    clockevents: remove the suspend/resume workaround^Wthinko
    
    In a desparate attempt to fix the suspend/resume problem on Andrews
    VAIO I added a workaround which enforced the broadcast of the oneshot
    timer on resume. This was actually resolving the problem on the VAIO
    but was just a stupid workaround, which was not tackling the root
    cause: the assignement of lower idle C-States in the ACPI processor_idle
    code. The cpuidle patches, which utilize the dynamic tick feature and
    go faster into deeper C-states exposed the problem again. The correct
    solution is the previous patch, which prevents lower C-states across
    the suspend/resume.
    
    Remove the enforcement code, including the conditional broadcast timer
    arming, which helped to pamper over the real problem for quite a time.
    The oneshot broadcast flag for the cpu, which runs the resume code can
    never be set at the time when this code is executed. It only gets set,
    when the CPU is entering a lower idle C-State.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index aab881c86a1a..0962e0577660 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -382,23 +382,8 @@ static int tick_broadcast_set_event(ktime_t expires, int force)
 
 int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
 {
-	int cpu = smp_processor_id();
-
-	/*
-	 * If the CPU is marked for broadcast, enforce oneshot
-	 * broadcast mode. The jinxed VAIO does not resume otherwise.
-	 * No idea why it ends up in a lower C State during resume
-	 * without notifying the clock events layer.
-	 */
-	if (cpu_isset(cpu, tick_broadcast_mask))
-		cpu_set(cpu, tick_broadcast_oneshot_mask);
-
 	clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
-
-	if(!cpus_empty(tick_broadcast_oneshot_mask))
-		tick_broadcast_set_event(ktime_get(), 1);
-
-	return cpu_isset(cpu, tick_broadcast_oneshot_mask);
+	return 0;
 }
 
 /*

commit 31d9b3938c0459e5e9755ce0a98ac1e24eeff972
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Sep 16 15:36:43 2007 +0200

    clockevents: do not shutdown the oneshot broadcast device
    
    When a cpu goes offline it is removed from the broadcast masks. If the
    mask becomes empty the code shuts down the broadcast device. This is
    wrong, because the broadcast device needs to be ready for the online
    cpu going idle (into a c-state, which stops the local apic timer).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 947959fb2bb5..aab881c86a1a 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -560,20 +560,17 @@ void tick_broadcast_switch_to_oneshot(void)
  */
 void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
 {
-	struct clock_event_device *bc;
 	unsigned long flags;
 	unsigned int cpu = *cpup;
 
 	spin_lock_irqsave(&tick_broadcast_lock, flags);
 
-	bc = tick_broadcast_device.evtdev;
+	/*
+	 * Clear the broadcast mask flag for the dead cpu, but do not
+	 * stop the broadcast device!
+	 */
 	cpu_clear(cpu, tick_broadcast_oneshot_mask);
 
-	if (tick_broadcast_device.mode == TICKDEV_MODE_ONESHOT) {
-		if (bc && cpus_empty(tick_broadcast_oneshot_mask))
-			clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
-	}
-
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 

commit 07eec6af448d13a6a520d9c6f06f2e87f61b567a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Sep 16 15:36:43 2007 +0200

    clockevents: Enforce oneshot broadcast when broadcast mask is set on resume
    
    The jinxed VAIO refuses to resume without hitting keys on the keyboard
    when this is not enforced. It is unclear why the cpu ends up in a lower
    C State without notifying the clock events layer, but enforcing the
    oneshot broadcast here is safe.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index db8e0f3d409b..947959fb2bb5 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -382,12 +382,23 @@ static int tick_broadcast_set_event(ktime_t expires, int force)
 
 int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
 {
+	int cpu = smp_processor_id();
+
+	/*
+	 * If the CPU is marked for broadcast, enforce oneshot
+	 * broadcast mode. The jinxed VAIO does not resume otherwise.
+	 * No idea why it ends up in a lower C State during resume
+	 * without notifying the clock events layer.
+	 */
+	if (cpu_isset(cpu, tick_broadcast_mask))
+		cpu_set(cpu, tick_broadcast_oneshot_mask);
+
 	clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
 
 	if(!cpus_empty(tick_broadcast_oneshot_mask))
 		tick_broadcast_set_event(ktime_get(), 1);
 
-	return cpu_isset(smp_processor_id(), tick_broadcast_oneshot_mask);
+	return cpu_isset(cpu, tick_broadcast_oneshot_mask);
 }
 
 /*

commit 5590a536c0bc403fc73908c66c1c88cbed735ecb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 21 04:37:35 2007 -0700

    clockevents: fix device replacement
    
    When a device is replaced by a better rated device, then the broadcast
    mode needs to be evaluated again. When the new device has no requirement
    for broadcasting, then the broadcast bits for the CPU must be cleared.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 8339af229cb9..db8e0f3d409b 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -31,6 +31,12 @@ struct tick_device tick_broadcast_device;
 static cpumask_t tick_broadcast_mask;
 static DEFINE_SPINLOCK(tick_broadcast_lock);
 
+#ifdef CONFIG_TICK_ONESHOT
+static void tick_broadcast_clear_oneshot(int cpu);
+#else
+static inline void tick_broadcast_clear_oneshot(int cpu) { }
+#endif
+
 /*
  * Debugging: see timer_list.c
  */
@@ -99,8 +105,19 @@ int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
 		cpu_set(cpu, tick_broadcast_mask);
 		tick_broadcast_start_periodic(tick_broadcast_device.evtdev);
 		ret = 1;
-	}
+	} else {
+		/*
+		 * When the new device is not affected by the stop
+		 * feature and the cpu is marked in the broadcast mask
+		 * then clear the broadcast bit.
+		 */
+		if (!(dev->features & CLOCK_EVT_FEAT_C3STOP)) {
+			int cpu = smp_processor_id();
 
+			cpu_clear(cpu, tick_broadcast_mask);
+			tick_broadcast_clear_oneshot(cpu);
+		}
+	}
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 	return ret;
 }
@@ -487,6 +504,16 @@ void tick_broadcast_oneshot_control(unsigned long reason)
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
+/*
+ * Reset the one shot broadcast for a cpu
+ *
+ * Called with tick_broadcast_lock held
+ */
+static void tick_broadcast_clear_oneshot(int cpu)
+{
+	cpu_clear(cpu, tick_broadcast_oneshot_mask);
+}
+
 /**
  * tick_broadcast_setup_highres - setup the broadcast device for highres
  */

commit 18de5bc4c1f1f1fa5e14f354a7603bd6e9d4e3b6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 21 04:37:34 2007 -0700

    clockevents: fix resume logic
    
    We need to make sure, that the clockevent devices are resumed, before
    the tick is resumed. The current resume logic does not guarantee this.
    
    Add CLOCK_EVT_MODE_RESUME and call the set mode functions of the clock
    event devices before resuming the tick / oneshot functionality.
    
    Fixup the existing users.
    
    Thanks to Nigel Cunningham for tracking down a long standing thinko,
    which affected the jinxed VAIO.
    
    [akpm@linux-foundation.org: xen build fix]
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 8001d37071f5..8339af229cb9 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -49,7 +49,7 @@ cpumask_t *tick_get_broadcast_mask(void)
  */
 static void tick_broadcast_start_periodic(struct clock_event_device *bc)
 {
-	if (bc && bc->mode == CLOCK_EVT_MODE_SHUTDOWN)
+	if (bc)
 		tick_setup_periodic(bc, 1);
 }
 
@@ -299,7 +299,7 @@ void tick_suspend_broadcast(void)
 	spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	bc = tick_broadcast_device.evtdev;
-	if (bc && tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
+	if (bc)
 		clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
 
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
@@ -316,6 +316,8 @@ int tick_resume_broadcast(void)
 	bc = tick_broadcast_device.evtdev;
 
 	if (bc) {
+		clockevents_set_mode(bc, CLOCK_EVT_MODE_RESUME);
+
 		switch (tick_broadcast_device.mode) {
 		case TICKDEV_MODE_PERIODIC:
 			if(!cpus_empty(tick_broadcast_mask))

commit 72fcde966252abd17d70e4e216a0411a34523a8f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 23 13:57:30 2007 -0700

    Ignore bogus ACPI info for offline CPUs
    
    Booting a SMP kernel with maxcpus=1 on a SMP system leads to a hard hang,
    because ACPI ignores the maxcpus setting and sends timer broadcast info for
    the offline CPUs.  This results in a stuck for ever call to
    smp_call_function_single() on an offline CPU.
    
    Ignore the bogus information and print a kernel error to remind ACPI
    folks to fix it.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index eadfce2fff74..8001d37071f5 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -243,11 +243,18 @@ void tick_broadcast_on_off(unsigned long reason, int *oncpu)
 {
 	int cpu = get_cpu();
 
-	if (cpu == *oncpu)
-		tick_do_broadcast_on_off(&reason);
-	else
-		smp_call_function_single(*oncpu, tick_do_broadcast_on_off,
-					 &reason, 1, 1);
+	if (!cpu_isset(*oncpu, cpu_online_map)) {
+		printk(KERN_ERR "tick-braodcast: ignoring broadcast for "
+		       "offline CPU #%d\n", *oncpu);
+	} else {
+
+		if (cpu == *oncpu)
+			tick_do_broadcast_on_off(&reason);
+		else
+			smp_call_function_single(*oncpu,
+						 tick_do_broadcast_on_off,
+						 &reason, 1, 1);
+	}
 	put_cpu();
 }
 

commit cd05a1f818073a623455a58e756c5b419fc98db9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 17 00:25:52 2007 +0100

    [PATCH] clockevents: Fix suspend/resume to disk hangs
    
    I finally found a dual core box, which survives suspend/resume without
    crashing in the middle of nowhere. Sigh, I never figured out from the
    code and the bug reports what's going on.
    
    The observed hangs are caused by a stale state transition of the clock
    event devices, which keeps the RCU synchronization away from completion,
    when the non boot CPU is brought back up.
    
    The suspend/resume in oneshot mode needs the similar care as the
    periodic mode during suspend to RAM. My assumption that the state
    transitions during the different shutdown/bringups of s2disk would go
    through the periodic boot phase and then switch over to highres resp.
    nohz mode were simply wrong.
    
    Add the appropriate suspend / resume handling for the non periodic
    modes.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 5567745470f7..eadfce2fff74 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -307,12 +307,19 @@ int tick_resume_broadcast(void)
 	spin_lock_irqsave(&tick_broadcast_lock, flags);
 
 	bc = tick_broadcast_device.evtdev;
-	if (bc) {
-		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC &&
-		    !cpus_empty(tick_broadcast_mask))
-			tick_broadcast_start_periodic(bc);
 
-		broadcast = cpu_isset(smp_processor_id(), tick_broadcast_mask);
+	if (bc) {
+		switch (tick_broadcast_device.mode) {
+		case TICKDEV_MODE_PERIODIC:
+			if(!cpus_empty(tick_broadcast_mask))
+				tick_broadcast_start_periodic(bc);
+			broadcast = cpu_isset(smp_processor_id(),
+					      tick_broadcast_mask);
+			break;
+		case TICKDEV_MODE_ONESHOT:
+			broadcast = tick_resume_broadcast_oneshot(bc);
+			break;
+		}
 	}
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 
@@ -347,6 +354,16 @@ static int tick_broadcast_set_event(ktime_t expires, int force)
 	}
 }
 
+int tick_resume_broadcast_oneshot(struct clock_event_device *bc)
+{
+	clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
+
+	if(!cpus_empty(tick_broadcast_oneshot_mask))
+		tick_broadcast_set_event(ktime_get(), 1);
+
+	return cpu_isset(smp_processor_id(), tick_broadcast_oneshot_mask);
+}
+
 /*
  * Reprogram the broadcast device:
  *

commit 6321dd60c76b2e12383bc06046288b15397ed3a0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Mar 6 08:25:42 2007 +0100

    [PATCH] Save/restore periodic tick information over suspend/resume
    
    The programming of periodic tick devices needs to be saved/restored
    across suspend/resume - otherwise we might end up with a system coming
    up that relies on getting a PIT (or HPET) interrupt, while those devices
    default to 'no interrupts' after powerup. (To confuse things it worked
    to a certain degree on some systems because the lapic gets initialized
    as a side-effect of SMP bootup.)
    
    This suspend / resume thing was dropped unintentionally during the
    last-minute -mm code reshuffling.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 12b3efeb9f6f..5567745470f7 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -284,6 +284,42 @@ void tick_shutdown_broadcast(unsigned int *cpup)
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
 
+void tick_suspend_broadcast(void)
+{
+	struct clock_event_device *bc;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tick_broadcast_lock, flags);
+
+	bc = tick_broadcast_device.evtdev;
+	if (bc && tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
+		clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
+
+	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+}
+
+int tick_resume_broadcast(void)
+{
+	struct clock_event_device *bc;
+	unsigned long flags;
+	int broadcast = 0;
+
+	spin_lock_irqsave(&tick_broadcast_lock, flags);
+
+	bc = tick_broadcast_device.evtdev;
+	if (bc) {
+		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC &&
+		    !cpus_empty(tick_broadcast_mask))
+			tick_broadcast_start_periodic(bc);
+
+		broadcast = cpu_isset(smp_processor_id(), tick_broadcast_mask);
+	}
+	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+
+	return broadcast;
+}
+
+
 #ifdef CONFIG_TICK_ONESHOT
 
 static cpumask_t tick_broadcast_oneshot_mask;

commit 289f480af87e45f7a6de6ba9b4c061c2e259fe98
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 16 01:28:15 2007 -0800

    [PATCH] Add debugging feature /proc/timer_list
    
    add /proc/timer_list, which prints all currently pending (high-res) timers,
    all clock-event sources and their parameters in a human-readable form.
    
    Sample output:
    
    Timer List Version: v0.1
    HRTIMER_MAX_CLOCK_BASES: 2
    now at 4246046273872 nsecs
    
    cpu: 0
     clock 0:
      .index:      0
      .resolution: 1 nsecs
      .get_time:   ktime_get_real
      .offset:     1273998312645738432 nsecs
    active timers:
     clock 1:
      .index:      1
      .resolution: 1 nsecs
      .get_time:   ktime_get
      .offset:     0 nsecs
    active timers:
     #0: <f5a90ec8>, hrtimer_sched_tick, hrtimer_stop_sched_tick, swapper/0
     # expires at 4246432689566 nsecs [in 386415694 nsecs]
     #1: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, pcscd/2050
     # expires at 4247018194689 nsecs [in 971920817 nsecs]
     #2: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, irqbalance/1909
     # expires at 4247351358392 nsecs [in 1305084520 nsecs]
     #3: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, crond/2157
     # expires at 4249097614968 nsecs [in 3051341096 nsecs]
     #4: <f5a90ec8>, it_real_fn, do_setitimer, syslogd/1888
     # expires at 4251329900926 nsecs [in 5283627054 nsecs]
      .expires_next   : 4246432689566 nsecs
      .hres_active    : 1
      .check_clocks   : 0
      .nr_events      : 31306
      .idle_tick      : 4246020791890 nsecs
      .tick_stopped   : 1
      .idle_jiffies   : 986504
      .idle_calls     : 40700
      .idle_sleeps    : 36014
      .idle_entrytime : 4246019418883 nsecs
      .idle_sleeptime : 4178181972709 nsecs
    
    cpu: 1
     clock 0:
      .index:      0
      .resolution: 1 nsecs
      .get_time:   ktime_get_real
      .offset:     1273998312645738432 nsecs
    active timers:
     clock 1:
      .index:      1
      .resolution: 1 nsecs
      .get_time:   ktime_get
      .offset:     0 nsecs
    active timers:
     #0: <f5a90ec8>, hrtimer_sched_tick, hrtimer_restart_sched_tick, swapper/0
     # expires at 4246050084568 nsecs [in 3810696 nsecs]
     #1: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, atd/2227
     # expires at 4261010635003 nsecs [in 14964361131 nsecs]
     #2: <f5a90ec8>, hrtimer_wakeup, do_nanosleep, smartd/2332
     # expires at 5469485798970 nsecs [in 1223439525098 nsecs]
      .expires_next   : 4246050084568 nsecs
      .hres_active    : 1
      .check_clocks   : 0
      .nr_events      : 24043
      .idle_tick      : 4246046084568 nsecs
      .tick_stopped   : 0
      .idle_jiffies   : 986510
      .idle_calls     : 26360
      .idle_sleeps    : 22551
      .idle_entrytime : 4246043874339 nsecs
      .idle_sleeptime : 4170763761184 nsecs
    
    tick_broadcast_mask: 00000003
    event_broadcast_mask: 00000001
    
    CPU#0's local event device:
    
    Clock Event Device: lapic
     capabilities:   0000000e
     max_delta_ns:   807385544
     min_delta_ns:   1443
     mult:           44624025
     shift:          32
     set_next_event: lapic_next_event
     set_mode:       lapic_timer_setup
     event_handler:  hrtimer_interrupt
      .installed:  1
      .expires:    4246432689566 nsecs
    
    CPU#1's local event device:
    
    Clock Event Device: lapic
     capabilities:   0000000e
     max_delta_ns:   807385544
     min_delta_ns:   1443
     mult:           44624025
     shift:          32
     set_next_event: lapic_next_event
     set_mode:       lapic_timer_setup
     event_handler:  hrtimer_interrupt
      .installed:  1
      .expires:    4246050084568 nsecs
    
    Clock Event Device: hpet
     capabilities:   00000007
     max_delta_ns:   2147483647
     min_delta_ns:   3352
     mult:           61496110
     shift:          32
     set_next_event: hpet_next_event
     set_mode:       hpet_set_mode
     event_handler:  handle_nextevt_broadcast
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 8314ecb32d33..12b3efeb9f6f 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -31,6 +31,19 @@ struct tick_device tick_broadcast_device;
 static cpumask_t tick_broadcast_mask;
 static DEFINE_SPINLOCK(tick_broadcast_lock);
 
+/*
+ * Debugging: see timer_list.c
+ */
+struct tick_device *tick_get_broadcast_device(void)
+{
+	return &tick_broadcast_device;
+}
+
+cpumask_t *tick_get_broadcast_mask(void)
+{
+	return &tick_broadcast_mask;
+}
+
 /*
  * Start the device in periodic mode
  */
@@ -275,6 +288,14 @@ void tick_shutdown_broadcast(unsigned int *cpup)
 
 static cpumask_t tick_broadcast_oneshot_mask;
 
+/*
+ * Debugging: see timer_list.c
+ */
+cpumask_t *tick_get_broadcast_oneshot_mask(void)
+{
+	return &tick_broadcast_oneshot_mask;
+}
+
 static int tick_broadcast_set_event(ktime_t expires, int force)
 {
 	struct clock_event_device *bc = tick_broadcast_device.evtdev;

commit 79bf2bb335b85db25d27421c798595a2fa2a0e82
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 16 01:28:03 2007 -0800

    [PATCH] tick-management: dyntick / highres functionality
    
    With Ingo Molnar <mingo@elte.hu>
    
    Add functions to provide dynamic ticks and high resolution timers.  The code
    which keeps track of jiffies and handles the long idle periods is shared
    between tick based and high resolution timer based dynticks.  The dyntick
    functionality can be disabled on the kernel commandline.  Provide also the
    infrastructure to support high resolution timers.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
index 0ee4968ff791..8314ecb32d33 100644
--- a/kernel/time/tick-broadcast.c
+++ b/kernel/time/tick-broadcast.c
@@ -29,7 +29,7 @@
 
 struct tick_device tick_broadcast_device;
 static cpumask_t tick_broadcast_mask;
-DEFINE_SPINLOCK(tick_broadcast_lock);
+static DEFINE_SPINLOCK(tick_broadcast_lock);
 
 /*
  * Start the device in periodic mode
@@ -215,6 +215,8 @@ static void tick_do_broadcast_on_off(void *why)
 	else {
 		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
 			tick_broadcast_start_periodic(bc);
+		else
+			tick_broadcast_setup_oneshot(bc);
 	}
 out:
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
@@ -268,3 +270,190 @@ void tick_shutdown_broadcast(unsigned int *cpup)
 
 	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
 }
+
+#ifdef CONFIG_TICK_ONESHOT
+
+static cpumask_t tick_broadcast_oneshot_mask;
+
+static int tick_broadcast_set_event(ktime_t expires, int force)
+{
+	struct clock_event_device *bc = tick_broadcast_device.evtdev;
+	ktime_t now = ktime_get();
+	int res;
+
+	for(;;) {
+		res = clockevents_program_event(bc, expires, now);
+		if (!res || !force)
+			return res;
+		now = ktime_get();
+		expires = ktime_add(now, ktime_set(0, bc->min_delta_ns));
+	}
+}
+
+/*
+ * Reprogram the broadcast device:
+ *
+ * Called with tick_broadcast_lock held and interrupts disabled.
+ */
+static int tick_broadcast_reprogram(void)
+{
+	ktime_t expires = { .tv64 = KTIME_MAX };
+	struct tick_device *td;
+	int cpu;
+
+	/*
+	 * Find the event which expires next:
+	 */
+	for (cpu = first_cpu(tick_broadcast_oneshot_mask); cpu != NR_CPUS;
+	     cpu = next_cpu(cpu, tick_broadcast_oneshot_mask)) {
+		td = &per_cpu(tick_cpu_device, cpu);
+		if (td->evtdev->next_event.tv64 < expires.tv64)
+			expires = td->evtdev->next_event;
+	}
+
+	if (expires.tv64 == KTIME_MAX)
+		return 0;
+
+	return tick_broadcast_set_event(expires, 0);
+}
+
+/*
+ * Handle oneshot mode broadcasting
+ */
+static void tick_handle_oneshot_broadcast(struct clock_event_device *dev)
+{
+	struct tick_device *td;
+	cpumask_t mask;
+	ktime_t now;
+	int cpu;
+
+	spin_lock(&tick_broadcast_lock);
+again:
+	dev->next_event.tv64 = KTIME_MAX;
+	mask = CPU_MASK_NONE;
+	now = ktime_get();
+	/* Find all expired events */
+	for (cpu = first_cpu(tick_broadcast_oneshot_mask); cpu != NR_CPUS;
+	     cpu = next_cpu(cpu, tick_broadcast_oneshot_mask)) {
+		td = &per_cpu(tick_cpu_device, cpu);
+		if (td->evtdev->next_event.tv64 <= now.tv64)
+			cpu_set(cpu, mask);
+	}
+
+	/*
+	 * Wakeup the cpus which have an expired event. The broadcast
+	 * device is reprogrammed in the return from idle code.
+	 */
+	if (!tick_do_broadcast(mask)) {
+		/*
+		 * The global event did not expire any CPU local
+		 * events. This happens in dyntick mode, as the
+		 * maximum PIT delta is quite small.
+		 */
+		if (tick_broadcast_reprogram())
+			goto again;
+	}
+	spin_unlock(&tick_broadcast_lock);
+}
+
+/*
+ * Powerstate information: The system enters/leaves a state, where
+ * affected devices might stop
+ */
+void tick_broadcast_oneshot_control(unsigned long reason)
+{
+	struct clock_event_device *bc, *dev;
+	struct tick_device *td;
+	unsigned long flags;
+	int cpu;
+
+	spin_lock_irqsave(&tick_broadcast_lock, flags);
+
+	/*
+	 * Periodic mode does not care about the enter/exit of power
+	 * states
+	 */
+	if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
+		goto out;
+
+	bc = tick_broadcast_device.evtdev;
+	cpu = smp_processor_id();
+	td = &per_cpu(tick_cpu_device, cpu);
+	dev = td->evtdev;
+
+	if (!(dev->features & CLOCK_EVT_FEAT_C3STOP))
+		goto out;
+
+	if (reason == CLOCK_EVT_NOTIFY_BROADCAST_ENTER) {
+		if (!cpu_isset(cpu, tick_broadcast_oneshot_mask)) {
+			cpu_set(cpu, tick_broadcast_oneshot_mask);
+			clockevents_set_mode(dev, CLOCK_EVT_MODE_SHUTDOWN);
+			if (dev->next_event.tv64 < bc->next_event.tv64)
+				tick_broadcast_set_event(dev->next_event, 1);
+		}
+	} else {
+		if (cpu_isset(cpu, tick_broadcast_oneshot_mask)) {
+			cpu_clear(cpu, tick_broadcast_oneshot_mask);
+			clockevents_set_mode(dev, CLOCK_EVT_MODE_ONESHOT);
+			if (dev->next_event.tv64 != KTIME_MAX)
+				tick_program_event(dev->next_event, 1);
+		}
+	}
+
+out:
+	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+}
+
+/**
+ * tick_broadcast_setup_highres - setup the broadcast device for highres
+ */
+void tick_broadcast_setup_oneshot(struct clock_event_device *bc)
+{
+	if (bc->mode != CLOCK_EVT_MODE_ONESHOT) {
+		bc->event_handler = tick_handle_oneshot_broadcast;
+		clockevents_set_mode(bc, CLOCK_EVT_MODE_ONESHOT);
+		bc->next_event.tv64 = KTIME_MAX;
+	}
+}
+
+/*
+ * Select oneshot operating mode for the broadcast device
+ */
+void tick_broadcast_switch_to_oneshot(void)
+{
+	struct clock_event_device *bc;
+	unsigned long flags;
+
+	spin_lock_irqsave(&tick_broadcast_lock, flags);
+
+	tick_broadcast_device.mode = TICKDEV_MODE_ONESHOT;
+	bc = tick_broadcast_device.evtdev;
+	if (bc)
+		tick_broadcast_setup_oneshot(bc);
+	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+}
+
+
+/*
+ * Remove a dead CPU from broadcasting
+ */
+void tick_shutdown_broadcast_oneshot(unsigned int *cpup)
+{
+	struct clock_event_device *bc;
+	unsigned long flags;
+	unsigned int cpu = *cpup;
+
+	spin_lock_irqsave(&tick_broadcast_lock, flags);
+
+	bc = tick_broadcast_device.evtdev;
+	cpu_clear(cpu, tick_broadcast_oneshot_mask);
+
+	if (tick_broadcast_device.mode == TICKDEV_MODE_ONESHOT) {
+		if (bc && cpus_empty(tick_broadcast_oneshot_mask))
+			clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
+	}
+
+	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+}
+
+#endif

commit f8381cba04ba8173fd5a2b8e5cd8b3290ee13a98
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 16 01:28:02 2007 -0800

    [PATCH] tick-management: broadcast functionality
    
    With Ingo Molnar <mingo@elte.hu>
    
    Add broadcast functionality, so per cpu clock event devices can be registered
    as dummy devices or switched from/to broadcast on demand.  The broadcast
    function distributes the events via the broadcast function of the clock event
    device.  This is primarily designed to replace the switch apic timer to / from
    IPI in power states, where the apic stops.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/time/tick-broadcast.c b/kernel/time/tick-broadcast.c
new file mode 100644
index 000000000000..0ee4968ff791
--- /dev/null
+++ b/kernel/time/tick-broadcast.c
@@ -0,0 +1,270 @@
+/*
+ * linux/kernel/time/tick-broadcast.c
+ *
+ * This file contains functions which emulate a local clock-event
+ * device via a broadcast event source.
+ *
+ * Copyright(C) 2005-2006, Thomas Gleixner <tglx@linutronix.de>
+ * Copyright(C) 2005-2007, Red Hat, Inc., Ingo Molnar
+ * Copyright(C) 2006-2007, Timesys Corp., Thomas Gleixner
+ *
+ * This code is licenced under the GPL version 2. For details see
+ * kernel-base/COPYING.
+ */
+#include <linux/cpu.h>
+#include <linux/err.h>
+#include <linux/hrtimer.h>
+#include <linux/irq.h>
+#include <linux/percpu.h>
+#include <linux/profile.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+
+#include "tick-internal.h"
+
+/*
+ * Broadcast support for broken x86 hardware, where the local apic
+ * timer stops in C3 state.
+ */
+
+struct tick_device tick_broadcast_device;
+static cpumask_t tick_broadcast_mask;
+DEFINE_SPINLOCK(tick_broadcast_lock);
+
+/*
+ * Start the device in periodic mode
+ */
+static void tick_broadcast_start_periodic(struct clock_event_device *bc)
+{
+	if (bc && bc->mode == CLOCK_EVT_MODE_SHUTDOWN)
+		tick_setup_periodic(bc, 1);
+}
+
+/*
+ * Check, if the device can be utilized as broadcast device:
+ */
+int tick_check_broadcast_device(struct clock_event_device *dev)
+{
+	if (tick_broadcast_device.evtdev ||
+	    (dev->features & CLOCK_EVT_FEAT_C3STOP))
+		return 0;
+
+	clockevents_exchange_device(NULL, dev);
+	tick_broadcast_device.evtdev = dev;
+	if (!cpus_empty(tick_broadcast_mask))
+		tick_broadcast_start_periodic(dev);
+	return 1;
+}
+
+/*
+ * Check, if the device is the broadcast device
+ */
+int tick_is_broadcast_device(struct clock_event_device *dev)
+{
+	return (dev && tick_broadcast_device.evtdev == dev);
+}
+
+/*
+ * Check, if the device is disfunctional and a place holder, which
+ * needs to be handled by the broadcast device.
+ */
+int tick_device_uses_broadcast(struct clock_event_device *dev, int cpu)
+{
+	unsigned long flags;
+	int ret = 0;
+
+	spin_lock_irqsave(&tick_broadcast_lock, flags);
+
+	/*
+	 * Devices might be registered with both periodic and oneshot
+	 * mode disabled. This signals, that the device needs to be
+	 * operated from the broadcast device and is a placeholder for
+	 * the cpu local device.
+	 */
+	if (!tick_device_is_functional(dev)) {
+		dev->event_handler = tick_handle_periodic;
+		cpu_set(cpu, tick_broadcast_mask);
+		tick_broadcast_start_periodic(tick_broadcast_device.evtdev);
+		ret = 1;
+	}
+
+	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+	return ret;
+}
+
+/*
+ * Broadcast the event to the cpus, which are set in the mask
+ */
+int tick_do_broadcast(cpumask_t mask)
+{
+	int ret = 0, cpu = smp_processor_id();
+	struct tick_device *td;
+
+	/*
+	 * Check, if the current cpu is in the mask
+	 */
+	if (cpu_isset(cpu, mask)) {
+		cpu_clear(cpu, mask);
+		td = &per_cpu(tick_cpu_device, cpu);
+		td->evtdev->event_handler(td->evtdev);
+		ret = 1;
+	}
+
+	if (!cpus_empty(mask)) {
+		/*
+		 * It might be necessary to actually check whether the devices
+		 * have different broadcast functions. For now, just use the
+		 * one of the first device. This works as long as we have this
+		 * misfeature only on x86 (lapic)
+		 */
+		cpu = first_cpu(mask);
+		td = &per_cpu(tick_cpu_device, cpu);
+		td->evtdev->broadcast(mask);
+		ret = 1;
+	}
+	return ret;
+}
+
+/*
+ * Periodic broadcast:
+ * - invoke the broadcast handlers
+ */
+static void tick_do_periodic_broadcast(void)
+{
+	cpumask_t mask;
+
+	spin_lock(&tick_broadcast_lock);
+
+	cpus_and(mask, cpu_online_map, tick_broadcast_mask);
+	tick_do_broadcast(mask);
+
+	spin_unlock(&tick_broadcast_lock);
+}
+
+/*
+ * Event handler for periodic broadcast ticks
+ */
+static void tick_handle_periodic_broadcast(struct clock_event_device *dev)
+{
+	dev->next_event.tv64 = KTIME_MAX;
+
+	tick_do_periodic_broadcast();
+
+	/*
+	 * The device is in periodic mode. No reprogramming necessary:
+	 */
+	if (dev->mode == CLOCK_EVT_MODE_PERIODIC)
+		return;
+
+	/*
+	 * Setup the next period for devices, which do not have
+	 * periodic mode:
+	 */
+	for (;;) {
+		ktime_t next = ktime_add(dev->next_event, tick_period);
+
+		if (!clockevents_program_event(dev, next, ktime_get()))
+			return;
+		tick_do_periodic_broadcast();
+	}
+}
+
+/*
+ * Powerstate information: The system enters/leaves a state, where
+ * affected devices might stop
+ */
+static void tick_do_broadcast_on_off(void *why)
+{
+	struct clock_event_device *bc, *dev;
+	struct tick_device *td;
+	unsigned long flags, *reason = why;
+	int cpu;
+
+	spin_lock_irqsave(&tick_broadcast_lock, flags);
+
+	cpu = smp_processor_id();
+	td = &per_cpu(tick_cpu_device, cpu);
+	dev = td->evtdev;
+	bc = tick_broadcast_device.evtdev;
+
+	/*
+	 * Is the device in broadcast mode forever or is it not
+	 * affected by the powerstate ?
+	 */
+	if (!dev || !tick_device_is_functional(dev) ||
+	    !(dev->features & CLOCK_EVT_FEAT_C3STOP))
+		goto out;
+
+	if (*reason == CLOCK_EVT_NOTIFY_BROADCAST_ON) {
+		if (!cpu_isset(cpu, tick_broadcast_mask)) {
+			cpu_set(cpu, tick_broadcast_mask);
+			if (td->mode == TICKDEV_MODE_PERIODIC)
+				clockevents_set_mode(dev,
+						     CLOCK_EVT_MODE_SHUTDOWN);
+		}
+	} else {
+		if (cpu_isset(cpu, tick_broadcast_mask)) {
+			cpu_clear(cpu, tick_broadcast_mask);
+			if (td->mode == TICKDEV_MODE_PERIODIC)
+				tick_setup_periodic(dev, 0);
+		}
+	}
+
+	if (cpus_empty(tick_broadcast_mask))
+		clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
+	else {
+		if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC)
+			tick_broadcast_start_periodic(bc);
+	}
+out:
+	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+}
+
+/*
+ * Powerstate information: The system enters/leaves a state, where
+ * affected devices might stop.
+ */
+void tick_broadcast_on_off(unsigned long reason, int *oncpu)
+{
+	int cpu = get_cpu();
+
+	if (cpu == *oncpu)
+		tick_do_broadcast_on_off(&reason);
+	else
+		smp_call_function_single(*oncpu, tick_do_broadcast_on_off,
+					 &reason, 1, 1);
+	put_cpu();
+}
+
+/*
+ * Set the periodic handler depending on broadcast on/off
+ */
+void tick_set_periodic_handler(struct clock_event_device *dev, int broadcast)
+{
+	if (!broadcast)
+		dev->event_handler = tick_handle_periodic;
+	else
+		dev->event_handler = tick_handle_periodic_broadcast;
+}
+
+/*
+ * Remove a CPU from broadcasting
+ */
+void tick_shutdown_broadcast(unsigned int *cpup)
+{
+	struct clock_event_device *bc;
+	unsigned long flags;
+	unsigned int cpu = *cpup;
+
+	spin_lock_irqsave(&tick_broadcast_lock, flags);
+
+	bc = tick_broadcast_device.evtdev;
+	cpu_clear(cpu, tick_broadcast_mask);
+
+	if (tick_broadcast_device.mode == TICKDEV_MODE_PERIODIC) {
+		if (bc && cpus_empty(tick_broadcast_mask))
+			clockevents_set_mode(bc, CLOCK_EVT_MODE_SHUTDOWN);
+	}
+
+	spin_unlock_irqrestore(&tick_broadcast_lock, flags);
+}
