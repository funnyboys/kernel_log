commit 8f4a8c12cafe54535f334a09be7ec7f236134764
Author: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
Date:   Thu Apr 18 17:50:41 2019 -0700

    kernel/watchdog_hld.c: hard lockup message should end with a newline
    
    Separate print_modules() and hard lockup error message.
    
    Before the patch:
    
      NMI watchdog: Watchdog detected hard LOCKUP on cpu 1Modules linked in: nls_cp437
    
    Link: http://lkml.kernel.org/r/20190412062557.2700-1-sergey.senozhatsky@gmail.com
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 71381168dede..247bf0b1582c 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -135,7 +135,8 @@ static void watchdog_overflow_callback(struct perf_event *event,
 		if (__this_cpu_read(hard_watchdog_warn) == true)
 			return;
 
-		pr_emerg("Watchdog detected hard LOCKUP on cpu %d", this_cpu);
+		pr_emerg("Watchdog detected hard LOCKUP on cpu %d\n",
+			 this_cpu);
 		print_modules();
 		print_irqtrace_events(current);
 		if (regs)

commit cb9d7fd51d9fbb329d182423bd7b92d0f8cb0e01
Author: Vincent Whitchurch <vincent.whitchurch@axis.com>
Date:   Tue Aug 21 17:25:07 2018 +0200

    watchdog: Mark watchdog touch functions as notrace
    
    Some architectures need to use stop_machine() to patch functions for
    ftrace, and the assumption is that the stopped CPUs do not make function
    calls to traceable functions when they are in the stopped state.
    
    Commit ce4f06dcbb5d ("stop_machine: Touch_nmi_watchdog() after
    MULTI_STOP_PREPARE") added calls to the watchdog touch functions from
    the stopped CPUs and those functions lack notrace annotations.  This
    leads to crashes when enabling/disabling ftrace on ARM kernels built
    with the Thumb-2 instruction set.
    
    Fix it by adding the necessary notrace annotations.
    
    Fixes: ce4f06dcbb5d ("stop_machine: Touch_nmi_watchdog() after MULTI_STOP_PREPARE")
    Signed-off-by: Vincent Whitchurch <vincent.whitchurch@axis.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: oleg@redhat.com
    Cc: tj@kernel.org
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20180821152507.18313-1-vincent.whitchurch@axis.com

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 1f7020d65d0a..71381168dede 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -29,7 +29,7 @@ static struct cpumask dead_events_mask;
 static unsigned long hardlockup_allcpu_dumped;
 static atomic_t watchdog_cpus = ATOMIC_INIT(0);
 
-void arch_touch_nmi_watchdog(void)
+notrace void arch_touch_nmi_watchdog(void)
 {
 	/*
 	 * Using __raw here because some code paths have

commit 1b6266ebe3da8198e9a02fbad77bbb56e2f7ce2e
Author: Sinan Kaya <okaya@kernel.org>
Date:   Thu Aug 2 23:09:41 2018 -0700

    watchdog: Reduce message verbosity
    
    Code is emitting the following error message during boot on systems
    without PMU hardware support while probing NMI capability.
    
     NMI watchdog: Perf event create on CPU 0 failed with -2
    
    This error is emitted as the perf subsystem returns -ENOENT due to lack of
    PMUs in the system.
    
    It is followed by the warning that NMI watchdog is disabled:
    
      NMI watchdog: Perf NMI watchdog permanently disabled
    
    While NMI disabled information is useful for ordinary users, seeing a PERF
    event create failed with error code -2 is not.
    
    Reduce the message severity to debug so that if debugging is still possible
    in case the error code returned by perf is required for analysis.
    
    Signed-off-by: Sinan Kaya <okaya@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Cc: Kate Stewart <kstewart@linuxfoundation.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Colin Ian King <colin.king@canonical.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=599368
    Link: https://lkml.kernel.org/r/20180803060943.2643-1-okaya@kernel.org

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index e449a23e9d59..1f7020d65d0a 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -175,8 +175,8 @@ static int hardlockup_detector_event_create(void)
 	evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
 					       watchdog_overflow_callback, NULL);
 	if (IS_ERR(evt)) {
-		pr_info("Perf event create on CPU %d failed with %ld\n", cpu,
-			PTR_ERR(evt));
+		pr_debug("Perf event create on CPU %d failed with %ld\n", cpu,
+			 PTR_ERR(evt));
 		return PTR_ERR(evt);
 	}
 	this_cpu_write(watchdog_ev, evt);

commit 649e441f49d4bfb717e9467950891dc452f4e063
Merge: 42f930da7f00 5cb0512c02ec
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Nov 4 08:53:04 2017 +0100

    Merge branch 'linus' into core/urgent, to pick up dependent commits
    
    We want to fix an objtool build warning that got introduced in the latest upstream kernel.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 71a62ceacdc8..4583feb66393 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Detect hard lockups on a system
  *

commit 42f930da7f00c0ab23df4c7aed36137f35988980
Author: Don Zickus <dzickus@redhat.com>
Date:   Wed Nov 1 14:11:27 2017 -0400

    watchdog/hardlockup/perf: Use atomics to track in-use cpu counter
    
    Guenter reported:
      There is still a problem. When running
        echo 6 > /proc/sys/kernel/watchdog_thresh
        echo 5 > /proc/sys/kernel/watchdog_thresh
      repeatedly, the message
    
       NMI watchdog: Enabled. Permanently consumes one hw-PMU counter.
    
      stops after a while (after ~10-30 iterations, with fluctuations).
      Maybe watchdog_cpus needs to be atomic ?
    
    That's correct as this again is affected by the asynchronous nature of the
    smpboot thread unpark mechanism.
    
    CPU 0                           CPU1                    CPU2
    write(watchdog_thresh, 6)
      stop()
        park()
      update()
      start()
        unpark()
                                    thread->unpark()
                                      cnt++;
    write(watchdog_thresh, 5)                               thread->unpark()
      stop()
        park()                      thread->park()
                                       cnt--;                 cnt++;
      update()
      start()
        unpark()
    
    That's not a functional problem, it just affects the informational message.
    
    Convert watchdog_cpus to atomic_t to prevent the problem
    
    Reported-and-tested-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20171101181126.j727fqjmdthjz4xk@redhat.com

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index a7f137c1933a..a84b205fac9a 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -12,6 +12,7 @@
 #define pr_fmt(fmt) "NMI watchdog: " fmt
 
 #include <linux/nmi.h>
+#include <linux/atomic.h>
 #include <linux/module.h>
 #include <linux/sched/debug.h>
 
@@ -25,7 +26,7 @@ static DEFINE_PER_CPU(struct perf_event *, dead_event);
 static struct cpumask dead_events_mask;
 
 static unsigned long hardlockup_allcpu_dumped;
-static unsigned int watchdog_cpus;
+static atomic_t watchdog_cpus = ATOMIC_INIT(0);
 
 void arch_touch_nmi_watchdog(void)
 {
@@ -189,7 +190,8 @@ void hardlockup_detector_perf_enable(void)
 	if (hardlockup_detector_event_create())
 		return;
 
-	if (!watchdog_cpus++)
+	/* use original value for check */
+	if (!atomic_fetch_inc(&watchdog_cpus))
 		pr_info("Enabled. Permanently consumes one hw-PMU counter.\n");
 
 	perf_event_enable(this_cpu_read(watchdog_ev));
@@ -207,7 +209,7 @@ void hardlockup_detector_perf_disable(void)
 		this_cpu_write(watchdog_ev, NULL);
 		this_cpu_write(dead_event, event);
 		cpumask_set_cpu(smp_processor_id(), &dead_events_mask);
-		watchdog_cpus--;
+		atomic_dec(&watchdog_cpus);
 	}
 }
 

commit 9c388a5ed1960b2ebbebd3dbe7553092b0c15ec1
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Oct 31 22:32:00 2017 +0100

    watchdog/harclockup/perf: Revert a33d44843d45 ("watchdog/hardlockup/perf: Simplify deferred event destroy")
    
    Guenter reported a crash in the watchdog/perf code, which is caused by
    cleanup() and enable() running concurrently. The reason for this is:
    
    The watchdog functions are serialized via the watchdog_mutex and cpu
    hotplug locking, but the enable of the perf based watchdog happens in
    context of the unpark callback of the smpboot thread. But that unpark
    function is not synchronous inside the locking. The unparking of the thread
    just wakes it up and leaves so there is no guarantee when the thread is
    executing.
    
    If it starts running _before_ the cleanup happened then it will create a
    event and overwrite the dead event pointer. The new event is then cleaned
    up because the event is marked dead.
    
        lock(watchdog_mutex);
        lockup_detector_reconfigure();
            cpus_read_lock();
            stop();
               park()
            update();
            start();
               unpark()
            cpus_read_unlock();             thread runs()
                                              overwrite dead event ptr
            cleanup();
              free new event, which is active inside perf....
        unlock(watchdog_mutex);
    
    The park side is safe as that actually waits for the thread to reach
    parked state.
    
    Commit a33d44843d45 removed the protection against this kind of scenario
    under the stupid assumption that the hotplug serialization and the
    watchdog_mutex cover everything.
    
    Bring it back.
    
    Reverts: a33d44843d45 ("watchdog/hardlockup/perf: Simplify deferred event destroy")
    Reported-and-tested-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Thomas Feels-stupid Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Don Zickus <dzickus@redhat.com>
    Link: https://lkml.kernel.org/r/alpine.DEB.2.20.1710312145190.1942@nanos

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 71a62ceacdc8..a7f137c1933a 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -21,6 +21,7 @@
 static DEFINE_PER_CPU(bool, hard_watchdog_warn);
 static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
 static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
+static DEFINE_PER_CPU(struct perf_event *, dead_event);
 static struct cpumask dead_events_mask;
 
 static unsigned long hardlockup_allcpu_dumped;
@@ -203,6 +204,8 @@ void hardlockup_detector_perf_disable(void)
 
 	if (event) {
 		perf_event_disable(event);
+		this_cpu_write(watchdog_ev, NULL);
+		this_cpu_write(dead_event, event);
 		cpumask_set_cpu(smp_processor_id(), &dead_events_mask);
 		watchdog_cpus--;
 	}
@@ -218,7 +221,7 @@ void hardlockup_detector_perf_cleanup(void)
 	int cpu;
 
 	for_each_cpu(cpu, &dead_events_mask) {
-		struct perf_event *event = per_cpu(watchdog_ev, cpu);
+		struct perf_event *event = per_cpu(dead_event, cpu);
 
 		/*
 		 * Required because for_each_cpu() reports  unconditionally
@@ -226,7 +229,7 @@ void hardlockup_detector_perf_cleanup(void)
 		 */
 		if (event)
 			perf_event_release_kernel(event);
-		per_cpu(watchdog_ev, cpu) = NULL;
+		per_cpu(dead_event, cpu) = NULL;
 	}
 	cpumask_clear(&dead_events_mask);
 }

commit 77c01d11bbb2b5c005347061bf543ab94878314c
Author: Colin Ian King <colin.king@canonical.com>
Date:   Tue Sep 26 10:36:03 2017 +0100

    watchdog/hardlockup/perf: Fix spelling mistake: "permanetely" -> "permanently"
    
    Trivial fix to spelling mistake in pr_info message
    
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Babu Moger <babu.moger@oracle.com>
    Link: https://lkml.kernel.org/r/20170926093603.7756-1-colin.king@canonical.com

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 204a8cadb717..71a62ceacdc8 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -280,7 +280,7 @@ int __init hardlockup_detector_perf_init(void)
 	int ret = hardlockup_detector_event_create();
 
 	if (ret) {
-		pr_info("Perf NMI watchdog permanetely disabled\n");
+		pr_info("Perf NMI watchdog permanently disabled\n");
 	} else {
 		perf_event_release_kernel(this_cpu_read(watchdog_ev));
 		this_cpu_write(watchdog_ev, NULL);

commit 115ef3b7e61ac64e32827611a127002672ed3725
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Sep 25 20:21:54 2017 +0200

    watchdog/hardlockup/perf: Cure UP damage
    
    for_each_cpu() unintuitively reports CPU0 as set independend of the actual
    cpumask content on UP kernels. That leads to a NULL pointer dereference
    when the cleanup function is invoked and there is no event to clean up.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index b2931154b5f2..204a8cadb717 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -220,8 +220,13 @@ void hardlockup_detector_perf_cleanup(void)
 	for_each_cpu(cpu, &dead_events_mask) {
 		struct perf_event *event = per_cpu(watchdog_ev, cpu);
 
+		/*
+		 * Required because for_each_cpu() reports  unconditionally
+		 * CPU0 as set on UP kernels. Sigh.
+		 */
+		if (event)
+			perf_event_release_kernel(event);
 		per_cpu(watchdog_ev, cpu) = NULL;
-		perf_event_release_kernel(event);
 	}
 	cpumask_clear(&dead_events_mask);
 }

commit a33d44843d4574ec05bec39527d8a87b7af2072c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:22 2017 +0200

    watchdog/hardlockup/perf: Simplify deferred event destroy
    
    Now that all functionality is properly serialized against CPU hotplug,
    remove the extra per cpu storage which holds the disabled events for
    cleanup. The core makes sure that cleanup happens before new events are
    created.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194148.340708074@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 509bb6b59c41..b2931154b5f2 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -21,7 +21,6 @@
 static DEFINE_PER_CPU(bool, hard_watchdog_warn);
 static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
 static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
-static DEFINE_PER_CPU(struct perf_event *, dead_event);
 static struct cpumask dead_events_mask;
 
 static unsigned long hardlockup_allcpu_dumped;
@@ -204,8 +203,6 @@ void hardlockup_detector_perf_disable(void)
 
 	if (event) {
 		perf_event_disable(event);
-		this_cpu_write(watchdog_ev, NULL);
-		this_cpu_write(dead_event, event);
 		cpumask_set_cpu(smp_processor_id(), &dead_events_mask);
 		watchdog_cpus--;
 	}
@@ -221,9 +218,9 @@ void hardlockup_detector_perf_cleanup(void)
 	int cpu;
 
 	for_each_cpu(cpu, &dead_events_mask) {
-		struct perf_event *event = per_cpu(dead_event, cpu);
+		struct perf_event *event = per_cpu(watchdog_ev, cpu);
 
-		per_cpu(dead_event, cpu) = NULL;
+		per_cpu(watchdog_ev, cpu) = NULL;
 		perf_event_release_kernel(event);
 	}
 	cpumask_clear(&dead_events_mask);

commit 146c9d0e9dfdb62ed6afd43cc263efafbbfd1dcf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:21 2017 +0200

    watchdog/hardlockup/perf: Use new perf CPU enable mechanism
    
    Get rid of the hodgepodge which tries to be smart about perf being
    unavailable and error printout rate limiting.
    
    That's all not required simply because this is never invoked when the perf
    NMI watchdog is not functional.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194148.259651788@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 99a3f22e48cc..509bb6b59c41 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -25,7 +25,7 @@ static DEFINE_PER_CPU(struct perf_event *, dead_event);
 static struct cpumask dead_events_mask;
 
 static unsigned long hardlockup_allcpu_dumped;
-static bool hardlockup_detector_disabled;
+static unsigned int watchdog_cpus;
 
 void arch_touch_nmi_watchdog(void)
 {
@@ -160,84 +160,6 @@ static void watchdog_overflow_callback(struct perf_event *event,
 	return;
 }
 
-/*
- * People like the simple clean cpu node info on boot.
- * Reduce the watchdog noise by only printing messages
- * that are different from what cpu0 displayed.
- */
-static unsigned long firstcpu_err;
-static atomic_t watchdog_cpus;
-
-int watchdog_nmi_enable(unsigned int cpu)
-{
-	struct perf_event_attr *wd_attr;
-	struct perf_event *event = per_cpu(watchdog_ev, cpu);
-	int firstcpu = 0;
-
-	/* nothing to do if the hard lockup detector is disabled */
-	if (!(watchdog_enabled & NMI_WATCHDOG_ENABLED))
-		goto out;
-
-	/* A failure disabled the hardlockup detector permanently */
-	if (hardlockup_detector_disabled)
-		return -ENODEV;
-
-	/* is it already setup and enabled? */
-	if (event && event->state > PERF_EVENT_STATE_OFF)
-		goto out;
-
-	/* it is setup but not enabled */
-	if (event != NULL)
-		goto out_enable;
-
-	if (atomic_inc_return(&watchdog_cpus) == 1)
-		firstcpu = 1;
-
-	wd_attr = &wd_hw_attr;
-	wd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);
-
-	/* Try to register using hardware perf events */
-	event = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
-
-	/* save the first cpu's error for future comparision */
-	if (firstcpu && IS_ERR(event))
-		firstcpu_err = PTR_ERR(event);
-
-	if (!IS_ERR(event)) {
-		/* only print for the first cpu initialized */
-		if (firstcpu || firstcpu_err)
-			pr_info("enabled on all CPUs, permanently consumes one hw-PMU counter.\n");
-		goto out_save;
-	}
-
-	/* skip displaying the same error again */
-	if (!firstcpu && (PTR_ERR(event) == firstcpu_err))
-		return PTR_ERR(event);
-
-	/* vary the KERN level based on the returned errno */
-	if (PTR_ERR(event) == -EOPNOTSUPP)
-		pr_info("disabled (cpu%i): not supported (no LAPIC?)\n", cpu);
-	else if (PTR_ERR(event) == -ENOENT)
-		pr_warn("disabled (cpu%i): hardware events not enabled\n",
-			 cpu);
-	else
-		pr_err("disabled (cpu%i): unable to create perf event: %ld\n",
-			cpu, PTR_ERR(event));
-
-	pr_info("Disabling hard lockup detector permanently\n");
-	hardlockup_detector_disabled = true;
-
-	return PTR_ERR(event);
-
-	/* success path */
-out_save:
-	per_cpu(watchdog_ev, cpu) = event;
-out_enable:
-	perf_event_enable(per_cpu(watchdog_ev, cpu));
-out:
-	return 0;
-}
-
 static int hardlockup_detector_event_create(void)
 {
 	unsigned int cpu = smp_processor_id();
@@ -267,6 +189,9 @@ void hardlockup_detector_perf_enable(void)
 	if (hardlockup_detector_event_create())
 		return;
 
+	if (!watchdog_cpus++)
+		pr_info("Enabled. Permanently consumes one hw-PMU counter.\n");
+
 	perf_event_enable(this_cpu_read(watchdog_ev));
 }
 
@@ -282,10 +207,7 @@ void hardlockup_detector_perf_disable(void)
 		this_cpu_write(watchdog_ev, NULL);
 		this_cpu_write(dead_event, event);
 		cpumask_set_cpu(smp_processor_id(), &dead_events_mask);
-
-		/* watchdog_nmi_enable() expects this to be zero initially. */
-		if (atomic_dec_and_test(&watchdog_cpus))
-			firstcpu_err = 0;
+		watchdog_cpus--;
 	}
 }
 

commit 2a1b8ee4f5665b4291e43e4a25d964c3eb2f4c32
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:20 2017 +0200

    watchdog/hardlockup/perf: Implement CPU enable replacement
    
    watchdog_nmi_enable() is an unparseable mess, Provide a clean perf specific
    implementation, which will be used when the existing setup/teardown mess is
    replaced.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194148.180215498@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index f7e752e6e9b4..99a3f22e48cc 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -259,6 +259,17 @@ static int hardlockup_detector_event_create(void)
 	return 0;
 }
 
+/**
+ * hardlockup_detector_perf_enable - Enable the local event
+ */
+void hardlockup_detector_perf_enable(void)
+{
+	if (hardlockup_detector_event_create())
+		return;
+
+	perf_event_enable(this_cpu_read(watchdog_ev));
+}
+
 /**
  * hardlockup_detector_perf_disable - Disable the local event
  */

commit 178b9f7a36d2c74a38274b66dd89f53611298a19
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:18 2017 +0200

    watchdog/hardlockup/perf: Implement init time perf validation
    
    The watchdog tries to create perf events even after it figured out that
    perf is not functional or the requested event is not supported.
    
    That's braindead as this can be done once at init time and if not supported
    the NMI watchdog can be turned off unconditonally.
    
    Implement the perf hardlockup detector functionality for that. This creates
    a new event create function, which will replace the unholy mess of the
    existing one in later patches.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194148.019090547@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 0aa191ee3d51..f7e752e6e9b4 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -238,6 +238,27 @@ int watchdog_nmi_enable(unsigned int cpu)
 	return 0;
 }
 
+static int hardlockup_detector_event_create(void)
+{
+	unsigned int cpu = smp_processor_id();
+	struct perf_event_attr *wd_attr;
+	struct perf_event *evt;
+
+	wd_attr = &wd_hw_attr;
+	wd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);
+
+	/* Try to register using hardware perf events */
+	evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
+					       watchdog_overflow_callback, NULL);
+	if (IS_ERR(evt)) {
+		pr_info("Perf event create on CPU %d failed with %ld\n", cpu,
+			PTR_ERR(evt));
+		return PTR_ERR(evt);
+	}
+	this_cpu_write(watchdog_ev, evt);
+	return 0;
+}
+
 /**
  * hardlockup_detector_perf_disable - Disable the local event
  */
@@ -315,3 +336,19 @@ void __init hardlockup_detector_perf_restart(void)
 			perf_event_enable(event);
 	}
 }
+
+/**
+ * hardlockup_detector_perf_init - Probe whether NMI event is available at all
+ */
+int __init hardlockup_detector_perf_init(void)
+{
+	int ret = hardlockup_detector_event_create();
+
+	if (ret) {
+		pr_info("Perf NMI watchdog permanetely disabled\n");
+	} else {
+		perf_event_release_kernel(this_cpu_read(watchdog_ev));
+		this_cpu_write(watchdog_ev, NULL);
+	}
+	return ret;
+}

commit 01f0a02701cbcf32d22cfc9d1ab9a3f0ff2ba68c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:05 2017 +0200

    watchdog/core: Remove the park_in_progress obfuscation
    
    Commit:
    
      b94f51183b06 ("kernel/watchdog: prevent false hardlockup on overloaded system")
    
    tries to fix the following issue:
    
    proc_write()
       set_sample_period()    <--- New sample period becoms visible
                              <----- Broken starts
       proc_watchdog_update()
         watchdog_enable_all_cpus()         watchdog_hrtimer_fn()
         update_watchdog_all_cpus()            restart_timer(sample_period)
            watchdog_park_threads()
    
                                            thread->park()
                                              disable_nmi()
                              <----- Broken ends
    
    The reason why this is broken is that the update of the watchdog threshold
    becomes immediately effective and visible for the hrtimer function which
    uses that value to rearm the timer. But the NMI/perf side still uses the
    old value up to the point where it is disabled. If the rate has been
    lowered then the NMI can run fast enough to 'detect' a hard lockup because
    the timer has not fired due to the longer period.
    
    The patch 'fixed' this by adding a variable:
    
    proc_write()
       set_sample_period()
                                            <----- Broken starts
       proc_watchdog_update()
         watchdog_enable_all_cpus()         watchdog_hrtimer_fn()
         update_watchdog_all_cpus()            restart_timer(sample_period)
             watchdog_park_threads()
              park_in_progress = 1
                                            <----- Broken ends
                                            nmi_watchdog()
                                              if (park_in_progress)
                                                 return;
    
    The only effect of this variable was to make the window where the breakage
    can hit small enough that it was not longer observable in testing. From a
    correctness point of view it is a pointless bandaid which merily papers
    over the root cause: the unsychronized update of the variable.
    
    Looking deeper into the related code pathes unearthed similar problems in
    the watchdog_start()/stop() functions.
    
     watchdog_start()
            perf_nmi_event_start()
            hrtimer_start()
    
     watchdog_stop()
            hrtimer_cancel()
            perf_nmi_event_stop()
    
    In both cases the call order is wrong because if the tasks gets preempted
    or the VM gets scheduled out long enough after the first call, then there is
    a chance that the next NMI will see a stale hrtimer interrupt count and
    trigger a false positive hard lockup splat.
    
    Get rid of park_in_progress so the code can be gradually deobfuscated and
    pruned from several layers of duct tape papering over the root cause,
    which has been either ignored or not understood at all.
    
    Once this is removed the underlying problem will be fixed by rewriting the
    proc interface to do a proper synchronized update.
    
    Address the start/stop() ordering problem as well by reverting the call
    order, so this part is at least correct now.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1709052038270.2393@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 94111ccb09b5..0aa191ee3d51 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -106,15 +106,12 @@ static struct perf_event_attr wd_hw_attr = {
 
 /* Callback function for perf event subsystem */
 static void watchdog_overflow_callback(struct perf_event *event,
-		 struct perf_sample_data *data,
-		 struct pt_regs *regs)
+				       struct perf_sample_data *data,
+				       struct pt_regs *regs)
 {
 	/* Ensure the watchdog never gets throttled */
 	event->hw.interrupts = 0;
 
-	if (atomic_read(&watchdog_park_in_progress) != 0)
-		return;
-
 	if (__this_cpu_read(watchdog_nmi_touch) == true) {
 		__this_cpu_write(watchdog_nmi_touch, false);
 		return;

commit 941154bd6937a710ae9193a3c733c0029e5ae7b8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:04 2017 +0200

    watchdog/hardlockup/perf: Prevent CPU hotplug deadlock
    
    The following deadlock is possible in the watchdog hotplug code:
    
      cpus_write_lock()
        ...
          takedown_cpu()
            smpboot_park_threads()
              smpboot_park_thread()
                kthread_park()
                  ->park() := watchdog_disable()
                    watchdog_nmi_disable()
                      perf_event_release_kernel();
                        put_event()
                          _free_event()
                            ->destroy() := hw_perf_event_destroy()
                              x86_release_hardware()
                                release_ds_buffers()
                                  get_online_cpus()
    
    when a per cpu watchdog perf event is destroyed which drops the last
    reference to the PMU hardware. The cleanup code there invokes
    get_online_cpus() which instantly deadlocks because the hotplug percpu
    rwsem is write locked.
    
    To solve this add a deferring mechanism:
    
      cpus_write_lock()
                               kthread_park()
                                watchdog_nmi_disable(deferred)
                                  perf_event_disable(event);
                                  move_event_to_deferred(event);
                               ....
      cpus_write_unlock()
      cleaup_deferred_events()
        perf_event_release_kernel()
    
    This is still properly serialized against concurrent hotplug via the
    cpu_add_remove_lock, which is held by the task which initiated the hotplug
    event.
    
    This is also used to handle event destruction when the watchdog threads are
    parked via other mechanisms than CPU hotplug.
    
    Analyzed-by: Peter Zijlstra <peterz@infradead.org>
    
    Reported-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194146.884469246@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 7b602714ea53..94111ccb09b5 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -21,6 +21,8 @@
 static DEFINE_PER_CPU(bool, hard_watchdog_warn);
 static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
 static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
+static DEFINE_PER_CPU(struct perf_event *, dead_event);
+static struct cpumask dead_events_mask;
 
 static unsigned long hardlockup_allcpu_dumped;
 static bool hardlockup_detector_disabled;
@@ -239,16 +241,18 @@ int watchdog_nmi_enable(unsigned int cpu)
 	return 0;
 }
 
-void watchdog_nmi_disable(unsigned int cpu)
+/**
+ * hardlockup_detector_perf_disable - Disable the local event
+ */
+void hardlockup_detector_perf_disable(void)
 {
-	struct perf_event *event = per_cpu(watchdog_ev, cpu);
+	struct perf_event *event = this_cpu_read(watchdog_ev);
 
 	if (event) {
 		perf_event_disable(event);
-		per_cpu(watchdog_ev, cpu) = NULL;
-
-		/* should be in cleanup, but blocks oprofile */
-		perf_event_release_kernel(event);
+		this_cpu_write(watchdog_ev, NULL);
+		this_cpu_write(dead_event, event);
+		cpumask_set_cpu(smp_processor_id(), &dead_events_mask);
 
 		/* watchdog_nmi_enable() expects this to be zero initially. */
 		if (atomic_dec_and_test(&watchdog_cpus))
@@ -256,6 +260,24 @@ void watchdog_nmi_disable(unsigned int cpu)
 	}
 }
 
+/**
+ * hardlockup_detector_perf_cleanup - Cleanup disabled events and destroy them
+ *
+ * Called from lockup_detector_cleanup(). Serialized by the caller.
+ */
+void hardlockup_detector_perf_cleanup(void)
+{
+	int cpu;
+
+	for_each_cpu(cpu, &dead_events_mask) {
+		struct perf_event *event = per_cpu(dead_event, cpu);
+
+		per_cpu(dead_event, cpu) = NULL;
+		perf_event_release_kernel(event);
+	}
+	cpumask_clear(&dead_events_mask);
+}
+
 /**
  * hardlockup_detector_perf_stop - Globally stop watchdog events
  *

commit 20d853fd0703b1d73c35a22024c0d4fcbcc57c8c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 12 21:37:03 2017 +0200

    watchdog/hardlockup/perf: Remove broken self disable on failure
    
    The self disabling feature is broken vs. CPU hotplug locking:
    
    CPU 0                      CPU 1
    cpus_write_lock();
     cpu_up(1)
       wait_for_completion()
                               ....
                               unpark_watchdog()
                               ->unpark()
                                 perf_event_create() <- fails
                                   watchdog_enable &= ~NMI_WATCHDOG;
                               ....
    cpus_write_unlock();
                               CPU 2
    cpus_write_lock()
     cpu_down(2)
       wait_for_completion()
                               wakeup(watchdog);
                                 watchdog()
                                 if (!(watchdog_enable & NMI_WATCHDOG))
                                    watchdog_nmi_disable()
                                      perf_event_disable()
                                      ....
                                      cpus_read_lock();
    
                               stop_smpboot_threads()
                                 park_watchdog();
                                   wait_for_completion(watchdog->parked);
    
    Result: End of hotplug and instantaneous full lockup of the machine.
    
    There is a similar problem with disabling the watchdog via the user space
    interface as the sysctl function fiddles with watchdog_enable directly.
    
    It's very debatable whether this is required at all. If the watchdog works
    nicely on N CPUs and it fails to enable on the N + 1 CPU either during
    hotplug or because the user space interface disabled it via sysctl cpumask
    and then some perf user grabbed the counter which is then unavailable for
    the watchdog when the sysctl cpumask gets changed back.
    
    There is no real justification for this.
    
    One of the reasons WHY this is done is the utter stupidity of the init code
    of the perf NMI watchdog. Instead of checking upfront at boot whether PERF
    is available and functional at all, it just does this check at run time
    over and over when user space fiddles with the sysctl. That's broken beyond
    repair along with the idiotic error code dependent warn level printks and
    the even more silly printk rate limiting.
    
    If the init code checks whether perf works at boot time, then this mess can
    be more or less avoided completely. Perf does not come magically into life
    at runtime. Brain usage while coding is overrated.
    
    Remove the cruft and add a temporary safe guard which gets removed later.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194146.806708429@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index c9586ebc2e98..7b602714ea53 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -23,6 +23,7 @@ static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
 static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
 
 static unsigned long hardlockup_allcpu_dumped;
+static bool hardlockup_detector_disabled;
 
 void arch_touch_nmi_watchdog(void)
 {
@@ -178,6 +179,10 @@ int watchdog_nmi_enable(unsigned int cpu)
 	if (!(watchdog_enabled & NMI_WATCHDOG_ENABLED))
 		goto out;
 
+	/* A failure disabled the hardlockup detector permanently */
+	if (hardlockup_detector_disabled)
+		return -ENODEV;
+
 	/* is it already setup and enabled? */
 	if (event && event->state > PERF_EVENT_STATE_OFF)
 		goto out;
@@ -206,18 +211,6 @@ int watchdog_nmi_enable(unsigned int cpu)
 		goto out_save;
 	}
 
-	/*
-	 * Disable the hard lockup detector if _any_ CPU fails to set up
-	 * set up the hardware perf event. The watchdog() function checks
-	 * the NMI_WATCHDOG_ENABLED bit periodically.
-	 *
-	 * The barriers are for syncing up watchdog_enabled across all the
-	 * cpus, as clear_bit() does not use barriers.
-	 */
-	smp_mb__before_atomic();
-	clear_bit(NMI_WATCHDOG_ENABLED_BIT, &watchdog_enabled);
-	smp_mb__after_atomic();
-
 	/* skip displaying the same error again */
 	if (!firstcpu && (PTR_ERR(event) == firstcpu_err))
 		return PTR_ERR(event);
@@ -232,7 +225,8 @@ int watchdog_nmi_enable(unsigned int cpu)
 		pr_err("disabled (cpu%i): unable to create perf event: %ld\n",
 			cpu, PTR_ERR(event));
 
-	pr_info("Shutting down hard lockup detector on all cpus\n");
+	pr_info("Disabling hard lockup detector permanently\n");
+	hardlockup_detector_disabled = true;
 
 	return PTR_ERR(event);
 

commit d0b6e0a8ef24b1b07078ababe5d91bcdf4f4264a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 12 21:36:55 2017 +0200

    watchdog/hardlockup: Provide interface to stop/restart perf events
    
    Provide an interface to stop and restart perf NMI watchdog events on all
    CPUs. This is only usable during init and especially for handling the perf
    HT bug on Intel machines. It's safe to use it this way as nothing can
    start/stop the NMI watchdog in parallel.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Link: http://lkml.kernel.org/r/20170912194146.167649596@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 3a09ea1b1d3d..c9586ebc2e98 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -261,3 +261,44 @@ void watchdog_nmi_disable(unsigned int cpu)
 			firstcpu_err = 0;
 	}
 }
+
+/**
+ * hardlockup_detector_perf_stop - Globally stop watchdog events
+ *
+ * Special interface for x86 to handle the perf HT bug.
+ */
+void __init hardlockup_detector_perf_stop(void)
+{
+	int cpu;
+
+	lockdep_assert_cpus_held();
+
+	for_each_online_cpu(cpu) {
+		struct perf_event *event = per_cpu(watchdog_ev, cpu);
+
+		if (event)
+			perf_event_disable(event);
+	}
+}
+
+/**
+ * hardlockup_detector_perf_restart - Globally restart watchdog events
+ *
+ * Special interface for x86 to handle the perf HT bug.
+ */
+void __init hardlockup_detector_perf_restart(void)
+{
+	int cpu;
+
+	lockdep_assert_cpus_held();
+
+	if (!(watchdog_enabled & NMI_WATCHDOG_ENABLED))
+		return;
+
+	for_each_online_cpu(cpu) {
+		struct perf_event *event = per_cpu(watchdog_ev, cpu);
+
+		if (event)
+			perf_event_enable(event);
+	}
+}

commit 7edaeb6841dfb27e362288ab8466ebdc4972e867
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Aug 15 09:50:13 2017 +0200

    kernel/watchdog: Prevent false positives with turbo modes
    
    The hardlockup detector on x86 uses a performance counter based on unhalted
    CPU cycles and a periodic hrtimer. The hrtimer period is about 2/5 of the
    performance counter period, so the hrtimer should fire 2-3 times before the
    performance counter NMI fires. The NMI code checks whether the hrtimer
    fired since the last invocation. If not, it assumess a hard lockup.
    
    The calculation of those periods is based on the nominal CPU
    frequency. Turbo modes increase the CPU clock frequency and therefore
    shorten the period of the perf/NMI watchdog. With extreme Turbo-modes (3x
    nominal frequency) the perf/NMI period is shorter than the hrtimer period
    which leads to false positives.
    
    A simple fix would be to shorten the hrtimer period, but that comes with
    the side effect of more frequent hrtimer and softlockup thread wakeups,
    which is not desired.
    
    Implement a low pass filter, which checks the perf/NMI period against
    kernel time. If the perf/NMI fires before 4/5 of the watchdog period has
    elapsed then the event is ignored and postponed to the next perf/NMI.
    
    That solves the problem and avoids the overhead of shorter hrtimer periods
    and more frequent softlockup thread wakeups.
    
    Fixes: 58687acba592 ("lockup_detector: Combine nmi_watchdog and softlockup detector")
    Reported-and-tested-by: Kan Liang <Kan.liang@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: dzickus@redhat.com
    Cc: prarit@redhat.com
    Cc: ak@linux.intel.com
    Cc: babu.moger@oracle.com
    Cc: peterz@infradead.org
    Cc: eranian@google.com
    Cc: acme@redhat.com
    Cc: stable@vger.kernel.org
    Cc: atomlin@redhat.com
    Cc: akpm@linux-foundation.org
    Cc: torvalds@linux-foundation.org
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1708150931310.1886@nanos

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 295a0d84934c..3a09ea1b1d3d 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -37,6 +37,62 @@ void arch_touch_nmi_watchdog(void)
 }
 EXPORT_SYMBOL(arch_touch_nmi_watchdog);
 
+#ifdef CONFIG_HARDLOCKUP_CHECK_TIMESTAMP
+static DEFINE_PER_CPU(ktime_t, last_timestamp);
+static DEFINE_PER_CPU(unsigned int, nmi_rearmed);
+static ktime_t watchdog_hrtimer_sample_threshold __read_mostly;
+
+void watchdog_update_hrtimer_threshold(u64 period)
+{
+	/*
+	 * The hrtimer runs with a period of (watchdog_threshold * 2) / 5
+	 *
+	 * So it runs effectively with 2.5 times the rate of the NMI
+	 * watchdog. That means the hrtimer should fire 2-3 times before
+	 * the NMI watchdog expires. The NMI watchdog on x86 is based on
+	 * unhalted CPU cycles, so if Turbo-Mode is enabled the CPU cycles
+	 * might run way faster than expected and the NMI fires in a
+	 * smaller period than the one deduced from the nominal CPU
+	 * frequency. Depending on the Turbo-Mode factor this might be fast
+	 * enough to get the NMI period smaller than the hrtimer watchdog
+	 * period and trigger false positives.
+	 *
+	 * The sample threshold is used to check in the NMI handler whether
+	 * the minimum time between two NMI samples has elapsed. That
+	 * prevents false positives.
+	 *
+	 * Set this to 4/5 of the actual watchdog threshold period so the
+	 * hrtimer is guaranteed to fire at least once within the real
+	 * watchdog threshold.
+	 */
+	watchdog_hrtimer_sample_threshold = period * 2;
+}
+
+static bool watchdog_check_timestamp(void)
+{
+	ktime_t delta, now = ktime_get_mono_fast_ns();
+
+	delta = now - __this_cpu_read(last_timestamp);
+	if (delta < watchdog_hrtimer_sample_threshold) {
+		/*
+		 * If ktime is jiffies based, a stalled timer would prevent
+		 * jiffies from being incremented and the filter would look
+		 * at a stale timestamp and never trigger.
+		 */
+		if (__this_cpu_inc_return(nmi_rearmed) < 10)
+			return false;
+	}
+	__this_cpu_write(nmi_rearmed, 0);
+	__this_cpu_write(last_timestamp, now);
+	return true;
+}
+#else
+static inline bool watchdog_check_timestamp(void)
+{
+	return true;
+}
+#endif
+
 static struct perf_event_attr wd_hw_attr = {
 	.type		= PERF_TYPE_HARDWARE,
 	.config		= PERF_COUNT_HW_CPU_CYCLES,
@@ -61,6 +117,9 @@ static void watchdog_overflow_callback(struct perf_event *event,
 		return;
 	}
 
+	if (!watchdog_check_timestamp())
+		return;
+
 	/* check for a hardlockup
 	 * This is done by making sure our timer interrupt
 	 * is incrementing.  The timer interrupt should have

commit 05a4a95279311c3a4633b4277a5d21cfd616c6c7
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jul 12 14:35:46 2017 -0700

    kernel/watchdog: split up config options
    
    Split SOFTLOCKUP_DETECTOR from LOCKUP_DETECTOR, and split
    HARDLOCKUP_DETECTOR_PERF from HARDLOCKUP_DETECTOR.
    
    LOCKUP_DETECTOR implies the general boot, sysctl, and programming
    interfaces for the lockup detectors.
    
    An architecture that wants to use a hard lockup detector must define
    HAVE_HARDLOCKUP_DETECTOR_PERF or HAVE_HARDLOCKUP_DETECTOR_ARCH.
    
    Alternatively an arch can define HAVE_NMI_WATCHDOG, which provides the
    minimum arch_touch_nmi_watchdog, and it otherwise does its own thing and
    does not implement the LOCKUP_DETECTOR interfaces.
    
    sparc is unusual in that it has started to implement some of the
    interfaces, but not fully yet.  It should probably be converted to a full
    HAVE_HARDLOCKUP_DETECTOR_ARCH.
    
    [npiggin@gmail.com: fix]
      Link: http://lkml.kernel.org/r/20170617223522.66c0ad88@roar.ozlabs.ibm.com
    Link: http://lkml.kernel.org/r/20170616065715.18390-4-npiggin@gmail.com
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Reviewed-by: Babu Moger <babu.moger@oracle.com>
    Tested-by: Babu Moger <babu.moger@oracle.com>   [sparc]
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 90d688df6ce1..295a0d84934c 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -22,39 +22,7 @@ static DEFINE_PER_CPU(bool, hard_watchdog_warn);
 static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
 static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
 
-/* boot commands */
-/*
- * Should we panic when a soft-lockup or hard-lockup occurs:
- */
-unsigned int __read_mostly hardlockup_panic =
-			CONFIG_BOOTPARAM_HARDLOCKUP_PANIC_VALUE;
 static unsigned long hardlockup_allcpu_dumped;
-/*
- * We may not want to enable hard lockup detection by default in all cases,
- * for example when running the kernel as a guest on a hypervisor. In these
- * cases this function can be called to disable hard lockup detection. This
- * function should only be executed once by the boot processor before the
- * kernel command line parameters are parsed, because otherwise it is not
- * possible to override this in hardlockup_panic_setup().
- */
-void hardlockup_detector_disable(void)
-{
-	watchdog_enabled &= ~NMI_WATCHDOG_ENABLED;
-}
-
-static int __init hardlockup_panic_setup(char *str)
-{
-	if (!strncmp(str, "panic", 5))
-		hardlockup_panic = 1;
-	else if (!strncmp(str, "nopanic", 7))
-		hardlockup_panic = 0;
-	else if (!strncmp(str, "0", 1))
-		watchdog_enabled &= ~NMI_WATCHDOG_ENABLED;
-	else if (!strncmp(str, "1", 1))
-		watchdog_enabled |= NMI_WATCHDOG_ENABLED;
-	return 1;
-}
-__setup("nmi_watchdog=", hardlockup_panic_setup);
 
 void arch_touch_nmi_watchdog(void)
 {

commit f2e0cff85ed111a3cf24d894c3fa11697dfae628
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jul 12 14:35:43 2017 -0700

    kernel/watchdog: introduce arch_touch_nmi_watchdog()
    
    For architectures that define HAVE_NMI_WATCHDOG, instead of having them
    provide the complete touch_nmi_watchdog() function, just have them
    provide arch_touch_nmi_watchdog().
    
    This gives the generic code more flexibility in implementing this
    function, and arch implementations don't miss out on touching the
    softlockup watchdog or other generic details.
    
    Link: http://lkml.kernel.org/r/20170616065715.18390-3-npiggin@gmail.com
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Don Zickus <dzickus@redhat.com>
    Reviewed-by: Babu Moger <babu.moger@oracle.com>
    Tested-by: Babu Moger <babu.moger@oracle.com>   [sparc]
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 54a427d1f344..90d688df6ce1 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -56,7 +56,7 @@ static int __init hardlockup_panic_setup(char *str)
 }
 __setup("nmi_watchdog=", hardlockup_panic_setup);
 
-void touch_nmi_watchdog(void)
+void arch_touch_nmi_watchdog(void)
 {
 	/*
 	 * Using __raw here because some code paths have
@@ -66,9 +66,8 @@ void touch_nmi_watchdog(void)
 	 * going off.
 	 */
 	raw_cpu_write(watchdog_nmi_touch, true);
-	touch_softlockup_watchdog();
 }
-EXPORT_SYMBOL(touch_nmi_watchdog);
+EXPORT_SYMBOL(arch_touch_nmi_watchdog);
 
 static struct perf_event_attr wd_hw_attr = {
 	.type		= PERF_TYPE_HARDWARE,

commit b17b01533b719e9949e437abf66436a875739b40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/debug.h>
    
    We are going to split <linux/sched/debug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/debug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index b5de262a9eb9..54a427d1f344 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -13,6 +13,8 @@
 
 #include <linux/nmi.h>
 #include <linux/module.h>
+#include <linux/sched/debug.h>
+
 #include <asm/irq_regs.h>
 #include <linux/perf_event.h>
 

commit 8dcde9def5a15df54fa4ca41f7750d80fa741d61
Author: Prarit Bhargava <prarit@redhat.com>
Date:   Wed Feb 22 15:40:56 2017 -0800

    kernel/watchdog.c: do not hardcode CPU 0 as the initial thread
    
    When CONFIG_BOOTPARAM_HOTPLUG_CPU0 is enabled, the socket containing the
    boot cpu can be replaced.  During the hot add event, the message
    
    NMI watchdog: enabled on all CPUs, permanently consumes one hw-PMU counter.
    
    is output implying that the NMI watchdog was disabled at some point.  This
    is not the case and the message has caused confusion for users of systems
    that support the removal of the boot cpu socket.
    
    The watchdog code is coded to assume that cpu 0 is always the first cpu to
    initialize the watchdog, and the last to stop its watchdog thread.  That
    is not the case for initializing if cpu 0 has been removed and added.  The
    removal case has never been correct because the smpboot code will remove
    the watchdog threads starting with the lowest cpu number.
    
    This patch adds watchdog_cpus to track the number of cpus with active NMI
    watchdog threads so that the first and last thread can be used to set and
    clear the value of firstcpu_err.  firstcpu_err is set when the first
    watchdog thread is enabled, and cleared when the last watchdog thread is
    disabled.
    
    Link: http://lkml.kernel.org/r/1480425321-32296-1-git-send-email-prarit@redhat.com
    Signed-off-by: Prarit Bhargava <prarit@redhat.com>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Joshua Hunt <johunt@akamai.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Babu Moger <babu.moger@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 12b8dd640786..b5de262a9eb9 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -137,12 +137,14 @@ static void watchdog_overflow_callback(struct perf_event *event,
  * Reduce the watchdog noise by only printing messages
  * that are different from what cpu0 displayed.
  */
-static unsigned long cpu0_err;
+static unsigned long firstcpu_err;
+static atomic_t watchdog_cpus;
 
 int watchdog_nmi_enable(unsigned int cpu)
 {
 	struct perf_event_attr *wd_attr;
 	struct perf_event *event = per_cpu(watchdog_ev, cpu);
+	int firstcpu = 0;
 
 	/* nothing to do if the hard lockup detector is disabled */
 	if (!(watchdog_enabled & NMI_WATCHDOG_ENABLED))
@@ -156,19 +158,22 @@ int watchdog_nmi_enable(unsigned int cpu)
 	if (event != NULL)
 		goto out_enable;
 
+	if (atomic_inc_return(&watchdog_cpus) == 1)
+		firstcpu = 1;
+
 	wd_attr = &wd_hw_attr;
 	wd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);
 
 	/* Try to register using hardware perf events */
 	event = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
 
-	/* save cpu0 error for future comparision */
-	if (cpu == 0 && IS_ERR(event))
-		cpu0_err = PTR_ERR(event);
+	/* save the first cpu's error for future comparision */
+	if (firstcpu && IS_ERR(event))
+		firstcpu_err = PTR_ERR(event);
 
 	if (!IS_ERR(event)) {
-		/* only print for cpu0 or different than cpu0 */
-		if (cpu == 0 || cpu0_err)
+		/* only print for the first cpu initialized */
+		if (firstcpu || firstcpu_err)
 			pr_info("enabled on all CPUs, permanently consumes one hw-PMU counter.\n");
 		goto out_save;
 	}
@@ -186,7 +191,7 @@ int watchdog_nmi_enable(unsigned int cpu)
 	smp_mb__after_atomic();
 
 	/* skip displaying the same error again */
-	if (cpu > 0 && (PTR_ERR(event) == cpu0_err))
+	if (!firstcpu && (PTR_ERR(event) == firstcpu_err))
 		return PTR_ERR(event);
 
 	/* vary the KERN level based on the returned errno */
@@ -222,9 +227,9 @@ void watchdog_nmi_disable(unsigned int cpu)
 
 		/* should be in cleanup, but blocks oprofile */
 		perf_event_release_kernel(event);
-	}
-	if (cpu == 0) {
+
 		/* watchdog_nmi_enable() expects this to be zero initially. */
-		cpu0_err = 0;
+		if (atomic_dec_and_test(&watchdog_cpus))
+			firstcpu_err = 0;
 	}
 }

commit b94f51183b0617e7b9b4fb4137d4cf1cab7547c2
Author: Don Zickus <dzickus@redhat.com>
Date:   Tue Jan 24 15:17:53 2017 -0800

    kernel/watchdog: prevent false hardlockup on overloaded system
    
    On an overloaded system, it is possible that a change in the watchdog
    threshold can be delayed long enough to trigger a false positive.
    
    This can easily be achieved by having a cpu spinning indefinitely on a
    task, while another cpu updates watchdog threshold.
    
    What happens is while trying to park the watchdog threads, the hrtimers
    on the other cpus trigger and reprogram themselves with the new slower
    watchdog threshold.  Meanwhile, the nmi watchdog is still programmed
    with the old faster threshold.
    
    Because the one cpu is blocked, it prevents the thread parking on the
    other cpus from completing, which is needed to shutdown the nmi watchdog
    and reprogram it correctly.  As a result, a false positive from the nmi
    watchdog is reported.
    
    Fix this by setting a park_in_progress flag to block all lockups until
    the parking is complete.
    
    Fix provided by Ulrich Obergfell.
    
    [akpm@linux-foundation.org: s/park_in_progress/watchdog_park_in_progress/]
    Link: http://lkml.kernel.org/r/1481041033-192236-1-git-send-email-dzickus@redhat.com
    Signed-off-by: Don Zickus <dzickus@redhat.com>
    Reviewed-by: Aaron Tomlin <atomlin@redhat.com>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
index 84016c8aee6b..12b8dd640786 100644
--- a/kernel/watchdog_hld.c
+++ b/kernel/watchdog_hld.c
@@ -84,6 +84,9 @@ static void watchdog_overflow_callback(struct perf_event *event,
 	/* Ensure the watchdog never gets throttled */
 	event->hw.interrupts = 0;
 
+	if (atomic_read(&watchdog_park_in_progress) != 0)
+		return;
+
 	if (__this_cpu_read(watchdog_nmi_touch) == true) {
 		__this_cpu_write(watchdog_nmi_touch, false);
 		return;

commit 73ce0511c43686095efd2f65ef564aab952e07bc
Author: Babu Moger <babu.moger@oracle.com>
Date:   Wed Dec 14 15:06:24 2016 -0800

    kernel/watchdog.c: move hardlockup detector to separate file
    
    Separate hardlockup code from watchdog.c and move it to watchdog_hld.c.
    It is mostly straight forward.  Remove everything inside
    CONFIG_HARDLOCKUP_DETECTORS.  This code will go to file watchdog_hld.c.
    Also update the makefile accordigly.
    
    Link: http://lkml.kernel.org/r/1478034826-43888-3-git-send-email-babu.moger@oracle.com
    Signed-off-by: Babu Moger <babu.moger@oracle.com>
    Acked-by: Don Zickus <dzickus@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Ulrich Obergfell <uobergfe@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Josh Hunt <johunt@akamai.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
new file mode 100644
index 000000000000..84016c8aee6b
--- /dev/null
+++ b/kernel/watchdog_hld.c
@@ -0,0 +1,227 @@
+/*
+ * Detect hard lockups on a system
+ *
+ * started by Don Zickus, Copyright (C) 2010 Red Hat, Inc.
+ *
+ * Note: Most of this code is borrowed heavily from the original softlockup
+ * detector, so thanks to Ingo for the initial implementation.
+ * Some chunks also taken from the old x86-specific nmi watchdog code, thanks
+ * to those contributors as well.
+ */
+
+#define pr_fmt(fmt) "NMI watchdog: " fmt
+
+#include <linux/nmi.h>
+#include <linux/module.h>
+#include <asm/irq_regs.h>
+#include <linux/perf_event.h>
+
+static DEFINE_PER_CPU(bool, hard_watchdog_warn);
+static DEFINE_PER_CPU(bool, watchdog_nmi_touch);
+static DEFINE_PER_CPU(struct perf_event *, watchdog_ev);
+
+/* boot commands */
+/*
+ * Should we panic when a soft-lockup or hard-lockup occurs:
+ */
+unsigned int __read_mostly hardlockup_panic =
+			CONFIG_BOOTPARAM_HARDLOCKUP_PANIC_VALUE;
+static unsigned long hardlockup_allcpu_dumped;
+/*
+ * We may not want to enable hard lockup detection by default in all cases,
+ * for example when running the kernel as a guest on a hypervisor. In these
+ * cases this function can be called to disable hard lockup detection. This
+ * function should only be executed once by the boot processor before the
+ * kernel command line parameters are parsed, because otherwise it is not
+ * possible to override this in hardlockup_panic_setup().
+ */
+void hardlockup_detector_disable(void)
+{
+	watchdog_enabled &= ~NMI_WATCHDOG_ENABLED;
+}
+
+static int __init hardlockup_panic_setup(char *str)
+{
+	if (!strncmp(str, "panic", 5))
+		hardlockup_panic = 1;
+	else if (!strncmp(str, "nopanic", 7))
+		hardlockup_panic = 0;
+	else if (!strncmp(str, "0", 1))
+		watchdog_enabled &= ~NMI_WATCHDOG_ENABLED;
+	else if (!strncmp(str, "1", 1))
+		watchdog_enabled |= NMI_WATCHDOG_ENABLED;
+	return 1;
+}
+__setup("nmi_watchdog=", hardlockup_panic_setup);
+
+void touch_nmi_watchdog(void)
+{
+	/*
+	 * Using __raw here because some code paths have
+	 * preemption enabled.  If preemption is enabled
+	 * then interrupts should be enabled too, in which
+	 * case we shouldn't have to worry about the watchdog
+	 * going off.
+	 */
+	raw_cpu_write(watchdog_nmi_touch, true);
+	touch_softlockup_watchdog();
+}
+EXPORT_SYMBOL(touch_nmi_watchdog);
+
+static struct perf_event_attr wd_hw_attr = {
+	.type		= PERF_TYPE_HARDWARE,
+	.config		= PERF_COUNT_HW_CPU_CYCLES,
+	.size		= sizeof(struct perf_event_attr),
+	.pinned		= 1,
+	.disabled	= 1,
+};
+
+/* Callback function for perf event subsystem */
+static void watchdog_overflow_callback(struct perf_event *event,
+		 struct perf_sample_data *data,
+		 struct pt_regs *regs)
+{
+	/* Ensure the watchdog never gets throttled */
+	event->hw.interrupts = 0;
+
+	if (__this_cpu_read(watchdog_nmi_touch) == true) {
+		__this_cpu_write(watchdog_nmi_touch, false);
+		return;
+	}
+
+	/* check for a hardlockup
+	 * This is done by making sure our timer interrupt
+	 * is incrementing.  The timer interrupt should have
+	 * fired multiple times before we overflow'd.  If it hasn't
+	 * then this is a good indication the cpu is stuck
+	 */
+	if (is_hardlockup()) {
+		int this_cpu = smp_processor_id();
+
+		/* only print hardlockups once */
+		if (__this_cpu_read(hard_watchdog_warn) == true)
+			return;
+
+		pr_emerg("Watchdog detected hard LOCKUP on cpu %d", this_cpu);
+		print_modules();
+		print_irqtrace_events(current);
+		if (regs)
+			show_regs(regs);
+		else
+			dump_stack();
+
+		/*
+		 * Perform all-CPU dump only once to avoid multiple hardlockups
+		 * generating interleaving traces
+		 */
+		if (sysctl_hardlockup_all_cpu_backtrace &&
+				!test_and_set_bit(0, &hardlockup_allcpu_dumped))
+			trigger_allbutself_cpu_backtrace();
+
+		if (hardlockup_panic)
+			nmi_panic(regs, "Hard LOCKUP");
+
+		__this_cpu_write(hard_watchdog_warn, true);
+		return;
+	}
+
+	__this_cpu_write(hard_watchdog_warn, false);
+	return;
+}
+
+/*
+ * People like the simple clean cpu node info on boot.
+ * Reduce the watchdog noise by only printing messages
+ * that are different from what cpu0 displayed.
+ */
+static unsigned long cpu0_err;
+
+int watchdog_nmi_enable(unsigned int cpu)
+{
+	struct perf_event_attr *wd_attr;
+	struct perf_event *event = per_cpu(watchdog_ev, cpu);
+
+	/* nothing to do if the hard lockup detector is disabled */
+	if (!(watchdog_enabled & NMI_WATCHDOG_ENABLED))
+		goto out;
+
+	/* is it already setup and enabled? */
+	if (event && event->state > PERF_EVENT_STATE_OFF)
+		goto out;
+
+	/* it is setup but not enabled */
+	if (event != NULL)
+		goto out_enable;
+
+	wd_attr = &wd_hw_attr;
+	wd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);
+
+	/* Try to register using hardware perf events */
+	event = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL);
+
+	/* save cpu0 error for future comparision */
+	if (cpu == 0 && IS_ERR(event))
+		cpu0_err = PTR_ERR(event);
+
+	if (!IS_ERR(event)) {
+		/* only print for cpu0 or different than cpu0 */
+		if (cpu == 0 || cpu0_err)
+			pr_info("enabled on all CPUs, permanently consumes one hw-PMU counter.\n");
+		goto out_save;
+	}
+
+	/*
+	 * Disable the hard lockup detector if _any_ CPU fails to set up
+	 * set up the hardware perf event. The watchdog() function checks
+	 * the NMI_WATCHDOG_ENABLED bit periodically.
+	 *
+	 * The barriers are for syncing up watchdog_enabled across all the
+	 * cpus, as clear_bit() does not use barriers.
+	 */
+	smp_mb__before_atomic();
+	clear_bit(NMI_WATCHDOG_ENABLED_BIT, &watchdog_enabled);
+	smp_mb__after_atomic();
+
+	/* skip displaying the same error again */
+	if (cpu > 0 && (PTR_ERR(event) == cpu0_err))
+		return PTR_ERR(event);
+
+	/* vary the KERN level based on the returned errno */
+	if (PTR_ERR(event) == -EOPNOTSUPP)
+		pr_info("disabled (cpu%i): not supported (no LAPIC?)\n", cpu);
+	else if (PTR_ERR(event) == -ENOENT)
+		pr_warn("disabled (cpu%i): hardware events not enabled\n",
+			 cpu);
+	else
+		pr_err("disabled (cpu%i): unable to create perf event: %ld\n",
+			cpu, PTR_ERR(event));
+
+	pr_info("Shutting down hard lockup detector on all cpus\n");
+
+	return PTR_ERR(event);
+
+	/* success path */
+out_save:
+	per_cpu(watchdog_ev, cpu) = event;
+out_enable:
+	perf_event_enable(per_cpu(watchdog_ev, cpu));
+out:
+	return 0;
+}
+
+void watchdog_nmi_disable(unsigned int cpu)
+{
+	struct perf_event *event = per_cpu(watchdog_ev, cpu);
+
+	if (event) {
+		perf_event_disable(event);
+		per_cpu(watchdog_ev, cpu) = NULL;
+
+		/* should be in cleanup, but blocks oprofile */
+		perf_event_release_kernel(event);
+	}
+	if (cpu == 0) {
+		/* watchdog_nmi_enable() expects this to be zero initially. */
+		cpu0_err = 0;
+	}
+}
