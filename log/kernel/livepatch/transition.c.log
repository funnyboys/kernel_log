commit 7e35e4eb7e56233dcf445992d7b835a9ba93408e
Author: Petr Mladek <pmladek@suse.com>
Date:   Wed Oct 30 16:43:09 2019 +0100

    livepatch: Keep replaced patches until post_patch callback is called
    
    Pre/post (un)patch callbacks might manipulate the system state. Cumulative
    livepatches might need to take over the changes made by the replaced
    ones. For this they might need to access some data stored or referenced
    by the old livepatches.
    
    Therefore the replaced livepatches have to stay around until post_patch()
    callback is called. It is achieved by calling the free functions later.
    It is the same location where disabled livepatches have already been
    freed.
    
    Link: http://lkml.kernel.org/r/20191030154313.13263-2-pmladek@suse.com
    To: Jiri Kosina <jikos@kernel.org>
    Cc: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Cc: Nicolai Stange <nstange@suse.de>
    Cc: live-patching@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Joe Lawrence <joe.lawrence@redhat.com>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index cdf318d86dd6..f6310f848f34 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -78,7 +78,7 @@ static void klp_complete_transition(void)
 		 klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
 
 	if (klp_transition_patch->replace && klp_target_state == KLP_PATCHED) {
-		klp_discard_replaced_patches(klp_transition_patch);
+		klp_unpatch_replaced_patches(klp_transition_patch);
 		klp_discard_nops(klp_transition_patch);
 	}
 
@@ -446,14 +446,14 @@ void klp_try_complete_transition(void)
 	klp_complete_transition();
 
 	/*
-	 * It would make more sense to free the patch in
+	 * It would make more sense to free the unused patches in
 	 * klp_complete_transition() but it is called also
 	 * from klp_cancel_transition().
 	 */
-	if (!patch->enabled) {
-		klp_free_patch_start(patch);
-		schedule_work(&patch->free_work);
-	}
+	if (!patch->enabled)
+		klp_free_patch_async(patch);
+	else if (patch->replace)
+		klp_free_replaced_patches_async(patch);
 }
 
 /*

commit db0457338ece7482378d88e50ad298191c3e6947
Merge: 1f7563f743d7 38195dd5e916
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 11 15:30:05 2019 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/livepatching/livepatching
    
    Pull livepatching updates from Jiri Kosina:
    
     - stacktrace handling improvements from Miroslav benes
    
     - debug output improvements from Petr Mladek
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/livepatching/livepatching:
      livepatch: Remove duplicate warning about missing reliable stacktrace support
      Revert "livepatch: Remove reliable stacktrace check in klp_try_switch_task()"
      stacktrace: Remove weak version of save_stack_trace_tsk_reliable()
      livepatch: Use static buffer for debugging messages under rq lock
      livepatch: Remove stale kobj_added entries from kernel-doc descriptions

commit ac59a471e9371e3184425efd6a2fd8ac5c7e4c2b
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Jun 11 16:13:20 2019 +0200

    livepatch: Remove duplicate warning about missing reliable stacktrace support
    
    WARN_ON_ONCE() could not be called safely under rq lock because
    of console deadlock issues. Moreover WARN_ON_ONCE() is superfluous in
    klp_check_stack(), because stack_trace_save_tsk_reliable() cannot return
    -ENOSYS thanks to klp_have_reliable_stack() check in
    klp_try_switch_task().
    
    [ mbenes: changelog edited ]
    Signed-off-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index cb85dae09ce5..3c764e73b032 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -259,7 +259,6 @@ static int klp_check_stack(struct task_struct *task, char *err_buf)
 	int ret, nr_entries;
 
 	ret = stack_trace_save_tsk_reliable(task, entries, ARRAY_SIZE(entries));
-	WARN_ON_ONCE(ret == -ENOSYS);
 	if (ret < 0) {
 		snprintf(err_buf, STACK_ERR_BUF_SIZE,
 			 "%s: %s:%d has an unreliable stack\n",

commit 67059d65f7da537c41513fc52e78eff096092b8c
Author: Miroslav Benes <mbenes@suse.cz>
Date:   Tue Jun 11 16:13:19 2019 +0200

    Revert "livepatch: Remove reliable stacktrace check in klp_try_switch_task()"
    
    This reverts commit 1d98a69e5cef3aeb68bcefab0e67e342d6bb4dad. Commit
    31adf2308f33 ("livepatch: Convert error about unsupported reliable
    stacktrace into a warning") weakened the enforcement for architectures
    to have reliable stack traces support. The system only warns now about
    it.
    
    It only makes sense to reintroduce the compile time checking in
    klp_try_switch_task() again and bail out early.
    
    Signed-off-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 0a3889c4f617..cb85dae09ce5 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -305,6 +305,13 @@ static bool klp_try_switch_task(struct task_struct *task)
 	if (task->patch_state == klp_target_state)
 		return true;
 
+	/*
+	 * For arches which don't have reliable stack traces, we have to rely
+	 * on other methods (e.g., switching tasks at kernel exit).
+	 */
+	if (!klp_have_reliable_stack())
+		return false;
+
 	/*
 	 * Now try to check the stack for any to-be-patched or to-be-unpatched
 	 * functions.  If all goes well, switch the task to the target patch

commit f36e664516b02c7f54bbd3094bab047d54bb5488
Author: Petr Mladek <pmladek@suse.com>
Date:   Fri May 31 09:41:47 2019 +0200

    livepatch: Use static buffer for debugging messages under rq lock
    
    The err_buf array uses 128 bytes of stack space.  Move it off the stack
    by making it static.  It's safe to use a shared buffer because
    klp_try_switch_task() is called under klp_mutex.
    
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index c53370d596be..0a3889c4f617 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -293,11 +293,11 @@ static int klp_check_stack(struct task_struct *task, char *err_buf)
  */
 static bool klp_try_switch_task(struct task_struct *task)
 {
+	static char err_buf[STACK_ERR_BUF_SIZE];
 	struct rq *rq;
 	struct rq_flags flags;
 	int ret;
 	bool success = false;
-	char err_buf[STACK_ERR_BUF_SIZE];
 
 	err_buf[0] = '\0';
 
@@ -340,7 +340,6 @@ static bool klp_try_switch_task(struct task_struct *task)
 		pr_debug("%s", err_buf);
 
 	return success;
-
 }
 
 /*

commit 1ccea77e2a2687cae171b7987eb44730ec8c6d5f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 15:51:43 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 13
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details you
      should have received a copy of the gnu general public license along
      with this program if not see http www gnu org licenses
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details [based]
      [from] [clk] [highbank] [c] you should have received a copy of the
      gnu general public license along with this program if not see http
      www gnu org licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 355 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Jilayne Lovejoy <opensource@jilayne.com>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190519154041.837383322@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index c53370d596be..abb2a4a2cbb2 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -1,20 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * transition.c - Kernel Live Patching transition functions
  *
  * Copyright (C) 2015-2016 Josh Poimboeuf <jpoimboe@redhat.com>
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version 2
- * of the License, or (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, see <http://www.gnu.org/licenses/>.
  */
 
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt

commit 25e39e32b0a3f99b9db320605f20f91d425b6a65
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 11:45:18 2019 +0200

    livepatch: Simplify stack trace retrieval
    
    Replace the indirection through struct stack_trace by using the storage
    array based interfaces.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: linux-mm@kvack.org
    Cc: David Rientjes <rientjes@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: kasan-dev@googlegroups.com
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: David Sterba <dsterba@suse.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: linux-btrfs@vger.kernel.org
    Cc: dm-devel@redhat.com
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: dri-devel@lists.freedesktop.org
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: linux-arch@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190425094803.437950229@linutronix.de

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 9c89ae8b337a..c53370d596be 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -202,15 +202,15 @@ void klp_update_patch_state(struct task_struct *task)
  * Determine whether the given stack trace includes any references to a
  * to-be-patched or to-be-unpatched function.
  */
-static int klp_check_stack_func(struct klp_func *func,
-				struct stack_trace *trace)
+static int klp_check_stack_func(struct klp_func *func, unsigned long *entries,
+				unsigned int nr_entries)
 {
 	unsigned long func_addr, func_size, address;
 	struct klp_ops *ops;
 	int i;
 
-	for (i = 0; i < trace->nr_entries; i++) {
-		address = trace->entries[i];
+	for (i = 0; i < nr_entries; i++) {
+		address = entries[i];
 
 		if (klp_target_state == KLP_UNPATCHED) {
 			 /*
@@ -254,29 +254,25 @@ static int klp_check_stack_func(struct klp_func *func,
 static int klp_check_stack(struct task_struct *task, char *err_buf)
 {
 	static unsigned long entries[MAX_STACK_ENTRIES];
-	struct stack_trace trace;
 	struct klp_object *obj;
 	struct klp_func *func;
-	int ret;
+	int ret, nr_entries;
 
-	trace.skip = 0;
-	trace.nr_entries = 0;
-	trace.max_entries = MAX_STACK_ENTRIES;
-	trace.entries = entries;
-	ret = save_stack_trace_tsk_reliable(task, &trace);
+	ret = stack_trace_save_tsk_reliable(task, entries, ARRAY_SIZE(entries));
 	WARN_ON_ONCE(ret == -ENOSYS);
-	if (ret) {
+	if (ret < 0) {
 		snprintf(err_buf, STACK_ERR_BUF_SIZE,
 			 "%s: %s:%d has an unreliable stack\n",
 			 __func__, task->comm, task->pid);
 		return ret;
 	}
+	nr_entries = ret;
 
 	klp_for_each_object(klp_transition_patch, obj) {
 		if (!obj->patched)
 			continue;
 		klp_for_each_func(obj, func) {
-			ret = klp_check_stack_func(func, &trace);
+			ret = klp_check_stack_func(func, entries, nr_entries);
 			if (ret) {
 				snprintf(err_buf, STACK_ERR_BUF_SIZE,
 					 "%s: %s:%d is sleeping on function %s\n",

commit f9d138145686b52b48ccb36557d6842076e2b9dd
Merge: 7185a96981a2 fbb76d579dff
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Tue Mar 5 15:56:59 2019 +0100

    Merge branch 'for-5.1/atomic-replace' into for-linus
    
    The atomic replace allows to create cumulative patches. They are useful when
    you maintain many livepatches and want to remove one that is lower on the
    stack. In addition it is very useful when more patches touch the same function
    and there are dependencies between them.
    
    It's also a feature some of the distros are using already to distribute
    their patches.

commit ecba29f434a8fa333356d54d2491d174c4aab8de
Author: Petr Mladek <pmladek@suse.com>
Date:   Mon Feb 4 14:56:50 2019 +0100

    livepatch: Introduce klp_for_each_patch macro
    
    There are already macros to iterate over struct klp_func and klp_object.
    
    Add also klp_for_each_patch(). But make it internal because also
    klp_patches list is internal.
    
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Joe Lawrence <joe.lawrence@redhat.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 300273819674..a3a6f32c6fd0 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -642,6 +642,6 @@ void klp_force_transition(void)
 	for_each_possible_cpu(cpu)
 		klp_update_patch_state(idle_task(cpu));
 
-	list_for_each_entry(patch, &klp_patches, list)
+	klp_for_each_patch(patch)
 		patch->forced = true;
 }

commit 0b3d52790e1cfd6b80b826a245d24859e89632f7
Author: Miroslav Benes <mbenes@suse.cz>
Date:   Tue Jan 15 17:45:07 2019 +0100

    livepatch: Remove signal sysfs attribute
    
    The fake signal is send automatically now. We can rely on it completely
    and remove the sysfs attribute.
    
    Signed-off-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index ea7697bb753e..183b2086ba03 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -347,6 +347,47 @@ static bool klp_try_switch_task(struct task_struct *task)
 
 }
 
+/*
+ * Sends a fake signal to all non-kthread tasks with TIF_PATCH_PENDING set.
+ * Kthreads with TIF_PATCH_PENDING set are woken up.
+ */
+static void klp_send_signals(void)
+{
+	struct task_struct *g, *task;
+
+	if (klp_signals_cnt == SIGNALS_TIMEOUT)
+		pr_notice("signaling remaining tasks\n");
+
+	read_lock(&tasklist_lock);
+	for_each_process_thread(g, task) {
+		if (!klp_patch_pending(task))
+			continue;
+
+		/*
+		 * There is a small race here. We could see TIF_PATCH_PENDING
+		 * set and decide to wake up a kthread or send a fake signal.
+		 * Meanwhile the task could migrate itself and the action
+		 * would be meaningless. It is not serious though.
+		 */
+		if (task->flags & PF_KTHREAD) {
+			/*
+			 * Wake up a kthread which sleeps interruptedly and
+			 * still has not been migrated.
+			 */
+			wake_up_state(task, TASK_INTERRUPTIBLE);
+		} else {
+			/*
+			 * Send fake signal to all non-kthread tasks which are
+			 * still not migrated.
+			 */
+			spin_lock_irq(&task->sighand->siglock);
+			signal_wake_up(task, 0);
+			spin_unlock_irq(&task->sighand->siglock);
+		}
+	}
+	read_unlock(&tasklist_lock);
+}
+
 /*
  * Try to switch all remaining tasks to the target patch state by walking the
  * stacks of sleeping tasks and looking for any to-be-patched or
@@ -586,47 +627,6 @@ void klp_copy_process(struct task_struct *child)
 	/* TIF_PATCH_PENDING gets copied in setup_thread_stack() */
 }
 
-/*
- * Sends a fake signal to all non-kthread tasks with TIF_PATCH_PENDING set.
- * Kthreads with TIF_PATCH_PENDING set are woken up.
- */
-void klp_send_signals(void)
-{
-	struct task_struct *g, *task;
-
-	if (klp_signals_cnt == SIGNALS_TIMEOUT)
-		pr_notice("signaling remaining tasks\n");
-
-	read_lock(&tasklist_lock);
-	for_each_process_thread(g, task) {
-		if (!klp_patch_pending(task))
-			continue;
-
-		/*
-		 * There is a small race here. We could see TIF_PATCH_PENDING
-		 * set and decide to wake up a kthread or send a fake signal.
-		 * Meanwhile the task could migrate itself and the action
-		 * would be meaningless. It is not serious though.
-		 */
-		if (task->flags & PF_KTHREAD) {
-			/*
-			 * Wake up a kthread which sleeps interruptedly and
-			 * still has not been migrated.
-			 */
-			wake_up_state(task, TASK_INTERRUPTIBLE);
-		} else {
-			/*
-			 * Send fake signal to all non-kthread tasks which are
-			 * still not migrated.
-			 */
-			spin_lock_irq(&task->sighand->siglock);
-			signal_wake_up(task, 0);
-			spin_unlock_irq(&task->sighand->siglock);
-		}
-	}
-	read_unlock(&tasklist_lock);
-}
-
 /*
  * Drop TIF_PATCH_PENDING of all tasks on admin's request. This forces an
  * existing transition to finish.

commit cba82dea30613346cf9a0532a41fc118bc3263af
Author: Miroslav Benes <mbenes@suse.cz>
Date:   Tue Jan 15 17:45:06 2019 +0100

    livepatch: Send a fake signal periodically
    
    An administrator may send a fake signal to all remaining blocking tasks
    of a running transition by writing to
    /sys/kernel/livepatch/<patch>/signal attribute. Let's do it
    automatically after 15 seconds. The timeout is chosen deliberately. It
    gives the tasks enough time to transition themselves.
    
    Theoretically, sending it once should be more than enough. However,
    every task must get outside of a patched function to be successfully
    transitioned. It could prove not to be simple and resending could be
    helpful in that case.
    
    A new workqueue job could be a cleaner solution to achieve it, but it
    could also introduce deadlocks and cause more headaches with
    synchronization and cancelling.
    
    [jkosina@suse.cz: removed added newline]
    Signed-off-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 300273819674..ea7697bb753e 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -29,10 +29,14 @@
 #define MAX_STACK_ENTRIES  100
 #define STACK_ERR_BUF_SIZE 128
 
+#define SIGNALS_TIMEOUT 15
+
 struct klp_patch *klp_transition_patch;
 
 static int klp_target_state = KLP_UNDEFINED;
 
+static unsigned int klp_signals_cnt;
+
 /*
  * This work can be performed periodically to finish patching or unpatching any
  * "straggler" tasks which failed to transition in the first attempt.
@@ -393,6 +397,10 @@ void klp_try_complete_transition(void)
 	put_online_cpus();
 
 	if (!complete) {
+		if (klp_signals_cnt && !(klp_signals_cnt % SIGNALS_TIMEOUT))
+			klp_send_signals();
+		klp_signals_cnt++;
+
 		/*
 		 * Some tasks weren't able to be switched over.  Try again
 		 * later and/or wait for other methods like kernel exit
@@ -454,6 +462,8 @@ void klp_start_transition(void)
 		if (task->patch_state != klp_target_state)
 			set_tsk_thread_flag(task, TIF_PATCH_PENDING);
 	}
+
+	klp_signals_cnt = 0;
 }
 
 /*
@@ -578,14 +588,14 @@ void klp_copy_process(struct task_struct *child)
 
 /*
  * Sends a fake signal to all non-kthread tasks with TIF_PATCH_PENDING set.
- * Kthreads with TIF_PATCH_PENDING set are woken up. Only admin can request this
- * action currently.
+ * Kthreads with TIF_PATCH_PENDING set are woken up.
  */
 void klp_send_signals(void)
 {
 	struct task_struct *g, *task;
 
-	pr_notice("signaling remaining tasks\n");
+	if (klp_signals_cnt == SIGNALS_TIMEOUT)
+		pr_notice("signaling remaining tasks\n");
 
 	read_lock(&tasklist_lock);
 	for_each_process_thread(g, task) {

commit d697bad588eb4e76311193e6eaacc7c7aaa5a4ba
Author: Petr Mladek <pmladek@suse.com>
Date:   Wed Jan 9 13:43:26 2019 +0100

    livepatch: Remove Nop structures when unused
    
    Replaced patches are removed from the stack when the transition is
    finished. It means that Nop structures will never be needed again
    and can be removed. Why should we care?
    
      + Nop structures give the impression that the function is patched
        even though the ftrace handler has no effect.
    
      + Ftrace handlers do not come for free. They cause slowdown that might
        be visible in some workloads. The ftrace-related slowdown might
        actually be the reason why the function is no longer patched in
        the new cumulative patch. One would expect that cumulative patch
        would help solve these problems as well.
    
      + Cumulative patches are supposed to replace any earlier version of
        the patch. The amount of NOPs depends on which version was replaced.
        This multiplies the amount of scenarios that might happen.
    
        One might say that NOPs are innocent. But there are even optimized
        NOP instructions for different processors, for example, see
        arch/x86/kernel/alternative.c. And klp_ftrace_handler() is much
        more complicated.
    
      + It sounds natural to clean up a mess that is no longer needed.
        It could only be worse if we do not do it.
    
    This patch allows to unpatch and free the dynamic structures independently
    when the transition finishes.
    
    The free part is a bit tricky because kobject free callbacks are called
    asynchronously. We could not wait for them easily. Fortunately, we do
    not have to. Any further access can be avoided by removing them from
    the dynamic lists.
    
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index f4c5908a9731..300273819674 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -85,8 +85,10 @@ static void klp_complete_transition(void)
 		 klp_transition_patch->mod->name,
 		 klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
 
-	if (klp_transition_patch->replace && klp_target_state == KLP_PATCHED)
+	if (klp_transition_patch->replace && klp_target_state == KLP_PATCHED) {
 		klp_discard_replaced_patches(klp_transition_patch);
+		klp_discard_nops(klp_transition_patch);
+	}
 
 	if (klp_target_state == KLP_UNPATCHED) {
 		/*

commit e1452b607c48c642caf57299f4da83aa002f8533
Author: Jason Baron <jbaron@akamai.com>
Date:   Wed Jan 9 13:43:25 2019 +0100

    livepatch: Add atomic replace
    
    Sometimes we would like to revert a particular fix. Currently, this
    is not easy because we want to keep all other fixes active and we
    could revert only the last applied patch.
    
    One solution would be to apply new patch that implemented all
    the reverted functions like in the original code. It would work
    as expected but there will be unnecessary redirections. In addition,
    it would also require knowing which functions need to be reverted at
    build time.
    
    Another problem is when there are many patches that touch the same
    functions. There might be dependencies between patches that are
    not enforced on the kernel side. Also it might be pretty hard to
    actually prepare the patch and ensure compatibility with the other
    patches.
    
    Atomic replace && cumulative patches:
    
    A better solution would be to create cumulative patch and say that
    it replaces all older ones.
    
    This patch adds a new "replace" flag to struct klp_patch. When it is
    enabled, a set of 'nop' klp_func will be dynamically created for all
    functions that are already being patched but that will no longer be
    modified by the new patch. They are used as a new target during
    the patch transition.
    
    The idea is to handle Nops' structures like the static ones. When
    the dynamic structures are allocated, we initialize all values that
    are normally statically defined.
    
    The only exception is "new_func" in struct klp_func. It has to point
    to the original function and the address is known only when the object
    (module) is loaded. Note that we really need to set it. The address is
    used, for example, in klp_check_stack_func().
    
    Nevertheless we still need to distinguish the dynamically allocated
    structures in some operations. For this, we add "nop" flag into
    struct klp_func and "dynamic" flag into struct klp_object. They
    need special handling in the following situations:
    
      + The structures are added into the lists of objects and functions
        immediately. In fact, the lists were created for this purpose.
    
      + The address of the original function is known only when the patched
        object (module) is loaded. Therefore it is copied later in
        klp_init_object_loaded().
    
      + The ftrace handler must not set PC to func->new_func. It would cause
        infinite loop because the address points back to the beginning of
        the original function.
    
      + The various free() functions must free the structure itself.
    
    Note that other ways to detect the dynamic structures are not considered
    safe. For example, even the statically defined struct klp_object might
    include empty funcs array. It might be there just to run some callbacks.
    
    Also note that the safe iterator must be used in the free() functions.
    Otherwise already freed structures might get accessed.
    
    Special callbacks handling:
    
    The callbacks from the replaced patches are _not_ called by intention.
    It would be pretty hard to define a reasonable semantic and implement it.
    
    It might even be counter-productive. The new patch is cumulative. It is
    supposed to include most of the changes from older patches. In most cases,
    it will not want to call pre_unpatch() post_unpatch() callbacks from
    the replaced patches. It would disable/break things for no good reasons.
    Also it should be easier to handle various scenarios in a single script
    in the new patch than think about interactions caused by running many
    scripts from older patches. Not to say that the old scripts even would
    not expect to be called in this situation.
    
    Removing replaced patches:
    
    One nice effect of the cumulative patches is that the code from the
    older patches is no longer used. Therefore the replaced patches can
    be removed. It has several advantages:
    
      + Nops' structs will no longer be necessary and might be removed.
        This would save memory, restore performance (no ftrace handler),
        allow clear view on what is really patched.
    
      + Disabling the patch will cause using the original code everywhere.
        Therefore the livepatch callbacks could handle only one scenario.
        Note that the complication is already complex enough when the patch
        gets enabled. It is currently solved by calling callbacks only from
        the new cumulative patch.
    
      + The state is clean in both the sysfs interface and lsmod. The modules
        with the replaced livepatches might even get removed from the system.
    
    Some people actually expected this behavior from the beginning. After all
    a cumulative patch is supposed to "completely" replace an existing one.
    It is like when a new version of an application replaces an older one.
    
    This patch does the first step. It removes the replaced patches from
    the list of patches. It is safe. The consistency model ensures that
    they are no longer used. By other words, each process works only with
    the structures from klp_transition_patch.
    
    The removal is done by a special function. It combines actions done by
    __disable_patch() and klp_complete_transition(). But it is a fast
    track without all the transaction-related stuff.
    
    Signed-off-by: Jason Baron <jbaron@akamai.com>
    [pmladek@suse.com: Split, reuse existing code, simplified]
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Jessica Yu <jeyu@kernel.org>
    Cc: Jiri Kosina <jikos@kernel.org>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index c9917a24b3a4..f4c5908a9731 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -85,6 +85,9 @@ static void klp_complete_transition(void)
 		 klp_transition_patch->mod->name,
 		 klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
 
+	if (klp_transition_patch->replace && klp_target_state == KLP_PATCHED)
+		klp_discard_replaced_patches(klp_transition_patch);
+
 	if (klp_target_state == KLP_UNPATCHED) {
 		/*
 		 * All tasks have transitioned to KLP_UNPATCHED so we can now

commit 958ef1e39d24d6cb8bf2a7406130a98c9564230f
Author: Petr Mladek <pmladek@suse.com>
Date:   Wed Jan 9 13:43:23 2019 +0100

    livepatch: Simplify API by removing registration step
    
    The possibility to re-enable a registered patch was useful for immediate
    patches where the livepatch module had to stay until the system reboot.
    The improved consistency model allows to achieve the same result by
    unloading and loading the livepatch module again.
    
    Also we are going to add a feature called atomic replace. It will allow
    to create a patch that would replace all already registered patches.
    The aim is to handle dependent patches more securely. It will obsolete
    the stack of patches that helped to handle the dependencies so far.
    Then it might be unclear when a cumulative patch re-enabling is safe.
    
    It would be complicated to support the many modes. Instead we could
    actually make the API and code easier to understand.
    
    Therefore, remove the two step public API. All the checks and init calls
    are moved from klp_register_patch() to klp_enabled_patch(). Also the patch
    is automatically freed, including the sysfs interface when the transition
    to the disabled state is completed.
    
    As a result, there is never a disabled patch on the top of the stack.
    Therefore we do not need to check the stack in __klp_enable_patch().
    And we could simplify the check in __klp_disable_patch().
    
    Also the API and logic is much easier. It is enough to call
    klp_enable_patch() in module_init() call. The patch can be disabled
    by writing '0' into /sys/kernel/livepatch/<patch>/enabled. Then the module
    can be removed once the transition finishes and sysfs interface is freed.
    
    The only problem is how to free the structures and kobjects safely.
    The operation is triggered from the sysfs interface. We could not put
    the related kobject from there because it would cause lock inversion
    between klp_mutex and kernfs locks, see kn->count lockdep map.
    
    Therefore, offload the free task to a workqueue. It is perfectly fine:
    
      + The patch can no longer be used in the livepatch operations.
    
      + The module could not be removed until the free operation finishes
        and module_put() is called.
    
      + The operation is asynchronous already when the first
        klp_try_complete_transition() fails and another call
        is queued with a delay.
    
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index a4c921364003..c9917a24b3a4 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -134,13 +134,6 @@ static void klp_complete_transition(void)
 	pr_notice("'%s': %s complete\n", klp_transition_patch->mod->name,
 		  klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
 
-	/*
-	 * patch->forced set implies unbounded increase of module's ref count if
-	 * the module is disabled/enabled in a loop.
-	 */
-	if (!klp_transition_patch->forced && klp_target_state == KLP_UNPATCHED)
-		module_put(klp_transition_patch->mod);
-
 	klp_target_state = KLP_UNDEFINED;
 	klp_transition_patch = NULL;
 }
@@ -357,6 +350,7 @@ void klp_try_complete_transition(void)
 {
 	unsigned int cpu;
 	struct task_struct *g, *task;
+	struct klp_patch *patch;
 	bool complete = true;
 
 	WARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);
@@ -405,7 +399,18 @@ void klp_try_complete_transition(void)
 	}
 
 	/* we're done, now cleanup the data structures */
+	patch = klp_transition_patch;
 	klp_complete_transition();
+
+	/*
+	 * It would make more sense to free the patch in
+	 * klp_complete_transition() but it is called also
+	 * from klp_cancel_transition().
+	 */
+	if (!patch->enabled) {
+		klp_free_patch_start(patch);
+		schedule_work(&patch->free_work);
+	}
 }
 
 /*

commit 68007289bf3cd937a5b8fc4987d2787167bd06ca
Author: Petr Mladek <pmladek@suse.com>
Date:   Wed Jan 9 13:43:22 2019 +0100

    livepatch: Don't block the removal of patches loaded after a forced transition
    
    module_put() is currently never called in klp_complete_transition() when
    klp_force is set. As a result, we might keep the reference count even when
    klp_enable_patch() fails and klp_cancel_transition() is called.
    
    This might give the impression that a module might get blocked in some
    strange init state. Fortunately, it is not the case. The reference count
    is ignored when mod->init fails and erroneous modules are always removed.
    
    Anyway, this might be confusing. Instead, this patch moves
    the global klp_forced flag into struct klp_patch. As a result,
    we block only modules that might still be in use after a forced
    transition. Newly loaded livepatches might be eventually completely
    removed later.
    
    It is not a big deal. But the code is at least consistent with
    the reality.
    
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Joe Lawrence <joe.lawrence@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index f27a378ad5e1..a4c921364003 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -33,8 +33,6 @@ struct klp_patch *klp_transition_patch;
 
 static int klp_target_state = KLP_UNDEFINED;
 
-static bool klp_forced = false;
-
 /*
  * This work can be performed periodically to finish patching or unpatching any
  * "straggler" tasks which failed to transition in the first attempt.
@@ -137,10 +135,10 @@ static void klp_complete_transition(void)
 		  klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
 
 	/*
-	 * klp_forced set implies unbounded increase of module's ref count if
+	 * patch->forced set implies unbounded increase of module's ref count if
 	 * the module is disabled/enabled in a loop.
 	 */
-	if (!klp_forced && klp_target_state == KLP_UNPATCHED)
+	if (!klp_transition_patch->forced && klp_target_state == KLP_UNPATCHED)
 		module_put(klp_transition_patch->mod);
 
 	klp_target_state = KLP_UNDEFINED;
@@ -620,6 +618,7 @@ void klp_send_signals(void)
  */
 void klp_force_transition(void)
 {
+	struct klp_patch *patch;
 	struct task_struct *g, *task;
 	unsigned int cpu;
 
@@ -633,5 +632,6 @@ void klp_force_transition(void)
 	for_each_possible_cpu(cpu)
 		klp_update_patch_state(idle_task(cpu));
 
-	klp_forced = true;
+	list_for_each_entry(patch, &klp_patches, list)
+		patch->forced = true;
 }

commit 19514910d021c93c7823ec32067e6b7dea224f0f
Author: Petr Mladek <pmladek@suse.com>
Date:   Wed Jan 9 13:43:19 2019 +0100

    livepatch: Change unsigned long old_addr -> void *old_func in struct klp_func
    
    The address of the to be patched function and new function is stored
    in struct klp_func as:
    
            void *new_func;
            unsigned long old_addr;
    
    The different naming scheme and type are derived from the way
    the addresses are set. @old_addr is assigned at runtime using
    kallsyms-based search. @new_func is statically initialized,
    for example:
    
      static struct klp_func funcs[] = {
            {
                    .old_name = "cmdline_proc_show",
                    .new_func = livepatch_cmdline_proc_show,
            }, { }
      };
    
    This patch changes unsigned long old_addr -> void *old_func. It removes
    some confusion when these address are later used in the code. It is
    motivated by a followup patch that adds special NOP struct klp_func
    where we want to assign func->new_func = func->old_addr respectively
    func->new_func = func->old_func.
    
    This patch does not modify the existing behavior.
    
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Joe Lawrence <joe.lawrence@redhat.com>
    Acked-by: Alice Ferrazzi <alice.ferrazzi@gmail.com>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 304d5eb8a98c..f27a378ad5e1 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -224,11 +224,11 @@ static int klp_check_stack_func(struct klp_func *func,
 			 * Check for the to-be-patched function
 			 * (the previous func).
 			 */
-			ops = klp_find_ops(func->old_addr);
+			ops = klp_find_ops(func->old_func);
 
 			if (list_is_singular(&ops->func_stack)) {
 				/* original function */
-				func_addr = func->old_addr;
+				func_addr = (unsigned long)func->old_func;
 				func_size = func->old_size;
 			} else {
 				/* previously patched function */

commit 6932689e4145f545062ca8c86cf76f38854d63d0
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Wed Nov 7 14:16:57 2018 -0800

    livepatch: Replace synchronize_sched() with synchronize_rcu()
    
    Now that synchronize_rcu() waits for preempt-disable regions of code
    as well as RCU read-side critical sections, synchronize_sched() can be
    replaced by synchronize_rcu().  This commit therefore makes this change,
    even though it is but a comment.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 5bc349805e03..304d5eb8a98c 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -52,7 +52,7 @@ static DECLARE_DELAYED_WORK(klp_transition_work, klp_transition_work_fn);
 
 /*
  * This function is just a stub to implement a hard force
- * of synchronize_sched(). This requires synchronizing
+ * of synchronize_rcu(). This requires synchronizing
  * tasks even in userspace and idle.
  */
 static void klp_sync(struct work_struct *work)
@@ -175,7 +175,7 @@ void klp_cancel_transition(void)
 void klp_update_patch_state(struct task_struct *task)
 {
 	/*
-	 * A variant of synchronize_sched() is used to allow patching functions
+	 * A variant of synchronize_rcu() is used to allow patching functions
 	 * where RCU is not watching, see klp_synchronize_transition().
 	 */
 	preempt_disable_notrace();

commit 1d98a69e5cef3aeb68bcefab0e67e342d6bb4dad
Author: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
Date:   Thu Jul 12 13:35:06 2018 +0530

    livepatch: Remove reliable stacktrace check in klp_try_switch_task()
    
    Support for immediate flag was removed by commit d0807da78e11
    ("livepatch: Remove immediate feature").  We bail out during
    patch registration for architectures, those don't support
    reliable stack trace. Remove the check in klp_try_switch_task(),
    as its not required.
    
    Signed-off-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 7c6631e693bc..5bc349805e03 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -309,13 +309,6 @@ static bool klp_try_switch_task(struct task_struct *task)
 	if (task->patch_state == klp_target_state)
 		return true;
 
-	/*
-	 * For arches which don't have reliable stack traces, we have to rely
-	 * on other methods (e.g., switching tasks at kernel exit).
-	 */
-	if (!klp_have_reliable_stack())
-		return false;
-
 	/*
 	 * Now try to check the stack for any to-be-patched or to-be-unpatched
 	 * functions.  If all goes well, switch the task to the target patch

commit d0807da78e11d46f18399cbf8c4028c731346766
Author: Miroslav Benes <mbenes@suse.cz>
Date:   Wed Jan 10 11:01:28 2018 +0100

    livepatch: Remove immediate feature
    
    Immediate flag has been used to disable per-task consistency and patch
    all tasks immediately. It could be useful if the patch doesn't change any
    function or data semantics.
    
    However, it causes problems on its own. The consistency problem is
    currently broken with respect to immediate patches.
    
    func            a
    patches         1i
                    2i
                    3
    
    When the patch 3 is applied, only 2i function is checked (by stack
    checking facility). There might be a task sleeping in 1i though. Such
    task is migrated to 3, because we do not check 1i in
    klp_check_stack_func() at all.
    
    Coming atomic replace feature would be easier to implement and more
    reliable without immediate.
    
    Thus, remove immediate feature completely and save us from the problems.
    
    Note that force feature has the similar problem. However it is
    considered as a last resort. If used, administrator should not apply any
    new live patches and should plan for reboot into an updated kernel.
    
    The architectures would now need to provide HAVE_RELIABLE_STACKTRACE to
    fully support livepatch.
    
    Signed-off-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index be5bfa533ee8..7c6631e693bc 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -82,7 +82,6 @@ static void klp_complete_transition(void)
 	struct klp_func *func;
 	struct task_struct *g, *task;
 	unsigned int cpu;
-	bool immediate_func = false;
 
 	pr_debug("'%s': completing %s transition\n",
 		 klp_transition_patch->mod->name,
@@ -104,16 +103,9 @@ static void klp_complete_transition(void)
 		klp_synchronize_transition();
 	}
 
-	if (klp_transition_patch->immediate)
-		goto done;
-
-	klp_for_each_object(klp_transition_patch, obj) {
-		klp_for_each_func(obj, func) {
+	klp_for_each_object(klp_transition_patch, obj)
+		klp_for_each_func(obj, func)
 			func->transition = false;
-			if (func->immediate)
-				immediate_func = true;
-		}
-	}
 
 	/* Prevent klp_ftrace_handler() from seeing KLP_UNDEFINED state */
 	if (klp_target_state == KLP_PATCHED)
@@ -132,7 +124,6 @@ static void klp_complete_transition(void)
 		task->patch_state = KLP_UNDEFINED;
 	}
 
-done:
 	klp_for_each_object(klp_transition_patch, obj) {
 		if (!klp_is_object_loaded(obj))
 			continue;
@@ -146,16 +137,11 @@ static void klp_complete_transition(void)
 		  klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
 
 	/*
-	 * See complementary comment in __klp_enable_patch() for why we
-	 * keep the module reference for immediate patches.
-	 *
-	 * klp_forced or immediate_func set implies unbounded increase of
-	 * module's ref count if the module is disabled/enabled in a loop.
+	 * klp_forced set implies unbounded increase of module's ref count if
+	 * the module is disabled/enabled in a loop.
 	 */
-	if (!klp_forced && !klp_transition_patch->immediate &&
-		!immediate_func && klp_target_state == KLP_UNPATCHED) {
+	if (!klp_forced && klp_target_state == KLP_UNPATCHED)
 		module_put(klp_transition_patch->mod);
-	}
 
 	klp_target_state = KLP_UNDEFINED;
 	klp_transition_patch = NULL;
@@ -223,9 +209,6 @@ static int klp_check_stack_func(struct klp_func *func,
 	struct klp_ops *ops;
 	int i;
 
-	if (func->immediate)
-		return 0;
-
 	for (i = 0; i < trace->nr_entries; i++) {
 		address = trace->entries[i];
 
@@ -387,13 +370,6 @@ void klp_try_complete_transition(void)
 
 	WARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);
 
-	/*
-	 * If the patch can be applied or reverted immediately, skip the
-	 * per-task transitions.
-	 */
-	if (klp_transition_patch->immediate)
-		goto success;
-
 	/*
 	 * Try to switch the tasks to the target patch state by walking their
 	 * stacks and looking for any to-be-patched or to-be-unpatched
@@ -437,7 +413,6 @@ void klp_try_complete_transition(void)
 		return;
 	}
 
-success:
 	/* we're done, now cleanup the data structures */
 	klp_complete_transition();
 }
@@ -457,13 +432,6 @@ void klp_start_transition(void)
 		  klp_transition_patch->mod->name,
 		  klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
 
-	/*
-	 * If the patch can be applied or reverted immediately, skip the
-	 * per-task transitions.
-	 */
-	if (klp_transition_patch->immediate)
-		return;
-
 	/*
 	 * Mark all normal tasks as needing a patch state update.  They'll
 	 * switch either in klp_try_complete_transition() or as they exit the
@@ -513,13 +481,6 @@ void klp_init_transition(struct klp_patch *patch, int state)
 	pr_debug("'%s': initializing %s transition\n", patch->mod->name,
 		 klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
 
-	/*
-	 * If the patch can be applied or reverted immediately, skip the
-	 * per-task transitions.
-	 */
-	if (patch->immediate)
-		return;
-
 	/*
 	 * Initialize all tasks to the initial patch state to prepare them for
 	 * switching to the target state.

commit c99a2be790b07752d8cc694434d3450afd4c5a00
Author: Miroslav Benes <mbenes@suse.cz>
Date:   Wed Nov 22 11:29:21 2017 +0100

    livepatch: force transition to finish
    
    If a task sleeps in a set of patched functions uninterruptedly, it could
    block the whole transition indefinitely.  Thus it may be useful to clear
    its TIF_PATCH_PENDING to allow the process to finish.
    
    Admin can do that now by writing to force sysfs attribute in livepatch
    sysfs directory. TIF_PATCH_PENDING is then cleared for all tasks and the
    transition can finish successfully.
    
    Important note! Administrator should not use this feature without a
    clearance from a patch distributor. It must be checked that by doing so
    the consistency model guarantees are not violated. Removal (rmmod) of
    patch modules is permanently disabled when the feature is used. It
    cannot be guaranteed there is no task sleeping in such module.
    
    Signed-off-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index edcfcb8ebb2d..be5bfa533ee8 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -33,6 +33,8 @@ struct klp_patch *klp_transition_patch;
 
 static int klp_target_state = KLP_UNDEFINED;
 
+static bool klp_forced = false;
+
 /*
  * This work can be performed periodically to finish patching or unpatching any
  * "straggler" tasks which failed to transition in the first attempt.
@@ -146,9 +148,12 @@ static void klp_complete_transition(void)
 	/*
 	 * See complementary comment in __klp_enable_patch() for why we
 	 * keep the module reference for immediate patches.
+	 *
+	 * klp_forced or immediate_func set implies unbounded increase of
+	 * module's ref count if the module is disabled/enabled in a loop.
 	 */
-	if (!klp_transition_patch->immediate && !immediate_func &&
-	    klp_target_state == KLP_UNPATCHED) {
+	if (!klp_forced && !klp_transition_patch->immediate &&
+		!immediate_func && klp_target_state == KLP_UNPATCHED) {
 		module_put(klp_transition_patch->mod);
 	}
 
@@ -649,3 +654,30 @@ void klp_send_signals(void)
 	}
 	read_unlock(&tasklist_lock);
 }
+
+/*
+ * Drop TIF_PATCH_PENDING of all tasks on admin's request. This forces an
+ * existing transition to finish.
+ *
+ * NOTE: klp_update_patch_state(task) requires the task to be inactive or
+ * 'current'. This is not the case here and the consistency model could be
+ * broken. Administrator, who is the only one to execute the
+ * klp_force_transitions(), has to be aware of this.
+ */
+void klp_force_transition(void)
+{
+	struct task_struct *g, *task;
+	unsigned int cpu;
+
+	pr_warn("forcing remaining tasks to the patched state\n");
+
+	read_lock(&tasklist_lock);
+	for_each_process_thread(g, task)
+		klp_update_patch_state(task);
+	read_unlock(&tasklist_lock);
+
+	for_each_possible_cpu(cpu)
+		klp_update_patch_state(idle_task(cpu));
+
+	klp_forced = true;
+}

commit 43347d56c8d9dd732cee2f8efd384ad21dd1f6c4
Author: Miroslav Benes <mbenes@suse.cz>
Date:   Wed Nov 15 14:50:13 2017 +0100

    livepatch: send a fake signal to all blocking tasks
    
    Live patching consistency model is of LEAVE_PATCHED_SET and
    SWITCH_THREAD. This means that all tasks in the system have to be marked
    one by one as safe to call a new patched function. Safe means when a
    task is not (sleeping) in a set of patched functions. That is, no
    patched function is on the task's stack. Another clearly safe place is
    the boundary between kernel and userspace. The patching waits for all
    tasks to get outside of the patched set or to cross the boundary. The
    transition is completed afterwards.
    
    The problem is that a task can block the transition for quite a long
    time, if not forever. It could sleep in a set of patched functions, for
    example.  Luckily we can force the task to leave the set by sending it a
    fake signal, that is a signal with no data in signal pending structures
    (no handler, no sign of proper signal delivered). Suspend/freezer use
    this to freeze the tasks as well. The task gets TIF_SIGPENDING set and
    is woken up (if it has been sleeping in the kernel before) or kicked by
    rescheduling IPI (if it was running on other CPU). This causes the task
    to go to kernel/userspace boundary where the signal would be handled and
    the task would be marked as safe in terms of live patching.
    
    There are tasks which are not affected by this technique though. The
    fake signal is not sent to kthreads. They should be handled differently.
    They can be woken up so they leave the patched set and their
    TIF_PATCH_PENDING can be cleared thanks to stack checking.
    
    For the sake of completeness, if the task is in TASK_RUNNING state but
    not currently running on some CPU it doesn't get the IPI, but it would
    eventually handle the signal anyway. Second, if the task runs in the
    kernel (in TASK_RUNNING state) it gets the IPI, but the signal is not
    handled on return from the interrupt. It would be handled on return to
    the userspace in the future when the fake signal is sent again. Stack
    checking deals with these cases in a better way.
    
    If the task was sleeping in a syscall it would be woken by our fake
    signal, it would check if TIF_SIGPENDING is set (by calling
    signal_pending() predicate) and return ERESTART* or EINTR. Syscalls with
    ERESTART* return values are restarted in case of the fake signal (see
    do_signal()). EINTR is propagated back to the userspace program. This
    could disturb the program, but...
    
    * each process dealing with signals should react accordingly to EINTR
      return values.
    * syscalls returning EINTR happen to be quite common situation in the
      system even if no fake signal is sent.
    * freezer sends the fake signal and does not deal with EINTR anyhow.
      Thus EINTR values are returned when the system is resumed.
    
    The very safe marking is done in architectures' "entry" on syscall and
    interrupt/exception exit paths, and in a stack checking functions of
    livepatch.  TIF_PATCH_PENDING is cleared and the next
    recalc_sigpending() drops TIF_SIGPENDING. In connection with this, also
    call klp_update_patch_state() before do_signal(), so that
    recalc_sigpending() in dequeue_signal() can clear TIF_PATCH_PENDING
    immediately and thus prevent a double call of do_signal().
    
    Note that the fake signal is not sent to stopped/traced tasks. Such task
    prevents the patching to finish till it continues again (is not traced
    anymore).
    
    Last, sending the fake signal is not automatic. It is done only when
    admin requests it by writing 1 to signal sysfs attribute in livepatch
    sysfs directory.
    
    Signed-off-by: Miroslav Benes <mbenes@suse.cz>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: x86@kernel.org
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 56add6327736..edcfcb8ebb2d 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -608,3 +608,44 @@ void klp_copy_process(struct task_struct *child)
 
 	/* TIF_PATCH_PENDING gets copied in setup_thread_stack() */
 }
+
+/*
+ * Sends a fake signal to all non-kthread tasks with TIF_PATCH_PENDING set.
+ * Kthreads with TIF_PATCH_PENDING set are woken up. Only admin can request this
+ * action currently.
+ */
+void klp_send_signals(void)
+{
+	struct task_struct *g, *task;
+
+	pr_notice("signaling remaining tasks\n");
+
+	read_lock(&tasklist_lock);
+	for_each_process_thread(g, task) {
+		if (!klp_patch_pending(task))
+			continue;
+
+		/*
+		 * There is a small race here. We could see TIF_PATCH_PENDING
+		 * set and decide to wake up a kthread or send a fake signal.
+		 * Meanwhile the task could migrate itself and the action
+		 * would be meaningless. It is not serious though.
+		 */
+		if (task->flags & PF_KTHREAD) {
+			/*
+			 * Wake up a kthread which sleeps interruptedly and
+			 * still has not been migrated.
+			 */
+			wake_up_state(task, TASK_INTERRUPTIBLE);
+		} else {
+			/*
+			 * Send fake signal to all non-kthread tasks which are
+			 * still not migrated.
+			 */
+			spin_lock_irq(&task->sighand->siglock);
+			signal_wake_up(task, 0);
+			spin_unlock_irq(&task->sighand->siglock);
+		}
+	}
+	read_unlock(&tasklist_lock);
+}

commit af026796054fb70439e919a925615e61b500ef6b
Author: Joe Lawrence <joe.lawrence@redhat.com>
Date:   Fri Oct 13 15:08:43 2017 -0400

    livepatch: add transition notices
    
    Log a few kernel debug messages at the beginning of the following livepatch
    transition functions:
    
      klp_complete_transition()
      klp_cancel_transition()
      klp_init_transition()
      klp_reverse_transition()
    
    Also update the log notice message in klp_start_transition() for similar
    verbiage as the above messages.
    
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Joe Lawrence <joe.lawrence@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 53887f0bca10..56add6327736 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -82,6 +82,10 @@ static void klp_complete_transition(void)
 	unsigned int cpu;
 	bool immediate_func = false;
 
+	pr_debug("'%s': completing %s transition\n",
+		 klp_transition_patch->mod->name,
+		 klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
+
 	if (klp_target_state == KLP_UNPATCHED) {
 		/*
 		 * All tasks have transitioned to KLP_UNPATCHED so we can now
@@ -163,6 +167,9 @@ void klp_cancel_transition(void)
 	if (WARN_ON_ONCE(klp_target_state != KLP_PATCHED))
 		return;
 
+	pr_debug("'%s': canceling patching transition, going to unpatch\n",
+		 klp_transition_patch->mod->name);
+
 	klp_target_state = KLP_UNPATCHED;
 	klp_complete_transition();
 }
@@ -441,7 +448,8 @@ void klp_start_transition(void)
 
 	WARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);
 
-	pr_notice("'%s': %s...\n", klp_transition_patch->mod->name,
+	pr_notice("'%s': starting %s transition\n",
+		  klp_transition_patch->mod->name,
 		  klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
 
 	/*
@@ -497,6 +505,9 @@ void klp_init_transition(struct klp_patch *patch, int state)
 	 */
 	klp_target_state = state;
 
+	pr_debug("'%s': initializing %s transition\n", patch->mod->name,
+		 klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
+
 	/*
 	 * If the patch can be applied or reverted immediately, skip the
 	 * per-task transitions.
@@ -562,6 +573,11 @@ void klp_reverse_transition(void)
 	unsigned int cpu;
 	struct task_struct *g, *task;
 
+	pr_debug("'%s': reversing transition from %s\n",
+		 klp_transition_patch->mod->name,
+		 klp_target_state == KLP_PATCHED ? "patching to unpatching" :
+						   "unpatching to patching");
+
 	klp_transition_patch->enabled = !klp_transition_patch->enabled;
 
 	klp_target_state = !klp_target_state;

commit 6116c3033a761611b1da980ea664c6ddff3eaed6
Author: Joe Lawrence <joe.lawrence@redhat.com>
Date:   Fri Oct 13 15:08:42 2017 -0400

    livepatch: move transition "complete" notice into klp_complete_transition()
    
    klp_complete_transition() performs a bit of housework before a
    transition to KLP_PATCHED or KLP_UNPATCHED is actually completed
    (including post-(un)patch callbacks).  To be consistent, move the
    transition "complete" kernel log notice out of
    klp_try_complete_transition() and into klp_complete_transition().
    
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Joe Lawrence <joe.lawrence@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 7bf55b7f3687..53887f0bca10 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -136,6 +136,9 @@ static void klp_complete_transition(void)
 			klp_post_unpatch_callback(obj);
 	}
 
+	pr_notice("'%s': %s complete\n", klp_transition_patch->mod->name,
+		  klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
+
 	/*
 	 * See complementary comment in __klp_enable_patch() for why we
 	 * keep the module reference for immediate patches.
@@ -423,9 +426,6 @@ void klp_try_complete_transition(void)
 	}
 
 success:
-	pr_notice("'%s': %s complete\n", klp_transition_patch->mod->name,
-		  klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
-
 	/* we're done, now cleanup the data structures */
 	klp_complete_transition();
 }

commit 93862e385ded7c60351e09fcd2a541d273650905
Author: Joe Lawrence <joe.lawrence@redhat.com>
Date:   Fri Oct 13 15:08:41 2017 -0400

    livepatch: add (un)patch callbacks
    
    Provide livepatch modules a klp_object (un)patching notification
    mechanism.  Pre and post-(un)patch callbacks allow livepatch modules to
    setup or synchronize changes that would be difficult to support in only
    patched-or-unpatched code contexts.
    
    Callbacks can be registered for target module or vmlinux klp_objects,
    but each implementation is klp_object specific.
    
      - Pre-(un)patch callbacks run before any (un)patching transition
        starts.
    
      - Post-(un)patch callbacks run once an object has been (un)patched and
        the klp_patch fully transitioned to its target state.
    
    Example use cases include modification of global data and registration
    of newly available services/handlers.
    
    See Documentation/livepatch/callbacks.txt for details and
    samples/livepatch/ for examples.
    
    Signed-off-by: Joe Lawrence <joe.lawrence@redhat.com>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index b004a1fb6032..7bf55b7f3687 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -109,9 +109,6 @@ static void klp_complete_transition(void)
 		}
 	}
 
-	if (klp_target_state == KLP_UNPATCHED && !immediate_func)
-		module_put(klp_transition_patch->mod);
-
 	/* Prevent klp_ftrace_handler() from seeing KLP_UNDEFINED state */
 	if (klp_target_state == KLP_PATCHED)
 		klp_synchronize_transition();
@@ -130,6 +127,24 @@ static void klp_complete_transition(void)
 	}
 
 done:
+	klp_for_each_object(klp_transition_patch, obj) {
+		if (!klp_is_object_loaded(obj))
+			continue;
+		if (klp_target_state == KLP_PATCHED)
+			klp_post_patch_callback(obj);
+		else if (klp_target_state == KLP_UNPATCHED)
+			klp_post_unpatch_callback(obj);
+	}
+
+	/*
+	 * See complementary comment in __klp_enable_patch() for why we
+	 * keep the module reference for immediate patches.
+	 */
+	if (!klp_transition_patch->immediate && !immediate_func &&
+	    klp_target_state == KLP_UNPATCHED) {
+		module_put(klp_transition_patch->mod);
+	}
+
 	klp_target_state = KLP_UNDEFINED;
 	klp_transition_patch = NULL;
 }

commit 842c08846420baa619fe3cb8c9af538efdb89428
Author: Petr Mladek <pmladek@suse.com>
Date:   Wed Jun 14 10:54:52 2017 +0200

    livepatch: Fix stacking of patches with respect to RCU
    
    rcu_read_(un)lock(), list_*_rcu(), and synchronize_rcu() are used for a secure
    access and manipulation of the list of patches that modify the same function.
    In particular, it is the variable func_stack that is accessible from the ftrace
    handler via struct ftrace_ops and klp_ops.
    
    Of course, it synchronizes also some states of the patch on the top of the
    stack, e.g. func->transition in klp_ftrace_handler.
    
    At the same time, this mechanism guards also the manipulation of
    task->patch_state. It is modified according to the state of the transition and
    the state of the process.
    
    Now, all this works well as long as RCU works well. Sadly livepatching might
    get into some corner cases when this is not true. For example, RCU is not
    watching when rcu_read_lock() is taken in idle threads.  It is because they
    might sleep and prevent reaching the grace period for too long.
    
    There are ways how to make RCU watching even in idle threads, see
    rcu_irq_enter(). But there is a small location inside RCU infrastructure when
    even this does not work.
    
    This small problematic location can be detected either before calling
    rcu_irq_enter() by rcu_irq_enter_disabled() or later by rcu_is_watching().
    Sadly, there is no safe way how to handle it.  Once we detect that RCU was not
    watching, we might see inconsistent state of the function stack and the related
    variables in klp_ftrace_handler(). Then we could do a wrong decision, use an
    incompatible implementation of the function and break the consistency of the
    system. We could warn but we could not avoid the damage.
    
    Fortunately, ftrace has similar problems and they seem to be solved well there.
    It uses a heavy weight implementation of some RCU operations. In particular, it
    replaces:
    
      + rcu_read_lock() with preempt_disable_notrace()
      + rcu_read_unlock() with preempt_enable_notrace()
      + synchronize_rcu() with schedule_on_each_cpu(sync_work)
    
    My understanding is that this is RCU implementation from a stone age. It meets
    the core RCU requirements but it is rather ineffective. Especially, it does not
    allow to batch or speed up the synchronize calls.
    
    On the other hand, it is very trivial. It allows to safely trace and/or
    livepatch even the RCU core infrastructure.  And the effectiveness is a not a
    big issue because using ftrace or livepatches on productive systems is a rare
    operation.  The safety is much more important than a negligible extra load.
    
    Note that the alternative implementation follows the RCU principles. Therefore,
         we could and actually must use list_*_rcu() variants when manipulating the
         func_stack.  These functions allow to access the pointers in the right
         order and with the right barriers. But they do not use any other
         information that would be set only by rcu_read_lock().
    
    Also note that there are actually two problems solved in ftrace:
    
    First, it cares about the consistency of RCU read sections.  It is being solved
    the way as described and used in this patch.
    
    Second, ftrace needs to make sure that nobody is inside the dynamic trampoline
    when it is being freed. For this, it also calls synchronize_rcu_tasks() in
    preemptive kernel in ftrace_shutdown().
    
    Livepatch has similar problem but it is solved by ftrace for free.
    klp_ftrace_handler() is a good guy and never sleeps. In addition, it is
    registered with FTRACE_OPS_FL_DYNAMIC. It causes that
    unregister_ftrace_function() calls:
    
            * schedule_on_each_cpu(ftrace_sync) - always
            * synchronize_rcu_tasks() - in preemptive kernel
    
    The effect is that nobody is neither inside the dynamic trampoline nor inside
    the ftrace handler after unregister_ftrace_function() returns.
    
    [jkosina@suse.cz: reformat changelog, fix comment]
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index adc0cc64aa4b..b004a1fb6032 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -48,6 +48,28 @@ static void klp_transition_work_fn(struct work_struct *work)
 }
 static DECLARE_DELAYED_WORK(klp_transition_work, klp_transition_work_fn);
 
+/*
+ * This function is just a stub to implement a hard force
+ * of synchronize_sched(). This requires synchronizing
+ * tasks even in userspace and idle.
+ */
+static void klp_sync(struct work_struct *work)
+{
+}
+
+/*
+ * We allow to patch also functions where RCU is not watching,
+ * e.g. before user_exit(). We can not rely on the RCU infrastructure
+ * to do the synchronization. Instead hard force the sched synchronization.
+ *
+ * This approach allows to use RCU functions for manipulating func_stack
+ * safely.
+ */
+static void klp_synchronize_transition(void)
+{
+	schedule_on_each_cpu(klp_sync);
+}
+
 /*
  * The transition to the target patch state is complete.  Clean up the data
  * structures.
@@ -73,7 +95,7 @@ static void klp_complete_transition(void)
 		 * func->transition gets cleared, the handler may choose a
 		 * removed function.
 		 */
-		synchronize_rcu();
+		klp_synchronize_transition();
 	}
 
 	if (klp_transition_patch->immediate)
@@ -92,7 +114,7 @@ static void klp_complete_transition(void)
 
 	/* Prevent klp_ftrace_handler() from seeing KLP_UNDEFINED state */
 	if (klp_target_state == KLP_PATCHED)
-		synchronize_rcu();
+		klp_synchronize_transition();
 
 	read_lock(&tasklist_lock);
 	for_each_process_thread(g, task) {
@@ -136,7 +158,11 @@ void klp_cancel_transition(void)
  */
 void klp_update_patch_state(struct task_struct *task)
 {
-	rcu_read_lock();
+	/*
+	 * A variant of synchronize_sched() is used to allow patching functions
+	 * where RCU is not watching, see klp_synchronize_transition().
+	 */
+	preempt_disable_notrace();
 
 	/*
 	 * This test_and_clear_tsk_thread_flag() call also serves as a read
@@ -153,7 +179,7 @@ void klp_update_patch_state(struct task_struct *task)
 	if (test_and_clear_tsk_thread_flag(task, TIF_PATCH_PENDING))
 		task->patch_state = READ_ONCE(klp_target_state);
 
-	rcu_read_unlock();
+	preempt_enable_notrace();
 }
 
 /*
@@ -539,7 +565,7 @@ void klp_reverse_transition(void)
 		clear_tsk_thread_flag(idle_task(cpu), TIF_PATCH_PENDING);
 
 	/* Let any remaining calls to klp_update_patch_state() complete */
-	synchronize_rcu();
+	klp_synchronize_transition();
 
 	klp_start_transition();
 }

commit e679af627fe875a51d40b9a2b17f08fbde36e0e2
Author: Petr Mladek <pmladek@suse.com>
Date:   Tue Apr 11 13:07:48 2017 +0200

    livepatch: Cancel transition a safe way for immediate patches
    
    klp_init_transition() does not set func->transition for immediate patches.
    Then klp_ftrace_handler() could use the new code immediately. As a result,
    it is not safe to put the livepatch module in klp_cancel_transition().
    
    This patch reverts most of the last minute changes klp_cancel_transition().
    It keeps the warning about a misuse because it still makes sense.
    
    Fixes: 3ec24776bfd0 ("livepatch: allow removal of a disabled patch")
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 2de09e0c4e5c..adc0cc64aa4b 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -120,31 +120,11 @@ static void klp_complete_transition(void)
  */
 void klp_cancel_transition(void)
 {
-	struct klp_patch *patch = klp_transition_patch;
-	struct klp_object *obj;
-	struct klp_func *func;
-	bool immediate_func = false;
-
 	if (WARN_ON_ONCE(klp_target_state != KLP_PATCHED))
 		return;
 
 	klp_target_state = KLP_UNPATCHED;
 	klp_complete_transition();
-
-	/*
-	 * In the enable error path, even immediate patches can be safely
-	 * removed because the transition hasn't been started yet.
-	 *
-	 * klp_complete_transition() doesn't have a module_put() for immediate
-	 * patches, so do it here.
-	 */
-	klp_for_each_object(patch, obj)
-		klp_for_each_func(obj, func)
-			if (func->immediate)
-				immediate_func = true;
-
-	if (patch->immediate || immediate_func)
-		module_put(patch->mod);
 }
 
 /*

commit 10517429b5ac242498d7d847f79f10c21d7eedb0
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Wed Mar 8 14:27:05 2017 +0100

    livepatch: make klp_mutex proper part of API
    
    klp_mutex is shared between core.c and transition.c, and as such would
    rather be properly located in a header so that we don't have to play
    'extern' games from .c sources.
    
    This also silences sparse warning (wrongly) suggesting that klp_mutex
    should be defined static.
    
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 0ab7abd53b0b..2de09e0c4e5c 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -21,6 +21,7 @@
 
 #include <linux/cpu.h>
 #include <linux/stacktrace.h>
+#include "core.h"
 #include "patch.h"
 #include "transition.h"
 #include "../sched/sched.h"
@@ -28,8 +29,6 @@
 #define MAX_STACK_ENTRIES  100
 #define STACK_ERR_BUF_SIZE 128
 
-extern struct mutex klp_mutex;
-
 struct klp_patch *klp_transition_patch;
 
 static int klp_target_state = KLP_UNDEFINED;

commit 3ec24776bfd09668079df7dca0c0136d80820ab4
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Mar 6 11:20:29 2017 -0600

    livepatch: allow removal of a disabled patch
    
    Currently we do not allow patch module to unload since there is no
    method to determine if a task is still running in the patched code.
    
    The consistency model gives us the way because when the unpatching
    finishes we know that all tasks were marked as safe to call an original
    function. Thus every new call to the function calls the original code
    and at the same time no task can be somewhere in the patched code,
    because it had to leave that code to be marked as safe.
    
    We can safely let the patch module go after that.
    
    Completion is used for synchronization between module removal and sysfs
    infrastructure in a similar way to commit 942e443127e9 ("module: Fix
    mod->mkobj.kobj potentially freed too early").
    
    Note that we still do not allow the removal for immediate model, that is
    no consistency model. The module refcount may increase in this case if
    somebody disables and enables the patch several times. This should not
    cause any harm.
    
    With this change a call to try_module_get() is moved to
    __klp_enable_patch from klp_register_patch to make module reference
    counting symmetric (module_put() is in a patch disable path) and to
    allow to take a new reference to a disabled module when being enabled.
    
    Finally, we need to be very careful about possible races between
    klp_unregister_patch(), kobject_put() functions and operations
    on the related sysfs files.
    
    kobject_put(&patch->kobj) must be called without klp_mutex. Otherwise,
    it might be blocked by enabled_store() that needs the mutex as well.
    In addition, enabled_store() must check if the patch was not
    unregisted in the meantime.
    
    There is no need to do the same for other kobject_put() callsites
    at the moment. Their sysfs operations neither take the lock nor
    they access any data that might be freed in the meantime.
    
    There was an attempt to use kobjects the right way and prevent these
    races by design. But it made the patch definition more complicated
    and opened another can of worms. See
    https://lkml.kernel.org/r/1464018848-4303-1-git-send-email-pmladek@suse.com
    
    [Thanks to Petr Mladek for improving the commit message.]
    
    Signed-off-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Petr Mladek <pmladek@suse.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 428533ec51b5..0ab7abd53b0b 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -59,6 +59,7 @@ static void klp_complete_transition(void)
 	struct klp_func *func;
 	struct task_struct *g, *task;
 	unsigned int cpu;
+	bool immediate_func = false;
 
 	if (klp_target_state == KLP_UNPATCHED) {
 		/*
@@ -79,9 +80,16 @@ static void klp_complete_transition(void)
 	if (klp_transition_patch->immediate)
 		goto done;
 
-	klp_for_each_object(klp_transition_patch, obj)
-		klp_for_each_func(obj, func)
+	klp_for_each_object(klp_transition_patch, obj) {
+		klp_for_each_func(obj, func) {
 			func->transition = false;
+			if (func->immediate)
+				immediate_func = true;
+		}
+	}
+
+	if (klp_target_state == KLP_UNPATCHED && !immediate_func)
+		module_put(klp_transition_patch->mod);
 
 	/* Prevent klp_ftrace_handler() from seeing KLP_UNDEFINED state */
 	if (klp_target_state == KLP_PATCHED)
@@ -113,8 +121,31 @@ static void klp_complete_transition(void)
  */
 void klp_cancel_transition(void)
 {
-	klp_target_state = !klp_target_state;
+	struct klp_patch *patch = klp_transition_patch;
+	struct klp_object *obj;
+	struct klp_func *func;
+	bool immediate_func = false;
+
+	if (WARN_ON_ONCE(klp_target_state != KLP_PATCHED))
+		return;
+
+	klp_target_state = KLP_UNPATCHED;
 	klp_complete_transition();
+
+	/*
+	 * In the enable error path, even immediate patches can be safely
+	 * removed because the transition hasn't been started yet.
+	 *
+	 * klp_complete_transition() doesn't have a module_put() for immediate
+	 * patches, so do it here.
+	 */
+	klp_for_each_object(patch, obj)
+		klp_for_each_func(obj, func)
+			if (func->immediate)
+				immediate_func = true;
+
+	if (patch->immediate || immediate_func)
+		module_put(patch->mod);
 }
 
 /*

commit d83a7cb375eec21f04c83542395d08b2f6641da2
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Feb 13 19:42:40 2017 -0600

    livepatch: change to a per-task consistency model
    
    Change livepatch to use a basic per-task consistency model.  This is the
    foundation which will eventually enable us to patch those ~10% of
    security patches which change function or data semantics.  This is the
    biggest remaining piece needed to make livepatch more generally useful.
    
    This code stems from the design proposal made by Vojtech [1] in November
    2014.  It's a hybrid of kGraft and kpatch: it uses kGraft's per-task
    consistency and syscall barrier switching combined with kpatch's stack
    trace switching.  There are also a number of fallback options which make
    it quite flexible.
    
    Patches are applied on a per-task basis, when the task is deemed safe to
    switch over.  When a patch is enabled, livepatch enters into a
    transition state where tasks are converging to the patched state.
    Usually this transition state can complete in a few seconds.  The same
    sequence occurs when a patch is disabled, except the tasks converge from
    the patched state to the unpatched state.
    
    An interrupt handler inherits the patched state of the task it
    interrupts.  The same is true for forked tasks: the child inherits the
    patched state of the parent.
    
    Livepatch uses several complementary approaches to determine when it's
    safe to patch tasks:
    
    1. The first and most effective approach is stack checking of sleeping
       tasks.  If no affected functions are on the stack of a given task,
       the task is patched.  In most cases this will patch most or all of
       the tasks on the first try.  Otherwise it'll keep trying
       periodically.  This option is only available if the architecture has
       reliable stacks (HAVE_RELIABLE_STACKTRACE).
    
    2. The second approach, if needed, is kernel exit switching.  A
       task is switched when it returns to user space from a system call, a
       user space IRQ, or a signal.  It's useful in the following cases:
    
       a) Patching I/O-bound user tasks which are sleeping on an affected
          function.  In this case you have to send SIGSTOP and SIGCONT to
          force it to exit the kernel and be patched.
       b) Patching CPU-bound user tasks.  If the task is highly CPU-bound
          then it will get patched the next time it gets interrupted by an
          IRQ.
       c) In the future it could be useful for applying patches for
          architectures which don't yet have HAVE_RELIABLE_STACKTRACE.  In
          this case you would have to signal most of the tasks on the
          system.  However this isn't supported yet because there's
          currently no way to patch kthreads without
          HAVE_RELIABLE_STACKTRACE.
    
    3. For idle "swapper" tasks, since they don't ever exit the kernel, they
       instead have a klp_update_patch_state() call in the idle loop which
       allows them to be patched before the CPU enters the idle state.
    
       (Note there's not yet such an approach for kthreads.)
    
    All the above approaches may be skipped by setting the 'immediate' flag
    in the 'klp_patch' struct, which will disable per-task consistency and
    patch all tasks immediately.  This can be useful if the patch doesn't
    change any function or data semantics.  Note that, even with this flag
    set, it's possible that some tasks may still be running with an old
    version of the function, until that function returns.
    
    There's also an 'immediate' flag in the 'klp_func' struct which allows
    you to specify that certain functions in the patch can be applied
    without per-task consistency.  This might be useful if you want to patch
    a common function like schedule(), and the function change doesn't need
    consistency but the rest of the patch does.
    
    For architectures which don't have HAVE_RELIABLE_STACKTRACE, the user
    must set patch->immediate which causes all tasks to be patched
    immediately.  This option should be used with care, only when the patch
    doesn't change any function or data semantics.
    
    In the future, architectures which don't have HAVE_RELIABLE_STACKTRACE
    may be allowed to use per-task consistency if we can come up with
    another way to patch kthreads.
    
    The /sys/kernel/livepatch/<patch>/transition file shows whether a patch
    is in transition.  Only a single patch (the topmost patch on the stack)
    can be in transition at a given time.  A patch can remain in transition
    indefinitely, if any of the tasks are stuck in the initial patch state.
    
    A transition can be reversed and effectively canceled by writing the
    opposite value to the /sys/kernel/livepatch/<patch>/enabled file while
    the transition is in progress.  Then all the tasks will attempt to
    converge back to the original patch state.
    
    [1] https://lkml.kernel.org/r/20141107140458.GA21774@suse.cz
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Ingo Molnar <mingo@kernel.org>        # for the scheduler changes
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
new file mode 100644
index 000000000000..428533ec51b5
--- /dev/null
+++ b/kernel/livepatch/transition.c
@@ -0,0 +1,543 @@
+/*
+ * transition.c - Kernel Live Patching transition functions
+ *
+ * Copyright (C) 2015-2016 Josh Poimboeuf <jpoimboe@redhat.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, see <http://www.gnu.org/licenses/>.
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/cpu.h>
+#include <linux/stacktrace.h>
+#include "patch.h"
+#include "transition.h"
+#include "../sched/sched.h"
+
+#define MAX_STACK_ENTRIES  100
+#define STACK_ERR_BUF_SIZE 128
+
+extern struct mutex klp_mutex;
+
+struct klp_patch *klp_transition_patch;
+
+static int klp_target_state = KLP_UNDEFINED;
+
+/*
+ * This work can be performed periodically to finish patching or unpatching any
+ * "straggler" tasks which failed to transition in the first attempt.
+ */
+static void klp_transition_work_fn(struct work_struct *work)
+{
+	mutex_lock(&klp_mutex);
+
+	if (klp_transition_patch)
+		klp_try_complete_transition();
+
+	mutex_unlock(&klp_mutex);
+}
+static DECLARE_DELAYED_WORK(klp_transition_work, klp_transition_work_fn);
+
+/*
+ * The transition to the target patch state is complete.  Clean up the data
+ * structures.
+ */
+static void klp_complete_transition(void)
+{
+	struct klp_object *obj;
+	struct klp_func *func;
+	struct task_struct *g, *task;
+	unsigned int cpu;
+
+	if (klp_target_state == KLP_UNPATCHED) {
+		/*
+		 * All tasks have transitioned to KLP_UNPATCHED so we can now
+		 * remove the new functions from the func_stack.
+		 */
+		klp_unpatch_objects(klp_transition_patch);
+
+		/*
+		 * Make sure klp_ftrace_handler() can no longer see functions
+		 * from this patch on the ops->func_stack.  Otherwise, after
+		 * func->transition gets cleared, the handler may choose a
+		 * removed function.
+		 */
+		synchronize_rcu();
+	}
+
+	if (klp_transition_patch->immediate)
+		goto done;
+
+	klp_for_each_object(klp_transition_patch, obj)
+		klp_for_each_func(obj, func)
+			func->transition = false;
+
+	/* Prevent klp_ftrace_handler() from seeing KLP_UNDEFINED state */
+	if (klp_target_state == KLP_PATCHED)
+		synchronize_rcu();
+
+	read_lock(&tasklist_lock);
+	for_each_process_thread(g, task) {
+		WARN_ON_ONCE(test_tsk_thread_flag(task, TIF_PATCH_PENDING));
+		task->patch_state = KLP_UNDEFINED;
+	}
+	read_unlock(&tasklist_lock);
+
+	for_each_possible_cpu(cpu) {
+		task = idle_task(cpu);
+		WARN_ON_ONCE(test_tsk_thread_flag(task, TIF_PATCH_PENDING));
+		task->patch_state = KLP_UNDEFINED;
+	}
+
+done:
+	klp_target_state = KLP_UNDEFINED;
+	klp_transition_patch = NULL;
+}
+
+/*
+ * This is called in the error path, to cancel a transition before it has
+ * started, i.e. klp_init_transition() has been called but
+ * klp_start_transition() hasn't.  If the transition *has* been started,
+ * klp_reverse_transition() should be used instead.
+ */
+void klp_cancel_transition(void)
+{
+	klp_target_state = !klp_target_state;
+	klp_complete_transition();
+}
+
+/*
+ * Switch the patched state of the task to the set of functions in the target
+ * patch state.
+ *
+ * NOTE: If task is not 'current', the caller must ensure the task is inactive.
+ * Otherwise klp_ftrace_handler() might read the wrong 'patch_state' value.
+ */
+void klp_update_patch_state(struct task_struct *task)
+{
+	rcu_read_lock();
+
+	/*
+	 * This test_and_clear_tsk_thread_flag() call also serves as a read
+	 * barrier (smp_rmb) for two cases:
+	 *
+	 * 1) Enforce the order of the TIF_PATCH_PENDING read and the
+	 *    klp_target_state read.  The corresponding write barrier is in
+	 *    klp_init_transition().
+	 *
+	 * 2) Enforce the order of the TIF_PATCH_PENDING read and a future read
+	 *    of func->transition, if klp_ftrace_handler() is called later on
+	 *    the same CPU.  See __klp_disable_patch().
+	 */
+	if (test_and_clear_tsk_thread_flag(task, TIF_PATCH_PENDING))
+		task->patch_state = READ_ONCE(klp_target_state);
+
+	rcu_read_unlock();
+}
+
+/*
+ * Determine whether the given stack trace includes any references to a
+ * to-be-patched or to-be-unpatched function.
+ */
+static int klp_check_stack_func(struct klp_func *func,
+				struct stack_trace *trace)
+{
+	unsigned long func_addr, func_size, address;
+	struct klp_ops *ops;
+	int i;
+
+	if (func->immediate)
+		return 0;
+
+	for (i = 0; i < trace->nr_entries; i++) {
+		address = trace->entries[i];
+
+		if (klp_target_state == KLP_UNPATCHED) {
+			 /*
+			  * Check for the to-be-unpatched function
+			  * (the func itself).
+			  */
+			func_addr = (unsigned long)func->new_func;
+			func_size = func->new_size;
+		} else {
+			/*
+			 * Check for the to-be-patched function
+			 * (the previous func).
+			 */
+			ops = klp_find_ops(func->old_addr);
+
+			if (list_is_singular(&ops->func_stack)) {
+				/* original function */
+				func_addr = func->old_addr;
+				func_size = func->old_size;
+			} else {
+				/* previously patched function */
+				struct klp_func *prev;
+
+				prev = list_next_entry(func, stack_node);
+				func_addr = (unsigned long)prev->new_func;
+				func_size = prev->new_size;
+			}
+		}
+
+		if (address >= func_addr && address < func_addr + func_size)
+			return -EAGAIN;
+	}
+
+	return 0;
+}
+
+/*
+ * Determine whether it's safe to transition the task to the target patch state
+ * by looking for any to-be-patched or to-be-unpatched functions on its stack.
+ */
+static int klp_check_stack(struct task_struct *task, char *err_buf)
+{
+	static unsigned long entries[MAX_STACK_ENTRIES];
+	struct stack_trace trace;
+	struct klp_object *obj;
+	struct klp_func *func;
+	int ret;
+
+	trace.skip = 0;
+	trace.nr_entries = 0;
+	trace.max_entries = MAX_STACK_ENTRIES;
+	trace.entries = entries;
+	ret = save_stack_trace_tsk_reliable(task, &trace);
+	WARN_ON_ONCE(ret == -ENOSYS);
+	if (ret) {
+		snprintf(err_buf, STACK_ERR_BUF_SIZE,
+			 "%s: %s:%d has an unreliable stack\n",
+			 __func__, task->comm, task->pid);
+		return ret;
+	}
+
+	klp_for_each_object(klp_transition_patch, obj) {
+		if (!obj->patched)
+			continue;
+		klp_for_each_func(obj, func) {
+			ret = klp_check_stack_func(func, &trace);
+			if (ret) {
+				snprintf(err_buf, STACK_ERR_BUF_SIZE,
+					 "%s: %s:%d is sleeping on function %s\n",
+					 __func__, task->comm, task->pid,
+					 func->old_name);
+				return ret;
+			}
+		}
+	}
+
+	return 0;
+}
+
+/*
+ * Try to safely switch a task to the target patch state.  If it's currently
+ * running, or it's sleeping on a to-be-patched or to-be-unpatched function, or
+ * if the stack is unreliable, return false.
+ */
+static bool klp_try_switch_task(struct task_struct *task)
+{
+	struct rq *rq;
+	struct rq_flags flags;
+	int ret;
+	bool success = false;
+	char err_buf[STACK_ERR_BUF_SIZE];
+
+	err_buf[0] = '\0';
+
+	/* check if this task has already switched over */
+	if (task->patch_state == klp_target_state)
+		return true;
+
+	/*
+	 * For arches which don't have reliable stack traces, we have to rely
+	 * on other methods (e.g., switching tasks at kernel exit).
+	 */
+	if (!klp_have_reliable_stack())
+		return false;
+
+	/*
+	 * Now try to check the stack for any to-be-patched or to-be-unpatched
+	 * functions.  If all goes well, switch the task to the target patch
+	 * state.
+	 */
+	rq = task_rq_lock(task, &flags);
+
+	if (task_running(rq, task) && task != current) {
+		snprintf(err_buf, STACK_ERR_BUF_SIZE,
+			 "%s: %s:%d is running\n", __func__, task->comm,
+			 task->pid);
+		goto done;
+	}
+
+	ret = klp_check_stack(task, err_buf);
+	if (ret)
+		goto done;
+
+	success = true;
+
+	clear_tsk_thread_flag(task, TIF_PATCH_PENDING);
+	task->patch_state = klp_target_state;
+
+done:
+	task_rq_unlock(rq, task, &flags);
+
+	/*
+	 * Due to console deadlock issues, pr_debug() can't be used while
+	 * holding the task rq lock.  Instead we have to use a temporary buffer
+	 * and print the debug message after releasing the lock.
+	 */
+	if (err_buf[0] != '\0')
+		pr_debug("%s", err_buf);
+
+	return success;
+
+}
+
+/*
+ * Try to switch all remaining tasks to the target patch state by walking the
+ * stacks of sleeping tasks and looking for any to-be-patched or
+ * to-be-unpatched functions.  If such functions are found, the task can't be
+ * switched yet.
+ *
+ * If any tasks are still stuck in the initial patch state, schedule a retry.
+ */
+void klp_try_complete_transition(void)
+{
+	unsigned int cpu;
+	struct task_struct *g, *task;
+	bool complete = true;
+
+	WARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);
+
+	/*
+	 * If the patch can be applied or reverted immediately, skip the
+	 * per-task transitions.
+	 */
+	if (klp_transition_patch->immediate)
+		goto success;
+
+	/*
+	 * Try to switch the tasks to the target patch state by walking their
+	 * stacks and looking for any to-be-patched or to-be-unpatched
+	 * functions.  If such functions are found on a stack, or if the stack
+	 * is deemed unreliable, the task can't be switched yet.
+	 *
+	 * Usually this will transition most (or all) of the tasks on a system
+	 * unless the patch includes changes to a very common function.
+	 */
+	read_lock(&tasklist_lock);
+	for_each_process_thread(g, task)
+		if (!klp_try_switch_task(task))
+			complete = false;
+	read_unlock(&tasklist_lock);
+
+	/*
+	 * Ditto for the idle "swapper" tasks.
+	 */
+	get_online_cpus();
+	for_each_possible_cpu(cpu) {
+		task = idle_task(cpu);
+		if (cpu_online(cpu)) {
+			if (!klp_try_switch_task(task))
+				complete = false;
+		} else if (task->patch_state != klp_target_state) {
+			/* offline idle tasks can be switched immediately */
+			clear_tsk_thread_flag(task, TIF_PATCH_PENDING);
+			task->patch_state = klp_target_state;
+		}
+	}
+	put_online_cpus();
+
+	if (!complete) {
+		/*
+		 * Some tasks weren't able to be switched over.  Try again
+		 * later and/or wait for other methods like kernel exit
+		 * switching.
+		 */
+		schedule_delayed_work(&klp_transition_work,
+				      round_jiffies_relative(HZ));
+		return;
+	}
+
+success:
+	pr_notice("'%s': %s complete\n", klp_transition_patch->mod->name,
+		  klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
+
+	/* we're done, now cleanup the data structures */
+	klp_complete_transition();
+}
+
+/*
+ * Start the transition to the specified target patch state so tasks can begin
+ * switching to it.
+ */
+void klp_start_transition(void)
+{
+	struct task_struct *g, *task;
+	unsigned int cpu;
+
+	WARN_ON_ONCE(klp_target_state == KLP_UNDEFINED);
+
+	pr_notice("'%s': %s...\n", klp_transition_patch->mod->name,
+		  klp_target_state == KLP_PATCHED ? "patching" : "unpatching");
+
+	/*
+	 * If the patch can be applied or reverted immediately, skip the
+	 * per-task transitions.
+	 */
+	if (klp_transition_patch->immediate)
+		return;
+
+	/*
+	 * Mark all normal tasks as needing a patch state update.  They'll
+	 * switch either in klp_try_complete_transition() or as they exit the
+	 * kernel.
+	 */
+	read_lock(&tasklist_lock);
+	for_each_process_thread(g, task)
+		if (task->patch_state != klp_target_state)
+			set_tsk_thread_flag(task, TIF_PATCH_PENDING);
+	read_unlock(&tasklist_lock);
+
+	/*
+	 * Mark all idle tasks as needing a patch state update.  They'll switch
+	 * either in klp_try_complete_transition() or at the idle loop switch
+	 * point.
+	 */
+	for_each_possible_cpu(cpu) {
+		task = idle_task(cpu);
+		if (task->patch_state != klp_target_state)
+			set_tsk_thread_flag(task, TIF_PATCH_PENDING);
+	}
+}
+
+/*
+ * Initialize the global target patch state and all tasks to the initial patch
+ * state, and initialize all function transition states to true in preparation
+ * for patching or unpatching.
+ */
+void klp_init_transition(struct klp_patch *patch, int state)
+{
+	struct task_struct *g, *task;
+	unsigned int cpu;
+	struct klp_object *obj;
+	struct klp_func *func;
+	int initial_state = !state;
+
+	WARN_ON_ONCE(klp_target_state != KLP_UNDEFINED);
+
+	klp_transition_patch = patch;
+
+	/*
+	 * Set the global target patch state which tasks will switch to.  This
+	 * has no effect until the TIF_PATCH_PENDING flags get set later.
+	 */
+	klp_target_state = state;
+
+	/*
+	 * If the patch can be applied or reverted immediately, skip the
+	 * per-task transitions.
+	 */
+	if (patch->immediate)
+		return;
+
+	/*
+	 * Initialize all tasks to the initial patch state to prepare them for
+	 * switching to the target state.
+	 */
+	read_lock(&tasklist_lock);
+	for_each_process_thread(g, task) {
+		WARN_ON_ONCE(task->patch_state != KLP_UNDEFINED);
+		task->patch_state = initial_state;
+	}
+	read_unlock(&tasklist_lock);
+
+	/*
+	 * Ditto for the idle "swapper" tasks.
+	 */
+	for_each_possible_cpu(cpu) {
+		task = idle_task(cpu);
+		WARN_ON_ONCE(task->patch_state != KLP_UNDEFINED);
+		task->patch_state = initial_state;
+	}
+
+	/*
+	 * Enforce the order of the task->patch_state initializations and the
+	 * func->transition updates to ensure that klp_ftrace_handler() doesn't
+	 * see a func in transition with a task->patch_state of KLP_UNDEFINED.
+	 *
+	 * Also enforce the order of the klp_target_state write and future
+	 * TIF_PATCH_PENDING writes to ensure klp_update_patch_state() doesn't
+	 * set a task->patch_state to KLP_UNDEFINED.
+	 */
+	smp_wmb();
+
+	/*
+	 * Set the func transition states so klp_ftrace_handler() will know to
+	 * switch to the transition logic.
+	 *
+	 * When patching, the funcs aren't yet in the func_stack and will be
+	 * made visible to the ftrace handler shortly by the calls to
+	 * klp_patch_object().
+	 *
+	 * When unpatching, the funcs are already in the func_stack and so are
+	 * already visible to the ftrace handler.
+	 */
+	klp_for_each_object(patch, obj)
+		klp_for_each_func(obj, func)
+			func->transition = true;
+}
+
+/*
+ * This function can be called in the middle of an existing transition to
+ * reverse the direction of the target patch state.  This can be done to
+ * effectively cancel an existing enable or disable operation if there are any
+ * tasks which are stuck in the initial patch state.
+ */
+void klp_reverse_transition(void)
+{
+	unsigned int cpu;
+	struct task_struct *g, *task;
+
+	klp_transition_patch->enabled = !klp_transition_patch->enabled;
+
+	klp_target_state = !klp_target_state;
+
+	/*
+	 * Clear all TIF_PATCH_PENDING flags to prevent races caused by
+	 * klp_update_patch_state() running in parallel with
+	 * klp_start_transition().
+	 */
+	read_lock(&tasklist_lock);
+	for_each_process_thread(g, task)
+		clear_tsk_thread_flag(task, TIF_PATCH_PENDING);
+	read_unlock(&tasklist_lock);
+
+	for_each_possible_cpu(cpu)
+		clear_tsk_thread_flag(idle_task(cpu), TIF_PATCH_PENDING);
+
+	/* Let any remaining calls to klp_update_patch_state() complete */
+	synchronize_rcu();
+
+	klp_start_transition();
+}
+
+/* Called from copy_process() during fork */
+void klp_copy_process(struct task_struct *child)
+{
+	child->patch_state = current->patch_state;
+
+	/* TIF_PATCH_PENDING gets copied in setup_thread_stack() */
+}
