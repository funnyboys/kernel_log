commit 97a32539b9568bb653683349e5a76d02ff3c3e2c
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Feb 3 17:37:17 2020 -0800

    proc: convert everything to "struct proc_ops"
    
    The most notable change is DEFINE_SHOW_ATTRIBUTE macro split in
    seq_file.h.
    
    Conversion rule is:
    
            llseek          => proc_lseek
            unlocked_ioctl  => proc_ioctl
    
            xxx             => proc_xxx
    
            delete ".owner = THIS_MODULE" line
    
    [akpm@linux-foundation.org: fix drivers/isdn/capi/kcapi_proc.c]
    [sfr@canb.auug.org.au: fix kernel/sched/psi.c]
      Link: http://lkml.kernel.org/r/20200122180545.36222f50@canb.auug.org.au
    Link: http://lkml.kernel.org/r/20191225172546.GB13378@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 4b144b02ca5d..6f69a4195d56 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -442,18 +442,18 @@ static ssize_t prof_cpu_mask_proc_write(struct file *file,
 	return err;
 }
 
-static const struct file_operations prof_cpu_mask_proc_fops = {
-	.open		= prof_cpu_mask_proc_open,
-	.read		= seq_read,
-	.llseek		= seq_lseek,
-	.release	= single_release,
-	.write		= prof_cpu_mask_proc_write,
+static const struct proc_ops prof_cpu_mask_proc_ops = {
+	.proc_open	= prof_cpu_mask_proc_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= single_release,
+	.proc_write	= prof_cpu_mask_proc_write,
 };
 
 void create_prof_cpu_mask(void)
 {
 	/* create /proc/irq/prof_cpu_mask */
-	proc_create("irq/prof_cpu_mask", 0600, NULL, &prof_cpu_mask_proc_fops);
+	proc_create("irq/prof_cpu_mask", 0600, NULL, &prof_cpu_mask_proc_ops);
 }
 
 /*
@@ -517,10 +517,10 @@ static ssize_t write_profile(struct file *file, const char __user *buf,
 	return count;
 }
 
-static const struct file_operations proc_profile_operations = {
-	.read		= read_profile,
-	.write		= write_profile,
-	.llseek		= default_llseek,
+static const struct proc_ops profile_proc_ops = {
+	.proc_read	= read_profile,
+	.proc_write	= write_profile,
+	.proc_lseek	= default_llseek,
 };
 
 int __ref create_proc_profile(void)
@@ -548,7 +548,7 @@ int __ref create_proc_profile(void)
 	err = 0;
 #endif
 	entry = proc_create("profile", S_IWUSR | S_IRUGO,
-			    NULL, &proc_profile_operations);
+			    NULL, &profile_proc_ops);
 	if (!entry)
 		goto err_state_onl;
 	proc_set_size(entry, (1 + prof_len) * sizeof(atomic_t));

commit ef70eff9dea66f38f8c2c2dcc7fe4b7a2bbb4921
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Wed Dec 4 16:50:50 2019 -0800

    kernel/profile.c: use cpumask_available to check for NULL cpumask
    
    When building with clang + -Wtautological-pointer-compare, these
    instances pop up:
    
      kernel/profile.c:339:6: warning: comparison of array 'prof_cpu_mask' not equal to a null pointer is always true [-Wtautological-pointer-compare]
              if (prof_cpu_mask != NULL)
                  ^~~~~~~~~~~~~    ~~~~
      kernel/profile.c:376:6: warning: comparison of array 'prof_cpu_mask' not equal to a null pointer is always true [-Wtautological-pointer-compare]
              if (prof_cpu_mask != NULL)
                  ^~~~~~~~~~~~~    ~~~~
      kernel/profile.c:406:26: warning: comparison of array 'prof_cpu_mask' not equal to a null pointer is always true [-Wtautological-pointer-compare]
              if (!user_mode(regs) && prof_cpu_mask != NULL &&
                                    ^~~~~~~~~~~~~    ~~~~
      3 warnings generated.
    
    This can be addressed with the cpumask_available helper, introduced in
    commit f7e30f01a9e2 ("cpumask: Add helper cpumask_available()") to fix
    warnings like this while keeping the code the same.
    
    Link: https://github.com/ClangBuiltLinux/linux/issues/747
    Link: http://lkml.kernel.org/r/20191022191957.9554-1-natechancellor@gmail.com
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index af7c94bf5fa1..4b144b02ca5d 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -336,7 +336,7 @@ static int profile_dead_cpu(unsigned int cpu)
 	struct page *page;
 	int i;
 
-	if (prof_cpu_mask != NULL)
+	if (cpumask_available(prof_cpu_mask))
 		cpumask_clear_cpu(cpu, prof_cpu_mask);
 
 	for (i = 0; i < 2; i++) {
@@ -373,7 +373,7 @@ static int profile_prepare_cpu(unsigned int cpu)
 
 static int profile_online_cpu(unsigned int cpu)
 {
-	if (prof_cpu_mask != NULL)
+	if (cpumask_available(prof_cpu_mask))
 		cpumask_set_cpu(cpu, prof_cpu_mask);
 
 	return 0;
@@ -403,7 +403,7 @@ void profile_tick(int type)
 {
 	struct pt_regs *regs = get_irq_regs();
 
-	if (!user_mode(regs) && prof_cpu_mask != NULL &&
+	if (!user_mode(regs) && cpumask_available(prof_cpu_mask) &&
 	    cpumask_test_cpu(smp_processor_id(), prof_cpu_mask))
 		profile_hit(type, (void *)profile_pc(regs));
 }

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 9c08a2c7cb1d..af7c94bf5fa1 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/kernel/profile.c
  *  Simple profiling. Manages a direct-mapped profile hit count buffer,

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 9aa2a4445b0d..9c08a2c7cb1d 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -16,7 +16,7 @@
 
 #include <linux/export.h>
 #include <linux/profile.h>
-#include <linux/bootmem.h>
+#include <linux/memblock.h>
 #include <linux/notifier.h>
 #include <linux/mm.h>
 #include <linux/cpumask.h>

commit 3905f9ad455e0fd2ddb557566c5561b4a3027c07
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 12:07:04 2017 +0100

    sched/headers: Prepare to move sched_info_on() and force_schedstat_enabled() from <linux/sched.h> to <linux/sched/stat.h>
    
    But first update usage sites with the new header dependency.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index f67ce0aa6bc4..9aa2a4445b0d 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -25,6 +25,8 @@
 #include <linux/mutex.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
+#include <linux/sched/stat.h>
+
 #include <asm/sections.h>
 #include <asm/irq_regs.h>
 #include <asm/ptrace.h>

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 2dbccf2d806c..f67ce0aa6bc4 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -408,7 +408,7 @@ void profile_tick(int type)
 #ifdef CONFIG_PROC_FS
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 
 static int prof_cpu_mask_proc_show(struct seq_file *m, void *v)
 {

commit e722d8daafb974b9ad1bbaf42f384a5ea5929f5f
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Wed Jul 13 17:16:59 2016 +0000

    profile: Convert to hotplug state machine
    
    Install the callbacks via the state machine and let the core invoke
    the callbacks on the already online CPUs. A lot of code is removed because
    the for-loop is used and create_hash_tables() is removed since its purpose
    is covered by the startup / teardown hooks.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160713153337.649867675@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index c2199e9901c9..2dbccf2d806c 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -328,68 +328,57 @@ static void do_profile_hits(int type, void *__pc, unsigned int nr_hits)
 	put_cpu();
 }
 
-static int profile_cpu_callback(struct notifier_block *info,
-					unsigned long action, void *__cpu)
+static int profile_dead_cpu(unsigned int cpu)
 {
-	int node, cpu = (unsigned long)__cpu;
 	struct page *page;
+	int i;
 
-	switch (action) {
-	case CPU_UP_PREPARE:
-	case CPU_UP_PREPARE_FROZEN:
-		node = cpu_to_mem(cpu);
-		per_cpu(cpu_profile_flip, cpu) = 0;
-		if (!per_cpu(cpu_profile_hits, cpu)[1]) {
-			page = __alloc_pages_node(node,
-					GFP_KERNEL | __GFP_ZERO,
-					0);
-			if (!page)
-				return notifier_from_errno(-ENOMEM);
-			per_cpu(cpu_profile_hits, cpu)[1] = page_address(page);
-		}
-		if (!per_cpu(cpu_profile_hits, cpu)[0]) {
-			page = __alloc_pages_node(node,
-					GFP_KERNEL | __GFP_ZERO,
-					0);
-			if (!page)
-				goto out_free;
-			per_cpu(cpu_profile_hits, cpu)[0] = page_address(page);
-		}
-		break;
-out_free:
-		page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
-		per_cpu(cpu_profile_hits, cpu)[1] = NULL;
-		__free_page(page);
-		return notifier_from_errno(-ENOMEM);
-	case CPU_ONLINE:
-	case CPU_ONLINE_FROZEN:
-		if (prof_cpu_mask != NULL)
-			cpumask_set_cpu(cpu, prof_cpu_mask);
-		break;
-	case CPU_UP_CANCELED:
-	case CPU_UP_CANCELED_FROZEN:
-	case CPU_DEAD:
-	case CPU_DEAD_FROZEN:
-		if (prof_cpu_mask != NULL)
-			cpumask_clear_cpu(cpu, prof_cpu_mask);
-		if (per_cpu(cpu_profile_hits, cpu)[0]) {
-			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[0]);
-			per_cpu(cpu_profile_hits, cpu)[0] = NULL;
+	if (prof_cpu_mask != NULL)
+		cpumask_clear_cpu(cpu, prof_cpu_mask);
+
+	for (i = 0; i < 2; i++) {
+		if (per_cpu(cpu_profile_hits, cpu)[i]) {
+			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[i]);
+			per_cpu(cpu_profile_hits, cpu)[i] = NULL;
 			__free_page(page);
 		}
-		if (per_cpu(cpu_profile_hits, cpu)[1]) {
-			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
-			per_cpu(cpu_profile_hits, cpu)[1] = NULL;
-			__free_page(page);
+	}
+	return 0;
+}
+
+static int profile_prepare_cpu(unsigned int cpu)
+{
+	int i, node = cpu_to_mem(cpu);
+	struct page *page;
+
+	per_cpu(cpu_profile_flip, cpu) = 0;
+
+	for (i = 0; i < 2; i++) {
+		if (per_cpu(cpu_profile_hits, cpu)[i])
+			continue;
+
+		page = __alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
+		if (!page) {
+			profile_dead_cpu(cpu);
+			return -ENOMEM;
 		}
-		break;
+		per_cpu(cpu_profile_hits, cpu)[i] = page_address(page);
+
 	}
-	return NOTIFY_OK;
+	return 0;
+}
+
+static int profile_online_cpu(unsigned int cpu)
+{
+	if (prof_cpu_mask != NULL)
+		cpumask_set_cpu(cpu, prof_cpu_mask);
+
+	return 0;
 }
+
 #else /* !CONFIG_SMP */
 #define profile_flip_buffers()		do { } while (0)
 #define profile_discard_flip_buffers()	do { } while (0)
-#define profile_cpu_callback		NULL
 
 static void do_profile_hits(int type, void *__pc, unsigned int nr_hits)
 {
@@ -531,83 +520,43 @@ static const struct file_operations proc_profile_operations = {
 	.llseek		= default_llseek,
 };
 
-#ifdef CONFIG_SMP
-static void profile_nop(void *unused)
-{
-}
-
-static int create_hash_tables(void)
+int __ref create_proc_profile(void)
 {
-	int cpu;
-
-	for_each_online_cpu(cpu) {
-		int node = cpu_to_mem(cpu);
-		struct page *page;
-
-		page = __alloc_pages_node(node,
-				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
-				0);
-		if (!page)
-			goto out_cleanup;
-		per_cpu(cpu_profile_hits, cpu)[1]
-				= (struct profile_hit *)page_address(page);
-		page = __alloc_pages_node(node,
-				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
-				0);
-		if (!page)
-			goto out_cleanup;
-		per_cpu(cpu_profile_hits, cpu)[0]
-				= (struct profile_hit *)page_address(page);
-	}
-	return 0;
-out_cleanup:
-	prof_on = 0;
-	smp_mb();
-	on_each_cpu(profile_nop, NULL, 1);
-	for_each_online_cpu(cpu) {
-		struct page *page;
-
-		if (per_cpu(cpu_profile_hits, cpu)[0]) {
-			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[0]);
-			per_cpu(cpu_profile_hits, cpu)[0] = NULL;
-			__free_page(page);
-		}
-		if (per_cpu(cpu_profile_hits, cpu)[1]) {
-			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
-			per_cpu(cpu_profile_hits, cpu)[1] = NULL;
-			__free_page(page);
-		}
-	}
-	return -1;
-}
-#else
-#define create_hash_tables()			({ 0; })
+	struct proc_dir_entry *entry;
+#ifdef CONFIG_SMP
+	enum cpuhp_state online_state;
 #endif
 
-int __ref create_proc_profile(void) /* false positive from hotcpu_notifier */
-{
-	struct proc_dir_entry *entry;
 	int err = 0;
 
 	if (!prof_on)
 		return 0;
-
-	cpu_notifier_register_begin();
-
-	if (create_hash_tables()) {
-		err = -ENOMEM;
-		goto out;
-	}
-
+#ifdef CONFIG_SMP
+	err = cpuhp_setup_state(CPUHP_PROFILE_PREPARE, "PROFILE_PREPARE",
+				profile_prepare_cpu, profile_dead_cpu);
+	if (err)
+		return err;
+
+	err = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, "AP_PROFILE_ONLINE",
+				profile_online_cpu, NULL);
+	if (err < 0)
+		goto err_state_prep;
+	online_state = err;
+	err = 0;
+#endif
 	entry = proc_create("profile", S_IWUSR | S_IRUGO,
 			    NULL, &proc_profile_operations);
 	if (!entry)
-		goto out;
+		goto err_state_onl;
 	proc_set_size(entry, (1 + prof_len) * sizeof(atomic_t));
-	__hotcpu_notifier(profile_cpu_callback, 0);
 
-out:
-	cpu_notifier_register_done();
+	return err;
+err_state_onl:
+#ifdef CONFIG_SMP
+	cpuhp_remove_state(online_state);
+err_state_prep:
+	cpuhp_remove_state(CPUHP_PROFILE_PREPARE);
+#endif
 	return err;
 }
 subsys_initcall(create_proc_profile);

commit ade356b99a4187578609f2a91c4d2ed88e4e70dc
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Mar 22 14:27:26 2016 -0700

    profile: hide unused functions when !CONFIG_PROC_FS
    
    A couple of functions and variables in the profile implementation are
    used only on SMP systems by the procfs code, but are unused if either
    procfs is disabled or in uniprocessor kernels.  gcc prints a harmless
    warning about the unused symbols:
    
      kernel/profile.c:243:13: error: 'profile_flip_buffers' defined but not used [-Werror=unused-function]
       static void profile_flip_buffers(void)
                   ^
      kernel/profile.c:266:13: error: 'profile_discard_flip_buffers' defined but not used [-Werror=unused-function]
       static void profile_discard_flip_buffers(void)
                   ^
      kernel/profile.c:330:12: error: 'profile_cpu_callback' defined but not used [-Werror=unused-function]
       static int profile_cpu_callback(struct notifier_block *info,
                  ^
    
    This adds further #ifdef to the file, to annotate exactly in which cases
    they are used.  I have done several thousand ARM randconfig kernels with
    this patch applied and no longer get any warnings in this file.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Robin Holt <robinmholt@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 51369697466e..c2199e9901c9 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -44,7 +44,7 @@ int prof_on __read_mostly;
 EXPORT_SYMBOL_GPL(prof_on);
 
 static cpumask_var_t prof_cpu_mask;
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && defined(CONFIG_PROC_FS)
 static DEFINE_PER_CPU(struct profile_hit *[2], cpu_profile_hits);
 static DEFINE_PER_CPU(int, cpu_profile_flip);
 static DEFINE_MUTEX(profile_flip_mutex);
@@ -202,7 +202,7 @@ int profile_event_unregister(enum profile_type type, struct notifier_block *n)
 }
 EXPORT_SYMBOL_GPL(profile_event_unregister);
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && defined(CONFIG_PROC_FS)
 /*
  * Each cpu has a pair of open-addressed hashtables for pending
  * profile hits. read_profile() IPI's all cpus to request them

commit cb2517653fccaf9f9b4ae968c7ee005c1bbacdc5
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Feb 5 09:08:36 2016 +0000

    sched/debug: Make schedstats a runtime tunable that is disabled by default
    
    schedstats is very useful during debugging and performance tuning but it
    incurs overhead to calculate the stats. As such, even though it can be
    disabled at build time, it is often enabled as the information is useful.
    
    This patch adds a kernel command-line and sysctl tunable to enable or
    disable schedstats on demand (when it's built in). It is disabled
    by default as someone who knows they need it can also learn to enable
    it when necessary.
    
    The benefits are dependent on how scheduler-intensive the workload is.
    If it is then the patch reduces the number of cycles spent calculating
    the stats with a small benefit from reducing the cache footprint of the
    scheduler.
    
    These measurements were taken from a 48-core 2-socket
    machine with Xeon(R) E5-2670 v3 cpus although they were also tested on a
    single socket machine 8-core machine with Intel i7-3770 processors.
    
    netperf-tcp
                               4.5.0-rc1             4.5.0-rc1
                                 vanilla          nostats-v3r1
    Hmean    64         560.45 (  0.00%)      575.98 (  2.77%)
    Hmean    128        766.66 (  0.00%)      795.79 (  3.80%)
    Hmean    256        950.51 (  0.00%)      981.50 (  3.26%)
    Hmean    1024      1433.25 (  0.00%)     1466.51 (  2.32%)
    Hmean    2048      2810.54 (  0.00%)     2879.75 (  2.46%)
    Hmean    3312      4618.18 (  0.00%)     4682.09 (  1.38%)
    Hmean    4096      5306.42 (  0.00%)     5346.39 (  0.75%)
    Hmean    8192     10581.44 (  0.00%)    10698.15 (  1.10%)
    Hmean    16384    18857.70 (  0.00%)    18937.61 (  0.42%)
    
    Small gains here, UDP_STREAM showed nothing intresting and neither did
    the TCP_RR tests. The gains on the 8-core machine were very similar.
    
    tbench4
                                     4.5.0-rc1             4.5.0-rc1
                                       vanilla          nostats-v3r1
    Hmean    mb/sec-1         500.85 (  0.00%)      522.43 (  4.31%)
    Hmean    mb/sec-2         984.66 (  0.00%)     1018.19 (  3.41%)
    Hmean    mb/sec-4        1827.91 (  0.00%)     1847.78 (  1.09%)
    Hmean    mb/sec-8        3561.36 (  0.00%)     3611.28 (  1.40%)
    Hmean    mb/sec-16       5824.52 (  0.00%)     5929.03 (  1.79%)
    Hmean    mb/sec-32      10943.10 (  0.00%)    10802.83 ( -1.28%)
    Hmean    mb/sec-64      15950.81 (  0.00%)    16211.31 (  1.63%)
    Hmean    mb/sec-128     15302.17 (  0.00%)    15445.11 (  0.93%)
    Hmean    mb/sec-256     14866.18 (  0.00%)    15088.73 (  1.50%)
    Hmean    mb/sec-512     15223.31 (  0.00%)    15373.69 (  0.99%)
    Hmean    mb/sec-1024    14574.25 (  0.00%)    14598.02 (  0.16%)
    Hmean    mb/sec-2048    13569.02 (  0.00%)    13733.86 (  1.21%)
    Hmean    mb/sec-3072    12865.98 (  0.00%)    13209.23 (  2.67%)
    
    Small gains of 2-4% at low thread counts and otherwise flat.  The
    gains on the 8-core machine were slightly different
    
    tbench4 on 8-core i7-3770 single socket machine
    Hmean    mb/sec-1        442.59 (  0.00%)      448.73 (  1.39%)
    Hmean    mb/sec-2        796.68 (  0.00%)      794.39 ( -0.29%)
    Hmean    mb/sec-4       1322.52 (  0.00%)     1343.66 (  1.60%)
    Hmean    mb/sec-8       2611.65 (  0.00%)     2694.86 (  3.19%)
    Hmean    mb/sec-16      2537.07 (  0.00%)     2609.34 (  2.85%)
    Hmean    mb/sec-32      2506.02 (  0.00%)     2578.18 (  2.88%)
    Hmean    mb/sec-64      2511.06 (  0.00%)     2569.16 (  2.31%)
    Hmean    mb/sec-128     2313.38 (  0.00%)     2395.50 (  3.55%)
    Hmean    mb/sec-256     2110.04 (  0.00%)     2177.45 (  3.19%)
    Hmean    mb/sec-512     2072.51 (  0.00%)     2053.97 ( -0.89%)
    
    In constract, this shows a relatively steady 2-3% gain at higher thread
    counts. Due to the nature of the patch and the type of workload, it's
    not a surprise that the result will depend on the CPU used.
    
    hackbench-pipes
                             4.5.0-rc1             4.5.0-rc1
                               vanilla          nostats-v3r1
    Amean    1        0.0637 (  0.00%)      0.0660 ( -3.59%)
    Amean    4        0.1229 (  0.00%)      0.1181 (  3.84%)
    Amean    7        0.1921 (  0.00%)      0.1911 (  0.52%)
    Amean    12       0.3117 (  0.00%)      0.2923 (  6.23%)
    Amean    21       0.4050 (  0.00%)      0.3899 (  3.74%)
    Amean    30       0.4586 (  0.00%)      0.4433 (  3.33%)
    Amean    48       0.5910 (  0.00%)      0.5694 (  3.65%)
    Amean    79       0.8663 (  0.00%)      0.8626 (  0.43%)
    Amean    110      1.1543 (  0.00%)      1.1517 (  0.22%)
    Amean    141      1.4457 (  0.00%)      1.4290 (  1.16%)
    Amean    172      1.7090 (  0.00%)      1.6924 (  0.97%)
    Amean    192      1.9126 (  0.00%)      1.9089 (  0.19%)
    
    Some small gains and losses and while the variance data is not included,
    it's close to the noise. The UMA machine did not show anything particularly
    different
    
    pipetest
                                 4.5.0-rc1             4.5.0-rc1
                                   vanilla          nostats-v2r2
    Min         Time        4.13 (  0.00%)        3.99 (  3.39%)
    1st-qrtle   Time        4.38 (  0.00%)        4.27 (  2.51%)
    2nd-qrtle   Time        4.46 (  0.00%)        4.39 (  1.57%)
    3rd-qrtle   Time        4.56 (  0.00%)        4.51 (  1.10%)
    Max-90%     Time        4.67 (  0.00%)        4.60 (  1.50%)
    Max-93%     Time        4.71 (  0.00%)        4.65 (  1.27%)
    Max-95%     Time        4.74 (  0.00%)        4.71 (  0.63%)
    Max-99%     Time        4.88 (  0.00%)        4.79 (  1.84%)
    Max         Time        4.93 (  0.00%)        4.83 (  2.03%)
    Mean        Time        4.48 (  0.00%)        4.39 (  1.91%)
    Best99%Mean Time        4.47 (  0.00%)        4.39 (  1.91%)
    Best95%Mean Time        4.46 (  0.00%)        4.38 (  1.93%)
    Best90%Mean Time        4.45 (  0.00%)        4.36 (  1.98%)
    Best50%Mean Time        4.36 (  0.00%)        4.25 (  2.49%)
    Best10%Mean Time        4.23 (  0.00%)        4.10 (  3.13%)
    Best5%Mean  Time        4.19 (  0.00%)        4.06 (  3.20%)
    Best1%Mean  Time        4.13 (  0.00%)        4.00 (  3.39%)
    
    Small improvement and similar gains were seen on the UMA machine.
    
    The gain is small but it stands to reason that doing less work in the
    scheduler is a good thing. The downside is that the lack of schedstats and
    tracepoints may be surprising to experts doing performance analysis until
    they find the existence of the schedstats= parameter or schedstats sysctl.
    It will be automatically activated for latencytop and sleep profiling to
    alleviate the problem. For tracepoints, there is a simple warning as it's
    not safe to activate schedstats in the context when it's known the tracepoint
    may be wanted but is unavailable.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Reviewed-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <mgalbraith@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1454663316-22048-1-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 99513e1160e5..51369697466e 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -59,6 +59,7 @@ int profile_setup(char *str)
 
 	if (!strncmp(str, sleepstr, strlen(sleepstr))) {
 #ifdef CONFIG_SCHEDSTATS
+		force_schedstat_enabled();
 		prof_on = SLEEP_PROFILING;
 		if (str[strlen(sleepstr)] == ',')
 			str += strlen(sleepstr) + 1;

commit 96db800f5d73cd5c49461253d45766e094f0f8c2
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Tue Sep 8 15:03:50 2015 -0700

    mm: rename alloc_pages_exact_node() to __alloc_pages_node()
    
    alloc_pages_exact_node() was introduced in commit 6484eb3e2a81 ("page
    allocator: do not check NUMA node ID when the caller knows the node is
    valid") as an optimized variant of alloc_pages_node(), that doesn't
    fallback to current node for nid == NUMA_NO_NODE.  Unfortunately the
    name of the function can easily suggest that the allocation is
    restricted to the given node and fails otherwise.  In truth, the node is
    only preferred, unless __GFP_THISNODE is passed among the gfp flags.
    
    The misleading name has lead to mistakes in the past, see for example
    commits 5265047ac301 ("mm, thp: really limit transparent hugepage
    allocation to local node") and b360edb43f8e ("mm, mempolicy:
    migrate_to_node should only migrate to node").
    
    Another issue with the name is that there's a family of
    alloc_pages_exact*() functions where 'exact' means exact size (instead
    of page order), which leads to more confusion.
    
    To prevent further mistakes, this patch effectively renames
    alloc_pages_exact_node() to __alloc_pages_node() to better convey that
    it's an optimized variant of alloc_pages_node() not intended for general
    usage.  Both functions get described in comments.
    
    It has been also considered to really provide a convenience function for
    allocations restricted to a node, but the major opinion seems to be that
    __GFP_THISNODE already provides that functionality and we shouldn't
    duplicate the API needlessly.  The number of users would be small
    anyway.
    
    Existing callers of alloc_pages_exact_node() are simply converted to
    call __alloc_pages_node(), with the exception of sba_alloc_coherent()
    which open-codes the check for NUMA_NO_NODE, so it is converted to use
    alloc_pages_node() instead.  This means it no longer performs some
    VM_BUG_ON checks, and since the current check for nid in
    alloc_pages_node() uses a 'nid < 0' comparison (which includes
    NUMA_NO_NODE), it may hide wrong values which would be previously
    exposed.
    
    Both differences will be rectified by the next patch.
    
    To sum up, this patch makes no functional changes, except temporarily
    hiding potentially buggy callers.  Restricting the checks in
    alloc_pages_node() is left for the next patch which can in turn expose
    more existing buggy callers.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Robin Holt <robinmholt@gmail.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Gleb Natapov <gleb@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Cliff Whickman <cpw@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index a7bcd28d6e9f..99513e1160e5 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -339,7 +339,7 @@ static int profile_cpu_callback(struct notifier_block *info,
 		node = cpu_to_mem(cpu);
 		per_cpu(cpu_profile_flip, cpu) = 0;
 		if (!per_cpu(cpu_profile_hits, cpu)[1]) {
-			page = alloc_pages_exact_node(node,
+			page = __alloc_pages_node(node,
 					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
@@ -347,7 +347,7 @@ static int profile_cpu_callback(struct notifier_block *info,
 			per_cpu(cpu_profile_hits, cpu)[1] = page_address(page);
 		}
 		if (!per_cpu(cpu_profile_hits, cpu)[0]) {
-			page = alloc_pages_exact_node(node,
+			page = __alloc_pages_node(node,
 					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
@@ -543,14 +543,14 @@ static int create_hash_tables(void)
 		int node = cpu_to_mem(cpu);
 		struct page *page;
 
-		page = alloc_pages_exact_node(node,
+		page = __alloc_pages_node(node,
 				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				0);
 		if (!page)
 			goto out_cleanup;
 		per_cpu(cpu_profile_hits, cpu)[1]
 				= (struct profile_hit *)page_address(page);
-		page = alloc_pages_exact_node(node,
+		page = __alloc_pages_node(node,
 				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				0);
 		if (!page)

commit ccbd59c1c104d6e42e949e2588563bfe25d9d98f
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Feb 13 14:38:13 2015 -0800

    profile: use %*pb[l] to print bitmaps including cpumasks and nodemasks
    
    printk and friends can now format bitmaps using '%*pb[l]'.  cpumask
    and nodemask also provide cpumask_pr_args() and nodemask_pr_args()
    respectively which can be used to generate the two printf arguments
    necessary to format the specified cpu/nodemask.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 54bf5ba26420..a7bcd28d6e9f 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -422,8 +422,7 @@ void profile_tick(int type)
 
 static int prof_cpu_mask_proc_show(struct seq_file *m, void *v)
 {
-	seq_cpumask(m, prof_cpu_mask);
-	seq_putc(m, '\n');
+	seq_printf(m, "%*pb\n", cpumask_pr_args(prof_cpu_mask));
 	return 0;
 }
 

commit f3da64d1ea12ed6cfe18e6dedbc9739743e0b884
Author: Fabian Frederick <fabf@skynet.be>
Date:   Fri Jun 6 14:37:30 2014 -0700

    kernel/profile.c: use static const char instead of static char
    
    schedstr, sleepstr and kvmstr are only used in strcmp & strlen
    
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index c1204e2af5b2..54bf5ba26420 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -52,9 +52,9 @@ static DEFINE_MUTEX(profile_flip_mutex);
 
 int profile_setup(char *str)
 {
-	static char schedstr[] = "schedule";
-	static char sleepstr[] = "sleep";
-	static char kvmstr[] = "kvm";
+	static const char schedstr[] = "schedule";
+	static const char sleepstr[] = "sleep";
+	static const char kvmstr[] = "kvm";
 	int par;
 
 	if (!strncmp(str, sleepstr, strlen(sleepstr))) {

commit aba871f1e9a8dad07d442913998cd679529f2784
Author: Fabian Frederick <fabf@skynet.be>
Date:   Fri Jun 6 14:37:29 2014 -0700

    kernel/profile.c: convert printk to pr_foo()
    
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index cb980f0c731b..c1204e2af5b2 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -64,12 +64,10 @@ int profile_setup(char *str)
 			str += strlen(sleepstr) + 1;
 		if (get_option(&str, &par))
 			prof_shift = par;
-		printk(KERN_INFO
-			"kernel sleep profiling enabled (shift: %ld)\n",
+		pr_info("kernel sleep profiling enabled (shift: %ld)\n",
 			prof_shift);
 #else
-		printk(KERN_WARNING
-			"kernel sleep profiling requires CONFIG_SCHEDSTATS\n");
+		pr_warn("kernel sleep profiling requires CONFIG_SCHEDSTATS\n");
 #endif /* CONFIG_SCHEDSTATS */
 	} else if (!strncmp(str, schedstr, strlen(schedstr))) {
 		prof_on = SCHED_PROFILING;
@@ -77,8 +75,7 @@ int profile_setup(char *str)
 			str += strlen(schedstr) + 1;
 		if (get_option(&str, &par))
 			prof_shift = par;
-		printk(KERN_INFO
-			"kernel schedule profiling enabled (shift: %ld)\n",
+		pr_info("kernel schedule profiling enabled (shift: %ld)\n",
 			prof_shift);
 	} else if (!strncmp(str, kvmstr, strlen(kvmstr))) {
 		prof_on = KVM_PROFILING;
@@ -86,13 +83,12 @@ int profile_setup(char *str)
 			str += strlen(kvmstr) + 1;
 		if (get_option(&str, &par))
 			prof_shift = par;
-		printk(KERN_INFO
-			"kernel KVM profiling enabled (shift: %ld)\n",
+		pr_info("kernel KVM profiling enabled (shift: %ld)\n",
 			prof_shift);
 	} else if (get_option(&str, &par)) {
 		prof_shift = par;
 		prof_on = CPU_PROFILING;
-		printk(KERN_INFO "kernel profiling enabled (shift: %ld)\n",
+		pr_info("kernel profiling enabled (shift: %ld)\n",
 			prof_shift);
 	}
 	return 1;

commit 467a9e1633043810259a7f5368fbcc1e84746137
Merge: b8780c363d80 a0e247a80592
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 7 14:55:46 2014 -0700

    Merge tag 'cpu-hotplug-3.15-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull CPU hotplug notifiers registration fixes from Rafael Wysocki:
     "The purpose of this single series of commits from Srivatsa S Bhat
      (with a small piece from Gautham R Shenoy) touching multiple
      subsystems that use CPU hotplug notifiers is to provide a way to
      register them that will not lead to deadlocks with CPU online/offline
      operations as described in the changelog of commit 93ae4f978ca7f ("CPU
      hotplug: Provide lockless versions of callback registration
      functions").
    
      The first three commits in the series introduce the API and document
      it and the rest simply goes through the users of CPU hotplug notifiers
      and converts them to using the new method"
    
    * tag 'cpu-hotplug-3.15-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (52 commits)
      net/iucv/iucv.c: Fix CPU hotplug callback registration
      net/core/flow.c: Fix CPU hotplug callback registration
      mm, zswap: Fix CPU hotplug callback registration
      mm, vmstat: Fix CPU hotplug callback registration
      profile: Fix CPU hotplug callback registration
      trace, ring-buffer: Fix CPU hotplug callback registration
      xen, balloon: Fix CPU hotplug callback registration
      hwmon, via-cputemp: Fix CPU hotplug callback registration
      hwmon, coretemp: Fix CPU hotplug callback registration
      thermal, x86-pkg-temp: Fix CPU hotplug callback registration
      octeon, watchdog: Fix CPU hotplug callback registration
      oprofile, nmi-timer: Fix CPU hotplug callback registration
      intel-idle: Fix CPU hotplug callback registration
      clocksource, dummy-timer: Fix CPU hotplug callback registration
      drivers/base/topology.c: Fix CPU hotplug callback registration
      acpi-cpufreq: Fix CPU hotplug callback registration
      zsmalloc: Fix CPU hotplug callback registration
      scsi, fcoe: Fix CPU hotplug callback registration
      scsi, bnx2fc: Fix CPU hotplug callback registration
      scsi, bnx2i: Fix CPU hotplug callback registration
      ...

commit c96d6660dc65b0a90aea9834bfd8be1d5656da18
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu Apr 3 14:48:35 2014 -0700

    kernel: audit/fix non-modular users of module_init in core code
    
    Code that is obj-y (always built-in) or dependent on a bool Kconfig
    (built-in or absent) can never be modular.  So using module_init as an
    alias for __initcall can be somewhat misleading.
    
    Fix these up now, so that we can relocate module_init from init.h into
    module.h in the future.  If we don't do this, we'd have to add module.h
    to obviously non-modular code, and that would be a worse thing.
    
    The audit targets the following module_init users for change:
     kernel/user.c                  obj-y
     kernel/kexec.c                 bool KEXEC (one instance per arch)
     kernel/profile.c               bool PROFILING
     kernel/hung_task.c             bool DETECT_HUNG_TASK
     kernel/sched/stats.c           bool SCHEDSTATS
     kernel/user_namespace.c        bool USER_NS
    
    Note that direct use of __initcall is discouraged, vs.  one of the
    priority categorized subgroups.  As __initcall gets mapped onto
    device_initcall, our use of subsys_initcall (which makes sense for these
    files) will thus change this registration from level 6-device to level
    4-subsys (i.e.  slightly earlier).  However no observable impact of that
    difference has been observed during testing.
    
    Also, two instances of missing ";" at EOL are fixed in kexec.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index ebdd9c1a86b4..1b266dbe755a 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -604,5 +604,5 @@ int __ref create_proc_profile(void) /* false positive from hotcpu_notifier */
 	hotcpu_notifier(profile_cpu_callback, 0);
 	return 0;
 }
-module_init(create_proc_profile);
+subsys_initcall(create_proc_profile);
 #endif /* CONFIG_PROC_FS */

commit c270a817196a9374a2dc730624d1501dced40b4d
Author: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
Date:   Tue Mar 11 02:12:08 2014 +0530

    profile: Fix CPU hotplug callback registration
    
    Subsystems that want to register CPU hotplug callbacks, as well as perform
    initialization for the CPUs that are already online, often do it as shown
    below:
    
            get_online_cpus();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            register_cpu_notifier(&foobar_cpu_notifier);
    
            put_online_cpus();
    
    This is wrong, since it is prone to ABBA deadlocks involving the
    cpu_add_remove_lock and the cpu_hotplug.lock (when running concurrently
    with CPU hotplug operations).
    
    Instead, the correct and race-free way of performing the callback
    registration is:
    
            cpu_notifier_register_begin();
    
            for_each_online_cpu(cpu)
                    init_cpu(cpu);
    
            /* Note the use of the double underscored version of the API */
            __register_cpu_notifier(&foobar_cpu_notifier);
    
            cpu_notifier_register_done();
    
    Fix the profile code by using this latter form of callback registration.
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Mauro Carvalho Chehab <mchehab@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/profile.c b/kernel/profile.c
index ebdd9c1a86b4..93b2a3fe0a64 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -591,18 +591,28 @@ static int create_hash_tables(void)
 int __ref create_proc_profile(void) /* false positive from hotcpu_notifier */
 {
 	struct proc_dir_entry *entry;
+	int err = 0;
 
 	if (!prof_on)
 		return 0;
-	if (create_hash_tables())
-		return -ENOMEM;
+
+	cpu_notifier_register_begin();
+
+	if (create_hash_tables()) {
+		err = -ENOMEM;
+		goto out;
+	}
+
 	entry = proc_create("profile", S_IWUSR | S_IRUGO,
 			    NULL, &proc_profile_operations);
 	if (!entry)
-		return 0;
+		goto out;
 	proc_set_size(entry, (1 + prof_len) * sizeof(atomic_t));
-	hotcpu_notifier(profile_cpu_callback, 0);
-	return 0;
+	__hotcpu_notifier(profile_cpu_callback, 0);
+
+out:
+	cpu_notifier_register_done();
+	return err;
 }
 module_init(create_proc_profile);
 #endif /* CONFIG_PROC_FS */

commit e97ca8e5b864f88b028c1759ba8536fa827d6d96
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Mon Mar 10 15:49:43 2014 -0700

    mm: fix GFP_THISNODE callers and clarify
    
    GFP_THISNODE is for callers that implement their own clever fallback to
    remote nodes.  It restricts the allocation to the specified node and
    does not invoke reclaim, assuming that the caller will take care of it
    when the fallback fails, e.g.  through a subsequent allocation request
    without GFP_THISNODE set.
    
    However, many current GFP_THISNODE users only want the node exclusive
    aspect of the flag, without actually implementing their own fallback or
    triggering reclaim if necessary.  This results in things like page
    migration failing prematurely even when there is easily reclaimable
    memory available, unless kswapd happens to be running already or a
    concurrent allocation attempt triggers the necessary reclaim.
    
    Convert all callsites that don't implement their own fallback strategy
    to __GFP_THISNODE.  This restricts the allocation a single node too, but
    at the same time allows the allocator to enter the slowpath, wake
    kswapd, and invoke direct reclaim if necessary, to make the allocation
    happen when memory is full.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Jan Stancek <jstancek@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 6631e1ef55ab..ebdd9c1a86b4 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -549,14 +549,14 @@ static int create_hash_tables(void)
 		struct page *page;
 
 		page = alloc_pages_exact_node(node,
-				GFP_KERNEL | __GFP_ZERO | GFP_THISNODE,
+				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				0);
 		if (!page)
 			goto out_cleanup;
 		per_cpu(cpu_profile_hits, cpu)[1]
 				= (struct profile_hit *)page_address(page);
 		page = alloc_pages_exact_node(node,
-				GFP_KERNEL | __GFP_ZERO | GFP_THISNODE,
+				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				0);
 		if (!page)
 			goto out_cleanup;

commit 0db0628d90125193280eabb501c94feaf48fa9ab
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jun 19 14:53:51 2013 -0400

    kernel: delete __cpuinit usage from all core kernel files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    This removes all the uses of the __cpuinit macros from C files in
    the core kernel directories (kernel, init, lib, mm, and include)
    that don't really have a specific maintainer.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/profile.c b/kernel/profile.c
index 0bf400737660..6631e1ef55ab 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -331,7 +331,7 @@ static void do_profile_hits(int type, void *__pc, unsigned int nr_hits)
 	put_cpu();
 }
 
-static int __cpuinit profile_cpu_callback(struct notifier_block *info,
+static int profile_cpu_callback(struct notifier_block *info,
 					unsigned long action, void *__cpu)
 {
 	int node, cpu = (unsigned long)__cpu;

commit 271a15eabe094538d958dc68ccfc9c36b699247a
Author: David Howells <dhowells@redhat.com>
Date:   Fri Apr 12 00:38:51 2013 +0100

    proc: Supply PDE attribute setting accessor functions
    
    Supply accessor functions to set attributes in proc_dir_entry structs.
    
    The following are supplied: proc_set_size() and proc_set_user().
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Mauro Carvalho Chehab <mchehab@redhat.com>
    cc: linuxppc-dev@lists.ozlabs.org
    cc: linux-media@vger.kernel.org
    cc: netdev@vger.kernel.org
    cc: linux-wireless@vger.kernel.org
    cc: linux-pci@vger.kernel.org
    cc: netfilter-devel@vger.kernel.org
    cc: alsa-devel@alsa-project.org
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/profile.c b/kernel/profile.c
index 524ce5e29d3f..0bf400737660 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -600,7 +600,7 @@ int __ref create_proc_profile(void) /* false positive from hotcpu_notifier */
 			    NULL, &proc_profile_operations);
 	if (!entry)
 		return 0;
-	entry->size = (1+prof_len) * sizeof(atomic_t);
+	proc_set_size(entry, (1 + prof_len) * sizeof(atomic_t));
 	hotcpu_notifier(profile_cpu_callback, 0);
 	return 0;
 }

commit fbd387aea0cb98c9d6e534c55d3d2ac83153348d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Apr 1 20:48:34 2013 -0400

    create_proc_cpu_mask() doesn't need an argument...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/profile.c b/kernel/profile.c
index dc3384ee874e..524ce5e29d3f 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -462,10 +462,10 @@ static const struct file_operations prof_cpu_mask_proc_fops = {
 	.write		= prof_cpu_mask_proc_write,
 };
 
-void create_prof_cpu_mask(struct proc_dir_entry *root_irq_dir)
+void create_prof_cpu_mask(void)
 {
 	/* create /proc/irq/prof_cpu_mask */
-	proc_create("prof_cpu_mask", 0600, root_irq_dir, &prof_cpu_mask_proc_fops);
+	proc_create("irq/prof_cpu_mask", 0600, NULL, &prof_cpu_mask_proc_fops);
 }
 
 /*

commit ba6fdda46b377034c782c0b89c8f1090b31eabd8
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Dec 22 16:59:51 2012 +0100

    profiling: Remove unused timer hook
    
    The last remaining user was oprofile and its use has been
    removed a while ago in commit bc078e4eab65f11bba
    ("oprofile: convert oprofile from timer_hook to hrtimer").
    
    There doesn't seem to be any upstream user of this hook
    for about two years now. And I'm not even aware of any out of
    tree user.
    
    Let's remove it.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1356191991-2251-1-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 1f391819c42f..dc3384ee874e 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -37,9 +37,6 @@ struct profile_hit {
 #define NR_PROFILE_HIT		(PAGE_SIZE/sizeof(struct profile_hit))
 #define NR_PROFILE_GRP		(NR_PROFILE_HIT/PROFILE_GRPSZ)
 
-/* Oprofile timer tick hook */
-static int (*timer_hook)(struct pt_regs *) __read_mostly;
-
 static atomic_t *prof_buffer;
 static unsigned long prof_len, prof_shift;
 
@@ -208,25 +205,6 @@ int profile_event_unregister(enum profile_type type, struct notifier_block *n)
 }
 EXPORT_SYMBOL_GPL(profile_event_unregister);
 
-int register_timer_hook(int (*hook)(struct pt_regs *))
-{
-	if (timer_hook)
-		return -EBUSY;
-	timer_hook = hook;
-	return 0;
-}
-EXPORT_SYMBOL_GPL(register_timer_hook);
-
-void unregister_timer_hook(int (*hook)(struct pt_regs *))
-{
-	WARN_ON(hook != timer_hook);
-	timer_hook = NULL;
-	/* make sure all CPUs see the NULL hook */
-	synchronize_sched();  /* Allow ongoing interrupts to complete. */
-}
-EXPORT_SYMBOL_GPL(unregister_timer_hook);
-
-
 #ifdef CONFIG_SMP
 /*
  * Each cpu has a pair of open-addressed hashtables for pending
@@ -436,8 +414,6 @@ void profile_tick(int type)
 {
 	struct pt_regs *regs = get_irq_regs();
 
-	if (type == CPU_PROFILING && timer_hook)
-		timer_hook(regs);
 	if (!user_mode(regs) && prof_cpu_mask != NULL &&
 	    cpumask_test_cpu(smp_processor_id(), prof_cpu_mask))
 		profile_hit(type, (void *)profile_pc(regs));

commit 6d49e352ae9aed3f599041b0c0389aa924815f14
Author: Nadia Yvette Chambers <nyc@holomorphy.com>
Date:   Thu Dec 6 10:39:54 2012 +0100

    propagate name change to comments in kernel source
    
    I've legally changed my name with New York State, the US Social Security
    Administration, et al. This patch propagates the name change and change
    in initials and login to comments in the kernel source as well.
    
    Signed-off-by: Nadia Yvette Chambers <nyc@holomorphy.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/profile.c b/kernel/profile.c
index 76b8e77773ee..1f391819c42f 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -8,9 +8,10 @@
  *  Scheduler profiling support, Arjan van de Ven and Ingo Molnar,
  *	Red Hat, July 2004
  *  Consolidation of architecture support code for profiling,
- *	William Irwin, Oracle, July 2004
+ *	Nadia Yvette Chambers, Oracle, July 2004
  *  Amortized hit count accounting via per-cpu open-addressed hashtables
- *	to resolve timer interrupt livelocks, William Irwin, Oracle, 2004
+ *	to resolve timer interrupt livelocks, Nadia Yvette Chambers,
+ *	Oracle, 2004
  */
 
 #include <linux/export.h>
@@ -256,7 +257,7 @@ EXPORT_SYMBOL_GPL(unregister_timer_hook);
  * pagetable hash functions, but uses a full hashtable full of finite
  * collision chains, not just pairs of them.
  *
- * -- wli
+ * -- nyc
  */
 static void __profile_flip_buffers(void *unused)
 {

commit 9984de1a5a8a96275fcab818f7419af5a3c86e71
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon May 23 14:51:41 2011 -0400

    kernel: Map most files to use export.h instead of module.h
    
    The changed files were only including linux/module.h for the
    EXPORT_SYMBOL infrastructure, and nothing else.  Revector them
    onto the isolated export header for faster compile times.
    
    Nothing to see here but a whole lot of instances of:
    
      -#include <linux/module.h>
      +#include <linux/export.h>
    
    This commit is only changing the kernel dir; next targets
    will probably be mm, fs, the arch dirs, etc.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/profile.c b/kernel/profile.c
index 961b389fe52f..76b8e77773ee 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -13,7 +13,7 @@
  *	to resolve timer interrupt livelocks, William Irwin, Oracle, 2004
  */
 
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/profile.h>
 #include <linux/bootmem.h>
 #include <linux/notifier.h>

commit 6f7bd76f05eb2bfbb48d58c0408a50a7e16b2423
Author: Rakib Mullick <rakib.mullick@gmail.com>
Date:   Thu May 26 16:26:00 2011 -0700

    kernel/profile.c: remove some duplicate code from profile_hits()
    
    profile_hits() has a common check for prof_on and prof_buffer regardless
    of SMP or !SMP.  So, remove some duplicate code by splitting profile_hits
    into two.
    
    [akpm@linux-foundation.org: make do_profile_hits static]
    Signed-off-by: Rakib Mullick <rakib.mullick@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 14c9f87b9fc9..961b389fe52f 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -303,14 +303,12 @@ static void profile_discard_flip_buffers(void)
 	mutex_unlock(&profile_flip_mutex);
 }
 
-void profile_hits(int type, void *__pc, unsigned int nr_hits)
+static void do_profile_hits(int type, void *__pc, unsigned int nr_hits)
 {
 	unsigned long primary, secondary, flags, pc = (unsigned long)__pc;
 	int i, j, cpu;
 	struct profile_hit *hits;
 
-	if (prof_on != type || !prof_buffer)
-		return;
 	pc = min((pc - (unsigned long)_stext) >> prof_shift, prof_len - 1);
 	i = primary = (pc & (NR_PROFILE_GRP - 1)) << PROFILE_GRPSHIFT;
 	secondary = (~(pc << 1) & (NR_PROFILE_GRP - 1)) << PROFILE_GRPSHIFT;
@@ -417,16 +415,20 @@ static int __cpuinit profile_cpu_callback(struct notifier_block *info,
 #define profile_discard_flip_buffers()	do { } while (0)
 #define profile_cpu_callback		NULL
 
-void profile_hits(int type, void *__pc, unsigned int nr_hits)
+static void do_profile_hits(int type, void *__pc, unsigned int nr_hits)
 {
 	unsigned long pc;
-
-	if (prof_on != type || !prof_buffer)
-		return;
 	pc = ((unsigned long)__pc - (unsigned long)_stext) >> prof_shift;
 	atomic_add(nr_hits, &prof_buffer[min(pc, prof_len - 1)]);
 }
 #endif /* !CONFIG_SMP */
+
+void profile_hits(int type, void *__pc, unsigned int nr_hits)
+{
+	if (prof_on != type || !prof_buffer)
+		return;
+	do_profile_hits(type, __pc, nr_hits);
+}
 EXPORT_SYMBOL_GPL(profile_hits);
 
 void profile_tick(int type)

commit 559fa6e76b271b98ff641fa2a968aa2439e43c28
Author: Jesper Juhl <jj@chaosbits.net>
Date:   Sat Oct 30 21:56:26 2010 +0200

    profile: Use vzalloc() rather than vmalloc() & memset()
    
    There's no reason to memset() manually when we have vzalloc().
    
    Signed-off-by: Jesper Juhl <jj@chaosbits.net>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: William Irwin <wli@holomorphy.com>
    LKML-Reference: <alpine.LNX.2.00.1010302150310.1572@swampdragon.chaosbits.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/profile.c b/kernel/profile.c
index 66f841b7fbd3..14c9f87b9fc9 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -126,11 +126,9 @@ int __ref profile_init(void)
 	if (prof_buffer)
 		return 0;
 
-	prof_buffer = vmalloc(buffer_bytes);
-	if (prof_buffer) {
-		memset(prof_buffer, 0, buffer_bytes);
+	prof_buffer = vzalloc(buffer_bytes);
+	if (prof_buffer)
 		return 0;
-	}
 
 	free_cpumask_var(prof_cpu_mask);
 	return -ENOMEM;

commit 6038f373a3dc1f1c26496e60b6c40b164716f07e
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Sun Aug 15 18:52:59 2010 +0200

    llseek: automatically add .llseek fop
    
    All file_operations should get a .llseek operation so we can make
    nonseekable_open the default for future file operations without a
    .llseek pointer.
    
    The three cases that we can automatically detect are no_llseek, seq_lseek
    and default_llseek. For cases where we can we can automatically prove that
    the file offset is always ignored, we use noop_llseek, which maintains
    the current behavior of not returning an error from a seek.
    
    New drivers should normally not use noop_llseek but instead use no_llseek
    and call nonseekable_open at open time.  Existing drivers can be converted
    to do the same when the maintainer knows for certain that no user code
    relies on calling seek on the device file.
    
    The generated code is often incorrectly indented and right now contains
    comments that clarify for each added line why a specific variant was
    chosen. In the version that gets submitted upstream, the comments will
    be gone and I will manually fix the indentation, because there does not
    seem to be a way to do that using coccinelle.
    
    Some amount of new code is currently sitting in linux-next that should get
    the same modifications, which I will do at the end of the merge window.
    
    Many thanks to Julia Lawall for helping me learn to write a semantic
    patch that does all this.
    
    ===== begin semantic patch =====
    // This adds an llseek= method to all file operations,
    // as a preparation for making no_llseek the default.
    //
    // The rules are
    // - use no_llseek explicitly if we do nonseekable_open
    // - use seq_lseek for sequential files
    // - use default_llseek if we know we access f_pos
    // - use noop_llseek if we know we don't access f_pos,
    //   but we still want to allow users to call lseek
    //
    @ open1 exists @
    identifier nested_open;
    @@
    nested_open(...)
    {
    <+...
    nonseekable_open(...)
    ...+>
    }
    
    @ open exists@
    identifier open_f;
    identifier i, f;
    identifier open1.nested_open;
    @@
    int open_f(struct inode *i, struct file *f)
    {
    <+...
    (
    nonseekable_open(...)
    |
    nested_open(...)
    )
    ...+>
    }
    
    @ read disable optional_qualifier exists @
    identifier read_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    expression E;
    identifier func;
    @@
    ssize_t read_f(struct file *f, char *p, size_t s, loff_t *off)
    {
    <+...
    (
       *off = E
    |
       *off += E
    |
       func(..., off, ...)
    |
       E = *off
    )
    ...+>
    }
    
    @ read_no_fpos disable optional_qualifier exists @
    identifier read_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    @@
    ssize_t read_f(struct file *f, char *p, size_t s, loff_t *off)
    {
    ... when != off
    }
    
    @ write @
    identifier write_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    expression E;
    identifier func;
    @@
    ssize_t write_f(struct file *f, const char *p, size_t s, loff_t *off)
    {
    <+...
    (
      *off = E
    |
      *off += E
    |
      func(..., off, ...)
    |
      E = *off
    )
    ...+>
    }
    
    @ write_no_fpos @
    identifier write_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    @@
    ssize_t write_f(struct file *f, const char *p, size_t s, loff_t *off)
    {
    ... when != off
    }
    
    @ fops0 @
    identifier fops;
    @@
    struct file_operations fops = {
     ...
    };
    
    @ has_llseek depends on fops0 @
    identifier fops0.fops;
    identifier llseek_f;
    @@
    struct file_operations fops = {
    ...
     .llseek = llseek_f,
    ...
    };
    
    @ has_read depends on fops0 @
    identifier fops0.fops;
    identifier read_f;
    @@
    struct file_operations fops = {
    ...
     .read = read_f,
    ...
    };
    
    @ has_write depends on fops0 @
    identifier fops0.fops;
    identifier write_f;
    @@
    struct file_operations fops = {
    ...
     .write = write_f,
    ...
    };
    
    @ has_open depends on fops0 @
    identifier fops0.fops;
    identifier open_f;
    @@
    struct file_operations fops = {
    ...
     .open = open_f,
    ...
    };
    
    // use no_llseek if we call nonseekable_open
    ////////////////////////////////////////////
    @ nonseekable1 depends on !has_llseek && has_open @
    identifier fops0.fops;
    identifier nso ~= "nonseekable_open";
    @@
    struct file_operations fops = {
    ...  .open = nso, ...
    +.llseek = no_llseek, /* nonseekable */
    };
    
    @ nonseekable2 depends on !has_llseek @
    identifier fops0.fops;
    identifier open.open_f;
    @@
    struct file_operations fops = {
    ...  .open = open_f, ...
    +.llseek = no_llseek, /* open uses nonseekable */
    };
    
    // use seq_lseek for sequential files
    /////////////////////////////////////
    @ seq depends on !has_llseek @
    identifier fops0.fops;
    identifier sr ~= "seq_read";
    @@
    struct file_operations fops = {
    ...  .read = sr, ...
    +.llseek = seq_lseek, /* we have seq_read */
    };
    
    // use default_llseek if there is a readdir
    ///////////////////////////////////////////
    @ fops1 depends on !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier readdir_e;
    @@
    // any other fop is used that changes pos
    struct file_operations fops = {
    ... .readdir = readdir_e, ...
    +.llseek = default_llseek, /* readdir is present */
    };
    
    // use default_llseek if at least one of read/write touches f_pos
    /////////////////////////////////////////////////////////////////
    @ fops2 depends on !fops1 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier read.read_f;
    @@
    // read fops use offset
    struct file_operations fops = {
    ... .read = read_f, ...
    +.llseek = default_llseek, /* read accesses f_pos */
    };
    
    @ fops3 depends on !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier write.write_f;
    @@
    // write fops use offset
    struct file_operations fops = {
    ... .write = write_f, ...
    +       .llseek = default_llseek, /* write accesses f_pos */
    };
    
    // Use noop_llseek if neither read nor write accesses f_pos
    ///////////////////////////////////////////////////////////
    
    @ fops4 depends on !fops1 && !fops2 && !fops3 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier read_no_fpos.read_f;
    identifier write_no_fpos.write_f;
    @@
    // write fops use offset
    struct file_operations fops = {
    ...
     .write = write_f,
     .read = read_f,
    ...
    +.llseek = noop_llseek, /* read and write both use no f_pos */
    };
    
    @ depends on has_write && !has_read && !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier write_no_fpos.write_f;
    @@
    struct file_operations fops = {
    ... .write = write_f, ...
    +.llseek = noop_llseek, /* write uses no f_pos */
    };
    
    @ depends on has_read && !has_write && !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier read_no_fpos.read_f;
    @@
    struct file_operations fops = {
    ... .read = read_f, ...
    +.llseek = noop_llseek, /* read uses no f_pos */
    };
    
    @ depends on !has_read && !has_write && !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    @@
    struct file_operations fops = {
    ...
    +.llseek = noop_llseek, /* no read or write fn */
    };
    ===== End semantic patch =====
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Julia Lawall <julia@diku.dk>
    Cc: Christoph Hellwig <hch@infradead.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index b22a899934cc..66f841b7fbd3 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -555,6 +555,7 @@ static ssize_t write_profile(struct file *file, const char __user *buf,
 static const struct file_operations proc_profile_operations = {
 	.read		= read_profile,
 	.write		= write_profile,
+	.llseek		= default_llseek,
 };
 
 #ifdef CONFIG_SMP

commit 3dd6b5fb436083fdd8af7ad2baa07db19deb73b2
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Wed May 26 14:45:04 2010 -0700

    numa: in-kernel profiling: use cpu_to_mem() for per cpu allocations
    
    In kernel profiling requires that we be able to allocate "local" memory
    for each cpu.  Use "cpu_to_mem()" instead of "cpu_to_node()" to support
    memoryless nodes.
    
    Depends on the "numa_mem_id()" patch.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Eric Whitney <eric.whitney@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 1e6a0d94ea63..b22a899934cc 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -365,7 +365,7 @@ static int __cpuinit profile_cpu_callback(struct notifier_block *info,
 	switch (action) {
 	case CPU_UP_PREPARE:
 	case CPU_UP_PREPARE_FROZEN:
-		node = cpu_to_node(cpu);
+		node = cpu_to_mem(cpu);
 		per_cpu(cpu_profile_flip, cpu) = 0;
 		if (!per_cpu(cpu_profile_hits, cpu)[1]) {
 			page = alloc_pages_exact_node(node,
@@ -567,7 +567,7 @@ static int create_hash_tables(void)
 	int cpu;
 
 	for_each_online_cpu(cpu) {
-		int node = cpu_to_node(cpu);
+		int node = cpu_to_mem(cpu);
 		struct page *page;
 
 		page = alloc_pages_exact_node(node,

commit 80b5184cc537718122e036afe7e62d202b70d077
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Wed May 26 14:43:32 2010 -0700

    kernel/: convert cpu notifier to return encapsulate errno value
    
    By the previous modification, the cpu notifier can return encapsulate
    errno value.  This converts the cpu notifiers for kernel/*.c
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index dfadc5b729f1..1e6a0d94ea63 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -372,7 +372,7 @@ static int __cpuinit profile_cpu_callback(struct notifier_block *info,
 					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
-				return NOTIFY_BAD;
+				return notifier_from_errno(-ENOMEM);
 			per_cpu(cpu_profile_hits, cpu)[1] = page_address(page);
 		}
 		if (!per_cpu(cpu_profile_hits, cpu)[0]) {
@@ -388,7 +388,7 @@ static int __cpuinit profile_cpu_callback(struct notifier_block *info,
 		page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
 		per_cpu(cpu_profile_hits, cpu)[1] = NULL;
 		__free_page(page);
-		return NOTIFY_BAD;
+		return notifier_from_errno(-ENOMEM);
 	case CPU_ONLINE:
 	case CPU_ONLINE_FROZEN:
 		if (prof_cpu_mask != NULL)

commit 16a2164bb03612efe79a76c73da6da44445b9287
Author: Hugh Dickins <hughd@google.com>
Date:   Fri May 14 19:44:10 2010 -0700

    profile: fix stats and data leakage
    
    If the kernel is large or the profiling step small, /proc/profile
    leaks data and readprofile shows silly stats, until readprofile -r
    has reset the buffer: clear the prof_buffer when it is vmalloc()ed.
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index a55d3a367ae8..dfadc5b729f1 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -127,8 +127,10 @@ int __ref profile_init(void)
 		return 0;
 
 	prof_buffer = vmalloc(buffer_bytes);
-	if (prof_buffer)
+	if (prof_buffer) {
+		memset(prof_buffer, 0, buffer_bytes);
 		return 0;
+	}
 
 	free_cpumask_var(prof_cpu_mask);
 	return -ENOMEM;

commit 583a22e7c154dc0a3938db522696b4bc7f098f59
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Sep 18 12:57:09 2009 -0700

    kernel/profile.c: Switch /proc/irq/prof_cpu_mask to seq_file
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/profile.c b/kernel/profile.c
index 419250ebec4d..a55d3a367ae8 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -442,48 +442,51 @@ void profile_tick(int type)
 
 #ifdef CONFIG_PROC_FS
 #include <linux/proc_fs.h>
+#include <linux/seq_file.h>
 #include <asm/uaccess.h>
 
-static int prof_cpu_mask_read_proc(char *page, char **start, off_t off,
-			int count, int *eof, void *data)
+static int prof_cpu_mask_proc_show(struct seq_file *m, void *v)
 {
-	int len = cpumask_scnprintf(page, count, data);
-	if (count - len < 2)
-		return -EINVAL;
-	len += sprintf(page + len, "\n");
-	return len;
+	seq_cpumask(m, prof_cpu_mask);
+	seq_putc(m, '\n');
+	return 0;
 }
 
-static int prof_cpu_mask_write_proc(struct file *file,
-	const char __user *buffer,  unsigned long count, void *data)
+static int prof_cpu_mask_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, prof_cpu_mask_proc_show, NULL);
+}
+
+static ssize_t prof_cpu_mask_proc_write(struct file *file,
+	const char __user *buffer, size_t count, loff_t *pos)
 {
-	struct cpumask *mask = data;
-	unsigned long full_count = count, err;
 	cpumask_var_t new_value;
+	int err;
 
 	if (!alloc_cpumask_var(&new_value, GFP_KERNEL))
 		return -ENOMEM;
 
 	err = cpumask_parse_user(buffer, count, new_value);
 	if (!err) {
-		cpumask_copy(mask, new_value);
-		err = full_count;
+		cpumask_copy(prof_cpu_mask, new_value);
+		err = count;
 	}
 	free_cpumask_var(new_value);
 	return err;
 }
 
+static const struct file_operations prof_cpu_mask_proc_fops = {
+	.open		= prof_cpu_mask_proc_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+	.write		= prof_cpu_mask_proc_write,
+};
+
 void create_prof_cpu_mask(struct proc_dir_entry *root_irq_dir)
 {
-	struct proc_dir_entry *entry;
-
 	/* create /proc/irq/prof_cpu_mask */
-	entry = create_proc_entry("prof_cpu_mask", 0600, root_irq_dir);
-	if (!entry)
-		return;
-	entry->data = prof_cpu_mask;
-	entry->read_proc = prof_cpu_mask_read_proc;
-	entry->write_proc = prof_cpu_mask_write_proc;
+	proc_create("prof_cpu_mask", 0600, root_irq_dir, &prof_cpu_mask_proc_fops);
 }
 
 /*

commit b62f495dad04fa94b5083aec638ff3072bccaaca
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed Jul 29 15:04:09 2009 -0700

    profile: suppress warning about large allocations when profile=1 is specified
    
    When profile= is used, a large buffer is allocated early at boot.  This
    can be larger than what the page allocator can provide so it prints a
    warning.  However, the caller is able to handle the situation so this
    patch suppresses the warning.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 69911b5745eb..419250ebec4d 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -117,11 +117,12 @@ int __ref profile_init(void)
 
 	cpumask_copy(prof_cpu_mask, cpu_possible_mask);
 
-	prof_buffer = kzalloc(buffer_bytes, GFP_KERNEL);
+	prof_buffer = kzalloc(buffer_bytes, GFP_KERNEL|__GFP_NOWARN);
 	if (prof_buffer)
 		return 0;
 
-	prof_buffer = alloc_pages_exact(buffer_bytes, GFP_KERNEL|__GFP_ZERO);
+	prof_buffer = alloc_pages_exact(buffer_bytes,
+					GFP_KERNEL|__GFP_ZERO|__GFP_NOWARN);
 	if (prof_buffer)
 		return 0;
 

commit 6484eb3e2a81807722c5f28efef94d8338b7b996
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Tue Jun 16 15:31:54 2009 -0700

    page allocator: do not check NUMA node ID when the caller knows the node is valid
    
    Callers of alloc_pages_node() can optionally specify -1 as a node to mean
    "allocate from the current node".  However, a number of the callers in
    fast paths know for a fact their node is valid.  To avoid a comparison and
    branch, this patch adds alloc_pages_exact_node() that only checks the nid
    with VM_BUG_ON().  Callers that know their node is valid are then
    converted.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Paul Mundt <lethal@linux-sh.org>      [for the SLOB NUMA bits]
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 28cf26ad2d24..69911b5745eb 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -365,7 +365,7 @@ static int __cpuinit profile_cpu_callback(struct notifier_block *info,
 		node = cpu_to_node(cpu);
 		per_cpu(cpu_profile_flip, cpu) = 0;
 		if (!per_cpu(cpu_profile_hits, cpu)[1]) {
-			page = alloc_pages_node(node,
+			page = alloc_pages_exact_node(node,
 					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
@@ -373,7 +373,7 @@ static int __cpuinit profile_cpu_callback(struct notifier_block *info,
 			per_cpu(cpu_profile_hits, cpu)[1] = page_address(page);
 		}
 		if (!per_cpu(cpu_profile_hits, cpu)[0]) {
-			page = alloc_pages_node(node,
+			page = alloc_pages_exact_node(node,
 					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
@@ -564,14 +564,14 @@ static int create_hash_tables(void)
 		int node = cpu_to_node(cpu);
 		struct page *page;
 
-		page = alloc_pages_node(node,
+		page = alloc_pages_exact_node(node,
 				GFP_KERNEL | __GFP_ZERO | GFP_THISNODE,
 				0);
 		if (!page)
 			goto out_cleanup;
 		per_cpu(cpu_profile_hits, cpu)[1]
 				= (struct profile_hit *)page_address(page);
-		page = alloc_pages_node(node,
+		page = alloc_pages_exact_node(node,
 				GFP_KERNEL | __GFP_ZERO | GFP_THISNODE,
 				0);
 		if (!page)

commit 38c7fed2f5ffee17e1fa3e0f78b0e1bf43d52d13
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon May 25 15:10:58 2009 +0300

    x86: remove some alloc_bootmem_cpumask_var calling
    
    Now that we set up the slab allocator earlier, we can get rid of some
    alloc_bootmem_cpumask_var() calls in boot code.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/kernel/profile.c b/kernel/profile.c
index 7724e0409bae..28cf26ad2d24 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -111,12 +111,6 @@ int __ref profile_init(void)
 	/* only text is profiled */
 	prof_len = (_etext - _stext) >> prof_shift;
 	buffer_bytes = prof_len*sizeof(atomic_t);
-	if (!slab_is_available()) {
-		prof_buffer = alloc_bootmem(buffer_bytes);
-		alloc_bootmem_cpumask_var(&prof_cpu_mask);
-		cpumask_copy(prof_cpu_mask, cpu_possible_mask);
-		return 0;
-	}
 
 	if (!alloc_cpumask_var(&prof_cpu_mask, GFP_KERNEL))
 		return -ENOMEM;

commit acd895795d35d7c6405f20301a846d16998795ec
Author: Hugh Dickins <hugh@veritas.com>
Date:   Mon Feb 9 19:20:50 2009 +0000

    profiling: fix broken profiling regression
    
    Impact: fix broken /proc/profile on UP machines
    
    Commit c309b917cab55799ea489d7b5f1b77025d9f8462 "cpumask: convert
    kernel/profile.c" broke profiling.  prof_cpu_mask was previously
    initialized to CPU_MASK_ALL, but left uninitialized in that commit.
    We need to copy cpu_possible_mask (cpu_online_mask is not enough).
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/profile.c b/kernel/profile.c
index 784933acf5b8..7724e0409bae 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -114,12 +114,15 @@ int __ref profile_init(void)
 	if (!slab_is_available()) {
 		prof_buffer = alloc_bootmem(buffer_bytes);
 		alloc_bootmem_cpumask_var(&prof_cpu_mask);
+		cpumask_copy(prof_cpu_mask, cpu_possible_mask);
 		return 0;
 	}
 
 	if (!alloc_cpumask_var(&prof_cpu_mask, GFP_KERNEL))
 		return -ENOMEM;
 
+	cpumask_copy(prof_cpu_mask, cpu_possible_mask);
+
 	prof_buffer = kzalloc(buffer_bytes, GFP_KERNEL);
 	if (prof_buffer)
 		return 0;

commit 26e5438e4b77f04a51870f9415ffed68004fac1d
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Jan 6 14:41:10 2009 -0800

    profile: don't include <asm/ptrace.h> twice.
    
    Currently, kernel/profile.c include <asm/ptrace.h> twice.  It can be
    removed.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index d18e2d2654f2..784933acf5b8 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -445,7 +445,6 @@ void profile_tick(int type)
 #ifdef CONFIG_PROC_FS
 #include <linux/proc_fs.h>
 #include <asm/uaccess.h>
-#include <asm/ptrace.h>
 
 static int prof_cpu_mask_read_proc(char *page, char **start, off_t off,
 			int count, int *eof, void *data)

commit c309b917cab55799ea489d7b5f1b77025d9f8462
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Jan 1 10:12:27 2009 +1030

    cpumask: convert kernel/profile.c
    
    Impact: Reduce kernel memory usage, use new cpumask API.
    
    Avoid a static cpumask_t for prof_cpu_mask, and an on-stack cpumask_t
    in prof_cpu_mask_write_proc.  Both become cpumask_var_t.
    
    prof_cpu_mask is only allocated when profiling is on, but the NULL
    checks are optimized out by gcc for the !CPUMASK_OFFSTACK case.
    
    Also removed some strange and unnecessary casts.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/kernel/profile.c b/kernel/profile.c
index 4cb7d68fed82..d18e2d2654f2 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -45,7 +45,7 @@ static unsigned long prof_len, prof_shift;
 int prof_on __read_mostly;
 EXPORT_SYMBOL_GPL(prof_on);
 
-static cpumask_t prof_cpu_mask = CPU_MASK_ALL;
+static cpumask_var_t prof_cpu_mask;
 #ifdef CONFIG_SMP
 static DEFINE_PER_CPU(struct profile_hit *[2], cpu_profile_hits);
 static DEFINE_PER_CPU(int, cpu_profile_flip);
@@ -113,9 +113,13 @@ int __ref profile_init(void)
 	buffer_bytes = prof_len*sizeof(atomic_t);
 	if (!slab_is_available()) {
 		prof_buffer = alloc_bootmem(buffer_bytes);
+		alloc_bootmem_cpumask_var(&prof_cpu_mask);
 		return 0;
 	}
 
+	if (!alloc_cpumask_var(&prof_cpu_mask, GFP_KERNEL))
+		return -ENOMEM;
+
 	prof_buffer = kzalloc(buffer_bytes, GFP_KERNEL);
 	if (prof_buffer)
 		return 0;
@@ -128,6 +132,7 @@ int __ref profile_init(void)
 	if (prof_buffer)
 		return 0;
 
+	free_cpumask_var(prof_cpu_mask);
 	return -ENOMEM;
 }
 
@@ -386,13 +391,15 @@ static int __cpuinit profile_cpu_callback(struct notifier_block *info,
 		return NOTIFY_BAD;
 	case CPU_ONLINE:
 	case CPU_ONLINE_FROZEN:
-		cpu_set(cpu, prof_cpu_mask);
+		if (prof_cpu_mask != NULL)
+			cpumask_set_cpu(cpu, prof_cpu_mask);
 		break;
 	case CPU_UP_CANCELED:
 	case CPU_UP_CANCELED_FROZEN:
 	case CPU_DEAD:
 	case CPU_DEAD_FROZEN:
-		cpu_clear(cpu, prof_cpu_mask);
+		if (prof_cpu_mask != NULL)
+			cpumask_clear_cpu(cpu, prof_cpu_mask);
 		if (per_cpu(cpu_profile_hits, cpu)[0]) {
 			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[0]);
 			per_cpu(cpu_profile_hits, cpu)[0] = NULL;
@@ -430,7 +437,8 @@ void profile_tick(int type)
 
 	if (type == CPU_PROFILING && timer_hook)
 		timer_hook(regs);
-	if (!user_mode(regs) && cpu_isset(smp_processor_id(), prof_cpu_mask))
+	if (!user_mode(regs) && prof_cpu_mask != NULL &&
+	    cpumask_test_cpu(smp_processor_id(), prof_cpu_mask))
 		profile_hit(type, (void *)profile_pc(regs));
 }
 
@@ -442,7 +450,7 @@ void profile_tick(int type)
 static int prof_cpu_mask_read_proc(char *page, char **start, off_t off,
 			int count, int *eof, void *data)
 {
-	int len = cpumask_scnprintf(page, count, (cpumask_t *)data);
+	int len = cpumask_scnprintf(page, count, data);
 	if (count - len < 2)
 		return -EINVAL;
 	len += sprintf(page + len, "\n");
@@ -452,16 +460,20 @@ static int prof_cpu_mask_read_proc(char *page, char **start, off_t off,
 static int prof_cpu_mask_write_proc(struct file *file,
 	const char __user *buffer,  unsigned long count, void *data)
 {
-	cpumask_t *mask = (cpumask_t *)data;
+	struct cpumask *mask = data;
 	unsigned long full_count = count, err;
-	cpumask_t new_value;
+	cpumask_var_t new_value;
 
-	err = cpumask_parse_user(buffer, count, &new_value);
-	if (err)
-		return err;
+	if (!alloc_cpumask_var(&new_value, GFP_KERNEL))
+		return -ENOMEM;
 
-	*mask = new_value;
-	return full_count;
+	err = cpumask_parse_user(buffer, count, new_value);
+	if (!err) {
+		cpumask_copy(mask, new_value);
+		err = full_count;
+	}
+	free_cpumask_var(new_value);
+	return err;
 }
 
 void create_prof_cpu_mask(struct proc_dir_entry *root_irq_dir)
@@ -472,7 +484,7 @@ void create_prof_cpu_mask(struct proc_dir_entry *root_irq_dir)
 	entry = create_proc_entry("prof_cpu_mask", 0600, root_irq_dir);
 	if (!entry)
 		return;
-	entry->data = (void *)&prof_cpu_mask;
+	entry->data = prof_cpu_mask;
 	entry->read_proc = prof_cpu_mask_read_proc;
 	entry->write_proc = prof_cpu_mask_write_proc;
 }

commit 33edcf133ba93ecba2e4b6472e97b689895d805c
Merge: be4d638c1597 3c92ec8ae91e
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Tue Dec 30 08:02:35 2008 +1030

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux-2.6

commit 29c0177e6a4ac094302bed54a1d4bbb6b740a9ef
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Dec 13 21:20:25 2008 +1030

    cpumask: change cpumask_scnprintf, cpumask_parse_user, cpulist_parse, and cpulist_scnprintf to take pointers.
    
    Impact: change calling convention of existing cpumask APIs
    
    Most cpumask functions started with cpus_: these have been replaced by
    cpumask_ ones which take struct cpumask pointers as expected.
    
    These four functions don't have good replacement names; fortunately
    they're rarely used, so we just change them over.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: paulus@samba.org
    Cc: mingo@redhat.com
    Cc: tony.luck@intel.com
    Cc: ralf@linux-mips.org
    Cc: Greg Kroah-Hartman <gregkh@suse.de>
    Cc: cl@linux-foundation.org
    Cc: srostedt@redhat.com

diff --git a/kernel/profile.c b/kernel/profile.c
index dc41827fbfee..7d620dfdde59 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -442,7 +442,7 @@ void profile_tick(int type)
 static int prof_cpu_mask_read_proc(char *page, char **start, off_t off,
 			int count, int *eof, void *data)
 {
-	int len = cpumask_scnprintf(page, count, *(cpumask_t *)data);
+	int len = cpumask_scnprintf(page, count, (cpumask_t *)data);
 	if (count - len < 2)
 		return -EINVAL;
 	len += sprintf(page + len, "\n");
@@ -456,7 +456,7 @@ static int prof_cpu_mask_write_proc(struct file *file,
 	unsigned long full_count = count, err;
 	cpumask_t new_value;
 
-	err = cpumask_parse_user(buffer, count, new_value);
+	err = cpumask_parse_user(buffer, count, &new_value);
 	if (err)
 		return err;
 

commit b8307db2477f9c551e54e0c7b643ea349a3349cd
Merge: f0461d0146ee 061e41fdb504
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 4 09:07:19 2008 +0100

    Merge commit 'v2.6.28-rc7' into tracing/core

commit 8419641450edc838a6ce7cdf0f99d262bf0af2d5
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Sat Nov 22 17:36:44 2008 +0000

    cpuinit fixes in kernel/*
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 5b7d1ac7124c..dc41827fbfee 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -351,7 +351,7 @@ void profile_hits(int type, void *__pc, unsigned int nr_hits)
 	put_cpu();
 }
 
-static int __devinit profile_cpu_callback(struct notifier_block *info,
+static int __cpuinit profile_cpu_callback(struct notifier_block *info,
 					unsigned long action, void *__cpu)
 {
 	int node, cpu = (unsigned long)__cpu;
@@ -596,7 +596,7 @@ static int create_hash_tables(void)
 #define create_hash_tables()			({ 0; })
 #endif
 
-int create_proc_profile(void)
+int __ref create_proc_profile(void) /* false positive from hotcpu_notifier */
 {
 	struct proc_dir_entry *entry;
 

commit 60a515132086b2c28a8141d873297fdf7a180ca7
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Nov 18 22:20:10 2008 -0800

    profiling: clean up profile_nop()
    
    Impact: cleanup
    
    No point in inlining this.
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/profile.c b/kernel/profile.c
index 5b7d1ac7124c..7f93a5042d3b 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -544,7 +544,7 @@ static const struct file_operations proc_profile_operations = {
 };
 
 #ifdef CONFIG_SMP
-static inline void profile_nop(void *unused)
+static void profile_nop(void *unused)
 {
 }
 

commit e270219f4372b58bd3eeac12bd9f7edc592b8f6b
Author: Rakib Mullick <rakib.mullick@gmail.com>
Date:   Tue Nov 18 10:15:24 2008 +0600

    kernel/profile.c: fix section mismatch warning
    
    Impact: fix section mismatch warning in kernel/profile.c
    
    Here, profile_nop function has been called from a non-init function
    create_hash_tables(void). Which generetes a section mismatch warning.
    Previously, create_hash_tables(void) was a init function. So, removing
    __init from create_hash_tables(void) requires profile_nop to be
    non-init.
    
    This patch makes profile_nop function inline and fixes the
    following warning:
    
     WARNING: vmlinux.o(.text+0x6ebb6): Section mismatch in reference from
     the function create_hash_tables() to the function
     .init.text:profile_nop()
     The function create_hash_tables() references
     the function __init profile_nop().
     This is often because create_hash_tables lacks a __init
     annotation or the annotation of profile_nop is wrong.
    
    Signed-off-by: Rakib Mullick <rakib.mullick@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/profile.c b/kernel/profile.c
index 9830a037d8db..5b7d1ac7124c 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -544,7 +544,7 @@ static const struct file_operations proc_profile_operations = {
 };
 
 #ifdef CONFIG_SMP
-static void __init profile_nop(void *unused)
+static inline void profile_nop(void *unused)
 {
 }
 

commit ce05fcc30ea41c85f9d50bee1ce289f7cb7fb223
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Wed Oct 29 14:01:07 2008 -0700

    kernel/profile: fix profile_init() section mismatch
    
    profile_init() calls in to alloc_bootmem() on early initialization.  While
    alloc_bootmem() is __init, the reference itself is safe in that it is
    tucked below a !slab_is_available() check.  So, flag profile_init() as
    __ref.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Cc: Dave Hansen <dave@linux.vnet.ibm.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index a9e422df6bf6..9830a037d8db 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -102,7 +102,7 @@ int profile_setup(char *str)
 __setup("profile=", profile_setup);
 
 
-int profile_init(void)
+int __ref profile_init(void)
 {
 	int buffer_bytes;
 	if (!prof_on)

commit 22b8ce94708f7cdf0b04965c6f7443dfd374c35c
Author: Dave Hansen <dave@linux.vnet.ibm.com>
Date:   Wed Oct 15 22:01:46 2008 -0700

    profiling: dynamically enable readprofile at runtime
    
    Way too often, I have a machine that exhibits some kind of crappy
    behavior.  The CPU looks wedged in the kernel or it is spending way too
    much system time and I wonder what is responsible.
    
    I try to run readprofile.  But, of course, Ubuntu doesn't enable it by
    default.  Dang!
    
    The reason we boot-time enable it is that it takes a big bufffer that we
    generally can only bootmem alloc.  But, does it hurt to at least try and
    runtime-alloc it?
    
    To use:
    echo 2 > /sys/kernel/profile
    
    Then run readprofile like normal.
    
    This should fix the compile issue with allmodconfig.  I've compile-tested
    on a bunch more configs now including a few more architectures.
    
    Signed-off-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index cd26bed4cc26..a9e422df6bf6 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -22,6 +22,8 @@
 #include <linux/cpu.h>
 #include <linux/highmem.h>
 #include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
 #include <asm/sections.h>
 #include <asm/irq_regs.h>
 #include <asm/ptrace.h>
@@ -50,11 +52,11 @@ static DEFINE_PER_CPU(int, cpu_profile_flip);
 static DEFINE_MUTEX(profile_flip_mutex);
 #endif /* CONFIG_SMP */
 
-static int __init profile_setup(char *str)
+int profile_setup(char *str)
 {
-	static char __initdata schedstr[] = "schedule";
-	static char __initdata sleepstr[] = "sleep";
-	static char __initdata kvmstr[] = "kvm";
+	static char schedstr[] = "schedule";
+	static char sleepstr[] = "sleep";
+	static char kvmstr[] = "kvm";
 	int par;
 
 	if (!strncmp(str, sleepstr, strlen(sleepstr))) {
@@ -100,14 +102,33 @@ static int __init profile_setup(char *str)
 __setup("profile=", profile_setup);
 
 
-void __init profile_init(void)
+int profile_init(void)
 {
+	int buffer_bytes;
 	if (!prof_on)
-		return;
+		return 0;
 
 	/* only text is profiled */
 	prof_len = (_etext - _stext) >> prof_shift;
-	prof_buffer = alloc_bootmem(prof_len*sizeof(atomic_t));
+	buffer_bytes = prof_len*sizeof(atomic_t);
+	if (!slab_is_available()) {
+		prof_buffer = alloc_bootmem(buffer_bytes);
+		return 0;
+	}
+
+	prof_buffer = kzalloc(buffer_bytes, GFP_KERNEL);
+	if (prof_buffer)
+		return 0;
+
+	prof_buffer = alloc_pages_exact(buffer_bytes, GFP_KERNEL|__GFP_ZERO);
+	if (prof_buffer)
+		return 0;
+
+	prof_buffer = vmalloc(buffer_bytes);
+	if (prof_buffer)
+		return 0;
+
+	return -ENOMEM;
 }
 
 /* Profile event notifications */
@@ -527,7 +548,7 @@ static void __init profile_nop(void *unused)
 {
 }
 
-static int __init create_hash_tables(void)
+static int create_hash_tables(void)
 {
 	int cpu;
 
@@ -575,14 +596,14 @@ static int __init create_hash_tables(void)
 #define create_hash_tables()			({ 0; })
 #endif
 
-static int __init create_proc_profile(void)
+int create_proc_profile(void)
 {
 	struct proc_dir_entry *entry;
 
 	if (!prof_on)
 		return 0;
 	if (create_hash_tables())
-		return -1;
+		return -ENOMEM;
 	entry = proc_create("profile", S_IWUSR | S_IRUGO,
 			    NULL, &proc_profile_operations);
 	if (!entry)

commit b03f6489f9f27dc519a4c60ebf39cc7b8a58eae7
Author: Adrian Bunk <bunk@kernel.org>
Date:   Fri Jul 25 01:45:35 2008 -0700

    build kernel/profile.o only when requested
    
    Build kernel/profile.o only if CONFIG_PROFILING is enabled.
    
    This makes CONFIG_PROFILING=n kernels smaller.
    
    As a bonus, some profile_tick() calls and one branch from schedule() are
    now eliminated with CONFIG_PROFILING=n (but I doubt these are
    measurable effects).
    
    This patch changes the effects of CONFIG_PROFILING=n, but I don't think
    having more than two choices would be the better choice.
    
    This patch also adds the name of the first parameter to the prototypes
    of profile_{hits,tick}() since I anyway had to add them for the dummy
    functions.
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 58926411eb2a..cd26bed4cc26 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -112,8 +112,6 @@ void __init profile_init(void)
 
 /* Profile event notifications */
 
-#ifdef CONFIG_PROFILING
-
 static BLOCKING_NOTIFIER_HEAD(task_exit_notifier);
 static ATOMIC_NOTIFIER_HEAD(task_free_notifier);
 static BLOCKING_NOTIFIER_HEAD(munmap_notifier);
@@ -203,8 +201,6 @@ void unregister_timer_hook(int (*hook)(struct pt_regs *))
 }
 EXPORT_SYMBOL_GPL(unregister_timer_hook);
 
-#endif /* CONFIG_PROFILING */
-
 
 #ifdef CONFIG_SMP
 /*

commit 15c8b6c1aaaf1c4edd67e2f02e4d8e1bd1a51c0d
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri May 9 09:39:44 2008 +0200

    on_each_cpu(): kill unused 'retry' parameter
    
    It's not even passed on to smp_call_function() anymore, since that
    was removed. So kill it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/profile.c b/kernel/profile.c
index ae7ead82cbc9..58926411eb2a 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -252,7 +252,7 @@ static void profile_flip_buffers(void)
 	mutex_lock(&profile_flip_mutex);
 	j = per_cpu(cpu_profile_flip, get_cpu());
 	put_cpu();
-	on_each_cpu(__profile_flip_buffers, NULL, 0, 1);
+	on_each_cpu(__profile_flip_buffers, NULL, 1);
 	for_each_online_cpu(cpu) {
 		struct profile_hit *hits = per_cpu(cpu_profile_hits, cpu)[j];
 		for (i = 0; i < NR_PROFILE_HIT; ++i) {
@@ -275,7 +275,7 @@ static void profile_discard_flip_buffers(void)
 	mutex_lock(&profile_flip_mutex);
 	i = per_cpu(cpu_profile_flip, get_cpu());
 	put_cpu();
-	on_each_cpu(__profile_flip_buffers, NULL, 0, 1);
+	on_each_cpu(__profile_flip_buffers, NULL, 1);
 	for_each_online_cpu(cpu) {
 		struct profile_hit *hits = per_cpu(cpu_profile_hits, cpu)[i];
 		memset(hits, 0, NR_PROFILE_HIT*sizeof(struct profile_hit));
@@ -558,7 +558,7 @@ static int __init create_hash_tables(void)
 out_cleanup:
 	prof_on = 0;
 	smp_mb();
-	on_each_cpu(profile_nop, NULL, 0, 1);
+	on_each_cpu(profile_nop, NULL, 1);
 	for_each_online_cpu(cpu) {
 		struct page *page;
 

commit c33fff0afbef4f0467c99e3f47ee7e98ae78c77e
Author: Denis V. Lunev <den@openvz.org>
Date:   Tue Apr 29 01:02:31 2008 -0700

    kernel: use non-racy method for proc entries creation
    
    Use proc_create()/proc_create_data() to make sure that ->proc_fops and ->data
    be setup before gluing PDE to main tree.
    
    Signed-off-by: Denis V. Lunev <den@openvz.org>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 606d7387265c..ae7ead82cbc9 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -587,10 +587,10 @@ static int __init create_proc_profile(void)
 		return 0;
 	if (create_hash_tables())
 		return -1;
-	entry = create_proc_entry("profile", S_IWUSR | S_IRUGO, NULL);
+	entry = proc_create("profile", S_IWUSR | S_IRUGO,
+			    NULL, &proc_profile_operations);
 	if (!entry)
 		return 0;
-	entry->proc_fops = &proc_profile_operations;
 	entry->size = (1+prof_len) * sizeof(atomic_t);
 	hotcpu_notifier(profile_cpu_callback, 0);
 	return 0;

commit a6550207538619bc9b90bac2e1d5e54902a432ad
Author: Matthew Wilcox <matthew@wil.cx>
Date:   Tue Feb 26 10:47:18 2008 -0500

    kernel: Remove unnecessary inclusions of asm/semaphore.h
    
    None of these files use any of the functionality promised by
    asm/semaphore.h.
    
    Signed-off-by: Matthew Wilcox <willy@linux.intel.com>

diff --git a/kernel/profile.c b/kernel/profile.c
index 3b7a1b055122..606d7387265c 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -23,7 +23,6 @@
 #include <linux/highmem.h>
 #include <linux/mutex.h>
 #include <asm/sections.h>
-#include <asm/semaphore.h>
 #include <asm/irq_regs.h>
 #include <asm/ptrace.h>
 

commit f8db694e46ac30c171eb3537aba677a5671cda02
Author: Jesper Juhl <jesper.juhl@gmail.com>
Date:   Fri Feb 8 04:20:24 2008 -0800

    Nuke a duplicate include from profile.c
    
    Remove duplicate inclusion of linux/profile.h from kernel/profile.c
    
    Signed-off-by: Jesper Juhl <jesper.juhl@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index e64c2da11c0f..3b7a1b055122 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -20,7 +20,6 @@
 #include <linux/mm.h>
 #include <linux/cpumask.h>
 #include <linux/cpu.h>
-#include <linux/profile.h>
 #include <linux/highmem.h>
 #include <linux/mutex.h>
 #include <asm/sections.h>

commit 1ad82fd547c716f96e544b477e0bdbfa2d647529
Author: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com>
Date:   Fri Jan 25 21:08:33 2008 +0100

    debug: clean up kernel/profile.c
    
     Before:
     total: 25 errors, 13 warnings, 602 lines checked
    
     After:
     total: 0 errors, 2 warnings, 601 lines checked
    
    No code changed:
    
    kernel/profile.o:
       text    data     bss     dec     hex filename
       3048     236      24    3308     cec profile.o.before
       3048     236      24    3308     cec profile.o.after
     md5:
       2501d64748a4d350dffb11203e2a5182  profile.o.before.asm
       2501d64748a4d350dffb11203e2a5182  profile.o.after.asm
    
    Signed-off-by: Paolo Ciarrocchi <paolo.ciarrocchi@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/profile.c b/kernel/profile.c
index 5e95330e5120..e64c2da11c0f 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -52,7 +52,7 @@ static DEFINE_PER_CPU(int, cpu_profile_flip);
 static DEFINE_MUTEX(profile_flip_mutex);
 #endif /* CONFIG_SMP */
 
-static int __init profile_setup(char * str)
+static int __init profile_setup(char *str)
 {
 	static char __initdata schedstr[] = "schedule";
 	static char __initdata sleepstr[] = "sleep";
@@ -104,28 +104,28 @@ __setup("profile=", profile_setup);
 
 void __init profile_init(void)
 {
-	if (!prof_on) 
+	if (!prof_on)
 		return;
- 
+
 	/* only text is profiled */
 	prof_len = (_etext - _stext) >> prof_shift;
 	prof_buffer = alloc_bootmem(prof_len*sizeof(atomic_t));
 }
 
 /* Profile event notifications */
- 
+
 #ifdef CONFIG_PROFILING
- 
+
 static BLOCKING_NOTIFIER_HEAD(task_exit_notifier);
 static ATOMIC_NOTIFIER_HEAD(task_free_notifier);
 static BLOCKING_NOTIFIER_HEAD(munmap_notifier);
- 
-void profile_task_exit(struct task_struct * task)
+
+void profile_task_exit(struct task_struct *task)
 {
 	blocking_notifier_call_chain(&task_exit_notifier, 0, task);
 }
- 
-int profile_handoff_task(struct task_struct * task)
+
+int profile_handoff_task(struct task_struct *task)
 {
 	int ret;
 	ret = atomic_notifier_call_chain(&task_free_notifier, 0, task);
@@ -137,52 +137,55 @@ void profile_munmap(unsigned long addr)
 	blocking_notifier_call_chain(&munmap_notifier, 0, (void *)addr);
 }
 
-int task_handoff_register(struct notifier_block * n)
+int task_handoff_register(struct notifier_block *n)
 {
 	return atomic_notifier_chain_register(&task_free_notifier, n);
 }
+EXPORT_SYMBOL_GPL(task_handoff_register);
 
-int task_handoff_unregister(struct notifier_block * n)
+int task_handoff_unregister(struct notifier_block *n)
 {
 	return atomic_notifier_chain_unregister(&task_free_notifier, n);
 }
+EXPORT_SYMBOL_GPL(task_handoff_unregister);
 
-int profile_event_register(enum profile_type type, struct notifier_block * n)
+int profile_event_register(enum profile_type type, struct notifier_block *n)
 {
 	int err = -EINVAL;
- 
+
 	switch (type) {
-		case PROFILE_TASK_EXIT:
-			err = blocking_notifier_chain_register(
-					&task_exit_notifier, n);
-			break;
-		case PROFILE_MUNMAP:
-			err = blocking_notifier_chain_register(
-					&munmap_notifier, n);
-			break;
+	case PROFILE_TASK_EXIT:
+		err = blocking_notifier_chain_register(
+				&task_exit_notifier, n);
+		break;
+	case PROFILE_MUNMAP:
+		err = blocking_notifier_chain_register(
+				&munmap_notifier, n);
+		break;
 	}
- 
+
 	return err;
 }
+EXPORT_SYMBOL_GPL(profile_event_register);
 
- 
-int profile_event_unregister(enum profile_type type, struct notifier_block * n)
+int profile_event_unregister(enum profile_type type, struct notifier_block *n)
 {
 	int err = -EINVAL;
- 
+
 	switch (type) {
-		case PROFILE_TASK_EXIT:
-			err = blocking_notifier_chain_unregister(
-					&task_exit_notifier, n);
-			break;
-		case PROFILE_MUNMAP:
-			err = blocking_notifier_chain_unregister(
-					&munmap_notifier, n);
-			break;
+	case PROFILE_TASK_EXIT:
+		err = blocking_notifier_chain_unregister(
+				&task_exit_notifier, n);
+		break;
+	case PROFILE_MUNMAP:
+		err = blocking_notifier_chain_unregister(
+				&munmap_notifier, n);
+		break;
 	}
 
 	return err;
 }
+EXPORT_SYMBOL_GPL(profile_event_unregister);
 
 int register_timer_hook(int (*hook)(struct pt_regs *))
 {
@@ -191,6 +194,7 @@ int register_timer_hook(int (*hook)(struct pt_regs *))
 	timer_hook = hook;
 	return 0;
 }
+EXPORT_SYMBOL_GPL(register_timer_hook);
 
 void unregister_timer_hook(int (*hook)(struct pt_regs *))
 {
@@ -199,13 +203,7 @@ void unregister_timer_hook(int (*hook)(struct pt_regs *))
 	/* make sure all CPUs see the NULL hook */
 	synchronize_sched();  /* Allow ongoing interrupts to complete. */
 }
-
-EXPORT_SYMBOL_GPL(register_timer_hook);
 EXPORT_SYMBOL_GPL(unregister_timer_hook);
-EXPORT_SYMBOL_GPL(task_handoff_register);
-EXPORT_SYMBOL_GPL(task_handoff_unregister);
-EXPORT_SYMBOL_GPL(profile_event_register);
-EXPORT_SYMBOL_GPL(profile_event_unregister);
 
 #endif /* CONFIG_PROFILING */
 
@@ -366,7 +364,7 @@ static int __devinit profile_cpu_callback(struct notifier_block *info,
 			per_cpu(cpu_profile_hits, cpu)[0] = page_address(page);
 		}
 		break;
-	out_free:
+out_free:
 		page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
 		per_cpu(cpu_profile_hits, cpu)[1] = NULL;
 		__free_page(page);
@@ -409,7 +407,6 @@ void profile_hits(int type, void *__pc, unsigned int nr_hits)
 	atomic_add(nr_hits, &prof_buffer[min(pc, prof_len - 1)]);
 }
 #endif /* !CONFIG_SMP */
-
 EXPORT_SYMBOL_GPL(profile_hits);
 
 void profile_tick(int type)
@@ -427,7 +424,7 @@ void profile_tick(int type)
 #include <asm/uaccess.h>
 #include <asm/ptrace.h>
 
-static int prof_cpu_mask_read_proc (char *page, char **start, off_t off,
+static int prof_cpu_mask_read_proc(char *page, char **start, off_t off,
 			int count, int *eof, void *data)
 {
 	int len = cpumask_scnprintf(page, count, *(cpumask_t *)data);
@@ -437,8 +434,8 @@ static int prof_cpu_mask_read_proc (char *page, char **start, off_t off,
 	return len;
 }
 
-static int prof_cpu_mask_write_proc (struct file *file, const char __user *buffer,
-					unsigned long count, void *data)
+static int prof_cpu_mask_write_proc(struct file *file,
+	const char __user *buffer,  unsigned long count, void *data)
 {
 	cpumask_t *mask = (cpumask_t *)data;
 	unsigned long full_count = count, err;
@@ -457,7 +454,8 @@ void create_prof_cpu_mask(struct proc_dir_entry *root_irq_dir)
 	struct proc_dir_entry *entry;
 
 	/* create /proc/irq/prof_cpu_mask */
-	if (!(entry = create_proc_entry("prof_cpu_mask", 0600, root_irq_dir)))
+	entry = create_proc_entry("prof_cpu_mask", 0600, root_irq_dir);
+	if (!entry)
 		return;
 	entry->data = (void *)&prof_cpu_mask;
 	entry->read_proc = prof_cpu_mask_read_proc;
@@ -475,7 +473,7 @@ read_profile(struct file *file, char __user *buf, size_t count, loff_t *ppos)
 {
 	unsigned long p = *ppos;
 	ssize_t read;
-	char * pnt;
+	char *pnt;
 	unsigned int sample_step = 1 << prof_shift;
 
 	profile_flip_buffers();
@@ -486,12 +484,12 @@ read_profile(struct file *file, char __user *buf, size_t count, loff_t *ppos)
 	read = 0;
 
 	while (p < sizeof(unsigned int) && count > 0) {
-		if (put_user(*((char *)(&sample_step)+p),buf))
+		if (put_user(*((char *)(&sample_step)+p), buf))
 			return -EFAULT;
 		buf++; p++; count--; read++;
 	}
 	pnt = (char *)prof_buffer + p - sizeof(atomic_t);
-	if (copy_to_user(buf,(void *)pnt,count))
+	if (copy_to_user(buf, (void *)pnt, count))
 		return -EFAULT;
 	read += count;
 	*ppos += read;
@@ -508,7 +506,7 @@ static ssize_t write_profile(struct file *file, const char __user *buf,
 			     size_t count, loff_t *ppos)
 {
 #ifdef CONFIG_SMP
-	extern int setup_profiling_timer (unsigned int multiplier);
+	extern int setup_profiling_timer(unsigned int multiplier);
 
 	if (count == sizeof(int)) {
 		unsigned int multiplier;
@@ -591,7 +589,8 @@ static int __init create_proc_profile(void)
 		return 0;
 	if (create_hash_tables())
 		return -1;
-	if (!(entry = create_proc_entry("profile", S_IWUSR | S_IRUGO, NULL)))
+	entry = create_proc_entry("profile", S_IWUSR | S_IRUGO, NULL);
+	if (!entry)
 		return 0;
 	entry->proc_fops = &proc_profile_operations;
 	entry->size = (1+prof_len) * sizeof(atomic_t);

commit b3da2a73ff5a2953a4ad8ebbf0aa7e6965ff9de2
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed Oct 24 18:23:50 2007 +0200

    sched: document profile=sleep requiring CONFIG_SCHEDSTATS
    
    profile=sleep only works if CONFIG_SCHEDSTATS is set. This patch notes
    the limitation in Documentation/kernel-parameters.txt and prints a
    warning at boot-time if profile=sleep is used without CONFIG_SCHEDSTAT.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/profile.c b/kernel/profile.c
index 631b75c25d7e..5e95330e5120 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -60,6 +60,7 @@ static int __init profile_setup(char * str)
 	int par;
 
 	if (!strncmp(str, sleepstr, strlen(sleepstr))) {
+#ifdef CONFIG_SCHEDSTATS
 		prof_on = SLEEP_PROFILING;
 		if (str[strlen(sleepstr)] == ',')
 			str += strlen(sleepstr) + 1;
@@ -68,6 +69,10 @@ static int __init profile_setup(char * str)
 		printk(KERN_INFO
 			"kernel sleep profiling enabled (shift: %ld)\n",
 			prof_shift);
+#else
+		printk(KERN_WARNING
+			"kernel sleep profiling requires CONFIG_SCHEDSTATS\n");
+#endif /* CONFIG_SCHEDSTATS */
 	} else if (!strncmp(str, schedstr, strlen(schedstr))) {
 		prof_on = SCHED_PROFILING;
 		if (str[strlen(schedstr)] == ',')

commit b012d346c0e914f505d92f730ad1758f73e2953b
Author: Adrian Bunk <bunk@kernel.org>
Date:   Tue Oct 16 23:29:26 2007 -0700

    make kernel/profile.c:time_hook static
    
    {,un}register_timer_hook() is the API that should be used.
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 6f69bf792d96..631b75c25d7e 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -37,7 +37,7 @@ struct profile_hit {
 #define NR_PROFILE_GRP		(NR_PROFILE_HIT/PROFILE_GRPSZ)
 
 /* Oprofile timer tick hook */
-int (*timer_hook)(struct pt_regs *) __read_mostly;
+static int (*timer_hook)(struct pt_regs *) __read_mostly;
 
 static atomic_t *prof_buffer;
 static unsigned long prof_len, prof_shift;

commit 4199cfa02b982f4c739e8a6a304d6a40e1935d25
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 01:25:34 2007 -0700

    Memoryless nodes: Allow profiling data to fall back to other nodes
    
    Processors on memoryless nodes must be able to fall back to remote nodes in
    order to get a profiling buffer.  This may lead to excessive NUMA traffic but
    I think we should allow this rather than failing.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Acked-by: Nishanth Aravamudan <nacc@us.ibm.com>
    Acked-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Acked-by: Bob Picco <bob.picco@hp.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@skynet.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index cb1e37d2dac3..6f69bf792d96 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -346,7 +346,7 @@ static int __devinit profile_cpu_callback(struct notifier_block *info,
 		per_cpu(cpu_profile_flip, cpu) = 0;
 		if (!per_cpu(cpu_profile_hits, cpu)[1]) {
 			page = alloc_pages_node(node,
-					GFP_KERNEL | __GFP_ZERO | GFP_THISNODE,
+					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
 				return NOTIFY_BAD;
@@ -354,7 +354,7 @@ static int __devinit profile_cpu_callback(struct notifier_block *info,
 		}
 		if (!per_cpu(cpu_profile_hits, cpu)[0]) {
 			page = alloc_pages_node(node,
-					GFP_KERNEL | __GFP_ZERO | GFP_THISNODE,
+					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
 				goto out_free;

commit cd5bfea278987ebfe60f3ff92a01696b17c4f978
Author: Peter Chubb <peterc@gelato.unsw.edu.au>
Date:   Fri Aug 10 13:01:10 2007 -0700

    fix compilation with gcc 4.2
    
    gcc-4.2 is a lot more picky about its symbol handling.  EXPORT_SYMBOL no
    longer works on symbols that are undefined or defined with static scope.
    
    For example, with CONFIG_PROFILE off, I see:
    
      kernel/profile.c:206: error: __ksymtab_profile_event_unregister causes a section type conflict
      kernel/profile.c:205: error: __ksymtab_profile_event_register causes a section type conflict
    
    This patch moves the EXPORTs inside the #ifdef CONFIG_PROFILE, so we
    only try to export symbols that are defined.
    
    Also, in kernel/kprobes.c there's an EXPORT_SYMBOL_GPL() for
    jprobes_return, which if CONFIG_JPROBES is undefined is a static
    inline and gives the same error.
    
    And in drivers/acpi/resources/rsxface.c, there's an
    ACPI_EXPORT_SYMBOPL() for a static symbol. If it's static, it's not
    accessible from outside the compilation unit, so should bot be exported.
    
    These three changes allow building a zx1_defconfig kernel with gcc 4.2
    on IA64.
    
    [akpm@linux-foundation.org: export jpobe_return properly]
    Signed-off-by: Peter Chubb <peterc@gelato.unsw.edu.au>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Len Brown <lenb@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 5b20fe977bed..cb1e37d2dac3 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -199,11 +199,11 @@ EXPORT_SYMBOL_GPL(register_timer_hook);
 EXPORT_SYMBOL_GPL(unregister_timer_hook);
 EXPORT_SYMBOL_GPL(task_handoff_register);
 EXPORT_SYMBOL_GPL(task_handoff_unregister);
+EXPORT_SYMBOL_GPL(profile_event_register);
+EXPORT_SYMBOL_GPL(profile_event_unregister);
 
 #endif /* CONFIG_PROFILING */
 
-EXPORT_SYMBOL_GPL(profile_event_register);
-EXPORT_SYMBOL_GPL(profile_event_unregister);
 
 #ifdef CONFIG_SMP
 /*

commit e8edc6e03a5c8562dc70a6d969f732bdb355a7e7
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon May 21 01:22:52 2007 +0400

    Detach sched.h from mm.h
    
    First thing mm.h does is including sched.h solely for can_do_mlock() inline
    function which has "current" dereference inside. By dealing with can_do_mlock()
    mm.h can be detached from sched.h which is good. See below, why.
    
    This patch
    a) removes unconditional inclusion of sched.h from mm.h
    b) makes can_do_mlock() normal function in mm/mlock.c
    c) exports can_do_mlock() to not break compilation
    d) adds sched.h inclusions back to files that were getting it indirectly.
    e) adds less bloated headers to some files (asm/signal.h, jiffies.h) that were
       getting them indirectly
    
    Net result is:
    a) mm.h users would get less code to open, read, preprocess, parse, ... if
       they don't need sched.h
    b) sched.h stops being dependency for significant number of files:
       on x86_64 allmodconfig touching sched.h results in recompile of 4083 files,
       after patch it's only 3744 (-8.3%).
    
    Cross-compile tested on
    
            all arm defconfigs, all mips defconfigs, all powerpc defconfigs,
            alpha alpha-up
            arm
            i386 i386-up i386-defconfig i386-allnoconfig
            ia64 ia64-up
            m68k
            mips
            parisc parisc-up
            powerpc powerpc-up
            s390 s390-up
            sparc sparc-up
            sparc64 sparc64-up
            um-x86_64
            x86_64 x86_64-up x86_64-defconfig x86_64-allnoconfig
    
    as well as my two usual configs.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index cc91b9bf759d..5b20fe977bed 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -26,6 +26,7 @@
 #include <asm/sections.h>
 #include <asm/semaphore.h>
 #include <asm/irq_regs.h>
+#include <asm/ptrace.h>
 
 struct profile_hit {
 	u32 pc, hits;

commit 8bb7844286fb8c9fce6f65d8288aeb09d03a5e0d
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed May 9 02:35:10 2007 -0700

    Add suspend-related notifications for CPU hotplug
    
    Since nonboot CPUs are now disabled after tasks and devices have been
    frozen and the CPU hotplug infrastructure is used for this purpose, we need
    special CPU hotplug notifications that will help the CPU-hotplug-aware
    subsystems distinguish normal CPU hotplug events from CPU hotplug events
    related to a system-wide suspend or resume operation in progress.  This
    patch introduces such notifications and causes them to be used during
    suspend and resume transitions.  It also changes all of the
    CPU-hotplug-aware subsystems to take these notifications into consideration
    (for now they are handled in the same way as the corresponding "normal"
    ones).
    
    [oleg@tv-sign.ru: cleanups]
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 9bfadb248dd8..cc91b9bf759d 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -340,6 +340,7 @@ static int __devinit profile_cpu_callback(struct notifier_block *info,
 
 	switch (action) {
 	case CPU_UP_PREPARE:
+	case CPU_UP_PREPARE_FROZEN:
 		node = cpu_to_node(cpu);
 		per_cpu(cpu_profile_flip, cpu) = 0;
 		if (!per_cpu(cpu_profile_hits, cpu)[1]) {
@@ -365,10 +366,13 @@ static int __devinit profile_cpu_callback(struct notifier_block *info,
 		__free_page(page);
 		return NOTIFY_BAD;
 	case CPU_ONLINE:
+	case CPU_ONLINE_FROZEN:
 		cpu_set(cpu, prof_cpu_mask);
 		break;
 	case CPU_UP_CANCELED:
+	case CPU_UP_CANCELED_FROZEN:
 	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
 		cpu_clear(cpu, prof_cpu_mask);
 		if (per_cpu(cpu_profile_hits, cpu)[0]) {
 			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[0]);

commit b653d081c17e26101980c858a9808740533b78b4
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sat Feb 10 01:45:54 2007 -0800

    [PATCH] proc: remove useless (and buggy) ->nlink settings
    
    Bug: pnx8550 code creates directory but resets ->nlink to 1.
    
    create_proc_entry() et al will correctly set ->nlink for you.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Corey Minyard <minyard@acm.org>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Greg KH <greg@kroah.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index d6579d511069..9bfadb248dd8 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -449,7 +449,6 @@ void create_prof_cpu_mask(struct proc_dir_entry *root_irq_dir)
 	/* create /proc/irq/prof_cpu_mask */
 	if (!(entry = create_proc_entry("prof_cpu_mask", 0600, root_irq_dir)))
 		return;
-	entry->nlink = 1;
 	entry->data = (void *)&prof_cpu_mask;
 	entry->read_proc = prof_cpu_mask_read_proc;
 	entry->write_proc = prof_cpu_mask_write_proc;

commit bbe1a59b3a3d4af3869647d294618a23f6c9c6a4
Author: Andrew Morton <akpm@osdl.org>
Date:   Mon Jan 22 20:40:33 2007 -0800

    [PATCH] fix "kvm: add vm exit profiling"
    
    export profile_hits() on !SMP too.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index a6574a18514e..d6579d511069 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -331,7 +331,6 @@ void profile_hits(int type, void *__pc, unsigned int nr_hits)
 	local_irq_restore(flags);
 	put_cpu();
 }
-EXPORT_SYMBOL_GPL(profile_hits);
 
 static int __devinit profile_cpu_callback(struct notifier_block *info,
 					unsigned long action, void *__cpu)
@@ -401,6 +400,8 @@ void profile_hits(int type, void *__pc, unsigned int nr_hits)
 }
 #endif /* !CONFIG_SMP */
 
+EXPORT_SYMBOL_GPL(profile_hits);
+
 void profile_tick(int type)
 {
 	struct pt_regs *regs = get_irq_regs();

commit 07031e14c1127fc7e1a5b98dfcc59f434e025104
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 10 23:15:38 2007 -0800

    [PATCH] KVM: add VM-exit profiling
    
    This adds the profile=kvm boot option, which enables KVM to profile VM
    exits.
    
    Use: "readprofile -m ./System.map | sort -n" to see the resulting
    output:
    
       [...]
       18246 serial_out                               148.3415
       18945 native_flush_tlb                         378.9000
       23618 serial_in                                212.7748
       29279 __spin_unlock_irq                        622.9574
       43447 native_apic_write                        2068.9048
       52702 enable_8259A_irq                         742.2817
       54250 vgacon_scroll                             89.3740
       67394 ide_inb                                  6126.7273
       79514 copy_page_range                           98.1654
       84868 do_wp_page                                86.6000
      140266 pit_read                                 783.6089
      151436 ide_outb                                 25239.3333
      152668 native_io_delay                          21809.7143
      174783 mask_and_ack_8259A                       783.7803
      362404 native_set_pte_at                        36240.4000
     1688747 total                                      0.5009
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 11550b2290b6..a6574a18514e 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -40,7 +40,10 @@ int (*timer_hook)(struct pt_regs *) __read_mostly;
 
 static atomic_t *prof_buffer;
 static unsigned long prof_len, prof_shift;
+
 int prof_on __read_mostly;
+EXPORT_SYMBOL_GPL(prof_on);
+
 static cpumask_t prof_cpu_mask = CPU_MASK_ALL;
 #ifdef CONFIG_SMP
 static DEFINE_PER_CPU(struct profile_hit *[2], cpu_profile_hits);
@@ -52,6 +55,7 @@ static int __init profile_setup(char * str)
 {
 	static char __initdata schedstr[] = "schedule";
 	static char __initdata sleepstr[] = "sleep";
+	static char __initdata kvmstr[] = "kvm";
 	int par;
 
 	if (!strncmp(str, sleepstr, strlen(sleepstr))) {
@@ -72,6 +76,15 @@ static int __init profile_setup(char * str)
 		printk(KERN_INFO
 			"kernel schedule profiling enabled (shift: %ld)\n",
 			prof_shift);
+	} else if (!strncmp(str, kvmstr, strlen(kvmstr))) {
+		prof_on = KVM_PROFILING;
+		if (str[strlen(kvmstr)] == ',')
+			str += strlen(kvmstr) + 1;
+		if (get_option(&str, &par))
+			prof_shift = par;
+		printk(KERN_INFO
+			"kernel KVM profiling enabled (shift: %ld)\n",
+			prof_shift);
 	} else if (get_option(&str, &par)) {
 		prof_shift = par;
 		prof_on = CPU_PROFILING;
@@ -318,6 +331,7 @@ void profile_hits(int type, void *__pc, unsigned int nr_hits)
 	local_irq_restore(flags);
 	put_cpu();
 }
+EXPORT_SYMBOL_GPL(profile_hits);
 
 static int __devinit profile_cpu_callback(struct notifier_block *info,
 					unsigned long action, void *__cpu)

commit a75acf850ca80136a4f845cf9a7cd26e7465c1f4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 5 16:36:29 2007 -0800

    [PATCH] profiling: fix sched profiling typo
    
    Fix sched profiling typo, introduced by the sleep profiling patch.  This
    bug caused profile=sched to not work.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index fb5e03d57e9d..11550b2290b6 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -63,7 +63,7 @@ static int __init profile_setup(char * str)
 		printk(KERN_INFO
 			"kernel sleep profiling enabled (shift: %ld)\n",
 			prof_shift);
-	} else if (!strncmp(str, sleepstr, strlen(sleepstr))) {
+	} else if (!strncmp(str, schedstr, strlen(schedstr))) {
 		prof_on = SCHED_PROFILING;
 		if (str[strlen(schedstr)] == ',')
 			str += strlen(schedstr) + 1;

commit 15ad7cdcfd76450d4beebc789ec646664238184d
Author: Helge Deller <deller@gmx.de>
Date:   Wed Dec 6 20:40:36 2006 -0800

    [PATCH] struct seq_operations and struct file_operations constification
    
     - move some file_operations structs into the .rodata section
    
     - move static strings from policy_types[] array into the .rodata section
    
     - fix generic seq_operations usages, so that those structs may be defined
       as "const" as well
    
    [akpm@osdl.org: couple of fixes]
    Signed-off-by: Helge Deller <deller@gmx.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 0961d93e1d91..fb5e03d57e9d 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -501,7 +501,7 @@ static ssize_t write_profile(struct file *file, const char __user *buf,
 	return count;
 }
 
-static struct file_operations proc_profile_operations = {
+static const struct file_operations proc_profile_operations = {
 	.read		= read_profile,
 	.write		= write_profile,
 };

commit 02316067852187b8bec781bec07410e91af79627
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 6 20:38:17 2006 -0800

    [PATCH] hotplug CPU: clean up hotcpu_notifier() use
    
    There was lots of #ifdef noise in the kernel due to hotcpu_notifier(fn,
    prio) not correctly marking 'fn' as used in the !HOTPLUG_CPU case, and thus
    generating compiler warnings of unused symbols, hence forcing people to add
    #ifdefs.
    
    the compiler can skip truly unused functions just fine:
    
        text    data     bss     dec     hex filename
     1624412  728710 3674856 6027978  5bfaca vmlinux.before
     1624412  728710 3674856 6027978  5bfaca vmlinux.after
    
    [akpm@osdl.org: topology.c fix]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 04fd84e8cdbe..0961d93e1d91 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -319,7 +319,6 @@ void profile_hits(int type, void *__pc, unsigned int nr_hits)
 	put_cpu();
 }
 
-#ifdef CONFIG_HOTPLUG_CPU
 static int __devinit profile_cpu_callback(struct notifier_block *info,
 					unsigned long action, void *__cpu)
 {
@@ -372,10 +371,10 @@ static int __devinit profile_cpu_callback(struct notifier_block *info,
 	}
 	return NOTIFY_OK;
 }
-#endif /* CONFIG_HOTPLUG_CPU */
 #else /* !CONFIG_SMP */
 #define profile_flip_buffers()		do { } while (0)
 #define profile_discard_flip_buffers()	do { } while (0)
+#define profile_cpu_callback		NULL
 
 void profile_hits(int type, void *__pc, unsigned int nr_hits)
 {

commit ece8a684c75df215320b4155944979e3f78c5c93
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 6 20:37:24 2006 -0800

    [PATCH] sleep profiling
    
    Implement prof=sleep profiling.  TASK_UNINTERRUPTIBLE sleeps will be taken
    as a profile hit, and every millisecond spent sleeping causes a profile-hit
    for the call site that initiated the sleep.
    
    Sample readprofile output on i386:
    
       306 ps2_sendbyte                               1.3973
       432 call_usermodehelper_keys                   1.9548
       484 ps2_command                                0.6453
       790 __driver_attach                            4.7879
      1593 msleep                                    44.2500
      3976 sync_buffer                               64.1290
      4076 do_lookup                                 12.4648
      8587 sync_page                                122.6714
     20820 total                                      0.0067
    
    (NOTE: architectures need to check whether get_wchan() can be called from
    deep within the wakeup path.)
    
    akpm: we need to mark more functions __sched.  lock_sock(), msleep(), others..
    
    akpm: the contention in do_lookup() is a surprise.  Presumably doing disk
    reads for directory contents while holding i_mutex.
    
    [akpm@osdl.org: various fixes]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 15b012df4ff1..04fd84e8cdbe 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -40,7 +40,7 @@ int (*timer_hook)(struct pt_regs *) __read_mostly;
 
 static atomic_t *prof_buffer;
 static unsigned long prof_len, prof_shift;
-static int prof_on __read_mostly;
+int prof_on __read_mostly;
 static cpumask_t prof_cpu_mask = CPU_MASK_ALL;
 #ifdef CONFIG_SMP
 static DEFINE_PER_CPU(struct profile_hit *[2], cpu_profile_hits);
@@ -51,9 +51,19 @@ static DEFINE_MUTEX(profile_flip_mutex);
 static int __init profile_setup(char * str)
 {
 	static char __initdata schedstr[] = "schedule";
+	static char __initdata sleepstr[] = "sleep";
 	int par;
 
-	if (!strncmp(str, schedstr, strlen(schedstr))) {
+	if (!strncmp(str, sleepstr, strlen(sleepstr))) {
+		prof_on = SLEEP_PROFILING;
+		if (str[strlen(sleepstr)] == ',')
+			str += strlen(sleepstr) + 1;
+		if (get_option(&str, &par))
+			prof_shift = par;
+		printk(KERN_INFO
+			"kernel sleep profiling enabled (shift: %ld)\n",
+			prof_shift);
+	} else if (!strncmp(str, sleepstr, strlen(sleepstr))) {
 		prof_on = SCHED_PROFILING;
 		if (str[strlen(schedstr)] == ',')
 			str += strlen(schedstr) + 1;
@@ -204,7 +214,8 @@ EXPORT_SYMBOL_GPL(profile_event_unregister);
  * positions to which hits are accounted during short intervals (e.g.
  * several seconds) is usually very small. Exclusion from buffer
  * flipping is provided by interrupt disablement (note that for
- * SCHED_PROFILING profile_hit() may be called from process context).
+ * SCHED_PROFILING or SLEEP_PROFILING profile_hit() may be called from
+ * process context).
  * The hash function is meant to be lightweight as opposed to strong,
  * and was vaguely inspired by ppc64 firmware-supported inverted
  * pagetable hash functions, but uses a full hashtable full of finite
@@ -257,7 +268,7 @@ static void profile_discard_flip_buffers(void)
 	mutex_unlock(&profile_flip_mutex);
 }
 
-void profile_hit(int type, void *__pc)
+void profile_hits(int type, void *__pc, unsigned int nr_hits)
 {
 	unsigned long primary, secondary, flags, pc = (unsigned long)__pc;
 	int i, j, cpu;
@@ -274,21 +285,31 @@ void profile_hit(int type, void *__pc)
 		put_cpu();
 		return;
 	}
+	/*
+	 * We buffer the global profiler buffer into a per-CPU
+	 * queue and thus reduce the number of global (and possibly
+	 * NUMA-alien) accesses. The write-queue is self-coalescing:
+	 */
 	local_irq_save(flags);
 	do {
 		for (j = 0; j < PROFILE_GRPSZ; ++j) {
 			if (hits[i + j].pc == pc) {
-				hits[i + j].hits++;
+				hits[i + j].hits += nr_hits;
 				goto out;
 			} else if (!hits[i + j].hits) {
 				hits[i + j].pc = pc;
-				hits[i + j].hits = 1;
+				hits[i + j].hits = nr_hits;
 				goto out;
 			}
 		}
 		i = (i + secondary) & (NR_PROFILE_HIT - 1);
 	} while (i != primary);
-	atomic_inc(&prof_buffer[pc]);
+
+	/*
+	 * Add the current hit(s) and flush the write-queue out
+	 * to the global buffer:
+	 */
+	atomic_add(nr_hits, &prof_buffer[pc]);
 	for (i = 0; i < NR_PROFILE_HIT; ++i) {
 		atomic_add(hits[i].hits, &prof_buffer[hits[i].pc]);
 		hits[i].pc = hits[i].hits = 0;
@@ -356,14 +377,14 @@ static int __devinit profile_cpu_callback(struct notifier_block *info,
 #define profile_flip_buffers()		do { } while (0)
 #define profile_discard_flip_buffers()	do { } while (0)
 
-void profile_hit(int type, void *__pc)
+void profile_hits(int type, void *__pc, unsigned int nr_hits)
 {
 	unsigned long pc;
 
 	if (prof_on != type || !prof_buffer)
 		return;
 	pc = ((unsigned long)__pc - (unsigned long)_stext) >> prof_shift;
-	atomic_inc(&prof_buffer[min(pc, prof_len - 1)]);
+	atomic_add(nr_hits, &prof_buffer[min(pc, prof_len - 1)]);
 }
 #endif /* !CONFIG_SMP */
 

commit 064b022c7adb2d853378078a9dc141f8288d1c73
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Dec 6 20:36:37 2006 -0800

    [PATCH] profile: fix uaccess handling
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index f940b462eec9..15b012df4ff1 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -442,7 +442,8 @@ read_profile(struct file *file, char __user *buf, size_t count, loff_t *ppos)
 	read = 0;
 
 	while (p < sizeof(unsigned int) && count > 0) {
-		put_user(*((char *)(&sample_step)+p),buf);
+		if (put_user(*((char *)(&sample_step)+p),buf))
+			return -EFAULT;
 		buf++; p++; count--; read++;
 	}
 	pnt = (char *)prof_buffer + p - sizeof(atomic_t);

commit 01a3ee2b203e511e20f98b85a9172fd32c53e87c
Author: Reinette Chatre <reinette.chatre@linux.intel.com>
Date:   Wed Oct 11 01:21:55 2006 -0700

    [PATCH] bitmap: parse input from kernel and user buffers
    
    lib/bitmap.c:bitmap_parse() is a library function that received as input a
    user buffer.  This seemed to have originated from the way the write_proc
    function of the /proc filesystem operates.
    
    This has been reworked to not use kmalloc and eliminates a lot of
    get_user() overhead by performing one access_ok before using __get_user().
    
    We need to test if we are in kernel or user space (is_user) and access the
    buffer differently.  We cannot use __get_user() to access kernel addresses
    in all cases, for example in architectures with separate address space for
    kernel and user.
    
    This function will be useful for other uses as well; for example, taking
    input for /sysfs instead of /proc, so it was changed to accept kernel
    buffers.  We have this use for the Linux UWB project, as part as the
    upcoming bandwidth allocator code.
    
    Only a few routines used this function and they were changed too.
    
    Signed-off-by: Reinette Chatre <reinette.chatre@linux.intel.com>
    Signed-off-by: Inaky Perez-Gonzalez <inaky@linux.intel.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Joe Korty <joe.korty@ccur.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 857300a2afec..f940b462eec9 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -399,7 +399,7 @@ static int prof_cpu_mask_write_proc (struct file *file, const char __user *buffe
 	unsigned long full_count = count, err;
 	cpumask_t new_value;
 
-	err = cpumask_parse(buffer, count, new_value);
+	err = cpumask_parse_user(buffer, count, new_value);
 	if (err)
 		return err;
 

commit 7d12e780e003f93433d49ce78cfedf4b4c52adc5
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 5 14:55:46 2006 +0100

    IRQ: Maintain regs pointer globally rather than passing to IRQ handlers
    
    Maintain a per-CPU global "struct pt_regs *" variable which can be used instead
    of passing regs around manually through all ~1800 interrupt handlers in the
    Linux kernel.
    
    The regs pointer is used in few places, but it potentially costs both stack
    space and code to pass it around.  On the FRV arch, removing the regs parameter
    from all the genirq function results in a 20% speed up of the IRQ exit path
    (ie: from leaving timer_interrupt() to leaving do_IRQ()).
    
    Where appropriate, an arch may override the generic storage facility and do
    something different with the variable.  On FRV, for instance, the address is
    maintained in GR28 at all times inside the kernel as part of general exception
    handling.
    
    Having looked over the code, it appears that the parameter may be handed down
    through up to twenty or so layers of functions.  Consider a USB character
    device attached to a USB hub, attached to a USB controller that posts its
    interrupts through a cascaded auxiliary interrupt controller.  A character
    device driver may want to pass regs to the sysrq handler through the input
    layer which adds another few layers of parameter passing.
    
    I've build this code with allyesconfig for x86_64 and i386.  I've runtested the
    main part of the code on FRV and i386, though I can't test most of the drivers.
    I've also done partial conversion for powerpc and MIPS - these at least compile
    with minimal configurations.
    
    This will affect all archs.  Mostly the changes should be relatively easy.
    Take do_IRQ(), store the regs pointer at the beginning, saving the old one:
    
            struct pt_regs *old_regs = set_irq_regs(regs);
    
    And put the old one back at the end:
    
            set_irq_regs(old_regs);
    
    Don't pass regs through to generic_handle_irq() or __do_IRQ().
    
    In timer_interrupt(), this sort of change will be necessary:
    
            -       update_process_times(user_mode(regs));
            -       profile_tick(CPU_PROFILING, regs);
            +       update_process_times(user_mode(get_irq_regs()));
            +       profile_tick(CPU_PROFILING);
    
    I'd like to move update_process_times()'s use of get_irq_regs() into itself,
    except that i386, alone of the archs, uses something other than user_mode().
    
    Some notes on the interrupt handling in the drivers:
    
     (*) input_dev() is now gone entirely.  The regs pointer is no longer stored in
         the input_dev struct.
    
     (*) finish_unlinks() in drivers/usb/host/ohci-q.c needs checking.  It does
         something different depending on whether it's been supplied with a regs
         pointer or not.
    
     (*) Various IRQ handler function pointers have been moved to type
         irq_handler_t.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>
    (cherry picked from 1b16e7ac850969f38b375e511e3fa2f474a33867 commit)

diff --git a/kernel/profile.c b/kernel/profile.c
index fb660c7d35ba..857300a2afec 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -25,6 +25,7 @@
 #include <linux/mutex.h>
 #include <asm/sections.h>
 #include <asm/semaphore.h>
+#include <asm/irq_regs.h>
 
 struct profile_hit {
 	u32 pc, hits;
@@ -366,8 +367,10 @@ void profile_hit(int type, void *__pc)
 }
 #endif /* !CONFIG_SMP */
 
-void profile_tick(int type, struct pt_regs *regs)
+void profile_tick(int type)
 {
+	struct pt_regs *regs = get_irq_regs();
+
 	if (type == CPU_PROFILING && timer_hook)
 		timer_hook(regs);
 	if (!user_mode(regs) && cpu_isset(smp_processor_id(), prof_cpu_mask))

commit fbd98167e653535c5816be154f2149c0efa7757d
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:45 2006 -0700

    [PATCH] Profiling: require buffer allocation on the correct node
    
    Profiling really suffers with off node buffers.  Fail if no memory is
    available on the nodes.  The profiling code can deal with these failures
    should they occur.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index d5bd75e7501c..fb660c7d35ba 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -309,13 +309,17 @@ static int __devinit profile_cpu_callback(struct notifier_block *info,
 		node = cpu_to_node(cpu);
 		per_cpu(cpu_profile_flip, cpu) = 0;
 		if (!per_cpu(cpu_profile_hits, cpu)[1]) {
-			page = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
+			page = alloc_pages_node(node,
+					GFP_KERNEL | __GFP_ZERO | GFP_THISNODE,
+					0);
 			if (!page)
 				return NOTIFY_BAD;
 			per_cpu(cpu_profile_hits, cpu)[1] = page_address(page);
 		}
 		if (!per_cpu(cpu_profile_hits, cpu)[0]) {
-			page = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
+			page = alloc_pages_node(node,
+					GFP_KERNEL | __GFP_ZERO | GFP_THISNODE,
+					0);
 			if (!page)
 				goto out_free;
 			per_cpu(cpu_profile_hits, cpu)[0] = page_address(page);
@@ -491,12 +495,16 @@ static int __init create_hash_tables(void)
 		int node = cpu_to_node(cpu);
 		struct page *page;
 
-		page = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
+		page = alloc_pages_node(node,
+				GFP_KERNEL | __GFP_ZERO | GFP_THISNODE,
+				0);
 		if (!page)
 			goto out_cleanup;
 		per_cpu(cpu_profile_hits, cpu)[1]
 				= (struct profile_hit *)page_address(page);
-		page = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
+		page = alloc_pages_node(node,
+				GFP_KERNEL | __GFP_ZERO | GFP_THISNODE,
+				0);
 		if (!page)
 			goto out_cleanup;
 		per_cpu(cpu_profile_hits, cpu)[0]

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jörn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jörn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/kernel/profile.c b/kernel/profile.c
index 5a730fdb1a2c..d5bd75e7501c 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -13,7 +13,6 @@
  *	to resolve timer interrupt livelocks, William Irwin, Oracle, 2004
  */
 
-#include <linux/config.h>
 #include <linux/module.h>
 #include <linux/profile.h>
 #include <linux/bootmem.h>

commit 9c7b216d23e820e0e148d5be01bbb5bd2d8378fe
Author: Chandra Seetharaman <sekharan@us.ibm.com>
Date:   Tue Jun 27 02:54:07 2006 -0700

    [PATCH] cpu hotplug: revert init patch submitted for 2.6.17
    
    In 2.6.17, there was a problem with cpu_notifiers and XFS.  I provided a
    band-aid solution to solve that problem.  In the process, i undid all the
    changes you both were making to ensure that these notifiers were available
    only at init time (unless CONFIG_HOTPLUG_CPU is defined).
    
    We deferred the real fix to 2.6.18.  Here is a set of patches that fixes the
    XFS problem cleanly and makes the cpu notifiers available only at init time
    (unless CONFIG_HOTPLUG_CPU is defined).
    
    If CONFIG_HOTPLUG_CPU is defined then cpu notifiers are available at run
    time.
    
    This patch reverts the notifier_call changes made in 2.6.17
    
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Cc: Ashok Raj <ashok.raj@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 68afe121e507..5a730fdb1a2c 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -299,7 +299,7 @@ void profile_hit(int type, void *__pc)
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
-static int profile_cpu_callback(struct notifier_block *info,
+static int __devinit profile_cpu_callback(struct notifier_block *info,
 					unsigned long action, void *__cpu)
 {
 	int node, cpu = (unsigned long)__cpu;

commit 83d722f7e198b034699b1500d98729beff930efd
Author: Chandra Seetharaman <sekharan@us.ibm.com>
Date:   Mon Apr 24 19:35:21 2006 -0700

    [PATCH] Remove __devinit and __cpuinit from notifier_call definitions
    
    Few of the notifier_chain_register() callers use __init in the definition
    of notifier_call.  It is incorrect as the function definition should be
    available after the initializations (they do not unregister them during
    initializations).
    
    This patch fixes all such usages to _not_ have the notifier_call __init
    section.
    
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 5a730fdb1a2c..68afe121e507 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -299,7 +299,7 @@ void profile_hit(int type, void *__pc)
 }
 
 #ifdef CONFIG_HOTPLUG_CPU
-static int __devinit profile_cpu_callback(struct notifier_block *info,
+static int profile_cpu_callback(struct notifier_block *info,
 					unsigned long action, void *__cpu)
 {
 	int node, cpu = (unsigned long)__cpu;

commit e041c683412d5bf44dc2b109053e3b837b71742d
Author: Alan Stern <stern@rowland.harvard.edu>
Date:   Mon Mar 27 01:16:30 2006 -0800

    [PATCH] Notifier chain update: API changes
    
    The kernel's implementation of notifier chains is unsafe.  There is no
    protection against entries being added to or removed from a chain while the
    chain is in use.  The issues were discussed in this thread:
    
        http://marc.theaimsgroup.com/?l=linux-kernel&m=113018709002036&w=2
    
    We noticed that notifier chains in the kernel fall into two basic usage
    classes:
    
            "Blocking" chains are always called from a process context
            and the callout routines are allowed to sleep;
    
            "Atomic" chains can be called from an atomic context and
            the callout routines are not allowed to sleep.
    
    We decided to codify this distinction and make it part of the API.  Therefore
    this set of patches introduces three new, parallel APIs: one for blocking
    notifiers, one for atomic notifiers, and one for "raw" notifiers (which is
    really just the old API under a new name).  New kinds of data structures are
    used for the heads of the chains, and new routines are defined for
    registration, unregistration, and calling a chain.  The three APIs are
    explained in include/linux/notifier.h and their implementation is in
    kernel/sys.c.
    
    With atomic and blocking chains, the implementation guarantees that the chain
    links will not be corrupted and that chain callers will not get messed up by
    entries being added or removed.  For raw chains the implementation provides no
    guarantees at all; users of this API must provide their own protections.  (The
    idea was that situations may come up where the assumptions of the atomic and
    blocking APIs are not appropriate, so it should be possible for users to
    handle these things in their own way.)
    
    There are some limitations, which should not be too hard to live with.  For
    atomic/blocking chains, registration and unregistration must always be done in
    a process context since the chain is protected by a mutex/rwsem.  Also, a
    callout routine for a non-raw chain must not try to register or unregister
    entries on its own chain.  (This did happen in a couple of places and the code
    had to be changed to avoid it.)
    
    Since atomic chains may be called from within an NMI handler, they cannot use
    spinlocks for synchronization.  Instead we use RCU.  The overhead falls almost
    entirely in the unregister routine, which is okay since unregistration is much
    less frequent that calling a chain.
    
    Here is the list of chains that we adjusted and their classifications.  None
    of them use the raw API, so for the moment it is only a placeholder.
    
      ATOMIC CHAINS
      -------------
    arch/i386/kernel/traps.c:               i386die_chain
    arch/ia64/kernel/traps.c:               ia64die_chain
    arch/powerpc/kernel/traps.c:            powerpc_die_chain
    arch/sparc64/kernel/traps.c:            sparc64die_chain
    arch/x86_64/kernel/traps.c:             die_chain
    drivers/char/ipmi/ipmi_si_intf.c:       xaction_notifier_list
    kernel/panic.c:                         panic_notifier_list
    kernel/profile.c:                       task_free_notifier
    net/bluetooth/hci_core.c:               hci_notifier
    net/ipv4/netfilter/ip_conntrack_core.c: ip_conntrack_chain
    net/ipv4/netfilter/ip_conntrack_core.c: ip_conntrack_expect_chain
    net/ipv6/addrconf.c:                    inet6addr_chain
    net/netfilter/nf_conntrack_core.c:      nf_conntrack_chain
    net/netfilter/nf_conntrack_core.c:      nf_conntrack_expect_chain
    net/netlink/af_netlink.c:               netlink_chain
    
      BLOCKING CHAINS
      ---------------
    arch/powerpc/platforms/pseries/reconfig.c:      pSeries_reconfig_chain
    arch/s390/kernel/process.c:             idle_chain
    arch/x86_64/kernel/process.c            idle_notifier
    drivers/base/memory.c:                  memory_chain
    drivers/cpufreq/cpufreq.c               cpufreq_policy_notifier_list
    drivers/cpufreq/cpufreq.c               cpufreq_transition_notifier_list
    drivers/macintosh/adb.c:                adb_client_list
    drivers/macintosh/via-pmu.c             sleep_notifier_list
    drivers/macintosh/via-pmu68k.c          sleep_notifier_list
    drivers/macintosh/windfarm_core.c       wf_client_list
    drivers/usb/core/notify.c               usb_notifier_list
    drivers/video/fbmem.c                   fb_notifier_list
    kernel/cpu.c                            cpu_chain
    kernel/module.c                         module_notify_list
    kernel/profile.c                        munmap_notifier
    kernel/profile.c                        task_exit_notifier
    kernel/sys.c                            reboot_notifier_list
    net/core/dev.c                          netdev_chain
    net/decnet/dn_dev.c:                    dnaddr_chain
    net/ipv4/devinet.c:                     inetaddr_chain
    
    It's possible that some of these classifications are wrong.  If they are,
    please let us know or submit a patch to fix them.  Note that any chain that
    gets called very frequently should be atomic, because the rwsem read-locking
    used for blocking chains is very likely to incur cache misses on SMP systems.
    (However, if the chain's callout routines may sleep then the chain cannot be
    atomic.)
    
    The patch set was written by Alan Stern and Chandra Seetharaman, incorporating
    material written by Keith Owens and suggestions from Paul McKenney and Andrew
    Morton.
    
    [jes@sgi.com: restructure the notifier chain initialization macros]
    Signed-off-by: Alan Stern <stern@rowland.harvard.edu>
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Signed-off-by: Jes Sorensen <jes@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index ad81f799a9b4..5a730fdb1a2c 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -87,72 +87,52 @@ void __init profile_init(void)
  
 #ifdef CONFIG_PROFILING
  
-static DECLARE_RWSEM(profile_rwsem);
-static DEFINE_RWLOCK(handoff_lock);
-static struct notifier_block * task_exit_notifier;
-static struct notifier_block * task_free_notifier;
-static struct notifier_block * munmap_notifier;
+static BLOCKING_NOTIFIER_HEAD(task_exit_notifier);
+static ATOMIC_NOTIFIER_HEAD(task_free_notifier);
+static BLOCKING_NOTIFIER_HEAD(munmap_notifier);
  
 void profile_task_exit(struct task_struct * task)
 {
-	down_read(&profile_rwsem);
-	notifier_call_chain(&task_exit_notifier, 0, task);
-	up_read(&profile_rwsem);
+	blocking_notifier_call_chain(&task_exit_notifier, 0, task);
 }
  
 int profile_handoff_task(struct task_struct * task)
 {
 	int ret;
-	read_lock(&handoff_lock);
-	ret = notifier_call_chain(&task_free_notifier, 0, task);
-	read_unlock(&handoff_lock);
+	ret = atomic_notifier_call_chain(&task_free_notifier, 0, task);
 	return (ret == NOTIFY_OK) ? 1 : 0;
 }
 
 void profile_munmap(unsigned long addr)
 {
-	down_read(&profile_rwsem);
-	notifier_call_chain(&munmap_notifier, 0, (void *)addr);
-	up_read(&profile_rwsem);
+	blocking_notifier_call_chain(&munmap_notifier, 0, (void *)addr);
 }
 
 int task_handoff_register(struct notifier_block * n)
 {
-	int err = -EINVAL;
-
-	write_lock(&handoff_lock);
-	err = notifier_chain_register(&task_free_notifier, n);
-	write_unlock(&handoff_lock);
-	return err;
+	return atomic_notifier_chain_register(&task_free_notifier, n);
 }
 
 int task_handoff_unregister(struct notifier_block * n)
 {
-	int err = -EINVAL;
-
-	write_lock(&handoff_lock);
-	err = notifier_chain_unregister(&task_free_notifier, n);
-	write_unlock(&handoff_lock);
-	return err;
+	return atomic_notifier_chain_unregister(&task_free_notifier, n);
 }
 
 int profile_event_register(enum profile_type type, struct notifier_block * n)
 {
 	int err = -EINVAL;
  
-	down_write(&profile_rwsem);
- 
 	switch (type) {
 		case PROFILE_TASK_EXIT:
-			err = notifier_chain_register(&task_exit_notifier, n);
+			err = blocking_notifier_chain_register(
+					&task_exit_notifier, n);
 			break;
 		case PROFILE_MUNMAP:
-			err = notifier_chain_register(&munmap_notifier, n);
+			err = blocking_notifier_chain_register(
+					&munmap_notifier, n);
 			break;
 	}
  
-	up_write(&profile_rwsem);
- 
 	return err;
 }
 
@@ -161,18 +141,17 @@ int profile_event_unregister(enum profile_type type, struct notifier_block * n)
 {
 	int err = -EINVAL;
  
-	down_write(&profile_rwsem);
- 
 	switch (type) {
 		case PROFILE_TASK_EXIT:
-			err = notifier_chain_unregister(&task_exit_notifier, n);
+			err = blocking_notifier_chain_unregister(
+					&task_exit_notifier, n);
 			break;
 		case PROFILE_MUNMAP:
-			err = notifier_chain_unregister(&munmap_notifier, n);
+			err = blocking_notifier_chain_unregister(
+					&munmap_notifier, n);
 			break;
 	}
 
-	up_write(&profile_rwsem);
 	return err;
 }
 

commit 97d1f15b7ef52c1e9c28dc48b454024bb53a5fd2
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Thu Mar 23 03:00:24 2006 -0800

    [PATCH] sem2mutex: kernel/
    
    Semaphore to mutex conversion.
    
    The conversion was generated via scripts, and the result was validated
    automatically via a script as well.
    
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index f89248e6d704..ad81f799a9b4 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -23,6 +23,7 @@
 #include <linux/cpu.h>
 #include <linux/profile.h>
 #include <linux/highmem.h>
+#include <linux/mutex.h>
 #include <asm/sections.h>
 #include <asm/semaphore.h>
 
@@ -44,7 +45,7 @@ static cpumask_t prof_cpu_mask = CPU_MASK_ALL;
 #ifdef CONFIG_SMP
 static DEFINE_PER_CPU(struct profile_hit *[2], cpu_profile_hits);
 static DEFINE_PER_CPU(int, cpu_profile_flip);
-static DECLARE_MUTEX(profile_flip_mutex);
+static DEFINE_MUTEX(profile_flip_mutex);
 #endif /* CONFIG_SMP */
 
 static int __init profile_setup(char * str)
@@ -243,7 +244,7 @@ static void profile_flip_buffers(void)
 {
 	int i, j, cpu;
 
-	down(&profile_flip_mutex);
+	mutex_lock(&profile_flip_mutex);
 	j = per_cpu(cpu_profile_flip, get_cpu());
 	put_cpu();
 	on_each_cpu(__profile_flip_buffers, NULL, 0, 1);
@@ -259,14 +260,14 @@ static void profile_flip_buffers(void)
 			hits[i].hits = hits[i].pc = 0;
 		}
 	}
-	up(&profile_flip_mutex);
+	mutex_unlock(&profile_flip_mutex);
 }
 
 static void profile_discard_flip_buffers(void)
 {
 	int i, cpu;
 
-	down(&profile_flip_mutex);
+	mutex_lock(&profile_flip_mutex);
 	i = per_cpu(cpu_profile_flip, get_cpu());
 	put_cpu();
 	on_each_cpu(__profile_flip_buffers, NULL, 0, 1);
@@ -274,7 +275,7 @@ static void profile_discard_flip_buffers(void)
 		struct profile_hit *hits = per_cpu(cpu_profile_hits, cpu)[i];
 		memset(hits, 0, NR_PROFILE_HIT*sizeof(struct profile_hit));
 	}
-	up(&profile_flip_mutex);
+	mutex_unlock(&profile_flip_mutex);
 }
 
 void profile_hit(int type, void *__pc)

commit 6c036527a630720063b67d9a65455e8caca2c8fa
Author: Christoph Lameter <christoph@lameter.com>
Date:   Thu Jul 7 17:56:59 2005 -0700

    [PATCH] mostly_read data section
    
    Add a new section called ".data.read_mostly" for data items that are read
    frequently and rarely written to like cpumaps etc.
    
    If these maps are placed in the .data section then these frequenly read
    items may end up in cachelines with data is is frequently updated.  In that
    case all processors in an SMP system must needlessly reload the cachelines
    again and again containing elements of those frequently used variables.
    
    The ability to share these cachelines will allow each cpu in an SMP system
    to keep local copies of those shared cachelines thereby optimizing
    performance.
    
    Signed-off-by: Alok N Kataria <alokk@calsoftinc.com>
    Signed-off-by: Shobhit Dayal <shobhit@calsoftinc.com>
    Signed-off-by: Christoph Lameter <christoph@scalex86.org>
    Signed-off-by: Shai Fultheim <shai@scalex86.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index ad8cbb75ffa2..f89248e6d704 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -35,11 +35,11 @@ struct profile_hit {
 #define NR_PROFILE_GRP		(NR_PROFILE_HIT/PROFILE_GRPSZ)
 
 /* Oprofile timer tick hook */
-int (*timer_hook)(struct pt_regs *);
+int (*timer_hook)(struct pt_regs *) __read_mostly;
 
 static atomic_t *prof_buffer;
 static unsigned long prof_len, prof_shift;
-static int prof_on;
+static int prof_on __read_mostly;
 static cpumask_t prof_cpu_mask = CPU_MASK_ALL;
 #ifdef CONFIG_SMP
 static DEFINE_PER_CPU(struct profile_hit *[2], cpu_profile_hits);

commit dfaa9c94b13071c9b5f8578d0ae99acc76c60139
Author: William Lee Irwin III <wli@holomorphy.com>
Date:   Mon May 16 21:53:58 2005 -0700

    [PATCH] profile.c: `schedule' parsing fix
    
    profile=schedule parsing is not quite what it should be.  First, str[7] is
    'e', not ',', but then even if it did fall through, prof_on =
    SCHED_PROFILING would be clobbered inside if (get_option(...)) So a small
    amount of rearrangement is done in this patch to correct it.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index 0221a50ca867..ad8cbb75ffa2 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -49,15 +49,19 @@ static DECLARE_MUTEX(profile_flip_mutex);
 
 static int __init profile_setup(char * str)
 {
+	static char __initdata schedstr[] = "schedule";
 	int par;
 
-	if (!strncmp(str, "schedule", 8)) {
+	if (!strncmp(str, schedstr, strlen(schedstr))) {
 		prof_on = SCHED_PROFILING;
-		printk(KERN_INFO "kernel schedule profiling enabled\n");
-		if (str[7] == ',')
-			str += 8;
-	}
-	if (get_option(&str,&par)) {
+		if (str[strlen(schedstr)] == ',')
+			str += strlen(schedstr) + 1;
+		if (get_option(&str, &par))
+			prof_shift = par;
+		printk(KERN_INFO
+			"kernel schedule profiling enabled (shift: %ld)\n",
+			prof_shift);
+	} else if (get_option(&str, &par)) {
 		prof_shift = par;
 		prof_on = CPU_PROFILING;
 		printk(KERN_INFO "kernel profiling enabled (shift: %ld)\n",

commit fbd568a3e61a7decb8a754ad952aaa5b5c82e9e5
Author: Paul E. McKenney <paulmck@us.ibm.com>
Date:   Sun May 1 08:59:04 2005 -0700

    [PATCH] Change synchronize_kernel to _rcu and _sched
    
    This patch changes calls to synchronize_kernel(), deprecated in the earlier
    "Deprecate synchronize_kernel, GPL replacement" patch to instead call the new
    synchronize_rcu() and synchronize_sched() APIs.
    
    Signed-off-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index a66be468c422..0221a50ca867 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -184,7 +184,7 @@ void unregister_timer_hook(int (*hook)(struct pt_regs *))
 	WARN_ON(hook != timer_hook);
 	timer_hook = NULL;
 	/* make sure all CPUs see the NULL hook */
-	synchronize_kernel();
+	synchronize_sched();  /* Allow ongoing interrupts to complete. */
 }
 
 EXPORT_SYMBOL_GPL(register_timer_hook);

commit d59dd4620fb8d6422555a9e2b82a707718e68327
Author: akpm@osdl.org <akpm@osdl.org>
Date:   Sun May 1 08:58:47 2005 -0700

    [PATCH] use smp_mb/wmb/rmb where possible
    
    Replace a number of memory barriers with smp_ variants.  This means we won't
    take the unnecessary hit on UP machines.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/profile.c b/kernel/profile.c
index a38fa70075fe..a66be468c422 100644
--- a/kernel/profile.c
+++ b/kernel/profile.c
@@ -522,7 +522,7 @@ static int __init create_hash_tables(void)
 	return 0;
 out_cleanup:
 	prof_on = 0;
-	mb();
+	smp_mb();
 	on_each_cpu(profile_nop, NULL, 0, 1);
 	for_each_online_cpu(cpu) {
 		struct page *page;

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/kernel/profile.c b/kernel/profile.c
new file mode 100644
index 000000000000..a38fa70075fe
--- /dev/null
+++ b/kernel/profile.c
@@ -0,0 +1,563 @@
+/*
+ *  linux/kernel/profile.c
+ *  Simple profiling. Manages a direct-mapped profile hit count buffer,
+ *  with configurable resolution, support for restricting the cpus on
+ *  which profiling is done, and switching between cpu time and
+ *  schedule() calls via kernel command line parameters passed at boot.
+ *
+ *  Scheduler profiling support, Arjan van de Ven and Ingo Molnar,
+ *	Red Hat, July 2004
+ *  Consolidation of architecture support code for profiling,
+ *	William Irwin, Oracle, July 2004
+ *  Amortized hit count accounting via per-cpu open-addressed hashtables
+ *	to resolve timer interrupt livelocks, William Irwin, Oracle, 2004
+ */
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/profile.h>
+#include <linux/bootmem.h>
+#include <linux/notifier.h>
+#include <linux/mm.h>
+#include <linux/cpumask.h>
+#include <linux/cpu.h>
+#include <linux/profile.h>
+#include <linux/highmem.h>
+#include <asm/sections.h>
+#include <asm/semaphore.h>
+
+struct profile_hit {
+	u32 pc, hits;
+};
+#define PROFILE_GRPSHIFT	3
+#define PROFILE_GRPSZ		(1 << PROFILE_GRPSHIFT)
+#define NR_PROFILE_HIT		(PAGE_SIZE/sizeof(struct profile_hit))
+#define NR_PROFILE_GRP		(NR_PROFILE_HIT/PROFILE_GRPSZ)
+
+/* Oprofile timer tick hook */
+int (*timer_hook)(struct pt_regs *);
+
+static atomic_t *prof_buffer;
+static unsigned long prof_len, prof_shift;
+static int prof_on;
+static cpumask_t prof_cpu_mask = CPU_MASK_ALL;
+#ifdef CONFIG_SMP
+static DEFINE_PER_CPU(struct profile_hit *[2], cpu_profile_hits);
+static DEFINE_PER_CPU(int, cpu_profile_flip);
+static DECLARE_MUTEX(profile_flip_mutex);
+#endif /* CONFIG_SMP */
+
+static int __init profile_setup(char * str)
+{
+	int par;
+
+	if (!strncmp(str, "schedule", 8)) {
+		prof_on = SCHED_PROFILING;
+		printk(KERN_INFO "kernel schedule profiling enabled\n");
+		if (str[7] == ',')
+			str += 8;
+	}
+	if (get_option(&str,&par)) {
+		prof_shift = par;
+		prof_on = CPU_PROFILING;
+		printk(KERN_INFO "kernel profiling enabled (shift: %ld)\n",
+			prof_shift);
+	}
+	return 1;
+}
+__setup("profile=", profile_setup);
+
+
+void __init profile_init(void)
+{
+	if (!prof_on) 
+		return;
+ 
+	/* only text is profiled */
+	prof_len = (_etext - _stext) >> prof_shift;
+	prof_buffer = alloc_bootmem(prof_len*sizeof(atomic_t));
+}
+
+/* Profile event notifications */
+ 
+#ifdef CONFIG_PROFILING
+ 
+static DECLARE_RWSEM(profile_rwsem);
+static DEFINE_RWLOCK(handoff_lock);
+static struct notifier_block * task_exit_notifier;
+static struct notifier_block * task_free_notifier;
+static struct notifier_block * munmap_notifier;
+ 
+void profile_task_exit(struct task_struct * task)
+{
+	down_read(&profile_rwsem);
+	notifier_call_chain(&task_exit_notifier, 0, task);
+	up_read(&profile_rwsem);
+}
+ 
+int profile_handoff_task(struct task_struct * task)
+{
+	int ret;
+	read_lock(&handoff_lock);
+	ret = notifier_call_chain(&task_free_notifier, 0, task);
+	read_unlock(&handoff_lock);
+	return (ret == NOTIFY_OK) ? 1 : 0;
+}
+
+void profile_munmap(unsigned long addr)
+{
+	down_read(&profile_rwsem);
+	notifier_call_chain(&munmap_notifier, 0, (void *)addr);
+	up_read(&profile_rwsem);
+}
+
+int task_handoff_register(struct notifier_block * n)
+{
+	int err = -EINVAL;
+
+	write_lock(&handoff_lock);
+	err = notifier_chain_register(&task_free_notifier, n);
+	write_unlock(&handoff_lock);
+	return err;
+}
+
+int task_handoff_unregister(struct notifier_block * n)
+{
+	int err = -EINVAL;
+
+	write_lock(&handoff_lock);
+	err = notifier_chain_unregister(&task_free_notifier, n);
+	write_unlock(&handoff_lock);
+	return err;
+}
+
+int profile_event_register(enum profile_type type, struct notifier_block * n)
+{
+	int err = -EINVAL;
+ 
+	down_write(&profile_rwsem);
+ 
+	switch (type) {
+		case PROFILE_TASK_EXIT:
+			err = notifier_chain_register(&task_exit_notifier, n);
+			break;
+		case PROFILE_MUNMAP:
+			err = notifier_chain_register(&munmap_notifier, n);
+			break;
+	}
+ 
+	up_write(&profile_rwsem);
+ 
+	return err;
+}
+
+ 
+int profile_event_unregister(enum profile_type type, struct notifier_block * n)
+{
+	int err = -EINVAL;
+ 
+	down_write(&profile_rwsem);
+ 
+	switch (type) {
+		case PROFILE_TASK_EXIT:
+			err = notifier_chain_unregister(&task_exit_notifier, n);
+			break;
+		case PROFILE_MUNMAP:
+			err = notifier_chain_unregister(&munmap_notifier, n);
+			break;
+	}
+
+	up_write(&profile_rwsem);
+	return err;
+}
+
+int register_timer_hook(int (*hook)(struct pt_regs *))
+{
+	if (timer_hook)
+		return -EBUSY;
+	timer_hook = hook;
+	return 0;
+}
+
+void unregister_timer_hook(int (*hook)(struct pt_regs *))
+{
+	WARN_ON(hook != timer_hook);
+	timer_hook = NULL;
+	/* make sure all CPUs see the NULL hook */
+	synchronize_kernel();
+}
+
+EXPORT_SYMBOL_GPL(register_timer_hook);
+EXPORT_SYMBOL_GPL(unregister_timer_hook);
+EXPORT_SYMBOL_GPL(task_handoff_register);
+EXPORT_SYMBOL_GPL(task_handoff_unregister);
+
+#endif /* CONFIG_PROFILING */
+
+EXPORT_SYMBOL_GPL(profile_event_register);
+EXPORT_SYMBOL_GPL(profile_event_unregister);
+
+#ifdef CONFIG_SMP
+/*
+ * Each cpu has a pair of open-addressed hashtables for pending
+ * profile hits. read_profile() IPI's all cpus to request them
+ * to flip buffers and flushes their contents to prof_buffer itself.
+ * Flip requests are serialized by the profile_flip_mutex. The sole
+ * use of having a second hashtable is for avoiding cacheline
+ * contention that would otherwise happen during flushes of pending
+ * profile hits required for the accuracy of reported profile hits
+ * and so resurrect the interrupt livelock issue.
+ *
+ * The open-addressed hashtables are indexed by profile buffer slot
+ * and hold the number of pending hits to that profile buffer slot on
+ * a cpu in an entry. When the hashtable overflows, all pending hits
+ * are accounted to their corresponding profile buffer slots with
+ * atomic_add() and the hashtable emptied. As numerous pending hits
+ * may be accounted to a profile buffer slot in a hashtable entry,
+ * this amortizes a number of atomic profile buffer increments likely
+ * to be far larger than the number of entries in the hashtable,
+ * particularly given that the number of distinct profile buffer
+ * positions to which hits are accounted during short intervals (e.g.
+ * several seconds) is usually very small. Exclusion from buffer
+ * flipping is provided by interrupt disablement (note that for
+ * SCHED_PROFILING profile_hit() may be called from process context).
+ * The hash function is meant to be lightweight as opposed to strong,
+ * and was vaguely inspired by ppc64 firmware-supported inverted
+ * pagetable hash functions, but uses a full hashtable full of finite
+ * collision chains, not just pairs of them.
+ *
+ * -- wli
+ */
+static void __profile_flip_buffers(void *unused)
+{
+	int cpu = smp_processor_id();
+
+	per_cpu(cpu_profile_flip, cpu) = !per_cpu(cpu_profile_flip, cpu);
+}
+
+static void profile_flip_buffers(void)
+{
+	int i, j, cpu;
+
+	down(&profile_flip_mutex);
+	j = per_cpu(cpu_profile_flip, get_cpu());
+	put_cpu();
+	on_each_cpu(__profile_flip_buffers, NULL, 0, 1);
+	for_each_online_cpu(cpu) {
+		struct profile_hit *hits = per_cpu(cpu_profile_hits, cpu)[j];
+		for (i = 0; i < NR_PROFILE_HIT; ++i) {
+			if (!hits[i].hits) {
+				if (hits[i].pc)
+					hits[i].pc = 0;
+				continue;
+			}
+			atomic_add(hits[i].hits, &prof_buffer[hits[i].pc]);
+			hits[i].hits = hits[i].pc = 0;
+		}
+	}
+	up(&profile_flip_mutex);
+}
+
+static void profile_discard_flip_buffers(void)
+{
+	int i, cpu;
+
+	down(&profile_flip_mutex);
+	i = per_cpu(cpu_profile_flip, get_cpu());
+	put_cpu();
+	on_each_cpu(__profile_flip_buffers, NULL, 0, 1);
+	for_each_online_cpu(cpu) {
+		struct profile_hit *hits = per_cpu(cpu_profile_hits, cpu)[i];
+		memset(hits, 0, NR_PROFILE_HIT*sizeof(struct profile_hit));
+	}
+	up(&profile_flip_mutex);
+}
+
+void profile_hit(int type, void *__pc)
+{
+	unsigned long primary, secondary, flags, pc = (unsigned long)__pc;
+	int i, j, cpu;
+	struct profile_hit *hits;
+
+	if (prof_on != type || !prof_buffer)
+		return;
+	pc = min((pc - (unsigned long)_stext) >> prof_shift, prof_len - 1);
+	i = primary = (pc & (NR_PROFILE_GRP - 1)) << PROFILE_GRPSHIFT;
+	secondary = (~(pc << 1) & (NR_PROFILE_GRP - 1)) << PROFILE_GRPSHIFT;
+	cpu = get_cpu();
+	hits = per_cpu(cpu_profile_hits, cpu)[per_cpu(cpu_profile_flip, cpu)];
+	if (!hits) {
+		put_cpu();
+		return;
+	}
+	local_irq_save(flags);
+	do {
+		for (j = 0; j < PROFILE_GRPSZ; ++j) {
+			if (hits[i + j].pc == pc) {
+				hits[i + j].hits++;
+				goto out;
+			} else if (!hits[i + j].hits) {
+				hits[i + j].pc = pc;
+				hits[i + j].hits = 1;
+				goto out;
+			}
+		}
+		i = (i + secondary) & (NR_PROFILE_HIT - 1);
+	} while (i != primary);
+	atomic_inc(&prof_buffer[pc]);
+	for (i = 0; i < NR_PROFILE_HIT; ++i) {
+		atomic_add(hits[i].hits, &prof_buffer[hits[i].pc]);
+		hits[i].pc = hits[i].hits = 0;
+	}
+out:
+	local_irq_restore(flags);
+	put_cpu();
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+static int __devinit profile_cpu_callback(struct notifier_block *info,
+					unsigned long action, void *__cpu)
+{
+	int node, cpu = (unsigned long)__cpu;
+	struct page *page;
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+		node = cpu_to_node(cpu);
+		per_cpu(cpu_profile_flip, cpu) = 0;
+		if (!per_cpu(cpu_profile_hits, cpu)[1]) {
+			page = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
+			if (!page)
+				return NOTIFY_BAD;
+			per_cpu(cpu_profile_hits, cpu)[1] = page_address(page);
+		}
+		if (!per_cpu(cpu_profile_hits, cpu)[0]) {
+			page = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
+			if (!page)
+				goto out_free;
+			per_cpu(cpu_profile_hits, cpu)[0] = page_address(page);
+		}
+		break;
+	out_free:
+		page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
+		per_cpu(cpu_profile_hits, cpu)[1] = NULL;
+		__free_page(page);
+		return NOTIFY_BAD;
+	case CPU_ONLINE:
+		cpu_set(cpu, prof_cpu_mask);
+		break;
+	case CPU_UP_CANCELED:
+	case CPU_DEAD:
+		cpu_clear(cpu, prof_cpu_mask);
+		if (per_cpu(cpu_profile_hits, cpu)[0]) {
+			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[0]);
+			per_cpu(cpu_profile_hits, cpu)[0] = NULL;
+			__free_page(page);
+		}
+		if (per_cpu(cpu_profile_hits, cpu)[1]) {
+			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
+			per_cpu(cpu_profile_hits, cpu)[1] = NULL;
+			__free_page(page);
+		}
+		break;
+	}
+	return NOTIFY_OK;
+}
+#endif /* CONFIG_HOTPLUG_CPU */
+#else /* !CONFIG_SMP */
+#define profile_flip_buffers()		do { } while (0)
+#define profile_discard_flip_buffers()	do { } while (0)
+
+void profile_hit(int type, void *__pc)
+{
+	unsigned long pc;
+
+	if (prof_on != type || !prof_buffer)
+		return;
+	pc = ((unsigned long)__pc - (unsigned long)_stext) >> prof_shift;
+	atomic_inc(&prof_buffer[min(pc, prof_len - 1)]);
+}
+#endif /* !CONFIG_SMP */
+
+void profile_tick(int type, struct pt_regs *regs)
+{
+	if (type == CPU_PROFILING && timer_hook)
+		timer_hook(regs);
+	if (!user_mode(regs) && cpu_isset(smp_processor_id(), prof_cpu_mask))
+		profile_hit(type, (void *)profile_pc(regs));
+}
+
+#ifdef CONFIG_PROC_FS
+#include <linux/proc_fs.h>
+#include <asm/uaccess.h>
+#include <asm/ptrace.h>
+
+static int prof_cpu_mask_read_proc (char *page, char **start, off_t off,
+			int count, int *eof, void *data)
+{
+	int len = cpumask_scnprintf(page, count, *(cpumask_t *)data);
+	if (count - len < 2)
+		return -EINVAL;
+	len += sprintf(page + len, "\n");
+	return len;
+}
+
+static int prof_cpu_mask_write_proc (struct file *file, const char __user *buffer,
+					unsigned long count, void *data)
+{
+	cpumask_t *mask = (cpumask_t *)data;
+	unsigned long full_count = count, err;
+	cpumask_t new_value;
+
+	err = cpumask_parse(buffer, count, new_value);
+	if (err)
+		return err;
+
+	*mask = new_value;
+	return full_count;
+}
+
+void create_prof_cpu_mask(struct proc_dir_entry *root_irq_dir)
+{
+	struct proc_dir_entry *entry;
+
+	/* create /proc/irq/prof_cpu_mask */
+	if (!(entry = create_proc_entry("prof_cpu_mask", 0600, root_irq_dir)))
+		return;
+	entry->nlink = 1;
+	entry->data = (void *)&prof_cpu_mask;
+	entry->read_proc = prof_cpu_mask_read_proc;
+	entry->write_proc = prof_cpu_mask_write_proc;
+}
+
+/*
+ * This function accesses profiling information. The returned data is
+ * binary: the sampling step and the actual contents of the profile
+ * buffer. Use of the program readprofile is recommended in order to
+ * get meaningful info out of these data.
+ */
+static ssize_t
+read_profile(struct file *file, char __user *buf, size_t count, loff_t *ppos)
+{
+	unsigned long p = *ppos;
+	ssize_t read;
+	char * pnt;
+	unsigned int sample_step = 1 << prof_shift;
+
+	profile_flip_buffers();
+	if (p >= (prof_len+1)*sizeof(unsigned int))
+		return 0;
+	if (count > (prof_len+1)*sizeof(unsigned int) - p)
+		count = (prof_len+1)*sizeof(unsigned int) - p;
+	read = 0;
+
+	while (p < sizeof(unsigned int) && count > 0) {
+		put_user(*((char *)(&sample_step)+p),buf);
+		buf++; p++; count--; read++;
+	}
+	pnt = (char *)prof_buffer + p - sizeof(atomic_t);
+	if (copy_to_user(buf,(void *)pnt,count))
+		return -EFAULT;
+	read += count;
+	*ppos += read;
+	return read;
+}
+
+/*
+ * Writing to /proc/profile resets the counters
+ *
+ * Writing a 'profiling multiplier' value into it also re-sets the profiling
+ * interrupt frequency, on architectures that support this.
+ */
+static ssize_t write_profile(struct file *file, const char __user *buf,
+			     size_t count, loff_t *ppos)
+{
+#ifdef CONFIG_SMP
+	extern int setup_profiling_timer (unsigned int multiplier);
+
+	if (count == sizeof(int)) {
+		unsigned int multiplier;
+
+		if (copy_from_user(&multiplier, buf, sizeof(int)))
+			return -EFAULT;
+
+		if (setup_profiling_timer(multiplier))
+			return -EINVAL;
+	}
+#endif
+	profile_discard_flip_buffers();
+	memset(prof_buffer, 0, prof_len * sizeof(atomic_t));
+	return count;
+}
+
+static struct file_operations proc_profile_operations = {
+	.read		= read_profile,
+	.write		= write_profile,
+};
+
+#ifdef CONFIG_SMP
+static void __init profile_nop(void *unused)
+{
+}
+
+static int __init create_hash_tables(void)
+{
+	int cpu;
+
+	for_each_online_cpu(cpu) {
+		int node = cpu_to_node(cpu);
+		struct page *page;
+
+		page = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
+		if (!page)
+			goto out_cleanup;
+		per_cpu(cpu_profile_hits, cpu)[1]
+				= (struct profile_hit *)page_address(page);
+		page = alloc_pages_node(node, GFP_KERNEL | __GFP_ZERO, 0);
+		if (!page)
+			goto out_cleanup;
+		per_cpu(cpu_profile_hits, cpu)[0]
+				= (struct profile_hit *)page_address(page);
+	}
+	return 0;
+out_cleanup:
+	prof_on = 0;
+	mb();
+	on_each_cpu(profile_nop, NULL, 0, 1);
+	for_each_online_cpu(cpu) {
+		struct page *page;
+
+		if (per_cpu(cpu_profile_hits, cpu)[0]) {
+			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[0]);
+			per_cpu(cpu_profile_hits, cpu)[0] = NULL;
+			__free_page(page);
+		}
+		if (per_cpu(cpu_profile_hits, cpu)[1]) {
+			page = virt_to_page(per_cpu(cpu_profile_hits, cpu)[1]);
+			per_cpu(cpu_profile_hits, cpu)[1] = NULL;
+			__free_page(page);
+		}
+	}
+	return -1;
+}
+#else
+#define create_hash_tables()			({ 0; })
+#endif
+
+static int __init create_proc_profile(void)
+{
+	struct proc_dir_entry *entry;
+
+	if (!prof_on)
+		return 0;
+	if (create_hash_tables())
+		return -1;
+	if (!(entry = create_proc_entry("profile", S_IWUSR | S_IRUGO, NULL)))
+		return 0;
+	entry->proc_fops = &proc_profile_operations;
+	entry->size = (1+prof_len) * sizeof(atomic_t);
+	hotcpu_notifier(profile_cpu_callback, 0);
+	return 0;
+}
+module_init(create_proc_profile);
+#endif /* CONFIG_PROC_FS */
