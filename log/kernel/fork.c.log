commit c17d1a3a8ee4dac7539d5c976b45d9300f6f10bc
Author: Weilong Chen <chenweilong@huawei.com>
Date:   Tue Jun 23 12:12:40 2020 +0800

    fork: annotate data race in copy_process()
    
    KCSAN reported data race reading and writing nr_threads and max_threads.
    The data race is intentional and benign. This is obvious from the comment
    above it and based on general consensus when discussing this issue. So
    there's no need for any heavy atomic or *_ONCE() machinery here.
    
    In accordance with the newly introduced data_race() annotation consensus,
    mark the offending line with data_race(). Here it's actually useful not
    just to silence KCSAN but to also clearly communicate that the race is
    intentional. This is especially helpful since nr_threads is otherwise
    protected by tasklist_lock.
    
    BUG: KCSAN: data-race in copy_process / copy_process
    
    write to 0xffffffff86205cf8 of 4 bytes by task 14779 on cpu 1:
      copy_process+0x2eba/0x3c40 kernel/fork.c:2273
      _do_fork+0xfe/0x7a0 kernel/fork.c:2421
      __do_sys_clone kernel/fork.c:2576 [inline]
      __se_sys_clone kernel/fork.c:2557 [inline]
      __x64_sys_clone+0x130/0x170 kernel/fork.c:2557
      do_syscall_64+0xcc/0x3a0 arch/x86/entry/common.c:294
      entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    read to 0xffffffff86205cf8 of 4 bytes by task 6944 on cpu 0:
      copy_process+0x94d/0x3c40 kernel/fork.c:1954
      _do_fork+0xfe/0x7a0 kernel/fork.c:2421
      __do_sys_clone kernel/fork.c:2576 [inline]
      __se_sys_clone kernel/fork.c:2557 [inline]
      __x64_sys_clone+0x130/0x170 kernel/fork.c:2557
      do_syscall_64+0xcc/0x3a0 arch/x86/entry/common.c:294
      entry_SYSCALL_64_after_hwframe+0x44/0xa9
    Link: https://groups.google.com/forum/#!msg/syzkaller-upstream-mo
    deration/thvp7AHs5Ew/aPdYLXfYBQAJ
    
    Reported-by: syzbot+52fced2d288f8ecd2b20@syzkaller.appspotmail.com
    Signed-off-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Weilong Chen <chenweilong@huawei.com>
    Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Christian Brauner <christian.brauner@ubuntu.com>
    Cc: Marco Elver <elver@google.com>
    [christian.brauner@ubuntu.com: rewrite commit message]
    Link: https://lore.kernel.org/r/20200623041240.154294-1-chenweilong@huawei.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 142b23645d82..efc5493203ae 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1977,7 +1977,7 @@ static __latent_entropy struct task_struct *copy_process(
 	 * to stop root fork bombs.
 	 */
 	retval = -EAGAIN;
-	if (nr_threads >= max_threads)
+	if (data_race(nr_threads >= max_threads))
 		goto bad_fork_cleanup_count;
 
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */

commit aaa2cc56c1cd757efec88a4978ffce4cbf884352
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:33 2020 -0700

    mmap locking API: convert nested write lock sites
    
    Add API for nested write locks and convert the few call sites doing that.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-7-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d61751ba10dc..142b23645d82 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -501,7 +501,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	/*
 	 * Not linked in yet - no deadlock potential:
 	 */
-	down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);
+	mmap_write_lock_nested(mm, SINGLE_DEPTH_NESTING);
 
 	/* No ordering required: file already has been exposed. */
 	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));

commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3603e14474cd..d61751ba10dc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -492,7 +492,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	LIST_HEAD(uf);
 
 	uprobe_start_dup_mmap();
-	if (down_write_killable(&oldmm->mmap_sem)) {
+	if (mmap_write_lock_killable(oldmm)) {
 		retval = -EINTR;
 		goto fail_uprobe_end;
 	}
@@ -617,9 +617,9 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	/* a new mm has just been created */
 	retval = arch_dup_mmap(oldmm, mm);
 out:
-	up_write(&mm->mmap_sem);
+	mmap_write_unlock(mm);
 	flush_tlb_mm(oldmm);
-	up_write(&oldmm->mmap_sem);
+	mmap_write_unlock(oldmm);
 	dup_userfaultfd_complete(&uf);
 fail_uprobe_end:
 	uprobe_end_dup_mmap();
@@ -649,9 +649,9 @@ static inline void mm_free_pgd(struct mm_struct *mm)
 #else
 static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 {
-	down_write(&oldmm->mmap_sem);
+	mmap_write_lock(oldmm);
 	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
-	up_write(&oldmm->mmap_sem);
+	mmap_write_unlock(oldmm);
 	return 0;
 }
 #define mm_alloc_pgd(mm)	(0)
@@ -1022,7 +1022,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	mm->vmacache_seqnum = 0;
 	atomic_set(&mm->mm_users, 1);
 	atomic_set(&mm->mm_count, 1);
-	init_rwsem(&mm->mmap_sem);
+	mmap_init_lock(mm);
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->core_state = NULL;
 	mm_pgtables_bytes_init(mm);

commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index cefe8745c46e..3603e14474cd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -96,7 +96,6 @@
 #include <linux/kasan.h>
 #include <linux/scs.h>
 
-#include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <linux/uaccess.h>
 #include <asm/mmu_context.h>

commit 9ff7258575d5fee011649d20cc56de720a395191
Merge: 051c3556e3d6 9d78edeaec75
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 4 13:54:34 2020 -0700

    Merge branch 'proc-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull proc updates from Eric Biederman:
     "This has four sets of changes:
    
       - modernize proc to support multiple private instances
    
       - ensure we see the exit of each process tid exactly
    
       - remove has_group_leader_pid
    
       - use pids not tasks in posix-cpu-timers lookup
    
      Alexey updated proc so each mount of proc uses a new superblock. This
      allows people to actually use mount options with proc with no fear of
      messing up another mount of proc. Given the kernel's internal mounts
      of proc for things like uml this was a real problem, and resulted in
      Android's hidepid mount options being ignored and introducing security
      issues.
    
      The rest of the changes are small cleanups and fixes that came out of
      my work to allow this change to proc. In essence it is swapping the
      pids in de_thread during exec which removes a special case the code
      had to handle. Then updating the code to stop handling that special
      case"
    
    * 'proc-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      proc: proc_pid_ns takes super_block as an argument
      remove the no longer needed pid_alive() check in __task_pid_nr_ns()
      posix-cpu-timers: Replace __get_task_for_clock with pid_for_clock
      posix-cpu-timers: Replace cpu_timer_pid_type with clock_pid_type
      posix-cpu-timers: Extend rcu_read_lock removing task_struct references
      signal: Remove has_group_leader_pid
      exec: Remove BUG_ON(has_group_leader_pid)
      posix-cpu-timer:  Unify the now redundant code in lookup_task
      posix-cpu-timer: Tidy up group_leader logic in lookup_task
      proc: Ensure we see the exit of each process tid exactly once
      rculist: Add hlists_swap_heads_rcu
      proc: Use PIDTYPE_TGID in next_tgid
      Use proc_pid_ns() to get pid_namespace from the proc superblock
      proc: use named enums for better readability
      proc: use human-readable values for hidepid
      docs: proc: add documentation for "hidepid=4" and "subset=pid" options and new mount behavior
      proc: add option to mount only a pids subset
      proc: instantiate only pids that we can ptrace on 'hidepid=4' mount option
      proc: allow to mount many instances of proc in one pid namespace
      proc: rename struct proc_fs_info to proc_fs_opts

commit 533b220f7be4e461a5222a223d169b42856741ef
Merge: 3ee3723b40d5 082af5ec5080
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 15:18:27 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "A sizeable pile of arm64 updates for 5.8.
    
      Summary below, but the big two features are support for Branch Target
      Identification and Clang's Shadow Call stack. The latter is currently
      arm64-only, but the high-level parts are all in core code so it could
      easily be adopted by other architectures pending toolchain support
    
      Branch Target Identification (BTI):
    
       - Support for ARMv8.5-BTI in both user- and kernel-space. This allows
         branch targets to limit the types of branch from which they can be
         called and additionally prevents branching to arbitrary code,
         although kernel support requires a very recent toolchain.
    
       - Function annotation via SYM_FUNC_START() so that assembly functions
         are wrapped with the relevant "landing pad" instructions.
    
       - BPF and vDSO updates to use the new instructions.
    
       - Addition of a new HWCAP and exposure of BTI capability to userspace
         via ID register emulation, along with ELF loader support for the
         BTI feature in .note.gnu.property.
    
       - Non-critical fixes to CFI unwind annotations in the sigreturn
         trampoline.
    
      Shadow Call Stack (SCS):
    
       - Support for Clang's Shadow Call Stack feature, which reserves
         platform register x18 to point at a separate stack for each task
         that holds only return addresses. This protects function return
         control flow from buffer overruns on the main stack.
    
       - Save/restore of x18 across problematic boundaries (user-mode,
         hypervisor, EFI, suspend, etc).
    
       - Core support for SCS, should other architectures want to use it
         too.
    
       - SCS overflow checking on context-switch as part of the existing
         stack limit check if CONFIG_SCHED_STACK_END_CHECK=y.
    
      CPU feature detection:
    
       - Removed numerous "SANITY CHECK" errors when running on a system
         with mismatched AArch32 support at EL1. This is primarily a concern
         for KVM, which disabled support for 32-bit guests on such a system.
    
       - Addition of new ID registers and fields as the architecture has
         been extended.
    
      Perf and PMU drivers:
    
       - Minor fixes and cleanups to system PMU drivers.
    
      Hardware errata:
    
       - Unify KVM workarounds for VHE and nVHE configurations.
    
       - Sort vendor errata entries in Kconfig.
    
      Secure Monitor Call Calling Convention (SMCCC):
    
       - Update to the latest specification from Arm (v1.2).
    
       - Allow PSCI code to query the SMCCC version.
    
      Software Delegated Exception Interface (SDEI):
    
       - Unexport a bunch of unused symbols.
    
       - Minor fixes to handling of firmware data.
    
      Pointer authentication:
    
       - Add support for dumping the kernel PAC mask in vmcoreinfo so that
         the stack can be unwound by tools such as kdump.
    
       - Simplification of key initialisation during CPU bringup.
    
      BPF backend:
    
       - Improve immediate generation for logical and add/sub instructions.
    
      vDSO:
    
       - Minor fixes to the linker flags for consistency with other
         architectures and support for LLVM's unwinder.
    
       - Clean up logic to initialise and map the vDSO into userspace.
    
      ACPI:
    
       - Work around for an ambiguity in the IORT specification relating to
         the "num_ids" field.
    
       - Support _DMA method for all named components rather than only PCIe
         root complexes.
    
       - Minor other IORT-related fixes.
    
      Miscellaneous:
    
       - Initialise debug traps early for KGDB and fix KDB cacheflushing
         deadlock.
    
       - Minor tweaks to early boot state (documentation update, set
         TEXT_OFFSET to 0x0, increase alignment of PE/COFF sections).
    
       - Refactoring and cleanup"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (148 commits)
      KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
      KVM: arm64: Check advertised Stage-2 page size capability
      arm64/cpufeature: Add get_arm64_ftr_reg_nowarn()
      ACPI/IORT: Remove the unused __get_pci_rid()
      arm64/cpuinfo: Add ID_MMFR4_EL1 into the cpuinfo_arm64 context
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR1 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR0 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64ISAR0 register
      arm64/cpufeature: Add remaining feature bits in ID_MMFR4 register
      arm64/cpufeature: Add remaining feature bits in ID_PFR0 register
      arm64/cpufeature: Introduce ID_MMFR5 CPU register
      arm64/cpufeature: Introduce ID_DFR1 CPU register
      arm64/cpufeature: Introduce ID_PFR2 CPU register
      arm64/cpufeature: Make doublelock a signed feature in ID_AA64DFR0
      arm64/cpufeature: Drop TraceFilt feature exposure from ID_DFR0 register
      arm64/cpufeature: Add explicit ftr_id_isar0[] for ID_ISAR0 register
      arm64: mm: Add asid_gen_match() helper
      firmware: smccc: Fix missing prototype warning for arm_smccc_version_init
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      ...

commit 2227e5b21aec6c5f7f6491352f0c19fd02d19418
Merge: 0bd957eb11cf cb3cb6733fbd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 12:56:29 2020 -0700

    Merge tag 'core-rcu-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The RCU updates for this cycle were:
    
       - RCU-tasks update, including addition of RCU Tasks Trace for BPF use
         and TASKS_RUDE_RCU
    
       - kfree_rcu() updates.
    
       - Remove scheduler locking restriction
    
       - RCU CPU stall warning updates.
    
       - Torture-test updates.
    
       - Miscellaneous fixes and other updates"
    
    * tag 'core-rcu-2020-06-01' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (103 commits)
      rcu: Allow for smp_call_function() running callbacks from idle
      rcu: Provide rcu_irq_exit_check_preempt()
      rcu: Abstract out rcu_irq_enter_check_tick() from rcu_nmi_enter()
      rcu: Provide __rcu_is_watching()
      rcu: Provide rcu_irq_exit_preempt()
      rcu: Make RCU IRQ enter/exit functions rely on in_nmi()
      rcu/tree: Mark the idle relevant functions noinstr
      x86: Replace ist_enter() with nmi_enter()
      x86/mce: Send #MC singal from task work
      x86/entry: Get rid of ist_begin/end_non_atomic()
      sched,rcu,tracing: Avoid tracing before in_nmi() is correct
      sh/ftrace: Move arch_ftrace_nmi_{enter,exit} into nmi exception
      lockdep: Always inline lockdep_{off,on}()
      hardirq/nmi: Allow nested nmi_enter()
      arm64: Prepare arch_nmi_enter() for recursion
      printk: Disallow instrumenting print_nmi_enter()
      printk: Prepare for nested printk_nmi_enter()
      rcutorture: Convert ULONG_CMP_LT() to time_before()
      torture: Add a --kasan argument
      torture: Save a few lines by using config_override_param initially
      ...

commit 9d78edeaec759f997c303f286ecd39daee166f2a
Author: Alexey Gladkov <gladkov.alexey@gmail.com>
Date:   Mon May 18 20:07:38 2020 +0200

    proc: proc_pid_ns takes super_block as an argument
    
    syzbot found that
    
      touch /proc/testfile
    
    causes NULL pointer dereference at tomoyo_get_local_path()
    because inode of the dentry is NULL.
    
    Before c59f415a7cb6, Tomoyo received pid_ns from proc's s_fs_info
    directly. Since proc_pid_ns() can only work with inode, using it in
    the tomoyo_get_local_path() was wrong.
    
    To avoid creating more functions for getting proc_ns, change the
    argument type of the proc_pid_ns() function. Then, Tomoyo can use
    the existing super_block to get pid_ns.
    
    Link: https://lkml.kernel.org/r/0000000000002f0c7505a5b0e04c@google.com
    Link: https://lkml.kernel.org/r/20200518180738.2939611-1-gladkov.alexey@gmail.com
    Reported-by: syzbot+c1af344512918c61362c@syzkaller.appspotmail.com
    Fixes: c59f415a7cb6 ("Use proc_pid_ns() to get pid_namespace from the proc superblock")
    Signed-off-by: Alexey Gladkov <gladkov.alexey@gmail.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4385f3d639f2..e7bdaccad942 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1745,7 +1745,7 @@ static void pidfd_show_fdinfo(struct seq_file *m, struct file *f)
 	pid_t nr = -1;
 
 	if (likely(pid_has_task(pid, PIDTYPE_PID))) {
-		ns = proc_pid_ns(file_inode(m->file));
+		ns = proc_pid_ns(file_inode(m->file)->i_sb);
 		nr = pid_nr_ns(pid, ns);
 	}
 

commit d08b9f0ca6605e13dcb48f04e55a30545b3c71eb
Author: Sami Tolvanen <samitolvanen@google.com>
Date:   Mon Apr 27 09:00:07 2020 -0700

    scs: Add support for Clang's Shadow Call Stack (SCS)
    
    This change adds generic support for Clang's Shadow Call Stack,
    which uses a shadow stack to protect return addresses from being
    overwritten by an attacker. Details are available here:
    
      https://clang.llvm.org/docs/ShadowCallStack.html
    
    Note that security guarantees in the kernel differ from the ones
    documented for user space. The kernel must store addresses of
    shadow stacks in memory, which means an attacker capable reading
    and writing arbitrary memory may be able to locate them and hijack
    control flow by modifying the stacks.
    
    Signed-off-by: Sami Tolvanen <samitolvanen@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
    [will: Numerous cosmetic changes]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8c700f881d92..f6339f9d232d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -94,6 +94,7 @@
 #include <linux/thread_info.h>
 #include <linux/stackleak.h>
 #include <linux/kasan.h>
+#include <linux/scs.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -456,6 +457,8 @@ void put_task_stack(struct task_struct *tsk)
 
 void free_task(struct task_struct *tsk)
 {
+	scs_release(tsk);
+
 #ifndef CONFIG_THREAD_INFO_IN_TASK
 	/*
 	 * The task is finally done with both the stack and thread_info,
@@ -840,6 +843,8 @@ void __init fork_init(void)
 			  NULL, free_vm_stack_cache);
 #endif
 
+	scs_init();
+
 	lockdep_init_task(&init_task);
 	uprobes_init();
 }
@@ -899,6 +904,10 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	if (err)
 		goto free_stack;
 
+	err = scs_prepare(tsk, node);
+	if (err)
+		goto free_stack;
+
 #ifdef CONFIG_SECCOMP
 	/*
 	 * We must handle setting up seccomp filters once we're under

commit 3f2c788a13143620c5471ac96ac4f033fc9ac3f3
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Thu May 7 12:32:14 2020 +0200

    fork: prevent accidental access to clone3 features
    
    Jan reported an issue where an interaction between sign-extending clone's
    flag argument on ppc64le and the new CLONE_INTO_CGROUP feature causes
    clone() to consistently fail with EBADF.
    
    The whole story is a little longer. The legacy clone() syscall is odd in a
    bunch of ways and here two things interact. First, legacy clone's flag
    argument is word-size dependent, i.e. it's an unsigned long whereas most
    system calls with flag arguments use int or unsigned int. Second, legacy
    clone() ignores unknown and deprecated flags. The two of them taken
    together means that users on 64bit systems can pass garbage for the upper
    32bit of the clone() syscall since forever and things would just work fine.
    Just try this on a 64bit kernel prior to v5.7-rc1 where this will succeed
    and on v5.7-rc1 where this will fail with EBADF:
    
    int main(int argc, char *argv[])
    {
            pid_t pid;
    
            /* Note that legacy clone() has different argument ordering on
             * different architectures so this won't work everywhere.
             *
             * Only set the upper 32 bits.
             */
            pid = syscall(__NR_clone, 0xffffffff00000000 | SIGCHLD,
                          NULL, NULL, NULL, NULL);
            if (pid < 0)
                    exit(EXIT_FAILURE);
            if (pid == 0)
                    exit(EXIT_SUCCESS);
            if (wait(NULL) != pid)
                    exit(EXIT_FAILURE);
    
            exit(EXIT_SUCCESS);
    }
    
    Since legacy clone() couldn't be extended this was not a problem so far and
    nobody really noticed or cared since nothing in the kernel ever bothered to
    look at the upper 32 bits.
    
    But once we introduced clone3() and expanded the flag argument in struct
    clone_args to 64 bit we opened this can of worms. With the first flag-based
    extension to clone3() making use of the upper 32 bits of the flag argument
    we've effectively made it possible for the legacy clone() syscall to reach
    clone3() only flags. The sign extension scenario is just the odd
    corner-case that we needed to figure this out.
    
    The reason we just realized this now and not already when we introduced
    CLONE_CLEAR_SIGHAND was that CLONE_INTO_CGROUP assumes that a valid cgroup
    file descriptor has been given. So the sign extension (or the user
    accidently passing garbage for the upper 32 bits) caused the
    CLONE_INTO_CGROUP bit to be raised and the kernel to error out when it
    didn't find a valid cgroup file descriptor.
    
    Let's fix this by always capping the upper 32 bits for all codepaths that
    are not aware of clone3() features. This ensures that we can't reach
    clone3() only features by accident via legacy clone as with the sign
    extension case and also that legacy clone() works exactly like before, i.e.
    ignoring any unknown flags.  This solution risks no regressions and is also
    pretty clean.
    
    Fixes: 7f192e3cd316 ("fork: add clone3")
    Fixes: ef2c41cf38a7 ("clone3: allow spawning processes into cgroups")
    Reported-by: Jan Stancek <jstancek@redhat.com>
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dmitry V. Levin <ldv@altlinux.org>
    Cc: Andreas Schwab <schwab@linux-m68k.org>
    Cc: Florian Weimer <fw@deneb.enyo.de>
    Cc: libc-alpha@sourceware.org
    Cc: stable@vger.kernel.org # 5.3+
    Link: https://sourceware.org/pipermail/libc-alpha/2020-May/113596.html
    Link: https://lore.kernel.org/r/20200507103214.77218-1-christian.brauner@ubuntu.com

diff --git a/kernel/fork.c b/kernel/fork.c
index 8c700f881d92..48ed22774efa 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2486,11 +2486,11 @@ long do_fork(unsigned long clone_flags,
 	      int __user *child_tidptr)
 {
 	struct kernel_clone_args args = {
-		.flags		= (clone_flags & ~CSIGNAL),
+		.flags		= (lower_32_bits(clone_flags) & ~CSIGNAL),
 		.pidfd		= parent_tidptr,
 		.child_tid	= child_tidptr,
 		.parent_tid	= parent_tidptr,
-		.exit_signal	= (clone_flags & CSIGNAL),
+		.exit_signal	= (lower_32_bits(clone_flags) & CSIGNAL),
 		.stack		= stack_start,
 		.stack_size	= stack_size,
 	};
@@ -2508,8 +2508,9 @@ long do_fork(unsigned long clone_flags,
 pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 {
 	struct kernel_clone_args args = {
-		.flags		= ((flags | CLONE_VM | CLONE_UNTRACED) & ~CSIGNAL),
-		.exit_signal	= (flags & CSIGNAL),
+		.flags		= ((lower_32_bits(flags) | CLONE_VM |
+				    CLONE_UNTRACED) & ~CSIGNAL),
+		.exit_signal	= (lower_32_bits(flags) & CSIGNAL),
 		.stack		= (unsigned long)fn,
 		.stack_size	= (unsigned long)arg,
 	};
@@ -2570,11 +2571,11 @@ SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 #endif
 {
 	struct kernel_clone_args args = {
-		.flags		= (clone_flags & ~CSIGNAL),
+		.flags		= (lower_32_bits(clone_flags) & ~CSIGNAL),
 		.pidfd		= parent_tidptr,
 		.child_tid	= child_tidptr,
 		.parent_tid	= parent_tidptr,
-		.exit_signal	= (clone_flags & CSIGNAL),
+		.exit_signal	= (lower_32_bits(clone_flags) & CSIGNAL),
 		.stack		= newsp,
 		.tls		= tls,
 	};

commit 276c410448dbca357a2bc3539acfe04862e5f172
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Tue Mar 17 16:02:06 2020 -0700

    rcu-tasks: Split ->trc_reader_need_end
    
    This commit splits ->trc_reader_need_end by using the rcu_special union.
    This change permits readers to check to see if a memory barrier is
    required without any added overhead in the common case where no such
    barrier is required.  This commit also adds the read-side checking.
    Later commits will add the machinery to properly set the new
    ->trc_reader_special.b.need_mb field.
    
    This commit also makes rcu_read_unlock_trace_special() tolerate nested
    read-side critical sections within interrupt and NMI handlers.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 72e9396235b4..96eb4b535ced 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1685,6 +1685,7 @@ static inline void rcu_copy_process(struct task_struct *p)
 #endif /* #ifdef CONFIG_TASKS_RCU */
 #ifdef CONFIG_TASKS_TRACE_RCU
 	p->trc_reader_nesting = 0;
+	p->trc_reader_special.s = 0;
 	INIT_LIST_HEAD(&p->trc_holdout_list);
 #endif /* #ifdef CONFIG_TASKS_TRACE_RCU */
 }

commit d5f177d35c24429c87db2567d20563fc16f7e8f6
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Mon Mar 9 19:56:53 2020 -0700

    rcu-tasks: Add an RCU Tasks Trace to simplify protection of tracing hooks
    
    Because RCU does not watch exception early-entry/late-exit, idle-loop,
    or CPU-hotplug execution, protection of tracing and BPF operations is
    needlessly complicated.  This commit therefore adds a variant of
    Tasks RCU that:
    
    o       Has explicit read-side markers to allow finite grace periods in
            the face of in-kernel loops for PREEMPT=n builds.  These markers
            are rcu_read_lock_trace() and rcu_read_unlock_trace().
    
    o       Protects code in the idle loop, exception entry/exit, and
            CPU-hotplug code paths.  In this respect, RCU-tasks trace is
            similar to SRCU, but with lighter-weight readers.
    
    o       Avoids expensive read-side instruction, having overhead similar
            to that of Preemptible RCU.
    
    There are of course downsides:
    
    o       The grace-period code can send IPIs to CPUs, even when those
            CPUs are in the idle loop or in nohz_full userspace.  This is
            mitigated by later commits.
    
    o       It is necessary to scan the full tasklist, much as for Tasks RCU.
    
    o       There is a single callback queue guarded by a single lock,
            again, much as for Tasks RCU.  However, those early use cases
            that request multiple grace periods in quick succession are
            expected to do so from a single task, which makes the single
            lock almost irrelevant.  If needed, multiple callback queues
            can be provided using any number of schemes.
    
    Perhaps most important, this variant of RCU does not affect the vanilla
    flavors, rcu_preempt and rcu_sched.  The fact that RCU Tasks Trace
    readers can operate from idle, offline, and exception entry/exit in no
    way enables rcu_preempt and rcu_sched readers to do so.
    
    The memory ordering was outlined here:
    https://lore.kernel.org/lkml/20200319034030.GX3199@paulmck-ThinkPad-P72/
    
    This effort benefited greatly from off-list discussions of BPF
    requirements with Alexei Starovoitov and Andrii Nakryiko.  At least
    some of the on-list discussions are captured in the Link: tags below.
    In addition, KCSAN was quite helpful in finding some early bugs.
    
    Link: https://lore.kernel.org/lkml/20200219150744.428764577@infradead.org/
    Link: https://lore.kernel.org/lkml/87mu8p797b.fsf@nanos.tec.linutronix.de/
    Link: https://lore.kernel.org/lkml/20200225221305.605144982@linutronix.de/
    Cc: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Cc: Andrii Nakryiko <andriin@fb.com>
    [ paulmck: Apply feedback from Steve Rostedt and Joel Fernandes. ]
    [ paulmck: Decrement trc_n_readers_need_end upon IPI failure. ]
    [ paulmck: Fix locking issue reported by rcutorture. ]
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8c700f881d92..72e9396235b4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1683,6 +1683,10 @@ static inline void rcu_copy_process(struct task_struct *p)
 	INIT_LIST_HEAD(&p->rcu_tasks_holdout_list);
 	p->rcu_tasks_idle_cpu = -1;
 #endif /* #ifdef CONFIG_TASKS_RCU */
+#ifdef CONFIG_TASKS_TRACE_RCU
+	p->trc_reader_nesting = 0;
+	INIT_LIST_HEAD(&p->trc_holdout_list);
+#endif /* #ifdef CONFIG_TASKS_TRACE_RCU */
 }
 
 struct pid *pidfd_pid(const struct file *file)

commit a966dcfe153ab0a3d8d79cd971a079411a489be7
Author: Eugene Syromiatnikov <esyr@redhat.com>
Date:   Sun Apr 12 22:26:58 2020 +0200

    clone3: add build-time CLONE_ARGS_SIZE_VER* validity checks
    
    CLONE_ARGS_SIZE_VER* macros are defined explicitly and not via
    the offsets of the relevant struct clone_args fields, which makes
    it rather error-prone, so it probably makes sense to add some
    compile-time checks for them (including the one that breaks
    on struct clone_args extension as a reminder to add a relevant
    size macro and a similar check).  Function copy_clone_args_from_user
    seems to be a good place for such checks.
    
    Signed-off-by: Eugene Syromiatnikov <esyr@redhat.com>
    Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
    Link: https://lore.kernel.org/r/20200412202658.GA31499@asgard.redhat.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3ab7cf88e455..8c700f881d92 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2605,6 +2605,14 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 	struct clone_args args;
 	pid_t *kset_tid = kargs->set_tid;
 
+	BUILD_BUG_ON(offsetofend(struct clone_args, tls) !=
+		     CLONE_ARGS_SIZE_VER0);
+	BUILD_BUG_ON(offsetofend(struct clone_args, set_tid_size) !=
+		     CLONE_ARGS_SIZE_VER1);
+	BUILD_BUG_ON(offsetofend(struct clone_args, cgroup) !=
+		     CLONE_ARGS_SIZE_VER2);
+	BUILD_BUG_ON(sizeof(struct clone_args) != CLONE_ARGS_SIZE_VER2);
+
 	if (unlikely(usize > PAGE_SIZE))
 		return -E2BIG;
 	if (unlikely(usize < CLONE_ARGS_SIZE_VER0))

commit 62173872ca65767c586217dec0a32485da8a2f07
Author: Eugene Syromiatnikov <esyr@redhat.com>
Date:   Sun Apr 12 22:31:23 2020 +0200

    clone3: add a check for the user struct size if CLONE_INTO_CGROUP is set
    
    Passing CLONE_INTO_CGROUP with an under-sized structure (that doesn't
    properly contain cgroup field) seems like garbage input, especially
    considering the fact that fd 0 is a valid descriptor.
    
    Signed-off-by: Eugene Syromiatnikov <esyr@redhat.com>
    Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
    Link: https://lore.kernel.org/r/20200412203123.GA5869@asgard.redhat.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index b4f7775623c8..3ab7cf88e455 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2631,7 +2631,8 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 		     !valid_signal(args.exit_signal)))
 		return -EINVAL;
 
-	if ((args.flags & CLONE_INTO_CGROUP) && args.cgroup > INT_MAX)
+	if ((args.flags & CLONE_INTO_CGROUP) &&
+	    (args.cgroup > INT_MAX || usize < CLONE_ARGS_SIZE_VER2))
 		return -EINVAL;
 
 	*kargs = (struct kernel_clone_args){

commit e82a118f57b89bbb437ce70780fc2678d5c281e5
Author: Eugene Syromiatnikov <esyr@redhat.com>
Date:   Sun Apr 12 22:25:33 2020 +0200

    clone3: fix cgroup argument sanity check
    
    Checking that cgroup field value of struct clone_args is less than 0
    is useless, as it is defined as unsigned 64-bit integer.  Moreover,
    it doesn't catch the situations where its higher bits are lost during
    the assignment to the cgroup field of the cgroup field of the internal
    struct kernel_clone_args (where it is declared as signed 32-bit
    integer), so it is still possible to pass garbage there.  A check
    against INT_MAX solves both these issues.
    
    Fixes: ef2c41cf38a7559b ("clone3: allow spawning processes into cgroups")
    Signed-off-by: Eugene Syromiatnikov <esyr@redhat.com>
    Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
    Link: https://lore.kernel.org/r/20200412202533.GA29554@asgard.redhat.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4385f3d639f2..b4f7775623c8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2631,7 +2631,7 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 		     !valid_signal(args.exit_signal)))
 		return -EINVAL;
 
-	if ((args.flags & CLONE_INTO_CGROUP) && args.cgroup < 0)
+	if ((args.flags & CLONE_INTO_CGROUP) && args.cgroup > INT_MAX)
 		return -EINVAL;
 
 	*kargs = (struct kernel_clone_args){

commit e39a4b332df69f6850efda5ce6d932ef14a39042
Author: Li Xinhai <lixinhai.lxh@gmail.com>
Date:   Mon Apr 6 20:03:39 2020 -0700

    mm: set vm_next and vm_prev to NULL in vm_area_dup()
    
    Set ->vm_next and ->vm_prev to NULL to prevent potential misuse from the
    new duplicated vma.
    
    Currently, only in fork path there are misuse for handling anon_vma.  No
    other bugs been revealed with this patch applied.
    
    Signed-off-by: Li Xinhai <lixinhai.lxh@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Rik van Riel <riel@redhat.com>
    Link: http://lkml.kernel.org/r/1581150928-3214-4-git-send-email-lixinhai.lxh@gmail.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d3a5915d3cfe..4385f3d639f2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -361,6 +361,7 @@ struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig)
 	if (new) {
 		*new = *orig;
 		INIT_LIST_HEAD(&new->anon_vma_chain);
+		new->vm_next = new->vm_prev = NULL;
 	}
 	return new;
 }
@@ -562,7 +563,6 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 		} else if (anon_vma_fork(tmp, mpnt))
 			goto fail_nomem_anon_vma_fork;
 		tmp->vm_flags &= ~(VM_LOCKED | VM_LOCKONFAULT);
-		tmp->vm_next = tmp->vm_prev = NULL;
 		file = tmp->vm_file;
 		if (file) {
 			struct inode *inode = file_inode(file);

commit 93949bb21b52aae924fd26f9775c8655dd9e885e
Author: Li Xinhai <lixinhai.lxh@gmail.com>
Date:   Mon Apr 6 20:03:33 2020 -0700

    mm: don't prepare anon_vma if vma has VM_WIPEONFORK
    
    Patch series "mm: Fix misuse of parent anon_vma in dup_mmap path".
    
    This patchset fixes the misuse of parenet anon_vma, which mainly caused by
    child vma's vm_next and vm_prev are left same as its parent after
    duplicate vma.  Finally, code reached parent vma's neighbor by referring
    pointer of child vma and executed wrong logic.
    
    The first two patches fix relevant issues, and the third patch sets
    vm_next and vm_prev to NULL when duplicate vma to prevent potential misuse
    in future.
    
    Effects of the first bug is that causes rmap code to check both parent and
    child's page table, although a page couldn't be mapped by both parent and
    child, because child vma has WIPEONFORK so all pages mapped by child are
    'new' and not relevant to parent.
    
    Effects of the second bug is that the relationship of anon_vma of parent
    and child are totallyconvoluted.  It would cause 'son', 'grandson', ...,
    etc, to share 'parent' anon_vma, which disobey the design rule of reusing
    anon_vma (the rule to be followed is that reusing should among vma of same
    process, and vma should not gone through fork).
    
    So, both issues should cause unnecessary rmap walking and have unexpected
    complexity.
    
    These two issues would not be directly visible, I used debugging code to
    check the anon_vma pointers of parent and child when inspecting the
    suspicious implementation of issue #2, then find the problem.
    
    This patch (of 3):
    
    In dup_mmap(), anon_vma_prepare() is called for vma has VM_WIPEONFORK, and
    parameter 'tmp' (i.e., the new vma of child) has same ->vm_next and
    ->vm_prev as its parent vma.  That allows anon_vma used by parent been
    mistakenly shared by child (find_mergeable_anon_vma() will do this reuse
    work).
    
    Besides this issue, call anon_vma_prepare() should be avoided because we
    don't copy page for this vma.  Preparing anon_vma will be handled during
    fault.
    
    Fixes: d2cd9ede6e19 ("mm,fork: introduce MADV_WIPEONFORK")
    Signed-off-by: Li Xinhai <lixinhai.lxh@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Link: http://lkml.kernel.org/r/1581150928-3214-2-git-send-email-lixinhai.lxh@gmail.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d2a967bf85d5..d3a5915d3cfe 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -553,10 +553,12 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 		if (retval)
 			goto fail_nomem_anon_vma_fork;
 		if (tmp->vm_flags & VM_WIPEONFORK) {
-			/* VM_WIPEONFORK gets a clean slate in the child. */
+			/*
+			 * VM_WIPEONFORK gets a clean slate in the child.
+			 * Don't prepare anon_vma until fault since we don't
+			 * copy page for current vma.
+			 */
 			tmp->anon_vma = NULL;
-			if (anon_vma_prepare(tmp))
-				goto fail_nomem_anon_vma_fork;
 		} else if (anon_vma_fork(tmp, mpnt))
 			goto fail_nomem_anon_vma_fork;
 		tmp->vm_flags &= ~(VM_LOCKED | VM_LOCKONFAULT);

commit d8836005236425cf3cfcc8967abd1d5c21f607f8
Merge: f2c3bec3c90d 0c05b9bdbfe5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 3 11:30:20 2020 -0700

    Merge branch 'for-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - Christian extended clone3 so that processes can be spawned into
       cgroups directly.
    
       This is not only neat in terms of semantics but also avoids grabbing
       the global cgroup_threadgroup_rwsem for migration.
    
     - Daniel added !root xattr support to cgroupfs.
    
       Userland already uses xattrs on cgroupfs for bookkeeping. This will
       allow delegated cgroups to support such usages.
    
     - Prateek tried to make cpuset hotplug handling synchronous but that
       led to possible deadlock scenarios. Reverted.
    
     - Other minor changes including release_agent_path handling cleanup.
    
    * 'for-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      docs: cgroup-v1: Document the cpuset_v2_mode mount option
      Revert "cpuset: Make cpuset hotplug synchronous"
      cgroupfs: Support user xattrs
      kernfs: Add option to enable user xattrs
      kernfs: Add removed_size out param for simple_xattr_set
      kernfs: kvmalloc xattr value instead of kmalloc
      cgroup: Restructure release_agent_path handling
      selftests/cgroup: add tests for cloning into cgroups
      clone3: allow spawning processes into cgroups
      cgroup: add cgroup_may_write() helper
      cgroup: refactor fork helpers
      cgroup: add cgroup_get_from_file() helper
      cgroup: unify attach permission checking
      cpuset: Make cpuset hotplug synchronous
      cgroup.c: Use built-in RCU list checking
      kselftest/cgroup: add cgroup destruction test
      cgroup: Clean up css_set task traversal

commit 6cad420cc695867b4ca710bac21fde21a4102e4b
Merge: 7be97138e727 77d6b9094819
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 2 13:55:34 2020 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
     "A large amount of MM, plenty more to come.
    
      Subsystems affected by this patch series:
       - tools
       - kthread
       - kbuild
       - scripts
       - ocfs2
       - vfs
       - mm: slub, kmemleak, pagecache, gup, swap, memcg, pagemap, mremap,
             sparsemem, kasan, pagealloc, vmscan, compaction, mempolicy,
             hugetlbfs, hugetlb"
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (155 commits)
      include/linux/huge_mm.h: check PageTail in hpage_nr_pages even when !THP
      mm/hugetlb: fix build failure with HUGETLB_PAGE but not HUGEBTLBFS
      selftests/vm: fix map_hugetlb length used for testing read and write
      mm/hugetlb: remove unnecessary memory fetch in PageHeadHuge()
      mm/hugetlb.c: clean code by removing unnecessary initialization
      hugetlb_cgroup: add hugetlb_cgroup reservation docs
      hugetlb_cgroup: add hugetlb_cgroup reservation tests
      hugetlb: support file_region coalescing again
      hugetlb_cgroup: support noreserve mappings
      hugetlb_cgroup: add accounting for shared mappings
      hugetlb: disable region_add file_region coalescing
      hugetlb_cgroup: add reservation accounting for private mappings
      mm/hugetlb_cgroup: fix hugetlb_cgroup migration
      hugetlb_cgroup: add interface for charge/uncharge hugetlb reservations
      hugetlb_cgroup: add hugetlb_cgroup reservation counter
      hugetlbfs: Use i_mmap_rwsem to address page fault/truncate race
      hugetlbfs: use i_mmap_rwsem for more pmd sharing synchronization
      mm/memblock.c: remove redundant assignment to variable max_addr
      mm: mempolicy: require at least one nodeid for MPOL_PREFERRED
      mm: mempolicy: use VM_BUG_ON_VMA in queue_pages_test_walk()
      ...

commit d987ca1c6b7e22fbd30664111e85cec7aa66000d
Merge: 919dce24701f d1e7fd6462ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 2 11:22:17 2020 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull exec/proc updates from Eric Biederman:
     "This contains two significant pieces of work: the work to sort out
      proc_flush_task, and the work to solve a deadlock between strace and
      exec.
    
      Fixing proc_flush_task so that it no longer requires a persistent
      mount makes improvements to proc possible. The removal of the
      persistent mount solves an old regression that that caused the hidepid
      mount option to only work on remount not on mount. The regression was
      found and reported by the Android folks. This further allows Alexey
      Gladkov's work making proc mount options specific to an individual
      mount of proc to move forward.
    
      The work on exec starts solving a long standing issue with exec that
      it takes mutexes of blocking userspace applications, which makes exec
      extremely deadlock prone. For the moment this adds a second mutex with
      a narrower scope that handles all of the easy cases. Which makes the
      tricky cases easy to spot. With a little luck the code to solve those
      deadlocks will be ready by next merge window"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (25 commits)
      signal: Extend exec_id to 64bits
      pidfd: Use new infrastructure to fix deadlocks in execve
      perf: Use new infrastructure to fix deadlocks in execve
      proc: io_accounting: Use new infrastructure to fix deadlocks in execve
      proc: Use new infrastructure to fix deadlocks in execve
      kernel/kcmp.c: Use new infrastructure to fix deadlocks in execve
      kernel: doc: remove outdated comment cred.c
      mm: docs: Fix a comment in process_vm_rw_core
      selftests/ptrace: add test cases for dead-locks
      exec: Fix a deadlock in strace
      exec: Add exec_update_mutex to replace cred_guard_mutex
      exec: Move exec_mmap right after de_thread in flush_old_exec
      exec: Move cleanup of posix timers on exec out of de_thread
      exec: Factor unshare_sighand out of de_thread and call it separately
      exec: Only compute current once in flush_old_exec
      pid: Improve the comment about waiting in zap_pid_ns_processes
      proc: Remove the now unnecessary internal mount of proc
      uml: Create a private mount of proc for mconsole
      uml: Don't consult current to find the proc_mnt in mconsole_proc
      proc: Use a list of inodes to flush from proc
      ...

commit f4b00eab5004e823f28a268580ae4ed16df9fabf
Author: Roman Gushchin <guro@fb.com>
Date:   Wed Apr 1 21:06:46 2020 -0700

    mm: kmem: rename memcg_kmem_(un)charge() into memcg_kmem_(un)charge_page()
    
    Rename (__)memcg_kmem_(un)charge() into (__)memcg_kmem_(un)charge_page()
    to better reflect what they are actually doing:
    
    1) call __memcg_kmem_(un)charge_memcg() to actually charge or uncharge
       the current memcg
    
    2) set or clear the PageKmemcg flag
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Link: http://lkml.kernel.org/r/20200109202659.752357-4-guro@fb.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d90af13431c7..e37e12c203d1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -281,7 +281,7 @@ static inline void free_thread_stack(struct task_struct *tsk)
 					     MEMCG_KERNEL_STACK_KB,
 					     -(int)(PAGE_SIZE / 1024));
 
-			memcg_kmem_uncharge(vm->pages[i], 0);
+			memcg_kmem_uncharge_page(vm->pages[i], 0);
 		}
 
 		for (i = 0; i < NR_CACHED_STACKS; i++) {
@@ -413,12 +413,13 @@ static int memcg_charge_kernel_stack(struct task_struct *tsk)
 
 		for (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++) {
 			/*
-			 * If memcg_kmem_charge() fails, page->mem_cgroup
-			 * pointer is NULL, and both memcg_kmem_uncharge()
+			 * If memcg_kmem_charge_page() fails, page->mem_cgroup
+			 * pointer is NULL, and both memcg_kmem_uncharge_page()
 			 * and mod_memcg_page_state() in free_thread_stack()
 			 * will ignore this page. So it's safe.
 			 */
-			ret = memcg_kmem_charge(vm->pages[i], GFP_KERNEL, 0);
+			ret = memcg_kmem_charge_page(vm->pages[i], GFP_KERNEL,
+						     0);
 			if (ret)
 				return ret;
 

commit 8380ce479010f2f779587b462a9b4681934297c3
Author: Roman Gushchin <guro@fb.com>
Date:   Sat Mar 28 19:17:25 2020 -0700

    mm: fork: fix kernel_stack memcg stats for various stack implementations
    
    Depending on CONFIG_VMAP_STACK and the THREAD_SIZE / PAGE_SIZE ratio the
    space for task stacks can be allocated using __vmalloc_node_range(),
    alloc_pages_node() and kmem_cache_alloc_node().
    
    In the first and the second cases page->mem_cgroup pointer is set, but
    in the third it's not: memcg membership of a slab page should be
    determined using the memcg_from_slab_page() function, which looks at
    page->slab_cache->memcg_params.memcg .  In this case, using
    mod_memcg_page_state() (as in account_kernel_stack()) is incorrect:
    page->mem_cgroup pointer is NULL even for pages charged to a non-root
    memory cgroup.
    
    It can lead to kernel_stack per-memcg counters permanently showing 0 on
    some architectures (depending on the configuration).
    
    In order to fix it, let's introduce a mod_memcg_obj_state() helper,
    which takes a pointer to a kernel object as a first argument, uses
    mem_cgroup_from_obj() to get a RCU-protected memcg pointer and calls
    mod_memcg_state().  It allows to handle all possible configurations
    (CONFIG_VMAP_STACK and various THREAD_SIZE/PAGE_SIZE values) without
    spilling any memcg/kmem specifics into fork.c .
    
    Note: This is a special version of the patch created for stable
    backports.  It contains code from the following two patches:
      - mm: memcg/slab: introduce mem_cgroup_from_obj()
      - mm: fork: fix kernel_stack memcg stats for various stack implementations
    
    [guro@fb.com: introduce mem_cgroup_from_obj()]
      Link: http://lkml.kernel.org/r/20200324004221.GA36662@carbon.dhcp.thefacebook.com
    Fixes: 4d96ba353075 ("mm: memcg/slab: stop setting page->mem_cgroup pointer for slab pages")
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Bharata B Rao <bharata@linux.ibm.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: <stable@vger.kernel.org>
    Link: http://lkml.kernel.org/r/20200303233550.251375-1-guro@fb.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 86425305cd4a..d90af13431c7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -397,8 +397,8 @@ static void account_kernel_stack(struct task_struct *tsk, int account)
 		mod_zone_page_state(page_zone(first_page), NR_KERNEL_STACK_KB,
 				    THREAD_SIZE / 1024 * account);
 
-		mod_memcg_page_state(first_page, MEMCG_KERNEL_STACK_KB,
-				     account * (THREAD_SIZE / 1024));
+		mod_memcg_obj_state(stack, MEMCG_KERNEL_STACK_KB,
+				    account * (THREAD_SIZE / 1024));
 	}
 }
 

commit 3e74fabd39710ee29fa25618d2c2b40cfa7d76c7
Author: Bernd Edlinger <bernd.edlinger@hotmail.de>
Date:   Fri Mar 20 21:26:04 2020 +0100

    exec: Fix a deadlock in strace
    
    This fixes a deadlock in the tracer when tracing a multi-threaded
    application that calls execve while more than one thread are running.
    
    I observed that when running strace on the gcc test suite, it always
    blocks after a while, when expect calls execve, because other threads
    have to be terminated.  They send ptrace events, but the strace is no
    longer able to respond, since it is blocked in vm_access.
    
    The deadlock is always happening when strace needs to access the
    tracees process mmap, while another thread in the tracee starts to
    execve a child process, but that cannot continue until the
    PTRACE_EVENT_EXIT is handled and the WIFEXITED event is received:
    
    strace          D    0 30614  30584 0x00000000
    Call Trace:
    __schedule+0x3ce/0x6e0
    schedule+0x5c/0xd0
    schedule_preempt_disabled+0x15/0x20
    __mutex_lock.isra.13+0x1ec/0x520
    __mutex_lock_killable_slowpath+0x13/0x20
    mutex_lock_killable+0x28/0x30
    mm_access+0x27/0xa0
    process_vm_rw_core.isra.3+0xff/0x550
    process_vm_rw+0xdd/0xf0
    __x64_sys_process_vm_readv+0x31/0x40
    do_syscall_64+0x64/0x220
    entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    expect          D    0 31933  30876 0x80004003
    Call Trace:
    __schedule+0x3ce/0x6e0
    schedule+0x5c/0xd0
    flush_old_exec+0xc4/0x770
    load_elf_binary+0x35a/0x16c0
    search_binary_handler+0x97/0x1d0
    __do_execve_file.isra.40+0x5d4/0x8a0
    __x64_sys_execve+0x49/0x60
    do_syscall_64+0x64/0x220
    entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    This changes mm_access to use the new exec_update_mutex
    instead of cred_guard_mutex.
    
    This patch is based on the following patch by Eric W. Biederman:
    "[PATCH 0/5] Infrastructure to allow fixing exec deadlocks"
    Link: https://lore.kernel.org/lkml/87v9ne5y4y.fsf_-_@x220.int.ebiederm.org/
    
    Signed-off-by: Bernd Edlinger <bernd.edlinger@hotmail.de>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 12896a6ecee6..e0668a79bca1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1224,7 +1224,7 @@ struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)
 	struct mm_struct *mm;
 	int err;
 
-	err =  mutex_lock_killable(&task->signal->cred_guard_mutex);
+	err =  mutex_lock_killable(&task->signal->exec_update_mutex);
 	if (err)
 		return ERR_PTR(err);
 
@@ -1234,7 +1234,7 @@ struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)
 		mmput(mm);
 		mm = ERR_PTR(-EACCES);
 	}
-	mutex_unlock(&task->signal->cred_guard_mutex);
+	mutex_unlock(&task->signal->exec_update_mutex);
 
 	return mm;
 }

commit eea9673250db4e854e9998ef9da6d4584857f0ea
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Mar 25 10:03:36 2020 -0500

    exec: Add exec_update_mutex to replace cred_guard_mutex
    
    The cred_guard_mutex is problematic as it is held over possibly
    indefinite waits for userspace.  The possible indefinite waits for
    userspace that I have identified are: The cred_guard_mutex is held in
    PTRACE_EVENT_EXIT waiting for the tracer.  The cred_guard_mutex is
    held over "put_user(0, tsk->clear_child_tid)" in exit_mm().  The
    cred_guard_mutex is held over "get_user(futex_offset, ...")  in
    exit_robust_list.  The cred_guard_mutex held over copy_strings.
    
    The functions get_user and put_user can trigger a page fault which can
    potentially wait indefinitely in the case of userfaultfd or if
    userspace implements part of the page fault path.
    
    In any of those cases the userspace process that the kernel is waiting
    for might make a different system call that winds up taking the
    cred_guard_mutex and result in deadlock.
    
    Holding a mutex over any of those possibly indefinite waits for
    userspace does not appear necessary.  Add exec_update_mutex that will
    just cover updating the process during exec where the permissions and
    the objects pointed to by the task struct may be out of sync.
    
    The plan is to switch the users of cred_guard_mutex to
    exec_update_mutex one by one.  This lets us move forward while still
    being careful and not introducing any regressions.
    
    Link: https://lore.kernel.org/lkml/20160921152946.GA24210@dhcp22.suse.cz/
    Link: https://lore.kernel.org/lkml/AM6PR03MB5170B06F3A2B75EFB98D071AE4E60@AM6PR03MB5170.eurprd03.prod.outlook.com/
    Link: https://lore.kernel.org/linux-fsdevel/20161102181806.GB1112@redhat.com/
    Link: https://lore.kernel.org/lkml/20160923095031.GA14923@redhat.com/
    Link: https://lore.kernel.org/lkml/20170213141452.GA30203@redhat.com/
    Ref: 45c1a159b85b ("Add PTRACE_O_TRACEVFORKDONE and PTRACE_O_TRACEEXIT facilities.")
    Ref: 456f17cd1a28 ("[PATCH] user-vm-unlock-2.5.31-A2")
    Reviewed-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Bernd Edlinger <bernd.edlinger@hotmail.de>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 60a1295f4384..12896a6ecee6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1594,6 +1594,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->oom_score_adj_min = current->signal->oom_score_adj_min;
 
 	mutex_init(&sig->cred_guard_mutex);
+	mutex_init(&sig->exec_update_mutex);
 
 	return 0;
 }

commit 0c282b068eb26db0e85e2ab4ec6d1e932acda841
Author: Madhuparna Bhowmik <madhuparnabhowmik10@gmail.com>
Date:   Mon Jan 27 23:28:21 2020 +0530

    fork: Use RCU_INIT_POINTER() instead of rcu_access_pointer()
    
    Use RCU_INIT_POINTER() instead of rcu_access_pointer() in
    copy_sighand().
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Madhuparna Bhowmik <madhuparnabhowmik10@gmail.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
    [christian.brauner@ubuntu.com: edit commit message]
    Link: https://lore.kernel.org/r/20200127175821.10833-1-madhuparnabhowmik10@gmail.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 60a1295f4384..86425305cd4a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1508,7 +1508,7 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 		return 0;
 	}
 	sig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);
-	rcu_assign_pointer(tsk->sighand, sig);
+	RCU_INIT_POINTER(tsk->sighand, sig);
 	if (!sig)
 		return -ENOMEM;
 

commit ef2c41cf38a7559bbf91af42d5b6a4429db8fc68
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Wed Feb 5 14:26:22 2020 +0100

    clone3: allow spawning processes into cgroups
    
    This adds support for creating a process in a different cgroup than its
    parent. Callers can limit and account processes and threads right from
    the moment they are spawned:
    - A service manager can directly spawn new services into dedicated
      cgroups.
    - A process can be directly created in a frozen cgroup and will be
      frozen as well.
    - The initial accounting jitter experienced by process supervisors and
      daemons is eliminated with this.
    - Threaded applications or even thread implementations can choose to
      create a specific cgroup layout where each thread is spawned
      directly into a dedicated cgroup.
    
    This feature is limited to the unified hierarchy. Callers need to pass
    a directory file descriptor for the target cgroup. The caller can
    choose to pass an O_PATH file descriptor. All usual migration
    restrictions apply, i.e. there can be no processes in inner nodes. In
    general, creating a process directly in a target cgroup adheres to all
    migration restrictions.
    
    One of the biggest advantages of this feature is that CLONE_INTO_GROUP does
    not need to grab the write side of the cgroup cgroup_threadgroup_rwsem.
    This global lock makes moving tasks/threads around super expensive. With
    clone3() this lock is avoided.
    
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: cgroups@vger.kernel.org
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9245b6e53f55..635d6369dfb9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2180,7 +2180,7 @@ static __latent_entropy struct task_struct *copy_process(
 	 * between here and cgroup_post_fork() if an organisation operation is in
 	 * progress.
 	 */
-	retval = cgroup_can_fork(p);
+	retval = cgroup_can_fork(p, args);
 	if (retval)
 		goto bad_fork_put_pidfd;
 
@@ -2287,7 +2287,7 @@ static __latent_entropy struct task_struct *copy_process(
 	write_unlock_irq(&tasklist_lock);
 
 	proc_fork_connector(p);
-	cgroup_post_fork(p);
+	cgroup_post_fork(p, args);
 	perf_event_fork(p);
 
 	trace_task_newtask(p, clone_flags);
@@ -2298,7 +2298,7 @@ static __latent_entropy struct task_struct *copy_process(
 bad_fork_cancel_cgroup:
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
-	cgroup_cancel_fork(p);
+	cgroup_cancel_fork(p, args);
 bad_fork_put_pidfd:
 	if (clone_flags & CLONE_PIDFD) {
 		fput(pidfile);
@@ -2627,6 +2627,9 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 		     !valid_signal(args.exit_signal)))
 		return -EINVAL;
 
+	if ((args.flags & CLONE_INTO_CGROUP) && args.cgroup < 0)
+		return -EINVAL;
+
 	*kargs = (struct kernel_clone_args){
 		.flags		= args.flags,
 		.pidfd		= u64_to_user_ptr(args.pidfd),
@@ -2637,6 +2640,7 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 		.stack_size	= args.stack_size,
 		.tls		= args.tls,
 		.set_tid_size	= args.set_tid_size,
+		.cgroup		= args.cgroup,
 	};
 
 	if (args.set_tid &&
@@ -2680,7 +2684,8 @@ static inline bool clone3_stack_valid(struct kernel_clone_args *kargs)
 static bool clone3_args_valid(struct kernel_clone_args *kargs)
 {
 	/* Verify that no unknown flags are passed along. */
-	if (kargs->flags & ~(CLONE_LEGACY_FLAGS | CLONE_CLEAR_SIGHAND))
+	if (kargs->flags &
+	    ~(CLONE_LEGACY_FLAGS | CLONE_CLEAR_SIGHAND | CLONE_INTO_CGROUP))
 		return false;
 
 	/*

commit 5a5cf5cb30d7815c01035fde4b84edef85d11c68
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Wed Feb 5 14:26:20 2020 +0100

    cgroup: refactor fork helpers
    
    This refactors the fork helpers so they can be easily modified in the
    next patches. The patch just moves the cgroup threadgroup rwsem grab and
    release into the helpers. They don't need to be directly exposed in fork.c.
    
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: cgroups@vger.kernel.org
    Acked-by: Michal Koutný <mkoutny@suse.com>
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 60a1295f4384..9245b6e53f55 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2174,7 +2174,6 @@ static __latent_entropy struct task_struct *copy_process(
 	INIT_LIST_HEAD(&p->thread_group);
 	p->task_works = NULL;
 
-	cgroup_threadgroup_change_begin(current);
 	/*
 	 * Ensure that the cgroup subsystem policies allow the new process to be
 	 * forked. It should be noted the the new process's css_set can be changed
@@ -2183,7 +2182,7 @@ static __latent_entropy struct task_struct *copy_process(
 	 */
 	retval = cgroup_can_fork(p);
 	if (retval)
-		goto bad_fork_cgroup_threadgroup_change_end;
+		goto bad_fork_put_pidfd;
 
 	/*
 	 * From this point on we must avoid any synchronous user-space
@@ -2289,7 +2288,6 @@ static __latent_entropy struct task_struct *copy_process(
 
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
-	cgroup_threadgroup_change_end(current);
 	perf_event_fork(p);
 
 	trace_task_newtask(p, clone_flags);
@@ -2301,8 +2299,6 @@ static __latent_entropy struct task_struct *copy_process(
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
 	cgroup_cancel_fork(p);
-bad_fork_cgroup_threadgroup_change_end:
-	cgroup_threadgroup_change_end(current);
 bad_fork_put_pidfd:
 	if (clone_flags & CLONE_PIDFD) {
 		fput(pidfile);

commit 39bed42de2e7d74686a2d5a45638d6a5d7e7d473
Merge: 83fa805bcbfc 5292e24a6acf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 29 19:56:50 2020 -0800

    Merge tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull mmu_notifier updates from Jason Gunthorpe:
     "This small series revises the names in mmu_notifier to make the code
      clearer and more readable"
    
    * tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma:
      mm/mmu_notifiers: Use 'interval_sub' as the variable for mmu_interval_notifier
      mm/mmu_notifiers: Use 'subscription' as the variable name for mmu_notifier
      mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions

commit e279160f491392f1345f6eb4b0eeec5a6a2ecdd7
Merge: 534b0a8b6774 fd928f3e32ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 27 16:47:05 2020 -0800

    Merge tag 'timers-core-2020-01-27' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "The timekeeping and timers departement provides:
    
       - Time namespace support:
    
         If a container migrates from one host to another then it expects
         that clocks based on MONOTONIC and BOOTTIME are not subject to
         disruption. Due to different boot time and non-suspended runtime
         these clocks can differ significantly on two hosts, in the worst
         case time goes backwards which is a violation of the POSIX
         requirements.
    
         The time namespace addresses this problem. It allows to set offsets
         for clock MONOTONIC and BOOTTIME once after creation and before
         tasks are associated with the namespace. These offsets are taken
         into account by timers and timekeeping including the VDSO.
    
         Offsets for wall clock based clocks (REALTIME/TAI) are not provided
         by this mechanism. While in theory possible, the overhead and code
         complexity would be immense and not justified by the esoteric
         potential use cases which were discussed at Plumbers '18.
    
         The overhead for tasks in the root namespace (ie where host time
         offsets = 0) is in the noise and great effort was made to ensure
         that especially in the VDSO. If time namespace is disabled in the
         kernel configuration the code is compiled out.
    
         Kudos to Andrei Vagin and Dmitry Sofanov who implemented this
         feature and kept on for more than a year addressing review
         comments, finding better solutions. A pleasant experience.
    
       - Overhaul of the alarmtimer device dependency handling to ensure
         that the init/suspend/resume ordering is correct.
    
       - A new clocksource/event driver for Microchip PIT64
    
       - Suspend/resume support for the Hyper-V clocksource
    
       - The usual pile of fixes, updates and improvements mostly in the
         driver code"
    
    * tag 'timers-core-2020-01-27' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (71 commits)
      alarmtimer: Make alarmtimer_get_rtcdev() a stub when CONFIG_RTC_CLASS=n
      alarmtimer: Use wakeup source from alarmtimer platform device
      alarmtimer: Make alarmtimer platform device child of RTC device
      alarmtimer: Update alarmtimer_get_rtcdev() docs to reflect reality
      hrtimer: Add missing sparse annotation for __run_timer()
      lib/vdso: Only read hrtimer_res when needed in __cvdso_clock_getres()
      MIPS: vdso: Define BUILD_VDSO32 when building a 32bit kernel
      clocksource/drivers/hyper-v: Set TSC clocksource as default w/ InvariantTSC
      clocksource/drivers/hyper-v: Untangle stimers and timesync from clocksources
      clocksource/drivers/timer-microchip-pit64b: Fix sparse warning
      clocksource/drivers/exynos_mct: Rename Exynos to lowercase
      clocksource/drivers/timer-ti-dm: Fix uninitialized pointer access
      clocksource/drivers/timer-ti-dm: Switch to platform_get_irq
      clocksource/drivers/timer-ti-dm: Convert to devm_platform_ioremap_resource
      clocksource/drivers/em_sti: Fix variable declaration in em_sti_probe
      clocksource/drivers/em_sti: Convert to devm_platform_ioremap_resource
      clocksource/drivers/bcm2835_timer: Fix memory leak of timer
      clocksource/drivers/cadence-ttc: Use ttc driver as platform driver
      clocksource/drivers/timer-microchip-pit64b: Add Microchip PIT64B support
      clocksource/drivers/hyper-v: Reserve PAGE_SIZE space for tsc page
      ...

commit 984cfe4e252681d516df056b982e3c47b66fba92
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Dec 18 13:40:35 2019 -0400

    mm/mmu_notifier: Rename struct mmu_notifier_mm to mmu_notifier_subscriptions
    
    The name mmu_notifier_mm implies that the thing is a mm_struct pointer,
    and is difficult to abbreviate. The struct is actually holding the
    interval tree and hlist containing the notifiers subscribed to a mm.
    
    Use 'subscriptions' as the variable name for this struct instead of the
    really terrible and misleading 'mmn_mm'.
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2508a4f238a3..047865086cdf 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -692,7 +692,7 @@ void __mmdrop(struct mm_struct *mm)
 	WARN_ON_ONCE(mm == current->active_mm);
 	mm_free_pgd(mm);
 	destroy_context(mm);
-	mmu_notifier_mm_destroy(mm);
+	mmu_notifier_subscriptions_destroy(mm);
 	check_mm(mm);
 	put_user_ns(mm->user_ns);
 	free_mm(mm);
@@ -1025,7 +1025,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
 	RCU_INIT_POINTER(mm->exe_file, NULL);
-	mmu_notifier_mm_init(mm);
+	mmu_notifier_subscriptions_init(mm);
 	init_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;

commit 769071ac9f20b6a447410c7eaa55d1a5233ef40c
Author: Andrei Vagin <avagin@openvz.org>
Date:   Tue Nov 12 01:26:52 2019 +0000

    ns: Introduce Time Namespace
    
    Time Namespace isolates clock values.
    
    The kernel provides access to several clocks CLOCK_REALTIME,
    CLOCK_MONOTONIC, CLOCK_BOOTTIME, etc.
    
    CLOCK_REALTIME
          System-wide clock that measures real (i.e., wall-clock) time.
    
    CLOCK_MONOTONIC
          Clock that cannot be set and represents monotonic time since
          some unspecified starting point.
    
    CLOCK_BOOTTIME
          Identical to CLOCK_MONOTONIC, except it also includes any time
          that the system is suspended.
    
    For many users, the time namespace means the ability to changes date and
    time in a container (CLOCK_REALTIME). Providing per namespace notions of
    CLOCK_REALTIME would be complex with a massive overhead, but has a dubious
    value.
    
    But in the context of checkpoint/restore functionality, monotonic and
    boottime clocks become interesting. Both clocks are monotonic with
    unspecified starting points. These clocks are widely used to measure time
    slices and set timers. After restoring or migrating processes, it has to be
    guaranteed that they never go backward. In an ideal case, the behavior of
    these clocks should be the same as for a case when a whole system is
    suspended. All this means that it is required to set CLOCK_MONOTONIC and
    CLOCK_BOOTTIME clocks, which can be achieved by adding per-namespace
    offsets for clocks.
    
    A time namespace is similar to a pid namespace in the way how it is
    created: unshare(CLONE_NEWTIME) system call creates a new time namespace,
    but doesn't set it to the current process. Then all children of the process
    will be born in the new time namespace, or a process can use the setns()
    system call to join a namespace.
    
    This scheme allows setting clock offsets for a namespace, before any
    processes appear in it.
    
    All available clone flags have been used, so CLONE_NEWTIME uses the highest
    bit of CSIGNAL. It means that it can be used only with the unshare() and
    the clone3() system calls.
    
    [ tglx: Adjusted paragraph about clone3() to reality and massaged the
            changelog a bit. ]
    
    Co-developed-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Andrei Vagin <avagin@gmail.com>
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://criu.org/Time_namespace
    Link: https://lists.openvz.org/pipermail/criu/2018-June/041504.html
    Link: https://lore.kernel.org/r/20191112012724.250792-4-dima@arista.com

diff --git a/kernel/fork.c b/kernel/fork.c
index 2508a4f238a3..363595815144 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1832,6 +1832,7 @@ static __latent_entropy struct task_struct *copy_process(
 	struct multiprocess_signals delayed;
 	struct file *pidfile = NULL;
 	u64 clone_flags = args->flags;
+	struct nsproxy *nsp = current->nsproxy;
 
 	/*
 	 * Don't allow sharing the root directory with processes in a different
@@ -1874,8 +1875,16 @@ static __latent_entropy struct task_struct *copy_process(
 	 */
 	if (clone_flags & CLONE_THREAD) {
 		if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
-		    (task_active_pid_ns(current) !=
-				current->nsproxy->pid_ns_for_children))
+		    (task_active_pid_ns(current) != nsp->pid_ns_for_children))
+			return ERR_PTR(-EINVAL);
+	}
+
+	/*
+	 * If the new process will be in a different time namespace
+	 * do not allow it to share VM or a thread group with the forking task.
+	 */
+	if (clone_flags & (CLONE_THREAD | CLONE_VM)) {
+		if (nsp->time_ns != nsp->time_ns_for_children)
 			return ERR_PTR(-EINVAL);
 	}
 
@@ -2811,7 +2820,8 @@ static int check_unshare_flags(unsigned long unshare_flags)
 	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
 				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
-				CLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP))
+				CLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP|
+				CLONE_NEWTIME))
 		return -EINVAL;
 	/*
 	 * Not implemented, but pretend it works if there is nothing

commit dd499f7a7e34270208350a849ef103c0b3ae477f
Author: Amanieu d'Antras <amanieu@gmail.com>
Date:   Thu Jan 2 18:24:13 2020 +0100

    clone3: ensure copy_thread_tls is implemented
    
    copy_thread implementations handle CLONE_SETTLS by reading the TLS
    value from the registers containing the syscall arguments for
    clone. This doesn't work with clone3 since the TLS value is passed
    in clone_args instead.
    
    Signed-off-by: Amanieu d'Antras <amanieu@gmail.com>
    Cc: <stable@vger.kernel.org> # 5.3.x
    Link: https://lore.kernel.org/r/20200102172413.654385-8-amanieu@gmail.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2508a4f238a3..080809560072 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2578,6 +2578,16 @@ SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 #endif
 
 #ifdef __ARCH_WANT_SYS_CLONE3
+
+/*
+ * copy_thread implementations handle CLONE_SETTLS by reading the TLS value from
+ * the registers containing the syscall arguments for clone. This doesn't work
+ * with clone3 since the TLS value is passed in clone_args instead.
+ */
+#ifndef CONFIG_HAVE_COPY_THREAD_TLS
+#error clone3 requires copy_thread_tls support in arch
+#endif
+
 noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 					      struct clone_args __user *uargs,
 					      size_t usize)

commit 043cf46825c102683b1027762c09c7e2b749e5a3
Merge: b22bfea7f16c 83bae01182ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 3 12:20:25 2019 -0800

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Ingo Molnar:
     "The main changes in the timer code in this cycle were:
    
       - Clockevent updates:
    
          - timer-of framework cleanups. (Geert Uytterhoeven)
    
          - Use timer-of for the renesas-ostm and the device name to prevent
            name collision in case of multiple timers. (Geert Uytterhoeven)
    
          - Check if there is an error after calling of_clk_get in asm9260
            (Chuhong Yuan)
    
       - ABI fix: Zero out high order bits of nanoseconds on compat
         syscalls. This got broken a year ago, with apparently no side
         effects so far.
    
         Since the kernel would use random data otherwise I don't think we'd
         have other options but to fix the bug, even if there was a side
         effect to applications (Dmitry Safonov)
    
       - Optimize ns_to_timespec64() on 32-bit systems: move away from
         div_s64_rem() which can be slow, to div_u64_rem() which is faster
         (Arnd Bergmann)
    
       - Annotate KCSAN-reported false positive data races in
         hrtimer_is_queued() users by moving timer->state handling over to
         the READ_ONCE()/WRITE_ONCE() APIs. This documents these accesses
         (Eric Dumazet)
    
       - Misc cleanups and small fixes"
    
    [ I undid the "ABI fix" and updated the comments instead. The reason
      there were apparently no side effects is that the fix was a no-op.
    
      The updated comment is to say _why_ it was a no-op.    - Linus ]
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      time: Zero the upper 32-bits in __kernel_timespec on 32-bit
      time: Rename tsk->real_start_time to ->start_boottime
      hrtimer: Remove the comment about not used HRTIMER_SOFTIRQ
      time: Fix spelling mistake in comment
      time: Optimize ns_to_timespec64()
      hrtimer: Annotate lockless access to timer->state
      clocksource/drivers/asm9260: Add a check for of_clk_get
      clocksource/drivers/renesas-ostm: Use unique device name instead of ostm
      clocksource/drivers/renesas-ostm: Convert to timer_of
      clocksource/drivers/timer-of: Use unique device name instead of timer
      clocksource/drivers/timer-of: Convert last full_name to %pOF

commit eafb149ed73a8bb8359c0ce027b98acd4e95b070
Author: Daniel Axtens <dja@axtens.net>
Date:   Sat Nov 30 17:54:57 2019 -0800

    fork: support VMAP_STACK with KASAN_VMALLOC
    
    Supporting VMAP_STACK with KASAN_VMALLOC is straightforward:
    
     - clear the shadow region of vmapped stacks when swapping them in
     - tweak Kconfig to allow VMAP_STACK to be turned on with KASAN
    
    Link: http://lkml.kernel.org/r/20191031093909.9228-4-dja@axtens.net
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0f0bac8318dd..21c6c1e29b98 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -93,6 +93,7 @@
 #include <linux/livepatch.h>
 #include <linux/thread_info.h>
 #include <linux/stackleak.h>
+#include <linux/kasan.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -223,6 +224,9 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 		if (!s)
 			continue;
 
+		/* Clear the KASAN shadow of the stack. */
+		kasan_unpoison_shadow(s->addr, THREAD_SIZE);
+
 		/* Clear stale pointers from reused stack. */
 		memset(s->addr, 0, THREAD_SIZE);
 

commit aa32f1169148beb90d71494e2f2a1999ba7b5366
Merge: d5bb349dbbe2 93f4e735b6d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 30 10:33:14 2019 -0800

    Merge tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull hmm updates from Jason Gunthorpe:
     "This is another round of bug fixing and cleanup. This time the focus
      is on the driver pattern to use mmu notifiers to monitor a VA range.
      This code is lifted out of many drivers and hmm_mirror directly into
      the mmu_notifier core and written using the best ideas from all the
      driver implementations.
    
      This removes many bugs from the drivers and has a very pleasing
      diffstat. More drivers can still be converted, but that is for another
      cycle.
    
       - A shared branch with RDMA reworking the RDMA ODP implementation
    
       - New mmu_interval_notifier API. This is focused on the use case of
         monitoring a VA and simplifies the process for drivers
    
       - A common seq-count locking scheme built into the
         mmu_interval_notifier API usable by drivers that call
         get_user_pages() or hmm_range_fault() with the VA range
    
       - Conversion of mlx5 ODP, hfi1, radeon, nouveau, AMD GPU, and Xen
         GntDev drivers to the new API. This deletes a lot of wonky driver
         code.
    
       - Two improvements for hmm_range_fault(), from testing done by Ralph"
    
    * tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma:
      mm/hmm: remove hmm_range_dma_map and hmm_range_dma_unmap
      mm/hmm: make full use of walk_page_range()
      xen/gntdev: use mmu_interval_notifier_insert
      mm/hmm: remove hmm_mirror and related
      drm/amdgpu: Use mmu_interval_notifier instead of hmm_mirror
      drm/amdgpu: Use mmu_interval_insert instead of hmm_mirror
      drm/amdgpu: Call find_vma under mmap_sem
      nouveau: use mmu_interval_notifier instead of hmm_mirror
      nouveau: use mmu_notifier directly for invalidate_range_start
      drm/radeon: use mmu_interval_notifier_insert
      RDMA/hfi1: Use mmu_interval_notifier_insert for user_exp_rcv
      RDMA/odp: Use mmu_interval_notifier_insert()
      mm/hmm: define the pre-processor related parts of hmm.h even if disabled
      mm/hmm: allow hmm_range to be used with a mmu_interval_notifier or hmm_mirror
      mm/mmu_notifier: add an interval tree notifier
      mm/mmu_notifier: define the header pre-processor parts even if disabled
      mm/hmm: allow snapshot of the special zero page

commit 168829ad09ca9cdfdc664b2110d0e3569932c12d
Merge: 1ae78780eda5 500543c53a54
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 16:02:40 2019 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - A comprehensive rewrite of the robust/PI futex code's exit handling
         to fix various exit races. (Thomas Gleixner et al)
    
       - Rework the generic REFCOUNT_FULL implementation using
         atomic_fetch_* operations so that the performance impact of the
         cmpxchg() loops is mitigated for common refcount operations.
    
         With these performance improvements the generic implementation of
         refcount_t should be good enough for everybody - and this got
         confirmed by performance testing, so remove ARCH_HAS_REFCOUNT and
         REFCOUNT_FULL entirely, leaving the generic implementation enabled
         unconditionally. (Will Deacon)
    
       - Other misc changes, fixes, cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      lkdtm: Remove references to CONFIG_REFCOUNT_FULL
      locking/refcount: Remove unused 'refcount_error_report()' function
      locking/refcount: Consolidate implementations of refcount_t
      locking/refcount: Consolidate REFCOUNT_{MAX,SATURATED} definitions
      locking/refcount: Move saturation warnings out of line
      locking/refcount: Improve performance of generic REFCOUNT_FULL code
      locking/refcount: Move the bulk of the REFCOUNT_FULL implementation into the <linux/refcount.h> header
      locking/refcount: Remove unused refcount_*_checked() variants
      locking/refcount: Ensure integer operands are treated as signed
      locking/refcount: Define constants for saturation and max refcount values
      futex: Prevent exit livelock
      futex: Provide distinct return value when owner is exiting
      futex: Add mutex around futex exit
      futex: Provide state handling for exec() as well
      futex: Sanitize exit state handling
      futex: Mark the begin of futex exit explicitly
      futex: Set task::futex_state to DEAD right after handling futex exit
      futex: Split futex_mm_release() for exit/exec
      exit/exec: Seperate mm_release()
      futex: Replace PF_EXITPIDONE with a state
      ...

commit 0acefef58451a995981e6d641220fecc53bd85a4
Merge: 9c91e6a5befb 11fde161ab37
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 25 18:36:49 2019 -0800

    Merge tag 'threads-v5.5' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull thread management updates from Christian Brauner:
    
     - A pidfd's fdinfo file currently contains the field "Pid:\t<pid>"
       where <pid> is the pid of the process in the pid namespace of the
       procfs instance the fdinfo file for the pidfd was opened in.
    
       The fdinfo file has now gained a new "NSpid:\t<ns-pid1>[\t<ns-pid2>[...]]"
       field which lists the pids of the process in all child pid namespaces
       provided the pid namespace of the procfs instance it is looked up
       under has an ancestoral relationship with the pid namespace of the
       process. If it does not 0 will be shown and no further pid namespaces
       will be listed. Tests included. (Christian Kellner)
    
     - If the process the pidfd references has already exited, print -1 for
       the Pid and NSpid fields in the pidfd's fdinfo file. Tests included.
       (me)
    
     - Add CLONE_CLEAR_SIGHAND. This lets callers clear all signal handler
       that are not SIG_DFL or SIG_IGN at process creation time. This
       originated as a feature request from glibc to improve performance and
       elimate races in their posix_spawn() implementation. Tests included.
       (me)
    
     - Add support for choosing a specific pid for a process with clone3().
       This is the feature which was part of the thread update for v5.4 but
       after a discussion at LPC in Lisbon we decided to delay it for one
       more cycle in order to make the interface more generic. This has now
       done. It is now possible to choose a specific pid in a whole pid
       namespaces (sub)hierarchy instead of just one pid namespace. In order
       to choose a specific pid the caller must have CAP_SYS_ADMIN in all
       owning user namespaces of the target pid namespaces. Tests included.
       (Adrian Reber)
    
     - Test improvements and extensions. (Andrei Vagin, me)
    
    * tag 'threads-v5.5' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      selftests/clone3: skip if clone3() is ENOSYS
      selftests/clone3: check that all pids are released on error paths
      selftests/clone3: report a correct number of fails
      selftests/clone3: flush stdout and stderr before clone3() and _exit()
      selftests: add tests for clone3() with *set_tid
      fork: extend clone3() to support setting a PID
      selftests: add tests for clone3()
      tests: test CLONE_CLEAR_SIGHAND
      clone3: add CLONE_CLEAR_SIGHAND
      pid: use pid_has_task() in pidfd_open()
      exit: use pid_has_task() in do_wait()
      pid: use pid_has_task() in __change_pid()
      test: verify fdinfo for pidfd of reaped process
      pidfd: check pid has attached task in fdinfo
      pidfd: add tests for NSpid info in fdinfo
      pidfd: add NSpid entries to fdinfo

commit 83bae01182ea755280adc1c3a24032d63a614ede
Merge: cf25e24db61c 7b8474466ed9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Nov 25 15:43:15 2019 +0100

    Merge branch 'timers/urgent' into timers/core, to pick up fix
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 107e899874e95dcddc779142942bf285eba38bc5
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Nov 12 16:22:21 2019 -0400

    mm/hmm: define the pre-processor related parts of hmm.h even if disabled
    
    Only the function calls are stubbed out with static inlines that always
    fail. This is the standard way to write a header for an optional component
    and makes it easier for drivers that only optionally need HMM_MIRROR.
    
    Link: https://lore.kernel.org/r/20191112202231.3856-5-jgg@ziepe.ca
    Reviewed-by: Jérôme Glisse <jglisse@redhat.com>
    Tested-by: Ralph Campbell <rcampbell@nvidia.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index bcdf53125210..ca39cfc404e3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -40,7 +40,6 @@
 #include <linux/binfmts.h>
 #include <linux/mman.h>
 #include <linux/mmu_notifier.h>
-#include <linux/hmm.h>
 #include <linux/fs.h>
 #include <linux/mm.h>
 #include <linux/vmacache.h>

commit 9e77716a75bc6cf54965e5ec069ba7c02b32251c
Author: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
Date:   Wed Nov 20 01:33:20 2019 +0100

    fork: fix pidfd_poll()'s return type
    
    pidfd_poll() is defined as returning 'unsigned int' but the
    .poll method is declared as returning '__poll_t', a bitwise type.
    
    Fix this by using the proper return type and using the EPOLL
    constants instead of the POLL ones, as required for __poll_t.
    
    Fixes: b53b0b9d9a61 ("pidfd: add polling support")
    Cc: Joel Fernandes (Google) <joel@joelfernandes.org>
    Cc: stable@vger.kernel.org # 5.3
    Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Reviewed-by: Christian Brauner <christian.brauner@ubuntu.com>
    Link: https://lore.kernel.org/r/20191120003320.31138-1-luc.vanoostenryck@gmail.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 55af6931c6ec..13b38794efb5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1708,11 +1708,11 @@ static void pidfd_show_fdinfo(struct seq_file *m, struct file *f)
 /*
  * Poll support for process exit notification.
  */
-static unsigned int pidfd_poll(struct file *file, struct poll_table_struct *pts)
+static __poll_t pidfd_poll(struct file *file, struct poll_table_struct *pts)
 {
 	struct task_struct *task;
 	struct pid *pid = file->private_data;
-	int poll_flags = 0;
+	__poll_t poll_flags = 0;
 
 	poll_wait(file, &pid->wait_pidfd, pts);
 
@@ -1724,7 +1724,7 @@ static unsigned int pidfd_poll(struct file *file, struct poll_table_struct *pts)
 	 * group, then poll(2) should block, similar to the wait(2) family.
 	 */
 	if (!task || (task->exit_state && thread_group_empty(task)))
-		poll_flags = POLLIN | POLLRDNORM;
+		poll_flags = EPOLLIN | EPOLLRDNORM;
 	rcu_read_unlock();
 
 	return poll_flags;

commit 150d71584b12809144b8145b817e83b81158ae5f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 6 22:55:39 2019 +0100

    futex: Split futex_mm_release() for exit/exec
    
    To allow separate handling of the futex exit state in the futex exit code
    for exit and exec, split futex_mm_release() into two functions and invoke
    them from the corresponding exit/exec_mm_release() callsites.
    
    Preparatory only, no functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191106224556.332094221@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index 096f9d840bb8..f1eb4d1f1a3b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1285,9 +1285,6 @@ static int wait_for_vfork_done(struct task_struct *child,
  */
 static void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 {
-	/* Get rid of any futexes when releasing the mm */
-	futex_mm_release(tsk);
-
 	uprobe_free_utask(tsk);
 
 	/* Get rid of any cached register state */
@@ -1322,11 +1319,13 @@ static void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 
 void exit_mm_release(struct task_struct *tsk, struct mm_struct *mm)
 {
+	futex_exit_release(tsk);
 	mm_release(tsk, mm);
 }
 
 void exec_mm_release(struct task_struct *tsk, struct mm_struct *mm)
 {
+	futex_exec_release(tsk);
 	mm_release(tsk, mm);
 }
 

commit 4610ba7ad877fafc0a25a30c6c82015304120426
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 6 22:55:38 2019 +0100

    exit/exec: Seperate mm_release()
    
    mm_release() contains the futex exit handling. mm_release() is called from
    do_exit()->exit_mm() and from exec()->exec_mm().
    
    In the exit_mm() case PF_EXITING and the futex state is updated. In the
    exec_mm() case these states are not touched.
    
    As the futex exit code needs further protections against exit races, this
    needs to be split into two functions.
    
    Preparatory only, no functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191106224556.240518241@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index bd7c218691d4..096f9d840bb8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1283,7 +1283,7 @@ static int wait_for_vfork_done(struct task_struct *child,
  * restoring the old one. . .
  * Eric Biederman 10 January 1998
  */
-void mm_release(struct task_struct *tsk, struct mm_struct *mm)
+static void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 {
 	/* Get rid of any futexes when releasing the mm */
 	futex_mm_release(tsk);
@@ -1320,6 +1320,16 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 		complete_vfork_done(tsk);
 }
 
+void exit_mm_release(struct task_struct *tsk, struct mm_struct *mm)
+{
+	mm_release(tsk, mm);
+}
+
+void exec_mm_release(struct task_struct *tsk, struct mm_struct *mm)
+{
+	mm_release(tsk, mm);
+}
+
 /**
  * dup_mm() - duplicates an existing mm structure
  * @tsk: the task_struct with which the new mm will be associated.

commit ba31c1a48538992316cc71ce94fa9cd3e7b427c0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 6 22:55:36 2019 +0100

    futex: Move futex exit handling into futex code
    
    The futex exit handling is #ifdeffed into mm_release() which is not pretty
    to begin with. But upcoming changes to address futex exit races need to add
    more functionality to this exit code.
    
    Split it out into a function, move it into futex code and make the various
    futex exit functions static.
    
    Preparatory only and no functional change.
    
    Folded build fix from Borislav.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191106224556.049705556@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index bcdf53125210..bd7c218691d4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1286,20 +1286,7 @@ static int wait_for_vfork_done(struct task_struct *child,
 void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 {
 	/* Get rid of any futexes when releasing the mm */
-#ifdef CONFIG_FUTEX
-	if (unlikely(tsk->robust_list)) {
-		exit_robust_list(tsk);
-		tsk->robust_list = NULL;
-	}
-#ifdef CONFIG_COMPAT
-	if (unlikely(tsk->compat_robust_list)) {
-		compat_exit_robust_list(tsk);
-		tsk->compat_robust_list = NULL;
-	}
-#endif
-	if (unlikely(!list_empty(&tsk->pi_state_list)))
-		exit_pi_state_list(tsk);
-#endif
+	futex_mm_release(tsk);
 
 	uprobe_free_utask(tsk);
 
@@ -2062,14 +2049,8 @@ static __latent_entropy struct task_struct *copy_process(
 #ifdef CONFIG_BLOCK
 	p->plug = NULL;
 #endif
-#ifdef CONFIG_FUTEX
-	p->robust_list = NULL;
-#ifdef CONFIG_COMPAT
-	p->compat_robust_list = NULL;
-#endif
-	INIT_LIST_HEAD(&p->pi_state_list);
-	p->pi_state_cache = NULL;
-#endif
+	futex_init_task(p);
+
 	/*
 	 * sigaltstack should be cleared when sharing the same VM
 	 */

commit 49cb2fc42ce4b7a656ee605e30c302efaa39c1a7
Author: Adrian Reber <areber@redhat.com>
Date:   Fri Nov 15 13:36:20 2019 +0100

    fork: extend clone3() to support setting a PID
    
    The main motivation to add set_tid to clone3() is CRIU.
    
    To restore a process with the same PID/TID CRIU currently uses
    /proc/sys/kernel/ns_last_pid. It writes the desired (PID - 1) to
    ns_last_pid and then (quickly) does a clone(). This works most of the
    time, but it is racy. It is also slow as it requires multiple syscalls.
    
    Extending clone3() to support *set_tid makes it possible restore a
    process using CRIU without accessing /proc/sys/kernel/ns_last_pid and
    race free (as long as the desired PID/TID is available).
    
    This clone3() extension places the same restrictions (CAP_SYS_ADMIN)
    on clone3() with *set_tid as they are currently in place for ns_last_pid.
    
    The original version of this change was using a single value for
    set_tid. At the 2019 LPC, after presenting set_tid, it was, however,
    decided to change set_tid to an array to enable setting the PID of a
    process in multiple PID namespaces at the same time. If a process is
    created in a PID namespace it is possible to influence the PID inside
    and outside of the PID namespace. Details also in the corresponding
    selftest.
    
    To create a process with the following PIDs:
    
          PID NS level         Requested PID
            0 (host)              31496
            1                        42
            2                         1
    
    For that example the two newly introduced parameters to struct
    clone_args (set_tid and set_tid_size) would need to be:
    
      set_tid[0] = 1;
      set_tid[1] = 42;
      set_tid[2] = 31496;
      set_tid_size = 3;
    
    If only the PIDs of the two innermost nested PID namespaces should be
    defined it would look like this:
    
      set_tid[0] = 1;
      set_tid[1] = 42;
      set_tid_size = 2;
    
    The PID of the newly created process would then be the next available
    free PID in the PID namespace level 0 (host) and 42 in the PID namespace
    at level 1 and the PID of the process in the innermost PID namespace
    would be 1.
    
    The set_tid array is used to specify the PID of a process starting
    from the innermost nested PID namespaces up to set_tid_size PID namespaces.
    
    set_tid_size cannot be larger then the current PID namespace level.
    
    Signed-off-by: Adrian Reber <areber@redhat.com>
    Reviewed-by: Christian Brauner <christian.brauner@ubuntu.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Dmitry Safonov <0x7f454c46@gmail.com>
    Acked-by: Andrei Vagin <avagin@gmail.com>
    Link: https://lore.kernel.org/r/20191115123621.142252-1-areber@redhat.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 954e875e72b1..417570263f1f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2087,7 +2087,8 @@ static __latent_entropy struct task_struct *copy_process(
 	stackleak_task_init(p);
 
 	if (pid != &init_struct_pid) {
-		pid = alloc_pid(p->nsproxy->pid_ns_for_children);
+		pid = alloc_pid(p->nsproxy->pid_ns_for_children, args->set_tid,
+				args->set_tid_size);
 		if (IS_ERR(pid)) {
 			retval = PTR_ERR(pid);
 			goto bad_fork_cleanup_thread;
@@ -2590,6 +2591,7 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 {
 	int err;
 	struct clone_args args;
+	pid_t *kset_tid = kargs->set_tid;
 
 	if (unlikely(usize > PAGE_SIZE))
 		return -E2BIG;
@@ -2600,6 +2602,15 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 	if (err)
 		return err;
 
+	if (unlikely(args.set_tid_size > MAX_PID_NS_LEVEL))
+		return -EINVAL;
+
+	if (unlikely(!args.set_tid && args.set_tid_size > 0))
+		return -EINVAL;
+
+	if (unlikely(args.set_tid && args.set_tid_size == 0))
+		return -EINVAL;
+
 	/*
 	 * Verify that higher 32bits of exit_signal are unset and that
 	 * it is a valid signal
@@ -2617,8 +2628,16 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 		.stack		= args.stack,
 		.stack_size	= args.stack_size,
 		.tls		= args.tls,
+		.set_tid_size	= args.set_tid_size,
 	};
 
+	if (args.set_tid &&
+		copy_from_user(kset_tid, u64_to_user_ptr(args.set_tid),
+			(kargs->set_tid_size * sizeof(pid_t))))
+		return -EFAULT;
+
+	kargs->set_tid = kset_tid;
+
 	return 0;
 }
 
@@ -2662,6 +2681,9 @@ SYSCALL_DEFINE2(clone3, struct clone_args __user *, uargs, size_t, size)
 	int err;
 
 	struct kernel_clone_args kargs;
+	pid_t set_tid[MAX_PID_NS_LEVEL];
+
+	kargs.set_tid = set_tid;
 
 	err = copy_clone_args_from_user(&kargs, uargs, size);
 	if (err)

commit cf25e24db61cc9df42c47485a2ec2bff4e9a3692
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 7 11:07:58 2019 +0100

    time: Rename tsk->real_start_time to ->start_boottime
    
    Since it stores CLOCK_BOOTTIME, not, as the name suggests,
    CLOCK_REALTIME, let's rename ->real_start_time to ->start_bootime.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index bcdf53125210..1392ee8f4848 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2130,7 +2130,7 @@ static __latent_entropy struct task_struct *copy_process(
 	 */
 
 	p->start_time = ktime_get_ns();
-	p->real_start_time = ktime_get_boottime_ns();
+	p->start_boottime = ktime_get_boottime_ns();
 
 	/*
 	 * Make it visible to the rest of the system, but dont wake it up yet.

commit fa729c4df558936b4a1a7b3e2234011f44ede28b
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Thu Oct 31 12:36:08 2019 +0100

    clone3: validate stack arguments
    
    Validate the stack arguments and setup the stack depening on whether or not
    it is growing down or up.
    
    Legacy clone() required userspace to know in which direction the stack is
    growing and pass down the stack pointer appropriately. To make things more
    confusing microblaze uses a variant of the clone() syscall selected by
    CONFIG_CLONE_BACKWARDS3 that takes an additional stack_size argument.
    IA64 has a separate clone2() syscall which also takes an additional
    stack_size argument. Finally, parisc has a stack that is growing upwards.
    Userspace therefore has a lot nasty code like the following:
    
     #define __STACK_SIZE (8 * 1024 * 1024)
     pid_t sys_clone(int (*fn)(void *), void *arg, int flags, int *pidfd)
     {
             pid_t ret;
             void *stack;
    
             stack = malloc(__STACK_SIZE);
             if (!stack)
                     return -ENOMEM;
    
     #ifdef __ia64__
             ret = __clone2(fn, stack, __STACK_SIZE, flags | SIGCHLD, arg, pidfd);
     #elif defined(__parisc__) /* stack grows up */
             ret = clone(fn, stack, flags | SIGCHLD, arg, pidfd);
     #else
             ret = clone(fn, stack + __STACK_SIZE, flags | SIGCHLD, arg, pidfd);
     #endif
             return ret;
     }
    
    or even crazier variants such as [3].
    
    With clone3() we have the ability to validate the stack. We can check that
    when stack_size is passed, the stack pointer is valid and the other way
    around. We can also check that the memory area userspace gave us is fine to
    use via access_ok(). Furthermore, we probably should not require
    userspace to know in which direction the stack is growing. It is easy
    for us to do this in the kernel and I couldn't find the original
    reasoning behind exposing this detail to userspace.
    
    /* Intentional user visible API change */
    clone3() was released with 5.3. Currently, it is not documented and very
    unclear to userspace how the stack and stack_size argument have to be
    passed. After talking to glibc folks we concluded that trying to change
    clone3() to setup the stack instead of requiring userspace to do this is
    the right course of action.
    Note, that this is an explicit change in user visible behavior we introduce
    with this patch. If it breaks someone's use-case we will revert! (And then
    e.g. place the new behavior under an appropriate flag.)
    Breaking someone's use-case is very unlikely though. First, neither glibc
    nor musl currently expose a wrapper for clone3(). Second, there is no real
    motivation for anyone to use clone3() directly since it does not provide
    features that legacy clone doesn't. New features for clone3() will first
    happen in v5.5 which is why v5.4 is still a good time to try and make that
    change now and backport it to v5.3. Searches on [4] did not reveal any
    packages calling clone3().
    
    [1]: https://lore.kernel.org/r/CAG48ez3q=BeNcuVTKBN79kJui4vC6nw0Bfq6xc-i0neheT17TA@mail.gmail.com
    [2]: https://lore.kernel.org/r/20191028172143.4vnnjpdljfnexaq5@wittgenstein
    [3]: https://github.com/systemd/systemd/blob/5238e9575906297608ff802a27e2ff9effa3b338/src/basic/raw-clone.h#L31
    [4]: https://codesearch.debian.net
    Fixes: 7f192e3cd316 ("fork: add clone3")
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Jann Horn <jannh@google.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-api@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: <stable@vger.kernel.org> # 5.3
    Cc: GNU C Library <libc-alpha@sourceware.org>
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Aleksa Sarai <cyphar@cyphar.com>
    Link: https://lore.kernel.org/r/20191031113608.20713-1-christian.brauner@ubuntu.com

diff --git a/kernel/fork.c b/kernel/fork.c
index bcdf53125210..55af6931c6ec 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2561,7 +2561,35 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 	return 0;
 }
 
-static bool clone3_args_valid(const struct kernel_clone_args *kargs)
+/**
+ * clone3_stack_valid - check and prepare stack
+ * @kargs: kernel clone args
+ *
+ * Verify that the stack arguments userspace gave us are sane.
+ * In addition, set the stack direction for userspace since it's easy for us to
+ * determine.
+ */
+static inline bool clone3_stack_valid(struct kernel_clone_args *kargs)
+{
+	if (kargs->stack == 0) {
+		if (kargs->stack_size > 0)
+			return false;
+	} else {
+		if (kargs->stack_size == 0)
+			return false;
+
+		if (!access_ok((void __user *)kargs->stack, kargs->stack_size))
+			return false;
+
+#if !defined(CONFIG_STACK_GROWSUP) && !defined(CONFIG_IA64)
+		kargs->stack += kargs->stack_size;
+#endif
+	}
+
+	return true;
+}
+
+static bool clone3_args_valid(struct kernel_clone_args *kargs)
 {
 	/*
 	 * All lower bits of the flag word are taken.
@@ -2581,6 +2609,9 @@ static bool clone3_args_valid(const struct kernel_clone_args *kargs)
 	    kargs->exit_signal)
 		return false;
 
+	if (!clone3_stack_valid(kargs))
+		return false;
+
 	return true;
 }
 

commit b612e5df4587c934bd056bf05f4a1deca4de4f75
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Mon Oct 14 12:45:37 2019 +0200

    clone3: add CLONE_CLEAR_SIGHAND
    
    Reset all signal handlers of the child not set to SIG_IGN to SIG_DFL.
    Mutually exclusive with CLONE_SIGHAND to not disturb other thread's
    signal handler.
    
    In the spirit of closer cooperation between glibc developers and kernel
    developers (cf. [2]) this patchset came out of a discussion on the glibc
    mailing list for improving posix_spawn() (cf. [1], [3], [4]). Kernel
    support for this feature has been explicitly requested by glibc and I
    see no reason not to help them with this.
    
    The child helper process on Linux posix_spawn must ensure that no signal
    handlers are enabled, so the signal disposition must be either SIG_DFL
    or SIG_IGN. However, it requires a sigprocmask to obtain the current
    signal mask and at least _NSIG sigaction calls to reset the signal
    handlers for each posix_spawn call or complex state tracking that might
    lead to data corruption in glibc. Adding this flags lets glibc avoid
    these problems.
    
    [1]: https://www.sourceware.org/ml/libc-alpha/2019-10/msg00149.html
    [3]: https://www.sourceware.org/ml/libc-alpha/2019-10/msg00158.html
    [4]: https://www.sourceware.org/ml/libc-alpha/2019-10/msg00160.html
    [2]: https://lwn.net/Articles/799331/
         '[...] by asking for better cooperation with the C-library projects
         in general. They should be copied on patches containing ABI
         changes, for example. I noted that there are often times where
         C-library developers wish the kernel community had done things
         differently; how could those be avoided in the future? Members of
         the audience suggested that more glibc developers should perhaps
         join the linux-api list. The other suggestion was to "copy Florian
         on everything".'
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: libc-alpha@sourceware.org
    Cc: linux-api@vger.kernel.org
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Link: https://lore.kernel.org/r/20191014104538.3096-1-christian.brauner@ubuntu.com

diff --git a/kernel/fork.c b/kernel/fork.c
index ffa314838b43..954e875e72b1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1517,6 +1517,11 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 	spin_lock_irq(&current->sighand->siglock);
 	memcpy(sig->action, current->sighand->action, sizeof(sig->action));
 	spin_unlock_irq(&current->sighand->siglock);
+
+	/* Reset all signal handler not set to SIG_IGN to SIG_DFL. */
+	if (clone_flags & CLONE_CLEAR_SIGHAND)
+		flush_signal_handlers(tsk, 0);
+
 	return 0;
 }
 
@@ -2619,11 +2624,8 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 
 static bool clone3_args_valid(const struct kernel_clone_args *kargs)
 {
-	/*
-	 * All lower bits of the flag word are taken.
-	 * Verify that no other unknown flags are passed along.
-	 */
-	if (kargs->flags & ~CLONE_LEGACY_FLAGS)
+	/* Verify that no unknown flags are passed along. */
+	if (kargs->flags & ~(CLONE_LEGACY_FLAGS | CLONE_CLEAR_SIGHAND))
 		return false;
 
 	/*
@@ -2633,6 +2635,10 @@ static bool clone3_args_valid(const struct kernel_clone_args *kargs)
 	if (kargs->flags & (CLONE_DETACHED | CSIGNAL))
 		return false;
 
+	if ((kargs->flags & (CLONE_SIGHAND | CLONE_CLEAR_SIGHAND)) ==
+	    (CLONE_SIGHAND | CLONE_CLEAR_SIGHAND))
+		return false;
+
 	if ((kargs->flags & (CLONE_THREAD | CLONE_PARENT)) &&
 	    kargs->exit_signal)
 		return false;

commit 3d6d8da48d0b214d65ea0227d47228abc75d7c88
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Thu Oct 17 12:18:28 2019 +0200

    pidfd: check pid has attached task in fdinfo
    
    Currently, when a task is dead we still print the pid it used to use in
    the fdinfo files of its pidfds. This doesn't make much sense since the
    pid may have already been reused. So verify that the task is still alive
    by introducing the pid_has_task() helper which will be used by other
    callers in follow-up patches.
    If the task is not alive anymore, we will print -1. This allows us to
    differentiate between a task not being present in a given pid namespace
    - in which case we already print 0 - and a task having been reaped.
    
    Note that this uses PIDTYPE_PID for the check. Technically, we could've
    checked PIDTYPE_TGID since pidfds currently only refer to thread-group
    leaders but if they won't anymore in the future then this check becomes
    problematic without it being immediately obvious to non-experts imho. If
    a thread is created via clone(CLONE_THREAD) than struct pid has a single
    non-empty list pid->tasks[PIDTYPE_PID] and this pid can't be used as a
    PIDTYPE_TGID meaning pid->tasks[PIDTYPE_TGID] will return NULL even
    though the thread-group leader might still be very much alive. So
    checking PIDTYPE_PID is fine and is easier to maintain should we ever
    allow pidfds to refer to threads.
    
    Cc: Jann Horn <jannh@google.com>
    Cc: Christian Kellner <christian@kellner.me>
    Cc: linux-api@vger.kernel.org
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Link: https://lore.kernel.org/r/20191017101832.5985-1-christian.brauner@ubuntu.com

diff --git a/kernel/fork.c b/kernel/fork.c
index 782986962d47..ffa314838b43 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1732,15 +1732,20 @@ static int pidfd_release(struct inode *inode, struct file *file)
  */
 static void pidfd_show_fdinfo(struct seq_file *m, struct file *f)
 {
-	struct pid_namespace *ns = proc_pid_ns(file_inode(m->file));
 	struct pid *pid = f->private_data;
-	pid_t nr = pid_nr_ns(pid, ns);
+	struct pid_namespace *ns;
+	pid_t nr = -1;
 
-	seq_put_decimal_ull(m, "Pid:\t", nr);
+	if (likely(pid_has_task(pid, PIDTYPE_PID))) {
+		ns = proc_pid_ns(file_inode(m->file));
+		nr = pid_nr_ns(pid, ns);
+	}
+
+	seq_put_decimal_ll(m, "Pid:\t", nr);
 
 #ifdef CONFIG_PID_NS
-	seq_put_decimal_ull(m, "\nNSpid:\t", nr);
-	if (nr) {
+	seq_put_decimal_ll(m, "\nNSpid:\t", nr);
+	if (nr > 0) {
 		int i;
 
 		/* If nr is non-zero it means that 'pid' is valid and that
@@ -1749,7 +1754,7 @@ static void pidfd_show_fdinfo(struct seq_file *m, struct file *f)
 		 * Start at one below the already printed level.
 		 */
 		for (i = ns->level + 1; i <= pid->level; i++)
-			seq_put_decimal_ull(m, "\t", pid->numbers[i].nr);
+			seq_put_decimal_ll(m, "\t", pid->numbers[i].nr);
 	}
 #endif
 	seq_putc(m, '\n');

commit 15d42eb26bdee47c0278fbdab4198577bc6a97b5
Author: Christian Kellner <christian@kellner.me>
Date:   Mon Oct 14 18:20:32 2019 +0200

    pidfd: add NSpid entries to fdinfo
    
    Currently, the fdinfo file contains the Pid field which shows the
    pid a given pidfd refers to in the pid namespace of the procfs
    instance. If pid namespaces are configured, also show an NSpid field
    for easy retrieval of the pid in all descendant pid namespaces. If
    the pid namespace of the process is not a descendant of the pid
    namespace of the procfs instance 0 will be shown as its first NSpid
    entry and no other entries will be shown. Add a block comment to
    pidfd_show_fdinfo with a detailed explanation of Pid and NSpid fields.
    
    Co-developed-by: Christian Brauner <christian.brauner@ubuntu.com>
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Signed-off-by: Christian Kellner <christian@kellner.me>
    Reviewed-by: Christian Brauner <christian.brauner@ubuntu.com>
    Link: https://lore.kernel.org/r/20191014162034.2185-1-ckellner@redhat.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index bcdf53125210..782986962d47 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1695,12 +1695,63 @@ static int pidfd_release(struct inode *inode, struct file *file)
 }
 
 #ifdef CONFIG_PROC_FS
+/**
+ * pidfd_show_fdinfo - print information about a pidfd
+ * @m: proc fdinfo file
+ * @f: file referencing a pidfd
+ *
+ * Pid:
+ * This function will print the pid that a given pidfd refers to in the
+ * pid namespace of the procfs instance.
+ * If the pid namespace of the process is not a descendant of the pid
+ * namespace of the procfs instance 0 will be shown as its pid. This is
+ * similar to calling getppid() on a process whose parent is outside of
+ * its pid namespace.
+ *
+ * NSpid:
+ * If pid namespaces are supported then this function will also print
+ * the pid of a given pidfd refers to for all descendant pid namespaces
+ * starting from the current pid namespace of the instance, i.e. the
+ * Pid field and the first entry in the NSpid field will be identical.
+ * If the pid namespace of the process is not a descendant of the pid
+ * namespace of the procfs instance 0 will be shown as its first NSpid
+ * entry and no others will be shown.
+ * Note that this differs from the Pid and NSpid fields in
+ * /proc/<pid>/status where Pid and NSpid are always shown relative to
+ * the  pid namespace of the procfs instance. The difference becomes
+ * obvious when sending around a pidfd between pid namespaces from a
+ * different branch of the tree, i.e. where no ancestoral relation is
+ * present between the pid namespaces:
+ * - create two new pid namespaces ns1 and ns2 in the initial pid
+ *   namespace (also take care to create new mount namespaces in the
+ *   new pid namespace and mount procfs)
+ * - create a process with a pidfd in ns1
+ * - send pidfd from ns1 to ns2
+ * - read /proc/self/fdinfo/<pidfd> and observe that both Pid and NSpid
+ *   have exactly one entry, which is 0
+ */
 static void pidfd_show_fdinfo(struct seq_file *m, struct file *f)
 {
 	struct pid_namespace *ns = proc_pid_ns(file_inode(m->file));
 	struct pid *pid = f->private_data;
+	pid_t nr = pid_nr_ns(pid, ns);
+
+	seq_put_decimal_ull(m, "Pid:\t", nr);
 
-	seq_put_decimal_ull(m, "Pid:\t", pid_nr_ns(pid, ns));
+#ifdef CONFIG_PID_NS
+	seq_put_decimal_ull(m, "\nNSpid:\t", nr);
+	if (nr) {
+		int i;
+
+		/* If nr is non-zero it means that 'pid' is valid and that
+		 * ns, i.e. the pid namespace associated with the procfs
+		 * instance, is in the pid namespace hierarchy of pid.
+		 * Start at one below the already printed level.
+		 */
+		for (i = ns->level + 1; i <= pid->level; i++)
+			seq_put_decimal_ull(m, "\t", pid->numbers[i].nr);
+	}
+#endif
 	seq_putc(m, '\n');
 }
 #endif

commit b0f53dbc4bc4c371f38b14c391095a3bb8a0bb40
Author: Michal Hocko <mhocko@suse.com>
Date:   Sun Oct 6 17:58:19 2019 -0700

    kernel/sysctl.c: do not override max_threads provided by userspace
    
    Partially revert 16db3d3f1170 ("kernel/sysctl.c: threads-max observe
    limits") because the patch is causing a regression to any workload which
    needs to override the auto-tuning of the limit provided by kernel.
    
    set_max_threads is implementing a boot time guesstimate to provide a
    sensible limit of the concurrently running threads so that runaways will
    not deplete all the memory.  This is a good thing in general but there
    are workloads which might need to increase this limit for an application
    to run (reportedly WebSpher MQ is affected) and that is simply not
    possible after the mentioned change.  It is also very dubious to
    override an admin decision by an estimation that doesn't have any direct
    relation to correctness of the kernel operation.
    
    Fix this by dropping set_max_threads from sysctl_max_threads so any
    value is accepted as long as it fits into MAX_THREADS which is important
    to check because allowing more threads could break internal robust futex
    restriction.  While at it, do not use MIN_THREADS as the lower boundary
    because it is also only a heuristic for automatic estimation and admin
    might have a good reason to stop new threads to be created even when
    below this limit.
    
    This became more severe when we switched x86 from 4k to 8k kernel
    stacks.  Starting since 6538b8ea886e ("x86_64: expand kernel stack to
    16K") (3.16) we use THREAD_SIZE_ORDER = 2 and that halved the auto-tuned
    value.
    
    In the particular case
    
      3.12
      kernel.threads-max = 515561
    
      4.4
      kernel.threads-max = 200000
    
    Neither of the two values is really insane on 32GB machine.
    
    I am not sure we want/need to tune the max_thread value further.  If
    anything the tuning should be removed altogether if proven not useful in
    general.  But we definitely need a way to override this auto-tuning.
    
    Link: http://lkml.kernel.org/r/20190922065801.GB18814@dhcp22.suse.cz
    Fixes: 16db3d3f1170 ("kernel/sysctl.c: threads-max observe limits")
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Heinrich Schuchardt <xypron.glpk@gmx.de>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1f6c45f6a734..bcdf53125210 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2925,7 +2925,7 @@ int sysctl_max_threads(struct ctl_table *table, int write,
 	struct ctl_table t;
 	int ret;
 	int threads = max_threads;
-	int min = MIN_THREADS;
+	int min = 1;
 	int max = MAX_THREADS;
 
 	t = *table;
@@ -2937,7 +2937,7 @@ int sysctl_max_threads(struct ctl_table *table, int write,
 	if (ret || !write)
 		return ret;
 
-	set_max_threads(threads);
+	max_threads = threads;
 
 	return 0;
 }

commit e524d16e7e324039f2a9f82e302f0a39ac7d5812
Merge: af0622f6ae41 341115822f88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 4 10:36:31 2019 -0700

    Merge tag 'copy-struct-from-user-v5.4-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull copy_struct_from_user() helper from Christian Brauner:
     "This contains the copy_struct_from_user() helper which got split out
      from the openat2() patchset. It is a generic interface designed to
      copy a struct from userspace.
    
      The helper will be especially useful for structs versioned by size of
      which we have quite a few. This allows for backwards compatibility,
      i.e. an extended struct can be passed to an older kernel, or a legacy
      struct can be passed to a newer kernel. For the first case (extended
      struct, older kernel) the new fields in an extended struct can be set
      to zero and the struct safely passed to an older kernel.
    
      The most obvious benefit is that this helper lets us get rid of
      duplicate code present in at least sched_setattr(), perf_event_open(),
      and clone3(). More importantly it will also help to ensure that users
      implementing versioning-by-size end up with the same core semantics.
    
      This point is especially crucial since we have at least one case where
      versioning-by-size is used but with slighly different semantics:
      sched_setattr(), perf_event_open(), and clone3() all do do similar
      checks to copy_struct_from_user() while rt_sigprocmask(2) always
      rejects differently-sized struct arguments.
    
      With this pull request we also switch over sched_setattr(),
      perf_event_open(), and clone3() to use the new helper"
    
    * tag 'copy-struct-from-user-v5.4-rc2' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      usercopy: Add parentheses around assignment in test_copy_struct_from_user
      perf_event_open: switch to copy_struct_from_user()
      sched_setattr: switch to copy_struct_from_user()
      clone3: switch to copy_struct_from_user()
      lib: introduce copy_struct_from_user() helper

commit 501bd0166eb949820c8e4204e62aaee5c89c84d9
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Fri Sep 27 17:28:42 2019 +0200

    fork: add kernel-doc for clone3
    
    Add kernel-doc for the clone3() syscall.
    
    Link: https://lore.kernel.org/r/20191001114701.24661-2-christian.brauner@ubuntu.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index f9572f416126..bf11cf39579a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2604,6 +2604,17 @@ static bool clone3_args_valid(const struct kernel_clone_args *kargs)
 	return true;
 }
 
+/**
+ * clone3 - create a new process with specific properties
+ * @uargs: argument structure
+ * @size:  size of @uargs
+ *
+ * clone3() is the extensible successor to clone()/clone2().
+ * It takes a struct as argument that is versioned by its size.
+ *
+ * Return: On success, a positive PID for the child process.
+ *         On error, a negative errno number.
+ */
 SYSCALL_DEFINE2(clone3, struct clone_args __user *, uargs, size_t, size)
 {
 	int err;

commit f14c234b4bc5184fd40d9a47830e5b32c3b36d49
Author: Aleksa Sarai <cyphar@cyphar.com>
Date:   Tue Oct 1 11:10:53 2019 +1000

    clone3: switch to copy_struct_from_user()
    
    Switch clone3() syscall from it's own copying struct clone_args from
    userspace to the new dedicated copy_struct_from_user() helper.
    
    The change is very straightforward, and helps unify the syscall
    interface for struct-from-userspace syscalls. Additionally, explicitly
    define CLONE_ARGS_SIZE_VER0 to match the other users of the
    struct-extension pattern.
    
    Signed-off-by: Aleksa Sarai <cyphar@cyphar.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Christian Brauner <christian.brauner@ubuntu.com>
    [christian.brauner@ubuntu.com: improve commit message]
    Link: https://lore.kernel.org/r/20191001011055.19283-3-cyphar@cyphar.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index f9572f416126..2ef529869c64 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2525,39 +2525,19 @@ SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 #ifdef __ARCH_WANT_SYS_CLONE3
 noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 					      struct clone_args __user *uargs,
-					      size_t size)
+					      size_t usize)
 {
+	int err;
 	struct clone_args args;
 
-	if (unlikely(size > PAGE_SIZE))
+	if (unlikely(usize > PAGE_SIZE))
 		return -E2BIG;
-
-	if (unlikely(size < sizeof(struct clone_args)))
+	if (unlikely(usize < CLONE_ARGS_SIZE_VER0))
 		return -EINVAL;
 
-	if (unlikely(!access_ok(uargs, size)))
-		return -EFAULT;
-
-	if (size > sizeof(struct clone_args)) {
-		unsigned char __user *addr;
-		unsigned char __user *end;
-		unsigned char val;
-
-		addr = (void __user *)uargs + sizeof(struct clone_args);
-		end = (void __user *)uargs + size;
-
-		for (; addr < end; addr++) {
-			if (get_user(val, addr))
-				return -EFAULT;
-			if (val)
-				return -E2BIG;
-		}
-
-		size = sizeof(struct clone_args);
-	}
-
-	if (copy_from_user(&args, uargs, size))
-		return -EFAULT;
+	err = copy_struct_from_user(&args, sizeof(args), uargs, usize);
+	if (err)
+		return err;
 
 	/*
 	 * Verify that higher 32bits of exit_signal are unset and that

commit 9c5efe9ae7df78600c0ee7bcce27516eb687fa6e
Merge: aefcf2f4b581 4892f51ad54d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 28 12:39:07 2019 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
    
     - Apply a number of membarrier related fixes and cleanups, which fixes
       a use-after-free race in the membarrier code
    
     - Introduce proper RCU protection for tasks on the runqueue - to get
       rid of the subtle task_rcu_dereference() interface that was easy to
       get wrong
    
     - Misc fixes, but also an EAS speedup
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/fair: Avoid redundant EAS calculation
      sched/core: Remove double update_max_interval() call on CPU startup
      sched/core: Fix preempt_schedule() interrupt return comment
      sched/fair: Fix -Wunused-but-set-variable warnings
      sched/core: Fix migration to invalid CPU in __set_cpus_allowed_ptr()
      sched/membarrier: Return -ENOMEM to userspace on memory allocation failure
      sched/membarrier: Skip IPIs when mm->mm_users == 1
      selftests, sched/membarrier: Add multi-threaded test
      sched/membarrier: Fix p->mm->membarrier_state racy load
      sched/membarrier: Call sync_core only before usermode for same mm
      sched/membarrier: Remove redundant check
      sched/membarrier: Fix private expedited registration check
      tasks, sched/core: RCUify the assignment of rq->curr
      tasks, sched/core: With a grace period after finish_task_switch(), remove unnecessary code
      tasks, sched/core: Ensure tasks are available for a grace period after leaving the runqueue
      tasks: Add a count of task RCU users
      sched/core: Convert vcpu_is_preempted() from macro to an inline function
      sched/fair: Remove unused cfs_rq_clock_task() function

commit 8495f7e6732ed248b648d36439795b42ec650b9e
Author: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
Date:   Wed Sep 25 16:47:27 2019 -0700

    fork: improve error message for corrupted page tables
    
    When a user process exits, the kernel cleans up the mm_struct of the user
    process and during cleanup, check_mm() checks the page tables of the user
    process for corruption (E.g: unexpected page flags set/cleared).  For
    corrupted page tables, the error message printed by check_mm() isn't very
    clear as it prints the loop index instead of page table type (E.g:
    Resident file mapping pages vs Resident shared memory pages).  The loop
    index in check_mm() is used to index rss_stat[] which represents
    individual memory type stats.  Hence, instead of printing index, print
    memory type, thereby improving error message.
    
    Without patch:
    --------------
    [  204.836425] mm/pgtable-generic.c:29: bad p4d 0000000089eb4e92(800000025f941467)
    [  204.836544] BUG: Bad rss-counter state mm:00000000f75895ea idx:0 val:2
    [  204.836615] BUG: Bad rss-counter state mm:00000000f75895ea idx:1 val:5
    [  204.836685] BUG: non-zero pgtables_bytes on freeing mm: 20480
    
    With patch:
    -----------
    [   69.815453] mm/pgtable-generic.c:29: bad p4d 0000000084653642(800000025ca37467)
    [   69.815872] BUG: Bad rss-counter state mm:00000000014a6c03 type:MM_FILEPAGES val:2
    [   69.815962] BUG: Bad rss-counter state mm:00000000014a6c03 type:MM_ANONPAGES val:5
    [   69.816050] BUG: non-zero pgtables_bytes on freeing mm: 20480
    
    Also, change print function (from printk(KERN_ALERT, ..) to pr_alert()) so
    that it matches the other print statement.
    
    Link: http://lkml.kernel.org/r/da75b5153f617f4c5739c08ee6ebeb3d19db0fbc.1565123758.git.sai.praneeth.prakhya@intel.com
    Signed-off-by: Sai Praneeth Prakhya <sai.praneeth.prakhya@intel.com>
    Reviewed-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Suggested-by: Dave Hansen <dave.hansen@intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Dave Hansen <dave.hansen@intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5a0fd518e04e..60763c043aa3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -125,6 +125,15 @@ int nr_threads;			/* The idle threads do not count.. */
 
 static int max_threads;		/* tunable limit on nr_threads */
 
+#define NAMED_ARRAY_INDEX(x)	[x] = __stringify(x)
+
+static const char * const resident_page_types[] = {
+	NAMED_ARRAY_INDEX(MM_FILEPAGES),
+	NAMED_ARRAY_INDEX(MM_ANONPAGES),
+	NAMED_ARRAY_INDEX(MM_SWAPENTS),
+	NAMED_ARRAY_INDEX(MM_SHMEMPAGES),
+};
+
 DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 
 __cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
@@ -645,12 +654,15 @@ static void check_mm(struct mm_struct *mm)
 {
 	int i;
 
+	BUILD_BUG_ON_MSG(ARRAY_SIZE(resident_page_types) != NR_MM_COUNTERS,
+			 "Please make sure 'struct resident_page_types[]' is updated as well");
+
 	for (i = 0; i < NR_MM_COUNTERS; i++) {
 		long x = atomic_long_read(&mm->rss_stat.count[i]);
 
 		if (unlikely(x))
-			printk(KERN_ALERT "BUG: Bad rss-counter state "
-					  "mm:%p idx:%d val:%ld\n", mm, i, x);
+			pr_alert("BUG: Bad rss-counter state mm:%p type:%s val:%ld\n",
+				 mm, resident_page_types[i], x);
 	}
 
 	if (mm_pgtables_bytes(mm))

commit 0ff7b2cfbae36ebcd216c6a5ad7f8534eebeaee2
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 14 07:33:58 2019 -0500

    tasks, sched/core: Ensure tasks are available for a grace period after leaving the runqueue
    
    In the ordinary case today the RCU grace period for a task_struct is
    triggered when another process wait's for it's zombine and causes the
    kernel to call release_task().  As the waiting task has to receive a
    signal and then act upon it before this happens, typically this will
    occur after the original task as been removed from the runqueue.
    
    Unfortunaty in some cases such as self reaping tasks it can be shown
    that release_task() will be called starting the grace period for
    task_struct long before the task leaves the runqueue.
    
    Therefore use put_task_struct_rcu_user() in finish_task_switch() to
    guarantee that the there is a RCU lifetime after the task
    leaves the runqueue.
    
    Besides the change in the start of the RCU grace period for the
    task_struct this change may cause perf_event_delayed_put and
    trace_sched_process_free.  The function perf_event_delayed_put boils
    down to just a WARN_ON for cases that I assume never show happen.  So
    I don't see any problem with delaying it.
    
    The function trace_sched_process_free is a trace point and thus
    visible to user space.  Occassionally userspace has the strangest
    dependencies so this has a miniscule chance of causing a regression.
    This change only changes the timing of when the tracepoint is called.
    The change in timing arguably gives userspace a more accurate picture
    of what is going on.  So I don't expect there to be a regression.
    
    In the case where a task self reaps we are pretty much guaranteed that
    the RCU grace period is delayed.  So we should get quite a bit of
    coverage in of this worst case for the change in a normal threaded
    workload.  So I expect any issues to turn up quickly or not at all.
    
    I have lightly tested this change and everything appears to work
    fine.
    
    Inspired-by: Linus Torvalds <torvalds@linux-foundation.org>
    Inspired-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King - ARM Linux admin <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/87r24jdpl5.fsf_-_@x220.int.ebiederm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7eefe338d7a2..d6e552552e52 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -902,10 +902,13 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	if (orig->cpus_ptr == &orig->cpus_mask)
 		tsk->cpus_ptr = &tsk->cpus_mask;
 
-	/* One for the user space visible state that goes away when reaped. */
-	refcount_set(&tsk->rcu_users, 1);
-	/* One for the rcu users, and one for the scheduler */
-	refcount_set(&tsk->usage, 2);
+	/*
+	 * One for the user space visible state that goes away when reaped.
+	 * One for the scheduler.
+	 */
+	refcount_set(&tsk->rcu_users, 2);
+	/* One for the rcu users */
+	refcount_set(&tsk->usage, 1);
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	tsk->btrace_seq = 0;
 #endif

commit 3fbd7ee285b2bbc6eebd15a3c8786d9776a402a8
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 14 07:33:34 2019 -0500

    tasks: Add a count of task RCU users
    
    Add a count of the number of RCU users (currently 1) of the task
    struct so that we can later add the scheduler case and get rid of the
    very subtle task_rcu_dereference(), and just use rcu_dereference().
    
    As suggested by Oleg have the count overlap rcu_head so that no
    additional space in task_struct is required.
    
    Inspired-by: Linus Torvalds <torvalds@linux-foundation.org>
    Inspired-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King - ARM Linux admin <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/87woebdplt.fsf_-_@x220.int.ebiederm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1d1cd06edbc1..7eefe338d7a2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -902,10 +902,9 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	if (orig->cpus_ptr == &orig->cpus_mask)
 		tsk->cpus_ptr = &tsk->cpus_mask;
 
-	/*
-	 * One for us, one for whoever does the "release_task()" (usually
-	 * parent)
-	 */
+	/* One for the user space visible state that goes away when reaped. */
+	refcount_set(&tsk->rcu_users, 1);
+	/* One for the rcu users, and one for the scheduler */
 	refcount_set(&tsk->usage, 2);
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	tsk->btrace_seq = 0;

commit 84da111de0b4be15bd500deff773f5116f39f7be
Merge: 227c3e9eb5cf 62974fc389b3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 21 10:07:42 2019 -0700

    Merge tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull hmm updates from Jason Gunthorpe:
     "This is more cleanup and consolidation of the hmm APIs and the very
      strongly related mmu_notifier interfaces. Many places across the tree
      using these interfaces are touched in the process. Beyond that a
      cleanup to the page walker API and a few memremap related changes
      round out the series:
    
       - General improvement of hmm_range_fault() and related APIs, more
         documentation, bug fixes from testing, API simplification &
         consolidation, and unused API removal
    
       - Simplify the hmm related kconfigs to HMM_MIRROR and DEVICE_PRIVATE,
         and make them internal kconfig selects
    
       - Hoist a lot of code related to mmu notifier attachment out of
         drivers by using a refcount get/put attachment idiom and remove the
         convoluted mmu_notifier_unregister_no_release() and related APIs.
    
       - General API improvement for the migrate_vma API and revision of its
         only user in nouveau
    
       - Annotate mmu_notifiers with lockdep and sleeping region debugging
    
      Two series unrelated to HMM or mmu_notifiers came along due to
      dependencies:
    
       - Allow pagemap's memremap_pages family of APIs to work without
         providing a struct device
    
       - Make walk_page_range() and related use a constant structure for
         function pointers"
    
    * tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (75 commits)
      libnvdimm: Enable unit test infrastructure compile checks
      mm, notifier: Catch sleeping/blocking for !blockable
      kernel.h: Add non_block_start/end()
      drm/radeon: guard against calling an unpaired radeon_mn_unregister()
      csky: add missing brackets in a macro for tlb.h
      pagewalk: use lockdep_assert_held for locking validation
      pagewalk: separate function pointers from iterator data
      mm: split out a new pagewalk.h header from mm.h
      mm/mmu_notifiers: annotate with might_sleep()
      mm/mmu_notifiers: prime lockdep
      mm/mmu_notifiers: add a lockdep map for invalidate_range_start/end
      mm/mmu_notifiers: remove the __mmu_notifier_invalidate_range_start/end exports
      mm/hmm: hmm_range_fault() infinite loop
      mm/hmm: hmm_range_fault() NULL pointer bug
      mm/hmm: fix hmm_range_fault()'s handling of swapped out pages
      mm/mmu_notifiers: remove unregister_no_release
      RDMA/odp: remove ib_ucontext from ib_umem
      RDMA/odp: use mmu_notifier_get/put for 'struct ib_ucontext_per_mm'
      RDMA/mlx5: Use odp instead of mr->umem in pagefault_mr
      RDMA/mlx5: Use ib_umem_start instead of umem.address
      ...

commit 7f2444d38f6bbfa12bc15e2533d8f9daa85ca02b
Merge: c5f12fdb8bd8 77b4b5420422
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 17 12:35:15 2019 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core timer updates from Thomas Gleixner:
     "Timers and timekeeping updates:
    
       - A large overhaul of the posix CPU timer code which is a preparation
         for moving the CPU timer expiry out into task work so it can be
         properly accounted on the task/process.
    
         An update to the bogus permission checks will come later during the
         merge window as feedback was not complete before heading of for
         travel.
    
       - Switch the timerqueue code to use cached rbtrees and get rid of the
         homebrewn caching of the leftmost node.
    
       - Consolidate hrtimer_init() + hrtimer_init_sleeper() calls into a
         single function
    
       - Implement the separation of hrtimers to be forced to expire in hard
         interrupt context even when PREEMPT_RT is enabled and mark the
         affected timers accordingly.
    
       - Implement a mechanism for hrtimers and the timer wheel to protect
         RT against priority inversion and live lock issues when a (hr)timer
         which should be canceled is currently executing the callback.
         Instead of infinitely spinning, the task which tries to cancel the
         timer blocks on a per cpu base expiry lock which is held and
         released by the (hr)timer expiry code.
    
       - Enable the Hyper-V TSC page based sched_clock for Hyper-V guests
         resulting in faster access to timekeeping functions.
    
       - Updates to various clocksource/clockevent drivers and their device
         tree bindings.
    
       - The usual small improvements all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (101 commits)
      posix-cpu-timers: Fix permission check regression
      posix-cpu-timers: Always clear head pointer on dequeue
      hrtimer: Add a missing bracket and hide `migration_base' on !SMP
      posix-cpu-timers: Make expiry_active check actually work correctly
      posix-timers: Unbreak CONFIG_POSIX_TIMERS=n build
      tick: Mark sched_timer to expire in hard interrupt context
      hrtimer: Add kernel doc annotation for HRTIMER_MODE_HARD
      x86/hyperv: Hide pv_ops access for CONFIG_PARAVIRT=n
      posix-cpu-timers: Utilize timerqueue for storage
      posix-cpu-timers: Move state tracking to struct posix_cputimers
      posix-cpu-timers: Deduplicate rlimit handling
      posix-cpu-timers: Remove pointless comparisons
      posix-cpu-timers: Get rid of 64bit divisions
      posix-cpu-timers: Consolidate timer expiry further
      posix-cpu-timers: Get rid of zero checks
      rlimit: Rewrite non-sensical RLIMIT_CPU comment
      posix-cpu-timers: Respect INFINITY for hard RTTIME limit
      posix-cpu-timers: Switch thread group sampling to array
      posix-cpu-timers: Restructure expiry array
      posix-cpu-timers: Remove cputime_expires
      ...

commit 76f0f227cffb570bc5ce343b1750f14907371d80
Merge: 58d4fafd0b4c 0d3d343560ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 15:32:01 2019 -0700

    Merge tag 'please-pull-ia64_for_5.4' of git://git.kernel.org/pub/scm/linux/kernel/git/aegl/linux
    
    Pull ia64 updates from Tony Luck:
     "The big change here is removal of support for SGI Altix"
    
    * tag 'please-pull-ia64_for_5.4' of git://git.kernel.org/pub/scm/linux/kernel/git/aegl/linux: (33 commits)
      genirq: remove the is_affinity_mask_valid hook
      ia64: remove CONFIG_SWIOTLB ifdefs
      ia64: remove support for machvecs
      ia64: move the screen_info setup to common code
      ia64: move the ROOT_DEV setup to common code
      ia64: rework iommu probing
      ia64: remove the unused sn_coherency_id symbol
      ia64: remove the SGI UV simulator support
      ia64: remove the zx1 swiotlb machvec
      ia64: remove CONFIG_ACPI ifdefs
      ia64: remove CONFIG_PCI ifdefs
      ia64: remove the hpsim platform
      ia64: remove now unused machvec indirections
      ia64: remove support for the SGI SN2 platform
      drivers: remove the SGI SN2 IOC4 base support
      drivers: remove the SGI SN2 IOC3 base support
      qla2xxx: remove SGI SN2 support
      qla1280: remove SGI SN2 support
      misc/sgi-xp: remove SGI SN2 support
      char/mspec: remove SGI SN2 support
      ...

commit c17112a5c413f20188da276c138484e7127cdc84
Merge: 4d856f72c10e 821cc7b0b205
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 09:28:19 2019 -0700

    Merge tag 'core-process-v5.4' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull pidfd/waitid updates from Christian Brauner:
     "This contains two features and various tests.
    
      First, it adds support for waiting on process through pidfds by adding
      the P_PIDFD type to the waitid() syscall. This completes the basic
      functionality of the pidfd api (cf. [1]). In the meantime we also have
      a new adition to the userspace projects that make use of the pidfd
      api. The qt project was nice enough to send a mail pointing out that
      they have a pr up to switch to the pidfd api (cf. [2]).
    
      Second, this tag contains an extension to the waitid() syscall to make
      it possible to wait on the current process group in a race free manner
      (even though the actual problem is very unlikely) by specifing 0
      together with the P_PGID type. This extension traces back to a
      discussion on the glibc development mailing list.
    
      There are also a range of tests for the features above. Additionally,
      the test-suite which detected the pidfd-polling race we fixed in [3]
      is included in this tag"
    
    [1] https://lwn.net/Articles/794707/
    [2] https://codereview.qt-project.org/c/qt/qtbase/+/108456
    [3] commit b191d6491be6 ("pidfd: fix a poll race when setting exit_state")
    
    * tag 'core-process-v5.4' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      waitid: Add support for waiting for the current process group
      tests: add pidfd poll tests
      tests: move common definitions and functions into pidfd.h
      pidfd: add pidfd_wait tests
      pidfd: add P_PIDFD to waitid()

commit a0eb9abd8af92d1aa34bc1e24dfbd1ba0bd6a56c
Author: Eugene Syromiatnikov <esyr@redhat.com>
Date:   Wed Sep 11 18:45:40 2019 +0100

    fork: block invalid exit signals with clone3()
    
    Previously, higher 32 bits of exit_signal fields were lost when copied
    to the kernel args structure (that uses int as a type for the respective
    field). Moreover, as Oleg has noted, exit_signal is used unchecked, so
    it has to be checked for sanity before use; for the legacy syscalls,
    applying CSIGNAL mask guarantees that it is at least non-negative;
    however, there's no such thing is done in clone3() code path, and that
    can break at least thread_group_leader.
    
    This commit adds a check to copy_clone_args_from_user() to verify that
    the exit signal is limited by CSIGNAL as with legacy clone() and that
    the signal is valid. With this we don't get the legacy clone behavior
    were an invalid signal could be handed down and would only be detected
    and ignored in do_notify_parent(). Users of clone3() will now get a
    proper error when they pass an invalid exit signal. Note, that this is
    not user-visible behavior since no kernel with clone3() has been
    released yet.
    
    The following program will cause a splat on a non-fixed clone3() version
    and will fail correctly on a fixed version:
    
     #define _GNU_SOURCE
     #include <linux/sched.h>
     #include <linux/types.h>
     #include <sched.h>
     #include <stdio.h>
     #include <stdlib.h>
     #include <sys/syscall.h>
     #include <sys/wait.h>
     #include <unistd.h>
    
     int main(int argc, char *argv[])
     {
            pid_t pid = -1;
            struct clone_args args = {0};
            args.exit_signal = -1;
    
            pid = syscall(__NR_clone3, &args, sizeof(struct clone_args));
            if (pid < 0)
                    exit(EXIT_FAILURE);
    
            if (pid == 0)
                    exit(EXIT_SUCCESS);
    
            wait(NULL);
    
            exit(EXIT_SUCCESS);
     }
    
    Fixes: 7f192e3cd316 ("fork: add clone3")
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Suggested-by: Dmitry V. Levin <ldv@altlinux.org>
    Signed-off-by: Eugene Syromiatnikov <esyr@redhat.com>
    Link: https://lore.kernel.org/r/4b38fa4ce420b119a4c6345f42fe3cec2de9b0b5.1568223594.git.esyr@redhat.com
    [christian.brauner@ubuntu.com: simplify check and rework commit message]
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2852d0e76ea3..541fd805fb88 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2338,6 +2338,8 @@ struct mm_struct *copy_init_mm(void)
  *
  * It copies the process, and if successful kick-starts
  * it and waits for it to finish using the VM if required.
+ *
+ * args->exit_signal is expected to be checked for sanity by the caller.
  */
 long _do_fork(struct kernel_clone_args *args)
 {
@@ -2562,6 +2564,14 @@ noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 	if (copy_from_user(&args, uargs, size))
 		return -EFAULT;
 
+	/*
+	 * Verify that higher 32bits of exit_signal are unset and that
+	 * it is a valid signal
+	 */
+	if (unlikely((args.exit_signal & ~((u64)CSIGNAL)) ||
+		     !valid_signal(args.exit_signal)))
+		return -EINVAL;
+
 	*kargs = (struct kernel_clone_args){
 		.flags		= args.flags,
 		.pidfd		= u64_to_user_ptr(args.pidfd),

commit 244d49e30653658d4e7e9b2b8427777cbbc5affe
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 21 21:09:24 2019 +0200

    posix-cpu-timers: Move state tracking to struct posix_cputimers
    
    Put it where it belongs and clean up the ifdeffery in fork completely.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190821192922.743229404@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index 52bfe7c20ff6..f1228d9f0b11 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1517,7 +1517,6 @@ void __cleanup_sighand(struct sighand_struct *sighand)
 	}
 }
 
-#ifdef CONFIG_POSIX_TIMERS
 /*
  * Initialize POSIX timer handling for a thread group.
  */
@@ -1528,12 +1527,7 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 
 	cpu_limit = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
 	posix_cputimers_group_init(pct, cpu_limit);
-	if (cpu_limit != RLIM_INFINITY)
-		sig->cputimer.running = true;
 }
-#else
-static inline void posix_cpu_timers_init_group(struct signal_struct *sig) { }
-#endif
 
 static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 {

commit 3a245c0f110e2bfcf7f2cd2248a29005c78999e3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 21 21:09:06 2019 +0200

    posix-cpu-timers: Move expiry cache into struct posix_cputimers
    
    The expiry cache belongs into the posix_cputimers container where the other
    cpu timers information is.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Link: https://lkml.kernel.org/r/20190821192921.014444012@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index b6a135e4275b..52bfe7c20ff6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1527,12 +1527,9 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 	unsigned long cpu_limit;
 
 	cpu_limit = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
-	if (cpu_limit != RLIM_INFINITY) {
-		sig->cputime_expires.prof_exp = cpu_limit * NSEC_PER_SEC;
+	posix_cputimers_group_init(pct, cpu_limit);
+	if (cpu_limit != RLIM_INFINITY)
 		sig->cputimer.running = true;
-	}
-
-	posix_cputimers_init(pct);
 }
 #else
 static inline void posix_cpu_timers_init_group(struct signal_struct *sig) { }
@@ -1638,22 +1635,6 @@ static void rt_mutex_init_task(struct task_struct *p)
 #endif
 }
 
-#ifdef CONFIG_POSIX_TIMERS
-/*
- * Initialize POSIX timer handling for a single task.
- */
-static void posix_cpu_timers_init(struct task_struct *tsk)
-{
-	tsk->cputime_expires.prof_exp = 0;
-	tsk->cputime_expires.virt_exp = 0;
-	tsk->cputime_expires.sched_exp = 0;
-
-	posix_cputimers_init(&tsk->posix_cputimers);
-}
-#else
-static inline void posix_cpu_timers_init(struct task_struct *tsk) { }
-#endif
-
 static inline void init_task_pid_links(struct task_struct *task)
 {
 	enum pid_type type;
@@ -1932,7 +1913,7 @@ static __latent_entropy struct task_struct *copy_process(
 	task_io_accounting_init(&p->ioac);
 	acct_clear_integrals(p);
 
-	posix_cpu_timers_init(p);
+	posix_cputimers_init(&p->posix_cputimers);
 
 	p->io_context = NULL;
 	audit_set_context(p, NULL);

commit 2b69942f9021bf75bd1b001f53bd2578361fadf3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 21 21:09:04 2019 +0200

    posix-cpu-timers: Create a container struct
    
    Per task/process data of posix CPU timers is all over the place which
    makes the code hard to follow and requires ifdeffery.
    
    Create a container to hold all this information in one place, so data is
    consolidated and the ifdeffery can be confined to the posix timer header
    file and removed from places like fork.
    
    As a first step, move the cpu_timers list head array into the new struct
    and clean up the initializers and simplify fork. The remaining #ifdef in
    fork will be removed later.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Link: https://lkml.kernel.org/r/20190821192920.819418976@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index d8ae0f1b4148..b6a135e4275b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1523,6 +1523,7 @@ void __cleanup_sighand(struct sighand_struct *sighand)
  */
 static void posix_cpu_timers_init_group(struct signal_struct *sig)
 {
+	struct posix_cputimers *pct = &sig->posix_cputimers;
 	unsigned long cpu_limit;
 
 	cpu_limit = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
@@ -1531,10 +1532,7 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 		sig->cputimer.running = true;
 	}
 
-	/* The timer lists. */
-	INIT_LIST_HEAD(&sig->cpu_timers[0]);
-	INIT_LIST_HEAD(&sig->cpu_timers[1]);
-	INIT_LIST_HEAD(&sig->cpu_timers[2]);
+	posix_cputimers_init(pct);
 }
 #else
 static inline void posix_cpu_timers_init_group(struct signal_struct *sig) { }
@@ -1649,9 +1647,8 @@ static void posix_cpu_timers_init(struct task_struct *tsk)
 	tsk->cputime_expires.prof_exp = 0;
 	tsk->cputime_expires.virt_exp = 0;
 	tsk->cputime_expires.sched_exp = 0;
-	INIT_LIST_HEAD(&tsk->cpu_timers[0]);
-	INIT_LIST_HEAD(&tsk->cpu_timers[1]);
-	INIT_LIST_HEAD(&tsk->cpu_timers[2]);
+
+	posix_cputimers_init(&tsk->posix_cputimers);
 }
 #else
 static inline void posix_cpu_timers_init(struct task_struct *tsk) { }

commit daa138a58c802e7b4c2fb73f9b85bb082616ef43
Merge: 6869b7b20659 fba0e448a2c5
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Wed Aug 21 14:12:29 2019 -0300

    Merge branch 'odp_fixes' into hmm.git
    
    From rdma.git
    
    Jason Gunthorpe says:
    
    ====================
    This is a collection of general cleanups for ODP to clarify some of the
    flows around umem creation and use of the interval tree.
    ====================
    
    The branch is based on v5.3-rc5 due to dependencies, and is being taken
    into hmm.git due to dependencies in the next patches.
    
    * odp_fixes:
      RDMA/mlx5: Use odp instead of mr->umem in pagefault_mr
      RDMA/mlx5: Use ib_umem_start instead of umem.address
      RDMA/core: Make invalidate_range a device operation
      RDMA/odp: Use kvcalloc for the dma_list and page_list
      RDMA/odp: Check for overflow when computing the umem_odp end
      RDMA/odp: Provide ib_umem_odp_release() to undo the allocs
      RDMA/odp: Split creating a umem_odp from ib_umem_get
      RDMA/odp: Make the three ways to create a umem_odp clear
      RMDA/odp: Consolidate umem_odp initialization
      RDMA/odp: Make it clearer when a umem is an implicit ODP umem
      RDMA/odp: Iterate over the whole rbtree directly
      RDMA/odp: Use the common interval tree library instead of generic
      RDMA/mlx5: Fix MR npages calculation for IB_ACCESS_HUGETLB
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

commit c7d8b7824ff9de866a356e1892dbe9f191aa5d06
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Aug 6 20:15:42 2019 -0300

    hmm: use mmu_notifier_get/put for 'struct hmm'
    
    This is a significant simplification, it eliminates all the remaining
    'hmm' stuff in mm_struct, eliminates krefing along the critical notifier
    paths, and takes away all the ugly locking and abuse of page_table_lock.
    
    mmu_notifier_get() provides the single struct hmm per struct mm which
    eliminates mm->hmm.
    
    It also directly guarantees that no mmu_notifier op callback is callable
    while concurrent free is possible, this eliminates all the krefs inside
    the mmu_notifier callbacks.
    
    The remaining krefs in the range code were overly cautious, drivers are
    already not permitted to free the mirror while a range exists.
    
    Link: https://lore.kernel.org/r/20190806231548.25242-6-jgg@ziepe.ca
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
    Tested-by: Ralph Campbell <rcampbell@nvidia.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index d8ae0f1b4148..bd4a0762f12f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1007,7 +1007,6 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	mm_init_owner(mm, p);
 	RCU_INIT_POINTER(mm->exe_file, NULL);
 	mmu_notifier_mm_init(mm);
-	hmm_mm_init(mm);
 	init_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;

commit 4189ff23489e6688ef4c7f5109df0cebbad425b1
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Aug 12 08:55:24 2019 +0200

    kernel: only define task_struct_whitelist conditionally
    
    If CONFIG_ARCH_TASK_STRUCT_ALLOCATOR is set task_struct_whitelist is
    never called, and thus generates a compiler warning.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Link: https://lkml.kernel.org/r/20190812065524.19959-5-hch@lst.de
    Signed-off-by: Tony Luck <tony.luck@intel.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2852d0e76ea3..f79e3da0caaf 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -768,6 +768,7 @@ static void set_max_threads(unsigned int max_threads_suggested)
 int arch_task_struct_size __read_mostly;
 #endif
 
+#ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 static void task_struct_whitelist(unsigned long *offset, unsigned long *size)
 {
 	/* Fetch thread_struct whitelist for the architecture. */
@@ -782,6 +783,7 @@ static void task_struct_whitelist(unsigned long *offset, unsigned long *size)
 	else
 		*offset += offsetof(struct task_struct, thread);
 }
+#endif /* CONFIG_ARCH_TASK_STRUCT_ALLOCATOR */
 
 void __init fork_init(void)
 {

commit 3695eae5fee0605f316fbaad0b9e3de791d7dfaf
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Sun Jul 28 00:22:29 2019 +0200

    pidfd: add P_PIDFD to waitid()
    
    This adds the P_PIDFD type to waitid().
    One of the last remaining bits for the pidfd api is to make it possible
    to wait on pidfds. With P_PIDFD added to waitid() the parts of userspace
    that want to use the pidfd api to exclusively manage processes can do so
    now.
    
    One of the things this will unblock in the future is the ability to make
    it possible to retrieve the exit status via waitid(P_PIDFD) for
    non-parent processes if handed a _suitable_ pidfd that has this feature
    set. This is similar to what you can do on FreeBSD with kqueue(). It
    might even end up being possible to wait on a process as a non-parent if
    an appropriate property is enabled on the pidfd.
    
    With P_PIDFD no scoping of the process identified by the pidfd is
    possible, i.e. it explicitly blocks things such as wait4(-1), wait4(0),
    waitid(P_ALL), waitid(P_PGID) etc. It only allows for semantics
    equivalent to wait4(pid), waitid(P_PID). Users that need scoping should
    rely on pid-based wait*() syscalls for now.
    
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Joel Fernandes (Google) <joel@joelfernandes.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Andy Lutomirsky <luto@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aleksa Sarai <cyphar@cyphar.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Link: https://lore.kernel.org/r/20190727222229.6516-2-christian@brauner.io

diff --git a/kernel/fork.c b/kernel/fork.c
index d8ae0f1b4148..b169e2ca2d84 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1690,6 +1690,14 @@ static inline void rcu_copy_process(struct task_struct *p)
 #endif /* #ifdef CONFIG_TASKS_RCU */
 }
 
+struct pid *pidfd_pid(const struct file *file)
+{
+	if (file->f_op == &pidfd_fops)
+		return file->private_data;
+
+	return ERR_PTR(-EBADF);
+}
+
 static int pidfd_release(struct inode *inode, struct file *file)
 {
 	struct pid *pid = file->private_data;

commit 16d51a590a8ce3befb1308e0e7ab77f3b661af33
Author: Jann Horn <jannh@google.com>
Date:   Tue Jul 16 17:20:45 2019 +0200

    sched/fair: Don't free p->numa_faults with concurrent readers
    
    When going through execve(), zero out the NUMA fault statistics instead of
    freeing them.
    
    During execve, the task is reachable through procfs and the scheduler. A
    concurrent /proc/*/sched reader can read data from a freed ->numa_faults
    allocation (confirmed by KASAN) and write it back to userspace.
    I believe that it would also be possible for a use-after-free read to occur
    through a race between a NUMA fault and execve(): task_numa_fault() can
    lead to task_numa_compare(), which invokes task_weight() on the currently
    running task of a different CPU.
    
    Another way to fix this would be to make ->numa_faults RCU-managed or add
    extra locking, but it seems easier to wipe the NUMA fault statistics on
    execve.
    
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will@kernel.org>
    Fixes: 82727018b0d3 ("sched/numa: Call task_numa_free() from do_execve()")
    Link: https://lkml.kernel.org/r/20190716152047.14424-1-jannh@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d8ae0f1b4148..2852d0e76ea3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -726,7 +726,7 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(tsk == current);
 
 	cgroup_free(tsk);
-	task_numa_free(tsk);
+	task_numa_free(tsk, true);
 	security_task_free(tsk);
 	exit_creds(tsk);
 	delayacct_tsk_free(tsk);

commit 3c69914b4c7b0b72ff0275c14743778057ee8a6e
Merge: 2954152298c3 69b53720e92c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 16 11:30:07 2019 -0700

    Merge tag 'for-linus-20190715' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull pidfd and clone3 fixes from Christian Brauner:
     "This contains a bugfix for CLONE_PIDFD when used with the legacy clone
      syscall, two fixes to ensure that syscall numbering and clone3
      entrypoint implementations will stay consistent, and an update for the
      maintainers file:
    
       - The addition of clone3 broke CLONE_PIDFD for legacy clone on all
         architectures that use do_fork() directly instead of calling the
         clone syscall itself. (Fwiw, cleaning do_fork() up is on my todo.)
    
         The reason this happened was that during conversion of _do_fork()
         to use struct kernel_clone_args we missed that do_fork() is called
         directly by various architectures. This is fixed by making sure
         that the pidfd argument in struct kernel_clone_args is correctly
         initialized with the parent_tidptr argument passed down from
         do_fork(). Additionally, do_fork() missed a check to make
         CLONE_PIDFD and CLONE_PARENT_SETTID mutually exclusive just a
         clone() does. This is now fixed too.
    
       - When clone3() was introduced we skipped architectures that require
         special handling for fork-like syscalls. Their syscall tables did
         not contain any mention of clone3().
    
         To make sure that Arnd's work to make syscall numbers on all
         architectures identical (minus alpha) was not for naught we are
         placing a comment in all syscall tables that do not yet implement
         clone3(). The comment makes it clear that 435 is reserved for
         clone3 and should not be used.
    
       - Also, this contains a patch to make the clone3() syscall definition
         in asm-generic/unist.h conditional on __ARCH_WANT_SYS_CLONE3. This
         lets us catch new architectures that implicitly make use of clone3
         without setting __ARCH_WANT_SYS_CLONE3 which is a good indicator
         that they did not check whether it needs special treatment or not.
    
       - Finally, this contains a patch to add me as maintainer for pidfd
         stuff so people can start blaming me (more)"
    
    * tag 'for-linus-20190715' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      MAINTAINERS: add new entry for pidfd api
      unistd: protect clone3 via __ARCH_WANT_SYS_CLONE3
      arch: mark syscall number 435 reserved for clone3
      clone: fix CLONE_PIDFD support

commit fec88ab0af9706b2201e5daf377c5031c62d11f7
Merge: fa6e951a2a44 cc5dfd59e375
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 14 19:42:11 2019 -0700

    Merge tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull HMM updates from Jason Gunthorpe:
     "Improvements and bug fixes for the hmm interface in the kernel:
    
       - Improve clarity, locking and APIs related to the 'hmm mirror'
         feature merged last cycle. In linux-next we now see AMDGPU and
         nouveau to be using this API.
    
       - Remove old or transitional hmm APIs. These are hold overs from the
         past with no users, or APIs that existed only to manage cross tree
         conflicts. There are still a few more of these cleanups that didn't
         make the merge window cut off.
    
       - Improve some core mm APIs:
           - export alloc_pages_vma() for driver use
           - refactor into devm_request_free_mem_region() to manage
             DEVICE_PRIVATE resource reservations
           - refactor duplicative driver code into the core dev_pagemap
             struct
    
       - Remove hmm wrappers of improved core mm APIs, instead have drivers
         use the simplified API directly
    
       - Remove DEVICE_PUBLIC
    
       - Simplify the kconfig flow for the hmm users and core code"
    
    * tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (42 commits)
      mm: don't select MIGRATE_VMA_HELPER from HMM_MIRROR
      mm: remove the HMM config option
      mm: sort out the DEVICE_PRIVATE Kconfig mess
      mm: simplify ZONE_DEVICE page private data
      mm: remove hmm_devmem_add
      mm: remove hmm_vma_alloc_locked_page
      nouveau: use devm_memremap_pages directly
      nouveau: use alloc_page_vma directly
      PCI/P2PDMA: use the dev_pagemap internal refcount
      device-dax: use the dev_pagemap internal refcount
      memremap: provide an optional internal refcount in struct dev_pagemap
      memremap: replace the altmap_valid field with a PGMAP_ALTMAP_VALID flag
      memremap: remove the data field in struct dev_pagemap
      memremap: add a migrate_to_ram method to struct dev_pagemap_ops
      memremap: lift the devmap_enable manipulation into devm_memremap_pages
      memremap: pass a struct dev_pagemap to ->kill and ->cleanup
      memremap: move dev_pagemap callbacks into a separate structure
      memremap: validate the pagemap type passed to devm_memremap_pages
      mm: factor out a devm_request_free_mem_region helper
      mm: export alloc_pages_vma
      ...

commit 028b6e8a89de9133a869bb4cd1bc72445b1ec8ca
Author: Dmitry V. Levin <ldv@altlinux.org>
Date:   Sun Jul 14 19:20:47 2019 +0300

    clone: fix CLONE_PIDFD support
    
    The introduction of clone3 syscall accidentally broke CLONE_PIDFD
    support in traditional clone syscall on compat x86 and those
    architectures that use do_fork to implement clone syscall.
    
    This bug was found by strace test suite.
    
    Link: https://strace.io/logs/strace/2019-07-12
    Fixes: 7f192e3cd316 ("fork: add clone3")
    Bisected-and-tested-by: Anatoly Pugachev <matorola@gmail.com>
    Signed-off-by: Dmitry V. Levin <ldv@altlinux.org>
    Link: https://lore.kernel.org/r/20190714162047.GB10389@altlinux.org
    Signed-off-by: Christian Brauner <christian@brauner.io>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8f3e2d97d771..ef1e05a68827 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2406,6 +2406,16 @@ long _do_fork(struct kernel_clone_args *args)
 	return nr;
 }
 
+bool legacy_clone_args_valid(const struct kernel_clone_args *kargs)
+{
+	/* clone(CLONE_PIDFD) uses parent_tidptr to return a pidfd */
+	if ((kargs->flags & CLONE_PIDFD) &&
+	    (kargs->flags & CLONE_PARENT_SETTID))
+		return false;
+
+	return true;
+}
+
 #ifndef CONFIG_HAVE_COPY_THREAD_TLS
 /* For compatibility with architectures that call do_fork directly rather than
  * using the syscall entry points below. */
@@ -2417,6 +2427,7 @@ long do_fork(unsigned long clone_flags,
 {
 	struct kernel_clone_args args = {
 		.flags		= (clone_flags & ~CSIGNAL),
+		.pidfd		= parent_tidptr,
 		.child_tid	= child_tidptr,
 		.parent_tid	= parent_tidptr,
 		.exit_signal	= (clone_flags & CSIGNAL),
@@ -2424,6 +2435,9 @@ long do_fork(unsigned long clone_flags,
 		.stack_size	= stack_size,
 	};
 
+	if (!legacy_clone_args_valid(&args))
+		return -EINVAL;
+
 	return _do_fork(&args);
 }
 #endif
@@ -2505,8 +2519,7 @@ SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 		.tls		= tls,
 	};
 
-	/* clone(CLONE_PIDFD) uses parent_tidptr to return a pidfd */
-	if ((clone_flags & CLONE_PIDFD) && (clone_flags & CLONE_PARENT_SETTID))
+	if (!legacy_clone_args_valid(&args))
 		return -EINVAL;
 
 	return _do_fork(&args);

commit 8f6ccf6159aed1f04c6d179f61f6fb2691261e84
Merge: 5450e8a316a6 d68dbb0c9ac8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 11 10:09:44 2019 -0700

    Merge tag 'clone3-v5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull clone3 system call from Christian Brauner:
     "This adds the clone3 syscall which is an extensible successor to clone
      after we snagged the last flag with CLONE_PIDFD during the 5.2 merge
      window for clone(). It cleanly supports all of the flags from clone()
      and thus all legacy workloads.
    
      There are few user visible differences between clone3 and clone.
      First, CLONE_DETACHED will cause EINVAL with clone3 so we can reuse
      this flag. Second, the CSIGNAL flag is deprecated and will cause
      EINVAL to be reported. It is superseeded by a dedicated "exit_signal"
      argument in struct clone_args thus freeing up even more flags. And
      third, clone3 gives CLONE_PIDFD a dedicated return argument in struct
      clone_args instead of abusing CLONE_PARENT_SETTID's parent_tidptr
      argument.
    
      The clone3 uapi is designed to be easy to handle on 32- and 64 bit:
    
        /* uapi */
        struct clone_args {
                __aligned_u64 flags;
                __aligned_u64 pidfd;
                __aligned_u64 child_tid;
                __aligned_u64 parent_tid;
                __aligned_u64 exit_signal;
                __aligned_u64 stack;
                __aligned_u64 stack_size;
                __aligned_u64 tls;
        };
    
      and a separate kernel struct is used that uses proper kernel typing:
    
        /* kernel internal */
        struct kernel_clone_args {
                u64 flags;
                int __user *pidfd;
                int __user *child_tid;
                int __user *parent_tid;
                int exit_signal;
                unsigned long stack;
                unsigned long stack_size;
                unsigned long tls;
        };
    
      The system call comes with a size argument which enables the kernel to
      detect what version of clone_args userspace is passing in. clone3
      validates that any additional bytes a given kernel does not know about
      are set to zero and that the size never exceeds a page.
    
      A nice feature is that this patchset allowed us to cleanup and
      simplify various core kernel codepaths in kernel/fork.c by making the
      internal _do_fork() function take struct kernel_clone_args even for
      legacy clone().
    
      This patch also unblocks the time namespace patchset which wants to
      introduce a new CLONE_TIMENS flag.
    
      Note, that clone3 has only been wired up for x86{_32,64}, arm{64}, and
      xtensa. These were the architectures that did not require special
      massaging.
    
      Other architectures treat fork-like system calls individually and
      after some back and forth neither Arnd nor I felt confident that we
      dared to add clone3 unconditionally to all architectures. We agreed to
      leave this up to individual architecture maintainers. This is why
      there's an additional patch that introduces __ARCH_WANT_SYS_CLONE3
      which any architecture can set once it has implemented support for
      clone3. The patch also adds a cond_syscall(clone3) for architectures
      such as nios2 or h8300 that generate their syscall table by simply
      including asm-generic/unistd.h. The hope is to get rid of
      __ARCH_WANT_SYS_CLONE3 and cond_syscall() rather soon"
    
    * tag 'clone3-v5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      arch: handle arches who do not yet define clone3
      arch: wire-up clone3() syscall
      fork: add clone3

commit 5450e8a316a64cddcbc15f90733ebc78aa736545
Merge: 29cd581b5949 172bb24a4f48
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 10 22:17:21 2019 -0700

    Merge tag 'pidfd-updates-v5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull pidfd updates from Christian Brauner:
     "This adds two main features.
    
       - First, it adds polling support for pidfds. This allows process
         managers to know when a (non-parent) process dies in a race-free
         way.
    
         The notification mechanism used follows the same logic that is
         currently used when the parent of a task is notified of a child's
         death. With this patchset it is possible to put pidfds in an
         {e}poll loop and get reliable notifications for process (i.e.
         thread-group) exit.
    
       - The second feature compliments the first one by making it possible
         to retrieve pollable pidfds for processes that were not created
         using CLONE_PIDFD.
    
         A lot of processes get created with traditional PID-based calls
         such as fork() or clone() (without CLONE_PIDFD). For these
         processes a caller can currently not create a pollable pidfd. This
         is a problem for Android's low memory killer (LMK) and service
         managers such as systemd.
    
      Both patchsets are accompanied by selftests.
    
      It's perhaps worth noting that the work done so far and the work done
      in this branch for pidfd_open() and polling support do already see
      some adoption:
    
       - Android is in the process of backporting this work to all their LTS
         kernels [1]
    
       - Service managers make use of pidfd_send_signal but will need to
         wait until we enable waiting on pidfds for full adoption.
    
       - And projects I maintain make use of both pidfd_send_signal and
         CLONE_PIDFD [2] and will use polling support and pidfd_open() too"
    
    [1] https://android-review.googlesource.com/q/topic:%22pidfd+polling+support+4.9+backport%22
        https://android-review.googlesource.com/q/topic:%22pidfd+polling+support+4.14+backport%22
        https://android-review.googlesource.com/q/topic:%22pidfd+polling+support+4.19+backport%22
    
    [2] https://github.com/lxc/lxc/blob/aab6e3eb73c343231cdde775db938994fc6f2803/src/lxc/start.c#L1753
    
    * tag 'pidfd-updates-v5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      tests: add pidfd_open() tests
      arch: wire-up pidfd_open()
      pid: add pidfd_open()
      pidfd: add polling selftests
      pidfd: add polling support

commit dad1c12ed831a7a89cc01e5582cd0b81a4be7f19
Merge: 090bc5a2a914 af24bde8df20
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 16:39:53 2019 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - Remove the unused per rq load array and all its infrastructure, by
       Dietmar Eggemann.
    
     - Add utilization clamping support by Patrick Bellasi. This is a
       refinement of the energy aware scheduling framework with support for
       boosting of interactive and capping of background workloads: to make
       sure critical GUI threads get maximum frequency ASAP, and to make
       sure background processing doesn't unnecessarily move to cpufreq
       governor to higher frequencies and less energy efficient CPU modes.
    
     - Add the bare minimum of tracepoints required for LISA EAS regression
       testing, by Qais Yousef - which allows automated testing of various
       power management features, including energy aware scheduling.
    
     - Restructure the former tsk_nr_cpus_allowed() facility that the -rt
       kernel used to modify the scheduler's CPU affinity logic such as
       migrate_disable() - introduce the task->cpus_ptr value instead of
       taking the address of &task->cpus_allowed directly - by Sebastian
       Andrzej Siewior.
    
     - Misc optimizations, fixes, cleanups and small enhancements - see the
       Git log for details.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (33 commits)
      sched/uclamp: Add uclamp support to energy_compute()
      sched/uclamp: Add uclamp_util_with()
      sched/cpufreq, sched/uclamp: Add clamps for FAIR and RT tasks
      sched/uclamp: Set default clamps for RT tasks
      sched/uclamp: Reset uclamp values on RESET_ON_FORK
      sched/uclamp: Extend sched_setattr() to support utilization clamping
      sched/core: Allow sched_setattr() to use the current policy
      sched/uclamp: Add system default clamps
      sched/uclamp: Enforce last task's UCLAMP_MAX
      sched/uclamp: Add bucket local max tracking
      sched/uclamp: Add CPU's clamp buckets refcounting
      sched/fair: Rename weighted_cpuload() to cpu_runnable_load()
      sched/debug: Export the newly added tracepoints
      sched/debug: Add sched_overutilized tracepoint
      sched/debug: Add new tracepoint to track PELT at se level
      sched/debug: Add new tracepoints to track PELT at rq level
      sched/debug: Add a new sched_trace_*() helper functions
      sched/autogroup: Make autogroup_path() always available
      sched/wait: Deduplicate code with do-while
      sched/topology: Remove unused 'sd' parameter from arch_scale_cpu_capacity()
      ...

commit e1928328699a582a540b105e5f4c160832a7fdcb
Merge: 46f1ec23a469 9156e545765e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 16:12:03 2019 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - rwsem scalability improvements, phase #2, by Waiman Long, which are
         rather impressive:
    
           "On a 2-socket 40-core 80-thread Skylake system with 40 reader
            and writer locking threads, the min/mean/max locking operations
            done in a 5-second testing window before the patchset were:
    
             40 readers, Iterations Min/Mean/Max = 1,807/1,808/1,810
             40 writers, Iterations Min/Mean/Max = 1,807/50,344/151,255
    
            After the patchset, they became:
    
             40 readers, Iterations Min/Mean/Max = 30,057/31,359/32,741
             40 writers, Iterations Min/Mean/Max = 94,466/95,845/97,098"
    
         There's a lot of changes to the locking implementation that makes
         it similar to qrwlock, including owner handoff for more fair
         locking.
    
         Another microbenchmark shows how across the spectrum the
         improvements are:
    
           "With a locking microbenchmark running on 5.1 based kernel, the
            total locking rates (in kops/s) on a 2-socket Skylake system
            with equal numbers of readers and writers (mixed) before and
            after this patchset were:
    
            # of Threads   Before Patch      After Patch
            ------------   ------------      -----------
                 2            2,618             4,193
                 4            1,202             3,726
                 8              802             3,622
                16              729             3,359
                32              319             2,826
                64              102             2,744"
    
         The changes are extensive and the patch-set has been through
         several iterations addressing various locking workloads. There
         might be more regressions, but unless they are pathological I
         believe we want to use this new implementation as the baseline
         going forward.
    
       - jump-label optimizations by Daniel Bristot de Oliveira: the primary
         motivation was to remove IPI disturbance of isolated RT-workload
         CPUs, which resulted in the implementation of batched jump-label
         updates. Beyond the improvement of the real-time characteristics
         kernel, in one test this patchset improved static key update
         overhead from 57 msecs to just 1.4 msecs - which is a nice speedup
         as well.
    
       - atomic64_t cross-arch type cleanups by Mark Rutland: over the last
         ~10 years of atomic64_t existence the various types used by the
         APIs only had to be self-consistent within each architecture -
         which means they became wildly inconsistent across architectures.
         Mark puts and end to this by reworking all the atomic64
         implementations to use 's64' as the base type for atomic64_t, and
         to ensure that this type is consistently used for parameters and
         return values in the API, avoiding further problems in this area.
    
       - A large set of small improvements to lockdep by Yuyang Du: type
         cleanups, output cleanups, function return type and othr cleanups
         all around the place.
    
       - A set of percpu ops cleanups and fixes by Peter Zijlstra.
    
       - Misc other changes - please see the Git log for more details"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (82 commits)
      locking/lockdep: increase size of counters for lockdep statistics
      locking/atomics: Use sed(1) instead of non-standard head(1) option
      locking/lockdep: Move mark_lock() inside CONFIG_TRACE_IRQFLAGS && CONFIG_PROVE_LOCKING
      x86/jump_label: Make tp_vec_nr static
      x86/percpu: Optimize raw_cpu_xchg()
      x86/percpu, sched/fair: Avoid local_clock()
      x86/percpu, x86/irq: Relax {set,get}_irq_regs()
      x86/percpu: Relax smp_processor_id()
      x86/percpu: Differentiate this_cpu_{}() and __this_cpu_{}()
      locking/rwsem: Guard against making count negative
      locking/rwsem: Adaptive disabling of reader optimistic spinning
      locking/rwsem: Enable time-based spinning on reader-owned rwsem
      locking/rwsem: Make rwsem->owner an atomic_long_t
      locking/rwsem: Enable readers spinning on writer
      locking/rwsem: Clarify usage of owner's nonspinaable bit
      locking/rwsem: Wake up almost all readers in wait queue
      locking/rwsem: More optimal RT task handling of null owner
      locking/rwsem: Always release wait_lock before waking up tasks
      locking/rwsem: Implement lock handoff to prevent lock starvation
      locking/rwsem: Make rwsem_spin_on_owner() return owner state
      ...

commit 927ba67a63c72ee87d655e30183d1576c3717d3e
Merge: 2a1ccd31420a 9176ab1b8480
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 11:06:29 2019 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "The timer and timekeeping departement delivers:
    
      Core:
    
       - The consolidation of the VDSO code into a generic library including
         the conversion of x86 and ARM64. Conversion of ARM and MIPS are en
         route through the relevant maintainer trees and should end up in
         5.4.
    
         This gets rid of the unnecessary different copies of the same code
         and brings all architectures on the same level of VDSO
         functionality.
    
       - Make the NTP user space interface more robust by restricting the
         TAI offset to prevent undefined behaviour. Includes a selftest.
    
       - Validate user input in the compat settimeofday() syscall to catch
         invalid values which would be turned into valid values by a
         multiplication overflow
    
       - Consolidate the time accessors
    
       - Small fixes, improvements and cleanups all over the place
    
      Drivers:
    
       - Support for the NXP system counter, TI davinci timer
    
       - Move the Microsoft HyperV clocksource/events code into the
         drivers/clocksource directory so it can be shared between x86 and
         ARM64.
    
       - Overhaul of the Tegra driver
    
       - Delay timer support for IXP4xx
    
       - Small fixes, improvements and cleanups as usual"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (71 commits)
      time: Validate user input in compat_settimeofday()
      timer: Document TIMER_PINNED
      clocksource/drivers: Continue making Hyper-V clocksource ISA agnostic
      clocksource/drivers: Make Hyper-V clocksource ISA agnostic
      MAINTAINERS: Fix Andy's surname and the directory entries of VDSO
      hrtimer: Use a bullet for the returns bullet list
      arm64: vdso: Fix compilation with clang older than 8
      arm64: compat: Fix __arch_get_hw_counter() implementation
      arm64: Fix __arch_get_hw_counter() implementation
      lib/vdso: Make delta calculation work correctly
      MAINTAINERS: Add entry for the generic VDSO library
      arm64: compat: No need for pre-ARMv7 barriers on an ARMv8 system
      arm64: vdso: Remove unnecessary asm-offsets.c definitions
      vdso: Remove superfluous #ifdef __KERNEL__ in vdso/datapage.h
      clocksource/drivers/davinci: Add support for clocksource
      clocksource/drivers/davinci: Add support for clockevents
      clocksource/drivers/tegra: Set up maximum-ticks limit properly
      clocksource/drivers/tegra: Cycles can't be 0
      clocksource/drivers/tegra: Restore base address before cleanup
      clocksource/drivers/tegra: Add verbose definition for 1MHz constant
      ...

commit 9ec3f4cb35bc8278f0582fed9f9229c9315c2ffb
Merge: c5d6c45e90c4 6fbc7275c7a9
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Tue Jul 2 14:34:43 2019 -0300

    Merge tag 'v5.2-rc7' into rdma.git hmm
    
    Required for dependencies in the next patches.

commit 28dd29c06d0dede4b32b2c559cff21955a830928
Author: Christian Brauner <christian@brauner.io>
Date:   Mon Jul 1 16:01:46 2019 +0200

    fork: return proper negative error code
    
    Make sure to return a proper negative error code from copy_process()
    when anon_inode_getfile() fails with CLONE_PIDFD.
    Otherwise _do_fork() will not detect an error and get_task_pid() will
    operator on a nonsensical pointer:
    
    R10: 0000000000000000 R11: 0000000000000246 R12: 00000000006dbc2c
    R13: 00007ffc15fbb0ff R14: 00007ff07e47e9c0 R15: 0000000000000000
    kasan: CONFIG_KASAN_INLINE enabled
    kasan: GPF could be caused by NULL-ptr deref or user memory access
    general protection fault: 0000 [#1] PREEMPT SMP KASAN
    CPU: 1 PID: 7990 Comm: syz-executor290 Not tainted 5.2.0-rc6+ #9
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS
    Google 01/01/2011
    RIP: 0010:__read_once_size include/linux/compiler.h:194 [inline]
    RIP: 0010:get_task_pid+0xe1/0x210 kernel/pid.c:372
    Code: 89 ff e8 62 27 5f 00 49 8b 07 44 89 f1 4c 8d bc c8 90 01 00 00 eb 0c
    e8 0d fe 25 00 49 81 c7 38 05 00 00 4c 89 f8 48 c1 e8 03 <80> 3c 18 00 74
    08 4c 89 ff e8 31 27 5f 00 4d 8b 37 e8 f9 47 12 00
    RSP: 0018:ffff88808a4a7d78 EFLAGS: 00010203
    RAX: 00000000000000a7 RBX: dffffc0000000000 RCX: ffff888088180600
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
    RBP: ffff88808a4a7d90 R08: ffffffff814fb3a8 R09: ffffed1015d66bf8
    R10: ffffed1015d66bf8 R11: 1ffff11015d66bf7 R12: 0000000000041ffc
    R13: 1ffff11011494fbc R14: 0000000000000000 R15: 000000000000053d
    FS:  00007ff07e47e700(0000) GS:ffff8880aeb00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00000000004b5100 CR3: 0000000094df2000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Call Trace:
      _do_fork+0x1b9/0x5f0 kernel/fork.c:2360
      __do_sys_clone kernel/fork.c:2454 [inline]
      __se_sys_clone kernel/fork.c:2448 [inline]
      __x64_sys_clone+0xc1/0xd0 kernel/fork.c:2448
      do_syscall_64+0xfe/0x140 arch/x86/entry/common.c:301
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    Link: https://lore.kernel.org/lkml/000000000000e0dc0d058c9e7142@google.com
    Reported-and-tested-by: syzbot+002e636502bc4b64eb5c@syzkaller.appspotmail.com
    Fixes: 6fd2fe494b17 ("copy_process(): don't use ksys_close() on cleanups")
    Cc: Jann Horn <jannh@google.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Christian Brauner <christian@brauner.io>

diff --git a/kernel/fork.c b/kernel/fork.c
index 61667909ce83..fe83343da24b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2036,6 +2036,7 @@ static __latent_entropy struct task_struct *copy_process(
 					      O_RDWR | O_CLOEXEC);
 		if (IS_ERR(pidfile)) {
 			put_unused_fd(pidfd);
+			retval = PTR_ERR(pidfile);
 			goto bad_fork_free_pid;
 		}
 		get_pid(pid);	/* held by pidfile now */

commit 1bf4580e00a248a2c86269125390eb3648e1877c
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Fri Jun 28 12:07:14 2019 -0700

    fork,memcg: alloc_thread_stack_node needs to set tsk->stack
    
    Commit 5eed6f1dff87 ("fork,memcg: fix crash in free_thread_stack on
    memcg charge fail") corrected two instances, but there was a third
    instance of this bug.
    
    Without setting tsk->stack, if memcg_charge_kernel_stack fails, it'll
    execute free_thread_stack() on a dangling pointer.
    
    Enterprise kernels are compiled with VMAP_STACK=y so this isn't
    critical, but custom VMAP_STACK=n builds should have some performance
    advantage, with the drawback of risking to fail fork because compaction
    didn't succeed.  So as long as VMAP_STACK=n is a supported option it's
    worth fixing it upstream.
    
    Link: http://lkml.kernel.org/r/20190619011450.28048-1-aarcange@redhat.com
    Fixes: 9b6f7e163cd0 ("mm: rework memcg kernel stack accounting")
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Acked-by: Roman Gushchin <guro@fb.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 399aca51ff75..61667909ce83 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -248,7 +248,11 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 	struct page *page = alloc_pages_node(node, THREADINFO_GFP,
 					     THREAD_SIZE_ORDER);
 
-	return page ? page_address(page) : NULL;
+	if (likely(page)) {
+		tsk->stack = page_address(page);
+		return tsk->stack;
+	}
+	return NULL;
 #endif
 }
 

commit b53b0b9d9a613c418057f6cb921c2f40a6f78c24
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Tue Apr 30 12:21:53 2019 -0400

    pidfd: add polling support
    
    This patch adds polling support to pidfd.
    
    Android low memory killer (LMK) needs to know when a process dies once
    it is sent the kill signal. It does so by checking for the existence of
    /proc/pid which is both racy and slow. For example, if a PID is reused
    between when LMK sends a kill signal and checks for existence of the
    PID, since the wrong PID is now possibly checked for existence.
    Using the polling support, LMK will be able to get notified when a process
    exists in race-free and fast way, and allows the LMK to do other things
    (such as by polling on other fds) while awaiting the process being killed
    to die.
    
    For notification to polling processes, we follow the same existing
    mechanism in the kernel used when the parent of the task group is to be
    notified of a child's death (do_notify_parent). This is precisely when the
    tasks waiting on a poll of pidfd are also awakened in this patch.
    
    We have decided to include the waitqueue in struct pid for the following
    reasons:
    1. The wait queue has to survive for the lifetime of the poll. Including
       it in task_struct would not be option in this case because the task can
       be reaped and destroyed before the poll returns.
    
    2. By including the struct pid for the waitqueue means that during
       de_thread(), the new thread group leader automatically gets the new
       waitqueue/pid even though its task_struct is different.
    
    Appropriate test cases are added in the second patch to provide coverage of
    all the cases the patch is handling.
    
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Daniel Colascione <dancol@google.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Tim Murray <timmurray@google.com>
    Cc: Jonathan Kowalski <bl0pbl33p@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: kernel-team@android.com
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Co-developed-by: Daniel Colascione <dancol@google.com>
    Signed-off-by: Daniel Colascione <dancol@google.com>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Christian Brauner <christian@brauner.io>

diff --git a/kernel/fork.c b/kernel/fork.c
index b4cba953040a..2cdf295b72c7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1704,8 +1704,34 @@ static void pidfd_show_fdinfo(struct seq_file *m, struct file *f)
 }
 #endif
 
+/*
+ * Poll support for process exit notification.
+ */
+static unsigned int pidfd_poll(struct file *file, struct poll_table_struct *pts)
+{
+	struct task_struct *task;
+	struct pid *pid = file->private_data;
+	int poll_flags = 0;
+
+	poll_wait(file, &pid->wait_pidfd, pts);
+
+	rcu_read_lock();
+	task = pid_task(pid, PIDTYPE_PID);
+	/*
+	 * Inform pollers only when the whole thread group exits.
+	 * If the thread group leader exits before all other threads in the
+	 * group, then poll(2) should block, similar to the wait(2) family.
+	 */
+	if (!task || (task->exit_state && thread_group_empty(task)))
+		poll_flags = POLLIN | POLLRDNORM;
+	rcu_read_unlock();
+
+	return poll_flags;
+}
+
 const struct file_operations pidfd_fops = {
 	.release = pidfd_release,
+	.poll = pidfd_poll,
 #ifdef CONFIG_PROC_FS
 	.show_fdinfo = pidfd_show_fdinfo,
 #endif

commit 6fd2fe494b17bf2dec37b610d23a43a72b16923a
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jun 26 22:22:09 2019 -0400

    copy_process(): don't use ksys_close() on cleanups
    
    anon_inode_getfd() should be used *ONLY* in situations when we are
    guaranteed to be past the last failure point (including copying the
    descriptor number to userland, at that).  And ksys_close() should
    not be used for cleanups at all.
    
    anon_inode_getfile() is there for all nontrivial cases like that.
    Just use that...
    
    Fixes: b3e583825266 ("clone: add CLONE_PIDFD")
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Reviewed-by: Jann Horn <jannh@google.com>
    Signed-off-by: Christian Brauner <christian@brauner.io>

diff --git a/kernel/fork.c b/kernel/fork.c
index 39a3adaa4ad1..399aca51ff75 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1712,31 +1712,6 @@ const struct file_operations pidfd_fops = {
 #endif
 };
 
-/**
- * pidfd_create() - Create a new pid file descriptor.
- *
- * @pid:  struct pid that the pidfd will reference
- *
- * This creates a new pid file descriptor with the O_CLOEXEC flag set.
- *
- * Note, that this function can only be called after the fd table has
- * been unshared to avoid leaking the pidfd to the new process.
- *
- * Return: On success, a cloexec pidfd is returned.
- *         On error, a negative errno number will be returned.
- */
-static int pidfd_create(struct pid *pid)
-{
-	int fd;
-
-	fd = anon_inode_getfd("[pidfd]", &pidfd_fops, get_pid(pid),
-			      O_RDWR | O_CLOEXEC);
-	if (fd < 0)
-		put_pid(pid);
-
-	return fd;
-}
-
 static void __delayed_free_task(struct rcu_head *rhp)
 {
 	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
@@ -1774,6 +1749,7 @@ static __latent_entropy struct task_struct *copy_process(
 	int pidfd = -1, retval;
 	struct task_struct *p;
 	struct multiprocess_signals delayed;
+	struct file *pidfile = NULL;
 
 	/*
 	 * Don't allow sharing the root directory with processes in a different
@@ -2046,11 +2022,20 @@ static __latent_entropy struct task_struct *copy_process(
 	 * if the fd table isn't shared).
 	 */
 	if (clone_flags & CLONE_PIDFD) {
-		retval = pidfd_create(pid);
+		retval = get_unused_fd_flags(O_RDWR | O_CLOEXEC);
 		if (retval < 0)
 			goto bad_fork_free_pid;
 
 		pidfd = retval;
+
+		pidfile = anon_inode_getfile("[pidfd]", &pidfd_fops, pid,
+					      O_RDWR | O_CLOEXEC);
+		if (IS_ERR(pidfile)) {
+			put_unused_fd(pidfd);
+			goto bad_fork_free_pid;
+		}
+		get_pid(pid);	/* held by pidfile now */
+
 		retval = put_user(pidfd, parent_tidptr);
 		if (retval)
 			goto bad_fork_put_pidfd;
@@ -2168,6 +2153,9 @@ static __latent_entropy struct task_struct *copy_process(
 		goto bad_fork_cancel_cgroup;
 	}
 
+	/* past the last point of failure */
+	if (pidfile)
+		fd_install(pidfd, pidfile);
 
 	init_task_pid_links(p);
 	if (likely(p->pid)) {
@@ -2234,8 +2222,10 @@ static __latent_entropy struct task_struct *copy_process(
 bad_fork_cgroup_threadgroup_change_end:
 	cgroup_threadgroup_change_end(current);
 bad_fork_put_pidfd:
-	if (clone_flags & CLONE_PIDFD)
-		ksys_close(pidfd);
+	if (clone_flags & CLONE_PIDFD) {
+		fput(pidfile);
+		put_unused_fd(pidfd);
+	}
 bad_fork_free_pid:
 	if (pid != &init_struct_pid)
 		free_pid(pid);

commit 9014143bab2f3bc0b9e5db3bc8d00e2a43e50fbd
Author: Dmitry V. Levin <ldv@altlinux.org>
Date:   Sun Jun 23 14:27:17 2019 +0300

    fork: don't check parent_tidptr with CLONE_PIDFD
    
    Give userspace a cheap and reliable way to tell whether CLONE_PIDFD is
    supported by the kernel or not. The easiest way is to pass an invalid
    file descriptor value in parent_tidptr, perform the syscall and verify
    that parent_tidptr has been changed to a valid file descriptor value.
    
    CLONE_PIDFD uses parent_tidptr to return pidfds. CLONE_PARENT_SETTID
    will use parent_tidptr to return the tid of the parent. The two flags
    cannot be used together. Old kernels that only support
    CLONE_PARENT_SETTID will not verify the value pointed to by
    parent_tidptr. This behavior is unchanged even with the introduction of
    CLONE_PIDFD.
    However, if CLONE_PIDFD is specified the kernel will currently check the
    value pointed to by parent_tidptr before placing the pidfd in the memory
    pointed to. EINVAL will be returned if the value in parent_tidptr is not
    0.
    
    If CLONE_PIDFD is supported and fd 0 is closed, then the returned pidfd
    can and likely will be 0 and parent_tidptr will be unchanged. This means
    userspace must either check CLONE_PIDFD support beforehand or check that
    fd 0 is not closed when invoking CLONE_PIDFD.
    
    The check for pidfd == 0 was introduced during the v5.2 merge window by
    commit b3e583825266 ("clone: add CLONE_PIDFD") to ensure that
    CLONE_PIDFD could be potentially extended by passing in flags through
    the return argument.
    
    However, that extension would look horrible, and with the upcoming
    introduction of the clone3 syscall in v5.3 there is no need to extend
    legacy clone syscall this way. (Even if it would need to be extended,
    CLONE_DETACHED can be reused with CLONE_PIDFD.)
    
    So remove the pidfd == 0 check. Userspace that needs to be portable to
    kernels without CLONE_PIDFD support can then be advised to initialize
    pidfd to -1 and check the pidfd value returned by CLONE_PIDFD.
    
    Fixes: b3e583825266 ("clone: add CLONE_PIDFD")
    Signed-off-by: Dmitry V. Levin <ldv@altlinux.org>
    Signed-off-by: Christian Brauner <christian@brauner.io>

diff --git a/kernel/fork.c b/kernel/fork.c
index 75675b9bf6df..39a3adaa4ad1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1822,8 +1822,6 @@ static __latent_entropy struct task_struct *copy_process(
 	}
 
 	if (clone_flags & CLONE_PIDFD) {
-		int reserved;
-
 		/*
 		 * - CLONE_PARENT_SETTID is useless for pidfds and also
 		 *   parent_tidptr is used to return pidfds.
@@ -1834,16 +1832,6 @@ static __latent_entropy struct task_struct *copy_process(
 		if (clone_flags &
 		    (CLONE_DETACHED | CLONE_PARENT_SETTID | CLONE_THREAD))
 			return ERR_PTR(-EINVAL);
-
-		/*
-		 * Verify that parent_tidptr is sane so we can potentially
-		 * reuse it later.
-		 */
-		if (get_user(reserved, parent_tidptr))
-			return ERR_PTR(-EFAULT);
-
-		if (reserved != 0)
-			return ERR_PTR(-EINVAL);
 	}
 
 	/*

commit 9285ec4c8b61d4930a575081abeba2cd4f449a74
Author: Jason A. Donenfeld <Jason@zx2c4.com>
Date:   Fri Jun 21 22:32:48 2019 +0200

    timekeeping: Use proper clock specifier names in functions
    
    This makes boot uniformly boottime and tai uniformly clocktai, to
    address the remaining oversights.
    
    Signed-off-by: Jason A. Donenfeld <Jason@zx2c4.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Link: https://lkml.kernel.org/r/20190621203249.3909-2-Jason@zx2c4.com

diff --git a/kernel/fork.c b/kernel/fork.c
index 75675b9bf6df..4722f1a320bf 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2139,7 +2139,7 @@ static __latent_entropy struct task_struct *copy_process(
 	 */
 
 	p->start_time = ktime_get_ns();
-	p->real_start_time = ktime_get_boot_ns();
+	p->real_start_time = ktime_get_boottime_ns();
 
 	/*
 	 * Make it visible to the rest of the system, but dont wake it up yet.

commit d68dbb0c9ac8b1ff52eb09aa58ce6358400fa939
Author: Christian Brauner <christian@brauner.io>
Date:   Fri Jun 21 01:26:35 2019 +0200

    arch: handle arches who do not yet define clone3
    
    This cleanly handles arches who do not yet define clone3.
    
    clone3() was initially placed under __ARCH_WANT_SYS_CLONE under the
    assumption that this would cleanly handle all architectures. It does
    not.
    Architectures such as nios2 or h8300 simply take the asm-generic syscall
    definitions and generate their syscall table from it. Since they don't
    define __ARCH_WANT_SYS_CLONE the build would fail complaining about
    sys_clone3 missing. The reason this doesn't happen for legacy clone is
    that nios2 and h8300 provide assembly stubs for sys_clone. This seems to
    be done for architectural reasons.
    
    The build failures for nios2 and h8300 were caught int -next luckily.
    The solution is to define __ARCH_WANT_SYS_CLONE3 that architectures can
    add. Additionally, we need a cond_syscall(clone3) for architectures such
    as nios2 or h8300 that generate their syscall table in the way I
    explained above.
    
    Fixes: 8f3220a80654 ("arch: wire-up clone3() syscall")
    Signed-off-by: Christian Brauner <christian@brauner.io>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Adrian Reber <adrian@lisas.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: linux-api@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: x86@kernel.org

diff --git a/kernel/fork.c b/kernel/fork.c
index 08ff131f26b4..98abea995629 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2490,7 +2490,9 @@ SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 
 	return _do_fork(&args);
 }
+#endif
 
+#ifdef __ARCH_WANT_SYS_CLONE3
 noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
 					      struct clone_args __user *uargs,
 					      size_t size)

commit c8a53b2db0aec40d8b217936e1b7f3d840c50390
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Thu May 23 10:36:46 2019 -0300

    mm/hmm: Hold a mmgrab from hmm to mm
    
    So long as a struct hmm pointer exists, so should the struct mm it is
    linked too. Hold the mmgrab() as soon as a hmm is created, and mmdrop() it
    once the hmm refcount goes to zero.
    
    Since mmdrop() (ie a 0 kref on struct mm) is now impossible with a !NULL
    mm->hmm delete the hmm_hmm_destroy().
    
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>
    Reviewed-by: Jérôme Glisse <jglisse@redhat.com>
    Reviewed-by: John Hubbard <jhubbard@nvidia.com>
    Reviewed-by: Ralph Campbell <rcampbell@nvidia.com>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Philip Yang <Philip.Yang@amd.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 75675b9bf6df..c704c3cedee7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -673,7 +673,6 @@ void __mmdrop(struct mm_struct *mm)
 	WARN_ON_ONCE(mm == current->active_mm);
 	mm_free_pgd(mm);
 	destroy_context(mm);
-	hmm_mm_destroy(mm);
 	mmu_notifier_mm_destroy(mm);
 	check_mm(mm);
 	put_user_ns(mm->user_ns);

commit 7f192e3cd316ba58c88dfa26796cf77789dd9872
Author: Christian Brauner <christian@brauner.io>
Date:   Sat May 25 11:36:41 2019 +0200

    fork: add clone3
    
    This adds the clone3 system call.
    
    As mentioned several times already (cf. [7], [8]) here's the promised
    patchset for clone3().
    
    We recently merged the CLONE_PIDFD patchset (cf. [1]). It took the last
    free flag from clone().
    
    Independent of the CLONE_PIDFD patchset a time namespace has been discussed
    at Linux Plumber Conference last year and has been sent out and reviewed
    (cf. [5]). It is expected that it will go upstream in the not too distant
    future. However, it relies on the addition of the CLONE_NEWTIME flag to
    clone(). The only other good candidate - CLONE_DETACHED - is currently not
    recyclable as we have identified at least two large or widely used
    codebases that currently pass this flag (cf. [2], [3], and [4]). Given that
    CLONE_PIDFD grabbed the last clone() flag the time namespace is effectively
    blocked. clone3() has the advantage that it will unblock this patchset
    again. In general, clone3() is extensible and allows for the implementation
    of new features.
    
    The idea is to keep clone3() very simple and close to the original clone(),
    specifically, to keep on supporting old clone()-based workloads.
    We know there have been various creative proposals how a new process
    creation syscall or even api is supposed to look like. Some people even
    going so far as to argue that the traditional fork()+exec() split should be
    abandoned in favor of an in-kernel version of spawn(). Independent of
    whether or not we personally think spawn() is a good idea this patchset has
    and does not want to have anything to do with this.
    One stance we take is that there's no real good alternative to
    clone()+exec() and we need and want to support this model going forward;
    independent of spawn().
    The following requirements guided clone3():
    - bump the number of available flags
    - move arguments that are currently passed as separate arguments
      in clone() into a dedicated struct clone_args
      - choose a struct layout that is easy to handle on 32 and on 64 bit
      - choose a struct layout that is extensible
      - give new flags that currently need to abuse another flag's dedicated
        return argument in clone() their own dedicated return argument
        (e.g. CLONE_PIDFD)
      - use a separate kernel internal struct kernel_clone_args that is
        properly typed according to current kernel conventions in fork.c and is
        different from  the uapi struct clone_args
    - port _do_fork() to use kernel_clone_args so that all process creation
      syscalls such as fork(), vfork(), clone(), and clone3() behave identical
      (Arnd suggested, that we can probably also port do_fork() itself in a
       separate patchset.)
    - ease of transition for userspace from clone() to clone3()
      This very much means that we do *not* remove functionality that userspace
      currently relies on as the latter is a good way of creating a syscall
      that won't be adopted.
    - do not try to be clever or complex: keep clone3() as dumb as possible
    
    In accordance with Linus suggestions (cf. [11]), clone3() has the following
    signature:
    
    /* uapi */
    struct clone_args {
            __aligned_u64 flags;
            __aligned_u64 pidfd;
            __aligned_u64 child_tid;
            __aligned_u64 parent_tid;
            __aligned_u64 exit_signal;
            __aligned_u64 stack;
            __aligned_u64 stack_size;
            __aligned_u64 tls;
    };
    
    /* kernel internal */
    struct kernel_clone_args {
            u64 flags;
            int __user *pidfd;
            int __user *child_tid;
            int __user *parent_tid;
            int exit_signal;
            unsigned long stack;
            unsigned long stack_size;
            unsigned long tls;
    };
    
    long sys_clone3(struct clone_args __user *uargs, size_t size)
    
    clone3() cleanly supports all of the supported flags from clone() and thus
    all legacy workloads.
    The advantage of sticking close to the old clone() is the low cost for
    userspace to switch to this new api. Quite a lot of userspace apis (e.g.
    pthreads) are based on the clone() syscall. With the new clone3() syscall
    supporting all of the old workloads and opening up the ability to add new
    features should make switching to it for userspace more appealing. In
    essence, glibc can just write a simple wrapper to switch from clone() to
    clone3().
    
    There has been some interest in this patchset already. We have received a
    patch from the CRIU corner for clone3() that would set the PID/TID of a
    restored process without /proc/sys/kernel/ns_last_pid to eliminate a race.
    
    /* User visible differences to legacy clone() */
    - CLONE_DETACHED will cause EINVAL with clone3()
    - CSIGNAL is deprecated
      It is superseeded by a dedicated "exit_signal" argument in struct
      clone_args freeing up space for additional flags.
      This is based on a suggestion from Andrei and Linus (cf. [9] and [10])
    
    /* References */
    [1]: b3e5838252665ee4cfa76b82bdf1198dca81e5be
    [2]: https://dxr.mozilla.org/mozilla-central/source/security/sandbox/linux/SandboxFilter.cpp#343
    [3]: https://git.musl-libc.org/cgit/musl/tree/src/thread/pthread_create.c#n233
    [4]: https://sources.debian.org/src/blcr/0.8.5-2.3/cr_module/cr_dump_self.c/?hl=740#L740
    [5]: https://lore.kernel.org/lkml/20190425161416.26600-1-dima@arista.com/
    [6]: https://lore.kernel.org/lkml/20190425161416.26600-2-dima@arista.com/
    [7]: https://lore.kernel.org/lkml/CAHrFyr5HxpGXA2YrKza-oB-GGwJCqwPfyhD-Y5wbktWZdt0sGQ@mail.gmail.com/
    [8]: https://lore.kernel.org/lkml/20190524102756.qjsjxukuq2f4t6bo@brauner.io/
    [9]: https://lore.kernel.org/lkml/20190529222414.GA6492@gmail.com/
    [10]: https://lore.kernel.org/lkml/CAHk-=whQP-Ykxi=zSYaV9iXsHsENa+2fdj-zYKwyeyed63Lsfw@mail.gmail.com/
    [11]: https://lore.kernel.org/lkml/CAHk-=wieuV4hGwznPsX-8E0G2FKhx3NjZ9X3dTKh5zKd+iqOBw@mail.gmail.com/
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Christian Brauner <christian@brauner.io>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Serge Hallyn <serge@hallyn.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Pavel Emelyanov <xemul@virtuozzo.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Adrian Reber <adrian@lisas.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrei Vagin <avagin@gmail.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: linux-api@vger.kernel.org

diff --git a/kernel/fork.c b/kernel/fork.c
index b4cba953040a..08ff131f26b4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1760,19 +1760,15 @@ static __always_inline void delayed_free_task(struct task_struct *tsk)
  * flags). The actual kick-off is left to the caller.
  */
 static __latent_entropy struct task_struct *copy_process(
-					unsigned long clone_flags,
-					unsigned long stack_start,
-					unsigned long stack_size,
-					int __user *parent_tidptr,
-					int __user *child_tidptr,
 					struct pid *pid,
 					int trace,
-					unsigned long tls,
-					int node)
+					int node,
+					struct kernel_clone_args *args)
 {
 	int pidfd = -1, retval;
 	struct task_struct *p;
 	struct multiprocess_signals delayed;
+	u64 clone_flags = args->flags;
 
 	/*
 	 * Don't allow sharing the root directory with processes in a different
@@ -1821,27 +1817,12 @@ static __latent_entropy struct task_struct *copy_process(
 	}
 
 	if (clone_flags & CLONE_PIDFD) {
-		int reserved;
-
 		/*
-		 * - CLONE_PARENT_SETTID is useless for pidfds and also
-		 *   parent_tidptr is used to return pidfds.
 		 * - CLONE_DETACHED is blocked so that we can potentially
 		 *   reuse it later for CLONE_PIDFD.
 		 * - CLONE_THREAD is blocked until someone really needs it.
 		 */
-		if (clone_flags &
-		    (CLONE_DETACHED | CLONE_PARENT_SETTID | CLONE_THREAD))
-			return ERR_PTR(-EINVAL);
-
-		/*
-		 * Verify that parent_tidptr is sane so we can potentially
-		 * reuse it later.
-		 */
-		if (get_user(reserved, parent_tidptr))
-			return ERR_PTR(-EFAULT);
-
-		if (reserved != 0)
+		if (clone_flags & (CLONE_DETACHED | CLONE_THREAD))
 			return ERR_PTR(-EINVAL);
 	}
 
@@ -1874,11 +1855,11 @@ static __latent_entropy struct task_struct *copy_process(
 	 * p->set_child_tid which is (ab)used as a kthread's data pointer for
 	 * kernel threads (PF_KTHREAD).
 	 */
-	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
+	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? args->child_tid : NULL;
 	/*
 	 * Clear TID on mm_release()?
 	 */
-	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;
+	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? args->child_tid : NULL;
 
 	ftrace_graph_init_task(p);
 
@@ -2037,7 +2018,8 @@ static __latent_entropy struct task_struct *copy_process(
 	retval = copy_io(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_namespaces;
-	retval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);
+	retval = copy_thread_tls(clone_flags, args->stack, args->stack_size, p,
+				 args->tls);
 	if (retval)
 		goto bad_fork_cleanup_io;
 
@@ -2062,7 +2044,7 @@ static __latent_entropy struct task_struct *copy_process(
 			goto bad_fork_free_pid;
 
 		pidfd = retval;
-		retval = put_user(pidfd, parent_tidptr);
+		retval = put_user(pidfd, args->pidfd);
 		if (retval)
 			goto bad_fork_put_pidfd;
 	}
@@ -2105,7 +2087,7 @@ static __latent_entropy struct task_struct *copy_process(
 		if (clone_flags & CLONE_PARENT)
 			p->exit_signal = current->group_leader->exit_signal;
 		else
-			p->exit_signal = (clone_flags & CSIGNAL);
+			p->exit_signal = args->exit_signal;
 		p->group_leader = p;
 		p->tgid = p->pid;
 	}
@@ -2313,8 +2295,11 @@ static inline void init_idle_pids(struct task_struct *idle)
 struct task_struct *fork_idle(int cpu)
 {
 	struct task_struct *task;
-	task = copy_process(CLONE_VM, 0, 0, NULL, NULL, &init_struct_pid, 0, 0,
-			    cpu_to_node(cpu));
+	struct kernel_clone_args args = {
+		.flags = CLONE_VM,
+	};
+
+	task = copy_process(&init_struct_pid, 0, cpu_to_node(cpu), &args);
 	if (!IS_ERR(task)) {
 		init_idle_pids(task);
 		init_idle(task, cpu);
@@ -2334,13 +2319,9 @@ struct mm_struct *copy_init_mm(void)
  * It copies the process, and if successful kick-starts
  * it and waits for it to finish using the VM if required.
  */
-long _do_fork(unsigned long clone_flags,
-	      unsigned long stack_start,
-	      unsigned long stack_size,
-	      int __user *parent_tidptr,
-	      int __user *child_tidptr,
-	      unsigned long tls)
+long _do_fork(struct kernel_clone_args *args)
 {
+	u64 clone_flags = args->flags;
 	struct completion vfork;
 	struct pid *pid;
 	struct task_struct *p;
@@ -2356,7 +2337,7 @@ long _do_fork(unsigned long clone_flags,
 	if (!(clone_flags & CLONE_UNTRACED)) {
 		if (clone_flags & CLONE_VFORK)
 			trace = PTRACE_EVENT_VFORK;
-		else if ((clone_flags & CSIGNAL) != SIGCHLD)
+		else if (args->exit_signal != SIGCHLD)
 			trace = PTRACE_EVENT_CLONE;
 		else
 			trace = PTRACE_EVENT_FORK;
@@ -2365,8 +2346,7 @@ long _do_fork(unsigned long clone_flags,
 			trace = 0;
 	}
 
-	p = copy_process(clone_flags, stack_start, stack_size, parent_tidptr,
-			 child_tidptr, NULL, trace, tls, NUMA_NO_NODE);
+	p = copy_process(NULL, trace, NUMA_NO_NODE, args);
 	add_latent_entropy();
 
 	if (IS_ERR(p))
@@ -2382,7 +2362,7 @@ long _do_fork(unsigned long clone_flags,
 	nr = pid_vnr(pid);
 
 	if (clone_flags & CLONE_PARENT_SETTID)
-		put_user(nr, parent_tidptr);
+		put_user(nr, args->parent_tid);
 
 	if (clone_flags & CLONE_VFORK) {
 		p->vfork_done = &vfork;
@@ -2414,8 +2394,16 @@ long do_fork(unsigned long clone_flags,
 	      int __user *parent_tidptr,
 	      int __user *child_tidptr)
 {
-	return _do_fork(clone_flags, stack_start, stack_size,
-			parent_tidptr, child_tidptr, 0);
+	struct kernel_clone_args args = {
+		.flags		= (clone_flags & ~CSIGNAL),
+		.child_tid	= child_tidptr,
+		.parent_tid	= parent_tidptr,
+		.exit_signal	= (clone_flags & CSIGNAL),
+		.stack		= stack_start,
+		.stack_size	= stack_size,
+	};
+
+	return _do_fork(&args);
 }
 #endif
 
@@ -2424,15 +2412,25 @@ long do_fork(unsigned long clone_flags,
  */
 pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 {
-	return _do_fork(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn,
-		(unsigned long)arg, NULL, NULL, 0);
+	struct kernel_clone_args args = {
+		.flags		= ((flags | CLONE_VM | CLONE_UNTRACED) & ~CSIGNAL),
+		.exit_signal	= (flags & CSIGNAL),
+		.stack		= (unsigned long)fn,
+		.stack_size	= (unsigned long)arg,
+	};
+
+	return _do_fork(&args);
 }
 
 #ifdef __ARCH_WANT_SYS_FORK
 SYSCALL_DEFINE0(fork)
 {
 #ifdef CONFIG_MMU
-	return _do_fork(SIGCHLD, 0, 0, NULL, NULL, 0);
+	struct kernel_clone_args args = {
+		.exit_signal = SIGCHLD,
+	};
+
+	return _do_fork(&args);
 #else
 	/* can not support in nommu mode */
 	return -EINVAL;
@@ -2443,8 +2441,12 @@ SYSCALL_DEFINE0(fork)
 #ifdef __ARCH_WANT_SYS_VFORK
 SYSCALL_DEFINE0(vfork)
 {
-	return _do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0,
-			0, NULL, NULL, 0);
+	struct kernel_clone_args args = {
+		.flags		= CLONE_VFORK | CLONE_VM,
+		.exit_signal	= SIGCHLD,
+	};
+
+	return _do_fork(&args);
 }
 #endif
 
@@ -2472,7 +2474,110 @@ SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 		 unsigned long, tls)
 #endif
 {
-	return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls);
+	struct kernel_clone_args args = {
+		.flags		= (clone_flags & ~CSIGNAL),
+		.pidfd		= parent_tidptr,
+		.child_tid	= child_tidptr,
+		.parent_tid	= parent_tidptr,
+		.exit_signal	= (clone_flags & CSIGNAL),
+		.stack		= newsp,
+		.tls		= tls,
+	};
+
+	/* clone(CLONE_PIDFD) uses parent_tidptr to return a pidfd */
+	if ((clone_flags & CLONE_PIDFD) && (clone_flags & CLONE_PARENT_SETTID))
+		return -EINVAL;
+
+	return _do_fork(&args);
+}
+
+noinline static int copy_clone_args_from_user(struct kernel_clone_args *kargs,
+					      struct clone_args __user *uargs,
+					      size_t size)
+{
+	struct clone_args args;
+
+	if (unlikely(size > PAGE_SIZE))
+		return -E2BIG;
+
+	if (unlikely(size < sizeof(struct clone_args)))
+		return -EINVAL;
+
+	if (unlikely(!access_ok(uargs, size)))
+		return -EFAULT;
+
+	if (size > sizeof(struct clone_args)) {
+		unsigned char __user *addr;
+		unsigned char __user *end;
+		unsigned char val;
+
+		addr = (void __user *)uargs + sizeof(struct clone_args);
+		end = (void __user *)uargs + size;
+
+		for (; addr < end; addr++) {
+			if (get_user(val, addr))
+				return -EFAULT;
+			if (val)
+				return -E2BIG;
+		}
+
+		size = sizeof(struct clone_args);
+	}
+
+	if (copy_from_user(&args, uargs, size))
+		return -EFAULT;
+
+	*kargs = (struct kernel_clone_args){
+		.flags		= args.flags,
+		.pidfd		= u64_to_user_ptr(args.pidfd),
+		.child_tid	= u64_to_user_ptr(args.child_tid),
+		.parent_tid	= u64_to_user_ptr(args.parent_tid),
+		.exit_signal	= args.exit_signal,
+		.stack		= args.stack,
+		.stack_size	= args.stack_size,
+		.tls		= args.tls,
+	};
+
+	return 0;
+}
+
+static bool clone3_args_valid(const struct kernel_clone_args *kargs)
+{
+	/*
+	 * All lower bits of the flag word are taken.
+	 * Verify that no other unknown flags are passed along.
+	 */
+	if (kargs->flags & ~CLONE_LEGACY_FLAGS)
+		return false;
+
+	/*
+	 * - make the CLONE_DETACHED bit reuseable for clone3
+	 * - make the CSIGNAL bits reuseable for clone3
+	 */
+	if (kargs->flags & (CLONE_DETACHED | CSIGNAL))
+		return false;
+
+	if ((kargs->flags & (CLONE_THREAD | CLONE_PARENT)) &&
+	    kargs->exit_signal)
+		return false;
+
+	return true;
+}
+
+SYSCALL_DEFINE2(clone3, struct clone_args __user *, uargs, size_t, size)
+{
+	int err;
+
+	struct kernel_clone_args kargs;
+
+	err = copy_clone_args_from_user(&kargs, uargs, size);
+	if (err)
+		return err;
+
+	if (!clone3_args_valid(&kargs))
+		return -EINVAL;
+
+	return _do_fork(&kargs);
 }
 #endif
 

commit e196e479a3b844da6e6e71e0d2a8694040cb4e52
Author: Yuyang Du <duyuyang@gmail.com>
Date:   Mon May 6 16:19:23 2019 +0800

    locking/lockdep: Use lockdep_init_task for task initiation consistently
    
    Despite that there is a lockdep_init_task() which does nothing, lockdep
    initiates tasks by assigning lockdep fields and does so inconsistently. Fix
    this by using lockdep_init_task().
    
    Signed-off-by: Yuyang Du <duyuyang@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bvanassche@acm.org
    Cc: frederic@kernel.org
    Cc: ming.lei@redhat.com
    Cc: will.deacon@arm.com
    Link: https://lkml.kernel.org/r/20190506081939.74287-8-duyuyang@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 75675b9bf6df..735d0b4a89e2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1984,9 +1984,6 @@ static __latent_entropy struct task_struct *copy_process(
 	p->pagefault_disabled = 0;
 
 #ifdef CONFIG_LOCKDEP
-	p->lockdep_depth = 0; /* no locks held yet */
-	p->curr_chain_key = 0;
-	p->lockdep_recursion = 0;
 	lockdep_init_task(p);
 #endif
 

commit 3bd3706251ee8ab67e69d9340ac2abdca217e733
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Apr 23 16:26:36 2019 +0200

    sched/core: Provide a pointer to the valid CPU mask
    
    In commit:
    
      4b53a3412d66 ("sched/core: Remove the tsk_nr_cpus_allowed() wrapper")
    
    the tsk_nr_cpus_allowed() wrapper was removed. There was not
    much difference in !RT but in RT we used this to implement
    migrate_disable(). Within a migrate_disable() section the CPU mask is
    restricted to single CPU while the "normal" CPU mask remains untouched.
    
    As an alternative implementation Ingo suggested to use:
    
            struct task_struct {
                    const cpumask_t         *cpus_ptr;
                    cpumask_t               cpus_mask;
            };
    with
            t->cpus_ptr = &t->cpus_mask;
    
    In -RT we then can switch the cpus_ptr to:
    
            t->cpus_ptr = &cpumask_of(task_cpu(p));
    
    in a migration disabled region. The rules are simple:
    
     - Code that 'uses' ->cpus_allowed would use the pointer.
     - Code that 'modifies' ->cpus_allowed would use the direct mask.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20190423142636.14347-1-bigeasy@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 75675b9bf6df..6be686283e55 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -894,6 +894,8 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 #ifdef CONFIG_STACKPROTECTOR
 	tsk->stack_canary = get_random_canary();
 #endif
+	if (orig->cpus_ptr == &orig->cpus_mask)
+		tsk->cpus_ptr = &tsk->cpus_mask;
 
 	/*
 	 * One for us, one for whoever does the "release_task()" (usually

commit 8856ae4df3e9b5295ea2da7ad3b00796386454ec
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Fri May 31 22:30:12 2019 -0700

    kernel/fork.c: make max_threads symbol static
    
    Fix build warning,
    kernel/fork.c:125:5: warning: symbol 'max_threads' was not declared. Should it be static?
    
    Link: http://lkml.kernel.org/r/20190516015118.140561-1-wangkefeng.wang@huawei.com
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Reported-by: Hulk Robot <hulkci@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b2b87d450b80..75675b9bf6df 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -123,7 +123,7 @@
 unsigned long total_forks;	/* Handle normal Linux uptimes. */
 int nr_threads;			/* The idle threads do not count.. */
 
-int max_threads;		/* tunable limit on nr_threads */
+static int max_threads;		/* tunable limit on nr_threads */
 
 DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b4cba953040a..b2b87d450b80 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/kernel/fork.c
  *

commit e02c9b0d65a7493180db45320f82482c6ba8ea57
Author: Lin Feng <linf@wangsu.com>
Date:   Tue May 14 15:42:34 2019 -0700

    kernel/latencytop.c: rename clear_all_latency_tracing to clear_tsk_latency_tracing
    
    The name clear_all_latency_tracing is misleading, in fact which only
    clear per task's latency_record[], and we do have another function named
    clear_global_latency_tracing which clear the global latency_record[]
    buffer.
    
    Link: http://lkml.kernel.org/r/20190226114602.16902-1-linf@wangsu.com
    Signed-off-by: Lin Feng <linf@wangsu.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Fabian Frederick <fabf@skynet.be>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b409e792aadc..b4cba953040a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2093,7 +2093,7 @@ static __latent_entropy struct task_struct *copy_process(
 #ifdef TIF_SYSCALL_EMU
 	clear_tsk_thread_flag(p, TIF_SYSCALL_EMU);
 #endif
-	clear_all_latency_tracing(p);
+	clear_tsk_latency_tracing(p);
 
 	/* ok, now we should be set up.. */
 	p->pid = pid_nr(pid);

commit c3f3ce049f7d97cc7ec9c01cb51d9ec74e0f37c2
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Tue May 14 15:40:46 2019 -0700

    userfaultfd: use RCU to free the task struct when fork fails
    
    The task structure is freed while get_mem_cgroup_from_mm() holds
    rcu_read_lock() and dereferences mm->owner.
    
      get_mem_cgroup_from_mm()                failing fork()
      ----                                    ---
      task = mm->owner
                                              mm->owner = NULL;
                                              free(task)
      if (task) *task; /* use after free */
    
    The fix consists in freeing the task with RCU also in the fork failure
    case, exactly like it always happens for the regular exit(2) path.  That
    is enough to make the rcu_read_lock hold in get_mem_cgroup_from_mm()
    (left side above) effective to avoid a use after free when dereferencing
    the task structure.
    
    An alternate possible fix would be to defer the delivery of the
    userfaultfd contexts to the monitor until after fork() is guaranteed to
    succeed.  Such a change would require more changes because it would
    create a strict ordering dependency where the uffd methods would need to
    be called beyond the last potentially failing branch in order to be
    safe.  This solution as opposed only adds the dependency to common code
    to set mm->owner to NULL and to free the task struct that was pointed by
    mm->owner with RCU, if fork ends up failing.  The userfaultfd methods
    can still be called anywhere during the fork runtime and the monitor
    will keep discarding orphaned "mm" coming from failed forks in userland.
    
    This race condition couldn't trigger if CONFIG_MEMCG was set =n at build
    time.
    
    [aarcange@redhat.com: improve changelog, reduce #ifdefs per Michal]
      Link: http://lkml.kernel.org/r/20190429035752.4508-1-aarcange@redhat.com
    Link: http://lkml.kernel.org/r/20190325225636.11635-2-aarcange@redhat.com
    Fixes: 893e26e61d04 ("userfaultfd: non-cooperative: Add fork() event")
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Tested-by: zhong jiang <zhongjiang@huawei.com>
    Reported-by: syzbot+cbb52e396df3e565ab02@syzkaller.appspotmail.com
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: zhong jiang <zhongjiang@huawei.com>
    Cc: syzbot+cbb52e396df3e565ab02@syzkaller.appspotmail.com
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 737db1828437..b409e792aadc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -955,6 +955,15 @@ static void mm_init_aio(struct mm_struct *mm)
 #endif
 }
 
+static __always_inline void mm_clear_owner(struct mm_struct *mm,
+					   struct task_struct *p)
+{
+#ifdef CONFIG_MEMCG
+	if (mm->owner == p)
+		WRITE_ONCE(mm->owner, NULL);
+#endif
+}
+
 static void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
 {
 #ifdef CONFIG_MEMCG
@@ -1343,6 +1352,7 @@ static struct mm_struct *dup_mm(struct task_struct *tsk,
 free_pt:
 	/* don't put binfmt in mmput, we haven't got module yet */
 	mm->binfmt = NULL;
+	mm_init_owner(mm, NULL);
 	mmput(mm);
 
 fail_nomem:
@@ -1726,6 +1736,21 @@ static int pidfd_create(struct pid *pid)
 	return fd;
 }
 
+static void __delayed_free_task(struct rcu_head *rhp)
+{
+	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
+
+	free_task(tsk);
+}
+
+static __always_inline void delayed_free_task(struct task_struct *tsk)
+{
+	if (IS_ENABLED(CONFIG_MEMCG))
+		call_rcu(&tsk->rcu, __delayed_free_task);
+	else
+		free_task(tsk);
+}
+
 /*
  * This creates a new process as a copy of the old one,
  * but does not actually start it yet.
@@ -2233,8 +2258,10 @@ static __latent_entropy struct task_struct *copy_process(
 bad_fork_cleanup_namespaces:
 	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
-	if (p->mm)
+	if (p->mm) {
+		mm_clear_owner(p->mm, p);
 		mmput(p->mm);
+	}
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
 		free_signal_struct(p->signal);
@@ -2265,7 +2292,7 @@ static __latent_entropy struct task_struct *copy_process(
 bad_fork_free:
 	p->state = TASK_DEAD;
 	put_task_stack(p);
-	free_task(p);
+	delayed_free_task(p);
 fork_out:
 	spin_lock_irq(&current->sighand->siglock);
 	hlist_del_init(&delayed.node);

commit c3b7112df86b769927a60a6d7175988ca3d60f09
Author: Christian Brauner <christian@brauner.io>
Date:   Fri May 10 11:53:46 2019 +0200

    fork: do not release lock that wasn't taken
    
    Avoid calling cgroup_threadgroup_change_end() without having called
    cgroup_threadgroup_change_begin() first.
    
    During process creation we need to check whether the cgroup we are in
    allows us to fork. To perform this check the cgroup needs to guard itself
    against threadgroup changes and takes a lock.
    Prior to CLONE_PIDFD the cleanup target "bad_fork_free_pid" would also need
    to call cgroup_threadgroup_change_end() because said lock had already been
    taken.
    However, this is not the case anymore with the addition of CLONE_PIDFD. We
    are now allocating a pidfd before we check whether the cgroup we're in can
    fork and thus prior to taking the lock. So when copy_process() fails at the
    right step it would release a lock we haven't taken.
    This bug is not even very subtle to be honest. It's just not very clear
    from the naming of cgroup_threadgroup_change_{begin,end}() that a lock is
    taken.
    
    Here's the relevant splat:
    
    entry_SYSENTER_compat+0x70/0x7f arch/x86/entry/entry_64_compat.S:139
    RIP: 0023:0xf7fec849
    Code: 85 d2 74 02 89 0a 5b 5d c3 8b 04 24 c3 8b 14 24 c3 8b 3c 24 c3 90 90
    90 90 90 90 90 90 90 90 90 90 51 52 55 89 e5 0f 34 cd 80 <5d> 5a 59 c3 90
    90 90 90 eb 0d 90 90 90 90 90 90 90 90 90 90 90 90
    RSP: 002b:00000000ffed5a8c EFLAGS: 00000246 ORIG_RAX: 0000000000000078
    RAX: ffffffffffffffda RBX: 0000000000003ffc RCX: 0000000000000000
    RDX: 00000000200005c0 RSI: 0000000000000000 RDI: 0000000000000000
    RBP: 0000000000000012 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000
    R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
    ------------[ cut here ]------------
    DEBUG_LOCKS_WARN_ON(depth <= 0)
    WARNING: CPU: 1 PID: 7744 at kernel/locking/lockdep.c:4052 __lock_release
    kernel/locking/lockdep.c:4052 [inline]
    WARNING: CPU: 1 PID: 7744 at kernel/locking/lockdep.c:4052
    lock_release+0x667/0xa00 kernel/locking/lockdep.c:4321
    Kernel panic - not syncing: panic_on_warn set ...
    CPU: 1 PID: 7744 Comm: syz-executor007 Not tainted 5.1.0+ #4
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS
    Google 01/01/2011
    Call Trace:
      __dump_stack lib/dump_stack.c:77 [inline]
      dump_stack+0x172/0x1f0 lib/dump_stack.c:113
      panic+0x2cb/0x65c kernel/panic.c:214
      __warn.cold+0x20/0x45 kernel/panic.c:566
      report_bug+0x263/0x2b0 lib/bug.c:186
      fixup_bug arch/x86/kernel/traps.c:179 [inline]
      fixup_bug arch/x86/kernel/traps.c:174 [inline]
      do_error_trap+0x11b/0x200 arch/x86/kernel/traps.c:272
      do_invalid_op+0x37/0x50 arch/x86/kernel/traps.c:291
      invalid_op+0x14/0x20 arch/x86/entry/entry_64.S:972
    RIP: 0010:__lock_release kernel/locking/lockdep.c:4052 [inline]
    RIP: 0010:lock_release+0x667/0xa00 kernel/locking/lockdep.c:4321
    Code: 0f 85 a0 03 00 00 8b 35 77 66 08 08 85 f6 75 23 48 c7 c6 a0 55 6b 87
    48 c7 c7 40 25 6b 87 4c 89 85 70 ff ff ff e8 b7 a9 eb ff <0f> 0b 4c 8b 85
    70 ff ff ff 4c 89 ea 4c 89 e6 4c 89 c7 e8 52 63 ff
    RSP: 0018:ffff888094117b48 EFLAGS: 00010086
    RAX: 0000000000000000 RBX: 1ffff11012822f6f RCX: 0000000000000000
    RDX: 0000000000000000 RSI: ffffffff815af236 RDI: ffffed1012822f5b
    RBP: ffff888094117c00 R08: ffff888092bfc400 R09: fffffbfff113301d
    R10: fffffbfff113301c R11: ffffffff889980e3 R12: ffffffff8a451df8
    R13: ffffffff8142e71f R14: ffffffff8a44cc80 R15: ffff888094117bd8
      percpu_up_read.constprop.0+0xcb/0x110 include/linux/percpu-rwsem.h:92
      cgroup_threadgroup_change_end include/linux/cgroup-defs.h:712 [inline]
      copy_process.part.0+0x47ff/0x6710 kernel/fork.c:2222
      copy_process kernel/fork.c:1772 [inline]
      _do_fork+0x25d/0xfd0 kernel/fork.c:2338
      __do_compat_sys_x86_clone arch/x86/ia32/sys_ia32.c:240 [inline]
      __se_compat_sys_x86_clone arch/x86/ia32/sys_ia32.c:236 [inline]
      __ia32_compat_sys_x86_clone+0xbc/0x140 arch/x86/ia32/sys_ia32.c:236
      do_syscall_32_irqs_on arch/x86/entry/common.c:334 [inline]
      do_fast_syscall_32+0x281/0xd54 arch/x86/entry/common.c:405
      entry_SYSENTER_compat+0x70/0x7f arch/x86/entry/entry_64_compat.S:139
    RIP: 0023:0xf7fec849
    Code: 85 d2 74 02 89 0a 5b 5d c3 8b 04 24 c3 8b 14 24 c3 8b 3c 24 c3 90 90
    90 90 90 90 90 90 90 90 90 90 51 52 55 89 e5 0f 34 cd 80 <5d> 5a 59 c3 90
    90 90 90 eb 0d 90 90 90 90 90 90 90 90 90 90 90 90
    RSP: 002b:00000000ffed5a8c EFLAGS: 00000246 ORIG_RAX: 0000000000000078
    RAX: ffffffffffffffda RBX: 0000000000003ffc RCX: 0000000000000000
    RDX: 00000000200005c0 RSI: 0000000000000000 RDI: 0000000000000000
    RBP: 0000000000000012 R08: 0000000000000000 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000
    R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
    Kernel Offset: disabled
    Rebooting in 86400 seconds..
    
    Reported-and-tested-by: syzbot+3286e58549edc479faae@syzkaller.appspotmail.com
    Fixes: b3e583825266 ("clone: add CLONE_PIDFD")
    Signed-off-by: Christian Brauner <christian@brauner.io>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5359facf9867..737db1828437 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2102,7 +2102,7 @@ static __latent_entropy struct task_struct *copy_process(
 	 */
 	retval = cgroup_can_fork(p);
 	if (retval)
-		goto bad_fork_put_pidfd;
+		goto bad_fork_cgroup_threadgroup_change_end;
 
 	/*
 	 * From this point on we must avoid any synchronous user-space
@@ -2217,11 +2217,12 @@ static __latent_entropy struct task_struct *copy_process(
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
 	cgroup_cancel_fork(p);
+bad_fork_cgroup_threadgroup_change_end:
+	cgroup_threadgroup_change_end(current);
 bad_fork_put_pidfd:
 	if (clone_flags & CLONE_PIDFD)
 		ksys_close(pidfd);
 bad_fork_free_pid:
-	cgroup_threadgroup_change_end(current);
 	if (pid != &init_struct_pid)
 		free_pid(pid);
 bad_fork_cleanup_thread:

commit abde77eb5c66b2f98539c4644b54f34b7e179e6b
Merge: 23c970608a09 f2b31bb59824
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 9 13:52:12 2019 -0700

    Merge branch 'for-5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "This includes Roman's cgroup2 freezer implementation.
    
      It's a separate machanism from cgroup1 freezer. Instead of blocking
      user tasks in arbitrary uninterruptible sleeps, the new implementation
      extends jobctl stop - frozen tasks are trapped in jobctl stop until
      thawed and can be killed and ptraced. Lots of thanks to Oleg for
      sheperding the effort.
    
      Other than that, there are a few trivial changes"
    
    * 'for-5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: never call do_group_exit() with task->frozen bit set
      kernel: cgroup: fix misuse of %x
      cgroup: get rid of cgroup_freezer_frozen_exit()
      cgroup: prevent spurious transition into non-frozen state
      cgroup: Remove unused cgrp variable
      cgroup: document cgroup v2 freezer interface
      cgroup: add tracing points for cgroup v2 freezer
      cgroup: make TRACE_CGROUP_PATH irq-safe
      kselftests: cgroup: add freezer controller self-tests
      kselftests: cgroup: don't fail on cg_kill_all() error in cg_destroy()
      cgroup: cgroup v2 freezer
      cgroup: protect cgroup->nr_(dying_)descendants by css_set_lock
      cgroup: implement __cgroup_task_count() helper
      cgroup: rename freezer.c into legacy_freezer.c
      cgroup: remove extra cgroup_migrate_finish() call

commit eac7078a0fff1e72cf2b641721e3f55ec7e5e21e
Merge: 41bc10cabe96 43c6afee48d4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 7 12:30:24 2019 -0700

    Merge tag 'pidfd-v5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull pidfd updates from Christian Brauner:
     "This patchset makes it possible to retrieve pidfds at process creation
      time by introducing the new flag CLONE_PIDFD to the clone() system
      call. Linus originally suggested to implement this as a new flag to
      clone() instead of making it a separate system call.
    
      After a thorough review from Oleg CLONE_PIDFD returns pidfds in the
      parent_tidptr argument. This means we can give back the associated pid
      and the pidfd at the same time. Access to process metadata information
      thus becomes rather trivial.
    
      As has been agreed, CLONE_PIDFD creates file descriptors based on
      anonymous inodes similar to the new mount api. They are made
      unconditional by this patchset as they are now needed by core kernel
      code (vfs, pidfd) even more than they already were before (timerfd,
      signalfd, io_uring, epoll etc.). The core patchset is rather small.
      The bulky looking changelist is caused by David's very simple changes
      to Kconfig to make anon inodes unconditional.
    
      A pidfd comes with additional information in fdinfo if the kernel
      supports procfs. The fdinfo file contains the pid of the process in
      the callers pid namespace in the same format as the procfs status
      file, i.e. "Pid:\t%d".
    
      To remove worries about missing metadata access this patchset comes
      with a sample/test program that illustrates how a combination of
      CLONE_PIDFD and pidfd_send_signal() can be used to gain race-free
      access to process metadata through /proc/<pid>.
    
      Further work based on this patchset has been done by Joel. His work
      makes pidfds pollable. It finished too late for this merge window. I
      would prefer to have it sitting in linux-next for a while and send it
      for inclusion during the 5.3 merge window"
    
    * tag 'pidfd-v5.2-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      samples: show race-free pidfd metadata access
      signal: support CLONE_PIDFD with pidfd_send_signal
      clone: add CLONE_PIDFD
      Make anon_inodes unconditional

commit b3e5838252665ee4cfa76b82bdf1198dca81e5be
Author: Christian Brauner <christian@brauner.io>
Date:   Wed Mar 27 13:04:15 2019 +0100

    clone: add CLONE_PIDFD
    
    This patchset makes it possible to retrieve pid file descriptors at
    process creation time by introducing the new flag CLONE_PIDFD to the
    clone() system call.  Linus originally suggested to implement this as a
    new flag to clone() instead of making it a separate system call.  As
    spotted by Linus, there is exactly one bit for clone() left.
    
    CLONE_PIDFD creates file descriptors based on the anonymous inode
    implementation in the kernel that will also be used to implement the new
    mount api.  They serve as a simple opaque handle on pids.  Logically,
    this makes it possible to interpret a pidfd differently, narrowing or
    widening the scope of various operations (e.g. signal sending).  Thus, a
    pidfd cannot just refer to a tgid, but also a tid, or in theory - given
    appropriate flag arguments in relevant syscalls - a process group or
    session. A pidfd does not represent a privilege.  This does not imply it
    cannot ever be that way but for now this is not the case.
    
    A pidfd comes with additional information in fdinfo if the kernel supports
    procfs.  The fdinfo file contains the pid of the process in the callers
    pid namespace in the same format as the procfs status file, i.e. "Pid:\t%d".
    
    As suggested by Oleg, with CLONE_PIDFD the pidfd is returned in the
    parent_tidptr argument of clone.  This has the advantage that we can
    give back the associated pid and the pidfd at the same time.
    
    To remove worries about missing metadata access this patchset comes with
    a sample program that illustrates how a combination of CLONE_PIDFD, and
    pidfd_send_signal() can be used to gain race-free access to process
    metadata through /proc/<pid>.  The sample program can easily be
    translated into a helper that would be suitable for inclusion in libc so
    that users don't have to worry about writing it themselves.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Christian Brauner <christian@brauner.io>
    Co-developed-by: Jann Horn <jannh@google.com>
    Signed-off-by: Jann Horn <jannh@google.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "Michael Kerrisk (man-pages)" <mtk.manpages@gmail.com>
    Cc: Andy Lutomirsky <luto@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aleksa Sarai <cyphar@cyphar.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9dcd18aa210b..e45f0acaf451 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -11,6 +11,7 @@
  * management can be a bitch. See 'mm/memory.c': 'copy_page_range()'
  */
 
+#include <linux/anon_inodes.h>
 #include <linux/slab.h>
 #include <linux/sched/autogroup.h>
 #include <linux/sched/mm.h>
@@ -21,6 +22,7 @@
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
 #include <linux/sched/cputime.h>
+#include <linux/seq_file.h>
 #include <linux/rtmutex.h>
 #include <linux/init.h>
 #include <linux/unistd.h>
@@ -1662,6 +1664,58 @@ static inline void rcu_copy_process(struct task_struct *p)
 #endif /* #ifdef CONFIG_TASKS_RCU */
 }
 
+static int pidfd_release(struct inode *inode, struct file *file)
+{
+	struct pid *pid = file->private_data;
+
+	file->private_data = NULL;
+	put_pid(pid);
+	return 0;
+}
+
+#ifdef CONFIG_PROC_FS
+static void pidfd_show_fdinfo(struct seq_file *m, struct file *f)
+{
+	struct pid_namespace *ns = proc_pid_ns(file_inode(m->file));
+	struct pid *pid = f->private_data;
+
+	seq_put_decimal_ull(m, "Pid:\t", pid_nr_ns(pid, ns));
+	seq_putc(m, '\n');
+}
+#endif
+
+const struct file_operations pidfd_fops = {
+	.release = pidfd_release,
+#ifdef CONFIG_PROC_FS
+	.show_fdinfo = pidfd_show_fdinfo,
+#endif
+};
+
+/**
+ * pidfd_create() - Create a new pid file descriptor.
+ *
+ * @pid:  struct pid that the pidfd will reference
+ *
+ * This creates a new pid file descriptor with the O_CLOEXEC flag set.
+ *
+ * Note, that this function can only be called after the fd table has
+ * been unshared to avoid leaking the pidfd to the new process.
+ *
+ * Return: On success, a cloexec pidfd is returned.
+ *         On error, a negative errno number will be returned.
+ */
+static int pidfd_create(struct pid *pid)
+{
+	int fd;
+
+	fd = anon_inode_getfd("[pidfd]", &pidfd_fops, get_pid(pid),
+			      O_RDWR | O_CLOEXEC);
+	if (fd < 0)
+		put_pid(pid);
+
+	return fd;
+}
+
 /*
  * This creates a new process as a copy of the old one,
  * but does not actually start it yet.
@@ -1674,13 +1728,14 @@ static __latent_entropy struct task_struct *copy_process(
 					unsigned long clone_flags,
 					unsigned long stack_start,
 					unsigned long stack_size,
+					int __user *parent_tidptr,
 					int __user *child_tidptr,
 					struct pid *pid,
 					int trace,
 					unsigned long tls,
 					int node)
 {
-	int retval;
+	int pidfd = -1, retval;
 	struct task_struct *p;
 	struct multiprocess_signals delayed;
 
@@ -1730,6 +1785,31 @@ static __latent_entropy struct task_struct *copy_process(
 			return ERR_PTR(-EINVAL);
 	}
 
+	if (clone_flags & CLONE_PIDFD) {
+		int reserved;
+
+		/*
+		 * - CLONE_PARENT_SETTID is useless for pidfds and also
+		 *   parent_tidptr is used to return pidfds.
+		 * - CLONE_DETACHED is blocked so that we can potentially
+		 *   reuse it later for CLONE_PIDFD.
+		 * - CLONE_THREAD is blocked until someone really needs it.
+		 */
+		if (clone_flags &
+		    (CLONE_DETACHED | CLONE_PARENT_SETTID | CLONE_THREAD))
+			return ERR_PTR(-EINVAL);
+
+		/*
+		 * Verify that parent_tidptr is sane so we can potentially
+		 * reuse it later.
+		 */
+		if (get_user(reserved, parent_tidptr))
+			return ERR_PTR(-EFAULT);
+
+		if (reserved != 0)
+			return ERR_PTR(-EINVAL);
+	}
+
 	/*
 	 * Force any signals received before this point to be delivered
 	 * before the fork happens.  Collect up signals sent to multiple
@@ -1936,6 +2016,22 @@ static __latent_entropy struct task_struct *copy_process(
 		}
 	}
 
+	/*
+	 * This has to happen after we've potentially unshared the file
+	 * descriptor table (so that the pidfd doesn't leak into the child
+	 * if the fd table isn't shared).
+	 */
+	if (clone_flags & CLONE_PIDFD) {
+		retval = pidfd_create(pid);
+		if (retval < 0)
+			goto bad_fork_free_pid;
+
+		pidfd = retval;
+		retval = put_user(pidfd, parent_tidptr);
+		if (retval)
+			goto bad_fork_put_pidfd;
+	}
+
 #ifdef CONFIG_BLOCK
 	p->plug = NULL;
 #endif
@@ -1996,7 +2092,7 @@ static __latent_entropy struct task_struct *copy_process(
 	 */
 	retval = cgroup_can_fork(p);
 	if (retval)
-		goto bad_fork_free_pid;
+		goto bad_fork_put_pidfd;
 
 	/*
 	 * From this point on we must avoid any synchronous user-space
@@ -2111,6 +2207,9 @@ static __latent_entropy struct task_struct *copy_process(
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
 	cgroup_cancel_fork(p);
+bad_fork_put_pidfd:
+	if (clone_flags & CLONE_PIDFD)
+		ksys_close(pidfd);
 bad_fork_free_pid:
 	cgroup_threadgroup_change_end(current);
 	if (pid != &init_struct_pid)
@@ -2176,7 +2275,7 @@ static inline void init_idle_pids(struct task_struct *idle)
 struct task_struct *fork_idle(int cpu)
 {
 	struct task_struct *task;
-	task = copy_process(CLONE_VM, 0, 0, NULL, &init_struct_pid, 0, 0,
+	task = copy_process(CLONE_VM, 0, 0, NULL, NULL, &init_struct_pid, 0, 0,
 			    cpu_to_node(cpu));
 	if (!IS_ERR(task)) {
 		init_idle_pids(task);
@@ -2223,7 +2322,7 @@ long _do_fork(unsigned long clone_flags,
 			trace = 0;
 	}
 
-	p = copy_process(clone_flags, stack_start, stack_size,
+	p = copy_process(clone_flags, stack_start, stack_size, parent_tidptr,
 			 child_tidptr, NULL, trace, tls, NUMA_NO_NODE);
 	add_latent_entropy();
 

commit 13585fa0668c724efab9635aaeef6ec390217415
Author: Nadav Amit <namit@vmware.com>
Date:   Thu Apr 25 17:11:25 2019 -0700

    fork: Provide a function for copying init_mm
    
    Provide a function for copying init_mm. This function will be later used
    for setting a temporary mm.
    
    Tested-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: <akpm@linux-foundation.org>
    Cc: <ard.biesheuvel@linaro.org>
    Cc: <deneen.t.dock@intel.com>
    Cc: <kernel-hardening@lists.openwall.com>
    Cc: <kristen@linux.intel.com>
    Cc: <linux_dti@icloud.com>
    Cc: <will.deacon@arm.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190426001143.4983-6-namit@vmware.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 44fba5e5e916..fbe9dfcd8680 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1299,13 +1299,20 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 		complete_vfork_done(tsk);
 }
 
-/*
- * Allocate a new mm structure and copy contents from the
- * mm structure of the passed in task structure.
+/**
+ * dup_mm() - duplicates an existing mm structure
+ * @tsk: the task_struct with which the new mm will be associated.
+ * @oldmm: the mm to duplicate.
+ *
+ * Allocates a new mm structure and duplicates the provided @oldmm structure
+ * content into it.
+ *
+ * Return: the duplicated mm or NULL on failure.
  */
-static struct mm_struct *dup_mm(struct task_struct *tsk)
+static struct mm_struct *dup_mm(struct task_struct *tsk,
+				struct mm_struct *oldmm)
 {
-	struct mm_struct *mm, *oldmm = current->mm;
+	struct mm_struct *mm;
 	int err;
 
 	mm = allocate_mm();
@@ -1372,7 +1379,7 @@ static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 	}
 
 	retval = -ENOMEM;
-	mm = dup_mm(tsk);
+	mm = dup_mm(tsk, current->mm);
 	if (!mm)
 		goto fail_nomem;
 
@@ -2187,6 +2194,11 @@ struct task_struct *fork_idle(int cpu)
 	return task;
 }
 
+struct mm_struct *copy_init_mm(void)
+{
+	return dup_mm(NULL, &init_mm);
+}
+
 /*
  *  Ok, this is the main fork-routine.
  *

commit aad42dd44db086c79ca3f470ad563d2ac4ac218d
Author: Nadav Amit <namit@vmware.com>
Date:   Fri Apr 26 16:22:44 2019 -0700

    uprobes: Initialize uprobes earlier
    
    In order to have a separate address space for text poking, we need to
    duplicate init_mm early during start_kernel(). This, however, introduces
    a problem since uprobes functions are called from dup_mmap(), but
    uprobes is still not initialized in this early stage.
    
    Since uprobes initialization is necassary for fork, and since all the
    dependant initialization has been done when fork is initialized (percpu
    and vmalloc), move uprobes initialization to fork_init(). It does not
    seem uprobes introduces any security problem for the poking_mm.
    
    Crash and burn if uprobes initialization fails, similarly to other early
    initializations. Change the init_probes() name to probes_init() to match
    other early initialization functions name convention.
    
    Reported-by: kernel test robot <lkp@intel.com>
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rick Edgecombe <rick.p.edgecombe@intel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: ard.biesheuvel@linaro.org
    Cc: deneen.t.dock@intel.com
    Cc: kernel-hardening@lists.openwall.com
    Cc: kristen@linux.intel.com
    Cc: linux_dti@icloud.com
    Cc: will.deacon@arm.com
    Link: https://lkml.kernel.org/r/20190426232303.28381-6-nadav.amit@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9dcd18aa210b..44fba5e5e916 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -815,6 +815,7 @@ void __init fork_init(void)
 #endif
 
 	lockdep_init_task(&init_task);
+	uprobes_init();
 }
 
 int __weak arch_dup_task_struct(struct task_struct *dst,

commit 76f969e8948d82e78e1bc4beb6b9465908e74873
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Apr 19 10:03:04 2019 -0700

    cgroup: cgroup v2 freezer
    
    Cgroup v1 implements the freezer controller, which provides an ability
    to stop the workload in a cgroup and temporarily free up some
    resources (cpu, io, network bandwidth and, potentially, memory)
    for some other tasks. Cgroup v2 lacks this functionality.
    
    This patch implements freezer for cgroup v2.
    
    Cgroup v2 freezer tries to put tasks into a state similar to jobctl
    stop. This means that tasks can be killed, ptraced (using
    PTRACE_SEIZE*), and interrupted. It is possible to attach to
    a frozen task, get some information (e.g. read registers) and detach.
    It's also possible to migrate a frozen tasks to another cgroup.
    
    This differs cgroup v2 freezer from cgroup v1 freezer, which mostly
    tried to imitate the system-wide freezer. However uninterruptible
    sleep is fine when all tasks are going to be frozen (hibernation case),
    it's not the acceptable state for some subset of the system.
    
    Cgroup v2 freezer is not supporting freezing kthreads.
    If a non-root cgroup contains kthread, the cgroup still can be frozen,
    but the kthread will remain running, the cgroup will be shown
    as non-frozen, and the notification will not be delivered.
    
    * PTRACE_ATTACH is not working because non-fatal signal delivery
    is blocked in frozen state.
    
    There are some interface differences between cgroup v1 and cgroup v2
    freezer too, which are required to conform the cgroup v2 interface
    design principles:
    1) There is no separate controller, which has to be turned on:
    the functionality is always available and is represented by
    cgroup.freeze and cgroup.events cgroup control files.
    2) The desired state is defined by the cgroup.freeze control file.
    Any hierarchical configuration is allowed.
    3) The interface is asynchronous. The actual state is available
    using cgroup.events control file ("frozen" field). There are no
    dedicated transitional states.
    4) It's allowed to make any changes with the cgroup hierarchy
    (create new cgroups, remove old cgroups, move tasks between cgroups)
    no matter if some cgroups are frozen.
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    No-objection-from-me-by: Oleg Nesterov <oleg@redhat.com>
    Cc: kernel-team@fb.com

diff --git a/kernel/fork.c b/kernel/fork.c
index 9dcd18aa210b..8097a0cce4db 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1222,7 +1222,9 @@ static int wait_for_vfork_done(struct task_struct *child,
 	int killed;
 
 	freezer_do_not_count();
+	cgroup_enter_frozen();
 	killed = wait_for_completion_killable(vfork);
+	cgroup_leave_frozen(false);
 	freezer_count();
 
 	if (killed) {

commit a50243b1ddcdd766d0d17fbfeeb1a22e62fdc461
Merge: 2901752c14b8 fca22e7e595f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 9 15:53:03 2019 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull rdma updates from Jason Gunthorpe:
     "This has been a slightly more active cycle than normal with ongoing
      core changes and quite a lot of collected driver updates.
    
       - Various driver fixes for bnxt_re, cxgb4, hns, mlx5, pvrdma, rxe
    
       - A new data transfer mode for HFI1 giving higher performance
    
       - Significant functional and bug fix update to the mlx5
         On-Demand-Paging MR feature
    
       - A chip hang reset recovery system for hns
    
       - Change mm->pinned_vm to an atomic64
    
       - Update bnxt_re to support a new 57500 chip
    
       - A sane netlink 'rdma link add' method for creating rxe devices and
         fixing the various unregistration race conditions in rxe's
         unregister flow
    
       - Allow lookup up objects by an ID over netlink
    
       - Various reworking of the core to driver interface:
           - drivers should not assume umem SGLs are in PAGE_SIZE chunks
           - ucontext is accessed via udata not other means
           - start to make the core code responsible for object memory
             allocation
           - drivers should convert struct device to struct ib_device via a
             helper
           - drivers have more tools to avoid use after unregister problems"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (280 commits)
      net/mlx5: ODP support for XRC transport is not enabled by default in FW
      IB/hfi1: Close race condition on user context disable and close
      RDMA/umem: Revert broken 'off by one' fix
      RDMA/umem: minor bug fix in error handling path
      RDMA/hns: Use GFP_ATOMIC in hns_roce_v2_modify_qp
      cxgb4: kfree mhp after the debug print
      IB/rdmavt: Fix concurrency panics in QP post_send and modify to error
      IB/rdmavt: Fix loopback send with invalidate ordering
      IB/iser: Fix dma_nents type definition
      IB/mlx5: Set correct write permissions for implicit ODP MR
      bnxt_re: Clean cq for kernel consumers only
      RDMA/uverbs: Don't do double free of allocated PD
      RDMA: Handle ucontext allocations by IB/core
      RDMA/core: Fix a WARN() message
      bnxt_re: fix the regression due to changes in alloc_pbl
      IB/mlx4: Increase the timeout for CM cache
      IB/core: Abort page fault handler silently during owning process exit
      IB/mlx5: Validate correct PD before prefetch MR
      IB/mlx5: Protect against prefetch of invalid MR
      RDMA/uverbs: Store PR pointer before it is overwritten
      ...

commit fd2081ffce4e8aa3b2085be3bc584523ddeedf02
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Thu Mar 7 16:31:31 2019 -0800

    kernel/fork.c: remove duplicated include
    
    Remove duplicated include.
    
    Link: http://lkml.kernel.org/r/20181209062952.17736-1-yuehaibing@huawei.com
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Reviewed-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 77059b211608..874e48c410f8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -77,7 +77,6 @@
 #include <linux/blkdev.h>
 #include <linux/fs_struct.h>
 #include <linux/magic.h>
-#include <linux/sched/mm.h>
 #include <linux/perf_event.h>
 #include <linux/posix-timers.h>
 #include <linux/user-return-notifier.h>

commit 70f8a3ca68d3e1f3344d959981ca55d5f6ec77f7
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Wed Feb 6 09:59:15 2019 -0800

    mm: make mm->pinned_vm an atomic64 counter
    
    Taking a sleeping lock to _only_ increment a variable is quite the
    overkill, and pretty much all users do this. Furthermore, some drivers
    (ie: infiniband and scif) that need pinned semantics can go to quite
    some trouble to actually delay via workqueue (un)accounting for pinned
    pages when not possible to acquire it.
    
    By making the counter atomic we no longer need to hold the mmap_sem and
    can simply some code around it for pinned_vm users. The counter is 64-bit
    such that we need not worry about overflows such as rdma user input
    controlled from userspace.
    
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index b69248e6f0e0..85e08c379a9e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -981,7 +981,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	mm_pgtables_bytes_init(mm);
 	mm->map_count = 0;
 	mm->locked_vm = 0;
-	mm->pinned_vm = 0;
+	atomic64_set(&mm->pinned_vm, 0);
 	memset(&mm->rss_stat, 0, sizeof(mm->rss_stat));
 	spin_lock_init(&mm->page_table_lock);
 	spin_lock_init(&mm->arg_lock);

commit f0b89d3958d73cd0785ec381f0ddf8efb6f183d8
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Jan 18 14:27:30 2019 +0200

    sched/core: Convert task_struct.stack_refcount to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable task_struct.stack_refcount is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts.
    
    The full comparison can be seen in
    https://lkml.org/lkml/2017/11/15/57 and it is hopefully soon
    in state to be merged to the documentation tree.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the task_struct.stack_refcount it might make a difference
    in following places:
    
     - try_get_task_stack(): increment in refcount_inc_not_zero() only
       guarantees control dependency on success vs. fully ordered
       atomic counterpart
     - put_task_stack(): decrement in refcount_dec_and_test() only
       provides RELEASE ordering and control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1547814450-18902-6-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3f7e192e29f2..77059b211608 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -429,7 +429,7 @@ static void release_task_stack(struct task_struct *tsk)
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 void put_task_stack(struct task_struct *tsk)
 {
-	if (atomic_dec_and_test(&tsk->stack_refcount))
+	if (refcount_dec_and_test(&tsk->stack_refcount))
 		release_task_stack(tsk);
 }
 #endif
@@ -447,7 +447,7 @@ void free_task(struct task_struct *tsk)
 	 * If the task had a separate stack allocation, it should be gone
 	 * by now.
 	 */
-	WARN_ON_ONCE(atomic_read(&tsk->stack_refcount) != 0);
+	WARN_ON_ONCE(refcount_read(&tsk->stack_refcount) != 0);
 #endif
 	rt_mutex_debug_task_free(tsk);
 	ftrace_graph_exit_task(tsk);
@@ -867,7 +867,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	tsk->stack_vm_area = stack_vm_area;
 #endif
 #ifdef CONFIG_THREAD_INFO_IN_TASK
-	atomic_set(&tsk->stack_refcount, 1);
+	refcount_set(&tsk->stack_refcount, 1);
 #endif
 
 	if (err)

commit ec1d281923cf81cc660343d0cb8ffc837ffb991d
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Jan 18 14:27:29 2019 +0200

    sched/core: Convert task_struct.usage to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable task_struct.usage is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts.
    
    The full comparison can be seen in
    https://lkml.org/lkml/2017/11/15/57 and it is hopefully soon
    in state to be merged to the documentation tree.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the task_struct.usage it might make a difference
    in following places:
    
     - put_task_struct(): decrement in refcount_dec_and_test() only
       provides RELEASE ordering and control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1547814450-18902-5-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 935a42d5f8ff..3f7e192e29f2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -717,7 +717,7 @@ static inline void put_signal_struct(struct signal_struct *sig)
 void __put_task_struct(struct task_struct *tsk)
 {
 	WARN_ON(!tsk->exit_state);
-	WARN_ON(atomic_read(&tsk->usage));
+	WARN_ON(refcount_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
 	cgroup_free(tsk);
@@ -896,7 +896,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	 * One for us, one for whoever does the "release_task()" (usually
 	 * parent)
 	 */
-	atomic_set(&tsk->usage, 2);
+	refcount_set(&tsk->usage, 2);
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	tsk->btrace_seq = 0;
 #endif

commit 60d4de3ff7f775509deba94b3db3c1abe55bf7a5
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Jan 18 14:27:27 2019 +0200

    sched/core: Convert signal_struct.sigcnt to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable signal_struct.sigcnt is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts.
    
    The full comparison can be seen in
    https://lkml.org/lkml/2017/11/15/57 and it is hopefully soon
    in state to be merged to the documentation tree.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the signal_struct.sigcnt it might make a difference
    in following places:
    
     - put_signal_struct(): decrement in refcount_dec_and_test() only
       provides RELEASE ordering and control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1547814450-18902-3-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 370856d4c0b3..935a42d5f8ff 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -710,7 +710,7 @@ static inline void free_signal_struct(struct signal_struct *sig)
 
 static inline void put_signal_struct(struct signal_struct *sig)
 {
-	if (atomic_dec_and_test(&sig->sigcnt))
+	if (refcount_dec_and_test(&sig->sigcnt))
 		free_signal_struct(sig);
 }
 
@@ -1527,7 +1527,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 
 	sig->nr_threads = 1;
 	atomic_set(&sig->live, 1);
-	atomic_set(&sig->sigcnt, 1);
+	refcount_set(&sig->sigcnt, 1);
 
 	/* list_add(thread_node, thread_head) without INIT_LIST_HEAD() */
 	sig->thread_head = (struct list_head)LIST_HEAD_INIT(tsk->thread_node);
@@ -2082,7 +2082,7 @@ static __latent_entropy struct task_struct *copy_process(
 		} else {
 			current->signal->nr_threads++;
 			atomic_inc(&current->signal->live);
-			atomic_inc(&current->signal->sigcnt);
+			refcount_inc(&current->signal->sigcnt);
 			task_join_group_stop(p);
 			list_add_tail_rcu(&p->thread_group,
 					  &p->group_leader->thread_group);

commit d036bda7d0e7269c2982eb979acfef855f5d7977
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Jan 18 14:27:26 2019 +0200

    sched/core: Convert sighand_struct.count to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable sighand_struct.count is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts.
    
    The full comparison can be seen in
    https://lkml.org/lkml/2017/11/15/57 and it is hopefully soon
    in state to be merged to the documentation tree.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the sighand_struct.count it might make a difference
    in following places:
    
     - __cleanup_sighand: decrement in refcount_dec_and_test() only
       provides RELEASE ordering and control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1547814450-18902-2-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b69248e6f0e0..370856d4c0b3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1463,7 +1463,7 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 	struct sighand_struct *sig;
 
 	if (clone_flags & CLONE_SIGHAND) {
-		atomic_inc(&current->sighand->count);
+		refcount_inc(&current->sighand->count);
 		return 0;
 	}
 	sig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);
@@ -1471,7 +1471,7 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 	if (!sig)
 		return -ENOMEM;
 
-	atomic_set(&sig->count, 1);
+	refcount_set(&sig->count, 1);
 	spin_lock_irq(&current->sighand->siglock);
 	memcpy(sig->action, current->sighand->action, sizeof(sig->action));
 	spin_unlock_irq(&current->sighand->siglock);
@@ -1480,7 +1480,7 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 
 void __cleanup_sighand(struct sighand_struct *sighand)
 {
-	if (atomic_dec_and_test(&sighand->count)) {
+	if (refcount_dec_and_test(&sighand->count)) {
 		signalfd_cleanup(sighand);
 		/*
 		 * sighand_cachep is SLAB_TYPESAFE_BY_RCU so we can free it
@@ -2439,7 +2439,7 @@ static int check_unshare_flags(unsigned long unshare_flags)
 			return -EINVAL;
 	}
 	if (unshare_flags & (CLONE_SIGHAND | CLONE_VM)) {
-		if (atomic_read(&current->sighand->count) > 1)
+		if (refcount_read(&current->sighand->count) > 1)
 			return -EINVAL;
 	}
 	if (unshare_flags & CLONE_VM) {

commit a88cc8da0279f8e481b0d90e51a0a1cffac55906
Merge: 9cb2feb4d21d 73444bc4d8f9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 8 18:58:29 2019 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge misc fixes from Andrew Morton:
     "14 fixes"
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>:
      mm, page_alloc: do not wake kswapd with zone lock held
      hugetlbfs: revert "use i_mmap_rwsem for more pmd sharing synchronization"
      hugetlbfs: revert "Use i_mmap_rwsem to fix page fault/truncate race"
      mm: page_mapped: don't assume compound page is huge or THP
      mm/memory.c: initialise mmu_notifier_range correctly
      tools/vm/page_owner: use page_owner_sort in the use example
      kasan: fix krealloc handling for tag-based mode
      kasan: make tag based mode work with CONFIG_HARDENED_USERCOPY
      kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
      mm, memcg: fix reclaim deadlock with writeback
      mm/usercopy.c: no check page span for stack objects
      slab: alien caches must not be initialized if the allocation of the alien cache failed
      fork, memcg: fix cached_stacks case
      zram: idle writeback fixes and cleanup

commit ba4a45746c362b665e245c50b870615f02f34781
Author: Shakeel Butt <shakeelb@google.com>
Date:   Tue Jan 8 15:22:57 2019 -0800

    fork, memcg: fix cached_stacks case
    
    Commit 5eed6f1dff87 ("fork,memcg: fix crash in free_thread_stack on
    memcg charge fail") fixes a crash caused due to failed memcg charge of
    the kernel stack.  However the fix misses the cached_stacks case which
    this patch fixes.  So, the same crash can happen if the memcg charge of
    a cached stack is failed.
    
    Link: http://lkml.kernel.org/r/20190102180145.57406-1-shakeelb@google.com
    Fixes: 5eed6f1dff87 ("fork,memcg: fix crash in free_thread_stack on memcg charge fail")
    Signed-off-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Rik van Riel <riel@surriel.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a60459947f18..5ad60d47f7e7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -217,6 +217,7 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 		memset(s->addr, 0, THREAD_SIZE);
 
 		tsk->stack_vm_area = s;
+		tsk->stack = s->addr;
 		return s->addr;
 	}
 

commit 7b55851367136b1efd84d98fea81ba57a98304cf
Author: David Herrmann <dh.herrmann@gmail.com>
Date:   Tue Jan 8 13:58:52 2019 +0100

    fork: record start_time late
    
    This changes the fork(2) syscall to record the process start_time after
    initializing the basic task structure but still before making the new
    process visible to user-space.
    
    Technically, we could record the start_time anytime during fork(2).  But
    this might lead to scenarios where a start_time is recorded long before
    a process becomes visible to user-space.  For instance, with
    userfaultfd(2) and TLS, user-space can delay the execution of fork(2)
    for an indefinite amount of time (and will, if this causes network
    access, or similar).
    
    By recording the start_time late, it much closer reflects the point in
    time where the process becomes live and can be observed by other
    processes.
    
    Lastly, this makes it much harder for user-space to predict and control
    the start_time they get assigned.  Previously, user-space could fork a
    process and stall it in copy_thread_tls() before its pid is allocated,
    but after its start_time is recorded.  This can be misused to later-on
    cycle through PIDs and resume the stalled fork(2) yielding a process
    that has the same pid and start_time as a process that existed before.
    This can be used to circumvent security systems that identify processes
    by their pid+start_time combination.
    
    Even though user-space was always aware that start_time recording is
    flaky (but several projects are known to still rely on start_time-based
    identification), changing the start_time to be recorded late will help
    mitigate existing attacks and make it much harder for user-space to
    control the start_time a process gets assigned.
    
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Tom Gundersen <teg@jklm.no>
    Signed-off-by: David Herrmann <dh.herrmann@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a60459947f18..7f49be94eba9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1833,8 +1833,6 @@ static __latent_entropy struct task_struct *copy_process(
 
 	posix_cpu_timers_init(p);
 
-	p->start_time = ktime_get_ns();
-	p->real_start_time = ktime_get_boot_ns();
 	p->io_context = NULL;
 	audit_set_context(p, NULL);
 	cgroup_fork(p);
@@ -2000,6 +1998,17 @@ static __latent_entropy struct task_struct *copy_process(
 	if (retval)
 		goto bad_fork_free_pid;
 
+	/*
+	 * From this point on we must avoid any synchronous user-space
+	 * communication until we take the tasklist-lock. In particular, we do
+	 * not want user-space to be able to predict the process start-time by
+	 * stalling fork(2) after we recorded the start_time but before it is
+	 * visible to the system.
+	 */
+
+	p->start_time = ktime_get_ns();
+	p->real_start_time = ktime_get_boot_ns();
+
 	/*
 	 * Make it visible to the rest of the system, but dont wake it up yet.
 	 * Need tasklist lock for parent etc handling!

commit fb5bf31722d0805a3f394f7d59f2e8cd07acccb7
Author: Yi Wang <wang.yi59@zte.com.cn>
Date:   Thu Jan 3 15:28:03 2019 -0800

    fork: fix some -Wmissing-prototypes warnings
    
    We get a warning when building kernel with W=1:
    
      kernel/fork.c:167:13: warning: no previous prototype for `arch_release_thread_stack' [-Wmissing-prototypes]
      kernel/fork.c:779:13: warning: no previous prototype for `fork_init' [-Wmissing-prototypes]
    
    Add the missing declaration in head file to fix this.
    
    Also, remove arch_release_thread_stack() completely because no arch
    seems to implement it since bb9d81264 (arch: remove tile port).
    
    Link: http://lkml.kernel.org/r/1542170087-23645-1-git-send-email-wang.yi59@zte.com.cn
    Signed-off-by: Yi Wang <wang.yi59@zte.com.cn>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d439c48ecf18..a60459947f18 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -164,10 +164,6 @@ static inline void free_task_struct(struct task_struct *tsk)
 }
 #endif
 
-void __weak arch_release_thread_stack(unsigned long *stack)
-{
-}
-
 #ifndef CONFIG_ARCH_THREAD_STACK_ALLOCATOR
 
 /*
@@ -422,7 +418,6 @@ static void release_task_stack(struct task_struct *tsk)
 		return;  /* Better to leak the stack than to free prematurely */
 
 	account_kernel_stack(tsk, -1);
-	arch_release_thread_stack(tsk->stack);
 	free_thread_stack(tsk);
 	tsk->stack = NULL;
 #ifdef CONFIG_VMAP_STACK

commit 0f4991e8fd48987ae476a92cdee6bfec4aff31b8
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Fri Dec 28 00:40:00 2018 -0800

    kernel/fork.c: mark 'stack_vm_area' with __maybe_unused
    
    Fixes gcc '-Wunused-but-set-variable' warning when CONFIG_VMAP_STACK is
    not set:
    
    kernel/fork.c: In function 'dup_task_struct':
    kernel/fork.c:843:20: warning:
     variable 'stack_vm_area' set but not used [-Wunused-but-set-variable]
    
    Link: http://lkml.kernel.org/r/1545965190-2381-1-git-send-email-yuehaibing@huawei.com
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c979605fe806..d439c48ecf18 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -841,7 +841,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 {
 	struct task_struct *tsk;
 	unsigned long *stack;
-	struct vm_struct *stack_vm_area;
+	struct vm_struct *stack_vm_area __maybe_unused;
 	int err;
 
 	if (node == NUMA_NO_NODE)

commit ca79b0c211af63fa3276f0e3fd7dd9ada2439839
Author: Arun KS <arunks@codeaurora.org>
Date:   Fri Dec 28 00:34:29 2018 -0800

    mm: convert totalram_pages and totalhigh_pages variables to atomic
    
    totalram_pages and totalhigh_pages are made static inline function.
    
    Main motivation was that managed_page_count_lock handling was complicating
    things.  It was discussed in length here,
    https://lore.kernel.org/patchwork/patch/995739/#1181785 So it seemes
    better to remove the lock and convert variables to atomic, with preventing
    poteintial store-to-read tearing as a bonus.
    
    [akpm@linux-foundation.org: coding style fixes]
    Link: http://lkml.kernel.org/r/1542090790-21750-4-git-send-email-arunks@codeaurora.org
    Signed-off-by: Arun KS <arunks@codeaurora.org>
    Suggested-by: Michal Hocko <mhocko@suse.com>
    Suggested-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8617a326e9f5..c979605fe806 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -744,7 +744,7 @@ void __init __weak arch_task_cache_init(void) { }
 static void set_max_threads(unsigned int max_threads_suggested)
 {
 	u64 threads;
-	unsigned long nr_pages = totalram_pages;
+	unsigned long nr_pages = totalram_pages();
 
 	/*
 	 * The number of threads shall be limited such that the thread

commit 3d6357de8aa09e1966770dc1171c72679946464f
Author: Arun KS <arunks@codeaurora.org>
Date:   Fri Dec 28 00:34:20 2018 -0800

    mm: reference totalram_pages and managed_pages once per function
    
    Patch series "mm: convert totalram_pages, totalhigh_pages and managed
    pages to atomic", v5.
    
    This series converts totalram_pages, totalhigh_pages and
    zone->managed_pages to atomic variables.
    
    totalram_pages, zone->managed_pages and totalhigh_pages updates are
    protected by managed_page_count_lock, but readers never care about it.
    Convert these variables to atomic to avoid readers potentially seeing a
    store tear.
    
    Main motivation was that managed_page_count_lock handling was complicating
    things.  It was discussed in length here,
    https://lore.kernel.org/patchwork/patch/995739/#1181785 It seemes better
    to remove the lock and convert variables to atomic.  With the change,
    preventing poteintial store-to-read tearing comes as a bonus.
    
    This patch (of 4):
    
    This is in preparation to a later patch which converts totalram_pages and
    zone->managed_pages to atomic variables.  Please note that re-reading the
    value might lead to a different value and as such it could lead to
    unexpected behavior.  There are no known bugs as a result of the current
    code but it is better to prevent from them in principle.
    
    Link: http://lkml.kernel.org/r/1542090790-21750-2-git-send-email-arunks@codeaurora.org
    Signed-off-by: Arun KS <arunks@codeaurora.org>
    Reviewed-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e2a5156bc9c3..8617a326e9f5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -744,15 +744,16 @@ void __init __weak arch_task_cache_init(void) { }
 static void set_max_threads(unsigned int max_threads_suggested)
 {
 	u64 threads;
+	unsigned long nr_pages = totalram_pages;
 
 	/*
 	 * The number of threads shall be limited such that the thread
 	 * structures may only consume a small part of the available memory.
 	 */
-	if (fls64(totalram_pages) + fls64(PAGE_SIZE) > 64)
+	if (fls64(nr_pages) + fls64(PAGE_SIZE) > 64)
 		threads = MAX_THREADS;
 	else
-		threads = div64_u64((u64) totalram_pages * (u64) PAGE_SIZE,
+		threads = div64_u64((u64) nr_pages * (u64) PAGE_SIZE,
 				    (u64) THREAD_SIZE * 8UL);
 
 	if (threads > max_threads_suggested)

commit 5eed6f1dff87bfb5e545935def3843edf42800f2
Author: Rik van Riel <riel@surriel.com>
Date:   Fri Dec 21 14:30:54 2018 -0800

    fork,memcg: fix crash in free_thread_stack on memcg charge fail
    
    Commit 9b6f7e163cd0 ("mm: rework memcg kernel stack accounting") will
    result in fork failing if allocating a kernel stack for a task in
    dup_task_struct exceeds the kernel memory allowance for that cgroup.
    
    Unfortunately, it also results in a crash.
    
    This is due to the code jumping to free_stack and calling
    free_thread_stack when the memcg kernel stack charge fails, but without
    tsk->stack pointing at the freshly allocated stack.
    
    This in turn results in the vfree_atomic in free_thread_stack oopsing
    with a backtrace like this:
    
    #5 [ffffc900244efc88] die at ffffffff8101f0ab
     #6 [ffffc900244efcb8] do_general_protection at ffffffff8101cb86
     #7 [ffffc900244efce0] general_protection at ffffffff818ff082
        [exception RIP: llist_add_batch+7]
        RIP: ffffffff8150d487  RSP: ffffc900244efd98  RFLAGS: 00010282
        RAX: 0000000000000000  RBX: ffff88085ef55980  RCX: 0000000000000000
        RDX: ffff88085ef55980  RSI: 343834343531203a  RDI: 343834343531203a
        RBP: ffffc900244efd98   R8: 0000000000000001   R9: ffff8808578c3600
        R10: 0000000000000000  R11: 0000000000000001  R12: ffff88029f6c21c0
        R13: 0000000000000286  R14: ffff880147759b00  R15: 0000000000000000
        ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018
     #8 [ffffc900244efda0] vfree_atomic at ffffffff811df2c7
     #9 [ffffc900244efdb8] copy_process at ffffffff81086e37
    #10 [ffffc900244efe98] _do_fork at ffffffff810884e0
    #11 [ffffc900244eff10] sys_vfork at ffffffff810887ff
    #12 [ffffc900244eff20] do_syscall_64 at ffffffff81002a43
        RIP: 000000000049b948  RSP: 00007ffcdb307830  RFLAGS: 00000246
        RAX: ffffffffffffffda  RBX: 0000000000896030  RCX: 000000000049b948
        RDX: 0000000000000000  RSI: 00007ffcdb307790  RDI: 00000000005d7421
        RBP: 000000000067370f   R8: 00007ffcdb3077b0   R9: 000000000001ed00
        R10: 0000000000000008  R11: 0000000000000246  R12: 0000000000000040
        R13: 000000000000000f  R14: 0000000000000000  R15: 000000000088d018
        ORIG_RAX: 000000000000003a  CS: 0033  SS: 002b
    
    The simplest fix is to assign tsk->stack right where it is allocated.
    
    Link: http://lkml.kernel.org/r/20181214231726.7ee4843c@imladris.surriel.com
    Fixes: 9b6f7e163cd0 ("mm: rework memcg kernel stack accounting")
    Signed-off-by: Rik van Riel <riel@surriel.com>
    Acked-by: Roman Gushchin <guro@fb.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 07cddff89c7b..e2a5156bc9c3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -240,8 +240,10 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 	 * free_thread_stack() can be called in interrupt context,
 	 * so cache the vm_struct.
 	 */
-	if (stack)
+	if (stack) {
 		tsk->stack_vm_area = find_vm_area(stack);
+		tsk->stack = stack;
+	}
 	return stack;
 #else
 	struct page *page = alloc_pages_node(node, THREADINFO_GFP,
@@ -288,7 +290,10 @@ static struct kmem_cache *thread_stack_cache;
 static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
 						  int node)
 {
-	return kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
+	unsigned long *stack;
+	stack = kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
+	tsk->stack = stack;
+	return stack;
 }
 
 static void free_thread_stack(struct task_struct *tsk)

commit 2d6bb6adb714b133db92ccd4bfc9c20f75f71f3f
Merge: 7c6c54b505b8 6fcde9046673
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 1 11:46:27 2018 -0700

    Merge tag 'stackleak-v4.20-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull stackleak gcc plugin from Kees Cook:
     "Please pull this new GCC plugin, stackleak, for v4.20-rc1. This plugin
      was ported from grsecurity by Alexander Popov. It provides efficient
      stack content poisoning at syscall exit. This creates a defense
      against at least two classes of flaws:
    
       - Uninitialized stack usage. (We continue to work on improving the
         compiler to do this in other ways: e.g. unconditional zero init was
         proposed to GCC and Clang, and more plugin work has started too).
    
       - Stack content exposure. By greatly reducing the lifetime of valid
         stack contents, exposures via either direct read bugs or unknown
         cache side-channels become much more difficult to exploit. This
         complements the existing buddy and heap poisoning options, but
         provides the coverage for stacks.
    
      The x86 hooks are included in this series (which have been reviewed by
      Ingo, Dave Hansen, and Thomas Gleixner). The arm64 hooks have already
      been merged through the arm64 tree (written by Laura Abbott and
      reviewed by Mark Rutland and Will Deacon).
    
      With VLAs having been removed this release, there is no need for
      alloca() protection, so it has been removed from the plugin"
    
    * tag 'stackleak-v4.20-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      arm64: Drop unneeded stackleak_check_alloca()
      stackleak: Allow runtime disabling of kernel stack erasing
      doc: self-protection: Add information about STACKLEAK feature
      fs/proc: Show STACKLEAK metrics in the /proc file system
      lkdtm: Add a test for STACKLEAK
      gcc-plugins: Add STACKLEAK plugin for tracking the kernel stack
      x86/entry: Add STACKLEAK erasing the kernel stack at the end of syscalls

commit eb414681d5a07d28d2ff90dc05f69ec6b232ebd2
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Oct 26 15:06:27 2018 -0700

    psi: pressure stall information for CPU, memory, and IO
    
    When systems are overcommitted and resources become contended, it's hard
    to tell exactly the impact this has on workload productivity, or how close
    the system is to lockups and OOM kills.  In particular, when machines work
    multiple jobs concurrently, the impact of overcommit in terms of latency
    and throughput on the individual job can be enormous.
    
    In order to maximize hardware utilization without sacrificing individual
    job health or risk complete machine lockups, this patch implements a way
    to quantify resource pressure in the system.
    
    A kernel built with CONFIG_PSI=y creates files in /proc/pressure/ that
    expose the percentage of time the system is stalled on CPU, memory, or IO,
    respectively.  Stall states are aggregate versions of the per-task delay
    accounting delays:
    
           cpu: some tasks are runnable but not executing on a CPU
           memory: tasks are reclaiming, or waiting for swapin or thrashing cache
           io: tasks are waiting for io completions
    
    These percentages of walltime can be thought of as pressure percentages,
    and they give a general sense of system health and productivity loss
    incurred by resource overcommit.  They can also indicate when the system
    is approaching lockup scenarios and OOMs.
    
    To do this, psi keeps track of the task states associated with each CPU
    and samples the time they spend in stall states.  Every 2 seconds, the
    samples are averaged across CPUs - weighted by the CPUs' non-idle time to
    eliminate artifacts from unused CPUs - and translated into percentages of
    walltime.  A running average of those percentages is maintained over 10s,
    1m, and 5m periods (similar to the loadaverage).
    
    [hannes@cmpxchg.org: doc fixlet, per Randy]
      Link: http://lkml.kernel.org/r/20180828205625.GA14030@cmpxchg.org
    [hannes@cmpxchg.org: code optimization]
      Link: http://lkml.kernel.org/r/20180907175015.GA8479@cmpxchg.org
    [hannes@cmpxchg.org: rename psi_clock() to psi_update_work(), per Peter]
      Link: http://lkml.kernel.org/r/20180907145404.GB11088@cmpxchg.org
    [hannes@cmpxchg.org: fix build]
      Link: http://lkml.kernel.org/r/20180913014222.GA2370@cmpxchg.org
    Link: http://lkml.kernel.org/r/20180828172258.3185-9-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Daniel Drake <drake@endlessm.com>
    Tested-by: Suren Baghdasaryan <surenb@google.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Johannes Weiner <jweiner@fb.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Enderborg <peter.enderborg@sony.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vinayak Menon <vinmenon@codeaurora.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3c719fec46c5..8f82a3bdcb8f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1822,6 +1822,10 @@ static __latent_entropy struct task_struct *copy_process(
 
 	p->default_timer_slack_ns = current->timer_slack_ns;
 
+#ifdef CONFIG_PSI
+	p->psi_flags = 0;
+#endif
+
 	task_io_accounting_init(&p->ioac);
 	acct_clear_integrals(p);
 

commit 9b6f7e163cd0f468d1b9696b785659d3c27c8667
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Oct 26 15:03:19 2018 -0700

    mm: rework memcg kernel stack accounting
    
    If CONFIG_VMAP_STACK is set, kernel stacks are allocated using
    __vmalloc_node_range() with __GFP_ACCOUNT.  So kernel stack pages are
    charged against corresponding memory cgroups on allocation and uncharged
    on releasing them.
    
    The problem is that we do cache kernel stacks in small per-cpu caches and
    do reuse them for new tasks, which can belong to different memory cgroups.
    
    Each stack page still holds a reference to the original cgroup, so the
    cgroup can't be released until the vmap area is released.
    
    To make this happen we need more than two subsequent exits without forks
    in between on the current cpu, which makes it very unlikely to happen.  As
    a result, I saw a significant number of dying cgroups (in theory, up to 2
    * number_of_cpu + number_of_tasks), which can't be released even by
    significant memory pressure.
    
    As a cgroup structure can take a significant amount of memory (first of
    all, per-cpu data like memcg statistics), it leads to a noticeable waste
    of memory.
    
    Link: http://lkml.kernel.org/r/20180827162621.30187-1-guro@fb.com
    Fixes: ac496bf48d97 ("fork: Optimize task creation by caching two thread stacks per CPU if CONFIG_VMAP_STACK=y")
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Reviewed-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Michal Hocko <mhocko@kernel.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f0b58479534f..3c719fec46c5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -223,9 +223,14 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 		return s->addr;
 	}
 
+	/*
+	 * Allocated stacks are cached and later reused by new threads,
+	 * so memcg accounting is performed manually on assigning/releasing
+	 * stacks to tasks. Drop __GFP_ACCOUNT.
+	 */
 	stack = __vmalloc_node_range(THREAD_SIZE, THREAD_ALIGN,
 				     VMALLOC_START, VMALLOC_END,
-				     THREADINFO_GFP,
+				     THREADINFO_GFP & ~__GFP_ACCOUNT,
 				     PAGE_KERNEL,
 				     0, node, __builtin_return_address(0));
 
@@ -248,9 +253,19 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 static inline void free_thread_stack(struct task_struct *tsk)
 {
 #ifdef CONFIG_VMAP_STACK
-	if (task_stack_vm_area(tsk)) {
+	struct vm_struct *vm = task_stack_vm_area(tsk);
+
+	if (vm) {
 		int i;
 
+		for (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++) {
+			mod_memcg_page_state(vm->pages[i],
+					     MEMCG_KERNEL_STACK_KB,
+					     -(int)(PAGE_SIZE / 1024));
+
+			memcg_kmem_uncharge(vm->pages[i], 0);
+		}
+
 		for (i = 0; i < NR_CACHED_STACKS; i++) {
 			if (this_cpu_cmpxchg(cached_stacks[i],
 					NULL, tsk->stack_vm_area) != NULL)
@@ -351,10 +366,6 @@ static void account_kernel_stack(struct task_struct *tsk, int account)
 					    NR_KERNEL_STACK_KB,
 					    PAGE_SIZE / 1024 * account);
 		}
-
-		/* All stack pages belong to the same memcg. */
-		mod_memcg_page_state(vm->pages[0], MEMCG_KERNEL_STACK_KB,
-				     account * (THREAD_SIZE / 1024));
 	} else {
 		/*
 		 * All stack pages are in the same zone and belong to the
@@ -370,6 +381,35 @@ static void account_kernel_stack(struct task_struct *tsk, int account)
 	}
 }
 
+static int memcg_charge_kernel_stack(struct task_struct *tsk)
+{
+#ifdef CONFIG_VMAP_STACK
+	struct vm_struct *vm = task_stack_vm_area(tsk);
+	int ret;
+
+	if (vm) {
+		int i;
+
+		for (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++) {
+			/*
+			 * If memcg_kmem_charge() fails, page->mem_cgroup
+			 * pointer is NULL, and both memcg_kmem_uncharge()
+			 * and mod_memcg_page_state() in free_thread_stack()
+			 * will ignore this page. So it's safe.
+			 */
+			ret = memcg_kmem_charge(vm->pages[i], GFP_KERNEL, 0);
+			if (ret)
+				return ret;
+
+			mod_memcg_page_state(vm->pages[i],
+					     MEMCG_KERNEL_STACK_KB,
+					     PAGE_SIZE / 1024);
+		}
+	}
+#endif
+	return 0;
+}
+
 static void release_task_stack(struct task_struct *tsk)
 {
 	if (WARN_ON(tsk->state != TASK_DEAD))
@@ -807,6 +847,9 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	if (!stack)
 		goto free_tsk;
 
+	if (memcg_charge_kernel_stack(tsk))
+		goto free_stack;
+
 	stack_vm_area = task_stack_vm_area(tsk);
 
 	err = arch_dup_task_struct(tsk, orig);

commit 1ed0cc5a01a4d868d9907ce96468c4b4c6709556
Author: Nadav Amit <namit@vmware.com>
Date:   Tue Sep 4 15:45:41 2018 -0700

    mm: respect arch_dup_mmap() return value
    
    Commit d70f2a14b72a ("include/linux/sched/mm.h: uninline mmdrop_async(),
    etc") ignored the return value of arch_dup_mmap(). As a result, on x86,
    a failure to duplicate the LDT (e.g. due to memory allocation error)
    would leave the duplicated memory mapping in an inconsistent state.
    
    Fix by using the return value, as it was before the change.
    
    Link: http://lkml.kernel.org/r/20180823051229.211856-1-namit@vmware.com
    Fixes: d70f2a14b72a4 ("include/linux/sched/mm.h: uninline mmdrop_async(), etc")
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: <stable@vger.kernel.org>
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d896e9ca38b0..f0b58479534f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -550,8 +550,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 			goto out;
 	}
 	/* a new mm has just been created */
-	arch_dup_mmap(oldmm, mm);
-	retval = 0;
+	retval = arch_dup_mmap(oldmm, mm);
 out:
 	up_write(&mm->mmap_sem);
 	flush_tlb_mm(oldmm);

commit afaef01c001537fa97a25092d7f54d764dc7d8c1
Author: Alexander Popov <alex.popov@linux.com>
Date:   Fri Aug 17 01:16:58 2018 +0300

    x86/entry: Add STACKLEAK erasing the kernel stack at the end of syscalls
    
    The STACKLEAK feature (initially developed by PaX Team) has the following
    benefits:
    
    1. Reduces the information that can be revealed through kernel stack leak
       bugs. The idea of erasing the thread stack at the end of syscalls is
       similar to CONFIG_PAGE_POISONING and memzero_explicit() in kernel
       crypto, which all comply with FDP_RIP.2 (Full Residual Information
       Protection) of the Common Criteria standard.
    
    2. Blocks some uninitialized stack variable attacks (e.g. CVE-2017-17712,
       CVE-2010-2963). That kind of bugs should be killed by improving C
       compilers in future, which might take a long time.
    
    This commit introduces the code filling the used part of the kernel
    stack with a poison value before returning to userspace. Full
    STACKLEAK feature also contains the gcc plugin which comes in a
    separate commit.
    
    The STACKLEAK feature is ported from grsecurity/PaX. More information at:
      https://grsecurity.net/
      https://pax.grsecurity.net/
    
    This code is modified from Brad Spengler/PaX Team's code in the last
    public patch of grsecurity/PaX based on our understanding of the code.
    Changes or omissions from the original code are ours and don't reflect
    the original grsecurity/PaX code.
    
    Performance impact:
    
    Hardware: Intel Core i7-4770, 16 GB RAM
    
    Test #1: building the Linux kernel on a single core
            0.91% slowdown
    
    Test #2: hackbench -s 4096 -l 2000 -g 15 -f 25 -P
            4.2% slowdown
    
    So the STACKLEAK description in Kconfig includes: "The tradeoff is the
    performance impact: on a single CPU system kernel compilation sees a 1%
    slowdown, other systems and workloads may vary and you are advised to
    test this feature on your expected workload before deploying it".
    
    Signed-off-by: Alexander Popov <alex.popov@linux.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d896e9ca38b0..47911e49c2b1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -91,6 +91,7 @@
 #include <linux/kcov.h>
 #include <linux/livepatch.h>
 #include <linux/thread_info.h>
+#include <linux/stackleak.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -1880,6 +1881,8 @@ static __latent_entropy struct task_struct *copy_process(
 	if (retval)
 		goto bad_fork_cleanup_io;
 
+	stackleak_task_init(p);
+
 	if (pid != &init_struct_pid) {
 		pid = alloc_pid(p->nsproxy->pid_ns_for_children);
 		if (IS_ERR(pid)) {

commit cd9b44f90763c3367e8dd0601849ffb028e8ba52
Merge: df2def49c57b 2a9d64810042
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 22 12:34:08 2018 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
    
     - the rest of MM
    
     - procfs updates
    
     - various misc things
    
     - more y2038 fixes
    
     - get_maintainer updates
    
     - lib/ updates
    
     - checkpatch updates
    
     - various epoll updates
    
     - autofs updates
    
     - hfsplus
    
     - some reiserfs work
    
     - fatfs updates
    
     - signal.c cleanups
    
     - ipc/ updates
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (166 commits)
      ipc/util.c: update return value of ipc_getref from int to bool
      ipc/util.c: further variable name cleanups
      ipc: simplify ipc initialization
      ipc: get rid of ids->tables_initialized hack
      lib/rhashtable: guarantee initial hashtable allocation
      lib/rhashtable: simplify bucket_table_alloc()
      ipc: drop ipc_lock()
      ipc/util.c: correct comment in ipc_obtain_object_check
      ipc: rename ipcctl_pre_down_nolock()
      ipc/util.c: use ipc_rcu_putref() for failues in ipc_addid()
      ipc: reorganize initialization of kern_ipc_perm.seq
      ipc: compute kern_ipc_perm.id under the ipc lock
      init/Kconfig: remove EXPERT from CHECKPOINT_RESTORE
      fs/sysv/inode.c: use ktime_get_real_seconds() for superblock stamp
      adfs: use timespec64 for time conversion
      kernel/sysctl.c: fix typos in comments
      drivers/rapidio/devices/rio_mport_cdev.c: remove redundant pointer md
      fork: don't copy inconsistent signal handler state to child
      signal: make get_signal() return bool
      signal: make sigkill_pending() return bool
      ...

commit 06e62a46bbba20aa5286102016a04214bb446141
Author: Jann Horn <jannh@google.com>
Date:   Tue Aug 21 22:00:58 2018 -0700

    fork: don't copy inconsistent signal handler state to child
    
    Before this change, if a multithreaded process forks while one of its
    threads is changing a signal handler using sigaction(), the memcpy() in
    copy_sighand() can race with the struct assignment in do_sigaction().  It
    isn't clear whether this can cause corruption of the userspace signal
    handler pointer, but it definitely can cause inconsistency between
    different fields of struct sigaction.
    
    Take the appropriate spinlock to avoid this.
    
    I have tested that this patch prevents inconsistency between sa_sigaction
    and sa_flags, which is possible before this patch.
    
    Link: http://lkml.kernel.org/r/20180702145108.73189-1-jannh@google.com
    Signed-off-by: Jann Horn <jannh@google.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: "Peter Zijlstra (Intel)" <peterz@infradead.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8bcef2413f1b..23eb960c701d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1427,7 +1427,9 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 		return -ENOMEM;
 
 	atomic_set(&sig->count, 1);
+	spin_lock_irq(&current->sighand->siglock);
 	memcpy(sig->action, current->sighand->action, sizeof(sig->action));
+	spin_unlock_irq(&current->sighand->siglock);
 	return 0;
 }
 

commit a2e514453861dd39b53b7a50b6771bd3f9852078
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Tue Aug 21 21:55:52 2018 -0700

    kernel/hung_task.c: allow to set checking interval separately from timeout
    
    Currently task hung checking interval is equal to timeout, as the result
    hung is detected anywhere between timeout and 2*timeout.  This is fine for
    most interactive environments, but this hurts automated testing setups
    (syzbot).  In an automated setup we need to strictly order CPU lockup <
    RCU stall < workqueue lockup < task hung < silent loss, so that RCU stall
    is not detected as task hung and task hung is not detected as silent
    machine loss.  The large variance in task hung detection timeout requires
    setting silent machine loss timeout to a very large value (e.g.  if task
    hung is 3 mins, then silent loss need to be set to ~7 mins).  The
    additional 3 minutes significantly reduce testing efficiency because
    usually we crash kernel within a minute, and this can add hours to bug
    localization process as it needs to do dozens of tests.
    
    Allow setting checking interval separately from timeout.  This allows to
    set timeout to, say, 3 minutes, but checking interval to 10 secs.
    
    The interval is controlled via a new hung_task_check_interval_secs sysctl,
    similar to the existing hung_task_timeout_secs sysctl.  The default value
    of 0 results in the current behavior: checking interval is equal to
    timeout.
    
    [akpm@linux-foundation.org: update hung_task_timeout_max's comment]
    Link: http://lkml.kernel.org/r/20180611111004.203513-1-dvyukov@google.com
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8c760effa42e..8bcef2413f1b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1302,6 +1302,7 @@ static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 	tsk->nvcsw = tsk->nivcsw = 0;
 #ifdef CONFIG_DETECT_HUNG_TASK
 	tsk->last_switch_count = tsk->nvcsw + tsk->nivcsw;
+	tsk->last_switch_time = 0;
 #endif
 
 	tsk->mm = NULL;

commit a670468f5e0b5fad4db6e4d195f15915dc2a35c1
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Aug 21 21:53:06 2018 -0700

    mm: zero out the vma in vma_init()
    
    Rather than in vm_area_alloc().  To ensure that the various oddball
    stack-based vmas are in a good state.  Some of the callers were zeroing
    them out, others were not.
    
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5ee74c113381..8c760effa42e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -310,8 +310,9 @@ static struct kmem_cache *mm_cachep;
 
 struct vm_area_struct *vm_area_alloc(struct mm_struct *mm)
 {
-	struct vm_area_struct *vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+	struct vm_area_struct *vma;
 
+	vma = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
 	if (vma)
 		vma_init(vma, mm);
 	return vma;

commit 0214f46b3a0383d6e33c297e7706216b6a550e4b
Merge: 40fafdcbcd7a 84fe4cc09abc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 21 13:47:29 2018 -0700

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull core signal handling updates from Eric Biederman:
     "It was observed that a periodic timer in combination with a
      sufficiently expensive fork could prevent fork from every completing.
      This contains the changes to remove the need for that restart.
    
      This set of changes is split into several parts:
    
       - The first part makes PIDTYPE_TGID a proper pid type instead
         something only for very special cases. The part starts using
         PIDTYPE_TGID enough so that in __send_signal where signals are
         actually delivered we know if the signal is being sent to a a group
         of processes or just a single process.
    
       - With that prep work out of the way the logic in fork is modified so
         that fork logically makes signals received while it is running
         appear to be received after the fork completes"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (22 commits)
      signal: Don't send signals to tasks that don't exist
      signal: Don't restart fork when signals come in.
      fork: Have new threads join on-going signal group stops
      fork: Skip setting TIF_SIGPENDING in ptrace_init_task
      signal: Add calculate_sigpending()
      fork: Unconditionally exit if a fatal signal is pending
      fork: Move and describe why the code examines PIDNS_ADDING
      signal: Push pid type down into complete_signal.
      signal: Push pid type down into __send_signal
      signal: Push pid type down into send_signal
      signal: Pass pid type into do_send_sig_info
      signal: Pass pid type into send_sigio_to_task & send_sigurg_to_task
      signal: Pass pid type into group_send_sig_info
      signal: Pass pid and pid type into send_sigqueue
      posix-timers: Noralize good_sigevent
      signal: Use PIDTYPE_TGID to clearly store where file signals will be sent
      pid: Implement PIDTYPE_TGID
      pids: Move the pgrp and session pid pointers from task_struct to signal_struct
      kvm: Don't open code task_pid in kvm_vcpu_ioctl
      pids: Compute task_tgid using signal->leader_pid
      ...

commit d46eb14b735b11927d4bdc2d1854c311af19de6d
Author: Shakeel Butt <shakeelb@google.com>
Date:   Fri Aug 17 15:46:39 2018 -0700

    fs: fsnotify: account fsnotify metadata to kmemcg
    
    Patch series "Directed kmem charging", v8.
    
    The Linux kernel's memory cgroup allows limiting the memory usage of the
    jobs running on the system to provide isolation between the jobs.  All
    the kernel memory allocated in the context of the job and marked with
    __GFP_ACCOUNT will also be included in the memory usage and be limited
    by the job's limit.
    
    The kernel memory can only be charged to the memcg of the process in
    whose context kernel memory was allocated.  However there are cases
    where the allocated kernel memory should be charged to the memcg
    different from the current processes's memcg.  This patch series
    contains two such concrete use-cases i.e.  fsnotify and buffer_head.
    
    The fsnotify event objects can consume a lot of system memory for large
    or unlimited queues if there is either no or slow listener.  The events
    are allocated in the context of the event producer.  However they should
    be charged to the event consumer.  Similarly the buffer_head objects can
    be allocated in a memcg different from the memcg of the page for which
    buffer_head objects are being allocated.
    
    To solve this issue, this patch series introduces mechanism to charge
    kernel memory to a given memcg.  In case of fsnotify events, the memcg
    of the consumer can be used for charging and for buffer_head, the memcg
    of the page can be charged.  For directed charging, the caller can use
    the scope API memalloc_[un]use_memcg() to specify the memcg to charge
    for all the __GFP_ACCOUNT allocations within the scope.
    
    This patch (of 2):
    
    A lot of memory can be consumed by the events generated for the huge or
    unlimited queues if there is either no or slow listener.  This can cause
    system level memory pressure or OOMs.  So, it's better to account the
    fsnotify kmem caches to the memcg of the listener.
    
    However the listener can be in a different memcg than the memcg of the
    producer and these allocations happen in the context of the event
    producer.  This patch introduces remote memcg charging API which the
    producer can use to charge the allocations to the memcg of the listener.
    
    There are seven fsnotify kmem caches and among them allocations from
    dnotify_struct_cache, dnotify_mark_cache, fanotify_mark_cache and
    inotify_inode_mark_cachep happens in the context of syscall from the
    listener.  So, SLAB_ACCOUNT is enough for these caches.
    
    The objects from fsnotify_mark_connector_cachep are not accounted as
    they are small compared to the notification mark or events and it is
    unclear whom to account connector to since it is shared by all events
    attached to the inode.
    
    The allocations from the event caches happen in the context of the event
    producer.  For such caches we will need to remote charge the allocations
    to the listener's memcg.  Thus we save the memcg reference in the
    fsnotify_group structure of the listener.
    
    This patch has also moved the members of fsnotify_group to keep the size
    same, at least for 64 bit build, even with additional member by filling
    the holes.
    
    [shakeelb@google.com: use GFP_KERNEL_ACCOUNT rather than open-coding it]
      Link: http://lkml.kernel.org/r/20180702215439.211597-1-shakeelb@google.com
    Link: http://lkml.kernel.org/r/20180627191250.209150-2-shakeelb@google.com
    Signed-off-by: Shakeel Butt <shakeelb@google.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Amir Goldstein <amir73il@gmail.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 33112315b5c0..5ee74c113381 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -871,6 +871,9 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	tsk->use_memdelay = 0;
 #endif
 
+#ifdef CONFIG_MEMCG
+	tsk->active_memcg = NULL;
+#endif
 	return tsk;
 
 free_stack:

commit 73ba2fb33c492916853dfe63e3b3163da0be661d
Merge: 958f338e96f8 b86d865cb1ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 10:23:25 2018 -0700

    Merge tag 'for-4.19/block-20180812' of git://git.kernel.dk/linux-block
    
    Pull block updates from Jens Axboe:
     "First pull request for this merge window, there will also be a
      followup request with some stragglers.
    
      This pull request contains:
    
       - Fix for a thundering heard issue in the wbt block code (Anchal
         Agarwal)
    
       - A few NVMe pull requests:
          * Improved tracepoints (Keith)
          * Larger inline data support for RDMA (Steve Wise)
          * RDMA setup/teardown fixes (Sagi)
          * Effects log suppor for NVMe target (Chaitanya Kulkarni)
          * Buffered IO suppor for NVMe target (Chaitanya Kulkarni)
          * TP4004 (ANA) support (Christoph)
          * Various NVMe fixes
    
       - Block io-latency controller support. Much needed support for
         properly containing block devices. (Josef)
    
       - Series improving how we handle sense information on the stack
         (Kees)
    
       - Lightnvm fixes and updates/improvements (Mathias/Javier et al)
    
       - Zoned device support for null_blk (Matias)
    
       - AIX partition fixes (Mauricio Faria de Oliveira)
    
       - DIF checksum code made generic (Max Gurtovoy)
    
       - Add support for discard in iostats (Michael Callahan / Tejun)
    
       - Set of updates for BFQ (Paolo)
    
       - Removal of async write support for bsg (Christoph)
    
       - Bio page dirtying and clone fixups (Christoph)
    
       - Set of bcache fix/changes (via Coly)
    
       - Series improving blk-mq queue setup/teardown speed (Ming)
    
       - Series improving merging performance on blk-mq (Ming)
    
       - Lots of other fixes and cleanups from a slew of folks"
    
    * tag 'for-4.19/block-20180812' of git://git.kernel.dk/linux-block: (190 commits)
      blkcg: Make blkg_root_lookup() work for queues in bypass mode
      bcache: fix error setting writeback_rate through sysfs interface
      null_blk: add lock drop/acquire annotation
      Blk-throttle: reduce tail io latency when iops limit is enforced
      block: paride: pd: mark expected switch fall-throughs
      block: Ensure that a request queue is dissociated from the cgroup controller
      block: Introduce blk_exit_queue()
      blkcg: Introduce blkg_root_lookup()
      block: Remove two superfluous #include directives
      blk-mq: count the hctx as active before allocating tag
      block: bvec_nr_vecs() returns value for wrong slab
      bcache: trivial - remove tailing backslash in macro BTREE_FLAG
      bcache: make the pr_err statement used for ENOENT only in sysfs_attatch section
      bcache: set max writeback rate when I/O request is idle
      bcache: add code comments for bset.c
      bcache: fix mistaken comments in request.c
      bcache: fix mistaken code comments in bcache.h
      bcache: add a comment in super.c
      bcache: avoid unncessary cache prefetch bch_btree_node_get()
      bcache: display rate debug parameters to 0 when writeback is not running
      ...

commit 203b4fc903b644223a27ad3f25f3a0f3a3911d1d
Merge: 7edcf0d314f6 765d28f13629
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 13 16:29:35 2018 -0700

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Thomas Gleixner:
    
     - Make lazy TLB mode even lazier to avoid pointless switch_mm()
       operations, which reduces CPU load by 1-2% for memcache workloads
    
     - Small cleanups and improvements all over the place
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86/mm: Remove redundant check for kmem_cache_create()
      arm/asm/tlb.h: Fix build error implicit func declaration
      x86/mm/tlb: Make clear_asid_other() static
      x86/mm/tlb: Skip atomic operations for 'init_mm' in switch_mm_irqs_off()
      x86/mm/tlb: Always use lazy TLB mode
      x86/mm/tlb: Only send page table free TLB flush to lazy TLB CPUs
      x86/mm/tlb: Make lazy TLB mode lazier
      x86/mm/tlb: Restructure switch_mm_irqs_off()
      x86/mm/tlb: Leave lazy TLB mode at page table free time
      mm: Allocate the mm_cpumask (mm->cpu_bitmap[]) dynamically based on nr_cpu_ids
      x86/mm: Add TLB purge to free pmd/pte page interfaces
      ioremap: Update pgtable free interfaces with addr
      x86/mm: Disable ioremap free page handling on x86-PAE

commit c3ad2c3b02e953ead2b8d52a0c9e70312930c3d0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jul 23 15:20:37 2018 -0500

    signal: Don't restart fork when signals come in.
    
    Wen Yang <wen.yang99@zte.com.cn> and majiang <ma.jiang@zte.com.cn>
    report that a periodic signal received during fork can cause fork to
    continually restart preventing an application from making progress.
    
    The code was being overly pessimistic.  Fork needs to guarantee that a
    signal sent to multiple processes is logically delivered before the
    fork and just to the forking process or logically delivered after the
    fork to both the forking process and it's newly spawned child.  For
    signals like periodic timers that are always delivered to a single
    process fork can safely complete and let them appear to logically
    delivered after the fork().
    
    While examining this issue I also discovered that fork today will miss
    signals delivered to multiple processes during the fork and handled by
    another thread.  Similarly the current code will also miss blocked
    signals that are delivered to multiple process, as those signals will
    not appear pending during fork.
    
    Add a list of each thread that is currently forking, and keep on that
    list a signal set that records all of the signals sent to multiple
    processes.  When fork completes initialize the new processes
    shared_pending signal set with it.  The calculate_sigpending function
    will see those signals and set TIF_SIGPENDING causing the new task to
    take the slow path to userspace to handle those signals.  Making it
    appear as if those signals were received immediately after the fork.
    
    It is not possible to send real time signals to multiple processes and
    exceptions don't go to multiple processes, which means that that are
    no signals sent to multiple processes that require siginfo.  This
    means it is safe to not bother collecting siginfo on signals sent
    during fork.
    
    The sigaction of a child of fork is initially the same as the
    sigaction of the parent process.  So a signal the parent ignores the
    child will also initially ignore.  Therefore it is safe to ignore
    signals sent to multiple processes and ignored by the forking process.
    
    Signals sent to only a single process or only a single thread and delivered
    during fork are treated as if they are received after the fork, and generally
    not dealt with.  They won't cause any problems.
    
    V2: Added removal from the multiprocess list on failure.
    V3: Use -ERESTARTNOINTR directly
    V4: - Don't queue both SIGCONT and SIGSTOP
        - Initialize signal_struct.multiprocess in init_task
        - Move setting of shared_pending to before the new task
          is visible to signals.  This prevents signals from comming
          in before shared_pending.signal is set to delayed.signal
          and being lost.
    V5: - rework list add and delete to account for idle threads
    v6: - Use sigdelsetmask when removing stop signals
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=200447
    Reported-by: Wen Yang <wen.yang99@zte.com.cn> and
    Reported-by: majiang <ma.jiang@zte.com.cn>
    Fixes: 4a2c7a7837da ("[PATCH] make fork() atomic wrt pgrp/session signals")
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index ab731e15a600..411e34acace7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1456,6 +1456,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	init_waitqueue_head(&sig->wait_chldexit);
 	sig->curr_target = tsk;
 	init_sigpending(&sig->shared_pending);
+	INIT_HLIST_HEAD(&sig->multiprocess);
 	seqlock_init(&sig->stats_lock);
 	prev_cputime_init(&sig->prev_cputime);
 
@@ -1602,6 +1603,7 @@ static __latent_entropy struct task_struct *copy_process(
 {
 	int retval;
 	struct task_struct *p;
+	struct multiprocess_signals delayed;
 
 	/*
 	 * Don't allow sharing the root directory with processes in a different
@@ -1649,6 +1651,24 @@ static __latent_entropy struct task_struct *copy_process(
 			return ERR_PTR(-EINVAL);
 	}
 
+	/*
+	 * Force any signals received before this point to be delivered
+	 * before the fork happens.  Collect up signals sent to multiple
+	 * processes that happen during the fork and delay them so that
+	 * they appear to happen after the fork.
+	 */
+	sigemptyset(&delayed.signal);
+	INIT_HLIST_NODE(&delayed.node);
+
+	spin_lock_irq(&current->sighand->siglock);
+	if (!(clone_flags & CLONE_THREAD))
+		hlist_add_head(&delayed.node, &current->signal->multiprocess);
+	recalc_sigpending();
+	spin_unlock_irq(&current->sighand->siglock);
+	retval = -ERESTARTNOINTR;
+	if (signal_pending(current))
+		goto fork_out;
+
 	retval = -ENOMEM;
 	p = dup_task_struct(current, node);
 	if (!p)
@@ -1934,22 +1954,6 @@ static __latent_entropy struct task_struct *copy_process(
 		goto bad_fork_cancel_cgroup;
 	}
 
-	if (!(clone_flags & CLONE_THREAD)) {
-		/*
-		 * Process group and session signals need to be delivered to just the
-		 * parent before the fork or both the parent and the child after the
-		 * fork. Restart if a signal comes in before we add the new process to
-		 * it's process group.
-		 * A fatal signal pending means that current will exit, so the new
-		 * thread can't slip out of an OOM kill (or normal SIGKILL).
-		 */
-		recalc_sigpending();
-		if (signal_pending(current)) {
-			retval = -ERESTARTNOINTR;
-			goto bad_fork_cancel_cgroup;
-		}
-	}
-
 
 	init_task_pid_links(p);
 	if (likely(p->pid)) {
@@ -1965,7 +1969,7 @@ static __latent_entropy struct task_struct *copy_process(
 				ns_of_pid(pid)->child_reaper = p;
 				p->signal->flags |= SIGNAL_UNKILLABLE;
 			}
-
+			p->signal->shared_pending.signal = delayed.signal;
 			p->signal->tty = tty_kref_get(current->signal->tty);
 			/*
 			 * Inherit has_child_subreaper flag under the same
@@ -1993,8 +1997,8 @@ static __latent_entropy struct task_struct *copy_process(
 		attach_pid(p, PIDTYPE_PID);
 		nr_threads++;
 	}
-
 	total_forks++;
+	hlist_del_init(&delayed.node);
 	spin_unlock(&current->sighand->siglock);
 	syscall_tracepoint_update(p);
 	write_unlock_irq(&tasklist_lock);
@@ -2059,6 +2063,9 @@ static __latent_entropy struct task_struct *copy_process(
 	put_task_stack(p);
 	free_task(p);
 fork_out:
+	spin_lock_irq(&current->sighand->siglock);
+	hlist_del_init(&delayed.node);
+	spin_unlock_irq(&current->sighand->siglock);
 	return ERR_PTR(retval);
 }
 

commit 05b9ba4b550ff67d7362608828405f9e389e8988
Merge: 704f83928c8e d72e90f33aa4
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sun Aug 5 19:32:09 2018 -0600

    Merge tag 'v4.18-rc6' into for-4.19/block2
    
    Pull in 4.18-rc6 to get the NVMe core AEN change to avoid a
    merge conflict down the line.
    
    Signed-of-by: Jens Axboe <axboe@kernel.dk>

commit 924de3b8c9410c404c6eda7abffd282b97b3ff7f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jul 23 13:38:00 2018 -0500

    fork: Have new threads join on-going signal group stops
    
    There are only two signals that are delivered to every member of a
    signal group: SIGSTOP and SIGKILL.  Signal delivery requires every
    signal appear to be delivered either before or after a clone syscall.
    SIGKILL terminates the clone so does not need to be considered.  Which
    leaves only SIGSTOP that needs to be considered when creating new
    threads.
    
    Today in the event of a group stop TIF_SIGPENDING will get set and the
    fork will restart ensuring the fork syscall participates in the group
    stop.
    
    A fork (especially of a process with a lot of memory) is one of the
    most expensive system so we really only want to restart a fork when
    necessary.
    
    It is easy so check to see if a SIGSTOP is ongoing and have the new
    thread join it immediate after the clone completes.  Making it appear
    the clone completed happened just before the SIGSTOP.
    
    The calculate_sigpending function will see the bits set in jobctl and
    set TIF_SIGPENDING to ensure the new task takes the slow path to userspace.
    
    V2: The call to task_join_group_stop was moved before the new task is
        added to the thread group list.  This should not matter as
        sighand->siglock is held over both the addition of the threads,
        the call to task_join_group_stop and do_signal_stop.  But the change
        is trivial and it is one less thing to worry about when reading
        the code.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 22d4cdb9a7ca..ab731e15a600 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1934,18 +1934,20 @@ static __latent_entropy struct task_struct *copy_process(
 		goto bad_fork_cancel_cgroup;
 	}
 
-	/*
-	 * Process group and session signals need to be delivered to just the
-	 * parent before the fork or both the parent and the child after the
-	 * fork. Restart if a signal comes in before we add the new process to
-	 * it's process group.
-	 * A fatal signal pending means that current will exit, so the new
-	 * thread can't slip out of an OOM kill (or normal SIGKILL).
-	*/
-	recalc_sigpending();
-	if (signal_pending(current)) {
-		retval = -ERESTARTNOINTR;
-		goto bad_fork_cancel_cgroup;
+	if (!(clone_flags & CLONE_THREAD)) {
+		/*
+		 * Process group and session signals need to be delivered to just the
+		 * parent before the fork or both the parent and the child after the
+		 * fork. Restart if a signal comes in before we add the new process to
+		 * it's process group.
+		 * A fatal signal pending means that current will exit, so the new
+		 * thread can't slip out of an OOM kill (or normal SIGKILL).
+		 */
+		recalc_sigpending();
+		if (signal_pending(current)) {
+			retval = -ERESTARTNOINTR;
+			goto bad_fork_cancel_cgroup;
+		}
 	}
 
 
@@ -1982,6 +1984,7 @@ static __latent_entropy struct task_struct *copy_process(
 			current->signal->nr_threads++;
 			atomic_inc(&current->signal->live);
 			atomic_inc(&current->signal->sigcnt);
+			task_join_group_stop(p);
 			list_add_tail_rcu(&p->thread_group,
 					  &p->group_leader->thread_group);
 			list_add_tail_rcu(&p->thread_node,

commit 2c323017e381c55c5ce2a603b8305bb18c1162cc
Author: Josef Bacik <josef@toxicpanda.com>
Date:   Tue Jul 31 12:39:04 2018 -0400

    blk-cgroup: clear the throttle queue on fork
    
    We were hitting a panic in production where we put too many times on the
    request queue.  This is because we'd get the throttle_queue of the
    parent if we fork()'ed while we needed to be throttled, but we didn't
    have a reference on it.  Instead just clear these flags on fork so the
    child doesn't pay for the sins of its father.
    
    Signed-off-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9440d61b925c..694ae0e56866 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -843,6 +843,11 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	tsk->fail_nth = 0;
 #endif
 
+#ifdef CONFIG_BLK_CGROUP
+	tsk->throttle_queue = NULL;
+	tsk->use_memdelay = 0;
+#endif
+
 	return tsk;
 
 free_stack:

commit 027232da7c7c1c7f04383f93bd798e475dde5285
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Jul 26 16:37:25 2018 -0700

    mm: introduce vma_init()
    
    Not all VMAs allocated with vm_area_alloc().  Some of them allocated on
    stack or in data segment.
    
    The new helper can be use to initialize VMA properly regardless where it
    was allocated.
    
    Link: http://lkml.kernel.org/r/20180724121139.62570-2-kirill.shutemov@linux.intel.com
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a191c05e757d..1b27babc4c78 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -312,10 +312,8 @@ struct vm_area_struct *vm_area_alloc(struct mm_struct *mm)
 {
 	struct vm_area_struct *vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 
-	if (vma) {
-		vma->vm_mm = mm;
-		INIT_LIST_HEAD(&vma->anon_vma_chain);
-	}
+	if (vma)
+		vma_init(vma, mm);
 	return vma;
 }
 

commit 7673bf553b2732a00f7644fb2adadda69389ab37
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jul 23 08:01:10 2018 -0500

    fork: Unconditionally exit if a fatal signal is pending
    
    In practice this does not change anything as testing for fatal_signal_pending
    and exiting for with an error code duplicates the work of the next clause
    which recalculates pending signals and then exits fork if any are pending.
    In both cases the pending signal will trigger the slow path when existing
    to userspace, and the fatal signal will cause do_exit to be called.
    
    The advantage of making this a separate test is that it makes it clear
    processing the fatal signal will terminate the fork, and it allows the
    rest of the signal logic to be updated without fear that this important
    case will be lost.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index b9c54318a292..22d4cdb9a7ca 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1928,6 +1928,12 @@ static __latent_entropy struct task_struct *copy_process(
 		goto bad_fork_cancel_cgroup;
 	}
 
+	/* Let kill terminate clone/fork in the middle */
+	if (fatal_signal_pending(current)) {
+		retval = -EINTR;
+		goto bad_fork_cancel_cgroup;
+	}
+
 	/*
 	 * Process group and session signals need to be delivered to just the
 	 * parent before the fork or both the parent and the child after the

commit 4ca1d3ee46130e9b939c02a93e3970dad151fed6
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Jul 13 15:30:33 2018 -0500

    fork: Move and describe why the code examines PIDNS_ADDING
    
    Normally this would be something that would be handled by handling
    signals that are sent to a group of processes but in this case the
    forking process is not a member of the group being signaled.  Thus
    special code is needed to prevent a race with pid namespaces exiting,
    and fork adding new processes within them.
    
    Move this test up before the signal restart just in case signals are
    also pending.  Fatal conditions should take presedence over restarts.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index cc5be0d01ce6..b9c54318a292 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1922,6 +1922,12 @@ static __latent_entropy struct task_struct *copy_process(
 
 	rseq_fork(p, clone_flags);
 
+	/* Don't start children in a dying pid namespace */
+	if (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {
+		retval = -ENOMEM;
+		goto bad_fork_cancel_cgroup;
+	}
+
 	/*
 	 * Process group and session signals need to be delivered to just the
 	 * parent before the fork or both the parent and the child after the
@@ -1935,10 +1941,7 @@ static __latent_entropy struct task_struct *copy_process(
 		retval = -ERESTARTNOINTR;
 		goto bad_fork_cancel_cgroup;
 	}
-	if (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {
-		retval = -ENOMEM;
-		goto bad_fork_cancel_cgroup;
-	}
+
 
 	init_task_pid_links(p);
 	if (likely(p->pid)) {

commit 490fc053865c9cc40f1085ef8a5504f5341f79d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 21 15:24:03 2018 -0700

    mm: make vm_area_alloc() initialize core fields
    
    Like vm_area_dup(), it initializes the anon_vma_chain head, and the
    basic mm pointer.
    
    The rest of the fields end up being different for different users,
    although the plan is to also initialize the 'vm_ops' field to a dummy
    entry.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 67253e41bfb0..a191c05e757d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -308,9 +308,15 @@ static struct kmem_cache *vm_area_cachep;
 /* SLAB cache for mm_struct structures (tsk->mm) */
 static struct kmem_cache *mm_cachep;
 
-struct vm_area_struct *vm_area_alloc(void)
+struct vm_area_struct *vm_area_alloc(struct mm_struct *mm)
 {
-	return kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+	struct vm_area_struct *vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+
+	if (vma) {
+		vma->vm_mm = mm;
+		INIT_LIST_HEAD(&vma->anon_vma_chain);
+	}
+	return vma;
 }
 
 struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig)

commit 95faf6992df468f617edb788da8c21c6eed0dfa7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 21 14:48:45 2018 -0700

    mm: make vm_area_dup() actually copy the old vma data
    
    .. and re-initialize th eanon_vma_chain head.
    
    This removes some boiler-plate from the users, and also makes it clear
    why it didn't need use the 'zalloc()' version.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0e23deb5acfc..67253e41bfb0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -315,7 +315,13 @@ struct vm_area_struct *vm_area_alloc(void)
 
 struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig)
 {
-	return kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+	struct vm_area_struct *new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+
+	if (new) {
+		*new = *orig;
+		INIT_LIST_HEAD(&new->anon_vma_chain);
+	}
+	return new;
 }
 
 void vm_area_free(struct vm_area_struct *vma)
@@ -473,8 +479,6 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 		tmp = vm_area_dup(mpnt);
 		if (!tmp)
 			goto fail_nomem;
-		*tmp = *mpnt;
-		INIT_LIST_HEAD(&tmp->anon_vma_chain);
 		retval = vma_dup_policy(mpnt, tmp);
 		if (retval)
 			goto fail_nomem_policy;

commit 3928d4f5ee37cdc523894f6e549e6aae521d8980
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 21 13:48:51 2018 -0700

    mm: use helper functions for allocating and freeing vm_area structs
    
    The vm_area_struct is one of the most fundamental memory management
    objects, but the management of it is entirely open-coded evertwhere,
    ranging from allocation and freeing (using kmem_cache_[z]alloc and
    kmem_cache_free) to initializing all the fields.
    
    We want to unify this in order to end up having some unified
    initialization of the vmas, and the first step to this is to at least
    have basic allocation functions.
    
    Right now those functions are literally just wrappers around the
    kmem_cache_*() calls.  This is a purely mechanical conversion:
    
        # new vma:
        kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL) -> vm_area_alloc()
    
        # copy old vma
        kmem_cache_alloc(vm_area_cachep, GFP_KERNEL) -> vm_area_dup(old)
    
        # free vma
        kmem_cache_free(vm_area_cachep, vma) -> vm_area_free(vma)
    
    to the point where the old vma passed in to the vm_area_dup() function
    isn't even used yet (because I've left all the old manual initialization
    alone).
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9440d61b925c..0e23deb5acfc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -303,11 +303,26 @@ struct kmem_cache *files_cachep;
 struct kmem_cache *fs_cachep;
 
 /* SLAB cache for vm_area_struct structures */
-struct kmem_cache *vm_area_cachep;
+static struct kmem_cache *vm_area_cachep;
 
 /* SLAB cache for mm_struct structures (tsk->mm) */
 static struct kmem_cache *mm_cachep;
 
+struct vm_area_struct *vm_area_alloc(void)
+{
+	return kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
+}
+
+struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig)
+{
+	return kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+}
+
+void vm_area_free(struct vm_area_struct *vma)
+{
+	kmem_cache_free(vm_area_cachep, vma);
+}
+
 static void account_kernel_stack(struct task_struct *tsk, int account)
 {
 	void *stack = task_stack_page(tsk);
@@ -455,7 +470,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 				goto fail_nomem;
 			charge = len;
 		}
-		tmp = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+		tmp = vm_area_dup(mpnt);
 		if (!tmp)
 			goto fail_nomem;
 		*tmp = *mpnt;
@@ -539,7 +554,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 fail_nomem_anon_vma_fork:
 	mpol_put(vma_policy(tmp));
 fail_nomem_policy:
-	kmem_cache_free(vm_area_cachep, tmp);
+	vm_area_free(tmp);
 fail_nomem:
 	retval = -ENOMEM;
 	vm_unacct_memory(charge);

commit 6883f81aac6f44e7df70a6af189b3689ff52cbfb
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sun Jun 4 04:32:13 2017 -0500

    pid: Implement PIDTYPE_TGID
    
    Everywhere except in the pid array we distinguish between a tasks pid and
    a tasks tgid (thread group id).  Even in the enumeration we want that
    distinction sometimes so we have added __PIDTYPE_TGID.  With leader_pid
    we almost have an implementation of PIDTYPE_TGID in struct signal_struct.
    
    Add PIDTYPE_TGID as a first class member of the pid_type enumeration and
    into the pids array.  Then remove the __PIDTYPE_TGID special case and the
    leader_pid in signal_struct.
    
    The net size increase is just an extra pointer added to struct pid and
    an extra pair of pointers of an hlist_node added to task_struct.
    
    The effect on code maintenance is the removal of a number of special
    cases today and the potential to remove many more special cases as
    PIDTYPE_TGID gets used to it's fullest.  The long term potential
    is allowing zombie thread group leaders to exit, which will remove
    a lot more special cases in the code.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index d2952162399b..cc5be0d01ce6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1946,6 +1946,7 @@ static __latent_entropy struct task_struct *copy_process(
 
 		init_task_pid(p, PIDTYPE_PID, pid);
 		if (thread_group_leader(p)) {
+			init_task_pid(p, PIDTYPE_TGID, pid);
 			init_task_pid(p, PIDTYPE_PGID, task_pgrp(current));
 			init_task_pid(p, PIDTYPE_SID, task_session(current));
 
@@ -1954,7 +1955,6 @@ static __latent_entropy struct task_struct *copy_process(
 				p->signal->flags |= SIGNAL_UNKILLABLE;
 			}
 
-			p->signal->leader_pid = pid;
 			p->signal->tty = tty_kref_get(current->signal->tty);
 			/*
 			 * Inherit has_child_subreaper flag under the same
@@ -1965,6 +1965,7 @@ static __latent_entropy struct task_struct *copy_process(
 							 p->real_parent->signal->is_child_subreaper;
 			list_add_tail(&p->sibling, &p->real_parent->children);
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
+			attach_pid(p, PIDTYPE_TGID);
 			attach_pid(p, PIDTYPE_PGID);
 			attach_pid(p, PIDTYPE_SID);
 			__this_cpu_inc(process_counts);

commit 2c4704756cab7cfa031ada4dab361562f0e357c0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Sep 26 13:06:43 2017 -0500

    pids: Move the pgrp and session pid pointers from task_struct to signal_struct
    
    To access these fields the code always has to go to group leader so
    going to signal struct is no loss and is actually a fundamental simplification.
    
    This saves a little bit of memory by only allocating the pid pointer array
    once instead of once for every thread, and even better this removes a
    few potential races caused by the fact that group_leader can be changed
    by de_thread, while signal_struct can not.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9440d61b925c..d2952162399b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1549,10 +1549,22 @@ static void posix_cpu_timers_init(struct task_struct *tsk)
 static inline void posix_cpu_timers_init(struct task_struct *tsk) { }
 #endif
 
+static inline void init_task_pid_links(struct task_struct *task)
+{
+	enum pid_type type;
+
+	for (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {
+		INIT_HLIST_NODE(&task->pid_links[type]);
+	}
+}
+
 static inline void
 init_task_pid(struct task_struct *task, enum pid_type type, struct pid *pid)
 {
-	 task->pids[type].pid = pid;
+	if (type == PIDTYPE_PID)
+		task->thread_pid = pid;
+	else
+		task->signal->pids[type] = pid;
 }
 
 static inline void rcu_copy_process(struct task_struct *p)
@@ -1928,6 +1940,7 @@ static __latent_entropy struct task_struct *copy_process(
 		goto bad_fork_cancel_cgroup;
 	}
 
+	init_task_pid_links(p);
 	if (likely(p->pid)) {
 		ptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);
 
@@ -2036,13 +2049,13 @@ static __latent_entropy struct task_struct *copy_process(
 	return ERR_PTR(retval);
 }
 
-static inline void init_idle_pids(struct pid_link *links)
+static inline void init_idle_pids(struct task_struct *idle)
 {
 	enum pid_type type;
 
 	for (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {
-		INIT_HLIST_NODE(&links[type].node); /* not really needed */
-		links[type].pid = &init_struct_pid;
+		INIT_HLIST_NODE(&idle->pid_links[type]); /* not really needed */
+		init_task_pid(idle, type, &init_struct_pid);
 	}
 }
 
@@ -2052,7 +2065,7 @@ struct task_struct *fork_idle(int cpu)
 	task = copy_process(CLONE_VM, 0, 0, NULL, &init_struct_pid, 0, 0,
 			    cpu_to_node(cpu));
 	if (!IS_ERR(task)) {
-		init_idle_pids(task->pids);
+		init_idle_pids(task);
 		init_idle(task, cpu);
 	}
 

commit c1a2f7f0c06454387c2cd7b93ff1491c715a8c69
Author: Rik van Riel <riel@surriel.com>
Date:   Mon Jul 16 15:03:31 2018 -0400

    mm: Allocate the mm_cpumask (mm->cpu_bitmap[]) dynamically based on nr_cpu_ids
    
    The mm_struct always contains a cpumask bitmap, regardless of
    CONFIG_CPUMASK_OFFSTACK. That means the first step can be to
    simplify things, and simply have one bitmask at the end of the
    mm_struct for the mm_cpumask.
    
    This does necessitate moving everything else in mm_struct into
    an anonymous sub-structure, which can be randomized when struct
    randomization is enabled.
    
    The second step is to determine the correct size for the
    mm_struct slab object from the size of the mm_struct
    (excluding the CPU bitmap) and the size the cpumask.
    
    For init_mm we can simply allocate the maximum size this
    kernel is compiled for, since we only have one init_mm
    in the system, anyway.
    
    Pointer magic by Mike Galbraith, to evade -Wstringop-overflow
    getting confused by the dynamically sized array.
    
    Tested-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Rik van Riel <riel@surriel.com>
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Rik van Riel <riel@surriel.com>
    Acked-by: Dave Hansen <dave.hansen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-team@fb.com
    Cc: luto@kernel.org
    Link: http://lkml.kernel.org/r/20180716190337.26133-2-riel@surriel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9440d61b925c..5b64c1b8461e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2253,6 +2253,8 @@ static void sighand_ctor(void *data)
 
 void __init proc_caches_init(void)
 {
+	unsigned int mm_size;
+
 	sighand_cachep = kmem_cache_create("sighand_cache",
 			sizeof(struct sighand_struct), 0,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_TYPESAFE_BY_RCU|
@@ -2269,15 +2271,16 @@ void __init proc_caches_init(void)
 			sizeof(struct fs_struct), 0,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,
 			NULL);
+
 	/*
-	 * FIXME! The "sizeof(struct mm_struct)" currently includes the
-	 * whole struct cpumask for the OFFSTACK case. We could change
-	 * this to *only* allocate as much of it as required by the
-	 * maximum number of CPU's we can ever have.  The cpumask_allocation
-	 * is at the end of the structure, exactly for that reason.
+	 * The mm_cpumask is located at the end of mm_struct, and is
+	 * dynamically sized based on the maximum CPU number this system
+	 * can have, taking hotplug into account (nr_cpu_ids).
 	 */
+	mm_size = sizeof(struct mm_struct) + cpumask_size();
+
 	mm_cachep = kmem_cache_create_usercopy("mm_struct",
-			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
+			mm_size, ARCH_MIN_MMSTRUCT_ALIGN,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,
 			offsetof(struct mm_struct, saved_auxv),
 			sizeof_field(struct mm_struct, saved_auxv),

commit 655c79bb40a0870adcd0871057d01de11625882b
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Thu Jun 14 15:26:34 2018 -0700

    mm: check for SIGKILL inside dup_mmap() loop
    
    As a theoretical problem, dup_mmap() of an mm_struct with 60000+ vmas
    can loop while potentially allocating memory, with mm->mmap_sem held for
    write by current thread.  This is bad if current thread was selected as
    an OOM victim, for current thread will continue allocations using memory
    reserves while OOM reaper is unable to reclaim memory.
    
    As an actually observable problem, it is not difficult to make OOM
    reaper unable to reclaim memory if the OOM victim is blocked at
    i_mmap_lock_write() in this loop.  Unfortunately, since nobody can
    explain whether it is safe to use killable wait there, let's check for
    SIGKILL before trying to allocate memory.  Even without an OOM event,
    there is no point with continuing the loop from the beginning if current
    thread is killed.
    
    I tested with debug printk().  This patch should be safe because we
    already fail if security_vm_enough_memory_mm() or
    kmem_cache_alloc(GFP_KERNEL) fails and exit_mmap() handles it.
    
       ***** Aborting dup_mmap() due to SIGKILL *****
       ***** Aborting dup_mmap() due to SIGKILL *****
       ***** Aborting dup_mmap() due to SIGKILL *****
       ***** Aborting dup_mmap() due to SIGKILL *****
       ***** Aborting exit_mmap() due to NULL mmap *****
    
    [akpm@linux-foundation.org: add comment]
    Link: http://lkml.kernel.org/r/201804071938.CDE04681.SOFVQJFtMHOOLF@I-love.SAKURA.ne.jp
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 92870be50bba..9440d61b925c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -440,6 +440,14 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 			continue;
 		}
 		charge = 0;
+		/*
+		 * Don't duplicate many vmas if we've been oom-killed (for
+		 * example)
+		 */
+		if (fatal_signal_pending(current)) {
+			retval = -EINTR;
+			goto out;
+		}
 		if (mpnt->vm_flags & VM_ACCOUNT) {
 			unsigned long len = vma_pages(mpnt);
 

commit 050e9baa9dc9fbd9ce2b27f0056990fc9e0a08a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 14 12:21:18 2018 +0900

    Kbuild: rename CC_STACKPROTECTOR[_STRONG] config variables
    
    The changes to automatically test for working stack protector compiler
    support in the Kconfig files removed the special STACKPROTECTOR_AUTO
    option that picked the strongest stack protector that the compiler
    supported.
    
    That was all a nice cleanup - it makes no sense to have the AUTO case
    now that the Kconfig phase can just determine the compiler support
    directly.
    
    HOWEVER.
    
    It also meant that doing "make oldconfig" would now _disable_ the strong
    stackprotector if you had AUTO enabled, because in a legacy config file,
    the sane stack protector configuration would look like
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      # CONFIG_CC_STACKPROTECTOR_NONE is not set
      # CONFIG_CC_STACKPROTECTOR_REGULAR is not set
      # CONFIG_CC_STACKPROTECTOR_STRONG is not set
      CONFIG_CC_STACKPROTECTOR_AUTO=y
    
    and when you ran this through "make oldconfig" with the Kbuild changes,
    it would ask you about the regular CONFIG_CC_STACKPROTECTOR (that had
    been renamed from CONFIG_CC_STACKPROTECTOR_REGULAR to just
    CONFIG_CC_STACKPROTECTOR), but it would think that the STRONG version
    used to be disabled (because it was really enabled by AUTO), and would
    disable it in the new config, resulting in:
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      CONFIG_CC_HAS_STACKPROTECTOR_NONE=y
      CONFIG_CC_STACKPROTECTOR=y
      # CONFIG_CC_STACKPROTECTOR_STRONG is not set
      CONFIG_CC_HAS_SANE_STACKPROTECTOR=y
    
    That's dangerously subtle - people could suddenly find themselves with
    the weaker stack protector setup without even realizing.
    
    The solution here is to just rename not just the old RECULAR stack
    protector option, but also the strong one.  This does that by just
    removing the CC_ prefix entirely for the user choices, because it really
    is not about the compiler support (the compiler support now instead
    automatially impacts _visibility_ of the options to users).
    
    This results in "make oldconfig" actually asking the user for their
    choice, so that we don't have any silent subtle security model changes.
    The end result would generally look like this:
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      CONFIG_CC_HAS_STACKPROTECTOR_NONE=y
      CONFIG_STACKPROTECTOR=y
      CONFIG_STACKPROTECTOR_STRONG=y
      CONFIG_CC_HAS_SANE_STACKPROTECTOR=y
    
    where the "CC_" versions really are about internal compiler
    infrastructure, not the user selections.
    
    Acked-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 08c6e5e217a0..92870be50bba 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -811,7 +811,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	clear_tsk_need_resched(tsk);
 	set_task_stack_end_magic(tsk);
 
-#ifdef CONFIG_CC_STACKPROTECTOR
+#ifdef CONFIG_STACKPROTECTOR
 	tsk->stack_canary = get_random_canary();
 #endif
 

commit d82991a8688ad128b46db1b42d5d84396487a508
Merge: f4e5b30d809d ccba8b64452b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jun 10 10:17:09 2018 -0700

    Merge branch 'core-rseq-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull restartable sequence support from Thomas Gleixner:
     "The restartable sequences syscall (finally):
    
      After a lot of back and forth discussion and massive delays caused by
      the speculative distraction of maintainers, the core set of
      restartable sequences has finally reached a consensus.
    
      It comes with the basic non disputed core implementation along with
      support for arm, powerpc and x86 and a full set of selftests
    
      It was exposed to linux-next earlier this week, so it does not fully
      comply with the merge window requirements, but there is really no
      point to drag it out for yet another cycle"
    
    * 'core-rseq-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      rseq/selftests: Provide Makefile, scripts, gitignore
      rseq/selftests: Provide parametrized tests
      rseq/selftests: Provide basic percpu ops test
      rseq/selftests: Provide basic test
      rseq/selftests: Provide rseq library
      selftests/lib.mk: Introduce OVERRIDE_TARGETS
      powerpc: Wire up restartable sequences system call
      powerpc: Add syscall detection for restartable sequences
      powerpc: Add support for restartable sequences
      x86: Wire up restartable sequence system call
      x86: Add support for restartable sequences
      arm: Wire up restartable sequences system call
      arm: Add syscall detection for restartable sequences
      arm: Add restartable sequences support
      rseq: Introduce restartable sequences system call
      uapi/headers: Provide types_32_64.h

commit 88aa7cc688d48ddd84558b41d5905a0db9535c4b
Author: Yang Shi <yang.shi@linux.alibaba.com>
Date:   Thu Jun 7 17:05:28 2018 -0700

    mm: introduce arg_lock to protect arg_start|end and env_start|end in mm_struct
    
    mmap_sem is on the hot path of kernel, and it very contended, but it is
    abused too.  It is used to protect arg_start|end and evn_start|end when
    reading /proc/$PID/cmdline and /proc/$PID/environ, but it doesn't make
    sense since those proc files just expect to read 4 values atomically and
    not related to VM, they could be set to arbitrary values by C/R.
    
    And, the mmap_sem contention may cause unexpected issue like below:
    
    INFO: task ps:14018 blocked for more than 120 seconds.
           Tainted: G            E 4.9.79-009.ali3000.alios7.x86_64 #1
     "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this
    message.
     ps              D    0 14018      1 0x00000004
     Call Trace:
       schedule+0x36/0x80
       rwsem_down_read_failed+0xf0/0x150
       call_rwsem_down_read_failed+0x18/0x30
       down_read+0x20/0x40
       proc_pid_cmdline_read+0xd9/0x4e0
       __vfs_read+0x37/0x150
       vfs_read+0x96/0x130
       SyS_read+0x55/0xc0
       entry_SYSCALL_64_fastpath+0x1a/0xc5
    
    Both Alexey Dobriyan and Michal Hocko suggested to use dedicated lock
    for them to mitigate the abuse of mmap_sem.
    
    So, introduce a new spinlock in mm_struct to protect the concurrent
    access to arg_start|end, env_start|end and others, as well as replace
    write map_sem to read to protect the race condition between prctl and
    sys_brk which might break check_data_rlimit(), and makes prctl more
    friendly to other VM operations.
    
    This patch just eliminates the abuse of mmap_sem, but it can't resolve
    the above hung task warning completely since the later
    access_remote_vm() call needs acquire mmap_sem.  The mmap_sem
    scalability issue will be solved in the future.
    
    [yang.shi@linux.alibaba.com: add comment about mmap_sem and arg_lock]
      Link: http://lkml.kernel.org/r/1524077799-80690-1-git-send-email-yang.shi@linux.alibaba.com
    Link: http://lkml.kernel.org/r/1523730291-109696-1-git-send-email-yang.shi@linux.alibaba.com
    Signed-off-by: Yang Shi <yang.shi@linux.alibaba.com>
    Reviewed-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mateusz Guzik <mguzik@redhat.com>
    Cc: Kirill Tkhai <ktkhai@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 80b48a8fb47b..c6d1c1ce9ed7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -899,6 +899,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	mm->pinned_vm = 0;
 	memset(&mm->rss_stat, 0, sizeof(mm->rss_stat));
 	spin_lock_init(&mm->page_table_lock);
+	spin_lock_init(&mm->arg_lock);
 	mm_init_cpumask(mm);
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);

commit 8b5c6a3a49d9ebc7dc288870b9c56c4f946035d8
Merge: 8b70543e9af0 5b71388663c0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 6 16:34:00 2018 -0700

    Merge tag 'audit-pr-20180605' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/audit
    
    Pull audit updates from Paul Moore:
     "Another reasonable chunk of audit changes for v4.18, thirteen patches
      in total.
    
      The thirteen patches can mostly be broken down into one of four
      categories: general bug fixes, accessor functions for audit state
      stored in the task_struct, negative filter matches on executable
      names, and extending the (relatively) new seccomp logging knobs to the
      audit subsystem.
    
      The main driver for the accessor functions from Richard are the
      changes we're working on to associate audit events with containers,
      but I think they have some standalone value too so I figured it would
      be good to get them in now.
    
      The seccomp/audit patches from Tyler apply the seccomp logging
      improvements from a few releases ago to audit's seccomp logging;
      starting with this patchset the changes in
      /proc/sys/kernel/seccomp/actions_logged should apply to both the
      standard kernel logging and audit.
    
      As usual, everything passes the audit-testsuite and it happens to
      merge cleanly with your tree"
    
    [ Heh, except it had trivial merge conflicts with the SELinux tree that
      also came in from Paul   - Linus ]
    
    * tag 'audit-pr-20180605' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/audit:
      audit: Fix wrong task in comparison of session ID
      audit: use existing session info function
      audit: normalize loginuid read access
      audit: use new audit_context access funciton for seccomp_actions_logged
      audit: use inline function to set audit context
      audit: use inline function to get audit context
      audit: convert sessionid unset to a macro
      seccomp: Don't special case audited processes when logging
      seccomp: Audit attempts to modify the actions_logged sysctl
      seccomp: Configurable separator for the actions_logged string
      seccomp: Separate read and write code for actions_logged sysctl
      audit: allow not equal op for audit by executable
      audit: add syscall information to FEATURE_CHANGE records

commit d7822b1e24f2df5df98c76f0e94a5416349ff759
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Sat Jun 2 08:43:54 2018 -0400

    rseq: Introduce restartable sequences system call
    
    Expose a new system call allowing each thread to register one userspace
    memory area to be used as an ABI between kernel and user-space for two
    purposes: user-space restartable sequences and quick access to read the
    current CPU number value from user-space.
    
    * Restartable sequences (per-cpu atomics)
    
    Restartables sequences allow user-space to perform update operations on
    per-cpu data without requiring heavy-weight atomic operations.
    
    The restartable critical sections (percpu atomics) work has been started
    by Paul Turner and Andrew Hunter. It lets the kernel handle restart of
    critical sections. [1] [2] The re-implementation proposed here brings a
    few simplifications to the ABI which facilitates porting to other
    architectures and speeds up the user-space fast path.
    
    Here are benchmarks of various rseq use-cases.
    
    Test hardware:
    
    arm32: ARMv7 Processor rev 4 (v7l) "Cubietruck", 2-core
    x86-64: Intel E5-2630 v3@2.40GHz, 16-core, hyperthreading
    
    The following benchmarks were all performed on a single thread.
    
    * Per-CPU statistic counter increment
    
                    getcpu+atomic (ns/op)    rseq (ns/op)    speedup
    arm32:                344.0                 31.4          11.0
    x86-64:                15.3                  2.0           7.7
    
    * LTTng-UST: write event 32-bit header, 32-bit payload into tracer
                 per-cpu buffer
    
                    getcpu+atomic (ns/op)    rseq (ns/op)    speedup
    arm32:               2502.0                 2250.0         1.1
    x86-64:               117.4                   98.0         1.2
    
    * liburcu percpu: lock-unlock pair, dereference, read/compare word
    
                    getcpu+atomic (ns/op)    rseq (ns/op)    speedup
    arm32:                751.0                 128.5          5.8
    x86-64:                53.4                  28.6          1.9
    
    * jemalloc memory allocator adapted to use rseq
    
    Using rseq with per-cpu memory pools in jemalloc at Facebook (based on
    rseq 2016 implementation):
    
    The production workload response-time has 1-2% gain avg. latency, and
    the P99 overall latency drops by 2-3%.
    
    * Reading the current CPU number
    
    Speeding up reading the current CPU number on which the caller thread is
    running is done by keeping the current CPU number up do date within the
    cpu_id field of the memory area registered by the thread. This is done
    by making scheduler preemption set the TIF_NOTIFY_RESUME flag on the
    current thread. Upon return to user-space, a notify-resume handler
    updates the current CPU value within the registered user-space memory
    area. User-space can then read the current CPU number directly from
    memory.
    
    Keeping the current cpu id in a memory area shared between kernel and
    user-space is an improvement over current mechanisms available to read
    the current CPU number, which has the following benefits over
    alternative approaches:
    
    - 35x speedup on ARM vs system call through glibc
    - 20x speedup on x86 compared to calling glibc, which calls vdso
      executing a "lsl" instruction,
    - 14x speedup on x86 compared to inlined "lsl" instruction,
    - Unlike vdso approaches, this cpu_id value can be read from an inline
      assembly, which makes it a useful building block for restartable
      sequences.
    - The approach of reading the cpu id through memory mapping shared
      between kernel and user-space is portable (e.g. ARM), which is not the
      case for the lsl-based x86 vdso.
    
    On x86, yet another possible approach would be to use the gs segment
    selector to point to user-space per-cpu data. This approach performs
    similarly to the cpu id cache, but it has two disadvantages: it is
    not portable, and it is incompatible with existing applications already
    using the gs segment selector for other purposes.
    
    Benchmarking various approaches for reading the current CPU number:
    
    ARMv7 Processor rev 4 (v7l)
    Machine model: Cubietruck
    - Baseline (empty loop):                                    8.4 ns
    - Read CPU from rseq cpu_id:                               16.7 ns
    - Read CPU from rseq cpu_id (lazy register):               19.8 ns
    - glibc 2.19-0ubuntu6.6 getcpu:                           301.8 ns
    - getcpu system call:                                     234.9 ns
    
    x86-64 Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz:
    - Baseline (empty loop):                                    0.8 ns
    - Read CPU from rseq cpu_id:                                0.8 ns
    - Read CPU from rseq cpu_id (lazy register):                0.8 ns
    - Read using gs segment selector:                           0.8 ns
    - "lsl" inline assembly:                                   13.0 ns
    - glibc 2.19-0ubuntu6 getcpu:                              16.6 ns
    - getcpu system call:                                      53.9 ns
    
    - Speed (benchmark taken on v8 of patchset)
    
    Running 10 runs of hackbench -l 100000 seems to indicate, contrary to
    expectations, that enabling CONFIG_RSEQ slightly accelerates the
    scheduler:
    
    Configuration: 2 sockets * 8-core Intel(R) Xeon(R) CPU E5-2630 v3 @
    2.40GHz (directly on hardware, hyperthreading disabled in BIOS, energy
    saving disabled in BIOS, turboboost disabled in BIOS, cpuidle.off=1
    kernel parameter), with a Linux v4.6 defconfig+localyesconfig,
    restartable sequences series applied.
    
    * CONFIG_RSEQ=n
    
    avg.:      41.37 s
    std.dev.:   0.36 s
    
    * CONFIG_RSEQ=y
    
    avg.:      40.46 s
    std.dev.:   0.33 s
    
    - Size
    
    On x86-64, between CONFIG_RSEQ=n/y, the text size increase of vmlinux is
    567 bytes, and the data size increase of vmlinux is 5696 bytes.
    
    [1] https://lwn.net/Articles/650333/
    [2] http://www.linuxplumbersconf.org/2013/ocw/system/presentations/1695/original/LPC%20-%20PerCpu%20Atomics.pdf
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Watson <davejwatson@fb.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Chris Lameter <cl@linux.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Andrew Hunter <ahh@google.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ben Maurer <bmaurer@fb.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: linux-api@vger.kernel.org
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20151027235635.16059.11630.stgit@pjt-glaptop.roam.corp.google.com
    Link: http://lkml.kernel.org/r/20150624222609.6116.86035.stgit@kitami.mtv.corp.google.com
    Link: https://lkml.kernel.org/r/20180602124408.8430-3-mathieu.desnoyers@efficios.com

diff --git a/kernel/fork.c b/kernel/fork.c
index a5d21c42acfc..70992bfeba81 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1899,6 +1899,8 @@ static __latent_entropy struct task_struct *copy_process(
 	 */
 	copy_seccomp(p);
 
+	rseq_fork(p, clone_flags);
+
 	/*
 	 * Process group and session signals need to be delivered to just the
 	 * parent before the fork or both the parent and the child after the

commit c0b0ae8a871bc2ebbe1ff9c9871efcf88994ffec
Author: Richard Guy Briggs <rgb@redhat.com>
Date:   Sat May 12 21:58:21 2018 -0400

    audit: use inline function to set audit context
    
    Recognizing that the audit context is an internal audit value, use an
    access function to set the audit context pointer for the task
    rather than reaching directly into the task struct to set it.
    
    Signed-off-by: Richard Guy Briggs <rgb@redhat.com>
    [PM: merge fuzz in audit.h]
    Signed-off-by: Paul Moore <paul@paul-moore.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 242c8c93d285..cd18448b025a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1713,7 +1713,7 @@ static __latent_entropy struct task_struct *copy_process(
 	p->start_time = ktime_get_ns();
 	p->real_start_time = ktime_get_boot_ns();
 	p->io_context = NULL;
-	p->audit_context = NULL;
+	audit_set_context(p, NULL);
 	cgroup_fork(p);
 #ifdef CONFIG_NUMA
 	p->mempolicy = mpol_dup(p->mempolicy);

commit e01e80634ecdde1dd113ac43b3adad21b47f3957
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Apr 20 14:55:31 2018 -0700

    fork: unconditionally clear stack on fork
    
    One of the classes of kernel stack content leaks[1] is exposing the
    contents of prior heap or stack contents when a new process stack is
    allocated.  Normally, those stacks are not zeroed, and the old contents
    remain in place.  In the face of stack content exposure flaws, those
    contents can leak to userspace.
    
    Fixing this will make the kernel no longer vulnerable to these flaws, as
    the stack will be wiped each time a stack is assigned to a new process.
    There's not a meaningful change in runtime performance; it almost looks
    like it provides a benefit.
    
    Performing back-to-back kernel builds before:
            Run times: 157.86 157.09 158.90 160.94 160.80
            Mean: 159.12
            Std Dev: 1.54
    
    and after:
            Run times: 159.31 157.34 156.71 158.15 160.81
            Mean: 158.46
            Std Dev: 1.46
    
    Instead of making this a build or runtime config, Andy Lutomirski
    recommended this just be enabled by default.
    
    [1] A noisy search for many kinds of stack content leaks can be seen here:
    https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=linux+kernel+stack+leak
    
    I did some more with perf and cycle counts on running 100,000 execs of
    /bin/true.
    
    before:
    Cycles: 218858861551 218853036130 214727610969 227656844122 224980542841
    Mean:  221015379122.60
    Std Dev: 4662486552.47
    
    after:
    Cycles: 213868945060 213119275204 211820169456 224426673259 225489986348
    Mean:  217745009865.40
    Std Dev: 5935559279.99
    
    It continues to look like it's faster, though the deviation is rather
    wide, but I'm not sure what I could do that would be less noisy.  I'm
    open to ideas!
    
    Link: http://lkml.kernel.org/r/20180221021659.GA37073@beast
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Rasmus Villemoes <rasmus.villemoes@prevas.dk>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 242c8c93d285..a5d21c42acfc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -216,10 +216,9 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 		if (!s)
 			continue;
 
-#ifdef CONFIG_DEBUG_KMEMLEAK
 		/* Clear stale pointers from reused stack. */
 		memset(s->addr, 0, THREAD_SIZE);
-#endif
+
 		tsk->stack_vm_area = s;
 		return s->addr;
 	}

commit 3eda69c92d4751977baf2d34e88a29d4b6affa7d
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Apr 5 16:25:12 2018 -0700

    kernel/fork.c: detect early free of a live mm
    
    KASAN splats indicate that in some cases we free a live mm, then
    continue to access it, with potentially disastrous results.  This is
    likely due to a mismatched mmdrop() somewhere in the kernel, but so far
    the culprit remains elusive.
    
    Let's have __mmdrop() verify that the mm isn't live for the current
    task, similar to the existing check for init_mm.  This way, we can catch
    this class of issue earlier, and without requiring KASAN.
    
    Currently, idle_task_exit() leaves active_mm stale after it switches to
    init_mm.  This isn't harmful, but will trigger the new assertions, so we
    must adjust idle_task_exit() to update active_mm.
    
    Link: http://lkml.kernel.org/r/20180312140103.19235-1-mark.rutland@arm.com
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f71b67dc156d..242c8c93d285 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -595,6 +595,8 @@ static void check_mm(struct mm_struct *mm)
 void __mmdrop(struct mm_struct *mm)
 {
 	BUG_ON(mm == &init_mm);
+	WARN_ON_ONCE(mm == current->mm);
+	WARN_ON_ONCE(mm == current->active_mm);
 	mm_free_pgd(mm);
 	destroy_context(mm);
 	hmm_mm_destroy(mm);

commit 9b32105ec6b13d32d5db6a6e7992c97ce54b5ea7
Author: Dominik Brodowski <linux@dominikbrodowski.net>
Date:   Sun Mar 11 11:34:42 2018 +0100

    kernel: add ksys_unshare() helper; remove in-kernel calls to sys_unshare()
    
    Using this helper allows us to avoid the in-kernel calls to the
    sys_unshare() syscall. The ksys_ prefix denotes that this function is meant
    as a drop-in replacement for the syscall. In particular, it uses the same
    calling convention as sys_unshare().
    
    This patch is part of a series which removes in-kernel calls to syscalls.
    On this basis, the syscall entry path can be streamlined. For details, see
    http://lkml.kernel.org/r/20180325162527.GA17492@light.dominikbrodowski.net
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Dominik Brodowski <linux@dominikbrodowski.net>

diff --git a/kernel/fork.c b/kernel/fork.c
index b1e031aac9db..f71b67dc156d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2354,7 +2354,7 @@ static int unshare_fd(unsigned long unshare_flags, struct files_struct **new_fdp
  * constructed. Here we are modifying the current, active,
  * task_struct.
  */
-SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
+int ksys_unshare(unsigned long unshare_flags)
 {
 	struct fs_struct *fs, *new_fs = NULL;
 	struct files_struct *fd, *new_fd = NULL;
@@ -2470,6 +2470,11 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 	return err;
 }
 
+SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
+{
+	return ksys_unshare(unshare_flags);
+}
+
 /*
  *	Helper to unshare the files of the current task.
  *	We don't want to expose copy_files internals to

commit 2de0db992de189fccc83fed57c30875144821491
Author: Dominik Brodowski <linux@dominikbrodowski.net>
Date:   Sun Mar 11 11:34:26 2018 +0100

    mm: use do_futex() instead of sys_futex() in mm_release()
    
    sys_futex() is a wrapper to do_futex() which does not modify any
    values here:
    
    - uaddr, val and val3 are kept the same
    
    - op is masked with FUTEX_CMD_MASK, but is always set to FUTEX_WAKE.
      Therefore, val2 is always 0.
    
    - as utime is set to NULL, *timeout is NULL
    
    This patch is part of a series which removes in-kernel calls to syscalls.
    On this basis, the syscall entry path can be streamlined. For details, see
    http://lkml.kernel.org/r/20180325162527.GA17492@light.dominikbrodowski.net
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Darren Hart <dvhart@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Dominik Brodowski <linux@dominikbrodowski.net>

diff --git a/kernel/fork.c b/kernel/fork.c
index e5d9d405ae4e..b1e031aac9db 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1198,8 +1198,8 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 			 * not set up a proper pointer then tough luck.
 			 */
 			put_user(0, tsk->clear_child_tid);
-			sys_futex(tsk->clear_child_tid, FUTEX_WAKE,
-					1, NULL, NULL, 0);
+			do_futex(tsk->clear_child_tid, FUTEX_WAKE,
+					1, NULL, NULL, 0, 0);
 		}
 		tsk->clear_child_tid = NULL;
 	}

commit d34bc48f8275b6ce0da44f639d68344891268ee9
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Wed Feb 21 14:45:17 2018 -0800

    include/linux/sched/mm.h: re-inline mmdrop()
    
    As Peter points out, Doing a CALL+RET for just the decrement is a bit silly.
    
    Fixes: d70f2a14b72a4bc ("include/linux/sched/mm.h: uninline mmdrop_async(), etc")
    Acked-by: Peter Zijlstra (Intel) <peterz@infraded.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index be8aa5b98666..e5d9d405ae4e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -592,7 +592,7 @@ static void check_mm(struct mm_struct *mm)
  * is dropped: either by a lazy thread or by
  * mmput. Free the page directory and the mm.
  */
-static void __mmdrop(struct mm_struct *mm)
+void __mmdrop(struct mm_struct *mm)
 {
 	BUG_ON(mm == &init_mm);
 	mm_free_pgd(mm);
@@ -603,18 +603,7 @@ static void __mmdrop(struct mm_struct *mm)
 	put_user_ns(mm->user_ns);
 	free_mm(mm);
 }
-
-void mmdrop(struct mm_struct *mm)
-{
-	/*
-	 * The implicit full barrier implied by atomic_dec_and_test() is
-	 * required by the membarrier system call before returning to
-	 * user-space, after storing to rq->curr.
-	 */
-	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
-		__mmdrop(mm);
-}
-EXPORT_SYMBOL_GPL(mmdrop);
+EXPORT_SYMBOL_GPL(__mmdrop);
 
 static void mmdrop_async_fn(struct work_struct *work)
 {

commit a2e5790d841658485d642196dbb0927303d6c22f
Merge: ab2d92ad881d 60c3e026d73c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 6 22:15:42 2018 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge misc updates from Andrew Morton:
    
     - kasan updates
    
     - procfs
    
     - lib/bitmap updates
    
     - other lib/ updates
    
     - checkpatch tweaks
    
     - rapidio
    
     - ubsan
    
     - pipe fixes and cleanups
    
     - lots of other misc bits
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (114 commits)
      Documentation/sysctl/user.txt: fix typo
      MAINTAINERS: update ARM/QUALCOMM SUPPORT patterns
      MAINTAINERS: update various PALM patterns
      MAINTAINERS: update "ARM/OXNAS platform support" patterns
      MAINTAINERS: update Cortina/Gemini patterns
      MAINTAINERS: remove ARM/CLKDEV SUPPORT file pattern
      MAINTAINERS: remove ANDROID ION pattern
      mm: docs: add blank lines to silence sphinx "Unexpected indentation" errors
      mm: docs: fix parameter names mismatch
      mm: docs: fixup punctuation
      pipe: read buffer limits atomically
      pipe: simplify round_pipe_size()
      pipe: reject F_SETPIPE_SZ with size over UINT_MAX
      pipe: fix off-by-one error when checking buffer limits
      pipe: actually allow root to exceed the pipe buffer limits
      pipe, sysctl: remove pipe_proc_fn()
      pipe, sysctl: drop 'min' parameter from pipe-max-size converter
      kasan: rework Kconfig settings
      crash_dump: is_kdump_kernel can be boolean
      kernel/mutex: mutex_is_locked can be boolean
      ...

commit 667b60946ef9b8375085c7b225229923c6f87308
Author: Marcos Paulo de Souza <marcos.souza.org@gmail.com>
Date:   Tue Feb 6 15:39:34 2018 -0800

    kernel/fork.c: add comment about usage of CLONE_FS flags and namespaces
    
    All other places that deals with namespaces have an explanation of why
    the restriction is there.
    
    The description added in this commit was based on commit e66eded8309e
    ("userns: Don't allow CLONE_NEWUSER | CLONE_FS").
    
    Link: http://lkml.kernel.org/r/20171112151637.13258-1-marcos.souza.org@gmail.com
    Signed-off-by: Marcos Paulo de Souza <marcos.souza.org@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0d62524c6660..b9d857fe2a5c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1587,6 +1587,10 @@ static __latent_entropy struct task_struct *copy_process(
 	int retval;
 	struct task_struct *p;
 
+	/*
+	 * Don't allow sharing the root directory with processes in a different
+	 * namespace
+	 */
 	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
 		return ERR_PTR(-EINVAL);
 

commit 9f5325aa37279d724e064d32a95e13231c0ade23
Author: Marcos Paulo de Souza <marcos.souza.org@gmail.com>
Date:   Tue Feb 6 15:39:30 2018 -0800

    kernel/fork.c: check error and return early
    
    Thus reducing one indentation level while maintaining the same rationale.
    
    Link: http://lkml.kernel.org/r/20171117002929.5155-1-marcos.souza.org@gmail.com
    Signed-off-by: Marcos Paulo de Souza <marcos.souza.org@gmail.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5c372c954f3b..0d62524c6660 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2062,6 +2062,8 @@ long _do_fork(unsigned long clone_flags,
 	      int __user *child_tidptr,
 	      unsigned long tls)
 {
+	struct completion vfork;
+	struct pid *pid;
 	struct task_struct *p;
 	int trace = 0;
 	long nr;
@@ -2087,43 +2089,40 @@ long _do_fork(unsigned long clone_flags,
 	p = copy_process(clone_flags, stack_start, stack_size,
 			 child_tidptr, NULL, trace, tls, NUMA_NO_NODE);
 	add_latent_entropy();
+
+	if (IS_ERR(p))
+		return PTR_ERR(p);
+
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.
 	 */
-	if (!IS_ERR(p)) {
-		struct completion vfork;
-		struct pid *pid;
-
-		trace_sched_process_fork(current, p);
+	trace_sched_process_fork(current, p);
 
-		pid = get_task_pid(p, PIDTYPE_PID);
-		nr = pid_vnr(pid);
+	pid = get_task_pid(p, PIDTYPE_PID);
+	nr = pid_vnr(pid);
 
-		if (clone_flags & CLONE_PARENT_SETTID)
-			put_user(nr, parent_tidptr);
-
-		if (clone_flags & CLONE_VFORK) {
-			p->vfork_done = &vfork;
-			init_completion(&vfork);
-			get_task_struct(p);
-		}
+	if (clone_flags & CLONE_PARENT_SETTID)
+		put_user(nr, parent_tidptr);
 
-		wake_up_new_task(p);
+	if (clone_flags & CLONE_VFORK) {
+		p->vfork_done = &vfork;
+		init_completion(&vfork);
+		get_task_struct(p);
+	}
 
-		/* forking complete and child started to run, tell ptracer */
-		if (unlikely(trace))
-			ptrace_event_pid(trace, pid);
+	wake_up_new_task(p);
 
-		if (clone_flags & CLONE_VFORK) {
-			if (!wait_for_vfork_done(p, &vfork))
-				ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);
-		}
+	/* forking complete and child started to run, tell ptracer */
+	if (unlikely(trace))
+		ptrace_event_pid(trace, pid);
 
-		put_pid(pid);
-	} else {
-		nr = PTR_ERR(p);
+	if (clone_flags & CLONE_VFORK) {
+		if (!wait_for_vfork_done(p, &vfork))
+			ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);
 	}
+
+	put_pid(pid);
 	return nr;
 }
 

commit 82845079160817cc6ac64e5321bbd935e0a47b3a
Merge: 32e839dda3ba 68c5735eaa5e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Feb 6 21:12:31 2018 +0100

    Merge branch 'linus' into sched/urgent, to resolve conflicts
    
     Conflicts:
            arch/arm64/kernel/entry.S
            arch/x86/Kconfig
            include/linux/sched/mm.h
            kernel/fork.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 617aebe6a97efa539cc4b8a52adccd89596e6be0
Merge: 0771ad44a20b e47e311843de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 3 16:25:42 2018 -0800

    Merge tag 'usercopy-v4.16-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull hardened usercopy whitelisting from Kees Cook:
     "Currently, hardened usercopy performs dynamic bounds checking on slab
      cache objects. This is good, but still leaves a lot of kernel memory
      available to be copied to/from userspace in the face of bugs.
    
      To further restrict what memory is available for copying, this creates
      a way to whitelist specific areas of a given slab cache object for
      copying to/from userspace, allowing much finer granularity of access
      control.
    
      Slab caches that are never exposed to userspace can declare no
      whitelist for their objects, thereby keeping them unavailable to
      userspace via dynamic copy operations. (Note, an implicit form of
      whitelisting is the use of constant sizes in usercopy operations and
      get_user()/put_user(); these bypass all hardened usercopy checks since
      these sizes cannot change at runtime.)
    
      This new check is WARN-by-default, so any mistakes can be found over
      the next several releases without breaking anyone's system.
    
      The series has roughly the following sections:
       - remove %p and improve reporting with offset
       - prepare infrastructure and whitelist kmalloc
       - update VFS subsystem with whitelists
       - update SCSI subsystem with whitelists
       - update network subsystem with whitelists
       - update process memory with whitelists
       - update per-architecture thread_struct with whitelists
       - update KVM with whitelists and fix ioctl bug
       - mark all other allocations as not whitelisted
       - update lkdtm for more sensible test overage"
    
    * tag 'usercopy-v4.16-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux: (38 commits)
      lkdtm: Update usercopy tests for whitelisting
      usercopy: Restrict non-usercopy caches to size 0
      kvm: x86: fix KVM_XEN_HVM_CONFIG ioctl
      kvm: whitelist struct kvm_vcpu_arch
      arm: Implement thread_struct whitelist for hardened usercopy
      arm64: Implement thread_struct whitelist for hardened usercopy
      x86: Implement thread_struct whitelist for hardened usercopy
      fork: Provide usercopy whitelisting for task_struct
      fork: Define usercopy region in thread_stack slab caches
      fork: Define usercopy region in mm_struct slab caches
      net: Restrict unwhitelisted proto caches to size 0
      sctp: Copy struct sctp_sock.autoclose to userspace using put_user()
      sctp: Define usercopy region in SCTP proto slab cache
      caif: Define usercopy region in caif proto slab cache
      ip: Define usercopy region in IP proto slab cache
      net: Define usercopy region in struct proto slab cache
      scsi: Define usercopy region in scsi_sense_cache slab cache
      cifs: Define usercopy region in cifs_request slab cache
      vxfs: Define usercopy region in vxfs_inode slab cache
      ufs: Define usercopy region in ufs_inode_cache slab cache
      ...

commit d70f2a14b72a4bc094cf3a92e4794644a7adc590
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Wed Jan 31 16:15:51 2018 -0800

    include/linux/sched/mm.h: uninline mmdrop_async(), etc
    
    mmdrop_async() is only used in fork.c.  Move that and its support
    functions into fork.c, uninline it all.
    
    Quite a lot of code gets moved around to avoid forward declarations.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2295fc69717f..5e6cf0dd031c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -77,6 +77,7 @@
 #include <linux/blkdev.h>
 #include <linux/fs_struct.h>
 #include <linux/magic.h>
+#include <linux/sched/mm.h>
 #include <linux/perf_event.h>
 #include <linux/posix-timers.h>
 #include <linux/user-return-notifier.h>
@@ -390,6 +391,241 @@ void free_task(struct task_struct *tsk)
 }
 EXPORT_SYMBOL(free_task);
 
+#ifdef CONFIG_MMU
+static __latent_entropy int dup_mmap(struct mm_struct *mm,
+					struct mm_struct *oldmm)
+{
+	struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
+	struct rb_node **rb_link, *rb_parent;
+	int retval;
+	unsigned long charge;
+	LIST_HEAD(uf);
+
+	uprobe_start_dup_mmap();
+	if (down_write_killable(&oldmm->mmap_sem)) {
+		retval = -EINTR;
+		goto fail_uprobe_end;
+	}
+	flush_cache_dup_mm(oldmm);
+	uprobe_dup_mmap(oldmm, mm);
+	/*
+	 * Not linked in yet - no deadlock potential:
+	 */
+	down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);
+
+	/* No ordering required: file already has been exposed. */
+	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
+
+	mm->total_vm = oldmm->total_vm;
+	mm->data_vm = oldmm->data_vm;
+	mm->exec_vm = oldmm->exec_vm;
+	mm->stack_vm = oldmm->stack_vm;
+
+	rb_link = &mm->mm_rb.rb_node;
+	rb_parent = NULL;
+	pprev = &mm->mmap;
+	retval = ksm_fork(mm, oldmm);
+	if (retval)
+		goto out;
+	retval = khugepaged_fork(mm, oldmm);
+	if (retval)
+		goto out;
+
+	prev = NULL;
+	for (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {
+		struct file *file;
+
+		if (mpnt->vm_flags & VM_DONTCOPY) {
+			vm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));
+			continue;
+		}
+		charge = 0;
+		if (mpnt->vm_flags & VM_ACCOUNT) {
+			unsigned long len = vma_pages(mpnt);
+
+			if (security_vm_enough_memory_mm(oldmm, len)) /* sic */
+				goto fail_nomem;
+			charge = len;
+		}
+		tmp = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
+		if (!tmp)
+			goto fail_nomem;
+		*tmp = *mpnt;
+		INIT_LIST_HEAD(&tmp->anon_vma_chain);
+		retval = vma_dup_policy(mpnt, tmp);
+		if (retval)
+			goto fail_nomem_policy;
+		tmp->vm_mm = mm;
+		retval = dup_userfaultfd(tmp, &uf);
+		if (retval)
+			goto fail_nomem_anon_vma_fork;
+		if (tmp->vm_flags & VM_WIPEONFORK) {
+			/* VM_WIPEONFORK gets a clean slate in the child. */
+			tmp->anon_vma = NULL;
+			if (anon_vma_prepare(tmp))
+				goto fail_nomem_anon_vma_fork;
+		} else if (anon_vma_fork(tmp, mpnt))
+			goto fail_nomem_anon_vma_fork;
+		tmp->vm_flags &= ~(VM_LOCKED | VM_LOCKONFAULT);
+		tmp->vm_next = tmp->vm_prev = NULL;
+		file = tmp->vm_file;
+		if (file) {
+			struct inode *inode = file_inode(file);
+			struct address_space *mapping = file->f_mapping;
+
+			get_file(file);
+			if (tmp->vm_flags & VM_DENYWRITE)
+				atomic_dec(&inode->i_writecount);
+			i_mmap_lock_write(mapping);
+			if (tmp->vm_flags & VM_SHARED)
+				atomic_inc(&mapping->i_mmap_writable);
+			flush_dcache_mmap_lock(mapping);
+			/* insert tmp into the share list, just after mpnt */
+			vma_interval_tree_insert_after(tmp, mpnt,
+					&mapping->i_mmap);
+			flush_dcache_mmap_unlock(mapping);
+			i_mmap_unlock_write(mapping);
+		}
+
+		/*
+		 * Clear hugetlb-related page reserves for children. This only
+		 * affects MAP_PRIVATE mappings. Faults generated by the child
+		 * are not guaranteed to succeed, even if read-only
+		 */
+		if (is_vm_hugetlb_page(tmp))
+			reset_vma_resv_huge_pages(tmp);
+
+		/*
+		 * Link in the new vma and copy the page table entries.
+		 */
+		*pprev = tmp;
+		pprev = &tmp->vm_next;
+		tmp->vm_prev = prev;
+		prev = tmp;
+
+		__vma_link_rb(mm, tmp, rb_link, rb_parent);
+		rb_link = &tmp->vm_rb.rb_right;
+		rb_parent = &tmp->vm_rb;
+
+		mm->map_count++;
+		if (!(tmp->vm_flags & VM_WIPEONFORK))
+			retval = copy_page_range(mm, oldmm, mpnt);
+
+		if (tmp->vm_ops && tmp->vm_ops->open)
+			tmp->vm_ops->open(tmp);
+
+		if (retval)
+			goto out;
+	}
+	/* a new mm has just been created */
+	arch_dup_mmap(oldmm, mm);
+	retval = 0;
+out:
+	up_write(&mm->mmap_sem);
+	flush_tlb_mm(oldmm);
+	up_write(&oldmm->mmap_sem);
+	dup_userfaultfd_complete(&uf);
+fail_uprobe_end:
+	uprobe_end_dup_mmap();
+	return retval;
+fail_nomem_anon_vma_fork:
+	mpol_put(vma_policy(tmp));
+fail_nomem_policy:
+	kmem_cache_free(vm_area_cachep, tmp);
+fail_nomem:
+	retval = -ENOMEM;
+	vm_unacct_memory(charge);
+	goto out;
+}
+
+static inline int mm_alloc_pgd(struct mm_struct *mm)
+{
+	mm->pgd = pgd_alloc(mm);
+	if (unlikely(!mm->pgd))
+		return -ENOMEM;
+	return 0;
+}
+
+static inline void mm_free_pgd(struct mm_struct *mm)
+{
+	pgd_free(mm, mm->pgd);
+}
+#else
+static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
+{
+	down_write(&oldmm->mmap_sem);
+	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
+	up_write(&oldmm->mmap_sem);
+	return 0;
+}
+#define mm_alloc_pgd(mm)	(0)
+#define mm_free_pgd(mm)
+#endif /* CONFIG_MMU */
+
+static void check_mm(struct mm_struct *mm)
+{
+	int i;
+
+	for (i = 0; i < NR_MM_COUNTERS; i++) {
+		long x = atomic_long_read(&mm->rss_stat.count[i]);
+
+		if (unlikely(x))
+			printk(KERN_ALERT "BUG: Bad rss-counter state "
+					  "mm:%p idx:%d val:%ld\n", mm, i, x);
+	}
+
+	if (mm_pgtables_bytes(mm))
+		pr_alert("BUG: non-zero pgtables_bytes on freeing mm: %ld\n",
+				mm_pgtables_bytes(mm));
+
+#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
+	VM_BUG_ON_MM(mm->pmd_huge_pte, mm);
+#endif
+}
+
+#define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
+#define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
+
+/*
+ * Called when the last reference to the mm
+ * is dropped: either by a lazy thread or by
+ * mmput. Free the page directory and the mm.
+ */
+static void __mmdrop(struct mm_struct *mm)
+{
+	BUG_ON(mm == &init_mm);
+	mm_free_pgd(mm);
+	destroy_context(mm);
+	hmm_mm_destroy(mm);
+	mmu_notifier_mm_destroy(mm);
+	check_mm(mm);
+	put_user_ns(mm->user_ns);
+	free_mm(mm);
+}
+
+void mmdrop(struct mm_struct *mm)
+{
+	if (unlikely(atomic_dec_and_test(&mm->mm_count)))
+		__mmdrop(mm);
+}
+EXPORT_SYMBOL_GPL(mmdrop);
+
+static void mmdrop_async_fn(struct work_struct *work)
+{
+	struct mm_struct *mm;
+
+	mm = container_of(work, struct mm_struct, async_put_work);
+	__mmdrop(mm);
+}
+
+static void mmdrop_async(struct mm_struct *mm)
+{
+	if (unlikely(atomic_dec_and_test(&mm->mm_count))) {
+		INIT_WORK(&mm->async_put_work, mmdrop_async_fn);
+		schedule_work(&mm->async_put_work);
+	}
+}
+
 static inline void free_signal_struct(struct signal_struct *sig)
 {
 	taskstats_tgid_free(sig);
@@ -594,181 +830,8 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	return NULL;
 }
 
-#ifdef CONFIG_MMU
-static __latent_entropy int dup_mmap(struct mm_struct *mm,
-					struct mm_struct *oldmm)
-{
-	struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
-	struct rb_node **rb_link, *rb_parent;
-	int retval;
-	unsigned long charge;
-	LIST_HEAD(uf);
-
-	uprobe_start_dup_mmap();
-	if (down_write_killable(&oldmm->mmap_sem)) {
-		retval = -EINTR;
-		goto fail_uprobe_end;
-	}
-	flush_cache_dup_mm(oldmm);
-	uprobe_dup_mmap(oldmm, mm);
-	/*
-	 * Not linked in yet - no deadlock potential:
-	 */
-	down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);
-
-	/* No ordering required: file already has been exposed. */
-	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
-
-	mm->total_vm = oldmm->total_vm;
-	mm->data_vm = oldmm->data_vm;
-	mm->exec_vm = oldmm->exec_vm;
-	mm->stack_vm = oldmm->stack_vm;
-
-	rb_link = &mm->mm_rb.rb_node;
-	rb_parent = NULL;
-	pprev = &mm->mmap;
-	retval = ksm_fork(mm, oldmm);
-	if (retval)
-		goto out;
-	retval = khugepaged_fork(mm, oldmm);
-	if (retval)
-		goto out;
-
-	prev = NULL;
-	for (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {
-		struct file *file;
-
-		if (mpnt->vm_flags & VM_DONTCOPY) {
-			vm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));
-			continue;
-		}
-		charge = 0;
-		if (mpnt->vm_flags & VM_ACCOUNT) {
-			unsigned long len = vma_pages(mpnt);
-
-			if (security_vm_enough_memory_mm(oldmm, len)) /* sic */
-				goto fail_nomem;
-			charge = len;
-		}
-		tmp = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
-		if (!tmp)
-			goto fail_nomem;
-		*tmp = *mpnt;
-		INIT_LIST_HEAD(&tmp->anon_vma_chain);
-		retval = vma_dup_policy(mpnt, tmp);
-		if (retval)
-			goto fail_nomem_policy;
-		tmp->vm_mm = mm;
-		retval = dup_userfaultfd(tmp, &uf);
-		if (retval)
-			goto fail_nomem_anon_vma_fork;
-		if (tmp->vm_flags & VM_WIPEONFORK) {
-			/* VM_WIPEONFORK gets a clean slate in the child. */
-			tmp->anon_vma = NULL;
-			if (anon_vma_prepare(tmp))
-				goto fail_nomem_anon_vma_fork;
-		} else if (anon_vma_fork(tmp, mpnt))
-			goto fail_nomem_anon_vma_fork;
-		tmp->vm_flags &= ~(VM_LOCKED | VM_LOCKONFAULT);
-		tmp->vm_next = tmp->vm_prev = NULL;
-		file = tmp->vm_file;
-		if (file) {
-			struct inode *inode = file_inode(file);
-			struct address_space *mapping = file->f_mapping;
-
-			get_file(file);
-			if (tmp->vm_flags & VM_DENYWRITE)
-				atomic_dec(&inode->i_writecount);
-			i_mmap_lock_write(mapping);
-			if (tmp->vm_flags & VM_SHARED)
-				atomic_inc(&mapping->i_mmap_writable);
-			flush_dcache_mmap_lock(mapping);
-			/* insert tmp into the share list, just after mpnt */
-			vma_interval_tree_insert_after(tmp, mpnt,
-					&mapping->i_mmap);
-			flush_dcache_mmap_unlock(mapping);
-			i_mmap_unlock_write(mapping);
-		}
-
-		/*
-		 * Clear hugetlb-related page reserves for children. This only
-		 * affects MAP_PRIVATE mappings. Faults generated by the child
-		 * are not guaranteed to succeed, even if read-only
-		 */
-		if (is_vm_hugetlb_page(tmp))
-			reset_vma_resv_huge_pages(tmp);
-
-		/*
-		 * Link in the new vma and copy the page table entries.
-		 */
-		*pprev = tmp;
-		pprev = &tmp->vm_next;
-		tmp->vm_prev = prev;
-		prev = tmp;
-
-		__vma_link_rb(mm, tmp, rb_link, rb_parent);
-		rb_link = &tmp->vm_rb.rb_right;
-		rb_parent = &tmp->vm_rb;
-
-		mm->map_count++;
-		if (!(tmp->vm_flags & VM_WIPEONFORK))
-			retval = copy_page_range(mm, oldmm, mpnt);
-
-		if (tmp->vm_ops && tmp->vm_ops->open)
-			tmp->vm_ops->open(tmp);
-
-		if (retval)
-			goto out;
-	}
-	/* a new mm has just been created */
-	retval = arch_dup_mmap(oldmm, mm);
-out:
-	up_write(&mm->mmap_sem);
-	flush_tlb_mm(oldmm);
-	up_write(&oldmm->mmap_sem);
-	dup_userfaultfd_complete(&uf);
-fail_uprobe_end:
-	uprobe_end_dup_mmap();
-	return retval;
-fail_nomem_anon_vma_fork:
-	mpol_put(vma_policy(tmp));
-fail_nomem_policy:
-	kmem_cache_free(vm_area_cachep, tmp);
-fail_nomem:
-	retval = -ENOMEM;
-	vm_unacct_memory(charge);
-	goto out;
-}
-
-static inline int mm_alloc_pgd(struct mm_struct *mm)
-{
-	mm->pgd = pgd_alloc(mm);
-	if (unlikely(!mm->pgd))
-		return -ENOMEM;
-	return 0;
-}
-
-static inline void mm_free_pgd(struct mm_struct *mm)
-{
-	pgd_free(mm, mm->pgd);
-}
-#else
-static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
-{
-	down_write(&oldmm->mmap_sem);
-	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
-	up_write(&oldmm->mmap_sem);
-	return 0;
-}
-#define mm_alloc_pgd(mm)	(0)
-#define mm_free_pgd(mm)
-#endif /* CONFIG_MMU */
-
 __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
 
-#define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
-#define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
-
 static unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;
 
 static int __init coredump_filter_setup(char *s)
@@ -858,27 +921,6 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	return NULL;
 }
 
-static void check_mm(struct mm_struct *mm)
-{
-	int i;
-
-	for (i = 0; i < NR_MM_COUNTERS; i++) {
-		long x = atomic_long_read(&mm->rss_stat.count[i]);
-
-		if (unlikely(x))
-			printk(KERN_ALERT "BUG: Bad rss-counter state "
-					  "mm:%p idx:%d val:%ld\n", mm, i, x);
-	}
-
-	if (mm_pgtables_bytes(mm))
-		pr_alert("BUG: non-zero pgtables_bytes on freeing mm: %ld\n",
-				mm_pgtables_bytes(mm));
-
-#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
-	VM_BUG_ON_MM(mm->pmd_huge_pte, mm);
-#endif
-}
-
 /*
  * Allocate and initialize an mm_struct.
  */
@@ -894,24 +936,6 @@ struct mm_struct *mm_alloc(void)
 	return mm_init(mm, current, current_user_ns());
 }
 
-/*
- * Called when the last reference to the mm
- * is dropped: either by a lazy thread or by
- * mmput. Free the page directory and the mm.
- */
-void __mmdrop(struct mm_struct *mm)
-{
-	BUG_ON(mm == &init_mm);
-	mm_free_pgd(mm);
-	destroy_context(mm);
-	hmm_mm_destroy(mm);
-	mmu_notifier_mm_destroy(mm);
-	check_mm(mm);
-	put_user_ns(mm->user_ns);
-	free_mm(mm);
-}
-EXPORT_SYMBOL_GPL(__mmdrop);
-
 static inline void __mmput(struct mm_struct *mm)
 {
 	VM_BUG_ON(atomic_read(&mm->mm_users));

commit 5905429ad85657c28d93ec3d826ddeea1f44c3ce
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Aug 16 13:00:58 2017 -0700

    fork: Provide usercopy whitelisting for task_struct
    
    While the blocked and saved_sigmask fields of task_struct are copied to
    userspace (via sigmask_to_save() and setup_rt_frame()), it is always
    copied with a static length (i.e. sizeof(sigset_t)).
    
    The only portion of task_struct that is potentially dynamically sized and
    may be copied to userspace is in the architecture-specific thread_struct
    at the end of task_struct.
    
    cache object allocation:
        kernel/fork.c:
            alloc_task_struct_node(...):
                return kmem_cache_alloc_node(task_struct_cachep, ...);
    
            dup_task_struct(...):
                ...
                tsk = alloc_task_struct_node(node);
    
            copy_process(...):
                ...
                dup_task_struct(...)
    
            _do_fork(...):
                ...
                copy_process(...)
    
    example usage trace:
    
        arch/x86/kernel/fpu/signal.c:
            __fpu__restore_sig(...):
                ...
                struct task_struct *tsk = current;
                struct fpu *fpu = &tsk->thread.fpu;
                ...
                __copy_from_user(&fpu->state.xsave, ..., state_size);
    
            fpu__restore_sig(...):
                ...
                return __fpu__restore_sig(...);
    
        arch/x86/kernel/signal.c:
            restore_sigcontext(...):
                ...
                fpu__restore_sig(...)
    
    This introduces arch_thread_struct_whitelist() to let an architecture
    declare specifically where the whitelist should be within thread_struct.
    If undefined, the entire thread_struct field is left whitelisted.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: "Mickaël Salaün" <mic@digikod.net>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Rik van Riel <riel@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0e086af148f2..5977e691c754 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -458,6 +458,21 @@ static void set_max_threads(unsigned int max_threads_suggested)
 int arch_task_struct_size __read_mostly;
 #endif
 
+static void task_struct_whitelist(unsigned long *offset, unsigned long *size)
+{
+	/* Fetch thread_struct whitelist for the architecture. */
+	arch_thread_struct_whitelist(offset, size);
+
+	/*
+	 * Handle zero-sized whitelist or empty thread_struct, otherwise
+	 * adjust offset to position of thread_struct in task_struct.
+	 */
+	if (unlikely(*size == 0))
+		*offset = 0;
+	else
+		*offset += offsetof(struct task_struct, thread);
+}
+
 void __init fork_init(void)
 {
 	int i;
@@ -466,11 +481,14 @@ void __init fork_init(void)
 #define ARCH_MIN_TASKALIGN	0
 #endif
 	int align = max_t(int, L1_CACHE_BYTES, ARCH_MIN_TASKALIGN);
+	unsigned long useroffset, usersize;
 
 	/* create a slab on which task_structs can be allocated */
-	task_struct_cachep = kmem_cache_create("task_struct",
+	task_struct_whitelist(&useroffset, &usersize);
+	task_struct_cachep = kmem_cache_create_usercopy("task_struct",
 			arch_task_struct_size, align,
-			SLAB_PANIC|SLAB_ACCOUNT, NULL);
+			SLAB_PANIC|SLAB_ACCOUNT,
+			useroffset, usersize, NULL);
 #endif
 
 	/* do the arch specific task caches init */

commit f9d29946c56734e954459bc9a0e688a8ae9b4cbf
Author: David Windsor <dave@nullcore.net>
Date:   Sat Jun 10 22:50:41 2017 -0400

    fork: Define usercopy region in thread_stack slab caches
    
    In support of usercopy hardening, this patch defines a region in the
    thread_stack slab caches in which userspace copy operations are allowed.
    Since the entire thread_stack needs to be available to userspace, the
    entire slab contents are whitelisted. Note that the slab-based thread
    stack is only present on systems with THREAD_SIZE < PAGE_SIZE and
    !CONFIG_VMAP_STACK.
    
    cache object allocation:
        kernel/fork.c:
            alloc_thread_stack_node(...):
                return kmem_cache_alloc_node(thread_stack_cache, ...)
    
            dup_task_struct(...):
                ...
                stack = alloc_thread_stack_node(...)
                ...
                tsk->stack = stack;
    
            copy_process(...):
                ...
                dup_task_struct(...)
    
            _do_fork(...):
                ...
                copy_process(...)
    
    This region is known as the slab cache's usercopy region. Slab caches
    can now check that each dynamically sized copy operation involving
    cache-managed memory falls entirely within the slab's usercopy region.
    
    This patch is modified from Brad Spengler/PaX Team's PAX_USERCOPY
    whitelisting code in the last public patch of grsecurity/PaX based on my
    understanding of the code. Changes or omissions from the original code are
    mine and don't reflect the original grsecurity/PaX code.
    
    Signed-off-by: David Windsor <dave@nullcore.net>
    [kees: adjust commit log, split patch, provide usage trace]
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Rik van Riel <riel@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 82f2a0441d3b..0e086af148f2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -282,8 +282,9 @@ static void free_thread_stack(struct task_struct *tsk)
 
 void thread_stack_cache_init(void)
 {
-	thread_stack_cache = kmem_cache_create("thread_stack", THREAD_SIZE,
-					      THREAD_SIZE, 0, NULL);
+	thread_stack_cache = kmem_cache_create_usercopy("thread_stack",
+					THREAD_SIZE, THREAD_SIZE, 0, 0,
+					THREAD_SIZE, NULL);
 	BUG_ON(thread_stack_cache == NULL);
 }
 # endif

commit 07dcd7fe89938934ddad65f738bc5aac89b8e54d
Author: David Windsor <dave@nullcore.net>
Date:   Tue Aug 15 16:45:00 2017 -0700

    fork: Define usercopy region in mm_struct slab caches
    
    In support of usercopy hardening, this patch defines a region in the
    mm_struct slab caches in which userspace copy operations are allowed.
    Only the auxv field is copied to userspace.
    
    cache object allocation:
        kernel/fork.c:
            #define allocate_mm()     (kmem_cache_alloc(mm_cachep, GFP_KERNEL))
    
            dup_mm():
                ...
                mm = allocate_mm();
    
            copy_mm(...):
                ...
                dup_mm();
    
            copy_process(...):
                ...
                copy_mm(...)
    
            _do_fork(...):
                ...
                copy_process(...)
    
    example usage trace:
    
        fs/binfmt_elf.c:
            create_elf_tables(...):
                ...
                elf_info = (elf_addr_t *)current->mm->saved_auxv;
                ...
                copy_to_user(..., elf_info, ei_index * sizeof(elf_addr_t))
    
            load_elf_binary(...):
                ...
                create_elf_tables(...);
    
    This region is known as the slab cache's usercopy region. Slab caches
    can now check that each dynamically sized copy operation involving
    cache-managed memory falls entirely within the slab's usercopy region.
    
    This patch is modified from Brad Spengler/PaX Team's PAX_USERCOPY
    whitelisting code in the last public patch of grsecurity/PaX based on my
    understanding of the code. Changes or omissions from the original code are
    mine and don't reflect the original grsecurity/PaX code.
    
    Signed-off-by: David Windsor <dave@nullcore.net>
    [kees: adjust commit log, split patch, provide usage trace]
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Rik van Riel <riel@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 432eadf6b58c..82f2a0441d3b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2225,9 +2225,11 @@ void __init proc_caches_init(void)
 	 * maximum number of CPU's we can ever have.  The cpumask_allocation
 	 * is at the end of the structure, exactly for that reason.
 	 */
-	mm_cachep = kmem_cache_create("mm_struct",
+	mm_cachep = kmem_cache_create_usercopy("mm_struct",
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,
+			offsetof(struct mm_struct, saved_auxv),
+			sizeof_field(struct mm_struct, saved_auxv),
 			NULL);
 	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC|SLAB_ACCOUNT);
 	mmap_init();

commit caf9a82657b313106aae8f4a35936c116a152299
Merge: 9c294ec08408 f6c4fd506cb6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 23 11:53:04 2017 -0800

    Merge branch 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 PTI preparatory patches from Thomas Gleixner:
     "Todays Advent calendar window contains twentyfour easy to digest
      patches. The original plan was to have twenty three matching the date,
      but a late fixup made that moot.
    
       - Move the cpu_entry_area mapping out of the fixmap into a separate
         address space. That's necessary because the fixmap becomes too big
         with NRCPUS=8192 and this caused already subtle and hard to
         diagnose failures.
    
         The top most patch is fresh from today and cures a brain slip of
         that tall grumpy german greybeard, who ignored the intricacies of
         32bit wraparounds.
    
       - Limit the number of CPUs on 32bit to 64. That's insane big already,
         but at least it's small enough to prevent address space issues with
         the cpu_entry_area map, which have been observed and debugged with
         the fixmap code
    
       - A few TLB flush fixes in various places plus documentation which of
         the TLB functions should be used for what.
    
       - Rename the SYSENTER stack to CPU_ENTRY_AREA stack as it is used for
         more than sysenter now and keeping the name makes backtraces
         confusing.
    
       - Prevent LDT inheritance on exec() by moving it to arch_dup_mmap(),
         which is only invoked on fork().
    
       - Make vysycall more robust.
    
       - A few fixes and cleanups of the debug_pagetables code. Check
         PAGE_PRESENT instead of checking the PTE for 0 and a cleanup of the
         C89 initialization of the address hint array which already was out
         of sync with the index enums.
    
       - Move the ESPFIX init to a different place to prepare for PTI.
    
       - Several code moves with no functional change to make PTI
         integration simpler and header files less convoluted.
    
       - Documentation fixes and clarifications"
    
    * 'x86-pti-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      x86/cpu_entry_area: Prevent wraparound in setup_cpu_entry_area_ptes() on 32bit
      init: Invoke init_espfix_bsp() from mm_init()
      x86/cpu_entry_area: Move it out of the fixmap
      x86/cpu_entry_area: Move it to a separate unit
      x86/mm: Create asm/invpcid.h
      x86/mm: Put MMU to hardware ASID translation in one place
      x86/mm: Remove hard-coded ASID limit checks
      x86/mm: Move the CR3 construction functions to tlbflush.h
      x86/mm: Add comments to clarify which TLB-flush functions are supposed to flush what
      x86/mm: Remove superfluous barriers
      x86/mm: Use __flush_tlb_one() for kernel memory
      x86/microcode: Dont abuse the TLB-flush interface
      x86/uv: Use the right TLB-flush API
      x86/entry: Rename SYSENTER_stack to CPU_ENTRY_AREA_entry_stack
      x86/doc: Remove obvious weirdnesses from the x86 MM layout documentation
      x86/mm/64: Improve the memory map documentation
      x86/ldt: Prevent LDT inheritance on exec
      x86/ldt: Rework locking
      arch, mm: Allow arch_dup_mmap() to fail
      x86/vsyscall/64: Warn and fail vsyscall emulation in NATIVE mode
      ...

commit c10e83f598d08046dd1ebc8360d4bb12d802d51b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 14 12:27:29 2017 +0100

    arch, mm: Allow arch_dup_mmap() to fail
    
    In order to sanitize the LDT initialization on x86 arch_dup_mmap() must be
    allowed to fail. Fix up all instances.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirsky <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: dan.j.williams@intel.com
    Cc: hughd@google.com
    Cc: keescook@google.com
    Cc: kirill.shutemov@linux.intel.com
    Cc: linux-mm@kvack.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 07cc743698d3..500ce64517d9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -721,8 +721,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 			goto out;
 	}
 	/* a new mm has just been created */
-	arch_dup_mmap(oldmm, mm);
-	retval = 0;
+	retval = arch_dup_mmap(oldmm, mm);
 out:
 	up_write(&mm->mmap_sem);
 	flush_tlb_mm(oldmm);

commit e8cfbc245e24887e3c30235f71e9e9405e0cfc39
Author: Gargi Sharma <gs051095@gmail.com>
Date:   Fri Nov 17 15:30:34 2017 -0800

    pid: remove pidhash
    
    pidhash is no longer required as all the information can be looked up
    from idr tree.  nr_hashed represented the number of pids that had been
    hashed.  Since, nr_hashed and PIDNS_HASH_ADDING are no longer relevant,
    it has been renamed to pid_allocated and PIDNS_ADDING respectively.
    
    [gs051095@gmail.com: v6]
      Link: http://lkml.kernel.org/r/1507760379-21662-3-git-send-email-gs051095@gmail.com
    Link: http://lkml.kernel.org/r/1507583624-22146-3-git-send-email-gs051095@gmail.com
    Signed-off-by: Gargi Sharma <gs051095@gmail.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>      [ia64]
    Cc: Julia Lawall <julia.lawall@lip6.fr>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Pavel Tatashin <pasha.tatashin@oracle.com>
    Cc: Kirill Tkhai <ktkhai@virtuozzo.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4e55eedba8d6..432eadf6b58c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1871,7 +1871,7 @@ static __latent_entropy struct task_struct *copy_process(
 		retval = -ERESTARTNOINTR;
 		goto bad_fork_cancel_cgroup;
 	}
-	if (unlikely(!(ns_of_pid(pid)->nr_hashed & PIDNS_HASH_ADDING))) {
+	if (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {
 		retval = -ENOMEM;
 		goto bad_fork_cancel_cgroup;
 	}

commit 75f296d93bcebcfe375884ddac79e30263a31766
Author: Levin, Alexander (Sasha Levin) <alexander.levin@verizon.com>
Date:   Wed Nov 15 17:35:54 2017 -0800

    kmemcheck: stop using GFP_NOTRACK and SLAB_NOTRACK
    
    Convert all allocations that used a NOTRACK flag to stop using it.
    
    Link: http://lkml.kernel.org/r/20171007030159.22241-3-alexander.levin@verizon.com
    Signed-off-by: Sasha Levin <alexander.levin@verizon.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tim Hansen <devtimhansen@gmail.com>
    Cc: Vegard Nossum <vegardno@ifi.uio.no>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 006dc5899a1a..4e55eedba8d6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -469,7 +469,7 @@ void __init fork_init(void)
 	/* create a slab on which task_structs can be allocated */
 	task_struct_cachep = kmem_cache_create("task_struct",
 			arch_task_struct_size, align,
-			SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT, NULL);
+			SLAB_PANIC|SLAB_ACCOUNT, NULL);
 #endif
 
 	/* do the arch specific task caches init */
@@ -2205,18 +2205,18 @@ void __init proc_caches_init(void)
 	sighand_cachep = kmem_cache_create("sighand_cache",
 			sizeof(struct sighand_struct), 0,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_TYPESAFE_BY_RCU|
-			SLAB_NOTRACK|SLAB_ACCOUNT, sighand_ctor);
+			SLAB_ACCOUNT, sighand_ctor);
 	signal_cachep = kmem_cache_create("signal_cache",
 			sizeof(struct signal_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,
 			NULL);
 	files_cachep = kmem_cache_create("files_cache",
 			sizeof(struct files_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,
 			NULL);
 	fs_cachep = kmem_cache_create("fs_cache",
 			sizeof(struct fs_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,
 			NULL);
 	/*
 	 * FIXME! The "sizeof(struct mm_struct)" currently includes the
@@ -2227,7 +2227,7 @@ void __init proc_caches_init(void)
 	 */
 	mm_cachep = kmem_cache_create("mm_struct",
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT,
 			NULL);
 	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC|SLAB_ACCOUNT);
 	mmap_init();

commit af5b0f6a09e42c9f4fa87735f2a366748767b686
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Nov 15 17:35:40 2017 -0800

    mm: consolidate page table accounting
    
    Currently, we account page tables separately for each page table level,
    but that's redundant -- we only make use of total memory allocated to
    page tables for oom_badness calculation.  We also provide the
    information to userspace, but it has dubious value there too.
    
    This patch switches page table accounting to single counter.
    
    mm->pgtables_bytes is now used to account all page table levels.  We use
    bytes, because page table size for different levels of page table tree
    may be different.
    
    The change has user-visible effect: we don't have VmPMD and VmPUD
    reported in /proc/[pid]/status.  Not sure if anybody uses them.  (As
    alternative, we can always report 0 kB for them.)
    
    OOM-killer report is also slightly changed: we now report pgtables_bytes
    instead of nr_ptes, nr_pmd, nr_puds.
    
    Apart from reducing number of counters per-mm, the benefit is that we
    now calculate oom_badness() more correctly for machines which have
    different size of page tables depending on level or where page tables
    are less than a page in size.
    
    The only downside can be debuggability because we do not know which page
    table level could leak.  But I do not remember many bugs that would be
    caught by separate counters so I wouldn't lose sleep over this.
    
    [akpm@linux-foundation.org: fix mm/huge_memory.c]
    Link: http://lkml.kernel.org/r/20171006100651.44742-2-kirill.shutemov@linux.intel.com
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    [kirill.shutemov@linux.intel.com: fix build]
      Link: http://lkml.kernel.org/r/20171016150113.ikfxy3e7zzfvsr4w@black.fi.intel.com
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 946922a30ede..006dc5899a1a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -817,9 +817,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	init_rwsem(&mm->mmap_sem);
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->core_state = NULL;
-	mm_nr_ptes_init(mm);
-	mm_nr_pmds_init(mm);
-	mm_nr_puds_init(mm);
+	mm_pgtables_bytes_init(mm);
 	mm->map_count = 0;
 	mm->locked_vm = 0;
 	mm->pinned_vm = 0;
@@ -873,15 +871,9 @@ static void check_mm(struct mm_struct *mm)
 					  "mm:%p idx:%d val:%ld\n", mm, i, x);
 	}
 
-	if (mm_nr_ptes(mm))
-		pr_alert("BUG: non-zero nr_ptes on freeing mm: %ld\n",
-				mm_nr_ptes(mm));
-	if (mm_nr_pmds(mm))
-		pr_alert("BUG: non-zero nr_pmds on freeing mm: %ld\n",
-				mm_nr_pmds(mm));
-	if (mm_nr_puds(mm))
-		pr_alert("BUG: non-zero nr_puds on freeing mm: %ld\n",
-				mm_nr_puds(mm));
+	if (mm_pgtables_bytes(mm))
+		pr_alert("BUG: non-zero pgtables_bytes on freeing mm: %ld\n",
+				mm_pgtables_bytes(mm));
 
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	VM_BUG_ON_MM(mm->pmd_huge_pte, mm);

commit c4812909f5d5a9b7f1c85a2d95be388a066cda52
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Nov 15 17:35:37 2017 -0800

    mm: introduce wrappers to access mm->nr_ptes
    
    Let's add wrappers for ->nr_ptes with the same interface as for nr_pmd
    and nr_pud.
    
    The patch also makes nr_ptes accounting dependent onto CONFIG_MMU.  Page
    table accounting doesn't make sense if you don't have page tables.
    
    It's preparation for consolidation of page-table counters in mm_struct.
    
    Link: http://lkml.kernel.org/r/20171006100651.44742-1-kirill.shutemov@linux.intel.com
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a4eb6f289365..946922a30ede 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -817,7 +817,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	init_rwsem(&mm->mmap_sem);
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->core_state = NULL;
-	atomic_long_set(&mm->nr_ptes, 0);
+	mm_nr_ptes_init(mm);
 	mm_nr_pmds_init(mm);
 	mm_nr_puds_init(mm);
 	mm->map_count = 0;
@@ -873,9 +873,9 @@ static void check_mm(struct mm_struct *mm)
 					  "mm:%p idx:%d val:%ld\n", mm, i, x);
 	}
 
-	if (atomic_long_read(&mm->nr_ptes))
+	if (mm_nr_ptes(mm))
 		pr_alert("BUG: non-zero nr_ptes on freeing mm: %ld\n",
-				atomic_long_read(&mm->nr_ptes));
+				mm_nr_ptes(mm));
 	if (mm_nr_pmds(mm))
 		pr_alert("BUG: non-zero nr_pmds on freeing mm: %ld\n",
 				mm_nr_pmds(mm));

commit b4e98d9ac775907cc53fb08fcb6776deb7694e30
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Nov 15 17:35:33 2017 -0800

    mm: account pud page tables
    
    On a machine with 5-level paging support a process can allocate
    significant amount of memory and stay unnoticed by oom-killer and memory
    cgroup.  The trick is to allocate a lot of PUD page tables.  We don't
    account PUD page tables, only PMD and PTE.
    
    We already addressed the same issue for PMD page tables, see commit
    dc6c9a35b66b ("mm: account pmd page tables to the process").
    Introduction of 5-level paging brings the same issue for PUD page
    tables.
    
    The patch expands accounting to PUD level.
    
    [kirill.shutemov@linux.intel.com: s/pmd_t/pud_t/]
      Link: http://lkml.kernel.org/r/20171004074305.x35eh5u7ybbt5kar@black.fi.intel.com
    [heiko.carstens@de.ibm.com: s390/mm: fix pud table accounting]
      Link: http://lkml.kernel.org/r/20171103090551.18231-1-heiko.carstens@de.ibm.com
    Link: http://lkml.kernel.org/r/20171002080427.3320-1-kirill.shutemov@linux.intel.com
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 07cc743698d3..a4eb6f289365 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -819,6 +819,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	mm->core_state = NULL;
 	atomic_long_set(&mm->nr_ptes, 0);
 	mm_nr_pmds_init(mm);
+	mm_nr_puds_init(mm);
 	mm->map_count = 0;
 	mm->locked_vm = 0;
 	mm->pinned_vm = 0;
@@ -878,6 +879,9 @@ static void check_mm(struct mm_struct *mm)
 	if (mm_nr_pmds(mm))
 		pr_alert("BUG: non-zero nr_pmds on freeing mm: %ld\n",
 				mm_nr_pmds(mm));
+	if (mm_nr_puds(mm))
+		pr_alert("BUG: non-zero nr_puds on freeing mm: %ld\n",
+				mm_nr_puds(mm));
 
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	VM_BUG_ON_MM(mm->pmd_huge_pte, mm);

commit ca182551857cc2c1e6a2b7f1e72090a137a15008
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Fri Oct 13 15:58:22 2017 -0700

    kmemleak: clear stale pointers from task stacks
    
    Kmemleak considers any pointers on task stacks as references.  This
    patch clears newly allocated and reused vmap stacks.
    
    Link: http://lkml.kernel.org/r/150728990124.744199.8403409836394318684.stgit@buzz
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e702cb9ffbd8..07cc743698d3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -215,6 +215,10 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 		if (!s)
 			continue;
 
+#ifdef CONFIG_DEBUG_KMEMLEAK
+		/* Clear stale pointers from reused stack. */
+		memset(s->addr, 0, THREAD_SIZE);
+#endif
 		tsk->stack_vm_area = s;
 		return s->addr;
 	}

commit a1b2289cef92ef0e9a92afcd2e1ea71d5bcaaf64
Author: Sherry Yang <sherryy@android.com>
Date:   Tue Oct 3 16:15:00 2017 -0700

    android: binder: drop lru lock in isolate callback
    
    Drop the global lru lock in isolate callback before calling
    zap_page_range which calls cond_resched, and re-acquire the global lru
    lock before returning.  Also change return code to LRU_REMOVED_RETRY.
    
    Use mmput_async when fail to acquire mmap sem in an atomic context.
    
    Fix "BUG: sleeping function called from invalid context"
    errors when CONFIG_DEBUG_ATOMIC_SLEEP is enabled.
    
    Also restore mmput_async, which was initially introduced in commit
    ec8d7c14ea14 ("mm, oom_reaper: do not mmput synchronously from the oom
    reaper context"), and was removed in commit 212925802454 ("mm: oom: let
    oom_reap_task and exit_mmap run concurrently").
    
    Link: http://lkml.kernel.org/r/20170914182231.90908-1-sherryy@android.com
    Fixes: f2517eb76f1f2 ("android: binder: Add global lru shrinker to binder")
    Signed-off-by: Sherry Yang <sherryy@android.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reported-by: Kyle Yan <kyan@codeaurora.org>
    Acked-by: Arve Hjønnevåg <arve@android.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Martijn Coenen <maco@google.com>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Riley Andrews <riandrews@android.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Hoeun Ryu <hoeun.ryu@gmail.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Vegard Nossum <vegard.nossum@oracle.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 10646182440f..e702cb9ffbd8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -946,6 +946,24 @@ void mmput(struct mm_struct *mm)
 }
 EXPORT_SYMBOL_GPL(mmput);
 
+#ifdef CONFIG_MMU
+static void mmput_async_fn(struct work_struct *work)
+{
+	struct mm_struct *mm = container_of(work, struct mm_struct,
+					    async_put_work);
+
+	__mmput(mm);
+}
+
+void mmput_async(struct mm_struct *mm)
+{
+	if (atomic_dec_and_test(&mm->mm_users)) {
+		INIT_WORK(&mm->async_put_work, mmput_async_fn);
+		schedule_work(&mm->async_put_work);
+	}
+}
+#endif
+
 /**
  * set_mm_exe_file - change a reference to the mm's executable file
  *

commit 7f85565a3f7194b966de71926471d69788b6b9c3
Merge: 680352bda57e 0c3014f22dec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 12 13:21:00 2017 -0700

    Merge tag 'selinux-pr-20170831' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/selinux
    
    Pull selinux updates from Paul Moore:
     "A relatively quiet period for SELinux, 11 patches with only two/three
      having any substantive changes.
    
      These noteworthy changes include another tweak to the NNP/nosuid
      handling, per-file labeling for cgroups, and an object class fix for
      AF_UNIX/SOCK_RAW sockets; the rest of the changes are minor tweaks or
      administrative updates (Stephen's email update explains the file
      explosion in the diffstat).
    
      Everything passes the selinux-testsuite"
    
    [ Also a couple of small patches from the security tree from Tetsuo
      Handa for Tomoyo and LSM cleanup. The separation of security policy
      updates wasn't all that clean - Linus ]
    
    * tag 'selinux-pr-20170831' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/selinux:
      selinux: constify nf_hook_ops
      selinux: allow per-file labeling for cgroupfs
      lsm_audit: update my email address
      selinux: update my email address
      MAINTAINERS: update the NetLabel and Labeled Networking information
      selinux: use GFP_NOWAIT in the AVC kmem_caches
      selinux: Generalize support for NNP/nosuid SELinux domain transitions
      selinux: genheaders should fail if too many permissions are defined
      selinux: update the selinux info in MAINTAINERS
      credits: update Paul Moore's info
      selinux: Assign proper class to PF_UNIX/SOCK_RAW sockets
      tomoyo: Update URLs in Documentation/admin-guide/LSM/tomoyo.rst
      LSM: Remove security_task_create() hook.

commit a23ba907d5e65d6aeea3e59c82fda9cd206a7aad
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Sep 8 16:15:01 2017 -0700

    locking/rtmutex: replace top-waiter and pi_waiters leftmost caching
    
    ... with the generic rbtree flavor instead. No changes
    in semantics whatsoever.
    
    Link: http://lkml.kernel.org/r/20170719014603.19029-10-dave@stgolabs.net
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2ccbbbfcb7b8..6f1b0af00bda 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1462,8 +1462,7 @@ static void rt_mutex_init_task(struct task_struct *p)
 {
 	raw_spin_lock_init(&p->pi_lock);
 #ifdef CONFIG_RT_MUTEXES
-	p->pi_waiters = RB_ROOT;
-	p->pi_waiters_leftmost = NULL;
+	p->pi_waiters = RB_ROOT_CACHED;
 	p->pi_top_task = NULL;
 	p->pi_blocked_on = NULL;
 #endif

commit 133ff0eac95b7dc6edf89dc51bd139a0630bbae7
Author: Jérôme Glisse <jglisse@redhat.com>
Date:   Fri Sep 8 16:11:23 2017 -0700

    mm/hmm: heterogeneous memory management (HMM for short)
    
    HMM provides 3 separate types of functionality:
        - Mirroring: synchronize CPU page table and device page table
        - Device memory: allocating struct page for device memory
        - Migration: migrating regular memory to device memory
    
    This patch introduces some common helpers and definitions to all of
    those 3 functionality.
    
    Link: http://lkml.kernel.org/r/20170817000548.32038-3-jglisse@redhat.com
    Signed-off-by: Jérôme Glisse <jglisse@redhat.com>
    Signed-off-by: Evgeny Baskakov <ebaskakov@nvidia.com>
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Signed-off-by: Mark Hairgrove <mhairgrove@nvidia.com>
    Signed-off-by: Sherry Cheung <SCheung@nvidia.com>
    Signed-off-by: Subhash Gutti <sgutti@nvidia.com>
    Cc: Aneesh Kumar <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Nellans <dnellans@nvidia.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Bob Liu <liubo95@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 24a4c0be80d5..2ccbbbfcb7b8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -37,6 +37,7 @@
 #include <linux/binfmts.h>
 #include <linux/mman.h>
 #include <linux/mmu_notifier.h>
+#include <linux/hmm.h>
 #include <linux/fs.h>
 #include <linux/mm.h>
 #include <linux/vmacache.h>
@@ -824,6 +825,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	mm_init_owner(mm, p);
 	RCU_INIT_POINTER(mm->exe_file, NULL);
 	mmu_notifier_mm_init(mm);
+	hmm_mm_init(mm);
 	init_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;
@@ -903,6 +905,7 @@ void __mmdrop(struct mm_struct *mm)
 	BUG_ON(mm == &init_mm);
 	mm_free_pgd(mm);
 	destroy_context(mm);
+	hmm_mm_destroy(mm);
 	mmu_notifier_mm_destroy(mm);
 	check_mm(mm);
 	put_user_ns(mm->user_ns);

commit d2cd9ede6e193dd7d88b6d27399e96229a551b19
Author: Rik van Riel <riel@redhat.com>
Date:   Wed Sep 6 16:25:15 2017 -0700

    mm,fork: introduce MADV_WIPEONFORK
    
    Introduce MADV_WIPEONFORK semantics, which result in a VMA being empty
    in the child process after fork.  This differs from MADV_DONTFORK in one
    important way.
    
    If a child process accesses memory that was MADV_WIPEONFORK, it will get
    zeroes.  The address ranges are still valid, they are just empty.
    
    If a child process accesses memory that was MADV_DONTFORK, it will get a
    segmentation fault, since those address ranges are no longer valid in
    the child after fork.
    
    Since MADV_DONTFORK also seems to be used to allow very large programs
    to fork in systems with strict memory overcommit restrictions, changing
    the semantics of MADV_DONTFORK might break existing programs.
    
    MADV_WIPEONFORK only works on private, anonymous VMAs.
    
    The use case is libraries that store or cache information, and want to
    know that they need to regenerate it in the child process after fork.
    
    Examples of this would be:
     - systemd/pulseaudio API checks (fail after fork) (replacing a getpid
       check, which is too slow without a PID cache)
     - PKCS#11 API reinitialization check (mandated by specification)
     - glibc's upcoming PRNG (reseed after fork)
     - OpenSSL PRNG (reseed after fork)
    
    The security benefits of a forking server having a re-inialized PRNG in
    every child process are pretty obvious.  However, due to libraries
    having all kinds of internal state, and programs getting compiled with
    many different versions of each library, it is unreasonable to expect
    calling programs to re-initialize everything manually after fork.
    
    A further complication is the proliferation of clone flags, programs
    bypassing glibc's functions to call clone directly, and programs calling
    unshare, causing the glibc pthread_atfork hook to not get called.
    
    It would be better to have the kernel take care of this automatically.
    
    The patch also adds MADV_KEEPONFORK, to undo the effects of a prior
    MADV_WIPEONFORK.
    
    This is similar to the OpenBSD minherit syscall with MAP_INHERIT_ZERO:
    
        https://man.openbsd.org/minherit.2
    
    [akpm@linux-foundation.org: numerically order arch/parisc/include/uapi/asm/mman.h #defines]
    Link: http://lkml.kernel.org/r/20170811212829.29186-3-riel@redhat.com
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Reported-by: Florian Weimer <fweimer@redhat.com>
    Reported-by: Colm MacCártaigh <colm@allcosts.net>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Drewry <wad@chromium.org>
    Cc: <linux-api@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7ed64600da6c..24a4c0be80d5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -657,7 +657,12 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 		retval = dup_userfaultfd(tmp, &uf);
 		if (retval)
 			goto fail_nomem_anon_vma_fork;
-		if (anon_vma_fork(tmp, mpnt))
+		if (tmp->vm_flags & VM_WIPEONFORK) {
+			/* VM_WIPEONFORK gets a clean slate in the child. */
+			tmp->anon_vma = NULL;
+			if (anon_vma_prepare(tmp))
+				goto fail_nomem_anon_vma_fork;
+		} else if (anon_vma_fork(tmp, mpnt))
 			goto fail_nomem_anon_vma_fork;
 		tmp->vm_flags &= ~(VM_LOCKED | VM_LOCKONFAULT);
 		tmp->vm_next = tmp->vm_prev = NULL;
@@ -701,7 +706,8 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 		rb_parent = &tmp->vm_rb;
 
 		mm->map_count++;
-		retval = copy_page_range(mm, oldmm, mpnt);
+		if (!(tmp->vm_flags & VM_WIPEONFORK))
+			retval = copy_page_range(mm, oldmm, mpnt);
 
 		if (tmp->vm_ops && tmp->vm_ops->open)
 			tmp->vm_ops->open(tmp);

commit 212925802454672e6cd2949a727f5e2c1377bf06
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Wed Sep 6 16:25:00 2017 -0700

    mm: oom: let oom_reap_task and exit_mmap run concurrently
    
    This is purely required because exit_aio() may block and exit_mmap() may
    never start, if the oom_reap_task cannot start running on a mm with
    mm_users == 0.
    
    At the same time if the OOM reaper doesn't wait at all for the memory of
    the current OOM candidate to be freed by exit_mmap->unmap_vmas, it would
    generate a spurious OOM kill.
    
    If it wasn't because of the exit_aio or similar blocking functions in
    the last mmput, it would be enough to change the oom_reap_task() in the
    case it finds mm_users == 0, to wait for a timeout or to wait for
    __mmput to set MMF_OOM_SKIP itself, but it's not just exit_mmap the
    problem here so the concurrency of exit_mmap and oom_reap_task is
    apparently warranted.
    
    It's a non standard runtime, exit_mmap() runs without mmap_sem, and
    oom_reap_task runs with the mmap_sem for reading as usual (kind of
    MADV_DONTNEED).
    
    The race between the two is solved with a combination of
    tsk_is_oom_victim() (serialized by task_lock) and MMF_OOM_SKIP
    (serialized by a dummy down_write/up_write cycle on the same lines of
    the ksm_exit method).
    
    If the oom_reap_task() may be running concurrently during exit_mmap,
    exit_mmap will wait it to finish in down_write (before taking down mm
    structures that would make the oom_reap_task fail with use after free).
    
    If exit_mmap comes first, oom_reap_task() will skip the mm if
    MMF_OOM_SKIP is already set and in turn all memory is already freed and
    furthermore the mm data structures may already have been taken down by
    free_pgtables.
    
    [aarcange@redhat.com: incremental one liner]
      Link: http://lkml.kernel.org/r/20170726164319.GC29716@redhat.com
    [rientjes@google.com: remove unused mmput_async]
      Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1708141733130.50317@chino.kir.corp.google.com
    [aarcange@redhat.com: microoptimization]
      Link: http://lkml.kernel.org/r/20170817171240.GB5066@redhat.com
    Link: http://lkml.kernel.org/r/20170726162912.GA29716@redhat.com
    Fixes: 26db62f179d1 ("oom: keep mm of the killed task available")
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Reported-by: David Rientjes <rientjes@google.com>
    Tested-by: David Rientjes <rientjes@google.com>
    Reviewed-by: Michal Hocko <mhocko@suse.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4e5345c07344..7ed64600da6c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -922,7 +922,6 @@ static inline void __mmput(struct mm_struct *mm)
 	}
 	if (mm->binfmt)
 		module_put(mm->binfmt->module);
-	set_bit(MMF_OOM_SKIP, &mm->flags);
 	mmdrop(mm);
 }
 
@@ -938,22 +937,6 @@ void mmput(struct mm_struct *mm)
 }
 EXPORT_SYMBOL_GPL(mmput);
 
-#ifdef CONFIG_MMU
-static void mmput_async_fn(struct work_struct *work)
-{
-	struct mm_struct *mm = container_of(work, struct mm_struct, async_put_work);
-	__mmput(mm);
-}
-
-void mmput_async(struct mm_struct *mm)
-{
-	if (atomic_dec_and_test(&mm->mm_users)) {
-		INIT_WORK(&mm->async_put_work, mmput_async_fn);
-		schedule_work(&mm->async_put_work);
-	}
-}
-#endif
-
 /**
  * set_mm_exe_file - change a reference to the mm's executable file
  *

commit 04759194dc447ff0b9ef35bc641ce3bb076c2930
Merge: 9e85ae6af6e9 d1be5c99a034
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 5 09:53:37 2017 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - VMAP_STACK support, allowing the kernel stacks to be allocated in the
       vmalloc space with a guard page for trapping stack overflows. One of
       the patches introduces THREAD_ALIGN and changes the generic
       alloc_thread_stack_node() to use this instead of THREAD_SIZE (no
       functional change for other architectures)
    
     - Contiguous PTE hugetlb support re-enabled (after being reverted a
       couple of times). We now have the semantics agreed in the generic mm
       layer together with API improvements so that the architecture code
       can detect between contiguous and non-contiguous huge PTEs
    
     - Initial support for persistent memory on ARM: DC CVAP instruction
       exposed to user space (HWCAP) and the in-kernel pmem API implemented
    
     - raid6 improvements for arm64: faster algorithm for the delta syndrome
       and implementation of the recovery routines using Neon
    
     - FP/SIMD refactoring and removal of support for Neon in interrupt
       context. This is in preparation for full SVE support
    
     - PTE accessors converted from inline asm to cmpxchg so that we can use
       LSE atomics if available (ARMv8.1)
    
     - Perf support for Cortex-A35 and A73
    
     - Non-urgent fixes and cleanups
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (75 commits)
      arm64: cleanup {COMPAT_,}SET_PERSONALITY() macro
      arm64: introduce separated bits for mm_context_t flags
      arm64: hugetlb: Cleanup setup_hugepagesz
      arm64: Re-enable support for contiguous hugepages
      arm64: hugetlb: Override set_huge_swap_pte_at() to support contiguous hugepages
      arm64: hugetlb: Override huge_pte_clear() to support contiguous hugepages
      arm64: hugetlb: Handle swap entries in huge_pte_offset() for contiguous hugepages
      arm64: hugetlb: Add break-before-make logic for contiguous entries
      arm64: hugetlb: Spring clean huge pte accessors
      arm64: hugetlb: Introduce pte_pgprot helper
      arm64: hugetlb: set_huge_pte_at Add WARN_ON on !pte_present
      arm64: kexec: have own crash_smp_send_stop() for crash dump for nonpanic cores
      arm64: dma-mapping: Mark atomic_pool as __ro_after_init
      arm64: dma-mapping: Do not pass data to gen_pool_set_algo()
      arm64: Remove the !CONFIG_ARM64_HW_AFDBM alternative code paths
      arm64: Ignore hardware dirty bit updates in ptep_set_wrprotect()
      arm64: Move PTE_RDONLY bit handling out of set_pte_at()
      kvm: arm64: Convert kvm_set_s2pte_readonly() from inline asm to cmpxchg()
      arm64: Convert pte handling from inline asm to using (cmp)xchg
      arm64: neon/efi: Make EFI fpsimd save/restore variables static
      ...

commit edc2988c548db05e33b921fed15821010bc74895
Merge: d82fed752942 81a84ad3cb57
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Sep 4 11:01:18 2017 +0200

    Merge branch 'linus' into locking/core, to fix up conflicts
    
     Conflicts:
            mm/page_alloc.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 355627f518978b5167256d27492fe0b343aaf2f2
Author: Eric Biggers <ebiggers@google.com>
Date:   Thu Aug 31 16:15:26 2017 -0700

    mm, uprobes: fix multiple free of ->uprobes_state.xol_area
    
    Commit 7c051267931a ("mm, fork: make dup_mmap wait for mmap_sem for
    write killable") made it possible to kill a forking task while it is
    waiting to acquire its ->mmap_sem for write, in dup_mmap().
    
    However, it was overlooked that this introduced an new error path before
    the new mm_struct's ->uprobes_state.xol_area has been set to NULL after
    being copied from the old mm_struct by the memcpy in dup_mm().  For a
    task that has previously hit a uprobe tracepoint, this resulted in the
    'struct xol_area' being freed multiple times if the task was killed at
    just the right time while forking.
    
    Fix it by setting ->uprobes_state.xol_area to NULL in mm_init() rather
    than in uprobe_dup_mmap().
    
    With CONFIG_UPROBE_EVENTS=y, the bug can be reproduced by the same C
    program given by commit 2b7e8665b4ff ("fork: fix incorrect fput of
    ->exe_file causing use-after-free"), provided that a uprobe tracepoint
    has been set on the fork_thread() function.  For example:
    
        $ gcc reproducer.c -o reproducer -lpthread
        $ nm reproducer | grep fork_thread
        0000000000400719 t fork_thread
        $ echo "p $PWD/reproducer:0x719" > /sys/kernel/debug/tracing/uprobe_events
        $ echo 1 > /sys/kernel/debug/tracing/events/uprobes/enable
        $ ./reproducer
    
    Here is the use-after-free reported by KASAN:
    
        BUG: KASAN: use-after-free in uprobe_clear_state+0x1c4/0x200
        Read of size 8 at addr ffff8800320a8b88 by task reproducer/198
    
        CPU: 1 PID: 198 Comm: reproducer Not tainted 4.13.0-rc7-00015-g36fde05f3fb5 #255
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.10.2-20170228_101828-anatol 04/01/2014
        Call Trace:
         dump_stack+0xdb/0x185
         print_address_description+0x7e/0x290
         kasan_report+0x23b/0x350
         __asan_report_load8_noabort+0x19/0x20
         uprobe_clear_state+0x1c4/0x200
         mmput+0xd6/0x360
         do_exit+0x740/0x1670
         do_group_exit+0x13f/0x380
         get_signal+0x597/0x17d0
         do_signal+0x99/0x1df0
         exit_to_usermode_loop+0x166/0x1e0
         syscall_return_slowpath+0x258/0x2c0
         entry_SYSCALL_64_fastpath+0xbc/0xbe
    
        ...
    
        Allocated by task 199:
         save_stack_trace+0x1b/0x20
         kasan_kmalloc+0xfc/0x180
         kmem_cache_alloc_trace+0xf3/0x330
         __create_xol_area+0x10f/0x780
         uprobe_notify_resume+0x1674/0x2210
         exit_to_usermode_loop+0x150/0x1e0
         prepare_exit_to_usermode+0x14b/0x180
         retint_user+0x8/0x20
    
        Freed by task 199:
         save_stack_trace+0x1b/0x20
         kasan_slab_free+0xa8/0x1a0
         kfree+0xba/0x210
         uprobe_clear_state+0x151/0x200
         mmput+0xd6/0x360
         copy_process.part.8+0x605f/0x65d0
         _do_fork+0x1a5/0xbd0
         SyS_clone+0x19/0x20
         do_syscall_64+0x22f/0x660
         return_from_SYSCALL_64+0x0/0x7a
    
    Note: without KASAN, you may instead see a "Bad page state" message, or
    simply a general protection fault.
    
    Link: http://lkml.kernel.org/r/20170830033303.17927-1-ebiggers3@gmail.com
    Fixes: 7c051267931a ("mm, fork: make dup_mmap wait for mmap_sem for write killable")
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: <stable@vger.kernel.org>    [4.7+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index cbbea277b3fb..b7e9e57b71ea 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -785,6 +785,13 @@ static void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
 #endif
 }
 
+static void mm_init_uprobes_state(struct mm_struct *mm)
+{
+#ifdef CONFIG_UPROBES
+	mm->uprobes_state.xol_area = NULL;
+#endif
+}
+
 static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	struct user_namespace *user_ns)
 {
@@ -812,6 +819,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;
 #endif
+	mm_init_uprobes_state(mm);
 
 	if (current->mm) {
 		mm->flags = current->mm->flags & MMF_INIT_MASK;

commit 2b7e8665b4ff51c034c55df3cff76518d1a9ee3a
Author: Eric Biggers <ebiggers@google.com>
Date:   Fri Aug 25 15:55:43 2017 -0700

    fork: fix incorrect fput of ->exe_file causing use-after-free
    
    Commit 7c051267931a ("mm, fork: make dup_mmap wait for mmap_sem for
    write killable") made it possible to kill a forking task while it is
    waiting to acquire its ->mmap_sem for write, in dup_mmap().
    
    However, it was overlooked that this introduced an new error path before
    a reference is taken on the mm_struct's ->exe_file.  Since the
    ->exe_file of the new mm_struct was already set to the old ->exe_file by
    the memcpy() in dup_mm(), it was possible for the mmput() in the error
    path of dup_mm() to drop a reference to ->exe_file which was never
    taken.
    
    This caused the struct file to later be freed prematurely.
    
    Fix it by updating mm_init() to NULL out the ->exe_file, in the same
    place it clears other things like the list of mmaps.
    
    This bug was found by syzkaller.  It can be reproduced using the
    following C program:
    
        #define _GNU_SOURCE
        #include <pthread.h>
        #include <stdlib.h>
        #include <sys/mman.h>
        #include <sys/syscall.h>
        #include <sys/wait.h>
        #include <unistd.h>
    
        static void *mmap_thread(void *_arg)
        {
            for (;;) {
                mmap(NULL, 0x1000000, PROT_READ,
                     MAP_POPULATE|MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);
            }
        }
    
        static void *fork_thread(void *_arg)
        {
            usleep(rand() % 10000);
            fork();
        }
    
        int main(void)
        {
            fork();
            fork();
            fork();
            for (;;) {
                if (fork() == 0) {
                    pthread_t t;
    
                    pthread_create(&t, NULL, mmap_thread, NULL);
                    pthread_create(&t, NULL, fork_thread, NULL);
                    usleep(rand() % 10000);
                    syscall(__NR_exit_group, 0);
                }
                wait(NULL);
            }
        }
    
    No special kernel config options are needed.  It usually causes a NULL
    pointer dereference in __remove_shared_vm_struct() during exit, or in
    dup_mmap() (which is usually inlined into copy_process()) during fork.
    Both are due to a vm_area_struct's ->vm_file being used after it's
    already been freed.
    
    Google Bug Id: 64772007
    
    Link: http://lkml.kernel.org/r/20170823211408.31198-1-ebiggers3@gmail.com
    Fixes: 7c051267931a ("mm, fork: make dup_mmap wait for mmap_sem for write killable")
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: <stable@vger.kernel.org>    [v4.7+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e075b7780421..cbbea277b3fb 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -806,6 +806,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	mm_init_cpumask(mm);
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
+	RCU_INIT_POINTER(mm->exe_file, NULL);
 	mmu_notifier_mm_init(mm);
 	init_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS

commit 48ac3c18cc62d4a23d5dc5c59f8720589d0de14b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jul 14 12:23:09 2017 +0100

    fork: allow arch-override of VMAP stack alignment
    
    In some cases, an architecture might wish its stacks to be aligned to a
    boundary larger than THREAD_SIZE. For example, using an alignment of
    double THREAD_SIZE can allow for stack overflows smaller than
    THREAD_SIZE to be detected by checking a single bit of the stack
    pointer.
    
    This patch allows architectures to override the alignment of VMAP'd
    stacks, by defining THREAD_ALIGN. Where not defined, this defaults to
    THREAD_SIZE, as is the case today.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: linux-kernel@vger.kernel.org

diff --git a/kernel/fork.c b/kernel/fork.c
index 17921b0390b4..f12882a2323b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -88,6 +88,7 @@
 #include <linux/sysctl.h>
 #include <linux/kcov.h>
 #include <linux/livepatch.h>
+#include <linux/thread_info.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -217,7 +218,7 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 		return s->addr;
 	}
 
-	stack = __vmalloc_node_range(THREAD_SIZE, THREAD_SIZE,
+	stack = __vmalloc_node_range(THREAD_SIZE, THREAD_ALIGN,
 				     VMALLOC_START, VMALLOC_END,
 				     THREADINFO_GFP,
 				     PAGE_KERNEL,

commit 040cca3ab2f6f8b8d26e0e4965abea2b9aa14818
Merge: ef0758dd0fd7 b2dbdf2ca1d2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Aug 11 13:51:59 2017 +0200

    Merge branch 'linus' into locking/core, to resolve conflicts
    
     Conflicts:
            include/linux/mm_types.h
            mm/huge_memory.c
    
    I removed the smp_mb__before_spinlock() like the following commit does:
    
      8b1b436dd1cc ("mm, locking: Rework {set,clear,mm}_tlb_flush_pending()")
    
    and fixed up the affected commits.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 16af97dc5a8975371a83d9e30a64038b48f40a2d
Author: Nadav Amit <nadav.amit@gmail.com>
Date:   Thu Aug 10 15:23:56 2017 -0700

    mm: migrate: prevent racy access to tlb_flush_pending
    
    Patch series "fixes of TLB batching races", v6.
    
    It turns out that Linux TLB batching mechanism suffers from various
    races.  Races that are caused due to batching during reclamation were
    recently handled by Mel and this patch-set deals with others.  The more
    fundamental issue is that concurrent updates of the page-tables allow
    for TLB flushes to be batched on one core, while another core changes
    the page-tables.  This other core may assume a PTE change does not
    require a flush based on the updated PTE value, while it is unaware that
    TLB flushes are still pending.
    
    This behavior affects KSM (which may result in memory corruption) and
    MADV_FREE and MADV_DONTNEED (which may result in incorrect behavior).  A
    proof-of-concept can easily produce the wrong behavior of MADV_DONTNEED.
    Memory corruption in KSM is harder to produce in practice, but was
    observed by hacking the kernel and adding a delay before flushing and
    replacing the KSM page.
    
    Finally, there is also one memory barrier missing, which may affect
    architectures with weak memory model.
    
    This patch (of 7):
    
    Setting and clearing mm->tlb_flush_pending can be performed by multiple
    threads, since mmap_sem may only be acquired for read in
    task_numa_work().  If this happens, tlb_flush_pending might be cleared
    while one of the threads still changes PTEs and batches TLB flushes.
    
    This can lead to the same race between migration and
    change_protection_range() that led to the introduction of
    tlb_flush_pending.  The result of this race was data corruption, which
    means that this patch also addresses a theoretically possible data
    corruption.
    
    An actual data corruption was not observed, yet the race was was
    confirmed by adding assertion to check tlb_flush_pending is not set by
    two threads, adding artificial latency in change_protection_range() and
    using sysctl to reduce kernel.numa_balancing_scan_delay_ms.
    
    Link: http://lkml.kernel.org/r/20170802000818.4760-2-namit@vmware.com
    Fixes: 20841405940e ("mm: fix TLB flush race between migration, and
    change_protection_range")
    Signed-off-by: Nadav Amit <namit@vmware.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 17921b0390b4..e075b7780421 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -807,7 +807,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
 	mmu_notifier_mm_init(mm);
-	clear_tlb_flush_pending(mm);
+	init_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;
 #endif

commit b09be676e0ff25bd6d2e7637e26d349f9109ad75
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Aug 7 16:12:52 2017 +0900

    locking/lockdep: Implement the 'crossrelease' feature
    
    Lockdep is a runtime locking correctness validator that detects and
    reports a deadlock or its possibility by checking dependencies between
    locks. It's useful since it does not report just an actual deadlock but
    also the possibility of a deadlock that has not actually happened yet.
    That enables problems to be fixed before they affect real systems.
    
    However, this facility is only applicable to typical locks, such as
    spinlocks and mutexes, which are normally released within the context in
    which they were acquired. However, synchronization primitives like page
    locks or completions, which are allowed to be released in any context,
    also create dependencies and can cause a deadlock.
    
    So lockdep should track these locks to do a better job. The 'crossrelease'
    implementation makes these primitives also be tracked.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: boqun.feng@gmail.com
    Cc: kernel-team@lge.com
    Cc: kirill@shutemov.name
    Cc: npiggin@gmail.com
    Cc: walken@google.com
    Cc: willy@infradead.org
    Link: http://lkml.kernel.org/r/1502089981-21272-6-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 17921b0390b4..cbf2221ee81a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -484,6 +484,8 @@ void __init fork_init(void)
 	cpuhp_setup_state(CPUHP_BP_PREPARE_DYN, "fork:vm_stack_cache",
 			  NULL, free_vm_stack_cache);
 #endif
+
+	lockdep_init_task(&init_task);
 }
 
 int __weak arch_dup_task_struct(struct task_struct *dst,
@@ -1691,6 +1693,7 @@ static __latent_entropy struct task_struct *copy_process(
 	p->lockdep_depth = 0; /* no locks held yet */
 	p->curr_chain_key = 0;
 	p->lockdep_recursion = 0;
+	lockdep_init_task(p);
 #endif
 
 #ifdef CONFIG_DEBUG_MUTEXES
@@ -1949,6 +1952,7 @@ static __latent_entropy struct task_struct *copy_process(
 bad_fork_cleanup_perf:
 	perf_event_free_task(p);
 bad_fork_cleanup_policy:
+	lockdep_free_task(p);
 #ifdef CONFIG_NUMA
 	mpol_put(p->mempolicy);
 bad_fork_cleanup_threadgroup_lock:

commit 53a2ebaaabc1eb8458796fec3bc1e0e80746b642
Merge: 3cf299314532 520eccdfe187
Author: James Morris <james.l.morris@oracle.com>
Date:   Tue Jul 25 10:44:18 2017 +1000

    sync to Linus v4.13-rc2 for subsystem developers to work against

commit 3cf29931453215536916d0c4da953fce1911ced3
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Fri Jul 14 19:38:36 2017 +0900

    LSM: Remove security_task_create() hook.
    
    Since commit a79be238600d1a03 ("selinux: Use task_alloc hook rather than
    task_create hook") changed to use task_alloc hook, task_create hook is
    no longer used.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: James Morris <james.l.morris@oracle.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index aa1076c5e4a9..3a13a940a6ea 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1568,10 +1568,6 @@ static __latent_entropy struct task_struct *copy_process(
 			return ERR_PTR(-EINVAL);
 	}
 
-	retval = security_task_create(clone_flags);
-	if (retval)
-		goto fork_out;
-
 	retval = -ENOMEM;
 	p = dup_task_struct(current, node);
 	if (!p)

commit 7cd815bce828220deffd1654265f0ef891567774
Author: Rik van Riel <riel@redhat.com>
Date:   Wed Jul 12 14:36:20 2017 -0700

    fork,random: use get_random_canary() to set tsk->stack_canary
    
    Use the ascii-armor canary to prevent unterminated C string overflows
    from being able to successfully overwrite the canary, even if they
    somehow obtain the canary value.
    
    Inspired by execshield ascii-armor and Daniel Micay's linux-hardened
    tree.
    
    Link: http://lkml.kernel.org/r/20170524155751.424-3-riel@redhat.com
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: Daniel Micay <danielmicay@gmail.com>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ade237a96308..17921b0390b4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -554,7 +554,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	set_task_stack_end_magic(tsk);
 
 #ifdef CONFIG_CC_STACKPROTECTOR
-	tsk->stack_canary = get_random_long();
+	tsk->stack_canary = get_random_canary();
 #endif
 
 	/*

commit e41d58185f1444368873d4d7422f7664a68be61d
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Wed Jul 12 14:34:35 2017 -0700

    fault-inject: support systematic fault injection
    
    Add /proc/self/task/<current-tid>/fail-nth file that allows failing
    0-th, 1-st, 2-nd and so on calls systematically.
    Excerpt from the added documentation:
    
     "Write to this file of integer N makes N-th call in the current task
      fail (N is 0-based). Read from this file returns a single char 'Y' or
      'N' that says if the fault setup with a previous write to this file
      was injected or not, and disables the fault if it wasn't yet injected.
      Note that this file enables all types of faults (slab, futex, etc).
      This setting takes precedence over all other generic settings like
      probability, interval, times, etc. But per-capability settings (e.g.
      fail_futex/ignore-private) take precedence over it. This feature is
      intended for systematic testing of faults in a single system call. See
      an example below"
    
    Why add a new setting:
    1. Existing settings are global rather than per-task.
       So parallel testing is not possible.
    2. attr->interval is close but it depends on attr->count
       which is non reset to 0, so interval does not work as expected.
    3. Trying to model this with existing settings requires manipulations
       of all of probability, interval, times, space, task-filter and
       unexposed count and per-task make-it-fail files.
    4. Existing settings are per-failure-type, and the set of failure
       types is potentially expanding.
    5. make-it-fail can't be changed by unprivileged user and aggressive
       stress testing better be done from an unprivileged user.
       Similarly, this would require opening the debugfs files to the
       unprivileged user, as he would need to reopen at least times file
       (not possible to pre-open before dropping privs).
    
    The proposed interface solves all of the above (see the example).
    
    We want to integrate this into syzkaller fuzzer.  A prototype has found
    10 bugs in kernel in first day of usage:
    
      https://groups.google.com/forum/#!searchin/syzkaller/%22FAULT_INJECTION%22%7Csort:relevance
    
    I've made the current interface work with all types of our sandboxes.
    For setuid the secret sauce was prctl(PR_SET_DUMPABLE, 1, 0, 0, 0) to
    make /proc entries non-root owned.  So I am fine with the current
    version of the code.
    
    [akpm@linux-foundation.org: fix build]
    Link: http://lkml.kernel.org/r/20170328130128.101773-1-dvyukov@google.com
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d2b9d7c31eaf..ade237a96308 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -573,6 +573,10 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 
 	kcov_task_init(tsk);
 
+#ifdef CONFIG_FAULT_INJECTION
+	tsk->fail_nth = 0;
+#endif
+
 	return tsk;
 
 free_stack:

commit 112166f88cf83dd11486cf1818672d42b540865b
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Jul 12 14:33:11 2017 -0700

    kernel/fork.c: virtually mapped stacks: do not disable interrupts
    
    The reason to disable interrupts seems to be to avoid switching to a
    different processor while handling per cpu data using individual loads and
    stores.  If we use per cpu RMV primitives we will not have to disable
    interrupts.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.20.1705171055130.5898@east.gentwo.org
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0f69a3e5281e..d2b9d7c31eaf 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -205,19 +205,17 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 	void *stack;
 	int i;
 
-	local_irq_disable();
 	for (i = 0; i < NR_CACHED_STACKS; i++) {
-		struct vm_struct *s = this_cpu_read(cached_stacks[i]);
+		struct vm_struct *s;
+
+		s = this_cpu_xchg(cached_stacks[i], NULL);
 
 		if (!s)
 			continue;
-		this_cpu_write(cached_stacks[i], NULL);
 
 		tsk->stack_vm_area = s;
-		local_irq_enable();
 		return s->addr;
 	}
-	local_irq_enable();
 
 	stack = __vmalloc_node_range(THREAD_SIZE, THREAD_SIZE,
 				     VMALLOC_START, VMALLOC_END,
@@ -245,19 +243,15 @@ static inline void free_thread_stack(struct task_struct *tsk)
 {
 #ifdef CONFIG_VMAP_STACK
 	if (task_stack_vm_area(tsk)) {
-		unsigned long flags;
 		int i;
 
-		local_irq_save(flags);
 		for (i = 0; i < NR_CACHED_STACKS; i++) {
-			if (this_cpu_read(cached_stacks[i]))
+			if (this_cpu_cmpxchg(cached_stacks[i],
+					NULL, tsk->stack_vm_area) != NULL)
 				continue;
 
-			this_cpu_write(cached_stacks[i], tsk->stack_vm_area);
-			local_irq_restore(flags);
 			return;
 		}
-		local_irq_restore(flags);
 
 		vfree_atomic(tsk->stack);
 		return;

commit 4fde846ac0f019b7c877da35e1c1517d79e17ffc
Merge: c3931a87db9e 242fc35290bd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 9 10:52:16 2017 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Thomas Gleixner:
     "This scheduler update provides:
    
       - The (hopefully) final fix for the vtime accounting issues which
         were around for quite some time
    
       - Use types known to user space in UAPI headers to unbreak user space
         builds
    
       - Make load balancing respect the current scheduling domain again
         instead of evaluating unrelated CPUs"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/headers/uapi: Fix linux/sched/types.h userspace compilation errors
      sched/fair: Fix load_balance() affinity redo path
      sched/cputime: Accumulate vtime on top of nsec clocksource
      sched/cputime: Move the vtime task fields to their own struct
      sched/cputime: Rename vtime fields
      sched/cputime: Always set tsk->vtime_snap_whence after accounting vtime
      vtime, sched/cputime: Remove vtime_account_user()
      Revert "sched/cputime: Refactor the cputime_adjust() code"

commit ed52be7bfd45533b194b429f43361493d24599a7
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Jul 6 15:40:49 2017 -0700

    mm: memcontrol: use generic mod_memcg_page_state for kmem pages
    
    The kmem-specific functions do the same thing.  Switch and drop.
    
    Link: http://lkml.kernel.org/r/20170530181724.27197-5-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vladimir Davydov <vdavydov.dev@gmail.com>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e53770d2bf95..aa01b810c0bd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -326,8 +326,8 @@ static void account_kernel_stack(struct task_struct *tsk, int account)
 		}
 
 		/* All stack pages belong to the same memcg. */
-		memcg_kmem_update_page_stat(vm->pages[0], MEMCG_KERNEL_STACK_KB,
-					    account * (THREAD_SIZE / 1024));
+		mod_memcg_page_state(vm->pages[0], MEMCG_KERNEL_STACK_KB,
+				     account * (THREAD_SIZE / 1024));
 	} else {
 		/*
 		 * All stack pages are in the same zone and belong to the
@@ -338,8 +338,8 @@ static void account_kernel_stack(struct task_struct *tsk, int account)
 		mod_zone_page_state(page_zone(first_page), NR_KERNEL_STACK_KB,
 				    THREAD_SIZE / 1024 * account);
 
-		memcg_kmem_update_page_stat(first_page, MEMCG_KERNEL_STACK_KB,
-					    account * (THREAD_SIZE / 1024));
+		mod_memcg_page_state(first_page, MEMCG_KERNEL_STACK_KB,
+				     account * (THREAD_SIZE / 1024));
 	}
 }
 

commit bac5b6b6b11560f323e71d0ebac4061cfe5f56c0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jun 29 19:15:10 2017 +0200

    sched/cputime: Move the vtime task fields to their own struct
    
    We are about to add vtime accumulation fields to the task struct. Let's
    avoid more bloatification and gather vtime information to their own
    struct.
    
    Tested-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1498756511-11714-5-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 83c4f9bf3e14..d927ec11aa7a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1637,9 +1637,9 @@ static __latent_entropy struct task_struct *copy_process(
 	prev_cputime_init(&p->prev_cputime);
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-	seqcount_init(&p->vtime_seqcount);
-	p->vtime_starttime = 0;
-	p->vtime_state = VTIME_INACTIVE;
+	seqcount_init(&p->vtime.seqcount);
+	p->vtime.starttime = 0;
+	p->vtime.state = VTIME_INACTIVE;
 #endif
 
 #if defined(SPLIT_RSS_COUNTING)

commit 60a9ce57e7c5ac1df3a39fb941022bbfa40c0862
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jun 29 19:15:09 2017 +0200

    sched/cputime: Rename vtime fields
    
    The current "snapshot" based naming on vtime fields suggests we record
    some past event but that's a low level picture of their actual purpose
    which comes out blurry. The real point of these fields is to run a basic
    state machine that tracks down cputime entry while switching between
    contexts.
    
    So lets reflect that with more meaningful names.
    
    Tested-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1498756511-11714-4-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e53770d2bf95..83c4f9bf3e14 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1638,8 +1638,8 @@ static __latent_entropy struct task_struct *copy_process(
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 	seqcount_init(&p->vtime_seqcount);
-	p->vtime_snap = 0;
-	p->vtime_snap_whence = VTIME_INACTIVE;
+	p->vtime_starttime = 0;
+	p->vtime_state = VTIME_INACTIVE;
 #endif
 
 #if defined(SPLIT_RSS_COUNTING)

commit d024baa58a4a7e5eb6058017771d15b9e47b56db
Merge: 77d64656950b 4d6501dce079
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat May 27 08:52:27 2017 -0700

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull kthread fix from Thomas Gleixner:
     "A single fix which prevents a use after free when kthread fork fails"
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      kthread: Fix use-after-free if kthread fork fails

commit 4d6501dce079c1eb6bf0b1d8f528a5e81770109e
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Tue May 9 09:39:59 2017 +0200

    kthread: Fix use-after-free if kthread fork fails
    
    If a kthread forks (e.g. usermodehelper since commit 1da5c46fa965) but
    fails in copy_process() between calling dup_task_struct() and setting
    p->set_child_tid, then the value of p->set_child_tid will be inherited
    from the parent and get prematurely freed by free_kthread_struct().
    
        kthread()
         - worker_thread()
            - process_one_work()
            |  - call_usermodehelper_exec_work()
            |     - kernel_thread()
            |        - _do_fork()
            |           - copy_process()
            |              - dup_task_struct()
            |                 - arch_dup_task_struct()
            |                    - tsk->set_child_tid = current->set_child_tid // implied
            |              - ...
            |              - goto bad_fork_*
            |              - ...
            |              - free_task(tsk)
            |                 - free_kthread_struct(tsk)
            |                    - kfree(tsk->set_child_tid)
            - ...
            - schedule()
               - __schedule()
                  - wq_worker_sleeping()
                     - kthread_data(task)->flags // UAF
    
    The problem started showing up with commit 1da5c46fa965 since it reused
    ->set_child_tid for the kthread worker data.
    
    A better long-term solution might be to get rid of the ->set_child_tid
    abuse. The comment in set_kthread_struct() also looks slightly wrong.
    
    Debugged-by: Jamie Iles <jamie.iles@oracle.com>
    Fixes: 1da5c46fa965 ("kthread: Make struct kthread kmalloc'ed")
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jamie Iles <jamie.iles@oracle.com>
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20170509073959.17858-1-vegard.nossum@oracle.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index d681f8f10d2d..b7cdea10239c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1553,6 +1553,18 @@ static __latent_entropy struct task_struct *copy_process(
 	if (!p)
 		goto fork_out;
 
+	/*
+	 * This _must_ happen before we call free_task(), i.e. before we jump
+	 * to any of the bad_fork_* labels. This is to avoid freeing
+	 * p->set_child_tid which is (ab)used as a kthread's data pointer for
+	 * kernel threads (PF_KTHREAD).
+	 */
+	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
+	/*
+	 * Clear TID on mm_release()?
+	 */
+	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;
+
 	ftrace_graph_init_task(p);
 
 	rt_mutex_init_task(p);
@@ -1716,11 +1728,6 @@ static __latent_entropy struct task_struct *copy_process(
 		}
 	}
 
-	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
-	/*
-	 * Clear TID on mm_release()?
-	 */
-	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;
 #ifdef CONFIG_BLOCK
 	p->plug = NULL;
 #endif

commit 3fd37226216620c1a468afa999739d5016fbc349
Author: Kirill Tkhai <ktkhai@virtuozzo.com>
Date:   Fri May 12 19:11:31 2017 +0300

    pid_ns: Fix race between setns'ed fork() and zap_pid_ns_processes()
    
    Imagine we have a pid namespace and a task from its parent's pid_ns,
    which made setns() to the pid namespace. The task is doing fork(),
    while the pid namespace's child reaper is dying. We have the race
    between them:
    
    Task from parent pid_ns             Child reaper
    copy_process()                      ..
      alloc_pid()                       ..
      ..                                zap_pid_ns_processes()
      ..                                  disable_pid_allocation()
      ..                                  read_lock(&tasklist_lock)
      ..                                  iterate over pids in pid_ns
      ..                                    kill tasks linked to pids
      ..                                  read_unlock(&tasklist_lock)
      write_lock_irq(&tasklist_lock);   ..
      attach_pid(p, PIDTYPE_PID);       ..
      ..                                ..
    
    So, just created task p won't receive SIGKILL signal,
    and the pid namespace will be in contradictory state.
    Only manual kill will help there, but does the userspace
    care about this? I suppose, the most users just inject
    a task into a pid namespace and wait a SIGCHLD from it.
    
    The patch fixes the problem. It simply checks for
    (pid_ns->nr_hashed & PIDNS_HASH_ADDING) in copy_process().
    We do it under the tasklist_lock, and can't skip
    PIDNS_HASH_ADDING as noted by Oleg:
    
    "zap_pid_ns_processes() does disable_pid_allocation()
    and then takes tasklist_lock to kill the whole namespace.
    Given that copy_process() checks PIDNS_HASH_ADDING
    under write_lock(tasklist) they can't race;
    if copy_process() takes this lock first, the new child will
    be killed, otherwise copy_process() can't miss
    the change in ->nr_hashed."
    
    If allocation is disabled, we just return -ENOMEM
    like it's made for such cases in alloc_pid().
    
    v2: Do not move disable_pid_allocation(), do not
    introduce a new variable in copy_process() and simplify
    the patch as suggested by Oleg Nesterov.
    Account the problem with double irq enabling
    found by Eric W. Biederman.
    
    Fixes: c876ad768215 ("pidns: Stop pid allocation when init dies")
    Signed-off-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    CC: Andrew Morton <akpm@linux-foundation.org>
    CC: Ingo Molnar <mingo@kernel.org>
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Oleg Nesterov <oleg@redhat.com>
    CC: Mike Rapoport <rppt@linux.vnet.ibm.com>
    CC: Michal Hocko <mhocko@suse.com>
    CC: Andy Lutomirski <luto@kernel.org>
    CC: "Eric W. Biederman" <ebiederm@xmission.com>
    CC: Andrei Vagin <avagin@openvz.org>
    CC: Cyrill Gorcunov <gorcunov@openvz.org>
    CC: Serge Hallyn <serge@hallyn.com>
    Cc: stable@vger.kernel.org
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 06d759ab4c62..aa1076c5e4a9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1845,11 +1845,13 @@ static __latent_entropy struct task_struct *copy_process(
 	*/
 	recalc_sigpending();
 	if (signal_pending(current)) {
-		spin_unlock(&current->sighand->siglock);
-		write_unlock_irq(&tasklist_lock);
 		retval = -ERESTARTNOINTR;
 		goto bad_fork_cancel_cgroup;
 	}
+	if (unlikely(!(ns_of_pid(pid)->nr_hashed & PIDNS_HASH_ADDING))) {
+		retval = -ENOMEM;
+		goto bad_fork_cancel_cgroup;
+	}
 
 	if (likely(p->pid)) {
 		ptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);
@@ -1907,6 +1909,8 @@ static __latent_entropy struct task_struct *copy_process(
 	return p;
 
 bad_fork_cancel_cgroup:
+	spin_unlock(&current->sighand->siglock);
+	write_unlock_irq(&tasklist_lock);
 	cgroup_cancel_fork(p);
 bad_fork_free_pid:
 	cgroup_threadgroup_change_end(current);

commit 1b84fc1503f24862d45eafb2e63da72b714da3fa
Merge: f1e0527d2db4 5ea30e4e5804
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 12 10:41:45 2017 -0700

    Merge branch 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull stackprotector fixlet from Ingo Molnar:
     "A single fix/enhancement to increase stackprotector canary randomness
      on 64-bit kernels with very little cost"
    
    * 'core-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      stackprotector: Increase the per-task stack canary's random range from 32 bits to 64 bits on 64-bit platforms

commit de4d195308ad589626571dbe5789cebf9695a204
Merge: dc9edaab90de 20652ed6e44f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 10 09:50:55 2017 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU updates from Ingo Molnar:
     "The main changes are:
    
       - Debloat RCU headers
    
       - Parallelize SRCU callback handling (plus overlapping patches)
    
       - Improve the performance of Tree SRCU on a CPU-hotplug stress test
    
       - Documentation updates
    
       - Miscellaneous fixes"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (74 commits)
      rcu: Open-code the rcu_cblist_n_lazy_cbs() function
      rcu: Open-code the rcu_cblist_n_cbs() function
      rcu: Open-code the rcu_cblist_empty() function
      rcu: Separately compile large rcu_segcblist functions
      srcu: Debloat the <linux/rcu_segcblist.h> header
      srcu: Adjust default auto-expediting holdoff
      srcu: Specify auto-expedite holdoff time
      srcu: Expedite first synchronize_srcu() when idle
      srcu: Expedited grace periods with reduced memory contention
      srcu: Make rcutorture writer stalls print SRCU GP state
      srcu: Exact tracking of srcu_data structures containing callbacks
      srcu: Make SRCU be built by default
      srcu: Fix Kconfig botch when SRCU not selected
      rcu: Make non-preemptive schedule be Tasks RCU quiescent state
      srcu: Expedite srcu_schedule_cbs_snp() callback invocation
      srcu: Parallelize callback handling
      kvm: Move srcu_struct fields to end of struct kvm
      rcu: Fix typo in PER_RCU_NODE_PERIOD header comment
      rcu: Use true/false in assignment to bool
      rcu: Use bool value directly
      ...

commit 19809c2da28aee5860ad9a2eff760730a0710df0
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 8 15:57:44 2017 -0700

    mm, vmalloc: use __GFP_HIGHMEM implicitly
    
    __vmalloc* allows users to provide gfp flags for the underlying
    allocation.  This API is quite popular
    
      $ git grep "=[[:space:]]__vmalloc\|return[[:space:]]*__vmalloc" | wc -l
      77
    
    The only problem is that many people are not aware that they really want
    to give __GFP_HIGHMEM along with other flags because there is really no
    reason to consume precious lowmemory on CONFIG_HIGHMEM systems for pages
    which are mapped to the kernel vmalloc space.  About half of users don't
    use this flag, though.  This signals that we make the API unnecessarily
    too complex.
    
    This patch simply uses __GFP_HIGHMEM implicitly when allocating pages to
    be mapped to the vmalloc space.  Current users which add __GFP_HIGHMEM
    are simplified and drop the flag.
    
    Link: http://lkml.kernel.org/r/20170307141020.29107-1-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Matthew Wilcox <mawilcox@microsoft.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Cristopher Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 55e325f4b457..08ba696aa561 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -221,7 +221,7 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 
 	stack = __vmalloc_node_range(THREAD_SIZE, THREAD_SIZE,
 				     VMALLOC_START, VMALLOC_END,
-				     THREADINFO_GFP | __GFP_HIGHMEM,
+				     THREADINFO_GFP,
 				     PAGE_KERNEL,
 				     0, node, __builtin_return_address(0));
 

commit 19659c59af52df22d1b85208a2c37b2d46290541
Author: Hoeun Ryu <hoeun.ryu@gmail.com>
Date:   Mon May 8 15:56:11 2017 -0700

    fork: free vmapped stacks in cache when cpus are offline
    
    Using virtually mapped stack, kernel stacks are allocated via vmalloc.
    
    In the current implementation, two stacks per cpu can be cached when
    tasks are freed and the cached stacks are used again in task
    duplications.  But the cached stacks may remain unfreed even when cpu
    are offline.  By adding a cpu hotplug callback to free the cached stacks
    when a cpu goes offline, the pages of the cached stacks are not wasted.
    
    Link: http://lkml.kernel.org/r/1487076043-17802-1-git-send-email-hoeun.ryu@gmail.com
    Signed-off-by: Hoeun Ryu <hoeun.ryu@gmail.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Mateusz Guzik <mguzik@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index dd5a371c392a..55e325f4b457 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -179,6 +179,24 @@ void __weak arch_release_thread_stack(unsigned long *stack)
  */
 #define NR_CACHED_STACKS 2
 static DEFINE_PER_CPU(struct vm_struct *, cached_stacks[NR_CACHED_STACKS]);
+
+static int free_vm_stack_cache(unsigned int cpu)
+{
+	struct vm_struct **cached_vm_stacks = per_cpu_ptr(cached_stacks, cpu);
+	int i;
+
+	for (i = 0; i < NR_CACHED_STACKS; i++) {
+		struct vm_struct *vm_stack = cached_vm_stacks[i];
+
+		if (!vm_stack)
+			continue;
+
+		vfree(vm_stack->addr);
+		cached_vm_stacks[i] = NULL;
+	}
+
+	return 0;
+}
 #endif
 
 static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
@@ -467,6 +485,11 @@ void __init fork_init(void)
 	for (i = 0; i < UCOUNT_COUNTS; i++) {
 		init_user_ns.ucount_max[i] = max_threads/2;
 	}
+
+#ifdef CONFIG_VMAP_STACK
+	cpuhp_setup_state(CPUHP_BP_PREPARE_DYN, "fork:vm_stack_cache",
+			  NULL, free_vm_stack_cache);
+#endif
 }
 
 int __weak arch_dup_task_struct(struct task_struct *dst,

commit 5ea30e4e58040cfd6434c2f33dc3ea76e2c15b05
Author: Daniel Micay <danielmicay@gmail.com>
Date:   Thu May 4 09:32:09 2017 -0400

    stackprotector: Increase the per-task stack canary's random range from 32 bits to 64 bits on 64-bit platforms
    
    The stack canary is an 'unsigned long' and should be fully initialized to
    random data rather than only 32 bits of random data.
    
    Signed-off-by: Daniel Micay <danielmicay@gmail.com>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: Arjan van Ven <arjan@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-hardening@lists.openwall.com
    Cc: stable@vger.kernel.org
    Link: http://lkml.kernel.org/r/20170504133209.3053-1-danielmicay@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3a4343cdfe90..d681f8f10d2d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -536,7 +536,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	set_task_stack_end_magic(tsk);
 
 #ifdef CONFIG_CC_STACKPROTECTOR
-	tsk->stack_canary = get_random_int();
+	tsk->stack_canary = get_random_long();
 #endif
 
 	/*

commit 0302e28dee643932ee7b3c112ebccdbb9f8ec32c
Merge: 89c9fea3c803 8979b02aaf1d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 3 08:50:52 2017 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "Highlights:
    
      IMA:
       - provide ">" and "<" operators for fowner/uid/euid rules
    
      KEYS:
       - add a system blacklist keyring
    
       - add KEYCTL_RESTRICT_KEYRING, exposes keyring link restriction
         functionality to userland via keyctl()
    
      LSM:
       - harden LSM API with __ro_after_init
    
       - add prlmit security hook, implement for SELinux
    
       - revive security_task_alloc hook
    
      TPM:
       - implement contextual TPM command 'spaces'"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (98 commits)
      tpm: Fix reference count to main device
      tpm_tis: convert to using locality callbacks
      tpm: fix handling of the TPM 2.0 event logs
      tpm_crb: remove a cruft constant
      keys: select CONFIG_CRYPTO when selecting DH / KDF
      apparmor: Make path_max parameter readonly
      apparmor: fix parameters so that the permission test is bypassed at boot
      apparmor: fix invalid reference to index variable of iterator line 836
      apparmor: use SHASH_DESC_ON_STACK
      security/apparmor/lsm.c: set debug messages
      apparmor: fix boolreturn.cocci warnings
      Smack: Use GFP_KERNEL for smk_netlbl_mls().
      smack: fix double free in smack_parse_opts_str()
      KEYS: add SP800-56A KDF support for DH
      KEYS: Keyring asymmetric key restrict method with chaining
      KEYS: Restrict asymmetric key linkage using a specific keychain
      KEYS: Add a lookup_restriction function for the asymmetric key type
      KEYS: Add KEYCTL_RESTRICT_KEYRING
      KEYS: Consistent ordering for __key_link_begin and restrict check
      KEYS: Add an optional lookup_restriction hook to key_type
      ...

commit 76f1948a79b26d5f57a5ee9941876b745c6baaea
Merge: 7af4c727c7b6 a0841609f658
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 2 18:24:16 2017 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching
    
    Pull livepatch updates from Jiri Kosina:
    
     - a per-task consistency model is being added for architectures that
       support reliable stack dumping (extending this, currently rather
       trivial set, is currently in the works).
    
       This extends the nature of the types of patches that can be applied
       by live patching infrastructure. The code stems from the design
       proposal made [1] back in November 2014. It's a hybrid of SUSE's
       kGraft and RH's kpatch, combining advantages of both: it uses
       kGraft's per-task consistency and syscall barrier switching combined
       with kpatch's stack trace switching. There are also a number of
       fallback options which make it quite flexible.
    
       Most of the heavy lifting done by Josh Poimboeuf with help from
       Miroslav Benes and Petr Mladek
    
       [1] https://lkml.kernel.org/r/20141107140458.GA21774@suse.cz
    
     - module load time patch optimization from Zhou Chengming
    
     - a few assorted small fixes
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/livepatching:
      livepatch: add missing printk newlines
      livepatch: Cancel transition a safe way for immediate patches
      livepatch: Reduce the time of finding module symbols
      livepatch: make klp_mutex proper part of API
      livepatch: allow removal of a disabled patch
      livepatch: add /proc/<pid>/patch_state
      livepatch: change to a per-task consistency model
      livepatch: store function sizes
      livepatch: use kstrtobool() in enabled_store()
      livepatch: move patching functions into patch.c
      livepatch: remove unnecessary object loaded check
      livepatch: separate enabled and patched states
      livepatch/s390: add TIF_PATCH_PENDING thread flag
      livepatch/s390: reorganize TIF thread flag bits
      livepatch/powerpc: add TIF_PATCH_PENDING thread flag
      livepatch/x86: add TIF_PATCH_PENDING thread flag
      livepatch: create temporary klp_update_patch_state() stub
      x86/entry: define _TIF_ALLWORK_MASK flags explicitly
      stacktrace/x86: add function for detecting reliable stack traces

commit 7c8c03bfc7b9f5211d8a69eab7fee99c9fb4f449
Merge: 6dc2cce93211 fd7647979a39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 20:23:17 2017 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf updates from Ingo Molnar:
     "The main changes in this cycle were:
    
      Kernel side changes:
    
       - Kprobes and uprobes changes:
          - Make their trampolines read-only while they are used
          - Make UPROBES_EVENTS default-y which is the distro practice
          - Apply misc fixes and robustization to probe point insertion.
    
       - add support for AMD IOMMU events
    
       - extend hw events on Intel Goldmont CPUs
    
       - ... plus misc fixes and updates.
    
      Tooling side changes:
    
       - support s390 jump instructions in perf annotate (Christian
         Borntraeger)
    
       - vendor hardware events updates (Andi Kleen)
    
       - add argument support for SDT events in powerpc (Ravi Bangoria)
    
       - beautify the statx syscall arguments in 'perf trace' (Arnaldo
         Carvalho de Melo)
    
       - handle inline functions in callchains (Jin Yao)
    
       - enable sorting by srcline as key (Milian Wolff)
    
       - add 'brstackinsn' field in 'perf script' to reuse the x86
         instruction decoder used in the Intel PT code to study hot paths to
         samples (Andi Kleen)
    
       - add PERF_RECORD_NAMESPACES so that the kernel can record
         information required to associate samples to namespaces, helping in
         container problem characterization. (Hari Bathini)
    
       - allow sorting by symbol_size in 'perf report' and 'perf top'
         (Charles Baylis)
    
       - in perf stat, make system wide (-a) the default option if no target
         was specified and one of following conditions is met:
          - no workload specified (current behaviour)
          - a workload is specified but all requested events are system wide
            ones, like uncore ones. (Jiri Olsa)
    
       - ... plus lots of other updates, enhancements, cleanups and fixes"
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (235 commits)
      perf tools: Fix the code to strip command name
      tools arch x86: Sync cpufeatures.h
      tools arch: Sync arch/x86/lib/memcpy_64.S with the kernel
      tools: Update asm-generic/mman-common.h copy from the kernel
      perf tools: Use just forward declarations for struct thread where possible
      perf tools: Add the right header to obtain PERF_ALIGN()
      perf tools: Remove poll.h and wait.h from util.h
      perf tools: Remove string.h, unistd.h and sys/stat.h from util.h
      perf tools: Remove stale prototypes from builtin.h
      perf tools: Remove string.h from util.h
      perf tools: Remove sys/ioctl.h from util.h
      perf tools: Remove a few more needless includes from util.h
      perf tools: Include sys/param.h where needed
      perf callchain: Move callchain specific routines from util.[ch]
      perf tools: Add compress.h for the *_decompress_to_file() headers
      perf mem: Fix display of data source snoop indication
      perf debug: Move dump_stack() and sighandler_dump_stack() to debug.h
      perf kvm: Make function only used by 'perf kvm' static
      perf tools: Move timestamp routines from util.h to time-utils.h
      perf tools: Move units conversion/formatting routines to separate object
      ...

commit 5f0d5a3ae7cff0d7fa943c199c3a2e44f23e1fac
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 18 02:53:44 2017 -0800

    mm: Rename SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU
    
    A group of Linux kernel hackers reported chasing a bug that resulted
    from their assumption that SLAB_DESTROY_BY_RCU provided an existence
    guarantee, that is, that no block from such a slab would be reallocated
    during an RCU read-side critical section.  Of course, that is not the
    case.  Instead, SLAB_DESTROY_BY_RCU only prevents freeing of an entire
    slab of blocks.
    
    However, there is a phrase for this, namely "type safety".  This commit
    therefore renames SLAB_DESTROY_BY_RCU to SLAB_TYPESAFE_BY_RCU in order
    to avoid future instances of this sort of confusion.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: <linux-mm@kvack.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    [ paulmck: Add comments mentioning the old name, as requested by Eric
      Dumazet, in order to help people familiar with the old name find
      the new one. ]
    Acked-by: David Rientjes <rientjes@google.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6c463c80e93d..9330ce24f1bb 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1313,7 +1313,7 @@ void __cleanup_sighand(struct sighand_struct *sighand)
 	if (atomic_dec_and_test(&sighand->count)) {
 		signalfd_cleanup(sighand);
 		/*
-		 * sighand_cachep is SLAB_DESTROY_BY_RCU so we can free it
+		 * sighand_cachep is SLAB_TYPESAFE_BY_RCU so we can free it
 		 * without an RCU grace period, see __lock_task_sighand().
 		 */
 		kmem_cache_free(sighand_cachep, sighand);
@@ -2144,7 +2144,7 @@ void __init proc_caches_init(void)
 {
 	sighand_cachep = kmem_cache_create("sighand_cache",
 			sizeof(struct sighand_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_DESTROY_BY_RCU|
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_TYPESAFE_BY_RCU|
 			SLAB_NOTRACK|SLAB_ACCOUNT, sighand_ctor);
 	signal_cachep = kmem_cache_create("signal_cache",
 			sizeof(struct signal_struct), 0,

commit e96a7705e7d3fef96aec9b590c63b2f6f7d2ba22
Author: Xunlei Pang <xlpang@redhat.com>
Date:   Thu Mar 23 15:56:08 2017 +0100

    sched/rtmutex/deadline: Fix a PI crash for deadline tasks
    
    A crash happened while I was playing with deadline PI rtmutex.
    
        BUG: unable to handle kernel NULL pointer dereference at 0000000000000018
        IP: [<ffffffff810eeb8f>] rt_mutex_get_top_task+0x1f/0x30
        PGD 232a75067 PUD 230947067 PMD 0
        Oops: 0000 [#1] SMP
        CPU: 1 PID: 10994 Comm: a.out Not tainted
    
        Call Trace:
        [<ffffffff810b658c>] enqueue_task+0x2c/0x80
        [<ffffffff810ba763>] activate_task+0x23/0x30
        [<ffffffff810d0ab5>] pull_dl_task+0x1d5/0x260
        [<ffffffff810d0be6>] pre_schedule_dl+0x16/0x20
        [<ffffffff8164e783>] __schedule+0xd3/0x900
        [<ffffffff8164efd9>] schedule+0x29/0x70
        [<ffffffff8165035b>] __rt_mutex_slowlock+0x4b/0xc0
        [<ffffffff81650501>] rt_mutex_slowlock+0xd1/0x190
        [<ffffffff810eeb33>] rt_mutex_timed_lock+0x53/0x60
        [<ffffffff810ecbfc>] futex_lock_pi.isra.18+0x28c/0x390
        [<ffffffff810ed8b0>] do_futex+0x190/0x5b0
        [<ffffffff810edd50>] SyS_futex+0x80/0x180
    
    This is because rt_mutex_enqueue_pi() and rt_mutex_dequeue_pi()
    are only protected by pi_lock when operating pi waiters, while
    rt_mutex_get_top_task(), will access them with rq lock held but
    not holding pi_lock.
    
    In order to tackle it, we introduce new "pi_top_task" pointer
    cached in task_struct, and add new rt_mutex_update_top_task()
    to update its value, it can be called by rt_mutex_setprio()
    which held both owner's pi_lock and rq lock. Thus "pi_top_task"
    can be safely accessed by enqueue_task_dl() under rq lock.
    
    Originally-From: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Xunlei Pang <xlpang@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: juri.lelli@arm.com
    Cc: bigeasy@linutronix.de
    Cc: mathieu.desnoyers@efficios.com
    Cc: jdesfossez@efficios.com
    Cc: bristot@redhat.com
    Link: http://lkml.kernel.org/r/20170323150216.157682758@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6c463c80e93d..b30196a00b0d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1438,6 +1438,7 @@ static void rt_mutex_init_task(struct task_struct *p)
 #ifdef CONFIG_RT_MUTEXES
 	p->pi_waiters = RB_ROOT;
 	p->pi_waiters_leftmost = NULL;
+	p->pi_top_task = NULL;
 	p->pi_blocked_on = NULL;
 #endif
 }

commit e4e55b47ed9ae2c05ff062601ff6dacbe9dc4775
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Fri Mar 24 20:46:33 2017 +0900

    LSM: Revive security_task_alloc() hook and per "struct task_struct" security blob.
    
    We switched from "struct task_struct"->security to "struct cred"->security
    in Linux 2.6.29. But not all LSM modules were happy with that change.
    TOMOYO LSM module is an example which want to use per "struct task_struct"
    security blob, for TOMOYO's security context is defined based on "struct
    task_struct" rather than "struct cred". AppArmor LSM module is another
    example which want to use it, for AppArmor is currently abusing the cred
    a little bit to store the change_hat and setexeccon info. Although
    security_task_free() hook was revived in Linux 3.4 because Yama LSM module
    wanted to release per "struct task_struct" security blob,
    security_task_alloc() hook and "struct task_struct"->security field were
    not revived. Nowadays, we are getting proposals of lightweight LSM modules
    which want to use per "struct task_struct" security blob.
    
    We are already allowing multiple concurrent LSM modules (up to one fully
    armored module which uses "struct cred"->security field or exclusive hooks
    like security_xfrm_state_pol_flow_match(), plus unlimited number of
    lightweight modules which do not use "struct cred"->security nor exclusive
    hooks) as long as they are built into the kernel. But this patch does not
    implement variable length "struct task_struct"->security field which will
    become needed when multiple LSM modules want to use "struct task_struct"->
    security field. Although it won't be difficult to implement variable length
    "struct task_struct"->security field, let's think about it after we merged
    this patch.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Acked-by: John Johansen <john.johansen@canonical.com>
    Acked-by: Serge Hallyn <serge@hallyn.com>
    Acked-by: Casey Schaufler <casey@schaufler-ca.com>
    Tested-by: Djalal Harouni <tixxdz@gmail.com>
    Acked-by: José Bollo <jobol@nonadev.net>
    Cc: Paul Moore <paul@paul-moore.com>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: Eric Paris <eparis@parisplace.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: James Morris <james.l.morris@oracle.com>
    Cc: José Bollo <jobol@nonadev.net>
    Signed-off-by: James Morris <james.l.morris@oracle.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6c463c80e93d..3d32513d6c73 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1679,9 +1679,12 @@ static __latent_entropy struct task_struct *copy_process(
 		goto bad_fork_cleanup_perf;
 	/* copy all the process information */
 	shm_init_task(p);
-	retval = copy_semundo(clone_flags, p);
+	retval = security_task_alloc(p, clone_flags);
 	if (retval)
 		goto bad_fork_cleanup_audit;
+	retval = copy_semundo(clone_flags, p);
+	if (retval)
+		goto bad_fork_cleanup_security;
 	retval = copy_files(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_semundo;
@@ -1903,6 +1906,8 @@ static __latent_entropy struct task_struct *copy_process(
 	exit_files(p); /* blocking */
 bad_fork_cleanup_semundo:
 	exit_sem(p);
+bad_fork_cleanup_security:
+	security_task_free(p);
 bad_fork_cleanup_audit:
 	audit_free(p);
 bad_fork_cleanup_perf:

commit e422267322cd319e2695a535e47c5b1feeac45eb
Author: Hari Bathini <hbathini@linux.vnet.ibm.com>
Date:   Wed Mar 8 02:11:36 2017 +0530

    perf: Add PERF_RECORD_NAMESPACES to include namespaces related info
    
    With the advert of container technologies like docker, that depend on
    namespaces for isolation, there is a need for tracing support for
    namespaces. This patch introduces new PERF_RECORD_NAMESPACES event for
    recording namespaces related info. By recording info for every
    namespace, it is left to userspace to take a call on the definition of a
    container and trace containers by updating perf tool accordingly.
    
    Each namespace has a combination of device and inode numbers. Though
    every namespace has the same device number currently, that may change in
    future to avoid the need for a namespace of namespaces. Considering such
    possibility, record both device and inode numbers separately for each
    namespace.
    
    Signed-off-by: Hari Bathini <hbathini@linux.vnet.ibm.com>
    Acked-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Alexei Starovoitov <ast@fb.com>
    Cc: Ananth N Mavinakayanahalli <ananth@linux.vnet.ibm.com>
    Cc: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Cc: Brendan Gregg <brendan.d.gregg@gmail.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Sargun Dhillon <sargun@sargun.me>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/148891929686.25309.2827618988917007768.stgit@hbathini.in.ibm.com
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6c463c80e93d..afa2947286cd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2352,6 +2352,8 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 		}
 	}
 
+	perf_event_namespaces(current);
+
 bad_unshare_cleanup_cred:
 	if (new_cred)
 		put_cred(new_cred);

commit d83a7cb375eec21f04c83542395d08b2f6641da2
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Feb 13 19:42:40 2017 -0600

    livepatch: change to a per-task consistency model
    
    Change livepatch to use a basic per-task consistency model.  This is the
    foundation which will eventually enable us to patch those ~10% of
    security patches which change function or data semantics.  This is the
    biggest remaining piece needed to make livepatch more generally useful.
    
    This code stems from the design proposal made by Vojtech [1] in November
    2014.  It's a hybrid of kGraft and kpatch: it uses kGraft's per-task
    consistency and syscall barrier switching combined with kpatch's stack
    trace switching.  There are also a number of fallback options which make
    it quite flexible.
    
    Patches are applied on a per-task basis, when the task is deemed safe to
    switch over.  When a patch is enabled, livepatch enters into a
    transition state where tasks are converging to the patched state.
    Usually this transition state can complete in a few seconds.  The same
    sequence occurs when a patch is disabled, except the tasks converge from
    the patched state to the unpatched state.
    
    An interrupt handler inherits the patched state of the task it
    interrupts.  The same is true for forked tasks: the child inherits the
    patched state of the parent.
    
    Livepatch uses several complementary approaches to determine when it's
    safe to patch tasks:
    
    1. The first and most effective approach is stack checking of sleeping
       tasks.  If no affected functions are on the stack of a given task,
       the task is patched.  In most cases this will patch most or all of
       the tasks on the first try.  Otherwise it'll keep trying
       periodically.  This option is only available if the architecture has
       reliable stacks (HAVE_RELIABLE_STACKTRACE).
    
    2. The second approach, if needed, is kernel exit switching.  A
       task is switched when it returns to user space from a system call, a
       user space IRQ, or a signal.  It's useful in the following cases:
    
       a) Patching I/O-bound user tasks which are sleeping on an affected
          function.  In this case you have to send SIGSTOP and SIGCONT to
          force it to exit the kernel and be patched.
       b) Patching CPU-bound user tasks.  If the task is highly CPU-bound
          then it will get patched the next time it gets interrupted by an
          IRQ.
       c) In the future it could be useful for applying patches for
          architectures which don't yet have HAVE_RELIABLE_STACKTRACE.  In
          this case you would have to signal most of the tasks on the
          system.  However this isn't supported yet because there's
          currently no way to patch kthreads without
          HAVE_RELIABLE_STACKTRACE.
    
    3. For idle "swapper" tasks, since they don't ever exit the kernel, they
       instead have a klp_update_patch_state() call in the idle loop which
       allows them to be patched before the CPU enters the idle state.
    
       (Note there's not yet such an approach for kthreads.)
    
    All the above approaches may be skipped by setting the 'immediate' flag
    in the 'klp_patch' struct, which will disable per-task consistency and
    patch all tasks immediately.  This can be useful if the patch doesn't
    change any function or data semantics.  Note that, even with this flag
    set, it's possible that some tasks may still be running with an old
    version of the function, until that function returns.
    
    There's also an 'immediate' flag in the 'klp_func' struct which allows
    you to specify that certain functions in the patch can be applied
    without per-task consistency.  This might be useful if you want to patch
    a common function like schedule(), and the function change doesn't need
    consistency but the rest of the patch does.
    
    For architectures which don't have HAVE_RELIABLE_STACKTRACE, the user
    must set patch->immediate which causes all tasks to be patched
    immediately.  This option should be used with care, only when the patch
    doesn't change any function or data semantics.
    
    In the future, architectures which don't have HAVE_RELIABLE_STACKTRACE
    may be allowed to use per-task consistency if we can come up with
    another way to patch kthreads.
    
    The /sys/kernel/livepatch/<patch>/transition file shows whether a patch
    is in transition.  Only a single patch (the topmost patch on the stack)
    can be in transition at a given time.  A patch can remain in transition
    indefinitely, if any of the tasks are stuck in the initial patch state.
    
    A transition can be reversed and effectively canceled by writing the
    opposite value to the /sys/kernel/livepatch/<patch>/enabled file while
    the transition is in progress.  Then all the tasks will attempt to
    converge back to the original patch state.
    
    [1] https://lkml.kernel.org/r/20141107140458.GA21774@suse.cz
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Ingo Molnar <mingo@kernel.org>        # for the scheduler changes
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6c463c80e93d..942cbcd07c18 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -87,6 +87,7 @@
 #include <linux/compiler.h>
 #include <linux/sysctl.h>
 #include <linux/kcov.h>
+#include <linux/livepatch.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -1797,6 +1798,8 @@ static __latent_entropy struct task_struct *copy_process(
 		p->parent_exec_id = current->self_exec_id;
 	}
 
+	klp_copy_process(p);
+
 	spin_lock(&current->sighand->siglock);
 
 	/*

commit 6bfbaa51ed47774492d83d182a86068cc35aa4c6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 21:37:49 2017 +0100

    sched/headers, RCU: Move rcu_copy_process() from <linux/sched/task.h> to kernel/fork.c
    
    Move rcu_copy_process() into kernel/fork.c, which is the only
    user of this inline function.
    
    This simplifies <linux/sched/task.h> to the level that <linux/sched.h>
    does not have to be included in it anymore - which change is done
    in a subsequent patch.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 916e78004b8f..6c463c80e93d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1465,6 +1465,21 @@ init_task_pid(struct task_struct *task, enum pid_type type, struct pid *pid)
 	 task->pids[type].pid = pid;
 }
 
+static inline void rcu_copy_process(struct task_struct *p)
+{
+#ifdef CONFIG_PREEMPT_RCU
+	p->rcu_read_lock_nesting = 0;
+	p->rcu_read_unlock_special.s = 0;
+	p->rcu_blocked_node = NULL;
+	INIT_LIST_HEAD(&p->rcu_node_entry);
+#endif /* #ifdef CONFIG_PREEMPT_RCU */
+#ifdef CONFIG_TASKS_RCU
+	p->rcu_tasks_holdout = false;
+	INIT_LIST_HEAD(&p->rcu_tasks_holdout_list);
+	p->rcu_tasks_idle_cpu = -1;
+#endif /* #ifdef CONFIG_TASKS_RCU */
+}
+
 /*
  * This creates a new process as a copy of the old one,
  * but does not actually start it yet.

commit 32ef5517c298042ed58408545f475df43afe1f24
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 11:48:36 2017 +0100

    sched/headers: Prepare to move cputime functionality from <linux/sched.h> into <linux/sched/cputime.h>
    
    Introduce a trivial, mostly empty <linux/sched/cputime.h> header
    to prepare for the moving of cputime functionality out of sched.h.
    
    Update all code that relies on these facilities.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c87b6f03c710..916e78004b8f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -20,6 +20,7 @@
 #include <linux/sched/stat.h>
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
+#include <linux/sched/cputime.h>
 #include <linux/rtmutex.h>
 #include <linux/init.h>
 #include <linux/unistd.h>

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0af64d5d8b73..c87b6f03c710 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -19,6 +19,7 @@
 #include <linux/sched/numa_balancing.h>
 #include <linux/sched/stat.h>
 #include <linux/sched/task.h>
+#include <linux/sched/task_stack.h>
 #include <linux/rtmutex.h>
 #include <linux/init.h>
 #include <linux/unistd.h>

commit 299300258d1bc4e997b7db340a2e06636757fe2e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task.h>
    
    We are going to split <linux/sched/task.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8632905f64c5..0af64d5d8b73 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -18,6 +18,7 @@
 #include <linux/sched/user.h>
 #include <linux/sched/numa_balancing.h>
 #include <linux/sched/stat.h>
+#include <linux/sched/task.h>
 #include <linux/rtmutex.h>
 #include <linux/init.h>
 #include <linux/unistd.h>

commit 03441a3482a31462c93509939a388877e3cd9261
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/stat.h>
    
    We are going to split <linux/sched/stat.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/stat.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8fbe8bcd1e20..8632905f64c5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -17,6 +17,7 @@
 #include <linux/sched/coredump.h>
 #include <linux/sched/user.h>
 #include <linux/sched/numa_balancing.h>
+#include <linux/sched/stat.h>
 #include <linux/rtmutex.h>
 #include <linux/init.h>
 #include <linux/unistd.h>

commit 037741a6d4ab2e4b0580dae97aca963d8a8dc68f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 10:08:30 2017 +0100

    sched/headers: Prepare for the removal of <linux/rtmutex.h> from <linux/sched.h>
    
    Fix up missing #includes in other places that rely on sched.h doing that for them.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 08d6c091ac7c..8fbe8bcd1e20 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -17,6 +17,7 @@
 #include <linux/sched/coredump.h>
 #include <linux/sched/user.h>
 #include <linux/sched/numa_balancing.h>
+#include <linux/rtmutex.h>
 #include <linux/init.h>
 #include <linux/unistd.h>
 #include <linux/module.h>

commit 6a3827d7509cbf96b7e961f8957c1f01d1bcf894
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:31 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/numa_balancing.h>
    
    We are going to split <linux/sched/numa_balancing.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/numa_balancing.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a5ad1e4ab604..08d6c091ac7c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -16,6 +16,7 @@
 #include <linux/sched/mm.h>
 #include <linux/sched/coredump.h>
 #include <linux/sched/user.h>
+#include <linux/sched/numa_balancing.h>
 #include <linux/init.h>
 #include <linux/unistd.h>
 #include <linux/module.h>

commit 8703e8a465b1e9cadc3680b4b1248f5987e54518
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/user.h>
    
    We are going to split <linux/sched/user.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/user.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7332448b668a..a5ad1e4ab604 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -15,6 +15,7 @@
 #include <linux/sched/autogroup.h>
 #include <linux/sched/mm.h>
 #include <linux/sched/coredump.h>
+#include <linux/sched/user.h>
 #include <linux/init.h>
 #include <linux/unistd.h>
 #include <linux/module.h>

commit f7ccbae45c5e2c1077654b0e857e7efb1aa31c92
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/coredump.h>
    
    We are going to split <linux/sched/coredump.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/coredump.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1875468e2a81..7332448b668a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -14,6 +14,7 @@
 #include <linux/slab.h>
 #include <linux/sched/autogroup.h>
 #include <linux/sched/mm.h>
+#include <linux/sched/coredump.h>
 #include <linux/init.h>
 #include <linux/unistd.h>
 #include <linux/module.h>

commit 6e84f31522f931027bf695752087ece278c10d3f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:29 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/mm.h>
    
    We are going to split <linux/sched/mm.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/mm.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    The APIs that are going to be moved first are:
    
       mm_alloc()
       __mmdrop()
       mmdrop()
       mmdrop_async_fn()
       mmdrop_async()
       mmget_not_zero()
       mmput()
       mmput_async()
       get_task_mm()
       mm_access()
       mm_release()
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 234f4bfb8efd..1875468e2a81 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -13,6 +13,7 @@
 
 #include <linux/slab.h>
 #include <linux/sched/autogroup.h>
+#include <linux/sched/mm.h>
 #include <linux/init.h>
 #include <linux/unistd.h>
 #include <linux/module.h>

commit 4eb5aaa3af8a54e5e9bac90e2b42bbab1f1ee5a3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:29 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/autogroup.h>
    
    We are going to split <linux/sched/autogroup.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/autogroup.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d043fedc03c8..234f4bfb8efd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -12,6 +12,7 @@
  */
 
 #include <linux/slab.h>
+#include <linux/sched/autogroup.h>
 #include <linux/init.h>
 #include <linux/unistd.h>
 #include <linux/module.h>

commit 780de9dd2720debc14c501dab4dc80d1f75ad50e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 11:50:56 2017 +0100

    sched/headers, cgroups: Remove the threadgroup_change_*() wrappery
    
    threadgroup_change_begin()/end() is a pointless wrapper around
    cgroup_threadgroup_change_begin()/end(), minus a might_sleep()
    in the !CONFIG_CGROUPS=y case.
    
    Remove the wrappery, move the might_sleep() (the down_read()
    already has a might_sleep() check).
    
    This debloats <linux/sched.h> a bit and simplifies this API.
    
    Update all call sites.
    
    No change in functionality.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 246bf9aaf9df..d043fedc03c8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1746,7 +1746,7 @@ static __latent_entropy struct task_struct *copy_process(
 	INIT_LIST_HEAD(&p->thread_group);
 	p->task_works = NULL;
 
-	threadgroup_change_begin(current);
+	cgroup_threadgroup_change_begin(current);
 	/*
 	 * Ensure that the cgroup subsystem policies allow the new process to be
 	 * forked. It should be noted the the new process's css_set can be changed
@@ -1843,7 +1843,7 @@ static __latent_entropy struct task_struct *copy_process(
 
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
-	threadgroup_change_end(current);
+	cgroup_threadgroup_change_end(current);
 	perf_event_fork(p);
 
 	trace_task_newtask(p, clone_flags);
@@ -1854,7 +1854,7 @@ static __latent_entropy struct task_struct *copy_process(
 bad_fork_cancel_cgroup:
 	cgroup_cancel_fork(p);
 bad_fork_free_pid:
-	threadgroup_change_end(current);
+	cgroup_threadgroup_change_end(current);
 	if (pid != &init_struct_pid)
 		free_pid(pid);
 bad_fork_cleanup_thread:

commit 3fce371bfac2be0396ffc1e763600e6c6b1bb52a
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Mon Feb 27 14:30:10 2017 -0800

    mm: add new mmget() helper
    
    Apart from adding the helper function itself, the rest of the kernel is
    converted mechanically using:
    
      git grep -l 'atomic_inc.*mm_users' | xargs sed -i 's/atomic_inc(&\(.*\)->mm_users);/mmget\(\1\);/'
      git grep -l 'atomic_inc.*mm_users' | xargs sed -i 's/atomic_inc(&\(.*\)\.mm_users);/mmget\(\&\1\);/'
    
    This is needed for a later patch that hooks into the helper, but might
    be a worthwhile cleanup on its own.
    
    (Michal Hocko provided most of the kerneldoc comment.)
    
    Link: http://lkml.kernel.org/r/20161218123229.22952-2-vegard.nossum@oracle.com
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 348fe73155bc..246bf9aaf9df 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1000,7 +1000,7 @@ struct mm_struct *get_task_mm(struct task_struct *task)
 		if (task->flags & PF_KTHREAD)
 			mm = NULL;
 		else
-			atomic_inc(&mm->mm_users);
+			mmget(mm);
 	}
 	task_unlock(task);
 	return mm;
@@ -1188,7 +1188,7 @@ static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 	vmacache_flush(tsk);
 
 	if (clone_flags & CLONE_VM) {
-		atomic_inc(&oldmm->mm_users);
+		mmget(oldmm);
 		mm = oldmm;
 		goto good_mm;
 	}

commit f1ef09fde17f9b77ca1435a5b53a28b203afb81c
Merge: ef96152e6a36 ace0c791e6c3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 23 20:33:51 2017 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull namespace updates from Eric Biederman:
     "There is a lot here. A lot of these changes result in subtle user
      visible differences in kernel behavior. I don't expect anything will
      care but I will revert/fix things immediately if any regressions show
      up.
    
      From Seth Forshee there is a continuation of the work to make the vfs
      ready for unpriviled mounts. We had thought the previous changes
      prevented the creation of files outside of s_user_ns of a filesystem,
      but it turns we missed the O_CREAT path. Ooops.
    
      Pavel Tikhomirov and Oleg Nesterov worked together to fix a long
      standing bug in the implemenation of PR_SET_CHILD_SUBREAPER where only
      children that are forked after the prctl are considered and not
      children forked before the prctl. The only known user of this prctl
      systemd forks all children after the prctl. So no userspace
      regressions will occur. Holding earlier forked children to the same
      rules as later forked children creates a semantic that is sane enough
      to allow checkpoing of processes that use this feature.
    
      There is a long delayed change by Nikolay Borisov to limit inotify
      instances inside a user namespace.
    
      Michael Kerrisk extends the API for files used to maniuplate
      namespaces with two new trivial ioctls to allow discovery of the
      hierachy and properties of namespaces.
    
      Konstantin Khlebnikov with the help of Al Viro adds code that when a
      network namespace exits purges it's sysctl entries from the dcache. As
      in some circumstances this could use a lot of memory.
    
      Vivek Goyal fixed a bug with stacked filesystems where the permissions
      on the wrong inode were being checked.
    
      I continue previous work on ptracing across exec. Allowing a file to
      be setuid across exec while being ptraced if the tracer has enough
      credentials in the user namespace, and if the process has CAP_SETUID
      in it's own namespace. Proc files for setuid or otherwise undumpable
      executables are now owned by the root in the user namespace of their
      mm. Allowing debugging of setuid applications in containers to work
      better.
    
      A bug I introduced with permission checking and automount is now
      fixed. The big change is to mark the mounts that the kernel initiates
      as a result of an automount. This allows the permission checks in sget
      to be safely suppressed for this kind of mount. As the permission
      check happened when the original filesystem was mounted.
    
      Finally a special case in the mount namespace is removed preventing
      unbounded chains in the mount hash table, and making the semantics
      simpler which benefits CRIU.
    
      The vfs fix along with related work in ima and evm I believe makes us
      ready to finish developing and merge fully unprivileged mounts of the
      fuse filesystem. The cleanups of the mount namespace makes discussing
      how to fix the worst case complexity of umount. The stacked filesystem
      fixes pave the way for adding multiple mappings for the filesystem
      uids so that efficient and safer containers can be implemented"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      proc/sysctl: Don't grab i_lock under sysctl_lock.
      vfs: Use upper filesystem inode in bprm_fill_uid()
      proc/sysctl: prune stale dentries during unregistering
      mnt: Tuck mounts under others instead of creating shadow/side mounts.
      prctl: propagate has_child_subreaper flag to every descendant
      introduce the walk_process_tree() helper
      nsfs: Add an ioctl() to return owner UID of a userns
      fs: Better permission checking for submounts
      exit: fix the setns() && PR_SET_CHILD_SUBREAPER interaction
      vfs: open() with O_CREAT should not create inodes with unknown ids
      nsfs: Add an ioctl() to return the namespace type
      proc: Better ownership of files for non-dumpable tasks in user namespaces
      exec: Remove LSM_UNSAFE_PTRACE_CAP
      exec: Test the ptracer's saved cred to see if the tracee can gain caps
      exec: Don't reset euid and egid when the tracee has CAP_SETUID
      inotify: Convert to using per-namespace limits

commit 893e26e61d04eac974ded0c11e1647b335c8cb7b
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Wed Feb 22 15:42:27 2017 -0800

    userfaultfd: non-cooperative: Add fork() event
    
    When the mm with uffd-ed vmas fork()-s the respective vmas notify their
    uffds with the event which contains a descriptor with new uffd.  This
    new descriptor can then be used to get events from the child and
    populate its mm with data.  Note, that there can be different uffd-s
    controlling different vmas within one mm, so first we should collect all
    those uffds (and ctx-s) in a list and then notify them all one by one
    but only once per fork().
    
    The context is created at fork() time but the descriptor, file struct
    and anon inode object is created at event read time.  So some trickery
    is added to the userfaultfd_ctx_read() to handle the ctx queues' locking
    vs file creation.
    
    Another thing worth noticing is that the task that fork()-s waits for
    the uffd event to get processed WITHOUT the mmap sem.
    
    [aarcange@redhat.com: build warning fix]
      Link: http://lkml.kernel.org/r/20161216144821.5183-10-aarcange@redhat.com
    Link: http://lkml.kernel.org/r/20161216144821.5183-9-aarcange@redhat.com
    Signed-off-by: Pavel Emelyanov <xemul@parallels.com>
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Michael Rapoport <RAPOPORT@il.ibm.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ff82e24573b6..d12fcc4db8a3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -55,6 +55,7 @@
 #include <linux/rmap.h>
 #include <linux/ksm.h>
 #include <linux/acct.h>
+#include <linux/userfaultfd_k.h>
 #include <linux/tsacct_kern.h>
 #include <linux/cn_proc.h>
 #include <linux/freezer.h>
@@ -561,6 +562,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	struct rb_node **rb_link, *rb_parent;
 	int retval;
 	unsigned long charge;
+	LIST_HEAD(uf);
 
 	uprobe_start_dup_mmap();
 	if (down_write_killable(&oldmm->mmap_sem)) {
@@ -617,12 +619,13 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 		if (retval)
 			goto fail_nomem_policy;
 		tmp->vm_mm = mm;
+		retval = dup_userfaultfd(tmp, &uf);
+		if (retval)
+			goto fail_nomem_anon_vma_fork;
 		if (anon_vma_fork(tmp, mpnt))
 			goto fail_nomem_anon_vma_fork;
-		tmp->vm_flags &=
-			~(VM_LOCKED|VM_LOCKONFAULT|VM_UFFD_MISSING|VM_UFFD_WP);
+		tmp->vm_flags &= ~(VM_LOCKED | VM_LOCKONFAULT);
 		tmp->vm_next = tmp->vm_prev = NULL;
-		tmp->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
 		file = tmp->vm_file;
 		if (file) {
 			struct inode *inode = file_inode(file);
@@ -678,6 +681,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 	up_write(&mm->mmap_sem);
 	flush_tlb_mm(oldmm);
 	up_write(&oldmm->mmap_sem);
+	dup_userfaultfd_complete(&uf);
 fail_uprobe_end:
 	uprobe_end_dup_mmap();
 	return retval;

commit 42e1b14b6e1455ece2ccbe474c25388d0230a590
Merge: 828cad8ea05d 95cb64c1fe61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 13:23:30 2017 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Implement wraparound-safe refcount_t and kref_t types based on
         generic atomic primitives (Peter Zijlstra)
    
       - Improve and fix the ww_mutex code (Nicolai Hähnle)
    
       - Add self-tests to the ww_mutex code (Chris Wilson)
    
       - Optimize percpu-rwsems with the 'rcuwait' mechanism (Davidlohr
         Bueso)
    
       - Micro-optimize the current-task logic all around the core kernel
         (Davidlohr Bueso)
    
       - Tidy up after recent optimizations: remove stale code and APIs,
         clean up the code (Waiman Long)
    
       - ... plus misc fixes, updates and cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (50 commits)
      fork: Fix task_struct alignment
      locking/spinlock/debug: Remove spinlock lockup detection code
      lockdep: Fix incorrect condition to print bug msgs for MAX_LOCKDEP_CHAIN_HLOCKS
      lkdtm: Convert to refcount_t testing
      kref: Implement 'struct kref' using refcount_t
      refcount_t: Introduce a special purpose refcount type
      sched/wake_q: Clarify queue reinit comment
      sched/wait, rcuwait: Fix typo in comment
      locking/mutex: Fix lockdep_assert_held() fail
      locking/rtmutex: Flip unlikely() branch to likely() in __rt_mutex_slowlock()
      locking/rwsem: Reinit wake_q after use
      locking/rwsem: Remove unnecessary atomic_long_t casts
      jump_labels: Move header guard #endif down where it belongs
      locking/atomic, kref: Implement kref_put_lock()
      locking/ww_mutex: Turn off __must_check for now
      locking/atomic, kref: Avoid more abuse
      locking/atomic, kref: Use kref_get_unless_zero() more
      locking/atomic, kref: Kill kref_sub()
      locking/atomic, kref: Add kref_read()
      locking/atomic, kref: Add KREF_INIT()
      ...

commit 828cad8ea05d194d8a9452e0793261c2024c23a2
Merge: 60c906bab124 bb3bac2ca9a3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 12:52:55 2017 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this (fairly busy) cycle were:
    
       - There was a class of scheduler bugs related to forgetting to update
         the rq-clock timestamp which can cause weird and hard to debug
         problems, so there's a new debug facility for this: which uncovered
         a whole lot of bugs which convinced us that we want to keep the
         debug facility.
    
         (Peter Zijlstra, Matt Fleming)
    
       - Various cputime related updates: eliminate cputime and use u64
         nanoseconds directly, simplify and improve the arch interfaces,
         implement delayed accounting more widely, etc. - (Frederic
         Weisbecker)
    
       - Move code around for better structure plus cleanups (Ingo Molnar)
    
       - Move IO schedule accounting deeper into the scheduler plus related
         changes to improve the situation (Tejun Heo)
    
       - ... plus a round of sched/rt and sched/deadline fixes, plus other
         fixes, updats and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (85 commits)
      sched/core: Remove unlikely() annotation from sched_move_task()
      sched/autogroup: Rename auto_group.[ch] to autogroup.[ch]
      sched/topology: Split out scheduler topology code from core.c into topology.c
      sched/core: Remove unnecessary #include headers
      sched/rq_clock: Consolidate the ordering of the rq_clock methods
      delayacct: Include <uapi/linux/taskstats.h>
      sched/core: Clean up comments
      sched/rt: Show the 'sched_rr_timeslice' SCHED_RR timeslice tuning knob in milliseconds
      sched/clock: Add dummy clear_sched_clock_stable() stub function
      sched/cputime: Remove generic asm headers
      sched/cputime: Remove unused nsec_to_cputime()
      s390, sched/cputime: Remove unused cputime definitions
      powerpc, sched/cputime: Remove unused cputime definitions
      s390, sched/cputime: Make arch_cpu_idle_time() to return nsecs
      ia64, sched/cputime: Remove unused cputime definitions
      ia64: Convert vtime to use nsec units directly
      ia64, sched/cputime: Move the nsecs based cputime headers to the last arch using it
      sched/cputime: Remove jiffies based cputime
      sched/cputime, vtime: Return nsecs instead of cputime_t to account
      sched/cputime: Complete nsec conversion of tick based accounting
      ...

commit 95cb64c1fe61e70685a95f6260c8e9cd219fe08c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Feb 18 15:26:45 2017 +0100

    fork: Fix task_struct alignment
    
    Stupid bug that wrecked the alignment of task_struct and causes WARN()s
    in the x86 FPU code on some platforms.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Tested-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: e274795ea7b7 ("locking/mutex: Fix mutex handoff")
    Link: http://lkml.kernel.org/r/20170218142645.GH6500@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a90510d0bbf8..ea33f8adc032 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -434,7 +434,7 @@ void __init fork_init(void)
 #ifndef ARCH_MIN_TASKALIGN
 #define ARCH_MIN_TASKALIGN	0
 #endif
-	int align = min_t(int, L1_CACHE_BYTES, ARCH_MIN_TASKALIGN);
+	int align = max_t(int, L1_CACHE_BYTES, ARCH_MIN_TASKALIGN);
 
 	/* create a slab on which task_structs can be allocated */
 	task_struct_cachep = kmem_cache_create("task_struct",

commit 749860ce242798fb090557a5a7868dee40af9268
Author: Pavel Tikhomirov <ptikhomirov@virtuozzo.com>
Date:   Mon Jan 30 18:06:12 2017 +0300

    prctl: propagate has_child_subreaper flag to every descendant
    
    If process forks some children when it has is_child_subreaper
    flag enabled they will inherit has_child_subreaper flag - first
    group, when is_child_subreaper is disabled forked children will
    not inherit it - second group. So child-subreaper does not reparent
    all his descendants when their parents die. Having these two
    differently behaving groups can lead to confusion. Also it is
    a problem for CRIU, as when we restore process tree we need to
    somehow determine which descendants belong to which group and
    much harder - to put them exactly to these group.
    
    To simplify these we can add a propagation of has_child_subreaper
    flag on PR_SET_CHILD_SUBREAPER, walking all descendants of child-
    subreaper to setup has_child_subreaper flag.
    
    In common cases when process like systemd first sets itself to
    be a child-subreaper and only after that forks its services, we will
    have zero-length list of descendants to walk. Testing with binary
    subtree of 2^15 processes prctl took < 0.007 sec and has shown close
    to linear dependency(~0.2 * n * usec) on lower numbers of processes.
    
    Moreover, I doubt someone intentionaly pre-forks the children whitch
    should reparent to init before becoming subreaper, because some our
    ancestor migh have had is_child_subreaper flag while forking our
    sub-tree and our childs will all inherit has_child_subreaper flag,
    and we have no way to influence it. And only way to check if we have
    no has_child_subreaper flag is to create some childs, kill them and
    see where they will reparent to.
    
    Using walk_process_tree helper to walk subtree, thanks to Oleg! Timing
    seems to be the same.
    
    Optimize:
    
    a) When descendant already has has_child_subreaper flag all his subtree
    has it too already.
    
    * for a) to be true need to move has_child_subreaper inheritance under
    the same tasklist_lock with adding task to its ->real_parent->children
    as without it process can inherit zero has_child_subreaper, then we
    set 1 to it's parent flag, check that parent has no more children, and
    only after child with wrong flag is added to the tree.
    
    * Also make these inheritance more clear by using real_parent instead of
    current, as on clone(CLONE_PARENT) if current has is_child_subreaper
    and real_parent has no is_child_subreaper or has_child_subreaper, child
    will have has_child_subreaper flag set without actually having a
    subreaper in it's ancestors.
    
    b) When some descendant is child_reaper, it's subtree is in different
    pidns from us(original child-subreaper) and processes from other pidns
    will never reparent to us.
    
    So we can skip their(a,b) subtree from walk.
    
    v2: switch to walk_process_tree() general helper, move
    has_child_subreaper inheritance
    v3: remove csr_descendant leftover, change current to real_parent
    in has_child_subreaper inheritance
    v4: small commit message fix
    
    Fixes: ebec18a6d3aa ("prctl: add PR_{SET,GET}_CHILD_SUBREAPER to allow simple process supervision")
    Signed-off-by: Pavel Tikhomirov <ptikhomirov@virtuozzo.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 135b7a49ad59..c814e590cf76 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1367,9 +1367,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->oom_score_adj = current->signal->oom_score_adj;
 	sig->oom_score_adj_min = current->signal->oom_score_adj_min;
 
-	sig->has_child_subreaper = current->signal->has_child_subreaper ||
-				   current->signal->is_child_subreaper;
-
 	mutex_init(&sig->cred_guard_mutex);
 
 	return 0;
@@ -1800,6 +1797,13 @@ static __latent_entropy struct task_struct *copy_process(
 
 			p->signal->leader_pid = pid;
 			p->signal->tty = tty_kref_get(current->signal->tty);
+			/*
+			 * Inherit has_child_subreaper flag under the same
+			 * tasklist_lock with adding child to the process tree
+			 * for propagate_has_child_subreaper optimization.
+			 */
+			p->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||
+							 p->real_parent->signal->is_child_subreaper;
 			list_add_tail(&p->sibling, &p->real_parent->children);
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			attach_pid(p, PIDTYPE_PGID);

commit 0f1b92cbdd0309afae0af1963e8cccddb3d2eaff
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jan 30 18:06:11 2017 +0300

    introduce the walk_process_tree() helper
    
    Add the new helper to walk the process tree, the next patch adds a user.
    Note that it visits the group leaders only, proc_visitor can do
    for_each_thread itself or we can trivially extend walk_process_tree() to
    do this.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Pavel Tikhomirov <ptikhomirov@virtuozzo.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 11c5c8ab827c..135b7a49ad59 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -2053,6 +2053,38 @@ SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 }
 #endif
 
+void walk_process_tree(struct task_struct *top, proc_visitor visitor, void *data)
+{
+	struct task_struct *leader, *parent, *child;
+	int res;
+
+	read_lock(&tasklist_lock);
+	leader = top = top->group_leader;
+down:
+	for_each_thread(leader, parent) {
+		list_for_each_entry(child, &parent->children, sibling) {
+			res = visitor(child, data);
+			if (res) {
+				if (res < 0)
+					goto out;
+				leader = child;
+				goto down;
+			}
+up:
+			;
+		}
+	}
+
+	if (leader != top) {
+		child = leader;
+		parent = child->real_parent;
+		leader = parent->group_leader;
+		goto up;
+	}
+out:
+	read_unlock(&tasklist_lock);
+}
+
 #ifndef ARCH_MIN_MMSTRUCT_ALIGN
 #define ARCH_MIN_MMSTRUCT_ALIGN 0
 #endif

commit ebd7e7fc4bc63be5eaf9da903b8060b02dd711ea
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:34 2017 +0100

    timers/posix-timers: Convert internals to use nsecs
    
    Use the new nsec based cputime accessors as part of the whole cputime
    conversion from cputime_t to nsecs.
    
    Also convert posix-cpu-timers to use nsec based internal counters to
    simplify it.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-19-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 11c5c8ab827c..09992ff2f8fa 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1313,7 +1313,7 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 
 	cpu_limit = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
 	if (cpu_limit != RLIM_INFINITY) {
-		sig->cputime_expires.prof_exp = secs_to_cputime(cpu_limit);
+		sig->cputime_expires.prof_exp = cpu_limit * NSEC_PER_SEC;
 		sig->cputimer.running = true;
 	}
 

commit b18b6a9cef7f30e9a8b7738d5fc8d568cf660855
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Sat Jan 21 00:09:08 2017 -0500

    timers: Omit POSIX timer stuff from task_struct when disabled
    
    When CONFIG_POSIX_TIMERS is disabled, it is preferable to remove related
    structures from struct task_struct and struct signal_struct as they
    won't contain anything useful and shouldn't be relied upon by mistake.
    Code still referencing those structures is also disabled here.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 11c5c8ab827c..105c6676d93b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1304,6 +1304,7 @@ void __cleanup_sighand(struct sighand_struct *sighand)
 	}
 }
 
+#ifdef CONFIG_POSIX_TIMERS
 /*
  * Initialize POSIX timer handling for a thread group.
  */
@@ -1322,6 +1323,9 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 	INIT_LIST_HEAD(&sig->cpu_timers[1]);
 	INIT_LIST_HEAD(&sig->cpu_timers[2]);
 }
+#else
+static inline void posix_cpu_timers_init_group(struct signal_struct *sig) { }
+#endif
 
 static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 {
@@ -1346,11 +1350,11 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	init_waitqueue_head(&sig->wait_chldexit);
 	sig->curr_target = tsk;
 	init_sigpending(&sig->shared_pending);
-	INIT_LIST_HEAD(&sig->posix_timers);
 	seqlock_init(&sig->stats_lock);
 	prev_cputime_init(&sig->prev_cputime);
 
 #ifdef CONFIG_POSIX_TIMERS
+	INIT_LIST_HEAD(&sig->posix_timers);
 	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	sig->real_timer.function = it_real_fn;
 #endif
@@ -1425,6 +1429,7 @@ static void rt_mutex_init_task(struct task_struct *p)
 #endif
 }
 
+#ifdef CONFIG_POSIX_TIMERS
 /*
  * Initialize POSIX timer handling for a single task.
  */
@@ -1437,6 +1442,9 @@ static void posix_cpu_timers_init(struct task_struct *tsk)
 	INIT_LIST_HEAD(&tsk->cpu_timers[1]);
 	INIT_LIST_HEAD(&tsk->cpu_timers[2]);
 }
+#else
+static inline void posix_cpu_timers_init(struct task_struct *tsk) { }
+#endif
 
 static inline void
 init_task_pid(struct task_struct *task, enum pid_type type, struct pid *pid)

commit e274795ea7b7caa0fd74ef651594382a69e2a951
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jan 11 14:17:48 2017 +0100

    locking/mutex: Fix mutex handoff
    
    While reviewing the ww_mutex patches, I noticed that it was still
    possible to (incorrectly) succeed for (incorrect) code like:
    
            mutex_lock(&a);
            mutex_lock(&a);
    
    This was possible if the second mutex_lock() would block (as expected)
    but then receive a spurious wakeup. At that point it would find itself
    at the front of the queue, request a handoff and instantly claim
    ownership and continue, since owner would point to itself.
    
    Avoid this scenario and simplify the code by introducing a third low
    bit to signal handoff pickup. So once we request handoff, unlock
    clears the handoff bit and sets the pickup bit along with the new
    owner.
    
    This also removes the need for the .handoff argument to
    __mutex_trylock(), since that becomes superfluous with PICKUP.
    
    In order to guarantee enough low bits, ensure task_struct alignment is
    at least L1_CACHE_BYTES (which seems a good ideal regardless).
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 9d659ae14b54 ("locking/mutex: Add lock handoff to avoid starvation")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 11c5c8ab827c..a90510d0bbf8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -432,11 +432,13 @@ void __init fork_init(void)
 	int i;
 #ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 #ifndef ARCH_MIN_TASKALIGN
-#define ARCH_MIN_TASKALIGN	L1_CACHE_BYTES
+#define ARCH_MIN_TASKALIGN	0
 #endif
+	int align = min_t(int, L1_CACHE_BYTES, ARCH_MIN_TASKALIGN);
+
 	/* create a slab on which task_structs can be allocated */
 	task_struct_cachep = kmem_cache_create("task_struct",
-			arch_task_struct_size, ARCH_MIN_TASKALIGN,
+			arch_task_struct_size, align,
 			SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT, NULL);
 #endif
 

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 869b8ccc00bf..11c5c8ab827c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -79,7 +79,7 @@
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 #include <asm/tlbflush.h>

commit 412ac77a9d3ec015524dacea905471d66480b7ac
Merge: dcdaa2f9480c 19339c251607
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 14 14:09:48 2016 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull namespace updates from Eric Biederman:
     "After a lot of discussion and work we have finally reachanged a basic
      understanding of what is necessary to make unprivileged mounts safe in
      the presence of EVM and IMA xattrs which the last commit in this
      series reflects. While technically it is a revert the comments it adds
      are important for people not getting confused in the future. Clearing
      up that confusion allows us to seriously work on unprivileged mounts
      of fuse in the next development cycle.
    
      The rest of the fixes in this set are in the intersection of user
      namespaces, ptrace, and exec. I started with the first fix which
      started a feedback cycle of finding additional issues during review
      and fixing them. Culiminating in a fix for a bug that has been present
      since at least Linux v1.0.
    
      Potentially these fixes were candidates for being merged during the rc
      cycle, and are certainly backport candidates but enough little things
      turned up during review and testing that I decided they should be
      handled as part of the normal development process just to be certain
      there were not any great surprises when it came time to backport some
      of these fixes"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      Revert "evm: Translate user/group ids relative to s_user_ns when computing HMAC"
      exec: Ensure mm->user_ns contains the execed files
      ptrace: Don't allow accessing an undumpable mm
      ptrace: Capture the ptracer's creds not PT_PTRACE_CAP
      mm: Add a user_ns owner to mm_struct and fix ptrace permission checks

commit 7b9dc3f75fc8be046e76387a22a21f421ce55b53
Merge: 36869cb93d36 bbc17bb8a89b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 13 10:41:53 2016 -0800

    Merge tag 'pm-4.10-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "Again, cpufreq gets more changes than the other parts this time (one
      new driver, one old driver less, a bunch of enhancements of the
      existing code, new CPU IDs, fixes, cleanups)
    
      There also are some changes in cpuidle (idle injection rework, a
      couple of new CPU IDs, online/offline rework in intel_idle, fixes and
      cleanups), in the generic power domains framework (mostly related to
      supporting power domains containing CPUs), and in the Operating
      Performance Points (OPP) library (mostly related to supporting devices
      with multiple voltage regulators)
    
      In addition to that, the system sleep state selection interface is
      modified to make it easier for distributions with unchanged user space
      to support suspend-to-idle as the default system suspend method, some
      issues are fixed in the PM core, the latency tolerance PM QoS
      framework is improved a bit, the Intel RAPL power capping driver is
      cleaned up and there are some fixes and cleanups in the devfreq
      subsystem
    
      Specifics:
    
       - New cpufreq driver for Broadcom STB SoCs and a Device Tree binding
         for it (Markus Mayer)
    
       - Support for ARM Integrator/AP and Integrator/CP in the generic DT
         cpufreq driver and elimination of the old Integrator cpufreq driver
         (Linus Walleij)
    
       - Support for the zx296718, r8a7743 and r8a7745, Socionext UniPhier,
         and PXA SoCs in the the generic DT cpufreq driver (Baoyou Xie,
         Geert Uytterhoeven, Masahiro Yamada, Robert Jarzmik)
    
       - cpufreq core fix to eliminate races that may lead to using inactive
         policy objects and related cleanups (Rafael Wysocki)
    
       - cpufreq schedutil governor update to make it use SCHED_FIFO kernel
         threads (instead of regular workqueues) for doing delayed work (to
         reduce the response latency in some cases) and related cleanups
         (Viresh Kumar)
    
       - New cpufreq sysfs attribute for resetting statistics (Markus Mayer)
    
       - cpufreq governors fixes and cleanups (Chen Yu, Stratos Karafotis,
         Viresh Kumar)
    
       - Support for using generic cpufreq governors in the intel_pstate
         driver (Rafael Wysocki)
    
       - Support for per-logical-CPU P-state limits and the EPP/EPB (Energy
         Performance Preference/Energy Performance Bias) knobs in the
         intel_pstate driver (Srinivas Pandruvada)
    
       - New CPU ID for Knights Mill in intel_pstate (Piotr Luc)
    
       - intel_pstate driver modification to use the P-state selection
         algorithm based on CPU load on platforms with the system profile in
         the ACPI tables set to "mobile" (Srinivas Pandruvada)
    
       - intel_pstate driver cleanups (Arnd Bergmann, Rafael Wysocki,
         Srinivas Pandruvada)
    
       - cpufreq powernv driver updates including fast switching support
         (for the schedutil governor), fixes and cleanus (Akshay Adiga,
         Andrew Donnellan, Denis Kirjanov)
    
       - acpi-cpufreq driver rework to switch it over to the new CPU
         offline/online state machine (Sebastian Andrzej Siewior)
    
       - Assorted cleanups in cpufreq drivers (Wei Yongjun, Prashanth
         Prakash)
    
       - Idle injection rework (to make it use the regular idle path instead
         of a home-grown custom one) and related powerclamp thermal driver
         updates (Peter Zijlstra, Jacob Pan, Petr Mladek, Sebastian Andrzej
         Siewior)
    
       - New CPU IDs for Atom Z34xx and Knights Mill in intel_idle (Andy
         Shevchenko, Piotr Luc)
    
       - intel_idle driver cleanups and switch over to using the new CPU
         offline/online state machine (Anna-Maria Gleixner, Sebastian
         Andrzej Siewior)
    
       - cpuidle DT driver update to support suspend-to-idle properly
         (Sudeep Holla)
    
       - cpuidle core cleanups and misc updates (Daniel Lezcano, Pan Bian,
         Rafael Wysocki)
    
       - Preliminary support for power domains including CPUs in the generic
         power domains (genpd) framework and related DT bindings (Lina Iyer)
    
       - Assorted fixes and cleanups in the generic power domains (genpd)
         framework (Colin Ian King, Dan Carpenter, Geert Uytterhoeven)
    
       - Preliminary support for devices with multiple voltage regulators
         and related fixes and cleanups in the Operating Performance Points
         (OPP) library (Viresh Kumar, Masahiro Yamada, Stephen Boyd)
    
       - System sleep state selection interface rework to make it easier to
         support suspend-to-idle as the default system suspend method
         (Rafael Wysocki)
    
       - PM core fixes and cleanups, mostly related to the interactions
         between the system suspend and runtime PM frameworks (Ulf Hansson,
         Sahitya Tummala, Tony Lindgren)
    
       - Latency tolerance PM QoS framework imorovements (Andrew Lutomirski)
    
       - New Knights Mill CPU ID for the Intel RAPL power capping driver
         (Piotr Luc)
    
       - Intel RAPL power capping driver fixes, cleanups and switch over to
         using the new CPU offline/online state machine (Jacob Pan, Thomas
         Gleixner, Sebastian Andrzej Siewior)
    
       - Fixes and cleanups in the exynos-ppmu, exynos-nocp, rk3399_dmc,
         rockchip-dfi devfreq drivers and the devfreq core (Axel Lin,
         Chanwoo Choi, Javier Martinez Canillas, MyungJoo Ham, Viresh Kumar)
    
       - Fix for false-positive KASAN warnings during resume from ACPI S3
         (suspend-to-RAM) on x86 (Josh Poimboeuf)
    
       - Memory map verification during resume from hibernation on x86 to
         ensure a consistent address space layout (Chen Yu)
    
       - Wakeup sources debugging enhancement (Xing Wei)
    
       - rockchip-io AVS driver cleanup (Shawn Lin)"
    
    * tag 'pm-4.10-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (127 commits)
      devfreq: rk3399_dmc: Don't use OPP structures outside of RCU locks
      devfreq: rk3399_dmc: Remove dangling rcu_read_unlock()
      devfreq: exynos: Don't use OPP structures outside of RCU locks
      Documentation: intel_pstate: Document HWP energy/performance hints
      cpufreq: intel_pstate: Support for energy performance hints with HWP
      cpufreq: intel_pstate: Add locking around HWP requests
      PM / sleep: Print active wakeup sources when blocking on wakeup_count reads
      PM / core: Fix bug in the error handling of async suspend
      PM / wakeirq: Fix dedicated wakeirq for drivers not using autosuspend
      PM / Domains: Fix compatible for domain idle state
      PM / OPP: Don't WARN on multiple calls to dev_pm_opp_set_regulators()
      PM / OPP: Allow platform specific custom set_opp() callbacks
      PM / OPP: Separate out _generic_set_opp()
      PM / OPP: Add infrastructure to manage multiple regulators
      PM / OPP: Pass struct dev_pm_opp_supply to _set_opp_voltage()
      PM / OPP: Manage supply's voltage/current in a separate structure
      PM / OPP: Don't use OPP structure outside of rcu protected section
      PM / OPP: Reword binding supporting multiple regulators per device
      PM / OPP: Fix incorrect cpu-supply property in binding
      cpuidle: Add a kerneldoc comment to cpuidle_use_deepest_state()
      ..

commit e34bac726d27056081d0250c0e173e4b155aa340
Merge: fe6bce8d30a8 39a0e975c37d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 20:50:02 2016 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
    
     - various misc bits
    
     - most of MM (quite a lot of MM material is awaiting the merge of
       linux-next dependencies)
    
     - kasan
    
     - printk updates
    
     - procfs updates
    
     - MAINTAINERS
    
     - /lib updates
    
     - checkpatch updates
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (123 commits)
      init: reduce rootwait polling interval time to 5ms
      binfmt_elf: use vmalloc() for allocation of vma_filesz
      checkpatch: don't emit unified-diff error for rename-only patches
      checkpatch: don't check c99 types like uint8_t under tools
      checkpatch: avoid multiple line dereferences
      checkpatch: don't check .pl files, improve absolute path commit log test
      scripts/checkpatch.pl: fix spelling
      checkpatch: don't try to get maintained status when --no-tree is given
      lib/ida: document locking requirements a bit better
      lib/rbtree.c: fix typo in comment of ____rb_erase_color
      lib/Kconfig.debug: make CONFIG_STRICT_DEVMEM depend on CONFIG_DEVMEM
      MAINTAINERS: add drm and drm/i915 irc channels
      MAINTAINERS: add "C:" for URI for chat where developers hang out
      MAINTAINERS: add drm and drm/i915 bug filing info
      MAINTAINERS: add "B:" for URI where to file bugs
      get_maintainer: look for arbitrary letter prefixes in sections
      printk: add Kconfig option to set default console loglevel
      printk/sound: handle more message headers
      printk/btrfs: handle more message headers
      printk/kdb: handle more message headers
      ...

commit 9465d9cc31fa732089cd8bec9f1bdfcdc174a5ce
Merge: e71c3978d6f9 c029a2bec66e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 19:56:15 2016 -0800

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "The time/timekeeping/timer folks deliver with this update:
    
       - Fix a reintroduced signed/unsigned issue and cleanup the whole
         signed/unsigned mess in the timekeeping core so this wont happen
         accidentaly again.
    
       - Add a new trace clock based on boot time
    
       - Prevent injection of random sleep times when PM tracing abuses the
         RTC for storage
    
       - Make posix timers configurable for real tiny systems
    
       - Add tracepoints for the alarm timer subsystem so timer based
         suspend wakeups can be instrumented
    
       - The usual pile of fixes and updates to core and drivers"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      timekeeping: Use mul_u64_u32_shr() instead of open coding it
      timekeeping: Get rid of pointless typecasts
      timekeeping: Make the conversion call chain consistently unsigned
      timekeeping_Force_unsigned_clocksource_to_nanoseconds_conversion
      alarmtimer: Add tracepoints for alarm timers
      trace: Update documentation for mono, mono_raw and boot clock
      trace: Add an option for boot clock as trace clock
      timekeeping: Add a fast and NMI safe boot clock
      timekeeping/clocksource_cyc2ns: Document intended range limitation
      timekeeping: Ignore the bogus sleep time if pm_trace is enabled
      selftests/timers: Fix spelling mistake "Asyncrhonous" -> "Asynchronous"
      clocksource/drivers/bcm2835_timer: Unmap region obtained by of_iomap
      clocksource/drivers/arm_arch_timer: Map frame with of_io_request_and_map()
      arm64: dts: rockchip: Arch counter doesn't tick in system suspend
      clocksource/drivers/arm_arch_timer: Don't assume clock runs in suspend
      posix-timers: Make them configurable
      posix_cpu_timers: Move the add_device_randomness() call to a proper place
      timer: Move sys_alarm from timer.c to itimer.c
      ptp_clock: Allow for it to be optional
      Kconfig: Regenerate *.c_shipped files after previous changes
      ...

commit 0f110a9b956c1678b53986b003d59794604807ba
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Mon Dec 12 16:44:14 2016 -0800

    kernel/fork: use vfree_atomic() to free thread stack
    
    vfree() is going to use sleeping lock.  Thread stack freed in atomic
    context, therefore we must use vfree_atomic() here.
    
    Link: http://lkml.kernel.org/r/1479474236-4139-6-git-send-email-hch@lst.de
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Jisheng Zhang <jszhang@marvell.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: John Dias <joaodias@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7ffa16033ded..00492b22adfe 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -229,7 +229,7 @@ static inline void free_thread_stack(struct task_struct *tsk)
 		}
 		local_irq_restore(flags);
 
-		vfree(tsk->stack);
+		vfree_atomic(tsk->stack);
 		return;
 	}
 #endif

commit 1da5c46fa965ff90f5ffc080b6ab3fae5e227bc3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Nov 29 18:50:57 2016 +0100

    kthread: Make struct kthread kmalloc'ed
    
    commit 23196f2e5f5d "kthread: Pin the stack via try_get_task_stack() /
    put_task_stack() in to_live_kthread() function" is a workaround for the
    fragile design of struct kthread being allocated on the task stack.
    
    struct kthread in its current form should be removed, but this needs
    cleanups outside of kthread.c.
    
    As a first step move struct kthread away from the task stack by making it
    kmalloc'ed. This allows to access kthread.exited without the magic of
    trying to pin task stack and the try logic in to_live_kthread().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Chunming Zhou <David1.Zhou@amd.com>
    Cc: Roman Pen <roman.penyaev@profitbricks.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20161129175057.GA5330@redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 600e93b5e539..7ffa16033ded 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -354,6 +354,8 @@ void free_task(struct task_struct *tsk)
 	ftrace_graph_exit_task(tsk);
 	put_seccomp_filter(tsk);
 	arch_release_task_struct(tsk);
+	if (tsk->flags & PF_KTHREAD)
+		free_kthread_struct(tsk);
 	free_task_struct(tsk);
 }
 EXPORT_SYMBOL(free_task);

commit 4e28ec3d5fe0b994fe87b2406d75d9c351ef4940
Merge: a2c1bc645e87 6af33995318f
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Dec 1 14:39:51 2016 +0100

    Merge back earlier cpuidle material for v4.10.

commit c1de45ca831acee9b72c9320dde447edafadb43f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 28 23:03:05 2016 -0800

    sched/idle: Add support for tasks that inject idle
    
    Idle injection drivers such as Intel powerclamp and ACPI PAD drivers use
    realtime tasks to take control of CPU then inject idle. There are two
    issues with this approach:
    
     1. Low efficiency: injected idle task is treated as busy so sched ticks
        do not stop during injected idle period, the result of these
        unwanted wakeups can be ~20% loss in power savings.
    
     2. Idle accounting: injected idle time is presented to user as busy.
    
    This patch addresses the issues by introducing a new PF_IDLE flag which
    allows any given task to be treated as idle task while the flag is set.
    Therefore, idle injection tasks can run through the normal flow of NOHZ
    idle enter/exit to get the correct accounting as well as tick stop when
    possible.
    
    The implication is that idle task is then no longer limited to PID == 0.
    
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 623259fc794d..5074b2f0827b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1537,7 +1537,7 @@ static __latent_entropy struct task_struct *copy_process(
 		goto bad_fork_cleanup_count;
 
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
-	p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);
+	p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE);
 	p->flags |= PF_FORKNOEXEC;
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);

commit bfedb589252c01fa505ac9f6f2a3d5d68d707ef4
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Oct 13 21:23:16 2016 -0500

    mm: Add a user_ns owner to mm_struct and fix ptrace permission checks
    
    During exec dumpable is cleared if the file that is being executed is
    not readable by the user executing the file.  A bug in
    ptrace_may_access allows reading the file if the executable happens to
    enter into a subordinate user namespace (aka clone(CLONE_NEWUSER),
    unshare(CLONE_NEWUSER), or setns(fd, CLONE_NEWUSER).
    
    This problem is fixed with only necessary userspace breakage by adding
    a user namespace owner to mm_struct, captured at the time of exec, so
    it is clear in which user namespace CAP_SYS_PTRACE must be present in
    to be able to safely give read permission to the executable.
    
    The function ptrace_may_access is modified to verify that the ptracer
    has CAP_SYS_ADMIN in task->mm->user_ns instead of task->cred->user_ns.
    This ensures that if the task changes it's cred into a subordinate
    user namespace it does not become ptraceable.
    
    The function ptrace_attach is modified to only set PT_PTRACE_CAP when
    CAP_SYS_PTRACE is held over task->mm->user_ns.  The intent of
    PT_PTRACE_CAP is to be a flag to note that whatever permission changes
    the task might go through the tracer has sufficient permissions for
    it not to be an issue.  task->cred->user_ns is always the same
    as or descendent of mm->user_ns.  Which guarantees that having
    CAP_SYS_PTRACE over mm->user_ns is the worst case for the tasks
    credentials.
    
    To prevent regressions mm->dumpable and mm->user_ns are not considered
    when a task has no mm.  As simply failing ptrace_may_attach causes
    regressions in privileged applications attempting to read things
    such as /proc/<pid>/stat
    
    Cc: stable@vger.kernel.org
    Acked-by: Kees Cook <keescook@chromium.org>
    Tested-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Fixes: 8409cca70561 ("userns: allow ptrace from non-init user namespaces")
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 997ac1d584f7..ba8a01564985 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -745,7 +745,8 @@ static void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
 #endif
 }
 
-static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
+static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
+	struct user_namespace *user_ns)
 {
 	mm->mmap = NULL;
 	mm->mm_rb = RB_ROOT;
@@ -785,6 +786,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	if (init_new_context(p, mm))
 		goto fail_nocontext;
 
+	mm->user_ns = get_user_ns(user_ns);
 	return mm;
 
 fail_nocontext:
@@ -830,7 +832,7 @@ struct mm_struct *mm_alloc(void)
 		return NULL;
 
 	memset(mm, 0, sizeof(*mm));
-	return mm_init(mm, current);
+	return mm_init(mm, current, current_user_ns());
 }
 
 /*
@@ -845,6 +847,7 @@ void __mmdrop(struct mm_struct *mm)
 	destroy_context(mm);
 	mmu_notifier_mm_destroy(mm);
 	check_mm(mm);
+	put_user_ns(mm->user_ns);
 	free_mm(mm);
 }
 EXPORT_SYMBOL_GPL(__mmdrop);
@@ -1126,7 +1129,7 @@ static struct mm_struct *dup_mm(struct task_struct *tsk)
 
 	memcpy(mm, oldmm, sizeof(*mm));
 
-	if (!mm_init(mm, tsk))
+	if (!mm_init(mm, tsk, mm->user_ns))
 		goto fail_nomem;
 
 	err = dup_mmap(mm, oldmm);

commit baa73d9e478ff32d62f3f9422822b59dd9a95a21
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Nov 11 00:10:10 2016 -0500

    posix-timers: Make them configurable
    
    Some embedded systems have no use for them.  This removes about
    25KB from the kernel binary size when configured out.
    
    Corresponding syscalls are routed to a stub logging the attempt to
    use those syscalls which should be enough of a clue if they were
    disabled without proper consideration. They are: timer_create,
    timer_gettime: timer_getoverrun, timer_settime, timer_delete,
    clock_adjtime, setitimer, getitimer, alarm.
    
    The clock_settime, clock_gettime, clock_getres and clock_nanosleep
    syscalls are replaced by simple wrappers compatible with CLOCK_REALTIME,
    CLOCK_MONOTONIC and CLOCK_BOOTTIME only which should cover the vast
    majority of use cases with very little code.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Richard Cochran <richardcochran@gmail.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Cc: Paul Bolle <pebolle@tiscali.nl>
    Cc: linux-kbuild@vger.kernel.org
    Cc: netdev@vger.kernel.org
    Cc: Michal Marek <mmarek@suse.com>
    Cc: Edward Cree <ecree@solarflare.com>
    Link: http://lkml.kernel.org/r/1478841010-28605-7-git-send-email-nicolas.pitre@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 623259fc794d..17da35fa77e7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1342,8 +1342,10 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	seqlock_init(&sig->stats_lock);
 	prev_cputime_init(&sig->prev_cputime);
 
+#ifdef CONFIG_POSIX_TIMERS
 	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	sig->real_timer.function = it_real_fn;
+#endif
 
 	task_lock(current->group_leader);
 	memcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);

commit 40565b5aedd6d0ca88b7dfd3859d709d2f6f8cf9
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Tue Nov 15 03:06:51 2016 +0100

    sched/cputime, powerpc, s390: Make scaled cputime arch specific
    
    Only s390 and powerpc have hardware facilities allowing to measure
    cputimes scaled by frequency. On all other architectures
    utimescaled/stimescaled are equal to utime/stime (however they are
    accounted separately).
    
    Remove {u,s}timescaled accounting on all architectures except
    powerpc and s390, where those values are explicitly accounted
    in the proper places.
    
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20161031162143.GB12646@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 997ac1d584f7..600e93b5e539 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1551,7 +1551,9 @@ static __latent_entropy struct task_struct *copy_process(
 	init_sigpending(&p->pending);
 
 	p->utime = p->stime = p->gtime = 0;
+#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
 	p->utimescaled = p->stimescaled = 0;
+#endif
 	prev_cputime_init(&p->prev_cputime);
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN

commit 405c0759712f57b680f66aee9c55cd06ad1cbdef
Author: Andy Lutomirski <luto@kernel.org>
Date:   Mon Oct 31 08:11:43 2016 -0700

    fork: Add task stack refcounting sanity check and prevent premature task stack freeing
    
    If something goes wrong with task stack refcounting and a stack
    refcount hits zero too early, warn and leak it rather than
    potentially freeing it early (and silently).
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/f29119c783a9680a4b4656e751b6123917ace94b.1477926663.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 623259fc794d..997ac1d584f7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -315,6 +315,9 @@ static void account_kernel_stack(struct task_struct *tsk, int account)
 
 static void release_task_stack(struct task_struct *tsk)
 {
+	if (WARN_ON(tsk->state != TASK_DEAD))
+		return;  /* Better to leak the stack than to free prematurely */
+
 	account_kernel_stack(tsk, -1);
 	arch_release_thread_stack(tsk->stack);
 	free_thread_stack(tsk);
@@ -1862,6 +1865,7 @@ static __latent_entropy struct task_struct *copy_process(
 	atomic_dec(&p->cred->user->processes);
 	exit_creds(p);
 bad_fork_free:
+	p->state = TASK_DEAD;
 	put_task_stack(p);
 	free_task(p);
 fork_out:

commit 9ffc66941df278c9f4df979b6bcf6c6ddafedd16
Merge: 133d970e0dad 0766f788eb72
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 15 10:03:15 2016 -0700

    Merge tag 'gcc-plugins-v4.9-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull gcc plugins update from Kees Cook:
     "This adds a new gcc plugin named "latent_entropy". It is designed to
      extract as much possible uncertainty from a running system at boot
      time as possible, hoping to capitalize on any possible variation in
      CPU operation (due to runtime data differences, hardware differences,
      SMP ordering, thermal timing variation, cache behavior, etc).
    
      At the very least, this plugin is a much more comprehensive example
      for how to manipulate kernel code using the gcc plugin internals"
    
    * tag 'gcc-plugins-v4.9-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      latent_entropy: Mark functions with __latent_entropy
      gcc-plugins: Add latent_entropy plugin

commit 0766f788eb727e2e330d55d30545db65bcf2623f
Author: Emese Revfy <re.emese@gmail.com>
Date:   Mon Jun 20 20:42:34 2016 +0200

    latent_entropy: Mark functions with __latent_entropy
    
    The __latent_entropy gcc attribute can be used only on functions and
    variables.  If it is on a function then the plugin will instrument it for
    gathering control-flow entropy. If the attribute is on a variable then
    the plugin will initialize it with random contents.  The variable must
    be an integer, an integer array type or a structure with integer fields.
    
    These specific functions have been selected because they are init
    functions (to help gather boot-time entropy), are called at unpredictable
    times, or they have variable loops, each of which provide some level of
    latent entropy.
    
    Signed-off-by: Emese Revfy <re.emese@gmail.com>
    [kees: expanded commit message]
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 001b18473a07..05393881ef39 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -404,7 +404,8 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 }
 
 #ifdef CONFIG_MMU
-static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
+static __latent_entropy int dup_mmap(struct mm_struct *mm,
+					struct mm_struct *oldmm)
 {
 	struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
 	struct rb_node **rb_link, *rb_parent;
@@ -1296,7 +1297,8 @@ init_task_pid(struct task_struct *task, enum pid_type type, struct pid *pid)
  * parts of the process environment (as per the clone
  * flags). The actual kick-off is left to the caller.
  */
-static struct task_struct *copy_process(unsigned long clone_flags,
+static __latent_entropy struct task_struct *copy_process(
+					unsigned long clone_flags,
 					unsigned long stack_start,
 					unsigned long stack_size,
 					int __user *child_tidptr,

commit 38addce8b600ca335dc86fa3d48c890f1c6fa1f4
Author: Emese Revfy <re.emese@gmail.com>
Date:   Mon Jun 20 20:41:19 2016 +0200

    gcc-plugins: Add latent_entropy plugin
    
    This adds a new gcc plugin named "latent_entropy". It is designed to
    extract as much possible uncertainty from a running system at boot time as
    possible, hoping to capitalize on any possible variation in CPU operation
    (due to runtime data differences, hardware differences, SMP ordering,
    thermal timing variation, cache behavior, etc).
    
    At the very least, this plugin is a much more comprehensive example for
    how to manipulate kernel code using the gcc plugin internals.
    
    The need for very-early boot entropy tends to be very architecture or
    system design specific, so this plugin is more suited for those sorts
    of special cases. The existing kernel RNG already attempts to extract
    entropy from reliable runtime variation, but this plugin takes the idea to
    a logical extreme by permuting a global variable based on any variation
    in code execution (e.g. a different value (and permutation function)
    is used to permute the global based on loop count, case statement,
    if/then/else branching, etc).
    
    To do this, the plugin starts by inserting a local variable in every
    marked function. The plugin then adds logic so that the value of this
    variable is modified by randomly chosen operations (add, xor and rol) and
    random values (gcc generates separate static values for each location at
    compile time and also injects the stack pointer at runtime). The resulting
    value depends on the control flow path (e.g., loops and branches taken).
    
    Before the function returns, the plugin mixes this local variable into
    the latent_entropy global variable. The value of this global variable
    is added to the kernel entropy pool in do_one_initcall() and _do_fork(),
    though it does not credit any bytes of entropy to the pool; the contents
    of the global are just used to mix the pool.
    
    Additionally, the plugin can pre-initialize arrays with build-time
    random contents, so that two different kernel builds running on identical
    hardware will not have the same starting values.
    
    Signed-off-by: Emese Revfy <re.emese@gmail.com>
    [kees: expanded commit message and code comments]
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index beb31725f7e2..001b18473a07 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1780,6 +1780,7 @@ long _do_fork(unsigned long clone_flags,
 
 	p = copy_process(clone_flags, stack_start, stack_size,
 			 child_tidptr, NULL, trace, tls, NUMA_NO_NODE);
+	add_latent_entropy();
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.

commit 6fcb52a56ff60d240f06296b12827e7f20d45f63
Author: Aaron Lu <aaron.lu@intel.com>
Date:   Fri Oct 7 17:00:08 2016 -0700

    thp: reduce usage of huge zero page's atomic counter
    
    The global zero page is used to satisfy an anonymous read fault.  If
    THP(Transparent HugePage) is enabled then the global huge zero page is
    used.  The global huge zero page uses an atomic counter for reference
    counting and is allocated/freed dynamically according to its counter
    value.
    
    CPU time spent on that counter will greatly increase if there are a lot
    of processes doing anonymous read faults.  This patch proposes a way to
    reduce the access to the global counter so that the CPU load can be
    reduced accordingly.
    
    To do this, a new flag of the mm_struct is introduced:
    MMF_USED_HUGE_ZERO_PAGE.  With this flag, the process only need to touch
    the global counter in two cases:
    
     1 The first time it uses the global huge zero page;
     2 The time when mm_user of its mm_struct reaches zero.
    
    Note that right now, the huge zero page is eligible to be freed as soon
    as its last use goes away.  With this patch, the page will not be
    eligible to be freed until the exit of the last process from which it
    was ever used.
    
    And with the use of mm_user, the kthread is not eligible to use huge
    zero page either.  Since no kthread is using huge zero page today, there
    is no difference after applying this patch.  But if that is not desired,
    I can change it to when mm_count reaches zero.
    
    Case used for test on Haswell EP:
    
      usemem -n 72 --readonly -j 0x200000 100G
    
    Which spawns 72 processes and each will mmap 100G anonymous space and
    then do read only access to that space sequentially with a step of 2MB.
    
      CPU cycles from perf report for base commit:
          54.03%  usemem   [kernel.kallsyms]   [k] get_huge_zero_page
      CPU cycles from perf report for this commit:
           0.11%  usemem   [kernel.kallsyms]   [k] mm_get_huge_zero_page
    
    Performance(throughput) of the workload for base commit: 1784430792
    Performance(throughput) of the workload for this commit: 4726928591
    164% increase.
    
    Runtime of the workload for base commit: 707592 us
    Runtime of the workload for this commit: 303970 us
    50% drop.
    
    Link: http://lkml.kernel.org/r/fe51a88f-446a-4622-1363-ad1282d71385@intel.com
    Signed-off-by: Aaron Lu <aaron.lu@intel.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Ebru Akagunduz <ebru.akagunduz@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9a8ec66cd4df..6d42242485cb 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -854,6 +854,7 @@ static inline void __mmput(struct mm_struct *mm)
 	ksm_exit(mm);
 	khugepaged_exit(mm); /* must run before exit_mmap */
 	exit_mmap(mm);
+	mm_put_huge_zero_page(mm);
 	set_mm_exe_file(mm, NULL);
 	if (!list_empty(&mm->mmlist)) {
 		spin_lock(&mmlist_lock);

commit 862e3073b3eed13f17bd6be6ca6052db15c0b728
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Oct 7 16:58:57 2016 -0700

    mm, oom: get rid of signal_struct::oom_victims
    
    After "oom: keep mm of the killed task available" we can safely detect
    an oom victim by checking task->signal->oom_mm so we do not need the
    signal_struct counter anymore so let's get rid of it.
    
    This alone wouldn't be sufficient for nommu archs because
    exit_oom_victim doesn't hide the process from the oom killer anymore.
    We can, however, mark the mm with a MMF flag in __mmput.  We can reuse
    MMF_OOM_REAPED and rename it to a more generic MMF_OOM_SKIP.
    
    Link: http://lkml.kernel.org/r/1472119394-11342-6-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5650e35dda43..9a8ec66cd4df 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -862,6 +862,7 @@ static inline void __mmput(struct mm_struct *mm)
 	}
 	if (mm->binfmt)
 		module_put(mm->binfmt->module);
+	set_bit(MMF_OOM_SKIP, &mm->flags);
 	mmdrop(mm);
 }
 

commit 7283094ec3db318e87ec9e31cf75f136ac2a4dd3
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Oct 7 16:58:54 2016 -0700

    kernel, oom: fix potential pgd_lock deadlock from __mmdrop
    
    Lockdep complains that __mmdrop is not safe from the softirq context:
    
      =================================
      [ INFO: inconsistent lock state ]
      4.6.0-oomfortification2-00011-geeb3eadeab96-dirty #949 Tainted: G        W
      ---------------------------------
      inconsistent {SOFTIRQ-ON-W} -> {IN-SOFTIRQ-W} usage.
      swapper/1/0 [HC0[0]:SC1[1]:HE1:SE0] takes:
       (pgd_lock){+.?...}, at: pgd_free+0x19/0x6b
      {SOFTIRQ-ON-W} state was registered at:
         __lock_acquire+0xa06/0x196e
         lock_acquire+0x139/0x1e1
         _raw_spin_lock+0x32/0x41
         __change_page_attr_set_clr+0x2a5/0xacd
         change_page_attr_set_clr+0x16f/0x32c
         set_memory_nx+0x37/0x3a
         free_init_pages+0x9e/0xc7
         alternative_instructions+0xa2/0xb3
         check_bugs+0xe/0x2d
         start_kernel+0x3ce/0x3ea
         x86_64_start_reservations+0x2a/0x2c
         x86_64_start_kernel+0x17a/0x18d
      irq event stamp: 105916
      hardirqs last  enabled at (105916): free_hot_cold_page+0x37e/0x390
      hardirqs last disabled at (105915): free_hot_cold_page+0x2c1/0x390
      softirqs last  enabled at (105878): _local_bh_enable+0x42/0x44
      softirqs last disabled at (105879): irq_exit+0x6f/0xd1
    
      other info that might help us debug this:
       Possible unsafe locking scenario:
    
             CPU0
             ----
        lock(pgd_lock);
        <Interrupt>
          lock(pgd_lock);
    
       *** DEADLOCK ***
    
      1 lock held by swapper/1/0:
       #0:  (rcu_callback){......}, at: rcu_process_callbacks+0x390/0x800
    
      stack backtrace:
      CPU: 1 PID: 0 Comm: swapper/1 Tainted: G        W       4.6.0-oomfortification2-00011-geeb3eadeab96-dirty #949
      Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Debian-1.8.2-1 04/01/2014
      Call Trace:
       <IRQ>
        print_usage_bug.part.25+0x259/0x268
        mark_lock+0x381/0x567
        __lock_acquire+0x993/0x196e
        lock_acquire+0x139/0x1e1
        _raw_spin_lock+0x32/0x41
        pgd_free+0x19/0x6b
        __mmdrop+0x25/0xb9
        __put_task_struct+0x103/0x11e
        delayed_put_task_struct+0x157/0x15e
        rcu_process_callbacks+0x660/0x800
        __do_softirq+0x1ec/0x4d5
        irq_exit+0x6f/0xd1
        smp_apic_timer_interrupt+0x42/0x4d
        apic_timer_interrupt+0x8e/0xa0
       <EOI>
        arch_cpu_idle+0xf/0x11
        default_idle_call+0x32/0x34
        cpu_startup_entry+0x20c/0x399
        start_secondary+0xfe/0x101
    
    More over commit a79e53d85683 ("x86/mm: Fix pgd_lock deadlock") was
    explicit about pgd_lock not to be called from the irq context.  This
    means that __mmdrop called from free_signal_struct has to be postponed
    to a user context.  We already have a similar mechanism for mmput_async
    so we can use it here as well.  This is safe because mm_count is pinned
    by mm_users.
    
    This fixes bug introduced by "oom: keep mm of the killed task available"
    
    Link: http://lkml.kernel.org/r/1472119394-11342-5-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 48cafe787b75..5650e35dda43 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -359,8 +359,12 @@ static inline void free_signal_struct(struct signal_struct *sig)
 {
 	taskstats_tgid_free(sig);
 	sched_autogroup_exit(sig);
+	/*
+	 * __mmdrop is not safe to call from softirq context on x86 due to
+	 * pgd_dtor so postpone it to the async context
+	 */
 	if (sig->oom_mm)
-		mmdrop(sig->oom_mm);
+		mmdrop_async(sig->oom_mm);
 	kmem_cache_free(signal_cachep, sig);
 }
 

commit 26db62f179d112d345031e14926a4cda9cd40d6e
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Oct 7 16:58:51 2016 -0700

    oom: keep mm of the killed task available
    
    oom_reap_task has to call exit_oom_victim in order to make sure that the
    oom vicim will not block the oom killer for ever.  This is, however,
    opening new problems (e.g oom_killer_disable exclusion - see commit
    74070542099c ("oom, suspend: fix oom_reaper vs.  oom_killer_disable
    race")).  exit_oom_victim should be only called from the victim's
    context ideally.
    
    One way to achieve this would be to rely on per mm_struct flags.  We
    already have MMF_OOM_REAPED to hide a task from the oom killer since
    "mm, oom: hide mm which is shared with kthread or global init". The
    problem is that the exit path:
    
      do_exit
        exit_mm
          tsk->mm = NULL;
          mmput
            __mmput
          exit_oom_victim
    
    doesn't guarantee that exit_oom_victim will get called in a bounded
    amount of time.  At least exit_aio depends on IO which might get blocked
    due to lack of memory and who knows what else is lurking there.
    
    This patch takes a different approach.  We remember tsk->mm into the
    signal_struct and bind it to the signal struct life time for all oom
    victims.  __oom_reap_task_mm as well as oom_scan_process_thread do not
    have to rely on find_lock_task_mm anymore and they will have a reliable
    reference to the mm struct.  As a result all the oom specific
    communication inside the OOM killer can be done via tsk->signal->oom_mm.
    
    Increasing the signal_struct for something as unlikely as the oom killer
    is far from ideal but this approach will make the code much more
    reasonable and long term we even might want to move task->mm into the
    signal_struct anyway.  In the next step we might want to make the oom
    killer exclusion and access to memory reserves completely independent
    which would be also nice.
    
    Link: http://lkml.kernel.org/r/1472119394-11342-4-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9a05bd93f8e7..48cafe787b75 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -359,6 +359,8 @@ static inline void free_signal_struct(struct signal_struct *sig)
 {
 	taskstats_tgid_free(sig);
 	sched_autogroup_exit(sig);
+	if (sig->oom_mm)
+		mmdrop(sig->oom_mm);
 	kmem_cache_free(signal_cachep, sig);
 }
 

commit 14986a34e1289424811443a524cdd9e1688c7913
Merge: 8d370595811e 069d5ac9ae0d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 6 09:52:23 2016 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull namespace updates from Eric Biederman:
     "This set of changes is a number of smaller things that have been
      overlooked in other development cycles focused on more fundamental
      change. The devpts changes are small things that were a distraction
      until we managed to kill off DEVPTS_MULTPLE_INSTANCES. There is an
      trivial regression fix to autofs for the unprivileged mount changes
      that went in last cycle. A pair of ioctls has been added by Andrey
      Vagin making it is possible to discover the relationships between
      namespaces when referring to them through file descriptors.
    
      The big user visible change is starting to add simple resource limits
      to catch programs that misbehave. With namespaces in general and user
      namespaces in particular allowing users to use more kinds of
      resources, it has become important to have something to limit errant
      programs. Because the purpose of these limits is to catch errant
      programs the code needs to be inexpensive to use as it always on, and
      the default limits need to be high enough that well behaved programs
      on well behaved systems don't encounter them.
    
      To this end, after some review I have implemented per user per user
      namespace limits, and use them to limit the number of namespaces. The
      limits being per user mean that one user can not exhause the limits of
      another user. The limits being per user namespace allow contexts where
      the limit is 0 and security conscious folks can remove from their
      threat anlysis the code used to manage namespaces (as they have
      historically done as it root only). At the same time the limits being
      per user namespace allow other parts of the system to use namespaces.
    
      Namespaces are increasingly being used in application sand boxing
      scenarios so an all or nothing disable for the entire system for the
      security conscious folks makes increasing use of these sandboxes
      impossible.
    
      There is also added a limit on the maximum number of mounts present in
      a single mount namespace. It is nontrivial to guess what a reasonable
      system wide limit on the number of mount structure in the kernel would
      be, especially as it various based on how a system is using
      containers. A limit on the number of mounts in a mount namespace
      however is much easier to understand and set. In most cases in
      practice only about 1000 mounts are used. Given that some autofs
      scenarious have the potential to be 30,000 to 50,000 mounts I have set
      the default limit for the number of mounts at 100,000 which is well
      above every known set of users but low enough that the mount hash
      tables don't degrade unreaonsably.
    
      These limits are a start. I expect this estabilishes a pattern that
      other limits for resources that namespaces use will follow. There has
      been interest in making inotify event limits per user per user
      namespace as well as interest expressed in making details about what
      is going on in the kernel more visible"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (28 commits)
      autofs:  Fix automounts by using current_real_cred()->uid
      mnt: Add a per mount namespace limit on the number of mounts
      netns: move {inc,dec}_net_namespaces into #ifdef
      nsfs: Simplify __ns_get_path
      tools/testing: add a test to check nsfs ioctl-s
      nsfs: add ioctl to get a parent namespace
      nsfs: add ioctl to get an owning user namespace for ns file descriptor
      kernel: add a helper to get an owning user namespace for a namespace
      devpts: Change the owner of /dev/pts/ptmx to the mounter of /dev/pts
      devpts: Remove sync_filesystems
      devpts: Make devpts_kill_sb safe if fsi is NULL
      devpts: Simplify devpts_mount by using mount_nodev
      devpts: Move the creation of /dev/pts/ptmx into fill_super
      devpts: Move parse_mount_options into fill_super
      userns: When the per user per user namespace limit is reached return ENOSPC
      userns; Document per user per user namespace limits.
      mntns: Add a limit on the number of mount namespaces.
      netns: Add a limit on the number of net namespaces
      cgroupns: Add a limit on the number of cgroup namespaces
      ipcns: Add a  limit on the number of ipc namespaces
      ...

commit ac496bf48d97f2503eaa353996a4dd5e4383eaf0
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Sep 15 22:45:49 2016 -0700

    fork: Optimize task creation by caching two thread stacks per CPU if CONFIG_VMAP_STACK=y
    
    vmalloc() is a bit slow, and pounding vmalloc()/vfree() will eventually
    force a global TLB flush.
    
    To reduce pressure on them, if CONFIG_VMAP_STACK=y, cache two thread
    stacks per CPU.  This will let us quickly allocate a hopefully
    cache-hot, TLB-hot stack under heavy forking workloads (shell script style).
    
    On my silly pthread_create() benchmark, it saves about 2 µs per
    pthread_create()+join() with CONFIG_VMAP_STACK=y.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/94811d8e3994b2e962f88866290017d498eb069c.1474003868.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5dd0a516626d..c060c7e7c247 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -159,15 +159,41 @@ void __weak arch_release_thread_stack(unsigned long *stack)
  * kmemcache based allocator.
  */
 # if THREAD_SIZE >= PAGE_SIZE || defined(CONFIG_VMAP_STACK)
+
+#ifdef CONFIG_VMAP_STACK
+/*
+ * vmalloc() is a bit slow, and calling vfree() enough times will force a TLB
+ * flush.  Try to minimize the number of calls by caching stacks.
+ */
+#define NR_CACHED_STACKS 2
+static DEFINE_PER_CPU(struct vm_struct *, cached_stacks[NR_CACHED_STACKS]);
+#endif
+
 static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 {
 #ifdef CONFIG_VMAP_STACK
-	void *stack = __vmalloc_node_range(THREAD_SIZE, THREAD_SIZE,
-					   VMALLOC_START, VMALLOC_END,
-					   THREADINFO_GFP | __GFP_HIGHMEM,
-					   PAGE_KERNEL,
-					   0, node,
-					   __builtin_return_address(0));
+	void *stack;
+	int i;
+
+	local_irq_disable();
+	for (i = 0; i < NR_CACHED_STACKS; i++) {
+		struct vm_struct *s = this_cpu_read(cached_stacks[i]);
+
+		if (!s)
+			continue;
+		this_cpu_write(cached_stacks[i], NULL);
+
+		tsk->stack_vm_area = s;
+		local_irq_enable();
+		return s->addr;
+	}
+	local_irq_enable();
+
+	stack = __vmalloc_node_range(THREAD_SIZE, THREAD_SIZE,
+				     VMALLOC_START, VMALLOC_END,
+				     THREADINFO_GFP | __GFP_HIGHMEM,
+				     PAGE_KERNEL,
+				     0, node, __builtin_return_address(0));
 
 	/*
 	 * We can't call find_vm_area() in interrupt context, and
@@ -187,10 +213,28 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 
 static inline void free_thread_stack(struct task_struct *tsk)
 {
-	if (task_stack_vm_area(tsk))
+#ifdef CONFIG_VMAP_STACK
+	if (task_stack_vm_area(tsk)) {
+		unsigned long flags;
+		int i;
+
+		local_irq_save(flags);
+		for (i = 0; i < NR_CACHED_STACKS; i++) {
+			if (this_cpu_read(cached_stacks[i]))
+				continue;
+
+			this_cpu_write(cached_stacks[i], tsk->stack_vm_area);
+			local_irq_restore(flags);
+			return;
+		}
+		local_irq_restore(flags);
+
 		vfree(tsk->stack);
-	else
-		__free_pages(virt_to_page(tsk->stack), THREAD_SIZE_ORDER);
+		return;
+	}
+#endif
+
+	__free_pages(virt_to_page(tsk->stack), THREAD_SIZE_ORDER);
 }
 # else
 static struct kmem_cache *thread_stack_cache;

commit 68f24b08ee892d47bdef925d676e1ae1ccc316f8
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Sep 15 22:45:48 2016 -0700

    sched/core: Free the stack early if CONFIG_THREAD_INFO_IN_TASK
    
    We currently keep every task's stack around until the task_struct
    itself is freed.  This means that we keep the stack allocation alive
    for longer than necessary and that, under load, we free stacks in
    big batches whenever RCU drops the last task reference.  Neither of
    these is good for reuse of cache-hot memory, and freeing in batches
    prevents us from usefully caching small numbers of vmalloced stacks.
    
    On architectures that have thread_info on the stack, we can't easily
    change this, but on architectures that set THREAD_INFO_IN_TASK, we
    can free it as soon as the task is dead.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/08ca06cde00ebed0046c5d26cbbf3fbb7ef5b812.1474003868.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0c240fd5beba..5dd0a516626d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -269,11 +269,40 @@ static void account_kernel_stack(struct task_struct *tsk, int account)
 	}
 }
 
-void free_task(struct task_struct *tsk)
+static void release_task_stack(struct task_struct *tsk)
 {
 	account_kernel_stack(tsk, -1);
 	arch_release_thread_stack(tsk->stack);
 	free_thread_stack(tsk);
+	tsk->stack = NULL;
+#ifdef CONFIG_VMAP_STACK
+	tsk->stack_vm_area = NULL;
+#endif
+}
+
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+void put_task_stack(struct task_struct *tsk)
+{
+	if (atomic_dec_and_test(&tsk->stack_refcount))
+		release_task_stack(tsk);
+}
+#endif
+
+void free_task(struct task_struct *tsk)
+{
+#ifndef CONFIG_THREAD_INFO_IN_TASK
+	/*
+	 * The task is finally done with both the stack and thread_info,
+	 * so free both.
+	 */
+	release_task_stack(tsk);
+#else
+	/*
+	 * If the task had a separate stack allocation, it should be gone
+	 * by now.
+	 */
+	WARN_ON_ONCE(atomic_read(&tsk->stack_refcount) != 0);
+#endif
 	rt_mutex_debug_task_free(tsk);
 	ftrace_graph_exit_task(tsk);
 	put_seccomp_filter(tsk);
@@ -411,6 +440,9 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 #ifdef CONFIG_VMAP_STACK
 	tsk->stack_vm_area = stack_vm_area;
 #endif
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+	atomic_set(&tsk->stack_refcount, 1);
+#endif
 
 	if (err)
 		goto free_stack;
@@ -1771,6 +1803,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	atomic_dec(&p->cred->user->processes);
 	exit_creds(p);
 bad_fork_free:
+	put_task_stack(p);
 	free_task(p);
 fork_out:
 	return ERR_PTR(retval);

commit d4b80afbba49e968623330f1336da8c724da8aad
Merge: fcd709ef20a9 4cea8776571b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Sep 15 08:24:53 2016 +0200

    Merge branch 'linus' into x86/asm, to pick up recent fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b9677faf45bcf4c63431b62758bfd895404f0f3f
Merge: 511a8cdb6505 b30069291dc7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 1 18:23:22 2016 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge fixes from Andrew Morton:
     "14 fixes"
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>:
      rapidio/tsi721: fix incorrect detection of address translation condition
      rapidio/documentation/mport_cdev: add missing parameter description
      kernel/fork: fix CLONE_CHILD_CLEARTID regression in nscd
      MAINTAINERS: Vladimir has moved
      mm, mempolicy: task->mempolicy must be NULL before dropping final reference
      printk/nmi: avoid direct printk()-s from __printk_nmi_flush()
      treewide: remove references to the now unnecessary DEFINE_PCI_DEVICE_TABLE
      drivers/scsi/wd719x.c: remove last declaration using DEFINE_PCI_DEVICE_TABLE
      mm, vmscan: only allocate and reclaim from zones with pages managed by the buddy allocator
      lib/test_hash.c: fix warning in preprocessor symbol evaluation
      lib/test_hash.c: fix warning in two-dimensional array init
      kconfig: tinyconfig: provide whole choice blocks to avoid warnings
      kexec: fix double-free when failing to relocate the purgatory
      mm, oom: prevent premature OOM killer invocation for high order request

commit 735f2770a770156100f534646158cb58cb8b2939
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Sep 1 16:15:13 2016 -0700

    kernel/fork: fix CLONE_CHILD_CLEARTID regression in nscd
    
    Commit fec1d0115240 ("[PATCH] Disable CLONE_CHILD_CLEARTID for abnormal
    exit") has caused a subtle regression in nscd which uses
    CLONE_CHILD_CLEARTID to clear the nscd_certainly_running flag in the
    shared databases, so that the clients are notified when nscd is
    restarted.  Now, when nscd uses a non-persistent database, clients that
    have it mapped keep thinking the database is being updated by nscd, when
    in fact nscd has created a new (anonymous) one (for non-persistent
    databases it uses an unlinked file as backend).
    
    The original proposal for the CLONE_CHILD_CLEARTID change claimed
    (https://lkml.org/lkml/2006/10/25/233):
    
    : The NPTL library uses the CLONE_CHILD_CLEARTID flag on clone() syscalls
    : on behalf of pthread_create() library calls.  This feature is used to
    : request that the kernel clear the thread-id in user space (at an address
    : provided in the syscall) when the thread disassociates itself from the
    : address space, which is done in mm_release().
    :
    : Unfortunately, when a multi-threaded process incurs a core dump (such as
    : from a SIGSEGV), the core-dumping thread sends SIGKILL signals to all of
    : the other threads, which then proceed to clear their user-space tids
    : before synchronizing in exit_mm() with the start of core dumping.  This
    : misrepresents the state of process's address space at the time of the
    : SIGSEGV and makes it more difficult for someone to debug NPTL and glibc
    : problems (misleading him/her to conclude that the threads had gone away
    : before the fault).
    :
    : The fix below is to simply avoid the CLONE_CHILD_CLEARTID action if a
    : core dump has been initiated.
    
    The resulting patch from Roland (https://lkml.org/lkml/2006/10/26/269)
    seems to have a larger scope than the original patch asked for.  It
    seems that limitting the scope of the check to core dumping should work
    for SIGSEGV issue describe above.
    
    [Changelog partly based on Andreas' description]
    Fixes: fec1d0115240 ("[PATCH] Disable CLONE_CHILD_CLEARTID for abnormal exit")
    Link: http://lkml.kernel.org/r/1471968749-26173-1-git-send-email-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Tested-by: William Preston <wpreston@suse.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Andreas Schwab <schwab@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index aaf782327bf3..93bdba13d7d9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -913,14 +913,12 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 	deactivate_mm(tsk, mm);
 
 	/*
-	 * If we're exiting normally, clear a user-space tid field if
-	 * requested.  We leave this alone when dying by signal, to leave
-	 * the value intact in a core dump, and to save the unnecessary
-	 * trouble, say, a killed vfork parent shouldn't touch this mm.
-	 * Userland only wants this done for a sys_exit.
+	 * Signal userspace if we're not exiting with a core dump
+	 * because we want to leave the value intact for debugging
+	 * purposes.
 	 */
 	if (tsk->clear_child_tid) {
-		if (!(tsk->flags & PF_SIGNALED) &&
+		if (!(tsk->signal->flags & SIGNAL_GROUP_COREDUMP) &&
 		    atomic_read(&mm->mm_users) > 1) {
 			/*
 			 * We don't check the error code - if userspace has

commit 511a8cdb650544b7efd1bbccf7967d3153aee5f6
Merge: 7d1ce606a379 5efc244346f9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 1 15:55:56 2016 -0700

    Merge branch 'stable-4.8' of git://git.infradead.org/users/pcmoore/audit
    
    Pull audit fixes from Paul Moore:
     "Two small patches to fix some bugs with the audit-by-executable
      functionality we introduced back in v4.3 (both patches are marked
      for the stable folks)"
    
    * 'stable-4.8' of git://git.infradead.org/users/pcmoore/audit:
      audit: fix exe_file access in audit_exe_compare
      mm: introduce get_task_exe_file

commit cd81a9170e69e018bbaba547c1fd85a585f5697a
Author: Mateusz Guzik <mguzik@redhat.com>
Date:   Tue Aug 23 16:20:38 2016 +0200

    mm: introduce get_task_exe_file
    
    For more convenient access if one has a pointer to the task.
    
    As a minor nit take advantage of the fact that only task lock + rcu are
    needed to safely grab ->exe_file. This saves mm refcount dance.
    
    Use the helper in proc_exe_link.
    
    Signed-off-by: Mateusz Guzik <mguzik@redhat.com>
    Acked-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Acked-by: Richard Guy Briggs <rgb@redhat.com>
    Cc: <stable@vger.kernel.org> # 4.3.x
    Signed-off-by: Paul Moore <paul@paul-moore.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index d277e83ed3e0..42451aeb245f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -773,6 +773,29 @@ struct file *get_mm_exe_file(struct mm_struct *mm)
 }
 EXPORT_SYMBOL(get_mm_exe_file);
 
+/**
+ * get_task_exe_file - acquire a reference to the task's executable file
+ *
+ * Returns %NULL if task's mm (if any) has no associated executable file or
+ * this is a kernel thread with borrowed mm (see the comment above get_task_mm).
+ * User must release file via fput().
+ */
+struct file *get_task_exe_file(struct task_struct *task)
+{
+	struct file *exe_file = NULL;
+	struct mm_struct *mm;
+
+	task_lock(task);
+	mm = task->mm;
+	if (mm) {
+		if (!(task->flags & PF_KTHREAD))
+			exe_file = get_mm_exe_file(mm);
+	}
+	task_unlock(task);
+	return exe_file;
+}
+EXPORT_SYMBOL(get_task_exe_file);
+
 /**
  * get_task_mm - acquire a reference to the task's mm
  *

commit ba14a194a434ccc8f733e263ad2ce941e35e5787
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Aug 11 02:35:21 2016 -0700

    fork: Add generic vmalloced stack support
    
    If CONFIG_VMAP_STACK=y is selected, kernel stacks are allocated with
    __vmalloc_node_range().
    
    Grsecurity has had a similar feature (called GRKERNSEC_KSTACKOVERFLOW=y)
    for a long time.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/14c07d4fd173a5b117f51e8b939f9f4323e39899.1470907718.git.luto@kernel.org
    [ Minor edits. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 52e725d4a866..9b85f6b2cdcd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -158,19 +158,39 @@ void __weak arch_release_thread_stack(unsigned long *stack)
  * Allocate pages if THREAD_SIZE is >= PAGE_SIZE, otherwise use a
  * kmemcache based allocator.
  */
-# if THREAD_SIZE >= PAGE_SIZE
-static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
-						  int node)
+# if THREAD_SIZE >= PAGE_SIZE || defined(CONFIG_VMAP_STACK)
+static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)
 {
+#ifdef CONFIG_VMAP_STACK
+	void *stack = __vmalloc_node_range(THREAD_SIZE, THREAD_SIZE,
+					   VMALLOC_START, VMALLOC_END,
+					   THREADINFO_GFP | __GFP_HIGHMEM,
+					   PAGE_KERNEL,
+					   0, node,
+					   __builtin_return_address(0));
+
+	/*
+	 * We can't call find_vm_area() in interrupt context, and
+	 * free_thread_stack() can be called in interrupt context,
+	 * so cache the vm_struct.
+	 */
+	if (stack)
+		tsk->stack_vm_area = find_vm_area(stack);
+	return stack;
+#else
 	struct page *page = alloc_pages_node(node, THREADINFO_GFP,
 					     THREAD_SIZE_ORDER);
 
 	return page ? page_address(page) : NULL;
+#endif
 }
 
-static inline void free_thread_stack(unsigned long *stack)
+static inline void free_thread_stack(struct task_struct *tsk)
 {
-	__free_pages(virt_to_page(stack), THREAD_SIZE_ORDER);
+	if (task_stack_vm_area(tsk))
+		vfree(tsk->stack);
+	else
+		__free_pages(virt_to_page(tsk->stack), THREAD_SIZE_ORDER);
 }
 # else
 static struct kmem_cache *thread_stack_cache;
@@ -181,9 +201,9 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
 	return kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
 }
 
-static void free_thread_stack(unsigned long *stack)
+static void free_thread_stack(struct task_struct *tsk)
 {
-	kmem_cache_free(thread_stack_cache, stack);
+	kmem_cache_free(thread_stack_cache, tsk->stack);
 }
 
 void thread_stack_cache_init(void)
@@ -213,24 +233,47 @@ struct kmem_cache *vm_area_cachep;
 /* SLAB cache for mm_struct structures (tsk->mm) */
 static struct kmem_cache *mm_cachep;
 
-static void account_kernel_stack(unsigned long *stack, int account)
+static void account_kernel_stack(struct task_struct *tsk, int account)
 {
-	/* All stack pages are in the same zone and belong to the same memcg. */
-	struct page *first_page = virt_to_page(stack);
+	void *stack = task_stack_page(tsk);
+	struct vm_struct *vm = task_stack_vm_area(tsk);
+
+	BUILD_BUG_ON(IS_ENABLED(CONFIG_VMAP_STACK) && PAGE_SIZE % 1024 != 0);
+
+	if (vm) {
+		int i;
+
+		BUG_ON(vm->nr_pages != THREAD_SIZE / PAGE_SIZE);
+
+		for (i = 0; i < THREAD_SIZE / PAGE_SIZE; i++) {
+			mod_zone_page_state(page_zone(vm->pages[i]),
+					    NR_KERNEL_STACK_KB,
+					    PAGE_SIZE / 1024 * account);
+		}
 
-	mod_zone_page_state(page_zone(first_page), NR_KERNEL_STACK_KB,
-			    THREAD_SIZE / 1024 * account);
+		/* All stack pages belong to the same memcg. */
+		memcg_kmem_update_page_stat(vm->pages[0], MEMCG_KERNEL_STACK_KB,
+					    account * (THREAD_SIZE / 1024));
+	} else {
+		/*
+		 * All stack pages are in the same zone and belong to the
+		 * same memcg.
+		 */
+		struct page *first_page = virt_to_page(stack);
+
+		mod_zone_page_state(page_zone(first_page), NR_KERNEL_STACK_KB,
+				    THREAD_SIZE / 1024 * account);
 
-	memcg_kmem_update_page_stat(
-		first_page, MEMCG_KERNEL_STACK_KB,
-		account * (THREAD_SIZE / 1024));
+		memcg_kmem_update_page_stat(first_page, MEMCG_KERNEL_STACK_KB,
+					    account * (THREAD_SIZE / 1024));
+	}
 }
 
 void free_task(struct task_struct *tsk)
 {
-	account_kernel_stack(tsk->stack, -1);
+	account_kernel_stack(tsk, -1);
 	arch_release_thread_stack(tsk->stack);
-	free_thread_stack(tsk->stack);
+	free_thread_stack(tsk);
 	rt_mutex_debug_task_free(tsk);
 	ftrace_graph_exit_task(tsk);
 	put_seccomp_filter(tsk);
@@ -342,6 +385,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 {
 	struct task_struct *tsk;
 	unsigned long *stack;
+	struct vm_struct *stack_vm_area;
 	int err;
 
 	if (node == NUMA_NO_NODE)
@@ -354,11 +398,23 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	if (!stack)
 		goto free_tsk;
 
+	stack_vm_area = task_stack_vm_area(tsk);
+
 	err = arch_dup_task_struct(tsk, orig);
+
+	/*
+	 * arch_dup_task_struct() clobbers the stack-related fields.  Make
+	 * sure they're properly initialized before using any stack-related
+	 * functions again.
+	 */
+	tsk->stack = stack;
+#ifdef CONFIG_VMAP_STACK
+	tsk->stack_vm_area = stack_vm_area;
+#endif
+
 	if (err)
 		goto free_stack;
 
-	tsk->stack = stack;
 #ifdef CONFIG_SECCOMP
 	/*
 	 * We must handle setting up seccomp filters once we're under
@@ -390,14 +446,14 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	tsk->task_frag.page = NULL;
 	tsk->wake_q.next = NULL;
 
-	account_kernel_stack(stack, 1);
+	account_kernel_stack(tsk, 1);
 
 	kcov_task_init(tsk);
 
 	return tsk;
 
 free_stack:
-	free_thread_stack(stack);
+	free_thread_stack(tsk);
 free_tsk:
 	free_task_struct(tsk);
 	return NULL;

commit 568ac888215c7fb2fabe8ea739b00ec3c1f5d440
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Wed Aug 10 15:43:06 2016 -0400

    cgroup: reduce read locked section of cgroup_threadgroup_rwsem during fork
    
    cgroup_threadgroup_rwsem is acquired in read mode during process exit
    and fork.  It is also grabbed in write mode during
    __cgroups_proc_write().  I've recently run into a scenario with lots
    of memory pressure and OOM and I am beginning to see
    
    systemd
    
     __switch_to+0x1f8/0x350
     __schedule+0x30c/0x990
     schedule+0x48/0xc0
     percpu_down_write+0x114/0x170
     __cgroup_procs_write.isra.12+0xb8/0x3c0
     cgroup_file_write+0x74/0x1a0
     kernfs_fop_write+0x188/0x200
     __vfs_write+0x6c/0xe0
     vfs_write+0xc0/0x230
     SyS_write+0x6c/0x110
     system_call+0x38/0xb4
    
    This thread is waiting on the reader of cgroup_threadgroup_rwsem to
    exit.  The reader itself is under memory pressure and has gone into
    reclaim after fork. There are times the reader also ends up waiting on
    oom_lock as well.
    
     __switch_to+0x1f8/0x350
     __schedule+0x30c/0x990
     schedule+0x48/0xc0
     jbd2_log_wait_commit+0xd4/0x180
     ext4_evict_inode+0x88/0x5c0
     evict+0xf8/0x2a0
     dispose_list+0x50/0x80
     prune_icache_sb+0x6c/0x90
     super_cache_scan+0x190/0x210
     shrink_slab.part.15+0x22c/0x4c0
     shrink_zone+0x288/0x3c0
     do_try_to_free_pages+0x1dc/0x590
     try_to_free_pages+0xdc/0x260
     __alloc_pages_nodemask+0x72c/0xc90
     alloc_pages_current+0xb4/0x1a0
     page_table_alloc+0xc0/0x170
     __pte_alloc+0x58/0x1f0
     copy_page_range+0x4ec/0x950
     copy_process.isra.5+0x15a0/0x1870
     _do_fork+0xa8/0x4b0
     ppc_clone+0x8/0xc
    
    In the meanwhile, all processes exiting/forking are blocked almost
    stalling the system.
    
    This patch moves the threadgroup_change_begin from before
    cgroup_fork() to just before cgroup_canfork().  There is no nee to
    worry about threadgroup changes till the task is actually added to the
    threadgroup.  This avoids having to call reclaim with
    cgroup_threadgroup_rwsem held.
    
    tj: Subject and description edits.
    
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Acked-by: Zefan Li <lizefan@huawei.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: stable@vger.kernel.org # v4.2+
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 52e725d4a866..aaf782327bf3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1404,7 +1404,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->real_start_time = ktime_get_boot_ns();
 	p->io_context = NULL;
 	p->audit_context = NULL;
-	threadgroup_change_begin(current);
 	cgroup_fork(p);
 #ifdef CONFIG_NUMA
 	p->mempolicy = mpol_dup(p->mempolicy);
@@ -1556,6 +1555,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	INIT_LIST_HEAD(&p->thread_group);
 	p->task_works = NULL;
 
+	threadgroup_change_begin(current);
 	/*
 	 * Ensure that the cgroup subsystem policies allow the new process to be
 	 * forked. It should be noted the the new process's css_set can be changed
@@ -1656,6 +1656,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cancel_cgroup:
 	cgroup_cancel_fork(p);
 bad_fork_free_pid:
+	threadgroup_change_end(current);
 	if (pid != &init_struct_pid)
 		free_pid(pid);
 bad_fork_cleanup_thread:
@@ -1688,7 +1689,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	mpol_put(p->mempolicy);
 bad_fork_cleanup_threadgroup_lock:
 #endif
-	threadgroup_change_end(current);
 	delayacct_tsk_free(p);
 bad_fork_cleanup_count:
 	atomic_dec(&p->cred->user->processes);

commit 25f9c0817c535a728c1088542230fa327c577c9e
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Aug 8 14:41:52 2016 -0500

    userns: Generalize the user namespace count into ucount
    
    The same kind of recursive sane default limit and policy
    countrol that has been implemented for the user namespace
    is desirable for the other namespaces, so generalize
    the user namespace refernce count into a ucount.
    
    Acked-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index d8cde533ace3..3cb4853a59aa 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -302,6 +302,7 @@ int arch_task_struct_size __read_mostly;
 
 void __init fork_init(void)
 {
+	int i;
 #ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 #ifndef ARCH_MIN_TASKALIGN
 #define ARCH_MIN_TASKALIGN	L1_CACHE_BYTES
@@ -322,7 +323,9 @@ void __init fork_init(void)
 	init_task.signal->rlim[RLIMIT_SIGPENDING] =
 		init_task.signal->rlim[RLIMIT_NPROC];
 
-	init_user_ns.max_user_namespaces = max_threads/2;
+	for (i = 0; i < UCOUNT_COUNTS; i++) {
+		init_user_ns.ucount_max[i] = max_threads/2;
+	}
 }
 
 int __weak arch_dup_task_struct(struct task_struct *dst,

commit f6b2db1a3e8d141dd144df58900fb0444d5d7c53
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Aug 8 13:54:50 2016 -0500

    userns: Make the count of user namespaces per user
    
    Add a structure that is per user and per user ns and use it to hold
    the count of user namespaces.  This makes prevents one user from
    creating denying service to another user by creating the maximum
    number of user namespaces.
    
    Rename the sysctl export of the maximum count from
    /proc/sys/userns/max_user_namespaces to /proc/sys/user/max_user_namespaces
    to reflect that the count is now per user.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index daa6a82b4900..d8cde533ace3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -322,7 +322,7 @@ void __init fork_init(void)
 	init_task.signal->rlim[RLIMIT_SIGPENDING] =
 		init_task.signal->rlim[RLIMIT_NPROC];
 
-	init_user_ns.max_user_namespaces = max_threads;
+	init_user_ns.max_user_namespaces = max_threads/2;
 }
 
 int __weak arch_dup_task_struct(struct task_struct *dst,

commit b376c3e1b6770ddcb4f0782be16358095fcea0b6
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Aug 8 13:41:24 2016 -0500

    userns: Add a limit on the number of user namespaces
    
    Export the export the maximum number of user namespaces as
    /proc/sys/userns/max_user_namespaces.
    
    Acked-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 52e725d4a866..daa6a82b4900 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -321,6 +321,8 @@ void __init fork_init(void)
 	init_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads/2;
 	init_task.signal->rlim[RLIMIT_SIGPENDING] =
 		init_task.signal->rlim[RLIMIT_NPROC];
+
+	init_user_ns.max_user_namespaces = max_threads;
 }
 
 int __weak arch_dup_task_struct(struct task_struct *dst,

commit efdc94907977d2db84b4b00cb9bd98ca011f6819
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 28 15:48:17 2016 -0700

    mm: fix memcg stack accounting for sub-page stacks
    
    We should account for stacks regardless of stack size, and we need to
    account in sub-page units if THREAD_SIZE < PAGE_SIZE.  Change the units
    to kilobytes and Move it into account_kernel_stack().
    
    Fixes: 12580e4b54ba8 ("mm: memcontrol: report kernel stack usage in cgroup2 memory.stat")
    Link: http://lkml.kernel.org/r/9b5314e3ee5eda61b0317ec1563768602c1ef438.1468523549.git.luto@kernel.org
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index af3637e0ee52..52e725d4a866 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -165,20 +165,12 @@ static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
 	struct page *page = alloc_pages_node(node, THREADINFO_GFP,
 					     THREAD_SIZE_ORDER);
 
-	if (page)
-		memcg_kmem_update_page_stat(page, MEMCG_KERNEL_STACK,
-					    1 << THREAD_SIZE_ORDER);
-
 	return page ? page_address(page) : NULL;
 }
 
 static inline void free_thread_stack(unsigned long *stack)
 {
-	struct page *page = virt_to_page(stack);
-
-	memcg_kmem_update_page_stat(page, MEMCG_KERNEL_STACK,
-				    -(1 << THREAD_SIZE_ORDER));
-	__free_pages(page, THREAD_SIZE_ORDER);
+	__free_pages(virt_to_page(stack), THREAD_SIZE_ORDER);
 }
 # else
 static struct kmem_cache *thread_stack_cache;
@@ -223,10 +215,15 @@ static struct kmem_cache *mm_cachep;
 
 static void account_kernel_stack(unsigned long *stack, int account)
 {
-	struct zone *zone = page_zone(virt_to_page(stack));
+	/* All stack pages are in the same zone and belong to the same memcg. */
+	struct page *first_page = virt_to_page(stack);
 
-	mod_zone_page_state(zone, NR_KERNEL_STACK_KB,
+	mod_zone_page_state(page_zone(first_page), NR_KERNEL_STACK_KB,
 			    THREAD_SIZE / 1024 * account);
+
+	memcg_kmem_update_page_stat(
+		first_page, MEMCG_KERNEL_STACK_KB,
+		account * (THREAD_SIZE / 1024));
 }
 
 void free_task(struct task_struct *tsk)

commit d30dd8be06a5ae640766b20ea9ae288832bd12ac
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Jul 28 15:48:14 2016 -0700

    mm: track NR_KERNEL_STACK in KiB instead of number of stacks
    
    Currently, NR_KERNEL_STACK tracks the number of kernel stacks in a zone.
    This only makes sense if each kernel stack exists entirely in one zone,
    and allowing vmapped stacks could break this assumption.
    
    Since frv has THREAD_SIZE < PAGE_SIZE, we need to track kernel stack
    allocations in a unit that divides both THREAD_SIZE and PAGE_SIZE on all
    architectures.  Keep it simple and use KiB.
    
    Link: http://lkml.kernel.org/r/083c71e642c5fa5f1b6898902e1b2db7b48940d4.1468523549.git.luto@kernel.org
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Reviewed-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index de21f25e0d2c..af3637e0ee52 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -225,7 +225,8 @@ static void account_kernel_stack(unsigned long *stack, int account)
 {
 	struct zone *zone = page_zone(virt_to_page(stack));
 
-	mod_zone_page_state(zone, NR_KERNEL_STACK, account);
+	mod_zone_page_state(zone, NR_KERNEL_STACK_KB,
+			    THREAD_SIZE / 1024 * account);
 }
 
 void free_task(struct task_struct *tsk)

commit 4949148ad433f6f11cf837978b2907092ec99f3a
Author: Vladimir Davydov <vdavydov@virtuozzo.com>
Date:   Tue Jul 26 15:24:24 2016 -0700

    mm: charge/uncharge kmemcg from generic page allocator paths
    
    Currently, to charge a non-slab allocation to kmemcg one has to use
    alloc_kmem_pages helper with __GFP_ACCOUNT flag.  A page allocated with
    this helper should finally be freed using free_kmem_pages, otherwise it
    won't be uncharged.
    
    This API suits its current users fine, but it turns out to be impossible
    to use along with page reference counting, i.e.  when an allocation is
    supposed to be freed with put_page, as it is the case with pipe or unix
    socket buffers.
    
    To overcome this limitation, this patch moves charging/uncharging to
    generic page allocator paths, i.e.  to __alloc_pages_nodemask and
    free_pages_prepare, and zaps alloc/free_kmem_pages helpers.  This way,
    one can use any of the available page allocation functions to get the
    allocated page charged to kmemcg - it's enough to pass __GFP_ACCOUNT,
    just like in case of kmalloc and friends.  A charged page will be
    automatically uncharged on free.
    
    To make it possible, we need to mark pages charged to kmemcg somehow.
    To avoid introducing a new page flag, we make use of page->_mapcount for
    marking such pages.  Since pages charged to kmemcg are not supposed to
    be mapped to userspace, it should work just fine.  There are other
    (ab)users of page->_mapcount - buddy and balloon pages - but we don't
    conflict with them.
    
    In case kmemcg is compiled out or not used at runtime, this patch
    introduces no overhead to generic page allocator paths.  If kmemcg is
    used, it will be plus one gfp flags check on alloc and plus one
    page->_mapcount check on free, which shouldn't hurt performance, because
    the data accessed are hot.
    
    Link: http://lkml.kernel.org/r/a9736d856f895bcb465d9f257b54efe32eda6f99.1464079538.git.vdavydov@virtuozzo.com
    Signed-off-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4a7ec0c6c88c..de21f25e0d2c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -162,8 +162,8 @@ void __weak arch_release_thread_stack(unsigned long *stack)
 static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
 						  int node)
 {
-	struct page *page = alloc_kmem_pages_node(node, THREADINFO_GFP,
-						  THREAD_SIZE_ORDER);
+	struct page *page = alloc_pages_node(node, THREADINFO_GFP,
+					     THREAD_SIZE_ORDER);
 
 	if (page)
 		memcg_kmem_update_page_stat(page, MEMCG_KERNEL_STACK,
@@ -178,7 +178,7 @@ static inline void free_thread_stack(unsigned long *stack)
 
 	memcg_kmem_update_page_stat(page, MEMCG_KERNEL_STACK,
 				    -(1 << THREAD_SIZE_ORDER));
-	__free_kmem_pages(page, THREAD_SIZE_ORDER);
+	__free_pages(page, THREAD_SIZE_ORDER);
 }
 # else
 static struct kmem_cache *thread_stack_cache;

commit 9521d39976db20f8ef9b56af66661482a17d5364
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Jun 25 21:53:30 2016 +1000

    Fix build break in fork.c when THREAD_SIZE < PAGE_SIZE
    
    Commit b235beea9e99 ("Clarify naming of thread info/stack allocators")
    breaks the build on some powerpc configs, where THREAD_SIZE < PAGE_SIZE:
    
      kernel/fork.c:235:2: error: implicit declaration of function 'free_thread_stack'
      kernel/fork.c:355:8: error: assignment from incompatible pointer type
        stack = alloc_thread_stack_node(tsk, node);
        ^
    
    Fix it by renaming free_stack() to free_thread_stack(), and updating the
    return type of alloc_thread_stack_node().
    
    Fixes: b235beea9e99 ("Clarify naming of thread info/stack allocators")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 37b9439b8c07..4a7ec0c6c88c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -183,13 +183,13 @@ static inline void free_thread_stack(unsigned long *stack)
 # else
 static struct kmem_cache *thread_stack_cache;
 
-static struct thread_info *alloc_thread_stack_node(struct task_struct *tsk,
+static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
 						  int node)
 {
 	return kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
 }
 
-static void free_stack(unsigned long *stack)
+static void free_thread_stack(unsigned long *stack)
 {
 	kmem_cache_free(thread_stack_cache, stack);
 }

commit b235beea9e996a4d36fed6cfef4801a3e7d7a9a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 24 15:09:37 2016 -0700

    Clarify naming of thread info/stack allocators
    
    We've had the thread info allocated together with the thread stack for
    most architectures for a long time (since the thread_info was split off
    from the task struct), but that is about to change.
    
    But the patches that move the thread info to be off-stack (and a part of
    the task struct instead) made it clear how confused the allocator and
    freeing functions are.
    
    Because the common case was that we share an allocation with the thread
    stack and the thread_info, the two pointers were identical.  That
    identity then meant that we would have things like
    
            ti = alloc_thread_info_node(tsk, node);
            ...
            tsk->stack = ti;
    
    which certainly _worked_ (since stack and thread_info have the same
    value), but is rather confusing: why are we assigning a thread_info to
    the stack? And if we move the thread_info away, the "confusing" code
    just gets to be entirely bogus.
    
    So remove all this confusion, and make it clear that we are doing the
    stack allocation by renaming and clarifying the function names to be
    about the stack.  The fact that the thread_info then shares the
    allocation is an implementation detail, and not really about the
    allocation itself.
    
    This is a pure renaming and type fix: we pass in the same pointer, it's
    just that we clarify what the pointer means.
    
    The ia64 code that actually only has one single allocation (for all of
    task_struct, thread_info and kernel thread stack) now looks a bit odd,
    but since "tsk->stack" is actually not even used there, that oddity
    doesn't matter.  It would be a separate thing to clean that up, I
    intentionally left the ia64 changes as a pure brute-force renaming and
    type change.
    
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5c2c355aa97f..37b9439b8c07 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -148,18 +148,18 @@ static inline void free_task_struct(struct task_struct *tsk)
 }
 #endif
 
-void __weak arch_release_thread_info(struct thread_info *ti)
+void __weak arch_release_thread_stack(unsigned long *stack)
 {
 }
 
-#ifndef CONFIG_ARCH_THREAD_INFO_ALLOCATOR
+#ifndef CONFIG_ARCH_THREAD_STACK_ALLOCATOR
 
 /*
  * Allocate pages if THREAD_SIZE is >= PAGE_SIZE, otherwise use a
  * kmemcache based allocator.
  */
 # if THREAD_SIZE >= PAGE_SIZE
-static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
+static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,
 						  int node)
 {
 	struct page *page = alloc_kmem_pages_node(node, THREADINFO_GFP,
@@ -172,33 +172,33 @@ static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 	return page ? page_address(page) : NULL;
 }
 
-static inline void free_thread_info(struct thread_info *ti)
+static inline void free_thread_stack(unsigned long *stack)
 {
-	struct page *page = virt_to_page(ti);
+	struct page *page = virt_to_page(stack);
 
 	memcg_kmem_update_page_stat(page, MEMCG_KERNEL_STACK,
 				    -(1 << THREAD_SIZE_ORDER));
 	__free_kmem_pages(page, THREAD_SIZE_ORDER);
 }
 # else
-static struct kmem_cache *thread_info_cache;
+static struct kmem_cache *thread_stack_cache;
 
-static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
+static struct thread_info *alloc_thread_stack_node(struct task_struct *tsk,
 						  int node)
 {
-	return kmem_cache_alloc_node(thread_info_cache, THREADINFO_GFP, node);
+	return kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
 }
 
-static void free_thread_info(struct thread_info *ti)
+static void free_stack(unsigned long *stack)
 {
-	kmem_cache_free(thread_info_cache, ti);
+	kmem_cache_free(thread_stack_cache, stack);
 }
 
-void thread_info_cache_init(void)
+void thread_stack_cache_init(void)
 {
-	thread_info_cache = kmem_cache_create("thread_info", THREAD_SIZE,
+	thread_stack_cache = kmem_cache_create("thread_stack", THREAD_SIZE,
 					      THREAD_SIZE, 0, NULL);
-	BUG_ON(thread_info_cache == NULL);
+	BUG_ON(thread_stack_cache == NULL);
 }
 # endif
 #endif
@@ -221,9 +221,9 @@ struct kmem_cache *vm_area_cachep;
 /* SLAB cache for mm_struct structures (tsk->mm) */
 static struct kmem_cache *mm_cachep;
 
-static void account_kernel_stack(struct thread_info *ti, int account)
+static void account_kernel_stack(unsigned long *stack, int account)
 {
-	struct zone *zone = page_zone(virt_to_page(ti));
+	struct zone *zone = page_zone(virt_to_page(stack));
 
 	mod_zone_page_state(zone, NR_KERNEL_STACK, account);
 }
@@ -231,8 +231,8 @@ static void account_kernel_stack(struct thread_info *ti, int account)
 void free_task(struct task_struct *tsk)
 {
 	account_kernel_stack(tsk->stack, -1);
-	arch_release_thread_info(tsk->stack);
-	free_thread_info(tsk->stack);
+	arch_release_thread_stack(tsk->stack);
+	free_thread_stack(tsk->stack);
 	rt_mutex_debug_task_free(tsk);
 	ftrace_graph_exit_task(tsk);
 	put_seccomp_filter(tsk);
@@ -343,7 +343,7 @@ void set_task_stack_end_magic(struct task_struct *tsk)
 static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 {
 	struct task_struct *tsk;
-	struct thread_info *ti;
+	unsigned long *stack;
 	int err;
 
 	if (node == NUMA_NO_NODE)
@@ -352,15 +352,15 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	if (!tsk)
 		return NULL;
 
-	ti = alloc_thread_info_node(tsk, node);
-	if (!ti)
+	stack = alloc_thread_stack_node(tsk, node);
+	if (!stack)
 		goto free_tsk;
 
 	err = arch_dup_task_struct(tsk, orig);
 	if (err)
-		goto free_ti;
+		goto free_stack;
 
-	tsk->stack = ti;
+	tsk->stack = stack;
 #ifdef CONFIG_SECCOMP
 	/*
 	 * We must handle setting up seccomp filters once we're under
@@ -392,14 +392,14 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 	tsk->task_frag.page = NULL;
 	tsk->wake_q.next = NULL;
 
-	account_kernel_stack(ti, 1);
+	account_kernel_stack(stack, 1);
 
 	kcov_task_init(tsk);
 
 	return tsk;
 
-free_ti:
-	free_thread_info(ti);
+free_stack:
+	free_thread_stack(stack);
 free_tsk:
 	free_task_struct(tsk);
 	return NULL;

commit 7ef949d77f95f0d129f0d404b336459a34a00101
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu May 26 15:16:22 2016 -0700

    mm: oom_reaper: remove some bloat
    
    mmput_async is currently used only from the oom_reaper which is defined
    only for CONFIG_MMU.  We can save work_struct in mm_struct for
    !CONFIG_MMU.
    
    [akpm@linux-foundation.org: fix typo, per Minchan]
    Link: http://lkml.kernel.org/r/20160520061658.GB19172@dhcp22.suse.cz
    Reported-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 47887bba944f..5c2c355aa97f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -736,6 +736,7 @@ void mmput(struct mm_struct *mm)
 }
 EXPORT_SYMBOL_GPL(mmput);
 
+#ifdef CONFIG_MMU
 static void mmput_async_fn(struct work_struct *work)
 {
 	struct mm_struct *mm = container_of(work, struct mm_struct, async_put_work);
@@ -749,6 +750,7 @@ void mmput_async(struct mm_struct *mm)
 		schedule_work(&mm->async_put_work);
 	}
 }
+#endif
 
 /**
  * set_mm_exe_file - change a reference to the mm's executable file

commit 7c051267931a9be9c6620cc17b362bc6ee6dedc8
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 23 16:25:48 2016 -0700

    mm, fork: make dup_mmap wait for mmap_sem for write killable
    
    dup_mmap needs to lock current's mm mmap_sem for write.  If the waiting
    task gets killed by the oom killer it would block oom_reaper from
    asynchronous address space reclaim and reduce the chances of timely OOM
    resolving.  Wait for the lock in the killable mode and return with EINTR
    if the task got killed while waiting.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e67d7b773348..47887bba944f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -414,7 +414,10 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	unsigned long charge;
 
 	uprobe_start_dup_mmap();
-	down_write(&oldmm->mmap_sem);
+	if (down_write_killable(&oldmm->mmap_sem)) {
+		retval = -EINTR;
+		goto fail_uprobe_end;
+	}
 	flush_cache_dup_mm(oldmm);
 	uprobe_dup_mmap(oldmm, mm);
 	/*
@@ -526,6 +529,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	up_write(&mm->mmap_sem);
 	flush_tlb_mm(oldmm);
 	up_write(&oldmm->mmap_sem);
+fail_uprobe_end:
 	uprobe_end_dup_mmap();
 	return retval;
 fail_nomem_anon_vma_fork:

commit 725fc629ff2545b061407305ae51016c9f928fce
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon May 23 16:24:05 2016 -0700

    kernek/fork.c: allocate idle task for a CPU always on its local node
    
    Linux preallocates the task structs of the idle tasks for all possible
    CPUs.  This currently means they all end up on node 0.  This also
    implies that the cache line of MWAIT, which is around the flags field in
    the task struct, are all located in node 0.
    
    We see a noticeable performance improvement on Knights Landing CPUs when
    the cache lines used for MWAIT are located in the local nodes of the
    CPUs using them.  I would expect this to give a (likely slight)
    improvement on other systems too.
    
    The patch implements placing the idle task in the node of its CPUs, by
    passing the right target node to copy_process()
    
    [akpm@linux-foundation.org: use NUMA_NO_NODE, not a bare -1]
    Link: http://lkml.kernel.org/r/1463492694-15833-1-git-send-email-andi@firstfloor.org
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 103d78fd8f75..e67d7b773348 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -340,13 +340,14 @@ void set_task_stack_end_magic(struct task_struct *tsk)
 	*stackend = STACK_END_MAGIC;	/* for overflow detection */
 }
 
-static struct task_struct *dup_task_struct(struct task_struct *orig)
+static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 {
 	struct task_struct *tsk;
 	struct thread_info *ti;
-	int node = tsk_fork_get_node(orig);
 	int err;
 
+	if (node == NUMA_NO_NODE)
+		node = tsk_fork_get_node(orig);
 	tsk = alloc_task_struct_node(node);
 	if (!tsk)
 		return NULL;
@@ -1276,7 +1277,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 					int __user *child_tidptr,
 					struct pid *pid,
 					int trace,
-					unsigned long tls)
+					unsigned long tls,
+					int node)
 {
 	int retval;
 	struct task_struct *p;
@@ -1328,7 +1330,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto fork_out;
 
 	retval = -ENOMEM;
-	p = dup_task_struct(current);
+	p = dup_task_struct(current, node);
 	if (!p)
 		goto fork_out;
 
@@ -1706,7 +1708,8 @@ static inline void init_idle_pids(struct pid_link *links)
 struct task_struct *fork_idle(int cpu)
 {
 	struct task_struct *task;
-	task = copy_process(CLONE_VM, 0, 0, NULL, &init_struct_pid, 0, 0);
+	task = copy_process(CLONE_VM, 0, 0, NULL, &init_struct_pid, 0, 0,
+			    cpu_to_node(cpu));
 	if (!IS_ERR(task)) {
 		init_idle_pids(task->pids);
 		init_idle(task, cpu);
@@ -1751,7 +1754,7 @@ long _do_fork(unsigned long clone_flags,
 	}
 
 	p = copy_process(clone_flags, stack_start, stack_size,
-			 child_tidptr, NULL, trace, tls);
+			 child_tidptr, NULL, trace, tls, NUMA_NO_NODE);
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.

commit 0740aa5f6375681c57488c4ea55d05a0341cfc9c
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 20 17:00:25 2016 -0700

    fork: free thread in copy_process on failure
    
    When using this program (as root):
    
            #include <err.h>
            #include <stdio.h>
            #include <stdlib.h>
            #include <unistd.h>
    
            #include <sys/io.h>
            #include <sys/types.h>
            #include <sys/wait.h>
    
            #define ITER 1000
            #define FORKERS 15
            #define THREADS (6000/FORKERS) // 1850 is proc max
    
            static void fork_100_wait()
            {
                    unsigned a, to_wait = 0;
    
                    printf("\t%d forking %d\n", THREADS, getpid());
    
                    for (a = 0; a < THREADS; a++) {
                            switch (fork()) {
                            case 0:
                                    usleep(1000);
                                    exit(0);
                                    break;
                            case -1:
                                    break;
                            default:
                                    to_wait++;
                                    break;
                            }
                    }
    
                    printf("\t%d forked from %d, waiting for %d\n", THREADS, getpid(),
                                    to_wait);
    
                    for (a = 0; a < to_wait; a++)
                            wait(NULL);
    
                    printf("\t%d waited from %d\n", THREADS, getpid());
            }
    
            static void run_forkers()
            {
                    pid_t forkers[FORKERS];
                    unsigned a;
    
                    for (a = 0; a < FORKERS; a++) {
                            switch ((forkers[a] = fork())) {
                            case 0:
                                    fork_100_wait();
                                    exit(0);
                                    break;
                            case -1:
                                    err(1, "DIE fork of %d'th forker", a);
                                    break;
                            default:
                                    break;
                            }
                    }
    
                    for (a = 0; a < FORKERS; a++)
                            waitpid(forkers[a], NULL, 0);
            }
    
            int main()
            {
                    unsigned a;
                    int ret;
    
                    ret = ioperm(10, 20, 0);
                    if (ret < 0)
                            err(1, "ioperm");
    
                    for (a = 0; a < ITER; a++)
                            run_forkers();
    
                    return 0;
            }
    
    kmemleak reports many occurences of this leak:
    unreferenced object 0xffff8805917c8000 (size 8192):
      comm "fork-leak", pid 2932, jiffies 4295354292 (age 1871.028s)
      hex dump (first 32 bytes):
        ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff  ................
        ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff  ................
      backtrace:
        [<ffffffff814cfbf5>] kmemdup+0x25/0x50
        [<ffffffff8103ab43>] copy_thread_tls+0x6c3/0x9a0
        [<ffffffff81150174>] copy_process+0x1a84/0x5790
        [<ffffffff811dc375>] wake_up_new_task+0x2d5/0x6f0
        [<ffffffff8115411d>] _do_fork+0x12d/0x820
    ...
    
    Due to the leakage of the memory items which should have been freed in
    arch/x86/kernel/process.c:exit_thread().
    
    Make sure the memory is freed when fork fails later in copy_process.
    This is done by calling exit_thread with the thread to kill.
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8fbed7194af1..103d78fd8f75 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1490,7 +1490,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		pid = alloc_pid(p->nsproxy->pid_ns_for_children);
 		if (IS_ERR(pid)) {
 			retval = PTR_ERR(pid);
-			goto bad_fork_cleanup_io;
+			goto bad_fork_cleanup_thread;
 		}
 	}
 
@@ -1652,6 +1652,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_free_pid:
 	if (pid != &init_struct_pid)
 		free_pid(pid);
+bad_fork_cleanup_thread:
+	exit_thread(p);
 bad_fork_cleanup_io:
 	if (p->io_context)
 		exit_io_context(p);

commit ec8d7c14ea14922fe21945b458a75e39f11dd832
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri May 20 16:57:21 2016 -0700

    mm, oom_reaper: do not mmput synchronously from the oom reaper context
    
    Tetsuo has properly noted that mmput slow path might get blocked waiting
    for another party (e.g.  exit_aio waits for an IO).  If that happens the
    oom_reaper would be put out of the way and will not be able to process
    next oom victim.  We should strive for making this context as reliable
    and independent on other subsystems as much as possible.
    
    Introduce mmput_async which will perform the slow path from an async
    (WQ) context.  This will delay the operation but that shouldn't be a
    problem because the oom_reaper has reclaimed the victim's address space
    for most cases as much as possible and the remaining context shouldn't
    bind too much memory anymore.  The only exception is when mmap_sem
    trylock has failed which shouldn't happen too often.
    
    The issue is only theoretical but not impossible.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3e8451527cbe..8fbed7194af1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -699,6 +699,26 @@ void __mmdrop(struct mm_struct *mm)
 }
 EXPORT_SYMBOL_GPL(__mmdrop);
 
+static inline void __mmput(struct mm_struct *mm)
+{
+	VM_BUG_ON(atomic_read(&mm->mm_users));
+
+	uprobe_clear_state(mm);
+	exit_aio(mm);
+	ksm_exit(mm);
+	khugepaged_exit(mm); /* must run before exit_mmap */
+	exit_mmap(mm);
+	set_mm_exe_file(mm, NULL);
+	if (!list_empty(&mm->mmlist)) {
+		spin_lock(&mmlist_lock);
+		list_del(&mm->mmlist);
+		spin_unlock(&mmlist_lock);
+	}
+	if (mm->binfmt)
+		module_put(mm->binfmt->module);
+	mmdrop(mm);
+}
+
 /*
  * Decrement the use count and release all resources for an mm.
  */
@@ -706,24 +726,24 @@ void mmput(struct mm_struct *mm)
 {
 	might_sleep();
 
+	if (atomic_dec_and_test(&mm->mm_users))
+		__mmput(mm);
+}
+EXPORT_SYMBOL_GPL(mmput);
+
+static void mmput_async_fn(struct work_struct *work)
+{
+	struct mm_struct *mm = container_of(work, struct mm_struct, async_put_work);
+	__mmput(mm);
+}
+
+void mmput_async(struct mm_struct *mm)
+{
 	if (atomic_dec_and_test(&mm->mm_users)) {
-		uprobe_clear_state(mm);
-		exit_aio(mm);
-		ksm_exit(mm);
-		khugepaged_exit(mm); /* must run before exit_mmap */
-		exit_mmap(mm);
-		set_mm_exe_file(mm, NULL);
-		if (!list_empty(&mm->mmlist)) {
-			spin_lock(&mmlist_lock);
-			list_del(&mm->mmlist);
-			spin_unlock(&mmlist_lock);
-		}
-		if (mm->binfmt)
-			module_put(mm->binfmt->module);
-		mmdrop(mm);
+		INIT_WORK(&mm->async_put_work, mmput_async_fn);
+		schedule_work(&mm->async_put_work);
 	}
 }
-EXPORT_SYMBOL_GPL(mmput);
 
 /**
  * set_mm_exe_file - change a reference to the mm's executable file

commit 2a74213838104a41588d86fd5e8d344972891ace
Author: Stas Sergeev <stsp@list.ru>
Date:   Thu Apr 14 23:20:04 2016 +0300

    signals/sigaltstack: Implement SS_AUTODISARM flag
    
    This patch implements the SS_AUTODISARM flag that can be OR-ed with
    SS_ONSTACK when forming ss_flags.
    
    When this flag is set, sigaltstack will be disabled when entering
    the signal handler; more precisely, after saving sas to uc_stack.
    When leaving the signal handler, the sigaltstack is restored by
    uc_stack.
    
    When this flag is used, it is safe to switch from sighandler with
    swapcontext(). Without this flag, the subsequent signal will corrupt
    the state of the switched-away sighandler.
    
    To detect the support of this functionality, one can do:
    
      err = sigaltstack(SS_DISABLE | SS_AUTODISARM);
      if (err && errno == EINVAL)
            unsupported();
    
    Signed-off-by: Stas Sergeev <stsp@list.ru>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Aleksa Sarai <cyphar@cyphar.com>
    Cc: Amanieu d'Antras <amanieu@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Heinrich Schuchardt <xypron.glpk@gmx.de>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Palmer Dabbelt <palmer@dabbelt.com>
    Cc: Paul Moore <pmoore@redhat.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Shuah Khan <shuahkh@osg.samsung.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: linux-api@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1460665206-13646-4-git-send-email-stsp@list.ru
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d277e83ed3e0..3e8451527cbe 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1494,7 +1494,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * sigaltstack should be cleared when sharing the same VM
 	 */
 	if ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)
-		p->sas_ss_sp = p->sas_ss_size = 0;
+		sas_ss_reset(p);
 
 	/*
 	 * Syscall tracing and stepping should be turned off in the

commit 5c9a8750a6409c63a0f01d51a9024861022f6593
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Tue Mar 22 14:27:30 2016 -0700

    kernel: add kcov code coverage
    
    kcov provides code coverage collection for coverage-guided fuzzing
    (randomized testing).  Coverage-guided fuzzing is a testing technique
    that uses coverage feedback to determine new interesting inputs to a
    system.  A notable user-space example is AFL
    (http://lcamtuf.coredump.cx/afl/).  However, this technique is not
    widely used for kernel testing due to missing compiler and kernel
    support.
    
    kcov does not aim to collect as much coverage as possible.  It aims to
    collect more or less stable coverage that is function of syscall inputs.
    To achieve this goal it does not collect coverage in soft/hard
    interrupts and instrumentation of some inherently non-deterministic or
    non-interesting parts of kernel is disbled (e.g.  scheduler, locking).
    
    Currently there is a single coverage collection mode (tracing), but the
    API anticipates additional collection modes.  Initially I also
    implemented a second mode which exposes coverage in a fixed-size hash
    table of counters (what Quentin used in his original patch).  I've
    dropped the second mode for simplicity.
    
    This patch adds the necessary support on kernel side.  The complimentary
    compiler support was added in gcc revision 231296.
    
    We've used this support to build syzkaller system call fuzzer, which has
    found 90 kernel bugs in just 2 months:
    
      https://github.com/google/syzkaller/wiki/Found-Bugs
    
    We've also found 30+ bugs in our internal systems with syzkaller.
    Another (yet unexplored) direction where kcov coverage would greatly
    help is more traditional "blob mutation".  For example, mounting a
    random blob as a filesystem, or receiving a random blob over wire.
    
    Why not gcov.  Typical fuzzing loop looks as follows: (1) reset
    coverage, (2) execute a bit of code, (3) collect coverage, repeat.  A
    typical coverage can be just a dozen of basic blocks (e.g.  an invalid
    input).  In such context gcov becomes prohibitively expensive as
    reset/collect coverage steps depend on total number of basic
    blocks/edges in program (in case of kernel it is about 2M).  Cost of
    kcov depends only on number of executed basic blocks/edges.  On top of
    that, kernel requires per-thread coverage because there are always
    background threads and unrelated processes that also produce coverage.
    With inlined gcov instrumentation per-thread coverage is not possible.
    
    kcov exposes kernel PCs and control flow to user-space which is
    insecure.  But debugfs should not be mapped as user accessible.
    
    Based on a patch by Quentin Casasnovas.
    
    [akpm@linux-foundation.org: make task_struct.kcov_mode have type `enum kcov_mode']
    [akpm@linux-foundation.org: unbreak allmodconfig]
    [akpm@linux-foundation.org: follow x86 Makefile layout standards]
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Cc: syzkaller <syzkaller@googlegroups.com>
    Cc: Vegard Nossum <vegard.nossum@oracle.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Tavis Ormandy <taviso@google.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Kees Cook <keescook@google.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: David Drysdale <drysdale@google.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5b8d1e7ceeea..d277e83ed3e0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -75,6 +75,7 @@
 #include <linux/aio.h>
 #include <linux/compiler.h>
 #include <linux/sysctl.h>
+#include <linux/kcov.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -392,6 +393,8 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 
 	account_kernel_stack(ti, 1);
 
+	kcov_task_init(tsk);
+
 	return tsk;
 
 free_ti:

commit 5518f66b5a64b76fd602a7baf60590cd838a2ca0
Merge: 643ad15d4741 fa5ff8a1c43f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 21 10:05:13 2016 -0700

    Merge branch 'for-4.6-ns' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup namespace support from Tejun Heo:
     "These are changes to implement namespace support for cgroup which has
      been pending for quite some time now.  It is very straight-forward and
      only affects what part of cgroup hierarchies are visible.
    
      After unsharing, mounting a cgroup fs will be scoped to the cgroups
      the task belonged to at the time of unsharing and the cgroup paths
      exposed to userland would be adjusted accordingly"
    
    * 'for-4.6-ns' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: fix and restructure error handling in copy_cgroup_ns()
      cgroup: fix alloc_cgroup_ns() error handling in copy_cgroup_ns()
      Add FS_USERNS_FLAG to cgroup fs
      cgroup: Add documentation for cgroup namespaces
      cgroup: mount cgroupns-root when inside non-init cgroupns
      kernfs: define kernfs_node_dentry
      cgroup: cgroup namespace setns support
      cgroup: introduce cgroup namespaces
      sched: new clone flag CLONE_NEWCGROUP for cgroup namespace
      kernfs: Add API to generate relative kernfs path

commit 12580e4b54ba8a1b22ec977c200be0174ca42348
Author: Vladimir Davydov <vdavydov@virtuozzo.com>
Date:   Thu Mar 17 14:17:38 2016 -0700

    mm: memcontrol: report kernel stack usage in cgroup2 memory.stat
    
    Show how much memory is allocated to kernel stacks.
    
    Signed-off-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2e391c754ae7..accb7221d547 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -164,12 +164,20 @@ static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 	struct page *page = alloc_kmem_pages_node(node, THREADINFO_GFP,
 						  THREAD_SIZE_ORDER);
 
+	if (page)
+		memcg_kmem_update_page_stat(page, MEMCG_KERNEL_STACK,
+					    1 << THREAD_SIZE_ORDER);
+
 	return page ? page_address(page) : NULL;
 }
 
 static inline void free_thread_info(struct thread_info *ti)
 {
-	free_kmem_pages((unsigned long)ti, THREAD_SIZE_ORDER);
+	struct page *page = virt_to_page(ti);
+
+	memcg_kmem_update_page_stat(page, MEMCG_KERNEL_STACK,
+				    -(1 << THREAD_SIZE_ORDER));
+	__free_kmem_pages(page, THREAD_SIZE_ORDER);
 }
 # else
 static struct kmem_cache *thread_info_cache;

commit a79a908fd2b080977b45bf103184b81c9d11ad07
Author: Aditya Kali <adityakali@google.com>
Date:   Fri Jan 29 02:54:06 2016 -0600

    cgroup: introduce cgroup namespaces
    
    Introduce the ability to create new cgroup namespace. The newly created
    cgroup namespace remembers the cgroup of the process at the point
    of creation of the cgroup namespace (referred as cgroupns-root).
    The main purpose of cgroup namespace is to virtualize the contents
    of /proc/self/cgroup file. Processes inside a cgroup namespace
    are only able to see paths relative to their namespace root
    (unless they are moved outside of their cgroupns-root, at which point
     they will see a relative path from their cgroupns-root).
    For a correctly setup container this enables container-tools
    (like libcontainer, lxc, lmctfy, etc.) to create completely virtualized
    containers without leaking system level cgroup hierarchy to the task.
    This patch only implements the 'unshare' part of the cgroupns.
    
    Signed-off-by: Aditya Kali <adityakali@google.com>
    Signed-off-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2e391c754ae7..6611a6267949 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1884,7 +1884,7 @@ static int check_unshare_flags(unsigned long unshare_flags)
 	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
 				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
-				CLONE_NEWUSER|CLONE_NEWPID))
+				CLONE_NEWUSER|CLONE_NEWPID|CLONE_NEWCGROUP))
 		return -EINVAL;
 	/*
 	 * Not implemented, but pretend it works if there is nothing

commit 84638335900f1995495838fe1bd4870c43ec1f67
Author: Konstantin Khlebnikov <koct9i@gmail.com>
Date:   Thu Jan 14 15:22:07 2016 -0800

    mm: rework virtual memory accounting
    
    When inspecting a vague code inside prctl(PR_SET_MM_MEM) call (which
    testing the RLIMIT_DATA value to figure out if we're allowed to assign
    new @start_brk, @brk, @start_data, @end_data from mm_struct) it's been
    commited that RLIMIT_DATA in a form it's implemented now doesn't do
    anything useful because most of user-space libraries use mmap() syscall
    for dynamic memory allocations.
    
    Linus suggested to convert RLIMIT_DATA rlimit into something suitable
    for anonymous memory accounting.  But in this patch we go further, and
    the changes are bundled together as:
    
     * keep vma counting if CONFIG_PROC_FS=n, will be used for limits
     * replace mm->shared_vm with better defined mm->data_vm
     * account anonymous executable areas as executable
     * account file-backed growsdown/up areas as stack
     * drop struct file* argument from vm_stat_account
     * enforce RLIMIT_DATA for size of data areas
    
    This way code looks cleaner: now code/stack/data classification depends
    only on vm_flags state:
    
     VM_EXEC & ~VM_WRITE            -> code  (VmExe + VmLib in proc)
     VM_GROWSUP | VM_GROWSDOWN      -> stack (VmStk)
     VM_WRITE & ~VM_SHARED & !stack -> data  (VmData)
    
    The rest (VmSize - VmData - VmStk - VmExe - VmLib) could be called
    "shared", but that might be strange beast like readonly-private or VM_IO
    area.
    
     - RLIMIT_AS            limits whole address space "VmSize"
     - RLIMIT_STACK         limits stack "VmStk" (but each vma individually)
     - RLIMIT_DATA          now limits "VmData"
    
    Signed-off-by: Konstantin Khlebnikov <koct9i@gmail.com>
    Signed-off-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Willy Tarreau <w@1wt.eu>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Kees Cook <keescook@google.com>
    Cc: Vladimir Davydov <vdavydov@virtuozzo.com>
    Cc: Pavel Emelyanov <xemul@virtuozzo.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 51915842f1c0..2e391c754ae7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -414,7 +414,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
 
 	mm->total_vm = oldmm->total_vm;
-	mm->shared_vm = oldmm->shared_vm;
+	mm->data_vm = oldmm->data_vm;
 	mm->exec_vm = oldmm->exec_vm;
 	mm->stack_vm = oldmm->stack_vm;
 
@@ -433,8 +433,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		struct file *file;
 
 		if (mpnt->vm_flags & VM_DONTCOPY) {
-			vm_stat_account(mm, mpnt->vm_flags, mpnt->vm_file,
-							-vma_pages(mpnt));
+			vm_stat_account(mm, mpnt->vm_flags, -vma_pages(mpnt));
 			continue;
 		}
 		charge = 0;

commit 5d097056c9a017a3b720849efb5432f37acabbac
Author: Vladimir Davydov <vdavydov@virtuozzo.com>
Date:   Thu Jan 14 15:18:21 2016 -0800

    kmemcg: account certain kmem allocations to memcg
    
    Mark those kmem allocations that are known to be easily triggered from
    userspace as __GFP_ACCOUNT/SLAB_ACCOUNT, which makes them accounted to
    memcg.  For the list, see below:
    
     - threadinfo
     - task_struct
     - task_delay_info
     - pid
     - cred
     - mm_struct
     - vm_area_struct and vm_region (nommu)
     - anon_vma and anon_vma_chain
     - signal_struct
     - sighand_struct
     - fs_struct
     - files_struct
     - fdtable and fdtable->full_fds_bits
     - dentry and external_name
     - inode for all filesystems. This is the most tedious part, because
       most filesystems overwrite the alloc_inode method.
    
    The list is far from complete, so feel free to add more objects.
    Nevertheless, it should be close to "account everything" approach and
    keep most workloads within bounds.  Malevolent users will be able to
    breach the limit, but this was possible even with the former "account
    everything" approach (simply because it did not account everything in
    fact).
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6774e6b2e96d..51915842f1c0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -300,9 +300,9 @@ void __init fork_init(void)
 #define ARCH_MIN_TASKALIGN	L1_CACHE_BYTES
 #endif
 	/* create a slab on which task_structs can be allocated */
-	task_struct_cachep =
-		kmem_cache_create("task_struct", arch_task_struct_size,
-			ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
+	task_struct_cachep = kmem_cache_create("task_struct",
+			arch_task_struct_size, ARCH_MIN_TASKALIGN,
+			SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT, NULL);
 #endif
 
 	/* do the arch specific task caches init */
@@ -1848,16 +1848,19 @@ void __init proc_caches_init(void)
 	sighand_cachep = kmem_cache_create("sighand_cache",
 			sizeof(struct sighand_struct), 0,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_DESTROY_BY_RCU|
-			SLAB_NOTRACK, sighand_ctor);
+			SLAB_NOTRACK|SLAB_ACCOUNT, sighand_ctor);
 	signal_cachep = kmem_cache_create("signal_cache",
 			sizeof(struct signal_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
+			NULL);
 	files_cachep = kmem_cache_create("files_cache",
 			sizeof(struct files_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
+			NULL);
 	fs_cachep = kmem_cache_create("fs_cache",
 			sizeof(struct fs_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
+			NULL);
 	/*
 	 * FIXME! The "sizeof(struct mm_struct)" currently includes the
 	 * whole struct cpumask for the OFFSTACK case. We could change
@@ -1867,8 +1870,9 @@ void __init proc_caches_init(void)
 	 */
 	mm_cachep = kmem_cache_create("mm_struct",
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
-	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK|SLAB_ACCOUNT,
+			NULL);
+	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC|SLAB_ACCOUNT);
 	mmap_init();
 	nsproxy_cache_init();
 }

commit 34a9304a96d6351c2d35dcdc9293258378fc0bd8
Merge: aee3bfa3307c 6255c46fa037
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 12 19:20:32 2016 -0800

    Merge branch 'for-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - cgroup v2 interface is now official.  It's no longer hidden behind a
       devel flag and can be mounted using the new cgroup2 fs type.
    
       Unfortunately, cpu v2 interface hasn't made it yet due to the
       discussion around in-process hierarchical resource distribution and
       only memory and io controllers can be used on the v2 interface at the
       moment.
    
     - The existing documentation which has always been a bit of mess is
       relocated under Documentation/cgroup-v1/. Documentation/cgroup-v2.txt
       is added as the authoritative documentation for the v2 interface.
    
     - Some features are added through for-4.5-ancestor-test branch to
       enable netfilter xt_cgroup match to use cgroup v2 paths.  The actual
       netfilter changes will be merged through the net tree which pulled in
       the said branch.
    
     - Various cleanups
    
    * 'for-4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: rename cgroup documentations
      cgroup: fix a typo.
      cgroup: Remove resource_counter.txt in Documentation/cgroup-legacy/00-INDEX.
      cgroup: demote subsystem init messages to KERN_DEBUG
      cgroup: Fix uninitialized variable warning
      cgroup: put controller Kconfig options in meaningful order
      cgroup: clean up the kernel configuration menu nomenclature
      cgroup_pids: fix a typo.
      Subject: cgroup: Fix incomplete dd command in blkio documentation
      cgroup: kill cgrp_ss_priv[CGROUP_CANFORK_COUNT] and friends
      cpuset: Replace all instances of time_t with time64_t
      cgroup: replace unified-hierarchy.txt with a proper cgroup v2 documentation
      cgroup: rename Documentation/cgroups/ to Documentation/cgroup-legacy/
      cgroup: replace __DEVEL__sane_behavior with cgroup2 fs type

commit 567bee2803cb46caeb6011de5b738fde33dc3896
Merge: aa0b7ae06387 093e5840ae76
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jan 6 11:02:29 2016 +0100

    Merge branch 'sched/urgent' into sched/core, to pick up fixes before merging new patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 093e5840ae76f1082633503964d035f40ed0216d
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Mon Dec 21 18:17:10 2015 +0100

    sched/core: Reset task's lockless wake-queues on fork()
    
    In the following commit:
    
      7675104990ed ("sched: Implement lockless wake-queues")
    
    we gained lockless wake-queues.
    
    The -RT kernel managed to lockup itself with those. There could be multiple
    attempts for task X to enqueue it for a wakeup _even_ if task X is already
    running.
    
    The reason is that task X could be runnable but not yet on CPU. The the
    task performing the wakeup did not leave the CPU it could performe
    multiple wakeups.
    
    With the proper timming task X could be running and enqueued for a
    wakeup. If this happens while X is performing a fork() then its its
    child will have a !NULL `wake_q` member copied.
    
    This is not a problem as long as the child task does not participate in
    lockless wakeups :)
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 7675104990ed ("sched: Implement lockless wake-queues")
    Link: http://lkml.kernel.org/r/20151221171710.GA5499@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fce002ee3ddf..1155eac61687 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -380,6 +380,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 #endif
 	tsk->splice_pipe = NULL;
 	tsk->task_frag.page = NULL;
+	tsk->wake_q.next = NULL;
 
 	account_kernel_stack(ti, 1);
 

commit b7ce2277f087fd052e7e1bbf432f7fecbee82bb6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 19 16:47:34 2015 +0100

    sched/cputime: Convert vtime_seqlock to seqcount
    
    The cputime can only be updated by the current task itself, even in
    vtime case. So we can safely use seqcount instead of seqlock as there
    is no writer concurrency involved.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447948054-28668-8-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c0a13706b1a7..eea32b55432a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1348,7 +1348,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	prev_cputime_init(&p->prev_cputime);
 
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-	seqlock_init(&p->vtime_seqlock);
+	seqcount_init(&p->vtime_seqcount);
 	p->vtime_snap = 0;
 	p->vtime_snap_whence = VTIME_INACTIVE;
 #endif

commit 7098c1eac75dc03fdbb7249171a6e68ce6044a5a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 19 16:47:30 2015 +0100

    sched/cputime: Clarify vtime symbols and document them
    
    VTIME_SLEEPING state happens either when:
    
    1) The task is sleeping and no tickless delta is to be added on the task
       cputime stats.
    2) The CPU isn't running vtime at all, so the same properties of 1) applies.
    
    Lets rename the vtime symbol to reflect both states.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1447948054-28668-4-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f97f2c449f5c..c0a13706b1a7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1350,7 +1350,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 	seqlock_init(&p->vtime_seqlock);
 	p->vtime_snap = 0;
-	p->vtime_snap_whence = VTIME_SLEEPING;
+	p->vtime_snap_whence = VTIME_INACTIVE;
 #endif
 
 #if defined(SPLIT_RSS_COUNTING)

commit b53202e6308939d33ba0c78712e850f891b4e76f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Dec 3 10:24:08 2015 -0500

    cgroup: kill cgrp_ss_priv[CGROUP_CANFORK_COUNT] and friends
    
    Now that nobody use the "priv" arg passed to can_fork/cancel_fork/fork we can
    kill CGROUP_CANFORK_COUNT/SUBSYS_TAG/etc and cgrp_ss_priv[] in copy_process().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fce002ee3ddf..ba7d1c037490 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1249,7 +1249,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 {
 	int retval;
 	struct task_struct *p;
-	void *cgrp_ss_priv[CGROUP_CANFORK_COUNT] = {};
 
 	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
 		return ERR_PTR(-EINVAL);
@@ -1526,7 +1525,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * between here and cgroup_post_fork() if an organisation operation is in
 	 * progress.
 	 */
-	retval = cgroup_can_fork(p, cgrp_ss_priv);
+	retval = cgroup_can_fork(p);
 	if (retval)
 		goto bad_fork_free_pid;
 
@@ -1608,7 +1607,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	write_unlock_irq(&tasklist_lock);
 
 	proc_fork_connector(p);
-	cgroup_post_fork(p, cgrp_ss_priv);
+	cgroup_post_fork(p);
 	threadgroup_change_end(current);
 	perf_event_fork(p);
 
@@ -1618,7 +1617,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	return p;
 
 bad_fork_cancel_cgroup:
-	cgroup_cancel_fork(p, cgrp_ss_priv);
+	cgroup_cancel_fork(p);
 bad_fork_free_pid:
 	if (pid != &init_struct_pid)
 		free_pid(pid);

commit c9e75f0492b248aeaa7af8991a6fc9a21506bc96
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Nov 27 19:57:19 2015 +0100

    cgroup: pids: fix race between cgroup_post_fork() and cgroup_migrate()
    
    If the new child migrates to another cgroup before cgroup_post_fork() calls
    subsys->fork(), then both pids_can_attach() and pids_fork() will do the same
    pids_uncharge(old_pids) + pids_charge(pids) sequence twice.
    
    Change copy_process() to call threadgroup_change_begin/threadgroup_change_end
    unconditionally. percpu_down_read() is cheap and this allows other cleanups,
    see the next changes.
    
    Also, this way we can unify cgroup_threadgroup_rwsem and dup_mmap_sem.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Zefan Li <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f97f2c449f5c..fce002ee3ddf 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1368,8 +1368,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->real_start_time = ktime_get_boot_ns();
 	p->io_context = NULL;
 	p->audit_context = NULL;
-	if (clone_flags & CLONE_THREAD)
-		threadgroup_change_begin(current);
+	threadgroup_change_begin(current);
 	cgroup_fork(p);
 #ifdef CONFIG_NUMA
 	p->mempolicy = mpol_dup(p->mempolicy);
@@ -1610,8 +1609,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	proc_fork_connector(p);
 	cgroup_post_fork(p, cgrp_ss_priv);
-	if (clone_flags & CLONE_THREAD)
-		threadgroup_change_end(current);
+	threadgroup_change_end(current);
 	perf_event_fork(p);
 
 	trace_task_newtask(p, clone_flags);
@@ -1652,8 +1650,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	mpol_put(p->mempolicy);
 bad_fork_cleanup_threadgroup_lock:
 #endif
-	if (clone_flags & CLONE_THREAD)
-		threadgroup_change_end(current);
+	threadgroup_change_end(current);
 	delayacct_tsk_free(p);
 bad_fork_cleanup_count:
 	atomic_dec(&p->cred->user->processes);

commit 2e3078af2c67730c479f1d183af5b367f5d95337
Merge: ea5c58e70c3a b3b0d09c7a23
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 5 23:10:54 2015 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge patch-bomb from Andrew Morton:
    
     - inotify tweaks
    
     - some ocfs2 updates (many more are awaiting review)
    
     - various misc bits
    
     - kernel/watchdog.c updates
    
     - Some of mm.  I have a huge number of MM patches this time and quite a
       lot of it is quite difficult and much will be held over to next time.
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (162 commits)
      selftests: vm: add tests for lock on fault
      mm: mlock: add mlock flags to enable VM_LOCKONFAULT usage
      mm: introduce VM_LOCKONFAULT
      mm: mlock: add new mlock system call
      mm: mlock: refactor mlock, munlock, and munlockall code
      kasan: always taint kernel on report
      mm, slub, kasan: enable user tracking by default with KASAN=y
      kasan: use IS_ALIGNED in memory_is_poisoned_8()
      kasan: Fix a type conversion error
      lib: test_kasan: add some testcases
      kasan: update reference to kasan prototype repo
      kasan: move KASAN_SANITIZE in arch/x86/boot/Makefile
      kasan: various fixes in documentation
      kasan: update log messages
      kasan: accurately determine the type of the bad access
      kasan: update reported bug types for kernel memory accesses
      kasan: update reported bug types for not user nor kernel memory accesses
      mm/kasan: prevent deadlock in kasan reporting
      mm/kasan: don't use kasan shadow pointer in generic functions
      mm/kasan: MODULE_VADDR is not available on all archs
      ...

commit de60f5f10c58d4f34b68622442c0e04180367f3f
Author: Eric B Munson <emunson@akamai.com>
Date:   Thu Nov 5 18:51:36 2015 -0800

    mm: introduce VM_LOCKONFAULT
    
    The cost of faulting in all memory to be locked can be very high when
    working with large mappings.  If only portions of the mapping will be used
    this can incur a high penalty for locking.
    
    For the example of a large file, this is the usage pattern for a large
    statical language model (probably applies to other statical or graphical
    models as well).  For the security example, any application transacting in
    data that cannot be swapped out (credit card data, medical records, etc).
    
    This patch introduces the ability to request that pages are not
    pre-faulted, but are placed on the unevictable LRU when they are finally
    faulted in.  The VM_LOCKONFAULT flag will be used together with VM_LOCKED
    and has no effect when set without VM_LOCKED.  Setting the VM_LOCKONFAULT
    flag for a VMA will cause pages faulted into that VMA to be added to the
    unevictable LRU when they are faulted or if they are already present, but
    will not cause any missing pages to be faulted in.
    
    Exposing this new lock state means that we cannot overload the meaning of
    the FOLL_POPULATE flag any longer.  Prior to this patch it was used to
    mean that the VMA for a fault was locked.  This means we need the new
    FOLL_MLOCK flag to communicate the locked state of a VMA.  FOLL_POPULATE
    will now only control if the VMA should be populated and in the case of
    VM_LOCKONFAULT, it will not be set.
    
    Signed-off-by: Eric B Munson <emunson@akamai.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Shuah Khan <shuahkh@osg.samsung.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6ac894244d39..a30fae45b486 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -454,7 +454,8 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		tmp->vm_mm = mm;
 		if (anon_vma_fork(tmp, mpnt))
 			goto fail_nomem_anon_vma_fork;
-		tmp->vm_flags &= ~(VM_LOCKED|VM_UFFD_MISSING|VM_UFFD_WP);
+		tmp->vm_flags &=
+			~(VM_LOCKED|VM_LOCKONFAULT|VM_UFFD_MISSING|VM_UFFD_WP);
 		tmp->vm_next = tmp->vm_prev = NULL;
 		tmp->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
 		file = tmp->vm_file;

commit 69234acee54407962a20bedf90ef9c96326994b5
Merge: 11eaaadb3ea3 d57456753787
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 5 14:51:32 2015 -0800

    Merge branch 'for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "The cgroup core saw several significant updates this cycle:
    
       - percpu_rwsem for threadgroup locking is reinstated.  This was
         temporarily dropped due to down_write latency issues.  Oleg's
         rework of percpu_rwsem which is scheduled to be merged in this
         merge window resolves the issue.
    
       - On the v2 hierarchy, when controllers are enabled and disabled, all
         operations are atomic and can fail and revert cleanly.  This allows
         ->can_attach() failure which is necessary for cpu RT slices.
    
       - Tasks now stay associated with the original cgroups after exit
         until released.  This allows tracking resources held by zombies
         (e.g.  pids) and makes it easy to find out where zombies came from
         on the v2 hierarchy.  The pids controller was broken before these
         changes as zombies escaped the limits; unfortunately, updating this
         behavior required too many invasive changes and I don't think it's
         a good idea to backport them, so the pids controller on 4.3, the
         first version which included the pids controller, will stay broken
         at least until I'm sure about the cgroup core changes.
    
       - Optimization of a couple common tests using static_key"
    
    * 'for-4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (38 commits)
      cgroup: fix race condition around termination check in css_task_iter_next()
      blkcg: don't create "io.stat" on the root cgroup
      cgroup: drop cgroup__DEVEL__legacy_files_on_dfl
      cgroup: replace error handling in cgroup_init() with WARN_ON()s
      cgroup: add cgroup_subsys->free() method and use it to fix pids controller
      cgroup: keep zombies associated with their original cgroups
      cgroup: make css_set_rwsem a spinlock and rename it to css_set_lock
      cgroup: don't hold css_set_rwsem across css task iteration
      cgroup: reorganize css_task_iter functions
      cgroup: factor out css_set_move_task()
      cgroup: keep css_set and task lists in chronological order
      cgroup: make cgroup_destroy_locked() test cgroup_is_populated()
      cgroup: make css_sets pin the associated cgroups
      cgroup: relocate cgroup_[try]get/put()
      cgroup: move check_for_release() invocation
      cgroup: replace cgroup_has_tasks() with cgroup_is_populated()
      cgroup: make cgroup->nr_populated count the number of populated css_sets
      cgroup: remove an unused parameter from cgroup_task_migrate()
      cgroup: fix too early usage of static_branch_disable()
      cgroup: make cgroup_update_dfl_csses() migrate all target processes atomically
      ...

commit 2e91fa7f6d451e3ea9fec999065d2fd199691f9d
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 15 16:41:53 2015 -0400

    cgroup: keep zombies associated with their original cgroups
    
    cgroup_exit() is called when a task exits and disassociates the
    exiting task from its cgroups and half-attach it to the root cgroup.
    This is unnecessary and undesirable.
    
    No controller actually needs an exiting task to be disassociated with
    non-root cgroups.  Both cpu and perf_event controllers update the
    association to the root cgroup from their exit callbacks just to keep
    consistent with the cgroup core behavior.
    
    Also, this disassociation makes it difficult to track resources held
    by zombies or determine where the zombies came from.  Currently, pids
    controller is completely broken as it uncharges on exit and zombies
    always escape the resource restriction.  With cgroup association being
    reset on exit, fixing it is pretty painful.
    
    There's no reason to reset cgroup membership on exit.  The zombie can
    be removed from its css_set so that it doesn't show up on
    "cgroup.procs" and thus can't be migrated or interfere with cgroup
    removal.  It can still pin and point to the css_set so that its cgroup
    membership is maintained.  This patch makes cgroup core keep zombies
    associated with their cgroups at the time of exit.
    
    * Previous patches decoupled populated_cnt tracking from css_set
      lifetime, so a dying task can be simply unlinked from its css_set
      while pinning and pointing to the css_set.  This keeps css_set
      association from task side alive while hiding it from "cgroup.procs"
      and populated_cnt tracking.  The css_set reference is dropped when
      the task_struct is freed.
    
    * ->exit() callback no longer needs the css arguments as the
      associated css never changes once PF_EXITING is set.  Removed.
    
    * cpu and perf_events controllers no longer need ->exit() callbacks.
      There's no reason to explicitly switch away on exit.  The final
      schedule out is enough.  The callbacks are removed.
    
    * On traditional hierarchies, nothing changes.  "/proc/PID/cgroup"
      still reports "/" for all zombies.  On the default hierarchy,
      "/proc/PID/cgroup" keeps reporting the cgroup that the task belonged
      to at the time of exit.  If the cgroup gets removed before the task
      is reaped, " (deleted)" is appended.
    
    v2: Build brekage due to missing dummy cgroup_free() when
        !CONFIG_CGROUP fixed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7d5f0f118a63..118743bb5964 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -251,6 +251,7 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
+	cgroup_free(tsk);
 	task_numa_free(tsk);
 	security_task_free(tsk);
 	exit_creds(tsk);

commit d5c373eb5610686162ff50429f63f4c00c554799
Author: Jason Low <jason.low2@hp.com>
Date:   Wed Oct 14 12:07:55 2015 -0700

    posix_cpu_timer: Convert cputimer->running to bool
    
    In the next patch in this series, a new field 'checking_timer' will
    be added to 'struct thread_group_cputimer'. Both this and the
    existing 'running' integer field are just used as boolean values. To
    save space in the structure, we can make both of these fields booleans.
    
    This is a preparatory patch to convert the existing running integer
    field to a boolean.
    
    Suggested-by: George Spelvin <linux@horizon.com>
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Reviewed: George Spelvin <linux@horizon.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: hideaki.kimura@hpe.com
    Cc: terry.rudd@hpe.com
    Cc: scott.norton@hpe.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1444849677-29330-4-git-send-email-jason.low2@hp.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2845623fb582..6ac894244d39 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1101,7 +1101,7 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 	cpu_limit = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
 	if (cpu_limit != RLIM_INFINITY) {
 		sig->cputime_expires.prof_exp = secs_to_cputime(cpu_limit);
-		sig->cputimer.running = 1;
+		sig->cputimer.running = true;
 	}
 
 	/* The timer lists. */

commit 1ed1328792ff46e4bb86a3d7f7be2971f4549f6c
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Sep 16 12:53:17 2015 -0400

    sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
    
    Note: This commit was originally committed as d59cfc09c32a but got
          reverted by 0c986253b939 due to the performance regression from
          the percpu_rwsem write down/up operations added to cgroup task
          migration path.  percpu_rwsem changes which alleviate the
          performance issue are pending for v4.4-rc1 merge window.
          Re-apply.
    
    The cgroup side of threadgroup locking uses signal_struct->group_rwsem
    to synchronize against threadgroup changes.  This per-process rwsem
    adds small overhead to thread creation, exit and exec paths, forces
    cgroup code paths to do lock-verify-unlock-retry dance in a couple
    places and makes it impossible to atomically perform operations across
    multiple processes.
    
    This patch replaces signal_struct->group_rwsem with a global
    percpu_rwsem cgroup_threadgroup_rwsem which is cheaper on the reader
    side and contained in cgroups proper.  This patch converts one-to-one.
    
    This does make writer side heavier and lower the granularity; however,
    cgroup process migration is a fairly cold path, we do want to optimize
    thread operations over it and cgroup migration operations don't take
    enough time for the lower granularity to matter.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/g/55F8097A.7000206@de.ibm.com
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2845623fb582..7d5f0f118a63 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1149,10 +1149,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	tty_audit_fork(sig);
 	sched_autogroup_fork(sig);
 
-#ifdef CONFIG_CGROUPS
-	init_rwsem(&sig->group_rwsem);
-#endif
-
 	sig->oom_score_adj = current->signal->oom_score_adj;
 	sig->oom_score_adj_min = current->signal->oom_score_adj_min;
 

commit 0c986253b939cc14c69d4adbe2b4121bdf4aa220
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Sep 16 11:51:12 2015 -0400

    Revert "sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem"
    
    This reverts commit d59cfc09c32a2ae31f1c3bc2983a0cd79afb3f14.
    
    d59cfc09c32a ("sched, cgroup: replace signal_struct->group_rwsem with
    a global percpu_rwsem") and b5ba75b5fc0e ("cgroup: simplify
    threadgroup locking") changed how cgroup synchronizes against task
    fork and exits so that it uses global percpu_rwsem instead of
    per-process rwsem; unfortunately, the write [un]lock paths of
    percpu_rwsem always involve synchronize_rcu_expedited() which turned
    out to be too expensive.
    
    Improvements for percpu_rwsem are scheduled to be merged in the coming
    v4.4-rc1 merge window which alleviates this issue.  For now, revert
    the two commits to restore per-process rwsem.  They will be re-applied
    for the v4.4-rc1 merge window.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Link: http://lkml.kernel.org/g/55F8097A.7000206@de.ibm.com
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: stable@vger.kernel.org # v4.2+

diff --git a/kernel/fork.c b/kernel/fork.c
index 7d5f0f118a63..2845623fb582 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1149,6 +1149,10 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	tty_audit_fork(sig);
 	sched_autogroup_fork(sig);
 
+#ifdef CONFIG_CGROUPS
+	init_rwsem(&sig->group_rwsem);
+#endif
+
 	sig->oom_score_adj = current->signal->oom_score_adj;
 	sig->oom_score_adj_min = current->signal->oom_score_adj_min;
 

commit 16ba6f811dfe44bc14f7946a4b257b85476fc16e
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Fri Sep 4 15:46:17 2015 -0700

    userfaultfd: add VM_UFFD_MISSING and VM_UFFD_WP
    
    These two flags gets set in vma->vm_flags to tell the VM common code
    if the userfaultfd is armed and in which mode (only tracking missing
    faults, only tracking wrprotect faults or both). If neither flags is
    set it means the userfaultfd is not armed on the vma.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Pavel Emelyanov <xemul@parallels.com>
    Cc: Sanidhya Kashyap <sanidhya.gatech@gmail.com>
    Cc: zhang.zhanghailiang@huawei.com
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Andres Lagar-Cavilla <andreslc@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Peter Feiner <pfeiner@google.com>
    Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "Huangpeng (Peter)" <peter.huangpeng@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ceb4eb4abb9d..7d5f0f118a63 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -454,7 +454,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		tmp->vm_mm = mm;
 		if (anon_vma_fork(tmp, mpnt))
 			goto fail_nomem_anon_vma_fork;
-		tmp->vm_flags &= ~VM_LOCKED;
+		tmp->vm_flags &= ~(VM_LOCKED|VM_UFFD_MISSING|VM_UFFD_WP);
 		tmp->vm_next = tmp->vm_prev = NULL;
 		tmp->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
 		file = tmp->vm_file;

commit 745f234be12b6191b15eae8dd415cc81a9137f47
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Fri Sep 4 15:46:14 2015 -0700

    userfaultfd: add vm_userfaultfd_ctx to the vm_area_struct
    
    This adds the vm_userfaultfd_ctx to the vm_area_struct.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Pavel Emelyanov <xemul@parallels.com>
    Cc: Sanidhya Kashyap <sanidhya.gatech@gmail.com>
    Cc: zhang.zhanghailiang@huawei.com
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Andres Lagar-Cavilla <andreslc@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Peter Feiner <pfeiner@google.com>
    Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "Huangpeng (Peter)" <peter.huangpeng@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 03aa2e6de7a4..ceb4eb4abb9d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -456,6 +456,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			goto fail_nomem_anon_vma_fork;
 		tmp->vm_flags &= ~VM_LOCKED;
 		tmp->vm_next = tmp->vm_prev = NULL;
+		tmp->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
 		file = tmp->vm_file;
 		if (file) {
 			struct inode *inode = file_inode(file);

commit 8bdc69b764013a9b5ebeef7df8f314f1066c5d79
Merge: 76ec51ef5edf 20f1f4b5ffb8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 2 08:04:23 2015 -0700

    Merge branch 'for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - a new PIDs controller is added.  It turns out that PIDs are actually
       an independent resource from kmem due to the limited PID space.
    
     - more core preparations for the v2 interface.  Once cpu side interface
       is settled, it should be ready for lifting the devel mask.
       for-4.3-unified-base was temporarily branched so that other trees
       (block) can pull cgroup core changes that blkcg changes depend on.
    
     - a non-critical idr_preload usage bug fix.
    
    * 'for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: pids: fix invalid get/put usage
      cgroup: introduce cgroup_subsys->legacy_name
      cgroup: don't print subsystems for the default hierarchy
      cgroup: make cftype->private a unsigned long
      cgroup: export cgrp_dfl_root
      cgroup: define controller file conventions
      cgroup: fix idr_preload usage
      cgroup: add documentation for the PIDs controller
      cgroup: implement the PIDs subsystem
      cgroup: allow a cgroup subsystem to reject a fork

commit 73b6fa8e49c2d13e04d20186261e5f7855c6d0bf
Merge: e713c80a4e49 4b75de861505
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 1 16:13:25 2015 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull user namespace updates from Eric Biederman:
     "This finishes up the changes to ensure proc and sysfs do not start
      implementing executable files, as the there are application today that
      are only secure because such files do not exist.
    
      It akso fixes a long standing misfeature of /proc/<pid>/mountinfo that
      did not show the proper source for files bind mounted from
      /proc/<pid>/ns/*.
    
      It also straightens out the handling of clone flags related to user
      namespaces, fixing an unnecessary failure of unshare(CLONE_NEWUSER)
      when files such as /proc/<pid>/environ are read while <pid> is calling
      unshare.  This winds up fixing a minor bug in unshare flag handling
      that dates back to the first version of unshare in the kernel.
    
      Finally, this fixes a minor regression caused by the introduction of
      sysfs_create_mount_point, which broke someone's in house application,
      by restoring the size of /sys/fs/cgroup to 0 bytes.  Apparently that
      application uses the directory size to determine if a tmpfs is mounted
      on /sys/fs/cgroup.
    
      The bind mount escape fixes are present in Al Viros for-next branch.
      and I expect them to come from there.  The bind mount escape is the
      last of the user namespace related security bugs that I am aware of"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      fs: Set the size of empty dirs to 0.
      userns,pidns: Force thread group sharing, not signal handler sharing.
      unshare: Unsharing a thread does not require unsharing a vm
      nsfs: Add a show_path method to fix mountinfo
      mnt: fs_fully_visible enforce noexec and nosuid  if !SB_I_NOEXEC
      vfs: Commit to never having exectuables on proc and sysfs.

commit a1d8561172f369ba56d636df49a6b4d6d77e2123
Merge: 3959df1dfb95 ff277d4250fe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 31 20:26:22 2015 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The biggest change in this cycle is the rewrite of the main SMP load
      balancing metric: the CPU load/utilization.  The main goal was to make
      the metric more precise and more representative - see the changelog of
      this commit for the gory details:
    
        9d89c257dfb9 ("sched/fair: Rewrite runnable load and utilization average tracking")
    
      It is done in a way that significantly reduces complexity of the code:
    
        5 files changed, 249 insertions(+), 494 deletions(-)
    
      and the performance testing results are encouraging.  Nevertheless we
      need to keep an eye on potential regressions, since this potentially
      affects every SMP workload in existence.
    
      This work comes from Yuyang Du.
    
      Other changes:
    
       - SCHED_DL updates.  (Andrea Parri)
    
       - Simplify architecture callbacks by removing finish_arch_switch().
         (Peter Zijlstra et al)
    
       - cputime accounting: guarantee stime + utime == rtime.  (Peter
         Zijlstra)
    
       - optimize idle CPU wakeups some more - inspired by Facebook server
         loads.  (Mike Galbraith)
    
       - stop_machine fixes and updates.  (Oleg Nesterov)
    
       - Introduce the 'trace_sched_waking' tracepoint.  (Peter Zijlstra)
    
       - sched/numa tweaks.  (Srikar Dronamraju)
    
       - misc fixes and small cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      sched/deadline: Fix comment in enqueue_task_dl()
      sched/deadline: Fix comment in push_dl_tasks()
      sched: Change the sched_class::set_cpus_allowed() calling context
      sched: Make sched_class::set_cpus_allowed() unconditional
      sched: Fix a race between __kthread_bind() and sched_setaffinity()
      sched: Ensure a task has a non-normalized vruntime when returning back to CFS
      sched/numa: Fix NUMA_DIRECT topology identification
      tile: Reorganize _switch_to()
      sched, sparc32: Update scheduler comments in copy_thread()
      sched: Remove finish_arch_switch()
      sched, tile: Remove finish_arch_switch
      sched, sh: Fold finish_arch_switch() into switch_to()
      sched, score: Remove finish_arch_switch()
      sched, avr32: Remove finish_arch_switch()
      sched, MIPS: Get rid of finish_arch_switch()
      sched, arm: Remove finish_arch_switch()
      sched/fair: Clean up load average references
      sched/fair: Provide runnable_load_avg back to cfs_rq
      sched/fair: Remove task and group entity load when they are dead
      sched/fair: Init cfs_rq's sched_entity load average
      ...

commit faf00da544045fdc1454f3b9e6d7f65c841de302
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Aug 10 18:25:44 2015 -0500

    userns,pidns: Force thread group sharing, not signal handler sharing.
    
    The code that places signals in signal queues computes the uids, gids,
    and pids at the time the signals are enqueued.  Which means that tasks
    that share signal queues must be in the same pid and user namespaces.
    
    Sharing signal handlers is fine, but bizarre.
    
    So make the code in fork and userns_install clearer by only testing
    for what is functionally necessary.
    
    Also update the comment in unshare about unsharing a user namespace to
    be a little more explicit and make a little more sense.
    
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index d544ae97f999..2c72b8a8ae24 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1273,10 +1273,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	/*
 	 * If the new process will be in a different pid or user namespace
-	 * do not allow it to share a thread group or signal handlers or
-	 * parent with the forking task.
+	 * do not allow it to share a thread group with the forking task.
 	 */
-	if (clone_flags & CLONE_SIGHAND) {
+	if (clone_flags & CLONE_THREAD) {
 		if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
 		    (task_active_pid_ns(current) !=
 				current->nsproxy->pid_ns_for_children))
@@ -1944,7 +1943,8 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 	int err;
 
 	/*
-	 * If unsharing a user namespace must also unshare the thread.
+	 * If unsharing a user namespace must also unshare the thread group
+	 * and unshare the filesystem root and working directories.
 	 */
 	if (unshare_flags & CLONE_NEWUSER)
 		unshare_flags |= CLONE_THREAD | CLONE_FS;

commit 12c641ab8270f787dfcce08b5f20ce8b65008096
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Aug 10 17:35:07 2015 -0500

    unshare: Unsharing a thread does not require unsharing a vm
    
    In the logic in the initial commit of unshare made creating a new
    thread group for a process, contingent upon creating a new memory
    address space for that process.  That is wrong.  Two separate
    processes in different thread groups can share a memory address space
    and clone allows creation of such proceses.
    
    This is significant because it was observed that mm_users > 1 does not
    mean that a process is multi-threaded, as reading /proc/PID/maps
    temporarily increments mm_users, which allows other processes to
    (accidentally) interfere with unshare() calls.
    
    Correct the check in check_unshare_flags() to test for
    !thread_group_empty() for CLONE_THREAD, CLONE_SIGHAND, and CLONE_VM.
    For sighand->count > 1 for CLONE_SIGHAND and CLONE_VM.
    For !current_is_single_threaded instead of mm_users > 1 for CLONE_VM.
    
    By using the correct checks in unshare this removes the possibility of
    an accidental denial of service attack.
    
    Additionally using the correct checks in unshare ensures that only an
    explicit unshare(CLONE_VM) can possibly trigger the slow path of
    current_is_single_threaded().  As an explict unshare(CLONE_VM) is
    pointless it is not expected there are many applications that make
    that call.
    
    Cc: stable@vger.kernel.org
    Fixes: b2e0d98705e60e45bbb3c0032c48824ad7ae0704 userns: Implement unshare of the user namespace
    Reported-by: Ricky Zhou <rickyz@chromium.org>
    Reported-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1bfefc6f96a4..d544ae97f999 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1866,13 +1866,21 @@ static int check_unshare_flags(unsigned long unshare_flags)
 				CLONE_NEWUSER|CLONE_NEWPID))
 		return -EINVAL;
 	/*
-	 * Not implemented, but pretend it works if there is nothing to
-	 * unshare. Note that unsharing CLONE_THREAD or CLONE_SIGHAND
-	 * needs to unshare vm.
+	 * Not implemented, but pretend it works if there is nothing
+	 * to unshare.  Note that unsharing the address space or the
+	 * signal handlers also need to unshare the signal queues (aka
+	 * CLONE_THREAD).
 	 */
 	if (unshare_flags & (CLONE_THREAD | CLONE_SIGHAND | CLONE_VM)) {
-		/* FIXME: get_task_mm() increments ->mm_users */
-		if (atomic_read(&current->mm->mm_users) > 1)
+		if (!thread_group_empty(current))
+			return -EINVAL;
+	}
+	if (unshare_flags & (CLONE_SIGHAND | CLONE_VM)) {
+		if (atomic_read(&current->sighand->count) > 1)
+			return -EINVAL;
+	}
+	if (unshare_flags & CLONE_VM) {
+		if (!current_is_single_threaded())
 			return -EINVAL;
 	}
 
@@ -1940,16 +1948,16 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 	 */
 	if (unshare_flags & CLONE_NEWUSER)
 		unshare_flags |= CLONE_THREAD | CLONE_FS;
-	/*
-	 * If unsharing a thread from a thread group, must also unshare vm.
-	 */
-	if (unshare_flags & CLONE_THREAD)
-		unshare_flags |= CLONE_VM;
 	/*
 	 * If unsharing vm, must also unshare signal handlers.
 	 */
 	if (unshare_flags & CLONE_VM)
 		unshare_flags |= CLONE_SIGHAND;
+	/*
+	 * If unsharing a signal handlers, must also unshare the signal queues.
+	 */
+	if (unshare_flags & CLONE_SIGHAND)
+		unshare_flags |= CLONE_THREAD;
 	/*
 	 * If unsharing namespace, must also unshare filesystem information.
 	 */

commit 9d7fb04276481c59610983362d8e023d262b58ca
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Jun 30 11:30:54 2015 +0200

    sched/cputime: Guarantee stime + utime == rtime
    
    While the current code guarantees monotonicity for stime and utime
    independently of one another, it does not guarantee that the sum of
    both is equal to the total time we started out with.
    
    This confuses things (and peoples) who look at this sum, like top, and
    will report >100% usage followed by a matching period of 0%.
    
    Rework the code to provide both individual monotonicity and a coherent
    sum.
    
    Suggested-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
    Reported-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
    Tested-by: Fredrik Markstrom <fredrik.markstrom@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: jason.low2@hp.com
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1bfefc6f96a4..6e8f807c5716 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1067,6 +1067,7 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 	rcu_assign_pointer(tsk->sighand, sig);
 	if (!sig)
 		return -ENOMEM;
+
 	atomic_set(&sig->count, 1);
 	memcpy(sig->action, current->sighand->action, sizeof(sig->action));
 	return 0;
@@ -1128,6 +1129,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	init_sigpending(&sig->shared_pending);
 	INIT_LIST_HEAD(&sig->posix_timers);
 	seqlock_init(&sig->stats_lock);
+	prev_cputime_init(&sig->prev_cputime);
 
 	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	sig->real_timer.function = it_real_fn;
@@ -1335,9 +1337,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->utime = p->stime = p->gtime = 0;
 	p->utimescaled = p->stimescaled = 0;
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-	p->prev_cputime.utime = p->prev_cputime.stime = 0;
-#endif
+	prev_cputime_init(&p->prev_cputime);
+
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 	seqlock_init(&p->vtime_seqlock);
 	p->vtime_snap = 0;

commit 5aaeb5c01c5b6c0be7b7aadbf3ace9f3a4458c3d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Jul 17 12:28:12 2015 +0200

    x86/fpu, sched: Introduce CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT and use it on x86
    
    Don't burden architectures without dynamic task_struct sizing
    with the overhead of dynamic sizing.
    
    Also optimize the x86 code a bit by caching task_struct_size.
    
    Acked-and-Tested-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437128892-9831-3-git-send-email-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 431b67a6098c..dbd9b8d7b7cc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -287,21 +287,20 @@ static void set_max_threads(unsigned int max_threads_suggested)
 	max_threads = clamp_t(u64, threads, MIN_THREADS, MAX_THREADS);
 }
 
-int __weak arch_task_struct_size(void)
-{
-	return sizeof(struct task_struct);
-}
+#ifdef CONFIG_ARCH_WANTS_DYNAMIC_TASK_STRUCT
+/* Initialized by the architecture: */
+int arch_task_struct_size __read_mostly;
+#endif
 
 void __init fork_init(void)
 {
-	int task_struct_size = arch_task_struct_size();
 #ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 #ifndef ARCH_MIN_TASKALIGN
 #define ARCH_MIN_TASKALIGN	L1_CACHE_BYTES
 #endif
 	/* create a slab on which task_structs can be allocated */
 	task_struct_cachep =
-		kmem_cache_create("task_struct", task_struct_size,
+		kmem_cache_create("task_struct", arch_task_struct_size,
 			ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
 #endif
 

commit 0c8c0f03e3a292e031596484275c14cf39c0ab7a
Author: Dave Hansen <dave@sr71.net>
Date:   Fri Jul 17 12:28:11 2015 +0200

    x86/fpu, sched: Dynamically allocate 'struct fpu'
    
    The FPU rewrite removed the dynamic allocations of 'struct fpu'.
    But, this potentially wastes massive amounts of memory (2k per
    task on systems that do not have AVX-512 for instance).
    
    Instead of having a separate slab, this patch just appends the
    space that we need to the 'task_struct' which we dynamically
    allocate already.  This saves from doing an extra slab
    allocation at fork().
    
    The only real downside here is that we have to stick everything
    and the end of the task_struct.  But, I think the
    BUILD_BUG_ON()s I stuck in there should keep that from being too
    fragile.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1437128892-9831-2-git-send-email-mingo@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1bfefc6f96a4..431b67a6098c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -287,15 +287,21 @@ static void set_max_threads(unsigned int max_threads_suggested)
 	max_threads = clamp_t(u64, threads, MIN_THREADS, MAX_THREADS);
 }
 
+int __weak arch_task_struct_size(void)
+{
+	return sizeof(struct task_struct);
+}
+
 void __init fork_init(void)
 {
+	int task_struct_size = arch_task_struct_size();
 #ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 #ifndef ARCH_MIN_TASKALIGN
 #define ARCH_MIN_TASKALIGN	L1_CACHE_BYTES
 #endif
 	/* create a slab on which task_structs can be allocated */
 	task_struct_cachep =
-		kmem_cache_create("task_struct", sizeof(struct task_struct),
+		kmem_cache_create("task_struct", task_struct_size,
 			ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
 #endif
 

commit 7e47682ea555e7c1edef1d8fd96e2aa4c12abe59
Author: Aleksa Sarai <cyphar@cyphar.com>
Date:   Tue Jun 9 21:32:09 2015 +1000

    cgroup: allow a cgroup subsystem to reject a fork
    
    Add a new cgroup subsystem callback can_fork that conditionally
    states whether or not the fork is accepted or rejected by a cgroup
    policy. In addition, add a cancel_fork callback so that if an error
    occurs later in the forking process, any state modified by can_fork can
    be reverted.
    
    Allow for a private opaque pointer to be passed from cgroup_can_fork to
    cgroup_post_fork, allowing for the fork state to be stored by each
    subsystem separately.
    
    Also add a tagging system for cgroup_subsys.h to allow for CGROUP_<TAG>
    enumerations to be be defined and used. In addition, explicitly add a
    CGROUP_CANFORK_COUNT macro to make arrays easier to define.
    
    This is in preparation for implementing the pids cgroup subsystem.
    
    Signed-off-by: Aleksa Sarai <cyphar@cyphar.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1bfefc6f96a4..40e3af12c55e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1239,6 +1239,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 {
 	int retval;
 	struct task_struct *p;
+	void *cgrp_ss_priv[CGROUP_CANFORK_COUNT] = {};
 
 	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
 		return ERR_PTR(-EINVAL);
@@ -1512,6 +1513,16 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	INIT_LIST_HEAD(&p->thread_group);
 	p->task_works = NULL;
 
+	/*
+	 * Ensure that the cgroup subsystem policies allow the new process to be
+	 * forked. It should be noted the the new process's css_set can be changed
+	 * between here and cgroup_post_fork() if an organisation operation is in
+	 * progress.
+	 */
+	retval = cgroup_can_fork(p, cgrp_ss_priv);
+	if (retval)
+		goto bad_fork_free_pid;
+
 	/*
 	 * Make it visible to the rest of the system, but dont wake it up yet.
 	 * Need tasklist lock for parent etc handling!
@@ -1548,7 +1559,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		spin_unlock(&current->sighand->siglock);
 		write_unlock_irq(&tasklist_lock);
 		retval = -ERESTARTNOINTR;
-		goto bad_fork_free_pid;
+		goto bad_fork_cancel_cgroup;
 	}
 
 	if (likely(p->pid)) {
@@ -1590,7 +1601,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	write_unlock_irq(&tasklist_lock);
 
 	proc_fork_connector(p);
-	cgroup_post_fork(p);
+	cgroup_post_fork(p, cgrp_ss_priv);
 	if (clone_flags & CLONE_THREAD)
 		threadgroup_change_end(current);
 	perf_event_fork(p);
@@ -1600,6 +1611,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	return p;
 
+bad_fork_cancel_cgroup:
+	cgroup_cancel_fork(p, cgrp_ss_priv);
 bad_fork_free_pid:
 	if (pid != &init_struct_pid)
 		free_pid(pid);

commit bbe179f88d39274630823a0dc07d2714fd19a103
Merge: 4b703b1d4c46 8a0792ef8e01
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 26 19:50:04 2015 -0700

    Merge branch 'for-4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - threadgroup_lock got reorganized so that its users can pick the
       actual locking mechanism to use.  Its only user - cgroups - is
       updated to use a percpu_rwsem instead of per-process rwsem.
    
       This makes things a bit lighter on hot paths and allows cgroups to
       perform and fail multi-task (a process) migrations atomically.
       Multi-task migrations are used in several places including the
       unified hierarchy.
    
     - Delegation rule and documentation added to unified hierarchy.  This
       will likely be the last interface update from the cgroup core side
       for unified hierarchy before lifting the devel mask.
    
     - Some groundwork for the pids controller which is scheduled to be
       merged in the coming devel cycle.
    
    * 'for-4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: add delegation section to unified hierarchy documentation
      cgroup: require write perm on common ancestor when moving processes on the default hierarchy
      cgroup: separate out cgroup_procs_write_permission() from __cgroup_procs_write()
      kernfs: make kernfs_get_inode() public
      MAINTAINERS: add a cgroup core co-maintainer
      cgroup: fix uninitialised iterator in for_each_subsys_which
      cgroup: replace explicit ss_mask checking with for_each_subsys_which
      cgroup: use bitmask to filter for_each_subsys
      cgroup: add seq_file forward declaration for struct cftype
      cgroup: simplify threadgroup locking
      sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
      sched, cgroup: reorganize threadgroup locking
      cgroup: switch to unsigned long for bitmasks
      cgroup: reorganize include/linux/cgroup.h
      cgroup: separate out include/linux/cgroup-defs.h
      cgroup: fix some comment typos

commit 3033f14ab78c326871a4902591c2518410add24a
Author: Josh Triplett <josh@joshtriplett.org>
Date:   Thu Jun 25 15:01:19 2015 -0700

    clone: support passing tls argument via C rather than pt_regs magic
    
    clone has some of the quirkiest syscall handling in the kernel, with a
    pile of special cases, historical curiosities, and architecture-specific
    calling conventions.  In particular, clone with CLONE_SETTLS accepts a
    parameter "tls" that the C entry point completely ignores and some
    assembly entry points overwrite; instead, the low-level arch-specific
    code pulls the tls parameter out of the arch-specific register captured
    as part of pt_regs on entry to the kernel.  That's a massive hack, and
    it makes the arch-specific code only work when called via the specific
    existing syscall entry points; because of this hack, any new clone-like
    system call would have to accept an identical tls argument in exactly
    the same arch-specific position, rather than providing a unified system
    call entry point across architectures.
    
    The first patch allows architectures to handle the tls argument via
    normal C parameter passing, if they opt in by selecting
    HAVE_COPY_THREAD_TLS.  The second patch makes 32-bit and 64-bit x86 opt
    into this.
    
    These two patches came out of the clone4 series, which isn't ready for
    this merge window, but these first two cleanup patches were entirely
    uncontroversial and have acks.  I'd like to go ahead and submit these
    two so that other architectures can begin building on top of this and
    opting into HAVE_COPY_THREAD_TLS.  However, I'm also happy to wait and
    send these through the next merge window (along with v3 of clone4) if
    anyone would prefer that.
    
    This patch (of 2):
    
    clone with CLONE_SETTLS accepts an argument to set the thread-local
    storage area for the new thread.  sys_clone declares an int argument
    tls_val in the appropriate point in the argument list (based on the
    various CLONE_BACKWARDS variants), but doesn't actually use or pass along
    that argument.  Instead, sys_clone calls do_fork, which calls
    copy_process, which calls the arch-specific copy_thread, and copy_thread
    pulls the corresponding syscall argument out of the pt_regs captured at
    kernel entry (knowing what argument of clone that architecture passes tls
    in).
    
    Apart from being awful and inscrutable, that also only works because only
    one code path into copy_thread can pass the CLONE_SETTLS flag, and that
    code path comes from sys_clone with its architecture-specific
    argument-passing order.  This prevents introducing a new version of the
    clone system call without propagating the same architecture-specific
    position of the tls argument.
    
    However, there's no reason to pull the argument out of pt_regs when
    sys_clone could just pass it down via C function call arguments.
    
    Introduce a new CONFIG_HAVE_COPY_THREAD_TLS for architectures to opt into,
    and a new copy_thread_tls that accepts the tls parameter as an additional
    unsigned long (syscall-argument-sized) argument.  Change sys_clone's tls
    argument to an unsigned long (which does not change the ABI), and pass
    that down to copy_thread_tls.
    
    Architectures that don't opt into copy_thread_tls will continue to ignore
    the C argument to sys_clone in favor of the pt_regs captured at kernel
    entry, and thus will be unable to introduce new versions of the clone
    syscall.
    
    Patch co-authored by Josh Triplett and Thiago Macieira.
    
    Signed-off-by: Josh Triplett <josh@joshtriplett.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thiago Macieira <thiago.macieira@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0bb88b555550..4c95cb34243c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1238,7 +1238,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 					unsigned long stack_size,
 					int __user *child_tidptr,
 					struct pid *pid,
-					int trace)
+					int trace,
+					unsigned long tls)
 {
 	int retval;
 	struct task_struct *p;
@@ -1447,7 +1448,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	retval = copy_io(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_namespaces;
-	retval = copy_thread(clone_flags, stack_start, stack_size, p);
+	retval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);
 	if (retval)
 		goto bad_fork_cleanup_io;
 
@@ -1659,7 +1660,7 @@ static inline void init_idle_pids(struct pid_link *links)
 struct task_struct *fork_idle(int cpu)
 {
 	struct task_struct *task;
-	task = copy_process(CLONE_VM, 0, 0, NULL, &init_struct_pid, 0);
+	task = copy_process(CLONE_VM, 0, 0, NULL, &init_struct_pid, 0, 0);
 	if (!IS_ERR(task)) {
 		init_idle_pids(task->pids);
 		init_idle(task, cpu);
@@ -1674,11 +1675,12 @@ struct task_struct *fork_idle(int cpu)
  * It copies the process, and if successful kick-starts
  * it and waits for it to finish using the VM if required.
  */
-long do_fork(unsigned long clone_flags,
+long _do_fork(unsigned long clone_flags,
 	      unsigned long stack_start,
 	      unsigned long stack_size,
 	      int __user *parent_tidptr,
-	      int __user *child_tidptr)
+	      int __user *child_tidptr,
+	      unsigned long tls)
 {
 	struct task_struct *p;
 	int trace = 0;
@@ -1703,7 +1705,7 @@ long do_fork(unsigned long clone_flags,
 	}
 
 	p = copy_process(clone_flags, stack_start, stack_size,
-			 child_tidptr, NULL, trace);
+			 child_tidptr, NULL, trace, tls);
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.
@@ -1744,20 +1746,34 @@ long do_fork(unsigned long clone_flags,
 	return nr;
 }
 
+#ifndef CONFIG_HAVE_COPY_THREAD_TLS
+/* For compatibility with architectures that call do_fork directly rather than
+ * using the syscall entry points below. */
+long do_fork(unsigned long clone_flags,
+	      unsigned long stack_start,
+	      unsigned long stack_size,
+	      int __user *parent_tidptr,
+	      int __user *child_tidptr)
+{
+	return _do_fork(clone_flags, stack_start, stack_size,
+			parent_tidptr, child_tidptr, 0);
+}
+#endif
+
 /*
  * Create a kernel thread.
  */
 pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 {
-	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn,
-		(unsigned long)arg, NULL, NULL);
+	return _do_fork(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn,
+		(unsigned long)arg, NULL, NULL, 0);
 }
 
 #ifdef __ARCH_WANT_SYS_FORK
 SYSCALL_DEFINE0(fork)
 {
 #ifdef CONFIG_MMU
-	return do_fork(SIGCHLD, 0, 0, NULL, NULL);
+	return _do_fork(SIGCHLD, 0, 0, NULL, NULL, 0);
 #else
 	/* can not support in nommu mode */
 	return -EINVAL;
@@ -1768,8 +1784,8 @@ SYSCALL_DEFINE0(fork)
 #ifdef __ARCH_WANT_SYS_VFORK
 SYSCALL_DEFINE0(vfork)
 {
-	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0,
-			0, NULL, NULL);
+	return _do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0,
+			0, NULL, NULL, 0);
 }
 #endif
 
@@ -1777,27 +1793,27 @@ SYSCALL_DEFINE0(vfork)
 #ifdef CONFIG_CLONE_BACKWARDS
 SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 		 int __user *, parent_tidptr,
-		 int, tls_val,
+		 unsigned long, tls,
 		 int __user *, child_tidptr)
 #elif defined(CONFIG_CLONE_BACKWARDS2)
 SYSCALL_DEFINE5(clone, unsigned long, newsp, unsigned long, clone_flags,
 		 int __user *, parent_tidptr,
 		 int __user *, child_tidptr,
-		 int, tls_val)
+		 unsigned long, tls)
 #elif defined(CONFIG_CLONE_BACKWARDS3)
 SYSCALL_DEFINE6(clone, unsigned long, clone_flags, unsigned long, newsp,
 		int, stack_size,
 		int __user *, parent_tidptr,
 		int __user *, child_tidptr,
-		int, tls_val)
+		unsigned long, tls)
 #else
 SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 		 int __user *, parent_tidptr,
 		 int __user *, child_tidptr,
-		 int, tls_val)
+		 unsigned long, tls)
 #endif
 {
-	return do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr);
+	return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls);
 }
 #endif
 

commit d59cfc09c32a2ae31f1c3bc2983a0cd79afb3f14
Author: Tejun Heo <tj@kernel.org>
Date:   Wed May 13 16:35:17 2015 -0400

    sched, cgroup: replace signal_struct->group_rwsem with a global percpu_rwsem
    
    The cgroup side of threadgroup locking uses signal_struct->group_rwsem
    to synchronize against threadgroup changes.  This per-process rwsem
    adds small overhead to thread creation, exit and exec paths, forces
    cgroup code paths to do lock-verify-unlock-retry dance in a couple
    places and makes it impossible to atomically perform operations across
    multiple processes.
    
    This patch replaces signal_struct->group_rwsem with a global
    percpu_rwsem cgroup_threadgroup_rwsem which is cheaper on the reader
    side and contained in cgroups proper.  This patch converts one-to-one.
    
    This does make writer side heavier and lower the granularity; however,
    cgroup process migration is a fairly cold path, we do want to optimize
    thread operations over it and cgroup migration operations don't take
    enough time for the lower granularity to matter.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 03c1eaaa6ef5..9531275e12a9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1144,10 +1144,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	tty_audit_fork(sig);
 	sched_autogroup_fork(sig);
 
-#ifdef CONFIG_CGROUPS
-	init_rwsem(&sig->group_rwsem);
-#endif
-
 	sig->oom_score_adj = current->signal->oom_score_adj;
 	sig->oom_score_adj_min = current->signal->oom_score_adj_min;
 

commit 8bcbde5480f9777f8b74d71493722c663e22c21b
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon May 11 17:52:06 2015 +0200

    sched/preempt, mm/fault: Count pagefault_disable() levels in pagefault_disabled
    
    Until now, pagefault_disable()/pagefault_enabled() used the preempt
    count to track whether in an environment with pagefaults disabled (can
    be queried via in_atomic()).
    
    This patch introduces a separate counter in task_struct to count the
    level of pagefault_disable() calls. We'll keep manipulating the preempt
    count to retain compatibility to existing pagefault handlers.
    
    It is now possible to verify whether in a pagefault_disable() envionment
    by calling pagefault_disabled(). In contrast to in_atomic() it will not
    be influenced by preempt_enable()/preempt_disable().
    
    This patch is based on a patch from Ingo Molnar.
    
    Reviewed-and-tested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: David.Laight@ACULAB.COM
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: airlied@linux.ie
    Cc: akpm@linux-foundation.org
    Cc: benh@kernel.crashing.org
    Cc: bigeasy@linutronix.de
    Cc: borntraeger@de.ibm.com
    Cc: daniel.vetter@intel.com
    Cc: heiko.carstens@de.ibm.com
    Cc: herbert@gondor.apana.org.au
    Cc: hocko@suse.cz
    Cc: hughd@google.com
    Cc: mst@redhat.com
    Cc: paulus@samba.org
    Cc: ralf@linux-mips.org
    Cc: schwidefsky@de.ibm.com
    Cc: yang.shi@windriver.com
    Link: http://lkml.kernel.org/r/1431359540-32227-2-git-send-email-dahi@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2e670864174f..0bb88b555550 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1393,6 +1393,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->hardirq_context = 0;
 	p->softirq_context = 0;
 #endif
+
+	p->pagefault_disabled = 0;
+
 #ifdef CONFIG_LOCKDEP
 	p->lockdep_depth = 0; /* no locks held yet */
 	p->curr_chain_key = 0;

commit 1018016c706f7ff9f56fde3a649789c47085a293
Author: Jason Low <jason.low2@hp.com>
Date:   Tue Apr 28 13:00:22 2015 -0700

    sched, timer: Replace spinlocks with atomics in thread_group_cputimer(), to improve scalability
    
    While running a database workload, we found a scalability issue with itimers.
    
    Much of the problem was caused by the thread_group_cputimer spinlock.
    Each time we account for group system/user time, we need to obtain a
    thread_group_cputimer's spinlock to update the timers. On larger systems
    (such as a 16 socket machine), this caused more than 30% of total time
    spent trying to obtain this kernel lock to update these group timer stats.
    
    This patch converts the timers to 64-bit atomic variables and use
    atomic add to update them without a lock. With this patch, the percent
    of total time spent updating thread group cputimer timers was reduced
    from 30% down to less than 1%.
    
    Note: On 32-bit systems using the generic 64-bit atomics, this causes
    sample_group_cputimer() to take locks 3 times instead of just 1 time.
    However, we tested this patch on a 32-bit system ARM system using the
    generic atomics and did not find the overhead to be much of an issue.
    An explanation for why this isn't an issue is that 32-bit systems usually
    have small numbers of CPUs, and cacheline contention from extra spinlocks
    called periodically is not really apparent on smaller systems.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Link: http://lkml.kernel.org/r/1430251224-5764-4-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 47c37a411a62..2e670864174f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1091,9 +1091,6 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 {
 	unsigned long cpu_limit;
 
-	/* Thread group counters. */
-	thread_group_cputime_init(sig);
-
 	cpu_limit = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
 	if (cpu_limit != RLIM_INFINITY) {
 		sig->cputime_expires.prof_exp = secs_to_cputime(cpu_limit);

commit 316c1608d15c736439d4065ed12f306db554b3da
Author: Jason Low <jason.low2@hp.com>
Date:   Tue Apr 28 13:00:20 2015 -0700

    sched, timer: Convert usages of ACCESS_ONCE() in the scheduler to READ_ONCE()/WRITE_ONCE()
    
    ACCESS_ONCE doesn't work reliably on non-scalar types. This patch removes
    the rest of the existing usages of ACCESS_ONCE() in the scheduler, and use
    the new READ_ONCE() and WRITE_ONCE() APIs as appropriate.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Waiman Long <Waiman.Long@hp.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1430251224-5764-2-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 03c1eaaa6ef5..47c37a411a62 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1094,7 +1094,7 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 	/* Thread group counters. */
 	thread_group_cputime_init(sig);
 
-	cpu_limit = ACCESS_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
+	cpu_limit = READ_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
 	if (cpu_limit != RLIM_INFINITY) {
 		sig->cputime_expires.prof_exp = secs_to_cputime(cpu_limit);
 		sig->cputimer.running = 1;

commit 11163348a23cdbcdca5fb42485418e75f8566a5c
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Thu Apr 16 12:49:12 2015 -0700

    oprofile: reduce mmap_sem hold for mm->exe_file
    
    sync_buffer() needs the mmap_sem for two distinct operations, both only
    occurring upon user context switch handling:
    
     1) Dealing with the exe_file.
    
     2) Adding the dcookie data as we need to lookup the vma that
       backs it. This is done via add_sample() and add_data().
    
    This patch isolates 1), for it will no longer need the mmap_sem for
    serialization.  However, for now, make of the more standard
    get_mm_exe_file(), requiring only holding the mmap_sem to read the value,
    and relying on reference counting to make sure that the exe file won't
    dissappear underneath us while doing the get dcookie.
    
    As a consequence, for 2) we move the mmap_sem locking into where we really
    need it, in lookup_dcookie().  The benefits are twofold: reduce mmap_sem
    hold times, and cleaner code.
    
    [akpm@linux-foundation.org: export get_mm_exe_file for arch/x86/oprofile/oprofile.ko]
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Robert Richter <rric@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0d23e76a0c61..03c1eaaa6ef5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -752,6 +752,7 @@ struct file *get_mm_exe_file(struct mm_struct *mm)
 	rcu_read_unlock();
 	return exe_file;
 }
+EXPORT_SYMBOL(get_mm_exe_file);
 
 /**
  * get_task_mm - acquire a reference to the task's mm

commit 6e399cd144d8500ffb5d40fa6848890e2580a80a
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Thu Apr 16 12:47:59 2015 -0700

    prctl: avoid using mmap_sem for exe_file serialization
    
    Oleg cleverly suggested using xchg() to set the new mm->exe_file instead
    of calling set_mm_exe_file() which requires some form of serialization --
    mmap_sem in this case.  For archs that do not have atomic rmw instructions
    we still fallback to a spinlock alternative, so this should always be
    safe.  As such, we only need the mmap_sem for looking up the backing
    vm_file, which can be done sharing the lock.  Naturally, this means we
    need to manually deal with both the new and old file reference counting,
    and we need not worry about the MMF_EXE_FILE_CHANGED bits, which can
    probably be deleted in the future anyway.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 259202637531..0d23e76a0c61 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -711,15 +711,22 @@ EXPORT_SYMBOL_GPL(mmput);
  *
  * This changes mm's executable file (shown as symlink /proc/[pid]/exe).
  *
- * Main users are mmput(), sys_execve() and sys_prctl(PR_SET_MM_MAP/EXE_FILE).
- * Callers prevent concurrent invocations: in mmput() nobody alive left,
- * in execve task is single-threaded, prctl holds mmap_sem exclusively.
+ * Main users are mmput() and sys_execve(). Callers prevent concurrent
+ * invocations: in mmput() nobody alive left, in execve task is single
+ * threaded. sys_prctl(PR_SET_MM_MAP/EXE_FILE) also needs to set the
+ * mm->exe_file, but does so without using set_mm_exe_file() in order
+ * to do avoid the need for any locks.
  */
 void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file)
 {
-	struct file *old_exe_file = rcu_dereference_protected(mm->exe_file,
-			!atomic_read(&mm->mm_users) || current->in_execve ||
-			lockdep_is_held(&mm->mmap_sem));
+	struct file *old_exe_file;
+
+	/*
+	 * It is safe to dereference the exe_file without RCU as
+	 * this function is only called if nobody else can access
+	 * this mm -- see comment above for justification.
+	 */
+	old_exe_file = rcu_dereference_raw(mm->exe_file);
 
 	if (new_exe_file)
 		get_file(new_exe_file);

commit 90f31d0ea88880f780574f3d0bb1a227c4c66ca3
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Thu Apr 16 12:47:56 2015 -0700

    mm: rcu-protected get_mm_exe_file()
    
    This patch removes mm->mmap_sem from mm->exe_file read side.
    Also it kills dup_mm_exe_file() and moves exe_file duplication into
    dup_mmap() where both mmap_sems are locked.
    
    [akpm@linux-foundation.org: fix comment typo]
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8807a129711b..259202637531 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -403,6 +403,9 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	 */
 	down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);
 
+	/* No ordering required: file already has been exposed. */
+	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
+
 	mm->total_vm = oldmm->total_vm;
 	mm->shared_vm = oldmm->shared_vm;
 	mm->exec_vm = oldmm->exec_vm;
@@ -528,7 +531,13 @@ static inline void mm_free_pgd(struct mm_struct *mm)
 	pgd_free(mm, mm->pgd);
 }
 #else
-#define dup_mmap(mm, oldmm)	(0)
+static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
+{
+	down_write(&oldmm->mmap_sem);
+	RCU_INIT_POINTER(mm->exe_file, get_mm_exe_file(oldmm));
+	up_write(&oldmm->mmap_sem);
+	return 0;
+}
 #define mm_alloc_pgd(mm)	(0)
 #define mm_free_pgd(mm)
 #endif /* CONFIG_MMU */
@@ -697,35 +706,46 @@ void mmput(struct mm_struct *mm)
 }
 EXPORT_SYMBOL_GPL(mmput);
 
+/**
+ * set_mm_exe_file - change a reference to the mm's executable file
+ *
+ * This changes mm's executable file (shown as symlink /proc/[pid]/exe).
+ *
+ * Main users are mmput(), sys_execve() and sys_prctl(PR_SET_MM_MAP/EXE_FILE).
+ * Callers prevent concurrent invocations: in mmput() nobody alive left,
+ * in execve task is single-threaded, prctl holds mmap_sem exclusively.
+ */
 void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file)
 {
+	struct file *old_exe_file = rcu_dereference_protected(mm->exe_file,
+			!atomic_read(&mm->mm_users) || current->in_execve ||
+			lockdep_is_held(&mm->mmap_sem));
+
 	if (new_exe_file)
 		get_file(new_exe_file);
-	if (mm->exe_file)
-		fput(mm->exe_file);
-	mm->exe_file = new_exe_file;
+	rcu_assign_pointer(mm->exe_file, new_exe_file);
+	if (old_exe_file)
+		fput(old_exe_file);
 }
 
+/**
+ * get_mm_exe_file - acquire a reference to the mm's executable file
+ *
+ * Returns %NULL if mm has no associated executable file.
+ * User must release file via fput().
+ */
 struct file *get_mm_exe_file(struct mm_struct *mm)
 {
 	struct file *exe_file;
 
-	/* We need mmap_sem to protect against races with removal of exe_file */
-	down_read(&mm->mmap_sem);
-	exe_file = mm->exe_file;
-	if (exe_file)
-		get_file(exe_file);
-	up_read(&mm->mmap_sem);
+	rcu_read_lock();
+	exe_file = rcu_dereference(mm->exe_file);
+	if (exe_file && !get_file_rcu(exe_file))
+		exe_file = NULL;
+	rcu_read_unlock();
 	return exe_file;
 }
 
-static void dup_mm_exe_file(struct mm_struct *oldmm, struct mm_struct *newmm)
-{
-	/* It's safe to write the exe_file pointer without exe_file_lock because
-	 * this is called during fork when the task is not yet in /proc */
-	newmm->exe_file = get_mm_exe_file(oldmm);
-}
-
 /**
  * get_task_mm - acquire a reference to the task's mm
  *
@@ -887,8 +907,6 @@ static struct mm_struct *dup_mm(struct task_struct *tsk)
 	if (!mm_init(mm, tsk))
 		goto fail_nomem;
 
-	dup_mm_exe_file(oldmm, mm);
-
 	err = dup_mmap(mm, oldmm);
 	if (err)
 		goto free_pt;

commit 16db3d3f1170fb0efca652c9378ce7c5f5cb4232
Author: Heinrich Schuchardt <xypron.glpk@gmx.de>
Date:   Thu Apr 16 12:47:50 2015 -0700

    kernel/sysctl.c: threads-max observe limits
    
    Users can change the maximum number of threads by writing to
    /proc/sys/kernel/threads-max.
    
    With the patch the value entered is checked against the same limits that
    apply when fork_init is called.
    
    Signed-off-by: Heinrich Schuchardt <xypron.glpk@gmx.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c7f2e1a4187a..8807a129711b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -74,6 +74,7 @@
 #include <linux/uprobes.h>
 #include <linux/aio.h>
 #include <linux/compiler.h>
+#include <linux/sysctl.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -266,7 +267,7 @@ void __init __weak arch_task_cache_init(void) { }
 /*
  * set_max_threads
  */
-static void set_max_threads(void)
+static void set_max_threads(unsigned int max_threads_suggested)
 {
 	u64 threads;
 
@@ -280,6 +281,9 @@ static void set_max_threads(void)
 		threads = div64_u64((u64) totalram_pages * (u64) PAGE_SIZE,
 				    (u64) THREAD_SIZE * 8UL);
 
+	if (threads > max_threads_suggested)
+		threads = max_threads_suggested;
+
 	max_threads = clamp_t(u64, threads, MIN_THREADS, MAX_THREADS);
 }
 
@@ -298,7 +302,7 @@ void __init fork_init(void)
 	/* do the arch specific task caches init */
 	arch_task_cache_init();
 
-	set_max_threads();
+	set_max_threads(MAX_THREADS);
 
 	init_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads/2;
 	init_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads/2;
@@ -2020,3 +2024,26 @@ int unshare_files(struct files_struct **displaced)
 	task_unlock(task);
 	return 0;
 }
+
+int sysctl_max_threads(struct ctl_table *table, int write,
+		       void __user *buffer, size_t *lenp, loff_t *ppos)
+{
+	struct ctl_table t;
+	int ret;
+	int threads = max_threads;
+	int min = MIN_THREADS;
+	int max = MAX_THREADS;
+
+	t = *table;
+	t.data = &threads;
+	t.extra1 = &min;
+	t.extra2 = &max;
+
+	ret = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);
+	if (ret || !write)
+		return ret;
+
+	set_max_threads(threads);
+
+	return 0;
+}

commit ac1b398de1ef94aeee8ba87b0120763526572a6e
Author: Heinrich Schuchardt <xypron.glpk@gmx.de>
Date:   Thu Apr 16 12:47:47 2015 -0700

    kernel/fork.c: avoid division by zero
    
    PAGE_SIZE is not guaranteed to be equal to or less than 8 times the
    THREAD_SIZE.
    
    E.g.  architecture hexagon may have page size 1M and thread size 4096.
    This would lead to a division by zero in the calculation of max_threads.
    
    With 32-bit calculation there is no solution which delivers valid results
    for all possible combinations of the parameters.  The code is only called
    once.  Hence a 64-bit calculation can be used as solution.
    
    [akpm@linux-foundation.org: use clamp_t(), per Oleg]
    Signed-off-by: Heinrich Schuchardt <xypron.glpk@gmx.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 01038e6f51a8..c7f2e1a4187a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -87,6 +87,16 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/task.h>
 
+/*
+ * Minimum number of threads to boot the kernel
+ */
+#define MIN_THREADS 20
+
+/*
+ * Maximum number of threads
+ */
+#define MAX_THREADS FUTEX_TID_MASK
+
 /*
  * Protected counters by write_lock_irq(&tasklist_lock)
  */
@@ -258,18 +268,19 @@ void __init __weak arch_task_cache_init(void) { }
  */
 static void set_max_threads(void)
 {
-	/*
-	 * The default maximum number of threads is set to a safe
-	 * value: the thread structures can take up at most one
-	 * eighth of the memory.
-	 */
-	max_threads = totalram_pages / (8 * THREAD_SIZE / PAGE_SIZE);
+	u64 threads;
 
 	/*
-	 * we need to allow at least 20 threads to boot a system
+	 * The number of threads shall be limited such that the thread
+	 * structures may only consume a small part of the available memory.
 	 */
-	if (max_threads < 20)
-		max_threads = 20;
+	if (fls64(totalram_pages) + fls64(PAGE_SIZE) > 64)
+		threads = MAX_THREADS;
+	else
+		threads = div64_u64((u64) totalram_pages * (u64) PAGE_SIZE,
+				    (u64) THREAD_SIZE * 8UL);
+
+	max_threads = clamp_t(u64, threads, MIN_THREADS, MAX_THREADS);
 }
 
 void __init fork_init(void)

commit ff691f6e03815dc8f99461ea509df863a879fc3a
Author: Heinrich Schuchardt <xypron.glpk@gmx.de>
Date:   Thu Apr 16 12:47:44 2015 -0700

    kernel/fork.c: new function for max_threads
    
    PAGE_SIZE is not guaranteed to be equal to or less than 8 times the
    THREAD_SIZE.
    
    E.g.  architecture hexagon may have page size 1M and thread size 4096.
    This would lead to a division by zero in the calculation of max_threads.
    
    With this patch the buggy code is moved to a separate function
    set_max_threads.  The error is not fixed.
    
    After fixing the problem in a separate patch the new function can be
    reused to adjust max_threads after adding or removing memory.
    
    Argument mempages of function fork_init() is removed as totalram_pages is
    an exported symbol.
    
    The creation of separate patches for refactoring to a new function and for
    fixing the logic was suggested by Ingo Molnar.
    
    Signed-off-by: Heinrich Schuchardt <xypron.glpk@gmx.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c507e29bcb01..01038e6f51a8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -253,7 +253,26 @@ EXPORT_SYMBOL_GPL(__put_task_struct);
 
 void __init __weak arch_task_cache_init(void) { }
 
-void __init fork_init(unsigned long mempages)
+/*
+ * set_max_threads
+ */
+static void set_max_threads(void)
+{
+	/*
+	 * The default maximum number of threads is set to a safe
+	 * value: the thread structures can take up at most one
+	 * eighth of the memory.
+	 */
+	max_threads = totalram_pages / (8 * THREAD_SIZE / PAGE_SIZE);
+
+	/*
+	 * we need to allow at least 20 threads to boot a system
+	 */
+	if (max_threads < 20)
+		max_threads = 20;
+}
+
+void __init fork_init(void)
 {
 #ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 #ifndef ARCH_MIN_TASKALIGN
@@ -268,18 +287,7 @@ void __init fork_init(unsigned long mempages)
 	/* do the arch specific task caches init */
 	arch_task_cache_init();
 
-	/*
-	 * The default maximum number of threads is set to a safe
-	 * value: the thread structures can take up at most one
-	 * eighth of the memory.
-	 */
-	max_threads = mempages / (8 * THREAD_SIZE / PAGE_SIZE);
-
-	/*
-	 * we need to allow at least 20 threads to boot a system
-	 */
-	if (max_threads < 20)
-		max_threads = 20;
+	set_max_threads();
 
 	init_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads/2;
 	init_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads/2;

commit 3ea7f5e25ec271909451b7dc17be37581b888de6
Author: Jean Delvare <jdelvare@suse.de>
Date:   Thu Apr 16 12:47:41 2015 -0700

    fork_init: update max_threads comment
    
    The comment explaining what value max_threads is set to is outdated.  The
    maximum memory consumption ratio for thread structures was 1/2 until
    February 2002, then it was briefly changed to 1/16 before being set to 1/8
    which we still use today.  The comment was never updated to reflect that
    change, it's about time.
    
    Signed-off-by: Jean Delvare <jdelvare@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d778016ac1e3..c507e29bcb01 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -270,8 +270,8 @@ void __init fork_init(unsigned long mempages)
 
 	/*
 	 * The default maximum number of threads is set to a safe
-	 * value: the thread structures can take up at most half
-	 * of memory.
+	 * value: the thread structures can take up at most one
+	 * eighth of the memory.
 	 */
 	max_threads = mempages / (8 * THREAD_SIZE / PAGE_SIZE);
 

commit 35f71bc0a09a45924bed268d8ccd0d3407bc476f
Author: Michal Hocko <mhocko@suse.cz>
Date:   Thu Apr 16 12:47:38 2015 -0700

    fork: report pid reservation failure properly
    
    copy_process will report any failure in alloc_pid as ENOMEM currently
    which is misleading because the pid allocation might fail not only when
    the memory is short but also when the pid space is consumed already.
    
    The current man page even mentions this case:
    
    : EAGAIN
    :
    :       A system-imposed limit on the number of threads was encountered.
    :       There are a number of limits that may trigger this error: the
    :       RLIMIT_NPROC soft resource limit (set via setrlimit(2)), which
    :       limits the number of processes and threads for a real user ID, was
    :       reached; the kernel's system-wide limit on the number of processes
    :       and threads, /proc/sys/kernel/threads-max, was reached (see
    :       proc(5)); or the maximum number of PIDs, /proc/sys/kernel/pid_max,
    :       was reached (see proc(5)).
    
    so the current behavior is also incorrect wrt.  documentation.  POSIX man
    page also suggest returing EAGAIN when the process count limit is reached.
    
    This patch simply propagates error code from alloc_pid and makes sure we
    return -EAGAIN due to reservation failure.  This will make behavior of
    fork closer to both our documentation and POSIX.
    
    alloc_pid might alsoo fail when the reaper in the pid namespace is dead
    (the namespace basically disallows all new processes) and there is no
    good error code which would match documented ones. We have traditionally
    returned ENOMEM for this case which is misleading as well but as per
    Eric W. Biederman this behavior is documented in man pid_namespaces(7)
    
    : If the "init" process of a PID namespace terminates, the kernel
    : terminates all of the processes in the namespace via a SIGKILL signal.
    : This behavior reflects the fact that the "init" process is essential for
    : the correct operation of a PID namespace.  In this case, a subsequent
    : fork(2) into this PID namespace will fail with the error ENOMEM; it is
    : not possible to create a new processes in a PID namespace whose "init"
    : process has terminated.
    
    and introducing a new error code would be too risky so let's stick to
    ENOMEM for this case.
    
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f2c1e7352298..d778016ac1e3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1403,10 +1403,11 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_io;
 
 	if (pid != &init_struct_pid) {
-		retval = -ENOMEM;
 		pid = alloc_pid(p->nsproxy->pid_ns_for_children);
-		if (!pid)
+		if (IS_ERR(pid)) {
+			retval = PTR_ERR(pid);
 			goto bad_fork_cleanup_io;
+		}
 	}
 
 	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;

commit 973f911f55a0e510dd6db8bbb29cd82ff138d3c0
Author: Richard Weinberger <richard@nod.at>
Date:   Mon Mar 30 08:14:16 2015 +0200

    Remove execution domain support
    
    All users of exec_domain are gone, now we can get rid
    of that abandoned feature.
    To not break existing userspace we keep a dummy
    /proc/execdomains file which will always contain
    "0-0     Linux                   [kernel]".
    
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/kernel/fork.c b/kernel/fork.c
index cf65139615a0..f2c1e7352298 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1279,9 +1279,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (nr_threads >= max_threads)
 		goto bad_fork_cleanup_count;
 
-	if (!try_module_get(task_thread_info(p)->exec_domain->module))
-		goto bad_fork_cleanup_count;
-
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);
 	p->flags |= PF_FORKNOEXEC;
@@ -1590,7 +1587,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (clone_flags & CLONE_THREAD)
 		threadgroup_change_end(current);
 	delayacct_tsk_free(p);
-	module_put(task_thread_info(p)->exec_domain->module);
 bad_fork_cleanup_count:
 	atomic_dec(&p->cred->user->processes);
 	exit_creds(p);

commit 2d2f5119b8bb057595e18f5b2f07aa097ea1b233
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Feb 12 14:59:59 2015 -0800

    mm: do not use mm->nr_pmds on !MMU configurations
    
    mm->nr_pmds doesn't make sense on !MMU configurations
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 66e19c251581..cf65139615a0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -555,9 +555,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->core_state = NULL;
 	atomic_long_set(&mm->nr_ptes, 0);
-#ifndef __PAGETABLE_PMD_FOLDED
-	atomic_long_set(&mm->nr_pmds, 0);
-#endif
+	mm_nr_pmds_init(mm);
 	mm->map_count = 0;
 	mm->locked_vm = 0;
 	mm->pinned_vm = 0;

commit b30fe6c7ced70f62862c3d09357e7e8084e98d9f
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Feb 11 15:26:53 2015 -0800

    mm: fix false-positive warning on exit due mm_nr_pmds(mm)
    
    The problem is that we check nr_ptes/nr_pmds in exit_mmap() which happens
    *before* pgd_free().  And if an arch does pte/pmd allocation in
    pgd_alloc() and frees them in pgd_free() we see offset in counters by the
    time of the checks.
    
    We tried to workaround this by offsetting expected counter value according
    to FIRST_USER_ADDRESS for both nr_pte and nr_pmd in exit_mmap().  But it
    doesn't work in some cases:
    
    1. ARM with LPAE enabled also has non-zero USER_PGTABLES_CEILING, but
       upper addresses occupied with huge pmd entries, so the trick with
       offsetting expected counter value will get really ugly: we will have
       to apply it nr_pmds, but not nr_ptes.
    
    2. Metag has non-zero FIRST_USER_ADDRESS, but doesn't do allocation
       pte/pmd page tables allocation in pgd_alloc(), just setup a pgd entry
       which is allocated at boot and shared accross all processes.
    
    The proposal is to move the check to check_mm() which happens *after*
    pgd_free() and do proper accounting during pgd_alloc() and pgd_free()
    which would bring counters to zero if nothing leaked.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reported-by: Tyler Baker <tyler.baker@linaro.org>
    Tested-by: Tyler Baker <tyler.baker@linaro.org>
    Tested-by: Nishanth Menon <nm@ti.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c99098c52641..66e19c251581 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -606,6 +606,14 @@ static void check_mm(struct mm_struct *mm)
 			printk(KERN_ALERT "BUG: Bad rss-counter state "
 					  "mm:%p idx:%d val:%ld\n", mm, i, x);
 	}
+
+	if (atomic_long_read(&mm->nr_ptes))
+		pr_alert("BUG: non-zero nr_ptes on freeing mm: %ld\n",
+				atomic_long_read(&mm->nr_ptes));
+	if (mm_nr_pmds(mm))
+		pr_alert("BUG: non-zero nr_pmds on freeing mm: %ld\n",
+				mm_nr_pmds(mm));
+
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	VM_BUG_ON_MM(mm->pmd_huge_pte, mm);
 #endif

commit dc6c9a35b66b520cf67e05d8ca60ebecad3b0479
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Feb 11 15:26:50 2015 -0800

    mm: account pmd page tables to the process
    
    Dave noticed that unprivileged process can allocate significant amount of
    memory -- >500 MiB on x86_64 -- and stay unnoticed by oom-killer and
    memory cgroup.  The trick is to allocate a lot of PMD page tables.  Linux
    kernel doesn't account PMD tables to the process, only PTE.
    
    The use-cases below use few tricks to allocate a lot of PMD page tables
    while keeping VmRSS and VmPTE low.  oom_score for the process will be 0.
    
            #include <errno.h>
            #include <stdio.h>
            #include <stdlib.h>
            #include <unistd.h>
            #include <sys/mman.h>
            #include <sys/prctl.h>
    
            #define PUD_SIZE (1UL << 30)
            #define PMD_SIZE (1UL << 21)
    
            #define NR_PUD 130000
    
            int main(void)
            {
                    char *addr = NULL;
                    unsigned long i;
    
                    prctl(PR_SET_THP_DISABLE);
                    for (i = 0; i < NR_PUD ; i++) {
                            addr = mmap(addr + PUD_SIZE, PUD_SIZE, PROT_WRITE|PROT_READ,
                                            MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);
                            if (addr == MAP_FAILED) {
                                    perror("mmap");
                                    break;
                            }
                            *addr = 'x';
                            munmap(addr, PMD_SIZE);
                            mmap(addr, PMD_SIZE, PROT_WRITE|PROT_READ,
                                            MAP_ANONYMOUS|MAP_PRIVATE|MAP_FIXED, -1, 0);
                            if (addr == MAP_FAILED)
                                    perror("re-mmap"), exit(1);
                    }
                    printf("PID %d consumed %lu KiB in PMD page tables\n",
                                    getpid(), i * 4096 >> 10);
                    return pause();
            }
    
    The patch addresses the issue by account PMD tables to the process the
    same way we account PTE.
    
    The main place where PMD tables is accounted is __pmd_alloc() and
    free_pmd_range(). But there're few corner cases:
    
     - HugeTLB can share PMD page tables. The patch handles by accounting
       the table to all processes who share it.
    
     - x86 PAE pre-allocates few PMD tables on fork.
    
     - Architectures with FIRST_USER_ADDRESS > 0. We need to adjust sanity
       check on exit(2).
    
    Accounting only happens on configuration where PMD page table's level is
    present (PMD is not folded).  As with nr_ptes we use per-mm counter.  The
    counter value is used to calculate baseline for badness score by
    oom-killer.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reported-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Hugh Dickins <hughd@google.com>
    Reviewed-by: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: David Rientjes <rientjes@google.com>
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b379d9abddc7..c99098c52641 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -555,6 +555,9 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->core_state = NULL;
 	atomic_long_set(&mm->nr_ptes, 0);
+#ifndef __PAGETABLE_PMD_FOLDED
+	atomic_long_set(&mm->nr_pmds, 0);
+#endif
 	mm->map_count = 0;
 	mm->locked_vm = 0;
 	mm->pinned_vm = 0;

commit 27ba0644ea9dfe6e7693abc85837b60e40583b96
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Feb 10 14:09:59 2015 -0800

    rmap: drop support of non-linear mappings
    
    We don't create non-linear mappings anymore.  Let's drop code which
    handles them in rmap.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4dc2ddade9f1..b379d9abddc7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -438,12 +438,8 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 				atomic_inc(&mapping->i_mmap_writable);
 			flush_dcache_mmap_lock(mapping);
 			/* insert tmp into the share list, just after mpnt */
-			if (unlikely(tmp->vm_flags & VM_NONLINEAR))
-				vma_nonlinear_insert(tmp,
-						&mapping->i_mmap_nonlinear);
-			else
-				vma_interval_tree_insert_after(tmp, mpnt,
-							&mapping->i_mmap);
+			vma_interval_tree_insert_after(tmp, mpnt,
+					&mapping->i_mmap);
 			flush_dcache_mmap_unlock(mapping);
 			i_mmap_unlock_write(mapping);
 		}

commit 83cde9e8ba95d180eaefefe834958fbf7008cf39
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Dec 12 16:54:21 2014 -0800

    mm: use new helper functions around the i_mmap_mutex
    
    Convert all open coded mutex_lock/unlock calls to the
    i_mmap_[lock/unlock]_write() helpers.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: "Kirill A. Shutemov" <kirill@shutemov.name>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9ca84189cfc2..4dc2ddade9f1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -433,7 +433,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			get_file(file);
 			if (tmp->vm_flags & VM_DENYWRITE)
 				atomic_dec(&inode->i_writecount);
-			mutex_lock(&mapping->i_mmap_mutex);
+			i_mmap_lock_write(mapping);
 			if (tmp->vm_flags & VM_SHARED)
 				atomic_inc(&mapping->i_mmap_writable);
 			flush_dcache_mmap_lock(mapping);
@@ -445,7 +445,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 				vma_interval_tree_insert_after(tmp, mpnt,
 							&mapping->i_mmap);
 			flush_dcache_mmap_unlock(mapping);
-			mutex_unlock(&mapping->i_mmap_mutex);
+			i_mmap_unlock_write(mapping);
 		}
 
 		/*

commit 392809b25833548ccfc55e61b76c8451a5073216
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sun Sep 28 23:44:18 2014 +0200

    signal: Document the RCU protection of ->sighand
    
    __cleanup_sighand() frees sighand without RCU grace period. This is
    correct but this looks "obviously buggy" and constantly confuses the
    readers, add the comments to explain how this works.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9b7d746d6d62..9ca84189cfc2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1022,11 +1022,14 @@ void __cleanup_sighand(struct sighand_struct *sighand)
 {
 	if (atomic_dec_and_test(&sighand->count)) {
 		signalfd_cleanup(sighand);
+		/*
+		 * sighand_cachep is SLAB_DESTROY_BY_RCU so we can free it
+		 * without an RCU grace period, see __lock_task_sighand().
+		 */
 		kmem_cache_free(sighand_cachep, sighand);
 	}
 }
 
-
 /*
  * Initialize POSIX timer handling for a thread group.
  */

commit faafcba3b5e15999cf75d5c5a513ac8e47e2545f
Merge: 13ead805c5a1 f10e00f4bf36
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 16:23:15 2014 +0200

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Optimized support for Intel "Cluster-on-Die" (CoD) topologies (Dave
         Hansen)
    
       - Various sched/idle refinements for better idle handling (Nicolas
         Pitre, Daniel Lezcano, Chuansheng Liu, Vincent Guittot)
    
       - sched/numa updates and optimizations (Rik van Riel)
    
       - sysbench speedup (Vincent Guittot)
    
       - capacity calculation cleanups/refactoring (Vincent Guittot)
    
       - Various cleanups to thread group iteration (Oleg Nesterov)
    
       - Double-rq-lock removal optimization and various refactorings
         (Kirill Tkhai)
    
       - various sched/deadline fixes
    
      ... and lots of other changes"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (72 commits)
      sched/dl: Use dl_bw_of() under rcu_read_lock_sched()
      sched/fair: Delete resched_cpu() from idle_balance()
      sched, time: Fix build error with 64 bit cputime_t on 32 bit systems
      sched: Improve sysbench performance by fixing spurious active migration
      sched/x86: Fix up typo in topology detection
      x86, sched: Add new topology for multi-NUMA-node CPUs
      sched/rt: Use resched_curr() in task_tick_rt()
      sched: Use rq->rd in sched_setaffinity() under RCU read lock
      sched: cleanup: Rename 'out_unlock' to 'out_free_new_mask'
      sched: Use dl_bw_of() under RCU read lock
      sched/fair: Remove duplicate code from can_migrate_task()
      sched, mips, ia64: Remove __ARCH_WANT_UNLOCKED_CTXSW
      sched: print_rq(): Don't use tasklist_lock
      sched: normalize_rt_tasks(): Don't use _irqsave for tasklist_lock, use task_rq_lock()
      sched: Fix the task-group check in tg_has_rt_tasks()
      sched/fair: Leverage the idle state info when choosing the "idlest" cpu
      sched: Let the scheduler see CPU idle states
      sched/deadline: Fix inter- exclusive cpusets migrations
      sched/deadline: Clear dl_entity params when setscheduling to different class
      sched/numa: Kill the wrong/dead TASK_DEAD check in task_numa_fault()
      ...

commit 96dad67ff244e797c4bc3e4f7f0fdaa0cfdf0a7d
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Thu Oct 9 15:28:39 2014 -0700

    mm: use VM_BUG_ON_MM where possible
    
    Dump the contents of the relevant struct_mm when we hit the bug condition.
    
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a91e47d86de2..8c162d102740 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -601,9 +601,8 @@ static void check_mm(struct mm_struct *mm)
 			printk(KERN_ALERT "BUG: Bad rss-counter state "
 					  "mm:%p idx:%d val:%ld\n", mm, i, x);
 	}
-
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
-	VM_BUG_ON(mm->pmd_huge_pte);
+	VM_BUG_ON_MM(mm->pmd_huge_pte, mm);
 #endif
 }
 

commit 6c72e3501d0d62fc064d3680e5234f3463ec5a86
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 2 16:17:02 2014 -0700

    perf: fix perf bug in fork()
    
    Oleg noticed that a cleanup by Sylvain actually uncovered a bug; by
    calling perf_event_free_task() when failing sched_fork() we will not yet
    have done the memset() on ->perf_event_ctxp[] and will therefore try and
    'free' the inherited contexts, which are still in use by the parent
    process.  This is bad..
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Reported-by: Sylvain 'ythier' Hitier <sylvain.hitier@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0cf9cdb6e491..a91e47d86de2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1360,7 +1360,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_policy;
 	retval = audit_alloc(p);
 	if (retval)
-		goto bad_fork_cleanup_policy;
+		goto bad_fork_cleanup_perf;
 	/* copy all the process information */
 	shm_init_task(p);
 	retval = copy_semundo(clone_flags, p);
@@ -1566,8 +1566,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	exit_sem(p);
 bad_fork_cleanup_audit:
 	audit_free(p);
-bad_fork_cleanup_policy:
+bad_fork_cleanup_perf:
 	perf_event_free_task(p);
+bad_fork_cleanup_policy:
 #ifdef CONFIG_NUMA
 	mpol_put(p->mempolicy);
 bad_fork_cleanup_threadgroup_lock:

commit d4311ff1a8da48d609db9500f121c15580dfeeb7
Author: Aaron Tomlin <atomlin@redhat.com>
Date:   Fri Sep 12 14:16:17 2014 +0100

    init/main.c: Give init_task a canary
    
    Tasks get their end of stack set to STACK_END_MAGIC with the
    aim to catch stack overruns. Currently this feature does not
    apply to init_task. This patch removes this restriction.
    
    Note that a similar patch was posted by Prarit Bhargava
    some time ago but was never merged:
    
      http://marc.info/?l=linux-kernel&m=127144305403241&w=2
    
    Signed-off-by: Aaron Tomlin <atomlin@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: aneesh.kumar@linux.vnet.ibm.com
    Cc: dzickus@redhat.com
    Cc: bmr@redhat.com
    Cc: jcastillo@redhat.com
    Cc: jgh@redhat.com
    Cc: minchan@kernel.org
    Cc: tglx@linutronix.de
    Cc: hannes@cmpxchg.org
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Daeseok Youn <daeseok.youn@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Fabian Frederick <fabf@skynet.be>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Michael Opdenacker <michael.opdenacker@free-electrons.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/1410527779-8133-2-git-send-email-atomlin@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9387ae8ab048..ad64248c4b18 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -294,11 +294,18 @@ int __weak arch_dup_task_struct(struct task_struct *dst,
 	return 0;
 }
 
+void set_task_stack_end_magic(struct task_struct *tsk)
+{
+	unsigned long *stackend;
+
+	stackend = end_of_stack(tsk);
+	*stackend = STACK_END_MAGIC;	/* for overflow detection */
+}
+
 static struct task_struct *dup_task_struct(struct task_struct *orig)
 {
 	struct task_struct *tsk;
 	struct thread_info *ti;
-	unsigned long *stackend;
 	int node = tsk_fork_get_node(orig);
 	int err;
 
@@ -328,8 +335,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	setup_thread_stack(tsk, orig);
 	clear_user_return_notifier(tsk);
 	clear_tsk_need_resched(tsk);
-	stackend = end_of_stack(tsk);
-	*stackend = STACK_END_MAGIC;	/* for overflow detection */
+	set_task_stack_end_magic(tsk);
 
 #ifdef CONFIG_CC_STACKPROTECTOR
 	tsk->stack_canary = get_random_int();

commit e78c3496790ee8a36522a838b59b388e8a709e65
Author: Rik van Riel <riel@redhat.com>
Date:   Sat Aug 16 13:40:10 2014 -0400

    time, signal: Protect resource use statistics with seqlock
    
    Both times() and clock_gettime(CLOCK_PROCESS_CPUTIME_ID) have scalability
    issues on large systems, due to both functions being serialized with a
    lock.
    
    The lock protects against reporting a wrong value, due to a thread in the
    task group exiting, its statistics reporting up to the signal struct, and
    that exited task's statistics being counted twice (or not at all).
    
    Protecting that with a lock results in times() and clock_gettime() being
    completely serialized on large systems.
    
    This can be fixed by using a seqlock around the events that gather and
    propagate statistics. As an additional benefit, the protection code can
    be moved into thread_group_cputime(), slightly simplifying the calling
    functions.
    
    In the case of posix_cpu_clock_get_task() things can be simplified a
    lot, because the calling function already ensures that the task sticks
    around, and the rest is now taken care of in thread_group_cputime().
    
    This way the statistics reporting code can run lockless.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Daeseok Youn <daeseok.youn@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guillaume Morin <guillaume@morinfr.org>
    Cc: Ionut Alexa <ionut.m.alexa@gmail.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Michal Schmidt <mschmidt@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: umgwanakikbuti@gmail.com
    Cc: fweisbec@gmail.com
    Cc: srao@redhat.com
    Cc: lwoodman@redhat.com
    Cc: atheurer@redhat.com
    Link: http://lkml.kernel.org/r/20140816134010.26a9b572@annuminas.surriel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0cf9cdb6e491..9387ae8ab048 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1068,6 +1068,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->curr_target = tsk;
 	init_sigpending(&sig->shared_pending);
 	INIT_LIST_HEAD(&sig->posix_timers);
+	seqlock_init(&sig->stats_lock);
 
 	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	sig->real_timer.function = it_real_fn;

commit 69f6a34bdeea4fec50bb90619bc9602973119572
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Sun Aug 10 20:50:30 2014 -0700

    seccomp: Replace BUG(!spin_is_locked()) with assert_spin_lock
    
    Current upstream kernel hangs with mips and powerpc targets in
    uniprocessor mode if SECCOMP is configured.
    
    Bisect points to commit dbd952127d11 ("seccomp: introduce writer locking").
    Turns out that code such as
            BUG_ON(!spin_is_locked(&list_lock));
    can not be used in uniprocessor mode because spin_is_locked() always
    returns false in this configuration, and that assert_spin_locked()
    exists for that very purpose and must be used instead.
    
    Fixes: dbd952127d11 ("seccomp: introduce writer locking")
    Cc: Kees Cook <keescook@chromium.org>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1380d8ace334..0cf9cdb6e491 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1105,7 +1105,7 @@ static void copy_seccomp(struct task_struct *p)
 	 * needed because this new task is not yet running and cannot
 	 * be racing exec.
 	 */
-	BUG_ON(!spin_is_locked(&current->sighand->siglock));
+	assert_spin_locked(&current->sighand->siglock);
 
 	/* Ref-count the new filter user, and assign it. */
 	get_seccomp_filter(current);

commit 4bb5f5d9395bc112d93a134d8f5b05611eddc9c0
Author: David Herrmann <dh.herrmann@gmail.com>
Date:   Fri Aug 8 14:25:25 2014 -0700

    mm: allow drivers to prevent new writable mappings
    
    This patch (of 6):
    
    The i_mmap_writable field counts existing writable mappings of an
    address_space.  To allow drivers to prevent new writable mappings, make
    this counter signed and prevent new writable mappings if it is negative.
    This is modelled after i_writecount and DENYWRITE.
    
    This will be required by the shmem-sealing infrastructure to prevent any
    new writable mappings after the WRITE seal has been set.  In case there
    exists a writable mapping, this operation will fail with EBUSY.
    
    Note that we rely on the fact that iff you already own a writable mapping,
    you can increase the counter without using the helpers.  This is the same
    that we do for i_writecount.
    
    Signed-off-by: David Herrmann <dh.herrmann@gmail.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Ryan Lortie <desrt@desrt.ca>
    Cc: Lennart Poettering <lennart@poettering.net>
    Cc: Daniel Mack <zonque@gmail.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fa9124322cd4..1380d8ace334 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -429,7 +429,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 				atomic_dec(&inode->i_writecount);
 			mutex_lock(&mapping->i_mmap_mutex);
 			if (tmp->vm_flags & VM_SHARED)
-				mapping->i_mmap_writable++;
+				atomic_inc(&mapping->i_mmap_writable);
 			flush_dcache_mmap_lock(mapping);
 			/* insert tmp into the share list, just after mpnt */
 			if (unlikely(tmp->vm_flags & VM_NONLINEAR))

commit ab602f799159393143d567e5c04b936fec79d6bd
Author: Jack Miller <millerjo@us.ibm.com>
Date:   Fri Aug 8 14:23:19 2014 -0700

    shm: make exit_shm work proportional to task activity
    
    This is small set of patches our team has had kicking around for a few
    versions internally that fixes tasks getting hung on shm_exit when there
    are many threads hammering it at once.
    
    Anton wrote a simple test to cause the issue:
    
      http://ozlabs.org/~anton/junkcode/bust_shm_exit.c
    
    Before applying this patchset, this test code will cause either hanging
    tracebacks or pthread out of memory errors.
    
    After this patchset, it will still produce output like:
    
      root@somehost:~# ./bust_shm_exit 1024 160
      ...
      INFO: rcu_sched detected stalls on CPUs/tasks: {} (detected by 116, t=2111 jiffies, g=241, c=240, q=7113)
      INFO: Stall ended before state dump start
      ...
    
    But the task will continue to run along happily, so we consider this an
    improvement over hanging, even if it's a bit noisy.
    
    This patch (of 3):
    
    exit_shm obtains the ipc_ns shm rwsem for write and holds it while it
    walks every shared memory segment in the namespace.  Thus the amount of
    work is related to the number of shm segments in the namespace not the
    number of segments that might need to be cleaned.
    
    In addition, this occurs after the task has been notified the thread has
    exited, so the number of tasks waiting for the ns shm rwsem can grow
    without bound until memory is exausted.
    
    Add a list to the task struct of all shmids allocated by this task.  Init
    the list head in copy_process.  Use the ns->rwsem for locking.  Add
    segments after id is added, remove before removing from id.
    
    On unshare of NEW_IPCNS orphan any ids as if the task had exited, similar
    to handling of semaphore undo.
    
    I chose a define for the init sequence since its a simple list init,
    otherwise it would require a function call to avoid include loops between
    the semaphore code and the task struct.  Converting the list_del to
    list_del_init for the unshare cases would remove the exit followed by
    init, but I left it blow up if not inited.
    
    Signed-off-by: Milton Miller <miltonm@bga.com>
    Signed-off-by: Jack Miller <millerjo@us.ibm.com>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Manfred Spraul <manfred@colorfullife.com>
    Cc: Anton Blanchard <anton@samba.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 86da59e165ad..fa9124322cd4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1362,6 +1362,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (retval)
 		goto bad_fork_cleanup_policy;
 	/* copy all the process information */
+	shm_init_task(p);
 	retval = copy_semundo(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_audit;
@@ -1913,6 +1914,11 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 			 */
 			exit_sem(current);
 		}
+		if (unshare_flags & CLONE_NEWIPC) {
+			/* Orphan segments in old ns (see sem above). */
+			exit_shm(current);
+			shm_init_task(current);
+		}
 
 		if (new_nsproxy)
 			switch_task_namespaces(current, new_nsproxy);

commit 33144e8429bd7fceacbb869a7f5061db42e13fe6
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Fri Aug 8 14:22:03 2014 -0700

    kernel/fork.c: make mm_init_owner static
    
    It's only used in fork.c:mm_init().
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index aff84f84b0d3..86da59e165ad 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -535,6 +535,13 @@ static void mm_init_aio(struct mm_struct *mm)
 #endif
 }
 
+static void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
+{
+#ifdef CONFIG_MEMCG
+	mm->owner = p;
+#endif
+}
+
 static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 {
 	mm->mmap = NULL;
@@ -1139,13 +1146,6 @@ static void rt_mutex_init_task(struct task_struct *p)
 #endif
 }
 
-#ifdef CONFIG_MEMCG
-void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
-{
-	mm->owner = p;
-}
-#endif /* CONFIG_MEMCG */
-
 /*
  * Initialize POSIX timer handling for a single task.
  */

commit 4f7d461433bb4a4deee61baefdac6cd1a1ecb546
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Fri Aug 8 14:22:01 2014 -0700

    fork: copy mm's vm usage counters under mmap_sem
    
    If a forking process has a thread calling (un)mmap (silly but still),
    the child process may have some of its mm's vm usage counters (total_vm
    and friends) screwed up, because currently they are copied from oldmm
    w/o holding any locks (memcpy in dup_mm).
    
    This patch moves the counters initialization to dup_mmap() to be called
    under oldmm->mmap_sem, which eliminates any possibility of race.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5a547a59a38a..aff84f84b0d3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -374,6 +374,11 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	 */
 	down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);
 
+	mm->total_vm = oldmm->total_vm;
+	mm->shared_vm = oldmm->shared_vm;
+	mm->exec_vm = oldmm->exec_vm;
+	mm->stack_vm = oldmm->stack_vm;
+
 	rb_link = &mm->mm_rb.rb_node;
 	rb_parent = NULL;
 	pprev = &mm->mmap;

commit ce65cefa5debefc0e81d0a533bda467f0aa67350
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Fri Aug 8 14:21:58 2014 -0700

    fork: reset mm->pinned_vm
    
    mm->pinned_vm counts pages of mm's address space that were permanently
    pinned in memory by increasing their reference counter. The counter was
    introduced by commit bc3e53f682d9 ("mm: distinguish between mlocked and
    pinned pages"), while before it locked_vm had been used for such pages.
    
    Obviously, we should reset the counter on fork if !CLONE_VM, just like
    we do with locked_vm, but currently we don't. Let's fix it.
    
    This patch will fix the contents of /proc/pid/status:VmPin.
    
    ib_umem_get[infiniband] and perf_mmap still check pinned_vm against
    RLIMIT_MEMLOCK.  It's left from the times when pinned pages were accounted
    under locked_vm, but today it looks wrong.  It isn't clear how we should
    deal with it.
    
    We still have some drivers accounting pinned pages under mm->locked_vm -
    this is what commit bc3e53f682d9 was fighting against.  It's
    infiniband/usnic and vfio.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Roland Dreier <roland@kernel.org>
    Cc: Sean Hefty <sean.hefty@intel.com>
    Cc: Hal Rosenstock <hal.rosenstock@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 418b52a9ec6a..5a547a59a38a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -543,6 +543,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	atomic_long_set(&mm->nr_ptes, 0);
 	mm->map_count = 0;
 	mm->locked_vm = 0;
+	mm->pinned_vm = 0;
 	memset(&mm->rss_stat, 0, sizeof(mm->rss_stat));
 	spin_lock_init(&mm->page_table_lock);
 	mm_init_cpumask(mm);

commit 41f727fde1fe40efeb4fef6fdce74ff794be5aeb
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Fri Aug 8 14:21:56 2014 -0700

    fork/exec: cleanup mm initialization
    
    mm initialization on fork/exec is spread all over the place, which makes
    the code look inconsistent.
    
    We have mm_init(), which is supposed to init/nullify mm's internals, but
    it doesn't init all the fields it should:
    
     - on fork ->mmap,mm_rb,vmacache_seqnum,map_count,mm_cpumask,locked_vm
       are zeroed in dup_mmap();
    
     - on fork ->pmd_huge_pte is zeroed in dup_mm(), immediately before
       calling mm_init();
    
     - ->cpu_vm_mask_var ptr is initialized by mm_init_cpumask(), which is
       called before mm_init() on both fork and exec;
    
     - ->context is initialized by init_new_context(), which is called after
       mm_init() on both fork and exec;
    
    Let's consolidate all the initializations in mm_init() to make the code
    look cleaner.
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f6f5086c9e7d..418b52a9ec6a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -374,12 +374,6 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	 */
 	down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);
 
-	mm->locked_vm = 0;
-	mm->mmap = NULL;
-	mm->vmacache_seqnum = 0;
-	mm->map_count = 0;
-	cpumask_clear(mm_cpumask(mm));
-	mm->mm_rb = RB_ROOT;
 	rb_link = &mm->mm_rb.rb_node;
 	rb_parent = NULL;
 	pprev = &mm->mmap;
@@ -538,17 +532,27 @@ static void mm_init_aio(struct mm_struct *mm)
 
 static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 {
+	mm->mmap = NULL;
+	mm->mm_rb = RB_ROOT;
+	mm->vmacache_seqnum = 0;
 	atomic_set(&mm->mm_users, 1);
 	atomic_set(&mm->mm_count, 1);
 	init_rwsem(&mm->mmap_sem);
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->core_state = NULL;
 	atomic_long_set(&mm->nr_ptes, 0);
+	mm->map_count = 0;
+	mm->locked_vm = 0;
 	memset(&mm->rss_stat, 0, sizeof(mm->rss_stat));
 	spin_lock_init(&mm->page_table_lock);
+	mm_init_cpumask(mm);
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
+	mmu_notifier_mm_init(mm);
 	clear_tlb_flush_pending(mm);
+#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
+	mm->pmd_huge_pte = NULL;
+#endif
 
 	if (current->mm) {
 		mm->flags = current->mm->flags & MMF_INIT_MASK;
@@ -558,11 +562,17 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 		mm->def_flags = 0;
 	}
 
-	if (likely(!mm_alloc_pgd(mm))) {
-		mmu_notifier_mm_init(mm);
-		return mm;
-	}
+	if (mm_alloc_pgd(mm))
+		goto fail_nopgd;
+
+	if (init_new_context(p, mm))
+		goto fail_nocontext;
 
+	return mm;
+
+fail_nocontext:
+	mm_free_pgd(mm);
+fail_nopgd:
 	free_mm(mm);
 	return NULL;
 }
@@ -596,7 +606,6 @@ struct mm_struct *mm_alloc(void)
 		return NULL;
 
 	memset(mm, 0, sizeof(*mm));
-	mm_init_cpumask(mm);
 	return mm_init(mm, current);
 }
 
@@ -828,17 +837,10 @@ static struct mm_struct *dup_mm(struct task_struct *tsk)
 		goto fail_nomem;
 
 	memcpy(mm, oldmm, sizeof(*mm));
-	mm_init_cpumask(mm);
 
-#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
-	mm->pmd_huge_pte = NULL;
-#endif
 	if (!mm_init(mm, tsk))
 		goto fail_nomem;
 
-	if (init_new_context(tsk, mm))
-		goto fail_nocontext;
-
 	dup_mm_exe_file(oldmm, mm);
 
 	err = dup_mmap(mm, oldmm);
@@ -860,15 +862,6 @@ static struct mm_struct *dup_mm(struct task_struct *tsk)
 
 fail_nomem:
 	return NULL;
-
-fail_nocontext:
-	/*
-	 * If init_new_context() failed, we cannot use mmput() to free the mm
-	 * because it calls destroy_context()
-	 */
-	mm_free_pgd(mm);
-	free_mm(mm);
-	return NULL;
 }
 
 static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)

commit 747db954cab64c6b7a95b121b517165f34751898
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Aug 8 14:19:24 2014 -0700

    mm: memcontrol: use page lists for uncharge batching
    
    Pages are now uncharged at release time, and all sources of batched
    uncharges operate on lists of pages.  Directly use those lists, and
    get rid of the per-task batching state.
    
    This also batches statistics accounting, in addition to the res
    counter charges, to reduce IRQ-disabling and re-enabling.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fbd3497b221f..f6f5086c9e7d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1346,10 +1346,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_DEBUG_MUTEXES
 	p->blocked_on = NULL; /* not blocked yet */
 #endif
-#ifdef CONFIG_MEMCG
-	p->memcg_batch.do_batch = 0;
-	p->memcg_batch.memcg = NULL;
-#endif
 #ifdef CONFIG_BCACHE
 	p->sequential_io	= 0;
 	p->sequential_io_avg	= 0;

commit bb2cbf5e9367d8598fecd0c48dead69560750223
Merge: e7fda6c4c3c1 478d085524c5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 6 08:06:39 2014 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "In this release:
    
       - PKCS#7 parser for the key management subsystem from David Howells
       - appoint Kees Cook as seccomp maintainer
       - bugfixes and general maintenance across the subsystem"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (94 commits)
      X.509: Need to export x509_request_asymmetric_key()
      netlabel: shorter names for the NetLabel catmap funcs/structs
      netlabel: fix the catmap walking functions
      netlabel: fix the horribly broken catmap functions
      netlabel: fix a problem when setting bits below the previously lowest bit
      PKCS#7: X.509 certificate issuer and subject are mandatory fields in the ASN.1
      tpm: simplify code by using %*phN specifier
      tpm: Provide a generic means to override the chip returned timeouts
      tpm: missing tpm_chip_put in tpm_get_random()
      tpm: Properly clean sysfs entries in error path
      tpm: Add missing tpm_do_selftest to ST33 I2C driver
      PKCS#7: Use x509_request_asymmetric_key()
      Revert "selinux: fix the default socket labeling in sock_graft()"
      X.509: x509_request_asymmetric_keys() doesn't need string length arguments
      PKCS#7: fix sparse non static symbol warning
      KEYS: revert encrypted key change
      ima: add support for measuring and appraising firmware
      firmware_class: perform new LSM checks
      security: introduce kernel_fw_from_file hook
      PKCS#7: Missing inclusion of linux/err.h
      ...

commit e7fda6c4c3c1a7d6996dd75fd84670fa0b5d448f
Merge: 08d69a257144 953dec21aed4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 5 17:46:42 2014 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer and time updates from Thomas Gleixner:
     "A rather large update of timers, timekeeping & co
    
       - Core timekeeping code is year-2038 safe now for 32bit machines.
         Now we just need to fix all in kernel users and the gazillion of
         user space interfaces which rely on timespec/timeval :)
    
       - Better cache layout for the timekeeping internal data structures.
    
       - Proper nanosecond based interfaces for in kernel users.
    
       - Tree wide cleanup of code which wants nanoseconds but does hoops
         and loops to convert back and forth from timespecs.  Some of it
         definitely belongs into the ugly code museum.
    
       - Consolidation of the timekeeping interface zoo.
    
       - A fast NMI safe accessor to clock monotonic for tracing.  This is a
         long standing request to support correlated user/kernel space
         traces.  With proper NTP frequency correction it's also suitable
         for correlation of traces accross separate machines.
    
       - Checkpoint/restart support for timerfd.
    
       - A few NOHZ[_FULL] improvements in the [hr]timer code.
    
       - Code move from kernel to kernel/time of all time* related code.
    
       - New clocksource/event drivers from the ARM universe.  I'm really
         impressed that despite an architected timer in the newer chips SoC
         manufacturers insist on inventing new and differently broken SoC
         specific timers.
    
    [ Ed. "Impressed"? I don't think that word means what you think it means ]
    
       - Another round of code move from arch to drivers.  Looks like most
         of the legacy mess in ARM regarding timers is sorted out except for
         a few obnoxious strongholds.
    
       - The usual updates and fixlets all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (114 commits)
      timekeeping: Fixup typo in update_vsyscall_old definition
      clocksource: document some basic timekeeping concepts
      timekeeping: Use cached ntp_tick_length when accumulating error
      timekeeping: Rework frequency adjustments to work better w/ nohz
      timekeeping: Minor fixup for timespec64->timespec assignment
      ftrace: Provide trace clocks monotonic
      timekeeping: Provide fast and NMI safe access to CLOCK_MONOTONIC
      seqcount: Add raw_write_seqcount_latch()
      seqcount: Provide raw_read_seqcount()
      timekeeping: Use tk_read_base as argument for timekeeping_get_ns()
      timekeeping: Create struct tk_read_base and use it in struct timekeeper
      timekeeping: Restructure the timekeeper some more
      clocksource: Get rid of cycle_last
      clocksource: Move cycle_last validation to core code
      clocksource: Make delta calculation a function
      wireless: ath9k: Get rid of timespec conversions
      drm: vmwgfx: Use nsec based interfaces
      drm: i915: Use nsec based interfaces
      timekeeping: Provide ktime_get_raw()
      hangcheck-timer: Use ktime_get_ns()
      ...

commit ccbf62d8a284cf181ac28c8e8407dd077d90dd4b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 16 21:04:34 2014 +0000

    sched: Make task->start_time nanoseconds based
    
    Simplify the timespec to nsec/usec conversions.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a7ab82db2f60..627b7f80afb0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1262,7 +1262,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	posix_cpu_timers_init(p);
 
-	ktime_get_ts(&p->start_time);
+	p->start_time = ktime_get_ns();
 	p->real_start_time = ktime_get_boot_ns();
 	p->io_context = NULL;
 	p->audit_context = NULL;

commit 57e0be041d9e21a7397eed3b67a7936ac4ac83c0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 16 21:04:32 2014 +0000

    sched: Make task->real_start_time nanoseconds based
    
    Simplify the only user of this data by removing the timespec
    conversion.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: John Stultz <john.stultz@linaro.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8f541930ce26..a7ab82db2f60 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1263,8 +1263,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	posix_cpu_timers_init(p);
 
 	ktime_get_ts(&p->start_time);
-	p->real_start_time = p->start_time;
-	monotonic_to_bootbased(&p->real_start_time);
+	p->real_start_time = ktime_get_boot_ns();
 	p->io_context = NULL;
 	p->audit_context = NULL;
 	if (clone_flags & CLONE_THREAD)

commit dbd952127d11bb44a4ea30b08cc60531b6a23d71
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Jun 27 15:18:48 2014 -0700

    seccomp: introduce writer locking
    
    Normally, task_struct.seccomp.filter is only ever read or modified by
    the task that owns it (current). This property aids in fast access
    during system call filtering as read access is lockless.
    
    Updating the pointer from another task, however, opens up race
    conditions. To allow cross-thread filter pointer updates, writes to the
    seccomp fields are now protected by the sighand spinlock (which is shared
    by all threads in the thread group). Read access remains lockless because
    pointer updates themselves are atomic.  However, writes (or cloning)
    often entail additional checking (like maximum instruction counts)
    which require locking to perform safely.
    
    In the case of cloning threads, the child is invisible to the system
    until it enters the task list. To make sure a child can't be cloned from
    a thread and left in a prior state, seccomp duplication is additionally
    moved under the sighand lock. Then parent and child are certain have
    the same seccomp state when they exit the lock.
    
    Based on patches by Will Drewry and David Drysdale.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Andy Lutomirski <luto@amacapital.net>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6a13c46cd87d..ed4bc339c9dc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -315,6 +315,15 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 		goto free_ti;
 
 	tsk->stack = ti;
+#ifdef CONFIG_SECCOMP
+	/*
+	 * We must handle setting up seccomp filters once we're under
+	 * the sighand lock in case orig has changed between now and
+	 * then. Until then, filter must be NULL to avoid messing up
+	 * the usage counts on the error path calling free_task.
+	 */
+	tsk->seccomp.filter = NULL;
+#endif
 
 	setup_thread_stack(tsk, orig);
 	clear_user_return_notifier(tsk);
@@ -1081,6 +1090,39 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	return 0;
 }
 
+static void copy_seccomp(struct task_struct *p)
+{
+#ifdef CONFIG_SECCOMP
+	/*
+	 * Must be called with sighand->lock held, which is common to
+	 * all threads in the group. Holding cred_guard_mutex is not
+	 * needed because this new task is not yet running and cannot
+	 * be racing exec.
+	 */
+	BUG_ON(!spin_is_locked(&current->sighand->siglock));
+
+	/* Ref-count the new filter user, and assign it. */
+	get_seccomp_filter(current);
+	p->seccomp = current->seccomp;
+
+	/*
+	 * Explicitly enable no_new_privs here in case it got set
+	 * between the task_struct being duplicated and holding the
+	 * sighand lock. The seccomp state and nnp must be in sync.
+	 */
+	if (task_no_new_privs(current))
+		task_set_no_new_privs(p);
+
+	/*
+	 * If the parent gained a seccomp mode after copying thread
+	 * flags and between before we held the sighand lock, we have
+	 * to manually enable the seccomp thread flag here.
+	 */
+	if (p->seccomp.mode != SECCOMP_MODE_DISABLED)
+		set_tsk_thread_flag(p, TIF_SECCOMP);
+#endif
+}
+
 SYSCALL_DEFINE1(set_tid_address, int __user *, tidptr)
 {
 	current->clear_child_tid = tidptr;
@@ -1196,7 +1238,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto fork_out;
 
 	ftrace_graph_init_task(p);
-	get_seccomp_filter(p);
 
 	rt_mutex_init_task(p);
 
@@ -1436,6 +1477,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	spin_lock(&current->sighand->siglock);
 
+	/*
+	 * Copy seccomp details explicitly here, in case they were changed
+	 * before holding sighand lock.
+	 */
+	copy_seccomp(p);
+
 	/*
 	 * Process group and session signals need to be delivered to just the
 	 * parent before the fork or both the parent and the child after the

commit afdb094380889222583df9ef803587f6b8a82c8d
Merge: be11e6d86081 1795cd9b3a91
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 16 21:57:38 2014 +0200

    Merge tag 'v3.16-rc5' into timers/core
    
    Reason: Bring in upstream modifications, so the pending changes which
    depend on them can be queued.

commit d26fad5b38e1c4667d4f2604936e59c837caa54d
Merge: e720fff6341f 1795cd9b3a91
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Jul 16 15:10:07 2014 +0200

    Merge tag 'v3.16-rc5' into sched/core, to refresh the branch before applying bigger tree-wide changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 466af29bf4270e84261712428a1304c28e3743fa
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jun 6 18:52:06 2014 +0200

    sched/deadline: Kill task_struct->pi_top_task
    
    Remove task_struct->pi_top_task. The only user, rt_mutex_setprio(),
    can use a local.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Daeseok Youn <daeseok.youn@gmail.com>
    Cc: Dario Faggioli <raistlin@linux.it>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matthew Dempsky <mdempsky@chromium.org>
    Cc: Michal Simek <michal.simek@xilinx.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20140606165206.GB29465@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d2799d1fc952..6ff87f4429a4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1095,7 +1095,6 @@ static void rt_mutex_init_task(struct task_struct *p)
 	p->pi_waiters = RB_ROOT;
 	p->pi_waiters_leftmost = NULL;
 	p->pi_blocked_on = NULL;
-	p->pi_top_task = NULL;
 #endif
 }
 

commit 4af4206be2bd1933cae20c2b6fb2058dbc887f7c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sun Apr 13 20:58:54 2014 +0200

    tracing: Fix syscall_*regfunc() vs copy_process() race
    
    syscall_regfunc() and syscall_unregfunc() should set/clear
    TIF_SYSCALL_TRACEPOINT system-wide, but do_each_thread() can race
    with copy_process() and miss the new child which was not added to
    the process/thread lists yet.
    
    Change copy_process() to update the child's TIF_SYSCALL_TRACEPOINT
    under tasklist.
    
    Link: http://lkml.kernel.org/p/20140413185854.GB20668@redhat.com
    
    Cc: stable@vger.kernel.org # 2.6.33
    Fixes: a871bd33a6c0 "tracing: Add syscall tracepoints"
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d2799d1fc952..6a13c46cd87d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1487,7 +1487,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	total_forks++;
 	spin_unlock(&current->sighand->siglock);
+	syscall_tracepoint_update(p);
 	write_unlock_irq(&tasklist_lock);
+
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
 	if (clone_flags & CLONE_THREAD)

commit f037c1171db79be2a047b1a5aafa2fd1f05051cb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jun 11 23:59:17 2014 +0000

    fork: Use ktime_get_ts()
    
    do_posix_clock_monotonic_gettime() is a leftover from the initial
    posix timer implementation which maps to ktime_get_ts().
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20140611234607.427408044@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index d2799d1fc952..ea0dd7075543 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1262,7 +1262,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	posix_cpu_timers_init(p);
 
-	do_posix_clock_monotonic_gettime(&p->start_time);
+	ktime_get_ts(&p->start_time);
 	p->real_start_time = p->start_time;
 	monotonic_to_bootbased(&p->real_start_time);
 	p->io_context = NULL;

commit 4e52365f279564cef0ddd41db5237f0471381093
Author: Matthew Dempsky <mdempsky@chromium.org>
Date:   Fri Jun 6 14:36:42 2014 -0700

    ptrace: fix fork event messages across pid namespaces
    
    When tracing a process in another pid namespace, it's important for fork
    event messages to contain the child's pid as seen from the tracer's pid
    namespace, not the parent's.  Otherwise, the tracer won't be able to
    correlate the fork event with later SIGTRAP signals it receives from the
    child.
    
    We still risk a race condition if a ptracer from a different pid
    namespace attaches after we compute the pid_t value.  However, sending a
    bogus fork event message in this unlikely scenario is still a vast
    improvement over the status quo where we always send bogus fork event
    messages to debuggers in a different pid namespace than the forking
    process.
    
    Signed-off-by: Matthew Dempsky <mdempsky@chromium.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Julien Tinnes <jln@chromium.org>
    Cc: Roland McGrath <mcgrathr@chromium.org>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0d53eb0dfb6f..d2799d1fc952 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1606,10 +1606,12 @@ long do_fork(unsigned long clone_flags,
 	 */
 	if (!IS_ERR(p)) {
 		struct completion vfork;
+		struct pid *pid;
 
 		trace_sched_process_fork(current, p);
 
-		nr = task_pid_vnr(p);
+		pid = get_task_pid(p, PIDTYPE_PID);
+		nr = pid_vnr(pid);
 
 		if (clone_flags & CLONE_PARENT_SETTID)
 			put_user(nr, parent_tidptr);
@@ -1624,12 +1626,14 @@ long do_fork(unsigned long clone_flags,
 
 		/* forking complete and child started to run, tell ptracer */
 		if (unlikely(trace))
-			ptrace_event(trace, nr);
+			ptrace_event_pid(trace, pid);
 
 		if (clone_flags & CLONE_VFORK) {
 			if (!wait_for_vfork_done(p, &vfork))
-				ptrace_event(PTRACE_EVENT_VFORK_DONE, nr);
+				ptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);
 		}
+
+		put_pid(pid);
 	} else {
 		nr = PTR_ERR(p);
 	}

commit f98bafa06a28fdfdd5c49f820f4d6560f636fc46
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 4 16:07:34 2014 -0700

    memcg: kill CONFIG_MM_OWNER
    
    CONFIG_MM_OWNER makes no sense.  It is not user-selectable, it is only
    selected by CONFIG_MEMCG automatically.  So we can kill this option in
    init/Kconfig and do s/CONFIG_MM_OWNER/CONFIG_MEMCG/ globally.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 59e3dcc5b8f2..0d53eb0dfb6f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1099,12 +1099,12 @@ static void rt_mutex_init_task(struct task_struct *p)
 #endif
 }
 
-#ifdef CONFIG_MM_OWNER
+#ifdef CONFIG_MEMCG
 void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
 {
 	mm->owner = p;
 }
-#endif /* CONFIG_MM_OWNER */
+#endif /* CONFIG_MEMCG */
 
 /*
  * Initialize POSIX timer handling for a single task.

commit 52383431b37cdbec63944e953ffc2698a7ad9722
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Wed Jun 4 16:06:39 2014 -0700

    mm: get rid of __GFP_KMEMCG
    
    Currently to allocate a page that should be charged to kmemcg (e.g.
    threadinfo), we pass __GFP_KMEMCG flag to the page allocator.  The page
    allocated is then to be freed by free_memcg_kmem_pages.  Apart from
    looking asymmetrical, this also requires intrusion to the general
    allocation path.  So let's introduce separate functions that will
    alloc/free pages charged to kmemcg.
    
    The new functions are called alloc_kmem_pages and free_kmem_pages.  They
    should be used when the caller actually would like to use kmalloc, but
    has to fall back to the page allocator for the allocation is large.
    They only differ from alloc_pages and free_pages in that besides
    allocating or freeing pages they also charge them to the kmem resource
    counter of the current memory cgroup.
    
    [sfr@canb.auug.org.au: export kmalloc_order() to modules]
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Glauber Costa <glommer@gmail.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 54a8d26f612f..59e3dcc5b8f2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -150,15 +150,15 @@ void __weak arch_release_thread_info(struct thread_info *ti)
 static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 						  int node)
 {
-	struct page *page = alloc_pages_node(node, THREADINFO_GFP_ACCOUNTED,
-					     THREAD_SIZE_ORDER);
+	struct page *page = alloc_kmem_pages_node(node, THREADINFO_GFP,
+						  THREAD_SIZE_ORDER);
 
 	return page ? page_address(page) : NULL;
 }
 
 static inline void free_thread_info(struct thread_info *ti)
 {
-	free_memcg_kmem_pages((unsigned long)ti, THREAD_SIZE_ORDER);
+	free_kmem_pages((unsigned long)ti, THREAD_SIZE_ORDER);
 }
 # else
 static struct kmem_cache *thread_info_cache;

commit 52f5684c8e1ec7463192aba8e2916df49807511a
Author: Gideon Israel Dsouza <gidisrael@gmail.com>
Date:   Mon Apr 7 15:39:20 2014 -0700

    kernel: use macros from compiler.h instead of __attribute__((...))
    
    To increase compiler portability there is <linux/compiler.h> which
    provides convenience macros for various gcc constructs.  Eg: __weak for
    __attribute__((weak)).  I've replaced all instances of gcc attributes
    with the right macro in the kernel subsystem.
    
    Signed-off-by: Gideon Israel Dsouza <gidisrael@gmail.com>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e905e9c6b224..54a8d26f612f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -73,6 +73,7 @@
 #include <linux/signalfd.h>
 #include <linux/uprobes.h>
 #include <linux/aio.h>
+#include <linux/compiler.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -286,7 +287,7 @@ void __init fork_init(unsigned long mempages)
 		init_task.signal->rlim[RLIMIT_NPROC];
 }
 
-int __attribute__((weak)) arch_dup_task_struct(struct task_struct *dst,
+int __weak arch_dup_task_struct(struct task_struct *dst,
 					       struct task_struct *src)
 {
 	*dst = *src;

commit f0432d159601f96839f514f286eaa5b75c4112dc
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 7 15:37:30 2014 -0700

    mm, mempolicy: remove per-process flag
    
    PF_MEMPOLICY is an unnecessary optimization for CONFIG_SLAB users.
    There's no significant performance degradation to checking
    current->mempolicy rather than current->flags & PF_MEMPOLICY in the
    allocation path, especially since this is considered unlikely().
    
    Running TCP_RR with netperf-2.4.5 through localhost on 16 cpu machine with
    64GB of memory and without a mempolicy:
    
            threads         before          after
            16              1249409         1244487
            32              1281786         1246783
            48              1239175         1239138
            64              1244642         1241841
            80              1244346         1248918
            96              1266436         1254316
            112             1307398         1312135
            128             1327607         1326502
    
    Per-process flags are a scarce resource so we should free them up whenever
    possible and make them available.  We'll be using it shortly for memcg oom
    reserves.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Tim Hockin <thockin@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c777964c0662..e905e9c6b224 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1276,7 +1276,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		p->mempolicy = NULL;
 		goto bad_fork_cleanup_threadgroup_lock;
 	}
-	mpol_fix_fork_child_flag(p);
 #endif
 #ifdef CONFIG_CPUSETS
 	p->cpuset_mem_spread_rotor = NUMA_NO_NODE;

commit 514ddb446c5c5a238eca32b7052b7a8accae4e93
Author: David Rientjes <rientjes@google.com>
Date:   Mon Apr 7 15:37:27 2014 -0700

    fork: collapse copy_flags into copy_process
    
    copy_flags() does not use the clone_flags formal and can be collapsed
    into copy_process() for cleaner code.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Tim Hockin <thockin@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index bc0e96b78dfd..c777964c0662 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1080,15 +1080,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	return 0;
 }
 
-static void copy_flags(unsigned long clone_flags, struct task_struct *p)
-{
-	unsigned long new_flags = p->flags;
-
-	new_flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);
-	new_flags |= PF_FORKNOEXEC;
-	p->flags = new_flags;
-}
-
 SYSCALL_DEFINE1(set_tid_address, int __user *, tidptr)
 {
 	current->clear_child_tid = tidptr;
@@ -1238,7 +1229,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_count;
 
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
-	copy_flags(clone_flags, p);
+	p->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);
+	p->flags |= PF_FORKNOEXEC;
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);
 	rcu_copy_process(p);

commit 615d6e8756c87149f2d4c1b93d471bca002bd849
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Mon Apr 7 15:37:25 2014 -0700

    mm: per-thread vma caching
    
    This patch is a continuation of efforts trying to optimize find_vma(),
    avoiding potentially expensive rbtree walks to locate a vma upon faults.
    The original approach (https://lkml.org/lkml/2013/11/1/410), where the
    largest vma was also cached, ended up being too specific and random,
    thus further comparison with other approaches were needed.  There are
    two things to consider when dealing with this, the cache hit rate and
    the latency of find_vma().  Improving the hit-rate does not necessarily
    translate in finding the vma any faster, as the overhead of any fancy
    caching schemes can be too high to consider.
    
    We currently cache the last used vma for the whole address space, which
    provides a nice optimization, reducing the total cycles in find_vma() by
    up to 250%, for workloads with good locality.  On the other hand, this
    simple scheme is pretty much useless for workloads with poor locality.
    Analyzing ebizzy runs shows that, no matter how many threads are
    running, the mmap_cache hit rate is less than 2%, and in many situations
    below 1%.
    
    The proposed approach is to replace this scheme with a small per-thread
    cache, maximizing hit rates at a very low maintenance cost.
    Invalidations are performed by simply bumping up a 32-bit sequence
    number.  The only expensive operation is in the rare case of a seq
    number overflow, where all caches that share the same address space are
    flushed.  Upon a miss, the proposed replacement policy is based on the
    page number that contains the virtual address in question.  Concretely,
    the following results are seen on an 80 core, 8 socket x86-64 box:
    
    1) System bootup: Most programs are single threaded, so the per-thread
       scheme does improve ~50% hit rate by just adding a few more slots to
       the cache.
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 50.61%   | 19.90            |
    | patched        | 73.45%   | 13.58            |
    +----------------+----------+------------------+
    
    2) Kernel build: This one is already pretty good with the current
       approach as we're dealing with good locality.
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 75.28%   | 11.03            |
    | patched        | 88.09%   | 9.31             |
    +----------------+----------+------------------+
    
    3) Oracle 11g Data Mining (4k pages): Similar to the kernel build workload.
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 70.66%   | 17.14            |
    | patched        | 91.15%   | 12.57            |
    +----------------+----------+------------------+
    
    4) Ebizzy: There's a fair amount of variation from run to run, but this
       approach always shows nearly perfect hit rates, while baseline is just
       about non-existent.  The amounts of cycles can fluctuate between
       anywhere from ~60 to ~116 for the baseline scheme, but this approach
       reduces it considerably.  For instance, with 80 threads:
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 1.06%    | 91.54            |
    | patched        | 99.97%   | 14.18            |
    +----------------+----------+------------------+
    
    [akpm@linux-foundation.org: fix nommu build, per Davidlohr]
    [akpm@linux-foundation.org: document vmacache_valid() logic]
    [akpm@linux-foundation.org: attempt to untangle header files]
    [akpm@linux-foundation.org: add vmacache_find() BUG_ON]
    [hughd@google.com: add vmacache_valid_mm() (from Oleg)]
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: adjust and enhance comments]
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reviewed-by: Michel Lespinasse <walken@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Tested-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e40c0a01d5a6..bc0e96b78dfd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -28,6 +28,8 @@
 #include <linux/mman.h>
 #include <linux/mmu_notifier.h>
 #include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/vmacache.h>
 #include <linux/nsproxy.h>
 #include <linux/capability.h>
 #include <linux/cpu.h>
@@ -364,7 +366,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 
 	mm->locked_vm = 0;
 	mm->mmap = NULL;
-	mm->mmap_cache = NULL;
+	mm->vmacache_seqnum = 0;
 	mm->map_count = 0;
 	cpumask_clear(mm_cpumask(mm));
 	mm->mm_rb = RB_ROOT;
@@ -882,6 +884,9 @@ static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 	if (!oldmm)
 		return 0;
 
+	/* initialize the new vmacache entries */
+	vmacache_flush(tsk);
+
 	if (clone_flags & CLONE_VM) {
 		atomic_inc(&oldmm->mm_users);
 		mm = oldmm;

commit a0715cc22601e8830ace98366c0c2bd8da52af52
Author: Alex Thorlton <athorlton@sgi.com>
Date:   Mon Apr 7 15:37:10 2014 -0700

    mm, thp: add VM_INIT_DEF_MASK and PRCTL_THP_DISABLE
    
    Add VM_INIT_DEF_MASK, to allow us to set the default flags for VMs.  It
    also adds a prctl control which allows us to set the THP disable bit in
    mm->def_flags so that VMs will pick up the setting as they are created.
    
    Signed-off-by: Alex Thorlton <athorlton@sgi.com>
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index abc45890f0a5..e40c0a01d5a6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -530,8 +530,6 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	atomic_set(&mm->mm_count, 1);
 	init_rwsem(&mm->mmap_sem);
 	INIT_LIST_HEAD(&mm->mmlist);
-	mm->flags = (current->mm) ?
-		(current->mm->flags & MMF_INIT_MASK) : default_dump_filter;
 	mm->core_state = NULL;
 	atomic_long_set(&mm->nr_ptes, 0);
 	memset(&mm->rss_stat, 0, sizeof(mm->rss_stat));
@@ -540,8 +538,15 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	mm_init_owner(mm, p);
 	clear_tlb_flush_pending(mm);
 
-	if (likely(!mm_alloc_pgd(mm))) {
+	if (current->mm) {
+		mm->flags = current->mm->flags & MMF_INIT_MASK;
+		mm->def_flags = current->mm->def_flags & VM_INIT_DEF_MASK;
+	} else {
+		mm->flags = default_dump_filter;
 		mm->def_flags = 0;
+	}
+
+	if (likely(!mm_alloc_pgd(mm))) {
 		mmu_notifier_mm_init(mm);
 		return mm;
 	}

commit 32d01dc7be4e725ab85ce1d74e8f4adc02ad68dd
Merge: 68114e5eb862 1ec41830e087
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 3 13:05:42 2014 -0700

    Merge branch 'for-3.15' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "A lot updates for cgroup:
    
       - The biggest one is cgroup's conversion to kernfs.  cgroup took
         after the long abandoned vfs-entangled sysfs implementation and
         made it even more convoluted over time.  cgroup's internal objects
         were fused with vfs objects which also brought in vfs locking and
         object lifetime rules.  Naturally, there are places where vfs rules
         don't fit and nasty hacks, such as credential switching or lock
         dance interleaving inode mutex and cgroup_mutex with object serial
         number comparison thrown in to decide whether the operation is
         actually necessary, needed to be employed.
    
         After conversion to kernfs, internal object lifetime and locking
         rules are mostly isolated from vfs interactions allowing shedding
         of several nasty hacks and overall simplification.  This will also
         allow implmentation of operations which may affect multiple cgroups
         which weren't possible before as it would have required nesting
         i_mutexes.
    
       - Various simplifications including dropping of module support,
         easier cgroup name/path handling, simplified cgroup file type
         handling and task_cg_lists optimization.
    
       - Prepatory changes for the planned unified hierarchy, which is still
         a patchset away from being actually operational.  The dummy
         hierarchy is updated to serve as the default unified hierarchy.
         Controllers which aren't claimed by other hierarchies are
         associated with it, which BTW was what the dummy hierarchy was for
         anyway.
    
       - Various fixes from Li and others.  This pull request includes some
         patches to add missing slab.h to various subsystems.  This was
         triggered xattr.h include removal from cgroup.h.  cgroup.h
         indirectly got included a lot of files which brought in xattr.h
         which brought in slab.h.
    
      There are several merge commits - one to pull in kernfs updates
      necessary for converting cgroup (already in upstream through
      driver-core), others for interfering changes in the fixes branch"
    
    * 'for-3.15' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (74 commits)
      cgroup: remove useless argument from cgroup_exit()
      cgroup: fix spurious lockdep warning in cgroup_exit()
      cgroup: Use RCU_INIT_POINTER(x, NULL) in cgroup.c
      cgroup: break kernfs active_ref protection in cgroup directory operations
      cgroup: fix cgroup_taskset walking order
      cgroup: implement CFTYPE_ONLY_ON_DFL
      cgroup: make cgrp_dfl_root mountable
      cgroup: drop const from @buffer of cftype->write_string()
      cgroup: rename cgroup_dummy_root and related names
      cgroup: move ->subsys_mask from cgroupfs_root to cgroup
      cgroup: treat cgroup_dummy_root as an equivalent hierarchy during rebinding
      cgroup: remove NULL checks from [pr_cont_]cgroup_{name|path}()
      cgroup: use cgroup_setup_root() to initialize cgroup_dummy_root
      cgroup: reorganize cgroup bootstrapping
      cgroup: relocate setting of CGRP_DEAD
      cpuset: use rcu_read_lock() to protect task_cs()
      cgroup_freezer: document freezer_fork() subtleties
      cgroup: update cgroup_transfer_tasks() to either succeed or fail
      cgroup: drop task_lock() protection around task->cgroups
      cgroup: update how a newly forked task gets associated with css_set
      ...

commit e8604cb43690b781f9a7ad4a770f3e10259fe939
Author: Li Zefan <lizefan@huawei.com>
Date:   Fri Mar 28 15:18:27 2014 +0800

    cgroup: fix spurious lockdep warning in cgroup_exit()
    
    cgroup_exit() is called in fork and exit path. If it's called in the
    failure path during fork, PF_EXITING isn't set, and then lockdep will
    complain.
    
    Fix this by removing cgroup_exit() in that failure path. cgroup_fork()
    does nothing that needs cleanup.
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a17621c6cd42..8852b3463ab7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1271,7 +1271,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (IS_ERR(p->mempolicy)) {
 		retval = PTR_ERR(p->mempolicy);
 		p->mempolicy = NULL;
-		goto bad_fork_cleanup_cgroup;
+		goto bad_fork_cleanup_threadgroup_lock;
 	}
 	mpol_fix_fork_child_flag(p);
 #endif
@@ -1524,11 +1524,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	perf_event_free_task(p);
 #ifdef CONFIG_NUMA
 	mpol_put(p->mempolicy);
-bad_fork_cleanup_cgroup:
+bad_fork_cleanup_threadgroup_lock:
 #endif
 	if (clone_flags & CLONE_THREAD)
 		threadgroup_change_end(current);
-	cgroup_exit(p, 0);
 	delayacct_tsk_free(p);
 	module_put(task_thread_info(p)->exec_domain->module);
 bad_fork_cleanup_count:

commit 156654f491dd8d52687a5fbe1637f472a52ce75b
Author: Mike Galbraith <bitbucket@online.de>
Date:   Fri Feb 28 07:23:11 2014 +0100

    sched/numa: Move task_numa_free() to __put_task_struct()
    
    Bad idea on -rt:
    
    [  908.026136]  [<ffffffff8150ad6a>] rt_spin_lock_slowlock+0xaa/0x2c0
    [  908.026145]  [<ffffffff8108f701>] task_numa_free+0x31/0x130
    [  908.026151]  [<ffffffff8108121e>] finish_task_switch+0xce/0x100
    [  908.026156]  [<ffffffff81509c0a>] thread_return+0x48/0x4ae
    [  908.026160]  [<ffffffff8150a095>] schedule+0x25/0xa0
    [  908.026163]  [<ffffffff8150ad95>] rt_spin_lock_slowlock+0xd5/0x2c0
    [  908.026170]  [<ffffffff810658cf>] get_signal_to_deliver+0xaf/0x680
    [  908.026175]  [<ffffffff8100242d>] do_signal+0x3d/0x5b0
    [  908.026179]  [<ffffffff81002a30>] do_notify_resume+0x90/0xe0
    [  908.026186]  [<ffffffff81513176>] int_signal+0x12/0x17
    [  908.026193]  [<00007ff2a388b1d0>] 0x7ff2a388b1cf
    
    and since upstream does not mind where we do this, be a bit nicer ...
    
    Signed-off-by: Mike Galbraith <bitbucket@online.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Mel Gorman <mgorman@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1393568591.6018.27.camel@marge.simpson.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a17621c6cd42..332688e5e7b4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -237,6 +237,7 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
+	task_numa_free(tsk);
 	security_task_free(tsk);
 	exit_creds(tsk);
 	delayacct_tsk_free(tsk);

commit 98611e4e6a2b4a03fd2d4750cce8e4455a995c8d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jan 23 15:55:52 2014 -0800

    exec: kill task_struct->did_exec
    
    We can kill either task->did_exec or PF_FORKNOEXEC, they are mutually
    exclusive.  The patch kills ->did_exec because it has a single user.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Kees Cook <keescook@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b6dd0bbf4240..a17621c6cd42 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1226,7 +1226,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (!try_module_get(task_thread_info(p)->exec_domain->module))
 		goto bad_fork_cleanup_count;
 
-	p->did_exec = 0;
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	copy_flags(clone_flags, p);
 	INIT_LIST_HEAD(&p->children);

commit 68ce670b6e8edc30551862e7f6a306e45389e189
Author: Daeseok Youn <daeseok.youn@gmail.com>
Date:   Thu Jan 23 15:55:48 2014 -0800

    kernel/fork.c: remove redundant NULL check in dup_mm()
    
    current->mm doesn't need a NULL check in dup_mm().  Becasue dup_mm() is
    used only in copy_mm() and current->mm is checked whether it is NULL or
    not in copy_mm() before calling dup_mm().
    
    Signed-off-by: Daeseok Youn <daeseok.youn@gmail.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 01ccc6109918..b6dd0bbf4240 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -805,9 +805,6 @@ static struct mm_struct *dup_mm(struct task_struct *tsk)
 	struct mm_struct *mm, *oldmm = current->mm;
 	int err;
 
-	if (!oldmm)
-		return NULL;
-
 	mm = allocate_mm();
 	if (!mm)
 		goto fail_nomem;

commit 5d59e18270d4769c9160c282b25c00b6fc004ffb
Author: Daeseok Youn <daeseok.youn@gmail.com>
Date:   Thu Jan 23 15:55:47 2014 -0800

    kernel/fork.c: fix coding style issues
    
    Fix errors reported by checkpatch.pl.  One error is parentheses, the other
    is a whitespace issue.
    
    Signed-off-by: Daeseok Youn <daeseok.youn@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5615ead014e3..01ccc6109918 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1654,7 +1654,7 @@ SYSCALL_DEFINE0(fork)
 	return do_fork(SIGCHLD, 0, 0, NULL, NULL);
 #else
 	/* can not support in nommu mode */
-	return(-EINVAL);
+	return -EINVAL;
 #endif
 }
 #endif
@@ -1662,7 +1662,7 @@ SYSCALL_DEFINE0(fork)
 #ifdef __ARCH_WANT_SYS_VFORK
 SYSCALL_DEFINE0(vfork)
 {
-	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0, 
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0,
 			0, NULL, NULL);
 }
 #endif

commit ff252c1fc537b0c9e40f62da0a9d11bf0737b7db
Author: DaeSeok Youn <daeseok.youn@gmail.com>
Date:   Thu Jan 23 15:55:46 2014 -0800

    kernel/fork.c: make dup_mm() static
    
    dup_mm() is used only in kernel/fork.c
    
    Signed-off-by: Daeseok Youn <daeseok.youn@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2f11bbe376b0..5615ead014e3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -800,7 +800,7 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
  * Allocate a new mm structure and copy contents from the
  * mm structure of the passed in task structure.
  */
-struct mm_struct *dup_mm(struct task_struct *tsk)
+static struct mm_struct *dup_mm(struct task_struct *tsk)
 {
 	struct mm_struct *mm, *oldmm = current->mm;
 	int err;

commit 0c740d0afc3bff0a097ad03a1c8df92757516f5c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jan 21 15:49:56 2014 -0800

    introduce for_each_thread() to replace the buggy while_each_thread()
    
    while_each_thread() and next_thread() should die, almost every lockless
    usage is wrong.
    
    1. Unless g == current, the lockless while_each_thread() is not safe.
    
       while_each_thread(g, t) can loop forever if g exits, next_thread()
       can't reach the unhashed thread in this case. Note that this can
       happen even if g is the group leader, it can exec.
    
    2. Even if while_each_thread() itself was correct, people often use
       it wrongly.
    
       It was never safe to just take rcu_read_lock() and loop unless
       you verify that pid_alive(g) == T, even the first next_thread()
       can point to the already freed/reused memory.
    
    This patch adds signal_struct->thread_head and task->thread_node to
    create the normal rcu-safe list with the stable head.  The new
    for_each_thread(g, t) helper is always safe under rcu_read_lock() as
    long as this task_struct can't go away.
    
    Note: of course it is ugly to have both task_struct->thread_node and the
    old task_struct->thread_group, we will kill it later, after we change
    the users of while_each_thread() to use for_each_thread().
    
    Perhaps we can kill it even before we convert all users, we can
    reimplement next_thread(t) using the new thread_head/thread_node.  But
    we can't do this right now because this will lead to subtle behavioural
    changes.  For example, do/while_each_thread() always sees at least one
    task, while for_each_thread() can do nothing if the whole thread group
    has died.  Or thread_group_empty(), currently its semantics is not clear
    unless thread_group_leader(p) and we need to audit the callers before we
    can change it.
    
    So this patch adds the new interface which has to coexist with the old
    one for some time, hopefully the next changes will be more or less
    straightforward and the old one will go away soon.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Sergey Dyasly <dserrg@gmail.com>
    Tested-by: Sergey Dyasly <dserrg@gmail.com>
    Reviewed-by: Sameer Nanda <snanda@chromium.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mandeep Singh Baines <msb@chromium.org>
    Cc: "Ma, Xindong" <xindong.ma@intel.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: "Tu, Xiaobing" <xiaobing.tu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 294189fc7ac8..2f11bbe376b0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1035,6 +1035,11 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->nr_threads = 1;
 	atomic_set(&sig->live, 1);
 	atomic_set(&sig->sigcnt, 1);
+
+	/* list_add(thread_node, thread_head) without INIT_LIST_HEAD() */
+	sig->thread_head = (struct list_head)LIST_HEAD_INIT(tsk->thread_node);
+	tsk->thread_node = (struct list_head)LIST_HEAD_INIT(sig->thread_head);
+
 	init_waitqueue_head(&sig->wait_chldexit);
 	sig->curr_target = tsk;
 	init_sigpending(&sig->shared_pending);
@@ -1474,6 +1479,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			atomic_inc(&current->signal->sigcnt);
 			list_add_tail_rcu(&p->thread_group,
 					  &p->group_leader->thread_group);
+			list_add_tail_rcu(&p->thread_node,
+					  &p->signal->thread_head);
 		}
 		attach_pid(p, PIDTYPE_PID);
 		nr_threads++;

commit a0fa1dd3cdbccec9597fe53b6177a9aa6e20f2f8
Merge: 9326657abe1a eaad45132c56
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 20 10:42:08 2014 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
    
     - Add the initial implementation of SCHED_DEADLINE support: a real-time
       scheduling policy where tasks that meet their deadlines and
       periodically execute their instances in less than their runtime quota
       see real-time scheduling and won't miss any of their deadlines.
       Tasks that go over their quota get delayed (Available to privileged
       users for now)
    
     - Clean up and fix preempt_enable_no_resched() abuse all around the
       tree
    
     - Do sched_clock() performance optimizations on x86 and elsewhere
    
     - Fix and improve auto-NUMA balancing
    
     - Fix and clean up the idle loop
    
     - Apply various cleanups and fixes
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (60 commits)
      sched: Fix __sched_setscheduler() nice test
      sched: Move SCHED_RESET_ON_FORK into attr::sched_flags
      sched: Fix up attr::sched_priority warning
      sched: Fix up scheduler syscall LTP fails
      sched: Preserve the nice level over sched_setscheduler() and sched_setparam() calls
      sched/core: Fix htmldocs warnings
      sched/deadline: No need to check p if dl_se is valid
      sched/deadline: Remove unused variables
      sched/deadline: Fix sparse static warnings
      m68k: Fix build warning in mac_via.h
      sched, thermal: Clean up preempt_enable_no_resched() abuse
      sched, net: Fixup busy_loop_us_clock()
      sched, net: Clean up preempt_enable_no_resched() abuse
      sched/preempt: Fix up missed PREEMPT_NEED_RESCHED folding
      sched/preempt, locking: Rework local_bh_{dis,en}able()
      sched/clock, x86: Avoid a runtime condition in native_sched_clock()
      sched/clock: Fix up clear_sched_clock_stable()
      sched/clock, x86: Use a static_key for sched_clock_stable
      sched/clock: Remove local_irq_disable() from the clocks
      sched/clock, x86: Rewrite cyc2ns() to avoid the need to disable IRQs
      ...

commit 48ba620aab90f4c7e9bb002e2f30863a4ea0f915
Merge: 8f211b6ccc01 41301ae78a99
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 17 17:29:36 2014 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull namespace fixes from Eric Biederman:
     "This is a set of 3 regression fixes.
    
      This fixes /proc/mounts when using "ip netns add <netns>" to display
      the actual mount point.
    
      This fixes a regression in clone that broke lxc-attach.
    
      This fixes a regression in the permission checks for mounting /proc
      that made proc unmountable if binfmt_misc was in use.  Oops.
    
      My apologies for sending this pull request so late.  Al Viro gave
      interesting review comments about the d_path fix that I wanted to
      address in detail before I sent this pull request.  Unfortunately a
      bad round of colds kept from addressing that in detail until today.
      The executive summary of the review was:
    
      Al: Is patching d_path really sufficient?
          The prepend_path, d_path, d_absolute_path, and __d_path family of
          functions is a really mess.
    
      Me: Yes, patching d_path is really sufficient.  Yes, the code is mess.
          No it is not appropriate to rewrite all of d_path for a regression
          that has existed for entirely too long already, when a two line
          change will do"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      vfs: Fix a regression in mounting proc
      fork:  Allow CLONE_PARENT after setns(CLONE_NEWPID)
      vfs: In d_path don't call d_dname on a mount point

commit 2d3d891d3344159d5b452a645e355bbe29591e8b
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 7 14:43:44 2013 +0100

    sched/deadline: Add SCHED_DEADLINE inheritance logic
    
    Some method to deal with rt-mutexes and make sched_dl interact with
    the current PI-coded is needed, raising all but trivial issues, that
    needs (according to us) to be solved with some restructuring of
    the pi-code (i.e., going toward a proxy execution-ish implementation).
    
    This is under development, in the meanwhile, as a temporary solution,
    what this commits does is:
    
     - ensure a pi-lock owner with waiters is never throttled down. Instead,
       when it runs out of runtime, it immediately gets replenished and it's
       deadline is postponed;
    
     - the scheduling parameters (relative deadline and default runtime)
       used for that replenishments --during the whole period it holds the
       pi-lock-- are the ones of the waiting task with earliest deadline.
    
    Acting this way, we provide some kind of boosting to the lock-owner,
    still by using the existing (actually, slightly modified by the previous
    commit) pi-architecture.
    
    We would stress the fact that this is only a surely needed, all but
    clean solution to the problem. In the end it's only a way to re-start
    discussion within the community. So, as always, comments, ideas, rants,
    etc.. are welcome! :-)
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    [ Added !RT_MUTEXES build fix. ]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-11-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7049ae526a54..01b450a61abd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1090,6 +1090,7 @@ static void rt_mutex_init_task(struct task_struct *p)
 	p->pi_waiters = RB_ROOT;
 	p->pi_waiters_leftmost = NULL;
 	p->pi_blocked_on = NULL;
+	p->pi_top_task = NULL;
 #endif
 }
 

commit fb00aca474405f4fa8a8519c3179fed722eabd83
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 7 14:43:43 2013 +0100

    rtmutex: Turn the plist into an rb-tree
    
    Turn the pi-chains from plist to rb-tree, in the rt_mutex code,
    and provide a proper comparison function for -deadline and
    -priority tasks.
    
    This is done mainly because:
     - classical prio field of the plist is just an int, which might
       not be enough for representing a deadline;
     - manipulating such a list would become O(nr_deadline_tasks),
       which might be to much, as the number of -deadline task increases.
    
    Therefore, an rb-tree is used, and tasks are queued in it according
    to the following logic:
     - among two -priority (i.e., SCHED_BATCH/OTHER/RR/FIFO) tasks, the
       one with the higher (lower, actually!) prio wins;
     - among a -priority and a -deadline task, the latter always wins;
     - among two -deadline tasks, the one with the earliest deadline
       wins.
    
    Queueing and dequeueing functions are changed accordingly, for both
    the list of a task's pi-waiters and the list of tasks blocked on
    a pi-lock.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-again-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-10-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e6c0f1a22914..7049ae526a54 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1087,7 +1087,8 @@ static void rt_mutex_init_task(struct task_struct *p)
 {
 	raw_spin_lock_init(&p->pi_lock);
 #ifdef CONFIG_RT_MUTEXES
-	plist_head_init(&p->pi_waiters);
+	p->pi_waiters = RB_ROOT;
+	p->pi_waiters_leftmost = NULL;
 	p->pi_blocked_on = NULL;
 #endif
 }

commit aab03e05e8f7e26f51dee792beddcb5cca9215a5
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 28 11:14:43 2013 +0100

    sched/deadline: Add SCHED_DEADLINE structures & implementation
    
    Introduces the data structures, constants and symbols needed for
    SCHED_DEADLINE implementation.
    
    Core data structure of SCHED_DEADLINE are defined, along with their
    initializers. Hooks for checking if a task belong to the new policy
    are also added where they are needed.
    
    Adds a scheduling class, in sched/dl.c and a new policy called
    SCHED_DEADLINE. It is an implementation of the Earliest Deadline
    First (EDF) scheduling algorithm, augmented with a mechanism (called
    Constant Bandwidth Server, CBS) that makes it possible to isolate
    the behaviour of tasks between each other.
    
    The typical -deadline task will be made up of a computation phase
    (instance) which is activated on a periodic or sporadic fashion. The
    expected (maximum) duration of such computation is called the task's
    runtime; the time interval by which each instance need to be completed
    is called the task's relative deadline. The task's absolute deadline
    is dynamically calculated as the time instant a task (better, an
    instance) activates plus the relative deadline.
    
    The EDF algorithms selects the task with the smallest absolute
    deadline as the one to be executed first, while the CBS ensures each
    task to run for at most its runtime every (relative) deadline
    length time interval, avoiding any interference between different
    tasks (bandwidth isolation).
    Thanks to this feature, also tasks that do not strictly comply with
    the computational model sketched above can effectively use the new
    policy.
    
    To summarize, this patch:
     - introduces the data structures, constants and symbols needed;
     - implements the core logic of the scheduling algorithm in the new
       scheduling class file;
     - provides all the glue code between the new scheduling class and
       the core scheduler and refines the interactions between sched/dl
       and the other existing scheduling classes.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Michael Trimarchi <michael@amarulasolutions.com>
    Signed-off-by: Fabio Checconi <fchecconi@gmail.com>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-4-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6023d150a305..e6c0f1a22914 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1311,7 +1311,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */
-	sched_fork(clone_flags, p);
+	retval = sched_fork(clone_flags, p);
+	if (retval)
+		goto bad_fork_cleanup_policy;
 
 	retval = perf_event_init_task(p);
 	if (retval)

commit 56b4811039174bba9cbd68318d0d8b1585b9eded
Merge: e777b63bbd58 9722c2dac708
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jan 13 13:35:28 2014 +0100

    Merge branch 'sched/urgent' into sched/core
    
    Pick up the latest fixes before applying new changes.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 20841405940e7be0617612d521e206e4b6b325db
Author: Rik van Riel <riel@redhat.com>
Date:   Wed Dec 18 17:08:44 2013 -0800

    mm: fix TLB flush race between migration, and change_protection_range
    
    There are a few subtle races, between change_protection_range (used by
    mprotect and change_prot_numa) on one side, and NUMA page migration and
    compaction on the other side.
    
    The basic race is that there is a time window between when the PTE gets
    made non-present (PROT_NONE or NUMA), and the TLB is flushed.
    
    During that time, a CPU may continue writing to the page.
    
    This is fine most of the time, however compaction or the NUMA migration
    code may come in, and migrate the page away.
    
    When that happens, the CPU may continue writing, through the cached
    translation, to what is no longer the current memory location of the
    process.
    
    This only affects x86, which has a somewhat optimistic pte_accessible.
    All other architectures appear to be safe, and will either always flush,
    or flush whenever there is a valid mapping, even with no permissions
    (SPARC).
    
    The basic race looks like this:
    
    CPU A                   CPU B                   CPU C
    
                                                    load TLB entry
    make entry PTE/PMD_NUMA
                            fault on entry
                                                    read/write old page
                            start migrating page
                            change PTE/PMD to new page
                                                    read/write old page [*]
    flush TLB
                                                    reload TLB from new entry
                                                    read/write new page
                                                    lose data
    
    [*] the old page may belong to a new user at this point!
    
    The obvious fix is to flush remote TLB entries, by making sure that
    pte_accessible aware of the fact that PROT_NONE and PROT_NUMA memory may
    still be accessible if there is a TLB flush pending for the mm.
    
    This should fix both NUMA migration and compaction.
    
    [mgorman@suse.de: fix build]
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 728d5be9548c..5721f0e3f2da 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -537,6 +537,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	spin_lock_init(&mm->page_table_lock);
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
+	clear_tlb_flush_pending(mm);
 
 	if (likely(!mm_alloc_pgd(mm))) {
 		mm->def_flags = 0;

commit bb8cbbfee68518796df4050868e5b0f5ad078f9f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Nov 13 15:36:12 2013 +0100

    tasks/fork: Remove unnecessary child->exit_state
    
    A zombie task obviously can't fork(), remove the unnecessary
    initialization of child->exit_state. It is zero anyway after
    dup_task_struct().
    
    Note: copy_process() is huge and it has a lot of chaotic
    initializations, probably it makes sense to move them into the
    new helper called by dup_task_struct().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: David Laight <David.Laight@ACULAB.COM>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20131113143612.GA10540@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 728d5be9548c..b3080823a24d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1402,13 +1402,11 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		p->tgid = p->pid;
 	}
 
-	p->pdeath_signal = 0;
-	p->exit_state = 0;
-
 	p->nr_dirtied = 0;
 	p->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);
 	p->dirty_paused_when = 0;
 
+	p->pdeath_signal = 0;
 	INIT_LIST_HEAD(&p->thread_group);
 	p->task_works = NULL;
 

commit 1f7f4dde5c945f41a7abc2285be43d918029ecc5
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Nov 14 21:10:16 2013 -0800

    fork:  Allow CLONE_PARENT after setns(CLONE_NEWPID)
    
    Serge Hallyn <serge.hallyn@ubuntu.com> writes:
    > Hi Oleg,
    >
    > commit 40a0d32d1eaffe6aac7324ca92604b6b3977eb0e :
    > "fork: unify and tighten up CLONE_NEWUSER/CLONE_NEWPID checks"
    > breaks lxc-attach in 3.12.  That code forks a child which does
    > setns() and then does a clone(CLONE_PARENT).  That way the
    > grandchild can be in the right namespaces (which the child was
    > not) and be a child of the original task, which is the monitor.
    >
    > lxc-attach in 3.11 was working fine with no side effects that I
    > could see.  Is there a real danger in allowing CLONE_PARENT
    > when current->nsproxy->pidns_for_children is not our pidns,
    > or was this done out of an "over-abundance of caution"?  Can we
    > safely revert that new extra check?
    
    The two fundamental things I know we can not allow are:
    - A shared signal queue aka CLONE_THREAD.  Because we compute the pid
      and uid of the signal when we place it in the queue.
    
    - Changing the pid and by extention pid_namespace of an existing
      process.
    
    From a parents perspective there is nothing special about the pid
    namespace, to deny CLONE_PARENT, because the parent simply won't know or
    care.
    
    From the childs perspective all that is special really are shared signal
    queues.
    
    User mode threading with CLONE_PARENT|CLONE_VM|CLONE_SIGHAND and tasks
    in different pid namespaces is almost certainly going to break because
    it is complicated.  But shared signal handlers can look at per thread
    information to know which pid namespace a process is in, so I don't know
    of any reason not to support CLONE_PARENT|CLONE_VM|CLONE_SIGHAND threads
    at the kernel level.  It would be absolutely stupid to implement but
    that is a different thing.
    
    So hmm.
    
    Because it can do no harm, and because it is a regression let's remove
    the CLONE_PARENT check and send it stable.
    
    Cc: stable@vger.kernel.org
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Acked-by: Serge E. Hallyn <serge.hallyn@ubuntu.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 728d5be9548c..f82fa2ee7581 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1171,7 +1171,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * do not allow it to share a thread group or signal handlers or
 	 * parent with the forking task.
 	 */
-	if (clone_flags & (CLONE_SIGHAND | CLONE_PARENT)) {
+	if (clone_flags & CLONE_SIGHAND) {
 		if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
 		    (task_active_pid_ns(current) !=
 				current->nsproxy->pid_ns_for_children))

commit e009bb30c8df8a52a9622b616b67436b6a03a0cd
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Nov 14 14:31:07 2013 -0800

    mm: implement split page table lock for PMD level
    
    The basic idea is the same as with PTE level: the lock is embedded into
    struct page of table's page.
    
    We can't use mm->pmd_huge_pte to store pgtables for THP, since we don't
    take mm->page_table_lock anymore.  Let's reuse page->lru of table's page
    for that.
    
    pgtable_pmd_page_ctor() returns true, if initialization is successful
    and false otherwise.  Current implementation never fails, but assumption
    that constructor can fail will help to port it to -rt where spinlock_t
    is rather huge and cannot be embedded into struct page -- dynamic
    allocation is required.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Tested-by: Alex Thorlton <athorlton@sgi.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "Eric W . Biederman" <ebiederm@xmission.com>
    Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Robin Holt <robinmholt@gmail.com>
    Cc: Sedat Dilek <sedat.dilek@gmail.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Hugh Dickins <hughd@google.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e2520756e005..728d5be9548c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -560,7 +560,7 @@ static void check_mm(struct mm_struct *mm)
 					  "mm:%p idx:%d val:%ld\n", mm, i, x);
 	}
 
-#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	VM_BUG_ON(mm->pmd_huge_pte);
 #endif
 }
@@ -814,7 +814,7 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	memcpy(mm, oldmm, sizeof(*mm));
 	mm_init_cpumask(mm);
 
-#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
 	mm->pmd_huge_pte = NULL;
 #endif
 	if (!mm_init(mm, tsk))

commit e1f56c89b040134add93f686931cc266541d239a
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Nov 14 14:30:48 2013 -0800

    mm: convert mm->nr_ptes to atomic_long_t
    
    With split page table lock for PMD level we can't hold mm->page_table_lock
    while updating nr_ptes.
    
    Let's convert it to atomic_long_t to avoid races.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Tested-by: Alex Thorlton <athorlton@sgi.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: "Eric W . Biederman" <ebiederm@xmission.com>
    Cc: "Paul E . McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Robin Holt <robinmholt@gmail.com>
    Cc: Sedat Dilek <sedat.dilek@gmail.com>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f6d11fc67f72..e2520756e005 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -532,7 +532,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	mm->flags = (current->mm) ?
 		(current->mm->flags & MMF_INIT_MASK) : default_dump_filter;
 	mm->core_state = NULL;
-	mm->nr_ptes = 0;
+	atomic_long_set(&mm->nr_ptes, 0);
 	memset(&mm->rss_stat, 0, sizeof(mm->rss_stat));
 	spin_lock_init(&mm->page_table_lock);
 	mm_init_aio(mm);

commit 39cf275a1a18ba3c7eb9b986c5c9b35b57332798
Merge: ad5d69899e52 e5137b50a064
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 12 10:20:12 2013 +0900

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The main changes in this cycle are:
    
       - (much) improved CONFIG_NUMA_BALANCING support from Mel Gorman, Rik
         van Riel, Peter Zijlstra et al.  Yay!
    
       - optimize preemption counter handling: merge the NEED_RESCHED flag
         into the preempt_count variable, by Peter Zijlstra.
    
       - wait.h fixes and code reorganization from Peter Zijlstra
    
       - cfs_bandwidth fixes from Ben Segall
    
       - SMP load-balancer cleanups from Peter Zijstra
    
       - idle balancer improvements from Jason Low
    
       - other fixes and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (129 commits)
      ftrace, sched: Add TRACE_FLAG_PREEMPT_RESCHED
      stop_machine: Fix race between stop_two_cpus() and stop_cpus()
      sched: Remove unnecessary iteration over sched domains to update nr_busy_cpus
      sched: Fix asymmetric scheduling for POWER7
      sched: Move completion code from core.c to completion.c
      sched: Move wait code from core.c to wait.c
      sched: Move wait.c into kernel/sched/
      sched/wait: Fix __wait_event_interruptible_lock_irq_timeout()
      sched: Avoid throttle_cfs_rq() racing with period_timer stopping
      sched: Guarantee new group-entities always have weight
      sched: Fix hrtimer_cancel()/rq->lock deadlock
      sched: Fix cfs_bandwidth misuse of hrtimer_expires_remaining
      sched: Fix race on toggling cfs_bandwidth_used
      sched: Remove extra put_online_cpus() inside sched_setaffinity()
      sched/rt: Fix task_tick_rt() comment
      sched/wait: Fix build breakage
      sched/wait: Introduce prepare_to_wait_event()
      sched/wait: Add ___wait_cond_timeout() to wait_event*_timeout() too
      sched: Remove get_online_cpus() usage
      sched: Fix race in migrate_swap_stop()
      ...

commit 3ab679661721b1ec2aaad99a801870ed59ab1110
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Oct 16 19:39:37 2013 +0200

    uprobes: Teach uprobe_copy_process() to handle CLONE_VFORK
    
    uprobe_copy_process() does nothing if the child shares ->mm with
    the forking process, but there is a special case: CLONE_VFORK.
    In this case it would be more correct to do dup_utask() but avoid
    dup_xol(). This is not that important, the child should not unwind
    its stack too much, this can corrupt the parent's stack, but at
    least we need this to allow to ret-probe __vfork() itself.
    
    Note: in theory, it would be better to check task_pt_regs(p)->sp
    instead of CLONE_VFORK, we need to dup_utask() if and only if the
    child can return from the function called by the parent. But this
    needs the arch-dependant helper, and I think that nobody actually
    does clone(same_stack, CLONE_VM).
    
    Reported-by: Martin Cermak <mcermak@redhat.com>
    Reported-by: David Smith <dsmith@redhat.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index d3603b81246b..8531609b6a82 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1489,7 +1489,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	perf_event_fork(p);
 
 	trace_task_newtask(p, clone_flags);
-	uprobe_copy_process(p);
+	uprobe_copy_process(p, clone_flags);
 
 	return p;
 

commit b68e0749100e1b901bf11330f149b321c082178e
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sun Oct 13 21:18:31 2013 +0200

    uprobes: Change the callsite of uprobe_copy_process()
    
    Preparation for the next patches.
    
    Move the callsite of uprobe_copy_process() in copy_process() down
    to the succesfull return. We do not care if copy_process() fails,
    uprobe_free_utask() won't be called in this case so the wrong
    ->utask != NULL doesn't matter.
    
    OTOH, with this change we know that copy_process() can't fail when
    uprobe_copy_process() is called, the new task should either return
    to user-mode or call do_exit(). This way uprobe_copy_process() can:
    
            1. setup p->utask != NULL if necessary
    
            2. setup uprobes_state.xol_area
    
            3. use task_work_add(p)
    
    Also, move the definition of uprobe_copy_process() down so that it
    can see get_utask().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 086fe73ad6bd..d3603b81246b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1373,7 +1373,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	INIT_LIST_HEAD(&p->pi_state_list);
 	p->pi_state_cache = NULL;
 #endif
-	uprobe_copy_process(p);
 	/*
 	 * sigaltstack should be cleared when sharing the same VM
 	 */
@@ -1490,6 +1489,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	perf_event_fork(p);
 
 	trace_task_newtask(p, clone_flags);
+	uprobe_copy_process(p);
 
 	return p;
 

commit 5e1576ed0e54d419286a8096133029062b6ad456
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Oct 7 11:29:26 2013 +0100

    sched/numa: Stay on the same node if CLONE_VM
    
    A newly spawned thread inside a process should stay on the same
    NUMA node as its parent. This prevents processes from being "torn"
    across multiple NUMA nodes every time they spawn a new thread.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-49-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7192d91b5415..c93be06dee87 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1310,7 +1310,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */
-	sched_fork(p);
+	sched_fork(clone_flags, p);
 
 	retval = perf_event_init_task(p);
 	if (retval)

commit b726b7dfb400c937546fa91cf8523dcb1aa2fc6e
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:28:53 2013 +0100

    Revert "mm: sched: numa: Delay PTE scanning until a task is scheduled on a new node"
    
    PTE scanning and NUMA hinting fault handling is expensive so commit
    5bca2303 ("mm: sched: numa: Delay PTE scanning until a task is scheduled
    on a new node") deferred the PTE scan until a task had been scheduled on
    another node. The problem is that in the purely shared memory case that
    this may never happen and no NUMA hinting fault information will be
    captured. We are not ruling out the possibility that something better
    can be done here but for now, this patch needs to be reverted and depend
    entirely on the scan_delay to avoid punishing short-lived processes.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-16-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 086fe73ad6bd..7192d91b5415 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -816,9 +816,6 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 	mm->pmd_huge_pte = NULL;
-#endif
-#ifdef CONFIG_NUMA_BALANCING
-	mm->first_nid = NUMA_PTE_SCAN_INIT;
 #endif
 	if (!mm_init(mm, tsk))
 		goto fail_nomem;

commit 9bf12df31f282e845b3dfaac1e5d5376a041da22
Merge: 399a946edbbe d9b2c8714aef
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 13 10:55:58 2013 -0700

    Merge git://git.kvack.org/~bcrl/aio-next
    
    Pull aio changes from Ben LaHaise:
     "First off, sorry for this pull request being late in the merge window.
      Al had raised a couple of concerns about 2 items in the series below.
      I addressed the first issue (the race introduced by Gu's use of
      mm_populate()), but he has not provided any further details on how he
      wants to rework the anon_inode.c changes (which were sent out months
      ago but have yet to be commented on).
    
      The bulk of the changes have been sitting in the -next tree for a few
      months, with all the issues raised being addressed"
    
    * git://git.kvack.org/~bcrl/aio-next: (22 commits)
      aio: rcu_read_lock protection for new rcu_dereference calls
      aio: fix race in ring buffer page lookup introduced by page migration support
      aio: fix rcu sparse warnings introduced by ioctx table lookup patch
      aio: remove unnecessary debugging from aio_free_ring()
      aio: table lookup: verify ctx pointer
      staging/lustre: kiocb->ki_left is removed
      aio: fix error handling and rcu usage in "convert the ioctx list to table lookup v3"
      aio: be defensive to ensure request batching is non-zero instead of BUG_ON()
      aio: convert the ioctx list to table lookup v3
      aio: double aio_max_nr in calculations
      aio: Kill ki_dtor
      aio: Kill ki_users
      aio: Kill unneeded kiocb members
      aio: Kill aio_rw_vect_retry()
      aio: Don't use ctx->tail unnecessarily
      aio: io_cancel() no longer returns the io_event
      aio: percpu ioctx refcount
      aio: percpu reqs_available
      aio: reqs_active -> reqs_available
      aio: fix build when migration is disabled
      ...

commit ef0855d334e1e4af7c3e0c42146a8479ea14a5ab
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 11 14:20:14 2013 -0700

    mm: mempolicy: turn vma_set_policy() into vma_dup_policy()
    
    Simple cleanup.  Every user of vma_set_policy() does the same work, this
    looks a bit annoying imho.  And the new trivial helper which does
    mpol_dup() + vma_set_policy() to simplify the callers.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 84703db06cf3..81ccb4f010c2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -351,7 +351,6 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	struct rb_node **rb_link, *rb_parent;
 	int retval;
 	unsigned long charge;
-	struct mempolicy *pol;
 
 	uprobe_start_dup_mmap();
 	down_write(&oldmm->mmap_sem);
@@ -400,11 +399,9 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			goto fail_nomem;
 		*tmp = *mpnt;
 		INIT_LIST_HEAD(&tmp->anon_vma_chain);
-		pol = mpol_dup(vma_policy(mpnt));
-		retval = PTR_ERR(pol);
-		if (IS_ERR(pol))
+		retval = vma_dup_policy(mpnt, tmp);
+		if (retval)
 			goto fail_nomem_policy;
-		vma_set_policy(tmp, pol);
 		tmp->vm_mm = mm;
 		if (anon_vma_fork(tmp, mpnt))
 			goto fail_nomem_anon_vma_fork;
@@ -472,7 +469,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	uprobe_end_dup_mmap();
 	return retval;
 fail_nomem_anon_vma_fork:
-	mpol_put(pol);
+	mpol_put(vma_policy(tmp));
 fail_nomem_policy:
 	kmem_cache_free(vm_area_cachep, tmp);
 fail_nomem:

commit 40a0d32d1eaffe6aac7324ca92604b6b3977eb0e
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 11 14:19:41 2013 -0700

    fork: unify and tighten up CLONE_NEWUSER/CLONE_NEWPID checks
    
    do_fork() denies CLONE_THREAD | CLONE_PARENT if NEWUSER | NEWPID.
    
    Then later copy_process() denies CLONE_SIGHAND if the new process will
    be in a different pid namespace (task_active_pid_ns() doesn't match
    current->nsproxy->pid_ns).
    
    This looks confusing and inconsistent.  CLONE_NEWPID is very similar to
    the case when ->pid_ns was already unshared, we want the same
    restrictions so copy_process() should also nack CLONE_PARENT.
    
    And it would be better to deny CLONE_NEWUSER && CLONE_SIGHAND as well
    just for consistency.
    
    Kill the "CLONE_NEWUSER | CLONE_NEWPID" check in do_fork() and change
    copy_process() to do the same check along with ->pid_ns check we already
    have.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Colin Walters <walters@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 68d508f2bfba..84703db06cf3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1173,13 +1173,16 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		return ERR_PTR(-EINVAL);
 
 	/*
-	 * If the new process will be in a different pid namespace don't
-	 * allow it to share a thread group or signal handlers with the
-	 * forking task.
+	 * If the new process will be in a different pid or user namespace
+	 * do not allow it to share a thread group or signal handlers or
+	 * parent with the forking task.
 	 */
-	if ((clone_flags & CLONE_SIGHAND) && (task_active_pid_ns(current) !=
-					current->nsproxy->pid_ns_for_children))
-		return ERR_PTR(-EINVAL);
+	if (clone_flags & (CLONE_SIGHAND | CLONE_PARENT)) {
+		if ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||
+		    (task_active_pid_ns(current) !=
+				current->nsproxy->pid_ns_for_children))
+			return ERR_PTR(-EINVAL);
+	}
 
 	retval = security_task_create(clone_flags);
 	if (retval)
@@ -1575,15 +1578,6 @@ long do_fork(unsigned long clone_flags,
 	int trace = 0;
 	long nr;
 
-	/*
-	 * Do some preliminary argument and permissions checking before we
-	 * actually start allocating stuff
-	 */
-	if (clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) {
-		if (clone_flags & (CLONE_THREAD|CLONE_PARENT))
-			return -EINVAL;
-	}
-
 	/*
 	 * Determine whether and which event to report to ptracer.  When
 	 * called from kernel_thread or CLONE_UNTRACED is explicitly

commit 5167246a8ad617df55717c2d901da5e2aedffcfa
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 11 14:19:40 2013 -0700

    pidns: kill the unnecessary CLONE_NEWPID in copy_process()
    
    Commit 8382fcac1b81 ("pidns: Outlaw thread creation after
    unshare(CLONE_NEWPID)") nacks CLONE_NEWPID if the forking process
    unshared pid_ns.  This is correct but unnecessary, copy_pid_ns() does
    the same check.
    
    Remove the CLONE_NEWPID check to cleanup the code and prepare for the
    next change.
    
    Test-case:
    
            static int child(void *arg)
            {
                    return 0;
            }
    
            static char stack[16 * 1024];
    
            int main(void)
            {
                    pid_t pid;
    
                    assert(unshare(CLONE_NEWUSER | CLONE_NEWPID) == 0);
    
                    pid = clone(child, stack + sizeof(stack) / 2,
                                    CLONE_NEWPID | SIGCHLD, NULL);
                    assert(pid < 0 && errno == EINVAL);
    
                    return 0;
            }
    
    clone(CLONE_NEWPID) correctly fails with or without this change.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Colin Walters <walters@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3561391ca450..68d508f2bfba 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1177,9 +1177,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * allow it to share a thread group or signal handlers with the
 	 * forking task.
 	 */
-	if ((clone_flags & (CLONE_SIGHAND | CLONE_NEWPID)) &&
-	    (task_active_pid_ns(current) !=
-	     current->nsproxy->pid_ns_for_children))
+	if ((clone_flags & CLONE_SIGHAND) && (task_active_pid_ns(current) !=
+					current->nsproxy->pid_ns_for_children))
 		return ERR_PTR(-EINVAL);
 
 	retval = security_task_create(clone_flags);

commit e79f525e99b04390ca4d2366309545a836c03bf1
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 11 14:19:38 2013 -0700

    pidns: fix vfork() after unshare(CLONE_NEWPID)
    
    Commit 8382fcac1b81 ("pidns: Outlaw thread creation after
    unshare(CLONE_NEWPID)") nacks CLONE_VM if the forking process unshared
    pid_ns, this obviously breaks vfork:
    
            int main(void)
            {
                    assert(unshare(CLONE_NEWUSER | CLONE_NEWPID) == 0);
                    assert(vfork() >= 0);
                    _exit(0);
                    return 0;
            }
    
    fails without this patch.
    
    Change this check to use CLONE_SIGHAND instead.  This also forbids
    CLONE_THREAD automatically, and this is what the comment implies.
    
    We could probably even drop CLONE_SIGHAND and use CLONE_THREAD, but it
    would be safer to not do this.  The current check denies CLONE_SIGHAND
    implicitely and there is no reason to change this.
    
    Eric said "CLONE_SIGHAND is fine.  CLONE_THREAD would be even better.
    Having shared signal handling between two different pid namespaces is
    the case that we are fundamentally guarding against."
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reported-by: Colin Walters <walters@redhat.com>
    Acked-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c9eaf2013002..3561391ca450 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1173,10 +1173,11 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		return ERR_PTR(-EINVAL);
 
 	/*
-	 * If the new process will be in a different pid namespace
-	 * don't allow the creation of threads.
+	 * If the new process will be in a different pid namespace don't
+	 * allow it to share a thread group or signal handlers with the
+	 * forking task.
 	 */
-	if ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&
+	if ((clone_flags & (CLONE_SIGHAND | CLONE_NEWPID)) &&
 	    (task_active_pid_ns(current) !=
 	     current->nsproxy->pid_ns_for_children))
 		return ERR_PTR(-EINVAL);

commit c7c4591db64dbd1e504bc4e2806d7ef290a3c81b
Merge: 11c7b03d42a8 c7b96acf1456
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 7 14:35:32 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull namespace changes from Eric Biederman:
     "This is an assorted mishmash of small cleanups, enhancements and bug
      fixes.
    
      The major theme is user namespace mount restrictions.  nsown_capable
      is killed as it encourages not thinking about details that need to be
      considered.  A very hard to hit pid namespace exiting bug was finally
      tracked and fixed.  A couple of cleanups to the basic namespace
      infrastructure.
    
      Finally there is an enhancement that makes per user namespace
      capabilities usable as capabilities, and an enhancement that allows
      the per userns root to nice other processes in the user namespace"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      userns:  Kill nsown_capable it makes the wrong thing easy
      capabilities: allow nice if we are privileged
      pidns: Don't have unshare(CLONE_NEWPID) imply CLONE_THREAD
      userns: Allow PR_CAPBSET_DROP in a user namespace.
      namespaces: Simplify copy_namespaces so it is clear what is going on.
      pidns: Fix hang in zap_pid_ns_processes by sending a potentially extra wakeup
      sysfs: Restrict mounting sysfs
      userns: Better restrictions on when proc and sysfs can be mounted
      vfs: Don't copy mount bind mounts of /proc/<pid>/ns/mnt between namespaces
      kernel/nsproxy.c: Improving a snippet of code.
      proc: Restrict mounting the proc filesystem
      vfs: Lock in place mounts from more privileged users

commit 6e556ce209b09528dbf1931cbfd5d323e1345926
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Mar 5 13:59:48 2013 -0800

    pidns: Don't have unshare(CLONE_NEWPID) imply CLONE_THREAD
    
    I goofed when I made unshare(CLONE_NEWPID) only work in a
    single-threaded process.  There is no need for that requirement and in
    fact I analyzied things right for setns.  The hard requirement
    is for tasks that share a VM to all be in the pid namespace and
    we properly prevent that in do_fork.
    
    Just to be certain I took a look through do_wait and
    forget_original_parent and there are no cases that make it any harder
    for children to be in the multiple pid namespaces than it is for
    children to be in the same pid namespace.  I also performed a check to
    see if there were in uses of task->nsproxy_pid_ns I was not familiar
    with, but it is only used when allocating a new pid for a new task,
    and in checks to prevent craziness from happening.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 66635c80a813..eb45f1d72703 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1817,11 +1817,6 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 	 */
 	if (unshare_flags & CLONE_NEWUSER)
 		unshare_flags |= CLONE_THREAD | CLONE_FS;
-	/*
-	 * If unsharing a pid namespace must also unshare the thread.
-	 */
-	if (unshare_flags & CLONE_NEWPID)
-		unshare_flags |= CLONE_THREAD;
 	/*
 	 * If unsharing a thread from a thread group, must also unshare vm.
 	 */

commit c2b1df2eb42978073ec27c99cc199d20ae48b849
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Aug 22 11:39:16 2013 -0700

    Rename nsproxy.pid_ns to nsproxy.pid_ns_for_children
    
    nsproxy.pid_ns is *not* the task's pid namespace.  The name should clarify
    that.
    
    This makes it more obvious that setns on a pid namespace is weird --
    it won't change the pid namespace shown in procfs.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Reviewed-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/fork.c b/kernel/fork.c
index e23bb19e2a3e..bf46287c91a4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1177,7 +1177,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * don't allow the creation of threads.
 	 */
 	if ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&
-	    (task_active_pid_ns(current) != current->nsproxy->pid_ns))
+	    (task_active_pid_ns(current) !=
+	     current->nsproxy->pid_ns_for_children))
 		return ERR_PTR(-EINVAL);
 
 	retval = security_task_create(clone_flags);
@@ -1351,7 +1352,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	if (pid != &init_struct_pid) {
 		retval = -ENOMEM;
-		pid = alloc_pid(p->nsproxy->pid_ns);
+		pid = alloc_pid(p->nsproxy->pid_ns_for_children);
 		if (!pid)
 			goto bad_fork_cleanup_io;
 	}

commit dfa9771a7c4784bafd0673bc7abcee3813088b77
Author: Michal Simek <michal.simek@xilinx.com>
Date:   Tue Aug 13 16:00:53 2013 -0700

    microblaze: fix clone syscall
    
    Fix inadvertent breakage in the clone syscall ABI for Microblaze that
    was introduced in commit f3268edbe6fe ("microblaze: switch to generic
    fork/vfork/clone").
    
    The Microblaze syscall ABI for clone takes the parent tid address in the
    4th argument; the third argument slot is used for the stack size.  The
    incorrectly-used CLONE_BACKWARDS type assigned parent tid to the 3rd
    slot.
    
    This commit restores the original ABI so that existing userspace libc
    code will work correctly.
    
    All kernel versions from v3.8-rc1 were affected.
    
    Signed-off-by: Michal Simek <michal.simek@xilinx.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 403d2bb8a968..e23bb19e2a3e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1679,6 +1679,12 @@ SYSCALL_DEFINE5(clone, unsigned long, newsp, unsigned long, clone_flags,
 		 int __user *, parent_tidptr,
 		 int __user *, child_tidptr,
 		 int, tls_val)
+#elif defined(CONFIG_CLONE_BACKWARDS3)
+SYSCALL_DEFINE6(clone, unsigned long, clone_flags, unsigned long, newsp,
+		int, stack_size,
+		int __user *, parent_tidptr,
+		int __user *, child_tidptr,
+		int, tls_val)
 #else
 SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 		 int __user *, parent_tidptr,

commit db446a08c23d5475e6b08c87acca79ebb20f283c
Author: Benjamin LaHaise <bcrl@kvack.org>
Date:   Tue Jul 30 12:54:40 2013 -0400

    aio: convert the ioctx list to table lookup v3
    
    On Wed, Jun 12, 2013 at 11:14:40AM -0700, Kent Overstreet wrote:
    > On Mon, Apr 15, 2013 at 02:40:55PM +0300, Octavian Purdila wrote:
    > > When using a large number of threads performing AIO operations the
    > > IOCTX list may get a significant number of entries which will cause
    > > significant overhead. For example, when running this fio script:
    > >
    > > rw=randrw; size=256k ;directory=/mnt/fio; ioengine=libaio; iodepth=1
    > > blocksize=1024; numjobs=512; thread; loops=100
    > >
    > > on an EXT2 filesystem mounted on top of a ramdisk we can observe up to
    > > 30% CPU time spent by lookup_ioctx:
    > >
    > >  32.51%  [guest.kernel]  [g] lookup_ioctx
    > >   9.19%  [guest.kernel]  [g] __lock_acquire.isra.28
    > >   4.40%  [guest.kernel]  [g] lock_release
    > >   4.19%  [guest.kernel]  [g] sched_clock_local
    > >   3.86%  [guest.kernel]  [g] local_clock
    > >   3.68%  [guest.kernel]  [g] native_sched_clock
    > >   3.08%  [guest.kernel]  [g] sched_clock_cpu
    > >   2.64%  [guest.kernel]  [g] lock_release_holdtime.part.11
    > >   2.60%  [guest.kernel]  [g] memcpy
    > >   2.33%  [guest.kernel]  [g] lock_acquired
    > >   2.25%  [guest.kernel]  [g] lock_acquire
    > >   1.84%  [guest.kernel]  [g] do_io_submit
    > >
    > > This patchs converts the ioctx list to a radix tree. For a performance
    > > comparison the above FIO script was run on a 2 sockets 8 core
    > > machine. This are the results (average and %rsd of 10 runs) for the
    > > original list based implementation and for the radix tree based
    > > implementation:
    > >
    > > cores         1         2         4         8         16        32
    > > list       109376 ms  69119 ms  35682 ms  22671 ms  19724 ms  16408 ms
    > > %rsd         0.69%      1.15%     1.17%     1.21%     1.71%     1.43%
    > > radix       73651 ms  41748 ms  23028 ms  16766 ms  15232 ms   13787 ms
    > > %rsd         1.19%      0.98%     0.69%     1.13%    0.72%      0.75%
    > > % of radix
    > > relative    66.12%     65.59%    66.63%    72.31%   77.26%     83.66%
    > > to list
    > >
    > > To consider the impact of the patch on the typical case of having
    > > only one ctx per process the following FIO script was run:
    > >
    > > rw=randrw; size=100m ;directory=/mnt/fio; ioengine=libaio; iodepth=1
    > > blocksize=1024; numjobs=1; thread; loops=100
    > >
    > > on the same system and the results are the following:
    > >
    > > list        58892 ms
    > > %rsd         0.91%
    > > radix       59404 ms
    > > %rsd         0.81%
    > > % of radix
    > > relative    100.87%
    > > to list
    >
    > So, I was just doing some benchmarking/profiling to get ready to send
    > out the aio patches I've got for 3.11 - and it looks like your patch is
    > causing a ~1.5% throughput regression in my testing :/
    ... <snip>
    
    I've got an alternate approach for fixing this wart in lookup_ioctx()...
    Instead of using an rbtree, just use the reserved id in the ring buffer
    header to index an array pointing the ioctx.  It's not finished yet, and
    it needs to be tidied up, but is most of the way there.
    
                    -ben
    --
    "Thought is the essence of where you are now."
    --
    kmo> And, a rework of Ben's code, but this was entirely his idea
    kmo>            -Kent
    
    bcrl> And fix the code to use the right mm_struct in kill_ioctx(), actually
    free memory.
    
    Signed-off-by: Benjamin LaHaise <bcrl@kvack.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 66635c80a813..db5f541c5488 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -522,7 +522,7 @@ static void mm_init_aio(struct mm_struct *mm)
 {
 #ifdef CONFIG_AIO
 	spin_lock_init(&mm->ioctx_lock);
-	INIT_HLIST_HEAD(&mm->ioctx_list);
+	mm->ioctx_table = NULL;
 #endif
 }
 

commit 0db0628d90125193280eabb501c94feaf48fa9ab
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jun 19 14:53:51 2013 -0400

    kernel: delete __cpuinit usage from all core kernel files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    This removes all the uses of the __cpuinit macros from C files in
    the core kernel directories (kernel, init, lib, mm, and include)
    that don't really have a specific maintainer.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 66635c80a813..403d2bb8a968 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1546,7 +1546,7 @@ static inline void init_idle_pids(struct pid_link *links)
 	}
 }
 
-struct task_struct * __cpuinit fork_idle(int cpu)
+struct task_struct *fork_idle(int cpu)
 {
 	struct task_struct *task;
 	task = copy_process(CLONE_VM, 0, 0, NULL, &init_struct_pid, 0);

commit 98d1e64f95b177d0f14efbdf695a1b28e1428035
Author: Michel Lespinasse <walken@google.com>
Date:   Wed Jul 10 16:05:12 2013 -0700

    mm: remove free_area_cache
    
    Since all architectures have been converted to use vm_unmapped_area(),
    there is no remaining use for the free_area_cache.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6e6a1c11b3e5..66635c80a813 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -365,8 +365,6 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	mm->locked_vm = 0;
 	mm->mmap = NULL;
 	mm->mmap_cache = NULL;
-	mm->free_area_cache = oldmm->mmap_base;
-	mm->cached_hole_size = ~0UL;
 	mm->map_count = 0;
 	cpumask_clear(mm_cpumask(mm));
 	mm->mm_rb = RB_ROOT;
@@ -540,8 +538,6 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	mm->nr_ptes = 0;
 	memset(&mm->rss_stat, 0, sizeof(mm->rss_stat));
 	spin_lock_init(&mm->page_table_lock);
-	mm->free_area_cache = TASK_UNMAPPED_BASE;
-	mm->cached_hole_size = ~0UL;
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
 

commit 18c830df771f2ba8b4699fea9af1492275ae627b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jul 3 15:08:32 2013 -0700

    kernel/fork.c:copy_process(): consolidate the lockless CLONE_THREAD checks
    
    copy_process() does a lot of "chaotic" initializations and checks
    CLONE_THREAD twice before it takes tasklist.  In particular it sets
    "p->group_leader = p" and then changes it again under tasklist if
    !thread_group_leader(p).
    
    This looks a bit confusing, lets create a single "if (CLONE_THREAD)" block
    which initializes ->exit_signal, ->group_leader, and ->tgid.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Sergey Dyasly <dserrg@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7d6962fb6156..6e6a1c11b3e5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1360,11 +1360,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			goto bad_fork_cleanup_io;
 	}
 
-	p->pid = pid_nr(pid);
-	p->tgid = p->pid;
-	if (clone_flags & CLONE_THREAD)
-		p->tgid = current->tgid;
-
 	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
 	/*
 	 * Clear TID on mm_release()?
@@ -1400,12 +1395,19 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	clear_all_latency_tracing(p);
 
 	/* ok, now we should be set up.. */
-	if (clone_flags & CLONE_THREAD)
+	p->pid = pid_nr(pid);
+	if (clone_flags & CLONE_THREAD) {
 		p->exit_signal = -1;
-	else if (clone_flags & CLONE_PARENT)
-		p->exit_signal = current->group_leader->exit_signal;
-	else
-		p->exit_signal = (clone_flags & CSIGNAL);
+		p->group_leader = current->group_leader;
+		p->tgid = current->tgid;
+	} else {
+		if (clone_flags & CLONE_PARENT)
+			p->exit_signal = current->group_leader->exit_signal;
+		else
+			p->exit_signal = (clone_flags & CSIGNAL);
+		p->group_leader = p;
+		p->tgid = p->pid;
+	}
 
 	p->pdeath_signal = 0;
 	p->exit_state = 0;
@@ -1414,15 +1416,13 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);
 	p->dirty_paused_when = 0;
 
-	/*
-	 * Ok, make it visible to the rest of the system.
-	 * We dont wake it up yet.
-	 */
-	p->group_leader = p;
 	INIT_LIST_HEAD(&p->thread_group);
 	p->task_works = NULL;
 
-	/* Need tasklist lock for parent etc handling! */
+	/*
+	 * Make it visible to the rest of the system, but dont wake it up yet.
+	 * Need tasklist lock for parent etc handling!
+	 */
 	write_lock_irq(&tasklist_lock);
 
 	/* CLONE_PARENT re-uses the old parent */
@@ -1476,7 +1476,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			current->signal->nr_threads++;
 			atomic_inc(&current->signal->live);
 			atomic_inc(&current->signal->sigcnt);
-			p->group_leader = current->group_leader;
 			list_add_tail_rcu(&p->thread_group,
 					  &p->group_leader->thread_group);
 		}

commit 8190773985141f063e1d6dc10200527c655abfb5
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jul 3 15:08:31 2013 -0700

    kernel/fork.c:copy_process(): don't add the uninitialized child to thread/task/pid lists
    
    copy_process() adds the new child to thread_group/init_task.tasks list and
    then does attach_pid(child, PIDTYPE_PID).  This means that the lockless
    next_thread() or next_task() can see this thread with the wrong pid.  Say,
    "ls /proc/pid/task" can list the same inode twice.
    
    We could move attach_pid(child, PIDTYPE_PID) up, but in this case
    find_task_by_vpid() can find the new thread before it was fully
    initialized.
    
    And this is already true for PIDTYPE_PGID/PIDTYPE_SID, With this patch
    copy_process() initializes child->pids[*].pid first, then calls
    attach_pid() to insert the task into the pid->tasks list.
    
    attach_pid() no longer need the "struct pid*" argument, it is always
    called after pid_link->pid was already set.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Sergey Dyasly <dserrg@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 417cb864e20c..7d6962fb6156 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1121,6 +1121,12 @@ static void posix_cpu_timers_init(struct task_struct *tsk)
 	INIT_LIST_HEAD(&tsk->cpu_timers[2]);
 }
 
+static inline void
+init_task_pid(struct task_struct *task, enum pid_type type, struct pid *pid)
+{
+	 task->pids[type].pid = pid;
+}
+
 /*
  * This creates a new process as a copy of the old one,
  * but does not actually start it yet.
@@ -1449,7 +1455,11 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (likely(p->pid)) {
 		ptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);
 
+		init_task_pid(p, PIDTYPE_PID, pid);
 		if (thread_group_leader(p)) {
+			init_task_pid(p, PIDTYPE_PGID, task_pgrp(current));
+			init_task_pid(p, PIDTYPE_SID, task_session(current));
+
 			if (is_child_reaper(pid)) {
 				ns_of_pid(pid)->child_reaper = p;
 				p->signal->flags |= SIGNAL_UNKILLABLE;
@@ -1457,10 +1467,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 			p->signal->leader_pid = pid;
 			p->signal->tty = tty_kref_get(current->signal->tty);
-			attach_pid(p, PIDTYPE_PGID, task_pgrp(current));
-			attach_pid(p, PIDTYPE_SID, task_session(current));
 			list_add_tail(&p->sibling, &p->real_parent->children);
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
+			attach_pid(p, PIDTYPE_PGID);
+			attach_pid(p, PIDTYPE_SID);
 			__this_cpu_inc(process_counts);
 		} else {
 			current->signal->nr_threads++;
@@ -1470,7 +1480,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			list_add_tail_rcu(&p->thread_group,
 					  &p->group_leader->thread_group);
 		}
-		attach_pid(p, PIDTYPE_PID, pid);
+		attach_pid(p, PIDTYPE_PID);
 		nr_threads++;
 	}
 

commit 80628ca06c5d42929de6bc22c0a41589a834d151
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jul 3 15:08:30 2013 -0700

    kernel/fork.c:copy_process(): unify CLONE_THREAD-or-thread_group_leader code
    
    Cleanup and preparation for the next changes.
    
    Move the "if (clone_flags & CLONE_THREAD)" code down under "if
    (likely(p->pid))" and turn it into into the "else" branch.  This makes the
    process/thread initialization more symmetrical and removes one check.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Sergey Dyasly <dserrg@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 09dbda38a54b..417cb864e20c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1446,14 +1446,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_free_pid;
 	}
 
-	if (clone_flags & CLONE_THREAD) {
-		current->signal->nr_threads++;
-		atomic_inc(&current->signal->live);
-		atomic_inc(&current->signal->sigcnt);
-		p->group_leader = current->group_leader;
-		list_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);
-	}
-
 	if (likely(p->pid)) {
 		ptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);
 
@@ -1470,6 +1462,13 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			list_add_tail(&p->sibling, &p->real_parent->children);
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			__this_cpu_inc(process_counts);
+		} else {
+			current->signal->nr_threads++;
+			atomic_inc(&current->signal->live);
+			atomic_inc(&current->signal->sigcnt);
+			p->group_leader = current->group_leader;
+			list_add_tail_rcu(&p->thread_group,
+					  &p->group_leader->thread_group);
 		}
 		attach_pid(p, PIDTYPE_PID, pid);
 		nr_threads++;

commit b57922b6c76c3ee401bb32fd3f298409dd6e6a53
Author: Eric Paris <eparis@redhat.com>
Date:   Wed Jul 3 15:08:29 2013 -0700

    fork: reorder permissions when violating number of processes limits
    
    When a task is attempting to violate the RLIMIT_NPROC limit we have a
    check to see if the task is sufficiently priviledged.  The check first
    looks at CAP_SYS_ADMIN, then CAP_SYS_RESOURCE, then if the task is uid=0.
    
    A result is that tasks which are allowed by the uid=0 check are first
    checked against the security subsystem.  This results in the security
    subsystem auditting a denial for sys_admin and sys_resource and then the
    task passing the uid=0 check.
    
    This patch rearranges the code to first check uid=0, since if we pass that
    we shouldn't hit the security system at all.  We then check sys_resource,
    since it is the smallest capability which will solve the problem.  Lastly
    we check the fallback everything cap_sysadmin.  We don't want to give this
    capability many places since it is so powerful.
    
    This will eliminate many of the false positive/needless denial messages we
    get when a root task tries to violate the nproc limit.  (note that
    kthreads count against root, so on a sufficiently large machine we can
    actually get past the default limits before any userspace tasks are
    launched.)
    
    Signed-off-by: Eric Paris <eparis@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 987b28a1f01b..09dbda38a54b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1199,8 +1199,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	retval = -EAGAIN;
 	if (atomic_read(&p->real_cred->user->processes) >=
 			task_rlimit(p, RLIMIT_NPROC)) {
-		if (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&
-		    p->real_cred->user != INIT_USER)
+		if (p->real_cred->user != INIT_USER &&
+		    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))
 			goto bad_fork_free;
 	}
 	current->flags &= ~PF_NPROC_EXCEEDED;

commit ebb37277796269da36a8bc5d72ed1e8e1fb7d34b
Merge: 4de13d7aa8f4 f50efd2fdbd9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 8 11:51:05 2013 -0700

    Merge branch 'for-3.10/drivers' of git://git.kernel.dk/linux-block
    
    Pull block driver updates from Jens Axboe:
     "It might look big in volume, but when categorized, not a lot of
      drivers are touched.  The pull request contains:
    
       - mtip32xx fixes from Micron.
    
       - A slew of drbd updates, this time in a nicer series.
    
       - bcache, a flash/ssd caching framework from Kent.
    
       - Fixes for cciss"
    
    * 'for-3.10/drivers' of git://git.kernel.dk/linux-block: (66 commits)
      bcache: Use bd_link_disk_holder()
      bcache: Allocator cleanup/fixes
      cciss: bug fix to prevent cciss from loading in kdump crash kernel
      cciss: add cciss_allow_hpsa module parameter
      drivers/block/mg_disk.c: add CONFIG_PM_SLEEP to suspend/resume functions
      mtip32xx: Workaround for unaligned writes
      bcache: Make sure blocksize isn't smaller than device blocksize
      bcache: Fix merge_bvec_fn usage for when it modifies the bvm
      bcache: Correctly check against BIO_MAX_PAGES
      bcache: Hack around stuff that clones up to bi_max_vecs
      bcache: Set ra_pages based on backing device's ra_pages
      bcache: Take data offset from the bdev superblock.
      mtip32xx: mtip32xx: Disable TRIM support
      mtip32xx: fix a smatch warning
      bcache: Disable broken btree fuzz tester
      bcache: Fix a format string overflow
      bcache: Fix a minor memory leak on device teardown
      bcache: Documentation updates
      bcache: Use WARN_ONCE() instead of __WARN()
      bcache: Add missing #include <linux/prefetch.h>
      ...

commit a27bb332c04cec8c4afd7912df0dc7890db27560
Author: Kent Overstreet <koverstreet@google.com>
Date:   Tue May 7 16:19:08 2013 -0700

    aio: don't include aio.h in sched.h
    
    Faster kernel compiles by way of fewer unnecessary includes.
    
    [akpm@linux-foundation.org: fix fallout]
    [akpm@linux-foundation.org: fix build]
    Signed-off-by: Kent Overstreet <koverstreet@google.com>
    Cc: Zach Brown <zab@redhat.com>
    Cc: Felipe Balbi <balbi@ti.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Mark Fasheh <mfasheh@suse.com>
    Cc: Joel Becker <jlbec@evilplan.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Asai Thambi S P <asamymuthupa@micron.com>
    Cc: Selvan Mani <smani@micron.com>
    Cc: Sam Bradshaw <sbradshaw@micron.com>
    Cc: Jeff Moyer <jmoyer@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Benjamin LaHaise <bcrl@kvack.org>
    Reviewed-by: "Theodore Ts'o" <tytso@mit.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7d40687b1434..c509cc4a0d53 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -70,6 +70,7 @@
 #include <linux/khugepaged.h>
 #include <linux/signalfd.h>
 #include <linux/uprobes.h>
+#include <linux/aio.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>

commit 08d76760832993050ad8c25e63b56773ef2ca303
Merge: 5f56886521d6 99e621f796d7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 1 07:21:43 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull compat cleanup from Al Viro:
     "Mostly about syscall wrappers this time; there will be another pile
      with patches in the same general area from various people, but I'd
      rather push those after both that and vfs.git pile are in."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal:
      syscalls.h: slightly reduce the jungles of macros
      get rid of union semop in sys_semctl(2) arguments
      make do_mremap() static
      sparc: no need to sign-extend in sync_file_range() wrapper
      ppc compat wrappers for add_key(2) and request_key(2) are pointless
      x86: trim sys_ia32.h
      x86: sys32_kill and sys32_mprotect are pointless
      get rid of compat_sys_semctl() and friends in case of ARCH_WANT_OLD_COMPAT_IPC
      merge compat sys_ipc instances
      consolidate compat lookup_dcookie()
      convert vmsplice to COMPAT_SYSCALL_DEFINE
      switch getrusage() to COMPAT_SYSCALL_DEFINE
      switch epoll_pwait to COMPAT_SYSCALL_DEFINE
      convert sendfile{,64} to COMPAT_SYSCALL_DEFINE
      switch signalfd{,4}() to COMPAT_SYSCALL_DEFINE
      make SYSCALL_DEFINE<n>-generated wrappers do asmlinkage_protect
      make HAVE_SYSCALL_WRAPPERS unconditional
      consolidate cond_syscall and SYSCALL_ALIAS declarations
      teach SYSCALL_DEFINE<n> how to deal with long long/unsigned long long
      get rid of duplicate logics in __SC_....[1-6] definitions

commit 16fa94b532b1958f508e07eca1a9256351241fbc
Merge: e0972916e8fe 25f55d9d01ad
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 07:43:28 2013 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The main changes in this development cycle were:
    
       - full dynticks preparatory work by Frederic Weisbecker
    
       - factor out the cpu time accounting code better, by Li Zefan
    
       - multi-CPU load balancer cleanups and improvements by Joonsoo Kim
    
       - various smaller fixes and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (45 commits)
      sched: Fix init NOHZ_IDLE flag
      sched: Prevent to re-select dst-cpu in load_balance()
      sched: Rename load_balance_tmpmask to load_balance_mask
      sched: Move up affinity check to mitigate useless redoing overhead
      sched: Don't consider other cpus in our group in case of NEWLY_IDLE
      sched: Explicitly cpu_idle_type checking in rebalance_domains()
      sched: Change position of resched_cpu() in load_balance()
      sched: Fix wrong rq's runnable_avg update with rt tasks
      sched: Document task_struct::personality field
      sched/cpuacct/UML: Fix header file dependency bug on the UML build
      cgroup: Kill subsys.active flag
      sched/cpuacct: No need to check subsys active state
      sched/cpuacct: Initialize cpuacct subsystem earlier
      sched/cpuacct: Initialize root cpuacct earlier
      sched/cpuacct: Allocate per_cpu cpuusage for root cpuacct statically
      sched/cpuacct: Clean up cpuacct.h
      sched/cpuacct: Remove redundant NULL checks in cpuacct_acount_field()
      sched/cpuacct: Remove redundant NULL checks in cpuacct_charge()
      sched/cpuacct: Add cpuacct_acount_field()
      sched/cpuacct: Add cpuacct_init()
      ...

commit cafe563591446cf80bfbc2fe3bc72a2e36cf1060
Author: Kent Overstreet <koverstreet@google.com>
Date:   Sat Mar 23 16:11:31 2013 -0700

    bcache: A block layer cache
    
    Does writethrough and writeback caching, handles unclean shutdown, and
    has a bunch of other nifty features motivated by real world usage.
    
    See the wiki at http://bcache.evilpiepirate.org for more.
    
    Signed-off-by: Kent Overstreet <koverstreet@google.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1766d324d5e3..7b54fb62332c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1303,6 +1303,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->memcg_batch.do_batch = 0;
 	p->memcg_batch.memcg = NULL;
 #endif
+#ifdef CONFIG_BCACHE
+	p->sequential_io	= 0;
+	p->sequential_io_avg	= 0;
+#endif
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p);

commit e66eded8309ebf679d3d3c1f5820d1f2ca332c71
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Mar 13 11:51:49 2013 -0700

    userns: Don't allow CLONE_NEWUSER | CLONE_FS
    
    Don't allowing sharing the root directory with processes in a
    different user namespace.  There doesn't seem to be any point, and to
    allow it would require the overhead of putting a user namespace
    reference in fs_struct (for permission checks) and incrementing that
    reference count on practically every call to fork.
    
    So just perform the inexpensive test of forbidding sharing fs_struct
    acrosss processes in different user namespaces.  We already disallow
    other forms of threading when unsharing a user namespace so this
    should be no real burden in practice.
    
    This updates setns, clone, and unshare to disallow multiple user
    namespaces sharing an fs_struct.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8d932b1c9056..1766d324d5e3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1141,6 +1141,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
 		return ERR_PTR(-EINVAL);
 
+	if ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))
+		return ERR_PTR(-EINVAL);
+
 	/*
 	 * Thread groups must share signals as well, and detached threads
 	 * can only be started up within the thread group.
@@ -1807,7 +1810,7 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 	 * If unsharing a user namespace must also unshare the thread.
 	 */
 	if (unshare_flags & CLONE_NEWUSER)
-		unshare_flags |= CLONE_THREAD;
+		unshare_flags |= CLONE_THREAD | CLONE_FS;
 	/*
 	 * If unsharing a pid namespace must also unshare the thread.
 	 */

commit 9fbc42eac1f6917081dc3b39922b2f1c57fdff28
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Feb 25 17:25:39 2013 +0100

    cputime: Dynamically scale cputime for full dynticks accounting
    
    The full dynticks cputime accounting is able to account either
    using the tick or the context tracking subsystem. This way
    the housekeeping CPU can keep the low overhead tick based
    solution.
    
    This latter mode has a low jiffies resolution granularity and
    need to be scaled against CFS precise runtime accounting to
    improve its result. We are doing this for CONFIG_TICK_CPU_ACCOUNTING,
    now we also need to expand it to full dynticks accounting dynamic
    off-case as well.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Mats Liljegren <mats.liljegren@enea.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8d932b1c9056..f3146ed49074 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1230,7 +1230,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->utime = p->stime = p->gtime = 0;
 	p->utimescaled = p->stimescaled = 0;
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
 	p->prev_cputime.utime = p->prev_cputime.stime = 0;
 #endif
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN

commit 2cf0966683430b6468f36ca20515a33ca7f2403c
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Jan 21 15:25:54 2013 -0500

    make SYSCALL_DEFINE<n>-generated wrappers do asmlinkage_protect
    
    ... and switch i386 to HAVE_SYSCALL_WRAPPERS, killing open-coded
    uses of asmlinkage_protect() in a bunch of syscalls.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8d932b1c9056..e1f34abe5887 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1674,10 +1674,7 @@ SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 		 int, tls_val)
 #endif
 {
-	long ret = do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr);
-	asmlinkage_protect(5, ret, clone_flags, newsp,
-			parent_tidptr, child_tidptr, tls_val);
-	return ret;
+	return do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr);
 }
 #endif
 

commit 6f977e6b2f75fdaccfd9be82bbf72fe5c60e8807
Author: Alan Cox <alan@linux.intel.com>
Date:   Wed Feb 27 17:03:23 2013 -0800

    fork: unshare: remove dead code
    
    If new_nsproxy is set we will always call switch_task_namespaces and
    then set new_nsproxy back to NULL so the reassignment and fall through
    check are redundant
    
    Signed-off-by: Alan Cox <alan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8f62b2a0f120..8d932b1c9056 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1861,10 +1861,8 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 			exit_sem(current);
 		}
 
-		if (new_nsproxy) {
+		if (new_nsproxy)
 			switch_task_namespaces(current, new_nsproxy);
-			new_nsproxy = NULL;
-		}
 
 		task_lock(current);
 
@@ -1894,9 +1892,6 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 		}
 	}
 
-	if (new_nsproxy)
-		put_nsproxy(new_nsproxy);
-
 bad_unshare_cleanup_cred:
 	if (new_cred)
 		put_cred(new_cred);

commit d895cb1af15c04c522a25c79cc429076987c089b
Merge: 9626357371b5 d3d009cb965e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 26 20:16:07 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs pile (part one) from Al Viro:
     "Assorted stuff - cleaning namei.c up a bit, fixing ->d_name/->d_parent
      locking violations, etc.
    
      The most visible changes here are death of FS_REVAL_DOT (replaced with
      "has ->d_weak_revalidate()") and a new helper getting from struct file
      to inode.  Some bits of preparation to xattr method interface changes.
    
      Misc patches by various people sent this cycle *and* ocfs2 fixes from
      several cycles ago that should've been upstream right then.
    
      PS: the next vfs pile will be xattr stuff."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (46 commits)
      saner proc_get_inode() calling conventions
      proc: avoid extra pde_put() in proc_fill_super()
      fs: change return values from -EACCES to -EPERM
      fs/exec.c: make bprm_mm_init() static
      ocfs2/dlm: use GFP_ATOMIC inside a spin_lock
      ocfs2: fix possible use-after-free with AIO
      ocfs2: Fix oops in ocfs2_fast_symlink_readpage() code path
      get_empty_filp()/alloc_file() leave both ->f_pos and ->f_version zero
      target: writev() on single-element vector is pointless
      export kernel_write(), convert open-coded instances
      fs: encode_fh: return FILEID_INVALID if invalid fid_type
      kill f_vfsmnt
      vfs: kill FS_REVAL_DOT by adding a d_weak_revalidate dentry op
      nfsd: handle vfs_getattr errors in acl protocol
      switch vfs_getattr() to struct path
      default SET_PERSONALITY() in linux/elf.h
      ceph: prepopulate inodes only when request is aborted
      d_hash_and_lookup(): export, switch open-coded instances
      9p: switch v9fs_set_create_acl() to inode+fid, do it before d_instantiate()
      9p: split dropping the acls from v9fs_set_create_acl()
      ...

commit 496ad9aa8ef448058e36ca7a787c61f2e63f0f54
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jan 23 17:07:38 2013 -0500

    new helper: file_inode(file)
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index c535f33bbb9c..4ff724f81f25 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -413,7 +413,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		tmp->vm_next = tmp->vm_prev = NULL;
 		file = tmp->vm_file;
 		if (file) {
-			struct inode *inode = file->f_path.dentry->d_inode;
+			struct inode *inode = file_inode(file);
 			struct address_space *mapping = file->f_mapping;
 
 			get_file(file);

commit b2c77a57e4a0a7877e357dead7ee8acc19944f3e
Merge: c3c186403c6a 6a61671bb2f3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Feb 5 13:10:33 2013 +0100

    Merge tag 'full-dynticks-cputime-for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/linux-dynticks into sched/core
    
    Pull full-dynticks (user-space execution is undisturbed and
    receives no timer IRQs) preparation changes that convert the
    cputime accounting code to be full-dynticks ready,
    from Frederic Weisbecker:
    
     "This implements the cputime accounting on full dynticks CPUs.
    
      Typical cputime stats infrastructure relies on the timer tick and
      its periodic polling on the CPU to account the amount of time
      spent by the CPUs and the tasks per high level domains such as
      userspace, kernelspace, guest, ...
    
      Now we are preparing to implement full dynticks capability on
      Linux for Real Time and HPC users who want full CPU isolation.
      This feature requires a cputime accounting that doesn't depend
      on the timer tick.
    
      To implement it, this new cputime infrastructure plugs into
      kernel/user/guest boundaries to take snapshots of cputime and
      flush these to the stats when needed. This performs pretty
      much like CONFIG_VIRT_CPU_ACCOUNTING except that context location
      and cputime snaphots are synchronized between write and read
      side such that the latter can safely retrieve the pending tickless
      cputime of a task and add it to its latest cputime snapshot to
      return the correct result to the user."
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6a61671bb2f3a1bd12cd17b8fca811a624782632
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Dec 16 20:00:34 2012 +0100

    cputime: Safely read cputime of full dynticks CPUs
    
    While remotely reading the cputime of a task running in a
    full dynticks CPU, the values stored in utime/stime fields
    of struct task_struct may be stale. Its values may be those
    of the last kernel <-> user transition time snapshot and
    we need to add the tickless time spent since this snapshot.
    
    To fix this, flush the cputime of the dynticks CPUs on
    kernel <-> user transition and record the time / context
    where we did this. Then on top of this snapshot and the current
    time, perform the fixup on the reader side from task_times()
    accessors.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [fixed kvm module related build errors]
    Signed-off-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 65ca6d27f24e..e68a95b4cf26 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1233,6 +1233,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING
 	p->prev_cputime.utime = p->prev_cputime.stime = 0;
 #endif
+#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
+	seqlock_init(&p->vtime_seqlock);
+	p->vtime_snap = 0;
+	p->vtime_snap_whence = VTIME_SLEEPING;
+#endif
+
 #if defined(SPLIT_RSS_COUNTING)
 	memset(&p->rss_stat, 0, sizeof(p->rss_stat));
 #endif

commit 3a142ed962958d3063f648738a3384ab90017100
Merge: edea0d03ee5f b1e0318b8cd4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 20 13:58:48 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull misc syscall fixes from Al Viro:
    
     - compat syscall fixes (discussed back in December)
    
     - a couple of "make life easier for sigaltstack stuff by reducing
       inter-tree dependencies"
    
     - fix up compiler/asmlinkage calling convention disagreement of
       sys_clone()
    
     - misc
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal:
      sys_clone() needs asmlinkage_protect
      make sure that /linuxrc has std{in,out,err}
      x32: fix sigtimedwait
      x32: fix waitid()
      switch compat_sys_wait4() and compat_sys_waitid() to COMPAT_SYSCALL_DEFINE
      switch compat_sys_sigaltstack() to COMPAT_SYSCALL_DEFINE
      CONFIG_GENERIC_SIGALTSTACK build breakage with asm-generic/syscalls.h
      Ensure that kernel_init_freeable() is not inlined into non __init code

commit b1e0318b8cd4bdbb0fbc48967b0350483ad9bd69
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Jan 19 22:13:34 2013 -0500

    sys_clone() needs asmlinkage_protect
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index a31b823b3c2d..e05cff2429b5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1660,8 +1660,10 @@ SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 		 int, tls_val)
 #endif
 {
-	return do_fork(clone_flags, newsp, 0,
-		parent_tidptr, child_tidptr);
+	long ret = do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr);
+	asmlinkage_protect(5, ret, clone_flags, newsp,
+			parent_tidptr, child_tidptr, tls_val);
+	return ret;
 }
 #endif
 

commit 8382fcac1b813ad0a4e68a838fc7ae93fa39eda0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Dec 20 19:26:06 2012 -0800

    pidns: Outlaw thread creation after unshare(CLONE_NEWPID)
    
    The sequence:
    unshare(CLONE_NEWPID)
    clone(CLONE_THREAD|CLONE_SIGHAND|CLONE_VM)
    
    Creates a new process in the new pid namespace without setting
    pid_ns->child_reaper.  After forking this results in a NULL
    pointer dereference.
    
    Avoid this and other nonsense scenarios that can show up after
    creating a new pid namespace with unshare by adding a new
    check in copy_prodcess.
    
    Pointed-out-by:  Oleg Nesterov <oleg@redhat.com>
    Acked-by:  Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index a31b823b3c2d..65ca6d27f24e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1166,6 +1166,14 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 				current->signal->flags & SIGNAL_UNKILLABLE)
 		return ERR_PTR(-EINVAL);
 
+	/*
+	 * If the new process will be in a different pid namespace
+	 * don't allow the creation of threads.
+	 */
+	if ((clone_flags & (CLONE_VM|CLONE_NEWPID)) &&
+	    (task_active_pid_ns(current) != current->nsproxy->pid_ns))
+		return ERR_PTR(-EINVAL);
+
 	retval = security_task_create(clone_flags);
 	if (retval)
 		goto fork_out;

commit 54d46ea993744c5408e39ce0cb4851e13cbea716
Merge: f59dc2bb5a50 50ececcfa7d1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 20 18:05:28 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull signal handling cleanups from Al Viro:
     "sigaltstack infrastructure + conversion for x86, alpha and um,
      COMPAT_SYSCALL_DEFINE infrastructure.
    
      Note that there are several conflicts between "unify
      SS_ONSTACK/SS_DISABLE definitions" and UAPI patches in mainline;
      resolution is trivial - just remove definitions of SS_ONSTACK and
      SS_DISABLED from arch/*/uapi/asm/signal.h; they are all identical and
      include/uapi/linux/signal.h contains the unified variant."
    
    Fixed up conflicts as per Al.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal:
      alpha: switch to generic sigaltstack
      new helpers: __save_altstack/__compat_save_altstack, switch x86 and um to those
      generic compat_sys_sigaltstack()
      introduce generic sys_sigaltstack(), switch x86 and um to it
      new helper: compat_user_stack_pointer()
      new helper: restore_altstack()
      unify SS_ONSTACK/SS_DISABLE definitions
      new helper: current_user_stack_pointer()
      missing user_stack_pointer() instances
      Bury the conditionals from kernel_thread/kernel_execve series
      COMPAT_SYSCALL_DEFINE: infrastructure

commit ae903caae267154de7cf8576b130ff474630596b
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Dec 14 12:44:11 2012 -0500

    Bury the conditionals from kernel_thread/kernel_execve series
    
    All architectures have
            CONFIG_GENERIC_KERNEL_THREAD
            CONFIG_GENERIC_KERNEL_EXECVE
            __ARCH_WANT_SYS_EXECVE
    None of them have __ARCH_WANT_KERNEL_EXECVE and there are only two callers
    of kernel_execve() (which is a trivial wrapper for do_execve() now) left.
    Kill the conditionals and make both callers use do_execve().
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 540730783433..389712ffc0ad 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1623,7 +1623,6 @@ long do_fork(unsigned long clone_flags,
 	return nr;
 }
 
-#ifdef CONFIG_GENERIC_KERNEL_THREAD
 /*
  * Create a kernel thread.
  */
@@ -1632,7 +1631,6 @@ pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn,
 		(unsigned long)arg, NULL, NULL);
 }
-#endif
 
 #ifdef __ARCH_WANT_SYS_FORK
 SYSCALL_DEFINE0(fork)

commit 2ad306b17c0ac5a1b1f250d5f772aeb87fdf1eba
Author: Glauber Costa <glommer@parallels.com>
Date:   Tue Dec 18 14:22:18 2012 -0800

    fork: protect architectures where THREAD_SIZE >= PAGE_SIZE against fork bombs
    
    Because those architectures will draw their stacks directly from the page
    allocator, rather than the slab cache, we can directly pass __GFP_KMEMCG
    flag, and issue the corresponding free_pages.
    
    This code path is taken when the architecture doesn't define
    CONFIG_ARCH_THREAD_INFO_ALLOCATOR (only ia64 seems to), and has
    THREAD_SIZE >= PAGE_SIZE.  Luckily, most - if not all - of the remaining
    architectures fall in this category.
    
    This will guarantee that every stack page is accounted to the memcg the
    process currently lives on, and will have the allocations to fail if they
    go over limit.
    
    For the time being, I am defining a new variant of THREADINFO_GFP, not to
    mess with the other path.  Once the slab is also tracked by memcg, we can
    get rid of that flag.
    
    Tested to successfully protect against :(){ :|:& };:
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Acked-by: Frederic Weisbecker <fweisbec@redhat.com>
    Acked-by: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: JoonSoo Kim <js1304@gmail.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Suleiman Souhlal <suleiman@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c36c4e301efe..85f6d536608d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -146,7 +146,7 @@ void __weak arch_release_thread_info(struct thread_info *ti)
 static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 						  int node)
 {
-	struct page *page = alloc_pages_node(node, THREADINFO_GFP,
+	struct page *page = alloc_pages_node(node, THREADINFO_GFP_ACCOUNTED,
 					     THREAD_SIZE_ORDER);
 
 	return page ? page_address(page) : NULL;
@@ -154,7 +154,7 @@ static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 
 static inline void free_thread_info(struct thread_info *ti)
 {
-	free_pages((unsigned long)ti, THREAD_SIZE_ORDER);
+	free_memcg_kmem_pages((unsigned long)ti, THREAD_SIZE_ORDER);
 }
 # else
 static struct kmem_cache *thread_info_cache;

commit 6a2b60b17b3e48a418695a94bd2420f6ab32e519
Merge: 9228ff90387e 98f842e675f9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 17 15:44:47 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull user namespace changes from Eric Biederman:
     "While small this set of changes is very significant with respect to
      containers in general and user namespaces in particular.  The user
      space interface is now complete.
    
      This set of changes adds support for unprivileged users to create user
      namespaces and as a user namespace root to create other namespaces.
      The tyranny of supporting suid root preventing unprivileged users from
      using cool new kernel features is broken.
    
      This set of changes completes the work on setns, adding support for
      the pid, user, mount namespaces.
    
      This set of changes includes a bunch of basic pid namespace
      cleanups/simplifications.  Of particular significance is the rework of
      the pid namespace cleanup so it no longer requires sending out
      tendrils into all kinds of unexpected cleanup paths for operation.  At
      least one case of broken error handling is fixed by this cleanup.
    
      The files under /proc/<pid>/ns/ have been converted from regular files
      to magic symlinks which prevents incorrect caching by the VFS,
      ensuring the files always refer to the namespace the process is
      currently using and ensuring that the ptrace_mayaccess permission
      checks are always applied.
    
      The files under /proc/<pid>/ns/ have been given stable inode numbers
      so it is now possible to see if different processes share the same
      namespaces.
    
      Through the David Miller's net tree are changes to relax many of the
      permission checks in the networking stack to allowing the user
      namespace root to usefully use the networking stack.  Similar changes
      for the mount namespace and the pid namespace are coming through my
      tree.
    
      Two small changes to add user namespace support were commited here adn
      in David Miller's -net tree so that I could complete the work on the
      /proc/<pid>/ns/ files in this tree.
    
      Work remains to make it safe to build user namespaces and 9p, afs,
      ceph, cifs, coda, gfs2, ncpfs, nfs, nfsd, ocfs2, and xfs so the
      Kconfig guard remains in place preventing that user namespaces from
      being built when any of those filesystems are enabled.
    
      Future design work remains to allow root users outside of the initial
      user namespace to mount more than just /proc and /sys."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (38 commits)
      proc: Usable inode numbers for the namespace file descriptors.
      proc: Fix the namespace inode permission checks.
      proc: Generalize proc inode allocation
      userns: Allow unprivilged mounts of proc and sysfs
      userns: For /proc/self/{uid,gid}_map derive the lower userns from the struct file
      procfs: Print task uids and gids in the userns that opened the proc file
      userns: Implement unshare of the user namespace
      userns: Implent proc namespace operations
      userns: Kill task_user_ns
      userns: Make create_new_namespaces take a user_ns parameter
      userns: Allow unprivileged use of setns.
      userns: Allow unprivileged users to create new namespaces
      userns: Allow setting a userns mapping to your current uid.
      userns: Allow chown and setgid preservation
      userns: Allow unprivileged users to create user namespaces.
      userns: Ignore suid and sgid on binaries if the uid or gid can not be mapped
      userns: fix return value on mntns_install() failure
      vfs: Allow unprivileged manipulation of the mount namespace.
      vfs: Only support slave subtrees across different user namespaces
      vfs: Add a user namespace reference from struct mnt_namespace
      ...

commit 3d59eebc5e137bd89c6351e4c70e90ba1d0dc234
Merge: 11520e5e7c18 4fc3f1d66b1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 16 14:33:25 2012 -0800

    Merge tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma
    
    Pull Automatic NUMA Balancing bare-bones from Mel Gorman:
     "There are three implementations for NUMA balancing, this tree
      (balancenuma), numacore which has been developed in tip/master and
      autonuma which is in aa.git.
    
      In almost all respects balancenuma is the dumbest of the three because
      its main impact is on the VM side with no attempt to be smart about
      scheduling.  In the interest of getting the ball rolling, it would be
      desirable to see this much merged for 3.8 with the view to building
      scheduler smarts on top and adapting the VM where required for 3.9.
    
      The most recent set of comparisons available from different people are
    
        mel:    https://lkml.org/lkml/2012/12/9/108
        mingo:  https://lkml.org/lkml/2012/12/7/331
        tglx:   https://lkml.org/lkml/2012/12/10/437
        srikar: https://lkml.org/lkml/2012/12/10/397
    
      The results are a mixed bag.  In my own tests, balancenuma does
      reasonably well.  It's dumb as rocks and does not regress against
      mainline.  On the other hand, Ingo's tests shows that balancenuma is
      incapable of converging for this workloads driven by perf which is bad
      but is potentially explained by the lack of scheduler smarts.  Thomas'
      results show balancenuma improves on mainline but falls far short of
      numacore or autonuma.  Srikar's results indicate we all suffer on a
      large machine with imbalanced node sizes.
    
      My own testing showed that recent numacore results have improved
      dramatically, particularly in the last week but not universally.
      We've butted heads heavily on system CPU usage and high levels of
      migration even when it shows that overall performance is better.
      There are also cases where it regresses.  Of interest is that for
      specjbb in some configurations it will regress for lower numbers of
      warehouses and show gains for higher numbers which is not reported by
      the tool by default and sometimes missed in treports.  Recently I
      reported for numacore that the JVM was crashing with
      NullPointerExceptions but currently it's unclear what the source of
      this problem is.  Initially I thought it was in how numacore batch
      handles PTEs but I'm no longer think this is the case.  It's possible
      numacore is just able to trigger it due to higher rates of migration.
    
      These reports were quite late in the cycle so I/we would like to start
      with this tree as it contains much of the code we can agree on and has
      not changed significantly over the last 2-3 weeks."
    
    * tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma: (50 commits)
      mm/rmap, migration: Make rmap_walk_anon() and try_to_unmap_anon() more scalable
      mm/rmap: Convert the struct anon_vma::mutex to an rwsem
      mm: migrate: Account a transhuge page properly when rate limiting
      mm: numa: Account for failed allocations and isolations as migration failures
      mm: numa: Add THP migration for the NUMA working set scanning fault case build fix
      mm: numa: Add THP migration for the NUMA working set scanning fault case.
      mm: sched: numa: Delay PTE scanning until a task is scheduled on a new node
      mm: sched: numa: Control enabling and disabling of NUMA balancing if !SCHED_DEBUG
      mm: sched: numa: Control enabling and disabling of NUMA balancing
      mm: sched: Adapt the scanning rate if a NUMA hinting fault does not migrate
      mm: numa: Use a two-stage filter to restrict pages being migrated for unlikely task<->node relationships
      mm: numa: migrate: Set last_nid on newly allocated page
      mm: numa: split_huge_page: Transfer last_nid on tail page
      mm: numa: Introduce last_nid to the page frame
      sched: numa: Slowly increase the scanning period as NUMA faults are handled
      mm: numa: Rate limit setting of pte_numa if node is saturated
      mm: numa: Rate limit the amount of memory that is migrated between nodes
      mm: numa: Structures for Migrate On Fault per NUMA migration rate limiting
      mm: numa: Migrate pages handled during a pmd_numa hinting fault
      mm: numa: Migrate on reference policy
      ...

commit 9977d9b379cb77e0f67bd6f4563618106e58e11d
Merge: cf4af0122157 541880d9a2c7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 12 12:22:13 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull big execve/kernel_thread/fork unification series from Al Viro:
     "All architectures are converted to new model.  Quite a bit of that
      stuff is actually shared with architecture trees; in such cases it's
      literally shared branch pulled by both, not a cherry-pick.
    
      A lot of ugliness and black magic is gone (-3KLoC total in this one):
    
       - kernel_thread()/kernel_execve()/sys_execve() redesign.
    
         We don't do syscalls from kernel anymore for either kernel_thread()
         or kernel_execve():
    
         kernel_thread() is essentially clone(2) with callback run before we
         return to userland, the callbacks either never return or do
         successful do_execve() before returning.
    
         kernel_execve() is a wrapper for do_execve() - it doesn't need to
         do transition to user mode anymore.
    
         As a result kernel_thread() and kernel_execve() are
         arch-independent now - they live in kernel/fork.c and fs/exec.c
         resp.  sys_execve() is also in fs/exec.c and it's completely
         architecture-independent.
    
       - daemonize() is gone, along with its parts in fs/*.c
    
       - struct pt_regs * is no longer passed to do_fork/copy_process/
         copy_thread/do_execve/search_binary_handler/->load_binary/do_coredump.
    
       - sys_fork()/sys_vfork()/sys_clone() unified; some architectures
         still need wrappers (ones with callee-saved registers not saved in
         pt_regs on syscall entry), but the main part of those suckers is in
         kernel/fork.c now."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (113 commits)
      do_coredump(): get rid of pt_regs argument
      print_fatal_signal(): get rid of pt_regs argument
      ptrace_signal(): get rid of unused arguments
      get rid of ptrace_signal_deliver() arguments
      new helper: signal_pt_regs()
      unify default ptrace_signal_deliver
      flagday: kill pt_regs argument of do_fork()
      death to idle_regs()
      don't pass regs to copy_process()
      flagday: don't pass regs to copy_thread()
      bfin: switch to generic vfork, get rid of pointless wrappers
      xtensa: switch to generic clone()
      openrisc: switch to use of generic fork and clone
      unicore32: switch to generic clone(2)
      score: switch to generic fork/vfork/clone
      c6x: sanitize copy_thread(), get rid of clone(2) wrapper, switch to generic clone()
      take sys_fork/sys_vfork/sys_clone prototypes to linux/syscalls.h
      mn10300: switch to generic fork/vfork/clone
      h8300: switch to generic fork/vfork/clone
      tile: switch to generic clone()
      ...
    
    Conflicts:
            arch/microblaze/include/asm/Kbuild

commit d206e09036d6201f90b2719484c8a59526c46125
Merge: fef3ff2eb777 15ef4ffaa797
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 12 08:18:24 2012 -0800

    Merge branch 'for-3.8' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup changes from Tejun Heo:
     "A lot of activities on cgroup side.  The big changes are focused on
      making cgroup hierarchy handling saner.
    
       - cgroup_rmdir() had peculiar semantics - it allowed cgroup
         destruction to be vetoed by individual controllers and tried to
         drain refcnt synchronously.  The vetoing never worked properly and
         caused good deal of contortions in cgroup.  memcg was the last
         reamining user.  Michal Hocko removed the usage and cgroup_rmdir()
         path has been simplified significantly.  This was done in a
         separate branch so that the memcg people can base further memcg
         changes on top.
    
       - The above allowed cleaning up cgroup lifecycle management and
         implementation of generic cgroup iterators which are used to
         improve hierarchy support.
    
       - cgroup_freezer updated to allow migration in and out of a frozen
         cgroup and handle hierarchy.  If a cgroup is frozen, all descendant
         cgroups are frozen.
    
       - netcls_cgroup and netprio_cgroup updated to handle hierarchy
         properly.
    
       - Various fixes and cleanups.
    
       - Two merge commits.  One to pull in memcg and rmdir cleanups (needed
         to build iterators).  The other pulled in cgroup/for-3.7-fixes for
         device_cgroup fixes so that further device_cgroup patches can be
         stacked on top."
    
    Fixed up a trivial conflict in mm/memcontrol.c as per Tejun (due to
    commit bea8c150a7 ("memcg: fix hotplugged memory zone oops") in master
    touching code close to commit 2ef37d3fe4 ("memcg: Simplify
    mem_cgroup_force_empty_list error handling") in for-3.8)
    
    * 'for-3.8' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (65 commits)
      cgroup: update Documentation/cgroups/00-INDEX
      cgroup_rm_file: don't delete the uncreated files
      cgroup: remove subsystem files when remounting cgroup
      cgroup: use cgroup_addrm_files() in cgroup_clear_directory()
      cgroup: warn about broken hierarchies only after css_online
      cgroup: list_del_init() on removed events
      cgroup: fix lockdep warning for event_control
      cgroup: move list add after list head initilization
      netprio_cgroup: allow nesting and inherit config on cgroup creation
      netprio_cgroup: implement netprio[_set]_prio() helpers
      netprio_cgroup: use cgroup->id instead of cgroup_netprio_state->prioidx
      netprio_cgroup: reimplement priomap expansion
      netprio_cgroup: shorten variable names in extend_netdev_table()
      netprio_cgroup: simplify write_priomap()
      netcls_cgroup: move config inheritance to ->css_online() and remove .broken_hierarchy marking
      cgroup: remove obsolete guarantee from cgroup_task_migrate.
      cgroup: add cgroup->id
      cgroup, cpuset: remove cgroup_subsys->post_clone()
      cgroup: s/CGRP_CLONE_CHILDREN/CGRP_CPUSET_CLONE_CHILDREN/
      cgroup: rename ->create/post_create/pre_destroy/destroy() to ->css_alloc/online/offline/free()
      ...

commit f57d54bab696133fae569c5f01352249c36fc74f
Merge: da830e589a45 c1ad41f1f727
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 11 18:21:38 2012 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The biggest change affects group scheduling: we now track the runnable
      average on a per-task entity basis, allowing a smoother, exponential
      decay average based load/weight estimation instead of the previous
      binary on-the-runqueue/off-the-runqueue load weight method.
    
      This will inevitably disturb workloads that were in some sort of
      borderline balancing state or unstable equilibrium, so an eye has to
      be kept on regressions.
    
      For that reason the new load average is only limited to group
      scheduling (shares distribution) at the moment (which was also hurting
      the most from the prior, crude weight calculation and whose scheduling
      quality wins most from this change) - but we plan to extend this to
      regular SMP balancing as well in the future, which will simplify and
      speed up things a bit.
    
      Other changes involve ongoing preparatory work to extend NOHZ to the
      scheduler as well, eventually allowing completely irq-free user-space
      execution."
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (33 commits)
      Revert "sched/autogroup: Fix crash on reboot when autogroup is disabled"
      cputime: Comment cputime's adjusting code
      cputime: Consolidate cputime adjustment code
      cputime: Rename thread_group_times to thread_group_cputime_adjusted
      cputime: Move thread_group_cputime() to sched code
      vtime: Warn if irqs aren't disabled on system time accounting APIs
      vtime: No need to disable irqs on vtime_account()
      vtime: Consolidate a bit the ctx switch code
      vtime: Explicitly account pending user time on process tick
      vtime: Remove the underscore prefix invasion
      sched/autogroup: Fix crash on reboot when autogroup is disabled
      cputime: Separate irqtime accounting from generic vtime
      cputime: Specialize irq vtime hooks
      kvm: Directly account vtime to system on guest switch
      vtime: Make vtime_account_system() irqsafe
      vtime: Gather vtime declarations to their own header file
      sched: Describe CFS load-balancer
      sched: Introduce temporary FAIR_GROUP_SCHED dependency for load-tracking
      sched: Make __update_entity_runnable_avg() fast
      sched: Update_cfs_shares at period edge
      ...

commit 5bca23035391928c4c7301835accca3551b96cc2
Author: Mel Gorman <mgorman@suse.de>
Date:   Thu Nov 22 14:40:03 2012 +0000

    mm: sched: numa: Delay PTE scanning until a task is scheduled on a new node
    
    Due to the fact that migrations are driven by the CPU a task is running
    on there is no point tracking NUMA faults until one task runs on a new
    node. This patch tracks the first node used by an address space. Until
    it changes, PTE scanning is disabled and no NUMA hinting faults are
    trapped. This should help workloads that are short-lived, do not care
    about NUMA placement or have bound themselves to a single node.
    
    This takes advantage of the logic in "mm: sched: numa: Implement slow
    start for working set sampling" to delay when the checks are made. This
    will take advantage of processes that set their CPU and node bindings
    early in their lifetime. It will also potentially allow any initial load
    balancing to take place.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8b20ab7d3aa2..296ea308096d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -820,6 +820,9 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 	mm->pmd_huge_pte = NULL;
+#endif
+#ifdef CONFIG_NUMA_BALANCING
+	mm->first_nid = NUMA_PTE_SCAN_INIT;
 #endif
 	if (!mm_init(mm, tsk))
 		goto fail_nomem;

commit e80d6661c3a5caa0cebec0853c6cb0db090fb506
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 23:10:08 2012 -0400

    flagday: kill pt_regs argument of do_fork()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0e68b6686acb..540730783433 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1527,8 +1527,6 @@ static inline void init_idle_pids(struct pid_link *links)
 struct task_struct * __cpuinit fork_idle(int cpu)
 {
 	struct task_struct *task;
-	struct pt_regs regs;
-
 	task = copy_process(CLONE_VM, 0, 0, NULL, &init_struct_pid, 0);
 	if (!IS_ERR(task)) {
 		init_idle_pids(task->pids);
@@ -1546,7 +1544,6 @@ struct task_struct * __cpuinit fork_idle(int cpu)
  */
 long do_fork(unsigned long clone_flags,
 	      unsigned long stack_start,
-	      struct pt_regs *regs,
 	      unsigned long stack_size,
 	      int __user *parent_tidptr,
 	      int __user *child_tidptr)
@@ -1576,7 +1573,7 @@ long do_fork(unsigned long clone_flags,
 	 * requested, no event is reported; otherwise, report if the event
 	 * for the type of forking is enabled.
 	 */
-	if (!(clone_flags & CLONE_UNTRACED) && likely(user_mode(regs))) {
+	if (!(clone_flags & CLONE_UNTRACED)) {
 		if (clone_flags & CLONE_VFORK)
 			trace = PTRACE_EVENT_VFORK;
 		else if ((clone_flags & CSIGNAL) != SIGCHLD)
@@ -1632,7 +1629,7 @@ long do_fork(unsigned long clone_flags,
  */
 pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 {
-	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn, NULL,
+	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn,
 		(unsigned long)arg, NULL, NULL);
 }
 #endif
@@ -1641,7 +1638,7 @@ pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 SYSCALL_DEFINE0(fork)
 {
 #ifdef CONFIG_MMU
-	return do_fork(SIGCHLD, 0, current_pt_regs(), 0, NULL, NULL);
+	return do_fork(SIGCHLD, 0, 0, NULL, NULL);
 #else
 	/* can not support in nommu mode */
 	return(-EINVAL);
@@ -1652,7 +1649,7 @@ SYSCALL_DEFINE0(fork)
 #ifdef __ARCH_WANT_SYS_VFORK
 SYSCALL_DEFINE0(vfork)
 {
-	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0, current_pt_regs(),
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0, 
 			0, NULL, NULL);
 }
 #endif
@@ -1675,7 +1672,7 @@ SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
 		 int, tls_val)
 #endif
 {
-	return do_fork(clone_flags, newsp, current_pt_regs(), 0,
+	return do_fork(clone_flags, newsp, 0,
 		parent_tidptr, child_tidptr);
 }
 #endif

commit 18c26c27ae0abe82253cb2e2363df465dbbb657e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:53:20 2012 -0400

    death to idle_regs()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index fa24a78b94f2..0e68b6686acb 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1514,12 +1514,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	return ERR_PTR(retval);
 }
 
-noinline struct pt_regs * __cpuinit __attribute__((weak)) idle_regs(struct pt_regs *regs)
-{
-	memset(regs, 0, sizeof(struct pt_regs));
-	return regs;
-}
-
 static inline void init_idle_pids(struct pid_link *links)
 {
 	enum pid_type type;

commit 62e791c1b8ea481c72c299dee4f62c04aaef765c
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:52:26 2012 -0400

    don't pass regs to copy_process()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index d96a562b1311..fa24a78b94f2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1127,7 +1127,6 @@ static void posix_cpu_timers_init(struct task_struct *tsk)
  */
 static struct task_struct *copy_process(unsigned long clone_flags,
 					unsigned long stack_start,
-					struct pt_regs *regs,
 					unsigned long stack_size,
 					int __user *child_tidptr,
 					struct pid *pid,
@@ -1536,8 +1535,7 @@ struct task_struct * __cpuinit fork_idle(int cpu)
 	struct task_struct *task;
 	struct pt_regs regs;
 
-	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL,
-			    &init_struct_pid, 0);
+	task = copy_process(CLONE_VM, 0, 0, NULL, &init_struct_pid, 0);
 	if (!IS_ERR(task)) {
 		init_idle_pids(task->pids);
 		init_idle(task, cpu);
@@ -1596,7 +1594,7 @@ long do_fork(unsigned long clone_flags,
 			trace = 0;
 	}
 
-	p = copy_process(clone_flags, stack_start, regs, stack_size,
+	p = copy_process(clone_flags, stack_start, stack_size,
 			 child_tidptr, NULL, trace);
 	/*
 	 * Do this prior waking up the new thread - the thread pointer

commit afa86fc426ff7e7f5477f15da9c405d08d5cf790
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:51:14 2012 -0400

    flagday: don't pass regs to copy_thread()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 27a337549dab..d96a562b1311 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1320,7 +1320,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	retval = copy_io(clone_flags, p);
 	if (retval)
 		goto bad_fork_cleanup_namespaces;
-	retval = copy_thread(clone_flags, stack_start, stack_size, p, regs);
+	retval = copy_thread(clone_flags, stack_start, stack_size, p);
 	if (retval)
 		goto bad_fork_cleanup_io;
 

commit d2125043aebf7f53cd1c72115c17b01d0bc06ce1
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Oct 23 13:17:59 2012 -0400

    generic sys_fork / sys_vfork / sys_clone
    
    ... and get rid of idiotic struct pt_regs * in asm-generic/syscalls.h
    prototypes of the same, while we are at it.  Eventually we want those
    in linux/syscalls.h, of course, but that'll have to wait a bit.
    
    Note that there are *three* variants of sys_clone() order of arguments.
    Braindamage galore...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8b20ab7d3aa2..27a337549dab 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1645,6 +1645,49 @@ pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
 }
 #endif
 
+#ifdef __ARCH_WANT_SYS_FORK
+SYSCALL_DEFINE0(fork)
+{
+#ifdef CONFIG_MMU
+	return do_fork(SIGCHLD, 0, current_pt_regs(), 0, NULL, NULL);
+#else
+	/* can not support in nommu mode */
+	return(-EINVAL);
+#endif
+}
+#endif
+
+#ifdef __ARCH_WANT_SYS_VFORK
+SYSCALL_DEFINE0(vfork)
+{
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0, current_pt_regs(),
+			0, NULL, NULL);
+}
+#endif
+
+#ifdef __ARCH_WANT_SYS_CLONE
+#ifdef CONFIG_CLONE_BACKWARDS
+SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
+		 int __user *, parent_tidptr,
+		 int, tls_val,
+		 int __user *, child_tidptr)
+#elif defined(CONFIG_CLONE_BACKWARDS2)
+SYSCALL_DEFINE5(clone, unsigned long, newsp, unsigned long, clone_flags,
+		 int __user *, parent_tidptr,
+		 int __user *, child_tidptr,
+		 int, tls_val)
+#else
+SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
+		 int __user *, parent_tidptr,
+		 int __user *, child_tidptr,
+		 int, tls_val)
+#endif
+{
+	return do_fork(clone_flags, newsp, current_pt_regs(), 0,
+		parent_tidptr, child_tidptr);
+}
+#endif
+
 #ifndef ARCH_MIN_MMSTRUCT_ALIGN
 #define ARCH_MIN_MMSTRUCT_ALIGN 0
 #endif

commit d37f761dbd276790f70dcf73a287fde2c3464482
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 22 00:58:35 2012 +0100

    cputime: Consolidate cputime adjustment code
    
    task_cputime_adjusted() and thread_group_cputime_adjusted()
    essentially share the same code. They just don't use the same
    source:
    
    * The first function uses the cputime in the task struct and the
    previous adjusted snapshot that ensures monotonicity.
    
    * The second adds the cputime of all tasks in the group and the
    previous adjusted snapshot of the whole group from the signal
    structure.
    
    Just consolidate the common code that does the adjustment. These
    functions just need to fetch the values from the appropriate
    source.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8b20ab7d3aa2..0e7cdb90476f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1222,7 +1222,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->utime = p->stime = p->gtime = 0;
 	p->utimescaled = p->stimescaled = 0;
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING
-	p->prev_utime = p->prev_stime = 0;
+	p->prev_cputime.utime = p->prev_cputime.stime = 0;
 #endif
 #if defined(SPLIT_RSS_COUNTING)
 	memset(&p->rss_stat, 0, sizeof(p->rss_stat));

commit b2e0d98705e60e45bbb3c0032c48824ad7ae0704
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Jul 26 05:15:35 2012 -0700

    userns: Implement unshare of the user namespace
    
    - Add CLONE_THREAD to the unshare flags if CLONE_NEWUSER is selected
      As changing user namespaces is only valid if all there is only
      a single thread.
    - Restore the code to add CLONE_VM if CLONE_THREAD is selected and
      the code to addCLONE_SIGHAND if CLONE_VM is selected.
      Making the constraints in the code clear.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8c29abb19014..38e53b87402c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1687,7 +1687,7 @@ static int check_unshare_flags(unsigned long unshare_flags)
 	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
 				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
-				CLONE_NEWPID))
+				CLONE_NEWUSER|CLONE_NEWPID))
 		return -EINVAL;
 	/*
 	 * Not implemented, but pretend it works if there is nothing to
@@ -1754,10 +1754,16 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 {
 	struct fs_struct *fs, *new_fs = NULL;
 	struct files_struct *fd, *new_fd = NULL;
+	struct cred *new_cred = NULL;
 	struct nsproxy *new_nsproxy = NULL;
 	int do_sysvsem = 0;
 	int err;
 
+	/*
+	 * If unsharing a user namespace must also unshare the thread.
+	 */
+	if (unshare_flags & CLONE_NEWUSER)
+		unshare_flags |= CLONE_THREAD;
 	/*
 	 * If unsharing a pid namespace must also unshare the thread.
 	 */
@@ -1795,11 +1801,15 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 	err = unshare_fd(unshare_flags, &new_fd);
 	if (err)
 		goto bad_unshare_cleanup_fs;
-	err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy, new_fs);
+	err = unshare_userns(unshare_flags, &new_cred);
 	if (err)
 		goto bad_unshare_cleanup_fd;
+	err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,
+					 new_cred, new_fs);
+	if (err)
+		goto bad_unshare_cleanup_cred;
 
-	if (new_fs || new_fd || do_sysvsem || new_nsproxy) {
+	if (new_fs || new_fd || do_sysvsem || new_cred || new_nsproxy) {
 		if (do_sysvsem) {
 			/*
 			 * CLONE_SYSVSEM is equivalent to sys_exit().
@@ -1832,11 +1842,20 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 		}
 
 		task_unlock(current);
+
+		if (new_cred) {
+			/* Install the new user namespace */
+			commit_creds(new_cred);
+			new_cred = NULL;
+		}
 	}
 
 	if (new_nsproxy)
 		put_nsproxy(new_nsproxy);
 
+bad_unshare_cleanup_cred:
+	if (new_cred)
+		put_cred(new_cred);
 bad_unshare_cleanup_fd:
 	if (new_fd)
 		put_files_struct(new_fd);

commit 5eaf563e53294d6696e651466697eb9d491f3946
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Nov 21 17:22:31 2011 -0800

    userns: Allow unprivileged users to create user namespaces.
    
    Now that we have been through every permission check in the kernel
    having uid == 0 and gid == 0 in your local user namespace no
    longer adds any special privileges.  Even having a full set
    of caps in your local user namespace is safe because capabilies
    are relative to your local user namespace, and do not confer
    unexpected privileges.
    
    Over the long term this should allow much more of the kernels
    functionality to be safely used by non-root users.  Functionality
    like unsharing the mount namespace that is only unsafe because
    it can fool applications whose privileges are raised when they
    are executed.  Since those applications have no privileges in
    a user namespaces it becomes safe to spoof and confuse those
    applications all you want.
    
    Those capabilities will still need to be enabled carefully because
    we may still need things like rlimits on the number of unprivileged
    mounts but that is to avoid DOS attacks not to avoid fooling root
    owned processes.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 811ffbad7889..8c29abb19014 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1569,14 +1569,6 @@ long do_fork(unsigned long clone_flags,
 		if (clone_flags & (CLONE_THREAD|CLONE_PARENT))
 			return -EINVAL;
 	}
-	if (clone_flags & CLONE_NEWUSER) {
-		/* hopefully this check will go away when userns support is
-		 * complete
-		 */
-		if (!capable(CAP_SYS_ADMIN) || !capable(CAP_SETUID) ||
-				!capable(CAP_SETGID))
-			return -EPERM;
-	}
 
 	/*
 	 * Determine whether and which event to report to ptracer.  When

commit 50804fe3737ca6a5942fdc2057a18a8141d00141
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Mar 2 15:41:50 2010 -0800

    pidns: Support unsharing the pid namespace.
    
    Unsharing of the pid namespace unlike unsharing of other namespaces
    does not take affect immediately.  Instead it affects the children
    created with fork and clone.  The first of these children becomes the init
    process of the new pid namespace, the rest become oddball children
    of pid 0.  From the point of view of the new pid namespace the process
    that created it is pid 0, as it's pid does not map.
    
    A couple of different semantics were considered but this one was
    settled on because it is easy to implement and it is usable from
    pam modules.  The core reasons for the existence of unshare.
    
    I took a survey of the callers of pam modules and the following
    appears to be a representative sample of their logic.
    {
            setup stuff include pam
            child = fork();
            if (!child) {
                    setuid()
                    exec /bin/bash
            }
            waitpid(child);
    
            pam and other cleanup
    }
    
    As you can see there is a fork to create the unprivileged user
    space process.  Which means that the unprivileged user space
    process will appear as pid 1 in the new pid namespace.  Further
    most login processes do not cope with extraneous children which
    means shifting the duty of reaping extraneous child process to
    the creator of those extraneous children makes the system more
    comprehensible.
    
    The practical reason for this set of pid namespace semantics is
    that it is simple to implement and verify they work correctly.
    Whereas an implementation that requres changing the struct
    pid on a process comes with a lot more races and pain.  Not
    the least of which is that glibc caches getpid().
    
    These semantics are implemented by having two notions
    of the pid namespace of a proces.  There is task_active_pid_ns
    which is the pid namspace the process was created with
    and the pid namespace that all pids are presented to
    that process in.  The task_active_pid_ns is stored
    in the struct pid of the task.
    
    Then there is the pid namespace that will be used for children
    that pid namespace is stored in task->nsproxy->pid_ns.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0f2bbce311fc..811ffbad7889 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1565,9 +1565,11 @@ long do_fork(unsigned long clone_flags,
 	 * Do some preliminary argument and permissions checking before we
 	 * actually start allocating stuff
 	 */
-	if (clone_flags & CLONE_NEWUSER) {
-		if (clone_flags & CLONE_THREAD)
+	if (clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) {
+		if (clone_flags & (CLONE_THREAD|CLONE_PARENT))
 			return -EINVAL;
+	}
+	if (clone_flags & CLONE_NEWUSER) {
 		/* hopefully this check will go away when userns support is
 		 * complete
 		 */
@@ -1692,7 +1694,8 @@ static int check_unshare_flags(unsigned long unshare_flags)
 {
 	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
-				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET))
+				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET|
+				CLONE_NEWPID))
 		return -EINVAL;
 	/*
 	 * Not implemented, but pretend it works if there is nothing to
@@ -1763,15 +1766,30 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 	int do_sysvsem = 0;
 	int err;
 
-	err = check_unshare_flags(unshare_flags);
-	if (err)
-		goto bad_unshare_out;
-
+	/*
+	 * If unsharing a pid namespace must also unshare the thread.
+	 */
+	if (unshare_flags & CLONE_NEWPID)
+		unshare_flags |= CLONE_THREAD;
+	/*
+	 * If unsharing a thread from a thread group, must also unshare vm.
+	 */
+	if (unshare_flags & CLONE_THREAD)
+		unshare_flags |= CLONE_VM;
+	/*
+	 * If unsharing vm, must also unshare signal handlers.
+	 */
+	if (unshare_flags & CLONE_VM)
+		unshare_flags |= CLONE_SIGHAND;
 	/*
 	 * If unsharing namespace, must also unshare filesystem information.
 	 */
 	if (unshare_flags & CLONE_NEWNS)
 		unshare_flags |= CLONE_FS;
+
+	err = check_unshare_flags(unshare_flags);
+	if (err)
+		goto bad_unshare_out;
 	/*
 	 * CLONE_NEWIPC must also detach from the undolist: after switching
 	 * to a new ipc namespace, the semaphore arrays from the old

commit 1c4042c29bd2e85aac4110552ca8ade763762e84
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jul 12 17:10:36 2010 -0700

    pidns: Consolidate initialzation of special init task state
    
    Instead of setting child_reaper and SIGNAL_UNKILLABLE one way
    for the system init process, and another way for pid namespace
    init processes test pid->nr == 1 and use the same code for both.
    
    For the global init this results in SIGNAL_UNKILLABLE being set
    much earlier in the initialization process.
    
    This is a small cleanup and it paves the way for allowing unshare and
    enter of the pid namespace as that path like our global init also will
    not set CLONE_NEWPID.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 666dc8b06606..0f2bbce311fc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1039,8 +1039,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	atomic_set(&sig->live, 1);
 	atomic_set(&sig->sigcnt, 1);
 	init_waitqueue_head(&sig->wait_chldexit);
-	if (clone_flags & CLONE_NEWPID)
-		sig->flags |= SIGNAL_UNKILLABLE;
 	sig->curr_target = tsk;
 	init_sigpending(&sig->shared_pending);
 	INIT_LIST_HEAD(&sig->posix_timers);
@@ -1441,8 +1439,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		ptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);
 
 		if (thread_group_leader(p)) {
-			if (is_child_reaper(pid))
+			if (is_child_reaper(pid)) {
 				ns_of_pid(pid)->child_reaper = p;
+				p->signal->flags |= SIGNAL_UNKILLABLE;
+			}
 
 			p->signal->leader_pid = pid;
 			p->signal->tty = tty_kref_get(current->signal->tty);

commit 0a01f2cc390e10633a54f72c608cc3fe19a50c3d
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Aug 1 10:33:47 2012 -0700

    pidns: Make the pidns proc mount/umount logic obvious.
    
    Track the number of pids in the proc hash table.  When the number of
    pids goes to 0 schedule work to unmount the kernel mount of proc.
    
    Move the mount of proc into alloc_pid when we allocate the pid for
    init.
    
    Remove the surprising calls of pid_ns_release proc in fork and
    proc_flush_task.  Those code paths really shouldn't know about proc
    namespace implementation details and people have demonstrated several
    times that finding and understanding those code paths is difficult and
    non-obvious.
    
    Because of the call path detach pid is alwasy called with the
    rtnl_lock held free_pid is not allowed to sleep, so the work to
    unmounting proc is moved to a work queue.  This has the side benefit
    of not blocking the entire world waiting for the unnecessary
    rcu_barrier in deactivate_locked_super.
    
    In the process of making the code clear and obvious this fixes a bug
    reported by Gao feng <gaofeng@cn.fujitsu.com> where we would leak a
    mount of proc during clone(CLONE_NEWPID|CLONE_NEWNET) if copy_pid_ns
    succeeded and copy_net_ns failed.
    
    Acked-by: "Serge E. Hallyn" <serge@hallyn.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7798c247f4b9..666dc8b06606 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1476,8 +1476,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (p->io_context)
 		exit_io_context(p);
 bad_fork_cleanup_namespaces:
-	if (unlikely(clone_flags & CLONE_NEWPID))
-		pid_ns_release_proc(p->nsproxy->pid_ns);
 	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
 	if (p->mm)

commit 17cf22c33e1f1b5e435469c84e43872579497653
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Mar 2 14:51:53 2010 -0800

    pidns: Use task_active_pid_ns where appropriate
    
    The expressions tsk->nsproxy->pid_ns and task_active_pid_ns
    aka ns_of_pid(task_pid(tsk)) should have the same number of
    cache line misses with the practical difference that
    ns_of_pid(task_pid(tsk)) is released later in a processes life.
    
    Furthermore by using task_active_pid_ns it becomes trivial
    to write an unshare implementation for the the pid namespace.
    
    So I have used task_active_pid_ns everywhere I can.
    
    In fork since the pid has not yet been attached to the
    process I use ns_of_pid, to achieve the same effect.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8b20ab7d3aa2..7798c247f4b9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1442,7 +1442,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 		if (thread_group_leader(p)) {
 			if (is_child_reaper(pid))
-				p->nsproxy->pid_ns->child_reaper = p;
+				ns_of_pid(pid)->child_reaper = p;
 
 			p->signal->leader_pid = pid;
 			p->signal->tty = tty_kref_get(current->signal->tty);

commit 32cdba1e05418909708a17e52505e8b2ba4381d1
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Nov 14 19:03:42 2012 +0100

    uprobes: Use percpu_rw_semaphore to fix register/unregister vs dup_mmap() race
    
    This was always racy, but 268720903f87e0b84b161626c4447b81671b5d18
    "uprobes: Rework register_for_each_vma() to make it O(n)" should be
    blamed anyway, it made everything worse and I didn't notice.
    
    register/unregister call build_map_info() and then do install/remove
    breakpoint for every mm which mmaps inode/offset. This can obviously
    race with fork()->dup_mmap() in between and we can miss the child.
    
    uprobe_register() could be easily fixed but unregister is much worse,
    the new mm inherits "int3" from parent and there is no way to detect
    this if uprobe goes away.
    
    So this patch simply adds percpu_down_read/up_read around dup_mmap(),
    and percpu_down_write/up_write into register_for_each_vma().
    
    This adds 2 new hooks into dup_mmap() but we can kill uprobe_dup_mmap()
    and fold it into uprobe_end_dup_mmap().
    
    Reported-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8b20ab7d3aa2..c497e57aa654 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -352,6 +352,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	unsigned long charge;
 	struct mempolicy *pol;
 
+	uprobe_start_dup_mmap();
 	down_write(&oldmm->mmap_sem);
 	flush_cache_dup_mm(oldmm);
 	uprobe_dup_mmap(oldmm, mm);
@@ -469,6 +470,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	up_write(&mm->mmap_sem);
 	flush_tlb_mm(oldmm);
 	up_write(&oldmm->mmap_sem);
+	uprobe_end_dup_mmap();
 	return retval;
 fail_nomem_anon_vma_fork:
 	mpol_put(pol);

commit 5edee61edeaaebafe584f8fb7074c1ef4658596b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Oct 16 15:03:14 2012 -0700

    cgroup: cgroup_subsys->fork() should be called after the task is added to css_set
    
    cgroup core has a bug which violates a basic rule about event
    notifications - when a new entity needs to be added, you add that to
    the notification list first and then make the new entity conform to
    the current state.  If done in the reverse order, an event happening
    inbetween will be lost.
    
    cgroup_subsys->fork() is invoked way before the new task is added to
    the css_set.  Currently, cgroup_freezer is the only user of ->fork()
    and uses it to make new tasks conform to the current state of the
    freezer.  If FROZEN state is requested while fork is in progress
    between cgroup_fork_callbacks() and cgroup_post_fork(), the child
    could escape freezing - the cgroup isn't frozen when ->fork() is
    called and the freezer couldn't see the new task on the css_set.
    
    This patch moves cgroup_subsys->fork() invocation to
    cgroup_post_fork() after the new task is added to the css_set.
    cgroup_fork_callbacks() is removed.
    
    Because now a task may be migrated during cgroup_subsys->fork(),
    freezer_fork() is updated so that it adheres to the usual RCU locking
    and the rather pointless comment on why locking can be different there
    is removed (if it doesn't make anything simpler, why even bother?).
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: stable@vger.kernel.org

diff --git a/kernel/fork.c b/kernel/fork.c
index 8b20ab7d3aa2..acc4cb62f32f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1135,7 +1135,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 {
 	int retval;
 	struct task_struct *p;
-	int cgroup_callbacks_done = 0;
 
 	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
 		return ERR_PTR(-EINVAL);
@@ -1393,12 +1392,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	INIT_LIST_HEAD(&p->thread_group);
 	p->task_works = NULL;
 
-	/* Now that the task is set up, run cgroup callbacks if
-	 * necessary. We need to run them before the task is visible
-	 * on the tasklist. */
-	cgroup_fork_callbacks(p);
-	cgroup_callbacks_done = 1;
-
 	/* Need tasklist lock for parent etc handling! */
 	write_lock_irq(&tasklist_lock);
 
@@ -1503,7 +1496,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 	if (clone_flags & CLONE_THREAD)
 		threadgroup_change_end(current);
-	cgroup_exit(p, cgroup_callbacks_done);
+	cgroup_exit(p, 0);
 	delayacct_tsk_free(p);
 	module_put(task_thread_info(p)->exec_domain->module);
 bad_fork_cleanup_count:

commit 42859eea96ba6beabfb0369a1eeffa3c7d2bd9cb
Merge: f59b51fe3d30 f322220d6159
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 10 12:02:25 2012 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull generic execve() changes from Al Viro:
     "This introduces the generic kernel_thread() and kernel_execve()
      functions, and switches x86, arm, alpha, um and s390 over to them."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (26 commits)
      s390: convert to generic kernel_execve()
      s390: switch to generic kernel_thread()
      s390: fold kernel_thread_helper() into ret_from_fork()
      s390: fold execve_tail() into start_thread(), convert to generic sys_execve()
      um: switch to generic kernel_thread()
      x86, um/x86: switch to generic sys_execve and kernel_execve
      x86: split ret_from_fork
      alpha: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      alpha: switch to generic kernel_thread()
      alpha: switch to generic sys_execve()
      arm: get rid of execve wrapper, switch to generic execve() implementation
      arm: optimized current_pt_regs()
      arm: introduce ret_from_kernel_execve(), switch to generic kernel_execve()
      arm: split ret_from_fork, simplify kernel_thread() [based on patch by rmk]
      generic sys_execve()
      generic kernel_execve()
      new helper: current_pt_regs()
      preparation for generic kernel_thread()
      um: kill thread->forking
      um: let signal_delivered() do SIGTRAP on singlestepping into handler
      ...

commit 9826a516ff77c5820e591211e4f3e58ff36f46be
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Oct 8 16:31:35 2012 -0700

    mm: interval tree updates
    
    Update the generic interval tree code that was introduced in "mm: replace
    vma prio_tree with an interval tree".
    
    Changes:
    
    - fixed 'endpoing' typo noticed by Andrew Morton
    
    - replaced include/linux/interval_tree_tmpl.h, which was used as a
      template (including it automatically defined the interval tree
      functions) with include/linux/interval_tree_generic.h, which only
      defines a preprocessor macro INTERVAL_TREE_DEFINE(), which itself
      defines the interval tree functions when invoked. Now that is a very
      long macro which is unfortunate, but it does make the usage sites
      (lib/interval_tree.c and mm/interval_tree.c) a bit nicer than previously.
    
    - make use of RB_DECLARE_CALLBACKS() in the INTERVAL_TREE_DEFINE() macro,
      instead of duplicating that code in the interval tree template.
    
    - replaced vma_interval_tree_add(), which was actually handling the
      nonlinear and interval tree cases, with vma_interval_tree_insert_after()
      which handles only the interval tree case and has an API that is more
      consistent with the other interval tree handling functions.
      The nonlinear case is now handled explicitly in kernel/fork.c dup_mmap().
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Daniel Santos <daniel.santos@pobox.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 90dace52715e..1cd7d581b3b2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -423,7 +423,12 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 				mapping->i_mmap_writable++;
 			flush_dcache_mmap_lock(mapping);
 			/* insert tmp into the share list, just after mpnt */
-			vma_interval_tree_add(tmp, mpnt, mapping);
+			if (unlikely(tmp->vm_flags & VM_NONLINEAR))
+				vma_nonlinear_insert(tmp,
+						&mapping->i_mmap_nonlinear);
+			else
+				vma_interval_tree_insert_after(tmp, mpnt,
+							&mapping->i_mmap);
 			flush_dcache_mmap_unlock(mapping);
 			mutex_unlock(&mapping->i_mmap_mutex);
 		}

commit 6b2dbba8b6ac4df26f72eda1e5ea7bab9f950e08
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Oct 8 16:31:25 2012 -0700

    mm: replace vma prio_tree with an interval tree
    
    Implement an interval tree as a replacement for the VMA prio_tree.  The
    algorithms are similar to lib/interval_tree.c; however that code can't be
    directly reused as the interval endpoints are not explicitly stored in the
    VMA.  So instead, the common algorithm is moved into a template and the
    details (node type, how to get interval endpoints from the node, etc) are
    filled in using the C preprocessor.
    
    Once the interval tree functions are available, using them as a
    replacement to the VMA prio tree is a relatively simple, mechanical job.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 972762e01024..90dace52715e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -423,7 +423,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 				mapping->i_mmap_writable++;
 			flush_dcache_mmap_lock(mapping);
 			/* insert tmp into the share list, just after mpnt */
-			vma_prio_tree_add(tmp, mpnt);
+			vma_interval_tree_add(tmp, mpnt, mapping);
 			flush_dcache_mmap_unlock(mapping);
 			mutex_unlock(&mapping->i_mmap_mutex);
 		}

commit 01dc52ebdf472f77cca623ca693ca24cfc0f1bbe
Author: Davidlohr Bueso <dave@gnu.org>
Date:   Mon Oct 8 16:29:30 2012 -0700

    oom: remove deprecated oom_adj
    
    The deprecated /proc/<pid>/oom_adj is scheduled for removal this month.
    
    Signed-off-by: Davidlohr Bueso <dave@gnu.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ec667f797af3..972762e01024 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1056,7 +1056,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	init_rwsem(&sig->group_rwsem);
 #endif
 
-	sig->oom_adj = current->signal->oom_adj;
 	sig->oom_score_adj = current->signal->oom_score_adj;
 	sig->oom_score_adj_min = current->signal->oom_score_adj_min;
 

commit e9714acf8c439688884234dcac2bfc38bb607d38
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Mon Oct 8 16:28:54 2012 -0700

    mm: kill vma flag VM_EXECUTABLE and mm->num_exe_file_vmas
    
    Currently the kernel sets mm->exe_file during sys_execve() and then tracks
    number of vmas with VM_EXECUTABLE flag in mm->num_exe_file_vmas, as soon
    as this counter drops to zero kernel resets mm->exe_file to NULL.  Plus it
    resets mm->exe_file at last mmput() when mm->mm_users drops to zero.
    
    VMA with VM_EXECUTABLE flag appears after mapping file with flag
    MAP_EXECUTABLE, such vmas can appears only at sys_execve() or after vma
    splitting, because sys_mmap ignores this flag.  Usually binfmt module sets
    mm->exe_file and mmaps executable vmas with this file, they hold
    mm->exe_file while task is running.
    
    comment from v2.6.25-6245-g925d1c4 ("procfs task exe symlink"),
    where all this stuff was introduced:
    
    > The kernel implements readlink of /proc/pid/exe by getting the file from
    > the first executable VMA.  Then the path to the file is reconstructed and
    > reported as the result.
    >
    > Because of the VMA walk the code is slightly different on nommu systems.
    > This patch avoids separate /proc/pid/exe code on nommu systems.  Instead of
    > walking the VMAs to find the first executable file-backed VMA we store a
    > reference to the exec'd file in the mm_struct.
    >
    > That reference would prevent the filesystem holding the executable file
    > from being unmounted even after unmapping the VMAs.  So we track the number
    > of VM_EXECUTABLE VMAs and drop the new reference when the last one is
    > unmapped.  This avoids pinning the mounted filesystem.
    
    exe_file's vma accounting is hooked into every file mmap/unmmap and vma
    split/merge just to fix some hypothetical pinning fs from umounting by mm,
    which already unmapped all its executable files, but still alive.
    
    Seems like currently nobody depends on this behaviour.  We can try to
    remove this logic and keep mm->exe_file until final mmput().
    
    mm->exe_file is still protected with mm->mmap_sem, because we want to
    change it via new sys_prctl(PR_SET_MM_EXE_FILE).  Also via this syscall
    task can change its mm->exe_file and unpin mountpoint explicitly.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Carsten Otte <cotte@de.ibm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James Morris <james.l.morris@oracle.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Kentaro Takeda <takedakn@nttdata.co.jp>
    Cc: Matt Helsley <matthltc@us.ibm.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a57a993681ed..ec667f797af3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -622,26 +622,6 @@ void mmput(struct mm_struct *mm)
 }
 EXPORT_SYMBOL_GPL(mmput);
 
-/*
- * We added or removed a vma mapping the executable. The vmas are only mapped
- * during exec and are not mapped with the mmap system call.
- * Callers must hold down_write() on the mm's mmap_sem for these
- */
-void added_exe_file_vma(struct mm_struct *mm)
-{
-	mm->num_exe_file_vmas++;
-}
-
-void removed_exe_file_vma(struct mm_struct *mm)
-{
-	mm->num_exe_file_vmas--;
-	if ((mm->num_exe_file_vmas == 0) && mm->exe_file) {
-		fput(mm->exe_file);
-		mm->exe_file = NULL;
-	}
-
-}
-
 void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file)
 {
 	if (new_exe_file)
@@ -649,7 +629,6 @@ void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file)
 	if (mm->exe_file)
 		fput(mm->exe_file);
 	mm->exe_file = new_exe_file;
-	mm->num_exe_file_vmas = 0;
 }
 
 struct file *get_mm_exe_file(struct mm_struct *mm)

commit 2dd8ad81e31d0d36a5d448329c646ab43eb17788
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Mon Oct 8 16:28:51 2012 -0700

    mm: use mm->exe_file instead of first VM_EXECUTABLE vma->vm_file
    
    Some security modules and oprofile still uses VM_EXECUTABLE for retrieving
    a task's executable file.  After this patch they will use mm->exe_file
    directly.  mm->exe_file is protected with mm->mmap_sem, so locking stays
    the same.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>                   [arch/tile]
    Acked-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>     [tomoyo]
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Carsten Otte <cotte@de.ibm.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Acked-by: James Morris <james.l.morris@oracle.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Kentaro Takeda <takedakn@nttdata.co.jp>
    Cc: Matt Helsley <matthltc@us.ibm.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Suresh Siddha <suresh.b.siddha@intel.com>
    Cc: Venkatesh Pallipadi <venki@google.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a2b1efc20928..a57a993681ed 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -656,8 +656,7 @@ struct file *get_mm_exe_file(struct mm_struct *mm)
 {
 	struct file *exe_file;
 
-	/* We need mmap_sem to protect against races with removal of
-	 * VM_EXECUTABLE vmas */
+	/* We need mmap_sem to protect against races with removal of exe_file */
 	down_read(&mm->mmap_sem);
 	exe_file = mm->exe_file;
 	if (exe_file)

commit aecdc33e111b2c447b622e287c6003726daa1426
Merge: a20acf99f75e a3a6cab5ea10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 13:38:27 2012 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking changes from David Miller:
    
     1) GRE now works over ipv6, from Dmitry Kozlov.
    
     2) Make SCTP more network namespace aware, from Eric Biederman.
    
     3) TEAM driver now works with non-ethernet devices, from Jiri Pirko.
    
     4) Make openvswitch network namespace aware, from Pravin B Shelar.
    
     5) IPV6 NAT implementation, from Patrick McHardy.
    
     6) Server side support for TCP Fast Open, from Jerry Chu and others.
    
     7) Packet BPF filter supports MOD and XOR, from Eric Dumazet and Daniel
        Borkmann.
    
     8) Increate the loopback default MTU to 64K, from Eric Dumazet.
    
     9) Use a per-task rather than per-socket page fragment allocator for
        outgoing networking traffic.  This benefits processes that have very
        many mostly idle sockets, which is quite common.
    
        From Eric Dumazet.
    
    10) Use up to 32K for page fragment allocations, with fallbacks to
        smaller sizes when higher order page allocations fail.  Benefits are
        a) less segments for driver to process b) less calls to page
        allocator c) less waste of space.
    
        From Eric Dumazet.
    
    11) Allow GRO to be used on GRE tunnels, from Eric Dumazet.
    
    12) VXLAN device driver, one way to handle VLAN issues such as the
        limitation of 4096 VLAN IDs yet still have some level of isolation.
        From Stephen Hemminger.
    
    13) As usual there is a large boatload of driver changes, with the scale
        perhaps tilted towards the wireless side this time around.
    
    Fix up various fairly trivial conflicts, mostly caused by the user
    namespace changes.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1012 commits)
      hyperv: Add buffer for extended info after the RNDIS response message.
      hyperv: Report actual status in receive completion packet
      hyperv: Remove extra allocated space for recv_pkt_list elements
      hyperv: Fix page buffer handling in rndis_filter_send_request()
      hyperv: Fix the missing return value in rndis_filter_set_packet_filter()
      hyperv: Fix the max_xfer_size in RNDIS initialization
      vxlan: put UDP socket in correct namespace
      vxlan: Depend on CONFIG_INET
      sfc: Fix the reported priorities of different filter types
      sfc: Remove EFX_FILTER_FLAG_RX_OVERRIDE_IP
      sfc: Fix loopback self-test with separate_tx_channels=1
      sfc: Fix MCDI structure field lookup
      sfc: Add parentheses around use of bitfield macro arguments
      sfc: Fix null function pointer in efx_sriov_channel_type
      vxlan: virtual extensible lan
      igmp: export symbol ip_mc_leave_group
      netlink: add attributes to fdb interface
      tg3: unconditionally select HWMON support when tg3 is enabled.
      Revert "net: ti cpsw ethernet: allow reading phy interface mode from DT"
      gre: fix sparse warning
      ...

commit 0b981cb94bc63a2d0e5eccccdca75fe57643ffce
Merge: 4cba3335826c fdf9c356502a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 10:43:39 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "Continued quest to clean up and enhance the cputime code by Frederic
      Weisbecker, in preparation for future tickless kernel features.
    
      Other than that, smallish changes."
    
    Fix up trivial conflicts due to additions next to each other in arch/{x86/}Kconfig
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      cputime: Make finegrained irqtime accounting generally available
      cputime: Gather time/stats accounting config options into a single menu
      ia64: Reuse system and user vtime accounting functions on task switch
      ia64: Consolidate user vtime accounting
      vtime: Consolidate system/idle context detection
      cputime: Use a proper subsystem naming for vtime related APIs
      sched: cpu_power: enable ARCH_POWER
      sched/nohz: Clean up select_nohz_load_balancer()
      sched: Fix load avg vs. cpu-hotplug
      sched: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW
      sched: Fix nohz_idle_balance()
      sched: Remove useless code in yield_to()
      sched: Add time unit suffix to sched sysctl knobs
      sched/debug: Limit sd->*_idx range on sysctl
      sched: Remove AFFINE_WAKEUPS feature flag
      s390: Remove leftover account_tick_vtime() header
      cputime: Consolidate vtime handling on context switch
      sched: Move cputime code to its own file
      cputime: Generalize CONFIG_VIRT_CPU_ACCOUNTING
      tile: Remove SD_PREFER_LOCAL leftover
      ...

commit 2aa3a7f8660355c3dddead17e224545c1a3d5a5f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Sep 21 19:55:31 2012 -0400

    preparation for generic kernel_thread()
    
    Let architectures select GENERIC_KERNEL_THREAD and have their copy_thread()
    treat NULL regs as "it came from kernel_thread(), sp argument contains
    the function new thread will be calling and stack_size - the argument for
    that function".  Switching the architectures begins shortly...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2c8857e12855..a42c62a8eb24 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1609,7 +1609,7 @@ long do_fork(unsigned long clone_flags,
 	 * requested, no event is reported; otherwise, report if the event
 	 * for the type of forking is enabled.
 	 */
-	if (likely(user_mode(regs)) && !(clone_flags & CLONE_UNTRACED)) {
+	if (!(clone_flags & CLONE_UNTRACED) && likely(user_mode(regs))) {
 		if (clone_flags & CLONE_VFORK)
 			trace = PTRACE_EVENT_VFORK;
 		else if ((clone_flags & CSIGNAL) != SIGCHLD)
@@ -1659,6 +1659,17 @@ long do_fork(unsigned long clone_flags,
 	return nr;
 }
 
+#ifdef CONFIG_GENERIC_KERNEL_THREAD
+/*
+ * Create a kernel thread.
+ */
+pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
+{
+	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, (unsigned long)fn, NULL,
+		(unsigned long)arg, NULL, NULL);
+}
+#endif
+
 #ifndef ARCH_MIN_MMSTRUCT_ALIGN
 #define ARCH_MIN_MMSTRUCT_ALIGN 0
 #endif

commit 5640f7685831e088fe6c2e1f863a6805962f8e81
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Sep 23 23:04:42 2012 +0000

    net: use a per task frag allocator
    
    We currently use a per socket order-0 page cache for tcp_sendmsg()
    operations.
    
    This page is used to build fragments for skbs.
    
    Its done to increase probability of coalescing small write() into
    single segments in skbs still in write queue (not yet sent)
    
    But it wastes a lot of memory for applications handling many mostly
    idle sockets, since each socket holds one page in sk->sk_sndmsg_page
    
    Its also quite inefficient to build TSO 64KB packets, because we need
    about 16 pages per skb on arches where PAGE_SIZE = 4096, so we hit
    page allocator more than wanted.
    
    This patch adds a per task frag allocator and uses bigger pages,
    if available. An automatic fallback is done in case of memory pressure.
    
    (up to 32768 bytes per frag, thats order-3 pages on x86)
    
    This increases TCP stream performance by 20% on loopback device,
    but also benefits on other network devices, since 8x less frags are
    mapped on transmit and unmapped on tx completion. Alexander Duyck
    mentioned a probable performance win on systems with IOMMU enabled.
    
    Its possible some SG enabled hardware cant cope with bigger fragments,
    but their ndo_start_xmit() should already handle this, splitting a
    fragment in sub fragments, since some arches have PAGE_SIZE=65536
    
    Successfully tested on various ethernet devices.
    (ixgbe, igb, bnx2x, tg3, mellanox mlx4)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Cc: Vijay Subramanian <subramanian.vijay@gmail.com>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Tested-by: Vijay Subramanian <subramanian.vijay@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2c8857e12855..01565b9ce0f3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -330,6 +330,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	tsk->btrace_seq = 0;
 #endif
 	tsk->splice_pipe = NULL;
+	tsk->task_frag.page = NULL;
 
 	account_kernel_stack(ti, 1);
 

commit f3e947867478af9a12b9956bcd000ac7613a8a95
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 12 11:22:00 2012 +0200

    sched: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW
    
    Now that the last architecture to use this has stopped doing so (ARM,
    thanks Catalin!) we can remove this complexity from the scheduler
    core.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Link: http://lkml.kernel.org/n/tip-g9p2a1w81xxbrze25v9zpzbf@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2c8857e12855..743d48f4d711 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1280,11 +1280,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	p->irq_events = 0;
-#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW
-	p->hardirqs_enabled = 1;
-#else
 	p->hardirqs_enabled = 0;
-#endif
 	p->hardirq_enable_ip = 0;
 	p->hardirq_enable_event = 0;
 	p->hardirq_disable_ip = _THIS_IP_;

commit 61559a8165da2b6bab7621ac36379c6280efacb6
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Aug 8 17:17:46 2012 +0200

    uprobes: Fold uprobe_reset_state() into uprobe_dup_mmap()
    
    Now that we have uprobe_dup_mmap() we can fold uprobe_reset_state()
    into the new hook and remove it. mmput()->uprobe_clear_state() can't
    be called before dup_mmap().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index cbb5f9fcd3e8..2343c9eaaaf4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -837,8 +837,6 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 	mm->pmd_huge_pte = NULL;
 #endif
-	uprobe_reset_state(mm);
-
 	if (!mm_init(mm, tsk))
 		goto fail_nomem;
 

commit f8ac4ec9c064b330dcc49e03c450fe74298c4622
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Aug 8 17:11:42 2012 +0200

    uprobes: Introduce MMF_HAS_UPROBES
    
    Add the new MMF_HAS_UPROBES flag. It is set by install_breakpoint()
    and it is copied by dup_mmap(), uprobe_pre_sstep_notifier() checks
    it to avoid the slow path if the task was never probed. Perhaps it
    makes sense to check it in valid_vma(is_register => false) as well.
    
    This needs the new dup_mmap()->uprobe_dup_mmap() hook. We can't use
    uprobe_reset_state() or put MMF_HAS_UPROBES into MMF_INIT_MASK, we
    need oldmm->mmap_sem to avoid the race with uprobe_register() or
    mmap() from another thread.
    
    Currently we never clear this bit, it can be false-positive after
    uprobe_unregister() or uprobe_munmap() or if dup_mmap() hits the
    probed VM_DONTCOPY vma. But this is fine correctness-wise and has
    no effect unless the task hits the non-uprobe breakpoint.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 912b6f6fe5b8..cbb5f9fcd3e8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -353,6 +353,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 
 	down_write(&oldmm->mmap_sem);
 	flush_cache_dup_mm(oldmm);
+	uprobe_dup_mmap(oldmm, mm);
 	/*
 	 * Not linked in yet - no deadlock potential:
 	 */

commit f1a45d023193f7d8e55e384090b645d609325393
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Aug 6 14:13:23 2012 +0200

    uprobes: Kill dup_mmap()->uprobe_mmap(), simplify uprobe_mmap/munmap
    
    1. Kill dup_mmap()->uprobe_mmap(), it was only needed to calculate
       new_mm->uprobes_state.count removed by the previous patch.
    
       If the forking process has a pending uprobe (int3) in vma, it will
       be copied by copy_page_range(), note that it checks vma->anon_vma
       so "Don't copy ptes" is not possible after install_breakpoint()
       which does anon_vma_prepare().
    
    2. Remove is_swbp_at_addr() and "int count" in uprobe_mmap(). Again,
       this was needed for uprobes_state.count.
    
       As a side effect this fixes the bug pointed out by Srikar,
       this code lacked the necessary put_uprobe().
    
    3. uprobe_munmap() becomes a nop after the previous patch. Remove the
       meaningless code but do not remove the helper, we will need it.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2c8857e12855..912b6f6fe5b8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -454,9 +454,6 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 
 		if (retval)
 			goto out;
-
-		if (file)
-			uprobe_mmap(tmp);
 	}
 	/* a new mm has just been created */
 	arch_dup_mmap(oldmm, mm);

commit c7a3a88c938fbe3d70c2278e082b80eb830d1c58
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sun Aug 19 19:10:42 2012 +0200

    uprobes: Fix mmap_region()'s mm->mm_rb corruption if uprobe_mmap() fails
    
    This patch fixes:
    
      https://bugzilla.redhat.com/show_bug.cgi?id=843640
    
    If mmap_region()->uprobe_mmap() fails, unmap_and_free_vma path
    does unmap_region() but does not remove the soon-to-be-freed vma
    from rb tree. Actually there are more problems but this is how
    William noticed this bug.
    
    Perhaps we could do do_munmap() + return in this case, but in
    fact it is simply wrong to abort if uprobe_mmap() fails. Until
    at least we move the !UPROBE_COPY_INSN code from
    install_breakpoint() to uprobe_register().
    
    For example, uprobe_mmap()->install_breakpoint() can fail if the
    probed insn is not supported (remember, uprobe_register()
    succeeds if nobody mmaps inode/offset), mmap() should not fail
    in this case.
    
    dup_mmap()->uprobe_mmap() is wrong too by the same reason,
    fork() can race with uprobe_register() and fail for no reason if
    it wins the race and does install_breakpoint() first.
    
    And, if nothing else, both mmap_region() and dup_mmap() return
    success if uprobe_mmap() fails. Change them to ignore the error
    code from uprobe_mmap().
    
    Reported-and-tested-by: William Cohen <wcohen@redhat.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: <stable@vger.kernel.org> # v3.5
    Cc: Anton Arapov <anton@redhat.com>
    Cc: William Cohen <wcohen@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20120819171042.GB26957@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3bd2280d79f6..2c8857e12855 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -455,8 +455,8 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		if (retval)
 			goto out;
 
-		if (file && uprobe_mmap(tmp))
-			goto out;
+		if (file)
+			uprobe_mmap(tmp);
 	}
 	/* a new mm has just been created */
 	arch_dup_mmap(oldmm, mm);

commit c255a458055e459f65eb7b7f51dc5dbdd0caf1d8
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Jul 31 16:43:02 2012 -0700

    memcg: rename config variables
    
    Sanity:
    
    CONFIG_CGROUP_MEM_RES_CTLR -> CONFIG_MEMCG
    CONFIG_CGROUP_MEM_RES_CTLR_SWAP -> CONFIG_MEMCG_SWAP
    CONFIG_CGROUP_MEM_RES_CTLR_SWAP_ENABLED -> CONFIG_MEMCG_SWAP_ENABLED
    CONFIG_CGROUP_MEM_RES_CTLR_KMEM -> CONFIG_MEMCG_KMEM
    
    [mhocko@suse.cz: fix missed bits]
    Cc: Glauber Costa <glommer@parallels.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index aaa8813c45d1..3bd2280d79f6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1306,7 +1306,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_DEBUG_MUTEXES
 	p->blocked_on = NULL; /* not blocked yet */
 #endif
-#ifdef CONFIG_CGROUP_MEM_RES_CTLR
+#ifdef CONFIG_MEMCG
 	p->memcg_batch.do_batch = 0;
 	p->memcg_batch.memcg = NULL;
 #endif

commit 44de9d0cad41f2c51ef26916842be046b582dcc9
Author: Huang Shijie <shijie8@gmail.com>
Date:   Tue Jul 31 16:41:49 2012 -0700

    mm: account the total_vm in the vm_stat_account()
    
    vm_stat_account() accounts the shared_vm, stack_vm and reserved_vm now.
    But we can also account for total_vm in the vm_stat_account() which makes
    the code tidy.
    
    Even for mprotect_fixup(), we can get the right result in the end.
    
    Signed-off-by: Huang Shijie <shijie8@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8efac1fe56bc..aaa8813c45d1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -381,10 +381,8 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		struct file *file;
 
 		if (mpnt->vm_flags & VM_DONTCOPY) {
-			long pages = vma_pages(mpnt);
-			mm->total_vm -= pages;
 			vm_stat_account(mm, mpnt->vm_flags, mpnt->vm_file,
-								-pages);
+							-vma_pages(mpnt));
 			continue;
 		}
 		charge = 0;

commit f19b9f74b7ea3b21ddcee55d852a6488239608a4
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Mon Jul 30 14:42:33 2012 -0700

    fork: fix error handling in dup_task()
    
    The function dup_task() may fail at the following function calls in the
    following order.
    
    0) alloc_task_struct_node()
    1) alloc_thread_info_node()
    2) arch_dup_task_struct()
    
    Error by 0) is not a matter, it can just return.  But error by 1) requires
    releasing task_struct allocated by 0) before it returns.  Likewise, error
    by 2) requires releasing task_struct and thread_info allocated by 0) and
    1).
    
    The existing error handling calls free_task_struct() and
    free_thread_info() which do not only release task_struct and thread_info,
    but also call architecture specific arch_release_task_struct() and
    arch_release_thread_info().
    
    The problem is that task_struct and thread_info are not fully initialized
    yet at this point, but arch_release_task_struct() and
    arch_release_thread_info() are called with them.
    
    For example, x86 defines its own arch_release_task_struct() that releases
    a task_xstate.  If alloc_thread_info_node() fails in dup_task(),
    arch_release_task_struct() is called with task_struct which is just
    allocated and filled with garbage in this error handling.
    
    This actually happened with tools/testing/fault-injection/failcmd.sh
    
            # env FAILCMD_TYPE=fail_page_alloc \
                    ./tools/testing/fault-injection/failcmd.sh --times=100 \
                    --min-order=0 --ignore-gfp-wait=0 \
                    -- make -C tools/testing/selftests/ run_tests
    
    In order to fix this issue, make free_{task_struct,thread_info}() not to
    call arch_release_{task_struct,thread_info}() and call
    arch_release_{task_struct,thread_info}() implicitly where needed.
    
    Default arch_release_task_struct() and arch_release_thread_info() are
    defined as empty by default.  So this change only affects the
    architectures which implement their own arch_release_task_struct() or
    arch_release_thread_info() as listed below.
    
    arch_release_task_struct(): x86, sh
    arch_release_thread_info(): mn10300, tile
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Salman Qazi <sqazi@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 088025bd1925..8efac1fe56bc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -114,6 +114,10 @@ int nr_processes(void)
 	return total;
 }
 
+void __weak arch_release_task_struct(struct task_struct *tsk)
+{
+}
+
 #ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 static struct kmem_cache *task_struct_cachep;
 
@@ -122,17 +126,17 @@ static inline struct task_struct *alloc_task_struct_node(int node)
 	return kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node);
 }
 
-void __weak arch_release_task_struct(struct task_struct *tsk) { }
-
 static inline void free_task_struct(struct task_struct *tsk)
 {
-	arch_release_task_struct(tsk);
 	kmem_cache_free(task_struct_cachep, tsk);
 }
 #endif
 
+void __weak arch_release_thread_info(struct thread_info *ti)
+{
+}
+
 #ifndef CONFIG_ARCH_THREAD_INFO_ALLOCATOR
-void __weak arch_release_thread_info(struct thread_info *ti) { }
 
 /*
  * Allocate pages if THREAD_SIZE is >= PAGE_SIZE, otherwise use a
@@ -150,7 +154,6 @@ static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 
 static inline void free_thread_info(struct thread_info *ti)
 {
-	arch_release_thread_info(ti);
 	free_pages((unsigned long)ti, THREAD_SIZE_ORDER);
 }
 # else
@@ -164,7 +167,6 @@ static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 
 static void free_thread_info(struct thread_info *ti)
 {
-	arch_release_thread_info(ti);
 	kmem_cache_free(thread_info_cache, ti);
 }
 
@@ -205,10 +207,12 @@ static void account_kernel_stack(struct thread_info *ti, int account)
 void free_task(struct task_struct *tsk)
 {
 	account_kernel_stack(tsk->stack, -1);
+	arch_release_thread_info(tsk->stack);
 	free_thread_info(tsk->stack);
 	rt_mutex_debug_task_free(tsk);
 	ftrace_graph_exit_task(tsk);
 	put_seccomp_filter(tsk);
+	arch_release_task_struct(tsk);
 	free_task_struct(tsk);
 }
 EXPORT_SYMBOL(free_task);
@@ -298,14 +302,12 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 		return NULL;
 
 	ti = alloc_thread_info_node(tsk, node);
-	if (!ti) {
-		free_task_struct(tsk);
-		return NULL;
-	}
+	if (!ti)
+		goto free_tsk;
 
 	err = arch_dup_task_struct(tsk, orig);
 	if (err)
-		goto out;
+		goto free_ti;
 
 	tsk->stack = ti;
 
@@ -333,8 +335,9 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 
 	return tsk;
 
-out:
+free_ti:
 	free_thread_info(ti);
+free_tsk:
 	free_task_struct(tsk);
 	return NULL;
 }

commit 87bec58a52652e2eb2a575692a40f9466c7bd31b
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Jul 30 14:42:31 2012 -0700

    revert "sched: Fix fork() error path to not crash"
    
    To make way for "fork: fix error handling in dup_task()", which fixes the
    errors more completely.
    
    Cc: Salman Qazi <sqazi@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2c1802948a38..088025bd1925 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -304,17 +304,12 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	}
 
 	err = arch_dup_task_struct(tsk, orig);
-
-	/*
-	 * We defer looking at err, because we will need this setup
-	 * for the clean up path to work correctly.
-	 */
-	tsk->stack = ti;
-	setup_thread_stack(tsk, orig);
-
 	if (err)
 		goto out;
 
+	tsk->stack = ti;
+
+	setup_thread_stack(tsk, orig);
 	clear_user_return_notifier(tsk);
 	clear_tsk_need_resched(tsk);
 	stackend = end_of_stack(tsk);

commit b2412b7fa7a3816fa8633dc2ff19f1a90aabe423
Author: Huang Shijie <shijie8@gmail.com>
Date:   Mon Jul 30 14:42:30 2012 -0700

    fork: use vma_pages() to simplify the code
    
    The current code can be replaced by vma_pages().  So use it to simplify
    the code.
    
    [akpm@linux-foundation.org: initialise `len' at its definition site]
    Signed-off-by: Huang Shijie <shijie8@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ff1cad3b7bdc..2c1802948a38 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -391,8 +391,8 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		}
 		charge = 0;
 		if (mpnt->vm_flags & VM_ACCOUNT) {
-			unsigned long len;
-			len = (mpnt->vm_end - mpnt->vm_start) >> PAGE_SHIFT;
+			unsigned long len = vma_pages(mpnt);
+
 			if (security_vm_enough_memory_mm(oldmm, len)) /* sic */
 				goto fail_nomem;
 			charge = len;

commit a66d2c8f7ec1284206ca7c14569e2a607583f1e3
Merge: a6be1fcbc57f 8cae6f7158ec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 23 12:27:27 2012 -0700

    Merge branch 'for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull the big VFS changes from Al Viro:
     "This one is *big* and changes quite a few things around VFS.  What's in there:
    
       - the first of two really major architecture changes - death to open
         intents.
    
         The former is finally there; it was very long in making, but with
         Miklos getting through really hard and messy final push in
         fs/namei.c, we finally have it.  Unlike his variant, this one
         doesn't introduce struct opendata; what we have instead is
         ->atomic_open() taking preallocated struct file * and passing
         everything via its fields.
    
         Instead of returning struct file *, it returns -E...  on error, 0
         on success and 1 in "deal with it yourself" case (e.g.  symlink
         found on server, etc.).
    
         See comments before fs/namei.c:atomic_open().  That made a lot of
         goodies finally possible and quite a few are in that pile:
         ->lookup(), ->d_revalidate() and ->create() do not get struct
         nameidata * anymore; ->lookup() and ->d_revalidate() get lookup
         flags instead, ->create() gets "do we want it exclusive" flag.
    
         With the introduction of new helper (kern_path_locked()) we are rid
         of all struct nameidata instances outside of fs/namei.c; it's still
         visible in namei.h, but not for long.  Come the next cycle,
         declaration will move either to fs/internal.h or to fs/namei.c
         itself.  [me, miklos, hch]
    
       - The second major change: behaviour of final fput().  Now we have
         __fput() done without any locks held by caller *and* not from deep
         in call stack.
    
         That obviously lifts a lot of constraints on the locking in there.
         Moreover, it's legal now to call fput() from atomic contexts (which
         has immediately simplified life for aio.c).  We also don't need
         anti-recursion logics in __scm_destroy() anymore.
    
         There is a price, though - the damn thing has become partially
         asynchronous.  For fput() from normal process we are guaranteed
         that pending __fput() will be done before the caller returns to
         userland, exits or gets stopped for ptrace.
    
         For kernel threads and atomic contexts it's done via
         schedule_work(), so theoretically we might need a way to make sure
         it's finished; so far only one such place had been found, but there
         might be more.
    
         There's flush_delayed_fput() (do all pending __fput()) and there's
         __fput_sync() (fput() analog doing __fput() immediately).  I hope
         we won't need them often; see warnings in fs/file_table.c for
         details.  [me, based on task_work series from Oleg merged last
         cycle]
    
       - sync series from Jan
    
       - large part of "death to sync_supers()" work from Artem; the only
         bits missing here are exofs and ext4 ones.  As far as I understand,
         those are going via the exofs and ext4 trees resp.; once they are
         in, we can put ->write_super() to the rest, along with the thread
         calling it.
    
       - preparatory bits from unionmount series (from dhowells).
    
       - assorted cleanups and fixes all over the place, as usual.
    
      This is not the last pile for this cycle; there's at least jlayton's
      ESTALE work and fsfreeze series (the latter - in dire need of fixes,
      so I'm not sure it'll make the cut this cycle).  I'll probably throw
      symlink/hardlink restrictions stuff from Kees into the next pile, too.
      Plus there's a lot of misc patches I hadn't thrown into that one -
      it's large enough as it is..."
    
    * 'for-linus-2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (127 commits)
      ext4: switch EXT4_IOC_RESIZE_FS to mnt_want_write_file()
      btrfs: switch btrfs_ioctl_balance() to mnt_want_write_file()
      switch dentry_open() to struct path, make it grab references itself
      spufs: shift dget/mntget towards dentry_open()
      zoran: don't bother with struct file * in zoran_map
      ecryptfs: don't reinvent the wheels, please - use struct completion
      don't expose I_NEW inodes via dentry->d_inode
      tidy up namei.c a bit
      unobfuscate follow_up() a bit
      ext3: pass custom EOF to generic_file_llseek_size()
      ext4: use core vfs llseek code for dir seeks
      vfs: allow custom EOF in generic_file_llseek code
      vfs: Avoid unnecessary WB_SYNC_NONE writeback during sys_sync and reorder sync passes
      vfs: Remove unnecessary flushing of block devices
      vfs: Make sys_sync writeout also block device inodes
      vfs: Create function for iterating over block devices
      vfs: Reorder operations during sys_sync
      quota: Move quota syncing to ->sync_fs method
      quota: Split dquot_quota_sync() to writeback and cache flushing part
      vfs: Move noop_backing_dev_info check from sync into writeback
      ...

commit 158e1645e07f3e9f7e4962d7a0997f5c3b98311b
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jun 27 09:24:13 2012 +0400

    trim task_work: get rid of hlist
    
    layout based on Oleg's suggestion; single-linked list,
    task->task_works points to the last element, forward pointer
    from said last element points to head.  I'd still prefer
    much more regular scheme with two pointers in task_work,
    but...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index ab5211b9e622..bebabad59202 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1415,7 +1415,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 */
 	p->group_leader = p;
 	INIT_LIST_HEAD(&p->thread_group);
-	INIT_HLIST_HEAD(&p->task_works);
+	p->task_works = NULL;
 
 	/* Now that the task is set up, run cgroup callbacks if
 	 * necessary. We need to run them before the task is visible

commit 164c33c6adee609b8b9062cce4c10f764d0dce13
Author: Salman Qazi <sqazi@google.com>
Date:   Mon Jun 25 18:18:15 2012 -0700

    sched: Fix fork() error path to not crash
    
    In dup_task_struct(), if arch_dup_task_struct() fails, the clean up
    code fails to clean up correctly.  That's because the clean up
    code depends on unininitalized ti->task pointer.  We fix this
    by making sure that the task and thread_info know about each other
    before we attempt to take the error path.
    
    Signed-off-by: Salman Qazi <sqazi@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120626011815.11323.5533.stgit@dungbeetle.mtv.corp.google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ab5211b9e622..f00e319d8376 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -304,12 +304,17 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	}
 
 	err = arch_dup_task_struct(tsk, orig);
-	if (err)
-		goto out;
 
+	/*
+	 * We defer looking at err, because we will need this setup
+	 * for the clean up path to work correctly.
+	 */
 	tsk->stack = ti;
-
 	setup_thread_stack(tsk, orig);
+
+	if (err)
+		goto out;
+
 	clear_user_return_notifier(tsk);
 	clear_tsk_need_resched(tsk);
 	stackend = end_of_stack(tsk);

commit 48d212a2eecaca2e1875925837ad27b2f43f48a3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 7 17:54:07 2012 -0700

    Revert "mm: correctly synchronize rss-counters at exit/exec"
    
    This reverts commit 40af1bbdca47e5c8a2044039bb78ca8fd8b20f94.
    
    It's horribly and utterly broken for at least the following reasons:
    
     - calling sync_mm_rss() from mmput() is fundamentally wrong, because
       there's absolutely no reason to believe that the task that does the
       mmput() always does it on its own VM.  Example: fork, ptrace, /proc -
       you name it.
    
     - calling it *after* having done mmdrop() on it is doubly insane, since
       the mm struct may well be gone now.
    
     - testing mm against NULL before you call it is insane too, since a
    NULL mm there would have caused oopses long before.
    
    .. and those are just the three bugs I found before I decided to give up
    looking for me and revert it asap.  I should have caught it before I
    even took it, but I trusted Andrew too much.
    
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Markus Trippelsdorf <markus@trippelsdorf.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0560781c6904..ab5211b9e622 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -619,14 +619,6 @@ void mmput(struct mm_struct *mm)
 			module_put(mm->binfmt->module);
 		mmdrop(mm);
 	}
-
-	/*
-	 * Final rss-counter synchronization. After this point there must be
-	 * no pagefaults into this mm from the current context.  Otherwise
-	 * mm->rss_stat will be inconsistent.
-	 */
-	if (mm)
-		sync_mm_rss(mm);
 }
 EXPORT_SYMBOL_GPL(mmput);
 

commit 40af1bbdca47e5c8a2044039bb78ca8fd8b20f94
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Thu Jun 7 14:21:14 2012 -0700

    mm: correctly synchronize rss-counters at exit/exec
    
    mm->rss_stat counters have per-task delta: task->rss_stat.  Before
    changing task->mm pointer the kernel must flush this delta with
    sync_mm_rss().
    
    do_exit() already calls sync_mm_rss() to flush the rss-counters before
    committing the rss statistics into task->signal->maxrss, taskstats,
    audit and other stuff.  Unfortunately the kernel does this before
    calling mm_release(), which can call put_user() for processing
    task->clear_child_tid.  So at this point we can trigger page-faults and
    task->rss_stat becomes non-zero again.  As a result mm->rss_stat becomes
    inconsistent and check_mm() will print something like this:
    
    | BUG: Bad rss-counter state mm:ffff88020813c380 idx:1 val:-1
    | BUG: Bad rss-counter state mm:ffff88020813c380 idx:2 val:1
    
    This patch moves sync_mm_rss() into mm_release(), and moves mm_release()
    out of do_exit() and calls it earlier.  After mm_release() there should
    be no pagefaults.
    
    [akpm@linux-foundation.org: tweak comment]
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Reported-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: <stable@vger.kernel.org>            [3.4.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ab5211b9e622..0560781c6904 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -619,6 +619,14 @@ void mmput(struct mm_struct *mm)
 			module_put(mm->binfmt->module);
 		mmdrop(mm);
 	}
+
+	/*
+	 * Final rss-counter synchronization. After this point there must be
+	 * no pagefaults into this mm from the current context.  Otherwise
+	 * mm->rss_stat will be inconsistent.
+	 */
+	if (mm)
+		sync_mm_rss(mm);
 }
 EXPORT_SYMBOL_GPL(mmput);
 

commit fb21affa49204acd409328415b49bfe90136653c
Merge: a00b6151a2ae f23ca335462e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 31 18:47:30 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull second pile of signal handling patches from Al Viro:
     "This one is just task_work_add() series + remaining prereqs for it.
    
      There probably will be another pull request from that tree this
      cycle - at least for helpers, to get them out of the way for per-arch
      fixes remaining in the tree."
    
    Fix trivial conflict in kernel/irq/manage.c: the merge of Andrew's pile
    had brought in commit 97fd75b7b8e0 ("kernel/irq/manage.c: use the
    pr_foo() infrastructure to prefix printks") which changed one of the
    pr_err() calls that this merge moves around.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal:
      keys: kill task_struct->replacement_session_keyring
      keys: kill the dummy key_replace_session_keyring()
      keys: change keyctl_session_to_parent() to use task_work_add()
      genirq: reimplement exit_irq_thread() hook via task_work_add()
      task_work_add: generic process-context callbacks
      avr32: missed _TIF_NOTIFY_RESUME on one of do_notify_resume callers
      parisc: need to check NOTIFY_RESUME when exiting from syscall
      move key_repace_session_keyring() into tracehook_notify_resume()
      TIF_NOTIFY_RESUME is defined on all targets now

commit 08615d7d85e5aa02c05bf6c4dde87d940e7f85f6
Merge: 9fdadb2cbaf4 0a4dd35c67b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 31 18:10:18 2012 -0700

    Merge branch 'akpm' (Andrew's patch-bomb)
    
    Merge misc patches from Andrew Morton:
    
     - the "misc" tree - stuff from all over the map
    
     - checkpatch updates
    
     - fatfs
    
     - kmod changes
    
     - procfs
    
     - cpumask
    
     - UML
    
     - kexec
    
     - mqueue
    
     - rapidio
    
     - pidns
    
     - some checkpoint-restore feature work.  Reluctantly.  Most of it
       delayed a release.  I'm still rather worried that we don't have a
       clear roadmap to completion for this work.
    
    * emailed from Andrew Morton <akpm@linux-foundation.org>: (78 patches)
      kconfig: update compression algorithm info
      c/r: prctl: add ability to set new mm_struct::exe_file
      c/r: prctl: extend PR_SET_MM to set up more mm_struct entries
      c/r: procfs: add arg_start/end, env_start/end and exit_code members to /proc/$pid/stat
      syscalls, x86: add __NR_kcmp syscall
      fs, proc: introduce /proc/<pid>/task/<tid>/children entry
      sysctl: make kernel.ns_last_pid control dependent on CHECKPOINT_RESTORE
      aio/vfs: cleanup of rw_copy_check_uvector() and compat_rw_copy_check_uvector()
      eventfd: change int to __u64 in eventfd_signal()
      fs/nls: add Apple NLS
      pidns: make killed children autoreap
      pidns: use task_active_pid_ns in do_notify_parent
      rapidio/tsi721: add DMA engine support
      rapidio: add DMA engine support for RIO data transfers
      ipc/mqueue: add rbtree node caching support
      tools/selftests: add mq_perf_tests
      ipc/mqueue: strengthen checks on mqueue creation
      ipc/mqueue: correct mq_attr_ok test
      ipc/mqueue: improve performance of send/recv
      selftests: add mq_open_tests
      ...

commit f7505d64f2db5da2d7d94873ddf2cd2524847061
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Thu May 31 16:26:21 2012 -0700

    fork: call complete_vfork_done() after clearing child_tid and flushing rss-counters
    
    Child should wake up the parent from vfork() only after finishing all
    operations with shared mm.  There is no sense in using
    CLONE_CHILD_CLEARTID together with CLONE_VFORK, but it looks more accurate
    now.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Markus Trippelsdorf <markus@trippelsdorf.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 017fb23d5983..2254fbf23567 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -787,9 +787,6 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 	/* Get rid of any cached register state */
 	deactivate_mm(tsk, mm);
 
-	if (tsk->vfork_done)
-		complete_vfork_done(tsk);
-
 	/*
 	 * If we're exiting normally, clear a user-space tid field if
 	 * requested.  We leave this alone when dying by signal, to leave
@@ -810,6 +807,13 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 		}
 		tsk->clear_child_tid = NULL;
 	}
+
+	/*
+	 * All done, finally we can wake up parent and return this mm to him.
+	 * Also kthread_stop() uses this completion for synchronization.
+	 */
+	if (tsk->vfork_done)
+		complete_vfork_done(tsk);
 }
 
 /*

commit 0d167518e045cc8bb63f0a8a0a85ad4fa4e0044f
Merge: 2f83766d4b18 ff26eaadf4d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 30 08:52:42 2012 -0700

    Merge branch 'for-3.5/core' of git://git.kernel.dk/linux-block
    
    Merge block/IO core bits from Jens Axboe:
     "This is a bit bigger on the core side than usual, but that is purely
      because we decided to hold off on parts of Tejun's submission on 3.4
      to give it a bit more time to simmer.  As a consequence, it's seen a
      long cycle in for-next.
    
      It contains:
    
       - Bug fix from Dan, wrong locking type.
       - Relax splice gifting restriction from Eric.
       - A ton of updates from Tejun, primarily for blkcg.  This improves
         the code a lot, making the API nicer and cleaner, and also includes
         fixes for how we handle and tie policies and re-activate on
         switches.  The changes also include generic bug fixes.
       - A simple fix from Vivek, along with a fix for doing proper delayed
         allocation of the blkcg stats."
    
    Fix up annoying conflict just due to different merge resolution in
    Documentation/feature-removal-schedule.txt
    
    * 'for-3.5/core' of git://git.kernel.dk/linux-block: (92 commits)
      blkcg: tg_stats_alloc_lock is an irq lock
      vmsplice: relax alignement requirements for SPLICE_F_GIFT
      blkcg: use radix tree to index blkgs from blkcg
      blkcg: fix blkcg->css ref leak in __blkg_lookup_create()
      block: fix elvpriv allocation failure handling
      block: collapse blk_alloc_request() into get_request()
      blkcg: collapse blkcg_policy_ops into blkcg_policy
      blkcg: embed struct blkg_policy_data in policy specific data
      blkcg: mass rename of blkcg API
      blkcg: style cleanups for blk-cgroup.h
      blkcg: remove blkio_group->path[]
      blkcg: blkg_rwstat_read() was missing inline
      blkcg: shoot down blkgs if all policies are deactivated
      blkcg: drop stuff unused after per-queue policy activation update
      blkcg: implement per-queue policy activation
      blkcg: add request_queue->root_blkg
      blkcg: make request_queue bypassing on allocation
      blkcg: make sure blkg_lookup() returns %NULL if @q is bypassing
      blkcg: make blkg_conf_prep() take @pol and return with queue lock held
      blkcg: remove static policy ID enums
      ...

commit 7edc8b0ac16cbaed7cb4ea4c6b95ce98d2997e84
Author: Siddhesh Poyarekar <siddhesh.poyarekar@gmail.com>
Date:   Tue May 29 15:06:22 2012 -0700

    mm/fork: fix overflow in vma length when copying mmap on clone
    
    The vma length in dup_mmap is calculated and stored in a unsigned int,
    which is insufficient and hence overflows for very large maps (beyond
    16TB). The following program demonstrates this:
    
    #include <stdio.h>
    #include <unistd.h>
    #include <sys/mman.h>
    
    #define GIG 1024 * 1024 * 1024L
    #define EXTENT 16393
    
    int main(void)
    {
            int i, r;
            void *m;
            char buf[1024];
    
            for (i = 0; i < EXTENT; i++) {
                    m = mmap(NULL, (size_t) 1 * 1024 * 1024 * 1024L,
                             PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, 0, 0);
    
                    if (m == (void *)-1)
                            printf("MMAP Failed: %d\n", m);
                    else
                            printf("%d : MMAP returned %p\n", i, m);
    
                    r = fork();
    
                    if (r == 0) {
                            printf("%d: successed\n", i);
                            return 0;
                    } else if (r < 0)
                            printf("FORK Failed: %d\n", r);
                    else if (r > 0)
                            wait(NULL);
            }
            return 0;
    }
    
    Increase the storage size of the result to unsigned long, which is
    sufficient for storing the difference between addresses.
    
    Signed-off-by: Siddhesh Poyarekar <siddhesh.poyarekar@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5b13eea2e757..017fb23d5983 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -386,7 +386,8 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		}
 		charge = 0;
 		if (mpnt->vm_flags & VM_ACCOUNT) {
-			unsigned int len = (mpnt->vm_end - mpnt->vm_start) >> PAGE_SHIFT;
+			unsigned long len;
+			len = (mpnt->vm_end - mpnt->vm_start) >> PAGE_SHIFT;
 			if (security_vm_enough_memory_mm(oldmm, len)) /* sic */
 				goto fail_nomem;
 			charge = len;

commit e709ffd6169ccd259eb5874e853303e91e94e829
Author: Rik van Riel <riel@redhat.com>
Date:   Tue May 29 15:06:18 2012 -0700

    mm: remove swap token code
    
    The swap token code no longer fits in with the current VM model.  It
    does not play well with cgroups or the better NUMA placement code in
    development, since we have only one swap token globally.
    
    It also has the potential to mess with scalability of the system, by
    increasing the number of non-reclaimable pages on the active and
    inactive anon LRU lists.
    
    Last but not least, the swap token code has been broken for a year
    without complaints, as reported by Konstantin Khlebnikov.  This suggests
    we no longer have much use for it.
    
    The days of sub-1G memory systems with heavy use of swap are over.  If
    we ever need thrashing reducing code in the future, we will have to
    implement something that does scale.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Hugh Dickins <hughd@google.com>
    Acked-by: Bob Picco <bpicco@meloft.net>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 47b4e4f379f9..5b13eea2e757 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -614,7 +614,6 @@ void mmput(struct mm_struct *mm)
 			list_del(&mm->mmlist);
 			spin_unlock(&mmlist_lock);
 		}
-		put_swap_token(mm);
 		if (mm->binfmt)
 			module_put(mm->binfmt->module);
 		mmdrop(mm);
@@ -831,10 +830,6 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	memcpy(mm, oldmm, sizeof(*mm));
 	mm_init_cpumask(mm);
 
-	/* Initializing for Swap token stuff */
-	mm->token_priority = 0;
-	mm->last_interval = 0;
-
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 	mm->pmd_huge_pte = NULL;
 #endif
@@ -913,10 +908,6 @@ static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 		goto fail_nomem;
 
 good_mm:
-	/* Initializing for Swap token stuff */
-	mm->token_priority = 0;
-	mm->last_interval = 0;
-
 	tsk->mm = mm;
 	tsk->active_mm = mm;
 	return 0;

commit 654443e20dfc0617231f28a07c96a979ee1a0239
Merge: 2c01e7bc46f1 9cba26e66d09
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 24 11:39:34 2012 -0700

    Merge branch 'perf-uprobes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull user-space probe instrumentation from Ingo Molnar:
     "The uprobes code originates from SystemTap and has been used for years
      in Fedora and RHEL kernels.  This version is much rewritten, reviews
      from PeterZ, Oleg and myself shaped the end result.
    
      This tree includes uprobes support in 'perf probe' - but SystemTap
      (and other tools) can take advantage of user probe points as well.
    
      Sample usage of uprobes via perf, for example to profile malloc()
      calls without modifying user-space binaries.
    
      First boot a new kernel with CONFIG_UPROBE_EVENT=y enabled.
    
      If you don't know which function you want to probe you can pick one
      from 'perf top' or can get a list all functions that can be probed
      within libc (binaries can be specified as well):
    
            $ perf probe -F -x /lib/libc.so.6
    
      To probe libc's malloc():
    
            $ perf probe -x /lib64/libc.so.6 malloc
            Added new event:
            probe_libc:malloc    (on 0x7eac0)
    
      You can now use it in all perf tools, such as:
    
            perf record -e probe_libc:malloc -aR sleep 1
    
      Make use of it to create a call graph (as the flat profile is going to
      look very boring):
    
            $ perf record -e probe_libc:malloc -gR make
            [ perf record: Woken up 173 times to write data ]
            [ perf record: Captured and wrote 44.190 MB perf.data (~1930712
    
            $ perf report | less
    
              32.03%            git  libc-2.15.so   [.] malloc
                                |
                                --- malloc
    
              29.49%            cc1  libc-2.15.so   [.] malloc
                                |
                                --- malloc
                                   |
                                   |--0.95%-- 0x208eb1000000000
                                   |
                                   |--0.63%-- htab_traverse_noresize
    
              11.04%             as  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
    
               7.15%             ld  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
    
               5.07%             sh  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
               4.99%  python-config  libc-2.15.so   [.] malloc
                      |
                      --- malloc
                         |
               4.54%           make  libc-2.15.so   [.] malloc
                               |
                               --- malloc
                                  |
                                  |--7.34%-- glob
                                  |          |
                                  |          |--93.18%-- 0x41588f
                                  |          |
                                  |           --6.82%-- glob
                                  |                     0x41588f
    
               ...
    
      Or:
    
            $ perf report -g flat | less
    
            # Overhead        Command  Shared Object      Symbol
            # ........  .............  .............  ..........
            #
              32.03%            git  libc-2.15.so   [.] malloc
                      27.19%
                          malloc
    
              29.49%            cc1  libc-2.15.so   [.] malloc
                      24.77%
                          malloc
    
              11.04%             as  libc-2.15.so   [.] malloc
                      11.02%
                          malloc
    
               7.15%             ld  libc-2.15.so   [.] malloc
                       6.57%
                          malloc
    
             ...
    
      The core uprobes design is fairly straightforward: uprobes probe
      points register themselves at (inode:offset) addresses of
      libraries/binaries, after which all existing (or new) vmas that map
      that address will have a software breakpoint injected at that address.
      vmas are COW-ed to preserve original content.  The probe points are
      kept in an rbtree.
    
      If user-space executes the probed inode:offset instruction address
      then an event is generated which can be recovered from the regular
      perf event channels and mmap-ed ring-buffer.
    
      Multiple probes at the same address are supported, they create a
      dynamic callback list of event consumers.
    
      The basic model is further complicated by the XOL speedup: the
      original instruction that is probed is copied (in an architecture
      specific fashion) and executed out of line when the probe triggers.
      The XOL area is a single vma per process, with a fixed number of
      entries (which limits probe execution parallelism).
    
      The API: uprobes are installed/removed via
      /sys/kernel/debug/tracing/uprobe_events, the API is integrated to
      align with the kprobes interface as much as possible, but is separate
      to it.
    
      Injecting a probe point is privileged operation, which can be relaxed
      by setting perf_paranoid to -1.
    
      You can use multiple probes as well and mix them with kprobes and
      regular PMU events or tracepoints, when instrumenting a task."
    
    Fix up trivial conflicts in mm/memory.c due to previous cleanup of
    unmap_single_vma().
    
    * 'perf-uprobes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (21 commits)
      perf probe: Detect probe target when m/x options are absent
      perf probe: Provide perf interface for uprobes
      tracing: Fix kconfig warning due to a typo
      tracing: Provide trace events interface for uprobes
      tracing: Extract out common code for kprobes/uprobes trace events
      tracing: Modify is_delete, is_return from int to bool
      uprobes/core: Decrement uprobe count before the pages are unmapped
      uprobes/core: Make background page replacement logic account for rss_stat counters
      uprobes/core: Optimize probe hits with the help of a counter
      uprobes/core: Allocate XOL slots for uprobes use
      uprobes/core: Handle breakpoint and singlestep exceptions
      uprobes/core: Rename bkpt to swbp
      uprobes/core: Make order of function parameters consistent across functions
      uprobes/core: Make macro names consistent
      uprobes: Update copyright notices
      uprobes/core: Move insn to arch specific structure
      uprobes/core: Remove uprobe_opcode_sz
      uprobes/core: Make instruction tables volatile
      uprobes: Move to kernel/events/
      uprobes/core: Clean up, refactor and improve the code
      ...

commit e73f8959af0439d114847eab5a8a5ce48f1217c4
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri May 11 10:59:07 2012 +1000

    task_work_add: generic process-context callbacks
    
    Provide a simple mechanism that allows running code in the (nonatomic)
    context of the arbitrary task.
    
    The caller does task_work_add(task, task_work) and this task executes
    task_work->func() either from do_notify_resume() or from do_exit().  The
    callback can rely on PF_EXITING to detect the latter case.
    
    "struct task_work" can be embedded in another struct, still it has "void
    *data" to handle the most common/simple case.
    
    This allows us to kill the ->replacement_session_keyring hack, and
    potentially this can have more users.
    
    Performance-wise, this adds 2 "unlikely(!hlist_empty())" checks into
    tracehook_notify_resume() and do_exit().  But at the same time we can
    remove the "replacement_session_keyring != NULL" checks from
    arch/*/signal.c and exit_creds().
    
    Note: task_work_add/task_work_run abuses ->pi_lock.  This is only because
    this lock is already used by lookup_pi_state() to synchronize with
    do_exit() setting PF_EXITING.  Fortunately the scope of this lock in
    task_work.c is really tiny, and the code is unlikely anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Smith <dsmith@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 05c813dc9ecc..a46db217a589 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1411,6 +1411,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 */
 	p->group_leader = p;
 	INIT_LIST_HEAD(&p->thread_group);
+	INIT_HLIST_HEAD(&p->task_works);
 
 	/* Now that the task is set up, run cgroup callbacks if
 	 * necessary. We need to run them before the task is visible

commit ec0d7f18ab7b5097d7c0c8f3d909ca1031b9d5cd
Merge: 269af9a1a08d 1dcc8d7ba235
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 10:59:07 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull fpu state cleanups from Ingo Molnar:
     "This tree streamlines further aspects of FPU handling by eliminating
      the prepare_to_copy() complication and moving that logic to
      arch_dup_task_struct().
    
      It also fixes the FPU dumps in threaded core dumps, removes and old
      (and now invalid) assumption plus micro-optimizes the exit path by
      avoiding an FPU save for dead tasks."
    
    Fixed up trivial add-add conflict in arch/sh/kernel/process.c that came
    in because we now do the FPU handling in arch_dup_task_struct() rather
    than the legacy (and now gone) prepare_to_copy().
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, fpu: drop the fpu state during thread exit
      x86, xsave: remove thread_has_fpu() bug check in __sanitize_i387_state()
      coredump: ensure the fpu state is flushed for proper multi-threaded core dump
      fork: move the real prepare_to_copy() users to arch_dup_task_struct()

commit cb60e3e65c1b96a4d6444a7a13dc7dd48bc15a2b
Merge: 99262a3dafa3 ff2bb047c4bc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 21 20:27:36 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "New notable features:
       - The seccomp work from Will Drewry
       - PR_{GET,SET}_NO_NEW_PRIVS from Andy Lutomirski
       - Longer security labels for Smack from Casey Schaufler
       - Additional ptrace restriction modes for Yama by Kees Cook"
    
    Fix up trivial context conflicts in arch/x86/Kconfig and include/linux/filter.h
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (65 commits)
      apparmor: fix long path failure due to disconnected path
      apparmor: fix profile lookup for unconfined
      ima: fix filename hint to reflect script interpreter name
      KEYS: Don't check for NULL key pointer in key_validate()
      Smack: allow for significantly longer Smack labels v4
      gfp flags for security_inode_alloc()?
      Smack: recursive tramsmute
      Yama: replace capable() with ns_capable()
      TOMOYO: Accept manager programs which do not start with / .
      KEYS: Add invalidation support
      KEYS: Do LRU discard in full keyrings
      KEYS: Permit in-place link replacement in keyring list
      KEYS: Perform RCU synchronisation on keys prior to key destruction
      KEYS: Announce key type (un)registration
      KEYS: Reorganise keys Makefile
      KEYS: Move the key config into security/keys/Kconfig
      KEYS: Use the compat keyctl() syscall wrapper on Sparc64 for Sparc32 compat
      Yama: remove an unused variable
      samples/seccomp: fix dependencies on arch macros
      Yama: add additional ptrace scopes
      ...

commit bf67f3a5c456a18f2e8d062f7e88506ef2cd9837
Merge: 226da0dbc84e 203dacbdca97
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 21 19:43:57 2012 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull smp hotplug cleanups from Thomas Gleixner:
     "This series is merily a cleanup of code copied around in arch/* and
      not changing any of the real cpu hotplug horrors yet.  I wish I'd had
      something more substantial for 3.5, but I underestimated the lurking
      horror..."
    
    Fix up trivial conflicts in arch/{arm,sparc,x86}/Kconfig and
    arch/sparc/include/asm/thread_info_32.h
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (79 commits)
      um: Remove leftover declaration of alloc_task_struct_node()
      task_allocator: Use config switches instead of magic defines
      sparc: Use common threadinfo allocator
      score: Use common threadinfo allocator
      sh-use-common-threadinfo-allocator
      mn10300: Use common threadinfo allocator
      powerpc: Use common threadinfo allocator
      mips: Use common threadinfo allocator
      hexagon: Use common threadinfo allocator
      m32r: Use common threadinfo allocator
      frv: Use common threadinfo allocator
      cris: Use common threadinfo allocator
      x86: Use common threadinfo allocator
      c6x: Use common threadinfo allocator
      fork: Provide kmemcache based thread_info allocator
      tile: Use common threadinfo allocator
      fork: Provide weak arch_release_[task_struct|thread_info] functions
      fork: Move thread info gfp flags to header
      fork: Remove the weak insanity
      sh: Remove cpu_idle_wait()
      ...

commit 55ccf3fe3f9a3441731aa79cf42a628fc4ecace9
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed May 16 15:03:51 2012 -0700

    fork: move the real prepare_to_copy() users to arch_dup_task_struct()
    
    Historical prepare_to_copy() is mostly a no-op, duplicated for majority of
    the architectures and the rest following the x86 model of flushing the extended
    register state like fpu there.
    
    Remove it and use the arch_dup_task_struct() instead.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1336692811-30576-1-git-send-email-suresh.b.siddha@intel.com
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 687a15d56243..7aed746ff47c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -261,8 +261,6 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	int node = tsk_fork_get_node(orig);
 	int err;
 
-	prepare_to_copy(orig);
-
 	tsk = alloc_task_struct_node(node);
 	if (!tsk)
 		return NULL;

commit 5e2bf0142231194d36fdc9596b36a261ed2b9fe7
Author: Mike Galbraith <efault@gmx.de>
Date:   Thu May 10 13:01:45 2012 -0700

    namespaces, pid_ns: fix leakage on fork() failure
    
    Fork() failure post namespace creation for a child cloned with
    CLONE_NEWPID leaks pid_namespace/mnt_cache due to proc being mounted
    during creation, but not unmounted during cleanup.  Call
    pid_ns_release_proc() during cleanup.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Cyrill Gorcunov <gorcunov@openvz.org>
    Cc: Louis Rilling <louis.rilling@kerlabs.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b9372a0bff18..687a15d56243 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -47,6 +47,7 @@
 #include <linux/audit.h>
 #include <linux/memcontrol.h>
 #include <linux/ftrace.h>
+#include <linux/proc_fs.h>
 #include <linux/profile.h>
 #include <linux/rmap.h>
 #include <linux/ksm.h>
@@ -1464,6 +1465,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (p->io_context)
 		exit_io_context(p);
 bad_fork_cleanup_namespaces:
+	if (unlikely(clone_flags & CLONE_NEWPID))
+		pid_ns_release_proc(p->nsproxy->pid_ns);
 	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
 	if (p->mm)

commit f5e10287367dcffb5504d19c83e85ca041ca2596
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 5 15:05:48 2012 +0000

    task_allocator: Use config switches instead of magic defines
    
    Replace __HAVE_ARCH_TASK_ALLOCATOR and __HAVE_ARCH_THREAD_ALLOCATOR
    with proper config switches.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Link: http://lkml.kernel.org/r/20120505150142.371309416@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index 7590bd6e8dff..a1793e442b20 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -111,7 +111,7 @@ int nr_processes(void)
 	return total;
 }
 
-#ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
+#ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 static struct kmem_cache *task_struct_cachep;
 
 static inline struct task_struct *alloc_task_struct_node(int node)
@@ -128,8 +128,7 @@ static inline void free_task_struct(struct task_struct *tsk)
 }
 #endif
 
-#ifndef __HAVE_ARCH_THREAD_INFO_ALLOCATOR
-
+#ifndef CONFIG_ARCH_THREAD_INFO_ALLOCATOR
 void __weak arch_release_thread_info(struct thread_info *ti) { }
 
 /*
@@ -243,7 +242,7 @@ void __init __weak arch_task_cache_init(void) { }
 
 void __init fork_init(unsigned long mempages)
 {
-#ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
+#ifndef CONFIG_ARCH_TASK_STRUCT_ALLOCATOR
 #ifndef ARCH_MIN_TASKALIGN
 #define ARCH_MIN_TASKALIGN	L1_CACHE_BYTES
 #endif

commit 0d15d74a1ead10673b5b1db66d4c90552769096c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 5 15:05:41 2012 +0000

    fork: Provide kmemcache based thread_info allocator
    
    Several architectures have their own kmemcache based thread allocator
    because THREAD_SIZE is smaller than PAGE_SIZE. Add it to the core code
    conditionally on THREAD_SIZE < PAGE_SIZE so the private copies can go.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120505150141.491002124@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index 2dfad0269674..7590bd6e8dff 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -132,6 +132,11 @@ static inline void free_task_struct(struct task_struct *tsk)
 
 void __weak arch_release_thread_info(struct thread_info *ti) { }
 
+/*
+ * Allocate pages if THREAD_SIZE is >= PAGE_SIZE, otherwise use a
+ * kmemcache based allocator.
+ */
+# if THREAD_SIZE >= PAGE_SIZE
 static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 						  int node)
 {
@@ -146,6 +151,28 @@ static inline void free_thread_info(struct thread_info *ti)
 	arch_release_thread_info(ti);
 	free_pages((unsigned long)ti, THREAD_SIZE_ORDER);
 }
+# else
+static struct kmem_cache *thread_info_cache;
+
+static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
+						  int node)
+{
+	return kmem_cache_alloc_node(thread_info_cache, THREADINFO_GFP, node);
+}
+
+static void free_thread_info(struct thread_info *ti)
+{
+	arch_release_thread_info(ti);
+	kmem_cache_free(thread_info_cache, ti);
+}
+
+void thread_info_cache_init(void)
+{
+	thread_info_cache = kmem_cache_create("thread_info", THREAD_SIZE,
+					      THREAD_SIZE, 0, NULL);
+	BUG_ON(thread_info_cache == NULL);
+}
+# endif
 #endif
 
 /* SLAB cache for signal_struct structures (tsk->signal) */

commit 41101809a865dd0be1b56eff46c83fad321870b2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 5 15:05:41 2012 +0000

    fork: Provide weak arch_release_[task_struct|thread_info] functions
    
    These functions allow us to move most of the duplicated thread_info
    allocators to the core code.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120505150141.366461660@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index 5d22b9b8cf7b..2dfad0269674 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -112,14 +112,26 @@ int nr_processes(void)
 }
 
 #ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
-# define alloc_task_struct_node(node)		\
-		kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node)
-# define free_task_struct(tsk)			\
-		kmem_cache_free(task_struct_cachep, (tsk))
 static struct kmem_cache *task_struct_cachep;
+
+static inline struct task_struct *alloc_task_struct_node(int node)
+{
+	return kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node);
+}
+
+void __weak arch_release_task_struct(struct task_struct *tsk) { }
+
+static inline void free_task_struct(struct task_struct *tsk)
+{
+	arch_release_task_struct(tsk);
+	kmem_cache_free(task_struct_cachep, tsk);
+}
 #endif
 
 #ifndef __HAVE_ARCH_THREAD_INFO_ALLOCATOR
+
+void __weak arch_release_thread_info(struct thread_info *ti) { }
+
 static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 						  int node)
 {
@@ -131,6 +143,7 @@ static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 
 static inline void free_thread_info(struct thread_info *ti)
 {
+	arch_release_thread_info(ti);
 	free_pages((unsigned long)ti, THREAD_SIZE_ORDER);
 }
 #endif

commit 2889f60814e15dea644782597d897cdba943564f
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 5 15:05:41 2012 +0000

    fork: Move thread info gfp flags to header
    
    These flags can be useful for extra allocations outside of the core
    code.
    
    Add __GFP_NOTRACK to them, so the archs which have kmemcheck do
    not have to provide extra allocators just for that reason.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120505150141.428211694@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index a79b36e2e912..5d22b9b8cf7b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -123,12 +123,8 @@ static struct kmem_cache *task_struct_cachep;
 static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
 						  int node)
 {
-#ifdef CONFIG_DEBUG_STACK_USAGE
-	gfp_t mask = GFP_KERNEL | __GFP_ZERO;
-#else
-	gfp_t mask = GFP_KERNEL;
-#endif
-	struct page *page = alloc_pages_node(node, mask, THREAD_SIZE_ORDER);
+	struct page *page = alloc_pages_node(node, THREADINFO_GFP,
+					     THREAD_SIZE_ORDER);
 
 	return page ? page_address(page) : NULL;
 }

commit 6c0a9fa62feb7e9fdefa9720bcc03040c9b0b311
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 5 15:05:40 2012 +0000

    fork: Remove the weak insanity
    
    We error out when compiling with gcc4.1.[01] as it miscompiles
    __weak. The workaround with magic defines is not longer
    necessary. Make it __weak again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20120505150141.306358267@linutronix.de

diff --git a/kernel/fork.c b/kernel/fork.c
index b9372a0bff18..a79b36e2e912 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -203,13 +203,7 @@ void __put_task_struct(struct task_struct *tsk)
 }
 EXPORT_SYMBOL_GPL(__put_task_struct);
 
-/*
- * macro override instead of weak attribute alias, to workaround
- * gcc 4.1.0 and 4.1.1 bugs with weak attribute and empty functions.
- */
-#ifndef arch_task_cache_init
-#define arch_task_cache_init()
-#endif
+void __init __weak arch_task_cache_init(void) { }
 
 void __init fork_init(unsigned long mempages)
 {

commit 6ac1ef482d7ae0c690f1640bf6eb818ff9a2d91e
Merge: 682968e0c425 a385ec4f11bd
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Apr 14 13:18:27 2012 +0200

    Merge branch 'perf/core' into perf/uprobes
    
    Merge in latest upstream (and the latest perf development tree),
    to prepare for tooling changes, and also to pick up v3.4 MM
    changes that the uprobes code needs to take care of.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e2cfabdfd075648216f99c2c03821cf3f47c1727
Author: Will Drewry <wad@chromium.org>
Date:   Thu Apr 12 16:47:57 2012 -0500

    seccomp: add system call filtering using BPF
    
    [This patch depends on luto@mit.edu's no_new_privs patch:
       https://lkml.org/lkml/2012/1/30/264
     The whole series including Andrew's patches can be found here:
       https://github.com/redpig/linux/tree/seccomp
     Complete diff here:
       https://github.com/redpig/linux/compare/1dc65fed...seccomp
    ]
    
    This patch adds support for seccomp mode 2.  Mode 2 introduces the
    ability for unprivileged processes to install system call filtering
    policy expressed in terms of a Berkeley Packet Filter (BPF) program.
    This program will be evaluated in the kernel for each system call
    the task makes and computes a result based on data in the format
    of struct seccomp_data.
    
    A filter program may be installed by calling:
      struct sock_fprog fprog = { ... };
      ...
      prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, &fprog);
    
    The return value of the filter program determines if the system call is
    allowed to proceed or denied.  If the first filter program installed
    allows prctl(2) calls, then the above call may be made repeatedly
    by a task to further reduce its access to the kernel.  All attached
    programs must be evaluated before a system call will be allowed to
    proceed.
    
    Filter programs will be inherited across fork/clone and execve.
    However, if the task attaching the filter is unprivileged
    (!CAP_SYS_ADMIN) the no_new_privs bit will be set on the task.  This
    ensures that unprivileged tasks cannot attach filters that affect
    privileged tasks (e.g., setuid binary).
    
    There are a number of benefits to this approach. A few of which are
    as follows:
    - BPF has been exposed to userland for a long time
    - BPF optimization (and JIT'ing) are well understood
    - Userland already knows its ABI: system call numbers and desired
      arguments
    - No time-of-check-time-of-use vulnerable data accesses are possible.
    - system call arguments are loaded on access only to minimize copying
      required for system call policy decisions.
    
    Mode 2 support is restricted to architectures that enable
    HAVE_ARCH_SECCOMP_FILTER.  In this patch, the primary dependency is on
    syscall_get_arguments().  The full desired scope of this feature will
    add a few minor additional requirements expressed later in this series.
    Based on discussion, SECCOMP_RET_ERRNO and SECCOMP_RET_TRACE seem to be
    the desired additional functionality.
    
    No architectures are enabled in this patch.
    
    Signed-off-by: Will Drewry <wad@chromium.org>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Reviewed-by: Indan Zupancic <indan@nul.nu>
    Acked-by: Eric Paris <eparis@redhat.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    
    v18: - rebase to v3.4-rc2
         - s/chk/check/ (akpm@linux-foundation.org,jmorris@namei.org)
         - allocate with GFP_KERNEL|__GFP_NOWARN (indan@nul.nu)
         - add a comment for get_u32 regarding endianness (akpm@)
         - fix other typos, style mistakes (akpm@)
         - added acked-by
    v17: - properly guard seccomp filter needed headers (leann@ubuntu.com)
         - tighten return mask to 0x7fff0000
    v16: - no change
    v15: - add a 4 instr penalty when counting a path to account for seccomp_filter
           size (indan@nul.nu)
         - drop the max insns to 256KB (indan@nul.nu)
         - return ENOMEM if the max insns limit has been hit (indan@nul.nu)
         - move IP checks after args (indan@nul.nu)
         - drop !user_filter check (indan@nul.nu)
         - only allow explicit bpf codes (indan@nul.nu)
         - exit_code -> exit_sig
    v14: - put/get_seccomp_filter takes struct task_struct
           (indan@nul.nu,keescook@chromium.org)
         - adds seccomp_chk_filter and drops general bpf_run/chk_filter user
         - add seccomp_bpf_load for use by net/core/filter.c
         - lower max per-process/per-hierarchy: 1MB
         - moved nnp/capability check prior to allocation
           (all of the above: indan@nul.nu)
    v13: - rebase on to 88ebdda6159ffc15699f204c33feb3e431bf9bdc
    v12: - added a maximum instruction count per path (indan@nul.nu,oleg@redhat.com)
         - removed copy_seccomp (keescook@chromium.org,indan@nul.nu)
         - reworded the prctl_set_seccomp comment (indan@nul.nu)
    v11: - reorder struct seccomp_data to allow future args expansion (hpa@zytor.com)
         - style clean up, @compat dropped, compat_sock_fprog32 (indan@nul.nu)
         - do_exit(SIGSYS) (keescook@chromium.org, luto@mit.edu)
         - pare down Kconfig doc reference.
         - extra comment clean up
    v10: - seccomp_data has changed again to be more aesthetically pleasing
           (hpa@zytor.com)
         - calling convention is noted in a new u32 field using syscall_get_arch.
           This allows for cross-calling convention tasks to use seccomp filters.
           (hpa@zytor.com)
         - lots of clean up (thanks, Indan!)
     v9: - n/a
     v8: - use bpf_chk_filter, bpf_run_filter. update load_fns
         - Lots of fixes courtesy of indan@nul.nu:
         -- fix up load behavior, compat fixups, and merge alloc code,
         -- renamed pc and dropped __packed, use bool compat.
         -- Added a hidden CONFIG_SECCOMP_FILTER to synthesize non-arch
            dependencies
     v7:  (massive overhaul thanks to Indan, others)
         - added CONFIG_HAVE_ARCH_SECCOMP_FILTER
         - merged into seccomp.c
         - minimal seccomp_filter.h
         - no config option (part of seccomp)
         - no new prctl
         - doesn't break seccomp on systems without asm/syscall.h
           (works but arg access always fails)
         - dropped seccomp_init_task, extra free functions, ...
         - dropped the no-asm/syscall.h code paths
         - merges with network sk_run_filter and sk_chk_filter
     v6: - fix memory leak on attach compat check failure
         - require no_new_privs || CAP_SYS_ADMIN prior to filter
           installation. (luto@mit.edu)
         - s/seccomp_struct_/seccomp_/ for macros/functions (amwang@redhat.com)
         - cleaned up Kconfig (amwang@redhat.com)
         - on block, note if the call was compat (so the # means something)
     v5: - uses syscall_get_arguments
           (indan@nul.nu,oleg@redhat.com, mcgrathr@chromium.org)
          - uses union-based arg storage with hi/lo struct to
            handle endianness.  Compromises between the two alternate
            proposals to minimize extra arg shuffling and account for
            endianness assuming userspace uses offsetof().
            (mcgrathr@chromium.org, indan@nul.nu)
          - update Kconfig description
          - add include/seccomp_filter.h and add its installation
          - (naive) on-demand syscall argument loading
          - drop seccomp_t (eparis@redhat.com)
     v4:  - adjusted prctl to make room for PR_[SG]ET_NO_NEW_PRIVS
          - now uses current->no_new_privs
            (luto@mit.edu,torvalds@linux-foundation.com)
          - assign names to seccomp modes (rdunlap@xenotime.net)
          - fix style issues (rdunlap@xenotime.net)
          - reworded Kconfig entry (rdunlap@xenotime.net)
     v3:  - macros to inline (oleg@redhat.com)
          - init_task behavior fixed (oleg@redhat.com)
          - drop creator entry and extra NULL check (oleg@redhat.com)
          - alloc returns -EINVAL on bad sizing (serge.hallyn@canonical.com)
          - adds tentative use of "always_unprivileged" as per
            torvalds@linux-foundation.org and luto@mit.edu
     v2:  - (patch 2 only)
    Signed-off-by: James Morris <james.l.morris@oracle.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index b9372a0bff18..f7cf6fb107ec 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -34,6 +34,7 @@
 #include <linux/cgroup.h>
 #include <linux/security.h>
 #include <linux/hugetlb.h>
+#include <linux/seccomp.h>
 #include <linux/swap.h>
 #include <linux/syscalls.h>
 #include <linux/jiffies.h>
@@ -170,6 +171,7 @@ void free_task(struct task_struct *tsk)
 	free_thread_info(tsk->stack);
 	rt_mutex_debug_task_free(tsk);
 	ftrace_graph_exit_task(tsk);
+	put_seccomp_filter(tsk);
 	free_task_struct(tsk);
 }
 EXPORT_SYMBOL(free_task);
@@ -1162,6 +1164,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto fork_out;
 
 	ftrace_graph_init_task(p);
+	get_seccomp_filter(p);
 
 	rt_mutex_init_task(p);
 

commit 959d851caa48829eb85cb85aa949fd6b4c5d5bc6
Merge: a5567932fc92 48ddbe194623
Author: Tejun Heo <tj@kernel.org>
Date:   Sun Apr 1 12:30:01 2012 -0700

    Merge branch 'for-3.5' of ../cgroup into block/for-3.5/core-merged
    
    cgroup/for-3.5 contains the following changes which blk-cgroup needs
    to proceed with the on-going cleanup.
    
    * Dynamic addition and removal of cftypes to make config/stat file
      handling modular for policies.
    
    * cgroup removal update to not wait for css references to drain to fix
      blkcg removal hang caused by cfq caching cfqgs.
    
    Pull in cgroup/for-3.5 into block/for-3.5/core.  This causes the
    following conflicts in block/blk-cgroup.c.
    
    * 761b3ef50e "cgroup: remove cgroup_subsys argument from callbacks"
      conflicts with blkiocg_pre_destroy() addition and blkiocg_attach()
      removal.  Resolved by removing @subsys from all subsys methods.
    
    * 676f7c8f84 "cgroup: relocate cftype and cgroup_subsys definitions in
      controllers" conflicts with ->pre_destroy() and ->attach() updates
      and removal of modular config.  Resolved by dropping forward
      declarations of the methods and applying updates to the relocated
      blkio_subsys.
    
    * 4baf6e3325 "cgroup: convert all non-memcg controllers to the new
      cftype interface" builds upon the previous item.  Resolved by adding
      ->base_cftypes to the relocated blkio_subsys.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit 682968e0c425c60f0dde37977e5beb2b12ddc4cc
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Fri Mar 30 23:56:46 2012 +0530

    uprobes/core: Optimize probe hits with the help of a counter
    
    Maintain a per-mm counter: number of uprobes that are inserted
    on this process address space.
    
    This counter can be used at probe hit time to determine if we
    need a lookup in the uprobes rbtree. Everytime a probe gets
    inserted successfully, the probe count is incremented and
    everytime a probe gets removed, the probe count is decremented.
    
    The new uprobe_munmap hook ensures the count is correct on a
    unmap or remap of a region. We expect that once a
    uprobe_munmap() is called, the vma goes away.  So
    uprobe_unregister() finding a probe to unregister would either
    mean unmap event hasnt occurred yet or a mmap event on the same
    executable file occured after a unmap event.
    
    Additionally, uprobe_mmap hook now also gets called:
    
     a. on every executable vma that is COWed at fork.
     b. a vma of interest is newly mapped; breakpoint insertion also
        happens at the required address.
    
    On process creation, make sure the probes count in the child is
    set correctly.
    
    Special cases that are taken care include:
    
     a. mremap
     b. VM_DONTCOPY vmas on fork()
     c. insertion/removal races in the parent during fork().
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@linux.vnet.ibm.com>
    Cc: Linux-mm <linux-mm@kvack.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Anton Arapov <anton@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120330182646.10018.85805.sendpatchset@srdronam.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3133b9da59d5..26a8f5c25805 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -421,6 +421,9 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 
 		if (retval)
 			goto out;
+
+		if (file && uprobe_mmap(tmp))
+			goto out;
 	}
 	/* a new mm has just been created */
 	arch_dup_mmap(oldmm, mm);

commit d4b3b6384f98f8692ad0209891ccdbc7e78bbefe
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Fri Mar 30 23:56:31 2012 +0530

    uprobes/core: Allocate XOL slots for uprobes use
    
    Uprobes executes the original instruction at a probed location
    out of line. For this, we allocate a page (per mm) upon the
    first uprobe hit, in the process user address space, divide it
    into slots that are used to store the actual instructions to be
    singlestepped. These slots are known as xol (execution out of
    line) slots.
    
    Care is taken to ensure that the allocation is in an unmapped
    area as close to the top of the user address space as possible,
    with appropriate permission settings to keep selinux like
    frameworks happy.
    
    Upon a uprobe hit, a free slot is acquired, and is released
    after the singlestep completes.
    
    Lots of improvements courtesy suggestions/inputs from Peter and
    Oleg.
    
    [ Folded a fix for build issue on powerpc fixed and reported by
      Stephen Rothwell. ]
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@linux.vnet.ibm.com>
    Cc: Linux-mm <linux-mm@kvack.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Anton Arapov <anton@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120330182631.10018.48175.sendpatchset@srdronam.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index eb7b63334009..3133b9da59d5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -554,6 +554,7 @@ void mmput(struct mm_struct *mm)
 	might_sleep();
 
 	if (atomic_dec_and_test(&mm->mm_users)) {
+		uprobe_clear_state(mm);
 		exit_aio(mm);
 		ksm_exit(mm);
 		khugepaged_exit(mm); /* must run before exit_mmap */
@@ -760,6 +761,7 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 	mm->pmd_huge_pte = NULL;
 #endif
+	uprobe_reset_state(mm);
 
 	if (!mm_init(mm, tsk))
 		goto fail_nomem;

commit ebec18a6d3aa1e7d84aab16225e87fd25170ec2b
Author: Lennart Poettering <lennart@poettering.net>
Date:   Fri Mar 23 15:01:54 2012 -0700

    prctl: add PR_{SET,GET}_CHILD_SUBREAPER to allow simple process supervision
    
    Userspace service managers/supervisors need to track their started
    services.  Many services daemonize by double-forking and get implicitly
    re-parented to PID 1.  The service manager will no longer be able to
    receive the SIGCHLD signals for them, and is no longer in charge of
    reaping the children with wait().  All information about the children is
    lost at the moment PID 1 cleans up the re-parented processes.
    
    With this prctl, a service manager process can mark itself as a sort of
    'sub-init', able to stay as the parent for all orphaned processes
    created by the started services.  All SIGCHLD signals will be delivered
    to the service manager.
    
    Receiving SIGCHLD and doing wait() is in cases of a service-manager much
    preferred over any possible asynchronous notification about specific
    PIDs, because the service manager has full access to the child process
    data in /proc and the PID can not be re-used until the wait(), the
    service-manager itself is in charge of, has happened.
    
    As a side effect, the relevant parent PID information does not get lost
    by a double-fork, which results in a more elaborate process tree and
    'ps' output:
    
    before:
      # ps afx
      253 ?        Ss     0:00 /bin/dbus-daemon --system --nofork
      294 ?        Sl     0:00 /usr/libexec/polkit-1/polkitd
      328 ?        S      0:00 /usr/sbin/modem-manager
      608 ?        Sl     0:00 /usr/libexec/colord
      658 ?        Sl     0:00 /usr/libexec/upowerd
      819 ?        Sl     0:00 /usr/libexec/imsettings-daemon
      916 ?        Sl     0:00 /usr/libexec/udisks-daemon
      917 ?        S      0:00  \_ udisks-daemon: not polling any devices
    
    after:
      # ps afx
      294 ?        Ss     0:00 /bin/dbus-daemon --system --nofork
      426 ?        Sl     0:00  \_ /usr/libexec/polkit-1/polkitd
      449 ?        S      0:00  \_ /usr/sbin/modem-manager
      635 ?        Sl     0:00  \_ /usr/libexec/colord
      705 ?        Sl     0:00  \_ /usr/libexec/upowerd
      959 ?        Sl     0:00  \_ /usr/libexec/udisks-daemon
      960 ?        S      0:00  |   \_ udisks-daemon: not polling any devices
      977 ?        Sl     0:00  \_ /usr/libexec/packagekitd
    
    This prctl is orthogonal to PID namespaces.  PID namespaces are isolated
    from each other, while a service management process usually requires the
    services to live in the same namespace, to be able to talk to each
    other.
    
    Users of this will be the systemd per-user instance, which provides
    init-like functionality for the user's login session and D-Bus, which
    activates bus services on-demand.  Both need init-like capabilities to
    be able to properly keep track of the services they start.
    
    Many thanks to Oleg for several rounds of review and insights.
    
    [akpm@linux-foundation.org: fix comment layout and spelling]
    [akpm@linux-foundation.org: add lengthy code comment from Oleg]
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Lennart Poettering <lennart@poettering.net>
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Acked-by: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 37674ec55cde..b9372a0bff18 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1051,6 +1051,9 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->oom_score_adj = current->signal->oom_score_adj;
 	sig->oom_score_adj_min = current->signal->oom_score_adj_min;
 
+	sig->has_child_subreaper = current->signal->has_child_subreaper ||
+				   current->signal->is_child_subreaper;
+
 	mutex_init(&sig->cred_guard_mutex);
 
 	return 0;

commit 95211279c5ad00a317c98221d7e4365e02f20836
Merge: 5375871d432a 12724850e806
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:04:48 2012 -0700

    Merge branch 'akpm' (Andrew's patch-bomb)
    
    Merge first batch of patches from Andrew Morton:
     "A few misc things and all the MM queue"
    
    * emailed from Andrew Morton <akpm@linux-foundation.org>: (92 commits)
      memcg: avoid THP split in task migration
      thp: add HPAGE_PMD_* definitions for !CONFIG_TRANSPARENT_HUGEPAGE
      memcg: clean up existing move charge code
      mm/memcontrol.c: remove unnecessary 'break' in mem_cgroup_read()
      mm/memcontrol.c: remove redundant BUG_ON() in mem_cgroup_usage_unregister_event()
      mm/memcontrol.c: s/stealed/stolen/
      memcg: fix performance of mem_cgroup_begin_update_page_stat()
      memcg: remove PCG_FILE_MAPPED
      memcg: use new logic for page stat accounting
      memcg: remove PCG_MOVE_LOCK flag from page_cgroup
      memcg: simplify move_account() check
      memcg: remove EXPORT_SYMBOL(mem_cgroup_update_page_stat)
      memcg: kill dead prev_priority stubs
      memcg: remove PCG_CACHE page_cgroup flag
      memcg: let css_get_next() rely upon rcu_read_lock()
      cgroup: revert ss_id_lock to spinlock
      idr: make idr_get_next() good for rcu_read_lock()
      memcg: remove unnecessary thp check in page stat accounting
      memcg: remove redundant returns
      memcg: enum lru_list lru
      ...

commit cc9a6c8776615f9c194ccf0b63a0aa5628235545
Author: Mel Gorman <mgorman@suse.de>
Date:   Wed Mar 21 16:34:11 2012 -0700

    cpuset: mm: reduce large amounts of memory barrier related damage v3
    
    Commit c0ff7453bb5c ("cpuset,mm: fix no node to alloc memory when
    changing cpuset's mems") wins a super prize for the largest number of
    memory barriers entered into fast paths for one commit.
    
    [get|put]_mems_allowed is incredibly heavy with pairs of full memory
    barriers inserted into a number of hot paths.  This was detected while
    investigating at large page allocator slowdown introduced some time
    after 2.6.32.  The largest portion of this overhead was shown by
    oprofile to be at an mfence introduced by this commit into the page
    allocator hot path.
    
    For extra style points, the commit introduced the use of yield() in an
    implementation of what looks like a spinning mutex.
    
    This patch replaces the full memory barriers on both read and write
    sides with a sequence counter with just read barriers on the fast path
    side.  This is much cheaper on some architectures, including x86.  The
    main bulk of the patch is the retry logic if the nodemask changes in a
    manner that can cause a false failure.
    
    While updating the nodemask, a check is made to see if a false failure
    is a risk.  If it is, the sequence number gets bumped and parallel
    allocators will briefly stall while the nodemask update takes place.
    
    In a page fault test microbenchmark, oprofile samples from
    __alloc_pages_nodemask went from 4.53% of all samples to 1.15%.  The
    actual results were
    
                                 3.3.0-rc3          3.3.0-rc3
                                 rc3-vanilla        nobarrier-v2r1
        Clients   1 UserTime       0.07 (  0.00%)   0.08 (-14.19%)
        Clients   2 UserTime       0.07 (  0.00%)   0.07 (  2.72%)
        Clients   4 UserTime       0.08 (  0.00%)   0.07 (  3.29%)
        Clients   1 SysTime        0.70 (  0.00%)   0.65 (  6.65%)
        Clients   2 SysTime        0.85 (  0.00%)   0.82 (  3.65%)
        Clients   4 SysTime        1.41 (  0.00%)   1.41 (  0.32%)
        Clients   1 WallTime       0.77 (  0.00%)   0.74 (  4.19%)
        Clients   2 WallTime       0.47 (  0.00%)   0.45 (  3.73%)
        Clients   4 WallTime       0.38 (  0.00%)   0.37 (  1.58%)
        Clients   1 Flt/sec/cpu  497620.28 (  0.00%) 520294.53 (  4.56%)
        Clients   2 Flt/sec/cpu  414639.05 (  0.00%) 429882.01 (  3.68%)
        Clients   4 Flt/sec/cpu  257959.16 (  0.00%) 258761.48 (  0.31%)
        Clients   1 Flt/sec      495161.39 (  0.00%) 517292.87 (  4.47%)
        Clients   2 Flt/sec      820325.95 (  0.00%) 850289.77 (  3.65%)
        Clients   4 Flt/sec      1020068.93 (  0.00%) 1022674.06 (  0.26%)
        MMTests Statistics: duration
        Sys Time Running Test (seconds)             135.68    132.17
        User+Sys Time Running Test (seconds)         164.2    160.13
        Total Elapsed Time (seconds)                123.46    120.87
    
    The overall improvement is small but the System CPU time is much
    improved and roughly in correlation to what oprofile reported (these
    performance figures are without profiling so skew is expected).  The
    actual number of page faults is noticeably improved.
    
    For benchmarks like kernel builds, the overall benefit is marginal but
    the system CPU time is slightly reduced.
    
    To test the actual bug the commit fixed I opened two terminals.  The
    first ran within a cpuset and continually ran a small program that
    faulted 100M of anonymous data.  In a second window, the nodemask of the
    cpuset was continually randomised in a loop.
    
    Without the commit, the program would fail every so often (usually
    within 10 seconds) and obviously with the commit everything worked fine.
    With this patch applied, it also worked fine so the fix should be
    functionally equivalent.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Christoph Lameter <cl@linux.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a9e99f3c18e0..9cc227d54102 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1237,6 +1237,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_CPUSETS
 	p->cpuset_mem_spread_rotor = NUMA_NO_NODE;
 	p->cpuset_slab_spread_rotor = NUMA_NO_NODE;
+	seqcount_init(&p->mems_allowed_seq);
 #endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	p->irq_events = 0;

commit c3f0327f8e9d7a503f0d64573c311eddd61f197d
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Wed Mar 21 16:33:48 2012 -0700

    mm: add rss counters consistency check
    
    Warn about non-zero rss counters at final mmdrop.
    
    This check will prevent reoccurences of bugs such as that fixed in "mm:
    fix rss count leakage during migration".
    
    I didn't hide this check under CONFIG_VM_DEBUG because it rather small and
    rss counters cover whole page-table management, so this is a good
    invariant.
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c4f38a849436..a9e99f3c18e0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -511,6 +511,23 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	return NULL;
 }
 
+static void check_mm(struct mm_struct *mm)
+{
+	int i;
+
+	for (i = 0; i < NR_MM_COUNTERS; i++) {
+		long x = atomic_long_read(&mm->rss_stat.count[i]);
+
+		if (unlikely(x))
+			printk(KERN_ALERT "BUG: Bad rss-counter state "
+					  "mm:%p idx:%d val:%ld\n", mm, i, x);
+	}
+
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+	VM_BUG_ON(mm->pmd_huge_pte);
+#endif
+}
+
 /*
  * Allocate and initialize an mm_struct.
  */
@@ -538,9 +555,7 @@ void __mmdrop(struct mm_struct *mm)
 	mm_free_pgd(mm);
 	destroy_context(mm);
 	mmu_notifier_mm_destroy(mm);
-#ifdef CONFIG_TRANSPARENT_HUGEPAGE
-	VM_BUG_ON(mm->pmd_huge_pte);
-#endif
+	check_mm(mm);
 	free_mm(mm);
 }
 EXPORT_SYMBOL_GPL(__mmdrop);

commit 3556485f1595e3964ba539e39ea682acbb835cee
Merge: b8716614a7cc 09f61cdbb32a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 21 13:25:04 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates for 3.4 from James Morris:
     "The main addition here is the new Yama security module from Kees Cook,
      which was discussed at the Linux Security Summit last year.  Its
      purpose is to collect miscellaneous DAC security enhancements in one
      place.  This also marks a departure in policy for LSM modules, which
      were previously limited to being standalone access control systems.
      Chromium OS is using Yama, and I believe there are plans for Ubuntu,
      at least.
    
      This patchset also includes maintenance updates for AppArmor, TOMOYO
      and others."
    
    Fix trivial conflict in <net/sock.h> due to the jumo_label->static_key
    rename.
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (38 commits)
      AppArmor: Fix location of const qualifier on generated string tables
      TOMOYO: Return error if fails to delete a domain
      AppArmor: add const qualifiers to string arrays
      AppArmor: Add ability to load extended policy
      TOMOYO: Return appropriate value to poll().
      AppArmor: Move path failure information into aa_get_name and rename
      AppArmor: Update dfa matching routines.
      AppArmor: Minor cleanup of d_namespace_path to consolidate error handling
      AppArmor: Retrieve the dentry_path for error reporting when path lookup fails
      AppArmor: Add const qualifiers to generated string tables
      AppArmor: Fix oops in policy unpack auditing
      AppArmor: Fix error returned when a path lookup is disconnected
      KEYS: testing wrong bit for KEY_FLAG_REVOKED
      TOMOYO: Fix mount flags checking order.
      security: fix ima kconfig warning
      AppArmor: Fix the error case for chroot relative path name lookup
      AppArmor: fix mapping of META_READ to audit and quiet flags
      AppArmor: Fix underflow in xindex calculation
      AppArmor: Fix dropping of allowed operations that are force audited
      AppArmor: Add mising end of structure test to caps unpacking
      ...

commit 5f8aadd8b9966d71a77bba52b9d499cc2f38269f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Mar 14 19:55:38 2012 +0100

    CLONE_PARENT shouldn't allow to set ->exit_signal
    
    The child must not control its ->exit_signal, it is the parent who
    decides which signal the child should use for notification.
    
    This means that CLONE_PARENT should not use "clone_flags & CSIGNAL",
    the forking task is the sibling of the new process and their parent
    doesn't control exit_signal in this case.
    
    This patch uses ->exit_signal of the forking process, but perhaps
    we should simply use SIGCHLD.
    
    We read group_leader->exit_signal lockless, this can race with the
    ORIGINAL_SIGNAL -> SIGCHLD transition, but this is fine.
    
    Potentially this change allows to kill self_exec_id/parent_exec_id.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 26a7a6707fa7..c4f38a849436 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1340,7 +1340,13 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	clear_all_latency_tracing(p);
 
 	/* ok, now we should be set up.. */
-	p->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);
+	if (clone_flags & CLONE_THREAD)
+		p->exit_signal = -1;
+	else if (clone_flags & CLONE_PARENT)
+		p->exit_signal = current->group_leader->exit_signal;
+	else
+		p->exit_signal = (clone_flags & CSIGNAL);
+
 	p->pdeath_signal = 0;
 	p->exit_state = 0;
 

commit 0326f5a94ddea33fa331b2519f4172f4fb387baa
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Tue Mar 13 23:30:11 2012 +0530

    uprobes/core: Handle breakpoint and singlestep exceptions
    
    Uprobes uses exception notifiers to get to know if a thread hit
    a breakpoint or a singlestep exception.
    
    When a thread hits a uprobe or is singlestepping post a uprobe
    hit, the uprobe exception notifier sets its TIF_UPROBE bit,
    which will then be checked on its return to userspace path
    (do_notify_resume() ->uprobe_notify_resume()), where the
    consumers handlers are run (in task context) based on the
    defined filters.
    
    Uprobe hits are thread specific and hence we need to maintain
    information about if a task hit a uprobe, what uprobe was hit,
    the slot where the original instruction was copied for xol so
    that it can be singlestepped with appropriate fixups.
    
    In some cases, special care is needed for instructions that are
    executed out of line (xol). These are architecture specific
    artefacts, such as handling RIP relative instructions on x86_64.
    
    Since the instruction at which the uprobe was inserted is
    executed out of line, architecture specific fixups are added so
    that the thread continues normal execution in the presence of a
    uprobe.
    
    Postpone the signals until we execute the probed insn.
    post_xol() path does a recalc_sigpending() before return to
    user-mode, this ensures the signal can't be lost.
    
    Uprobes relies on DIE_DEBUG notification to notify if a
    singlestep is complete.
    
    Adds x86 specific uprobe exception notifiers and appropriate
    hooks needed to determine a uprobe hit and subsequent post
    processing.
    
    Add requisite x86 fixups for xol for uprobes. Specific cases
    needing fixups include relative jumps (x86_64), calls, etc.
    
    Where possible, we check and skip singlestepping the
    breakpointed instructions. For now we skip single byte as well
    as few multibyte nop instructions. However this can be extended
    to other instructions too.
    
    Credits to Oleg Nesterov for suggestions/patches related to
    signal, breakpoint, singlestep handling code.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@linux.vnet.ibm.com>
    Cc: Linux-mm <linux-mm@kvack.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120313180011.29771.89027.sendpatchset@srdronam.in.ibm.com
    [ Performed various cleanliness edits ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index e2cd3e2a5ae8..eb7b63334009 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -67,6 +67,7 @@
 #include <linux/oom.h>
 #include <linux/khugepaged.h>
 #include <linux/signalfd.h>
+#include <linux/uprobes.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -701,6 +702,8 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 		exit_pi_state_list(tsk);
 #endif
 
+	uprobe_free_utask(tsk);
+
 	/* Get rid of any cached register state */
 	deactivate_mm(tsk, mm);
 
@@ -1295,6 +1298,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	INIT_LIST_HEAD(&p->pi_state_list);
 	p->pi_state_cache = NULL;
 #endif
+	uprobe_copy_process(p);
 	/*
 	 * sigaltstack should be cleared when sharing the same VM
 	 */

commit 3d48749d93a3dce732dd30a14002ab90ec4355f3
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Mar 5 13:15:25 2012 -0800

    block: ioc_task_link() can't fail
    
    ioc_task_link() is used to share %current's ioc on clone.  If
    %current->io_context is set, %current is guaranteed to have refcount
    on the ioc and, thus, ioc_task_link() can't fail.
    
    Replace error checking in ioc_task_link() with WARN_ON_ONCE() and make
    it just increment refcount and nr_tasks.
    
    -v2: Description typo fix (Vivek).
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/kernel/fork.c b/kernel/fork.c
index b77fd559c78e..a1b632713e43 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -901,9 +901,8 @@ static int copy_io(unsigned long clone_flags, struct task_struct *tsk)
 	 * Share io context with parent, if CLONE_IO is set
 	 */
 	if (clone_flags & CLONE_IO) {
-		tsk->io_context = ioc_task_link(ioc);
-		if (unlikely(!tsk->io_context))
-			return -ENOMEM;
+		ioc_task_link(ioc);
+		tsk->io_context = ioc;
 	} else if (ioprio_valid(ioc->ioprio)) {
 		new_ioc = get_task_io_context(tsk, GFP_KERNEL, NUMA_NO_NODE);
 		if (unlikely(!new_ioc))

commit 6e27f63edbd7ab893258e16500171dd1270a1369
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 5 14:59:14 2012 -0800

    vfork: kill PF_STARTING
    
    Previously it was (ab)used by utrace.  Then it was wrongly used by the
    scheduler code.
    
    Currently it is not used, kill it before it finds the new erroneous user.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 44b0e21af50e..26a7a6707fa7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1046,7 +1046,6 @@ static void copy_flags(unsigned long clone_flags, struct task_struct *p)
 
 	new_flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);
 	new_flags |= PF_FORKNOEXEC;
-	new_flags |= PF_STARTING;
 	p->flags = new_flags;
 }
 
@@ -1579,14 +1578,6 @@ long do_fork(unsigned long clone_flags,
 			get_task_struct(p);
 		}
 
-		/*
-		 * We set PF_STARTING at creation in case tracing wants to
-		 * use this to distinguish a fully live task from one that
-		 * hasn't finished SIGSTOP raising yet.  Now we clear it
-		 * and set the child going.
-		 */
-		p->flags &= ~PF_STARTING;
-
 		wake_up_new_task(p);
 
 		/* forking complete and child started to run, tell ptracer */

commit 57b59c4a1400fa6c34764eab2e35a8762dc05a09
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 5 14:59:13 2012 -0800

    coredump_wait: don't call complete_vfork_done()
    
    Now that CLONE_VFORK is killable, coredump_wait() no longer needs
    complete_vfork_done().  zap_threads() should find and kill all tasks with
    the same ->mm, this includes our parent if ->vfork_done is set.
    
    mm_release() becomes the only caller, unexport complete_vfork_done().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 892c534ce6e3..44b0e21af50e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -668,7 +668,7 @@ struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)
 	return mm;
 }
 
-void complete_vfork_done(struct task_struct *tsk)
+static void complete_vfork_done(struct task_struct *tsk)
 {
 	struct completion *vfork;
 

commit d68b46fe16ad59b3a5f51ec73daaa5dc06753798
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 5 14:59:13 2012 -0800

    vfork: make it killable
    
    Make vfork() killable.
    
    Change do_fork(CLONE_VFORK) to do wait_for_completion_killable().  If it
    fails we do not return to the user-mode and never touch the memory shared
    with our child.
    
    However, in this case we should clear child->vfork_done before return, we
    use task_lock() in do_fork()->wait_for_vfork_done() and
    complete_vfork_done() to serialize with each other.
    
    Note: now that we use task_lock() we don't really need completion, we
    could turn task->vfork_done into "task_struct *wake_up_me" but this needs
    some complications.
    
    NOTE: this and the next patches do not affect in-kernel users of
    CLONE_VFORK, kernel threads run with all signals ignored including
    SIGKILL/SIGSTOP.
    
    However this is obviously the user-visible change.  Not only a fatal
    signal can kill the vforking parent, a sub-thread can do execve or
    exit_group() and kill the thread sleeping in vfork().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index cf3d96379608..892c534ce6e3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -670,10 +670,34 @@ struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)
 
 void complete_vfork_done(struct task_struct *tsk)
 {
-	struct completion *vfork_done = tsk->vfork_done;
+	struct completion *vfork;
 
-	tsk->vfork_done = NULL;
-	complete(vfork_done);
+	task_lock(tsk);
+	vfork = tsk->vfork_done;
+	if (likely(vfork)) {
+		tsk->vfork_done = NULL;
+		complete(vfork);
+	}
+	task_unlock(tsk);
+}
+
+static int wait_for_vfork_done(struct task_struct *child,
+				struct completion *vfork)
+{
+	int killed;
+
+	freezer_do_not_count();
+	killed = wait_for_completion_killable(vfork);
+	freezer_count();
+
+	if (killed) {
+		task_lock(child);
+		child->vfork_done = NULL;
+		task_unlock(child);
+	}
+
+	put_task_struct(child);
+	return killed;
 }
 
 /* Please note the differences between mmput and mm_release.
@@ -717,7 +741,8 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 	 * If we're exiting normally, clear a user-space tid field if
 	 * requested.  We leave this alone when dying by signal, to leave
 	 * the value intact in a core dump, and to save the unnecessary
-	 * trouble otherwise.  Userland only wants this done for a sys_exit.
+	 * trouble, say, a killed vfork parent shouldn't touch this mm.
+	 * Userland only wants this done for a sys_exit.
 	 */
 	if (tsk->clear_child_tid) {
 		if (!(tsk->flags & PF_SIGNALED) &&
@@ -1551,6 +1576,7 @@ long do_fork(unsigned long clone_flags,
 		if (clone_flags & CLONE_VFORK) {
 			p->vfork_done = &vfork;
 			init_completion(&vfork);
+			get_task_struct(p);
 		}
 
 		/*
@@ -1568,10 +1594,8 @@ long do_fork(unsigned long clone_flags,
 			ptrace_event(trace, nr);
 
 		if (clone_flags & CLONE_VFORK) {
-			freezer_do_not_count();
-			wait_for_completion(&vfork);
-			freezer_count();
-			ptrace_event(PTRACE_EVENT_VFORK_DONE, nr);
+			if (!wait_for_vfork_done(p, &vfork))
+				ptrace_event(PTRACE_EVENT_VFORK_DONE, nr);
 		}
 	} else {
 		nr = PTR_ERR(p);

commit c415c3b47ea2754659d915cca387a20999044163
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 5 14:59:13 2012 -0800

    vfork: introduce complete_vfork_done()
    
    No functional changes.
    
    Move the clear-and-complete-vfork_done code into the new trivial helper,
    complete_vfork_done().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e2cd3e2a5ae8..cf3d96379608 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -668,6 +668,14 @@ struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)
 	return mm;
 }
 
+void complete_vfork_done(struct task_struct *tsk)
+{
+	struct completion *vfork_done = tsk->vfork_done;
+
+	tsk->vfork_done = NULL;
+	complete(vfork_done);
+}
+
 /* Please note the differences between mmput and mm_release.
  * mmput is called whenever we stop holding onto a mm_struct,
  * error success whatever.
@@ -683,8 +691,6 @@ struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)
  */
 void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 {
-	struct completion *vfork_done = tsk->vfork_done;
-
 	/* Get rid of any futexes when releasing the mm */
 #ifdef CONFIG_FUTEX
 	if (unlikely(tsk->robust_list)) {
@@ -704,11 +710,8 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 	/* Get rid of any cached register state */
 	deactivate_mm(tsk, mm);
 
-	/* notify parent sleeping on vfork() */
-	if (vfork_done) {
-		tsk->vfork_done = NULL;
-		complete(vfork_done);
-	}
+	if (tsk->vfork_done)
+		complete_vfork_done(tsk);
 
 	/*
 	 * If we're exiting normally, clear a user-space tid field if

commit d80e731ecab420ddcb79ee9d0ac427acbc187b4b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Feb 24 20:07:11 2012 +0100

    epoll: introduce POLLFREE to flush ->signalfd_wqh before kfree()
    
    This patch is intentionally incomplete to simplify the review.
    It ignores ep_unregister_pollwait() which plays with the same wqh.
    See the next change.
    
    epoll assumes that the EPOLL_CTL_ADD'ed file controls everything
    f_op->poll() needs. In particular it assumes that the wait queue
    can't go away until eventpoll_release(). This is not true in case
    of signalfd, the task which does EPOLL_CTL_ADD uses its ->sighand
    which is not connected to the file.
    
    This patch adds the special event, POLLFREE, currently only for
    epoll. It expects that init_poll_funcptr()'ed hook should do the
    necessary cleanup. Perhaps it should be defined as EPOLLFREE in
    eventpoll.
    
    __cleanup_sighand() is changed to do wake_up_poll(POLLFREE) if
    ->signalfd_wqh is not empty, we add the new signalfd_cleanup()
    helper.
    
    ep_poll_callback(POLLFREE) simply does list_del_init(task_list).
    This make this poll entry inconsistent, but we don't care. If you
    share epoll fd which contains our sigfd with another process you
    should blame yourself. signalfd is "really special". I simply do
    not know how we can define the "right" semantics if it used with
    epoll.
    
    The main problem is, epoll calls signalfd_poll() once to establish
    the connection with the wait queue, after that signalfd_poll(NULL)
    returns the different/inconsistent results depending on who does
    EPOLL_CTL_MOD/signalfd_read/etc. IOW: apart from sigmask, signalfd
    has nothing to do with the file, it works with the current thread.
    
    In short: this patch is the hack which tries to fix the symptoms.
    It also assumes that nobody can take tasklist_lock under epoll
    locks, this seems to be true.
    
    Note:
    
            - we do not have wake_up_all_poll() but wake_up_poll()
              is fine, poll/epoll doesn't use WQ_FLAG_EXCLUSIVE.
    
            - signalfd_cleanup() uses POLLHUP along with POLLFREE,
              we need a couple of simple changes in eventpoll.c to
              make sure it can't be "lost".
    
    Reported-by: Maxime Bizon <mbizon@freebox.fr>
    Cc: <stable@kernel.org>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b77fd559c78e..e2cd3e2a5ae8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -66,6 +66,7 @@
 #include <linux/user-return-notifier.h>
 #include <linux/oom.h>
 #include <linux/khugepaged.h>
+#include <linux/signalfd.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -935,8 +936,10 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 
 void __cleanup_sighand(struct sighand_struct *sighand)
 {
-	if (atomic_dec_and_test(&sighand->count))
+	if (atomic_dec_and_test(&sighand->count)) {
+		signalfd_cleanup(sighand);
 		kmem_cache_free(sighand_cachep, sighand);
+	}
 }
 
 

commit 191c542442fdf53cc3c496c00be13367fd9cd42d
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Mon Feb 13 03:58:52 2012 +0000

    mm: collapse security_vm_enough_memory() variants into a single function
    
    Collapse security_vm_enough_memory() variants into a single function.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f0e7781ba9b4..d5ebddf317a9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -355,7 +355,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		charge = 0;
 		if (mpnt->vm_flags & VM_ACCOUNT) {
 			unsigned int len = (mpnt->vm_end - mpnt->vm_start) >> PAGE_SHIFT;
-			if (security_vm_enough_memory(len))
+			if (security_vm_enough_memory_mm(oldmm, len)) /* sic */
 				goto fail_nomem;
 			charge = len;
 		}

commit 3ec1e88b33a3bdd852ce8e014052acec7a9da8b5
Merge: 8df54d622a12 d8c66c5d5924
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 11 10:07:11 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.dk/linux-block
    
    Says Jens:
    
     "Time to push off some of the pending items.  I really wanted to wait
      until we had the regression nailed, but alas it's not quite there yet.
      But I'm very confident that it's "just" a missing expire on exit, so
      fix from Tejun should be fairly trivial.  I'm headed out for a week on
      the slopes.
    
      - Killing the barrier part of mtip32xx.  It doesn't really support
        barriers, and it doesn't need them (writes are fully ordered).
    
      - A few fixes from Dan Carpenter, preventing overflows of integer
        multiplication.
    
      - A fixup for loop, fixing a previous commit that didn't quite solve
        the partial read problem from Dave Young.
    
      - A bio integer overflow fix from Kent Overstreet.
    
      - Improvement/fix of the door "keep locked" part of the cdrom shared
        code from Paolo Benzini.
    
      - A few cfq fixes from Shaohua Li.
    
      - A fix for bsg sysfs warning when removing a file it did not create
        from Stanislaw Gruszka.
    
      - Two fixes for floppy from Vivek, preventing a crash.
    
      - A few block core fixes from Tejun.  One killing the over-optimized
        ioc exit path, cleaning that up nicely.  Two others fixing an oops
        on elevator switch, due to calling into the scheduler merge check
        code without holding the queue lock."
    
    * 'for-linus' of git://git.kernel.dk/linux-block:
      block: fix lockdep warning on io_context release put_io_context()
      relay: prevent integer overflow in relay_open()
      loop: zero fill bio instead of return -EIO for partial read
      bio: don't overflow in bio_get_nr_vecs()
      floppy: Fix a crash during rmmod
      floppy: Cleanup disk->queue before caling put_disk() if add_disk() was never called
      cdrom: move shared static to cdrom_device_info
      bsg: fix sysfs link remove warning
      block: don't call elevator callbacks for plug merges
      block: separate out blk_rq_merge_ok() and blk_try_merge() from elevator functions
      mtip32xx: removed the irrelevant argument of mtip_hw_submit_io() and the unused member of struct driver_data
      block: strip out locking optimization in put_io_context()
      cdrom: use copy_to_user() without the underscores
      block: fix ioc locking warning
      block: fix NULL icq_cache reference
      block,cfq: change code order

commit 1a2a4d06e1e95260c470ebe3a945f61bbe8c1fd8
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Dec 21 12:17:03 2011 -0800

    security: create task_free security callback
    
    The current LSM interface to cred_free is not sufficient for allowing
    an LSM to track the life and death of a task. This patch adds the
    task_free hook so that an LSM can clean up resources on task death.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1b2ef3c23ae4..f0e7781ba9b4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -192,6 +192,7 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
+	security_task_free(tsk);
 	exit_creds(tsk);
 	delayacct_tsk_free(tsk);
 	put_signal_struct(tsk->signal);

commit 11a3122f6cf2d988a77eb8883d0fc49cd013a6d5
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 7 07:51:30 2012 +0100

    block: strip out locking optimization in put_io_context()
    
    put_io_context() performed a complex trylock dancing to avoid
    deferring ioc release to workqueue.  It was also broken on UP because
    trylock was always assumed to succeed which resulted in unbalanced
    preemption count.
    
    While there are ways to fix the UP breakage, even the most
    pathological microbench (forced ioc allocation and tight fork/exit
    loop) fails to show any appreciable performance benefit of the
    optimization.  Strip it out.  If there turns out to be workloads which
    are affected by this change, simpler optimization from the discussion
    thread can be applied later.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    LKML-Reference: <1328514611.21268.66.camel@sli10-conroe>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 051f090d40c1..c574aefa8d1b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -890,7 +890,7 @@ static int copy_io(unsigned long clone_flags, struct task_struct *tsk)
 			return -ENOMEM;
 
 		new_ioc->ioprio = ioc->ioprio;
-		put_io_context(new_ioc, NULL);
+		put_io_context(new_ioc);
 	}
 #endif
 	return 0;

commit 8cdb878dcb359fd1137e9abdee9322f5e9bcfdf8
Author: Christopher Yeoh <cyeoh@au1.ibm.com>
Date:   Thu Feb 2 11:34:09 2012 +1030

    Fix race in process_vm_rw_core
    
    This fixes the race in process_vm_core found by Oleg (see
    
      http://article.gmane.org/gmane.linux.kernel/1235667/
    
    for details).
    
    This has been updated since I last sent it as the creation of the new
    mm_access() function did almost exactly the same thing as parts of the
    previous version of this patch did.
    
    In order to use mm_access() even when /proc isn't enabled, we move it to
    kernel/fork.c where other related process mm access functions already
    are.
    
    Signed-off-by: Chris Yeoh <yeohc@au1.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 051f090d40c1..1b2ef3c23ae4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -647,6 +647,26 @@ struct mm_struct *get_task_mm(struct task_struct *task)
 }
 EXPORT_SYMBOL_GPL(get_task_mm);
 
+struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)
+{
+	struct mm_struct *mm;
+	int err;
+
+	err =  mutex_lock_killable(&task->signal->cred_guard_mutex);
+	if (err)
+		return ERR_PTR(err);
+
+	mm = get_task_mm(task);
+	if (mm && mm != current->mm &&
+			!ptrace_may_access(task, mode)) {
+		mmput(mm);
+		mm = ERR_PTR(-EACCES);
+	}
+	mutex_unlock(&task->signal->cred_guard_mutex);
+
+	return mm;
+}
+
 /* Please note the differences between mmput and mm_release.
  * mmput is called whenever we stop holding onto a mm_struct,
  * error success whatever.

commit f429ee3b808118591d1f3cdf3c0d0793911a5677
Merge: 22b4eb5e3174 c158a35c8a68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 17 16:06:51 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/audit
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/audit: (29 commits)
      audit: no leading space in audit_log_d_path prefix
      audit: treat s_id as an untrusted string
      audit: fix signedness bug in audit_log_execve_info()
      audit: comparison on interprocess fields
      audit: implement all object interfield comparisons
      audit: allow interfield comparison between gid and ogid
      audit: complex interfield comparison helper
      audit: allow interfield comparison in audit rules
      Kernel: Audit Support For The ARM Platform
      audit: do not call audit_getname on error
      audit: only allow tasks to set their loginuid if it is -1
      audit: remove task argument to audit_set_loginuid
      audit: allow audit matching on inode gid
      audit: allow matching on obj_uid
      audit: remove audit_finish_fork as it can't be called
      audit: reject entry,always rules
      audit: inline audit_free to simplify the look of generic code
      audit: drop audit_set_macxattr as it doesn't do anything
      audit: inline checks for not needing to collect aux records
      audit: drop some potentially inadvisable likely notations
      ...
    
    Use evil merge to fix up grammar mistakes in Kconfig file.
    
    Bad speling and horrible grammar (and copious swearing) is to be
    expected, but let's keep it to commit messages and comments, rather than
    expose it to users in config help texts or printouts.

commit 6422e78de6880c66a82af512d9bd0c85eb62e661
Author: Eric Paris <eparis@redhat.com>
Date:   Tue Jan 3 14:23:07 2012 -0500

    audit: remove audit_finish_fork as it can't be called
    
    Audit entry,always rules are not allowed and are automatically changed in
    exit,always rules in userspace.  The kernel refuses to load such rules.
    
    Thus a task in the middle of a syscall (and thus in audit_finish_fork())
    can only be in one of two states: AUDIT_BUILD_CONTEXT or AUDIT_DISABLED.
    Since the current task cannot be in AUDIT_RECORD_CONTEXT we aren't every
    going to actually use the code in audit_finish_fork() since it will
    return without doing anything.  Thus drop the code.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 443f5125f11e..c1e5c21f48c1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1525,8 +1525,6 @@ long do_fork(unsigned long clone_flags,
 			init_completion(&vfork);
 		}
 
-		audit_finish_fork(p);
-
 		/*
 		 * We set PF_STARTING at creation in case tracing wants to
 		 * use this to distinguish a fully live task from one that

commit b3c9dd182ed3bdcdaf0e42625a35924b0497afdc
Merge: 83c2f912b43c 5d381efb3d1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 15 12:24:45 2012 -0800

    Merge branch 'for-3.3/core' of git://git.kernel.dk/linux-block
    
    * 'for-3.3/core' of git://git.kernel.dk/linux-block: (37 commits)
      Revert "block: recursive merge requests"
      block: Stop using macro stubs for the bio data integrity calls
      blockdev: convert some macros to static inlines
      fs: remove unneeded plug in mpage_readpages()
      block: Add BLKROTATIONAL ioctl
      block: Introduce blk_set_stacking_limits function
      block: remove WARN_ON_ONCE() in exit_io_context()
      block: an exiting task should be allowed to create io_context
      block: ioc_cgroup_changed() needs to be exported
      block: recursive merge requests
      block, cfq: fix empty queue crash caused by request merge
      block, cfq: move icq creation and rq->elv.icq association to block core
      block, cfq: restructure io_cq creation path for io_context interface cleanup
      block, cfq: move io_cq exit/release to blk-ioc.c
      block, cfq: move icq cache management to block core
      block, cfq: move io_cq lookup to blk-ioc.c
      block, cfq: move cfqd->icq_list to request_queue and add request->elv.icq
      block, cfq: reorganize cfq_io_context into generic and cfq specific parts
      block: remove elevator_queue->ops
      block: reorder elevator switch sequence
      ...
    
    Fix up conflicts in:
     - block/blk-cgroup.c
            Switch from can_attach_task to can_attach
     - block/cfq-iosched.c
            conflict with now removed cic index changes (we now use q->id instead)

commit 001a541ea9163ace5e8243ee0e907ad80a4c0ec2
Merge: 40ba587923ae bc31b86a5923
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 10 16:59:59 2012 -0800

    Merge branch 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux
    
    * 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux:
      writeback: move MIN_WRITEBACK_PAGES to fs-writeback.c
      writeback: balanced_rate cannot exceed write bandwidth
      writeback: do strict bdi dirty_exceeded
      writeback: avoid tiny dirty poll intervals
      writeback: max, min and target dirty pause time
      writeback: dirty ratelimit - think time compensation
      btrfs: fix dirtied pages accounting on sub-page writes
      writeback: fix dirtied pages accounting on redirty
      writeback: fix dirtied pages accounting on sub-page writes
      writeback: charge leaked page dirties to active tasks
      writeback: Include all dirty inodes in background writeback

commit 43d2b113241d6797b890318767e0af78e313414b
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Jan 10 15:08:09 2012 -0800

    tracepoint: add tracepoints for debugging oom_score_adj
    
    oom_score_adj is used for guarding processes from OOM-Killer.  One of
    problem is that it's inherited at fork().  When a daemon set oom_score_adj
    and make children, it's hard to know where the value is set.
    
    This patch adds some tracepoints useful for debugging. This patch adds
    3 trace points.
      - creating new task
      - renaming a task (exec)
      - set oom_score_adj
    
    To debug, users need to enable some trace pointer. Maybe filtering is useful as
    
    # EVENT=/sys/kernel/debug/tracing/events/task/
    # echo "oom_score_adj != 0" > $EVENT/task_newtask/filter
    # echo "oom_score_adj != 0" > $EVENT/task_rename/filter
    # echo 1 > $EVENT/enable
    # EVENT=/sys/kernel/debug/tracing/events/oom/
    # echo 1 > $EVENT/enable
    
    output will be like this.
    # grep oom /sys/kernel/debug/tracing/trace
    bash-7699  [007] d..3  5140.744510: oom_score_adj_update: pid=7699 comm=bash oom_score_adj=-1000
    bash-7699  [007] ...1  5151.818022: task_newtask: pid=7729 comm=bash clone_flags=1200011 oom_score_adj=-1000
    ls-7729  [003] ...2  5151.818504: task_rename: pid=7729 oldcomm=bash newcomm=ls oom_score_adj=-1000
    bash-7699  [002] ...1  5175.701468: task_newtask: pid=7730 comm=bash clone_flags=1200011 oom_score_adj=-1000
    grep-7730  [007] ...2  5175.701993: task_rename: pid=7730 oldcomm=bash newcomm=grep oom_score_adj=-1000
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b00711ce7c13..5e1391b5ade0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -76,6 +76,9 @@
 
 #include <trace/events/sched.h>
 
+#define CREATE_TRACE_POINTS
+#include <trace/events/task.h>
+
 /*
  * Protected counters by write_lock_irq(&tasklist_lock)
  */
@@ -1370,6 +1373,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (clone_flags & CLONE_THREAD)
 		threadgroup_change_end(current);
 	perf_event_fork(p);
+
+	trace_task_newtask(p, clone_flags);
+
 	return p;
 
 bad_fork_free_pid:

commit db0c2bf69aa095d4a6de7b1145f29fe9a7c0f6a3
Merge: ac69e0928054 0d19ea866562
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 9 12:59:24 2012 -0800

    Merge branch 'for-3.3' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    * 'for-3.3' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (21 commits)
      cgroup: fix to allow mounting a hierarchy by name
      cgroup: move assignement out of condition in cgroup_attach_proc()
      cgroup: Remove task_lock() from cgroup_post_fork()
      cgroup: add sparse annotation to cgroup_iter_start() and cgroup_iter_end()
      cgroup: mark cgroup_rmdir_waitq and cgroup_attach_proc() as static
      cgroup: only need to check oldcgrp==newgrp once
      cgroup: remove redundant get/put of task struct
      cgroup: remove redundant get/put of old css_set from migrate
      cgroup: Remove unnecessary task_lock before fetching css_set on migration
      cgroup: Drop task_lock(parent) on cgroup_fork()
      cgroups: remove redundant get/put of css_set from css_set_check_fetched()
      resource cgroups: remove bogus cast
      cgroup: kill subsys->can_attach_task(), pre_attach() and attach_task()
      cgroup, cpuset: don't use ss->pre_attach()
      cgroup: don't use subsys->can_attach_task() or ->attach_task()
      cgroup: introduce cgroup_taskset and use it in subsys->can_attach(), cancel_attach() and attach()
      cgroup: improve old cgroup handling in cgroup_attach_proc()
      cgroup: always lock threadgroup during migration
      threadgroup: extend threadgroup_lock() to cover exit and exec
      threadgroup: rename signal->threadgroup_fork_lock to ->group_rwsem
      ...
    
    Fix up conflict in kernel/cgroup.c due to commit e0197aae59e5: "cgroups:
    fix a css_set not found bug in cgroup_attach_proc" that already
    mentioned that the bug is fixed (differently) in Tejun's cgroup
    patchset. This one, in other words.

commit eb59c505f8a5906ad2e053d14fab50eb8574fd6f
Merge: 1619ed8f6095 c233523b3d39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 8 13:10:57 2012 -0800

    Merge branch 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    * 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (76 commits)
      PM / Hibernate: Implement compat_ioctl for /dev/snapshot
      PM / Freezer: fix return value of freezable_schedule_timeout_killable()
      PM / shmobile: Allow the A4R domain to be turned off at run time
      PM / input / touchscreen: Make st1232 use device PM QoS constraints
      PM / QoS: Introduce dev_pm_qos_add_ancestor_request()
      PM / shmobile: Remove the stay_on flag from SH7372's PM domains
      PM / shmobile: Don't include SH7372's INTCS in syscore suspend/resume
      PM / shmobile: Add support for the sh7372 A4S power domain / sleep mode
      PM: Drop generic_subsys_pm_ops
      PM / Sleep: Remove forward-only callbacks from AMBA bus type
      PM / Sleep: Remove forward-only callbacks from platform bus type
      PM: Run the driver callback directly if the subsystem one is not there
      PM / Sleep: Make pm_op() and pm_noirq_op() return callback pointers
      PM/Devfreq: Add Exynos4-bus device DVFS driver for Exynos4210/4212/4412.
      PM / Sleep: Merge internal functions in generic_ops.c
      PM / Sleep: Simplify generic system suspend callbacks
      PM / Hibernate: Remove deprecated hibernation snapshot ioctls
      PM / Sleep: Fix freezer failures due to racy usermodehelper_is_disabled()
      ARM: S3C64XX: Implement basic power domain support
      PM / shmobile: Use common always on power domain governor
      ...
    
    Fix up trivial conflict in fs/xfs/xfs_buf.c due to removal of unused
    XBT_FORCE_SLEEP bit

commit 83712358ba0a1497ce59a4f84ce4dd0f803fe6fc
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Sat Jun 11 19:25:42 2011 -0600

    writeback: dirty ratelimit - think time compensation
    
    Compensate the task's think time when computing the final pause time,
    so that ->dirty_ratelimit can be executed accurately.
    
            think time := time spend outside of balance_dirty_pages()
    
    In the rare case that the task slept longer than the 200ms period time
    (result in negative pause time), the sleep time will be compensated in
    the following periods, too, if it's less than 1 second.
    
    Accumulated errors are carefully avoided as long as the max pause area
    is not hitted.
    
    Pseudo code:
    
            period = pages_dirtied / task_ratelimit;
            think = jiffies - dirty_paused_when;
            pause = period - think;
    
    1) normal case: period > think
    
            pause = period - think
            dirty_paused_when = jiffies + pause
            nr_dirtied = 0
    
                                 period time
                  |===============================>|
                      think time      pause time
                  |===============>|==============>|
            ------|----------------|---------------|------------------------
            dirty_paused_when   jiffies
    
    2) no pause case: period <= think
    
            don't pause; reduce future pause time by:
            dirty_paused_when += period
            nr_dirtied = 0
    
                               period time
                  |===============================>|
                                      think time
                  |===================================================>|
            ------|--------------------------------+-------------------|----
            dirty_paused_when                                       jiffies
    
    Acked-by: Jan Kara <jack@suse.cz>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index da4a6a10d088..f8668cf6a32d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1296,6 +1296,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->nr_dirtied = 0;
 	p->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);
+	p->dirty_paused_when = 0;
 
 	/*
 	 * Ok, make it visible to the rest of the system.

commit 648616343cdbe904c585a6c12e323d3b3c72e46f
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Thu Dec 15 14:56:09 2011 +0100

    [S390] cputime: add sparse checking and cleanup
    
    Make cputime_t and cputime64_t nocast to enable sparse checking to
    detect incorrect use of cputime. Drop the cputime macros for simple
    scalar operations. The conversion macros are still needed.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index da4a6a10d088..b058c5820ecd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1023,8 +1023,8 @@ void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
  */
 static void posix_cpu_timers_init(struct task_struct *tsk)
 {
-	tsk->cputime_expires.prof_exp = cputime_zero;
-	tsk->cputime_expires.virt_exp = cputime_zero;
+	tsk->cputime_expires.prof_exp = 0;
+	tsk->cputime_expires.virt_exp = 0;
 	tsk->cputime_expires.sched_exp = 0;
 	INIT_LIST_HEAD(&tsk->cpu_timers[0]);
 	INIT_LIST_HEAD(&tsk->cpu_timers[1]);
@@ -1132,14 +1132,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	init_sigpending(&p->pending);
 
-	p->utime = cputime_zero;
-	p->stime = cputime_zero;
-	p->gtime = cputime_zero;
-	p->utimescaled = cputime_zero;
-	p->stimescaled = cputime_zero;
+	p->utime = p->stime = p->gtime = 0;
+	p->utimescaled = p->stimescaled = 0;
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING
-	p->prev_utime = cputime_zero;
-	p->prev_stime = cputime_zero;
+	p->prev_utime = p->prev_stime = 0;
 #endif
 #if defined(SPLIT_RSS_COUNTING)
 	memset(&p->rss_stat, 0, sizeof(p->rss_stat));

commit b2efa05265d62bc29f3a64400fad4b44340eedb8
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:39 2011 +0100

    block, cfq: unlink cfq_io_context's immediately
    
    cic is association between io_context and request_queue.  A cic is
    linked from both ioc and q and should be destroyed when either one
    goes away.  As ioc and q both have their own locks, locking becomes a
    bit complex - both orders work for removal from one but not from the
    other.
    
    Currently, cfq tries to circumvent this locking order issue with RCU.
    ioc->lock nests inside queue_lock but the radix tree and cic's are
    also protected by RCU allowing either side to walk their lists without
    grabbing lock.
    
    This rather unconventional use of RCU quickly devolves into extremely
    fragile convolution.  e.g. The following is from cfqd going away too
    soon after ioc and q exits raced.
    
     general protection fault: 0000 [#1] PREEMPT SMP
     CPU 2
     Modules linked in:
     [   88.503444]
     Pid: 599, comm: hexdump Not tainted 3.1.0-rc10-work+ #158 Bochs Bochs
     RIP: 0010:[<ffffffff81397628>]  [<ffffffff81397628>] cfq_exit_single_io_context+0x58/0xf0
     ...
     Call Trace:
      [<ffffffff81395a4a>] call_for_each_cic+0x5a/0x90
      [<ffffffff81395ab5>] cfq_exit_io_context+0x15/0x20
      [<ffffffff81389130>] exit_io_context+0x100/0x140
      [<ffffffff81098a29>] do_exit+0x579/0x850
      [<ffffffff81098d5b>] do_group_exit+0x5b/0xd0
      [<ffffffff81098de7>] sys_exit_group+0x17/0x20
      [<ffffffff81b02f2b>] system_call_fastpath+0x16/0x1b
    
    The only real hot path here is cic lookup during request
    initialization and avoiding extra locking requires very confined use
    of RCU.  This patch makes cic removal from both ioc and request_queue
    perform double-locking and unlink immediately.
    
    * From q side, the change is almost trivial as ioc->lock nests inside
      queue_lock.  It just needs to grab each ioc->lock as it walks
      cic_list and unlink it.
    
    * From ioc side, it's a bit more difficult because of inversed lock
      order.  ioc needs its lock to walk its cic_list but can't grab the
      matching queue_lock and needs to perform unlock-relock dancing.
    
      Unlinking is now wholly done from put_io_context() and fast path is
      optimized by using the queue_lock the caller already holds, which is
      by far the most common case.  If the ioc accessed multiple devices,
      it tries with trylock.  In unlikely cases of fast path failure, it
      falls back to full double-locking dance from workqueue.
    
    Double-locking isn't the prettiest thing in the world but it's *far*
    simpler and more understandable than RCU trick without adding any
    meaningful overhead.
    
    This still leaves a lot of now unnecessary RCU logics.  Future patches
    will trim them.
    
    -v2: Vivek pointed out that cic->q was being dereferenced after
         cic->release() was called.  Updated to use local variable @this_q
         instead.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5bcfc739bb7c..2753449f2038 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -887,7 +887,7 @@ static int copy_io(unsigned long clone_flags, struct task_struct *tsk)
 			return -ENOMEM;
 
 		new_ioc->ioprio = ioc->ioprio;
-		put_io_context(new_ioc);
+		put_io_context(new_ioc, NULL);
 	}
 #endif
 	return 0;

commit 6e736be7f282fff705db7c34a15313281b372a76
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Dec 14 00:33:38 2011 +0100

    block: make ioc get/put interface more conventional and fix race on alloction
    
    Ignoring copy_io() during fork, io_context can be allocated from two
    places - current_io_context() and set_task_ioprio().  The former is
    always called from local task while the latter can be called from
    different task.  The synchornization between them are peculiar and
    dubious.
    
    * current_io_context() doesn't grab task_lock() and assumes that if it
      saw %NULL ->io_context, it would stay that way until allocation and
      assignment is complete.  It has smp_wmb() between alloc/init and
      assignment.
    
    * set_task_ioprio() grabs task_lock() for assignment and does
      smp_read_barrier_depends() between "ioc = task->io_context" and "if
      (ioc)".  Unfortunately, this doesn't achieve anything - the latter
      is not a dependent load of the former.  ie, if ioc itself were being
      dereferenced "ioc->xxx", it would mean something (not sure what tho)
      but as the code currently stands, the dependent read barrier is
      noop.
    
    As only one of the the two test-assignment sequences is task_lock()
    protected, the task_lock() can't do much about race between the two.
    Nothing prevents current_io_context() and set_task_ioprio() allocating
    its own ioc for the same task and overwriting the other's.
    
    Also, set_task_ioprio() can race with exiting task and create a new
    ioc after exit_io_context() is finished.
    
    ioc get/put doesn't have any reason to be complex.  The only hot path
    is accessing the existing ioc of %current, which is simple to achieve
    given that ->io_context is never destroyed as long as the task is
    alive.  All other paths can happily go through task_lock() like all
    other task sub structures without impacting anything.
    
    This patch updates ioc get/put so that it becomes more conventional.
    
    * alloc_io_context() is replaced with get_task_io_context().  This is
      the only interface which can acquire access to ioc of another task.
      On return, the caller has an explicit reference to the object which
      should be put using put_io_context() afterwards.
    
    * The functionality of current_io_context() remains the same but when
      creating a new ioc, it shares the code path with
      get_task_io_context() and always goes through task_lock().
    
    * get_io_context() now means incrementing ref on an ioc which the
      caller already has access to (be that an explicit refcnt or implicit
      %current one).
    
    * PF_EXITING inhibits creation of new io_context and once
      exit_io_context() is finished, it's guaranteed that both ioc
      acquisition functions return %NULL.
    
    * All users are updated.  Most are trivial but
      smp_read_barrier_depends() removal from cfq_get_io_context() needs a
      bit of explanation.  I suppose the original intention was to ensure
      ioc->ioprio is visible when set_task_ioprio() allocates new
      io_context and installs it; however, this wouldn't have worked
      because set_task_ioprio() doesn't have wmb between init and install.
      There are other problems with this which will be fixed in another
      patch.
    
    * While at it, use NUMA_NO_NODE instead of -1 for wildcard node
      specification.
    
    -v2: Vivek spotted contamination from debug patch.  Removed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/kernel/fork.c b/kernel/fork.c
index da4a6a10d088..5bcfc739bb7c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -870,6 +870,7 @@ static int copy_io(unsigned long clone_flags, struct task_struct *tsk)
 {
 #ifdef CONFIG_BLOCK
 	struct io_context *ioc = current->io_context;
+	struct io_context *new_ioc;
 
 	if (!ioc)
 		return 0;
@@ -881,11 +882,12 @@ static int copy_io(unsigned long clone_flags, struct task_struct *tsk)
 		if (unlikely(!tsk->io_context))
 			return -ENOMEM;
 	} else if (ioprio_valid(ioc->ioprio)) {
-		tsk->io_context = alloc_io_context(GFP_KERNEL, -1);
-		if (unlikely(!tsk->io_context))
+		new_ioc = get_task_io_context(tsk, GFP_KERNEL, NUMA_NO_NODE);
+		if (unlikely(!new_ioc))
 			return -ENOMEM;
 
-		tsk->io_context->ioprio = ioc->ioprio;
+		new_ioc->ioprio = ioc->ioprio;
+		put_io_context(new_ioc);
 	}
 #endif
 	return 0;

commit 257058ae2b971646b96ab3a15605ac69186e562a
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Dec 12 18:12:21 2011 -0800

    threadgroup: rename signal->threadgroup_fork_lock to ->group_rwsem
    
    Make the following renames to prepare for extension of threadgroup
    locking.
    
    * s/signal->threadgroup_fork_lock/signal->group_rwsem/
    * s/threadgroup_fork_read_lock()/threadgroup_change_begin()/
    * s/threadgroup_fork_read_unlock()/threadgroup_change_end()/
    * s/threadgroup_fork_write_lock()/threadgroup_lock()/
    * s/threadgroup_fork_write_unlock()/threadgroup_unlock()/
    
    This patch doesn't cause any behavior change.
    
    -v2: Rename threadgroup_change_done() to threadgroup_change_end() per
         KAMEZAWA's suggestion.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Menage <paul@paulmenage.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 827808613847..d4ac9e3e0075 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -972,7 +972,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sched_autogroup_fork(sig);
 
 #ifdef CONFIG_CGROUPS
-	init_rwsem(&sig->threadgroup_fork_lock);
+	init_rwsem(&sig->group_rwsem);
 #endif
 
 	sig->oom_adj = current->signal->oom_adj;
@@ -1157,7 +1157,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->io_context = NULL;
 	p->audit_context = NULL;
 	if (clone_flags & CLONE_THREAD)
-		threadgroup_fork_read_lock(current);
+		threadgroup_change_begin(current);
 	cgroup_fork(p);
 #ifdef CONFIG_NUMA
 	p->mempolicy = mpol_dup(p->mempolicy);
@@ -1372,7 +1372,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
 	if (clone_flags & CLONE_THREAD)
-		threadgroup_fork_read_unlock(current);
+		threadgroup_change_end(current);
 	perf_event_fork(p);
 	return p;
 
@@ -1407,7 +1407,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_cgroup:
 #endif
 	if (clone_flags & CLONE_THREAD)
-		threadgroup_fork_read_unlock(current);
+		threadgroup_change_end(current);
 	cgroup_exit(p, cgroup_callbacks_done);
 	delayacct_tsk_free(p);
 	module_put(task_thread_info(p)->exec_domain->module);

commit 986b11c3ee9e0eace25fe74a502205f7fe8c179b
Merge: bb58dd5d1ffa 24b7ead3fb0b
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed Nov 23 21:09:02 2011 +0100

    Merge branch 'pm-freezer' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/misc into pm-freezer
    
    * 'pm-freezer' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/misc: (24 commits)
      freezer: fix wait_event_freezable/__thaw_task races
      freezer: kill unused set_freezable_with_signal()
      dmatest: don't use set_freezable_with_signal()
      usb_storage: don't use set_freezable_with_signal()
      freezer: remove unused @sig_only from freeze_task()
      freezer: use lock_task_sighand() in fake_signal_wake_up()
      freezer: restructure __refrigerator()
      freezer: fix set_freezable[_with_signal]() race
      freezer: remove should_send_signal() and update frozen()
      freezer: remove now unused TIF_FREEZE
      freezer: make freezing() test freeze conditions in effect instead of TIF_FREEZE
      cgroup_freezer: prepare for removal of TIF_FREEZE
      freezer: clean up freeze_processes() failure path
      freezer: kill PF_FREEZING
      freezer: test freezable conditions while holding freezer_lock
      freezer: make freezing indicate freeze condition in effect
      freezer: use dedicated lock instead of task_lock() + memory barrier
      freezer: don't distinguish nosig tasks on thaw
      freezer: remove racy clear_freeze_flag() and set PF_NOFREEZE on dead tasks
      freezer: rename thaw_process() to __thaw_task() and simplify the implementation
      ...

commit a3201227f803ad7fd43180c5195dbe5a2bf998aa
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Nov 21 12:32:25 2011 -0800

    freezer: make freezing() test freeze conditions in effect instead of TIF_FREEZE
    
    Using TIF_FREEZE for freezing worked when there was only single
    freezing condition (the PM one); however, now there is also the
    cgroup_freezer and single bit flag is getting clumsy.
    thaw_processes() is already testing whether cgroup freezing in in
    effect to avoid thawing tasks which were frozen by both PM and cgroup
    freezers.
    
    This is racy (nothing prevents race against cgroup freezing) and
    fragile.  A much simpler way is to test actual freeze conditions from
    freezing() - ie. directly test whether PM or cgroup freezing is in
    effect.
    
    This patch adds variables to indicate whether and what type of
    freezing conditions are in effect and reimplements freezing() such
    that it directly tests whether any of the two freezing conditions is
    active and the task should freeze.  On fast path, freezing() is still
    very cheap - it only tests system_freezing_cnt.
    
    This makes the clumsy dancing aroung TIF_FREEZE unnecessary and
    freeze/thaw operations more usual - updating state variables for the
    new state and nudging target tasks so that they notice the new state
    and comply.  As long as the nudging happens after state update, it's
    race-free.
    
    * This allows use of freezing() in freeze_task().  Replace the open
      coded tests with freezing().
    
    * p != current test is added to warning printing conditions in
      try_to_freeze_tasks() failure path.  This is necessary as freezing()
      is now true for the task which initiated freezing too.
    
    -v2: Oleg pointed out that re-freezing FROZEN cgroup could increment
         system_freezing_cnt.  Fixed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Paul Menage <paul@paulmenage.org>  (for the cgroup portions)

diff --git a/kernel/fork.c b/kernel/fork.c
index ba0d17261329..d53316e88d9d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -997,7 +997,6 @@ static void copy_flags(unsigned long clone_flags, struct task_struct *p)
 	new_flags |= PF_FORKNOEXEC;
 	new_flags |= PF_STARTING;
 	p->flags = new_flags;
-	clear_freeze_flag(p);
 }
 
 SYSCALL_DEFINE1(set_tid_address, int __user *, tidptr)

commit 468e6a20afaccb67e2a7d7f60d301f90e1c6f301
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Wed Sep 7 10:41:32 2011 -0600

    writeback: remove vm_dirties and task->dirties
    
    They are not used any more.
    
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index ba0d17261329..da4a6a10d088 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -162,7 +162,6 @@ static void account_kernel_stack(struct thread_info *ti, int account)
 
 void free_task(struct task_struct *tsk)
 {
-	prop_local_destroy_single(&tsk->dirties);
 	account_kernel_stack(tsk->stack, -1);
 	free_thread_info(tsk->stack);
 	rt_mutex_debug_task_free(tsk);
@@ -274,10 +273,6 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 
 	tsk->stack = ti;
 
-	err = prop_local_init_single(&tsk->dirties);
-	if (err)
-		goto out;
-
 	setup_thread_stack(tsk, orig);
 	clear_user_return_notifier(tsk);
 	clear_tsk_need_resched(tsk);

commit 208bca0860406d16398145ddd950036a737c3c9d
Merge: 6aad3738f6a7 0e175a1835ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 19:02:23 2011 -0800

    Merge branch 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux
    
    * 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux:
      writeback: Add a 'reason' to wb_writeback_work
      writeback: send work item to queue_io, move_expired_inodes
      writeback: trace event balance_dirty_pages
      writeback: trace event bdi_dirty_ratelimit
      writeback: fix ppc compile warnings on do_div(long long, unsigned long)
      writeback: per-bdi background threshold
      writeback: dirty position control - bdi reserve area
      writeback: control dirty pause time
      writeback: limit max dirty pause time
      writeback: IO-less balance_dirty_pages()
      writeback: per task dirty rate limit
      writeback: stabilize bdi->dirty_ratelimit
      writeback: dirty rate control
      writeback: add bg_threshold parameter to __bdi_update_bandwidth()
      writeback: dirty position control
      writeback: account per-bdi accumulated dirtied pages

commit c9f01245b6a7d77d17deaa71af10f6aca14fa24e
Author: David Rientjes <rientjes@google.com>
Date:   Mon Oct 31 17:07:15 2011 -0700

    oom: remove oom_disable_count
    
    This removes mm->oom_disable_count entirely since it's unnecessary and
    currently buggy.  The counter was intended to be per-process but it's
    currently decremented in the exit path for each thread that exits, causing
    it to underflow.
    
    The count was originally intended to prevent oom killing threads that
    share memory with threads that cannot be killed since it doesn't lead to
    future memory freeing.  The counter could be fixed to represent all
    threads sharing the same mm, but it's better to remove the count since:
    
     - it is possible that the OOM_DISABLE thread sharing memory with the
       victim is waiting on that thread to exit and will actually cause
       future memory freeing, and
    
     - there is no guarantee that a thread is disabled from oom killing just
       because another thread sharing its mm is oom disabled.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8e6b6f4fb272..70d76191afb9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -501,7 +501,6 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 	mm->cached_hole_size = ~0UL;
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
-	atomic_set(&mm->oom_disable_count, 0);
 
 	if (likely(!mm_alloc_pgd(mm))) {
 		mm->def_flags = 0;
@@ -816,8 +815,6 @@ static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 	/* Initializing for Swap token stuff */
 	mm->token_priority = 0;
 	mm->last_interval = 0;
-	if (tsk->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)
-		atomic_inc(&mm->oom_disable_count);
 
 	tsk->mm = mm;
 	tsk->active_mm = mm;
@@ -1391,13 +1388,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_namespaces:
 	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
-	if (p->mm) {
-		task_lock(p);
-		if (p->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)
-			atomic_dec(&p->mm->oom_disable_count);
-		task_unlock(p);
+	if (p->mm)
 		mmput(p->mm);
-	}
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
 		free_signal_struct(p->signal);

commit 9d823e8f6b1b7b39f952d7d1795f29162143a433
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Sat Jun 11 18:10:12 2011 -0600

    writeback: per task dirty rate limit
    
    Add two fields to task_struct.
    
    1) account dirtied pages in the individual tasks, for accuracy
    2) per-task balance_dirty_pages() call intervals, for flexibility
    
    The balance_dirty_pages() call interval (ie. nr_dirtied_pause) will
    scale near-sqrt to the safety gap between dirty pages and threshold.
    
    The main problem of per-task nr_dirtied is, if 1k+ tasks start dirtying
    pages at exactly the same time, each task will be assigned a large
    initial nr_dirtied_pause, so that the dirty threshold will be exceeded
    long before each task reached its nr_dirtied_pause and hence call
    balance_dirty_pages().
    
    The solution is to watch for the number of pages dirtied on each CPU in
    between the calls into balance_dirty_pages(). If it exceeds ratelimit_pages
    (3% dirty threshold), force call balance_dirty_pages() for a chance to
    set bdi->dirty_exceeded. In normal situations, this safeguarding
    condition is not expected to trigger at all.
    
    On the sqrt in dirty_poll_interval():
    
    It will serve as an initial guess when dirty pages are still in the
    freerun area.
    
    When dirty pages are floating inside the dirty control scope [freerun,
    limit], a followup patch will use some refined dirty poll interval to
    get the desired pause time.
    
       thresh-dirty (MB)    sqrt
                       1      16
                       2      22
                       4      32
                       8      45
                      16      64
                      32      90
                      64     128
                     128     181
                     256     256
                     512     362
                    1024     512
    
    The above table means, given 1MB (or 1GB) gap and the dd tasks polling
    balance_dirty_pages() on every 16 (or 512) pages, the dirty limit won't
    be exceeded as long as there are less than 16 (or 512) concurrent dd's.
    
    So sqrt naturally leads to less overheads and more safe concurrent tasks
    for large memory servers, which have large (thresh-freerun) gaps.
    
    peter: keep the per-CPU ratelimit for safeguarding the 1k+ tasks case
    
    CC: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Andrea Righi <andrea@betterlinux.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8e6b6f4fb272..cc0815df99f2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1302,6 +1302,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->pdeath_signal = 0;
 	p->exit_state = 0;
 
+	p->nr_dirtied = 0;
+	p->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);
+
 	/*
 	 * Ok, make it visible to the rest of the system.
 	 * We dont wake it up yet.

commit 72fa59970f8698023045ab0713d66f3f4f96945c
Author: Vasiliy Kulikov <segoon@openwall.com>
Date:   Mon Aug 8 19:02:04 2011 +0400

    move RLIMIT_NPROC check from set_user() to do_execve_common()
    
    The patch http://lkml.org/lkml/2003/7/13/226 introduced an RLIMIT_NPROC
    check in set_user() to check for NPROC exceeding via setuid() and
    similar functions.
    
    Before the check there was a possibility to greatly exceed the allowed
    number of processes by an unprivileged user if the program relied on
    rlimit only.  But the check created new security threat: many poorly
    written programs simply don't check setuid() return code and believe it
    cannot fail if executed with root privileges.  So, the check is removed
    in this patch because of too often privilege escalations related to
    buggy programs.
    
    The NPROC can still be enforced in the common code flow of daemons
    spawning user processes.  Most of daemons do fork()+setuid()+execve().
    The check introduced in execve() (1) enforces the same limit as in
    setuid() and (2) doesn't create similar security issues.
    
    Neil Brown suggested to track what specific process has exceeded the
    limit by setting PF_NPROC_EXCEEDED process flag.  With the change only
    this process would fail on execve(), and other processes' execve()
    behaviour is not changed.
    
    Solar Designer suggested to re-check whether NPROC limit is still
    exceeded at the moment of execve().  If the process was sleeping for
    days between set*uid() and execve(), and the NPROC counter step down
    under the limit, the defered execve() failure because NPROC limit was
    exceeded days ago would be unexpected.  If the limit is not exceeded
    anymore, we clear the flag on successful calls to execve() and fork().
    
    The flag is also cleared on successful calls to set_user() as the limit
    was exceeded for the previous user, not the current one.
    
    Similar check was introduced in -ow patches (without the process flag).
    
    v3 - clear PF_NPROC_EXCEEDED on successful calls to set_user().
    
    Reviewed-by: James Morris <jmorris@namei.org>
    Signed-off-by: Vasiliy Kulikov <segoon@openwall.com>
    Acked-by: NeilBrown <neilb@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e7ceaca89609..8e6b6f4fb272 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1111,6 +1111,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		    p->real_cred->user != INIT_USER)
 			goto bad_fork_free;
 	}
+	current->flags &= ~PF_NPROC_EXCEEDED;
 
 	retval = copy_creds(p, clone_flags);
 	if (retval < 0)

commit fb0a685cb95a0267a96153af2f72486f27be5847
Author: Daniel Rebelo de Oliveira <psykon@gmail.com>
Date:   Tue Jul 26 16:08:39 2011 -0700

    kernel/fork.c: fix a few coding style issues
    
    Signed-off-by: Daniel Rebelo de Oliveira <psykon@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e33177edb3bf..e7ceaca89609 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -80,7 +80,7 @@
  * Protected counters by write_lock_irq(&tasklist_lock)
  */
 unsigned long total_forks;	/* Handle normal Linux uptimes. */
-int nr_threads; 		/* The idle threads do not count.. */
+int nr_threads;			/* The idle threads do not count.. */
 
 int max_threads;		/* tunable limit on nr_threads */
 
@@ -232,7 +232,7 @@ void __init fork_init(unsigned long mempages)
 	/*
 	 * we need to allow at least 20 threads to boot a system
 	 */
-	if(max_threads < 20)
+	if (max_threads < 20)
 		max_threads = 20;
 
 	init_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads/2;
@@ -268,7 +268,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 		return NULL;
 	}
 
- 	err = arch_dup_task_struct(tsk, orig);
+	err = arch_dup_task_struct(tsk, orig);
 	if (err)
 		goto out;
 
@@ -288,8 +288,11 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	tsk->stack_canary = get_random_int();
 #endif
 
-	/* One for us, one for whoever does the "release_task()" (usually parent) */
-	atomic_set(&tsk->usage,2);
+	/*
+	 * One for us, one for whoever does the "release_task()" (usually
+	 * parent)
+	 */
+	atomic_set(&tsk->usage, 2);
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	tsk->btrace_seq = 0;
 #endif
@@ -437,7 +440,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	goto out;
 }
 
-static inline int mm_alloc_pgd(struct mm_struct * mm)
+static inline int mm_alloc_pgd(struct mm_struct *mm)
 {
 	mm->pgd = pgd_alloc(mm);
 	if (unlikely(!mm->pgd))
@@ -445,7 +448,7 @@ static inline int mm_alloc_pgd(struct mm_struct * mm)
 	return 0;
 }
 
-static inline void mm_free_pgd(struct mm_struct * mm)
+static inline void mm_free_pgd(struct mm_struct *mm)
 {
 	pgd_free(mm, mm->pgd);
 }
@@ -482,7 +485,7 @@ static void mm_init_aio(struct mm_struct *mm)
 #endif
 }
 
-static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
+static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)
 {
 	atomic_set(&mm->mm_users, 1);
 	atomic_set(&mm->mm_count, 1);
@@ -513,9 +516,9 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 /*
  * Allocate and initialize an mm_struct.
  */
-struct mm_struct * mm_alloc(void)
+struct mm_struct *mm_alloc(void)
 {
-	struct mm_struct * mm;
+	struct mm_struct *mm;
 
 	mm = allocate_mm();
 	if (!mm)
@@ -583,7 +586,7 @@ void added_exe_file_vma(struct mm_struct *mm)
 void removed_exe_file_vma(struct mm_struct *mm)
 {
 	mm->num_exe_file_vmas--;
-	if ((mm->num_exe_file_vmas == 0) && mm->exe_file){
+	if ((mm->num_exe_file_vmas == 0) && mm->exe_file) {
 		fput(mm->exe_file);
 		mm->exe_file = NULL;
 	}
@@ -775,9 +778,9 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	return NULL;
 }
 
-static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
+static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
 {
-	struct mm_struct * mm, *oldmm;
+	struct mm_struct *mm, *oldmm;
 	int retval;
 
 	tsk->min_flt = tsk->maj_flt = 0;
@@ -844,7 +847,7 @@ static int copy_fs(unsigned long clone_flags, struct task_struct *tsk)
 	return 0;
 }
 
-static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
+static int copy_files(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct files_struct *oldf, *newf;
 	int error = 0;
@@ -1166,11 +1169,11 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	cgroup_fork(p);
 #ifdef CONFIG_NUMA
 	p->mempolicy = mpol_dup(p->mempolicy);
- 	if (IS_ERR(p->mempolicy)) {
- 		retval = PTR_ERR(p->mempolicy);
- 		p->mempolicy = NULL;
- 		goto bad_fork_cleanup_cgroup;
- 	}
+	if (IS_ERR(p->mempolicy)) {
+		retval = PTR_ERR(p->mempolicy);
+		p->mempolicy = NULL;
+		goto bad_fork_cleanup_cgroup;
+	}
 	mpol_fix_fork_child_flag(p);
 #endif
 #ifdef CONFIG_CPUSETS
@@ -1216,25 +1219,33 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	retval = perf_event_init_task(p);
 	if (retval)
 		goto bad_fork_cleanup_policy;
-
-	if ((retval = audit_alloc(p)))
+	retval = audit_alloc(p);
+	if (retval)
 		goto bad_fork_cleanup_policy;
 	/* copy all the process information */
-	if ((retval = copy_semundo(clone_flags, p)))
+	retval = copy_semundo(clone_flags, p);
+	if (retval)
 		goto bad_fork_cleanup_audit;
-	if ((retval = copy_files(clone_flags, p)))
+	retval = copy_files(clone_flags, p);
+	if (retval)
 		goto bad_fork_cleanup_semundo;
-	if ((retval = copy_fs(clone_flags, p)))
+	retval = copy_fs(clone_flags, p);
+	if (retval)
 		goto bad_fork_cleanup_files;
-	if ((retval = copy_sighand(clone_flags, p)))
+	retval = copy_sighand(clone_flags, p);
+	if (retval)
 		goto bad_fork_cleanup_fs;
-	if ((retval = copy_signal(clone_flags, p)))
+	retval = copy_signal(clone_flags, p);
+	if (retval)
 		goto bad_fork_cleanup_sighand;
-	if ((retval = copy_mm(clone_flags, p)))
+	retval = copy_mm(clone_flags, p);
+	if (retval)
 		goto bad_fork_cleanup_signal;
-	if ((retval = copy_namespaces(clone_flags, p)))
+	retval = copy_namespaces(clone_flags, p);
+	if (retval)
 		goto bad_fork_cleanup_mm;
-	if ((retval = copy_io(clone_flags, p)))
+	retval = copy_io(clone_flags, p);
+	if (retval)
 		goto bad_fork_cleanup_namespaces;
 	retval = copy_thread(clone_flags, stack_start, stack_size, p, regs);
 	if (retval)
@@ -1256,7 +1267,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	/*
 	 * Clear TID on mm_release()?
 	 */
-	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;
+	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr : NULL;
 #ifdef CONFIG_BLOCK
 	p->plug = NULL;
 #endif
@@ -1324,7 +1335,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * it's process group.
 	 * A fatal signal pending means that current will exit, so the new
 	 * thread can't slip out of an OOM kill (or normal SIGKILL).
- 	 */
+	*/
 	recalc_sigpending();
 	if (signal_pending(current)) {
 		spin_unlock(&current->sighand->siglock);
@@ -1685,12 +1696,14 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 	 */
 	if (unshare_flags & (CLONE_NEWIPC|CLONE_SYSVSEM))
 		do_sysvsem = 1;
-	if ((err = unshare_fs(unshare_flags, &new_fs)))
+	err = unshare_fs(unshare_flags, &new_fs);
+	if (err)
 		goto bad_unshare_out;
-	if ((err = unshare_fd(unshare_flags, &new_fd)))
+	err = unshare_fd(unshare_flags, &new_fd);
+	if (err)
 		goto bad_unshare_cleanup_fs;
-	if ((err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,
-			new_fs)))
+	err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy, new_fs);
+	if (err)
 		goto bad_unshare_cleanup_fd;
 
 	if (new_fs || new_fd || do_sysvsem || new_nsproxy) {

commit 778d3b0ff0654ad7092bf823fd32010066b12365
Author: Michal Hocko <mhocko@suse.cz>
Date:   Tue Jul 26 16:08:30 2011 -0700

    cpusets: randomize node rotor used in cpuset_mem_spread_node()
    
    [ This patch has already been accepted as commit 0ac0c0d0f837 but later
      reverted (commit 35926ff5fba8) because it itroduced arch specific
      __node_random which was defined only for x86 code so it broke other
      archs.  This is a followup without any arch specific code.  Other than
      that there are no functional changes.]
    
    Some workloads that create a large number of small files tend to assign
    too many pages to node 0 (multi-node systems).  Part of the reason is
    that the rotor (in cpuset_mem_spread_node()) used to assign nodes starts
    at node 0 for newly created tasks.
    
    This patch changes the rotor to be initialized to a random node number
    of the cpuset.
    
    [akpm@linux-foundation.org: fix layout]
    [Lee.Schermerhorn@hp.com: Define stub numa_random() for !NUMA configuration]
    [mhocko@suse.cz: Make it arch independent]
    [akpm@linux-foundation.org: fix CONFIG_NUMA=y, MAX_NUMNODES>1 build]
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Paul Menage <menage@google.com>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: Robin Holt <holt@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Paul Menage <menage@google.com>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Robin Holt <holt@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 17bf7c8d6511..e33177edb3bf 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1173,6 +1173,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
  	}
 	mpol_fix_fork_child_flag(p);
 #endif
+#ifdef CONFIG_CPUSETS
+	p->cpuset_mem_spread_rotor = NUMA_NO_NODE;
+	p->cpuset_slab_spread_rotor = NUMA_NO_NODE;
+#endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	p->irq_events = 0;
 #ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW

commit 096a705bbc080a4041636d07514560da8d78acbe
Merge: fea80311a939 5757a6d76cdf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 10:33:36 2011 -0700

    Merge branch 'for-3.1/core' of git://git.kernel.dk/linux-block
    
    * 'for-3.1/core' of git://git.kernel.dk/linux-block: (24 commits)
      block: strict rq_affinity
      backing-dev: use synchronize_rcu_expedited instead of synchronize_rcu
      block: fix patch import error in max_discard_sectors check
      block: reorder request_queue to remove 64 bit alignment padding
      CFQ: add think time check for group
      CFQ: add think time check for service tree
      CFQ: move think time check variables to a separate struct
      fixlet: Remove fs_excl from struct task.
      cfq: Remove special treatment for metadata rqs.
      block: document blk_plug list access
      block: avoid building too big plug list
      compat_ioctl: fix make headers_check regression
      block: eliminate potential for infinite loop in blkdev_issue_discard
      compat_ioctl: fix warning caused by qemu
      block: flush MEDIA_CHANGE from drivers on close(2)
      blk-throttle: Make total_nr_queued unsigned
      block: Add __attribute__((format(printf...) and fix fallout
      fs/partitions/check.c: make local symbols static
      block:remove some spare spaces in genhd.c
      block:fix the comment error in blkdev.h
      ...

commit bbd9d6f7fbb0305c9a592bf05a32e87eb364a4ff
Merge: 8e204874db00 5a9a43646cf7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 22 19:02:39 2011 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs-2.6: (107 commits)
      vfs: use ERR_CAST for err-ptr tossing in lookup_instantiate_filp
      isofs: Remove global fs lock
      jffs2: fix IN_DELETE_SELF on overwriting rename() killing a directory
      fix IN_DELETE_SELF on overwriting rename() on ramfs et.al.
      mm/truncate.c: fix build for CONFIG_BLOCK not enabled
      fs:update the NOTE of the file_operations structure
      Remove dead code in dget_parent()
      AFS: Fix silly characters in a comment
      switch d_add_ci() to d_splice_alias() in "found negative" case as well
      simplify gfs2_lookup()
      jfs_lookup(): don't bother with . or ..
      get rid of useless dget_parent() in btrfs rename() and link()
      get rid of useless dget_parent() in fs/btrfs/ioctl.c
      fs: push i_mutex and filemap_write_and_wait down into ->fsync() handlers
      drivers: fix up various ->llseek() implementations
      fs: handle SEEK_HOLE/SEEK_DATA properly in all fs's that define their own llseek
      Ext4: handle SEEK_HOLE/SEEK_DATA generically
      Btrfs: implement our own ->llseek
      fs: add SEEK_HOLE and SEEK_DATA flags
      reiserfs: make reiserfs default to barrier=flush
      ...
    
    Fix up trivial conflicts in fs/xfs/linux-2.6/xfs_super.c due to the new
    shrinker callout for the inode cache, that clashed with the xfs code to
    start the periodic workers later.

commit 75b56ec294b074d70f8a676ab02611a3fea76cab
Merge: 6d16d6d9bb6f efbe2eee6dc0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 22 16:43:21 2011 -0700

    Merge branch 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-locking-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      lockdep: Fix lockdep_no_validate against IRQ states
      mutex: Make mutex_destroy() an inline function
      plist: Remove the need to supply locks to plist heads
      lockup detector: Fix reference to the non-existent CONFIG_DETECT_SOFTLOCKUP option

commit 6657719390cd05be45f4e3b501d8bb46889c0a19
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Jun 28 15:41:10 2011 -0400

    make sure that nsproxy_cache is initialized early enough
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0276c30401a0..31fa13e63b70 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1574,6 +1574,7 @@ void __init proc_caches_init(void)
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
 	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC);
 	mmap_init();
+	nsproxy_cache_init();
 }
 
 /*

commit dcace06cc29df927a74a6bc0e57b9bef87704377
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jul 8 19:13:54 2011 +0200

    ptrace: mv send-SIGSTOP from do_fork() to ptrace_init_task()
    
    If the new child is traced, do_fork() adds the pending SIGSTOP.
    It assumes that either it is traced because of auto-attach or the
    tracer attached later, in both cases sigaddset/set_thread_flag is
    correct even if SIGSTOP is already pending.
    
    Now that we have PTRACE_SEIZE this is no longer right in the latter
    case. If the tracer does PTRACE_SEIZE after copy_process() makes the
    child visible the queued SIGSTOP is wrong.
    
    We could check PT_SEIZED bit and change ptrace_attach() to set both
    PT_PTRACED and PT_SEIZED bits simultaneously but see the next patch,
    we need to know whether this child was auto-attached or not anyway.
    
    So this patch simply moves this code to ptrace_init_task(), this
    way we can never race with ptrace_attach().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3c72a5b321a7..4d4117e01504 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -37,7 +37,6 @@
 #include <linux/swap.h>
 #include <linux/syscalls.h>
 #include <linux/jiffies.h>
-#include <linux/tracehook.h>
 #include <linux/futex.h>
 #include <linux/compat.h>
 #include <linux/kthread.h>
@@ -1521,17 +1520,6 @@ long do_fork(unsigned long clone_flags,
 
 		audit_finish_fork(p);
 
-		/*
-		 * Child is ready but hasn't started running yet.  Queue
-		 * SIGSTOP if it's gonna be ptraced - it doesn't matter who
-		 * attached/attaching to this task, the pending SIGSTOP is
-		 * right in any case.
-		 */
-		if (unlikely(p->ptrace)) {
-			sigaddset(&p->pending.signal, SIGSTOP);
-			set_tsk_thread_flag(p, TIF_SIGPENDING);
-		}
-
 		/*
 		 * We set PF_STARTING at creation in case tracing wants to
 		 * use this to distinguish a fully live task from one that

commit 4aede84b33d6beb401136a3deca0651ae07c5e99
Author: Justin TerAvest <teravest@google.com>
Date:   Tue Jul 12 08:31:45 2011 +0200

    fixlet: Remove fs_excl from struct task.
    
    fs_excl is a poor man's priority inheritance for filesystems to hint to
    the block layer that an operation is important. It was never clearly
    specified, not widely adopted, and will not prevent starvation in many
    cases (like across cgroups).
    
    fs_excl was introduced with the time sliced CFQ IO scheduler, to
    indicate when a process held FS exclusive resources and thus needed
    a boost.
    
    It doesn't cover all file systems, and it was never fully complete.
    Lets kill it.
    
    Signed-off-by: Justin TerAvest <teravest@google.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0276c30401a0..30a0e8607223 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -291,7 +291,6 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 
 	/* One for us, one for whoever does the "release_task()" (usually parent) */
 	atomic_set(&tsk->usage,2);
-	atomic_set(&tsk->fs_excl, 0);
 #ifdef CONFIG_BLK_DEV_IO_TRACE
 	tsk->btrace_seq = 0;
 #endif

commit 732375c6a5a4cc825b676c922d547aba96b8ce15
Author: Dima Zavin <dima@android.com>
Date:   Thu Jul 7 17:27:59 2011 -0700

    plist: Remove the need to supply locks to plist heads
    
    This was legacy code brought over from the RT tree and
    is no longer necessary.
    
    Signed-off-by: Dima Zavin <dima@android.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Daniel Walker <dwalker@codeaurora.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Link: http://lkml.kernel.org/r/1310084879-10351-2-git-send-email-dima@android.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0276c30401a0..7517a53d50e3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1013,7 +1013,7 @@ static void rt_mutex_init_task(struct task_struct *p)
 {
 	raw_spin_lock_init(&p->pi_lock);
 #ifdef CONFIG_RT_MUTEXES
-	plist_head_init_raw(&p->pi_waiters, &p->pi_lock);
+	plist_head_init(&p->pi_waiters);
 	p->pi_blocked_on = NULL;
 #endif
 }

commit 4b9d33e6d83cc05a8005a8f9a8b9677fa0f53626
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jun 17 16:50:38 2011 +0200

    ptrace: kill clone/exec tracehooks
    
    At this point, tracehooks aren't useful to mainline kernel and mostly
    just add an extra layer of obfuscation.  Although they have comments,
    without actual in-kernel users, it is difficult to tell what are their
    assumptions and they're actually trying to achieve.  To mainline
    kernel, they just aren't worth keeping around.
    
    This patch kills the following clone and exec related tracehooks.
    
            tracehook_prepare_clone()
            tracehook_finish_clone()
            tracehook_report_clone()
            tracehook_report_clone_complete()
            tracehook_unsafe_exec()
    
    The changes are mostly trivial - logic is moved to the caller and
    comments are merged and adjusted appropriately.
    
    The only exception is in check_unsafe_exec() where LSM_UNSAFE_PTRACE*
    are OR'd to bprm->unsafe instead of setting it, which produces the
    same result as the field is always zero on entry.  It also tests
    p->ptrace instead of (p->ptrace & PT_PTRACED) for consistency, which
    also gives the same result.
    
    This doesn't introduce any behavior change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index d4f0dff9d617..3c72a5b321a7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1340,7 +1340,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	}
 
 	if (likely(p->pid)) {
-		tracehook_finish_clone(p, clone_flags, trace);
+		ptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);
 
 		if (thread_group_leader(p)) {
 			if (is_child_reaper(pid))
@@ -1481,10 +1481,22 @@ long do_fork(unsigned long clone_flags,
 	}
 
 	/*
-	 * When called from kernel_thread, don't do user tracing stuff.
+	 * Determine whether and which event to report to ptracer.  When
+	 * called from kernel_thread or CLONE_UNTRACED is explicitly
+	 * requested, no event is reported; otherwise, report if the event
+	 * for the type of forking is enabled.
 	 */
-	if (likely(user_mode(regs)))
-		trace = tracehook_prepare_clone(clone_flags);
+	if (likely(user_mode(regs)) && !(clone_flags & CLONE_UNTRACED)) {
+		if (clone_flags & CLONE_VFORK)
+			trace = PTRACE_EVENT_VFORK;
+		else if ((clone_flags & CSIGNAL) != SIGCHLD)
+			trace = PTRACE_EVENT_CLONE;
+		else
+			trace = PTRACE_EVENT_FORK;
+
+		if (likely(!ptrace_event_enabled(current, trace)))
+			trace = 0;
+	}
 
 	p = copy_process(clone_flags, stack_start, regs, stack_size,
 			 child_tidptr, NULL, trace);
@@ -1508,20 +1520,31 @@ long do_fork(unsigned long clone_flags,
 		}
 
 		audit_finish_fork(p);
-		tracehook_report_clone(regs, clone_flags, nr, p);
+
+		/*
+		 * Child is ready but hasn't started running yet.  Queue
+		 * SIGSTOP if it's gonna be ptraced - it doesn't matter who
+		 * attached/attaching to this task, the pending SIGSTOP is
+		 * right in any case.
+		 */
+		if (unlikely(p->ptrace)) {
+			sigaddset(&p->pending.signal, SIGSTOP);
+			set_tsk_thread_flag(p, TIF_SIGPENDING);
+		}
 
 		/*
 		 * We set PF_STARTING at creation in case tracing wants to
 		 * use this to distinguish a fully live task from one that
-		 * hasn't gotten to tracehook_report_clone() yet.  Now we
-		 * clear it and set the child going.
+		 * hasn't finished SIGSTOP raising yet.  Now we clear it
+		 * and set the child going.
 		 */
 		p->flags &= ~PF_STARTING;
 
 		wake_up_new_task(p);
 
-		tracehook_report_clone_complete(trace, regs,
-						clone_flags, nr, p);
+		/* forking complete and child started to run, tell ptracer */
+		if (unlikely(trace))
+			ptrace_event(trace, nr);
 
 		if (clone_flags & CLONE_VFORK) {
 			freezer_do_not_count();

commit a288eecce5253cc1565d400a52b9b476a157e040
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jun 17 16:50:37 2011 +0200

    ptrace: kill trivial tracehooks
    
    At this point, tracehooks aren't useful to mainline kernel and mostly
    just add an extra layer of obfuscation.  Although they have comments,
    without actual in-kernel users, it is difficult to tell what are their
    assumptions and they're actually trying to achieve.  To mainline
    kernel, they just aren't worth keeping around.
    
    This patch kills the following trivial tracehooks.
    
    * Ones testing whether task is ptraced.  Replace with ->ptrace test.
    
            tracehook_expect_breakpoints()
            tracehook_consider_ignored_signal()
            tracehook_consider_fatal_signal()
    
    * ptrace_event() wrappers.  Call directly.
    
            tracehook_report_exec()
            tracehook_report_exit()
            tracehook_report_vfork_done()
    
    * ptrace_release_task() wrapper.  Call directly.
    
            tracehook_finish_release_task()
    
    * noop
    
            tracehook_prepare_release_task()
            tracehook_report_death()
    
    This doesn't introduce any behavior change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0276c30401a0..d4f0dff9d617 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1527,7 +1527,7 @@ long do_fork(unsigned long clone_flags,
 			freezer_do_not_count();
 			wait_for_completion(&vfork);
 			freezer_count();
-			tracehook_report_vfork_done(p, nr);
+			ptrace_event(PTRACE_EVENT_VFORK_DONE, nr);
 		}
 	} else {
 		nr = PTR_ERR(p);

commit 6345d24daf0c1fffe6642081d783cdf653ebaa5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 29 11:32:28 2011 -0700

    mm: Fix boot crash in mm_alloc()
    
    Thomas Gleixner reports that we now have a boot crash triggered by
    CONFIG_CPUMASK_OFFSTACK=y:
    
        BUG: unable to handle kernel NULL pointer dereference at   (null)
        IP: [<c11ae035>] find_next_bit+0x55/0xb0
        Call Trace:
         [<c11addda>] cpumask_any_but+0x2a/0x70
         [<c102396b>] flush_tlb_mm+0x2b/0x80
         [<c1022705>] pud_populate+0x35/0x50
         [<c10227ba>] pgd_alloc+0x9a/0xf0
         [<c103a3fc>] mm_init+0xec/0x120
         [<c103a7a3>] mm_alloc+0x53/0xd0
    
    which was introduced by commit de03c72cfce5 ("mm: convert
    mm->cpu_vm_cpumask into cpumask_var_t"), and is due to wrong ordering of
    mm_init() vs mm_init_cpumask
    
    Thomas wrote a patch to just fix the ordering of initialization, but I
    hate the new double allocation in the fork path, so I ended up instead
    doing some more radical surgery to clean it all up.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ca406d916713..0276c30401a0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -484,20 +484,6 @@ static void mm_init_aio(struct mm_struct *mm)
 #endif
 }
 
-int mm_init_cpumask(struct mm_struct *mm, struct mm_struct *oldmm)
-{
-#ifdef CONFIG_CPUMASK_OFFSTACK
-	if (!alloc_cpumask_var(&mm->cpu_vm_mask_var, GFP_KERNEL))
-		return -ENOMEM;
-
-	if (oldmm)
-		cpumask_copy(mm_cpumask(mm), mm_cpumask(oldmm));
-	else
-		memset(mm_cpumask(mm), 0, cpumask_size());
-#endif
-	return 0;
-}
-
 static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 {
 	atomic_set(&mm->mm_users, 1);
@@ -538,17 +524,8 @@ struct mm_struct * mm_alloc(void)
 		return NULL;
 
 	memset(mm, 0, sizeof(*mm));
-	mm = mm_init(mm, current);
-	if (!mm)
-		return NULL;
-
-	if (mm_init_cpumask(mm, NULL)) {
-		mm_free_pgd(mm);
-		free_mm(mm);
-		return NULL;
-	}
-
-	return mm;
+	mm_init_cpumask(mm);
+	return mm_init(mm, current);
 }
 
 /*
@@ -559,7 +536,6 @@ struct mm_struct * mm_alloc(void)
 void __mmdrop(struct mm_struct *mm)
 {
 	BUG_ON(mm == &init_mm);
-	free_cpumask_var(mm->cpu_vm_mask_var);
 	mm_free_pgd(mm);
 	destroy_context(mm);
 	mmu_notifier_mm_destroy(mm);
@@ -753,6 +729,7 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 		goto fail_nomem;
 
 	memcpy(mm, oldmm, sizeof(*mm));
+	mm_init_cpumask(mm);
 
 	/* Initializing for Swap token stuff */
 	mm->token_priority = 0;
@@ -765,9 +742,6 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	if (!mm_init(mm, tsk))
 		goto fail_nomem;
 
-	if (mm_init_cpumask(mm, oldmm))
-		goto fail_nocpumask;
-
 	if (init_new_context(tsk, mm))
 		goto fail_nocontext;
 
@@ -794,9 +768,6 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	return NULL;
 
 fail_nocontext:
-	free_cpumask_var(mm->cpu_vm_mask_var);
-
-fail_nocpumask:
 	/*
 	 * If init_new_context() failed, we cannot use mmput() to free the mm
 	 * because it calls destroy_context()
@@ -1591,6 +1562,13 @@ void __init proc_caches_init(void)
 	fs_cachep = kmem_cache_create("fs_cache",
 			sizeof(struct fs_struct), 0,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
+	/*
+	 * FIXME! The "sizeof(struct mm_struct)" currently includes the
+	 * whole struct cpumask for the OFFSTACK case. We could change
+	 * this to *only* allocate as much of it as required by the
+	 * maximum number of CPU's we can ever have.  The cpumask_allocation
+	 * is at the end of the structure, exactly for that reason.
+	 */
 	mm_cachep = kmem_cache_create("mm_struct",
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);

commit 3864601387cf4196371e3c1897fdffa5228296f9
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Thu May 26 16:25:46 2011 -0700

    mm: extract exe_file handling from procfs
    
    Setup and cleanup of mm_struct->exe_file is currently done in fs/proc/.
    This was because exe_file was needed only for /proc/<pid>/exe.  Since we
    will need the exe_file functionality also for core dumps (so core name can
    contain full binary path), built this functionality always into the
    kernel.
    
    To achieve that move that out of proc FS to the kernel/ where in fact it
    should belong.  By doing that we can make dup_mm_exe_file static.  Also we
    can drop linux/proc_fs.h inclusion in fs/exec.c and kernel/fork.c.
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1f84099ecce6..ca406d916713 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -59,7 +59,6 @@
 #include <linux/taskstats_kern.h>
 #include <linux/random.h>
 #include <linux/tty.h>
-#include <linux/proc_fs.h>
 #include <linux/blkdev.h>
 #include <linux/fs_struct.h>
 #include <linux/magic.h>
@@ -597,6 +596,57 @@ void mmput(struct mm_struct *mm)
 }
 EXPORT_SYMBOL_GPL(mmput);
 
+/*
+ * We added or removed a vma mapping the executable. The vmas are only mapped
+ * during exec and are not mapped with the mmap system call.
+ * Callers must hold down_write() on the mm's mmap_sem for these
+ */
+void added_exe_file_vma(struct mm_struct *mm)
+{
+	mm->num_exe_file_vmas++;
+}
+
+void removed_exe_file_vma(struct mm_struct *mm)
+{
+	mm->num_exe_file_vmas--;
+	if ((mm->num_exe_file_vmas == 0) && mm->exe_file){
+		fput(mm->exe_file);
+		mm->exe_file = NULL;
+	}
+
+}
+
+void set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file)
+{
+	if (new_exe_file)
+		get_file(new_exe_file);
+	if (mm->exe_file)
+		fput(mm->exe_file);
+	mm->exe_file = new_exe_file;
+	mm->num_exe_file_vmas = 0;
+}
+
+struct file *get_mm_exe_file(struct mm_struct *mm)
+{
+	struct file *exe_file;
+
+	/* We need mmap_sem to protect against races with removal of
+	 * VM_EXECUTABLE vmas */
+	down_read(&mm->mmap_sem);
+	exe_file = mm->exe_file;
+	if (exe_file)
+		get_file(exe_file);
+	up_read(&mm->mmap_sem);
+	return exe_file;
+}
+
+static void dup_mm_exe_file(struct mm_struct *oldmm, struct mm_struct *newmm)
+{
+	/* It's safe to write the exe_file pointer without exe_file_lock because
+	 * this is called during fork when the task is not yet in /proc */
+	newmm->exe_file = get_mm_exe_file(oldmm);
+}
+
 /**
  * get_task_mm - acquire a reference to the task's mm
  *

commit a77aea92010acf54ad785047234418d5d68772e2
Author: Daniel Lezcano <daniel.lezcano@free.fr>
Date:   Thu May 26 16:25:23 2011 -0700

    cgroup: remove the ns_cgroup
    
    The ns_cgroup is an annoying cgroup at the namespace / cgroup frontier and
    leads to some problems:
    
      * cgroup creation is out-of-control
      * cgroup name can conflict when pids are looping
      * it is not possible to have a single process handling a lot of
        namespaces without falling in a exponential creation time
      * we may want to create a namespace without creating a cgroup
    
      The ns_cgroup was replaced by a compatibility flag 'clone_children',
      where a newly created cgroup will copy the parent cgroup values.
      The userspace has to manually create a cgroup and add a task to
      the 'tasks' file.
    
    This patch removes the ns_cgroup as suggested in the following thread:
    
    https://lists.linux-foundation.org/pipermail/containers/2009-June/018616.html
    
    The 'cgroup_clone' function is removed because it is no longer used.
    
    This is a userspace-visible change.  Commit 45531757b45c ("cgroup: notify
    ns_cgroup deprecated") (merged into 2.6.27) caused the kernel to emit a
    printk warning users that the feature is planned for removal.  Since that
    time we have heard from XXX users who were affected by this.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@free.fr>
    Signed-off-by: Serge E. Hallyn <serge.hallyn@canonical.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Jamal Hadi Salim <hadi@cyberus.ca>
    Reviewed-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Paul Menage <menage@google.com>
    Acked-by: Matt Helsley <matthltc@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1fa9d940e301..1f84099ecce6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1229,12 +1229,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (clone_flags & CLONE_THREAD)
 		p->tgid = current->tgid;
 
-	if (current->nsproxy != p->nsproxy) {
-		retval = ns_cgroup_clone(p, pid);
-		if (retval)
-			goto bad_fork_free_pid;
-	}
-
 	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
 	/*
 	 * Clear TID on mm_release()?

commit 4714d1d32d97239fb5ae3e10521d3f133a899b66
Author: Ben Blum <bblum@andrew.cmu.edu>
Date:   Thu May 26 16:25:18 2011 -0700

    cgroups: read-write lock CLONE_THREAD forking per threadgroup
    
    Adds functionality to read/write lock CLONE_THREAD fork()ing per-threadgroup
    
    Add an rwsem that lives in a threadgroup's signal_struct that's taken for
    reading in the fork path, under CONFIG_CGROUPS.  If another part of the
    kernel later wants to use such a locking mechanism, the CONFIG_CGROUPS
    ifdefs should be changed to a higher-up flag that CGROUPS and the other
    system would both depend on.
    
    This is a pre-patch for cgroup-procs-write.patch.
    
    Signed-off-by: Ben Blum <bblum@andrew.cmu.edu>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Matt Helsley <matthltc@us.ibm.com>
    Reviewed-by: Paul Menage <menage@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Miao Xie <miaox@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8e7e135d0817..1fa9d940e301 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -957,6 +957,10 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	tty_audit_fork(sig);
 	sched_autogroup_fork(sig);
 
+#ifdef CONFIG_CGROUPS
+	init_rwsem(&sig->threadgroup_fork_lock);
+#endif
+
 	sig->oom_adj = current->signal->oom_adj;
 	sig->oom_score_adj = current->signal->oom_score_adj;
 	sig->oom_score_adj_min = current->signal->oom_score_adj_min;
@@ -1138,6 +1142,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	monotonic_to_bootbased(&p->real_start_time);
 	p->io_context = NULL;
 	p->audit_context = NULL;
+	if (clone_flags & CLONE_THREAD)
+		threadgroup_fork_read_lock(current);
 	cgroup_fork(p);
 #ifdef CONFIG_NUMA
 	p->mempolicy = mpol_dup(p->mempolicy);
@@ -1342,6 +1348,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	write_unlock_irq(&tasklist_lock);
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
+	if (clone_flags & CLONE_THREAD)
+		threadgroup_fork_read_unlock(current);
 	perf_event_fork(p);
 	return p;
 
@@ -1380,6 +1388,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	mpol_put(p->mempolicy);
 bad_fork_cleanup_cgroup:
 #endif
+	if (clone_flags & CLONE_THREAD)
+		threadgroup_fork_read_unlock(current);
 	cgroup_exit(p, cgroup_callbacks_done);
 	delayacct_tsk_free(p);
 	module_put(task_thread_info(p)->exec_domain->module);

commit de03c72cfce5b263a674d04348b58475ec50163c
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue May 24 17:12:15 2011 -0700

    mm: convert mm->cpu_vm_cpumask into cpumask_var_t
    
    cpumask_t is very big struct and cpu_vm_mask is placed wrong position.
    It might lead to reduce cache hit ratio.
    
    This patch has two change.
    1) Move the place of cpumask into last of mm_struct. Because usually cpumask
       is accessed only front bits when the system has cpu-hotplug capability
    2) Convert cpu_vm_mask into cpumask_var_t. It may help to reduce memory
       footprint if cpumask_size() will use nr_cpumask_bits properly in future.
    
    In addition, this patch change the name of cpu_vm_mask with cpu_vm_mask_var.
    It may help to detect out of tree cpu_vm_mask users.
    
    This patch has no functional change.
    
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 927692734bcf..8e7e135d0817 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -485,6 +485,20 @@ static void mm_init_aio(struct mm_struct *mm)
 #endif
 }
 
+int mm_init_cpumask(struct mm_struct *mm, struct mm_struct *oldmm)
+{
+#ifdef CONFIG_CPUMASK_OFFSTACK
+	if (!alloc_cpumask_var(&mm->cpu_vm_mask_var, GFP_KERNEL))
+		return -ENOMEM;
+
+	if (oldmm)
+		cpumask_copy(mm_cpumask(mm), mm_cpumask(oldmm));
+	else
+		memset(mm_cpumask(mm), 0, cpumask_size());
+#endif
+	return 0;
+}
+
 static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 {
 	atomic_set(&mm->mm_users, 1);
@@ -521,10 +535,20 @@ struct mm_struct * mm_alloc(void)
 	struct mm_struct * mm;
 
 	mm = allocate_mm();
-	if (mm) {
-		memset(mm, 0, sizeof(*mm));
-		mm = mm_init(mm, current);
+	if (!mm)
+		return NULL;
+
+	memset(mm, 0, sizeof(*mm));
+	mm = mm_init(mm, current);
+	if (!mm)
+		return NULL;
+
+	if (mm_init_cpumask(mm, NULL)) {
+		mm_free_pgd(mm);
+		free_mm(mm);
+		return NULL;
 	}
+
 	return mm;
 }
 
@@ -536,6 +560,7 @@ struct mm_struct * mm_alloc(void)
 void __mmdrop(struct mm_struct *mm)
 {
 	BUG_ON(mm == &init_mm);
+	free_cpumask_var(mm->cpu_vm_mask_var);
 	mm_free_pgd(mm);
 	destroy_context(mm);
 	mmu_notifier_mm_destroy(mm);
@@ -690,6 +715,9 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	if (!mm_init(mm, tsk))
 		goto fail_nomem;
 
+	if (mm_init_cpumask(mm, oldmm))
+		goto fail_nocpumask;
+
 	if (init_new_context(tsk, mm))
 		goto fail_nocontext;
 
@@ -716,6 +744,9 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	return NULL;
 
 fail_nocontext:
+	free_cpumask_var(mm->cpu_vm_mask_var);
+
+fail_nocpumask:
 	/*
 	 * If init_new_context() failed, we cannot use mmput() to free the mm
 	 * because it calls destroy_context()

commit 3d48ae45e72390ddf8cc5256ac32ed6f7a19cbea
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 24 17:12:06 2011 -0700

    mm: Convert i_mmap_lock to a mutex
    
    Straightforward conversion of i_mmap_lock to a mutex.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4eef925477fc..927692734bcf 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -383,14 +383,14 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			get_file(file);
 			if (tmp->vm_flags & VM_DENYWRITE)
 				atomic_dec(&inode->i_writecount);
-			spin_lock(&mapping->i_mmap_lock);
+			mutex_lock(&mapping->i_mmap_mutex);
 			if (tmp->vm_flags & VM_SHARED)
 				mapping->i_mmap_writable++;
 			flush_dcache_mmap_lock(mapping);
 			/* insert tmp into the share list, just after mpnt */
 			vma_prio_tree_add(tmp, mpnt);
 			flush_dcache_mmap_unlock(mapping);
-			spin_unlock(&mapping->i_mmap_lock);
+			mutex_unlock(&mapping->i_mmap_mutex);
 		}
 
 		/*

commit 97a894136f29802da19a15541de3c019e1ca147e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 24 17:12:04 2011 -0700

    mm: Remove i_mmap_lock lockbreak
    
    Hugh says:
     "The only significant loser, I think, would be page reclaim (when
      concurrent with truncation): could spin for a long time waiting for
      the i_mmap_mutex it expects would soon be dropped? "
    
    Counter points:
     - cpu contention makes the spin stop (need_resched())
     - zap pages should be freeing pages at a higher rate than reclaim
       ever can
    
    I think the simplification of the truncate code is definitely worth it.
    
    Effectively reverts: 2aa15890f3c ("mm: prevent concurrent
    unmap_mapping_range() on the same inode") and takes out the code that
    caused its problem.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2b44d82b8237..4eef925477fc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -386,7 +386,6 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			spin_lock(&mapping->i_mmap_lock);
 			if (tmp->vm_flags & VM_SHARED)
 				mapping->i_mmap_writable++;
-			tmp->vm_truncate_count = mpnt->vm_truncate_count;
 			flush_dcache_mmap_lock(mapping);
 			/* insert tmp into the share list, just after mpnt */
 			vma_prio_tree_add(tmp, mpnt);

commit 3e51e3edfd81bfd9853ad7de91167e4ce33d0fe7
Author: Samir Bellabes <sam@synack.fr>
Date:   Wed May 11 18:18:05 2011 +0200

    sched: Remove unused parameters from sched_fork() and wake_up_new_task()
    
    sched_fork() and wake_up_new_task() are defined with a parameter
    'unsigned long clone_flags', which is unused.
    
    This patch removes the parameters.
    
    Signed-off-by: Samir Bellabes <sam@synack.fr>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1305130685-1047-1-git-send-email-sam@synack.fr
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index aca62871a4f9..2b44d82b8237 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1152,7 +1152,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */
-	sched_fork(p, clone_flags);
+	sched_fork(p);
 
 	retval = perf_event_init_task(p);
 	if (retval)
@@ -1463,7 +1463,7 @@ long do_fork(unsigned long clone_flags,
 		 */
 		p->flags &= ~PF_STARTING;
 
-		wake_up_new_task(p, clone_flags);
+		wake_up_new_task(p);
 
 		tracehook_report_clone_complete(trace, regs,
 						clone_flags, nr, p);

commit 625f2a378e5a10f45fdc37932fc9f8a21676de9e
Author: Jonathan Corbet <corbet@lwn.net>
Date:   Fri Apr 22 11:19:10 2011 -0600

    sched: Get rid of lock_depth
    
    Neil Brown pointed out that lock_depth somehow escaped the BKL
    removal work.  Let's get rid of it now.
    
    Note that the perf scripting utilities still have a bunch of
    code for dealing with common_lock_depth in tracepoints; I have
    left that in place in case anybody wants to use that code with
    older kernels.
    
    Suggested-by: Neil Brown <neilb@suse.de>
    Signed-off-by: Jonathan Corbet <corbet@lwn.net>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20110422111910.456c0e84@bike.lwn.net
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index e7548dee636b..aca62871a4f9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1103,7 +1103,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	posix_cpu_timers_init(p);
 
-	p->lock_depth = -1;		/* -1 = no lock */
 	do_posix_clock_monotonic_gettime(&p->start_time);
 	p->real_start_time = p->start_time;
 	monotonic_to_bootbased(&p->real_start_time);

commit 6c5103890057b1bb781b26b7aae38d33e4c517d8
Merge: 3dab04e6978e 9d2e157d970a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 10:16:26 2011 -0700

    Merge branch 'for-2.6.39/core' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.39/core' of git://git.kernel.dk/linux-2.6-block: (65 commits)
      Documentation/iostats.txt: bit-size reference etc.
      cfq-iosched: removing unnecessary think time checking
      cfq-iosched: Don't clear queue stats when preempt.
      blk-throttle: Reset group slice when limits are changed
      blk-cgroup: Only give unaccounted_time under debug
      cfq-iosched: Don't set active queue in preempt
      block: fix non-atomic access to genhd inflight structures
      block: attempt to merge with existing requests on plug flush
      block: NULL dereference on error path in __blkdev_get()
      cfq-iosched: Don't update group weights when on service tree
      fs: assign sb->s_bdi to default_backing_dev_info if the bdi is going away
      block: Require subsystems to explicitly allocate bio_set integrity mempool
      jbd2: finish conversion from WRITE_SYNC_PLUG to WRITE_SYNC and explicit plugging
      jbd: finish conversion from WRITE_SYNC_PLUG to WRITE_SYNC and explicit plugging
      fs: make fsync_buffers_list() plug
      mm: make generic_writepages() use plugging
      blk-cgroup: Add unaccounted time to timeslice_used.
      block: fixup plugging stubs for !CONFIG_BLOCK
      block: remove obsolete comments for blkdev_issue_zeroout.
      blktrace: Use rq->cmd_flags directly in blk_add_trace_rq.
      ...
    
    Fix up conflicts in fs/{aio.c,super.c}

commit 4308eebbeb2026827d4492ce8c23d99f7f144a82
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Mar 23 16:43:13 2011 -0700

    pidns: call pid_ns_prepare_proc() from create_pid_namespace()
    
    Reorganize proc_get_sb() so it can be called before the struct pid of the
    first process is allocated.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@free.fr>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Serge E. Hallyn <serge@hallyn.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 17aed4378eda..457fff2e17e0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1187,12 +1187,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		pid = alloc_pid(p->nsproxy->pid_ns);
 		if (!pid)
 			goto bad_fork_cleanup_io;
-
-		if (clone_flags & CLONE_NEWPID) {
-			retval = pid_ns_prepare_proc(p->nsproxy->pid_ns);
-			if (retval < 0)
-				goto bad_fork_free_pid;
-		}
 	}
 
 	p->pid = pid_nr(pid);

commit 45a68628d37222e655219febce9e91b6484789b2
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Mar 23 16:43:12 2011 -0700

    pid: remove the child_reaper special case in init/main.c
    
    This patchset is a cleanup and a preparation to unshare the pid namespace.
    These prerequisites prepare for Eric's patchset to give a file descriptor
    to a namespace and join an existing namespace.
    
    This patch:
    
    It turns out that the existing assignment in copy_process of the
    child_reaper can handle the initial assignment of child_reaper we just
    need to generalize the test in kernel/fork.c
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@free.fr>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Serge E. Hallyn <serge@hallyn.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f2b494d7c557..17aed4378eda 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1296,7 +1296,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		tracehook_finish_clone(p, clone_flags, trace);
 
 		if (thread_group_leader(p)) {
-			if (clone_flags & CLONE_NEWPID)
+			if (is_child_reaper(pid))
 				p->nsproxy->pid_ns->child_reaper = p;
 
 			p->signal->leader_pid = pid;

commit 9bfb23fc4a481650e60d22dbe84c0fd5a9d49bba
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Mar 22 16:34:09 2011 -0700

    sys_unshare: remove the dead CLONE_THREAD/SIGHAND/VM code
    
    Cleanup: kill the dead code which does nothing but complicates the code
    and confuses the reader.
    
    sys_unshare(CLONE_THREAD/SIGHAND/VM) is not really implemented, and I
    doubt very much it will ever work.  At least, nobody even tried since the
    original 99d1419d96d7df9cfa56 ("unshare system call -v5: system call
    handler function") was applied more than 4 years ago.
    
    And the code is not consistent.  unshare_thread() always fails
    unconditionally, while unshare_sighand() and unshare_vm() pretend to work
    if there is nothing to unshare.
    
    Remove unshare_thread(), unshare_sighand(), unshare_vm() helpers and
    related variables and add a simple CLONE_THREAD | CLONE_SIGHAND| CLONE_VM
    check into check_unshare_flags().
    
    Also, move the "CLONE_NEWNS needs CLONE_FS" check from
    check_unshare_flags() to sys_unshare().  This looks more consistent and
    matches the similar do_sysvsem check in sys_unshare().
    
    Note: with or without this patch "atomic_read(mm->mm_users) > 1" can give
    a false positive due to get_task_mm().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Janak Desai <janak@us.ibm.com>
    Cc: Daniel Lezcano <daniel.lezcano@free.fr>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a8f64f8ec7e1..f2b494d7c557 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1519,38 +1519,24 @@ void __init proc_caches_init(void)
 }
 
 /*
- * Check constraints on flags passed to the unshare system call and
- * force unsharing of additional process context as appropriate.
+ * Check constraints on flags passed to the unshare system call.
  */
-static void check_unshare_flags(unsigned long *flags_ptr)
+static int check_unshare_flags(unsigned long unshare_flags)
 {
+	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
+				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
+				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET))
+		return -EINVAL;
 	/*
-	 * If unsharing a thread from a thread group, must also
-	 * unshare vm.
-	 */
-	if (*flags_ptr & CLONE_THREAD)
-		*flags_ptr |= CLONE_VM;
-
-	/*
-	 * If unsharing vm, must also unshare signal handlers.
-	 */
-	if (*flags_ptr & CLONE_VM)
-		*flags_ptr |= CLONE_SIGHAND;
-
-	/*
-	 * If unsharing namespace, must also unshare filesystem information.
+	 * Not implemented, but pretend it works if there is nothing to
+	 * unshare. Note that unsharing CLONE_THREAD or CLONE_SIGHAND
+	 * needs to unshare vm.
 	 */
-	if (*flags_ptr & CLONE_NEWNS)
-		*flags_ptr |= CLONE_FS;
-}
-
-/*
- * Unsharing of tasks created with CLONE_THREAD is not supported yet
- */
-static int unshare_thread(unsigned long unshare_flags)
-{
-	if (unshare_flags & CLONE_THREAD)
-		return -EINVAL;
+	if (unshare_flags & (CLONE_THREAD | CLONE_SIGHAND | CLONE_VM)) {
+		/* FIXME: get_task_mm() increments ->mm_users */
+		if (atomic_read(&current->mm->mm_users) > 1)
+			return -EINVAL;
+	}
 
 	return 0;
 }
@@ -1576,34 +1562,6 @@ static int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)
 	return 0;
 }
 
-/*
- * Unsharing of sighand is not supported yet
- */
-static int unshare_sighand(unsigned long unshare_flags, struct sighand_struct **new_sighp)
-{
-	struct sighand_struct *sigh = current->sighand;
-
-	if ((unshare_flags & CLONE_SIGHAND) && atomic_read(&sigh->count) > 1)
-		return -EINVAL;
-	else
-		return 0;
-}
-
-/*
- * Unshare vm if it is being shared
- */
-static int unshare_vm(unsigned long unshare_flags, struct mm_struct **new_mmp)
-{
-	struct mm_struct *mm = current->mm;
-
-	if ((unshare_flags & CLONE_VM) &&
-	    (mm && atomic_read(&mm->mm_users) > 1)) {
-		return -EINVAL;
-	}
-
-	return 0;
-}
-
 /*
  * Unshare file descriptor table if it is being shared
  */
@@ -1632,23 +1590,21 @@ static int unshare_fd(unsigned long unshare_flags, struct files_struct **new_fdp
  */
 SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 {
-	int err = 0;
 	struct fs_struct *fs, *new_fs = NULL;
-	struct sighand_struct *new_sigh = NULL;
-	struct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;
 	struct files_struct *fd, *new_fd = NULL;
 	struct nsproxy *new_nsproxy = NULL;
 	int do_sysvsem = 0;
+	int err;
 
-	check_unshare_flags(&unshare_flags);
-
-	/* Return -EINVAL for all unsupported flags */
-	err = -EINVAL;
-	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
-				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
-				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET))
+	err = check_unshare_flags(unshare_flags);
+	if (err)
 		goto bad_unshare_out;
 
+	/*
+	 * If unsharing namespace, must also unshare filesystem information.
+	 */
+	if (unshare_flags & CLONE_NEWNS)
+		unshare_flags |= CLONE_FS;
 	/*
 	 * CLONE_NEWIPC must also detach from the undolist: after switching
 	 * to a new ipc namespace, the semaphore arrays from the old
@@ -1656,21 +1612,15 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 	 */
 	if (unshare_flags & (CLONE_NEWIPC|CLONE_SYSVSEM))
 		do_sysvsem = 1;
-	if ((err = unshare_thread(unshare_flags)))
-		goto bad_unshare_out;
 	if ((err = unshare_fs(unshare_flags, &new_fs)))
-		goto bad_unshare_cleanup_thread;
-	if ((err = unshare_sighand(unshare_flags, &new_sigh)))
-		goto bad_unshare_cleanup_fs;
-	if ((err = unshare_vm(unshare_flags, &new_mm)))
-		goto bad_unshare_cleanup_sigh;
+		goto bad_unshare_out;
 	if ((err = unshare_fd(unshare_flags, &new_fd)))
-		goto bad_unshare_cleanup_vm;
+		goto bad_unshare_cleanup_fs;
 	if ((err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,
 			new_fs)))
 		goto bad_unshare_cleanup_fd;
 
-	if (new_fs ||  new_mm || new_fd || do_sysvsem || new_nsproxy) {
+	if (new_fs || new_fd || do_sysvsem || new_nsproxy) {
 		if (do_sysvsem) {
 			/*
 			 * CLONE_SYSVSEM is equivalent to sys_exit().
@@ -1696,19 +1646,6 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 			spin_unlock(&fs->lock);
 		}
 
-		if (new_mm) {
-			mm = current->mm;
-			active_mm = current->active_mm;
-			current->mm = new_mm;
-			current->active_mm = new_mm;
-			if (current->signal->oom_score_adj == OOM_SCORE_ADJ_MIN) {
-				atomic_dec(&mm->oom_disable_count);
-				atomic_inc(&new_mm->oom_disable_count);
-			}
-			activate_mm(active_mm, new_mm);
-			new_mm = mm;
-		}
-
 		if (new_fd) {
 			fd = current->files;
 			current->files = new_fd;
@@ -1725,20 +1662,10 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 	if (new_fd)
 		put_files_struct(new_fd);
 
-bad_unshare_cleanup_vm:
-	if (new_mm)
-		mmput(new_mm);
-
-bad_unshare_cleanup_sigh:
-	if (new_sigh)
-		if (atomic_dec_and_test(&new_sigh->count))
-			kmem_cache_free(sighand_cachep, new_sigh);
-
 bad_unshare_cleanup_fs:
 	if (new_fs)
 		free_fs_struct(new_fs);
 
-bad_unshare_cleanup_thread:
 bad_unshare_out:
 	return err;
 }

commit 207205a2ba2655652fe46a60b49838af6c16a919
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Mar 22 16:30:44 2011 -0700

    kthread: NUMA aware kthread_create_on_node()
    
    All kthreads being created from a single helper task, they all use memory
    from a single node for their kernel stack and task struct.
    
    This patch suite creates kthread_create_on_node(), adding a 'cpu' parameter
    to parameters already used by kthread_create().
    
    This parameter serves in allocating memory for the new kthread on its
    memory node if possible.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index cbc6adc6e891..a8f64f8ec7e1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -40,6 +40,7 @@
 #include <linux/tracehook.h>
 #include <linux/futex.h>
 #include <linux/compat.h>
+#include <linux/kthread.h>
 #include <linux/task_io_accounting_ops.h>
 #include <linux/rcupdate.h>
 #include <linux/ptrace.h>
@@ -254,7 +255,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	struct task_struct *tsk;
 	struct thread_info *ti;
 	unsigned long *stackend;
-	int node = numa_node_id();
+	int node = tsk_fork_get_node(orig);
 	int err;
 
 	prepare_to_copy(orig);

commit b6a84016bd2598e35ead635147fa53619982648d
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Mar 22 16:30:42 2011 -0700

    mm: NUMA aware alloc_thread_info_node()
    
    Add a node parameter to alloc_thread_info(), and change its name to
    alloc_thread_info_node()
    
    This change is needed to allow NUMA aware kthread_create_on_cpu()
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index cffbe8a4e1fc..cbc6adc6e891 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -117,14 +117,17 @@ static struct kmem_cache *task_struct_cachep;
 #endif
 
 #ifndef __HAVE_ARCH_THREAD_INFO_ALLOCATOR
-static inline struct thread_info *alloc_thread_info(struct task_struct *tsk)
+static struct thread_info *alloc_thread_info_node(struct task_struct *tsk,
+						  int node)
 {
 #ifdef CONFIG_DEBUG_STACK_USAGE
 	gfp_t mask = GFP_KERNEL | __GFP_ZERO;
 #else
 	gfp_t mask = GFP_KERNEL;
 #endif
-	return (struct thread_info *)__get_free_pages(mask, THREAD_SIZE_ORDER);
+	struct page *page = alloc_pages_node(node, mask, THREAD_SIZE_ORDER);
+
+	return page ? page_address(page) : NULL;
 }
 
 static inline void free_thread_info(struct thread_info *ti)
@@ -260,7 +263,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	if (!tsk)
 		return NULL;
 
-	ti = alloc_thread_info(tsk);
+	ti = alloc_thread_info_node(tsk, node);
 	if (!ti) {
 		free_task_struct(tsk);
 		return NULL;

commit 504f52b5439aaf26d3e2c1d45ec10fce38c8dd27
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Mar 22 16:30:41 2011 -0700

    mm: NUMA aware alloc_task_struct_node()
    
    All kthreads being created from a single helper task, they all use memory
    from a single node for their kernel stack and task struct.
    
    This patch suite creates kthread_create_on_cpu(), adding a 'cpu' parameter
    to parameters already used by kthread_create().
    
    This parameter serves in allocating memory for the new kthread on its
    memory node if available.
    
    Users of this new function are : ksoftirqd, kworker, migration, pktgend...
    
    This patch:
    
    Add a node parameter to alloc_task_struct(), and change its name to
    alloc_task_struct_node()
    
    This change is needed to allow NUMA aware kthread_create_on_cpu()
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 05b92c457010..cffbe8a4e1fc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -109,8 +109,10 @@ int nr_processes(void)
 }
 
 #ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
-# define alloc_task_struct()	kmem_cache_alloc(task_struct_cachep, GFP_KERNEL)
-# define free_task_struct(tsk)	kmem_cache_free(task_struct_cachep, (tsk))
+# define alloc_task_struct_node(node)		\
+		kmem_cache_alloc_node(task_struct_cachep, GFP_KERNEL, node)
+# define free_task_struct(tsk)			\
+		kmem_cache_free(task_struct_cachep, (tsk))
 static struct kmem_cache *task_struct_cachep;
 #endif
 
@@ -249,12 +251,12 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	struct task_struct *tsk;
 	struct thread_info *ti;
 	unsigned long *stackend;
-
+	int node = numa_node_id();
 	int err;
 
 	prepare_to_copy(orig);
 
-	tsk = alloc_task_struct();
+	tsk = alloc_task_struct_node(node);
 	if (!tsk)
 		return NULL;
 

commit 77c100c83e84316ced2507c5799f79c2c80bc6b9
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Feb 1 09:51:46 2011 -0500

    export pid symbols needed for kvm_vcpu_on_spin
    
    Export the symbols required for a race-free kvm_vcpu_on_spin.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 25e429152ddc..05b92c457010 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -193,6 +193,7 @@ void __put_task_struct(struct task_struct *tsk)
 	if (!profile_handoff_task(tsk))
 		free_task(tsk);
 }
+EXPORT_SYMBOL_GPL(__put_task_struct);
 
 /*
  * macro override instead of weak attribute alias, to workaround

commit 73c101011926c5832e6e141682180c4debe2cf45
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Tue Mar 8 13:19:51 2011 +0100

    block: initial patch for on-stack per-task plugging
    
    This patch adds support for creating a queuing context outside
    of the queue itself. This enables us to batch up pieces of IO
    before grabbing the block device queue lock and submitting them to
    the IO scheduler.
    
    The context is created on the stack of the process and assigned in
    the task structure, so that we can auto-unplug it if we hit a schedule
    event.
    
    The current queue plugging happens implicitly if IO is submitted to
    an empty device, yet callers have to remember to unplug that IO when
    they are going to wait for it. This is an ugly API and has caused bugs
    in the past. Additionally, it requires hacks in the vm (->sync_page()
    callback) to handle that logic. By switching to an explicit plugging
    scheme we make the API a lot nicer and can get rid of the ->sync_page()
    hack in the vm.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 25e429152ddc..027c80e5162f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1204,6 +1204,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * Clear TID on mm_release()?
 	 */
 	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;
+#ifdef CONFIG_BLOCK
+	p->plug = NULL;
+#endif
 #ifdef CONFIG_FUTEX
 	p->robust_list = NULL;
 #ifdef CONFIG_COMPAT

commit ba76149f47d8c939efa0acc07a191237af900471
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Jan 13 15:46:58 2011 -0800

    thp: khugepaged
    
    Add khugepaged to relocate fragmented pages into hugepages if new
    hugepages become available.  (this is indipendent of the defrag logic that
    will have to make new hugepages available)
    
    The fundamental reason why khugepaged is unavoidable, is that some memory
    can be fragmented and not everything can be relocated.  So when a virtual
    machine quits and releases gigabytes of hugepages, we want to use those
    freely available hugepages to create huge-pmd in the other virtual
    machines that may be running on fragmented memory, to maximize the CPU
    efficiency at all times.  The scan is slow, it takes nearly zero cpu time,
    except when it copies data (in which case it means we definitely want to
    pay for that cpu time) so it seems a good tradeoff.
    
    In addition to the hugepages being released by other process releasing
    memory, we have the strong suspicion that the performance impact of
    potentially defragmenting hugepages during or before each page fault could
    lead to more performance inconsistency than allocating small pages at
    first and having them collapsed into large pages later...  if they prove
    themselfs to be long lived mappings (khugepaged scan is slow so short
    lived mappings have low probability to run into khugepaged if compared to
    long lived mappings).
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f78f50ba6cb2..25e429152ddc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -66,6 +66,7 @@
 #include <linux/posix-timers.h>
 #include <linux/user-return-notifier.h>
 #include <linux/oom.h>
+#include <linux/khugepaged.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -328,6 +329,9 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	rb_parent = NULL;
 	pprev = &mm->mmap;
 	retval = ksm_fork(mm, oldmm);
+	if (retval)
+		goto out;
+	retval = khugepaged_fork(mm, oldmm);
 	if (retval)
 		goto out;
 
@@ -546,6 +550,7 @@ void mmput(struct mm_struct *mm)
 	if (atomic_dec_and_test(&mm->mm_users)) {
 		exit_aio(mm);
 		ksm_exit(mm);
+		khugepaged_exit(mm); /* must run before exit_mmap */
 		exit_mmap(mm);
 		set_mm_exe_file(mm, NULL);
 		if (!list_empty(&mm->mmlist)) {

commit e7a00c45f29c0155007aa150bf231a70fa470365
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Jan 13 15:46:45 2011 -0800

    thp: add pmd_huge_pte to mm_struct
    
    This increase the size of the mm struct a bit but it is needed to
    preallocate one pte for each hugepage so that split_huge_page will not
    require a fail path.  Guarantee of success is a fundamental property of
    split_huge_page to avoid decrasing swapping reliability and to avoid
    adding -ENOMEM fail paths that would otherwise force the hugepage-unaware
    VM code to learn rolling back in the middle of its pte mangling operations
    (if something we need it to learn handling pmd_trans_huge natively rather
    being capable of rollback).  When split_huge_page runs a pte is needed to
    succeed the split, to map the newly splitted regular pages with a regular
    pte.  This way all existing VM code remains backwards compatible by just
    adding a split_huge_page* one liner.  The memory waste of those
    preallocated ptes is negligible and so it is worth it.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1499607e4da2..f78f50ba6cb2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -529,6 +529,9 @@ void __mmdrop(struct mm_struct *mm)
 	mm_free_pgd(mm);
 	destroy_context(mm);
 	mmu_notifier_mm_destroy(mm);
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+	VM_BUG_ON(mm->pmd_huge_pte);
+#endif
 	free_mm(mm);
 }
 EXPORT_SYMBOL_GPL(__mmdrop);
@@ -669,6 +672,10 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	mm->token_priority = 0;
 	mm->last_interval = 0;
 
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+	mm->pmd_huge_pte = NULL;
+#endif
+
 	if (!mm_init(mm, tsk))
 		goto fail_nomem;
 

commit dabb16f639820267b3850d804571c70bd93d4e07
Author: Mandeep Singh Baines <msb@chromium.org>
Date:   Thu Jan 13 15:46:05 2011 -0800

    oom: allow a non-CAP_SYS_RESOURCE proces to oom_score_adj down
    
    We'd like to be able to oom_score_adj a process up/down as it
    enters/leaves the foreground.  Currently, it is not possible to oom_adj
    down without CAP_SYS_RESOURCE.  This patch allows a task to decrease its
    oom_score_adj back to the value that a CAP_SYS_RESOURCE thread set it to
    or its inherited value at fork.  Assuming the thread that has forked it
    has oom_score_adj of 0, each process could decrease it back from 0 upon
    activation unless a CAP_SYS_RESOURCE thread elevated it to something
    higher.
    
    Alternative considered:
    
    * a setuid binary
    * a daemon with CAP_SYS_RESOURCE
    
    Since you don't wan't all processes to be able to reduce their oom_adj, a
    setuid or daemon implementation would be complex.  The alternatives also
    have much higher overhead.
    
    This patch updated from original patch based on feedback from David
    Rientjes.
    
    Signed-off-by: Mandeep Singh Baines <msb@chromium.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Ying Han <yinghan@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 76a1fdd80bdf..1499607e4da2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -910,6 +910,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 
 	sig->oom_adj = current->signal->oom_adj;
 	sig->oom_score_adj = current->signal->oom_score_adj;
+	sig->oom_score_adj_min = current->signal->oom_score_adj_min;
 
 	mutex_init(&sig->cred_guard_mutex);
 

commit 43bb40c9e3aa51a3b038c9df2c9afb4d4685614d
Author: Dave Jones <davej@redhat.com>
Date:   Thu Jan 13 15:45:40 2011 -0800

    sched: remove long deprecated CLONE_STOPPED flag
    
    This warning was added in commit bdff746a3915 ("clone: prepare to recycle
    CLONE_STOPPED") three years ago.  2.6.26 came and went.  As far as I know,
    no-one is actually using CLONE_STOPPED.
    
    Signed-off-by: Dave Jones <davej@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d9b44f20b6b0..76a1fdd80bdf 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1409,23 +1409,6 @@ long do_fork(unsigned long clone_flags,
 			return -EPERM;
 	}
 
-	/*
-	 * We hope to recycle these flags after 2.6.26
-	 */
-	if (unlikely(clone_flags & CLONE_STOPPED)) {
-		static int __read_mostly count = 100;
-
-		if (count > 0 && printk_ratelimit()) {
-			char comm[TASK_COMM_LEN];
-
-			count--;
-			printk(KERN_INFO "fork(): process `%s' used deprecated "
-					"clone flags 0x%lx\n",
-				get_task_comm(comm, current),
-				clone_flags & CLONE_STOPPED);
-		}
-	}
-
 	/*
 	 * When called from kernel_thread, don't do user tracing stuff.
 	 */
@@ -1464,16 +1447,7 @@ long do_fork(unsigned long clone_flags,
 		 */
 		p->flags &= ~PF_STARTING;
 
-		if (unlikely(clone_flags & CLONE_STOPPED)) {
-			/*
-			 * We'll start up with an immediate SIGSTOP.
-			 */
-			sigaddset(&p->pending.signal, SIGSTOP);
-			set_tsk_thread_flag(p, TIF_SIGPENDING);
-			__set_task_state(p, TASK_STOPPED);
-		} else {
-			wake_up_new_task(p, clone_flags);
-		}
+		wake_up_new_task(p, clone_flags);
 
 		tracehook_report_clone_complete(trace, regs,
 						clone_flags, nr, p);

commit 72eb6a791459c87a0340318840bb3bd9252b627b
Merge: 23d69b09b78c 55ee4ef30241
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 7 17:02:58 2011 -0800

    Merge branch 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-2.6.38' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (30 commits)
      gameport: use this_cpu_read instead of lookup
      x86: udelay: Use this_cpu_read to avoid address calculation
      x86: Use this_cpu_inc_return for nmi counter
      x86: Replace uses of current_cpu_data with this_cpu ops
      x86: Use this_cpu_ops to optimize code
      vmstat: User per cpu atomics to avoid interrupt disable / enable
      irq_work: Use per cpu atomics instead of regular atomics
      cpuops: Use cmpxchg for xchg to avoid lock semantics
      x86: this_cpu_cmpxchg and this_cpu_xchg operations
      percpu: Generic this_cpu_cmpxchg() and this_cpu_xchg support
      percpu,x86: relocate this_cpu_add_return() and friends
      connector: Use this_cpu operations
      xen: Use this_cpu_inc_return
      taskstats: Use this_cpu_ops
      random: Use this_cpu_inc_return
      fs: Use this_cpu_inc_return in buffer.c
      highmem: Use this_cpu_xx_return() operations
      vmstat: Use this_cpu_inc_return for vm statistics
      x86: Support for this_cpu_add, sub, dec, inc_return
      percpu: Generic support for this_cpu_add, sub, dec, inc_return
      ...
    
    Fixed up conflicts: in arch/x86/kernel/{apic/nmi.c, apic/x2apic_uv_x.c, process.c}
    as per Tejun.

commit 1c5354de90c900b369e2ebd36c3a065ede29eb93
Author: Mike Galbraith <efault@gmx.de>
Date:   Wed Jan 5 11:16:04 2011 +0100

    sched: Move sched_autogroup_exit() to free_signal_struct()
    
    Per Oleg's suggestion, undo fork failure free/put_signal_struct change,
    and move sched_autogroup_exit() to free_signal_struct() instead.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1294222564.8369.6.camel@marge.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7d164e25b0f0..dc1a8bbcea7b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -169,15 +169,14 @@ EXPORT_SYMBOL(free_task);
 static inline void free_signal_struct(struct signal_struct *sig)
 {
 	taskstats_tgid_free(sig);
+	sched_autogroup_exit(sig);
 	kmem_cache_free(signal_cachep, sig);
 }
 
 static inline void put_signal_struct(struct signal_struct *sig)
 {
-	if (atomic_dec_and_test(&sig->sigcnt)) {
-		sched_autogroup_exit(sig);
+	if (atomic_dec_and_test(&sig->sigcnt))
 		free_signal_struct(sig);
-	}
 }
 
 void __put_task_struct(struct task_struct *tsk)
@@ -1318,7 +1317,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	}
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
-		put_signal_struct(p->signal);
+		free_signal_struct(p->signal);
 bad_fork_cleanup_sighand:
 	__cleanup_sighand(p->sighand);
 bad_fork_cleanup_fs:

commit 27066fd484a32c80630136aa2b91c980f3198f9d
Merge: 101e5f77bf35 3c0eee3fe6a3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 5 14:14:42 2011 +0100

    Merge commit 'v2.6.37' into sched/core
    
    Merge reason: Merge the final .37 tree.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 101e5f77bf35679809586e250b6c62193d2ed179
Author: Mike Galbraith <efault@gmx.de>
Date:   Fri Dec 31 09:32:30 2010 +0100

    sched, autogroup: Fix reference leak
    
    The cgroup exit mess also uncovered a struct autogroup reference leak.
    copy_process() was simply freeing vs putting the signal_struct,
    stranding a reference.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    LKML-Reference: <1293784350.6839.2.camel@marge.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index b6f2475f1e83..067244495966 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1317,7 +1317,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	}
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
-		free_signal_struct(p->signal);
+		put_signal_struct(p->signal);
 bad_fork_cleanup_sighand:
 	__cleanup_sighand(p->sighand);
 bad_fork_cleanup_fs:

commit 909ea96468096b07fbb41aaf69be060d92bd9271
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Dec 8 16:22:55 2010 +0100

    core: Replace __get_cpu_var with __this_cpu_read if not used for an address.
    
    __get_cpu_var() can be replaced with this_cpu_read and will then use a
    single read instruction with implied address calculation to access the
    correct per cpu instance.
    
    However, the address of a per cpu variable passed to __this_cpu_read()
    cannot be determined (since it's an implied address conversion through
    segment prefixes).  Therefore apply this only to uses of __get_cpu_var
    where the address of the variable is not used.
    
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3b159c5991b7..e05e27de67df 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1282,7 +1282,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			attach_pid(p, PIDTYPE_SID, task_session(current));
 			list_add_tail(&p->sibling, &p->real_parent->children);
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
-			__get_cpu_var(process_counts)++;
+			__this_cpu_inc(process_counts);
 		}
 		attach_pid(p, PIDTYPE_PID, pid);
 		nr_threads++;

commit f26f9aff6aaf67e9a430d16c266f91b13a5bff64
Author: Mike Galbraith <efault@gmx.de>
Date:   Wed Dec 8 11:05:42 2010 +0100

    Sched: fix skip_clock_update optimization
    
    idle_balance() drops/retakes rq->lock, leaving the previous task
    vulnerable to set_tsk_need_resched().  Clear it after we return
    from balancing instead, and in setup_thread_stack() as well, so
    no successfully descheduled or never scheduled task has it set.
    
    Need resched confused the skip_clock_update logic, which assumes
    that the next call to update_rq_clock() will come nearly immediately
    after being set.  Make the optimization robust against the waking
    a sleeper before it sucessfully deschedules case by checking that
    the current task has not been dequeued before setting the flag,
    since it is that useless clock update we're trying to save, and
    clear unconditionally in schedule() proper instead of conditionally
    in put_prev_task().
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Reported-by: Bjoern B. Brandenburg <bbb.lst@gmail.com>
    Tested-by: Yong Zhang <yong.zhang0@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: stable@kernel.org
    LKML-Reference: <1291802742.1417.9.camel@marge.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3b159c5991b7..5447dc7defa9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -273,6 +273,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 
 	setup_thread_stack(tsk, orig);
 	clear_user_return_notifier(tsk);
+	clear_tsk_need_resched(tsk);
 	stackend = end_of_stack(tsk);
 	*stackend = STACK_END_MAGIC;	/* for overflow detection */
 

commit 5091faa449ee0b7d73bc296a93bca9540fc51d0a
Author: Mike Galbraith <efault@gmx.de>
Date:   Tue Nov 30 14:18:03 2010 +0100

    sched: Add 'autogroup' scheduling feature: automated per session task groups
    
    A recurring complaint from CFS users is that parallel kbuild has
    a negative impact on desktop interactivity.  This patch
    implements an idea from Linus, to automatically create task
    groups.  Currently, only per session autogroups are implemented,
    but the patch leaves the way open for enhancement.
    
    Implementation: each task's signal struct contains an inherited
    pointer to a refcounted autogroup struct containing a task group
    pointer, the default for all tasks pointing to the
    init_task_group.  When a task calls setsid(), a new task group
    is created, the process is moved into the new task group, and a
    reference to the preveious task group is dropped.  Child
    processes inherit this task group thereafter, and increase it's
    refcount.  When the last thread of a process exits, the
    process's reference is dropped, such that when the last process
    referencing an autogroup exits, the autogroup is destroyed.
    
    At runqueue selection time, IFF a task has no cgroup assignment,
    its current autogroup is used.
    
    Autogroup bandwidth is controllable via setting it's nice level
    through the proc filesystem:
    
      cat /proc/<pid>/autogroup
    
    Displays the task's group and the group's nice level.
    
      echo <nice level> > /proc/<pid>/autogroup
    
    Sets the task group's shares to the weight of nice <level> task.
    Setting nice level is rate limited for !admin users due to the
    abuse risk of task group locking.
    
    The feature is enabled from boot by default if
    CONFIG_SCHED_AUTOGROUP=y is selected, but can be disabled via
    the boot option noautogroup, and can also be turned on/off on
    the fly via:
    
      echo [01] > /proc/sys/kernel/sched_autogroup_enabled
    
    ... which will automatically move tasks to/from the root task group.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Markus Trippelsdorf <markus@trippelsdorf.de>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    [ Removed the task_group_path() debug code, and fixed !EVENTFD build failure. ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <1290281700.28711.9.camel@maggy.simson.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3b159c5991b7..b6f2475f1e83 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -174,8 +174,10 @@ static inline void free_signal_struct(struct signal_struct *sig)
 
 static inline void put_signal_struct(struct signal_struct *sig)
 {
-	if (atomic_dec_and_test(&sig->sigcnt))
+	if (atomic_dec_and_test(&sig->sigcnt)) {
+		sched_autogroup_exit(sig);
 		free_signal_struct(sig);
+	}
 }
 
 void __put_task_struct(struct task_struct *tsk)
@@ -904,6 +906,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	posix_cpu_timers_init_group(sig);
 
 	tty_audit_fork(sig);
+	sched_autogroup_fork(sig);
 
 	sig->oom_adj = current->signal->oom_adj;
 	sig->oom_score_adj = current->signal->oom_score_adj;

commit 9b1bf12d5d51bca178dea21b04a0805e29d60cf1
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Wed Oct 27 15:34:08 2010 -0700

    signals: move cred_guard_mutex from task_struct to signal_struct
    
    Oleg Nesterov pointed out we have to prevent multiple-threads-inside-exec
    itself and we can reuse ->cred_guard_mutex for it.  Yes, concurrent
    execve() has no worth.
    
    Let's move ->cred_guard_mutex from task_struct to signal_struct.  It
    naturally prevent multiple-threads-inside-exec.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e87aaaaf5131..3b159c5991b7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -908,6 +908,8 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->oom_adj = current->signal->oom_adj;
 	sig->oom_score_adj = current->signal->oom_score_adj;
 
+	mutex_init(&sig->cred_guard_mutex);
+
 	return 0;
 }
 

commit 3d5992d2ac7dc09aed8ab537cba074589f0f0a52
Author: Ying Han <yinghan@google.com>
Date:   Tue Oct 26 14:21:23 2010 -0700

    oom: add per-mm oom disable count
    
    It's pointless to kill a task if another thread sharing its mm cannot be
    killed to allow future memory freeing.  A subsequent patch will prevent
    kills in such cases, but first it's necessary to have a way to flag a task
    that shares memory with an OOM_DISABLE task that doesn't incur an
    additional tasklist scan, which would make select_bad_process() an O(n^2)
    function.
    
    This patch adds an atomic counter to struct mm_struct that follows how
    many threads attached to it have an oom_score_adj of OOM_SCORE_ADJ_MIN.
    They cannot be killed by the kernel, so their memory cannot be freed in
    oom conditions.
    
    This only requires task_lock() on the task that we're operating on, it
    does not require mm->mmap_sem since task_lock() pins the mm and the
    operation is atomic.
    
    [rientjes@google.com: changelog and sys_unshare() code]
    [rientjes@google.com: protect oom_disable_count with task_lock in fork]
    [rientjes@google.com: use old_mm for oom_disable_count in exec]
    Signed-off-by: Ying Han <yinghan@google.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c445f8cc408d..e87aaaaf5131 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -65,6 +65,7 @@
 #include <linux/perf_event.h>
 #include <linux/posix-timers.h>
 #include <linux/user-return-notifier.h>
+#include <linux/oom.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -488,6 +489,7 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 	mm->cached_hole_size = ~0UL;
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
+	atomic_set(&mm->oom_disable_count, 0);
 
 	if (likely(!mm_alloc_pgd(mm))) {
 		mm->def_flags = 0;
@@ -741,6 +743,8 @@ static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
 	/* Initializing for Swap token stuff */
 	mm->token_priority = 0;
 	mm->last_interval = 0;
+	if (tsk->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)
+		atomic_inc(&mm->oom_disable_count);
 
 	tsk->mm = mm;
 	tsk->active_mm = mm;
@@ -1299,8 +1303,13 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_namespaces:
 	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
-	if (p->mm)
+	if (p->mm) {
+		task_lock(p);
+		if (p->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)
+			atomic_dec(&p->mm->oom_disable_count);
+		task_unlock(p);
 		mmput(p->mm);
+	}
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
 		free_signal_struct(p->signal);
@@ -1693,6 +1702,10 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 			active_mm = current->active_mm;
 			current->mm = new_mm;
 			current->active_mm = new_mm;
+			if (current->signal->oom_score_adj == OOM_SCORE_ADJ_MIN) {
+				atomic_dec(&mm->oom_disable_count);
+				atomic_inc(&new_mm->oom_disable_count);
+			}
 			activate_mm(active_mm, new_mm);
 			new_mm = mm;
 		}

commit a247c3a97a0216b18a46243eda26081f1928ec37
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Wed Sep 22 13:05:12 2010 -0700

    rmap: fix walk during fork
    
    The below bug in fork led to the rmap walk finding the parent huge-pmd
    twice instead of just once, because the anon_vma_chain objects of the
    child vma still point to the vma->vm_mm of the parent.
    
    The patch fixes it by making the rmap walk accurate during fork.  It's not
    a big deal normally but it worth being accurate considering the cost is
    the same.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Johannes Weiner <jweiner@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b7e9d60a675d..c445f8cc408d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -356,10 +356,10 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		if (IS_ERR(pol))
 			goto fail_nomem_policy;
 		vma_set_policy(tmp, pol);
+		tmp->vm_mm = mm;
 		if (anon_vma_fork(tmp, mpnt))
 			goto fail_nomem_anon_vma_fork;
 		tmp->vm_flags &= ~VM_LOCKED;
-		tmp->vm_mm = mm;
 		tmp->vm_next = tmp->vm_prev = NULL;
 		file = tmp->vm_file;
 		if (file) {

commit 297c5eee372478fc32fec5fe8eed711eedb13f3d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 20 16:24:55 2010 -0700

    mm: make the vma list be doubly linked
    
    It's a really simple list, and several of the users want to go backwards
    in it to find the previous vma.  So rather than have to look up the
    previous entry with 'find_vma_prev()' or something similar, just make it
    doubly linked instead.
    
    Tested-by: Ian Campbell <ijc@hellion.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 856eac3ec52e..b7e9d60a675d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -300,7 +300,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 #ifdef CONFIG_MMU
 static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 {
-	struct vm_area_struct *mpnt, *tmp, **pprev;
+	struct vm_area_struct *mpnt, *tmp, *prev, **pprev;
 	struct rb_node **rb_link, *rb_parent;
 	int retval;
 	unsigned long charge;
@@ -328,6 +328,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	if (retval)
 		goto out;
 
+	prev = NULL;
 	for (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {
 		struct file *file;
 
@@ -359,7 +360,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			goto fail_nomem_anon_vma_fork;
 		tmp->vm_flags &= ~VM_LOCKED;
 		tmp->vm_mm = mm;
-		tmp->vm_next = NULL;
+		tmp->vm_next = tmp->vm_prev = NULL;
 		file = tmp->vm_file;
 		if (file) {
 			struct inode *inode = file->f_path.dentry->d_inode;
@@ -392,6 +393,8 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		 */
 		*pprev = tmp;
 		pprev = &tmp->vm_next;
+		tmp->vm_prev = prev;
+		prev = tmp;
 
 		__vma_link_rb(mm, tmp, rb_link, rb_parent);
 		rb_link = &tmp->vm_rb.rb_right;

commit 2a4419b5b2a77f3f4537c14f7ad7df95770655dd
Author: Nick Piggin <npiggin@kernel.dk>
Date:   Wed Aug 18 04:37:33 2010 +1000

    fs: fs_struct rwlock to spinlock
    
    fs: fs_struct rwlock to spinlock
    
    struct fs_struct.lock is an rwlock with the read-side used to protect root and
    pwd members while taking references to them. Taking a reference to a path
    typically requires just 2 atomic ops, so the critical section is very small.
    Parallel read-side operations would have cacheline contention on the lock, the
    dentry, and the vfsmount cachelines, so the rwlock is unlikely to ever give a
    real parallelism increase.
    
    Replace it with a spinlock to avoid one or two atomic operations in typical
    path lookup fastpath.
    
    Signed-off-by: Nick Piggin <npiggin@kernel.dk>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 98b450876f93..856eac3ec52e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -752,13 +752,13 @@ static int copy_fs(unsigned long clone_flags, struct task_struct *tsk)
 	struct fs_struct *fs = current->fs;
 	if (clone_flags & CLONE_FS) {
 		/* tsk->fs is already what we want */
-		write_lock(&fs->lock);
+		spin_lock(&fs->lock);
 		if (fs->in_exec) {
-			write_unlock(&fs->lock);
+			spin_unlock(&fs->lock);
 			return -EAGAIN;
 		}
 		fs->users++;
-		write_unlock(&fs->lock);
+		spin_unlock(&fs->lock);
 		return 0;
 	}
 	tsk->fs = copy_fs_struct(fs);
@@ -1676,13 +1676,13 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 
 		if (new_fs) {
 			fs = current->fs;
-			write_lock(&fs->lock);
+			spin_lock(&fs->lock);
 			current->fs = new_fs;
 			if (--fs->users)
 				new_fs = NULL;
 			else
 				new_fs = fs;
-			write_unlock(&fs->lock);
+			spin_unlock(&fs->lock);
 		}
 
 		if (new_mm) {

commit a63d83f427fbce97a6cea0db2e64b0eb8435cd10
Author: David Rientjes <rientjes@google.com>
Date:   Mon Aug 9 17:19:46 2010 -0700

    oom: badness heuristic rewrite
    
    This a complete rewrite of the oom killer's badness() heuristic which is
    used to determine which task to kill in oom conditions.  The goal is to
    make it as simple and predictable as possible so the results are better
    understood and we end up killing the task which will lead to the most
    memory freeing while still respecting the fine-tuning from userspace.
    
    Instead of basing the heuristic on mm->total_vm for each task, the task's
    rss and swap space is used instead.  This is a better indication of the
    amount of memory that will be freeable if the oom killed task is chosen
    and subsequently exits.  This helps specifically in cases where KDE or
    GNOME is chosen for oom kill on desktop systems instead of a memory
    hogging task.
    
    The baseline for the heuristic is a proportion of memory that each task is
    currently using in memory plus swap compared to the amount of "allowable"
    memory.  "Allowable," in this sense, means the system-wide resources for
    unconstrained oom conditions, the set of mempolicy nodes, the mems
    attached to current's cpuset, or a memory controller's limit.  The
    proportion is given on a scale of 0 (never kill) to 1000 (always kill),
    roughly meaning that if a task has a badness() score of 500 that the task
    consumes approximately 50% of allowable memory resident in RAM or in swap
    space.
    
    The proportion is always relative to the amount of "allowable" memory and
    not the total amount of RAM systemwide so that mempolicies and cpusets may
    operate in isolation; they shall not need to know the true size of the
    machine on which they are running if they are bound to a specific set of
    nodes or mems, respectively.
    
    Root tasks are given 3% extra memory just like __vm_enough_memory()
    provides in LSMs.  In the event of two tasks consuming similar amounts of
    memory, it is generally better to save root's task.
    
    Because of the change in the badness() heuristic's baseline, it is also
    necessary to introduce a new user interface to tune it.  It's not possible
    to redefine the meaning of /proc/pid/oom_adj with a new scale since the
    ABI cannot be changed for backward compatability.  Instead, a new tunable,
    /proc/pid/oom_score_adj, is added that ranges from -1000 to +1000.  It may
    be used to polarize the heuristic such that certain tasks are never
    considered for oom kill while others may always be considered.  The value
    is added directly into the badness() score so a value of -500, for
    example, means to discount 50% of its memory consumption in comparison to
    other tasks either on the system, bound to the mempolicy, in the cpuset,
    or sharing the same memory controller.
    
    /proc/pid/oom_adj is changed so that its meaning is rescaled into the
    units used by /proc/pid/oom_score_adj, and vice versa.  Changing one of
    these per-task tunables will rescale the value of the other to an
    equivalent meaning.  Although /proc/pid/oom_adj was originally defined as
    a bitshift on the badness score, it now shares the same linear growth as
    /proc/pid/oom_score_adj but with different granularity.  This is required
    so the ABI is not broken with userspace applications and allows oom_adj to
    be deprecated for future removal.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a82a65cef741..98b450876f93 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -899,6 +899,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	tty_audit_fork(sig);
 
 	sig->oom_adj = current->signal->oom_adj;
+	sig->oom_score_adj = current->signal->oom_score_adj;
 
 	return 0;
 }

commit 21aa9af03d06cb1d19a3738e5cf12acff984e69b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 8 21:40:37 2010 +0200

    sched: add hooks for workqueue
    
    Concurrency managed workqueue needs to know when workers are going to
    sleep and waking up.  Using these two hooks, cmwq keeps track of the
    current concurrency level and throttles execution of new works if it's
    too high and wakes up another worker from the sleep hook if it becomes
    too low.
    
    This patch introduces PF_WQ_WORKER to identify workqueue workers and
    adds the following two hooks.
    
    * wq_worker_waking_up(): called when a worker is woken up.
    
    * wq_worker_sleeping(): called when a worker is going to sleep and may
      return a pointer to a local task which should be woken up.  The
      returned task is woken up using try_to_wake_up_local() which is
      simplified ttwu which is called under rq lock and can only wake up
      local tasks.
    
    Both hooks are currently defined as noop in kernel/workqueue_sched.h.
    Later cmwq implementation will replace them with proper
    implementation.
    
    These hooks are hard coded as they'll always be enabled.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index b6cce14ba047..a82a65cef741 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -907,7 +907,7 @@ static void copy_flags(unsigned long clone_flags, struct task_struct *p)
 {
 	unsigned long new_flags = p->flags;
 
-	new_flags &= ~PF_SUPERPRIV;
+	new_flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER);
 	new_flags |= PF_FORKNOEXEC;
 	new_flags |= PF_STARTING;
 	p->flags = new_flags;

commit 35926ff5fba8245bd1c6ac04155048f6f89232b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 30 09:00:03 2010 -0700

    Revert "cpusets: randomize node rotor used in cpuset_mem_spread_node()"
    
    This reverts commit 0ac0c0d0f837c499afd02a802f9cf52d3027fa3b, which
    caused cross-architecture build problems for all the wrong reasons.
    IA64 already added its own version of __node_random(), but the fact is,
    there is nothing architectural about the function, and the original
    commit was just badly done. Revert it, since no fix is forthcoming.
    
    Requested-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index bf9fef6d1bfe..b6cce14ba047 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1086,10 +1086,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
  	}
 	mpol_fix_fork_child_flag(p);
 #endif
-#ifdef CONFIG_CPUSETS
-	p->cpuset_mem_spread_rotor = node_random(p->mems_allowed);
-	p->cpuset_slab_spread_rotor = node_random(p->mems_allowed);
-#endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	p->irq_events = 0;
 #ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW

commit f106eee10038c2ee5b6056aaf3f6d5229be6dcdd
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:44:11 2010 -0700

    pids: fix fork_idle() to setup ->pids correctly
    
    copy_process(pid => &init_struct_pid) doesn't do attach_pid/etc.
    
    It shouldn't, but this means that the idle threads run with the wrong
    pids copied from the caller's task_struct. In x86 case the caller is
    either kernel_init() thread or keventd.
    
    In particular, this means that after the series of cpu_up/cpu_down an
    idle thread (which never exits) can run with .pid pointing to nowhere.
    
    Change fork_idle() to initialize idle->pids[] correctly. We only set
    .pid = &init_struct_pid but do not add .node to list, INIT_TASK() does
    the same for the boot-cpu idle thread (swapper).
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Mathias Krause <Mathias.Krause@secunet.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d32410bd4be7..bf9fef6d1bfe 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1338,6 +1338,16 @@ noinline struct pt_regs * __cpuinit __attribute__((weak)) idle_regs(struct pt_re
 	return regs;
 }
 
+static inline void init_idle_pids(struct pid_link *links)
+{
+	enum pid_type type;
+
+	for (type = PIDTYPE_PID; type < PIDTYPE_MAX; ++type) {
+		INIT_HLIST_NODE(&links[type].node); /* not really needed */
+		links[type].pid = &init_struct_pid;
+	}
+}
+
 struct task_struct * __cpuinit fork_idle(int cpu)
 {
 	struct task_struct *task;
@@ -1345,8 +1355,10 @@ struct task_struct * __cpuinit fork_idle(int cpu)
 
 	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL,
 			    &init_struct_pid, 0);
-	if (!IS_ERR(task))
+	if (!IS_ERR(task)) {
+		init_idle_pids(task->pids);
 		init_idle(task, cpu);
+	}
 
 	return task;
 }

commit b3ac022cb9dc5883505a88b159d1b240ad1ef405
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:24 2010 -0700

    proc: turn signal_struct->count into "int nr_threads"
    
    No functional changes, just s/atomic_t count/int nr_threads/.
    
    With the recent changes this counter has a single user, get_nr_threads()
    And, none of its callers need the really accurate number of threads, not
    to mention each caller obviously races with fork/exit.  It is only used to
    report this value to the user-space, except first_tid() uses it to avoid
    the unnecessary while_each_thread() loop in the unlikely case.
    
    It is a bit sad we need a word in struct signal_struct for this, perhaps
    we can change get_nr_threads() to approximate the number of threads using
    signal->live and kill ->nr_threads later.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 40cd099cfc1b..d32410bd4be7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -877,9 +877,9 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	if (!sig)
 		return -ENOMEM;
 
-	atomic_set(&sig->sigcnt, 1);
-	atomic_set(&sig->count, 1);
+	sig->nr_threads = 1;
 	atomic_set(&sig->live, 1);
+	atomic_set(&sig->sigcnt, 1);
 	init_waitqueue_head(&sig->wait_chldexit);
 	if (clone_flags & CLONE_NEWPID)
 		sig->flags |= SIGNAL_UNKILLABLE;
@@ -1256,9 +1256,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	}
 
 	if (clone_flags & CLONE_THREAD) {
-		atomic_inc(&current->signal->sigcnt);
-		atomic_inc(&current->signal->count);
+		current->signal->nr_threads++;
 		atomic_inc(&current->signal->live);
+		atomic_inc(&current->signal->sigcnt);
 		p->group_leader = current->group_leader;
 		list_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);
 	}

commit 6e1be45aa6ba6a36c0312f65ecf311135c73001d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:21 2010 -0700

    check_unshare_flags: kill the bogus CLONE_SIGHAND/sig->count check
    
    check_unshare_flags(CLONE_SIGHAND) adds CLONE_THREAD to *flags_ptr if the
    task is multithreaded to ensure unshare_thread() will fail.
    
    Not only this is a bit strange way to return the error, this is absolutely
    meaningless.  If signal->count > 1 then sighand->count must be also > 1,
    and unshare_sighand() will fail anyway.
    
    In fact, all CLONE_THREAD/SIGHAND/VM checks inside sys_unshare() do not
    look right.  Fortunately this code doesn't really work anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7701470ea1b8..40cd099cfc1b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1517,14 +1517,6 @@ static void check_unshare_flags(unsigned long *flags_ptr)
 	if (*flags_ptr & CLONE_VM)
 		*flags_ptr |= CLONE_SIGHAND;
 
-	/*
-	 * If unsharing signal handlers and the task was created
-	 * using CLONE_THREAD, then must unshare the thread
-	 */
-	if ((*flags_ptr & CLONE_SIGHAND) &&
-	    (atomic_read(&current->signal->count) > 1))
-		*flags_ptr |= CLONE_THREAD;
-
 	/*
 	 * If unsharing namespace, must also unshare filesystem information.
 	 */

commit 97101eb41d0d3c97543878ce40e0b8a8b2747ed7
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:20 2010 -0700

    exit: move taskstats_tgid_free() from __exit_signal() to free_signal_struct()
    
    Move taskstats_tgid_free() from __exit_signal() to free_signal_struct().
    
    This way signal->stats never points to nowhere and we can read ->stats
    lockless.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 58f8611b1ac6..7701470ea1b8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -167,6 +167,7 @@ EXPORT_SYMBOL(free_task);
 
 static inline void free_signal_struct(struct signal_struct *sig)
 {
+	taskstats_tgid_free(sig);
 	kmem_cache_free(signal_cachep, sig);
 }
 

commit a705be6b5e8b05f2ae51536ec709de921960326c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:19 2010 -0700

    kill the obsolete thread_group_cputime_free() helper
    
    Kill the empty thread_group_cputime_free() helper.  It was needed to free
    the per-cpu data which we no longer have.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e08e3012cd6b..58f8611b1ac6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -167,7 +167,6 @@ EXPORT_SYMBOL(free_task);
 
 static inline void free_signal_struct(struct signal_struct *sig)
 {
-	thread_group_cputime_free(sig);
 	kmem_cache_free(signal_cachep, sig);
 }
 

commit ea6d290ca34c4fd91b7348338c0cc7bdeff94a35
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:16 2010 -0700

    signals: make task_struct->signal immutable/refcountable
    
    We have a lot of problems with accessing task_struct->signal, it can
    "disappear" at any moment.  Even current can't use its ->signal safely
    after exit_notify().  ->siglock helps, but it is not convenient, not
    always possible, and sometimes it makes sense to use task->signal even
    after this task has already dead.
    
    This patch adds the reference counter, sigcnt, into signal_struct.  This
    reference is owned by task_struct and it is dropped in
    __put_task_struct().  Perhaps it makes sense to export
    get/put_signal_struct() later, but currently I don't see the immediate
    reason.
    
    Rename __cleanup_signal() to free_signal_struct() and unexport it.  With
    the previous changes it does nothing except kmem_cache_free().
    
    Change __exit_signal() to not clear/free ->signal, it will be freed when
    the last reference to any thread in the thread group goes away.
    
    Note:
            - when the last thead exits signal->tty can point to nowhere, see
              the next patch.
    
            - with or without this patch signal_struct->count should go away,
              or at least it should be "int nr_threads" for fs/proc. This will
              be addressed later.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b7879ef6e7cd..e08e3012cd6b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -165,6 +165,18 @@ void free_task(struct task_struct *tsk)
 }
 EXPORT_SYMBOL(free_task);
 
+static inline void free_signal_struct(struct signal_struct *sig)
+{
+	thread_group_cputime_free(sig);
+	kmem_cache_free(signal_cachep, sig);
+}
+
+static inline void put_signal_struct(struct signal_struct *sig)
+{
+	if (atomic_dec_and_test(&sig->sigcnt))
+		free_signal_struct(sig);
+}
+
 void __put_task_struct(struct task_struct *tsk)
 {
 	WARN_ON(!tsk->exit_state);
@@ -173,6 +185,7 @@ void __put_task_struct(struct task_struct *tsk)
 
 	exit_creds(tsk);
 	delayacct_tsk_free(tsk);
+	put_signal_struct(tsk->signal);
 
 	if (!profile_handoff_task(tsk))
 		free_task(tsk);
@@ -864,6 +877,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	if (!sig)
 		return -ENOMEM;
 
+	atomic_set(&sig->sigcnt, 1);
 	atomic_set(&sig->count, 1);
 	atomic_set(&sig->live, 1);
 	init_waitqueue_head(&sig->wait_chldexit);
@@ -889,12 +903,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	return 0;
 }
 
-void __cleanup_signal(struct signal_struct *sig)
-{
-	thread_group_cputime_free(sig);
-	kmem_cache_free(signal_cachep, sig);
-}
-
 static void copy_flags(unsigned long clone_flags, struct task_struct *p)
 {
 	unsigned long new_flags = p->flags;
@@ -1248,6 +1256,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	}
 
 	if (clone_flags & CLONE_THREAD) {
+		atomic_inc(&current->signal->sigcnt);
 		atomic_inc(&current->signal->count);
 		atomic_inc(&current->signal->live);
 		p->group_leader = current->group_leader;
@@ -1294,7 +1303,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		mmput(p->mm);
 bad_fork_cleanup_signal:
 	if (!(clone_flags & CLONE_THREAD))
-		__cleanup_signal(p->signal);
+		free_signal_struct(p->signal);
 bad_fork_cleanup_sighand:
 	__cleanup_sighand(p->sighand);
 bad_fork_cleanup_fs:

commit 4dec2a91fd7e8815d730afbfdcf085cbf53433ac
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:15 2010 -0700

    fork/exit: move tty_kref_put() outside of __cleanup_signal()
    
    tty_kref_put() has two callsites in copy_process() paths,
    
            1. if copy_process() suceeds it is called before we copy
               signal->tty from parent
    
            2. otherwise it is called from __cleanup_signal() under
               bad_fork_cleanup_signal: label
    
    In both cases tty_kref_put() is not right and unneeded because we don't
    have the balancing tty_kref_get().  Fortunately, this is harmless because
    this can only happen without CLONE_THREAD, and in this case signal->tty
    must be NULL.
    
    Remove tty_kref_put() from copy_process() and __cleanup_signal(), and
    change another caller of __cleanup_signal(), __exit_signal(), to call
    tty_kref_put() by hand.
    
    I hope this change makes sense by itself, but it is also needed to make
    ->signal refcountable.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Alan Cox <alan@linux.intel.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Greg KH <greg@kroah.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2e9cc3139ec6..b7879ef6e7cd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -892,7 +892,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 void __cleanup_signal(struct signal_struct *sig)
 {
 	thread_group_cputime_free(sig);
-	tty_kref_put(sig->tty);
 	kmem_cache_free(signal_cachep, sig);
 }
 
@@ -1263,7 +1262,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 				p->nsproxy->pid_ns->child_reaper = p;
 
 			p->signal->leader_pid = pid;
-			tty_kref_put(p->signal->tty);
 			p->signal->tty = tty_kref_get(current->signal->tty);
 			attach_pid(p, PIDTYPE_PGID, task_pgrp(current));
 			attach_pid(p, PIDTYPE_SID, task_session(current));

commit 0ac0c0d0f837c499afd02a802f9cf52d3027fa3b
Author: Jack Steiner <steiner@sgi.com>
Date:   Wed May 26 14:42:51 2010 -0700

    cpusets: randomize node rotor used in cpuset_mem_spread_node()
    
    Some workloads that create a large number of small files tend to assign
    too many pages to node 0 (multi-node systems).  Part of the reason is that
    the rotor (in cpuset_mem_spread_node()) used to assign nodes starts at
    node 0 for newly created tasks.
    
    This patch changes the rotor to be initialized to a random node number of
    the cpuset.
    
    [akpm@linux-foundation.org: fix layout]
    [Lee.Schermerhorn@hp.com: Define stub numa_random() for !NUMA configuration]
    Signed-off-by: Jack Steiner <steiner@sgi.com>
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Paul Menage <menage@google.com>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: Robin Holt <holt@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4d57d9e3a6e9..2e9cc3139ec6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1079,6 +1079,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
  	}
 	mpol_fix_fork_child_flag(p);
 #endif
+#ifdef CONFIG_CPUSETS
+	p->cpuset_mem_spread_rotor = node_random(p->mems_allowed);
+	p->cpuset_slab_spread_rotor = node_random(p->mems_allowed);
+#endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	p->irq_events = 0;
 #ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW

commit 4d7b4ac22fbec1a03206c6cde353f2fd6942f828
Merge: 3aaf51ace597 94f3ca95787a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 18 08:19:03 2010 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (311 commits)
      perf tools: Add mode to build without newt support
      perf symbols: symbol inconsistency message should be done only at verbose=1
      perf tui: Add explicit -lslang option
      perf options: Type check all the remaining OPT_ variants
      perf options: Type check OPT_BOOLEAN and fix the offenders
      perf options: Check v type in OPT_U?INTEGER
      perf options: Introduce OPT_UINTEGER
      perf tui: Add workaround for slang < 2.1.4
      perf record: Fix bug mismatch with -c option definition
      perf options: Introduce OPT_U64
      perf tui: Add help window to show key associations
      perf tui: Make <- exit menus too
      perf newt: Add single key shortcuts for zoom into DSO and threads
      perf newt: Exit browser unconditionally when CTRL+C, q or Q is pressed
      perf newt: Fix the 'A'/'a' shortcut for annotate
      perf newt: Make <- exit the ui_browser
      x86, perf: P4 PMU - fix counters management logic
      perf newt: Make <- zoom out filters
      perf report: Report number of events, not samples
      perf hist: Clarify events_stats fields usage
      ...
    
    Fix up trivial conflicts in kernel/fork.c and tools/perf/builtin-record.c

commit 34441427aab4bdb3069a4ffcda69a99357abcb2e
Author: Robin Holt <holt@sgi.com>
Date:   Tue May 11 14:06:46 2010 -0700

    revert "procfs: provide stack information for threads" and its fixup commits
    
    Originally, commit d899bf7b ("procfs: provide stack information for
    threads") attempted to introduce a new feature for showing where the
    threadstack was located and how many pages are being utilized by the
    stack.
    
    Commit c44972f1 ("procfs: disable per-task stack usage on NOMMU") was
    applied to fix the NO_MMU case.
    
    Commit 89240ba0 ("x86, fs: Fix x86 procfs stack information for threads on
    64-bit") was applied to fix a bug in ia32 executables being loaded.
    
    Commit 9ebd4eba7 ("procfs: fix /proc/<pid>/stat stack pointer for kernel
    threads") was applied to fix a bug which had kernel threads printing a
    userland stack address.
    
    Commit 1306d603f ('proc: partially revert "procfs: provide stack
    information for threads"') was then applied to revert the stack pages
    being used to solve a significant performance regression.
    
    This patch nearly undoes the effect of all these patches.
    
    The reason for reverting these is it provides an unusable value in
    field 28.  For x86_64, a fork will result in the task->stack_start
    value being updated to the current user top of stack and not the stack
    start address.  This unpredictability of the stack_start value makes
    it worthless.  That includes the intended use of showing how much stack
    space a thread has.
    
    Other architectures will get different values.  As an example, ia64
    gets 0.  The do_fork() and copy_process() functions appear to treat the
    stack_start and stack_size parameters as architecture specific.
    
    I only partially reverted c44972f1 ("procfs: disable per-task stack usage
    on NOMMU") .  If I had completely reverted it, I would have had to change
    mm/Makefile only build pagewalk.o when CONFIG_PROC_PAGE_MONITOR is
    configured.  Since I could not test the builds without significant effort,
    I decided to not change mm/Makefile.
    
    I only partially reverted 89240ba0 ("x86, fs: Fix x86 procfs stack
    information for threads on 64-bit") .  I left the KSTK_ESP() change in
    place as that seemed worthwhile.
    
    Signed-off-by: Robin Holt <holt@sgi.com>
    Cc: Stefani Seibold <stefani@seibold.net>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 44b0791b0a2e..4c14942a0ee3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1114,8 +1114,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->bts = NULL;
 
-	p->stack_start = stack_start;
-
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p, clone_flags);
 

commit ca7e0c612005937a4a5a75d3fed90459993de65c
Merge: 8141d0050d76 f5284e763578
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Apr 8 13:36:36 2010 +0200

    Merge branch 'linus' into perf/core
    
    Semantic conflict: arch/x86/kernel/cpu/perf_event_intel_ds.c
    
    Merge reason: pick up latest fixes, fix the conflict
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a3a2e76c77fa22b114e421ac11dec0c56c3503fb
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Apr 6 14:34:42 2010 -0700

    mm: avoid null-pointer deref in sync_mm_rss()
    
    - We weren't zeroing p->rss_stat[] at fork()
    
    - Consequently sync_mm_rss() was dereferencing tsk->mm for kernel
      threads and was oopsing.
    
    - Make __sync_task_rss_stat() static, too.
    
    Addresses https://bugzilla.kernel.org/show_bug.cgi?id=15648
    
    [akpm@linux-foundation.org: remove the BUG_ON(!mm->rss)]
    Reported-by: Troels Liebe Bentsen <tlb@rapanden.dk>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4799c5f0e6d0..44b0791b0a2e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1052,6 +1052,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->prev_utime = cputime_zero;
 	p->prev_stime = cputime_zero;
 #endif
+#if defined(SPLIT_RSS_COUNTING)
+	memset(&p->rss_stat, 0, sizeof(p->rss_stat));
+#endif
 
 	p->default_timer_slack_ns = current->timer_slack_ns;
 

commit faa4602e47690fb11221e00f9b9697c8dc0d4b19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:50 2010 +0100

    x86, perf, bts, mm: Delete the never used BTS-ptrace code
    
    Support for the PMU's BTS features has been upstreamed in
    v2.6.32, but we still have the old and disabled ptrace-BTS,
    as Linus noticed it not so long ago.
    
    It's buggy: TIF_DEBUGCTLMSR is trampling all over that MSR without
    regard for other uses (perf) and doesn't provide the flexibility
    needed for perf either.
    
    Its users are ptrace-block-step and ptrace-bts, since ptrace-bts
    was never used and ptrace-block-step can be implemented using a
    much simpler approach.
    
    So axe all 3000 lines of it. That includes the *locked_memory*()
    APIs in mm/mlock.c as well.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100325135413.938004390@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4799c5f0e6d0..d67f1dbfbe03 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1108,9 +1108,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->memcg_batch.do_batch = 0;
 	p->memcg_batch.memcg = NULL;
 #endif
-
-	p->bts = NULL;
-
 	p->stack_start = stack_start;
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */

commit 4e3eaddd142e2142c048c5052a0a9d2604fccfc6
Merge: 8655e7e3ddec b97c4bc16734
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 13 14:43:01 2010 -0800

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      locking: Make sparse work with inline spinlocks and rwlocks
      x86/mce: Fix RCU lockdep splats
      rcu: Increase RCU CPU stall timeouts if PROVE_RCU
      ftrace: Replace read_barrier_depends() with rcu_dereference_raw()
      rcu: Suppress RCU lockdep warnings during early boot
      rcu, ftrace: Fix RCU lockdep splat in ftrace_perf_buf_prepare()
      rcu: Suppress __mpol_dup() false positive from RCU lockdep
      rcu: Make rcu_read_lock_sched_held() handle !PREEMPT
      rcu: Add control variables to lockdep_rcu_dereference() diagnostics
      rcu, cgroup: Relax the check in task_subsys_state() as early boot is now handled by lockdep-RCU
      rcu: Use wrapper function instead of exporting tasklist_lock
      sched, rcu: Fix rcu_dereference() for RCU-lockdep
      rcu: Make task_subsys_state() RCU-lockdep checks handle boot-time use
      rcu: Fix holdoff for accelerated GPs for last non-dynticked CPU
      x86/gart: Unexport gart_iommu_aperture
    
    Fix trivial conflicts in kernel/trace/ftrace.c

commit 93c59907c6f247d09239135caecf294a106a2ae0
Author: Veaceslav Falico <vfalico@redhat.com>
Date:   Wed Mar 10 15:23:03 2010 -0800

    copy_signal() cleanup: clean thread_group_cputime_init()
    
    Remove unneeded initializations in thread_group_cputime_init() and in
    posix_cpu_timers_init_group().  They are useless after kmem_cache_zalloc()
    was used in copy_signal().
    
    Signed-off-by: Veaceslav Falico <vfalico@redhat.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ce2666f84d85..1beb6c303c41 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -833,17 +833,6 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 	/* Thread group counters. */
 	thread_group_cputime_init(sig);
 
-	/* Expiration times and increments. */
-	sig->it[CPUCLOCK_PROF].expires = cputime_zero;
-	sig->it[CPUCLOCK_PROF].incr = cputime_zero;
-	sig->it[CPUCLOCK_VIRT].expires = cputime_zero;
-	sig->it[CPUCLOCK_VIRT].incr = cputime_zero;
-
-	/* Cached expiration times. */
-	sig->cputime_expires.prof_exp = cputime_zero;
-	sig->cputime_expires.virt_exp = cputime_zero;
-	sig->cputime_expires.sched_exp = 0;
-
 	cpu_limit = ACCESS_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
 	if (cpu_limit != RLIM_INFINITY) {
 		sig->cputime_expires.prof_exp = secs_to_cputime(cpu_limit);

commit a56704ef6b0c5796c9ff38cc78aa232dfb9644d7
Author: Veaceslav Falico <vfalico@redhat.com>
Date:   Wed Mar 10 15:23:01 2010 -0800

    copy_signal() cleanup: use zalloc and remove initializations
    
    Use kmem_cache_zalloc() on signal creation and remove unneeded
    initialization lines in copy_signal().
    
    Signed-off-by: Veaceslav Falico <vfalico@redhat.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b0ec34abc0bb..ce2666f84d85 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -863,7 +863,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	if (clone_flags & CLONE_THREAD)
 		return 0;
 
-	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);
+	sig = kmem_cache_zalloc(signal_cachep, GFP_KERNEL);
 	tsk->signal = sig;
 	if (!sig)
 		return -ENOMEM;
@@ -871,46 +871,21 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	atomic_set(&sig->count, 1);
 	atomic_set(&sig->live, 1);
 	init_waitqueue_head(&sig->wait_chldexit);
-	sig->flags = 0;
 	if (clone_flags & CLONE_NEWPID)
 		sig->flags |= SIGNAL_UNKILLABLE;
-	sig->group_exit_code = 0;
-	sig->group_exit_task = NULL;
-	sig->group_stop_count = 0;
 	sig->curr_target = tsk;
 	init_sigpending(&sig->shared_pending);
 	INIT_LIST_HEAD(&sig->posix_timers);
 
 	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
-	sig->it_real_incr.tv64 = 0;
 	sig->real_timer.function = it_real_fn;
 
-	sig->leader = 0;	/* session leadership doesn't inherit */
-	sig->tty_old_pgrp = NULL;
-	sig->tty = NULL;
-
-	sig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;
-	sig->gtime = cputime_zero;
-	sig->cgtime = cputime_zero;
-#ifndef CONFIG_VIRT_CPU_ACCOUNTING
-	sig->prev_utime = sig->prev_stime = cputime_zero;
-#endif
-	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
-	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
-	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;
-	sig->maxrss = sig->cmaxrss = 0;
-	task_io_accounting_init(&sig->ioac);
-	sig->sum_sched_runtime = 0;
-	taskstats_tgid_init(sig);
-
 	task_lock(current->group_leader);
 	memcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);
 	task_unlock(current->group_leader);
 
 	posix_cpu_timers_init_group(sig);
 
-	acct_init_pacct(&sig->pacct);
-
 	tty_audit_fork(sig);
 
 	sig->oom_adj = current->signal->oom_adj;

commit 78d7d407b62a021e6d2e8dc24c0b90e390ab58a1
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri Mar 5 13:42:54 2010 -0800

    kernel core: use helpers for rlimits
    
    Make sure compiler won't do weird things with limits.  E.g.  fetching them
    twice may return 2 different values after writable limits are implemented.
    
    I.e.  either use rlimit helpers added in commit 3e10e716abf3 ("resource:
    add helpers for fetching rlimits") or ACCESS_ONCE if not applicable.
    
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: john stultz <johnstul@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index bab7b254ad39..b0ec34abc0bb 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -828,6 +828,8 @@ void __cleanup_sighand(struct sighand_struct *sighand)
  */
 static void posix_cpu_timers_init_group(struct signal_struct *sig)
 {
+	unsigned long cpu_limit;
+
 	/* Thread group counters. */
 	thread_group_cputime_init(sig);
 
@@ -842,9 +844,9 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 	sig->cputime_expires.virt_exp = cputime_zero;
 	sig->cputime_expires.sched_exp = 0;
 
-	if (sig->rlim[RLIMIT_CPU].rlim_cur != RLIM_INFINITY) {
-		sig->cputime_expires.prof_exp =
-			secs_to_cputime(sig->rlim[RLIMIT_CPU].rlim_cur);
+	cpu_limit = ACCESS_ONCE(sig->rlim[RLIMIT_CPU].rlim_cur);
+	if (cpu_limit != RLIM_INFINITY) {
+		sig->cputime_expires.prof_exp = secs_to_cputime(cpu_limit);
 		sig->cputimer.running = 1;
 	}
 
@@ -1037,7 +1039,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 	retval = -EAGAIN;
 	if (atomic_read(&p->real_cred->user->processes) >=
-			p->signal->rlim[RLIMIT_NPROC].rlim_cur) {
+			task_rlimit(p, RLIMIT_NPROC)) {
 		if (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&
 		    p->real_cred->user != INIT_USER)
 			goto bad_fork_free;

commit 5beb49305251e5669852ed541e8e2f2f7696c53e
Author: Rik van Riel <riel@redhat.com>
Date:   Fri Mar 5 13:42:07 2010 -0800

    mm: change anon_vma linking to fix multi-process server scalability issue
    
    The old anon_vma code can lead to scalability issues with heavily forking
    workloads.  Specifically, each anon_vma will be shared between the parent
    process and all its child processes.
    
    In a workload with 1000 child processes and a VMA with 1000 anonymous
    pages per process that get COWed, this leads to a system with a million
    anonymous pages in the same anon_vma, each of which is mapped in just one
    of the 1000 processes.  However, the current rmap code needs to walk them
    all, leading to O(N) scanning complexity for each page.
    
    This can result in systems where one CPU is walking the page tables of
    1000 processes in page_referenced_one, while all other CPUs are stuck on
    the anon_vma lock.  This leads to catastrophic failure for a benchmark
    like AIM7, where the total number of processes can reach in the tens of
    thousands.  Real workloads are still a factor 10 less process intensive
    than AIM7, but they are catching up.
    
    This patch changes the way anon_vmas and VMAs are linked, which allows us
    to associate multiple anon_vmas with a VMA.  At fork time, each child
    process gets its own anon_vmas, in which its COWed pages will be
    instantiated.  The parents' anon_vma is also linked to the VMA, because
    non-COWed pages could be present in any of the children.
    
    This reduces rmap scanning complexity to O(1) for the pages of the 1000
    child processes, with O(N) complexity for at most 1/N pages in the system.
     This reduces the average scanning cost in heavily forking workloads from
    O(N) to 2.
    
    The only real complexity in this patch stems from the fact that linking a
    VMA to anon_vmas now involves memory allocations.  This means vma_adjust
    can fail, if it needs to attach a VMA to anon_vma structures.  This in
    turn means error handling needs to be added to the calling functions.
    
    A second source of complexity is that, because there can be multiple
    anon_vmas, the anon_vma linking in vma_adjust can no longer be done under
    "the" anon_vma lock.  To prevent the rmap code from walking up an
    incomplete VMA, this patch introduces the VM_LOCK_RMAP VMA flag.  This bit
    flag uses the same slot as the NOMMU VM_MAPPED_COPY, with an ifdef in mm.h
    to make sure it is impossible to compile a kernel that needs both symbolic
    values for the same bitflag.
    
    Some test results:
    
    Without the anon_vma changes, when AIM7 hits around 9.7k users (on a test
    box with 16GB RAM and not quite enough IO), the system ends up running
    >99% in system time, with every CPU on the same anon_vma lock in the
    pageout code.
    
    With these changes, AIM7 hits the cross-over point around 29.7k users.
    This happens with ~99% IO wait time, there never seems to be any spike in
    system time.  The anon_vma lock contention appears to be resolved.
    
    [akpm@linux-foundation.org: cleanups]
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7616bcf107b9..bab7b254ad39 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -329,15 +329,17 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		if (!tmp)
 			goto fail_nomem;
 		*tmp = *mpnt;
+		INIT_LIST_HEAD(&tmp->anon_vma_chain);
 		pol = mpol_dup(vma_policy(mpnt));
 		retval = PTR_ERR(pol);
 		if (IS_ERR(pol))
 			goto fail_nomem_policy;
 		vma_set_policy(tmp, pol);
+		if (anon_vma_fork(tmp, mpnt))
+			goto fail_nomem_anon_vma_fork;
 		tmp->vm_flags &= ~VM_LOCKED;
 		tmp->vm_mm = mm;
 		tmp->vm_next = NULL;
-		anon_vma_link(tmp);
 		file = tmp->vm_file;
 		if (file) {
 			struct inode *inode = file->f_path.dentry->d_inode;
@@ -392,6 +394,8 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	flush_tlb_mm(oldmm);
 	up_write(&oldmm->mmap_sem);
 	return retval;
+fail_nomem_anon_vma_fork:
+	mpol_put(pol);
 fail_nomem_policy:
 	kmem_cache_free(vm_area_cachep, tmp);
 fail_nomem:

commit d559db086ff5be9bcc259e5aa50bf3d881eaf1d1
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Fri Mar 5 13:41:39 2010 -0800

    mm: clean up mm_counter
    
    Presently, per-mm statistics counter is defined by macro in sched.h
    
    This patch modifies it to
      - defined in mm.h as inlinf functions
      - use array instead of macro's name creation.
    
    This patch is for reducing patch size in future patch to modify
    implementation of per-mm counter.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 17bbf093356d..7616bcf107b9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -455,8 +455,7 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 		(current->mm->flags & MMF_INIT_MASK) : default_dump_filter;
 	mm->core_state = NULL;
 	mm->nr_ptes = 0;
-	set_mm_counter(mm, file_rss, 0);
-	set_mm_counter(mm, anon_rss, 0);
+	memset(&mm->rss_stat, 0, sizeof(mm->rss_stat));
 	spin_lock_init(&mm->page_table_lock);
 	mm->free_area_cache = TASK_UNMAPPED_BASE;
 	mm->cached_hole_size = ~0UL;

commit db1466b3e1bd1727375cdbfcbea4bcce2f860f61
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 3 07:46:56 2010 -0800

    rcu: Use wrapper function instead of exporting tasklist_lock
    
    Lockdep-RCU commit d11c563d exported tasklist_lock, which is not
    a good thing.  This patch instead exports a function that uses
    lockdep to check whether tasklist_lock is held.
    
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    Cc: Christoph Hellwig <hch@lst.de>
    LKML-Reference: <1267631219-8713-1-git-send-email-paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 17bbf093356d..8691c540a470 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -86,7 +86,14 @@ int max_threads;		/* tunable limit on nr_threads */
 DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 
 __cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
-EXPORT_SYMBOL_GPL(tasklist_lock);
+
+#ifdef CONFIG_PROVE_RCU
+int lockdep_tasklist_lock_is_held(void)
+{
+	return lockdep_is_held(&tasklist_lock);
+}
+EXPORT_SYMBOL_GPL(lockdep_tasklist_lock_is_held);
+#endif /* #ifdef CONFIG_PROVE_RCU */
 
 int nr_processes(void)
 {

commit d11c563dd20ff35da5652c3e1c989d9e10e1d6d0
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Feb 22 17:04:50 2010 -0800

    sched: Use lockdep-based checking on rcu_dereference()
    
    Update the rcu_dereference() usages to take advantage of the new
    lockdep-based checking.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1266887105-1528-6-git-send-email-paulmck@linux.vnet.ibm.com>
    [ -v2: fix allmodconfig missing symbol export build failure on x86 ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index f88bd984df35..17bbf093356d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -86,6 +86,7 @@ int max_threads;		/* tunable limit on nr_threads */
 DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 
 __cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
+EXPORT_SYMBOL_GPL(tasklist_lock);
 
 int nr_processes(void)
 {

commit fabf318e5e4bda0aca2b0d617b191884fda62703
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jan 21 21:04:57 2010 +0100

    sched: Fix fork vs hotplug vs cpuset namespaces
    
    There are a number of issues:
    
    1) TASK_WAKING vs cgroup_clone (cpusets)
    
    copy_process():
    
      sched_fork()
        child->state = TASK_WAKING; /* waiting for wake_up_new_task() */
      if (current->nsproxy != p->nsproxy)
         ns_cgroup_clone()
           cgroup_clone()
             mutex_lock(inode->i_mutex)
             mutex_lock(cgroup_mutex)
             cgroup_attach_task()
               ss->can_attach()
               ss->attach() [ -> cpuset_attach() ]
                 cpuset_attach_task()
                   set_cpus_allowed_ptr();
                     while (child->state == TASK_WAKING)
                       cpu_relax();
    will deadlock the system.
    
    
    2) cgroup_clone (cpusets) vs copy_process
    
    So even if the above would work we still have:
    
    copy_process():
    
      if (current->nsproxy != p->nsproxy)
         ns_cgroup_clone()
           cgroup_clone()
             mutex_lock(inode->i_mutex)
             mutex_lock(cgroup_mutex)
             cgroup_attach_task()
               ss->can_attach()
               ss->attach() [ -> cpuset_attach() ]
                 cpuset_attach_task()
                   set_cpus_allowed_ptr();
      ...
    
      p->cpus_allowed = current->cpus_allowed
    
    over-writing the modified cpus_allowed.
    
    
    3) fork() vs hotplug
    
      if we unplug the child's cpu after the sanity check when the child
      gets attached to the task_list but before wake_up_new_task() shit
      will meet with fan.
    
    Solve all these issues by moving fork cpu selection into
    wake_up_new_task().
    
    Reported-by: Serge E. Hallyn <serue@us.ibm.com>
    Tested-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1264106190.4283.1314.camel@laptop>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5b2959b3ffc2..f88bd984df35 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1241,21 +1241,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	/* Need tasklist lock for parent etc handling! */
 	write_lock_irq(&tasklist_lock);
 
-	/*
-	 * The task hasn't been attached yet, so its cpus_allowed mask will
-	 * not be changed, nor will its assigned CPU.
-	 *
-	 * The cpus_allowed mask of the parent may have changed after it was
-	 * copied first time - so re-copy it here, then check the child's CPU
-	 * to ensure it is on a valid CPU (and if not, just force it back to
-	 * parent's CPU). This avoids alot of nasty races.
-	 */
-	p->cpus_allowed = current->cpus_allowed;
-	p->rt.nr_cpus_allowed = current->rt.nr_cpus_allowed;
-	if (unlikely(!cpu_isset(task_cpu(p), p->cpus_allowed) ||
-			!cpu_online(task_cpu(p))))
-		set_task_cpu(p, smp_processor_id());
-
 	/* CLONE_PARENT re-uses the old parent */
 	if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
 		p->real_parent = current->real_parent;

commit 9cd80bbb07fcd6d4d037fad4297496d3b132ac6b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Dec 17 15:27:15 2009 -0800

    do_wait() optimization: do not place sub-threads on task_struct->children list
    
    Thanks to Roland who pointed out de_thread() issues.
    
    Currently we add sub-threads to ->real_parent->children list.  This buys
    nothing but slows down do_wait().
    
    With this patch ->children contains only main threads (group leaders).
    The only complication is that forget_original_parent() should iterate over
    sub-threads by hand, and de_thread() needs another list_replace() when it
    changes ->group_leader.
    
    Henceforth do_wait_thread() can never see task_detached() && !EXIT_DEAD
    tasks, we can remove this check (and we can unify do_wait_thread() and
    ptrace_do_wait()).
    
    This change can confuse the optimistic search in mm_update_next_owner(),
    but this is fixable and minor.
    
    Perhaps badness() and oom_kill_process() should be updated, but they
    should be fixed in any case.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Ratan Nalumasu <rnalumasu@gmail.com>
    Cc: Vitaly Mayatskikh <vmayatsk@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 202a0ba63d3c..5b2959b3ffc2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1291,7 +1291,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	}
 
 	if (likely(p->pid)) {
-		list_add_tail(&p->sibling, &p->real_parent->children);
 		tracehook_finish_clone(p, clone_flags, trace);
 
 		if (thread_group_leader(p)) {
@@ -1303,6 +1302,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			p->signal->tty = tty_kref_get(current->signal->tty);
 			attach_pid(p, PIDTYPE_PGID, task_pgrp(current));
 			attach_pid(p, PIDTYPE_SID, task_session(current));
+			list_add_tail(&p->sibling, &p->real_parent->children);
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;
 		}

commit 6580807da14c423f0d0a708108e6df6ebc8bc83d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Dec 15 16:47:16 2009 -0800

    ptrace: copy_process() should disable stepping
    
    If the tracee calls fork() after PTRACE_SINGLESTEP, the forked child
    starts with TIF_SINGLESTEP/X86_EFLAGS_TF bits copied from ptraced parent.
    This is not right, especially when the new child is not auto-attaced: in
    this case it is killed by SIGTRAP.
    
    Change copy_process() to call user_disable_single_step(). Tested on x86.
    
    Test-case:
    
            #include <stdio.h>
            #include <unistd.h>
            #include <signal.h>
            #include <sys/ptrace.h>
            #include <sys/wait.h>
            #include <assert.h>
    
            int main(void)
            {
                    int pid, status;
    
                    if (!(pid = fork())) {
                            assert(ptrace(PTRACE_TRACEME) == 0);
                            kill(getpid(), SIGSTOP);
    
                            if (!fork()) {
                                    /* kernel bug: this child will be killed by SIGTRAP */
                                    printf("Hello world\n");
                                    return 43;
                            }
    
                            wait(&status);
                            return WEXITSTATUS(status);
                    }
    
                    for (;;) {
                            assert(pid == wait(&status));
                            if (WIFEXITED(status))
                                    break;
                            assert(ptrace(PTRACE_SINGLESTEP, pid, 0,0) == 0);
                    }
    
                    assert(WEXITSTATUS(status) == 43);
                    return 0;
            }
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b6cbd33dde80..202a0ba63d3c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1210,9 +1210,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		p->sas_ss_sp = p->sas_ss_size = 0;
 
 	/*
-	 * Syscall tracing should be turned off in the child regardless
-	 * of CLONE_PTRACE.
+	 * Syscall tracing and stepping should be turned off in the
+	 * child regardless of CLONE_PTRACE.
 	 */
+	user_disable_single_step(p);
 	clear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);
 #ifdef TIF_SYSCALL_EMU
 	clear_tsk_thread_flag(p, TIF_SYSCALL_EMU);

commit 569b846df54ffb2827b83ce3244c5f032394cba4
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Dec 15 16:47:03 2009 -0800

    memcg: coalesce uncharge during unmap/truncate
    
    In massive parallel enviroment, res_counter can be a performance
    bottleneck.  One strong techinque to reduce lock contention is reducing
    calls by coalescing some amount of calls into one.
    
    Considering charge/uncharge chatacteristic,
            - charge is done one by one via demand-paging.
            - uncharge is done by
                    - in chunk at munmap, truncate, exit, execve...
                    - one by one via vmscan/paging.
    
    It seems we have a chance to coalesce uncharges for improving scalability
    at unmap/truncation.
    
    This patch is a for coalescing uncharge.  For avoiding scattering memcg's
    structure to functions under /mm, this patch adds memcg batch uncharge
    information to the task.  A reason for per-task batching is for making use
    of caller's context information.  We do batched uncharge (deleyed
    uncharge) when truncation/unmap occurs but do direct uncharge when
    uncharge is called by memory reclaim (vmscan.c).
    
    The degree of coalescing depends on callers
      - at invalidate/trucate... pagevec size
      - at unmap ....ZAP_BLOCK_SIZE
    (memory itself will be freed in this degree.)
    Then, we'll not coalescing too much.
    
    On x86-64 8cpu server, I tested overheads of memcg at page fault by
    running a program which does map/fault/unmap in a loop. Running
    a task per a cpu by taskset and see sum of the number of page faults
    in 60secs.
    
    [without memcg config]
      40156968  page-faults              #      0.085 M/sec   ( +-   0.046% )
      27.67 cache-miss/faults
    [root cgroup]
      36659599  page-faults              #      0.077 M/sec   ( +-   0.247% )
      31.58 miss/faults
    [in a child cgroup]
      18444157  page-faults              #      0.039 M/sec   ( +-   0.133% )
      69.96 miss/faults
    [child with this patch]
      27133719  page-faults              #      0.057 M/sec   ( +-   0.155% )
      47.16 miss/faults
    
    We can see some amounts of improvement.
    (root cgroup doesn't affected by this patch)
    Another patch for "charge" will follow this and above will be improved more.
    
    Changelog(since 2009/10/02):
     - renamed filed of memcg_batch (as pages to bytes, memsw to memsw_bytes)
     - some clean up and commentary/description updates.
     - added initialize code to copy_process(). (possible bug fix)
    
    Changelog(old):
     - fixed !CONFIG_MEM_CGROUP case.
     - rebased onto the latest mmotm + softlimit fix patches.
     - unified patch for callers
     - added commetns.
     - make ->do_batch as bool.
     - removed css_get() at el. We don't need it.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9bd91447e052..b6cbd33dde80 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1127,6 +1127,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_DEBUG_MUTEXES
 	p->blocked_on = NULL; /* not blocked yet */
 #endif
+#ifdef CONFIG_CGROUP_MEM_RES_CTLR
+	p->memcg_batch.do_batch = 0;
+	p->memcg_batch.memcg = NULL;
+#endif
 
 	p->bts = NULL;
 

commit 1d615482547584b9a8bb6316a58fed6ce90dd9ff
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 17 14:54:03 2009 +0100

    sched: Convert pi_lock to raw_spinlock
    
    Convert locks which cannot be sleeping locks in preempt-rt to
    raw_spinlocks.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1415dc4598ae..9bd91447e052 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -939,9 +939,9 @@ SYSCALL_DEFINE1(set_tid_address, int __user *, tidptr)
 
 static void rt_mutex_init_task(struct task_struct *p)
 {
-	spin_lock_init(&p->pi_lock);
+	raw_spin_lock_init(&p->pi_lock);
 #ifdef CONFIG_RT_MUTEXES
-	plist_head_init(&p->pi_waiters, &p->pi_lock);
+	plist_head_init_raw(&p->pi_waiters, &p->pi_lock);
 	p->pi_blocked_on = NULL;
 #endif
 }

commit 6035ccd8e9e40bb654fbfdef325902ab531679a5
Merge: 23eb3b64b5e4 878eaddd05d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 8 08:19:16 2009 -0800

    Merge branch 'for-2.6.33' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.33' of git://git.kernel.dk/linux-2.6-block: (113 commits)
      cfq-iosched: Do not access cfqq after freeing it
      block: include linux/err.h to use ERR_PTR
      cfq-iosched: use call_rcu() instead of doing grace period stall on queue exit
      blkio: Allow CFQ group IO scheduling even when CFQ is a module
      blkio: Implement dynamic io controlling policy registration
      blkio: Export some symbols from blkio as its user CFQ can be a module
      block: Fix io_context leak after failure of clone with CLONE_IO
      block: Fix io_context leak after clone with CLONE_IO
      cfq-iosched: make nonrot check logic consistent
      io controller: quick fix for blk-cgroup and modular CFQ
      cfq-iosched: move IO controller declerations to a header file
      cfq-iosched: fix compile problem with !CONFIG_CGROUP
      blkio: Documentation
      blkio: Wait on sync-noidle queue even if rq_noidle = 1
      blkio: Implement group_isolation tunable
      blkio: Determine async workload length based on total number of queues
      blkio: Wait for cfq queue to get backlogged if group is empty
      blkio: Propagate cgroup weight updation to cfq groups
      blkio: Drop the reference to queue once the task changes cgroup
      blkio: Provide some isolation between groups
      ...

commit ed9216c1717a3f3738a77908aff78995ea69e7ff
Merge: d7fc02c7bae7 d5696725b2a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 8 08:02:38 2009 -0800

    Merge branch 'kvm-updates/2.6.33' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    * 'kvm-updates/2.6.33' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (84 commits)
      KVM: VMX: Fix comparison of guest efer with stale host value
      KVM: s390: Fix prefix register checking in arch/s390/kvm/sigp.c
      KVM: Drop user return notifier when disabling virtualization on a cpu
      KVM: VMX: Disable unrestricted guest when EPT disabled
      KVM: x86 emulator: limit instructions to 15 bytes
      KVM: s390: Make psw available on all exits, not just a subset
      KVM: x86: Add KVM_GET/SET_VCPU_EVENTS
      KVM: VMX: Report unexpected simultaneous exceptions as internal errors
      KVM: Allow internal errors reported to userspace to carry extra data
      KVM: Reorder IOCTLs in main kvm.h
      KVM: x86: Polish exception injection via KVM_SET_GUEST_DEBUG
      KVM: only clear irq_source_id if irqchip is present
      KVM: x86: disallow KVM_{SET,GET}_LAPIC without allocated in-kernel lapic
      KVM: x86: disallow multiple KVM_CREATE_IRQCHIP
      KVM: VMX: Remove vmx->msr_offset_efer
      KVM: MMU: update invlpg handler comment
      KVM: VMX: move CR3/PDPTR update to vmx_set_cr3
      KVM: remove duplicated task_switch check
      KVM: powerpc: Fix BUILD_BUG_ON condition
      KVM: VMX: Use shared msr infrastructure
      ...
    
    Trivial conflicts due to new Kconfig options in arch/Kconfig and kernel/Makefile

commit b69f2292063d2caf37ca9aec7d63ded203701bf3
Author: Louis Rilling <louis.rilling@kerlabs.com>
Date:   Fri Dec 4 14:52:42 2009 +0100

    block: Fix io_context leak after failure of clone with CLONE_IO
    
    With CLONE_IO, parent's io_context->nr_tasks is incremented, but never
    decremented whenever copy_process() fails afterwards, which prevents
    exit_io_context() from calling IO schedulers exit functions.
    
    Give a task_struct to exit_io_context(), and call exit_io_context() instead of
    put_io_context() in copy_process() cleanup path.
    
    Signed-off-by: Louis Rilling <louis.rilling@kerlabs.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 166b8c49257c..607353425bb0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1310,7 +1310,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (pid != &init_struct_pid)
 		free_pid(pid);
 bad_fork_cleanup_io:
-	put_io_context(p->io_context);
+	if (p->io_context)
+		exit_io_context(p);
 bad_fork_cleanup_namespaces:
 	exit_task_namespaces(p);
 bad_fork_cleanup_mm:

commit 58988b07cfe2ebe9f9b39d973fd8a083390e749f
Merge: 22763c5cf369 8e7cac79808b
Author: Avi Kivity <avi@redhat.com>
Date:   Thu Dec 3 09:30:06 2009 +0200

    Merge remote branch 'tip/x86/entry' into kvm-updates/2.6.33
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

commit 0cf55e1ec08bb5a22e068309e2d8ba1180ab4239
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Wed Dec 2 17:28:07 2009 +0900

    sched, cputime: Introduce thread_group_times()
    
    This is a real fix for problem of utime/stime values decreasing
    described in the thread:
    
       http://lkml.org/lkml/2009/11/3/522
    
    Now cputime is accounted in the following way:
    
     - {u,s}time in task_struct are increased every time when the thread
       is interrupted by a tick (timer interrupt).
    
     - When a thread exits, its {u,s}time are added to signal->{u,s}time,
       after adjusted by task_times().
    
     - When all threads in a thread_group exits, accumulated {u,s}time
       (and also c{u,s}time) in signal struct are added to c{u,s}time
       in signal struct of the group's parent.
    
    So {u,s}time in task struct are "raw" tick count, while
    {u,s}time and c{u,s}time in signal struct are "adjusted" values.
    
    And accounted values are used by:
    
     - task_times(), to get cputime of a thread:
       This function returns adjusted values that originates from raw
       {u,s}time and scaled by sum_exec_runtime that accounted by CFS.
    
     - thread_group_cputime(), to get cputime of a thread group:
       This function returns sum of all {u,s}time of living threads in
       the group, plus {u,s}time in the signal struct that is sum of
       adjusted cputimes of all exited threads belonged to the group.
    
    The problem is the return value of thread_group_cputime(),
    because it is mixed sum of "raw" value and "adjusted" value:
    
      group's {u,s}time = foreach(thread){{u,s}time} + exited({u,s}time)
    
    This misbehavior can break {u,s}time monotonicity.
    Assume that if there is a thread that have raw values greater
    than adjusted values (e.g. interrupted by 1000Hz ticks 50 times
    but only runs 45ms) and if it exits, cputime will decrease (e.g.
    -5ms).
    
    To fix this, we could do:
    
      group's {u,s}time = foreach(t){task_times(t)} + exited({u,s}time)
    
    But task_times() contains hard divisions, so applying it for
    every thread should be avoided.
    
    This patch fixes the above problem in the following way:
    
     - Modify thread's exit (= __exit_signal()) not to use task_times().
       It means {u,s}time in signal struct accumulates raw values instead
       of adjusted values.  As the result it makes thread_group_cputime()
       to return pure sum of "raw" values.
    
     - Introduce a new function thread_group_times(*task, *utime, *stime)
       that converts "raw" values of thread_group_cputime() to "adjusted"
       values, in same calculation procedure as task_times().
    
     - Modify group's exit (= wait_task_zombie()) to use this introduced
       thread_group_times().  It make c{u,s}time in signal struct to
       have adjusted values like before this patch.
    
     - Replace some thread_group_cputime() by thread_group_times().
       This replacements are only applied where conveys the "adjusted"
       cputime to users, and where already uses task_times() near by it.
       (i.e. sys_times(), getrusage(), and /proc/<PID>/stat.)
    
    This patch have a positive side effect:
    
     - Before this patch, if a group contains many short-life threads
       (e.g. runs 0.9ms and not interrupted by ticks), the group's
       cputime could be invisible since thread's cputime was accumulated
       after adjusted: imagine adjustment function as adj(ticks, runtime),
         {adj(0, 0.9) + adj(0, 0.9) + ....} = {0 + 0 + ....} = 0.
       After this patch it will not happen because the adjustment is
       applied after accumulated.
    
    v2:
     - remove if()s, put new variables into signal_struct.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Spencer Candland <spencer@bluehost.com>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    LKML-Reference: <4B162517.8040909@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index ad7cb6d1193c..3d6f121bbe8a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -884,6 +884,9 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;
 	sig->gtime = cputime_zero;
 	sig->cgtime = cputime_zero;
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING
+	sig->prev_utime = sig->prev_stime = cputime_zero;
+#endif
 	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
 	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
 	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;

commit d99ca3b977fc5a93141304f571475c2af9e6c1c5
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Wed Dec 2 17:26:47 2009 +0900

    sched, cputime: Cleanups related to task_times()
    
    - Remove if({u,s}t)s because no one call it with NULL now.
    - Use cputime_{add,sub}().
    - Add ifndef-endif for prev_{u,s}time since they are used
      only when !VIRT_CPU_ACCOUNTING.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Spencer Candland <spencer@bluehost.com>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    LKML-Reference: <4B1624C7.7040302@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 166b8c49257c..ad7cb6d1193c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1066,8 +1066,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->gtime = cputime_zero;
 	p->utimescaled = cputime_zero;
 	p->stimescaled = cputime_zero;
+#ifndef CONFIG_VIRT_CPU_ACCOUNTING
 	p->prev_utime = cputime_zero;
 	p->prev_stime = cputime_zero;
+#endif
 
 	p->default_timer_slack_ns = current->timer_slack_ns;
 

commit 8e7cac79808b62f242069a6ac88d364d35621371
Author: Avi Kivity <avi@redhat.com>
Date:   Sun Nov 29 16:34:48 2009 +0200

    core: Fix user return notifier on fork()
    
    fork() clones all thread_info flags, including
    TIF_USER_RETURN_NOTIFY; if the new task is first scheduled on a cpu
    which doesn't have user return notifiers set, this causes user
    return notifiers to trigger without any way of clearing itself.
    
    This is easy to trigger with a forky workload on the host in
    parallel with kvm, resulting in a cpu in an endless loop on the
    verge of returning to userspace.
    
    Fix by dropping the TIF_USER_RETURN_NOTIFY immediately after fork.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>
    LKML-Reference: <1259505288-16559-1-git-send-email-avi@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 266c6af6ef1b..1b7512d5a64a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -64,6 +64,7 @@
 #include <linux/magic.h>
 #include <linux/perf_event.h>
 #include <linux/posix-timers.h>
+#include <linux/user-return-notifier.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -249,6 +250,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 		goto out;
 
 	setup_thread_stack(tsk, orig);
+	clear_user_return_notifier(tsk);
 	stackend = end_of_stack(tsk);
 	*stackend = STACK_END_MAGIC;	/* for overflow detection */
 

commit 1d510750941a53a1d3049c1d33c75d6dfcd78618
Author: Ian Campbell <Ian.Campbell@citrix.com>
Date:   Tue Nov 3 10:11:14 2009 +0000

    Correct nr_processes() when CPUs have been unplugged
    
    nr_processes() returns the sum of the per cpu counter process_counts for
    all online CPUs. This counter is incremented for the current CPU on
    fork() and decremented for the current CPU on exit(). Since a process
    does not necessarily fork and exit on the same CPU the process_count for
    an individual CPU can be either positive or negative and effectively has
    no meaning in isolation.
    
    Therefore calculating the sum of process_counts over only the online
    CPUs omits the processes which were started or stopped on any CPU which
    has since been unplugged. Only the sum of process_counts across all
    possible CPUs has meaning.
    
    The only caller of nr_processes() is proc_root_getattr() which
    calculates the number of links to /proc as
            stat->nlink = proc_root.nlink + nr_processes();
    
    You don't have to be all that unlucky for the nr_processes() to return a
    negative value leading to a negative number of links (or rather, an
    apparently enormous number of links). If this happens then you can get
    failures where things like "ls /proc" start to fail because they got an
    -EOVERFLOW from some stat() call.
    
    Example with some debugging inserted to show what goes on:
            # ps haux|wc -l
            nr_processes: CPU0:     90
            nr_processes: CPU1:     1030
            nr_processes: CPU2:     -900
            nr_processes: CPU3:     -136
            nr_processes: TOTAL:    84
            proc_root_getattr. nlink 12 + nr_processes() 84 = 96
            84
            # echo 0 >/sys/devices/system/cpu/cpu1/online
            # ps haux|wc -l
            nr_processes: CPU0:     85
            nr_processes: CPU2:     -901
            nr_processes: CPU3:     -137
            nr_processes: TOTAL:    -953
            proc_root_getattr. nlink 12 + nr_processes() -953 = -941
            75
            # stat /proc/
            nr_processes: CPU0:     84
            nr_processes: CPU2:     -901
            nr_processes: CPU3:     -137
            nr_processes: TOTAL:    -954
            proc_root_getattr. nlink 12 + nr_processes() -954 = -942
              File: `/proc/'
              Size: 0               Blocks: 0          IO Block: 1024   directory
            Device: 3h/3d   Inode: 1           Links: 4294966354
            Access: (0555/dr-xr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)
            Access: 2009-11-03 09:06:55.000000000 +0000
            Modify: 2009-11-03 09:06:55.000000000 +0000
            Change: 2009-11-03 09:06:55.000000000 +0000
    
    I'm not 100% convinced that the per_cpu regions remain valid for offline
    CPUs, although my testing suggests that they do. If not then I think the
    correct solution would be to aggregate the process_count for a given CPU
    into a global base value in cpu_down().
    
    This bug appears to pre-date the transition to git and it looks like it
    may even have been present in linux-2.6.0-test7-bk3 since it looks like
    the code Rusty patched in http://lwn.net/Articles/64773/ was already
    wrong.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4c20fff8c13a..166b8c49257c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -91,7 +91,7 @@ int nr_processes(void)
 	int cpu;
 	int total = 0;
 
-	for_each_online_cpu(cpu)
+	for_each_possible_cpu(cpu)
 		total += per_cpu(process_counts, cpu);
 
 	return total;

commit f579bbcd9bb8b688df03191b92c56ab8af4d6322
Merge: e80fb7e52fd3 da085681014f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 8 12:16:35 2009 -0700

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      futex: fix requeue_pi key imbalance
      futex: Fix typo in FUTEX_WAIT/WAKE_BITSET_PRIVATE definitions
      rcu: Place root rcu_node structure in separate lockdep class
      rcu: Make hot-unplugged CPU relinquish its own RCU callbacks
      rcu: Move rcu_barrier() to rcutree
      futex: Move exit_pi_state() call to release_mm()
      futex: Nullify robust lists after cleanup
      futex: Fix locking imbalance
      panic: Fix panic message visibility by calling bust_spinlocks(0) before dying
      rcu: Replace the rcu_barrier enum with pointer to call_rcu*() function
      rcu: Clean up code based on review feedback from Josh Triplett, part 4
      rcu: Clean up code based on review feedback from Josh Triplett, part 3
      rcu: Fix rcu_lock_map build failure on CONFIG_PROVE_LOCKING=y
      rcu: Clean up code to address Ingo's checkpatch feedback
      rcu: Clean up code based on review feedback from Josh Triplett, part 2
      rcu: Clean up code based on review feedback from Josh Triplett

commit 322a2c100a8998158445599ea437fb556aa95b11
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Oct 5 18:18:03 2009 +0200

    futex: Move exit_pi_state() call to release_mm()
    
    exit_pi_state() is called from do_exit() but not from do_execve().
    Move it to release_mm() so it gets called from do_execve() as well.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <new-submission>
    Cc: stable@kernel.org
    Cc: Anirban Sinha <ani@anirban.org>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 88ef51ca84de..341965b0ab1c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -553,6 +553,8 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 		tsk->compat_robust_list = NULL;
 	}
 #endif
+	if (unlikely(!list_empty(&tsk->pi_state_list)))
+		exit_pi_state_list(tsk);
 #endif
 
 	/* Get rid of any cached register state */

commit fc6b177dee33365ccb29fe6d2092223cf8d679f9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 5 18:17:32 2009 +0200

    futex: Nullify robust lists after cleanup
    
    The robust list pointers of user space held futexes are kept intact
    over an exec() call. When the exec'ed task exits exit_robust_list() is
    called with the stale pointer. The risk of corruption is minimal, but
    still it is incorrect to keep the pointers valid. Actually glibc
    should uninstall the robust list before calling exec() but we have to
    deal with it anyway.
    
    Nullify the pointers after [compat_]exit_robust_list() has been
    called.
    
    Reported-by: Anirban Sinha <ani@anirban.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <new-submission>
    Cc: stable@kernel.org

diff --git a/kernel/fork.c b/kernel/fork.c
index bfee931ee3fb..88ef51ca84de 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -543,11 +543,15 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 
 	/* Get rid of any futexes when releasing the mm */
 #ifdef CONFIG_FUTEX
-	if (unlikely(tsk->robust_list))
+	if (unlikely(tsk->robust_list)) {
 		exit_robust_list(tsk);
+		tsk->robust_list = NULL;
+	}
 #ifdef CONFIG_COMPAT
-	if (unlikely(tsk->compat_robust_list))
+	if (unlikely(tsk->compat_robust_list)) {
 		compat_exit_robust_list(tsk);
+		tsk->compat_robust_list = NULL;
+	}
 #endif
 #endif
 

commit 801460d0cf5c5288153b722565773059b0f44348
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Wed Sep 23 15:57:41 2009 -0700

    task_struct cleanup: move binfmt field to mm_struct
    
    Because the binfmt is not different between threads in the same process,
    it can be moved from task_struct to mm_struct.  And binfmt moudle is
    handled per mm_struct instead of task_struct.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e49f181ba1ca..266c6af6ef1b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -518,6 +518,8 @@ void mmput(struct mm_struct *mm)
 			spin_unlock(&mmlist_lock);
 		}
 		put_swap_token(mm);
+		if (mm->binfmt)
+			module_put(mm->binfmt->module);
 		mmdrop(mm);
 	}
 }
@@ -643,9 +645,14 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	mm->hiwater_rss = get_mm_rss(mm);
 	mm->hiwater_vm = mm->total_vm;
 
+	if (mm->binfmt && !try_module_get(mm->binfmt->module))
+		goto free_pt;
+
 	return mm;
 
 free_pt:
+	/* don't put binfmt in mmput, we haven't got module yet */
+	mm->binfmt = NULL;
 	mmput(mm);
 
 fail_nomem:
@@ -1037,9 +1044,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (!try_module_get(task_thread_info(p)->exec_domain->module))
 		goto bad_fork_cleanup_count;
 
-	if (p->binfmt && !try_module_get(p->binfmt->module))
-		goto bad_fork_cleanup_put_domain;
-
 	p->did_exec = 0;
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	copy_flags(clone_flags, p);
@@ -1327,9 +1331,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 	cgroup_exit(p, cgroup_callbacks_done);
 	delayacct_tsk_free(p);
-	if (p->binfmt)
-		module_put(p->binfmt->module);
-bad_fork_cleanup_put_domain:
 	module_put(task_thread_info(p)->exec_domain->module);
 bad_fork_cleanup_count:
 	atomic_dec(&p->cred->user->processes);

commit 858f09930b32c11b40fd0c5c467982ba09b10894
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Sep 23 15:57:32 2009 -0700

    aio: ifdef fields in mm_struct
    
    ->ioctx_lock and ->ioctx_list are used only under CONFIG_AIO.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Zach Brown <zach.brown@oracle.com>
    Cc: Benjamin LaHaise <bcrl@kvack.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b51fd2ccb2f1..e49f181ba1ca 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -434,6 +434,14 @@ __setup("coredump_filter=", coredump_filter_setup);
 
 #include <linux/init_task.h>
 
+static void mm_init_aio(struct mm_struct *mm)
+{
+#ifdef CONFIG_AIO
+	spin_lock_init(&mm->ioctx_lock);
+	INIT_HLIST_HEAD(&mm->ioctx_list);
+#endif
+}
+
 static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 {
 	atomic_set(&mm->mm_users, 1);
@@ -447,10 +455,9 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 	set_mm_counter(mm, file_rss, 0);
 	set_mm_counter(mm, anon_rss, 0);
 	spin_lock_init(&mm->page_table_lock);
-	spin_lock_init(&mm->ioctx_lock);
-	INIT_HLIST_HEAD(&mm->ioctx_list);
 	mm->free_area_cache = TASK_UNMAPPED_BASE;
 	mm->cached_hole_size = ~0UL;
+	mm_init_aio(mm);
 	mm_init_owner(mm, p);
 
 	if (likely(!mm_alloc_pgd(mm))) {

commit 123be07b0b399670a7cc3d82fef0cb4f93ef885c
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Wed Sep 23 15:57:20 2009 -0700

    fork(): disable CLONE_PARENT for init
    
    When global or container-init processes use CLONE_PARENT, they create a
    multi-rooted process tree.  Besides siblings of global init remain as
    zombies on exit since they are not reaped by their parent (swapper).  So
    prevent global and container-inits from creating siblings.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Acked-by: Eric W. Biederman <ebiederm@xmission.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Oren Laadan <orenl@cs.columbia.edu>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 51ad0b0b7266..b51fd2ccb2f1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -979,6 +979,16 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))
 		return ERR_PTR(-EINVAL);
 
+	/*
+	 * Siblings of global init remain as zombies on exit since they are
+	 * not reaped by their parent (swapper). To solve this and to avoid
+	 * multi-rooted process trees, prevent global and container-inits
+	 * from creating siblings.
+	 */
+	if ((clone_flags & CLONE_PARENT) &&
+				current->signal->flags & SIGNAL_UNKILLABLE)
+		return ERR_PTR(-EINVAL);
+
 	retval = security_task_create(clone_flags);
 	if (retval)
 		goto fork_out;

commit 31bbb9b58d1e8ebcf2b28c95c2250a9f8e31e397
Merge: ff830b8e5f99 3f0a525ebf4b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 23 09:46:15 2009 -0700

    Merge branch 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      itimers: Add tracepoints for itimer
      hrtimer: Add tracepoint for hrtimers
      timers: Add tracepoints for timer_list timers
      cputime: Optimize jiffies_to_cputime(1)
      itimers: Simplify arm_timer() code a bit
      itimers: Fix periodic tics precision
      itimers: Merge ITIMER_VIRT and ITIMER_PROF
    
    Trivial header file include conflicts in kernel/fork.c

commit d899bf7b55f503ba7d3d07ed27c3a37e270fa7db
Author: Stefani Seibold <stefani@seibold.net>
Date:   Tue Sep 22 16:45:40 2009 -0700

    procfs: provide stack information for threads
    
    A patch to give a better overview of the userland application stack usage,
    especially for embedded linux.
    
    Currently you are only able to dump the main process/thread stack usage
    which is showed in /proc/pid/status by the "VmStk" Value.  But you get no
    information about the consumed stack memory of the the threads.
    
    There is an enhancement in the /proc/<pid>/{task/*,}/*maps and which marks
    the vm mapping where the thread stack pointer reside with "[thread stack
    xxxxxxxx]".  xxxxxxxx is the maximum size of stack.  This is a value
    information, because libpthread doesn't set the start of the stack to the
    top of the mapped area, depending of the pthread usage.
    
    A sample output of /proc/<pid>/task/<tid>/maps looks like:
    
    08048000-08049000 r-xp 00000000 03:00 8312       /opt/z
    08049000-0804a000 rw-p 00001000 03:00 8312       /opt/z
    0804a000-0806b000 rw-p 00000000 00:00 0          [heap]
    a7d12000-a7d13000 ---p 00000000 00:00 0
    a7d13000-a7f13000 rw-p 00000000 00:00 0          [thread stack: 001ff4b4]
    a7f13000-a7f14000 ---p 00000000 00:00 0
    a7f14000-a7f36000 rw-p 00000000 00:00 0
    a7f36000-a8069000 r-xp 00000000 03:00 4222       /lib/libc.so.6
    a8069000-a806b000 r--p 00133000 03:00 4222       /lib/libc.so.6
    a806b000-a806c000 rw-p 00135000 03:00 4222       /lib/libc.so.6
    a806c000-a806f000 rw-p 00000000 00:00 0
    a806f000-a8083000 r-xp 00000000 03:00 14462      /lib/libpthread.so.0
    a8083000-a8084000 r--p 00013000 03:00 14462      /lib/libpthread.so.0
    a8084000-a8085000 rw-p 00014000 03:00 14462      /lib/libpthread.so.0
    a8085000-a8088000 rw-p 00000000 00:00 0
    a8088000-a80a4000 r-xp 00000000 03:00 8317       /lib/ld-linux.so.2
    a80a4000-a80a5000 r--p 0001b000 03:00 8317       /lib/ld-linux.so.2
    a80a5000-a80a6000 rw-p 0001c000 03:00 8317       /lib/ld-linux.so.2
    afaf5000-afb0a000 rw-p 00000000 00:00 0          [stack]
    ffffe000-fffff000 r-xp 00000000 00:00 0          [vdso]
    
    Also there is a new entry "stack usage" in /proc/<pid>/{task/*,}/status
    which will you give the current stack usage in kb.
    
    A sample output of /proc/self/status looks like:
    
    Name:   cat
    State:  R (running)
    Tgid:   507
    Pid:    507
    .
    .
    .
    CapBnd: fffffffffffffeff
    voluntary_ctxt_switches:        0
    nonvoluntary_ctxt_switches:     0
    Stack usage:    12 kB
    
    I also fixed stack base address in /proc/<pid>/{task/*,}/stat to the base
    address of the associated thread stack and not the one of the main
    process.  This makes more sense.
    
    [akpm@linux-foundation.org: fs/proc/array.c now needs walk_page_range()]
    Signed-off-by: Stefani Seibold <stefani@seibold.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7cf45812ce84..8f45b0ebdda7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1095,6 +1095,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->bts = NULL;
 
+	p->stack_start = stack_start;
+
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p, clone_flags);
 

commit 1f10206cf8e945220f7220a809d8bfc15c21f9a5
Author: Jiri Pirko <jpirko@redhat.com>
Date:   Tue Sep 22 16:44:10 2009 -0700

    getrusage: fill ru_maxrss value
    
    Make ->ru_maxrss value in struct rusage filled accordingly to rss hiwater
    mark.  This struct is filled as a parameter to getrusage syscall.
    ->ru_maxrss value is set to KBs which is the way it is done in BSD
    systems.  /usr/bin/time (gnu time) application converts ->ru_maxrss to KBs
    which seems to be incorrect behavior.  Maintainer of this util was
    notified by me with the patch which corrects it and cc'ed.
    
    To make this happen we extend struct signal_struct by two fields.  The
    first one is ->maxrss which we use to store rss hiwater of the task.  The
    second one is ->cmaxrss which we use to store highest rss hiwater of all
    task childs.  These values are used in k_getrusage() to actually fill
    ->ru_maxrss.  k_getrusage() uses current rss hiwater value directly if mm
    struct exists.
    
    Note:
    exec() clear mm->hiwater_rss, but doesn't clear sig->maxrss.
    it is intetionally behavior. *BSD getrusage have exec() inheriting.
    
    test programs
    ========================================================
    
    getrusage.c
    ===========
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
     #include <sys/types.h>
     #include <sys/time.h>
     #include <sys/resource.h>
     #include <sys/types.h>
     #include <sys/wait.h>
     #include <unistd.h>
     #include <signal.h>
     #include <sys/mman.h>
    
     #include "common.h"
    
     #define err(str) perror(str), exit(1)
    
    int main(int argc, char** argv)
    {
            int status;
    
            printf("allocate 100MB\n");
            consume(100);
    
            printf("testcase1: fork inherit? \n");
            printf("  expect: initial.self ~= child.self\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
            } else {
                    show_rusage("fork child");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase2: fork inherit? (cont.) \n");
            printf("  expect: initial.children ~= 100MB, but child.children = 0\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
            } else {
                    show_rusage("child");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase3: fork + malloc \n");
            printf("  expect: child.self ~= initial.self + 50MB\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
            } else {
                    printf("allocate +50MB\n");
                    consume(50);
                    show_rusage("fork child");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase4: grandchild maxrss\n");
            printf("  expect: post_wait.children ~= 300MB\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
                    show_rusage("post_wait");
            } else {
                    system("./child -n 0 -g 300");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase5: zombie\n");
            printf("  expect: pre_wait ~= initial, IOW the zombie process is not accounted.\n");
            printf("          post_wait ~= 400MB, IOW wait() collect child's max_rss. \n");
            show_rusage("initial");
            if (__fork()) {
                    sleep(1); /* children become zombie */
                    show_rusage("pre_wait");
                    wait(&status);
                    show_rusage("post_wait");
            } else {
                    system("./child -n 400");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase6: SIG_IGN\n");
            printf("  expect: initial ~= after_zombie (child's 500MB alloc should be ignored).\n");
            show_rusage("initial");
            signal(SIGCHLD, SIG_IGN);
            if (__fork()) {
                    sleep(1); /* children become zombie */
                    show_rusage("after_zombie");
            } else {
                    system("./child -n 500");
                    _exit(0);
            }
            printf("\n");
            signal(SIGCHLD, SIG_DFL);
    
            printf("testcase7: exec (without fork) \n");
            printf("  expect: initial ~= exec \n");
            show_rusage("initial");
            execl("./child", "child", "-v", NULL);
    
            return 0;
    }
    
    child.c
    =======
     #include <sys/types.h>
     #include <unistd.h>
     #include <sys/types.h>
     #include <sys/wait.h>
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
     #include <sys/types.h>
     #include <sys/time.h>
     #include <sys/resource.h>
    
     #include "common.h"
    
    int main(int argc, char** argv)
    {
            int status;
            int c;
            long consume_size = 0;
            long grandchild_consume_size = 0;
            int show = 0;
    
            while ((c = getopt(argc, argv, "n:g:v")) != -1) {
                    switch (c) {
                    case 'n':
                            consume_size = atol(optarg);
                            break;
                    case 'v':
                            show = 1;
                            break;
                    case 'g':
    
                            grandchild_consume_size = atol(optarg);
                            break;
                    default:
                            break;
                    }
            }
    
            if (show)
                    show_rusage("exec");
    
            if (consume_size) {
                    printf("child alloc %ldMB\n", consume_size);
                    consume(consume_size);
            }
    
            if (grandchild_consume_size) {
                    if (fork()) {
                            wait(&status);
                    } else {
                            printf("grandchild alloc %ldMB\n", grandchild_consume_size);
                            consume(grandchild_consume_size);
    
                            exit(0);
                    }
            }
    
            return 0;
    }
    
    common.c
    ========
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
     #include <sys/types.h>
     #include <sys/time.h>
     #include <sys/resource.h>
     #include <sys/types.h>
     #include <sys/wait.h>
     #include <unistd.h>
     #include <signal.h>
     #include <sys/mman.h>
    
     #include "common.h"
     #define err(str) perror(str), exit(1)
    
    void show_rusage(char *prefix)
    {
            int err, err2;
            struct rusage rusage_self;
            struct rusage rusage_children;
    
            printf("%s: ", prefix);
            err = getrusage(RUSAGE_SELF, &rusage_self);
            if (!err)
                    printf("self %ld ", rusage_self.ru_maxrss);
            err2 = getrusage(RUSAGE_CHILDREN, &rusage_children);
            if (!err2)
                    printf("children %ld ", rusage_children.ru_maxrss);
    
            printf("\n");
    }
    
    /* Some buggy OS need this worthless CPU waste. */
    void make_pagefault(void)
    {
            void *addr;
            int size = getpagesize();
            int i;
    
            for (i=0; i<1000; i++) {
                    addr = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANON, -1, 0);
                    if (addr == MAP_FAILED)
                            err("make_pagefault");
                    memset(addr, 0, size);
                    munmap(addr, size);
            }
    }
    
    void consume(int mega)
    {
            size_t sz = mega * 1024 * 1024;
            void *ptr;
    
            ptr = malloc(sz);
            memset(ptr, 0, sz);
            make_pagefault();
    }
    
    pid_t __fork(void)
    {
            pid_t pid;
    
            pid = fork();
            make_pagefault();
    
            return pid;
    }
    
    common.h
    ========
    void show_rusage(char *prefix);
    void make_pagefault(void);
    void consume(int mega);
    pid_t __fork(void);
    
    FreeBSD result (expected result)
    ========================================================
    allocate 100MB
    testcase1: fork inherit?
      expect: initial.self ~= child.self
    initial: self 103492 children 0
    fork child: self 103540 children 0
    
    testcase2: fork inherit? (cont.)
      expect: initial.children ~= 100MB, but child.children = 0
    initial: self 103540 children 103540
    child: self 103564 children 0
    
    testcase3: fork + malloc
      expect: child.self ~= initial.self + 50MB
    initial: self 103564 children 103564
    allocate +50MB
    fork child: self 154860 children 0
    
    testcase4: grandchild maxrss
      expect: post_wait.children ~= 300MB
    initial: self 103564 children 154860
    grandchild alloc 300MB
    post_wait: self 103564 children 308720
    
    testcase5: zombie
      expect: pre_wait ~= initial, IOW the zombie process is not accounted.
              post_wait ~= 400MB, IOW wait() collect child's max_rss.
    initial: self 103564 children 308720
    child alloc 400MB
    pre_wait: self 103564 children 308720
    post_wait: self 103564 children 411312
    
    testcase6: SIG_IGN
      expect: initial ~= after_zombie (child's 500MB alloc should be ignored).
    initial: self 103564 children 411312
    child alloc 500MB
    after_zombie: self 103624 children 411312
    
    testcase7: exec (without fork)
      expect: initial ~= exec
    initial: self 103624 children 411312
    exec: self 103624 children 411312
    
    Linux result (actual test result)
    ========================================================
    allocate 100MB
    testcase1: fork inherit?
      expect: initial.self ~= child.self
    initial: self 102848 children 0
    fork child: self 102572 children 0
    
    testcase2: fork inherit? (cont.)
      expect: initial.children ~= 100MB, but child.children = 0
    initial: self 102876 children 102644
    child: self 102572 children 0
    
    testcase3: fork + malloc
      expect: child.self ~= initial.self + 50MB
    initial: self 102876 children 102644
    allocate +50MB
    fork child: self 153804 children 0
    
    testcase4: grandchild maxrss
      expect: post_wait.children ~= 300MB
    initial: self 102876 children 153864
    grandchild alloc 300MB
    post_wait: self 102876 children 307536
    
    testcase5: zombie
      expect: pre_wait ~= initial, IOW the zombie process is not accounted.
              post_wait ~= 400MB, IOW wait() collect child's max_rss.
    initial: self 102876 children 307536
    child alloc 400MB
    pre_wait: self 102876 children 307536
    post_wait: self 102876 children 410076
    
    testcase6: SIG_IGN
      expect: initial ~= after_zombie (child's 500MB alloc should be ignored).
    initial: self 102876 children 410076
    child alloc 500MB
    after_zombie: self 102880 children 410076
    
    testcase7: exec (without fork)
      expect: initial ~= exec
    initial: self 102880 children 410076
    exec: self 102880 children 410076
    
    Signed-off-by: Jiri Pirko <jpirko@redhat.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1020977b57ca..7cf45812ce84 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -866,6 +866,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
 	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
 	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;
+	sig->maxrss = sig->cmaxrss = 0;
 	task_io_accounting_init(&sig->ioac);
 	sig->sum_sched_runtime = 0;
 	taskstats_tgid_init(sig);

commit 28b83c5193e7ab951e402252278f2cc79dc4d298
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Mon Sep 21 17:03:13 2009 -0700

    oom: move oom_adj value from task_struct to signal_struct
    
    Currently, OOM logic callflow is here.
    
        __out_of_memory()
            select_bad_process()            for each task
                badness()                   calculate badness of one task
                    oom_kill_process()      search child
                        oom_kill_task()     kill target task and mm shared tasks with it
    
    example, process-A have two thread, thread-A and thread-B and it have very
    fat memory and each thread have following oom_adj and oom_score.
    
         thread-A: oom_adj = OOM_DISABLE, oom_score = 0
         thread-B: oom_adj = 0,           oom_score = very-high
    
    Then, select_bad_process() select thread-B, but oom_kill_task() refuse
    kill the task because thread-A have OOM_DISABLE.  Thus __out_of_memory()
    call select_bad_process() again.  but select_bad_process() select the same
    task.  It mean kernel fall in livelock.
    
    The fact is, select_bad_process() must select killable task.  otherwise
    OOM logic go into livelock.
    
    And root cause is, oom_adj shouldn't be per-thread value.  it should be
    per-process value because OOM-killer kill a process, not thread.  Thus
    This patch moves oomkilladj (now more appropriately named oom_adj) from
    struct task_struct to struct signal_struct.  it naturally prevent
    select_bad_process() choose wrong task.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Paul Menage <menage@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 73a442b7be6d..1020977b57ca 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -880,6 +880,8 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 
 	tty_audit_fork(sig);
 
+	sig->oom_adj = current->signal->oom_adj;
+
 	return 0;
 }
 

commit 1c2fb7a4c2ca7a958b02bc1e615d0254990bba8d
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Mon Sep 21 17:02:22 2009 -0700

    ksm: fix deadlock with munlock in exit_mmap
    
    Rawhide users have reported hang at startup when cryptsetup is run: the
    same problem can be simply reproduced by running a program int main() {
    mlockall(MCL_CURRENT | MCL_FUTURE); return 0; }
    
    The problem is that exit_mmap() applies munlock_vma_pages_all() to
    clean up VM_LOCKED areas, and its current implementation (stupidly)
    tries to fault in absent pages, for example where PROT_NONE prevented
    them being faulted in when mlocking.  Whereas the "ksm: fix oom
    deadlock" patch, knowing there's a race by which KSM might try to fault
    in pages after exit_mmap() had finally zapped the range, backs out of
    such faults doing nothing when its ksm_test_exit() notices mm_users 0.
    
    So revert that part of "ksm: fix oom deadlock" which moved the
    ksm_exit() call from before exit_mmap() to the middle of exit_mmap();
    and remove those ksm_test_exit() checks from the page fault paths, so
    allowing the munlocking to proceed without interference.
    
    ksm_exit, if there are rmap_items still chained on this mm slot, takes
    mmap_sem write side: so preventing KSM from working on an mm while
    exit_mmap runs.  And KSM will bail out as soon as it notices that
    mm_users is already zero, thanks to its internal ksm_test_exit checks.
    So that when a task is killed by OOM killer or the user, KSM will not
    indefinitely prevent it from running exit_mmap to release its memory.
    
    This does break a part of what "ksm: fix oom deadlock" was trying to
    achieve.  When unmerging KSM (echo 2 >/sys/kernel/mm/ksm), and even
    when ksmd itself has to cancel a KSM page, it is possible that the
    first OOM-kill victim would be the KSM process being faulted: then its
    memory won't be freed until a second victim has been selected (freeing
    memory for the unmerging fault to complete).
    
    But the OOM killer is already liable to kill a second victim once the
    intended victim's p->mm goes to NULL: so there's not much point in
    rejecting this KSM patch before fixing that OOM behaviour.  It is very
    much more important to allow KSM users to boot up, than to haggle over
    an unlikely and poorly supported OOM case.
    
    We also intend to fix munlocking to not fault pages: at which point
    this patch _could_ be reverted; though that would be controversial, so
    we hope to find a better solution.
    
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Justin M. Forbes <jforbes@redhat.com>
    Acked-for-now-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Izik Eidus <ieidus@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 42f20f565b16..73a442b7be6d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -501,6 +501,7 @@ void mmput(struct mm_struct *mm)
 
 	if (atomic_dec_and_test(&mm->mm_users)) {
 		exit_aio(mm);
+		ksm_exit(mm);
 		exit_mmap(mm);
 		set_mm_exe_file(mm, NULL);
 		if (!list_empty(&mm->mmlist)) {

commit 9ba6929480088a85c1ff60a4b1f1c9fc80dbd2b7
Author: Hugh Dickins <hugh.dickins@tiscali.co.uk>
Date:   Mon Sep 21 17:02:20 2009 -0700

    ksm: fix oom deadlock
    
    There's a now-obvious deadlock in KSM's out-of-memory handling:
    imagine ksmd or KSM_RUN_UNMERGE handling, holding ksm_thread_mutex,
    trying to allocate a page to break KSM in an mm which becomes the
    OOM victim (quite likely in the unmerge case): it's killed and goes
    to exit, and hangs there waiting to acquire ksm_thread_mutex.
    
    Clearly we must not require ksm_thread_mutex in __ksm_exit, simple
    though that made everything else: perhaps use mmap_sem somehow?
    And part of the answer lies in the comments on unmerge_ksm_pages:
    __ksm_exit should also leave all the rmap_item removal to ksmd.
    
    But there's a fundamental problem, that KSM relies upon mmap_sem to
    guarantee the consistency of the mm it's dealing with, yet exit_mmap
    tears down an mm without taking mmap_sem.  And bumping mm_users won't
    help at all, that just ensures that the pages the OOM killer assumes
    are on their way to being freed will not be freed.
    
    The best answer seems to be, to move the ksm_exit callout from just
    before exit_mmap, to the middle of exit_mmap: after the mm's pages
    have been freed (if the mmu_gather is flushed), but before its page
    tables and vma structures have been freed; and down_write,up_write
    mmap_sem there to serialize with KSM's own reliance on mmap_sem.
    
    But KSM then needs to be careful, whenever it downs mmap_sem, to
    check that the mm is not already exiting: there's a danger of using
    find_vma on a layout that's being torn apart, or writing into page
    tables which have been freed for reuse; and even do_anonymous_page
    and __do_fault need to check they're not being called by break_ksm
    to reinstate a pte after zap_pte_range has zapped that page table.
    
    Though it might be clearer to add an exiting flag, set while holding
    mmap_sem in __ksm_exit, that wouldn't cover the issue of reinstating
    a zapped pte.  All we need is to check whether mm_users is 0 - but
    must remember that ksmd may detect that before __ksm_exit is reached.
    So, ksm_test_exit(mm) added to comment such checks on mm->mm_users.
    
    __ksm_exit now has to leave clearing up the rmap_items to ksmd,
    that needs ksm_thread_mutex; but shift the exiting mm just after the
    ksm_scan cursor so that it will soon be dealt with.  __ksm_enter raise
    mm_count to hold the mm_struct, ksmd's exit processing (exactly like
    its processing when it finds all VM_MERGEABLEs unmapped) mmdrop it,
    similar procedure for KSM_RUN_UNMERGE (which has stopped ksmd).
    
    But also give __ksm_exit a fast path: when there's no complication
    (no rmap_items attached to mm and it's not at the ksm_scan cursor),
    it can safely do all the exiting work itself.  This is not just an
    optimization: when ksmd is not running, the raised mm_count would
    otherwise leak mm_structs.
    
    Signed-off-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Acked-by: Izik Eidus <ieidus@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 73a442b7be6d..42f20f565b16 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -501,7 +501,6 @@ void mmput(struct mm_struct *mm)
 
 	if (atomic_dec_and_test(&mm->mm_users)) {
 		exit_aio(mm);
-		ksm_exit(mm);
 		exit_mmap(mm);
 		set_mm_exe_file(mm, NULL);
 		if (!list_empty(&mm->mmlist)) {

commit f8af4da3b4c14e7267c4ffb952079af3912c51c5
Author: Hugh Dickins <hugh.dickins@tiscali.co.uk>
Date:   Mon Sep 21 17:01:57 2009 -0700

    ksm: the mm interface to ksm
    
    This patch presents the mm interface to a dummy version of ksm.c, for
    better scrutiny of that interface: the real ksm.c follows later.
    
    When CONFIG_KSM is not set, madvise(2) reject MADV_MERGEABLE and
    MADV_UNMERGEABLE with EINVAL, since that seems more helpful than
    pretending that they can be serviced.  But when CONFIG_KSM=y, accept them
    even if KSM is not currently running, and even on areas which KSM will not
    touch (e.g.  hugetlb or shared file or special driver mappings).
    
    Like other madvices, report ENOMEM despite success if any area in the
    range is unmapped, and use EAGAIN to report out of memory.
    
    Define vma flag VM_MERGEABLE to identify an area on which KSM may try
    merging pages: leave it to ksm_madvise() to decide whether to set it.
    Define mm flag MMF_VM_MERGEABLE to identify an mm which might contain
    VM_MERGEABLE areas, to minimize callouts when forking or exiting.
    
    Based upon earlier patches by Chris Wright and Izik Eidus.
    
    Signed-off-by: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Signed-off-by: Chris Wright <chrisw@redhat.com>
    Signed-off-by: Izik Eidus <ieidus@redhat.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d4638c8cc19e..73a442b7be6d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -49,6 +49,7 @@
 #include <linux/ftrace.h>
 #include <linux/profile.h>
 #include <linux/rmap.h>
+#include <linux/ksm.h>
 #include <linux/acct.h>
 #include <linux/tsacct_kern.h>
 #include <linux/cn_proc.h>
@@ -299,6 +300,9 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	rb_link = &mm->mm_rb.rb_node;
 	rb_parent = NULL;
 	pprev = &mm->mmap;
+	retval = ksm_fork(mm, oldmm);
+	if (retval)
+		goto out;
 
 	for (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {
 		struct file *file;
@@ -435,7 +439,8 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 	atomic_set(&mm->mm_count, 1);
 	init_rwsem(&mm->mmap_sem);
 	INIT_LIST_HEAD(&mm->mmlist);
-	mm->flags = (current->mm) ? current->mm->flags : default_dump_filter;
+	mm->flags = (current->mm) ?
+		(current->mm->flags & MMF_INIT_MASK) : default_dump_filter;
 	mm->core_state = NULL;
 	mm->nr_ptes = 0;
 	set_mm_counter(mm, file_rss, 0);
@@ -496,6 +501,7 @@ void mmput(struct mm_struct *mm)
 
 	if (atomic_dec_and_test(&mm->mm_users)) {
 		exit_aio(mm);
+		ksm_exit(mm);
 		exit_mmap(mm);
 		set_mm_exe_file(mm, NULL);
 		if (!list_empty(&mm->mmlist)) {

commit c6a7f5728a1db45d30df55a01adc130b4ab0327c
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Mon Sep 21 17:01:32 2009 -0700

    mm: oom analysis: Show kernel stack usage in /proc/meminfo and OOM log output
    
    The amount of memory allocated to kernel stacks can become significant and
    cause OOM conditions.  However, we do not display the amount of memory
    consumed by stacks.
    
    Add code to display the amount of memory used for stacks in /proc/meminfo.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Reviewed-by: Minchan Kim <minchan.kim@gmail.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2cebfb23b0b8..d4638c8cc19e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -136,9 +136,17 @@ struct kmem_cache *vm_area_cachep;
 /* SLAB cache for mm_struct structures (tsk->mm) */
 static struct kmem_cache *mm_cachep;
 
+static void account_kernel_stack(struct thread_info *ti, int account)
+{
+	struct zone *zone = page_zone(virt_to_page(ti));
+
+	mod_zone_page_state(zone, NR_KERNEL_STACK, account);
+}
+
 void free_task(struct task_struct *tsk)
 {
 	prop_local_destroy_single(&tsk->dirties);
+	account_kernel_stack(tsk->stack, -1);
 	free_thread_info(tsk->stack);
 	rt_mutex_debug_task_free(tsk);
 	ftrace_graph_exit_task(tsk);
@@ -253,6 +261,9 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	tsk->btrace_seq = 0;
 #endif
 	tsk->splice_pipe = NULL;
+
+	account_kernel_stack(ti, 1);
+
 	return tsk;
 
 out:

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index bfee931ee3fb..2cebfb23b0b8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -61,7 +61,7 @@
 #include <linux/blkdev.h>
 #include <linux/fs_struct.h>
 #include <linux/magic.h>
-#include <linux/perf_counter.h>
+#include <linux/perf_event.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -1078,7 +1078,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p, clone_flags);
 
-	retval = perf_counter_init_task(p);
+	retval = perf_event_init_task(p);
 	if (retval)
 		goto bad_fork_cleanup_policy;
 
@@ -1253,7 +1253,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	write_unlock_irq(&tasklist_lock);
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
-	perf_counter_fork(p);
+	perf_event_fork(p);
 	return p;
 
 bad_fork_free_pid:
@@ -1280,7 +1280,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_audit:
 	audit_free(p);
 bad_fork_cleanup_policy:
-	perf_counter_free_task(p);
+	perf_event_free_task(p);
 #ifdef CONFIG_NUMA
 	mpol_put(p->mempolicy);
 bad_fork_cleanup_cgroup:

commit eee2775d9924b22643bd89b2e568cc5eed7e8a04
Merge: 53e16fbd3000 7db905e636f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 11 13:20:18 2009 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (28 commits)
      rcu: Move end of special early-boot RCU operation earlier
      rcu: Changes from reviews: avoid casts, fix/add warnings, improve comments
      rcu: Create rcutree plugins to handle hotplug CPU for multi-level trees
      rcu: Remove lockdep annotations from RCU's _notrace() API members
      rcu: Add #ifdef to suppress __rcu_offline_cpu() warning in !HOTPLUG_CPU builds
      rcu: Add CPU-offline processing for single-node configurations
      rcu: Add "notrace" to RCU function headers used by ftrace
      rcu: Remove CONFIG_PREEMPT_RCU
      rcu: Merge preemptable-RCU functionality into hierarchical RCU
      rcu: Simplify rcu_pending()/rcu_check_callbacks() API
      rcu: Use debugfs_remove_recursive() simplify code.
      rcu: Merge per-RCU-flavor initialization into pre-existing macro
      rcu: Fix online/offline indication for rcudata.csv trace file
      rcu: Consolidate sparse and lockdep declarations in include/linux/rcupdate.h
      rcu: Renamings to increase RCU clarity
      rcu: Move private definitions from include/linux/rcutree.h to kernel/rcutree.h
      rcu: Expunge lingering references to CONFIG_CLASSIC_RCU, optimize on !SMP
      rcu: Delay rcu_barrier() wait until beginning of next CPU-hotunplug operation.
      rcu: Fix typo in rcu_irq_exit() comment header
      rcu: Make rcupreempt_trace.c look at offline CPUs
      ...

commit a3c8b97396ef42edfb845788ba6f53b2a93ce980
Merge: 74fca6a42863 9f0ab4a3f0fd
Author: James Morris <jmorris@namei.org>
Date:   Fri Sep 11 08:04:49 2009 +1000

    Merge branch 'next' into for-linus

commit 29e2035bddecce3eb584a8304528b50da8370a24
Merge: 868489660dab 37d0892c5a94
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Sep 4 09:28:52 2009 +0200

    Merge branch 'linus' into core/rcu
    
    Merge reason: Avoid fuzz in init/main.c and update from rc6 to rc8.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e0e817392b9acf2c98d3be80c233dddb1b52003d
Author: David Howells <dhowells@redhat.com>
Date:   Wed Sep 2 09:13:40 2009 +0100

    CRED: Add some configurable debugging [try #6]
    
    Add a config option (CONFIG_DEBUG_CREDENTIALS) to turn on some debug checking
    for credential management.  The additional code keeps track of the number of
    pointers from task_structs to any given cred struct, and checks to see that
    this number never exceeds the usage count of the cred struct (which includes
    all references, not just those from task_structs).
    
    Furthermore, if SELinux is enabled, the code also checks that the security
    pointer in the cred struct is never seen to be invalid.
    
    This attempts to catch the bug whereby inode_has_perm() faults in an nfsd
    kernel thread on seeing cred->security be a NULL pointer (it appears that the
    credential struct has been previously released):
    
            http://www.kerneloops.org/oops.php?number=252883
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 144326b7af50..043b5d88049b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -152,8 +152,7 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
-	put_cred(tsk->real_cred);
-	put_cred(tsk->cred);
+	exit_creds(tsk);
 	delayacct_tsk_free(tsk);
 
 	if (!profile_handoff_task(tsk))
@@ -1307,8 +1306,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	module_put(task_thread_info(p)->exec_domain->module);
 bad_fork_cleanup_count:
 	atomic_dec(&p->cred->user->processes);
-	put_cred(p->real_cred);
-	put_cred(p->cred);
+	exit_creds(p);
 bad_fork_free:
 	free_task(p);
 fork_out:

commit f71bb0ac5e85410601b0db29d7b1635345ea61a4
Merge: 7285dd7fd375 a42548a18866
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Aug 29 10:34:18 2009 +0200

    Merge branch 'timers/posixtimers' into timers/tracing
    
    Merge reason: timer tracepoint patches depend on both branches
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 4ab6c08336535f8c8e42cf45d7adeda882eff06e
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Aug 26 14:29:24 2009 -0700

    clone(): fix race between copy_process() and de_thread()
    
    Spotted by Hiroshi Shimamoto who also provided the test-case below.
    
    copy_process() uses signal->count as a reference counter, but it is not.
    This test case
    
            #include <sys/types.h>
            #include <sys/wait.h>
            #include <unistd.h>
            #include <stdio.h>
            #include <errno.h>
            #include <pthread.h>
    
            void *null_thread(void *p)
            {
                    for (;;)
                            sleep(1);
    
                    return NULL;
            }
    
            void *exec_thread(void *p)
            {
                    execl("/bin/true", "/bin/true", NULL);
    
                    return null_thread(p);
            }
    
            int main(int argc, char **argv)
            {
                    for (;;) {
                            pid_t pid;
                            int ret, status;
    
                            pid = fork();
                            if (pid < 0)
                                    break;
    
                            if (!pid) {
                                    pthread_t tid;
    
                                    pthread_create(&tid, NULL, exec_thread, NULL);
                                    for (;;)
                                            pthread_create(&tid, NULL, null_thread, NULL);
                            }
    
                            do {
                                    ret = waitpid(pid, &status, 0);
                            } while (ret == -1 && errno == EINTR);
                    }
    
                    return 0;
            }
    
    quickly creates an unkillable task.
    
    If copy_process(CLONE_THREAD) races with de_thread()
    copy_signal()->atomic(signal->count) breaks the signal->notify_count
    logic, and the execing thread can hang forever in kernel space.
    
    Change copy_process() to increment count/live only when we know for sure
    we can't fail.  In this case the forked thread will take care of its
    reference to signal correctly.
    
    If copy_process() fails, check CLONE_THREAD flag.  If it it set - do
    nothing, the counters were not changed and current belongs to the same
    thread group.  If it is not set, ->signal must be released in any case
    (and ->count must be == 1), the forked child is the only thread in the
    thread group.
    
    We need more cleanups here, in particular signal->count should not be used
    by de_thread/__exit_signal at all.  This patch only fixes the bug.
    
    Reported-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Tested-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 144326b7af50..e6c04d462ab2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -815,11 +815,8 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct signal_struct *sig;
 
-	if (clone_flags & CLONE_THREAD) {
-		atomic_inc(&current->signal->count);
-		atomic_inc(&current->signal->live);
+	if (clone_flags & CLONE_THREAD)
 		return 0;
-	}
 
 	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);
 	tsk->signal = sig;
@@ -877,16 +874,6 @@ void __cleanup_signal(struct signal_struct *sig)
 	kmem_cache_free(signal_cachep, sig);
 }
 
-static void cleanup_signal(struct task_struct *tsk)
-{
-	struct signal_struct *sig = tsk->signal;
-
-	atomic_dec(&sig->live);
-
-	if (atomic_dec_and_test(&sig->count))
-		__cleanup_signal(sig);
-}
-
 static void copy_flags(unsigned long clone_flags, struct task_struct *p)
 {
 	unsigned long new_flags = p->flags;
@@ -1239,6 +1226,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	}
 
 	if (clone_flags & CLONE_THREAD) {
+		atomic_inc(&current->signal->count);
+		atomic_inc(&current->signal->live);
 		p->group_leader = current->group_leader;
 		list_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);
 	}
@@ -1282,7 +1271,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (p->mm)
 		mmput(p->mm);
 bad_fork_cleanup_signal:
-	cleanup_signal(p);
+	if (!(clone_flags & CLONE_THREAD))
+		__cleanup_signal(p->signal);
 bad_fork_cleanup_sighand:
 	__cleanup_sighand(p->sighand);
 bad_fork_cleanup_fs:

commit f41d911f8c49a5d65c86504c19e8204bb605c4fd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:52 2009 -0700

    rcu: Merge preemptable-RCU functionality into hierarchical RCU
    
    Create a kernel/rcutree_plugin.h file that contains definitions
    for preemptable RCU (or, under the #else branch of the #ifdef,
    empty definitions for the classic non-preemptable semantics).
    These definitions fit into plugins defined in kernel/rcutree.c
    for this purpose.
    
    This variant of preemptable RCU uses a new algorithm whose
    read-side expense is roughly that of classic hierarchical RCU
    under CONFIG_PREEMPT. This new algorithm's update-side expense
    is similar to that of classic hierarchical RCU, and, in absence
    of read-side preemption or blocking, is exactly that of classic
    hierarchical RCU.  Perhaps more important, this new algorithm
    has a much simpler implementation, saving well over 1,000 lines
    of code compared to mainline's implementation of preemptable
    RCU, which will hopefully be retired in favor of this new
    algorithm.
    
    The simplifications are obtained by maintaining per-task
    nesting state for running tasks, and using a simple
    lock-protected algorithm to handle accounting when tasks block
    within RCU read-side critical sections, making use of lessons
    learned while creating numerous user-level RCU implementations
    over the past 18 months.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <12509746134003-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 021e1138556e..642e8b5edf00 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1022,10 +1022,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	copy_flags(clone_flags, p);
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);
-#ifdef CONFIG_PREEMPT_RCU
-	p->rcu_read_lock_nesting = 0;
-	p->rcu_flipctr_idx = 0;
-#endif /* #ifdef CONFIG_PREEMPT_RCU */
+	rcu_copy_process(p);
 	p->vfork_done = NULL;
 	spin_lock_init(&p->alloc_lock);
 

commit 0753ba01e126020bf0f8150934903b48935b697d
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Tue Aug 18 14:11:10 2009 -0700

    mm: revert "oom: move oom_adj value"
    
    The commit 2ff05b2b (oom: move oom_adj value) moveed the oom_adj value to
    the mm_struct.  It was a very good first step for sanitize OOM.
    
    However Paul Menage reported the commit makes regression to his job
    scheduler.  Current OOM logic can kill OOM_DISABLED process.
    
    Why? His program has the code of similar to the following.
    
            ...
            set_oom_adj(OOM_DISABLE); /* The job scheduler never killed by oom */
            ...
            if (vfork() == 0) {
                    set_oom_adj(0); /* Invoked child can be killed */
                    execve("foo-bar-cmd");
            }
            ....
    
    vfork() parent and child are shared the same mm_struct.  then above
    set_oom_adj(0) doesn't only change oom_adj for vfork() child, it's also
    change oom_adj for vfork() parent.  Then, vfork() parent (job scheduler)
    lost OOM immune and it was killed.
    
    Actually, fork-setting-exec idiom is very frequently used in userland program.
    We must not break this assumption.
    
    Then, this patch revert commit 2ff05b2b and related commit.
    
    Reverted commit list
    ---------------------
    - commit 2ff05b2b4e (oom: move oom_adj value from task_struct to mm_struct)
    - commit 4d8b9135c3 (oom: avoid unnecessary mm locking and scanning for OOM_DISABLE)
    - commit 8123681022 (oom: only oom kill exiting tasks with attached memory)
    - commit 933b787b57 (mm: copy over oom_adj value at fork time)
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Paul Menage <menage@google.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 021e1138556e..144326b7af50 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -426,7 +426,6 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 	init_rwsem(&mm->mmap_sem);
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->flags = (current->mm) ? current->mm->flags : default_dump_filter;
-	mm->oom_adj = (current->mm) ? current->mm->oom_adj : 0;
 	mm->core_state = NULL;
 	mm->nr_ptes = 0;
 	set_mm_counter(mm, file_rss, 0);

commit 9c8a8228d0827e0d91d28527209988f672f97d28
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Thu Aug 6 15:09:28 2009 -0700

    execve: must clear current->clear_child_tid
    
    While looking at Jens Rosenboom bug report
    (http://lkml.org/lkml/2009/7/27/35) about strange sys_futex call done from
    a dying "ps" program, we found following problem.
    
    clone() syscall has special support for TID of created threads.  This
    support includes two features.
    
    One (CLONE_CHILD_SETTID) is to set an integer into user memory with the
    TID value.
    
    One (CLONE_CHILD_CLEARTID) is to clear this same integer once the created
    thread dies.
    
    The integer location is a user provided pointer, provided at clone()
    time.
    
    kernel keeps this pointer value into current->clear_child_tid.
    
    At execve() time, we should make sure kernel doesnt keep this user
    provided pointer, as full user memory is replaced by a new one.
    
    As glibc fork() actually uses clone() syscall with CLONE_CHILD_SETTID and
    CLONE_CHILD_CLEARTID set, chances are high that we might corrupt user
    memory in forked processes.
    
    Following sequence could happen:
    
    1) bash (or any program) starts a new process, by a fork() call that
       glibc maps to a clone( ...  CLONE_CHILD_SETTID | CLONE_CHILD_CLEARTID
       ...) syscall
    
    2) When new process starts, its current->clear_child_tid is set to a
       location that has a meaning only in bash (or initial program) context
       (&THREAD_SELF->tid)
    
    3) This new process does the execve() syscall to start a new program.
       current->clear_child_tid is left unchanged (a non NULL value)
    
    4) If this new program creates some threads, and initial thread exits,
       kernel will attempt to clear the integer pointed by
       current->clear_child_tid from mm_release() :
    
            if (tsk->clear_child_tid
                && !(tsk->flags & PF_SIGNALED)
                && atomic_read(&mm->mm_users) > 1) {
                    u32 __user * tidptr = tsk->clear_child_tid;
                    tsk->clear_child_tid = NULL;
    
                    /*
                     * We don't check the error code - if userspace has
                     * not set up a proper pointer then tough luck.
                     */
    << here >>      put_user(0, tidptr);
                    sys_futex(tidptr, FUTEX_WAKE, 1, NULL, NULL, 0);
            }
    
    5) OR : if new program is not multi-threaded, but spied by /proc/pid
       users (ps command for example), mm_users > 1, and the exiting program
       could corrupt 4 bytes in a persistent memory area (shm or memory mapped
       file)
    
    If current->clear_child_tid points to a writeable portion of memory of the
    new program, kernel happily and silently corrupts 4 bytes of memory, with
    unexpected effects.
    
    Fix is straightforward and should not break any sane program.
    
    Reported-by: Jens Rosenboom <jens@mcbone.net>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sonny Rao <sonnyrao@us.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ulrich Drepper <drepper@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 466531eb92cc..021e1138556e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -568,18 +568,18 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 	 * the value intact in a core dump, and to save the unnecessary
 	 * trouble otherwise.  Userland only wants this done for a sys_exit.
 	 */
-	if (tsk->clear_child_tid
-	    && !(tsk->flags & PF_SIGNALED)
-	    && atomic_read(&mm->mm_users) > 1) {
-		u32 __user * tidptr = tsk->clear_child_tid;
+	if (tsk->clear_child_tid) {
+		if (!(tsk->flags & PF_SIGNALED) &&
+		    atomic_read(&mm->mm_users) > 1) {
+			/*
+			 * We don't check the error code - if userspace has
+			 * not set up a proper pointer then tough luck.
+			 */
+			put_user(0, tsk->clear_child_tid);
+			sys_futex(tsk->clear_child_tid, FUTEX_WAKE,
+					1, NULL, NULL, 0);
+		}
 		tsk->clear_child_tid = NULL;
-
-		/*
-		 * We don't check the error code - if userspace has
-		 * not set up a proper pointer then tough luck.
-		 */
-		put_user(0, tidptr);
-		sys_futex(tidptr, FUTEX_WAKE, 1, NULL, NULL, 0);
 	}
 }
 

commit 42c4ab41a176ee784c0f28c0b29025a8fc34f05a
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Wed Jul 29 12:15:26 2009 +0200

    itimers: Merge ITIMER_VIRT and ITIMER_PROF
    
    Both cpu itimers have same data flow in the few places, this
    patch make unification of code related with VIRT and PROF
    itimers.
    
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    LKML-Reference: <1248862529-6063-2-git-send-email-sgruszka@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 29b532e718f7..893ab0bf5e39 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -62,6 +62,7 @@
 #include <linux/fs_struct.h>
 #include <linux/magic.h>
 #include <linux/perf_counter.h>
+#include <linux/posix-timers.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -790,10 +791,10 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 	thread_group_cputime_init(sig);
 
 	/* Expiration times and increments. */
-	sig->it_virt_expires = cputime_zero;
-	sig->it_virt_incr = cputime_zero;
-	sig->it_prof_expires = cputime_zero;
-	sig->it_prof_incr = cputime_zero;
+	sig->it[CPUCLOCK_PROF].expires = cputime_zero;
+	sig->it[CPUCLOCK_PROF].incr = cputime_zero;
+	sig->it[CPUCLOCK_VIRT].expires = cputime_zero;
+	sig->it[CPUCLOCK_VIRT].incr = cputime_zero;
 
 	/* Cached expiration times. */
 	sig->cputime_expires.prof_exp = cputime_zero;

commit 9f498cc5be7e013d8d6e4c616980ed0ffc8680d2
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jul 23 14:46:33 2009 +0200

    perf_counter: Full task tracing
    
    In order to be able to distinguish between no samples due to
    inactivity and no samples due to task ended, Arjan asked for
    PERF_EVENT_EXIT events. This is useful to the boot delay
    instrumentation (bootchart) app.
    
    This patch changes the PERF_EVENT_FORK to be emitted on every
    clone, and adds PERF_EVENT_EXIT to be emitted on task exit,
    after the task's counters have been closed.
    
    This task tracing is controlled through: attr.comm || attr.mmap
    and through the new attr.task field.
    
    Suggested-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Anton Blanchard <anton@samba.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    [ cleaned up perf_counter.h a bit ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 29b532e718f7..466531eb92cc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1269,6 +1269,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	write_unlock_irq(&tasklist_lock);
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
+	perf_counter_fork(p);
 	return p;
 
 bad_fork_free_pid:
@@ -1410,9 +1411,6 @@ long do_fork(unsigned long clone_flags,
 			init_completion(&vfork);
 		}
 
-		if (!(clone_flags & CLONE_THREAD))
-			perf_counter_fork(p);
-
 		audit_finish_fork(p);
 		tracehook_report_clone(regs, clone_flags, nr, p);
 

commit 933b787b57ca8bdc0fc8fb2cbf67b5e6d21beb84
Author: Rik van Riel <riel@redhat.com>
Date:   Wed Jul 29 15:02:07 2009 -0700

    mm: copy over oom_adj value at fork time
    
    Fix a post-2.6.31 regression which was introduced by
    2ff05b2b4eac2e63d345fc731ea151a060247f53 ("oom: move oom_adj value from
    task_struct to mm_struct").
    
    After moving the oom_adj value from the task struct to the mm_struct, the
    oom_adj value was no longer properly inherited by child processes.
    
    Copying over the oom_adj value at fork time fixes that bug.
    
    [kosaki.motohiro@jp.fujitsu.com: test for current->mm before dereferencing it]
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Reported-by: Paul Menage <manage@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9b42695f0d14..29b532e718f7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -426,6 +426,7 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 	init_rwsem(&mm->mmap_sem);
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->flags = (current->mm) ? current->mm->flags : default_dump_filter;
+	mm->oom_adj = (current->mm) ? current->mm->oom_adj : 0;
 	mm->core_state = NULL;
 	mm->nr_ptes = 0;
 	set_mm_counter(mm, file_rss, 0);

commit 3c3301083e3bea4d14c597106c7b20b4b85fc03d
Merge: 612e900c286a 0fdc7e67dd31
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 22 11:41:56 2009 -0700

    Merge branch 'perf-counters-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/peterz/linux-2.6-perf
    
    * 'perf-counters-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/peterz/linux-2.6-perf: (31 commits)
      perf_counter tools: Give perf top inherit option
      perf_counter tools: Fix vmlinux symbol generation breakage
      perf_counter: Detect debugfs location
      perf_counter: Add tracepoint support to perf list, perf stat
      perf symbol: C++ demangling
      perf: avoid structure size confusion by using a fixed size
      perf_counter: Fix throttle/unthrottle event logging
      perf_counter: Improve perf stat and perf record option parsing
      perf_counter: PERF_SAMPLE_ID and inherited counters
      perf_counter: Plug more stack leaks
      perf: Fix stack data leak
      perf_counter: Remove unused variables
      perf_counter: Make call graph option consistent
      perf_counter: Add perf record option to log addresses
      perf_counter: Log vfork as a fork event
      perf_counter: Synthesize VDSO mmap event
      perf_counter: Make sure we dont leak kernel memory to userspace
      perf_counter tools: Fix index boundary check
      perf_counter: Fix the tracepoint channel to perfcounters
      perf_counter, x86: Extend perf_counter Pentium M support
      ...

commit ed900c054b541254f0ce5cedaf75206e29bd614e
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Jul 16 15:44:29 2009 +0200

    perf_counter: Log vfork as a fork event
    
    Right now we don't output vfork events. Even though we should
    always see an exec after a vfork, we may get perfcounter
    samples between the vfork and exec. These samples can lead to
    some confusion when parsing perfcounter data.
    
    To keep things consistent we should always log a fork event. It
    will result in a little more log data, but is less confusing to
    trace parsing tools.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <20090716104817.589309391@samba.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 467746b3f0aa..4812d60b29f8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1408,14 +1408,11 @@ long do_fork(unsigned long clone_flags,
 		if (clone_flags & CLONE_VFORK) {
 			p->vfork_done = &vfork;
 			init_completion(&vfork);
-		} else if (!(clone_flags & CLONE_VM)) {
-			/*
-			 * vfork will do an exec which will call
-			 * set_task_comm()
-			 */
-			perf_counter_fork(p);
 		}
 
+		if (!(clone_flags & CLONE_THREAD))
+			perf_counter_fork(p);
+
 		audit_finish_fork(p);
 		tracehook_report_clone(regs, clone_flags, nr, p);
 

commit b43f3cbd21ffbd719fd4fa6642bfe6af255ded34
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Jul 8 01:54:37 2009 +0400

    headers: mnt_namespace.h redux
    
    Fix various silly problems wrt mnt_namespace.h:
    
     - exit_mnt_ns() isn't used, remove it
     - done that, sched.h and nsproxy.h inclusions aren't needed
     - mount.h inclusion was need for vfsmount_lock, but no longer
     - remove mnt_namespace.h inclusion from files which don't use anything
       from mnt_namespace.h
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 467746b3f0aa..bd2959228871 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -17,7 +17,6 @@
 #include <linux/module.h>
 #include <linux/vmalloc.h>
 #include <linux/completion.h>
-#include <linux/mnt_namespace.h>
 #include <linux/personality.h>
 #include <linux/mempolicy.h>
 #include <linux/sem.h>

commit 72a1de39f89325a834a8c70b2a0d8f71d919f640
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:37 2009 -0700

    copy_process(): remove the unneeded clear_tsk_thread_flag(TIF_SIGPENDING)
    
    The forked child can have TIF_SIGPENDING if it was copied from parent's
    ti->flags.  But this is harmless and actually almost never happens,
    because copy_process() can't succeed if signal_pending() == T.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index be022c200da6..467746b3f0aa 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1029,7 +1029,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->vfork_done = NULL;
 	spin_lock_init(&p->alloc_lock);
 
-	clear_tsk_thread_flag(p, TIF_SIGPENDING);
 	init_sigpending(&p->pending);
 
 	p->utime = cputime_zero;

commit 2dff440525f8faba8836e9f05297b76f23b4af30
Author: Vegard Nossum <vegard.nossum@gmail.com>
Date:   Sat May 31 15:56:17 2008 +0200

    kmemcheck: add mm functions
    
    With kmemcheck enabled, the slab allocator needs to do this:
    
    1. Tell kmemcheck to allocate the shadow memory which stores the status of
       each byte in the allocation proper, e.g. whether it is initialized or
       uninitialized.
    2. Tell kmemcheck which parts of memory that should be marked uninitialized.
       There are actually a few more states, such as "not yet allocated" and
       "recently freed".
    
    If a slab cache is set up using the SLAB_NOTRACK flag, it will never return
    memory that can take page faults because of kmemcheck.
    
    If a slab cache is NOT set up using the SLAB_NOTRACK flag, callers can still
    request memory with the __GFP_NOTRACK flag. This does not prevent the page
    faults from occuring, however, but marks the object in question as being
    initialized so that no warnings will ever be produced for this object.
    
    In addition to (and in contrast to) __GFP_NOTRACK, the
    __GFP_NOTRACK_FALSE_POSITIVE flag indicates that the allocation should
    not be tracked _because_ it would produce a false positive. Their values
    are identical, but need not be so in the future (for example, we could now
    enable/disable false positives with a config option).
    
    Parts of this patch were contributed by Pekka Enberg but merged for
    atomicity.
    
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    
    [rebased for mainline inclusion]
    Signed-off-by: Vegard Nossum <vegard.nossum@gmail.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4430eb1376f2..be022c200da6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -178,7 +178,7 @@ void __init fork_init(unsigned long mempages)
 	/* create a slab on which task_structs can be allocated */
 	task_struct_cachep =
 		kmem_cache_create("task_struct", sizeof(struct task_struct),
-			ARCH_MIN_TASKALIGN, SLAB_PANIC, NULL);
+			ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
 #endif
 
 	/* do the arch specific task caches init */
@@ -1470,20 +1470,20 @@ void __init proc_caches_init(void)
 {
 	sighand_cachep = kmem_cache_create("sighand_cache",
 			sizeof(struct sighand_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_DESTROY_BY_RCU,
-			sighand_ctor);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_DESTROY_BY_RCU|
+			SLAB_NOTRACK, sighand_ctor);
 	signal_cachep = kmem_cache_create("signal_cache",
 			sizeof(struct signal_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
 	files_cachep = kmem_cache_create("files_cache",
 			sizeof(struct files_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
 	fs_cachep = kmem_cache_create("fs_cache",
 			sizeof(struct fs_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
 	mm_cachep = kmem_cache_create("mm_struct",
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_NOTRACK, NULL);
 	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC);
 	mmap_init();
 }

commit 940010c5a314a7bd9b498593bc6ba1718ac5aec5
Merge: 8dc8e5e8bc0c 991ec02cdca3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jun 11 17:55:42 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/irqinit.c
            arch/x86/kernel/irqinit_64.c
            arch/x86/kernel/traps.c
            arch/x86/mm/fault.c
            include/linux/sched.h
            kernel/exit.c

commit 991ec02cdca33b03a132a0cacfe6f0aa0be9aa8d
Merge: 862366118026 84047e360af0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 19:58:10 2009 -0700

    Merge branch 'tracing-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      function-graph: always initialize task ret_stack
      function-graph: move initialization of new tasks up in fork
      function-graph: add memory barriers for accessing task's ret_stack
      function-graph: enable the stack after initialization of other variables
      function-graph: only allocate init tasks if it was not already done
    
    Manually fix trivial conflict in kernel/trace/ftrace.c

commit 862366118026a358882eefc70238dbcc3db37aac
Merge: 57eee9ae7bbc 511b01bdf64a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 10 19:53:40 2009 -0700

    Merge branch 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (244 commits)
      Revert "x86, bts: reenable ptrace branch trace support"
      tracing: do not translate event helper macros in print format
      ftrace/documentation: fix typo in function grapher name
      tracing/events: convert block trace points to TRACE_EVENT(), fix !CONFIG_BLOCK
      tracing: add protection around module events unload
      tracing: add trace_seq_vprint interface
      tracing: fix the block trace points print size
      tracing/events: convert block trace points to TRACE_EVENT()
      ring-buffer: fix ret in rb_add_time_stamp
      ring-buffer: pass in lockdep class key for reader_lock
      tracing: add annotation to what type of stack trace is recorded
      tracing: fix multiple use of __print_flags and __print_symbolic
      tracing/events: fix output format of user stack
      tracing/events: fix output format of kernel stack
      tracing/trace_stack: fix the number of entries in the header
      ring-buffer: discard timestamps that are at the start of the buffer
      ring-buffer: try to discard unneeded timestamps
      ring-buffer: fix bug in ring_buffer_discard_commit
      ftrace: do not profile functions when disabled
      tracing: make trace pipe recognize latency format flag
      ...

commit 087eb437051b3de817720f9c80c440fc9e7dcce8
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jun 4 16:29:07 2009 -0700

    ptrace: tracehook_report_clone: fix false positives
    
    The "trace || CLONE_PTRACE" check in tracehook_report_clone() is not right,
    
    - If the untraced task does clone(CLONE_PTRACE) the new child is not traced,
      we must not queue SIGSTOP.
    
    - If we forked the traced task, but the tracer exits and untraces both the
      forking task and the new child (after copy_process() drops tasklist_lock),
      we should not queue SIGSTOP too.
    
    Change the code to check task_ptrace() != 0 instead. This is still racy, but
    the race is harmless.
    
    We can race with another tracer attaching to this child, or the tracer can
    exit and detach in parallel. But giwen that we didn't do wake_up_new_task()
    yet, the child must have the pending SIGSTOP anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b9e2edd00726..875ffbdd96d0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1409,7 +1409,7 @@ long do_fork(unsigned long clone_flags,
 		}
 
 		audit_finish_fork(p);
-		tracehook_report_clone(trace, regs, clone_flags, nr, p);
+		tracehook_report_clone(regs, clone_flags, nr, p);
 
 		/*
 		 * We set PF_STARTING at creation in case tracing wants to

commit 60313ebed739b331e8e61079da27a11ee3b73a30
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Jun 4 16:53:44 2009 +0200

    perf_counter: Add fork event
    
    Create a fork event so that we can easily clone the comm and
    dso maps without having to generate all those events.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index b7d7a9f0bd7a..f4466ca37ece 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1412,12 +1412,12 @@ long do_fork(unsigned long clone_flags,
 		if (clone_flags & CLONE_VFORK) {
 			p->vfork_done = &vfork;
 			init_completion(&vfork);
-		} else {
+		} else if (!(clone_flags & CLONE_VM)) {
 			/*
 			 * vfork will do an exec which will call
 			 * set_task_comm()
 			 */
-			perf_counter_comm(p);
+			perf_counter_fork(p);
 		}
 
 		audit_finish_fork(p);

commit 226f62fdd53d5b2c74e242aa11f6ad43d0285d3f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Jun 3 11:23:56 2009 +0200

    perf_counter: Add a comm hook for pure fork()s
    
    I noticed missing COMM events and found that we missed
    reporting them for pure forks.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: John Kacur <jkacur@redhat.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 23bf757ed321..b7d7a9f0bd7a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1412,6 +1412,12 @@ long do_fork(unsigned long clone_flags,
 		if (clone_flags & CLONE_VFORK) {
 			p->vfork_done = &vfork;
 			init_completion(&vfork);
+		} else {
+			/*
+			 * vfork will do an exec which will call
+			 * set_task_comm()
+			 */
+			perf_counter_comm(p);
 		}
 
 		audit_finish_fork(p);

commit f7e8b616ed1cc6f790b82324bce8a2a60295e5c2
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jun 2 16:39:48 2009 -0400

    function-graph: move initialization of new tasks up in fork
    
    When the function graph tracer is enabled, all new tasks must allocate
    a ret_stack to place the return address of functions. This is because
    the function graph tracer will replace the real return address with a
    call to the tracing of the exit function.
    
    This initialization happens in fork, but it happens too late. If fork
    fails, then it will call free_task and that calls the freeing of this
    ret_stack. But before initialization happens, the new (failed) task
    points to its parents ret_stack. If a fork failure happens during
    the function trace, it would be catastrophic for the parent.
    
    Also, there's no need to call ftrace_graph_exit_task from fork, since
    it is called by free_task which fork calls on failure.
    
    [ Impact: prevent crash during failed fork running function graph tracer ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b9e2edd00726..c4b1e35c430b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -982,6 +982,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (!p)
 		goto fork_out;
 
+	ftrace_graph_init_task(p);
+
 	rt_mutex_init_task(p);
 
 #ifdef CONFIG_PROVE_LOCKING
@@ -1131,8 +1133,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		}
 	}
 
-	ftrace_graph_init_task(p);
-
 	p->pid = pid_nr(pid);
 	p->tgid = p->pid;
 	if (clone_flags & CLONE_THREAD)
@@ -1141,7 +1141,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (current->nsproxy != p->nsproxy) {
 		retval = ns_cgroup_clone(p, pid);
 		if (retval)
-			goto bad_fork_free_graph;
+			goto bad_fork_free_pid;
 	}
 
 	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
@@ -1233,7 +1233,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		spin_unlock(&current->sighand->siglock);
 		write_unlock_irq(&tasklist_lock);
 		retval = -ERESTARTNOINTR;
-		goto bad_fork_free_graph;
+		goto bad_fork_free_pid;
 	}
 
 	if (clone_flags & CLONE_THREAD) {
@@ -1268,8 +1268,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	cgroup_post_fork(p);
 	return p;
 
-bad_fork_free_graph:
-	ftrace_graph_exit_task(p);
 bad_fork_free_pid:
 	if (pid != &init_struct_pid)
 		free_pid(pid);

commit bbbee90829304d156c12b171c0ac7e6e1aba8b90
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 29 14:25:58 2009 +0200

    perf_counter: Ammend cleanup in fork() fail
    
    When fork() fails we cannot use perf_counter_exit_task() since that
    assumes to operate on current. Write a new helper that cleans up
    unused/clean contexts.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: John Kacur <jkacur@redhat.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index c07c3335ceac..23bf757ed321 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1298,7 +1298,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_audit:
 	audit_free(p);
 bad_fork_cleanup_policy:
-	perf_counter_exit_task(p);
+	perf_counter_free_task(p);
 #ifdef CONFIG_NUMA
 	mpol_put(p->mempolicy);
 bad_fork_cleanup_cgroup:

commit 6ab423e0eaca827fbd201ca4ae7d4f8573a366b2
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon May 25 14:45:27 2009 +0200

    perf_counter: Propagate inheritance failures down the fork() path
    
    Fail fork() when we fail inheritance for some reason (-ENOMEM most likely).
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: John Kacur <jkacur@redhat.com>
    LKML-Reference: <20090525124600.324656474@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 675e01e9072a..c07c3335ceac 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1095,7 +1095,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p, clone_flags);
-	perf_counter_init_task(p);
+
+	retval = perf_counter_init_task(p);
+	if (retval)
+		goto bad_fork_cleanup_policy;
 
 	if ((retval = audit_alloc(p)))
 		goto bad_fork_cleanup_policy;
@@ -1295,6 +1298,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_audit:
 	audit_free(p);
 bad_fork_cleanup_policy:
+	perf_counter_exit_task(p);
 #ifdef CONFIG_NUMA
 	mpol_put(p->mempolicy);
 bad_fork_cleanup_cgroup:

commit e4cbb4e3ac8b09fdb11e39e5a5611bfab0a7cd1a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue May 19 15:50:30 2009 +0200

    perf_counter: Move child perfcounter init to after scheduler init
    
    Initialize a task's perfcounters (inherit from parent, etc.) after
    the child task's scheduler fields have been initialized already.
    
    [ Impact: cleanup ]
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: John Kacur <jkacur@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index e72a09f5355b..675e01e9072a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -984,7 +984,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto fork_out;
 
 	rt_mutex_init_task(p);
-	perf_counter_init_task(p);
 
 #ifdef CONFIG_PROVE_LOCKING
 	DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);
@@ -1096,6 +1095,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p, clone_flags);
+	perf_counter_init_task(p);
 
 	if ((retval = audit_alloc(p)))
 		goto bad_fork_cleanup_policy;

commit a63eaf34ae60bdb067a354cc8def2e8f4a01f5f4
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri May 22 14:17:31 2009 +1000

    perf_counter: Dynamically allocate tasks' perf_counter_context struct
    
    This replaces the struct perf_counter_context in the task_struct with
    a pointer to a dynamically allocated perf_counter_context struct.  The
    main reason for doing is this is to allow us to transfer a
    perf_counter_context from one task to another when we do lazy PMU
    switching in a later patch.
    
    This has a few side-benefits: the task_struct becomes a little smaller,
    we save some memory because only tasks that have perf_counters attached
    get a perf_counter_context allocated for them, and we can remove the
    inclusion of <linux/perf_counter.h> in sched.h, meaning that we don't
    end up recompiling nearly everything whenever perf_counter.h changes.
    
    The perf_counter_context structures are reference-counted and freed
    when the last reference is dropped.  A context can have references
    from its task and the counters on its task.  Counters can outlive the
    task so it is possible that a context will be freed well after its
    task has exited.
    
    Contexts are allocated on fork if the parent had a context, or
    otherwise the first time that a per-task counter is created on a task.
    In the latter case, we set the context pointer in the task struct
    locklessly using an atomic compare-and-exchange operation in case we
    raced with some other task in creating a context for the subject task.
    
    This also removes the task pointer from the perf_counter struct.  The
    task pointer was not used anywhere and would make it harder to move a
    context from one task to another.  Anything that needed to know which
    task a counter was attached to was already using counter->ctx->task.
    
    The __perf_counter_init_context function moves up in perf_counter.c
    so that it can be called from find_get_context, and now initializes
    the refcount, but is otherwise unchanged.
    
    We were potentially calling list_del_counter twice: once from
    __perf_counter_exit_task when the task exits and once from
    __perf_counter_remove_from_context when the counter's fd gets closed.
    This adds a check in list_del_counter so it doesn't do anything if
    the counter has already been removed from the lists.
    
    Since perf_counter_task_sched_in doesn't do anything if the task doesn't
    have a context, and leaves cpuctx->task_ctx = NULL, this adds code to
    __perf_install_in_context to set cpuctx->task_ctx if necessary, i.e. in
    the case where the current task adds the first counter to itself and
    thus creates a context for itself.
    
    This also adds similar code to __perf_counter_enable to handle a
    similar situation which can arise when the counters have been disabled
    using prctl; that also leaves cpuctx->task_ctx = NULL.
    
    [ Impact: refactor counter context management to prepare for new feature ]
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <18966.10075.781053.231153@cargo.ozlabs.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index d32fef4d38e5..e72a09f5355b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -63,6 +63,7 @@
 #include <linux/fs_struct.h>
 #include <trace/sched.h>
 #include <linux/magic.h>
+#include <linux/perf_counter.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>

commit 0ad5d703c6c0fcd385d956555460df95dff7eb7e
Merge: 44347d947f62 1cb81b143fa8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 7 11:18:34 2009 +0200

    Merge branch 'tracing/hw-branch-tracing' into tracing/core
    
    Merge reason: this topic is ready for upstream now. It passed
                  Oleg's review and Andrew had no further mm/*
                  objections/observations either.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e7fd5d4b3d240f42c30a9e3d20a4689c4d3a795a
Merge: 1130b0296184 56a50adda49b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 29 14:46:59 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Merge reason: This brach was on -rc1, refresh it to almost-rc4 to pick up
                  the latest upstream fixes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 416dfdcdb894432547ead4fcb9fa6a36b396059e
Merge: 56449f437add 091069740304
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 24 10:11:18 2009 +0200

    Merge commit 'v2.6.30-rc3' into tracing/hw-branch-tracing
    
    Conflicts:
            arch/x86/kernel/ptrace.c
    
    Merge reason: fix the conflict above, and also pick up the CONFIG_BROKEN
                  dependency change from upstream so that we can remove it
                  here.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ad8d75fff811a6a230f7f43b05a6483099349533
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Apr 14 19:39:12 2009 -0400

    tracing/events: move trace point headers into include/trace/events
    
    Impact: clean up
    
    Create a sub directory in include/trace called events to keep the
    trace point headers in their own separate directory. Only headers that
    declare trace points should be defined in this directory.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Neil Horman <nhorman@tuxdriver.com>
    Cc: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4bebf2639235..085f73ebcea6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -61,7 +61,6 @@
 #include <linux/proc_fs.h>
 #include <linux/blkdev.h>
 #include <linux/fs_struct.h>
-#include <trace/sched.h>
 #include <linux/magic.h>
 
 #include <asm/pgtable.h>
@@ -71,6 +70,8 @@
 #include <asm/cacheflush.h>
 #include <asm/tlbflush.h>
 
+#include <trace/events/sched.h>
+
 /*
  * Protected counters by write_lock_irq(&tasklist_lock)
  */

commit a8d154b009168337494fbf345671bab74d3e4b8b
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 10 09:36:00 2009 -0400

    tracing: create automated trace defines
    
    This patch lowers the number of places a developer must modify to add
    new tracepoints. The current method to add a new tracepoint
    into an existing system is to write the trace point macro in the
    trace header with one of the macros TRACE_EVENT, TRACE_FORMAT or
    DECLARE_TRACE, then they must add the same named item into the C file
    with the macro DEFINE_TRACE(name) and then add the trace point.
    
    This change cuts out the needing to add the DEFINE_TRACE(name).
    Every file that uses the tracepoint must still include the trace/<type>.h
    file, but the one C file must also add a define before the including
    of that file.
    
     #define CREATE_TRACE_POINTS
     #include <trace/mytrace.h>
    
    This will cause the trace/mytrace.h file to also produce the C code
    necessary to implement the trace point.
    
    Note, if more than one trace/<type>.h is used to create the C code
    it is best to list them all together.
    
     #define CREATE_TRACE_POINTS
     #include <trace/foo.h>
     #include <trace/bar.h>
     #include <trace/fido.h>
    
    Thanks to Mathieu Desnoyers and Christoph Hellwig for coming up with
    the cleaner solution of the define above the includes over my first
    design to have the C code include a "special" header.
    
    This patch converts sched, irq and lockdep and skb to use this new
    method.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Neil Horman <nhorman@tuxdriver.com>
    Cc: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b9e2edd00726..4bebf2639235 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -83,8 +83,6 @@ DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 
 __cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
 
-DEFINE_TRACE(sched_process_fork);
-
 int nr_processes(void)
 {
 	int cpu;

commit 422a253483aa5de71a2bcdc27b0aa023053f97f8
Merge: 91e58b6e95a9 022624a758dc 9756b15e1b58 8f2e586567b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 9 10:35:30 2009 -0700

    Merge branches 'core-fixes-for-linus', 'irq-fixes-for-linus' and 'timers-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      printk: fix wrong format string iter for printk
      futex: comment requeue key reference semantics
    
    * 'irq-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      irq: fix cpumask memory leak on offstack cpumask kernels
    
    * 'timers-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      posix-timers: fix RLIMIT_CPU && setitimer(CPUCLOCK_PROF)
      posix-timers: fix RLIMIT_CPU && fork()
      timers: add missing kernel-doc

commit 6279a751fe096a21dc7704e918d570d3ff06e769
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Mar 27 01:06:07 2009 +0100

    posix-timers: fix RLIMIT_CPU && fork()
    
    See http://bugzilla.kernel.org/show_bug.cgi?id=12911
    
    copy_signal() copies signal->rlim, but RLIMIT_CPU is "lost". Because
    posix_cpu_timers_init_group() sets cputime_expires.prof_exp = 0 and thus
    fastpath_timer_check() returns false unless we have other expired cpu timers.
    
    Change copy_signal() to set cputime_expires.prof_exp if we have RLIMIT_CPU.
    Also, set cputimer.running = 1 in that case. This is not strictly necessary,
    but imho makes sense.
    
    Reported-by: Peter Lojkin <ia6432@inbox.ru>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Peter Lojkin <ia6432@inbox.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: stable@kernel.org
    LKML-Reference: <20090327000607.GA10104@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4854c2c4a82e..9b51a1b190d4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -808,6 +808,12 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 	sig->cputime_expires.virt_exp = cputime_zero;
 	sig->cputime_expires.sched_exp = 0;
 
+	if (sig->rlim[RLIMIT_CPU].rlim_cur != RLIM_INFINITY) {
+		sig->cputime_expires.prof_exp =
+			secs_to_cputime(sig->rlim[RLIMIT_CPU].rlim_cur);
+		sig->cputimer.running = 1;
+	}
+
 	/* The timer lists. */
 	INIT_LIST_HEAD(&sig->cpu_timers[0]);
 	INIT_LIST_HEAD(&sig->cpu_timers[1]);
@@ -823,11 +829,8 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 		atomic_inc(&current->signal->live);
 		return 0;
 	}
-	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);
-
-	if (sig)
-		posix_cpu_timers_init_group(sig);
 
+	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);
 	tsk->signal = sig;
 	if (!sig)
 		return -ENOMEM;
@@ -865,6 +868,8 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	memcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);
 	task_unlock(current->group_leader);
 
+	posix_cpu_timers_init_group(sig);
+
 	acct_init_pacct(&sig->pacct);
 
 	tty_audit_fork(sig);

commit 5ea472a77f8e4811ceee3f44a9deda6ad6e8b789
Merge: 6c009ecef8cc 577c9c456f0e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 8 10:35:30 2009 +0200

    Merge commit 'v2.6.30-rc1' into perfcounters/core
    
    Conflicts:
            arch/powerpc/include/asm/systbl.h
            arch/powerpc/include/asm/unistd.h
            include/linux/init_task.h
    
    Merge reason: the conflicts are non-trivial: PowerPC placement
                  of sys_perf_counter_open has to be mixed with the
                  new preadv/pwrite syscalls.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 0f4814065ff8c24ca8bfd75c9b73502be152c287
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Apr 3 16:43:48 2009 +0200

    x86, ptrace: add bts context unconditionally
    
    Add the ptrace bts context field to task_struct unconditionally.
    
    Initialize the field directly in copy_process().
    Remove all the unneeded functionality used to initialize that field.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Cc: roland@redhat.com
    Cc: eranian@googlemail.com
    Cc: oleg@redhat.com
    Cc: juan.villacis@intel.com
    Cc: ak@linux.jf.intel.com
    LKML-Reference: <20090403144603.292754000@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 660c2b8765bc..69bde7a22e9b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1086,8 +1086,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_DEBUG_MUTEXES
 	p->blocked_on = NULL; /* not blocked yet */
 #endif
-	if (unlikely(current->ptrace))
-		ptrace_fork(p, clone_flags);
+
+	p->bts = NULL;
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p, clone_flags);

commit 5e34437840d33554f69380584311743b39e8fbeb
Merge: 77d05632baee d508afb437da
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Apr 7 11:15:40 2009 +0200

    Merge branch 'linus' into core/softlockup
    
    Conflicts:
            kernel/sysctl.c

commit f541ae326fa120fa5c57433e4d9a133df212ce41
Merge: e255357764f9 0221c81b1b8e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 6 09:02:57 2009 +0200

    Merge branch 'linus' into perfcounters/core-v2
    
    Merge reason: we have gathered quite a few conflicts, need to merge upstream
    
    Conflicts:
            arch/powerpc/kernel/Makefile
            arch/x86/ia32/ia32entry.S
            arch/x86/include/asm/hardirq.h
            arch/x86/include/asm/unistd_32.h
            arch/x86/include/asm/unistd_64.h
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/irq.c
            arch/x86/kernel/syscall_table_32.S
            arch/x86/mm/iomap_32.c
            include/linux/sched.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 8fe74cf053de7ad2124a894996f84fa890a81093
Merge: c2eb2fa6d2b6 ced117c73edc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 2 21:09:10 2009 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs-2.6:
      Remove two unneeded exports and make two symbols static in fs/mpage.c
      Cleanup after commit 585d3bc06f4ca57f975a5a1f698f65a45ea66225
      Trim includes of fdtable.h
      Don't crap into descriptor table in binfmt_som
      Trim includes in binfmt_elf
      Don't mess with descriptor table in load_elf_binary()
      Get rid of indirect include of fs_struct.h
      New helper - current_umask()
      check_unsafe_exec() doesn't care about signal handlers sharing
      New locking/refcounting for fs_struct
      Take fs_struct handling to new file (fs/fs_struct.c)
      Get rid of bumping fs_struct refcount in pivot_root(2)
      Kill unsharing fs_struct in __set_personality()

commit 1b0f7ffd0ea27cd3a0b9ca04e3df9522048c32a3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:39 2009 -0700

    pids: kill signal_struct-> __pgrp/__session and friends
    
    We are wasting 2 words in signal_struct without any reason to implement
    task_pgrp_nr() and task_session_nr().
    
    task_session_nr() has no callers since
    2e2ba22ea4fd4bb85f0fa37c521066db6775cbef, we can remove it.
    
    task_pgrp_nr() is still (I believe wrongly) used in fs/autofsX and
    fs/coda.
    
    This patch reimplements task_pgrp_nr() via task_pgrp_nr_ns(), and kills
    __pgrp/__session and the related helpers.
    
    The change in drivers/char/tty_io.c is cosmetic, but hopefully makes sense
    anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Alan Cox <number6@the-village.bc.nu>          [tty parts]
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index adbea16ec649..f74458231449 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1265,8 +1265,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			p->signal->leader_pid = pid;
 			tty_kref_put(p->signal->tty);
 			p->signal->tty = tty_kref_get(current->signal->tty);
-			set_task_pgrp(p, task_pgrp_nr(current));
-			set_task_session(p, task_session_nr(current));
 			attach_pid(p, PIDTYPE_PGID, task_pgrp(current));
 			attach_pid(p, PIDTYPE_SID, task_session(current));
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);

commit b3bfa0cba867f23365b81658b47efd906830879b
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Thu Apr 2 16:58:08 2009 -0700

    signals: protect cinit from blocked fatal signals
    
    Normally SIG_DFL signals to global and container-init are dropped early.
    But if a signal is blocked when it is posted, we cannot drop the signal
    since the receiver may install a handler before unblocking the signal.
    Once this signal is queued however, the receiver container-init has no way
    of knowing if the signal was sent from an ancestor or descendant
    namespace.  This patch ensures that contianer-init drops all SIG_DFL
    signals in get_signal_to_deliver() except SIGKILL/SIGSTOP.
    
    If SIGSTOP/SIGKILL originate from a descendant of container-init they are
    never queued (i.e dropped in sig_ignored() in an earler patch).
    
    If SIGSTOP/SIGKILL originate from parent namespace, the signal is queued
    and container-init processes the signal.
    
    IOW, if get_signal_to_deliver() sees a sig_kernel_only() signal for global
    or container-init, the signal must have been generated internally or must
    have come from an ancestor ns and we process the signal.
    
    Further, the signal_group_exit() check was needed to cover the case of a
    multi-threaded init sending SIGKILL to other threads when doing an exit()
    or exec().  But since the new sig_kernel_only() check covers the SIGKILL,
    the signal_group_exit() check is no longer needed and can be removed.
    
    Finally, now that we have all pieces in place, set SIGNAL_UNKILLABLE for
    container-inits.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Daniel Lezcano <daniel.lezcano@free.fr>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d7eb727eb535..adbea16ec649 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -841,6 +841,8 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	atomic_set(&sig->live, 1);
 	init_waitqueue_head(&sig->wait_chldexit);
 	sig->flags = 0;
+	if (clone_flags & CLONE_NEWPID)
+		sig->flags |= SIGNAL_UNKILLABLE;
 	sig->group_exit_code = 0;
 	sig->group_exit_task = NULL;
 	sig->group_stop_count = 0;

commit 6f2c55b843836d26528c56a0968689accaedbc67
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 2 16:56:59 2009 -0700

    Simplify copy_thread()
    
    First argument unused since 2.3.11.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 51d1aa21483b..d7eb727eb535 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1125,7 +1125,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_mm;
 	if ((retval = copy_io(clone_flags, p)))
 		goto bad_fork_cleanup_namespaces;
-	retval = copy_thread(0, clone_flags, stack_start, stack_size, p, regs);
+	retval = copy_thread(clone_flags, stack_start, stack_size, p, regs);
 	if (retval)
 		goto bad_fork_cleanup_io;
 

commit 33e5d76979cf01e3834814fe0aea569d1d602c1a
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 2 16:56:32 2009 -0700

    nommu: fix a number of issues with the per-MM VMA patch
    
    Fix a number of issues with the per-MM VMA patch:
    
     (1) Make mmap_pages_allocated an atomic_long_t, just in case this is used on
         a NOMMU system with more than 2G pages.  Makes no difference on a 32-bit
         system.
    
     (2) Report vma->vm_pgoff * PAGE_SIZE as a 64-bit value, not a 32-bit value,
         lest it overflow.
    
     (3) Move the allocation of the vm_area_struct slab back for fork.c.
    
     (4) Use KMEM_CACHE() for both vm_area_struct and vm_region slabs.
    
     (5) Use BUG_ON() rather than if () BUG().
    
     (6) Make the default validate_nommu_regions() a static inline rather than a
         #define.
    
     (7) Make free_page_series()'s objection to pages with a refcount != 1 more
         informative.
    
     (8) Adjust the __put_nommu_region() banner comment to indicate that the
         semaphore must be held for writing.
    
     (9) Limit the number of warnings about munmaps of non-mmapped regions.
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Cc: Greg Ungerer <gerg@snapgear.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 47c15840a381..51d1aa21483b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1488,6 +1488,7 @@ void __init proc_caches_init(void)
 	mm_cachep = kmem_cache_create("mm_struct",
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
+	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC);
 	mmap_init();
 }
 

commit 5ad4e53bd5406ee214ddc5a41f03f779b8b2d526
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Mar 29 19:50:06 2009 -0400

    Get rid of indirect include of fs_struct.h
    
    Don't pull it in sched.h; very few files actually need it and those
    can include directly.  sched.h itself only needs forward declaration
    of struct fs_struct;
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 51f138a131de..e82a14577a98 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -60,6 +60,7 @@
 #include <linux/tty.h>
 #include <linux/proc_fs.h>
 #include <linux/blkdev.h>
+#include <linux/fs_struct.h>
 #include <trace/sched.h>
 #include <linux/magic.h>
 

commit 498052bba55ecaff58db6a1436b0e25bfd75a7ff
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Mar 30 07:20:30 2009 -0400

    New locking/refcounting for fs_struct
    
    * all changes of current->fs are done under task_lock and write_lock of
      old fs->lock
    * refcount is not atomic anymore (same protection)
    * its decrements are done when removing reference from current; at the
      same time we decide whether to free it.
    * put_fs_struct() is gone
    * new field - ->in_exec.  Set by check_unsafe_exec() if we are trying to do
      execve() and only subthreads share fs_struct.  Cleared when finishing exec
      (success and failure alike).  Makes CLONE_FS fail with -EAGAIN if set.
    * check_unsafe_exec() may fail with -EAGAIN if another execve() from subthread
      is in progress.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 05c02dc586b1..51f138a131de 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -683,11 +683,19 @@ static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
 
 static int copy_fs(unsigned long clone_flags, struct task_struct *tsk)
 {
+	struct fs_struct *fs = current->fs;
 	if (clone_flags & CLONE_FS) {
-		atomic_inc(&current->fs->count);
+		/* tsk->fs is already what we want */
+		write_lock(&fs->lock);
+		if (fs->in_exec) {
+			write_unlock(&fs->lock);
+			return -EAGAIN;
+		}
+		fs->users++;
+		write_unlock(&fs->lock);
 		return 0;
 	}
-	tsk->fs = copy_fs_struct(current->fs);
+	tsk->fs = copy_fs_struct(fs);
 	if (!tsk->fs)
 		return -ENOMEM;
 	return 0;
@@ -1518,12 +1526,16 @@ static int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)
 {
 	struct fs_struct *fs = current->fs;
 
-	if ((unshare_flags & CLONE_FS) &&
-	    (fs && atomic_read(&fs->count) > 1)) {
-		*new_fsp = copy_fs_struct(current->fs);
-		if (!*new_fsp)
-			return -ENOMEM;
-	}
+	if (!(unshare_flags & CLONE_FS) || !fs)
+		return 0;
+
+	/* don't need lock here; in the worst case we'll do useless copy */
+	if (fs->users == 1)
+		return 0;
+
+	*new_fsp = copy_fs_struct(fs);
+	if (!*new_fsp)
+		return -ENOMEM;
 
 	return 0;
 }
@@ -1639,8 +1651,13 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 
 		if (new_fs) {
 			fs = current->fs;
+			write_lock(&fs->lock);
 			current->fs = new_fs;
-			new_fs = fs;
+			if (--fs->users)
+				new_fs = NULL;
+			else
+				new_fs = fs;
+			write_unlock(&fs->lock);
 		}
 
 		if (new_mm) {
@@ -1679,7 +1696,7 @@ SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 
 bad_unshare_cleanup_fs:
 	if (new_fs)
-		put_fs_struct(new_fs);
+		free_fs_struct(new_fs);
 
 bad_unshare_cleanup_thread:
 bad_unshare_out:

commit 3e93cd671813e204c258f1e6c797959920cf7772
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Mar 29 19:00:13 2009 -0400

    Take fs_struct handling to new file (fs/fs_struct.c)
    
    Pure code move; two new helper functions for nfsd and daemonize
    (unshare_fs_struct() and daemonize_fs_struct() resp.; for now -
    the same code as used to be in callers).  unshare_fs_struct()
    exported (for nfsd, as copy_fs_struct()/exit_fs() used to be),
    copy_fs_struct() and exit_fs() don't need exports anymore.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 47c15840a381..05c02dc586b1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -681,38 +681,13 @@ static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
 	return retval;
 }
 
-static struct fs_struct *__copy_fs_struct(struct fs_struct *old)
-{
-	struct fs_struct *fs = kmem_cache_alloc(fs_cachep, GFP_KERNEL);
-	/* We don't need to lock fs - think why ;-) */
-	if (fs) {
-		atomic_set(&fs->count, 1);
-		rwlock_init(&fs->lock);
-		fs->umask = old->umask;
-		read_lock(&old->lock);
-		fs->root = old->root;
-		path_get(&old->root);
-		fs->pwd = old->pwd;
-		path_get(&old->pwd);
-		read_unlock(&old->lock);
-	}
-	return fs;
-}
-
-struct fs_struct *copy_fs_struct(struct fs_struct *old)
-{
-	return __copy_fs_struct(old);
-}
-
-EXPORT_SYMBOL_GPL(copy_fs_struct);
-
 static int copy_fs(unsigned long clone_flags, struct task_struct *tsk)
 {
 	if (clone_flags & CLONE_FS) {
 		atomic_inc(&current->fs->count);
 		return 0;
 	}
-	tsk->fs = __copy_fs_struct(current->fs);
+	tsk->fs = copy_fs_struct(current->fs);
 	if (!tsk->fs)
 		return -ENOMEM;
 	return 0;
@@ -1545,7 +1520,7 @@ static int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)
 
 	if ((unshare_flags & CLONE_FS) &&
 	    (fs && atomic_read(&fs->count) > 1)) {
-		*new_fsp = __copy_fs_struct(current->fs);
+		*new_fsp = copy_fs_struct(current->fs);
 		if (!*new_fsp)
 			return -ENOMEM;
 	}

commit 9489424454c93f4d225d7af47978f8c7e84bf4d4
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Mar 30 22:05:12 2009 -0600

    cpumask: use mm_cpumask() wrapper: kernel/fork.c
    
    Impact: futureproof
    
    Makes code futureproof against the impending change to mm->cpu_vm_mask.
    
    It's also a chance to use the new cpumask_ ops which take a pointer.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6715ebc3761d..47c15840a381 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -284,7 +284,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	mm->free_area_cache = oldmm->mmap_base;
 	mm->cached_hole_size = ~0UL;
 	mm->map_count = 0;
-	cpus_clear(mm->cpu_vm_mask);
+	cpumask_clear(mm_cpumask(mm));
 	mm->mm_rb = RB_ROOT;
 	rb_link = &mm->mm_rb.rb_node;
 	rb_parent = NULL;

commit 6e15cf04860074ad032e88c306bea656bbdd0f22
Merge: be0ea69674ed 60db56422043
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 26 21:39:17 2009 +0100

    Merge branch 'core/percpu' into percpu-cpumask-x86-for-linus-2
    
    Conflicts:
            arch/parisc/kernel/irq.c
            arch/x86/include/asm/fixmap_64.h
            arch/x86/include/asm/setup.h
            kernel/irq/handle.c
    
    Semantic merge:
            arch/x86/include/asm/fixmap.h
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 2d5516cbb9daf7d0e342a2e3b0fc6f8c39a81205
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 2 22:58:45 2009 +0100

    copy_process: fix CLONE_PARENT && parent_exec_id interaction
    
    CLONE_PARENT can fool the ->self_exec_id/parent_exec_id logic. If we
    re-use the old parent, we must also re-use ->parent_exec_id to make
    sure exit_notify() sees the right ->xxx_exec_id's when the CLONE_PARENT'ed
    task exits.
    
    Also, move down the "p->parent_exec_id = p->self_exec_id" thing, to place
    two different cases together.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Serge E. Hallyn <serge@hallyn.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a66fbde20715..4854c2c4a82e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1179,10 +1179,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 	clear_all_latency_tracing(p);
 
-	/* Our parent execution domain becomes current domain
-	   These must match for thread signalling to apply */
-	p->parent_exec_id = p->self_exec_id;
-
 	/* ok, now we should be set up.. */
 	p->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);
 	p->pdeath_signal = 0;
@@ -1220,10 +1216,13 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		set_task_cpu(p, smp_processor_id());
 
 	/* CLONE_PARENT re-uses the old parent */
-	if (clone_flags & (CLONE_PARENT|CLONE_THREAD))
+	if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
 		p->real_parent = current->real_parent;
-	else
+		p->parent_exec_id = current->parent_exec_id;
+	} else {
 		p->real_parent = current;
+		p->parent_exec_id = current->self_exec_id;
+	}
 
 	spin_lock(&current->sighand->siglock);
 

commit f8a6b2b9cee298a9663cbe38ce1eb5240987cb62
Merge: ba1511bf7fbd 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:44:22 2009 +0100

    Merge branch 'linus' into x86/apic
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c
            arch/x86/mm/fault.c

commit e9c4ffb11f0b19005b5b9dc8481687a3637e5887
Merge: 4bcf349a0f90 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:34:07 2009 +0100

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c

commit 871cafcc962fa1655c44b4f0e54d4c5cc14e273c
Merge: cf2592f59c0e b578f3fcca1e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 12 13:08:57 2009 +0100

    Merge branch 'linus' into core/softlockup

commit 94dba895333a4321f27360e42b807260ae36bda4
Merge: 9ce04f9238ca 4da94d49b2ec
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 11 08:24:32 2009 -0800

    Merge branch 'timers-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      timers: fix TIMER_ABSTIME for process wide cpu timers
      timers: split process wide cpu clocks/timers, fix
      x86: clean up hpet timer reinit
      timers: split process wide cpu clocks/timers, remove spurious warning
      timers: split process wide cpu clocks/timers
      signal: re-add dead task accumulation stats.
      x86: fix hpet timer reinit for x86_64
      sched: fix nohz load balancer on cpu offline

commit 9ce04f9238cafcfd09a502f2bc8c13b5f44ec590
Merge: b3f2caaaa824 06eb23b1ba39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 11 08:23:22 2009 -0800

    Merge branch 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      ptrace, x86: fix the usage of ptrace_fork()
      i8327: fix outb() parameter order
      x86: fix math_emu register frame access
      x86: math_emu info cleanup
      x86: include correct %gs in a.out core dump
      x86, vmi: put a missing paravirt_release_pmd in pgd_dtor
      x86: find nr_irqs_gsi with mp_ioapic_routing
      x86: add clflush before monitor for Intel 7400 series
      x86: disable intel_iommu support by default
      x86: don't apply __supported_pte_mask to non-present ptes
      x86: fix grammar in user-visible BIOS warning
      x86/Kconfig.cpu: make Kconfig help readable in the console
      x86, 64-bit: print DMI info in the oops trace

commit 06eb23b1ba39c61ee5d5faeb42a097635693e370
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Feb 9 02:02:33 2009 +0100

    ptrace, x86: fix the usage of ptrace_fork()
    
    I noticed by pure accident we have ptrace_fork() and friends. This was
    added by "x86, bts: add fork and exit handling", commit
    bf53de907dfdaac178c92d774aae7370d7b97d20.
    
    I can't test this, ds_request_bts() returns -EOPNOTSUPP, but I strongly
    believe this needs the fix. I think something like this program
    
            int main(void)
            {
                    int pid = fork();
    
                    if (!pid) {
                            ptrace(PTRACE_TRACEME, 0, NULL, NULL);
                            kill(getpid(), SIGSTOP);
                            fork();
                    } else {
                            struct ptrace_bts_config bts = {
                                    .flags = PTRACE_BTS_O_ALLOC,
                                    .size  = 4 * 4096,
                            };
    
                            wait(NULL);
    
                            ptrace(PTRACE_SETOPTIONS, pid, NULL, PTRACE_O_TRACEFORK);
                            ptrace(PTRACE_BTS_CONFIG, pid, &bts, sizeof(bts));
                            ptrace(PTRACE_CONT, pid, NULL, NULL);
    
                            sleep(1);
                    }
    
                    return 0;
            }
    
    should crash the kernel.
    
    If the task is traced by its natural parent ptrace_reparented() returns 0
    but we should clear ->btsxxx anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 242a706e7721..43c039d55e95 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1093,7 +1093,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_DEBUG_MUTEXES
 	p->blocked_on = NULL; /* not blocked yet */
 #endif
-	if (unlikely(ptrace_reparented(current)))
+	if (unlikely(current->ptrace))
 		ptrace_fork(p, clone_flags);
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */

commit 95fd4845ed0ffcab305b4f30ce1c12dc34f1b56c
Merge: d278c4843562 8e4921515c1a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Feb 11 09:22:04 2009 +0100

    Merge commit 'v2.6.29-rc4' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/setup_percpu.c
            arch/x86/mm/fault.c
            drivers/acpi/processor_idle.c
            kernel/irq/handle.c

commit 17406b82d621930cca8ccc1272cdac9a7dae8e40
Author: Mandeep Singh Baines <msb@google.com>
Date:   Fri Feb 6 15:37:47 2009 -0800

    softlockup: remove timestamp checking from hung_task
    
    Impact: saves sizeof(long) bytes per task_struct
    
    By guaranteeing that sysctl_hung_task_timeout_secs have elapsed between
    tasklist scans we can avoid using timestamps.
    
    Signed-off-by: Mandeep Singh Baines <msb@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index fb9444282836..bf582f75014b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -639,6 +639,9 @@ static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
 
 	tsk->min_flt = tsk->maj_flt = 0;
 	tsk->nvcsw = tsk->nivcsw = 0;
+#ifdef CONFIG_DETECT_HUNG_TASK
+	tsk->last_switch_count = tsk->nvcsw + tsk->nivcsw;
+#endif
 
 	tsk->mm = NULL;
 	tsk->active_mm = NULL;
@@ -1041,11 +1044,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->default_timer_slack_ns = current->timer_slack_ns;
 
-#ifdef CONFIG_DETECT_HUNG_TASK
-	p->last_switch_count = 0;
-	p->last_switch_timestamp = 0;
-#endif
-
 	task_io_accounting_init(&p->ioac);
 	acct_clear_integrals(p);
 

commit 04ec93fe9bc98e3bd8560f79f56fed66dfae40d5
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Feb 6 08:17:19 2009 +0000

    fork.c: fix NULL pointer dereference when nr_threads == threads-max
    
    I happened to forked lots of processes, and hit NULL pointer dereference.
    It is because in copy_process() after checking max_threads, 0 is returned
    but not -EAGAIN.
    
    The bug is introduced by "CRED: Detach the credentials from task_struct"
    (commit f1752eec6145c97163dbce62d17cf5d928e28a27).
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 242a706e7721..6d5dbb7a13e2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1005,6 +1005,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * triggers too late. This doesn't hurt, the check is only there
 	 * to stop root fork bombs.
 	 */
+	retval = -EAGAIN;
 	if (nr_threads >= max_threads)
 		goto bad_fork_cleanup_count;
 

commit 32bd671d6cbeda60dc73be77fa2b9037d9a9bfa0
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Feb 5 12:24:15 2009 +0100

    signal: re-add dead task accumulation stats.
    
    We're going to split the process wide cpu accounting into two parts:
    
     - clocks; which can take all the time they want since they run
               from user context.
    
     - timers; which need constant time tracing but can affort the overhead
               because they're default off -- and rare.
    
    The clock readout will go back to a full sum of the thread group, for this
    we need to re-add the exit stats that were removed in the initial itimer
    rework (f06febc9: timers: fix itimer/many thread hang).
    
    Furthermore, since that full sum can be rather slow for large thread groups
    and we have the complete dead task stats, revert the do_notify_parent time
    computation.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 242a706e7721..e8e854a04ad2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -851,13 +851,14 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->tty_old_pgrp = NULL;
 	sig->tty = NULL;
 
-	sig->cutime = sig->cstime = cputime_zero;
+	sig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;
 	sig->gtime = cputime_zero;
 	sig->cgtime = cputime_zero;
 	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
 	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
 	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;
 	task_io_accounting_init(&sig->ioac);
+	sig->sum_sched_runtime = 0;
 	taskstats_tgid_init(sig);
 
 	task_lock(current->group_leader);

commit 3ddeb51d9c83931c1ca6abf76a38934c5a1ed918
Merge: 5a611268b69f 5ee810072175
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 27 12:01:51 2009 +0100

    Merge branch 'linus' into core/percpu
    
    Conflicts:
            arch/x86/kernel/setup_percpu.c

commit 1e70c7f7a9d4a3d2cc78b40e1d7768d99cd79899
Merge: 810ee58de26c 1d4a7f1c4faf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 26 09:47:43 2009 -0800

    Merge branch 'timers-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'timers-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      hrtimers: fix inconsistent lock state on resume in hres_timers_resume
      time-sched.c: tick_nohz_update_jiffies should be static
      locking, hpet: annotate false positive warning
      kernel/fork.c: unused variable 'ret'
      itimers: remove the per-cpu-ish-ness

commit bfe2a3c3b5bf479788d5d5c5561346be6b169043
Merge: 77835492ed48 35d266a24796
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 23 10:20:15 2009 +0100

    Merge branch 'core/percpu' into perfcounters/core
    
    Conflicts:
            arch/x86/include/asm/hardirq_32.h
            arch/x86/include/asm/hardirq_64.h
    
    Semantic merge:
            arch/x86/include/asm/hardirq.h
            [ added apic_perf_irqs field. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 77835492ed489c0b870f82f4c50687bd267acc0a
Merge: af37501c7921 1de9e8e70f5a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 21 16:37:27 2009 +0100

    Merge commit 'v2.6.29-rc2' into perfcounters/core
    
    Conflicts:
            include/linux/syscalls.h

commit 198030782cedf25391e67e7c88b04f87a5eb6563
Merge: 4ec71fa2d2c3 92181f190b64
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 21 10:39:51 2009 +0100

    Merge branch 'x86/mm' into core/percpu
    
    Conflicts:
            arch/x86/mm/fault.c

commit b2b062b8163391c42b3219d466ca1ac9742b9c7b
Merge: a9de18eb761f 99937d6455ce
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 18 18:37:14 2009 +0100

    Merge branch 'core/percpu' into stackprotector
    
    Conflicts:
            arch/x86/include/asm/pda.h
            arch/x86/include/asm/system.h
    
    Also, moved include/asm-x86/stackprotector.h to arch/x86/include/asm.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit af432eb1cc3178ec7109aca2283aafb1c12ccac1
Author: Mandeep Singh Baines <msb@google.com>
Date:   Fri Jan 16 09:09:38 2009 -0800

    softlockup: fix to allow compiling with !DETECT_HUNG_TASK
    
    Fixes the following compile error:
    
     kernel/fork.c:1049: error: 'struct task_struct' has no member named 'last_switch_count'
     kernel/fork.c:1050: error: 'struct task_struct' has no member named 'last_switch_timestamp'
    
    Signed-off-by: Mandeep Singh Baines <msb@google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1d68f1255dd8..fb9444282836 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1041,7 +1041,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->default_timer_slack_ns = current->timer_slack_ns;
 
-#ifdef CONFIG_DETECT_SOFTLOCKUP
+#ifdef CONFIG_DETECT_HUNG_TASK
 	p->last_switch_count = 0;
 	p->last_switch_timestamp = 0;
 #endif

commit 6559eed8ca7db0531a207cd80be5e28cd6f213c5
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 14 14:14:32 2009 +0100

    [CVE-2009-0029] System call wrappers part 30
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8eb37d38c6a4..bf0cef8bbdf2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1603,7 +1603,7 @@ static int unshare_fd(unsigned long unshare_flags, struct files_struct **new_fdp
  * constructed. Here we are modifying the current, active,
  * task_struct.
  */
-asmlinkage long sys_unshare(unsigned long unshare_flags)
+SYSCALL_DEFINE1(unshare, unsigned long, unshare_flags)
 {
 	int err = 0;
 	struct fs_struct *fs, *new_fs = NULL;

commit 17da2bd90abf428523de0fb98f7075e00e3ed42e
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 14 14:14:10 2009 +0100

    [CVE-2009-0029] System call wrappers part 08
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1d68f1255dd8..8eb37d38c6a4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -901,7 +901,7 @@ static void copy_flags(unsigned long clone_flags, struct task_struct *p)
 	clear_freeze_flag(p);
 }
 
-asmlinkage long sys_set_tid_address(int __user *tidptr)
+SYSCALL_DEFINE1(set_tid_address, int __user *, tidptr)
 {
 	current->clear_child_tid = tidptr;
 

commit 783adf42cf039083dd3c734c07c3bdc707e2bb15
Author: Steven Noonan <steven@uplinklabs.net>
Date:   Sun Jan 11 01:04:21 2009 -0800

    kernel/fork.c: unused variable 'ret'
    
    Removed the unused variable.
    
    Signed-off-by: Steven Noonan <steven@uplinklabs.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index e995899ea83f..81da4aae85cb 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -817,7 +817,6 @@ static void posix_cpu_timers_init_group(struct signal_struct *sig)
 static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct signal_struct *sig;
-	int ret;
 
 	if (clone_flags & CLONE_THREAD) {
 		atomic_inc(&current->signal->count);

commit d19b85db9d5c44a4c21dcb10d6fbadaa4425ab2a
Merge: 490dea45d00f c59765042f53
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 11 15:34:05 2009 +0100

    Merge commit 'v2.6.29-rc1' into timers/urgent

commit 506c10f26c481b7f8ef27c1c79290f68989b2e9e
Merge: e1df957670ae c59765042f53
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 11 02:42:53 2009 +0100

    Merge commit 'v2.6.29-rc1' into perfcounters/core
    
    Conflicts:
            include/linux/kernel_stat.h

commit c40f6f8bbc4cbd2902671aacd587400ddca62627
Merge: 1a7d0f0bec4b cb6ff208076b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 9 14:00:58 2009 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-2.6-nommu
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-2.6-nommu:
      NOMMU: Support XIP on initramfs
      NOMMU: Teach kobjsize() about VMA regions.
      FLAT: Don't attempt to expand the userspace stack to fill the space allocated
      FDPIC: Don't attempt to expand the userspace stack to fill the space allocated
      NOMMU: Improve procfs output using per-MM VMAs
      NOMMU: Make mmap allocation page trimming behaviour configurable.
      NOMMU: Make VMAs per MM as for MMU-mode linux
      NOMMU: Delete askedalloc and realalloc variables
      NOMMU: Rename ARM's struct vm_region
      NOMMU: Fix cleanup handling in ramfs_nommu_get_umapped_area()

commit 61bce0f1371cfff497fe85594fd39d1a0b15ebe1
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Jan 7 18:08:49 2009 -0800

    pid: generalize task_active_pid_ns
    
    Currently task_active_pid_ns is not safe to call after a task becomes a
    zombie and exit_task_namespaces is called, as nsproxy becomes NULL.  By
    reading the pid namespace from the pid of the task we can trivially solve
    this problem at the cost of one extra memory read in what should be the
    same cacheline as we read the namespace from.
    
    When moving things around I have made task_active_pid_ns out of line
    because keeping it in pid_namespace.h would require adding includes of
    pid.h and sched.h that I don't think we want.
    
    This change does make task_active_pid_ns unsafe to call during
    copy_process until we attach a pid on the task_struct which seems to be a
    reasonable trade off.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Bastian Blank <bastian@waldi.eu.org>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Nadia Derbey <Nadia.Derbey@bull.net>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7b8f2a78be3d..4018308048cf 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1126,12 +1126,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	if (pid != &init_struct_pid) {
 		retval = -ENOMEM;
-		pid = alloc_pid(task_active_pid_ns(p));
+		pid = alloc_pid(p->nsproxy->pid_ns);
 		if (!pid)
 			goto bad_fork_cleanup_io;
 
 		if (clone_flags & CLONE_NEWPID) {
-			retval = pid_ns_prepare_proc(task_active_pid_ns(p));
+			retval = pid_ns_prepare_proc(p->nsproxy->pid_ns);
 			if (retval < 0)
 				goto bad_fork_free_pid;
 		}

commit 8feae13110d60cc6287afabc2887366b0eb226c2
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jan 8 12:04:47 2009 +0000

    NOMMU: Make VMAs per MM as for MMU-mode linux
    
    Make VMAs per mm_struct as for MMU-mode linux.  This solves two problems:
    
     (1) In SYSV SHM where nattch for a segment does not reflect the number of
         shmat's (and forks) done.
    
     (2) In mmap() where the VMA's vm_mm is set to point to the parent mm by an
         exec'ing process when VM_EXECUTABLE is specified, regardless of the fact
         that a VMA might be shared and already have its vm_mm assigned to another
         process or a dead process.
    
    A new struct (vm_region) is introduced to track a mapped region and to remember
    the circumstances under which it may be shared and the vm_list_struct structure
    is discarded as it's no longer required.
    
    This patch makes the following additional changes:
    
     (1) Regions are now allocated with alloc_pages() rather than kmalloc() and
         with no recourse to __GFP_COMP, so the pages are not composite.  Instead,
         each page has a reference on it held by the region.  Anything else that is
         interested in such a page will have to get a reference on it to retain it.
         When the pages are released due to unmapping, each page is passed to
         put_page() and will be freed when the page usage count reaches zero.
    
     (2) Excess pages are trimmed after an allocation as the allocation must be
         made as a power-of-2 quantity of pages.
    
     (3) VMAs are added to the parent MM's R/B tree and mmap lists.  As an MM may
         end up with overlapping VMAs within the tree, the VMA struct address is
         appended to the sort key.
    
     (4) Non-anonymous VMAs are now added to the backing inode's prio list.
    
     (5) Holes may be punched in anonymous VMAs with munmap(), releasing parts of
         the backing region.  The VMA and region structs will be split if
         necessary.
    
     (6) sys_shmdt() only releases one attachment to a SYSV IPC shared memory
         segment instead of all the attachments at that addresss.  Multiple
         shmat()'s return the same address under NOMMU-mode instead of different
         virtual addresses as under MMU-mode.
    
     (7) Core dumping for ELF-FDPIC requires fewer exceptions for NOMMU-mode.
    
     (8) /proc/maps is now the global list of mapped regions, and may list bits
         that aren't actually mapped anywhere.
    
     (9) /proc/meminfo gains a line (tagged "MmapCopy") that indicates the amount
         of RAM currently allocated by mmap to hold mappable regions that can't be
         mapped directly.  These are copies of the backing device or file if not
         anonymous.
    
    These changes make NOMMU mode more similar to MMU mode.  The downside is that
    NOMMU mode requires some extra memory to track things over NOMMU without this
    patch (VMAs are no longer shared, and there are now region structs).
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Mike Frysinger <vapier.adi@gmail.com>
    Acked-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7b8f2a78be3d..0bce4a43bb37 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1481,12 +1481,10 @@ void __init proc_caches_init(void)
 	fs_cachep = kmem_cache_create("fs_cache",
 			sizeof(struct fs_struct), 0,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
-	vm_area_cachep = kmem_cache_create("vm_area_struct",
-			sizeof(struct vm_area_struct), 0,
-			SLAB_PANIC, NULL);
 	mm_cachep = kmem_cache_create("mm_struct",
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
+	mmap_init();
 }
 
 /*

commit 490dea45d00f01847ebebd007685d564aaf2cd98
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 24 17:06:57 2008 +0100

    itimers: remove the per-cpu-ish-ness
    
    Either we bounce once cacheline per cpu per tick, yielding n^2 bounces
    or we just bounce a single..
    
    Also, using per-cpu allocations for the thread-groups complicates the
    per-cpu allocator in that its currently aimed to be a fixed sized
    allocator and the only possible extention to that would be vmap based,
    which is seriously constrained on 32 bit archs.
    
    So making the per-cpu memory requirement depend on the number of
    processes is an issue.
    
    Lastly, it didn't deal with cpu-hotplug, although admittedly that might
    be fixable.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7b8f2a78be3d..7087d8c0e5e2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -820,14 +820,15 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	int ret;
 
 	if (clone_flags & CLONE_THREAD) {
-		ret = thread_group_cputime_clone_thread(current);
-		if (likely(!ret)) {
-			atomic_inc(&current->signal->count);
-			atomic_inc(&current->signal->live);
-		}
-		return ret;
+		atomic_inc(&current->signal->count);
+		atomic_inc(&current->signal->live);
+		return 0;
 	}
 	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);
+
+	if (sig)
+		posix_cpu_timers_init_group(sig);
+
 	tsk->signal = sig;
 	if (!sig)
 		return -ENOMEM;
@@ -864,8 +865,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	memcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);
 	task_unlock(current->group_leader);
 
-	posix_cpu_timers_init_group(sig);
-
 	acct_init_pacct(&sig->pacct);
 
 	tty_audit_fork(sig);

commit 4cb0e11b15d2badad455fcd538af0cccf05dc012
Author: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
Date:   Tue Jan 6 14:42:47 2009 -0800

    coredump_filter: permit changing of the default filter
    
    Introduce a new kernel parameter `coredump_filter'.  Setting a value to
    this parameter causes the default bitmask of coredump_filter to be
    changed.
    
    It is useful for users to change coredump_filter settings for the whole
    system at boot time.  Without this parameter, users have to change
    coredump_filter settings for each /proc/<pid>/ in an initializing script.
    
    Signed-off-by: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 23b912116675..7b8f2a78be3d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -400,6 +400,18 @@ __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
 #define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
 #define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
 
+static unsigned long default_dump_filter = MMF_DUMP_FILTER_DEFAULT;
+
+static int __init coredump_filter_setup(char *s)
+{
+	default_dump_filter =
+		(simple_strtoul(s, NULL, 0) << MMF_DUMP_FILTER_SHIFT) &
+		MMF_DUMP_FILTER_MASK;
+	return 1;
+}
+
+__setup("coredump_filter=", coredump_filter_setup);
+
 #include <linux/init_task.h>
 
 static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
@@ -408,8 +420,7 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 	atomic_set(&mm->mm_count, 1);
 	init_rwsem(&mm->mmap_sem);
 	INIT_LIST_HEAD(&mm->mmlist);
-	mm->flags = (current->mm) ? current->mm->flags
-				  : MMF_DUMP_FILTER_DEFAULT;
+	mm->flags = (current->mm) ? current->mm->flags : default_dump_filter;
 	mm->core_state = NULL;
 	mm->nr_ptes = 0;
 	set_mm_counter(mm, file_rss, 0);

commit 60348802e9cb137ee86590c3e4c57c1ec2e8fc69
Author: Zhaolei <zhaolei@cn.fujitsu.com>
Date:   Tue Jan 6 14:40:46 2009 -0800

    fork.c: cleanup for copy_sighand()
    
    Check CLONE_SIGHAND only is enough, because combination of CLONE_THREAD and
    CLONE_SIGHAND is already done in copy_process().
    
    Impact: cleanup, no functionality changed
    
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 43cbf30669e6..23b912116675 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -758,7 +758,7 @@ static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct sighand_struct *sig;
 
-	if (clone_flags & (CLONE_SIGHAND | CLONE_THREAD)) {
+	if (clone_flags & CLONE_SIGHAND) {
 		atomic_inc(&current->sighand->count);
 		return 0;
 	}

commit a9de18eb761f7c1c860964b2e5addc1a35c7e861
Merge: b2aaf8f74cdc 6a94cb73064c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 31 08:31:57 2008 +0100

    Merge branch 'linus' into stackprotector
    
    Conflicts:
            arch/x86/include/asm/pda.h
            kernel/fork.c

commit e1df957670aef74ffd9a4ad93e6d2c90bf6b4845
Merge: 2b583d8bc8d7 3c92ec8ae91e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Dec 29 09:45:15 2008 +0100

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            fs/exec.c
            include/linux/init_task.h
    
    Simple context conflicts.

commit abf137dd7712132ee56d5b3143c2ff61a72a5faa
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Dec 9 08:11:22 2008 +0100

    aio: make the lookup_ioctx() lockless
    
    The mm->ioctx_list is currently protected by a reader-writer lock,
    so we always grab that lock on the read side for doing ioctx
    lookups. As the workload is extremely reader biased, turn this into
    an rcu hlist so we can make lookup_ioctx() lockless. Get rid of
    the rwlock and use a spinlock for providing update side exclusion.
    
    There's usually only 1 entry on this list, so it doesn't make sense
    to look into fancier data structures.
    
    Reviewed-by: Jeff Moyer <jmoyer@redhat.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6144b36cd897..43cbf30669e6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -415,8 +415,8 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 	set_mm_counter(mm, file_rss, 0);
 	set_mm_counter(mm, anon_rss, 0);
 	spin_lock_init(&mm->page_table_lock);
-	rwlock_init(&mm->ioctx_list_lock);
-	mm->ioctx_list = NULL;
+	spin_lock_init(&mm->ioctx_lock);
+	INIT_HLIST_HEAD(&mm->ioctx_list);
 	mm->free_area_cache = TASK_UNMAPPED_BASE;
 	mm->cached_hole_size = ~0UL;
 	mm_init_owner(mm, p);

commit b0f4b285d7ed174804658539129a834270f4829a
Merge: be9c5ae4eeec 5250d329e38c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 28 12:21:10 2008 -0800

    Merge branch 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (241 commits)
      sched, trace: update trace_sched_wakeup()
      tracing/ftrace: don't trace on early stage of a secondary cpu boot, v3
      Revert "x86: disable X86_PTRACE_BTS"
      ring-buffer: prevent false positive warning
      ring-buffer: fix dangling commit race
      ftrace: enable format arguments checking
      x86, bts: memory accounting
      x86, bts: add fork and exit handling
      ftrace: introduce tracing_reset_online_cpus() helper
      tracing: fix warnings in kernel/trace/trace_sched_switch.c
      tracing: fix warning in kernel/trace/trace.c
      tracing/ring-buffer: remove unused ring_buffer size
      trace: fix task state printout
      ftrace: add not to regex on filtering functions
      trace: better use of stack_trace_enabled for boot up code
      trace: add a way to enable or disable the stack tracer
      x86: entry_64 - introduce FTRACE_ frame macro v2
      tracing/ftrace: add the printk-msg-only option
      tracing/ftrace: use preempt_enable_no_resched_notrace in ring_buffer_time_stamp()
      x86, bts: correctly report invalid bts records
      ...
    
    Fixed up trivial conflict in scripts/recordmcount.pl due to SH bits
    being already partly merged by the SH merge.

commit cbacc2c7f066a1e01b33b0e27ae5efbf534bc2db
Merge: 4a6908a3a050 74192246910f
Author: James Morris <jmorris@namei.org>
Date:   Thu Dec 25 11:40:09 2008 +1100

    Merge branch 'next' into for-linus

commit bf53de907dfdaac178c92d774aae7370d7b97d20
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Dec 19 15:10:24 2008 +0100

    x86, bts: add fork and exit handling
    
    Impact: introduce new ptrace facility
    
    Add arch_ptrace_untrace() function that is called when the tracer
    detaches (either voluntarily or when the tracing task dies);
    ptrace_disable() is only called on a voluntary detach.
    
    Add ptrace_fork() and arch_ptrace_fork(). They are called when a
    traced task is forked.
    
    Clear DS and BTS related fields on fork.
    
    Release DS resources and reclaim memory in ptrace_untrace(). This
    releases resources already when the tracing task dies. We used to do
    that when the traced task dies.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7b93da72d4a2..65ce60adc8e8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1096,6 +1096,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_DEBUG_MUTEXES
 	p->blocked_on = NULL; /* not blocked yet */
 #endif
+	if (unlikely(ptrace_reparented(current)))
+		ptrace_fork(p, clone_flags);
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p, clone_flags);

commit 30cd324e9787ccc9a5ede59742d5409857550692
Merge: c71dd42db2c6 6d102bc68f3d 3d9101e92529
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 19 09:42:40 2008 +0100

    Merge branches 'tracing/ftrace', 'tracing/ring-buffer' and 'tracing/urgent' into tracing/core
    
    Conflicts:
            include/linux/ftrace.h

commit 92bf73e90a35d40ebc1446488218f03833b36f86
Merge: 447557ac7ce1 915b0d0104b7
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 12 12:00:02 2008 +0100

    Merge branch 'x86/irq' into perfcounters/core
    
    ( with manual semantic merge of arch/x86/kernel/cpu/perf_counter.c )

commit b88ed20594db2c685555b68c52b693b75738b2f5
Author: Hugh Dickins <hugh@veritas.com>
Date:   Wed Dec 10 20:48:52 2008 +0000

    fix mapping_writably_mapped()
    
    Lee Schermerhorn noticed yesterday that I broke the mapping_writably_mapped
    test in 2.6.7!  Bad bad bug, good good find.
    
    The i_mmap_writable count must be incremented for VM_SHARED (just as
    i_writecount is for VM_DENYWRITE, but while holding the i_mmap_lock)
    when dup_mmap() copies the vma for fork: it has its own more optimal
    version of __vma_link_file(), and I missed this out.  So the count
    was later going down to 0 (dangerous) when one end unmapped, then
    wrapping negative (inefficient) when the other end unmapped.
    
    The only impact on x86 would have been that setting a mandatory lock on
    a file which has at some time been opened O_RDWR and mapped MAP_SHARED
    (but not necessarily PROT_WRITE) across a fork, might fail with -EAGAIN
    when it should succeed, or succeed when it should fail.
    
    But those architectures which rely on flush_dcache_page() to flush
    userspace modifications back into the page before the kernel reads it,
    may in some cases have skipped the flush after such a fork - though any
    repetitive test will soon wrap the count negative, in which case it will
    flush_dcache_page() unnecessarily.
    
    Fix would be a two-liner, but mapping variable added, and comment moved.
    
    Reported-by: Lee Schermerhorn <Lee.Schermerhorn@hp.com>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8d6a7dd9282b..495da2e9a8b4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -315,17 +315,20 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		file = tmp->vm_file;
 		if (file) {
 			struct inode *inode = file->f_path.dentry->d_inode;
+			struct address_space *mapping = file->f_mapping;
+
 			get_file(file);
 			if (tmp->vm_flags & VM_DENYWRITE)
 				atomic_dec(&inode->i_writecount);
-
-			/* insert tmp into the share list, just after mpnt */
-			spin_lock(&file->f_mapping->i_mmap_lock);
+			spin_lock(&mapping->i_mmap_lock);
+			if (tmp->vm_flags & VM_SHARED)
+				mapping->i_mmap_writable++;
 			tmp->vm_truncate_count = mpnt->vm_truncate_count;
-			flush_dcache_mmap_lock(file->f_mapping);
+			flush_dcache_mmap_lock(mapping);
+			/* insert tmp into the share list, just after mpnt */
 			vma_prio_tree_add(tmp, mpnt);
-			flush_dcache_mmap_unlock(file->f_mapping);
-			spin_unlock(&file->f_mapping->i_mmap_lock);
+			flush_dcache_mmap_unlock(mapping);
+			spin_unlock(&mapping->i_mmap_lock);
 		}
 
 		/*

commit a64e64944f4b8ce3288519555dbaa0232414b8ac
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Nov 12 18:37:41 2008 -0500

    [PATCH] return records for fork() both to child and parent
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2a372a0e206f..8d6a7dd9282b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1398,6 +1398,7 @@ long do_fork(unsigned long clone_flags,
 			init_completion(&vfork);
 		}
 
+		audit_finish_fork(p);
 		tracehook_report_clone(trace, regs, clone_flags, nr, p);
 
 		/*

commit 0793a61d4df8daeac6492dbf8d2f3e5713caae5e
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 4 20:12:29 2008 +0100

    performance counters: core code
    
    Implement the core kernel bits of Performance Counters subsystem.
    
    The Linux Performance Counter subsystem provides an abstraction of
    performance counter hardware capabilities. It provides per task and per
    CPU counters, and it provides event capabilities on top of those.
    
    Performance counters are accessed via special file descriptors.
    There's one file descriptor per virtual counter used.
    
    The special file descriptor is opened via the perf_counter_open()
    system call:
    
     int
     perf_counter_open(u32 hw_event_type,
                       u32 hw_event_period,
                       u32 record_type,
                       pid_t pid,
                       int cpu);
    
    The syscall returns the new fd. The fd can be used via the normal
    VFS system calls: read() can be used to read the counter, fcntl()
    can be used to set the blocking mode, etc.
    
    Multiple counters can be kept open at a time, and the counters
    can be poll()ed.
    
    See more details in Documentation/perf-counters.txt.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2a372a0e206f..441fadff1fa4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -975,6 +975,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto fork_out;
 
 	rt_mutex_init_task(p);
+	perf_counter_init_task(p);
 
 #ifdef CONFIG_PROVE_LOCKING
 	DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);

commit 7657d90497f98426af17f0ac633a9b335bb7a8fb
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Wed Dec 3 13:17:33 2008 -0600

    user namespaces: require cap_set{ug}id for CLONE_NEWUSER
    
    While ideally CLONE_NEWUSER will eventually require no
    privilege, the required permission checks are currently
    not there.  As a result, CLONE_NEWUSER has the same effect
    as a setuid(0)+setgroups(1,"0").  While we already require
    CAP_SYS_ADMIN, requiring CAP_SETUID and CAP_SETGID seems
    appropriate.
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1dd89451fae4..e3a85b33107e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1344,7 +1344,8 @@ long do_fork(unsigned long clone_flags,
 		/* hopefully this check will go away when userns support is
 		 * complete
 		 */
-		if (!capable(CAP_SYS_ADMIN))
+		if (!capable(CAP_SYS_ADMIN) || !capable(CAP_SETUID) ||
+				!capable(CAP_SETGID))
 			return -EPERM;
 	}
 

commit e8e1abe92fd7ea9d823a3aaf81d10e2cba593b6b
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Dec 3 11:04:51 2008 -0500

    ftrace: fix race in function graph during fork
    
    Impact: graph tracer race/crash fix
    
    There is a nasy race in startup of a new process running the
    function graph tracer. In fork.c:
    
            total_forks++;
            spin_unlock(&current->sighand->siglock);
            write_unlock_irq(&tasklist_lock);
            ftrace_graph_init_task(p);
            proc_fork_connector(p);
            cgroup_post_fork(p);
            return p;
    
    The new task is free to run as soon as the tasklist_lock is released.
    This is before the ftrace_graph_init_task. If the task does run
    it will be using the same ret_stack and curr_ret_stack as the parent.
    This will cause crashes that are difficult to debug.
    
    This patch moves the ftrace_graph_init_task to just after the alloc_pid
    code. This fixes the above race.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5f82a999c032..7407ab319875 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1137,6 +1137,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		}
 	}
 
+	ftrace_graph_init_task(p);
+
 	p->pid = pid_nr(pid);
 	p->tgid = p->pid;
 	if (clone_flags & CLONE_THREAD)
@@ -1145,7 +1147,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (current->nsproxy != p->nsproxy) {
 		retval = ns_cgroup_clone(p, pid);
 		if (retval)
-			goto bad_fork_free_pid;
+			goto bad_fork_free_graph;
 	}
 
 	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
@@ -1238,7 +1240,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		spin_unlock(&current->sighand->siglock);
 		write_unlock_irq(&tasklist_lock);
 		retval = -ERESTARTNOINTR;
-		goto bad_fork_free_pid;
+		goto bad_fork_free_graph;
 	}
 
 	if (clone_flags & CLONE_THREAD) {
@@ -1271,11 +1273,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	total_forks++;
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
-	ftrace_graph_init_task(p);
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
 	return p;
 
+bad_fork_free_graph:
+	ftrace_graph_exit_task(p);
 bad_fork_free_pid:
 	if (pid != &init_struct_pid)
 		free_pid(pid);

commit fb52607afcd0629776f1dc9e657647ceae81dd50
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 25 21:07:04 2008 +0100

    tracing/function-return-tracer: change the name into function-graph-tracer
    
    Impact: cleanup
    
    This patch changes the name of the "return function tracer" into
    function-graph-tracer which is a more suitable name for a tracing
    which makes one able to retrieve the ordered call stack during
    the code flow.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index d6e1a3205f62..5f82a999c032 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -140,7 +140,7 @@ void free_task(struct task_struct *tsk)
 	prop_local_destroy_single(&tsk->dirties);
 	free_thread_info(tsk->stack);
 	rt_mutex_debug_task_free(tsk);
-	ftrace_retfunc_exit_task(tsk);
+	ftrace_graph_exit_task(tsk);
 	free_task_struct(tsk);
 }
 EXPORT_SYMBOL(free_task);
@@ -1271,7 +1271,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	total_forks++;
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
-	ftrace_retfunc_init_task(p);
+	ftrace_graph_init_task(p);
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
 	return p;

commit 18b6e0414e42d95183f07d8177e3ff0241abd825
Author: Serge Hallyn <serue@us.ibm.com>
Date:   Wed Oct 15 16:38:45 2008 -0500

    User namespaces: set of cleanups (v2)
    
    The user_ns is moved from nsproxy to user_struct, so that a struct
    cred by itself is sufficient to determine access (which it otherwise
    would not be).  Corresponding ecryptfs fixes (by David Howells) are
    here as well.
    
    Fix refcounting.  The following rules now apply:
            1. The task pins the user struct.
            2. The user struct pins its user namespace.
            3. The user namespace pins the struct user which created it.
    
    User namespaces are cloned during copy_creds().  Unsharing a new user_ns
    is no longer possible.  (We could re-add that, but it'll cause code
    duplication and doesn't seem useful if PAM doesn't need to clone user
    namespaces).
    
    When a user namespace is created, its first user (uid 0) gets empty
    keyrings and a clean group_info.
    
    This incorporates a previous patch by David Howells.  Here
    is his original patch description:
    
    >I suggest adding the attached incremental patch.  It makes the following
    >changes:
    >
    > (1) Provides a current_user_ns() macro to wrap accesses to current's user
    >     namespace.
    >
    > (2) Fixes eCryptFS.
    >
    > (3) Renames create_new_userns() to create_user_ns() to be more consistent
    >     with the other associated functions and because the 'new' in the name is
    >     superfluous.
    >
    > (4) Moves the argument and permission checks made for CLONE_NEWUSER to the
    >     beginning of do_fork() so that they're done prior to making any attempts
    >     at allocation.
    >
    > (5) Calls create_user_ns() after prepare_creds(), and gives it the new creds
    >     to fill in rather than have it return the new root user.  I don't imagine
    >     the new root user being used for anything other than filling in a cred
    >     struct.
    >
    >     This also permits me to get rid of a get_uid() and a free_uid(), as the
    >     reference the creds were holding on the old user_struct can just be
    >     transferred to the new namespace's creator pointer.
    >
    > (6) Makes create_user_ns() reset the UIDs and GIDs of the creds under
    >     preparation rather than doing it in copy_creds().
    >
    >David
    
    >Signed-off-by: David Howells <dhowells@redhat.com>
    
    Changelog:
            Oct 20: integrate dhowells comments
                    1. leave thread_keyring alone
                    2. use current_user_ns() in set_user()
    
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 29c18c14812d..1dd89451fae4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -976,7 +976,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (atomic_read(&p->real_cred->user->processes) >=
 			p->signal->rlim[RLIMIT_NPROC].rlim_cur) {
 		if (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&
-		    p->real_cred->user != current->nsproxy->user_ns->root_user)
+		    p->real_cred->user != INIT_USER)
 			goto bad_fork_free;
 	}
 
@@ -1334,6 +1334,20 @@ long do_fork(unsigned long clone_flags,
 	int trace = 0;
 	long nr;
 
+	/*
+	 * Do some preliminary argument and permissions checking before we
+	 * actually start allocating stuff
+	 */
+	if (clone_flags & CLONE_NEWUSER) {
+		if (clone_flags & CLONE_THREAD)
+			return -EINVAL;
+		/* hopefully this check will go away when userns support is
+		 * complete
+		 */
+		if (!capable(CAP_SYS_ADMIN))
+			return -EPERM;
+	}
+
 	/*
 	 * We hope to recycle these flags after 2.6.26
 	 */
@@ -1581,8 +1595,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	err = -EINVAL;
 	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
-				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWUSER|
-				CLONE_NEWNET))
+				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWNET))
 		goto bad_unshare_out;
 
 	/*

commit 65afa5e603d507014580ead016ec887b49e1afa6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 23 18:43:39 2008 +0100

    tracing/function-return-tracer: free the return stack on free_task()
    
    Impact: avoid losing some traces when a task is freed
    
    do_exit() is not the last function called when a task finishes.
    There are still some functions which are to be called such as
    ree_task().  So we delay the freeing of the return stack to the
    last moment.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index fbf4a4c0a628..d6e1a3205f62 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -140,6 +140,7 @@ void free_task(struct task_struct *tsk)
 	prop_local_destroy_single(&tsk->dirties);
 	free_thread_info(tsk->stack);
 	rt_mutex_debug_task_free(tsk);
+	ftrace_retfunc_exit_task(tsk);
 	free_task_struct(tsk);
 }
 EXPORT_SYMBOL(free_task);

commit 82f60f0bc854aada696f27d863c03bef91f1509d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Nov 23 09:18:56 2008 +0100

    tracing/function-return-tracer: clean up task start/exit callbacks
    
    Impact: cleanup
    
    Eliminate #ifdefs in core code by using empty inline functions.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index d1eb30e69ccc..fbf4a4c0a628 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1270,9 +1270,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	total_forks++;
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
-#ifdef CONFIG_FUNCTION_RET_TRACER
 	ftrace_retfunc_init_task(p);
-#endif
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
 	return p;

commit f201ae2356c74bcae130b2177b3dca903ea98071
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 23 06:22:56 2008 +0100

    tracing/function-return-tracer: store return stack into task_struct and allocate it dynamically
    
    Impact: use deeper function tracing depth safely
    
    Some tests showed that function return tracing needed a more deeper depth
    of function calls. But it could be unsafe to store these return addresses
    to the stack.
    
    So these arrays will now be allocated dynamically into task_struct of current
    only when the tracer is activated.
    
    Typical scheme when tracer is activated:
    - allocate a return stack for each task in global list.
    - fork: allocate the return stack for the newly created task
    - exit: free return stack of current
    - idle init: same as fork
    
    I chose a default depth of 50. I don't have overruns anymore.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index ac62f43ee430..d1eb30e69ccc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -47,6 +47,7 @@
 #include <linux/mount.h>
 #include <linux/audit.h>
 #include <linux/memcontrol.h>
+#include <linux/ftrace.h>
 #include <linux/profile.h>
 #include <linux/rmap.h>
 #include <linux/acct.h>
@@ -1269,6 +1270,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	total_forks++;
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
+#ifdef CONFIG_FUNCTION_RET_TRACER
+	ftrace_retfunc_init_task(p);
+#endif
 	proc_fork_connector(p);
 	cgroup_post_fork(p);
 	return p;

commit 9676e73a9e0cbdc521e1ebf4e13e6e5aada34247
Merge: 5a209c2d58e7 86fa2f606745 6d5b43a67acc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Nov 19 10:04:25 2008 +0100

    Merge branches 'tracing/ftrace' and 'tracing/urgent' into tracing/core
    
    Conflicts:
            kernel/trace/ftrace.c
    
    [ We conflicted here because we backported a few fixes to
      tracing/urgent - which has different internal APIs. ]

commit f3a5c547012a09f38f7c27b17a8e3150b69cd259
Merge: e50a906e0200 4e14e833ac3b
Author: James Morris <jmorris@namei.org>
Date:   Tue Nov 18 18:52:37 2008 +1100

    Merge branch 'master' into next
    
    Conflicts:
            fs/cifs/misc.c
    
    Merge to resolve above, per the patch below.
    
    Signed-off-by: James Morris <jmorris@namei.org>
    
    diff --cc fs/cifs/misc.c
    index ec36410,addd1dc..0000000
    --- a/fs/cifs/misc.c
    +++ b/fs/cifs/misc.c
    @@@ -347,13 -338,13 +338,13 @@@ header_assemble(struct smb_hdr *buffer
                    /*  BB Add support for establishing new tCon and SMB Session  */
                    /*      with userid/password pairs found on the smb session   */
                    /*      for other target tcp/ip addresses               BB    */
     -                              if (current->fsuid != treeCon->ses->linux_uid) {
     +                              if (current_fsuid() != treeCon->ses->linux_uid) {
                                            cFYI(1, ("Multiuser mode and UID "
                                                     "did not match tcon uid"));
    -                                       read_lock(&GlobalSMBSeslock);
    -                                       list_for_each(temp_item, &GlobalSMBSessionList) {
    -                                               ses = list_entry(temp_item, struct cifsSesInfo, cifsSessionList);
    +                                       read_lock(&cifs_tcp_ses_lock);
    +                                       list_for_each(temp_item, &treeCon->ses->server->smb_ses_list) {
    +                                               ses = list_entry(temp_item, struct cifsSesInfo, smb_ses_list);
     -                                              if (ses->linux_uid == current->fsuid) {
     +                                              if (ses->linux_uid == current_fsuid()) {
                                                            if (ses->server == treeCon->ses->server) {
                                                                    cFYI(1, ("found matching uid substitute right smb_uid"));
                                                                    buffer->Uid = ses->Suid;

commit 7e066fb870fcd1025ec3ba7bbde5d541094f4ce1
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Fri Nov 14 17:47:47 2008 -0500

    tracepoints: add DECLARE_TRACE() and DEFINE_TRACE()
    
    Impact: API *CHANGE*. Must update all tracepoint users.
    
    Add DEFINE_TRACE() to tracepoints to let them declare the tracepoint
    structure in a single spot for all the kernel. It helps reducing memory
    consumption, especially when declaring a lot of tracepoints, e.g. for
    kmalloc tracing.
    
    *API CHANGE WARNING*: now, DECLARE_TRACE() must be used in headers for
    tracepoint declarations rather than DEFINE_TRACE(). This is the sane way
    to do it. The name previously used was misleading.
    
    Updates scheduler instrumentation to follow this API change.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index f6083561dfe0..0837d0deee5f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -79,6 +79,8 @@ DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 
 __cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
 
+DEFINE_TRACE(sched_process_fork);
+
 int nr_processes(void)
 {
 	int cpu;

commit 8141c7f3e7aee618312fa1c15109e1219de784a7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 15 10:20:36 2008 -0800

    Move "exit_robust_list" into mm_release()
    
    We don't want to get rid of the futexes just at exit() time, we want to
    drop them when doing an execve() too, since that gets rid of the
    previous VM image too.
    
    Doing it at mm_release() time means that we automatically always do it
    when we disassociate a VM map from the task.
    
    Reported-by: pageexec@freemail.hu
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Alex Efros <powerman@powerman.name>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f6083561dfe0..2a372a0e206f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -40,6 +40,7 @@
 #include <linux/jiffies.h>
 #include <linux/tracehook.h>
 #include <linux/futex.h>
+#include <linux/compat.h>
 #include <linux/task_io_accounting_ops.h>
 #include <linux/rcupdate.h>
 #include <linux/ptrace.h>
@@ -519,6 +520,16 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 {
 	struct completion *vfork_done = tsk->vfork_done;
 
+	/* Get rid of any futexes when releasing the mm */
+#ifdef CONFIG_FUTEX
+	if (unlikely(tsk->robust_list))
+		exit_robust_list(tsk);
+#ifdef CONFIG_COMPAT
+	if (unlikely(tsk->compat_robust_list))
+		compat_exit_robust_list(tsk);
+#endif
+#endif
+
 	/* Get rid of any cached register state */
 	deactivate_mm(tsk, mm);
 

commit 3b11a1decef07c19443d24ae926982bc8ec9f4c0
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:26 2008 +1100

    CRED: Differentiate objective and effective subjective credentials on a task
    
    Differentiate the objective and real subjective credentials from the effective
    subjective credentials on a task by introducing a second credentials pointer
    into the task_struct.
    
    task_struct::real_cred then refers to the objective and apparent real
    subjective credentials of a task, as perceived by the other tasks in the
    system.
    
    task_struct::cred then refers to the effective subjective credentials of a
    task, as used by that task when it's actually running.  These are not visible
    to the other tasks in the system.
    
    __task_cred(task) then refers to the objective/real credentials of the task in
    question.
    
    current_cred() refers to the effective subjective credentials of the current
    task.
    
    prepare_creds() uses the objective creds as a base and commit_creds() changes
    both pointers in the task_struct (indeed commit_creds() requires them to be the
    same).
    
    override_creds() and revert_creds() change the subjective creds pointer only,
    and the former returns the old subjective creds.  These are used by NFSD,
    faccessat() and do_coredump(), and will by used by CacheFiles.
    
    In SELinux, current_has_perm() is provided as an alternative to
    task_has_perm().  This uses the effective subjective context of current,
    whereas task_has_perm() uses the objective/real context of the subject.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 82a7948a664e..af0d0f04585c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -146,6 +146,7 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
+	put_cred(tsk->real_cred);
 	put_cred(tsk->cred);
 	delayacct_tsk_free(tsk);
 
@@ -961,10 +962,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
 #endif
 	retval = -EAGAIN;
-	if (atomic_read(&p->cred->user->processes) >=
+	if (atomic_read(&p->real_cred->user->processes) >=
 			p->signal->rlim[RLIMIT_NPROC].rlim_cur) {
 		if (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&
-		    p->cred->user != current->nsproxy->user_ns->root_user)
+		    p->real_cred->user != current->nsproxy->user_ns->root_user)
 			goto bad_fork_free;
 	}
 
@@ -1278,6 +1279,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	module_put(task_thread_info(p)->exec_domain->module);
 bad_fork_cleanup_count:
 	atomic_dec(&p->cred->user->processes);
+	put_cred(p->real_cred);
 	put_cred(p->cred);
 bad_fork_free:
 	free_task(p);

commit d84f4f992cbd76e8f39c488cf0c5d123843923b1
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:23 2008 +1100

    CRED: Inaugurate COW credentials
    
    Inaugurate copy-on-write credentials management.  This uses RCU to manage the
    credentials pointer in the task_struct with respect to accesses by other tasks.
    A process may only modify its own credentials, and so does not need locking to
    access or modify its own credentials.
    
    A mutex (cred_replace_mutex) is added to the task_struct to control the effect
    of PTRACE_ATTACHED on credential calculations, particularly with respect to
    execve().
    
    With this patch, the contents of an active credentials struct may not be
    changed directly; rather a new set of credentials must be prepared, modified
    and committed using something like the following sequence of events:
    
            struct cred *new = prepare_creds();
            int ret = blah(new);
            if (ret < 0) {
                    abort_creds(new);
                    return ret;
            }
            return commit_creds(new);
    
    There are some exceptions to this rule: the keyrings pointed to by the active
    credentials may be instantiated - keyrings violate the COW rule as managing
    COW keyrings is tricky, given that it is possible for a task to directly alter
    the keys in a keyring in use by another task.
    
    To help enforce this, various pointers to sets of credentials, such as those in
    the task_struct, are declared const.  The purpose of this is compile-time
    discouragement of altering credentials through those pointers.  Once a set of
    credentials has been made public through one of these pointers, it may not be
    modified, except under special circumstances:
    
      (1) Its reference count may incremented and decremented.
    
      (2) The keyrings to which it points may be modified, but not replaced.
    
    The only safe way to modify anything else is to create a replacement and commit
    using the functions described in Documentation/credentials.txt (which will be
    added by a later patch).
    
    This patch and the preceding patches have been tested with the LTP SELinux
    testsuite.
    
    This patch makes several logical sets of alteration:
    
     (1) execve().
    
         This now prepares and commits credentials in various places in the
         security code rather than altering the current creds directly.
    
     (2) Temporary credential overrides.
    
         do_coredump() and sys_faccessat() now prepare their own credentials and
         temporarily override the ones currently on the acting thread, whilst
         preventing interference from other threads by holding cred_replace_mutex
         on the thread being dumped.
    
         This will be replaced in a future patch by something that hands down the
         credentials directly to the functions being called, rather than altering
         the task's objective credentials.
    
     (3) LSM interface.
    
         A number of functions have been changed, added or removed:
    
         (*) security_capset_check(), ->capset_check()
         (*) security_capset_set(), ->capset_set()
    
             Removed in favour of security_capset().
    
         (*) security_capset(), ->capset()
    
             New.  This is passed a pointer to the new creds, a pointer to the old
             creds and the proposed capability sets.  It should fill in the new
             creds or return an error.  All pointers, barring the pointer to the
             new creds, are now const.
    
         (*) security_bprm_apply_creds(), ->bprm_apply_creds()
    
             Changed; now returns a value, which will cause the process to be
             killed if it's an error.
    
         (*) security_task_alloc(), ->task_alloc_security()
    
             Removed in favour of security_prepare_creds().
    
         (*) security_cred_free(), ->cred_free()
    
             New.  Free security data attached to cred->security.
    
         (*) security_prepare_creds(), ->cred_prepare()
    
             New. Duplicate any security data attached to cred->security.
    
         (*) security_commit_creds(), ->cred_commit()
    
             New. Apply any security effects for the upcoming installation of new
             security by commit_creds().
    
         (*) security_task_post_setuid(), ->task_post_setuid()
    
             Removed in favour of security_task_fix_setuid().
    
         (*) security_task_fix_setuid(), ->task_fix_setuid()
    
             Fix up the proposed new credentials for setuid().  This is used by
             cap_set_fix_setuid() to implicitly adjust capabilities in line with
             setuid() changes.  Changes are made to the new credentials, rather
             than the task itself as in security_task_post_setuid().
    
         (*) security_task_reparent_to_init(), ->task_reparent_to_init()
    
             Removed.  Instead the task being reparented to init is referred
             directly to init's credentials.
    
             NOTE!  This results in the loss of some state: SELinux's osid no
             longer records the sid of the thread that forked it.
    
         (*) security_key_alloc(), ->key_alloc()
         (*) security_key_permission(), ->key_permission()
    
             Changed.  These now take cred pointers rather than task pointers to
             refer to the security context.
    
     (4) sys_capset().
    
         This has been simplified and uses less locking.  The LSM functions it
         calls have been merged.
    
     (5) reparent_to_kthreadd().
    
         This gives the current thread the same credentials as init by simply using
         commit_thread() to point that way.
    
     (6) __sigqueue_alloc() and switch_uid()
    
         __sigqueue_alloc() can't stop the target task from changing its creds
         beneath it, so this function gets a reference to the currently applicable
         user_struct which it then passes into the sigqueue struct it returns if
         successful.
    
         switch_uid() is now called from commit_creds(), and possibly should be
         folded into that.  commit_creds() should take care of protecting
         __sigqueue_alloc().
    
     (7) [sg]et[ug]id() and co and [sg]et_current_groups.
    
         The set functions now all use prepare_creds(), commit_creds() and
         abort_creds() to build and check a new set of credentials before applying
         it.
    
         security_task_set[ug]id() is called inside the prepared section.  This
         guarantees that nothing else will affect the creds until we've finished.
    
         The calling of set_dumpable() has been moved into commit_creds().
    
         Much of the functionality of set_user() has been moved into
         commit_creds().
    
         The get functions all simply access the data directly.
    
     (8) security_task_prctl() and cap_task_prctl().
    
         security_task_prctl() has been modified to return -ENOSYS if it doesn't
         want to handle a function, or otherwise return the return value directly
         rather than through an argument.
    
         Additionally, cap_task_prctl() now prepares a new set of credentials, even
         if it doesn't end up using it.
    
     (9) Keyrings.
    
         A number of changes have been made to the keyrings code:
    
         (a) switch_uid_keyring(), copy_keys(), exit_keys() and suid_keys() have
             all been dropped and built in to the credentials functions directly.
             They may want separating out again later.
    
         (b) key_alloc() and search_process_keyrings() now take a cred pointer
             rather than a task pointer to specify the security context.
    
         (c) copy_creds() gives a new thread within the same thread group a new
             thread keyring if its parent had one, otherwise it discards the thread
             keyring.
    
         (d) The authorisation key now points directly to the credentials to extend
             the search into rather pointing to the task that carries them.
    
         (e) Installing thread, process or session keyrings causes a new set of
             credentials to be created, even though it's not strictly necessary for
             process or session keyrings (they're shared).
    
    (10) Usermode helper.
    
         The usermode helper code now carries a cred struct pointer in its
         subprocess_info struct instead of a new session keyring pointer.  This set
         of credentials is derived from init_cred and installed on the new process
         after it has been cloned.
    
         call_usermodehelper_setup() allocates the new credentials and
         call_usermodehelper_freeinfo() discards them if they haven't been used.  A
         special cred function (prepare_usermodeinfo_creds()) is provided
         specifically for call_usermodehelper_setup() to call.
    
         call_usermodehelper_setkeys() adjusts the credentials to sport the
         supplied keyring as the new session keyring.
    
    (11) SELinux.
    
         SELinux has a number of changes, in addition to those to support the LSM
         interface changes mentioned above:
    
         (a) selinux_setprocattr() no longer does its check for whether the
             current ptracer can access processes with the new SID inside the lock
             that covers getting the ptracer's SID.  Whilst this lock ensures that
             the check is done with the ptracer pinned, the result is only valid
             until the lock is released, so there's no point doing it inside the
             lock.
    
    (12) is_single_threaded().
    
         This function has been extracted from selinux_setprocattr() and put into
         a file of its own in the lib/ directory as join_session_keyring() now
         wants to use it too.
    
         The code in SELinux just checked to see whether a task shared mm_structs
         with other tasks (CLONE_VM), but that isn't good enough.  We really want
         to know if they're part of the same thread group (CLONE_THREAD).
    
    (13) nfsd.
    
         The NFS server daemon now has to use the COW credentials to set the
         credentials it is going to use.  It really needs to pass the credentials
         down to the functions it calls, but it can't do that until other patches
         in this series have been applied.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ded1972672a3..82a7948a664e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1084,10 +1084,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_sighand;
 	if ((retval = copy_mm(clone_flags, p)))
 		goto bad_fork_cleanup_signal;
-	if ((retval = copy_keys(clone_flags, p)))
-		goto bad_fork_cleanup_mm;
 	if ((retval = copy_namespaces(clone_flags, p)))
-		goto bad_fork_cleanup_keys;
+		goto bad_fork_cleanup_mm;
 	if ((retval = copy_io(clone_flags, p)))
 		goto bad_fork_cleanup_namespaces;
 	retval = copy_thread(0, clone_flags, stack_start, stack_size, p, regs);
@@ -1252,8 +1250,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	put_io_context(p->io_context);
 bad_fork_cleanup_namespaces:
 	exit_task_namespaces(p);
-bad_fork_cleanup_keys:
-	exit_keys(p);
 bad_fork_cleanup_mm:
 	if (p->mm)
 		mmput(p->mm);
@@ -1281,6 +1277,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_put_domain:
 	module_put(task_thread_info(p)->exec_domain->module);
 bad_fork_cleanup_count:
+	atomic_dec(&p->cred->user->processes);
 	put_cred(p->cred);
 bad_fork_free:
 	free_task(p);

commit bb952bb98a7e479262c7eb25d5592545a3af147d
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:20 2008 +1100

    CRED: Separate per-task-group keyrings from signal_struct
    
    Separate per-task-group keyrings from signal_struct and dangle their anchor
    from the cred struct rather than the signal_struct.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: James Morris <jmorris@namei.org>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c932e283ddfc..ded1972672a3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -802,12 +802,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	if (!sig)
 		return -ENOMEM;
 
-	ret = copy_thread_group_keys(tsk);
-	if (ret < 0) {
-		kmem_cache_free(signal_cachep, sig);
-		return ret;
-	}
-
 	atomic_set(&sig->count, 1);
 	atomic_set(&sig->live, 1);
 	init_waitqueue_head(&sig->wait_chldexit);
@@ -852,7 +846,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 void __cleanup_signal(struct signal_struct *sig)
 {
 	thread_group_cputime_free(sig);
-	exit_thread_group_keys(sig);
 	tty_kref_put(sig->tty);
 	kmem_cache_free(signal_cachep, sig);
 }

commit f1752eec6145c97163dbce62d17cf5d928e28a27
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:17 2008 +1100

    CRED: Detach the credentials from task_struct
    
    Detach the credentials from task_struct, duplicating them in copy_process()
    and releasing them in __put_task_struct().
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 81fdc7733908..c932e283ddfc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -146,9 +146,7 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
-	security_task_free(tsk);
-	free_uid(tsk->__temp_cred.user);
-	put_group_info(tsk->__temp_cred.group_info);
+	put_cred(tsk->cred);
 	delayacct_tsk_free(tsk);
 
 	if (!profile_handoff_task(tsk))
@@ -969,7 +967,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);
 	DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
 #endif
-	p->cred = &p->__temp_cred;
 	retval = -EAGAIN;
 	if (atomic_read(&p->cred->user->processes) >=
 			p->signal->rlim[RLIMIT_NPROC].rlim_cur) {
@@ -978,9 +975,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			goto bad_fork_free;
 	}
 
-	atomic_inc(&p->cred->user->__count);
-	atomic_inc(&p->cred->user->processes);
-	get_group_info(p->cred->group_info);
+	retval = copy_creds(p, clone_flags);
+	if (retval < 0)
+		goto bad_fork_free;
 
 	/*
 	 * If multiple threads are within copy_process(), then this check
@@ -1035,9 +1032,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	do_posix_clock_monotonic_gettime(&p->start_time);
 	p->real_start_time = p->start_time;
 	monotonic_to_bootbased(&p->real_start_time);
-#ifdef CONFIG_SECURITY
-	p->cred->security = NULL;
-#endif
 	p->io_context = NULL;
 	p->audit_context = NULL;
 	cgroup_fork(p);
@@ -1082,10 +1076,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p, clone_flags);
 
-	if ((retval = security_task_alloc(p)))
-		goto bad_fork_cleanup_policy;
 	if ((retval = audit_alloc(p)))
-		goto bad_fork_cleanup_security;
+		goto bad_fork_cleanup_policy;
 	/* copy all the process information */
 	if ((retval = copy_semundo(clone_flags, p)))
 		goto bad_fork_cleanup_audit;
@@ -1284,8 +1276,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	exit_sem(p);
 bad_fork_cleanup_audit:
 	audit_free(p);
-bad_fork_cleanup_security:
-	security_task_free(p);
 bad_fork_cleanup_policy:
 #ifdef CONFIG_NUMA
 	mpol_put(p->mempolicy);
@@ -1298,9 +1288,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_put_domain:
 	module_put(task_thread_info(p)->exec_domain->module);
 bad_fork_cleanup_count:
-	put_group_info(p->cred->group_info);
-	atomic_dec(&p->cred->user->processes);
-	free_uid(p->cred->user);
+	put_cred(p->cred);
 bad_fork_free:
 	free_task(p);
 fork_out:

commit b6dff3ec5e116e3af6f537d4caedcad6b9e5082a
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:16 2008 +1100

    CRED: Separate task security context from task_struct
    
    Separate the task security context from task_struct.  At this point, the
    security data is temporarily embedded in the task_struct with two pointers
    pointing to it.
    
    Note that the Alpha arch is altered as it refers to (E)UID and (E)GID in
    entry.S via asm-offsets.
    
    With comment fixes Signed-off-by: Marc Dionne <marc.c.dionne@gmail.com>
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f6083561dfe0..81fdc7733908 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -147,8 +147,8 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(tsk == current);
 
 	security_task_free(tsk);
-	free_uid(tsk->user);
-	put_group_info(tsk->group_info);
+	free_uid(tsk->__temp_cred.user);
+	put_group_info(tsk->__temp_cred.group_info);
 	delayacct_tsk_free(tsk);
 
 	if (!profile_handoff_task(tsk))
@@ -969,17 +969,18 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);
 	DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
 #endif
+	p->cred = &p->__temp_cred;
 	retval = -EAGAIN;
-	if (atomic_read(&p->user->processes) >=
+	if (atomic_read(&p->cred->user->processes) >=
 			p->signal->rlim[RLIMIT_NPROC].rlim_cur) {
 		if (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&
-		    p->user != current->nsproxy->user_ns->root_user)
+		    p->cred->user != current->nsproxy->user_ns->root_user)
 			goto bad_fork_free;
 	}
 
-	atomic_inc(&p->user->__count);
-	atomic_inc(&p->user->processes);
-	get_group_info(p->group_info);
+	atomic_inc(&p->cred->user->__count);
+	atomic_inc(&p->cred->user->processes);
+	get_group_info(p->cred->group_info);
 
 	/*
 	 * If multiple threads are within copy_process(), then this check
@@ -1035,9 +1036,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->real_start_time = p->start_time;
 	monotonic_to_bootbased(&p->real_start_time);
 #ifdef CONFIG_SECURITY
-	p->security = NULL;
+	p->cred->security = NULL;
 #endif
-	p->cap_bset = current->cap_bset;
 	p->io_context = NULL;
 	p->audit_context = NULL;
 	cgroup_fork(p);
@@ -1298,9 +1298,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_put_domain:
 	module_put(task_thread_info(p)->exec_domain->module);
 bad_fork_cleanup_count:
-	put_group_info(p->group_info);
-	atomic_dec(&p->user->processes);
-	free_uid(p->user);
+	put_group_info(p->cred->group_info);
+	atomic_dec(&p->cred->user->processes);
+	free_uid(p->cred->user);
 bad_fork_free:
 	free_task(p);
 fork_out:

commit 268a3dcfea2077fca60d3715caa5c96f9b5e6ea7
Merge: c4bd822e7b12 592aa999d6a2
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Oct 22 09:48:06 2008 +0200

    Merge branch 'timers/range-hrtimers' into v28-range-hrtimers-for-linus-v2
    
    Conflicts:
    
            kernel/time/tick-sched.c
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 92b29b86fe2e183d44eb467e5e74a5f718ef2e43
Merge: b9d7ccf56be1 98d9c66ab074
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 20 13:35:07 2008 -0700

    Merge branch 'tracing-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (131 commits)
      tracing/fastboot: improve help text
      tracing/stacktrace: improve help text
      tracing/fastboot: fix initcalls disposition in bootgraph.pl
      tracing/fastboot: fix bootgraph.pl initcall name regexp
      tracing/fastboot: fix issues and improve output of bootgraph.pl
      tracepoints: synchronize unregister static inline
      tracepoints: tracepoint_synchronize_unregister()
      ftrace: make ftrace_test_p6nop disassembler-friendly
      markers: fix synchronize marker unregister static inline
      tracing/fastboot: add better resolution to initcall debug/tracing
      trace: add build-time check to avoid overrunning hex buffer
      ftrace: fix hex output mode of ftrace
      tracing/fastboot: fix initcalls disposition in bootgraph.pl
      tracing/fastboot: fix printk format typo in boot tracer
      ftrace: return an error when setting a nonexistent tracer
      ftrace: make some tracers reentrant
      ring-buffer: make reentrant
      ring-buffer: move page indexes into page headers
      tracing/fastboot: only trace non-module initcalls
      ftrace: move pc counter in irqtrace
      ...
    
    Manually fix conflicts:
     - init/main.c: initcall tracing
     - kernel/module.c: verbose level vs tracepoints
     - scripts/bootgraph.pl: fallout from cherry-picking commits.

commit c465a76af658b443075d6efee1c3131257643020
Merge: 2d42244ae71d 1b02469088ac fb02fbc14d17 d40e944c25fb 1508487e7f16 322acf6585f3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Oct 20 13:14:06 2008 +0200

    Merge branches 'timers/clocksource', 'timers/hrtimers', 'timers/nohz', 'timers/ntp', 'timers/posixtimers' and 'timers/debug' into v28-timers-for-linus

commit 651dab4264e4ba0e563f5ff56f748127246e9065
Merge: 40b860625355 2e532d68a2b3
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Fri Oct 17 09:20:26 2008 -0700

    Merge commit 'linus/master' into merge-linus
    
    Conflicts:
    
            arch/x86/kvm/i8254.c

commit b2aaf8f74cdc84a9182f6cabf198b7763bcb9d40
Merge: 4f962d4d6592 278429cff880
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Oct 15 13:46:29 2008 +0200

    Merge branch 'linus' into stackprotector
    
    Conflicts:
            arch/x86/kernel/Makefile
            include/asm-x86/pda.h

commit 0a16b6075843325dc402edf80c1662838b929aff
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Fri Jul 18 12:16:17 2008 -0400

    tracing, sched: LTTng instrumentation - scheduler
    
    Instrument the scheduler activity (sched_switch, migration, wakeups,
    wait for a task, signal delivery) and process/thread
    creation/destruction (fork, exit, kthread stop). Actually, kthread
    creation is not instrumented in this patch because it is architecture
    dependent. It allows to connect tracers such as ftrace which detects
    scheduling latencies, good/bad scheduler decisions. Tools like LTTng can
    export this scheduler information along with instrumentation of the rest
    of the kernel activity to perform post-mortem analysis on the scheduler
    activity.
    
    About the performance impact of tracepoints (which is comparable to
    markers), even without immediate values optimizations, tests done by
    Hideo Aoki on ia64 show no regression. His test case was using hackbench
    on a kernel where scheduler instrumentation (about 5 events in code
    scheduler code) was added. See the "Tracepoints" patch header for
    performance result detail.
    
    Changelog :
    
    - Change instrumentation location and parameter to match ftrace
      instrumentation, previously done with kernel markers.
    
    [ mingo@elte.hu: conflict resolutions ]
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Acked-by: 'Peter Zijlstra' <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 30de644a40c4..cfaff92f61ff 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -58,6 +58,7 @@
 #include <linux/tty.h>
 #include <linux/proc_fs.h>
 #include <linux/blkdev.h>
+#include <trace/sched.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -1364,6 +1365,8 @@ long do_fork(unsigned long clone_flags,
 	if (!IS_ERR(p)) {
 		struct completion vfork;
 
+		trace_sched_process_fork(current, p);
+
 		nr = task_pid_vnr(p);
 
 		if (clone_flags & CLONE_PARENT_SETTID)

commit 9c9f4ded90a59eee84e15f5fd38c03d60184e112
Author: Alan Cox <alan@redhat.com>
Date:   Mon Oct 13 10:37:26 2008 +0100

    tty: Add a kref count
    
    Introduce a kref to the tty structure and use it to protect the tty->signal
    tty references. For now we don't introduce it for anything else.
    
    Signed-off-by: Alan Cox <alan@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7ce2ebe84796..30de644a40c4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -802,6 +802,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 
 	sig->leader = 0;	/* session leadership doesn't inherit */
 	sig->tty_old_pgrp = NULL;
+	sig->tty = NULL;
 
 	sig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;
 	sig->gtime = cputime_zero;
@@ -838,6 +839,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 void __cleanup_signal(struct signal_struct *sig)
 {
 	exit_thread_group_keys(sig);
+	tty_kref_put(sig->tty);
 	kmem_cache_free(signal_cachep, sig);
 }
 
@@ -1227,7 +1229,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 				p->nsproxy->pid_ns->child_reaper = p;
 
 			p->signal->leader_pid = pid;
-			p->signal->tty = current->signal->tty;
+			tty_kref_put(p->signal->tty);
+			p->signal->tty = tty_kref_get(current->signal->tty);
 			set_task_pgrp(p, task_pgrp_nr(current));
 			set_task_session(p, task_session_nr(current));
 			attach_pid(p, PIDTYPE_PGID, task_pgrp(current));

commit bb34d92f643086d546b49cef680f6f305ed84414
Author: Frank Mayhar <fmayhar@google.com>
Date:   Fri Sep 12 09:54:39 2008 -0700

    timers: fix itimer/many thread hang, v2
    
    This is the second resubmission of the posix timer rework patch, posted
    a few days ago.
    
    This includes the changes from the previous resubmittion, which addressed
    Oleg Nesterov's comments, removing the RCU stuff from the patch and
    un-inlining the thread_group_cputime() function for SMP.
    
    In addition, per Ingo Molnar it simplifies the UP code, consolidating much
    of it with the SMP version and depending on lower-level SMP/UP handling to
    take care of the differences.
    
    It also cleans up some UP compile errors, moves the scheduler stats-related
    macros into kernel/sched_stats.h, cleans up a merge error in
    kernel/fork.c and has a few other minor fixes and cleanups as suggested
    by Oleg and Ingo. Thanks for the review, guys.
    
    Signed-off-by: Frank Mayhar <fmayhar@google.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1181b9aac48e..021ae012cc75 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -791,7 +791,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	int ret;
 
 	if (clone_flags & CLONE_THREAD) {
-		ret = thread_group_cputime_clone_thread(current, tsk);
+		ret = thread_group_cputime_clone_thread(current);
 		if (likely(!ret)) {
 			atomic_inc(&current->signal->count);
 			atomic_inc(&current->signal->live);
@@ -834,9 +834,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
 	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;
 	task_io_accounting_init(&sig->ioac);
-	INIT_LIST_HEAD(&sig->cpu_timers[0]);
-	INIT_LIST_HEAD(&sig->cpu_timers[1]);
-	INIT_LIST_HEAD(&sig->cpu_timers[2]);
 	taskstats_tgid_init(sig);
 
 	task_lock(current->group_leader);

commit 430b5294bd72c085c730e1e4b86580f164d976bf
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Sep 14 16:33:01 2008 +0200

    timers: fix itimer/many thread hang, fix
    
    fix:
    
     kernel/fork.c:843: error: ‘struct signal_struct’ has no member named ‘sum_sched_runtime’
     kernel/irq/handle.c:117: warning: ‘sparse_irq_lock’ defined but not used
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index a8ac2efb8e30..1181b9aac48e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -834,7 +834,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
 	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;
 	task_io_accounting_init(&sig->ioac);
-	sig->sum_sched_runtime = 0;
 	INIT_LIST_HEAD(&sig->cpu_timers[0]);
 	INIT_LIST_HEAD(&sig->cpu_timers[1]);
 	INIT_LIST_HEAD(&sig->cpu_timers[2]);

commit f06febc96ba8e0af80bcc3eaec0a109e88275fac
Author: Frank Mayhar <fmayhar@google.com>
Date:   Fri Sep 12 09:54:39 2008 -0700

    timers: fix itimer/many thread hang
    
    Overview
    
    This patch reworks the handling of POSIX CPU timers, including the
    ITIMER_PROF, ITIMER_VIRT timers and rlimit handling.  It was put together
    with the help of Roland McGrath, the owner and original writer of this code.
    
    The problem we ran into, and the reason for this rework, has to do with using
    a profiling timer in a process with a large number of threads.  It appears
    that the performance of the old implementation of run_posix_cpu_timers() was
    at least O(n*3) (where "n" is the number of threads in a process) or worse.
    Everything is fine with an increasing number of threads until the time taken
    for that routine to run becomes the same as or greater than the tick time, at
    which point things degrade rather quickly.
    
    This patch fixes bug 9906, "Weird hang with NPTL and SIGPROF."
    
    Code Changes
    
    This rework corrects the implementation of run_posix_cpu_timers() to make it
    run in constant time for a particular machine.  (Performance may vary between
    one machine and another depending upon whether the kernel is built as single-
    or multiprocessor and, in the latter case, depending upon the number of
    running processors.)  To do this, at each tick we now update fields in
    signal_struct as well as task_struct.  The run_posix_cpu_timers() function
    uses those fields to make its decisions.
    
    We define a new structure, "task_cputime," to contain user, system and
    scheduler times and use these in appropriate places:
    
    struct task_cputime {
            cputime_t utime;
            cputime_t stime;
            unsigned long long sum_exec_runtime;
    };
    
    This is included in the structure "thread_group_cputime," which is a new
    substructure of signal_struct and which varies for uniprocessor versus
    multiprocessor kernels.  For uniprocessor kernels, it uses "task_cputime" as
    a simple substructure, while for multiprocessor kernels it is a pointer:
    
    struct thread_group_cputime {
            struct task_cputime totals;
    };
    
    struct thread_group_cputime {
            struct task_cputime *totals;
    };
    
    We also add a new task_cputime substructure directly to signal_struct, to
    cache the earliest expiration of process-wide timers, and task_cputime also
    replaces the it_*_expires fields of task_struct (used for earliest expiration
    of thread timers).  The "thread_group_cputime" structure contains process-wide
    timers that are updated via account_user_time() and friends.  In the non-SMP
    case the structure is a simple aggregator; unfortunately in the SMP case that
    simplicity was not achievable due to cache-line contention between CPUs (in
    one measured case performance was actually _worse_ on a 16-cpu system than
    the same test on a 4-cpu system, due to this contention).  For SMP, the
    thread_group_cputime counters are maintained as a per-cpu structure allocated
    using alloc_percpu().  The timer functions update only the timer field in
    the structure corresponding to the running CPU, obtained using per_cpu_ptr().
    
    We define a set of inline functions in sched.h that we use to maintain the
    thread_group_cputime structure and hide the differences between UP and SMP
    implementations from the rest of the kernel.  The thread_group_cputime_init()
    function initializes the thread_group_cputime structure for the given task.
    The thread_group_cputime_alloc() is a no-op for UP; for SMP it calls the
    out-of-line function thread_group_cputime_alloc_smp() to allocate and fill
    in the per-cpu structures and fields.  The thread_group_cputime_free()
    function, also a no-op for UP, in SMP frees the per-cpu structures.  The
    thread_group_cputime_clone_thread() function (also a UP no-op) for SMP calls
    thread_group_cputime_alloc() if the per-cpu structures haven't yet been
    allocated.  The thread_group_cputime() function fills the task_cputime
    structure it is passed with the contents of the thread_group_cputime fields;
    in UP it's that simple but in SMP it must also safely check that tsk->signal
    is non-NULL (if it is it just uses the appropriate fields of task_struct) and,
    if so, sums the per-cpu values for each online CPU.  Finally, the three
    functions account_group_user_time(), account_group_system_time() and
    account_group_exec_runtime() are used by timer functions to update the
    respective fields of the thread_group_cputime structure.
    
    Non-SMP operation is trivial and will not be mentioned further.
    
    The per-cpu structure is always allocated when a task creates its first new
    thread, via a call to thread_group_cputime_clone_thread() from copy_signal().
    It is freed at process exit via a call to thread_group_cputime_free() from
    cleanup_signal().
    
    All functions that formerly summed utime/stime/sum_sched_runtime values from
    from all threads in the thread group now use thread_group_cputime() to
    snapshot the values in the thread_group_cputime structure or the values in
    the task structure itself if the per-cpu structure hasn't been allocated.
    
    Finally, the code in kernel/posix-cpu-timers.c has changed quite a bit.
    The run_posix_cpu_timers() function has been split into a fast path and a
    slow path; the former safely checks whether there are any expired thread
    timers and, if not, just returns, while the slow path does the heavy lifting.
    With the dedicated thread group fields, timers are no longer "rebalanced" and
    the process_timer_rebalance() function and related code has gone away.  All
    summing loops are gone and all code that used them now uses the
    thread_group_cputime() inline.  When process-wide timers are set, the new
    task_cputime structure in signal_struct is used to cache the earliest
    expiration; this is checked in the fast path.
    
    Performance
    
    The fix appears not to add significant overhead to existing operations.  It
    generally performs the same as the current code except in two cases, one in
    which it performs slightly worse (Case 5 below) and one in which it performs
    very significantly better (Case 2 below).  Overall it's a wash except in those
    two cases.
    
    I've since done somewhat more involved testing on a dual-core Opteron system.
    
    Case 1: With no itimer running, for a test with 100,000 threads, the fixed
            kernel took 1428.5 seconds, 513 seconds more than the unfixed system,
            all of which was spent in the system.  There were twice as many
            voluntary context switches with the fix as without it.
    
    Case 2: With an itimer running at .01 second ticks and 4000 threads (the most
            an unmodified kernel can handle), the fixed kernel ran the test in
            eight percent of the time (5.8 seconds as opposed to 70 seconds) and
            had better tick accuracy (.012 seconds per tick as opposed to .023
            seconds per tick).
    
    Case 3: A 4000-thread test with an initial timer tick of .01 second and an
            interval of 10,000 seconds (i.e. a timer that ticks only once) had
            very nearly the same performance in both cases:  6.3 seconds elapsed
            for the fixed kernel versus 5.5 seconds for the unfixed kernel.
    
    With fewer threads (eight in these tests), the Case 1 test ran in essentially
    the same time on both the modified and unmodified kernels (5.2 seconds versus
    5.8 seconds).  The Case 2 test ran in about the same time as well, 5.9 seconds
    versus 5.4 seconds but again with much better tick accuracy, .013 seconds per
    tick versus .025 seconds per tick for the unmodified kernel.
    
    Since the fix affected the rlimit code, I also tested soft and hard CPU limits.
    
    Case 4: With a hard CPU limit of 20 seconds and eight threads (and an itimer
            running), the modified kernel was very slightly favored in that while
            it killed the process in 19.997 seconds of CPU time (5.002 seconds of
            wall time), only .003 seconds of that was system time, the rest was
            user time.  The unmodified kernel killed the process in 20.001 seconds
            of CPU (5.014 seconds of wall time) of which .016 seconds was system
            time.  Really, though, the results were too close to call.  The results
            were essentially the same with no itimer running.
    
    Case 5: With a soft limit of 20 seconds and a hard limit of 2000 seconds
            (where the hard limit would never be reached) and an itimer running,
            the modified kernel exhibited worse tick accuracy than the unmodified
            kernel: .050 seconds/tick versus .028 seconds/tick.  Otherwise,
            performance was almost indistinguishable.  With no itimer running this
            test exhibited virtually identical behavior and times in both cases.
    
    In times past I did some limited performance testing.  those results are below.
    
    On a four-cpu Opteron system without this fix, a sixteen-thread test executed
    in 3569.991 seconds, of which user was 3568.435s and system was 1.556s.  On
    the same system with the fix, user and elapsed time were about the same, but
    system time dropped to 0.007 seconds.  Performance with eight, four and one
    thread were comparable.  Interestingly, the timer ticks with the fix seemed
    more accurate:  The sixteen-thread test with the fix received 149543 ticks
    for 0.024 seconds per tick, while the same test without the fix received 58720
    for 0.061 seconds per tick.  Both cases were configured for an interval of
    0.01 seconds.  Again, the other tests were comparable.  Each thread in this
    test computed the primes up to 25,000,000.
    
    I also did a test with a large number of threads, 100,000 threads, which is
    impossible without the fix.  In this case each thread computed the primes only
    up to 10,000 (to make the runtime manageable).  System time dominated, at
    1546.968 seconds out of a total 2176.906 seconds (giving a user time of
    629.938s).  It received 147651 ticks for 0.015 seconds per tick, still quite
    accurate.  There is obviously no comparable test without the fix.
    
    Signed-off-by: Frank Mayhar <fmayhar@google.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7ce2ebe84796..a8ac2efb8e30 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -759,15 +759,44 @@ void __cleanup_sighand(struct sighand_struct *sighand)
 		kmem_cache_free(sighand_cachep, sighand);
 }
 
+
+/*
+ * Initialize POSIX timer handling for a thread group.
+ */
+static void posix_cpu_timers_init_group(struct signal_struct *sig)
+{
+	/* Thread group counters. */
+	thread_group_cputime_init(sig);
+
+	/* Expiration times and increments. */
+	sig->it_virt_expires = cputime_zero;
+	sig->it_virt_incr = cputime_zero;
+	sig->it_prof_expires = cputime_zero;
+	sig->it_prof_incr = cputime_zero;
+
+	/* Cached expiration times. */
+	sig->cputime_expires.prof_exp = cputime_zero;
+	sig->cputime_expires.virt_exp = cputime_zero;
+	sig->cputime_expires.sched_exp = 0;
+
+	/* The timer lists. */
+	INIT_LIST_HEAD(&sig->cpu_timers[0]);
+	INIT_LIST_HEAD(&sig->cpu_timers[1]);
+	INIT_LIST_HEAD(&sig->cpu_timers[2]);
+}
+
 static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct signal_struct *sig;
 	int ret;
 
 	if (clone_flags & CLONE_THREAD) {
-		atomic_inc(&current->signal->count);
-		atomic_inc(&current->signal->live);
-		return 0;
+		ret = thread_group_cputime_clone_thread(current, tsk);
+		if (likely(!ret)) {
+			atomic_inc(&current->signal->count);
+			atomic_inc(&current->signal->live);
+		}
+		return ret;
 	}
 	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);
 	tsk->signal = sig;
@@ -795,15 +824,10 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->it_real_incr.tv64 = 0;
 	sig->real_timer.function = it_real_fn;
 
-	sig->it_virt_expires = cputime_zero;
-	sig->it_virt_incr = cputime_zero;
-	sig->it_prof_expires = cputime_zero;
-	sig->it_prof_incr = cputime_zero;
-
 	sig->leader = 0;	/* session leadership doesn't inherit */
 	sig->tty_old_pgrp = NULL;
 
-	sig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;
+	sig->cutime = sig->cstime = cputime_zero;
 	sig->gtime = cputime_zero;
 	sig->cgtime = cputime_zero;
 	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
@@ -820,14 +844,8 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	memcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);
 	task_unlock(current->group_leader);
 
-	if (sig->rlim[RLIMIT_CPU].rlim_cur != RLIM_INFINITY) {
-		/*
-		 * New sole thread in the process gets an expiry time
-		 * of the whole CPU time limit.
-		 */
-		tsk->it_prof_expires =
-			secs_to_cputime(sig->rlim[RLIMIT_CPU].rlim_cur);
-	}
+	posix_cpu_timers_init_group(sig);
+
 	acct_init_pacct(&sig->pacct);
 
 	tty_audit_fork(sig);
@@ -837,6 +855,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 
 void __cleanup_signal(struct signal_struct *sig)
 {
+	thread_group_cputime_free(sig);
 	exit_thread_group_keys(sig);
 	kmem_cache_free(signal_cachep, sig);
 }
@@ -885,6 +904,19 @@ void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
 }
 #endif /* CONFIG_MM_OWNER */
 
+/*
+ * Initialize POSIX timer handling for a single task.
+ */
+static void posix_cpu_timers_init(struct task_struct *tsk)
+{
+	tsk->cputime_expires.prof_exp = cputime_zero;
+	tsk->cputime_expires.virt_exp = cputime_zero;
+	tsk->cputime_expires.sched_exp = 0;
+	INIT_LIST_HEAD(&tsk->cpu_timers[0]);
+	INIT_LIST_HEAD(&tsk->cpu_timers[1]);
+	INIT_LIST_HEAD(&tsk->cpu_timers[2]);
+}
+
 /*
  * This creates a new process as a copy of the old one,
  * but does not actually start it yet.
@@ -995,12 +1027,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	task_io_accounting_init(&p->ioac);
 	acct_clear_integrals(p);
 
-	p->it_virt_expires = cputime_zero;
-	p->it_prof_expires = cputime_zero;
-	p->it_sched_expires = 0;
-	INIT_LIST_HEAD(&p->cpu_timers[0]);
-	INIT_LIST_HEAD(&p->cpu_timers[1]);
-	INIT_LIST_HEAD(&p->cpu_timers[2]);
+	posix_cpu_timers_init(p);
 
 	p->lock_depth = -1;		/* -1 = no lock */
 	do_posix_clock_monotonic_gettime(&p->start_time);
@@ -1201,21 +1228,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (clone_flags & CLONE_THREAD) {
 		p->group_leader = current->group_leader;
 		list_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);
-
-		if (!cputime_eq(current->signal->it_virt_expires,
-				cputime_zero) ||
-		    !cputime_eq(current->signal->it_prof_expires,
-				cputime_zero) ||
-		    current->signal->rlim[RLIMIT_CPU].rlim_cur != RLIM_INFINITY ||
-		    !list_empty(&current->signal->cpu_timers[0]) ||
-		    !list_empty(&current->signal->cpu_timers[1]) ||
-		    !list_empty(&current->signal->cpu_timers[2])) {
-			/*
-			 * Have child wake up on its first tick to check
-			 * for process CPU timers.
-			 */
-			p->it_prof_expires = jiffies_to_cputime(1);
-		}
 	}
 
 	if (likely(p->pid)) {

commit 6976675d94042fbd446231d1bd8b7de71a980ada
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Mon Sep 1 15:52:40 2008 -0700

    hrtimer: create a "timer_slack" field in the task struct
    
    We want to be able to control the default "rounding" that is used by
    select() and poll() and friends. This is a per process property
    (so that we can have a "nice" like program to start certain programs with
    a looser or stricter rounding) that can be set/get via a prctl().
    
    For this purpose, a field called "timer_slack_ns" is added to the task
    struct. In addition, a field called "default_timer_slack"ns" is added
    so that tasks easily can temporarily to a more/less accurate slack and then
    back to the default.
    
    The default value of the slack is set to 50 usec; this is significantly less
    than 2.6.27's average select() and poll() timing error but still allows
    the kernel to group timers somewhat to preserve power behavior. Applications
    and admins can override this via the prctl()
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7ce2ebe84796..4308d75f0fa5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -987,6 +987,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->prev_utime = cputime_zero;
 	p->prev_stime = cputime_zero;
 
+	p->default_timer_slack_ns = current->timer_slack_ns;
+
 #ifdef CONFIG_DETECT_SOFTLOCKUP
 	p->last_switch_count = 0;
 	p->last_switch_timestamp = 0;

commit cddb8a5c14aa89810b40495d94d3d2a0faee6619
Author: Andrea Arcangeli <andrea@qumranet.com>
Date:   Mon Jul 28 15:46:29 2008 -0700

    mmu-notifiers: core
    
    With KVM/GFP/XPMEM there isn't just the primary CPU MMU pointing to pages.
     There are secondary MMUs (with secondary sptes and secondary tlbs) too.
    sptes in the kvm case are shadow pagetables, but when I say spte in
    mmu-notifier context, I mean "secondary pte".  In GRU case there's no
    actual secondary pte and there's only a secondary tlb because the GRU
    secondary MMU has no knowledge about sptes and every secondary tlb miss
    event in the MMU always generates a page fault that has to be resolved by
    the CPU (this is not the case of KVM where the a secondary tlb miss will
    walk sptes in hardware and it will refill the secondary tlb transparently
    to software if the corresponding spte is present).  The same way
    zap_page_range has to invalidate the pte before freeing the page, the spte
    (and secondary tlb) must also be invalidated before any page is freed and
    reused.
    
    Currently we take a page_count pin on every page mapped by sptes, but that
    means the pages can't be swapped whenever they're mapped by any spte
    because they're part of the guest working set.  Furthermore a spte unmap
    event can immediately lead to a page to be freed when the pin is released
    (so requiring the same complex and relatively slow tlb_gather smp safe
    logic we have in zap_page_range and that can be avoided completely if the
    spte unmap event doesn't require an unpin of the page previously mapped in
    the secondary MMU).
    
    The mmu notifiers allow kvm/GRU/XPMEM to attach to the tsk->mm and know
    when the VM is swapping or freeing or doing anything on the primary MMU so
    that the secondary MMU code can drop sptes before the pages are freed,
    avoiding all page pinning and allowing 100% reliable swapping of guest
    physical address space.  Furthermore it avoids the code that teardown the
    mappings of the secondary MMU, to implement a logic like tlb_gather in
    zap_page_range that would require many IPI to flush other cpu tlbs, for
    each fixed number of spte unmapped.
    
    To make an example: if what happens on the primary MMU is a protection
    downgrade (from writeable to wrprotect) the secondary MMU mappings will be
    invalidated, and the next secondary-mmu-page-fault will call
    get_user_pages and trigger a do_wp_page through get_user_pages if it
    called get_user_pages with write=1, and it'll re-establishing an updated
    spte or secondary-tlb-mapping on the copied page.  Or it will setup a
    readonly spte or readonly tlb mapping if it's a guest-read, if it calls
    get_user_pages with write=0.  This is just an example.
    
    This allows to map any page pointed by any pte (and in turn visible in the
    primary CPU MMU), into a secondary MMU (be it a pure tlb like GRU, or an
    full MMU with both sptes and secondary-tlb like the shadow-pagetable layer
    with kvm), or a remote DMA in software like XPMEM (hence needing of
    schedule in XPMEM code to send the invalidate to the remote node, while no
    need to schedule in kvm/gru as it's an immediate event like invalidating
    primary-mmu pte).
    
    At least for KVM without this patch it's impossible to swap guests
    reliably.  And having this feature and removing the page pin allows
    several other optimizations that simplify life considerably.
    
    Dependencies:
    
    1) mm_take_all_locks() to register the mmu notifier when the whole VM
       isn't doing anything with "mm".  This allows mmu notifier users to keep
       track if the VM is in the middle of the invalidate_range_begin/end
       critical section with an atomic counter incraese in range_begin and
       decreased in range_end.  No secondary MMU page fault is allowed to map
       any spte or secondary tlb reference, while the VM is in the middle of
       range_begin/end as any page returned by get_user_pages in that critical
       section could later immediately be freed without any further
       ->invalidate_page notification (invalidate_range_begin/end works on
       ranges and ->invalidate_page isn't called immediately before freeing
       the page).  To stop all page freeing and pagetable overwrites the
       mmap_sem must be taken in write mode and all other anon_vma/i_mmap
       locks must be taken too.
    
    2) It'd be a waste to add branches in the VM if nobody could possibly
       run KVM/GRU/XPMEM on the kernel, so mmu notifiers will only enabled if
       CONFIG_KVM=m/y.  In the current kernel kvm won't yet take advantage of
       mmu notifiers, but this already allows to compile a KVM external module
       against a kernel with mmu notifiers enabled and from the next pull from
       kvm.git we'll start using them.  And GRU/XPMEM will also be able to
       continue the development by enabling KVM=m in their config, until they
       submit all GRU/XPMEM GPLv2 code to the mainline kernel.  Then they can
       also enable MMU_NOTIFIERS in the same way KVM does it (even if KVM=n).
       This guarantees nobody selects MMU_NOTIFIER=y if KVM and GRU and XPMEM
       are all =n.
    
    The mmu_notifier_register call can fail because mm_take_all_locks may be
    interrupted by a signal and return -EINTR.  Because mmu_notifier_reigster
    is used when a driver startup, a failure can be gracefully handled.  Here
    an example of the change applied to kvm to register the mmu notifiers.
    Usually when a driver startups other allocations are required anyway and
    -ENOMEM failure paths exists already.
    
     struct  kvm *kvm_arch_create_vm(void)
     {
            struct kvm *kvm = kzalloc(sizeof(struct kvm), GFP_KERNEL);
    +       int err;
    
            if (!kvm)
                    return ERR_PTR(-ENOMEM);
    
            INIT_LIST_HEAD(&kvm->arch.active_mmu_pages);
    
    +       kvm->arch.mmu_notifier.ops = &kvm_mmu_notifier_ops;
    +       err = mmu_notifier_register(&kvm->arch.mmu_notifier, current->mm);
    +       if (err) {
    +               kfree(kvm);
    +               return ERR_PTR(err);
    +       }
    +
            return kvm;
     }
    
    mmu_notifier_unregister returns void and it's reliable.
    
    The patch also adds a few needed but missing includes that would prevent
    kernel to compile after these changes on non-x86 archs (x86 didn't need
    them by luck).
    
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: fix mm/filemap_xip.c build]
    [akpm@linux-foundation.org: fix mm/mmu_notifier.c build]
    Signed-off-by: Andrea Arcangeli <andrea@qumranet.com>
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Jack Steiner <steiner@sgi.com>
    Cc: Robin Holt <holt@sgi.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Kanoj Sarcar <kanojsarcar@yahoo.com>
    Cc: Roland Dreier <rdreier@cisco.com>
    Cc: Steve Wise <swise@opengridcomputing.com>
    Cc: Avi Kivity <avi@qumranet.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Anthony Liguori <aliguori@us.ibm.com>
    Cc: Chris Wright <chrisw@redhat.com>
    Cc: Marcelo Tosatti <marcelo@kvack.org>
    Cc: Eric Dumazet <dada1@cosmosbay.com>
    Cc: "Paul E. McKenney" <paulmck@us.ibm.com>
    Cc: Izik Eidus <izike@qumranet.com>
    Cc: Anthony Liguori <aliguori@us.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8214ba7c8bb1..7ce2ebe84796 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -27,6 +27,7 @@
 #include <linux/key.h>
 #include <linux/binfmts.h>
 #include <linux/mman.h>
+#include <linux/mmu_notifier.h>
 #include <linux/fs.h>
 #include <linux/nsproxy.h>
 #include <linux/capability.h>
@@ -414,6 +415,7 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 
 	if (likely(!mm_alloc_pgd(mm))) {
 		mm->def_flags = 0;
+		mmu_notifier_mm_init(mm);
 		return mm;
 	}
 
@@ -446,6 +448,7 @@ void __mmdrop(struct mm_struct *mm)
 	BUG_ON(mm == &init_mm);
 	mm_free_pgd(mm);
 	destroy_context(mm);
+	mmu_notifier_mm_destroy(mm);
 	free_mm(mm);
 }
 EXPORT_SYMBOL_GPL(__mmdrop);

commit 5995477ab7f3522c497c9c4a1c55373e9d655574
Author: Andrea Righi <righi.andrea@gmail.com>
Date:   Sun Jul 27 17:29:15 2008 +0200

    task IO accounting: improve code readability
    
    Put all i/o statistics in struct proc_io_accounting and use inline functions to
    initialize and increment statistics, removing a lot of single variable
    assignments.
    
    This also reduces the kernel size as following (with CONFIG_TASK_XACCT=y and
    CONFIG_TASK_IO_ACCOUNTING=y).
    
        text    data     bss     dec     hex filename
       11651       0       0   11651    2d83 kernel/exit.o.before
       11619       0       0   11619    2d63 kernel/exit.o.after
       10886     132     136   11154    2b92 kernel/fork.o.before
       10758     132     136   11026    2b12 kernel/fork.o.after
    
     3082029  807968 4818600 8708597  84e1f5 vmlinux.o.before
     3081869  807968 4818600 8708437  84e155 vmlinux.o.after
    
    Signed-off-by: Andrea Righi <righi.andrea@gmail.com>
    Acked-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5e050c1317c4..8214ba7c8bb1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -806,12 +806,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
 	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
 	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;
-#ifdef CONFIG_TASK_XACCT
-	sig->rchar = sig->wchar = sig->syscr = sig->syscw = 0;
-#endif
-#ifdef CONFIG_TASK_IO_ACCOUNTING
-	memset(&sig->ioac, 0, sizeof(sig->ioac));
-#endif
+	task_io_accounting_init(&sig->ioac);
 	sig->sum_sched_runtime = 0;
 	INIT_LIST_HEAD(&sig->cpu_timers[0]);
 	INIT_LIST_HEAD(&sig->cpu_timers[1]);
@@ -994,13 +989,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->last_switch_timestamp = 0;
 #endif
 
-#ifdef CONFIG_TASK_XACCT
-	p->rchar = 0;		/* I/O counter: bytes read */
-	p->wchar = 0;		/* I/O counter: bytes written */
-	p->syscr = 0;		/* I/O counter: read syscalls */
-	p->syscw = 0;		/* I/O counter: write syscalls */
-#endif
-	task_io_accounting_init(p);
+	task_io_accounting_init(&p->ioac);
 	acct_clear_integrals(p);
 
 	p->it_virt_expires = cputime_zero;

commit 7f2da1e7d0330395e5e9e350b879b98a1ea495df
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat May 10 20:44:54 2008 -0400

    [PATCH] kill altroot
    
    long overdue...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index abb3ed6298f6..5e050c1317c4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -657,13 +657,6 @@ static struct fs_struct *__copy_fs_struct(struct fs_struct *old)
 		path_get(&old->root);
 		fs->pwd = old->pwd;
 		path_get(&old->pwd);
-		if (old->altroot.dentry) {
-			fs->altroot = old->altroot;
-			path_get(&old->altroot);
-		} else {
-			fs->altroot.mnt = NULL;
-			fs->altroot.dentry = NULL;
-		}
 		read_unlock(&old->lock);
 	}
 	return fs;

commit daded34be96b1975ff8539ff62ad8b158ce7d842
Author: Roland McGrath <roland@redhat.com>
Date:   Fri Jul 25 19:45:47 2008 -0700

    tracehook: vfork-done
    
    This moves the PTRACE_EVENT_VFORK_DONE tracing into a tracehook.h inline,
    tracehook_report_vfork_done().  The change has no effect, just clean-up.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b42f8ed23611..abb3ed6298f6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1414,10 +1414,7 @@ long do_fork(unsigned long clone_flags,
 			freezer_do_not_count();
 			wait_for_completion(&vfork);
 			freezer_count();
-			if (unlikely (current->ptrace & PT_TRACE_VFORK_DONE)) {
-				current->ptrace_message = nr;
-				ptrace_notify ((PTRACE_EVENT_VFORK_DONE << 8) | SIGTRAP);
-			}
+			tracehook_report_vfork_done(p, nr);
 		}
 	} else {
 		nr = PTR_ERR(p);

commit 09a05394fe2448a4139b014936330af23fa7ec83
Author: Roland McGrath <roland@redhat.com>
Date:   Fri Jul 25 19:45:47 2008 -0700

    tracehook: clone
    
    This moves all the ptrace initialization and tracing logic for task
    creation into tracehook.h and ptrace.h inlines.  It reorganizes the code
    slightly, but should not change any behavior.
    
    There are four tracehook entry points, at each important stage of task
    creation.  This keeps the interface from the core fork.c code fairly
    clean, while supporting the complex setup required for ptrace or something
    like it.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 80e83e459b17..b42f8ed23611 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -37,6 +37,7 @@
 #include <linux/swap.h>
 #include <linux/syscalls.h>
 #include <linux/jiffies.h>
+#include <linux/tracehook.h>
 #include <linux/futex.h>
 #include <linux/task_io_accounting_ops.h>
 #include <linux/rcupdate.h>
@@ -865,8 +866,7 @@ static void copy_flags(unsigned long clone_flags, struct task_struct *p)
 
 	new_flags &= ~PF_SUPERPRIV;
 	new_flags |= PF_FORKNOEXEC;
-	if (!(clone_flags & CLONE_PTRACE))
-		p->ptrace = 0;
+	new_flags |= PF_STARTING;
 	p->flags = new_flags;
 	clear_freeze_flag(p);
 }
@@ -907,7 +907,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 					struct pt_regs *regs,
 					unsigned long stack_size,
 					int __user *child_tidptr,
-					struct pid *pid)
+					struct pid *pid,
+					int trace)
 {
 	int retval;
 	struct task_struct *p;
@@ -1163,8 +1164,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 */
 	p->group_leader = p;
 	INIT_LIST_HEAD(&p->thread_group);
-	INIT_LIST_HEAD(&p->ptrace_entry);
-	INIT_LIST_HEAD(&p->ptraced);
 
 	/* Now that the task is set up, run cgroup callbacks if
 	 * necessary. We need to run them before the task is visible
@@ -1195,7 +1194,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		p->real_parent = current->real_parent;
 	else
 		p->real_parent = current;
-	p->parent = p->real_parent;
 
 	spin_lock(&current->sighand->siglock);
 
@@ -1237,8 +1235,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	if (likely(p->pid)) {
 		list_add_tail(&p->sibling, &p->real_parent->children);
-		if (unlikely(p->ptrace & PT_PTRACED))
-			__ptrace_link(p, current->parent);
+		tracehook_finish_clone(p, clone_flags, trace);
 
 		if (thread_group_leader(p)) {
 			if (clone_flags & CLONE_NEWPID)
@@ -1323,29 +1320,13 @@ struct task_struct * __cpuinit fork_idle(int cpu)
 	struct pt_regs regs;
 
 	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL,
-				&init_struct_pid);
+			    &init_struct_pid, 0);
 	if (!IS_ERR(task))
 		init_idle(task, cpu);
 
 	return task;
 }
 
-static int fork_traceflag(unsigned clone_flags)
-{
-	if (clone_flags & CLONE_UNTRACED)
-		return 0;
-	else if (clone_flags & CLONE_VFORK) {
-		if (current->ptrace & PT_TRACE_VFORK)
-			return PTRACE_EVENT_VFORK;
-	} else if ((clone_flags & CSIGNAL) != SIGCHLD) {
-		if (current->ptrace & PT_TRACE_CLONE)
-			return PTRACE_EVENT_CLONE;
-	} else if (current->ptrace & PT_TRACE_FORK)
-		return PTRACE_EVENT_FORK;
-
-	return 0;
-}
-
 /*
  *  Ok, this is the main fork-routine.
  *
@@ -1380,14 +1361,14 @@ long do_fork(unsigned long clone_flags,
 		}
 	}
 
-	if (unlikely(current->ptrace)) {
-		trace = fork_traceflag (clone_flags);
-		if (trace)
-			clone_flags |= CLONE_PTRACE;
-	}
+	/*
+	 * When called from kernel_thread, don't do user tracing stuff.
+	 */
+	if (likely(user_mode(regs)))
+		trace = tracehook_prepare_clone(clone_flags);
 
 	p = copy_process(clone_flags, stack_start, regs, stack_size,
-			child_tidptr, NULL);
+			 child_tidptr, NULL, trace);
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.
@@ -1405,24 +1386,30 @@ long do_fork(unsigned long clone_flags,
 			init_completion(&vfork);
 		}
 
-		if ((p->ptrace & PT_PTRACED) || (clone_flags & CLONE_STOPPED)) {
+		tracehook_report_clone(trace, regs, clone_flags, nr, p);
+
+		/*
+		 * We set PF_STARTING at creation in case tracing wants to
+		 * use this to distinguish a fully live task from one that
+		 * hasn't gotten to tracehook_report_clone() yet.  Now we
+		 * clear it and set the child going.
+		 */
+		p->flags &= ~PF_STARTING;
+
+		if (unlikely(clone_flags & CLONE_STOPPED)) {
 			/*
 			 * We'll start up with an immediate SIGSTOP.
 			 */
 			sigaddset(&p->pending.signal, SIGSTOP);
 			set_tsk_thread_flag(p, TIF_SIGPENDING);
-		}
-
-		if (!(clone_flags & CLONE_STOPPED))
-			wake_up_new_task(p, clone_flags);
-		else
 			__set_task_state(p, TASK_STOPPED);
-
-		if (unlikely (trace)) {
-			current->ptrace_message = nr;
-			ptrace_notify ((trace << 8) | SIGTRAP);
+		} else {
+			wake_up_new_task(p, clone_flags);
 		}
 
+		tracehook_report_clone_complete(trace, regs,
+						clone_flags, nr, p);
+
 		if (clone_flags & CLONE_VFORK) {
 			freezer_do_not_count();
 			wait_for_completion(&vfork);

commit 51cc50685a4275c6a02653670af9f108a64e01cf
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Jul 25 19:45:34 2008 -0700

    SL*B: drop kmem cache argument from constructor
    
    Kmem cache passed to constructor is only needed for constructors that are
    themselves multiplexeres.  Nobody uses this "feature", nor does anybody uses
    passed kmem cache in non-trivial way, so pass only pointer to object.
    
    Non-trivial places are:
            arch/powerpc/mm/init_64.c
            arch/powerpc/mm/hugetlbpage.c
    
    This is flag day, yes.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Jon Tollefson <kniht@linux.vnet.ibm.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Matt Mackall <mpm@selenic.com>
    [akpm@linux-foundation.org: fix arch/powerpc/mm/hugetlbpage.c]
    [akpm@linux-foundation.org: fix mm/slab.c]
    [akpm@linux-foundation.org: fix ubifs]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b99d73e971a4..80e83e459b17 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1442,7 +1442,7 @@ long do_fork(unsigned long clone_flags,
 #define ARCH_MIN_MMSTRUCT_ALIGN 0
 #endif
 
-static void sighand_ctor(struct kmem_cache *cachep, void *data)
+static void sighand_ctor(void *data)
 {
 	struct sighand_struct *sighand = data;
 

commit 297c5d92634c809cef23d73e7b2556f2528ff7e2
Author: Andrea Righi <righi.andrea@gmail.com>
Date:   Fri Jul 25 01:48:49 2008 -0700

    task IO accounting: provide distinct tgid/tid I/O statistics
    
    Report per-thread I/O statistics in /proc/pid/task/tid/io and aggregate
    parent I/O statistics in /proc/pid/io.  This approach follows the same
    model used to account per-process and per-thread CPU times.
    
    As a practial application, this allows for example to quickly find the top
    I/O consumer when a process spawns many child threads that perform the
    actual I/O work, because the aggregated I/O statistics can always be found
    in /proc/pid/io.
    
    [ Oleg Nesterov points out that we should check that the task is still
      alive before we iterate over the threads, but also says that we can do
      that fixup on top of this later.  - Linus ]
    
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Andrea Righi <righi.andrea@gmail.com>
    Cc: Matt Heaton <matt@hostmonster.com>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Acked-by-with-comments: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 813d5c89b9d5..b99d73e971a4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -812,6 +812,12 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
 	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
 	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;
+#ifdef CONFIG_TASK_XACCT
+	sig->rchar = sig->wchar = sig->syscr = sig->syscw = 0;
+#endif
+#ifdef CONFIG_TASK_IO_ACCOUNTING
+	memset(&sig->ioac, 0, sizeof(sig->ioac));
+#endif
 	sig->sum_sched_runtime = 0;
 	INIT_LIST_HEAD(&sig->cpu_timers[0]);
 	INIT_LIST_HEAD(&sig->cpu_timers[1]);

commit 999d9fc1670bc082928b93b11d1f2e0e417d973c
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:41 2008 -0700

    coredump: move mm->core_waiters into struct core_state
    
    Move mm->core_waiters into "struct core_state" allocated on stack.  This
    shrinks mm_struct a little bit and allows further changes.
    
    This patch mostly does s/core_waiters/core_state.  The only essential
    change is that coredump_wait() must clear mm->core_state before return.
    
    The coredump_wait()'s path is uglified and .text grows by 30 bytes, this
    is fixed by the next patch.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index eeaec6893b0d..813d5c89b9d5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -400,7 +400,7 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->flags = (current->mm) ? current->mm->flags
 				  : MMF_DUMP_FILTER_DEFAULT;
-	mm->core_waiters = 0;
+	mm->core_state = NULL;
 	mm->nr_ptes = 0;
 	set_mm_counter(mm, file_rss, 0);
 	set_mm_counter(mm, anon_rss, 0);

commit 246bb0b1deb29726990620d8b5e55ca29f331362
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:38 2008 -0700

    kill PF_BORROWED_MM in favour of PF_KTHREAD
    
    Kill PF_BORROWED_MM.  Change use_mm/unuse_mm to not play with ->flags, and
    do s/PF_BORROWED_MM/PF_KTHREAD/ for a couple of other users.
    
    No functional changes yet.  But this allows us to do further
    fixes/cleanups.
    
    oom_kill/ptrace/etc often check "p->mm != NULL" to filter out the
    kthreads, this is wrong because of use_mm().  The problem with
    PF_BORROWED_MM is that we need task_lock() to avoid races.  With this
    patch we can check PF_KTHREAD directly, or use a simple lockless helper:
    
            /* The result must not be dereferenced !!! */
            struct mm_struct *__get_task_mm(struct task_struct *tsk)
            {
                    if (tsk->flags & PF_KTHREAD)
                            return NULL;
                    return tsk->mm;
            }
    
    Note also ecard_task().  It runs with ->mm != NULL, but it's the kernel
    thread without PF_BORROWED_MM.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 228f80c9155a..eeaec6893b0d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -474,7 +474,7 @@ EXPORT_SYMBOL_GPL(mmput);
 /**
  * get_task_mm - acquire a reference to the task's mm
  *
- * Returns %NULL if the task has no mm.  Checks PF_BORROWED_MM (meaning
+ * Returns %NULL if the task has no mm.  Checks PF_KTHREAD (meaning
  * this kernel workthread has transiently adopted a user mm with use_mm,
  * to do its AIO) is not set and if so returns a reference to it, after
  * bumping up the use count.  User must release the mm via mmput()
@@ -487,7 +487,7 @@ struct mm_struct *get_task_mm(struct task_struct *task)
 	task_lock(task);
 	mm = task->mm;
 	if (mm) {
-		if (task->flags & PF_BORROWED_MM)
+		if (task->flags & PF_KTHREAD)
 			mm = NULL;
 		else
 			atomic_inc(&mm->mm_users);

commit e885dcde75685e09f23cffae1f6d5169c105b8a0
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Fri Jul 25 01:47:06 2008 -0700

    cgroup_clone: use pid of newly created task for new cgroup
    
    cgroup_clone creates a new cgroup with the pid of the task.  This works
    correctly for unshare, but for clone cgroup_clone is called from
    copy_namespaces inside copy_process, which happens before the new pid is
    created.  As a result, the new cgroup was created with current's pid.
    This patch:
    
            1. Moves the call inside copy_process to after the new pid
               is created
            2. Passes the struct pid into ns_cgroup_clone (as it is not
               yet attached to the task)
            3. Passes a name from ns_cgroup_clone() into cgroup_clone()
               so as to keep cgroup_clone() itself simpler
            4. Uses pid_vnr() to get the process id value, so that the
               pid used to name the new cgroup is always the pid as it
               would be known to the task which did the cloning or
               unsharing.  I think that is the most intuitive thing to
               do.  This way, task t1 does clone(CLONE_NEWPID) to get
               t2, which does clone(CLONE_NEWPID) to get t3, then the
               cgroup for t3 will be named for the pid by which t2 knows
               t3.
    
    (Thanks to Dan Smith for finding the main bug)
    
    Changelog:
            June 11: Incorporate Paul Menage's feedback:  don't pass
                     NULL to ns_cgroup_clone from unshare, and reduce
                     patch size by using 'nodename' in cgroup_clone.
            June 10: Original version
    
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Serge Hallyn <serge@us.ibm.com>
    Acked-by: Paul Menage <menage@google.com>
    Tested-by: Dan Smith <danms@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5a5d6fef341d..228f80c9155a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1107,6 +1107,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (clone_flags & CLONE_THREAD)
 		p->tgid = current->tgid;
 
+	if (current->nsproxy != p->nsproxy) {
+		retval = ns_cgroup_clone(p, pid);
+		if (retval)
+			goto bad_fork_free_pid;
+	}
+
 	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
 	/*
 	 * Clear TID on mm_release()?

commit b69c49b78457f681ecfb3147bd968434ee6559c1
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Fri Jul 25 01:45:40 2008 -0700

    clean up duplicated alloc/free_thread_info
    
    We duplicate alloc/free_thread_info defines on many platforms (the
    majority uses __get_free_pages/free_pages).  This patch defines common
    defines and removes these duplicated defines.
    __HAVE_ARCH_THREAD_INFO_ALLOCATOR is introduced for platforms that do
    something different.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 552c8d8e77ad..5a5d6fef341d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -93,6 +93,23 @@ int nr_processes(void)
 static struct kmem_cache *task_struct_cachep;
 #endif
 
+#ifndef __HAVE_ARCH_THREAD_INFO_ALLOCATOR
+static inline struct thread_info *alloc_thread_info(struct task_struct *tsk)
+{
+#ifdef CONFIG_DEBUG_STACK_USAGE
+	gfp_t mask = GFP_KERNEL | __GFP_ZERO;
+#else
+	gfp_t mask = GFP_KERNEL;
+#endif
+	return (struct thread_info *)__get_free_pages(mask, THREAD_SIZE_ORDER);
+}
+
+static inline void free_thread_info(struct thread_info *ti)
+{
+	free_pages((unsigned long)ti, THREAD_SIZE_ORDER);
+}
+#endif
+
 /* SLAB cache for signal_struct structures (tsk->signal) */
 static struct kmem_cache *signal_cachep;
 

commit a1e78772d72b2616ed20e54896e68e0e7044854e
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed Jul 23 21:27:23 2008 -0700

    hugetlb: reserve huge pages for reliable MAP_PRIVATE hugetlbfs mappings until fork()
    
    This patch reserves huge pages at mmap() time for MAP_PRIVATE mappings in
    a similar manner to the reservations taken for MAP_SHARED mappings.  The
    reserve count is accounted both globally and on a per-VMA basis for
    private mappings.  This guarantees that a process that successfully calls
    mmap() will successfully fault all pages in the future unless fork() is
    called.
    
    The characteristics of private mappings of hugetlbfs files behaviour after
    this patch are;
    
    1. The process calling mmap() is guaranteed to succeed all future faults until
       it forks().
    2. On fork(), the parent may die due to SIGKILL on writes to the private
       mapping if enough pages are not available for the COW. For reasonably
       reliable behaviour in the face of a small huge page pool, children of
       hugepage-aware processes should not reference the mappings; such as
       might occur when fork()ing to exec().
    3. On fork(), the child VMAs inherit no reserves. Reads on pages already
       faulted by the parent will succeed. Successful writes will depend on enough
       huge pages being free in the pool.
    4. Quotas of the hugetlbfs mount are checked at reserve time for the mapper
       and at fault time otherwise.
    
    Before this patch, all reads or writes in the child potentially needs page
    allocations that can later lead to the death of the parent.  This applies
    to reads and writes of uninstantiated pages as well as COW.  After the
    patch it is only a write to an instantiated page that causes problems.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Acked-by: Adam Litke <agl@us.ibm.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index adefc1131f27..552c8d8e77ad 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -33,6 +33,7 @@
 #include <linux/cpu.h>
 #include <linux/cgroup.h>
 #include <linux/security.h>
+#include <linux/hugetlb.h>
 #include <linux/swap.h>
 #include <linux/syscalls.h>
 #include <linux/jiffies.h>
@@ -306,6 +307,14 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			spin_unlock(&file->f_mapping->i_mmap_lock);
 		}
 
+		/*
+		 * Clear hugetlb-related page reserves for children. This only
+		 * affects MAP_PRIVATE mappings. Faults generated by the child
+		 * are not guaranteed to succeed, even if read-only
+		 */
+		if (is_vm_hugetlb_page(tmp))
+			reset_vma_resv_huge_pages(tmp);
+
 		/*
 		 * Link in the new vma and copy the page table entries.
 		 */

commit f470021adb9190819c03d6d8c5c860a17480aa6d
Author: Roland McGrath <roland@redhat.com>
Date:   Mon Mar 24 18:36:23 2008 -0700

    ptrace children revamp
    
    ptrace no longer fiddles with the children/sibling links, and the
    old ptrace_children list is gone.  Now ptrace, whether of one's own
    children or another's via PTRACE_ATTACH, just uses the new ptraced
    list instead.
    
    There should be no user-visible difference that matters.  The only
    change is the order in which do_wait() sees multiple stopped
    children and stopped ptrace attachees.  Since wait_task_stopped()
    was changed earlier so it no longer reorders the children list, we
    already know this won't cause any new problems.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4bd2f516401f..adefc1131f27 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1125,8 +1125,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 */
 	p->group_leader = p;
 	INIT_LIST_HEAD(&p->thread_group);
-	INIT_LIST_HEAD(&p->ptrace_children);
-	INIT_LIST_HEAD(&p->ptrace_list);
+	INIT_LIST_HEAD(&p->ptrace_entry);
+	INIT_LIST_HEAD(&p->ptraced);
 
 	/* Now that the task is set up, run cgroup callbacks if
 	 * necessary. We need to run them before the task is visible
@@ -1198,7 +1198,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	}
 
 	if (likely(p->pid)) {
-		add_parent(p);
+		list_add_tail(&p->sibling, &p->real_parent->children);
 		if (unlikely(p->ptrace & PT_PTRACED))
 			__ptrace_link(p, current->parent);
 

commit 40e7babbb52b4b57721b9175aed7a14d93bf242f
Merge: 948769a5ba30 d12c1a37925a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 14 14:55:13 2008 -0700

    Merge branch 'core/locking' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core/locking' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      lockdep: fix kernel/fork.c warning
      lockdep: fix ftrace irq tracing false positive
      lockdep: remove duplicate definition of STATIC_LOCKDEP_MAP_INIT
      lockdep: add lock_class information to lock_chain and output it
      lockdep: add lock_class information to lock_chain and output it
      lockdep: output lock_class key instead of address for forward dependency output
      __mutex_lock_common: use signal_pending_state()
      mutex-debug: check mutex magic before owner
    
    Fixed up conflict in kernel/fork.c manually

commit e18425a0abc8eafa8e98ecffac517bb0c0904f4b
Merge: d1794f2c5b58 5806b81ac1c0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 14 14:49:54 2008 -0700

    Merge branch 'tracing/for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing/for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (228 commits)
      ftrace: build fix for ftraced_suspend
      ftrace: separate out the function enabled variable
      ftrace: add ftrace_kill_atomic
      ftrace: use current CPU for function startup
      ftrace: start wakeup tracing after setting function tracer
      ftrace: check proper config for preempt type
      ftrace: trace schedule
      ftrace: define function trace nop
      ftrace: move sched_switch enable after markers
      ftrace: prevent ftrace modifications while being kprobe'd, v2
      fix "ftrace: store mcount address in rec->ip"
      mmiotrace broken in linux-next (8-bit writes only)
      ftrace: avoid modifying kprobe'd records
      ftrace: freeze kprobe'd records
      kprobes: enable clean usage of get_kprobe
      ftrace: store mcount address in rec->ip
      ftrace: build fix with gcc 4.3
      namespacecheck: fixes
      ftrace: fix "notrace" filtering priority
      ftrace: fix printout
      ...

commit d12c1a37925a8ec386994169605fe99217295199
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 14 12:09:28 2008 +0200

    lockdep: fix kernel/fork.c warning
    
    fix:
    
    [    0.184011] ------------[ cut here ]------------
    [    0.188011] WARNING: at kernel/fork.c:918 copy_process+0x1c0/0x1084()
    [    0.192011] Pid: 0, comm: swapper Not tainted 2.6.26-tip-00351-g01d4a50-dirty #14521
    [    0.196011]  [<c0135d48>] warn_on_slowpath+0x3c/0x60
    [    0.200012]  [<c016f805>] ? __alloc_pages_internal+0x92/0x36b
    [    0.208012]  [<c033de5e>] ? __spin_lock_init+0x24/0x4a
    [    0.212012]  [<c01347e3>] copy_process+0x1c0/0x1084
    [    0.216013]  [<c013575f>] do_fork+0xb8/0x1ad
    [    0.220013]  [<c034f75e>] ? acpi_os_release_lock+0x8/0xa
    [    0.228013]  [<c034ff7a>] ? acpi_os_vprintf+0x20/0x24
    [    0.232014]  [<c01129ee>] kernel_thread+0x75/0x7d
    [    0.236014]  [<c0a491eb>] ? kernel_init+0x0/0x24a
    [    0.240014]  [<c0a491eb>] ? kernel_init+0x0/0x24a
    [    0.244014]  [<c01151b0>] ? kernel_thread_helper+0x0/0x10
    [    0.252015]  [<c06c6ac0>] rest_init+0x14/0x50
    [    0.256015]  [<c0a498ce>] start_kernel+0x2b9/0x2c0
    [    0.260015]  [<c0a4904f>] __init_begin+0x4f/0x57
    [    0.264016]  =======================
    [    0.268016] ---[ end trace 4eaa2a86a8e2da22 ]---
    [    0.272016] enabled ExtINT on CPU#0
    
    which occurs if CONFIG_TRACE_IRQFLAGS=y, CONFIG_DEBUG_LOCKDEP=y,
    but CONFIG_PROVE_LOCKING is disabled.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 19908b26cf80..cdb1f82d3bd2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -909,7 +909,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	rt_mutex_init_task(p);
 
-#ifdef CONFIG_TRACE_IRQFLAGS
+#ifdef CONFIG_PROVE_LOCKING
 	DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);
 	DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
 #endif

commit da9cbc87395308a21465bd25441297bbba0477e1
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Jun 30 20:42:08 2008 +0200

    block: blkdev.h cleanup, move iocontext stuff to iocontext.h
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 19908b26cf80..b71ccd09fc8d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -23,6 +23,7 @@
 #include <linux/sem.h>
 #include <linux/file.h>
 #include <linux/fdtable.h>
+#include <linux/iocontext.h>
 #include <linux/key.h>
 #include <linux/binfmts.h>
 #include <linux/mman.h>

commit 7c9f8861e6c9c839f913e49b98c3854daca18f27
Author: Eric Sandeen <sandeen@sandeen.net>
Date:   Tue Apr 22 16:38:23 2008 -0500

    stackprotector: use canary at end of stack to indicate overruns at oops time
    
    (Updated with a common max-stack-used checker that knows about
    the canary, as suggested by Joe Perches)
    
    Use a canary at the end of the stack to clearly indicate
    at oops time whether the stack has ever overflowed.
    
    This is a very simple implementation with a couple of
    drawbacks:
    
    1) a thread may legitimately use exactly up to the last
       word on the stack
    
     -- but the chances of doing this and then oopsing later seem slim
    
    2) it's possible that the stack usage isn't dense enough
       that the canary location could get skipped over
    
     -- but the worst that happens is that we don't flag the overrun
     -- though this happens fairly often in my testing :(
    
    With the code in place, an intentionally-bloated stack oops might
    do:
    
    BUG: unable to handle kernel paging request at ffff8103f84cc680
    IP: [<ffffffff810253df>] update_curr+0x9a/0xa8
    PGD 8063 PUD 0
    Thread overran stack or stack corrupted
    Oops: 0000 [1] SMP
    CPU 0
    ...
    
    ... unless the stack overrun is so bad that it corrupts some other
    thread.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 19908b26cf80..d428336e7aa1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -54,6 +54,7 @@
 #include <linux/tty.h>
 #include <linux/proc_fs.h>
 #include <linux/blkdev.h>
+#include <linux/magic.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -186,6 +187,8 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 {
 	struct task_struct *tsk;
 	struct thread_info *ti;
+	unsigned long *stackend;
+
 	int err;
 
 	prepare_to_copy(orig);
@@ -211,6 +214,8 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 		goto out;
 
 	setup_thread_stack(tsk, orig);
+	stackend = end_of_stack(tsk);
+	*stackend = STACK_END_MAGIC;	/* for overflow detection */
 
 #ifdef CONFIG_CC_STACKPROTECTOR
 	tsk->stack_canary = get_random_int();

commit 81d68a96a39844853b37f20cc8282d9b65b78ef3
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:42 2008 +0200

    ftrace: trace irq disabled critical timings
    
    This patch adds latency tracing for critical timings
    (how long interrupts are disabled for).
    
     "irqsoff" is added to /debugfs/tracing/available_tracers
    
    Note:
      tracing_max_latency
        also holds the max latency for irqsoff (in usecs).
       (default to large number so one must start latency tracing)
    
      tracing_thresh
        threshold (in usecs) to always print out if irqs off
        is detected to be longer than stated here.
        If irq_thresh is non-zero, then max_irq_latency
        is ignored.
    
    Here's an example of a trace with ftrace_enabled = 0
    
    =======
    preemption latency trace v1.1.5 on 2.6.24-rc7
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    --------------------------------------------------------------------
     latency: 100 us, #3/3, CPU#1 | (M:rt VP:0, KP:0, SP:0 HP:0 #P:2)
        -----------------
        | task: swapper-0 (uid:0 nice:0 policy:0 rt_prio:0)
        -----------------
     => started at: _spin_lock_irqsave+0x2a/0xb7
     => ended at:   _spin_unlock_irqrestore+0x32/0x5f
    
                     _------=> CPU#
                    / _-----=> irqs-off
                   | / _----=> need-resched
                   || / _---=> hardirq/softirq
                   ||| / _--=> preempt-depth
                   |||| /
                   |||||     delay
       cmd     pid ||||| time  |   caller
          \   /    |||||   \   |   /
     swapper-0     1d.s3    0us+: _spin_lock_irqsave+0x2a/0xb7 (e1000_update_stats+0x47/0x64c [e1000])
     swapper-0     1d.s3  100us : _spin_unlock_irqrestore+0x32/0x5f (e1000_update_stats+0x641/0x64c [e1000])
     swapper-0     1d.s3  100us : trace_hardirqs_on_caller+0x75/0x89 (_spin_unlock_irqrestore+0x32/0x5f)
    
    vim:ft=help
    =======
    
    And this is a trace with ftrace_enabled == 1
    
    =======
    preemption latency trace v1.1.5 on 2.6.24-rc7
    --------------------------------------------------------------------
     latency: 102 us, #12/12, CPU#1 | (M:rt VP:0, KP:0, SP:0 HP:0 #P:2)
        -----------------
        | task: swapper-0 (uid:0 nice:0 policy:0 rt_prio:0)
        -----------------
     => started at: _spin_lock_irqsave+0x2a/0xb7
     => ended at:   _spin_unlock_irqrestore+0x32/0x5f
    
                     _------=> CPU#
                    / _-----=> irqs-off
                   | / _----=> need-resched
                   || / _---=> hardirq/softirq
                   ||| / _--=> preempt-depth
                   |||| /
                   |||||     delay
       cmd     pid ||||| time  |   caller
          \   /    |||||   \   |   /
     swapper-0     1dNs3    0us+: _spin_lock_irqsave+0x2a/0xb7 (e1000_update_stats+0x47/0x64c [e1000])
     swapper-0     1dNs3   46us : e1000_read_phy_reg+0x16/0x225 [e1000] (e1000_update_stats+0x5e2/0x64c [e1000])
     swapper-0     1dNs3   46us : e1000_swfw_sync_acquire+0x10/0x99 [e1000] (e1000_read_phy_reg+0x49/0x225 [e1000])
     swapper-0     1dNs3   46us : e1000_get_hw_eeprom_semaphore+0x12/0xa6 [e1000] (e1000_swfw_sync_acquire+0x36/0x99 [e1000])
     swapper-0     1dNs3   47us : __const_udelay+0x9/0x47 (e1000_read_phy_reg+0x116/0x225 [e1000])
     swapper-0     1dNs3   47us+: __delay+0x9/0x50 (__const_udelay+0x45/0x47)
     swapper-0     1dNs3   97us : preempt_schedule+0xc/0x84 (__delay+0x4e/0x50)
     swapper-0     1dNs3   98us : e1000_swfw_sync_release+0xc/0x55 [e1000] (e1000_read_phy_reg+0x211/0x225 [e1000])
     swapper-0     1dNs3   99us+: e1000_put_hw_eeprom_semaphore+0x9/0x35 [e1000] (e1000_swfw_sync_release+0x50/0x55 [e1000])
     swapper-0     1dNs3  101us : _spin_unlock_irqrestore+0xe/0x5f (e1000_update_stats+0x641/0x64c [e1000])
     swapper-0     1dNs3  102us : _spin_unlock_irqrestore+0x32/0x5f (e1000_update_stats+0x641/0x64c [e1000])
     swapper-0     1dNs3  102us : trace_hardirqs_on_caller+0x75/0x89 (_spin_unlock_irqrestore+0x32/0x5f)
    
    vim:ft=help
    =======
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 19908b26cf80..d66d676dc362 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -909,7 +909,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	rt_mutex_init_task(p);
 
-#ifdef CONFIG_TRACE_IRQFLAGS
+#if defined(CONFIG_TRACE_IRQFLAGS) && defined(CONFIG_LOCKDEP)
 	DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);
 	DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
 #endif

commit 02afc6267f6d55d47aba9fcafdbd1b7230d2294a
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu May 8 19:42:56 2008 -0400

    [PATCH] dup_fd() fixes, part 1
    
    Move the sucker to fs/file.c in preparation to the rest
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 933e60ebccae..19908b26cf80 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -660,136 +660,6 @@ static int copy_fs(unsigned long clone_flags, struct task_struct *tsk)
 	return 0;
 }
 
-static int count_open_files(struct fdtable *fdt)
-{
-	int size = fdt->max_fds;
-	int i;
-
-	/* Find the last open fd */
-	for (i = size/(8*sizeof(long)); i > 0; ) {
-		if (fdt->open_fds->fds_bits[--i])
-			break;
-	}
-	i = (i+1) * 8 * sizeof(long);
-	return i;
-}
-
-static struct files_struct *alloc_files(void)
-{
-	struct files_struct *newf;
-	struct fdtable *fdt;
-
-	newf = kmem_cache_alloc(files_cachep, GFP_KERNEL);
-	if (!newf)
-		goto out;
-
-	atomic_set(&newf->count, 1);
-
-	spin_lock_init(&newf->file_lock);
-	newf->next_fd = 0;
-	fdt = &newf->fdtab;
-	fdt->max_fds = NR_OPEN_DEFAULT;
-	fdt->close_on_exec = (fd_set *)&newf->close_on_exec_init;
-	fdt->open_fds = (fd_set *)&newf->open_fds_init;
-	fdt->fd = &newf->fd_array[0];
-	INIT_RCU_HEAD(&fdt->rcu);
-	fdt->next = NULL;
-	rcu_assign_pointer(newf->fdt, fdt);
-out:
-	return newf;
-}
-
-/*
- * Allocate a new files structure and copy contents from the
- * passed in files structure.
- * errorp will be valid only when the returned files_struct is NULL.
- */
-static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
-{
-	struct files_struct *newf;
-	struct file **old_fds, **new_fds;
-	int open_files, size, i;
-	struct fdtable *old_fdt, *new_fdt;
-
-	*errorp = -ENOMEM;
-	newf = alloc_files();
-	if (!newf)
-		goto out;
-
-	spin_lock(&oldf->file_lock);
-	old_fdt = files_fdtable(oldf);
-	new_fdt = files_fdtable(newf);
-	open_files = count_open_files(old_fdt);
-
-	/*
-	 * Check whether we need to allocate a larger fd array and fd set.
-	 * Note: we're not a clone task, so the open count won't change.
-	 */
-	if (open_files > new_fdt->max_fds) {
-		new_fdt->max_fds = 0;
-		spin_unlock(&oldf->file_lock);
-		spin_lock(&newf->file_lock);
-		*errorp = expand_files(newf, open_files-1);
-		spin_unlock(&newf->file_lock);
-		if (*errorp < 0)
-			goto out_release;
-		new_fdt = files_fdtable(newf);
-		/*
-		 * Reacquire the oldf lock and a pointer to its fd table
-		 * who knows it may have a new bigger fd table. We need
-		 * the latest pointer.
-		 */
-		spin_lock(&oldf->file_lock);
-		old_fdt = files_fdtable(oldf);
-	}
-
-	old_fds = old_fdt->fd;
-	new_fds = new_fdt->fd;
-
-	memcpy(new_fdt->open_fds->fds_bits,
-		old_fdt->open_fds->fds_bits, open_files/8);
-	memcpy(new_fdt->close_on_exec->fds_bits,
-		old_fdt->close_on_exec->fds_bits, open_files/8);
-
-	for (i = open_files; i != 0; i--) {
-		struct file *f = *old_fds++;
-		if (f) {
-			get_file(f);
-		} else {
-			/*
-			 * The fd may be claimed in the fd bitmap but not yet
-			 * instantiated in the files array if a sibling thread
-			 * is partway through open().  So make sure that this
-			 * fd is available to the new process.
-			 */
-			FD_CLR(open_files - i, new_fdt->open_fds);
-		}
-		rcu_assign_pointer(*new_fds++, f);
-	}
-	spin_unlock(&oldf->file_lock);
-
-	/* compute the remainder to be cleared */
-	size = (new_fdt->max_fds - open_files) * sizeof(struct file *);
-
-	/* This is long word aligned thus could use a optimized version */
-	memset(new_fds, 0, size);
-
-	if (new_fdt->max_fds > open_files) {
-		int left = (new_fdt->max_fds-open_files)/8;
-		int start = open_files / (8 * sizeof(unsigned long));
-
-		memset(&new_fdt->open_fds->fds_bits[start], 0, left);
-		memset(&new_fdt->close_on_exec->fds_bits[start], 0, left);
-	}
-
-	return newf;
-
-out_release:
-	kmem_cache_free(files_cachep, newf);
-out:
-	return NULL;
-}
-
 static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 {
 	struct files_struct *oldf, *newf;

commit 9f3acc3140444a900ab280de942291959f0f615d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Apr 24 07:44:08 2008 -0400

    [PATCH] split linux/file.h
    
    Initial splitoff of the low-level stuff; taken to fdtable.h
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2bb675af4de3..933e60ebccae 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -22,6 +22,7 @@
 #include <linux/mempolicy.h>
 #include <linux/sem.h>
 #include <linux/file.h>
+#include <linux/fdtable.h>
 #include <linux/key.h>
 #include <linux/binfmts.h>
 #include <linux/mman.h>

commit db51aeccd7097ce19a522a4c5ff91c320f870e2b
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Apr 30 00:52:52 2008 -0700

    signals: microoptimize the usage of ->curr_target
    
    Suggested by Roland McGrath.
    
    Initialize signal->curr_target in copy_signal().  This way ->curr_target is
    never == NULL, we can kill the check in __group_complete_signal's hot path.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 068ffe007529..2bb675af4de3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -892,7 +892,7 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	sig->group_exit_code = 0;
 	sig->group_exit_task = NULL;
 	sig->group_stop_count = 0;
-	sig->curr_target = NULL;
+	sig->curr_target = tsk;
 	init_sigpending(&sig->shared_pending);
 	INIT_LIST_HEAD(&sig->posix_timers);
 

commit 925d1c401fa6cfd0df5d2e37da8981494ccdec07
Author: Matt Helsley <matthltc@us.ibm.com>
Date:   Tue Apr 29 01:01:36 2008 -0700

    procfs task exe symlink
    
    The kernel implements readlink of /proc/pid/exe by getting the file from
    the first executable VMA.  Then the path to the file is reconstructed and
    reported as the result.
    
    Because of the VMA walk the code is slightly different on nommu systems.
    This patch avoids separate /proc/pid/exe code on nommu systems.  Instead of
    walking the VMAs to find the first executable file-backed VMA we store a
    reference to the exec'd file in the mm_struct.
    
    That reference would prevent the filesystem holding the executable file
    from being unmounted even after unmapping the VMAs.  So we track the number
    of VM_EXECUTABLE VMAs and drop the new reference when the last one is
    unmapped.  This avoids pinning the mounted filesystem.
    
    [akpm@linux-foundation.org: improve comments]
    [yamamoto@valinux.co.jp: fix dup_mmap]
    Signed-off-by: Matt Helsley <matthltc@us.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: David Howells <dhowells@redhat.com>
    Cc:"Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: YAMAMOTO Takashi <yamamoto@valinux.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index de5c16c6b6ec..068ffe007529 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -431,6 +431,7 @@ void mmput(struct mm_struct *mm)
 	if (atomic_dec_and_test(&mm->mm_users)) {
 		exit_aio(mm);
 		exit_mmap(mm);
+		set_mm_exe_file(mm, NULL);
 		if (!list_empty(&mm->mmlist)) {
 			spin_lock(&mmlist_lock);
 			list_del(&mm->mmlist);
@@ -543,6 +544,8 @@ struct mm_struct *dup_mm(struct task_struct *tsk)
 	if (init_new_context(tsk, mm))
 		goto fail_nocontext;
 
+	dup_mm_exe_file(oldmm, mm);
+
 	err = dup_mmap(mm, oldmm);
 	if (err)
 		goto free_pt;

commit 6013f67fc1a4c7fa5bcab2d39c1eaa3e260c7ac1
Author: Manfred Spraul <manfred@colorfullife.com>
Date:   Tue Apr 29 01:00:59 2008 -0700

    ipc: sysvsem: force unshare(CLONE_SYSVSEM) when CLONE_NEWIPC
    
    sys_unshare(CLONE_NEWIPC) doesn't handle the undo lists properly, this can
    cause a kernel memory corruption.  CLONE_NEWIPC must detach from the existing
    undo lists.
    
    Fix, part 2: perform an implicit CLONE_SYSVSEM in CLONE_NEWIPC.  CLONE_NEWIPC
    creates a new IPC namespace, the task cannot access the existing semaphore
    arrays after the unshare syscall.  Thus the task can/must detach from the
    existing undo list entries, too.
    
    This fixes the kernel corruption, because it makes it impossible that
    undo records from two different namespaces are in sysvsem.undo_list.
    
    Signed-off-by: Manfred Spraul <manfred@colorfullife.com>
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Michael Kerrisk <mtk.manpages@googlemail.com>
    Cc: Pierre Peiffer <peifferp@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 01666979beac..de5c16c6b6ec 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1696,7 +1696,12 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 				CLONE_NEWNET))
 		goto bad_unshare_out;
 
-	if (unshare_flags & CLONE_SYSVSEM)
+	/*
+	 * CLONE_NEWIPC must also detach from the undolist: after switching
+	 * to a new ipc namespace, the semaphore arrays from the old
+	 * namespace are unreachable.
+	 */
+	if (unshare_flags & (CLONE_NEWIPC|CLONE_SYSVSEM))
 		do_sysvsem = 1;
 	if ((err = unshare_thread(unshare_flags)))
 		goto bad_unshare_out;

commit 9edff4ab1f8d82675277a04e359d0ed8bf14a7b7
Author: Manfred Spraul <manfred@colorfullife.com>
Date:   Tue Apr 29 01:00:57 2008 -0700

    ipc: sysvsem: implement sys_unshare(CLONE_SYSVSEM)
    
    sys_unshare(CLONE_NEWIPC) doesn't handle the undo lists properly, this can
    cause a kernel memory corruption.  CLONE_NEWIPC must detach from the existing
    undo lists.
    
    Fix, part 1: add support for sys_unshare(CLONE_SYSVSEM)
    
    The original reason to not support it was the potential (inevitable?)
    confusion due to the fact that sys_unshare(CLONE_SYSVSEM) has the
    inverse meaning of clone(CLONE_SYSVSEM).
    
    Our two most reasonable options then appear to be (1) fully support
    CLONE_SYSVSEM, or (2) continue to refuse explicit CLONE_SYSVSEM,
    but always do it anyway on unshare(CLONE_SYSVSEM).  This patch does
    (1).
    
    Changelog:
            Apr 16: SEH: switch to Manfred's alternative patch which
                    removes the unshare_semundo() function which
                    always refused CLONE_SYSVSEM.
    
    Signed-off-by: Manfred Spraul <manfred@colorfullife.com>
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Michael Kerrisk <mtk.manpages@googlemail.com>
    Cc: Pierre Peiffer <peifferp@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 156db96ff754..01666979beac 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1668,18 +1668,6 @@ static int unshare_fd(unsigned long unshare_flags, struct files_struct **new_fdp
 	return 0;
 }
 
-/*
- * Unsharing of semundo for tasks created with CLONE_SYSVSEM is not
- * supported yet
- */
-static int unshare_semundo(unsigned long unshare_flags, struct sem_undo_list **new_ulistp)
-{
-	if (unshare_flags & CLONE_SYSVSEM)
-		return -EINVAL;
-
-	return 0;
-}
-
 /*
  * unshare allows a process to 'unshare' part of the process
  * context which was originally shared using clone.  copy_*
@@ -1695,8 +1683,8 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	struct sighand_struct *new_sigh = NULL;
 	struct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;
 	struct files_struct *fd, *new_fd = NULL;
-	struct sem_undo_list *new_ulist = NULL;
 	struct nsproxy *new_nsproxy = NULL;
+	int do_sysvsem = 0;
 
 	check_unshare_flags(&unshare_flags);
 
@@ -1708,6 +1696,8 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 				CLONE_NEWNET))
 		goto bad_unshare_out;
 
+	if (unshare_flags & CLONE_SYSVSEM)
+		do_sysvsem = 1;
 	if ((err = unshare_thread(unshare_flags)))
 		goto bad_unshare_out;
 	if ((err = unshare_fs(unshare_flags, &new_fs)))
@@ -1718,13 +1708,17 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		goto bad_unshare_cleanup_sigh;
 	if ((err = unshare_fd(unshare_flags, &new_fd)))
 		goto bad_unshare_cleanup_vm;
-	if ((err = unshare_semundo(unshare_flags, &new_ulist)))
-		goto bad_unshare_cleanup_fd;
 	if ((err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,
 			new_fs)))
-		goto bad_unshare_cleanup_semundo;
+		goto bad_unshare_cleanup_fd;
 
-	if (new_fs ||  new_mm || new_fd || new_ulist || new_nsproxy) {
+	if (new_fs ||  new_mm || new_fd || do_sysvsem || new_nsproxy) {
+		if (do_sysvsem) {
+			/*
+			 * CLONE_SYSVSEM is equivalent to sys_exit().
+			 */
+			exit_sem(current);
+		}
 
 		if (new_nsproxy) {
 			switch_task_namespaces(current, new_nsproxy);
@@ -1760,7 +1754,6 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	if (new_nsproxy)
 		put_nsproxy(new_nsproxy);
 
-bad_unshare_cleanup_semundo:
 bad_unshare_cleanup_fd:
 	if (new_fd)
 		put_files_struct(new_fd);

commit cf475ad28ac35cc9ba612d67158f29b73b38b05d
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Tue Apr 29 01:00:16 2008 -0700

    cgroups: add an owner to the mm_struct
    
    Remove the mem_cgroup member from mm_struct and instead adds an owner.
    
    This approach was suggested by Paul Menage.  The advantage of this approach
    is that, once the mm->owner is known, using the subsystem id, the cgroup
    can be determined.  It also allows several control groups that are
    virtually grouped by mm_struct, to exist independent of the memory
    controller i.e., without adding mem_cgroup's for each controller, to
    mm_struct.
    
    A new config option CONFIG_MM_OWNER is added and the memory resource
    controller selects this config option.
    
    This patch also adds cgroup callbacks to notify subsystems when mm->owner
    changes.  The mm_cgroup_changed callback is called with the task_lock() of
    the new task held and is called just prior to changing the mm->owner.
    
    I am indebted to Paul Menage for the several reviews of this patchset and
    helping me make it lighter and simpler.
    
    This patch was tested on a powerpc box, it was compiled with both the
    MM_OWNER config turned on and off.
    
    After the thread group leader exits, it's moved to init_css_state by
    cgroup_exit(), thus all future charges from runnings threads would be
    redirected to the init_css_set's subsystem.
    
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Pavel Emelianov <xemul@openvz.org>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Sudhir Kumar <skumar@linux.vnet.ibm.com>
    Cc: YAMAMOTO Takashi <yamamoto@valinux.co.jp>
    Cc: Hirokazu Takahashi <taka@valinux.co.jp>
    Cc: David Rientjes <rientjes@google.com>,
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Reviewed-by: Paul Menage <menage@google.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6067e429f281..156db96ff754 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -381,14 +381,13 @@ static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 	mm->ioctx_list = NULL;
 	mm->free_area_cache = TASK_UNMAPPED_BASE;
 	mm->cached_hole_size = ~0UL;
-	mm_init_cgroup(mm, p);
+	mm_init_owner(mm, p);
 
 	if (likely(!mm_alloc_pgd(mm))) {
 		mm->def_flags = 0;
 		return mm;
 	}
 
-	mm_free_cgroup(mm);
 	free_mm(mm);
 	return NULL;
 }
@@ -438,7 +437,6 @@ void mmput(struct mm_struct *mm)
 			spin_unlock(&mmlist_lock);
 		}
 		put_swap_token(mm);
-		mm_free_cgroup(mm);
 		mmdrop(mm);
 	}
 }
@@ -982,6 +980,13 @@ static void rt_mutex_init_task(struct task_struct *p)
 #endif
 }
 
+#ifdef CONFIG_MM_OWNER
+void mm_init_owner(struct mm_struct *mm, struct task_struct *p)
+{
+	mm->owner = p;
+}
+#endif /* CONFIG_MM_OWNER */
+
 /*
  * This creates a new process as a copy of the old one,
  * but does not actually start it yet.

commit 846a16bf0fc80dc95a414ffce465e3cbf9680247
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:09 2008 -0700

    mempolicy: rename mpol_copy to mpol_dup
    
    This patch renames mpol_copy() to mpol_dup() because, well, that's what it
    does.  Like, e.g., strdup() for strings, mpol_dup() takes a pointer to an
    existing mempolicy, allocates a new one and copies the contents.
    
    In a later patch, I want to use the name mpol_copy() to copy the contents from
    one mempolicy to another like, e.g., strcpy() does for strings.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1a5ae2084574..6067e429f281 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -279,7 +279,7 @@ static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		if (!tmp)
 			goto fail_nomem;
 		*tmp = *mpnt;
-		pol = mpol_copy(vma_policy(mpnt));
+		pol = mpol_dup(vma_policy(mpnt));
 		retval = PTR_ERR(pol);
 		if (IS_ERR(pol))
 			goto fail_nomem_policy;
@@ -1116,7 +1116,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->audit_context = NULL;
 	cgroup_fork(p);
 #ifdef CONFIG_NUMA
- 	p->mempolicy = mpol_copy(p->mempolicy);
+	p->mempolicy = mpol_dup(p->mempolicy);
  	if (IS_ERR(p->mempolicy)) {
  		retval = PTR_ERR(p->mempolicy);
  		p->mempolicy = NULL;

commit f0be3d32b05d3fea2fcdbbb81a39dac2a7163169
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:08 2008 -0700

    mempolicy: rename mpol_free to mpol_put
    
    This is a change that was requested some time ago by Mel Gorman.  Makes sense
    to me, so here it is.
    
    Note: I retain the name "mpol_free_shared_policy()" because it actually does
    free the shared_policy, which is NOT a reference counted object.  However, ...
    
    The mempolicy object[s] referenced by the shared_policy are reference counted,
    so mpol_put() is used to release the reference held by the shared_policy.  The
    mempolicy might not be freed at this time, because some task attached to the
    shared object associated with the shared policy may be in the process of
    allocating a page based on the mempolicy.  In that case, the task performing
    the allocation will hold a reference on the mempolicy, obtained via
    mpol_shared_policy_lookup().  The mempolicy will be freed when all tasks
    holding such a reference have called mpol_put() for the mempolicy.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c674aa8d3c31..1a5ae2084574 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1374,7 +1374,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	security_task_free(p);
 bad_fork_cleanup_policy:
 #ifdef CONFIG_NUMA
-	mpol_free(p->mempolicy);
+	mpol_put(p->mempolicy);
 bad_fork_cleanup_cgroup:
 #endif
 	cgroup_exit(p, cgroup_callbacks_done);

commit 402b08622d9ac6e32e25289573272e0f21bb58a7
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Tue Mar 25 18:47:10 2008 +0100

    s390: KVM preparation: provide hook to enable pgstes in user pagetable
    
    The SIE instruction on s390 uses the 2nd half of the page table page to
    virtualize the storage keys of a guest. This patch offers the s390_enable_sie
    function, which reorganizes the page tables of a single-threaded process to
    reserve space in the page table:
    s390_enable_sie makes sure that the process is single threaded and then uses
    dup_mm to create a new mm with reorganized page tables. The old mm is freed
    and the process has now a page status extended field after every page table.
    
    Code that wants to exploit pgstes should SELECT CONFIG_PGSTE.
    
    This patch has a small common code hit, namely making dup_mm non-static.
    
    Edit (Carsten): I've modified Martin's patch, following Jeremy Fitzhardinge's
    review feedback. Now we do have the prototype for dup_mm in
    include/linux/sched.h. Following Martin's suggestion, s390_enable_sie() does now
    call task_lock() to prevent race against ptrace modification of mm_users.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index cb46befdd3a0..c674aa8d3c31 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -521,7 +521,7 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
  * Allocate a new mm structure and copy contents from the
  * mm structure of the passed in task structure.
  */
-static struct mm_struct *dup_mm(struct task_struct *tsk)
+struct mm_struct *dup_mm(struct task_struct *tsk)
 {
 	struct mm_struct *mm, *oldmm = current->mm;
 	int err;

commit 50704516f334d5036c09b0ecc0064598f7c5596f
Author: Al Viro <viro@ZenIV.linux.org.uk>
Date:   Sat Apr 26 05:25:00 2008 +0100

    Fix uninitialized 'copy' in unshare_files
    
            Arrgghhh...
    
    Sorry about that, I'd been sure I'd folded that one, but it actually got
    lost.  Please apply - that breaks execve().
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Tested-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index efb618fc8ffe..cb46befdd3a0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1787,7 +1787,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 int unshare_files(struct files_struct **displaced)
 {
 	struct task_struct *task = current;
-	struct files_struct *copy;
+	struct files_struct *copy = NULL;
 	int error;
 
 	error = unshare_fd(CLONE_FILES, &copy);

commit 3b1253880b7a9e6db54b943b2d40bcf2202f58ab
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Apr 22 05:31:30 2008 -0400

    [PATCH] sanitize unshare_files/reset_files_struct
    
    * let unshare_files() give caller the displaced files_struct
    * don't bother with grabbing reference only to drop it in the
      caller if it hadn't been shared in the first place
    * in that form unshare_files() is trivially implemented via
      unshare_fd(), so we eliminate the duplicate logics in fork.c
    * reset_files_struct() is not just only called for current;
      it will break the system if somebody ever calls it for anything
      else (we can't modify ->files of somebody else).  Lose the
      task_struct * argument.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2fc11f2e2b21..efb618fc8ffe 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -840,36 +840,6 @@ static int copy_io(unsigned long clone_flags, struct task_struct *tsk)
 	return 0;
 }
 
-/*
- *	Helper to unshare the files of the current task.
- *	We don't want to expose copy_files internals to
- *	the exec layer of the kernel.
- */
-
-int unshare_files(void)
-{
-	struct files_struct *files  = current->files;
-	struct files_struct *newf;
-	int error = 0;
-
-	BUG_ON(!files);
-
-	/* This can race but the race causes us to copy when we don't
-	   need to and drop the copy */
-	if(atomic_read(&files->count) == 1)
-	{
-		atomic_inc(&files->count);
-		return 0;
-	}
-	newf = dup_fd(files, &error);
-	if (newf) {
-		task_lock(current);
-		current->files = newf;
-		task_unlock(current);
-	}
-	return error;
-}
-
 static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct sighand_struct *sig;
@@ -1807,3 +1777,27 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 bad_unshare_out:
 	return err;
 }
+
+/*
+ *	Helper to unshare the files of the current task.
+ *	We don't want to expose copy_files internals to
+ *	the exec layer of the kernel.
+ */
+
+int unshare_files(struct files_struct **displaced)
+{
+	struct task_struct *task = current;
+	struct files_struct *copy;
+	int error;
+
+	error = unshare_fd(CLONE_FILES, &copy);
+	if (error || !copy) {
+		*displaced = NULL;
+		return error;
+	}
+	*displaced = task->files;
+	task_lock(task);
+	task->files = copy;
+	task_unlock(task);
+	return 0;
+}

commit fd8328be874f4190a811c58cd4778ec2c74d2c05
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Apr 22 05:11:59 2008 -0400

    [PATCH] sanitize handling of shared descriptor tables in failing execve()
    
    * unshare_files() can fail; doing it after irreversible actions is wrong
      and de_thread() is certainly irreversible.
    * since we do it unconditionally anyway, we might as well do it in do_execve()
      and save ourselves the PITA in binfmt handlers, etc.
    * while we are at it, binfmt_som actually leaked files_struct on failure.
    
    As a side benefit, unshare_files(), put_files_struct() and reset_files_struct()
    become unexported.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 76f05a08062b..2fc11f2e2b21 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -870,8 +870,6 @@ int unshare_files(void)
 	return error;
 }
 
-EXPORT_SYMBOL(unshare_files);
-
 static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct sighand_struct *sig;

commit 6b335d9c80d7f3c2a3f6545f664ae9007a0f3821
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Apr 22 04:45:46 2008 -0400

    [PATCH] close race in unshare_files()
    
    updating current->files requires task_lock
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index 89fe414645e9..76f05a08062b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -805,12 +805,6 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 		goto out;
 	}
 
-	/*
-	 * Note: we may be using current for both targets (See exec.c)
-	 * This works because we cache current->files (old) as oldf. Don't
-	 * break this.
-	 */
-	tsk->files = NULL;
 	newf = dup_fd(oldf, &error);
 	if (!newf)
 		goto out;
@@ -855,7 +849,8 @@ static int copy_io(unsigned long clone_flags, struct task_struct *tsk)
 int unshare_files(void)
 {
 	struct files_struct *files  = current->files;
-	int rc;
+	struct files_struct *newf;
+	int error = 0;
 
 	BUG_ON(!files);
 
@@ -866,10 +861,13 @@ int unshare_files(void)
 		atomic_inc(&files->count);
 		return 0;
 	}
-	rc = copy_files(0, current);
-	if(rc)
-		current->files = files;
-	return rc;
+	newf = dup_fd(files, &error);
+	if (newf) {
+		task_lock(current);
+		current->files = newf;
+		task_unlock(current);
+	}
+	return error;
 }
 
 EXPORT_SYMBOL(unshare_files);

commit 2adee9b30d1382fba97825b9c50e4f50a0117c36
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed Apr 16 10:25:35 2008 +0200

    x86: fpu xstate split fix
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 44a18192c420..89fe414645e9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -132,9 +132,13 @@ void __put_task_struct(struct task_struct *tsk)
 		free_task(tsk);
 }
 
-void __attribute__((weak)) arch_task_cache_init(void)
-{
-}
+/*
+ * macro override instead of weak attribute alias, to workaround
+ * gcc 4.1.0 and 4.1.1 bugs with weak attribute and empty functions.
+ */
+#ifndef arch_task_cache_init
+#define arch_task_cache_init()
+#endif
 
 void __init fork_init(unsigned long mempages)
 {

commit 61c4628b538608c1a85211ed8438136adfeb9a95
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Mon Mar 10 15:28:04 2008 -0700

    x86, fpu: split FPU state from task struct - v5
    
    Split the FPU save area from the task struct. This allows easy migration
    of FPU context, and it's generally cleaner. It also allows the following
    two optimizations:
    
    1) only allocate when the application actually uses FPU, so in the first
    lazy FPU trap. This could save memory for non-fpu using apps. Next patch
    does this lazy allocation.
    
    2) allocate the right size for the actual cpu rather than 512 bytes always.
    Patches enabling xsave/xrstor support (coming shortly) will take advantage
    of this.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9c042f901570..44a18192c420 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -132,6 +132,10 @@ void __put_task_struct(struct task_struct *tsk)
 		free_task(tsk);
 }
 
+void __attribute__((weak)) arch_task_cache_init(void)
+{
+}
+
 void __init fork_init(unsigned long mempages)
 {
 #ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
@@ -144,6 +148,9 @@ void __init fork_init(unsigned long mempages)
 			ARCH_MIN_TASKALIGN, SLAB_PANIC, NULL);
 #endif
 
+	/* do the arch specific task caches init */
+	arch_task_cache_init();
+
 	/*
 	 * The default maximum number of threads is set to a safe
 	 * value: the thread structures can take up at most half
@@ -163,6 +170,13 @@ void __init fork_init(unsigned long mempages)
 		init_task.signal->rlim[RLIMIT_NPROC];
 }
 
+int __attribute__((weak)) arch_dup_task_struct(struct task_struct *dst,
+					       struct task_struct *src)
+{
+	*dst = *src;
+	return 0;
+}
+
 static struct task_struct *dup_task_struct(struct task_struct *orig)
 {
 	struct task_struct *tsk;
@@ -181,15 +195,15 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 		return NULL;
 	}
 
-	*tsk = *orig;
+ 	err = arch_dup_task_struct(tsk, orig);
+	if (err)
+		goto out;
+
 	tsk->stack = ti;
 
 	err = prop_local_init_single(&tsk->dirties);
-	if (err) {
-		free_thread_info(ti);
-		free_task_struct(tsk);
-		return NULL;
-	}
+	if (err)
+		goto out;
 
 	setup_thread_stack(tsk, orig);
 
@@ -205,6 +219,11 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 #endif
 	tsk->splice_pipe = NULL;
 	return tsk;
+
+out:
+	free_thread_info(ti);
+	free_task_struct(tsk);
+	return NULL;
 }
 
 #ifdef CONFIG_MMU

commit 1d4a788f15302877ff2cb08f22009e290a36a209
Author: YAMAMOTO Takashi <yamamoto@valinux.co.jp>
Date:   Fri Mar 28 14:15:50 2008 -0700

    memcgroup: fix spurious EBUSY on memory cgroup removal
    
    Call mm_free_cgroup earlier.  Otherwise a reference due to lazy mm switching
    can prevent cgroup removal.
    
    Signed-off-by: YAMAMOTO Takashi <yamamoto@valinux.co.jp>
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Paul Menage <menage@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index dd249c37b3a3..9c042f901570 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -394,7 +394,6 @@ void __mmdrop(struct mm_struct *mm)
 {
 	BUG_ON(mm == &init_mm);
 	mm_free_pgd(mm);
-	mm_free_cgroup(mm);
 	destroy_context(mm);
 	free_mm(mm);
 }
@@ -416,6 +415,7 @@ void mmput(struct mm_struct *mm)
 			spin_unlock(&mmlist_lock);
 		}
 		put_swap_token(mm);
+		mm_free_cgroup(mm);
 		mmdrop(mm);
 	}
 }

commit 6ac08c39a16f72c2d3e845cb6849a1392fa03e80
Author: Jan Blunck <jblunck@suse.de>
Date:   Thu Feb 14 19:34:38 2008 -0800

    Use struct path in fs_struct
    
    * Use struct path in fs_struct.
    
    Signed-off-by: Andreas Gruenbacher <agruen@suse.de>
    Signed-off-by: Jan Blunck <jblunck@suse.de>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4363a4eb84e3..dd249c37b3a3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -600,16 +600,16 @@ static struct fs_struct *__copy_fs_struct(struct fs_struct *old)
 		rwlock_init(&fs->lock);
 		fs->umask = old->umask;
 		read_lock(&old->lock);
-		fs->rootmnt = mntget(old->rootmnt);
-		fs->root = dget(old->root);
-		fs->pwdmnt = mntget(old->pwdmnt);
-		fs->pwd = dget(old->pwd);
-		if (old->altroot) {
-			fs->altrootmnt = mntget(old->altrootmnt);
-			fs->altroot = dget(old->altroot);
+		fs->root = old->root;
+		path_get(&old->root);
+		fs->pwd = old->pwd;
+		path_get(&old->pwd);
+		if (old->altroot.dentry) {
+			fs->altroot = old->altroot;
+			path_get(&old->altroot);
 		} else {
-			fs->altrootmnt = NULL;
-			fs->altroot = NULL;
+			fs->altroot.mnt = NULL;
+			fs->altroot.dentry = NULL;
 		}
 		read_unlock(&old->lock);
 	}

commit 7ad5b3a505e68cfdc342933d6e0fc0eaa5e0a4f7
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Fri Feb 8 04:19:53 2008 -0800

    kernel: remove fastcall in kernel/*
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 31a2bad63a08..4363a4eb84e3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -390,7 +390,7 @@ struct mm_struct * mm_alloc(void)
  * is dropped: either by a lazy thread or by
  * mmput. Free the page directory and the mm.
  */
-void fastcall __mmdrop(struct mm_struct *mm)
+void __mmdrop(struct mm_struct *mm)
 {
 	BUG_ON(mm == &init_mm);
 	mm_free_pgd(mm);

commit 6c5f3e7b43300508fe3947ff3cfff0f86043bb57
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Feb 8 04:19:20 2008 -0800

    Pidns: make full use of xxx_vnr() calls
    
    Some time ago the xxx_vnr() calls (e.g.  pid_vnr or find_task_by_vpid) were
    _all_ converted to operate on the current pid namespace.  After this each call
    like xxx_nr_ns(foo, current->nsproxy->pid_ns) is nothing but a xxx_vnr(foo)
    one.
    
    Switch all the xxx_nr_ns() callers to use the xxx_vnr() calls where
    appropriate.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Reviewed-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ca54d9704644..31a2bad63a08 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1488,13 +1488,7 @@ long do_fork(unsigned long clone_flags,
 	if (!IS_ERR(p)) {
 		struct completion vfork;
 
-		/*
-		 * this is enough to call pid_nr_ns here, but this if
-		 * improves optimisation of regular fork()
-		 */
-		nr = (clone_flags & CLONE_NEWPID) ?
-			task_pid_nr_ns(p, current->nsproxy->pid_ns) :
-				task_pid_vnr(p);
+		nr = task_pid_vnr(p);
 
 		if (clone_flags & CLONE_PARENT_SETTID)
 			put_user(nr, parent_tidptr);

commit fea9d175545b38cb3e84569400419eb81bc90fa3
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:19 2008 -0800

    ITIMER_REAL: convert to use struct pid
    
    signal_struct->tsk points to the ->group_leader and thus we have the nasty
    code in de_thread() which has to change it and restart ->real_timer if the
    leader is changed.
    
    Use "struct pid *leader_pid" instead.  This also allows us to kill now
    unneeded send_group_sig_info().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Davide Libenzi <davidel@xmailserver.org>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Roland McGrath <roland@redhat.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b2ef8e4fad70..ca54d9704644 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -909,7 +909,6 @@ static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	sig->it_real_incr.tv64 = 0;
 	sig->real_timer.function = it_real_fn;
-	sig->tsk = tsk;
 
 	sig->it_virt_expires = cputime_zero;
 	sig->it_virt_incr = cputime_zero;
@@ -1338,6 +1337,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			if (clone_flags & CLONE_NEWPID)
 				p->nsproxy->pid_ns->child_reaper = p;
 
+			p->signal->leader_pid = pid;
 			p->signal->tty = current->signal->tty;
 			set_task_pgrp(p, task_pgrp_nr(current));
 			set_task_session(p, task_session_nr(current));

commit 78fb74669e80883323391090e4d26d17fe29488f
Author: Pavel Emelianov <xemul@openvz.org>
Date:   Thu Feb 7 00:13:51 2008 -0800

    Memory controller: accounting setup
    
    Basic setup routines, the mm_struct has a pointer to the cgroup that
    it belongs to and the the page has a page_cgroup associated with it.
    
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Cc: Kirill Korotaev <dev@sw.ru>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3995297567a9..b2ef8e4fad70 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -40,6 +40,7 @@
 #include <linux/ptrace.h>
 #include <linux/mount.h>
 #include <linux/audit.h>
+#include <linux/memcontrol.h>
 #include <linux/profile.h>
 #include <linux/rmap.h>
 #include <linux/acct.h>
@@ -340,7 +341,7 @@ __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
 
 #include <linux/init_task.h>
 
-static struct mm_struct * mm_init(struct mm_struct * mm)
+static struct mm_struct * mm_init(struct mm_struct * mm, struct task_struct *p)
 {
 	atomic_set(&mm->mm_users, 1);
 	atomic_set(&mm->mm_count, 1);
@@ -357,11 +358,14 @@ static struct mm_struct * mm_init(struct mm_struct * mm)
 	mm->ioctx_list = NULL;
 	mm->free_area_cache = TASK_UNMAPPED_BASE;
 	mm->cached_hole_size = ~0UL;
+	mm_init_cgroup(mm, p);
 
 	if (likely(!mm_alloc_pgd(mm))) {
 		mm->def_flags = 0;
 		return mm;
 	}
+
+	mm_free_cgroup(mm);
 	free_mm(mm);
 	return NULL;
 }
@@ -376,7 +380,7 @@ struct mm_struct * mm_alloc(void)
 	mm = allocate_mm();
 	if (mm) {
 		memset(mm, 0, sizeof(*mm));
-		mm = mm_init(mm);
+		mm = mm_init(mm, current);
 	}
 	return mm;
 }
@@ -390,6 +394,7 @@ void fastcall __mmdrop(struct mm_struct *mm)
 {
 	BUG_ON(mm == &init_mm);
 	mm_free_pgd(mm);
+	mm_free_cgroup(mm);
 	destroy_context(mm);
 	free_mm(mm);
 }
@@ -511,7 +516,7 @@ static struct mm_struct *dup_mm(struct task_struct *tsk)
 	mm->token_priority = 0;
 	mm->last_interval = 0;
 
-	if (!mm_init(mm))
+	if (!mm_init(mm, tsk))
 		goto fail_nomem;
 
 	if (init_new_context(tsk, mm))

commit 6b2fb3c65844452bb9e8b449d50863d1b36c5dc0
Author: Adrian Bunk <bunk@kernel.org>
Date:   Wed Feb 6 01:37:55 2008 -0800

    idle_regs() must be __cpuinit
    
    Fix the following section mismatch with CONFIG_HOTPLUG=n,
    CONFIG_HOTPLUG_CPU=y:
    
    WARNING: vmlinux.o(.text+0x399a6): Section mismatch: reference to .init.text.5:idle_regs (between 'fork_idle' and 'get_task_mm')
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8adfe5ddb688..3995297567a9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1399,7 +1399,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	return ERR_PTR(retval);
 }
 
-noinline struct pt_regs * __devinit __attribute__((weak)) idle_regs(struct pt_regs *regs)
+noinline struct pt_regs * __cpuinit __attribute__((weak)) idle_regs(struct pt_regs *regs)
 {
 	memset(regs, 0, sizeof(struct pt_regs));
 	return regs;

commit d9ae90ac4bdce769ddb27c2e24c3351a30c3daf8
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Feb 6 01:36:13 2008 -0800

    use __set_task_state() for TRACED/STOPPED tasks
    
    1. It is much easier to grep for ->state change if __set_task_state() is used
       instead of the direct assignment.
    
    2. ptrace_stop() and handle_group_stop() use set_task_state() which adds the
       unneeded mb() (btw even if we use mb() it is still possible that do_wait()
       sees the new ->state but not ->exit_code, but this is ok).
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2b55b74cd999..8adfe5ddb688 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1510,7 +1510,7 @@ long do_fork(unsigned long clone_flags,
 		if (!(clone_flags & CLONE_STOPPED))
 			wake_up_new_task(p, clone_flags);
 		else
-			p->state = TASK_STOPPED;
+			__set_task_state(p, TASK_STOPPED);
 
 		if (unlikely (trace)) {
 			current->ptrace_message = nr;

commit 3b7391de67da515c91f48aa371de77cb6cc5c07e
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Feb 4 22:29:45 2008 -0800

    capabilities: introduce per-process capability bounding set
    
    The capability bounding set is a set beyond which capabilities cannot grow.
     Currently cap_bset is per-system.  It can be manipulated through sysctl,
    but only init can add capabilities.  Root can remove capabilities.  By
    default it includes all caps except CAP_SETPCAP.
    
    This patch makes the bounding set per-process when file capabilities are
    enabled.  It is inherited at fork from parent.  Noone can add elements,
    CAP_SETPCAP is required to remove them.
    
    One example use of this is to start a safer container.  For instance, until
    device namespaces or per-container device whitelists are introduced, it is
    best to take CAP_MKNOD away from a container.
    
    The bounding set will not affect pP and pE immediately.  It will only
    affect pP' and pE' after subsequent exec()s.  It also does not affect pI,
    and exec() does not constrain pI'.  So to really start a shell with no way
    of regain CAP_MKNOD, you would do
    
            prctl(PR_CAPBSET_DROP, CAP_MKNOD);
            cap_t cap = cap_get_proc();
            cap_value_t caparray[1];
            caparray[0] = CAP_MKNOD;
            cap_set_flag(cap, CAP_INHERITABLE, 1, caparray, CAP_DROP);
            cap_set_proc(cap);
            cap_free(cap);
    
    The following test program will get and set the bounding
    set (but not pI).  For instance
    
            ./bset get
                    (lists capabilities in bset)
            ./bset drop cap_net_raw
                    (starts shell with new bset)
                    (use capset, setuid binary, or binary with
                    file capabilities to try to increase caps)
    
    ************************************************************
    cap_bound.c
    ************************************************************
     #include <sys/prctl.h>
     #include <linux/capability.h>
     #include <sys/types.h>
     #include <unistd.h>
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
    
     #ifndef PR_CAPBSET_READ
     #define PR_CAPBSET_READ 23
     #endif
    
     #ifndef PR_CAPBSET_DROP
     #define PR_CAPBSET_DROP 24
     #endif
    
    int usage(char *me)
    {
            printf("Usage: %s get\n", me);
            printf("       %s drop <capability>\n", me);
            return 1;
    }
    
     #define numcaps 32
    char *captable[numcaps] = {
            "cap_chown",
            "cap_dac_override",
            "cap_dac_read_search",
            "cap_fowner",
            "cap_fsetid",
            "cap_kill",
            "cap_setgid",
            "cap_setuid",
            "cap_setpcap",
            "cap_linux_immutable",
            "cap_net_bind_service",
            "cap_net_broadcast",
            "cap_net_admin",
            "cap_net_raw",
            "cap_ipc_lock",
            "cap_ipc_owner",
            "cap_sys_module",
            "cap_sys_rawio",
            "cap_sys_chroot",
            "cap_sys_ptrace",
            "cap_sys_pacct",
            "cap_sys_admin",
            "cap_sys_boot",
            "cap_sys_nice",
            "cap_sys_resource",
            "cap_sys_time",
            "cap_sys_tty_config",
            "cap_mknod",
            "cap_lease",
            "cap_audit_write",
            "cap_audit_control",
            "cap_setfcap"
    };
    
    int getbcap(void)
    {
            int comma=0;
            unsigned long i;
            int ret;
    
            printf("i know of %d capabilities\n", numcaps);
            printf("capability bounding set:");
            for (i=0; i<numcaps; i++) {
                    ret = prctl(PR_CAPBSET_READ, i);
                    if (ret < 0)
                            perror("prctl");
                    else if (ret==1)
                            printf("%s%s", (comma++) ? ", " : " ", captable[i]);
            }
            printf("\n");
            return 0;
    }
    
    int capdrop(char *str)
    {
            unsigned long i;
    
            int found=0;
            for (i=0; i<numcaps; i++) {
                    if (strcmp(captable[i], str) == 0) {
                            found=1;
                            break;
                    }
            }
            if (!found)
                    return 1;
            if (prctl(PR_CAPBSET_DROP, i)) {
                    perror("prctl");
                    return 1;
            }
            return 0;
    }
    
    int main(int argc, char *argv[])
    {
            if (argc<2)
                    return usage(argv[0]);
            if (strcmp(argv[1], "get")==0)
                    return getbcap();
            if (strcmp(argv[1], "drop")!=0 || argc<3)
                    return usage(argv[0]);
            if (capdrop(argv[2])) {
                    printf("unknown capability\n");
                    return 1;
            }
            return execl("/bin/bash", "/bin/bash", NULL);
    }
    ************************************************************
    
    [serue@us.ibm.com: fix typo]
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew G. Morgan <morgan@kernel.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Casey Schaufler <casey@schaufler-ca.com>a
    Signed-off-by: "Serge E. Hallyn" <serue@us.ibm.com>
    Tested-by: Jiri Slaby <jirislaby@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1160f87ba700..2b55b74cd999 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1118,6 +1118,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_SECURITY
 	p->security = NULL;
 #endif
+	p->cap_bset = current->cap_bset;
 	p->io_context = NULL;
 	p->audit_context = NULL;
 	cgroup_fork(p);

commit 5e5419734c8719cbc01af959ad9c0844002c0df5
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Feb 4 22:29:14 2008 -0800

    add mm argument to pte/pmd/pud/pgd_free
    
    (with Martin Schwidefsky <schwidefsky@de.ibm.com>)
    
    The pgd/pud/pmd/pte page table allocation functions get a mm_struct pointer as
    first argument.  The free functions do not get the mm_struct argument.  This
    is 1) asymmetrical and 2) to do mm related page table allocations the mm
    argument is needed on the free function as well.
    
    [kamalesh@linux.vnet.ibm.com: i386 fix]
    [akpm@linux-foundation.org: coding-syle fixes]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6caf4f23206b..1160f87ba700 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -325,7 +325,7 @@ static inline int mm_alloc_pgd(struct mm_struct * mm)
 
 static inline void mm_free_pgd(struct mm_struct * mm)
 {
-	pgd_free(mm->pgd);
+	pgd_free(mm, mm->pgd);
 }
 #else
 #define dup_mmap(mm, oldmm)	(0)

commit bdff746a3915f109bd13730b6847e33e17e91ed3
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Mon Feb 4 22:27:22 2008 -0800

    clone: prepare to recycle CLONE_STOPPED
    
    Ulrich says that we never used this clone flags and that nothing should be
    using it.
    
    As we're down to only a single bit left in clone's flags argument, let's add a
    warning to check that no userspace is actually using it.  Hopefully we will
    be able to recycle it.
    
    Roland said:
    
      CLONE_STOPPED was previously used by some NTPL versions when under
      thread_db (i.e.  only when being actively debugged by gdb), but not for a
      long time now, and it never worked reliably when it was used.  Removing it
      seems fine to me.
    
    [akpm@linux-foundation.org: it looks like CLONE_DETACHED is being used]
    Cc: Ulrich Drepper <drepper@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 05e0b6f4365b..6caf4f23206b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1450,6 +1450,23 @@ long do_fork(unsigned long clone_flags,
 	int trace = 0;
 	long nr;
 
+	/*
+	 * We hope to recycle these flags after 2.6.26
+	 */
+	if (unlikely(clone_flags & CLONE_STOPPED)) {
+		static int __read_mostly count = 100;
+
+		if (count > 0 && printk_ratelimit()) {
+			char comm[TASK_COMM_LEN];
+
+			count--;
+			printk(KERN_INFO "fork(): process `%s' used deprecated "
+					"clone flags 0x%lx\n",
+				get_task_comm(comm, current),
+				clone_flags & CLONE_STOPPED);
+		}
+	}
+
 	if (unlikely(current->ptrace)) {
 		trace = fork_traceflag (clone_flags);
 		if (trace)

commit 6d4e4c4fca5be806b888d606894d914847e82d78
Author: Avi Kivity <avi@qumranet.com>
Date:   Wed Nov 21 16:41:05 2007 +0200

    KVM: Disallow fork() and similar games when using a VM
    
    We don't want the meaning of guest userspace changing under our feet.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 314f5101d2b0..05e0b6f4365b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -393,6 +393,7 @@ void fastcall __mmdrop(struct mm_struct *mm)
 	destroy_context(mm);
 	free_mm(mm);
 }
+EXPORT_SYMBOL_GPL(__mmdrop);
 
 /*
  * Decrement the use count and release all resources for an mm.

commit fadad878cc0640cc9cd5569998bf54b693f7b38b
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:54:47 2008 +0100

    kernel: add CLONE_IO to specifically request sharing of IO contexts
    
    syslets (or other threads/processes that want io context sharing) can
    set this to enforce sharing of io context.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1987c57abb08..314f5101d2b0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -792,15 +792,21 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 	return error;
 }
 
-static int copy_io(struct task_struct *tsk)
+static int copy_io(unsigned long clone_flags, struct task_struct *tsk)
 {
 #ifdef CONFIG_BLOCK
 	struct io_context *ioc = current->io_context;
 
 	if (!ioc)
 		return 0;
-
-	if (ioprio_valid(ioc->ioprio)) {
+	/*
+	 * Share io context with parent, if CLONE_IO is set
+	 */
+	if (clone_flags & CLONE_IO) {
+		tsk->io_context = ioc_task_link(ioc);
+		if (unlikely(!tsk->io_context))
+			return -ENOMEM;
+	} else if (ioprio_valid(ioc->ioprio)) {
 		tsk->io_context = alloc_io_context(GFP_KERNEL, -1);
 		if (unlikely(!tsk->io_context))
 			return -ENOMEM;
@@ -1176,7 +1182,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_mm;
 	if ((retval = copy_namespaces(clone_flags, p)))
 		goto bad_fork_cleanup_keys;
-	if ((retval = copy_io(p)))
+	if ((retval = copy_io(clone_flags, p)))
 		goto bad_fork_cleanup_namespaces;
 	retval = copy_thread(0, clone_flags, stack_start, stack_size, p, regs);
 	if (retval)

commit d38ecf935fcb10264a6bc190855d9595165e6eeb
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:53:35 2008 +0100

    io context sharing: preliminary support
    
    Detach task state from ioc, instead keep track of how many processes
    are accessing the ioc.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2a86c9dff744..1987c57abb08 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -805,7 +805,6 @@ static int copy_io(struct task_struct *tsk)
 		if (unlikely(!tsk->io_context))
 			return -ENOMEM;
 
-		tsk->io_context->task = tsk;
 		tsk->io_context->ioprio = ioc->ioprio;
 	}
 #endif

commit fd0928df98b9578be8a786ac0cb78a47a5e17a20
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Jan 24 08:52:45 2008 +0100

    ioprio: move io priority from task_struct to io_context
    
    This is where it belongs and then it doesn't take up space for a
    process that doesn't do IO.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 39d22b3357de..2a86c9dff744 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -51,6 +51,7 @@
 #include <linux/random.h>
 #include <linux/tty.h>
 #include <linux/proc_fs.h>
+#include <linux/blkdev.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -791,6 +792,26 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 	return error;
 }
 
+static int copy_io(struct task_struct *tsk)
+{
+#ifdef CONFIG_BLOCK
+	struct io_context *ioc = current->io_context;
+
+	if (!ioc)
+		return 0;
+
+	if (ioprio_valid(ioc->ioprio)) {
+		tsk->io_context = alloc_io_context(GFP_KERNEL, -1);
+		if (unlikely(!tsk->io_context))
+			return -ENOMEM;
+
+		tsk->io_context->task = tsk;
+		tsk->io_context->ioprio = ioc->ioprio;
+	}
+#endif
+	return 0;
+}
+
 /*
  *	Helper to unshare the files of the current task.
  *	We don't want to expose copy_files internals to
@@ -1156,15 +1177,17 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_mm;
 	if ((retval = copy_namespaces(clone_flags, p)))
 		goto bad_fork_cleanup_keys;
+	if ((retval = copy_io(p)))
+		goto bad_fork_cleanup_namespaces;
 	retval = copy_thread(0, clone_flags, stack_start, stack_size, p, regs);
 	if (retval)
-		goto bad_fork_cleanup_namespaces;
+		goto bad_fork_cleanup_io;
 
 	if (pid != &init_struct_pid) {
 		retval = -ENOMEM;
 		pid = alloc_pid(task_active_pid_ns(p));
 		if (!pid)
-			goto bad_fork_cleanup_namespaces;
+			goto bad_fork_cleanup_io;
 
 		if (clone_flags & CLONE_NEWPID) {
 			retval = pid_ns_prepare_proc(task_active_pid_ns(p));
@@ -1234,9 +1257,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	/* Need tasklist lock for parent etc handling! */
 	write_lock_irq(&tasklist_lock);
 
-	/* for sys_ioprio_set(IOPRIO_WHO_PGRP) */
-	p->ioprio = current->ioprio;
-
 	/*
 	 * The task hasn't been attached yet, so its cpus_allowed mask will
 	 * not be changed, nor will its assigned CPU.
@@ -1328,6 +1348,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_free_pid:
 	if (pid != &init_struct_pid)
 		free_pid(pid);
+bad_fork_cleanup_io:
+	put_io_context(p->io_context);
 bad_fork_cleanup_namespaces:
 	exit_task_namespaces(p);
 bad_fork_cleanup_keys:

commit 9745512ce79de686df354dc70a8d1a74d801892d
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Fri Jan 25 21:08:34 2008 +0100

    sched: latencytop support
    
    LatencyTOP kernel infrastructure; it measures latencies in the
    scheduler and tracks it system wide and per process.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0c969f4fade0..39d22b3357de 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1205,6 +1205,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #ifdef TIF_SYSCALL_EMU
 	clear_tsk_thread_flag(p, TIF_SYSCALL_EMU);
 #endif
+	clear_all_latency_tracing(p);
 
 	/* Our parent execution domain becomes current domain
 	   These must match for thread signalling to apply */

commit 6f505b16425a51270058e4a93441fe64de3dd435
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jan 25 21:08:30 2008 +0100

    sched: rt group scheduling
    
    Extend group scheduling to also cover the realtime classes. It uses the time
    limiting introduced by the previous patch to allow multiple realtime groups.
    
    The hard time limit is required to keep behaviour deterministic.
    
    The algorithms used make the realtime scheduler O(tg), linear scaling wrt the
    number of task groups. This is the worst case behaviour I can't seem to get out
    of, the avg. case of the algorithms can be improved, I focused on correctness
    and worst case.
    
    [ akpm@linux-foundation.org: move side-effects out of BUG_ON(). ]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9f8ef32cbc7a..0c969f4fade0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1246,7 +1246,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * parent's CPU). This avoids alot of nasty races.
 	 */
 	p->cpus_allowed = current->cpus_allowed;
-	p->nr_cpus_allowed = current->nr_cpus_allowed;
+	p->rt.nr_cpus_allowed = current->rt.nr_cpus_allowed;
 	if (unlikely(!cpu_isset(task_cpu(p), p->cpus_allowed) ||
 			!cpu_online(task_cpu(p))))
 		set_task_cpu(p, smp_processor_id());

commit e260be673a15b6125068270e0216a3bfbfc12f87
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jan 25 21:08:24 2008 +0100

    Preempt-RCU: implementation
    
    This patch implements a new version of RCU which allows its read-side
    critical sections to be preempted. It uses a set of counter pairs
    to keep track of the read-side critical sections and flips them
    when all tasks exit read-side critical section. The details
    of this implementation can be found in this paper -
    
            http://www.rdrop.com/users/paulmck/RCU/OLSrtRCU.2006.08.11a.pdf
    
    and the article-
    
            http://lwn.net/Articles/253651/
    
    This patch was developed as a part of the -rt kernel development and
    meant to provide better latencies when read-side critical sections of
    RCU don't disable preemption.  As a consequence of keeping track of RCU
    readers, the readers have a slight overhead (optimizations in the paper).
    This implementation co-exists with the "classic" RCU implementations
    and can be switched to at compiler.
    
    Also includes RCU tracing summarized in debugfs.
    
    [ akpm@linux-foundation.org: build fixes on non-preempt architectures ]
    
    Signed-off-by: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Paul E. McKenney <paulmck@us.ibm.com>
    Reviewed-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 930c51865ab4..9f8ef32cbc7a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1045,6 +1045,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	copy_flags(clone_flags, p);
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);
+#ifdef CONFIG_PREEMPT_RCU
+	p->rcu_read_lock_nesting = 0;
+	p->rcu_flipctr_idx = 0;
+#endif /* #ifdef CONFIG_PREEMPT_RCU */
 	p->vfork_done = NULL;
 	spin_lock_init(&p->alloc_lock);
 

commit 73fe6aae84400e2b475e2a1dc4e8592cd3ed6e69
Author: Gregory Haskins <ghaskins@novell.com>
Date:   Fri Jan 25 21:08:07 2008 +0100

    sched: add RT-balance cpu-weight
    
    Some RT tasks (particularly kthreads) are bound to one specific CPU.
    It is fairly common for two or more bound tasks to get queued up at the
    same time.  Consider, for instance, softirq_timer and softirq_sched.  A
    timer goes off in an ISR which schedules softirq_thread to run at RT50.
    Then the timer handler determines that it's time to smp-rebalance the
    system so it schedules softirq_sched to run.  So we are in a situation
    where we have two RT50 tasks queued, and the system will go into
    rt-overload condition to request other CPUs for help.
    
    This causes two problems in the current code:
    
    1) If a high-priority bound task and a low-priority unbounded task queue
       up behind the running task, we will fail to ever relocate the unbounded
       task because we terminate the search on the first unmovable task.
    
    2) We spend precious futile cycles in the fast-path trying to pull
       overloaded tasks over.  It is therefore optimial to strive to avoid the
       overhead all together if we can cheaply detect the condition before
       overload even occurs.
    
    This patch tries to achieve this optimization by utilizing the hamming
    weight of the task->cpus_allowed mask.  A weight of 1 indicates that
    the task cannot be migrated.  We will then utilize this information to
    skip non-migratable tasks and to eliminate uncessary rebalance attempts.
    
    We introduce a per-rq variable to count the number of migratable tasks
    that are currently running.  We only go into overload if we have more
    than one rt task, AND at least one of them is migratable.
    
    In addition, we introduce a per-task variable to cache the cpus_allowed
    weight, since the hamming calculation is probably relatively expensive.
    We only update the cached value when the mask is updated which should be
    relatively infrequent, especially compared to scheduling frequency
    in the fast path.
    
    Signed-off-by: Gregory Haskins <ghaskins@novell.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 09c0b90a69cc..930c51865ab4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1242,6 +1242,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * parent's CPU). This avoids alot of nasty races.
 	 */
 	p->cpus_allowed = current->cpus_allowed;
+	p->nr_cpus_allowed = current->nr_cpus_allowed;
 	if (unlikely(!cpu_isset(task_cpu(p), p->cpus_allowed) ||
 			!cpu_online(task_cpu(p))))
 		set_task_cpu(p, smp_processor_id());

commit 82a1fcb90287052aabfa235e7ffc693ea003fe69
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 25 21:08:02 2008 +0100

    softlockup: automatically detect hung TASK_UNINTERRUPTIBLE tasks
    
    this patch extends the soft-lockup detector to automatically
    detect hung TASK_UNINTERRUPTIBLE tasks. Such hung tasks are
    printed the following way:
    
     ------------------>
     INFO: task prctl:3042 blocked for more than 120 seconds.
     "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message
     prctl         D fd5e3793     0  3042   2997
            f6050f38 00000046 00000001 fd5e3793 00000009 c06d8264 c06dae80 00000286
            f6050f40 f6050f00 f7d34d90 f7d34fc8 c1e1be80 00000001 f6050000 00000000
            f7e92d00 00000286 f6050f18 c0489d1a f6050f40 00006605 00000000 c0133a5b
     Call Trace:
      [<c04883a5>] schedule_timeout+0x6d/0x8b
      [<c04883d8>] schedule_timeout_uninterruptible+0x15/0x17
      [<c0133a76>] msleep+0x10/0x16
      [<c0138974>] sys_prctl+0x30/0x1e2
      [<c0104c52>] sysenter_past_esp+0x5f/0xa5
      =======================
     2 locks held by prctl/3042:
     #0:  (&sb->s_type->i_mutex_key#5){--..}, at: [<c0197d11>] do_fsync+0x38/0x7a
     #1:  (jbd_handle){--..}, at: [<c01ca3d2>] journal_start+0xc7/0xe9
     <------------------
    
    the current default timeout is 120 seconds. Such messages are printed
    up to 10 times per bootup. If the system has crashed already then the
    messages are not printed.
    
    if lockdep is enabled then all held locks are printed as well.
    
    this feature is a natural extension to the softlockup-detector (kernel
    locked up without scheduling) and to the NMI watchdog (kernel locked up
    with IRQs disabled).
    
    [ Gautham R Shenoy <ego@in.ibm.com>: CPU hotplug fixes. ]
    [ Andrew Morton <akpm@linux-foundation.org>: build warning fix. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8dd8ff281009..09c0b90a69cc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1059,6 +1059,11 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->prev_utime = cputime_zero;
 	p->prev_stime = cputime_zero;
 
+#ifdef CONFIG_DETECT_SOFTLOCKUP
+	p->last_switch_count = 0;
+	p->last_switch_timestamp = 0;
+#endif
+
 #ifdef CONFIG_TASK_XACCT
 	p->rchar = 0;		/* I/O counter: bytes read */
 	p->wchar = 0;		/* I/O counter: bytes written */

commit 5cd17569fd0eeca510735e63a6061291e3971bf6
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Dec 4 23:45:04 2007 -0800

    fix clone(CLONE_NEWPID)
    
    Currently we are complicating the code in copy_process, the clone ABI, and
    if we fix the bugs sys_setsid itself, with an unnecessary open coded
    version of sys_setsid.
    
    So just simplify everything and don't special case the session and pgrp of
    the initial process in a pid namespace.
    
    Having this special case actually presents to user space the classic linux
    startup conditions with session == pgrp == 0 for /sbin/init.
    
    We already handle sending signals to processes in a child pid namespace.
    
    We need to handle sending signals to processes in a parent pid namespace
    for cases like SIGCHILD and SIGIO.
    
    This makes nothing extra visible inside a pid namespace.  So this extra
    special case appears to have no redeeming merits.
    
    Further removing this special case increases the flexibility of how we can
    use pid namespaces, by not requiring the initial process in a pid namespace
    to be a daemon.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8ca1a14cdc8c..8dd8ff281009 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1292,23 +1292,14 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			__ptrace_link(p, current->parent);
 
 		if (thread_group_leader(p)) {
-			if (clone_flags & CLONE_NEWPID) {
+			if (clone_flags & CLONE_NEWPID)
 				p->nsproxy->pid_ns->child_reaper = p;
-				p->signal->tty = NULL;
-				set_task_pgrp(p, p->pid);
-				set_task_session(p, p->pid);
-				attach_pid(p, PIDTYPE_PGID, pid);
-				attach_pid(p, PIDTYPE_SID, pid);
-			} else {
-				p->signal->tty = current->signal->tty;
-				set_task_pgrp(p, task_pgrp_nr(current));
-				set_task_session(p, task_session_nr(current));
-				attach_pid(p, PIDTYPE_PGID,
-						task_pgrp(current));
-				attach_pid(p, PIDTYPE_SID,
-						task_session(current));
-			}
 
+			p->signal->tty = current->signal->tty;
+			set_task_pgrp(p, task_pgrp_nr(current));
+			set_task_session(p, task_session_nr(current));
+			attach_pid(p, PIDTYPE_PGID, task_pgrp(current));
+			attach_pid(p, PIDTYPE_SID, task_session(current));
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;
 		}

commit 3c90e6e99b08f01d5684a3a07cceae6a543e4fa8
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Fri Nov 9 22:39:39 2007 +0100

    sched: fix copy_namespace() <-> sched_fork() dependency in do_fork
    
    Sukadev Bhattiprolu reported a kernel crash with control groups.
    There are couple of problems discovered by Suka's test:
    
    - The test requires the cgroup filesystem to be mounted with
      atleast the cpu and ns options (i.e both namespace and cpu
      controllers are active in the same hierarchy).
    
            # mkdir /dev/cpuctl
            # mount -t cgroup -ocpu,ns none cpuctl
            (or simply)
            # mount -t cgroup none cpuctl -> Will activate all controllers
                                             in same hierarchy.
    
    - The test invokes clone() with CLONE_NEWNS set. This causes a a new child
      to be created, also a new group (do_fork->copy_namespaces->ns_cgroup_clone->
      cgroup_clone) and the child is attached to the new group (cgroup_clone->
      attach_task->sched_move_task). At this point in time, the child's scheduler
      related fields are uninitialized (including its on_rq field, which it has
      inherited from parent). As a result sched_move_task thinks its on
      runqueue, when it isn't.
    
      As a solution to this problem, I moved sched_fork() call, which
      initializes scheduler related fields on a new task, before
      copy_namespaces(). I am not sure though whether moving up will
      cause other side-effects. Do you see any issue?
    
    - The second problem exposed by this test is that task_new_fair()
      assumes that parent and child will be part of the same group (which
      needn't be as this test shows). As a result, cfs_rq->curr can be NULL
      for the child.
    
      The solution is to test for curr pointer being NULL in
      task_new_fair().
    
    With the patch below, I could run ns_exec() fine w/o a crash.
    
    Reported-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 28a740151988..8ca1a14cdc8c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1123,6 +1123,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->blocked_on = NULL; /* not blocked yet */
 #endif
 
+	/* Perform scheduler related setup. Assign this task to a CPU. */
+	sched_fork(p, clone_flags);
+
 	if ((retval = security_task_alloc(p)))
 		goto bad_fork_cleanup_policy;
 	if ((retval = audit_alloc(p)))
@@ -1212,9 +1215,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	INIT_LIST_HEAD(&p->ptrace_children);
 	INIT_LIST_HEAD(&p->ptrace_list);
 
-	/* Perform scheduler related setup. Assign this task to a CPU. */
-	sched_fork(p, clone_flags);
-
 	/* Now that the task is set up, run cgroup callbacks if
 	 * necessary. We need to run them before the task is visible
 	 * on the tasklist. */

commit 9301899be75b464ef097f0b5af7af6d9bd8f68a7
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Tue Oct 30 00:26:32 2007 +0100

    sched: fix /proc/<PID>/stat stime/utime monotonicity, part 2
    
    Extend Peter's patch to fix accounting issues, by keeping stime
    monotonic too.
    
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Tested-by: Frans Pop <elendil@planet.nl>

diff --git a/kernel/fork.c b/kernel/fork.c
index a65bfc47177c..28a740151988 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1057,6 +1057,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->utimescaled = cputime_zero;
 	p->stimescaled = cputime_zero;
 	p->prev_utime = cputime_zero;
+	p->prev_stime = cputime_zero;
 
 #ifdef CONFIG_TASK_XACCT
 	p->rchar = 0;		/* I/O counter: bytes read */

commit 73a2bcb0edb9ffb0b007b3546b430e2c6e415eee
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Oct 29 21:18:11 2007 +0100

    sched: keep utime/stime monotonic
    
    keep utime/stime monotonic.
    
    cpustats use utime/stime as a ratio against sum_exec_runtime, as a
    consequence it can happen - when the ratio changes faster than time
    accumulates - that either can be appear to go backwards.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index ddafdfac9456..a65bfc47177c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1056,6 +1056,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->gtime = cputime_zero;
 	p->utimescaled = cputime_zero;
 	p->stimescaled = cputime_zero;
+	p->prev_utime = cputime_zero;
 
 #ifdef CONFIG_TASK_XACCT
 	p->rchar = 0;		/* I/O counter: bytes read */

commit a39bc51691a0c8880b7d10fa7c2f034f3ba9a037
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Thu Oct 18 23:41:10 2007 -0700

    Uninline fork.c/exit.c
    
    Save ~650 bytes here.
    
    add/remove: 4/0 grow/shrink: 0/7 up/down: 430/-1088 (-658)
    function                                     old     new   delta
    __copy_fs_struct                               -     202    +202
    __put_fs_struct                                -     112    +112
    __exit_fs                                      -      58     +58
    __exit_files                                   -      58     +58
    exit_files                                    58       2     -56
    put_fs_struct                                112       5    -107
    exit_fs                                      161       2    -159
    sys_unshare                                  774     590    -184
    copy_process                                4031    3840    -191
    do_exit                                     1791    1597    -194
    copy_fs_struct                               202       5    -197
    
    No difference in lmbench lat_proc tests on 2-way Opteron 246.
    Smaaaal degradation on UP P4 (within errors).
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 43958d41e65a..ddafdfac9456 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -206,7 +206,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 }
 
 #ifdef CONFIG_MMU
-static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
+static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 {
 	struct vm_area_struct *mpnt, *tmp, **pprev;
 	struct rb_node **rb_link, *rb_parent;
@@ -584,7 +584,7 @@ static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
 	return retval;
 }
 
-static inline struct fs_struct *__copy_fs_struct(struct fs_struct *old)
+static struct fs_struct *__copy_fs_struct(struct fs_struct *old)
 {
 	struct fs_struct *fs = kmem_cache_alloc(fs_cachep, GFP_KERNEL);
 	/* We don't need to lock fs - think why ;-) */
@@ -616,7 +616,7 @@ struct fs_struct *copy_fs_struct(struct fs_struct *old)
 
 EXPORT_SYMBOL_GPL(copy_fs_struct);
 
-static inline int copy_fs(unsigned long clone_flags, struct task_struct * tsk)
+static int copy_fs(unsigned long clone_flags, struct task_struct *tsk)
 {
 	if (clone_flags & CLONE_FS) {
 		atomic_inc(&current->fs->count);
@@ -819,7 +819,7 @@ int unshare_files(void)
 
 EXPORT_SYMBOL(unshare_files);
 
-static inline int copy_sighand(unsigned long clone_flags, struct task_struct * tsk)
+static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct sighand_struct *sig;
 
@@ -842,7 +842,7 @@ void __cleanup_sighand(struct sighand_struct *sighand)
 		kmem_cache_free(sighand_cachep, sighand);
 }
 
-static inline int copy_signal(unsigned long clone_flags, struct task_struct * tsk)
+static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
 {
 	struct signal_struct *sig;
 	int ret;
@@ -924,7 +924,7 @@ void __cleanup_signal(struct signal_struct *sig)
 	kmem_cache_free(signal_cachep, sig);
 }
 
-static inline void cleanup_signal(struct task_struct *tsk)
+static void cleanup_signal(struct task_struct *tsk)
 {
 	struct signal_struct *sig = tsk->signal;
 
@@ -934,7 +934,7 @@ static inline void cleanup_signal(struct task_struct *tsk)
 		__cleanup_signal(sig);
 }
 
-static inline void copy_flags(unsigned long clone_flags, struct task_struct *p)
+static void copy_flags(unsigned long clone_flags, struct task_struct *p)
 {
 	unsigned long new_flags = p->flags;
 
@@ -953,7 +953,7 @@ asmlinkage long sys_set_tid_address(int __user *tidptr)
 	return task_pid_vnr(current);
 }
 
-static inline void rt_mutex_init_task(struct task_struct *p)
+static void rt_mutex_init_task(struct task_struct *p)
 {
 	spin_lock_init(&p->pi_lock);
 #ifdef CONFIG_RT_MUTEXES
@@ -1385,7 +1385,7 @@ struct task_struct * __cpuinit fork_idle(int cpu)
 	return task;
 }
 
-static inline int fork_traceflag (unsigned clone_flags)
+static int fork_traceflag(unsigned clone_flags)
 {
 	if (clone_flags & CLONE_UNTRACED)
 		return 0;
@@ -1521,7 +1521,7 @@ void __init proc_caches_init(void)
  * Check constraints on flags passed to the unshare system call and
  * force unsharing of additional process context as appropriate.
  */
-static inline void check_unshare_flags(unsigned long *flags_ptr)
+static void check_unshare_flags(unsigned long *flags_ptr)
 {
 	/*
 	 * If unsharing a thread from a thread group, must also

commit a24efe62dd165be7d03431cf936ae17d345fed69
Author: Mariusz Kozlowski <m.kozlowski@tuxland.pl>
Date:   Thu Oct 18 23:41:09 2007 -0700

    kernel/fork.c: remove unneeded variable initialization in copy_process()
    
    This initialization of is not needed so just remove it.
    
    Signed-off-by: Mariusz Kozlowski <m.kozlowski@tuxland.pl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9d40367dd5db..43958d41e65a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -978,7 +978,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 					struct pid *pid)
 {
 	int retval;
-	struct task_struct *p = NULL;
+	struct task_struct *p;
 	int cgroup_callbacks_done = 0;
 
 	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))

commit 9a2e70572e94e21e7ec4186702d045415422bda0
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:39 2007 -0700

    Isolate the explicit usage of signal->pgrp
    
    The pgrp field is not used widely around the kernel so it is now marked as
    deprecated with appropriate comment.
    
    The initialization of INIT_SIGNALS is trimmed because
    a) they are set to 0 automatically;
    b) gcc cannot properly initialize two anonymous (the second one
       is the one with the session) unions. In this particular case
       to make it compile we'd have to add some field initialized
       right before the .pgrp.
    
    This is the same patch as the 1ec320afdc9552c92191d5f89fcd1ebe588334ca one
    (from Cedric), but for the pgrp field.
    
    Some progress report:
    
    We have to deprecate the pid, tgid, session and pgrp fields on struct
    task_struct and struct signal_struct.  The session and pgrp are already
    deprecated.  The tgid value is close to being such - the worst known usage
    in in fs/locks.c and audit code.  The pid field deprecation is mainly
    blocked by numerous printk-s around the kernel that print the tsk->pid to
    log.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 240aa6601f5b..9d40367dd5db 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1293,13 +1293,13 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			if (clone_flags & CLONE_NEWPID) {
 				p->nsproxy->pid_ns->child_reaper = p;
 				p->signal->tty = NULL;
-				p->signal->pgrp = p->pid;
+				set_task_pgrp(p, p->pid);
 				set_task_session(p, p->pid);
 				attach_pid(p, PIDTYPE_PGID, pid);
 				attach_pid(p, PIDTYPE_SID, pid);
 			} else {
 				p->signal->tty = current->signal->tty;
-				p->signal->pgrp = task_pgrp_nr(current);
+				set_task_pgrp(p, task_pgrp_nr(current));
 				set_task_session(p, task_session_nr(current));
 				attach_pid(p, PIDTYPE_PGID,
 						task_pgrp(current));

commit 270f722d4d5f94b02fd48eed47e57917ab00a858
Author: Eugene Teo <eugeneteo@kernel.sg>
Date:   Thu Oct 18 23:40:38 2007 -0700

    Fix tsk->exit_state usage
    
    tsk->exit_state can only be 0, EXIT_ZOMBIE, or EXIT_DEAD.  A non-zero test
    is the same as tsk->exit_state & (EXIT_ZOMBIE | EXIT_DEAD), so just testing
    tsk->exit_state is sufficient.
    
    Signed-off-by: Eugene Teo <eugeneteo@kernel.sg>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a794bfcf6003..240aa6601f5b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -117,7 +117,7 @@ EXPORT_SYMBOL(free_task);
 
 void __put_task_struct(struct task_struct *tsk)
 {
-	WARN_ON(!(tsk->exit_state & (EXIT_DEAD | EXIT_ZOMBIE)));
+	WARN_ON(!tsk->exit_state);
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);
 

commit b488893a390edfe027bae7a46e9af8083e740668
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:14 2007 -0700

    pid namespaces: changes to show virtual ids to user
    
    This is the largest patch in the set. Make all (I hope) the places where
    the pid is shown to or get from user operate on the virtual pids.
    
    The idea is:
     - all in-kernel data structures must store either struct pid itself
       or the pid's global nr, obtained with pid_nr() call;
     - when seeking the task from kernel code with the stored id one
       should use find_task_by_pid() call that works with global pids;
     - when showing pid's numerical value to the user the virtual one
       should be used, but however when one shows task's pid outside this
       task's namespace the global one is to be used;
     - when getting the pid from userspace one need to consider this as
       the virtual one and use appropriate task/pid-searching functions.
    
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: nuther build fix]
    [akpm@linux-foundation.org: yet nuther build fix]
    [akpm@linux-foundation.org: remove unneeded casts]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Alexey Dobriyan <adobriyan@openvz.org>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ce9297e4e7d4..a794bfcf6003 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -950,7 +950,7 @@ asmlinkage long sys_set_tid_address(int __user *tidptr)
 {
 	current->clear_child_tid = tidptr;
 
-	return current->pid;
+	return task_pid_vnr(current);
 }
 
 static inline void rt_mutex_init_task(struct task_struct *p)

commit 6f4e643353aea52d80f33960bd88954a7c074f0f
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:11 2007 -0700

    pid namespaces: initialize the namespace's proc_mnt
    
    The namespace's proc_mnt must be kern_mount-ed to make this pointer always
    valid, independently of whether the user space mounted the proc or not.  This
    solves raced in proc_flush_task, etc.  with the proc_mnt switching from NULL
    to not-NULL.
    
    The initialization is done after the init's pid is created and hashed to make
    proc_get_sb() finr it and get for root inode.
    
    Sice the namespace holds the vfsmnt, vfsmnt holds the superblock and the
    superblock holds the namespace we must explicitly break this circle to destroy
    all the stuff.  This is done after the init of the namespace dies.  Running a
    few steps forward - when init exits it will kill all its children, so no
    proc_mnt will be needed after its death.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f252784f9330..ce9297e4e7d4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -50,6 +50,7 @@
 #include <linux/taskstats_kern.h>
 #include <linux/random.h>
 #include <linux/tty.h>
+#include <linux/proc_fs.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -1150,6 +1151,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		pid = alloc_pid(task_active_pid_ns(p));
 		if (!pid)
 			goto bad_fork_cleanup_namespaces;
+
+		if (clone_flags & CLONE_NEWPID) {
+			retval = pid_ns_prepare_proc(task_active_pid_ns(p));
+			if (retval < 0)
+				goto bad_fork_free_pid;
+		}
 	}
 
 	p->pid = pid_nr(pid);

commit 30e49c263e36341b60b735cbef5ca37912549264
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:10 2007 -0700

    pid namespaces: allow cloning of new namespace
    
    When clone() is invoked with CLONE_NEWPID, create a new pid namespace and then
    create a new struct pid for the new process.  Allocate pid_t's for the new
    process in the new pid namespace and all ancestor pid namespaces.  Make the
    newly cloned process the session and process group leader.
    
    Since the active pid namespace is special and expected to be the first entry
    in pid->upid_list, preserve the order of pid namespaces.
    
    The size of 'struct pid' is dependent on the the number of pid namespaces the
    process exists in, so we use multiple pid-caches'.  Only one pid cache is
    created during system startup and this used by processes that exist only in
    init_pid_ns.
    
    When a process clones its pid namespace, we create additional pid caches as
    necessary and use the pid cache to allocate 'struct pids' for that depth.
    
    Note, that with this patch the newly created namespace won't work, since the
    rest of the kernel still uses global pids, but this is to be fixed soon.  Init
    pid namespace still works.
    
    [oleg@tv-sign.ru: merge fix]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index bab34192799b..f252784f9330 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -973,7 +973,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 					unsigned long stack_start,
 					struct pt_regs *regs,
 					unsigned long stack_size,
-					int __user *parent_tidptr,
 					int __user *child_tidptr,
 					struct pid *pid)
 {
@@ -1043,11 +1042,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->did_exec = 0;
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	copy_flags(clone_flags, p);
-	retval = -EFAULT;
-	if (clone_flags & CLONE_PARENT_SETTID)
-		if (put_user(p->pid, parent_tidptr))
-			goto bad_fork_cleanup_delays_binfmt;
-
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);
 	p->vfork_done = NULL;
@@ -1289,11 +1283,22 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			__ptrace_link(p, current->parent);
 
 		if (thread_group_leader(p)) {
-			p->signal->tty = current->signal->tty;
-			p->signal->pgrp = task_pgrp_nr(current);
-			set_task_session(p, task_session_nr(current));
-			attach_pid(p, PIDTYPE_PGID, task_pgrp(current));
-			attach_pid(p, PIDTYPE_SID, task_session(current));
+			if (clone_flags & CLONE_NEWPID) {
+				p->nsproxy->pid_ns->child_reaper = p;
+				p->signal->tty = NULL;
+				p->signal->pgrp = p->pid;
+				set_task_session(p, p->pid);
+				attach_pid(p, PIDTYPE_PGID, pid);
+				attach_pid(p, PIDTYPE_SID, pid);
+			} else {
+				p->signal->tty = current->signal->tty;
+				p->signal->pgrp = task_pgrp_nr(current);
+				set_task_session(p, task_session_nr(current));
+				attach_pid(p, PIDTYPE_PGID,
+						task_pgrp(current));
+				attach_pid(p, PIDTYPE_SID,
+						task_session(current));
+			}
 
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;
@@ -1339,7 +1344,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_cgroup:
 #endif
 	cgroup_exit(p, cgroup_callbacks_done);
-bad_fork_cleanup_delays_binfmt:
 	delayacct_tsk_free(p);
 	if (p->binfmt)
 		module_put(p->binfmt->module);
@@ -1366,7 +1370,7 @@ struct task_struct * __cpuinit fork_idle(int cpu)
 	struct task_struct *task;
 	struct pt_regs regs;
 
-	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL, NULL,
+	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL,
 				&init_struct_pid);
 	if (!IS_ERR(task))
 		init_idle(task, cpu);
@@ -1414,7 +1418,7 @@ long do_fork(unsigned long clone_flags,
 	}
 
 	p = copy_process(clone_flags, stack_start, regs, stack_size,
-			parent_tidptr, child_tidptr, NULL);
+			child_tidptr, NULL);
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.
@@ -1422,7 +1426,16 @@ long do_fork(unsigned long clone_flags,
 	if (!IS_ERR(p)) {
 		struct completion vfork;
 
-		nr = pid_nr(task_pid(p));
+		/*
+		 * this is enough to call pid_nr_ns here, but this if
+		 * improves optimisation of regular fork()
+		 */
+		nr = (clone_flags & CLONE_NEWPID) ?
+			task_pid_nr_ns(p, current->nsproxy->pid_ns) :
+				task_pid_vnr(p);
+
+		if (clone_flags & CLONE_PARENT_SETTID)
+			put_user(nr, parent_tidptr);
 
 		if (clone_flags & CLONE_VFORK) {
 			p->vfork_done = &vfork;

commit 425fb2b4bf5dde24be4a82e9a2c344bb49ac92e4
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:07 2007 -0700

    pid namespaces: move alloc_pid() lower in copy_process()
    
    When we create new namespace we will need to allocate the struct pid, that
    will have one extra struct upid in array, comparing to the parent.
    
    Thus we need to know the new namespace (if any) in alloc_pid() to init this
    struct upid properly, so move the alloc_pid() call lower in copy_process().
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 984d259e172d..bab34192799b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1040,16 +1040,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (p->binfmt && !try_module_get(p->binfmt->module))
 		goto bad_fork_cleanup_put_domain;
 
-	if (pid != &init_struct_pid) {
-		pid = alloc_pid(task_active_pid_ns(p));
-		if (!pid)
-			goto bad_fork_put_binfmt_module;
-	}
-
 	p->did_exec = 0;
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	copy_flags(clone_flags, p);
-	p->pid = pid_nr(pid);
 	retval = -EFAULT;
 	if (clone_flags & CLONE_PARENT_SETTID)
 		if (put_user(p->pid, parent_tidptr))
@@ -1133,10 +1126,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->blocked_on = NULL; /* not blocked yet */
 #endif
 
-	p->tgid = p->pid;
-	if (clone_flags & CLONE_THREAD)
-		p->tgid = current->tgid;
-
 	if ((retval = security_task_alloc(p)))
 		goto bad_fork_cleanup_policy;
 	if ((retval = audit_alloc(p)))
@@ -1162,6 +1151,18 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (retval)
 		goto bad_fork_cleanup_namespaces;
 
+	if (pid != &init_struct_pid) {
+		retval = -ENOMEM;
+		pid = alloc_pid(task_active_pid_ns(p));
+		if (!pid)
+			goto bad_fork_cleanup_namespaces;
+	}
+
+	p->pid = pid_nr(pid);
+	p->tgid = p->pid;
+	if (clone_flags & CLONE_THREAD)
+		p->tgid = current->tgid;
+
 	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
 	/*
 	 * Clear TID on mm_release()?
@@ -1259,7 +1260,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		spin_unlock(&current->sighand->siglock);
 		write_unlock_irq(&tasklist_lock);
 		retval = -ERESTARTNOINTR;
-		goto bad_fork_cleanup_namespaces;
+		goto bad_fork_free_pid;
 	}
 
 	if (clone_flags & CLONE_THREAD) {
@@ -1308,6 +1309,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	cgroup_post_fork(p);
 	return p;
 
+bad_fork_free_pid:
+	if (pid != &init_struct_pid)
+		free_pid(pid);
 bad_fork_cleanup_namespaces:
 	exit_task_namespaces(p);
 bad_fork_cleanup_keys:
@@ -1337,9 +1341,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	cgroup_exit(p, cgroup_callbacks_done);
 bad_fork_cleanup_delays_binfmt:
 	delayacct_tsk_free(p);
-	if (pid != &init_struct_pid)
-		free_pid(pid);
-bad_fork_put_binfmt_module:
 	if (p->binfmt)
 		module_put(p->binfmt->module);
 bad_fork_cleanup_put_domain:

commit 8ef047aaaeb811247a5639c92e2f2ae1221a28dd
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:05 2007 -0700

    pid namespaces: make alloc_pid(), free_pid() and put_pid() work with struct upid
    
    Each struct upid element of struct pid has to be initialized properly, i.e.
    its nr mst be allocated from appropriate pidmap and ns set to appropriate
    namespace.
    
    When allocating a new pid, we need to know the namespace this pid will live
    in, so the additional argument is added to alloc_pid().
    
    On the other hand, the rest of the kernel still uses the pid->nr and
    pid->pid_chain fields, so these ones are still initialized, but this will be
    removed soon.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d2f4a420a5b9..984d259e172d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1041,7 +1041,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_put_domain;
 
 	if (pid != &init_struct_pid) {
-		pid = alloc_pid();
+		pid = alloc_pid(task_active_pid_ns(p));
 		if (!pid)
 			goto bad_fork_put_binfmt_module;
 	}

commit cf7b708c8d1d7a27736771bcf4c457b332b0f818
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:39:54 2007 -0700

    Make access to task's nsproxy lighter
    
    When someone wants to deal with some other taks's namespaces it has to lock
    the task and then to get the desired namespace if the one exists.  This is
    slow on read-only paths and may be impossible in some cases.
    
    E.g.  Oleg recently noticed a race between unshare() and the (sent for
    review in cgroups) pid namespaces - when the task notifies the parent it
    has to know the parent's namespace, but taking the task_lock() is
    impossible there - the code is under write locked tasklist lock.
    
    On the other hand switching the namespace on task (daemonize) and releasing
    the namespace (after the last task exit) is rather rare operation and we
    can sacrifice its speed to solve the issues above.
    
    The access to other task namespaces is proposed to be performed
    like this:
    
         rcu_read_lock();
         nsproxy = task_nsproxy(tsk);
         if (nsproxy != NULL) {
                 / *
                   * work with the namespaces here
                   * e.g. get the reference on one of them
                   * /
         } / *
             * NULL task_nsproxy() means that this task is
             * almost dead (zombie)
             * /
         rcu_read_unlock();
    
    This patch has passed the review by Eric and Oleg :) and,
    of course, tested.
    
    [clg@fr.ibm.com: fix unshare()]
    [ebiederm@xmission.com: Update get_net_ns_by_pid]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2deaf481efab..d2f4a420a5b9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1632,7 +1632,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	struct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;
 	struct files_struct *fd, *new_fd = NULL;
 	struct sem_undo_list *new_ulist = NULL;
-	struct nsproxy *new_nsproxy = NULL, *old_nsproxy = NULL;
+	struct nsproxy *new_nsproxy = NULL;
 
 	check_unshare_flags(&unshare_flags);
 
@@ -1662,14 +1662,13 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 
 	if (new_fs ||  new_mm || new_fd || new_ulist || new_nsproxy) {
 
-		task_lock(current);
-
 		if (new_nsproxy) {
-			old_nsproxy = current->nsproxy;
-			current->nsproxy = new_nsproxy;
-			new_nsproxy = old_nsproxy;
+			switch_task_namespaces(current, new_nsproxy);
+			new_nsproxy = NULL;
 		}
 
+		task_lock(current);
+
 		if (new_fs) {
 			fs = current->fs;
 			current->fs = new_fs;

commit a6f5e06378970a2687332c2d54046245fcff1e7e
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Thu Oct 18 23:39:53 2007 -0700

    pid namespaces: move alloc_pid() to copy_process()
    
    Move alloc_pid() into copy_process().  This will keep all pid and pid
    namespace code together and simplify error handling when we support multiple
    pid namespaces.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Pavel Emelianov <xemul@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Herbert Poetzel <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fd65bca38a93..2deaf481efab 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1040,6 +1040,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (p->binfmt && !try_module_get(p->binfmt->module))
 		goto bad_fork_cleanup_put_domain;
 
+	if (pid != &init_struct_pid) {
+		pid = alloc_pid();
+		if (!pid)
+			goto bad_fork_put_binfmt_module;
+	}
+
 	p->did_exec = 0;
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	copy_flags(clone_flags, p);
@@ -1331,6 +1337,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	cgroup_exit(p, cgroup_callbacks_done);
 bad_fork_cleanup_delays_binfmt:
 	delayacct_tsk_free(p);
+	if (pid != &init_struct_pid)
+		free_pid(pid);
+bad_fork_put_binfmt_module:
 	if (p->binfmt)
 		module_put(p->binfmt->module);
 bad_fork_cleanup_put_domain:
@@ -1395,19 +1404,16 @@ long do_fork(unsigned long clone_flags,
 {
 	struct task_struct *p;
 	int trace = 0;
-	struct pid *pid = alloc_pid();
 	long nr;
 
-	if (!pid)
-		return -EAGAIN;
-	nr = pid->nr;
 	if (unlikely(current->ptrace)) {
 		trace = fork_traceflag (clone_flags);
 		if (trace)
 			clone_flags |= CLONE_PTRACE;
 	}
 
-	p = copy_process(clone_flags, stack_start, regs, stack_size, parent_tidptr, child_tidptr, pid);
+	p = copy_process(clone_flags, stack_start, regs, stack_size,
+			parent_tidptr, child_tidptr, NULL);
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.
@@ -1415,6 +1421,8 @@ long do_fork(unsigned long clone_flags,
 	if (!IS_ERR(p)) {
 		struct completion vfork;
 
+		nr = pid_nr(task_pid(p));
+
 		if (clone_flags & CLONE_VFORK) {
 			p->vfork_done = &vfork;
 			init_completion(&vfork);
@@ -1448,7 +1456,6 @@ long do_fork(unsigned long clone_flags,
 			}
 		}
 	} else {
-		free_pid(pid);
 		nr = PTR_ERR(p);
 	}
 	return nr;

commit a47afb0f9d794d525a372c8d69902147cc88222a
Author: Pavel Emelianov <xemul@openvz.org>
Date:   Thu Oct 18 23:39:46 2007 -0700

    pid namespaces: round up the API
    
    The set of functions process_session, task_session, process_group and
    task_pgrp is confusing, as the names can be mixed with each other when looking
    at the code for a long time.
    
    The proposals are to
    * equip the functions that return the integer with _nr suffix to
      represent that fact,
    * and to make all functions work with task (not process) by making
      the common prefix of the same name.
    
    For monotony the routines signal_session() and set_signal_session() are
    replaced with task_session_nr() and set_task_session(), especially since they
    are only used with the explicit task->signal dereference.
    
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Acked-by: Serge E. Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 61516b89cb6c..fd65bca38a93 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1283,8 +1283,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 		if (thread_group_leader(p)) {
 			p->signal->tty = current->signal->tty;
-			p->signal->pgrp = process_group(current);
-			set_signal_session(p->signal, process_session(current));
+			p->signal->pgrp = task_pgrp_nr(current);
+			set_task_session(p, task_session_nr(current));
 			attach_pid(p, PIDTYPE_PGID, task_pgrp(current));
 			attach_pid(p, PIDTYPE_SID, task_session(current));
 

commit 8793d854edbc2774943a4b0de3304dc73991159a
Author: Paul Menage <menage@google.com>
Date:   Thu Oct 18 23:39:39 2007 -0700

    Task Control Groups: make cpusets a client of cgroups
    
    Remove the filesystem support logic from the cpusets system and makes cpusets
    a cgroup subsystem
    
    The "cpuset" filesystem becomes a dummy filesystem; attempts to mount it get
    passed through to the cgroup filesystem with the appropriate options to
    emulate the old cpuset filesystem behaviour.
    
    Signed-off-by: Paul Menage <menage@google.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fcac38929245..61516b89cb6c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -29,7 +29,6 @@
 #include <linux/nsproxy.h>
 #include <linux/capability.h>
 #include <linux/cpu.h>
-#include <linux/cpuset.h>
 #include <linux/cgroup.h>
 #include <linux/security.h>
 #include <linux/swap.h>
@@ -1089,7 +1088,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 	p->io_context = NULL;
 	p->audit_context = NULL;
-	cpuset_fork(p);
 	cgroup_fork(p);
 #ifdef CONFIG_NUMA
  	p->mempolicy = mpol_copy(p->mempolicy);
@@ -1330,7 +1328,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	mpol_free(p->mempolicy);
 bad_fork_cleanup_cgroup:
 #endif
-	cpuset_exit(p);
 	cgroup_exit(p, cgroup_callbacks_done);
 bad_fork_cleanup_delays_binfmt:
 	delayacct_tsk_free(p);

commit 817929ec274bcfe771586d338bb31d1659615686
Author: Paul Menage <menage@google.com>
Date:   Thu Oct 18 23:39:36 2007 -0700

    Task Control Groups: shared cgroup subsystem group arrays
    
    Replace the struct css_set embedded in task_struct with a pointer; all tasks
    that have the same set of memberships across all hierarchies will share a
    css_set object, and will be linked via their css_sets field to the "tasks"
    list_head in the css_set.
    
    Assuming that many tasks share the same cgroup assignments, this reduces
    overall space usage and keeps the size of the task_struct down (three pointers
    added to task_struct compared to a non-cgroups kernel, no matter how many
    subsystems are registered).
    
    [akpm@linux-foundation.org: fix a printk]
    [akpm@linux-foundation.org: build fix]
    Signed-off-by: Paul Menage <menage@google.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e7c181454dca..fcac38929245 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1301,6 +1301,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
 	proc_fork_connector(p);
+	cgroup_post_fork(p);
 	return p;
 
 bad_fork_cleanup_namespaces:

commit b4f48b6363c81ca743ef46943ef23fd72e60f679
Author: Paul Menage <menage@google.com>
Date:   Thu Oct 18 23:39:33 2007 -0700

    Task Control Groups: add fork()/exit() hooks
    
    This adds the necessary hooks to the fork() and exit() paths to ensure
    that new children inherit their parent's cgroup assignments, and that
    exiting processes release reference counts on their cgroups.
    
    Signed-off-by: Paul Menage <menage@google.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2ce28f165e31..e7c181454dca 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -30,6 +30,7 @@
 #include <linux/capability.h>
 #include <linux/cpu.h>
 #include <linux/cpuset.h>
+#include <linux/cgroup.h>
 #include <linux/security.h>
 #include <linux/swap.h>
 #include <linux/syscalls.h>
@@ -979,6 +980,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 {
 	int retval;
 	struct task_struct *p = NULL;
+	int cgroup_callbacks_done = 0;
 
 	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
 		return ERR_PTR(-EINVAL);
@@ -1088,12 +1090,13 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->io_context = NULL;
 	p->audit_context = NULL;
 	cpuset_fork(p);
+	cgroup_fork(p);
 #ifdef CONFIG_NUMA
  	p->mempolicy = mpol_copy(p->mempolicy);
  	if (IS_ERR(p->mempolicy)) {
  		retval = PTR_ERR(p->mempolicy);
  		p->mempolicy = NULL;
- 		goto bad_fork_cleanup_cpuset;
+ 		goto bad_fork_cleanup_cgroup;
  	}
 	mpol_fix_fork_child_flag(p);
 #endif
@@ -1204,6 +1207,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	sched_fork(p, clone_flags);
 
+	/* Now that the task is set up, run cgroup callbacks if
+	 * necessary. We need to run them before the task is visible
+	 * on the tasklist. */
+	cgroup_fork_callbacks(p);
+	cgroup_callbacks_done = 1;
+
 	/* Need tasklist lock for parent etc handling! */
 	write_lock_irq(&tasklist_lock);
 
@@ -1318,9 +1327,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_policy:
 #ifdef CONFIG_NUMA
 	mpol_free(p->mempolicy);
-bad_fork_cleanup_cpuset:
+bad_fork_cleanup_cgroup:
 #endif
 	cpuset_exit(p);
+	cgroup_exit(p, cgroup_callbacks_done);
 bad_fork_cleanup_delays_binfmt:
 	delayacct_tsk_free(p);
 	if (p->binfmt)

commit c66f08be7e3ad0a28bcd9a0aef766fdf08ea0ec6
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Oct 18 03:06:34 2007 -0700

    Add scaled time to taskstats based process accounting
    
    This adds items to the taststats struct to account for user and system
    time based on scaling the CPU frequency and instruction issue rates.
    
    Adds account_(user|system)_time_scaled callbacks which architectures
    can use to account for time using this mechanism.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1232aac6a1cd..2ce28f165e31 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1059,6 +1059,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->utime = cputime_zero;
 	p->stime = cputime_zero;
 	p->gtime = cputime_zero;
+	p->utimescaled = cputime_zero;
+	p->stimescaled = cputime_zero;
 
 #ifdef CONFIG_TASK_XACCT
 	p->rchar = 0;		/* I/O counter: bytes read */

commit 23ff4440243fe3fa42515d18aa213be14bb706ee
Author: Daniel Walker <dwalker@mvista.com>
Date:   Thu Oct 18 03:06:07 2007 -0700

    whitespace fixes: fork
    
    Signed-off-by: Daniel Walker <dwalker@mvista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 125246fc75a5..1232aac6a1cd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -268,7 +268,7 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			get_file(file);
 			if (tmp->vm_flags & VM_DENYWRITE)
 				atomic_dec(&inode->i_writecount);
-      
+
 			/* insert tmp into the share list, just after mpnt */
 			spin_lock(&file->f_mapping->i_mmap_lock);
 			tmp->vm_truncate_count = mpnt->vm_truncate_count;
@@ -331,7 +331,7 @@ static inline void mm_free_pgd(struct mm_struct * mm)
 #define mm_free_pgd(mm)
 #endif /* CONFIG_MMU */
 
- __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
+__cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
 
 #define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
 #define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
@@ -738,8 +738,8 @@ static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 	/* compute the remainder to be cleared */
 	size = (new_fdt->max_fds - open_files) * sizeof(struct file *);
 
-	/* This is long word aligned thus could use a optimized version */ 
-	memset(new_fds, 0, size); 
+	/* This is long word aligned thus could use a optimized version */
+	memset(new_fds, 0, size);
 
 	if (new_fdt->max_fds > open_files) {
 		int left = (new_fdt->max_fds-open_files)/8;
@@ -1069,12 +1069,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	task_io_accounting_init(p);
 	acct_clear_integrals(p);
 
- 	p->it_virt_expires = cputime_zero;
+	p->it_virt_expires = cputime_zero;
 	p->it_prof_expires = cputime_zero;
- 	p->it_sched_expires = 0;
- 	INIT_LIST_HEAD(&p->cpu_timers[0]);
- 	INIT_LIST_HEAD(&p->cpu_timers[1]);
- 	INIT_LIST_HEAD(&p->cpu_timers[2]);
+	p->it_sched_expires = 0;
+	INIT_LIST_HEAD(&p->cpu_timers[0]);
+	INIT_LIST_HEAD(&p->cpu_timers[1]);
+	INIT_LIST_HEAD(&p->cpu_timers[2]);
 
 	p->lock_depth = -1;		/* -1 = no lock */
 	do_posix_clock_monotonic_gettime(&p->start_time);
@@ -1239,7 +1239,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * A fatal signal pending means that current will exit, so the new
 	 * thread can't slip out of an OOM kill (or normal SIGKILL).
  	 */
- 	recalc_sigpending();
+	recalc_sigpending();
 	if (signal_pending(current)) {
 		spin_unlock(&current->sighand->siglock);
 		write_unlock_irq(&tasklist_lock);

commit 6212e3a388fdda3f19fa660ef5a30edf54d1dcfd
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Oct 18 03:04:56 2007 -0700

    Remove struct task_struct::io_wait
    
    Hell knows what happened in commit 63b05203af57e7de4f3bb63b8b81d43bc196d32b
    during 2.6.9 development.  Commit introduced io_wait field which remained
    write-only than and still remains write-only.
    
    Also garbage collect macros which "use" io_wait.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7e455a93a75a..125246fc75a5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1084,7 +1084,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->security = NULL;
 #endif
 	p->io_context = NULL;
-	p->io_wait = NULL;
 	p->audit_context = NULL;
 	cpuset_fork(p);
 #ifdef CONFIG_NUMA

commit 2e1318956ce6bf149af5c5e98499b5cd99f99c89
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Thu Oct 18 03:04:45 2007 -0700

    freezer: prevent new tasks from inheriting TIF_FREEZE set
    
    Tasks should go to the refrigerator only if explicitly requested to do that by
    the freezer and not as a result of inheriting the TIF_FREEZE flag set from the
    parent.  Make it happen.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Nigel Cunningham <nigel@nigel.suspend2.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 490495a39c7e..7e455a93a75a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -942,6 +942,7 @@ static inline void copy_flags(unsigned long clone_flags, struct task_struct *p)
 	if (!(clone_flags & CLONE_PTRACE))
 		p->ptrace = 0;
 	p->flags = new_flags;
+	clear_freeze_flag(p);
 }
 
 asmlinkage long sys_set_tid_address(int __user *tidptr)

commit 57c521ce6125e15e99e56c902cb8da96bee7b36d
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Tue Oct 16 23:31:35 2007 -0700

    ifdef struct task_struct::security
    
    For those who don't care about CONFIG_SECURITY.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Serge E. Hallyn" <serge@hallyn.com>
    Cc: Casey Schaufler <casey@schaufler-ca.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3e764f6f2be1..490495a39c7e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1079,7 +1079,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	do_posix_clock_monotonic_gettime(&p->start_time);
 	p->real_start_time = p->start_time;
 	monotonic_to_bootbased(&p->real_start_time);
+#ifdef CONFIG_SECURITY
 	p->security = NULL;
+#endif
 	p->io_context = NULL;
 	p->io_wait = NULL;
 	p->audit_context = NULL;

commit 42b2dd0a02c512cf59c96f5c227bf54bfe5bbf08
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Tue Oct 16 23:27:30 2007 -0700

    Shrink task_struct if CONFIG_FUTEX=n
    
    robust_list, compat_robust_list, pi_state_list, pi_state_cache are
    really used if futexes are on.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8a97e92c604f..3e764f6f2be1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1156,13 +1156,14 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	 * Clear TID on mm_release()?
 	 */
 	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;
+#ifdef CONFIG_FUTEX
 	p->robust_list = NULL;
 #ifdef CONFIG_COMPAT
 	p->compat_robust_list = NULL;
 #endif
 	INIT_LIST_HEAD(&p->pi_state_list);
 	p->pi_state_cache = NULL;
-
+#endif
 	/*
 	 * sigaltstack should be cleared when sharing the same VM
 	 */

commit 4ba9b9d0ba0a49d91fa6417c7510ee36f48cf957
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Oct 16 23:25:51 2007 -0700

    Slab API: remove useless ctor parameter and reorder parameters
    
    Slab constructors currently have a flags parameter that is never used.  And
    the order of the arguments is opposite to other slab functions.  The object
    pointer is placed before the kmem_cache pointer.
    
    Convert
    
            ctor(void *object, struct kmem_cache *s, unsigned long flags)
    
    to
    
            ctor(struct kmem_cache *s, void *object)
    
    throughout the kernel
    
    [akpm@linux-foundation.org: coupla fixes]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 163325af8179..8a97e92c604f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1445,8 +1445,7 @@ long do_fork(unsigned long clone_flags,
 #define ARCH_MIN_MMSTRUCT_ALIGN 0
 #endif
 
-static void sighand_ctor(void *data, struct kmem_cache *cachep,
-			unsigned long flags)
+static void sighand_ctor(struct kmem_cache *cachep, void *data)
 {
 	struct sighand_struct *sighand = data;
 

commit 3e26c149c358529b1605f8959341d34bc4b880a3
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:50 2007 -0700

    mm: dirty balancing for tasks
    
    Based on ideas of Andrew:
      http://marc.info/?l=linux-kernel&m=102912915020543&w=2
    
    Scale the bdi dirty limit inversly with the tasks dirty rate.
    This makes heavy writers have a lower dirty limit than the occasional writer.
    
    Andrea proposed something similar:
      http://lwn.net/Articles/152277/
    
    The main disadvantage to his patch is that he uses an unrelated quantity to
    measure time, which leaves him with a workload dependant tunable. Other than
    that the two approaches appear quite similar.
    
    [akpm@linux-foundation.org: fix warning]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3fc3c1383912..163325af8179 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -107,6 +107,7 @@ static struct kmem_cache *mm_cachep;
 
 void free_task(struct task_struct *tsk)
 {
+	prop_local_destroy_single(&tsk->dirties);
 	free_thread_info(tsk->stack);
 	rt_mutex_debug_task_free(tsk);
 	free_task_struct(tsk);
@@ -163,6 +164,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 {
 	struct task_struct *tsk;
 	struct thread_info *ti;
+	int err;
 
 	prepare_to_copy(orig);
 
@@ -178,6 +180,14 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 
 	*tsk = *orig;
 	tsk->stack = ti;
+
+	err = prop_local_init_single(&tsk->dirties);
+	if (err) {
+		free_thread_info(ti);
+		free_task_struct(tsk);
+		return NULL;
+	}
+
 	setup_thread_stack(tsk, orig);
 
 #ifdef CONFIG_CC_STACKPROTECTOR

commit 9ac52315d4cf5f561f36dabaf0720c00d3553162
Author: Laurent Vivier <Laurent.Vivier@bull.net>
Date:   Mon Oct 15 17:00:19 2007 +0200

    sched: guest CPU accounting: add guest-CPU /proc/<pid>/stat fields
    
    like for cpustat, introduce the "gtime" (guest time of the task) and
    "cgtime" (guest time of the task children) fields for the
    tasks. Modify signal_struct and task_struct.
    
    Modify /proc/<pid>/stat to display these new fields.
    
    Signed-off-by: Laurent Vivier <Laurent.Vivier@bull.net>
    Acked-by: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5e67f90a1694..3fc3c1383912 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -877,6 +877,8 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	sig->tty_old_pgrp = NULL;
 
 	sig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;
+	sig->gtime = cputime_zero;
+	sig->cgtime = cputime_zero;
 	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
 	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
 	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;
@@ -1045,6 +1047,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->utime = cputime_zero;
 	p->stime = cputime_zero;
+	p->gtime = cputime_zero;
 
 #ifdef CONFIG_TASK_XACCT
 	p->rchar = 0;		/* I/O counter: bytes read */

commit 9dd776b6d7b0b85966b6ddd03e2b2aae59012ab1
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Sep 26 22:04:26 2007 -0700

    [NET]: Add network namespace clone & unshare support.
    
    This patch allows you to create a new network namespace
    using sys_clone, or sys_unshare.
    
    As the network namespace is still experimental and under development
    clone and unshare support is only made available when CONFIG_NET_NS is
    selected at compile time.
    
    As this patch introduces network namespace support into code paths
    that exist when the CONFIG_NET is not selected there are a few
    additions made to net_namespace.h to allow a few more functions
    to be used when the networking stack is not compiled in.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/fork.c b/kernel/fork.c
index 33f12f48684a..5e67f90a1694 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1608,7 +1608,8 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	err = -EINVAL;
 	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
-				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWUSER))
+				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWUSER|
+				CLONE_NEWNET))
 		goto bad_unshare_out;
 
 	if ((err = unshare_thread(unshare_flags)))

commit b8fceee17a310f189188599a8fa5e9beaff57eb0
Author: Davide Libenzi <davidel@xmailserver.org>
Date:   Thu Sep 20 12:40:16 2007 -0700

    signalfd simplification
    
    This simplifies signalfd code, by avoiding it to remain attached to the
    sighand during its lifetime.
    
    In this way, the signalfd remain attached to the sighand only during
    poll(2) (and select and epoll) and read(2).  This also allows to remove
    all the custom "tsk == current" checks in kernel/signal.c, since
    dequeue_signal() will only be called by "current".
    
    I think this is also what Ben was suggesting time ago.
    
    The external effect of this, is that a thread can extract only its own
    private signals and the group ones.  I think this is an acceptable
    behaviour, in that those are the signals the thread would be able to
    fetch w/out signalfd.
    
    Signed-off-by: Davide Libenzi <davidel@xmailserver.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7332e236d367..33f12f48684a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1438,7 +1438,7 @@ static void sighand_ctor(void *data, struct kmem_cache *cachep,
 	struct sighand_struct *sighand = data;
 
 	spin_lock_init(&sighand->siglock);
-	INIT_LIST_HEAD(&sighand->signalfd_list);
+	init_waitqueue_head(&sighand->signalfd_wqh);
 }
 
 void __init proc_caches_init(void)

commit 20c2df83d25c6a95affe6157a4c9cac4cf5ffaac
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Jul 20 10:11:58 2007 +0900

    mm: Remove slab destructors from kmem_cache_create().
    
    Slab destructors were no longer supported after Christoph's
    c59def9f222d44bb7e2f0a559f2906191a0862d7 change. They've been
    BUGs for both slab and slub, and slob never supported them
    either.
    
    This rips out support for the dtor pointer from kmem_cache_create()
    completely and fixes up every single callsite in the kernel (there were
    about 224, not including the slab allocator definitions themselves,
    or the documentation references).
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 469838998220..7332e236d367 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -137,7 +137,7 @@ void __init fork_init(unsigned long mempages)
 	/* create a slab on which task_structs can be allocated */
 	task_struct_cachep =
 		kmem_cache_create("task_struct", sizeof(struct task_struct),
-			ARCH_MIN_TASKALIGN, SLAB_PANIC, NULL, NULL);
+			ARCH_MIN_TASKALIGN, SLAB_PANIC, NULL);
 #endif
 
 	/*
@@ -1446,22 +1446,22 @@ void __init proc_caches_init(void)
 	sighand_cachep = kmem_cache_create("sighand_cache",
 			sizeof(struct sighand_struct), 0,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_DESTROY_BY_RCU,
-			sighand_ctor, NULL);
+			sighand_ctor);
 	signal_cachep = kmem_cache_create("signal_cache",
 			sizeof(struct signal_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
-	files_cachep = kmem_cache_create("files_cache", 
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
+	files_cachep = kmem_cache_create("files_cache",
 			sizeof(struct files_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
-	fs_cachep = kmem_cache_create("fs_cache", 
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
+	fs_cachep = kmem_cache_create("fs_cache",
 			sizeof(struct fs_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
 	vm_area_cachep = kmem_cache_create("vm_area_struct",
 			sizeof(struct vm_area_struct), 0,
-			SLAB_PANIC, NULL, NULL);
+			SLAB_PANIC, NULL);
 	mm_cachep = kmem_cache_create("mm_struct",
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
 }
 
 /*

commit d7e28ffe6c74416b54345d6004fd0964c115b12c
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Jul 19 01:49:23 2007 -0700

    lguest: the host code
    
    This is the code for the "lg.ko" module, which allows lguest guests to
    be launched.
    
    [akpm@linux-foundation.org: update for futex-new-private-futexes]
    [akpm@linux-foundation.org: build fix]
    [jmorris@namei.org: lguest: use hrtimers]
    [akpm@linux-foundation.org: x86_64 build fix]
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Andi Kleen <ak@suse.de>
    Cc: Eric Dumazet <dada1@cosmosbay.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e7a2d995b087..469838998220 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -127,7 +127,6 @@ void __put_task_struct(struct task_struct *tsk)
 	if (!profile_handoff_task(tsk))
 		free_task(tsk);
 }
-EXPORT_SYMBOL_GPL(__put_task_struct);
 
 void __init fork_init(unsigned long mempages)
 {

commit 5992b6dac0d23a2b51a1ccbaf8f1a2e62097b12b
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Jul 19 01:49:21 2007 -0700

    lguest: export symbols for lguest as a module
    
    lguest does some fairly lowlevel things to support a host, which
    normal modules don't need:
    
    math_state_restore:
            When the guest triggers a Device Not Available fault, we need
            to be able to restore the FPU
    
    __put_task_struct:
            We need to hold a reference to another task for inter-guest
            I/O, and put_task_struct() is an inline function which calls
            __put_task_struct.
    
    access_process_vm:
            We need to access another task for inter-guest I/O.
    
    map_vm_area & __get_vm_area:
            We need to map the switcher shim (ie. monitor) at 0xFFC01000.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 469838998220..e7a2d995b087 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -127,6 +127,7 @@ void __put_task_struct(struct task_struct *tsk)
 	if (!profile_handoff_task(tsk))
 		free_task(tsk);
 }
+EXPORT_SYMBOL_GPL(__put_task_struct);
 
 void __init fork_init(unsigned long mempages)
 {

commit 3cb4a0bb1e773e3c41800b33a3f7dab32bd06c64
Author: Kawai, Hidehiro <hidehiro.kawai.ez@hitachi.com>
Date:   Thu Jul 19 01:48:28 2007 -0700

    coredump masking: add an interface for core dump filter
    
    This patch adds an interface to set/reset flags which determines each memory
    segment should be dumped or not when a core file is generated.
    
    /proc/<pid>/coredump_filter file is provided to access the flags.  You can
    change the flag status for a particular process by writing to or reading from
    the file.
    
    The flag status is inherited to the child process when it is created.
    
    Signed-off-by: Hidehiro Kawai <hidehiro.kawai.ez@hitachi.com>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ba39bdb2a7b8..469838998220 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -334,6 +334,8 @@ static struct mm_struct * mm_init(struct mm_struct * mm)
 	atomic_set(&mm->mm_count, 1);
 	init_rwsem(&mm->mmap_sem);
 	INIT_LIST_HEAD(&mm->mmlist);
+	mm->flags = (current->mm) ? current->mm->flags
+				  : MMF_DUMP_FILTER_DEFAULT;
 	mm->core_waiters = 0;
 	mm->nr_ptes = 0;
 	set_mm_counter(mm, file_rss, 0);

commit 831441862956fffa17b9801db37e6ea1650b0f69
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Tue Jul 17 04:03:35 2007 -0700

    Freezer: make kernel threads nonfreezable by default
    
    Currently, the freezer treats all tasks as freezable, except for the kernel
    threads that explicitly set the PF_NOFREEZE flag for themselves.  This
    approach is problematic, since it requires every kernel thread to either
    set PF_NOFREEZE explicitly, or call try_to_freeze(), even if it doesn't
    care for the freezing of tasks at all.
    
    It seems better to only require the kernel threads that want to or need to
    be frozen to use some freezer-related code and to remove any
    freezer-related code from the other (nonfreezable) kernel threads, which is
    done in this patch.
    
    The patch causes all kernel threads to be nonfreezable by default (ie.  to
    have PF_NOFREEZE set by default) and introduces the set_freezable()
    function that should be called by the freezable kernel threads in order to
    unset PF_NOFREEZE.  It also makes all of the currently freezable kernel
    threads call set_freezable(), so it shouldn't cause any (intentional)
    change of behaviour to appear.  Additionally, it updates documentation to
    describe the freezing of tasks more accurately.
    
    [akpm@linux-foundation.org: build fixes]
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Nigel Cunningham <nigel@nigel.suspend2.net>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7c5c5888e00a..ba39bdb2a7b8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -923,7 +923,7 @@ static inline void copy_flags(unsigned long clone_flags, struct task_struct *p)
 {
 	unsigned long new_flags = p->flags;
 
-	new_flags &= ~(PF_SUPERPRIV | PF_NOFREEZE);
+	new_flags &= ~PF_SUPERPRIV;
 	new_flags |= PF_FORKNOEXEC;
 	if (!(clone_flags & CLONE_PTRACE))
 		p->ptrace = 0;

commit 77ec739d8d0979477fc91f530403805afa2581a4
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Sun Jul 15 23:41:01 2007 -0700

    user namespace: add unshare
    
    This patch enables the unshare of user namespaces.
    
    It adds a new clone flag CLONE_NEWUSER and implements copy_user_ns() which
    resets the current user_struct and adds a new root user (uid == 0)
    
    For now, unsharing the user namespace allows a process to reset its
    user_struct accounting and uid 0 in the new user namespace should be contained
    using appropriate means, for instance selinux
    
    The plan, when the full support is complete (all uid checks covered), is to
    keep the original user's rights in the original namespace, and let a process
    become uid 0 in the new namespace, with full capabilities to the new
    namespace.
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Acked-by: Pavel Emelianov <xemul@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: Andrew Morgan <agm@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 13cf0978780a..7c5c5888e00a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1606,7 +1606,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	err = -EINVAL;
 	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
 				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
-				CLONE_NEWUTS|CLONE_NEWIPC))
+				CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWUSER))
 		goto bad_unshare_out;
 
 	if ((err = unshare_thread(unshare_flags)))

commit acce292c82d4d82d35553b928df2b0597c3a9c78
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Sun Jul 15 23:40:59 2007 -0700

    user namespace: add the framework
    
    Basically, it will allow a process to unshare its user_struct table,
    resetting at the same time its own user_struct and all the associated
    accounting.
    
    A new root user (uid == 0) is added to the user namespace upon creation.
    Such root users have full privileges and it seems that theses privileges
    should be controlled through some means (process capabilities ?)
    
    The unshare is not included in this patch.
    
    Changes since [try #4]:
            - Updated get_user_ns and put_user_ns to accept NULL, and
              get_user_ns to return the namespace.
    
    Changes since [try #3]:
            - moved struct user_namespace to files user_namespace.{c,h}
    
    Changes since [try #2]:
            - removed struct user_namespace* argument from find_user()
    
    Changes since [try #1]:
            - removed struct user_namespace* argument from find_user()
            - added a root_user per user namespace
    
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: Pavel Emelianov <xemul@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: Andrew Morgan <agm@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4015912aaac2..13cf0978780a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1002,7 +1002,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (atomic_read(&p->user->processes) >=
 			p->signal->rlim[RLIMIT_NPROC].rlim_cur) {
 		if (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&
-				p->user != &root_user)
+		    p->user != current->nsproxy->user_ns->root_user)
 			goto bad_fork_free;
 	}
 

commit 522ed7767e800cff6c650ec64b0ee0677303119c
Author: Miloslav Trmac <mitr@redhat.com>
Date:   Sun Jul 15 23:40:56 2007 -0700

    Audit: add TTY input auditing
    
    Add TTY input auditing, used to audit system administrator's actions.  This is
    required by various security standards such as DCID 6/3 and PCI to provide
    non-repudiation of administrator's actions and to allow a review of past
    actions if the administrator seems to overstep their duties or if the system
    becomes misconfigured for unknown reasons.  These requirements do not make it
    necessary to audit TTY output as well.
    
    Compared to an user-space keylogger, this approach records TTY input using the
    audit subsystem, correlated with other audit events, and it is completely
    transparent to the user-space application (e.g.  the console ioctls still
    work).
    
    TTY input auditing works on a higher level than auditing all system calls
    within the session, which would produce an overwhelming amount of mostly
    useless audit events.
    
    Add an "audit_tty" attribute, inherited across fork ().  Data read from TTYs
    by process with the attribute is sent to the audit subsystem by the kernel.
    The audit netlink interface is extended to allow modifying the audit_tty
    attribute, and to allow sending explanatory audit events from user-space (for
    example, a shell might send an event containing the final command, after the
    interactive command-line editing and history expansion is performed, which
    might be difficult to decipher from the TTY input alone).
    
    Because the "audit_tty" attribute is inherited across fork (), it would be set
    e.g.  for sshd restarted within an audited session.  To prevent this, the
    audit_tty attribute is cleared when a process with no open TTY file
    descriptors (e.g.  after daemon startup) opens a TTY.
    
    See https://www.redhat.com/archives/linux-audit/2007-June/msg00000.html for a
    more detailed rationale document for an older version of this patch.
    
    [akpm@linux-foundation.org: build fix]
    Signed-off-by: Miloslav Trmac <mitr@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Paul Fulghum <paulkf@microgate.com>
    Cc: Casey Schaufler <casey@schaufler-ca.com>
    Cc: Steve Grubb <sgrubb@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 344d693fdc78..4015912aaac2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -49,6 +49,7 @@
 #include <linux/delayacct.h>
 #include <linux/taskstats_kern.h>
 #include <linux/random.h>
+#include <linux/tty.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -897,6 +898,8 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	}
 	acct_init_pacct(&sig->pacct);
 
+	tty_audit_fork(sig);
+
 	return 0;
 }
 

commit 924b42d5a2dbe508407a0a6290d3751f826bccdd
Author: Tomas Janousek <tjanouse@redhat.com>
Date:   Sun Jul 15 23:39:42 2007 -0700

    Use boot based time for process start time and boot time in /proc
    
    Commit 411187fb05cd11676b0979d9fbf3291db69dbce2 caused boot time to move and
    process start times to become invalid after suspend.  Using boot based time
    for those restores the old behaviour and fixes the issue.
    
    [akpm@linux-foundation.org: little cleanup]
    Signed-off-by: Tomas Janousek <tjanouse@redhat.com>
    Cc: Tomas Smetana <tsmetana@redhat.com>
    Acked-by: John Stultz <johnstul@us.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index da3a155bba0d..344d693fdc78 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1059,6 +1059,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->lock_depth = -1;		/* -1 = no lock */
 	do_posix_clock_monotonic_gettime(&p->start_time);
+	p->real_start_time = p->start_time;
+	monotonic_to_bootbased(&p->real_start_time);
 	p->security = NULL;
 	p->io_context = NULL;
 	p->io_wait = NULL;

commit 172ba844a8851c3edd13c0a979cdf46bd5e3cc1a
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Mon Jul 9 18:52:00 2007 +0200

    sched: update delay-accounting to use CFS's precise stats
    
    update delay-accounting to use CFS's precise stats.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 73ad5cda1bcd..da3a155bba0d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -877,7 +877,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
 	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
 	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;
-	sig->sched_time = 0;
+	sig->sum_sched_runtime = 0;
 	INIT_LIST_HEAD(&sig->cpu_timers[0]);
 	INIT_LIST_HEAD(&sig->cpu_timers[1]);
 	INIT_LIST_HEAD(&sig->cpu_timers[2]);
@@ -1040,7 +1040,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	p->utime = cputime_zero;
 	p->stime = cputime_zero;
- 	p->sched_time = 0;
+
 #ifdef CONFIG_TASK_XACCT
 	p->rchar = 0;		/* I/O counter: bytes read */
 	p->wchar = 0;		/* I/O counter: bytes written */

commit ba96a0c88098697a63e80157718b7440414ed24d
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Wed May 23 13:57:25 2007 -0700

    freezer: fix vfork problem
    
    Currently try_to_freeze_tasks() has to wait until all of the vforked processes
    exit and for this reason every user can make it fail.  To fix this problem we
    can introduce the additional process flag PF_FREEZER_SKIP to be used by tasks
    that do not want to be counted as freezable by the freezer and want to have
    TIF_FREEZE set nevertheless.  Then, this flag can be set by tasks using
    sys_vfork() before they call wait_for_completion(&vfork) and cleared after
    they have woken up.  After clearing it, the tasks should call try_to_freeze()
    as soon as possible.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 87069cfc18a1..73ad5cda1bcd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -45,6 +45,7 @@
 #include <linux/acct.h>
 #include <linux/tsacct_kern.h>
 #include <linux/cn_proc.h>
+#include <linux/freezer.h>
 #include <linux/delayacct.h>
 #include <linux/taskstats_kern.h>
 #include <linux/random.h>
@@ -1405,7 +1406,9 @@ long do_fork(unsigned long clone_flags,
 		}
 
 		if (clone_flags & CLONE_VFORK) {
+			freezer_do_not_count();
 			wait_for_completion(&vfork);
+			freezer_count();
 			if (unlikely (current->ptrace & PT_TRACE_VFORK_DONE)) {
 				current->ptrace_message = nr;
 				ptrace_notify ((PTRACE_EVENT_VFORK_DONE << 8) | SIGTRAP);

commit a35afb830f8d71ec211531aeb9a621b09a2efb39
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed May 16 22:10:57 2007 -0700

    Remove SLAB_CTOR_CONSTRUCTOR
    
    SLAB_CTOR_CONSTRUCTOR is always specified. No point in checking it.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Steven French <sfrench@us.ibm.com>
    Cc: Michael Halcrow <mhalcrow@us.ibm.com>
    Cc: OGAWA Hirofumi <hirofumi@mail.parknet.co.jp>
    Cc: Miklos Szeredi <miklos@szeredi.hu>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Dave Kleikamp <shaggy@austin.ibm.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Cc: "J. Bruce Fields" <bfields@fieldses.org>
    Cc: Anton Altaparmakov <aia21@cantab.net>
    Cc: Mark Fasheh <mark.fasheh@oracle.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Jan Kara <jack@ucw.cz>
    Cc: David Chinner <dgc@sgi.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 49530e40ea8b..87069cfc18a1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1427,10 +1427,8 @@ static void sighand_ctor(void *data, struct kmem_cache *cachep,
 {
 	struct sighand_struct *sighand = data;
 
-	if (flags & SLAB_CTOR_CONSTRUCTOR) {
-		spin_lock_init(&sighand->siglock);
-		INIT_LIST_HEAD(&sighand->signalfd_list);
-	}
+	spin_lock_init(&sighand->siglock);
+	INIT_LIST_HEAD(&sighand->signalfd_list);
 }
 
 void __init proc_caches_init(void)

commit fba2afaaec790dc5ab4ae8827972f342211bbb86
Author: Davide Libenzi <davidel@xmailserver.org>
Date:   Thu May 10 22:23:13 2007 -0700

    signal/timer/event: signalfd core
    
    This patch series implements the new signalfd() system call.
    
    I took part of the original Linus code (and you know how badly it can be
    broken :), and I added even more breakage ;) Signals are fetched from the same
    signal queue used by the process, so signalfd will compete with standard
    kernel delivery in dequeue_signal().  If you want to reliably fetch signals on
    the signalfd file, you need to block them with sigprocmask(SIG_BLOCK).  This
    seems to be working fine on my Dual Opteron machine.  I made a quick test
    program for it:
    
    http://www.xmailserver.org/signafd-test.c
    
    The signalfd() system call implements signal delivery into a file descriptor
    receiver.  The signalfd file descriptor if created with the following API:
    
    int signalfd(int ufd, const sigset_t *mask, size_t masksize);
    
    The "ufd" parameter allows to change an existing signalfd sigmask, w/out going
    to close/create cycle (Linus idea).  Use "ufd" == -1 if you want a brand new
    signalfd file.
    
    The "mask" allows to specify the signal mask of signals that we are interested
    in.  The "masksize" parameter is the size of "mask".
    
    The signalfd fd supports the poll(2) and read(2) system calls.  The poll(2)
    will return POLLIN when signals are available to be dequeued.  As a direct
    consequence of supporting the Linux poll subsystem, the signalfd fd can use
    used together with epoll(2) too.
    
    The read(2) system call will return a "struct signalfd_siginfo" structure in
    the userspace supplied buffer.  The return value is the number of bytes copied
    in the supplied buffer, or -1 in case of error.  The read(2) call can also
    return 0, in case the sighand structure to which the signalfd was attached,
    has been orphaned.  The O_NONBLOCK flag is also supported, and read(2) will
    return -EAGAIN in case no signal is available.
    
    If the size of the buffer passed to read(2) is lower than sizeof(struct
    signalfd_siginfo), -EINVAL is returned.  A read from the signalfd can also
    return -ERESTARTSYS in case a signal hits the process.  The format of the
    struct signalfd_siginfo is, and the valid fields depends of the (->code &
    __SI_MASK) value, in the same way a struct siginfo would:
    
    struct signalfd_siginfo {
            __u32 signo;    /* si_signo */
            __s32 err;      /* si_errno */
            __s32 code;     /* si_code */
            __u32 pid;      /* si_pid */
            __u32 uid;      /* si_uid */
            __s32 fd;       /* si_fd */
            __u32 tid;      /* si_fd */
            __u32 band;     /* si_band */
            __u32 overrun;  /* si_overrun */
            __u32 trapno;   /* si_trapno */
            __s32 status;   /* si_status */
            __s32 svint;    /* si_int */
            __u64 svptr;    /* si_ptr */
            __u64 utime;    /* si_utime */
            __u64 stime;    /* si_stime */
            __u64 addr;     /* si_addr */
    };
    
    [akpm@linux-foundation.org: fix signalfd_copyinfo() on i386]
    Signed-off-by: Davide Libenzi <davidel@xmailserver.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 083bf8953ce8..49530e40ea8b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1422,12 +1422,15 @@ long do_fork(unsigned long clone_flags,
 #define ARCH_MIN_MMSTRUCT_ALIGN 0
 #endif
 
-static void sighand_ctor(void *data, struct kmem_cache *cachep, unsigned long flags)
+static void sighand_ctor(void *data, struct kmem_cache *cachep,
+			unsigned long flags)
 {
 	struct sighand_struct *sighand = data;
 
-	if (flags & SLAB_CTOR_CONSTRUCTOR)
+	if (flags & SLAB_CTOR_CONSTRUCTOR) {
 		spin_lock_init(&sighand->siglock);
+		INIT_LIST_HEAD(&sighand->signalfd_list);
+	}
 }
 
 void __init proc_caches_init(void)
@@ -1453,7 +1456,6 @@ void __init proc_caches_init(void)
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
 }
 
-
 /*
  * Check constraints on flags passed to the unshare system call and
  * force unsharing of additional process context as appropriate.

commit 0800d30832ddecf2d9dc40068fed9df95930a8f1
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Thu May 10 22:23:04 2007 -0700

    Use task_pgrp() task_session() in copy_process()
    
    Use task_pgrp() and task_session() in copy_process(), and avoid find_pid()
    call when attaching the task to its process group and session.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: <containers@lists.osdl.org>
    Acked-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index cf13c44f3da3..083bf8953ce8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1249,14 +1249,11 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			__ptrace_link(p, current->parent);
 
 		if (thread_group_leader(p)) {
-			pid_t pgid = process_group(current);
-			pid_t sid = process_session(current);
-
 			p->signal->tty = current->signal->tty;
-			p->signal->pgrp = pgid;
+			p->signal->pgrp = process_group(current);
 			set_signal_session(p->signal, process_session(current));
-			attach_pid(p, PIDTYPE_PGID, find_pid(pgid));
-			attach_pid(p, PIDTYPE_SID, find_pid(sid));
+			attach_pid(p, PIDTYPE_PGID, task_pgrp(current));
+			attach_pid(p, PIDTYPE_SID, task_session(current));
 
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;

commit 85868995d9d82de5b0de38d695559daddffef893
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Thu May 10 22:23:03 2007 -0700

    Use struct pid parameter in copy_process()
    
    Modify copy_process() to take a struct pid * parameter instead of a pid_t.
    This simplifies the code a bit and also avoids having to call find_pid() to
    convert the pid_t to a struct pid.
    
    Changelog:
            - Fixed Badari Pulavarty's comments and passed in &init_struct_pid
              from fork_idle().
            - Fixed Eric Biederman's comments and simplified this patch and
              used a new patch to remove the likely(pid) check.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: <containers@lists.osdl.org>
    Acked-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6031800c94cf..cf13c44f3da3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -956,7 +956,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 					unsigned long stack_size,
 					int __user *parent_tidptr,
 					int __user *child_tidptr,
-					int pid)
+					struct pid *pid)
 {
 	int retval;
 	struct task_struct *p = NULL;
@@ -1023,7 +1023,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->did_exec = 0;
 	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	copy_flags(clone_flags, p);
-	p->pid = pid;
+	p->pid = pid_nr(pid);
 	retval = -EFAULT;
 	if (clone_flags & CLONE_PARENT_SETTID)
 		if (put_user(p->pid, parent_tidptr))
@@ -1261,7 +1261,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;
 		}
-		attach_pid(p, PIDTYPE_PID, find_pid(p->pid));
+		attach_pid(p, PIDTYPE_PID, pid);
 		nr_threads++;
 	}
 
@@ -1325,7 +1325,8 @@ struct task_struct * __cpuinit fork_idle(int cpu)
 	struct task_struct *task;
 	struct pt_regs regs;
 
-	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL, NULL, 0);
+	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL, NULL,
+				&init_struct_pid);
 	if (!IS_ERR(task))
 		init_idle(task, cpu);
 
@@ -1375,7 +1376,7 @@ long do_fork(unsigned long clone_flags,
 			clone_flags |= CLONE_PTRACE;
 	}
 
-	p = copy_process(clone_flags, stack_start, regs, stack_size, parent_tidptr, child_tidptr, nr);
+	p = copy_process(clone_flags, stack_start, regs, stack_size, parent_tidptr, child_tidptr, pid);
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.

commit e713d0dab21a68500720e222fa02567fc7dfb14b
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Thu May 10 22:22:58 2007 -0700

    attach_pid() with struct pid parameter
    
    attach_pid() currently takes a pid_t and then uses find_pid() to find the
    corresponding struct pid.  Sometimes we already have the struct pid.  We can
    then skip find_pid() if attach_pid() were to take a struct pid parameter.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: <containers@lists.osdl.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index da92e01aba6b..6031800c94cf 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1249,16 +1249,19 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 			__ptrace_link(p, current->parent);
 
 		if (thread_group_leader(p)) {
+			pid_t pgid = process_group(current);
+			pid_t sid = process_session(current);
+
 			p->signal->tty = current->signal->tty;
-			p->signal->pgrp = process_group(current);
+			p->signal->pgrp = pgid;
 			set_signal_session(p->signal, process_session(current));
-			attach_pid(p, PIDTYPE_PGID, process_group(p));
-			attach_pid(p, PIDTYPE_SID, process_session(p));
+			attach_pid(p, PIDTYPE_PGID, find_pid(pgid));
+			attach_pid(p, PIDTYPE_SID, find_pid(sid));
 
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;
 		}
-		attach_pid(p, PIDTYPE_PID, p->pid);
+		attach_pid(p, PIDTYPE_PID, find_pid(p->pid));
 		nr_threads++;
 	}
 

commit 6eaeeaba39e5fa3d52a0bb8de15e995516ae251a
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu May 10 22:22:37 2007 -0700

    getrusage(): fill ru_inblock and ru_oublock fields if possible
    
    If CONFIG_TASK_IO_ACCOUNTING is defined, we update io accounting counters for
    each task.
    
    This patch permits reporting of values using the well known getrusage()
    syscall, filling ru_inblock and ru_oublock instead of null values.
    
    As TASK_IO_ACCOUNTING currently counts bytes counts, we approximate blocks
    count doing : nr_blocks = nr_bytes / 512
    
    Example of use :
    ----------------------
    After patch is applied, /usr/bin/time command can now give a good
    approximation of IO that the process had to do.
    
    $ /usr/bin/time grep tototo /usr/include/*
    Command exited with non-zero status 1
    0.00user 0.02system 0:02.11elapsed 1%CPU (0avgtext+0avgdata 0maxresident)k
    24288inputs+0outputs (0major+259minor)pagefaults 0swaps
    
    $ /usr/bin/time dd if=/dev/zero of=/tmp/testfile count=1000
    1000+0 enregistrements lus
    1000+0 enregistrements écrits
    512000 octets (512 kB) copiés, 0,00326601 seconde, 157 MB/s
    0.00user 0.00system 0:00.00elapsed 80%CPU (0avgtext+0avgdata 0maxresident)k
    0inputs+3000outputs (0major+299minor)pagefaults 0swaps
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5dd3979747f5..da92e01aba6b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -875,6 +875,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	sig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;
 	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
 	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
+	sig->inblock = sig->oublock = sig->cinblock = sig->coublock = 0;
 	sig->sched_time = 0;
 	INIT_LIST_HEAD(&sig->cpu_timers[0]);
 	INIT_LIST_HEAD(&sig->cpu_timers[1]);

commit f7e4217b007d1f73e7e3cf10ba4fea4a608c603f
Author: Roman Zippel <zippel@linux-m68k.org>
Date:   Wed May 9 02:35:17 2007 -0700

    rename thread_info to stack
    
    This finally renames the thread_info field in task structure to stack, so that
    the assumptions about this field are gone and archs have more freedom about
    placing the thread_info structure.
    
    Nonbroken archs which have a proper thread pointer can do the access to both
    current thread and task structure via a single pointer.
    
    It'll allow for a few more cleanups of the fork code, from which e.g.  ia64
    could benefit.
    
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    [akpm@linux-foundation.org: build fix]
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Ian Molton <spyro@f2s.com>
    Cc: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Cc: Greg Ungerer <gerg@uclinux.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp>
    Cc: Richard Curnow <rc@rc0.org.uk>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Miles Bader <uclinux-v850@lsi.nec.co.jp>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Chris Zankel <chris@zankel.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a8dd75d4992b..5dd3979747f5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -105,7 +105,7 @@ static struct kmem_cache *mm_cachep;
 
 void free_task(struct task_struct *tsk)
 {
-	free_thread_info(tsk->thread_info);
+	free_thread_info(tsk->stack);
 	rt_mutex_debug_task_free(tsk);
 	free_task_struct(tsk);
 }
@@ -175,7 +175,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	}
 
 	*tsk = *orig;
-	tsk->thread_info = ti;
+	tsk->stack = ti;
 	setup_thread_stack(tsk, orig);
 
 #ifdef CONFIG_CC_STACKPROTECTOR

commit e63340ae6b6205fef26b40a75673d1c9c0c8bb90
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Tue May 8 00:28:08 2007 -0700

    header cleaning: don't include smp_lock.h when not used
    
    Remove includes of <linux/smp_lock.h> where it is not used/needed.
    Suggested by Al Viro.
    
    Builds cleanly on x86_64, i386, alpha, ia64, powerpc, sparc,
    sparc64, and arm (all 59 defconfigs).
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fd211b9dddd4..a8dd75d4992b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -14,7 +14,6 @@
 #include <linux/slab.h>
 #include <linux/init.h>
 #include <linux/unistd.h>
-#include <linux/smp_lock.h>
 #include <linux/module.h>
 #include <linux/vmalloc.h>
 #include <linux/completion.h>

commit e3222c4ecc649c4ae568e61dda9349482401b501
Author: Badari Pulavarty <pbadari@us.ibm.com>
Date:   Tue May 8 00:25:21 2007 -0700

    Merge sys_clone()/sys_unshare() nsproxy and namespace handling
    
    sys_clone() and sys_unshare() both makes copies of nsproxy and its associated
    namespaces.  But they have different code paths.
    
    This patch merges all the nsproxy and its associated namespace copy/clone
    handling (as much as possible).  Posted on container list earlier for
    feedback.
    
    - Create a new nsproxy and its associated namespaces and pass it back to
      caller to attach it to right process.
    
    - Changed all copy_*_ns() routines to return a new copy of namespace
      instead of attaching it to task->nsproxy.
    
    - Moved the CAP_SYS_ADMIN checks out of copy_*_ns() routines.
    
    - Removed unnessary !ns checks from copy_*_ns() and added BUG_ON()
      just incase.
    
    - Get rid of all individual unshare_*_ns() routines and make use of
      copy_*_ns() instead.
    
    [akpm@osdl.org: cleanups, warning fix]
    [clg@fr.ibm.com: remove dup_namespaces() declaration]
    [serue@us.ibm.com: fix CONFIG_IPC_NS=n, clone(CLONE_NEWIPC) retval]
    [akpm@linux-foundation.org: fix build with CONFIG_SYSVIPC=n]
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: <containers@lists.osdl.org>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b7d169def942..fd211b9dddd4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1515,26 +1515,6 @@ static int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)
 	return 0;
 }
 
-/*
- * Unshare the mnt_namespace structure if it is being shared
- */
-static int unshare_mnt_namespace(unsigned long unshare_flags,
-		struct mnt_namespace **new_nsp, struct fs_struct *new_fs)
-{
-	struct mnt_namespace *ns = current->nsproxy->mnt_ns;
-
-	if ((unshare_flags & CLONE_NEWNS) && ns) {
-		if (!capable(CAP_SYS_ADMIN))
-			return -EPERM;
-
-		*new_nsp = dup_mnt_ns(current, new_fs ? new_fs : current->fs);
-		if (!*new_nsp)
-			return -ENOMEM;
-	}
-
-	return 0;
-}
-
 /*
  * Unsharing of sighand is not supported yet
  */
@@ -1593,16 +1573,6 @@ static int unshare_semundo(unsigned long unshare_flags, struct sem_undo_list **n
 	return 0;
 }
 
-#ifndef CONFIG_IPC_NS
-static inline int unshare_ipcs(unsigned long flags, struct ipc_namespace **ns)
-{
-	if (flags & CLONE_NEWIPC)
-		return -EINVAL;
-
-	return 0;
-}
-#endif
-
 /*
  * unshare allows a process to 'unshare' part of the process
  * context which was originally shared using clone.  copy_*
@@ -1615,14 +1585,11 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 {
 	int err = 0;
 	struct fs_struct *fs, *new_fs = NULL;
-	struct mnt_namespace *ns, *new_ns = NULL;
 	struct sighand_struct *new_sigh = NULL;
 	struct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;
 	struct files_struct *fd, *new_fd = NULL;
 	struct sem_undo_list *new_ulist = NULL;
 	struct nsproxy *new_nsproxy = NULL, *old_nsproxy = NULL;
-	struct uts_namespace *uts, *new_uts = NULL;
-	struct ipc_namespace *ipc, *new_ipc = NULL;
 
 	check_unshare_flags(&unshare_flags);
 
@@ -1637,36 +1604,24 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		goto bad_unshare_out;
 	if ((err = unshare_fs(unshare_flags, &new_fs)))
 		goto bad_unshare_cleanup_thread;
-	if ((err = unshare_mnt_namespace(unshare_flags, &new_ns, new_fs)))
-		goto bad_unshare_cleanup_fs;
 	if ((err = unshare_sighand(unshare_flags, &new_sigh)))
-		goto bad_unshare_cleanup_ns;
+		goto bad_unshare_cleanup_fs;
 	if ((err = unshare_vm(unshare_flags, &new_mm)))
 		goto bad_unshare_cleanup_sigh;
 	if ((err = unshare_fd(unshare_flags, &new_fd)))
 		goto bad_unshare_cleanup_vm;
 	if ((err = unshare_semundo(unshare_flags, &new_ulist)))
 		goto bad_unshare_cleanup_fd;
-	if ((err = unshare_utsname(unshare_flags, &new_uts)))
+	if ((err = unshare_nsproxy_namespaces(unshare_flags, &new_nsproxy,
+			new_fs)))
 		goto bad_unshare_cleanup_semundo;
-	if ((err = unshare_ipcs(unshare_flags, &new_ipc)))
-		goto bad_unshare_cleanup_uts;
-
-	if (new_ns || new_uts || new_ipc) {
-		old_nsproxy = current->nsproxy;
-		new_nsproxy = dup_namespaces(old_nsproxy);
-		if (!new_nsproxy) {
-			err = -ENOMEM;
-			goto bad_unshare_cleanup_ipc;
-		}
-	}
 
-	if (new_fs || new_ns || new_mm || new_fd || new_ulist ||
-				new_uts || new_ipc) {
+	if (new_fs ||  new_mm || new_fd || new_ulist || new_nsproxy) {
 
 		task_lock(current);
 
 		if (new_nsproxy) {
+			old_nsproxy = current->nsproxy;
 			current->nsproxy = new_nsproxy;
 			new_nsproxy = old_nsproxy;
 		}
@@ -1677,12 +1632,6 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 			new_fs = fs;
 		}
 
-		if (new_ns) {
-			ns = current->nsproxy->mnt_ns;
-			current->nsproxy->mnt_ns = new_ns;
-			new_ns = ns;
-		}
-
 		if (new_mm) {
 			mm = current->mm;
 			active_mm = current->active_mm;
@@ -1698,32 +1647,12 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 			new_fd = fd;
 		}
 
-		if (new_uts) {
-			uts = current->nsproxy->uts_ns;
-			current->nsproxy->uts_ns = new_uts;
-			new_uts = uts;
-		}
-
-		if (new_ipc) {
-			ipc = current->nsproxy->ipc_ns;
-			current->nsproxy->ipc_ns = new_ipc;
-			new_ipc = ipc;
-		}
-
 		task_unlock(current);
 	}
 
 	if (new_nsproxy)
 		put_nsproxy(new_nsproxy);
 
-bad_unshare_cleanup_ipc:
-	if (new_ipc)
-		put_ipc_ns(new_ipc);
-
-bad_unshare_cleanup_uts:
-	if (new_uts)
-		put_uts_ns(new_uts);
-
 bad_unshare_cleanup_semundo:
 bad_unshare_cleanup_fd:
 	if (new_fd)
@@ -1738,10 +1667,6 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		if (atomic_dec_and_test(&new_sigh->count))
 			kmem_cache_free(sighand_cachep, new_sigh);
 
-bad_unshare_cleanup_ns:
-	if (new_ns)
-		put_mnt_ns(new_ns);
-
 bad_unshare_cleanup_fs:
 	if (new_fs)
 		put_fs_struct(new_fs);

commit 50953fe9e00ebbeffa032a565ab2f08312d51a87
Author: Christoph Lameter <clameter@sgi.com>
Date:   Sun May 6 14:50:16 2007 -0700

    slab allocators: Remove SLAB_DEBUG_INITIAL flag
    
    I have never seen a use of SLAB_DEBUG_INITIAL.  It is only supported by
    SLAB.
    
    I think its purpose was to have a callback after an object has been freed
    to verify that the state is the constructor state again?  The callback is
    performed before each freeing of an object.
    
    I would think that it is much easier to check the object state manually
    before the free.  That also places the check near the code object
    manipulation of the object.
    
    Also the SLAB_DEBUG_INITIAL callback is only performed if the kernel was
    compiled with SLAB debugging on.  If there would be code in a constructor
    handling SLAB_DEBUG_INITIAL then it would have to be conditional on
    SLAB_DEBUG otherwise it would just be dead code.  But there is no such code
    in the kernel.  I think SLUB_DEBUG_INITIAL is too problematic to make real
    use of, difficult to understand and there are easier ways to accomplish the
    same effect (i.e.  add debug code before kfree).
    
    There is a related flag SLAB_CTOR_VERIFY that is frequently checked to be
    clear in fs inode caches.  Remove the pointless checks (they would even be
    pointless without removeal of SLAB_DEBUG_INITIAL) from the fs constructors.
    
    This is the last slab flag that SLUB did not support.  Remove the check for
    unimplemented flags from SLUB.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ffccefb28b6a..b7d169def942 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1425,8 +1425,7 @@ static void sighand_ctor(void *data, struct kmem_cache *cachep, unsigned long fl
 {
 	struct sighand_struct *sighand = data;
 
-	if ((flags & (SLAB_CTOR_VERIFY | SLAB_CTOR_CONSTRUCTOR)) ==
-					SLAB_CTOR_CONSTRUCTOR)
+	if (flags & SLAB_CTOR_CONSTRUCTOR)
 		spin_lock_init(&sighand->siglock);
 }
 

commit d6dd61c831226f9cd7750885da04d360d6455101
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Wed May 2 19:27:14 2007 +0200

    [PATCH] x86: PARAVIRT: add hooks to intercept mm creation and destruction
    
    Add hooks to allow a paravirt implementation to track the lifetime of
    an mm.  Paravirtualization requires three hooks, but only two are
    needed in common code.  They are:
    
    arch_dup_mmap, which is called when a new mmap is created at fork
    
    arch_exit_mmap, which is called when the last process reference to an
      mm is dropped, which typically happens on exit and exec.
    
    The third hook is activate_mm, which is called from the arch-specific
    activate_mm() macro/function, and so doesn't need stub versions for
    other architectures.  It's called when an mm is first used.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Cc: linux-arch@vger.kernel.org
    Cc: James Bottomley <James.Bottomley@SteelEye.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6af959c034d8..ffccefb28b6a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -286,6 +286,8 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		if (retval)
 			goto out;
 	}
+	/* a new mm has just been created */
+	arch_dup_mmap(oldmm, mm);
 	retval = 0;
 out:
 	up_write(&mm->mmap_sem);

commit e29e175b0f40cffc86068156733def14a7a533ab
Author: Zilvinas Valinskas <zilvinas@wilibox.com>
Date:   Fri Mar 16 13:38:34 2007 -0800

    [PATCH] initialise pi_lock if CONFIG_RT_MUTEXES=N
    
    Fixes a bogus lockdep warning which causes lockdep to disable itself.
    
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d154cc786489..6af959c034d8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -933,8 +933,8 @@ asmlinkage long sys_set_tid_address(int __user *tidptr)
 
 static inline void rt_mutex_init_task(struct task_struct *p)
 {
-#ifdef CONFIG_RT_MUTEXES
 	spin_lock_init(&p->pi_lock);
+#ifdef CONFIG_RT_MUTEXES
 	plist_head_init(&p->pi_waiters, &p->pi_lock);
 	p->pi_blocked_on = NULL;
 #endif

commit c9cb2e3d7c9178ab75d0942f96abb3abe0369906
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 16 01:27:49 2007 -0800

    [PATCH] hrtimers: namespace and enum cleanup
    
    - hrtimers did not use the hrtimer_restart enum and relied on the implict
      int representation. Fix the prototypes and the functions using the enums.
    - Use seperate name spaces for the enumerations
    - Convert hrtimer_restart macro to inline function
    - Add comments
    
    No functional changes.
    
    [akpm@osdl.org: fix input driver]
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: john stultz <johnstul@us.ibm.com>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Cc: Dmitry Torokhov <dtor@mail.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0b6293d94d96..d154cc786489 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -858,7 +858,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	init_sigpending(&sig->shared_pending);
 	INIT_LIST_HEAD(&sig->posix_timers);
 
-	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_REL);
+	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	sig->it_real_incr.tv64 = 0;
 	sig->real_timer.function = it_real_fn;
 	sig->tsk = tsk;

commit ab521dc0f8e117fd808d3e425216864d60390500
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Feb 12 00:53:00 2007 -0800

    [PATCH] tty: update the tty layer to work with struct pid
    
    Of kernel subsystems that work with pids the tty layer is probably the largest
    consumer.  But it has the nice virtue that the assiation with a session only
    lasts until the session leader exits.  Which means that no reference counting
    is required.  So using struct pid winds up being a simple optimization to
    avoid hash table lookups.
    
    In the long term the use of pid_nr also ensures that when we have multiple pid
    spaces mixed everything will work correctly.
    
    Signed-off-by: Eric W. Biederman <eric@maxwell.lnxi.com>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 80284eb488ce..0b6293d94d96 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -869,7 +869,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	sig->it_prof_incr = cputime_zero;
 
 	sig->leader = 0;	/* session leadership doesn't inherit */
-	sig->tty_old_pgrp = 0;
+	sig->tty_old_pgrp = NULL;
 
 	sig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;
 	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;

commit 4b98d11b40f03382918796f3c5c936d5495d20a4
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Sat Feb 10 01:46:45 2007 -0800

    [PATCH] ifdef ->rchar, ->wchar, ->syscr, ->syscw from task_struct
    
    They are fat: 4x8 bytes in task_struct.
    They are uncoditionally updated in every fork, read, write and sendfile.
    They are used only if you have some "extended acct fields feature".
    
    And please, please, please, read(2) knows about bytes, not characters,
    why it is called "rchar"?
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d57118da73ff..80284eb488ce 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1038,10 +1038,12 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->utime = cputime_zero;
 	p->stime = cputime_zero;
  	p->sched_time = 0;
+#ifdef CONFIG_TASK_XACCT
 	p->rchar = 0;		/* I/O counter: bytes read */
 	p->wchar = 0;		/* I/O counter: bytes written */
 	p->syscr = 0;		/* I/O counter: read syscalls */
 	p->syscw = 0;		/* I/O counter: write syscalls */
+#endif
 	task_io_accounting_init(p);
 	acct_clear_integrals(p);
 

commit 9abcf40b1d1443e6f0ef86e6a822193142a34abc
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Thu Feb 1 13:52:48 2007 +0000

    [PATCH] fork_idle() should be __cpuinit, not __devinit
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fc723e595cd5..d57118da73ff 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1313,7 +1313,7 @@ noinline struct pt_regs * __devinit __attribute__((weak)) idle_regs(struct pt_re
 	return regs;
 }
 
-struct task_struct * __devinit fork_idle(int cpu)
+struct task_struct * __cpuinit fork_idle(int cpu)
 {
 	struct task_struct *task;
 	struct pt_regs regs;

commit 444f378b237a0f728f5c4aba752c08d13c209344
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Tue Jan 30 13:35:18 2007 -0800

    Revert "[PATCH] namespaces: fix exit race by splitting exit"
    
    This reverts commit 7a238fcba0629b6f2edbcd37458bae56fcf36be5 in
    preparation for a better and simpler fix proposed by Eric Biederman
    (and fixed up by Serge Hallyn)
    
    Acked-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4cf868458f06..fc723e595cd5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1265,7 +1265,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	return p;
 
 bad_fork_cleanup_namespaces:
-	put_and_finalize_nsproxy(p->nsproxy);
+	exit_task_namespaces(p);
 bad_fork_cleanup_keys:
 	exit_keys(p);
 bad_fork_cleanup_mm:
@@ -1711,7 +1711,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	}
 
 	if (new_nsproxy)
-		put_and_finalize_nsproxy(new_nsproxy);
+		put_nsproxy(new_nsproxy);
 
 bad_unshare_cleanup_ipc:
 	if (new_ipc)

commit 7a238fcba0629b6f2edbcd37458bae56fcf36be5
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Jan 29 13:19:40 2007 -0800

    [PATCH] namespaces: fix exit race by splitting exit
    
    Fix exit race by splitting the nsproxy putting into two pieces.  First
    piece reduces the nsproxy refcount.  If we dropped the last reference, then
    it puts the mnt_ns, and returns the nsproxy as a hint to the caller.  Else
    it returns NULL.  The second piece of exiting task namespaces sets
    tsk->nsproxy to NULL, and drops the references to other namespaces and
    frees the nsproxy only if an nsproxy was passed in.
    
    A little awkward and should probably be reworked, but hopefully it fixes
    the NFS oops.
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Daniel Hokka Zakrisson <daniel@hozac.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fc723e595cd5..4cf868458f06 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1265,7 +1265,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	return p;
 
 bad_fork_cleanup_namespaces:
-	exit_task_namespaces(p);
+	put_and_finalize_nsproxy(p->nsproxy);
 bad_fork_cleanup_keys:
 	exit_keys(p);
 bad_fork_cleanup_mm:
@@ -1711,7 +1711,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	}
 
 	if (new_nsproxy)
-		put_nsproxy(new_nsproxy);
+		put_and_finalize_nsproxy(new_nsproxy);
 
 bad_unshare_cleanup_ipc:
 	if (new_ipc)

commit ec8c0446b6e2b67b5c8813eb517f4bf00efa99a9
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Dec 12 17:14:57 2006 +0000

    [PATCH] Optimize D-cache alias handling on fork
    
    Virtually index, physically tagged cache architectures can get away
    without cache flushing when forking.  This patch adds a new cache
    flushing function flush_cache_dup_mm(struct mm_struct *) which for the
    moment I've implemented to do the same thing on all architectures
    except on MIPS where it's a no-op.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d16c566eb645..fc723e595cd5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -203,7 +203,7 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 	struct mempolicy *pol;
 
 	down_write(&oldmm->mmap_sem);
-	flush_cache_mm(oldmm);
+	flush_cache_dup_mm(oldmm);
 	/*
 	 * Not linked in yet - no deadlock potential:
 	 */

commit 4fd45812cbe875a620c86a096a5d46c742694b7e
Author: Vadim Lobanov <vlobanov@speakeasy.net>
Date:   Sun Dec 10 02:21:17 2006 -0800

    [PATCH] fdtable: Remove the free_files field
    
    An fdtable can either be embedded inside a files_struct or standalone (after
    being expanded).  When an fdtable is being discarded after all RCU references
    to it have expired, we must either free it directly, in the standalone case,
    or free the files_struct it is contained within, in the embedded case.
    
    Currently the free_files field controls this behavior, but we can get rid of
    it entirely, as all the necessary information is already recorded.  We can
    distinguish embedded and standalone fdtables using max_fds, and if it is
    embedded we can divine the relevant files_struct using container_of().
    
    Signed-off-by: Vadim Lobanov <vlobanov@speakeasy.net>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index aba595424f78..d16c566eb645 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -645,7 +645,6 @@ static struct files_struct *alloc_files(void)
 	fdt->open_fds = (fd_set *)&newf->open_fds_init;
 	fdt->fd = &newf->fd_array[0];
 	INIT_RCU_HEAD(&fdt->rcu);
-	fdt->free_files = NULL;
 	fdt->next = NULL;
 	rcu_assign_pointer(newf->fdt, fdt);
 out:

commit bbea9f69668a3d0cf9feba15a724cd02896f8675
Author: Vadim Lobanov <vlobanov@speakeasy.net>
Date:   Sun Dec 10 02:21:12 2006 -0800

    [PATCH] fdtable: Make fdarray and fdsets equal in size
    
    Currently, each fdtable supports three dynamically-sized arrays of data: the
    fdarray and two fdsets.  The code allows the number of fds supported by the
    fdarray (fdtable->max_fds) to differ from the number of fds supported by each
    of the fdsets (fdtable->max_fdset).
    
    In practice, it is wasteful for these two sizes to differ: whenever we hit a
    limit on the smaller-capacity structure, we will reallocate the entire fdtable
    and all the dynamic arrays within it, so any delta in the memory used by the
    larger-capacity structure will never be touched at all.
    
    Rather than hogging this excess, we shouldn't even allocate it in the first
    place, and keep the capacities of the fdarray and the fdsets equal.  This
    patch removes fdtable->max_fdset.  As an added bonus, most of the supporting
    code becomes simpler.
    
    Signed-off-by: Vadim Lobanov <vlobanov@speakeasy.net>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 30eab4f063cd..aba595424f78 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -614,7 +614,7 @@ static inline int copy_fs(unsigned long clone_flags, struct task_struct * tsk)
 
 static int count_open_files(struct fdtable *fdt)
 {
-	int size = fdt->max_fdset;
+	int size = fdt->max_fds;
 	int i;
 
 	/* Find the last open fd */
@@ -641,7 +641,6 @@ static struct files_struct *alloc_files(void)
 	newf->next_fd = 0;
 	fdt = &newf->fdtab;
 	fdt->max_fds = NR_OPEN_DEFAULT;
-	fdt->max_fdset = EMBEDDED_FD_SET_SIZE;
 	fdt->close_on_exec = (fd_set *)&newf->close_on_exec_init;
 	fdt->open_fds = (fd_set *)&newf->open_fds_init;
 	fdt->fd = &newf->fd_array[0];
@@ -662,7 +661,7 @@ static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 {
 	struct files_struct *newf;
 	struct file **old_fds, **new_fds;
-	int open_files, size, i, expand;
+	int open_files, size, i;
 	struct fdtable *old_fdt, *new_fdt;
 
 	*errorp = -ENOMEM;
@@ -673,25 +672,14 @@ static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 	spin_lock(&oldf->file_lock);
 	old_fdt = files_fdtable(oldf);
 	new_fdt = files_fdtable(newf);
-	size = old_fdt->max_fdset;
 	open_files = count_open_files(old_fdt);
-	expand = 0;
 
 	/*
-	 * Check whether we need to allocate a larger fd array or fd set.
-	 * Note: we're not a clone task, so the open count won't  change.
+	 * Check whether we need to allocate a larger fd array and fd set.
+	 * Note: we're not a clone task, so the open count won't change.
 	 */
-	if (open_files > new_fdt->max_fdset) {
-		new_fdt->max_fdset = 0;
-		expand = 1;
-	}
 	if (open_files > new_fdt->max_fds) {
 		new_fdt->max_fds = 0;
-		expand = 1;
-	}
-
-	/* if the old fdset gets grown now, we'll only copy up to "size" fds */
-	if (expand) {
 		spin_unlock(&oldf->file_lock);
 		spin_lock(&newf->file_lock);
 		*errorp = expand_files(newf, open_files-1);
@@ -739,8 +727,8 @@ static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 	/* This is long word aligned thus could use a optimized version */ 
 	memset(new_fds, 0, size); 
 
-	if (new_fdt->max_fdset > open_files) {
-		int left = (new_fdt->max_fdset-open_files)/8;
+	if (new_fdt->max_fds > open_files) {
+		int left = (new_fdt->max_fds-open_files)/8;
 		int start = open_files / (8 * sizeof(unsigned long));
 
 		memset(&new_fdt->open_fds->fds_bits[start], 0, left);

commit f3d19c90fb117a5f080310a4592929aa8e1ad8e9
Author: Vadim Lobanov <vlobanov@speakeasy.net>
Date:   Sun Dec 10 02:21:09 2006 -0800

    [PATCH] fdtable: Delete pointless code in dup_fd()
    
    The dup_fd() function creates a new files_struct and fdtable embedded inside
    that files_struct, and then possibly expands the fdtable using expand_files().
    
    The out_release error path is invoked when expand_files() returns an error
    code.  However, when this attempt to expand fails, the fdtable is left in its
    original embedded form, so it is pointless to try to free the associated
    fdarray and fdsets.
    
    Signed-off-by: Vadim Lobanov <vlobanov@speakeasy.net>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 086e172d0d3d..30eab4f063cd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -711,8 +711,10 @@ static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 	old_fds = old_fdt->fd;
 	new_fds = new_fdt->fd;
 
-	memcpy(new_fdt->open_fds->fds_bits, old_fdt->open_fds->fds_bits, open_files/8);
-	memcpy(new_fdt->close_on_exec->fds_bits, old_fdt->close_on_exec->fds_bits, open_files/8);
+	memcpy(new_fdt->open_fds->fds_bits,
+		old_fdt->open_fds->fds_bits, open_files/8);
+	memcpy(new_fdt->close_on_exec->fds_bits,
+		old_fdt->close_on_exec->fds_bits, open_files/8);
 
 	for (i = open_files; i != 0; i--) {
 		struct file *f = *old_fds++;
@@ -745,14 +747,11 @@ static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 		memset(&new_fdt->close_on_exec->fds_bits[start], 0, left);
 	}
 
-out:
 	return newf;
 
 out_release:
-	free_fdset (new_fdt->close_on_exec, new_fdt->max_fdset);
-	free_fdset (new_fdt->open_fds, new_fdt->max_fdset);
-	free_fd_array(new_fdt->fd, new_fdt->max_fds);
 	kmem_cache_free(files_cachep, newf);
+out:
 	return NULL;
 }
 

commit 7c3ab7381e79dfc7db14a67c6f4f3285664e1ec2
Author: Andrew Morton <akpm@osdl.org>
Date:   Sun Dec 10 02:19:19 2006 -0800

    [PATCH] io-accounting: core statistics
    
    The present per-task IO accounting isn't very useful.  It simply counts the
    number of bytes passed into read() and write().  So if a process reads 1MB
    from an already-cached file, it is accused of having performed 1MB of I/O,
    which is wrong.
    
    (David Wright had some comments on the applicability of the present logical IO accounting:
    
      For billing purposes it is useless but for workload analysis it is very
      useful
    
      read_bytes/read_calls  average read request size
      write_bytes/write_calls average write request size
    
      read_bytes/read_blocks ie logical/physical can indicate hit rate or thrashing
      write_bytes/write_blocks  ie logical/physical  guess since pdflush writes can
                                                    be missed
    
      I often look for logical larger than physical to see filesystem cache
      problems.  And the bytes/cpusec can help find applications that are
      dominating the cache and causing slow interactive response from page cache
      contention.
    
      I want to find the IO intensive applications and make sure they are doing
      efficient IO.  Thus the acctcms(sysV) or csacms command would give the high
      IO commands).
    
    This patchset adds new accounting which tries to be more accurate.  We account
    for three things:
    
    reads:
    
      attempt to count the number of bytes which this process really did cause
      to be fetched from the storage layer.  Done at the submit_bio() level, so it
      is accurate for block-backed filesystems.  I also attempt to wire up NFS and
      CIFS.
    
    writes:
    
      attempt to count the number of bytes which this process caused to be sent
      to the storage layer.  This is done at page-dirtying time.
    
      The big inaccuracy here is truncate.  If a process writes 1MB to a file
      and then deletes the file, it will in fact perform no writeout.  But it will
      have been accounted as having caused 1MB of write.
    
      So...
    
    cancelled_writes:
    
      account the number of bytes which this process caused to not happen, by
      truncating pagecache.
    
      We _could_ just subtract this from the process's `write' accounting.  But
      that means that some processes would be reported to have done negative
      amounts of write IO, which is silly.
    
      So we just report the raw number and punt this decision up to userspace.
    
    Now, we _could_ account for writes at the physical I/O level.  But
    
    - This would require that we track memory-dirtying tasks at the per-page
      level (would require a new pointer in struct page).
    
    - It would mean that IO statistics for a process are usually only available
      long after that process has exitted.  Which means that we probably cannot
      communicate this info via taskstats.
    
    This patch:
    
    Wire up the kernel-private data structures and the accessor functions to
    manipulate them.
    
    Cc: Jay Lan <jlan@sgi.com>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Chris Sturtivant <csturtiv@sgi.com>
    Cc: Tony Ernst <tee@sgi.com>
    Cc: Guillaume Thouvenin <guillaume.thouvenin@bull.net>
    Cc: David Wright <daw@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8c859eef8e6a..086e172d0d3d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -36,6 +36,7 @@
 #include <linux/syscalls.h>
 #include <linux/jiffies.h>
 #include <linux/futex.h>
+#include <linux/task_io_accounting_ops.h>
 #include <linux/rcupdate.h>
 #include <linux/ptrace.h>
 #include <linux/mount.h>
@@ -1055,6 +1056,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->wchar = 0;		/* I/O counter: bytes written */
 	p->syscr = 0;		/* I/O counter: read syscalls */
 	p->syscw = 0;		/* I/O counter: write syscalls */
+	task_io_accounting_init(p);
 	acct_clear_integrals(p);
 
  	p->it_virt_expires = cputime_zero;

commit 6b3286ed1169d74fea401367d6d4d6c6ec758a81
Author: Kirill Korotaev <dev@sw.ru>
Date:   Fri Dec 8 02:37:56 2006 -0800

    [PATCH] rename struct namespace to struct mnt_namespace
    
    Rename 'struct namespace' to 'struct mnt_namespace' to avoid confusion with
    other namespaces being developped for the containers : pid, uts, ipc, etc.
    'namespace' variables and attributes are also renamed to 'mnt_ns'
    
    Signed-off-by: Kirill Korotaev <dev@sw.ru>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 60d2644bfe85..8c859eef8e6a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -18,7 +18,7 @@
 #include <linux/module.h>
 #include <linux/vmalloc.h>
 #include <linux/completion.h>
-#include <linux/namespace.h>
+#include <linux/mnt_namespace.h>
 #include <linux/personality.h>
 #include <linux/mempolicy.h>
 #include <linux/sem.h>
@@ -1525,17 +1525,18 @@ static int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)
 }
 
 /*
- * Unshare the namespace structure if it is being shared
+ * Unshare the mnt_namespace structure if it is being shared
  */
-static int unshare_namespace(unsigned long unshare_flags, struct namespace **new_nsp, struct fs_struct *new_fs)
+static int unshare_mnt_namespace(unsigned long unshare_flags,
+		struct mnt_namespace **new_nsp, struct fs_struct *new_fs)
 {
-	struct namespace *ns = current->nsproxy->namespace;
+	struct mnt_namespace *ns = current->nsproxy->mnt_ns;
 
 	if ((unshare_flags & CLONE_NEWNS) && ns) {
 		if (!capable(CAP_SYS_ADMIN))
 			return -EPERM;
 
-		*new_nsp = dup_namespace(current, new_fs ? new_fs : current->fs);
+		*new_nsp = dup_mnt_ns(current, new_fs ? new_fs : current->fs);
 		if (!*new_nsp)
 			return -ENOMEM;
 	}
@@ -1623,7 +1624,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 {
 	int err = 0;
 	struct fs_struct *fs, *new_fs = NULL;
-	struct namespace *ns, *new_ns = NULL;
+	struct mnt_namespace *ns, *new_ns = NULL;
 	struct sighand_struct *new_sigh = NULL;
 	struct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;
 	struct files_struct *fd, *new_fd = NULL;
@@ -1645,7 +1646,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		goto bad_unshare_out;
 	if ((err = unshare_fs(unshare_flags, &new_fs)))
 		goto bad_unshare_cleanup_thread;
-	if ((err = unshare_namespace(unshare_flags, &new_ns, new_fs)))
+	if ((err = unshare_mnt_namespace(unshare_flags, &new_ns, new_fs)))
 		goto bad_unshare_cleanup_fs;
 	if ((err = unshare_sighand(unshare_flags, &new_sigh)))
 		goto bad_unshare_cleanup_ns;
@@ -1686,8 +1687,8 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		}
 
 		if (new_ns) {
-			ns = current->nsproxy->namespace;
-			current->nsproxy->namespace = new_ns;
+			ns = current->nsproxy->mnt_ns;
+			current->nsproxy->mnt_ns = new_ns;
 			new_ns = ns;
 		}
 
@@ -1748,7 +1749,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 
 bad_unshare_cleanup_ns:
 	if (new_ns)
-		put_namespace(new_ns);
+		put_mnt_ns(new_ns);
 
 bad_unshare_cleanup_fs:
 	if (new_fs)

commit 1ec320afdc9552c92191d5f89fcd1ebe588334ca
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Fri Dec 8 02:37:55 2006 -0800

    [PATCH] add process_session() helper routine: deprecate old field
    
    Add an anonymous union and ((deprecated)) to catch direct usage of the
    session field.
    
    [akpm@osdl.org: fix various missed conversions]
    [jdike@addtoit.com: fix UML bug]
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 298c4d6ab512..60d2644bfe85 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1259,7 +1259,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		if (thread_group_leader(p)) {
 			p->signal->tty = current->signal->tty;
 			p->signal->pgrp = process_group(current);
-			p->signal->session = process_session(current);
+			set_signal_session(p->signal, process_session(current));
 			attach_pid(p, PIDTYPE_PGID, process_group(p));
 			attach_pid(p, PIDTYPE_SID, process_session(p));
 

commit 937949d9edbf4049bd41af6c9f92c26280584564
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Fri Dec 8 02:37:54 2006 -0800

    [PATCH] add process_session() helper routine
    
    Replace occurences of task->signal->session by a new process_session() helper
    routine.
    
    It will be useful for pid namespaces to abstract the session pid number.
    
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 597707819327..298c4d6ab512 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1259,9 +1259,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		if (thread_group_leader(p)) {
 			p->signal->tty = current->signal->tty;
 			p->signal->pgrp = process_group(current);
-			p->signal->session = current->signal->session;
+			p->signal->session = process_session(current);
 			attach_pid(p, PIDTYPE_PGID, process_group(p));
-			attach_pid(p, PIDTYPE_SID, p->signal->session);
+			attach_pid(p, PIDTYPE_SID, process_session(p));
 
 			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;

commit f3a43f3f64bff8e205c3702f6b4804d66e306848
Author: Josef "Jeff" Sipek <jsipek@cs.sunysb.edu>
Date:   Fri Dec 8 02:36:43 2006 -0800

    [PATCH] kernel: change uses of f_{dentry, vfsmnt} to use f_path
    
    Change all the uses of f_{dentry,vfsmnt} to f_path.{dentry,mnt} in
    linux/kernel/.
    
    Signed-off-by: Josef "Jeff" Sipek <jsipek@cs.sunysb.edu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f387a1393ca5..597707819327 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -252,7 +252,7 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		anon_vma_link(tmp);
 		file = tmp->vm_file;
 		if (file) {
-			struct inode *inode = file->f_dentry->d_inode;
+			struct inode *inode = file->f_path.dentry->d_inode;
 			get_file(file);
 			if (tmp->vm_flags & VM_DENYWRITE)
 				atomic_dec(&inode->i_writecount);

commit dae3c5a0b7052ad7dd9fa78c51ecfab828c5007b
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Dec 8 02:36:09 2006 -0800

    [PATCH] sys_unshare: remove a broken CLONE_SIGHAND code
    
    sys_unshare(CLONE_SIGHAND) is broken, the code under 'if (new_sigh)' is
    never executed but very wrong. Just remove it to avoid a confusion,
    task_lock() has nothing to do with ->sighand changing.
    
    Also, change the comment in unshare_sighand(). Yes, CLONE_THREAD implies
    CLONE_SIGHAND, but still it looks confusing. Also, we don't need to check
    current->sighand != NULL.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7f2e31ba33af..f387a1393ca5 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1544,15 +1544,13 @@ static int unshare_namespace(unsigned long unshare_flags, struct namespace **new
 }
 
 /*
- * Unsharing of sighand for tasks created with CLONE_SIGHAND is not
- * supported yet
+ * Unsharing of sighand is not supported yet
  */
 static int unshare_sighand(unsigned long unshare_flags, struct sighand_struct **new_sighp)
 {
 	struct sighand_struct *sigh = current->sighand;
 
-	if ((unshare_flags & CLONE_SIGHAND) &&
-	    (sigh && atomic_read(&sigh->count) > 1))
+	if ((unshare_flags & CLONE_SIGHAND) && atomic_read(&sigh->count) > 1)
 		return -EINVAL;
 	else
 		return 0;
@@ -1626,7 +1624,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	int err = 0;
 	struct fs_struct *fs, *new_fs = NULL;
 	struct namespace *ns, *new_ns = NULL;
-	struct sighand_struct *sigh, *new_sigh = NULL;
+	struct sighand_struct *new_sigh = NULL;
 	struct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;
 	struct files_struct *fd, *new_fd = NULL;
 	struct sem_undo_list *new_ulist = NULL;
@@ -1671,7 +1669,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		}
 	}
 
-	if (new_fs || new_ns || new_sigh || new_mm || new_fd || new_ulist ||
+	if (new_fs || new_ns || new_mm || new_fd || new_ulist ||
 				new_uts || new_ipc) {
 
 		task_lock(current);
@@ -1693,12 +1691,6 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 			new_ns = ns;
 		}
 
-		if (new_sigh) {
-			sigh = current->sighand;
-			rcu_assign_pointer(current->sighand, new_sigh);
-			new_sigh = sigh;
-		}
-
 		if (new_mm) {
 			mm = current->mm;
 			active_mm = current->active_mm;

commit 4522d58275f124105819723e24e912c8e5bf3cdd
Merge: 6cf24f031bc9 64a26a731235
Author: Linus Torvalds <torvalds@woody.osdl.org>
Date:   Thu Dec 7 08:59:11 2006 -0800

    Merge branch 'for-linus' of git://one.firstfloor.org/home/andi/git/linux-2.6
    
    * 'for-linus' of git://one.firstfloor.org/home/andi/git/linux-2.6: (156 commits)
      [PATCH] x86-64: Export smp_call_function_single
      [PATCH] i386: Clean up smp_tune_scheduling()
      [PATCH] unwinder: move .eh_frame to RODATA
      [PATCH] unwinder: fully support linker generated .eh_frame_hdr section
      [PATCH] x86-64: don't use set_irq_regs()
      [PATCH] x86-64: check vector in setup_ioapic_dest to verify if need setup_IO_APIC_irq
      [PATCH] x86-64: Make ix86 default to HIGHMEM4G instead of NOHIGHMEM
      [PATCH] i386: replace kmalloc+memset with kzalloc
      [PATCH] x86-64: remove remaining pc98 code
      [PATCH] x86-64: remove unused variable
      [PATCH] x86-64: Fix constraints in atomic_add_return()
      [PATCH] x86-64: fix asm constraints in i386 atomic_add_return
      [PATCH] x86-64: Correct documentation for bzImage protocol v2.05
      [PATCH] x86-64: replace kmalloc+memset with kzalloc in MTRR code
      [PATCH] x86-64: Fix numaq build error
      [PATCH] x86-64: include/asm-x86_64/cpufeature.h isn't a userspace header
      [PATCH] unwinder: Add debugging output to the Dwarf2 unwinder
      [PATCH] x86-64: Clarify error message in GART code
      [PATCH] x86-64: Fix interrupt race in idle callback (3rd try)
      [PATCH] x86-64: Remove unwind stack pointer alignment forcing again
      ...
    
    Fixed conflict in include/linux/uaccess.h manually
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 34ec12349c8a9505adc59d72f92b4595bc2483ff
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Dec 6 20:36:52 2006 -0800

    [PATCH] taskstats: cleanup ->signal->stats allocation
    
    Allocate ->signal->stats on demand in taskstats_exit(), this allows us to
    remove taskstats_tgid_alloc() (the last non-trivial inline) from taskstat's
    public interface.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f37980df1d58..658838148647 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -847,7 +847,6 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	if (clone_flags & CLONE_THREAD) {
 		atomic_inc(&current->signal->count);
 		atomic_inc(&current->signal->live);
-		taskstats_tgid_alloc(current);
 		return 0;
 	}
 	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);

commit fec1d0115240593b39898289e6e1413ea6e44a84
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Dec 6 20:36:34 2006 -0800

    [PATCH] Disable CLONE_CHILD_CLEARTID for abnormal exit
    
    The CLONE_CHILD_CLEARTID flag is used by NPTL to have its threads
    communicate via memory/futex when they exit, so pthread_join can
    synchronize using a simple futex wait.  The word of user memory where NPTL
    stores a thread's own TID is what it passes; this gets reset to zero at
    thread exit.
    
    It is not desireable to touch this user memory when threads are dying due
    to a fatal signal.  A core dump is more usefully representative of the
    dying program state if the threads live at the time of the crash have their
    NPTL data structures unperturbed.  The userland expectation of
    CLONE_CHILD_CLEARTID has only ever been that it works for a thread making
    an _exit system call.
    
    This problem was identified by Ernie Petrides <petrides@redhat.com>.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Ernie Petrides <petrides@redhat.com>
    Cc: Jakub Jelinek <jakub@redhat.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Ulrich Drepper <drepper@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2cf74edd3295..f37980df1d58 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -448,7 +448,16 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 		tsk->vfork_done = NULL;
 		complete(vfork_done);
 	}
-	if (tsk->clear_child_tid && atomic_read(&mm->mm_users) > 1) {
+
+	/*
+	 * If we're exiting normally, clear a user-space tid field if
+	 * requested.  We leave this alone when dying by signal, to leave
+	 * the value intact in a core dump, and to save the unnecessary
+	 * trouble otherwise.  Userland only wants this done for a sys_exit.
+	 */
+	if (tsk->clear_child_tid
+	    && !(tsk->flags & PF_SIGNALED)
+	    && atomic_read(&mm->mm_users) > 1) {
 		u32 __user * tidptr = tsk->clear_child_tid;
 		tsk->clear_child_tid = NULL;
 

commit e18b890bb0881bbab6f4f1a6cd20d9c60d66b003
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:20 2006 -0800

    [PATCH] slab: remove kmem_cache_t
    
    Replace all uses of kmem_cache_t with struct kmem_cache.
    
    The patch was generated using the following script:
    
            #!/bin/sh
            #
            # Replace one string by another in all the kernel sources.
            #
    
            set -e
    
            for file in `find * -name "*.c" -o -name "*.h"|xargs grep -l $1`; do
                    quilt add $file
                    sed -e "1,\$s/$1/$2/g" $file >/tmp/$$
                    mv /tmp/$$ $file
                    quilt refresh
            done
    
    The script was run like this
    
            sh replace kmem_cache_t "struct kmem_cache"
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 711aa5f10da7..2cf74edd3295 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -82,26 +82,26 @@ int nr_processes(void)
 #ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
 # define alloc_task_struct()	kmem_cache_alloc(task_struct_cachep, GFP_KERNEL)
 # define free_task_struct(tsk)	kmem_cache_free(task_struct_cachep, (tsk))
-static kmem_cache_t *task_struct_cachep;
+static struct kmem_cache *task_struct_cachep;
 #endif
 
 /* SLAB cache for signal_struct structures (tsk->signal) */
-static kmem_cache_t *signal_cachep;
+static struct kmem_cache *signal_cachep;
 
 /* SLAB cache for sighand_struct structures (tsk->sighand) */
-kmem_cache_t *sighand_cachep;
+struct kmem_cache *sighand_cachep;
 
 /* SLAB cache for files_struct structures (tsk->files) */
-kmem_cache_t *files_cachep;
+struct kmem_cache *files_cachep;
 
 /* SLAB cache for fs_struct structures (tsk->fs) */
-kmem_cache_t *fs_cachep;
+struct kmem_cache *fs_cachep;
 
 /* SLAB cache for vm_area_struct structures */
-kmem_cache_t *vm_area_cachep;
+struct kmem_cache *vm_area_cachep;
 
 /* SLAB cache for mm_struct structures (tsk->mm) */
-static kmem_cache_t *mm_cachep;
+static struct kmem_cache *mm_cachep;
 
 void free_task(struct task_struct *tsk)
 {
@@ -1421,7 +1421,7 @@ long do_fork(unsigned long clone_flags,
 #define ARCH_MIN_MMSTRUCT_ALIGN 0
 #endif
 
-static void sighand_ctor(void *data, kmem_cache_t *cachep, unsigned long flags)
+static void sighand_ctor(void *data, struct kmem_cache *cachep, unsigned long flags)
 {
 	struct sighand_struct *sighand = data;
 

commit e94b1766097d53e6f3ccfb36c8baa562ffeda3fc
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:17 2006 -0800

    [PATCH] slab: remove SLAB_KERNEL
    
    SLAB_KERNEL is an alias of GFP_KERNEL.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 5678e6c61ef2..711aa5f10da7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -237,7 +237,7 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 				goto fail_nomem;
 			charge = len;
 		}
-		tmp = kmem_cache_alloc(vm_area_cachep, SLAB_KERNEL);
+		tmp = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
 		if (!tmp)
 			goto fail_nomem;
 		*tmp = *mpnt;
@@ -319,7 +319,7 @@ static inline void mm_free_pgd(struct mm_struct * mm)
 
  __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
 
-#define allocate_mm()	(kmem_cache_alloc(mm_cachep, SLAB_KERNEL))
+#define allocate_mm()	(kmem_cache_alloc(mm_cachep, GFP_KERNEL))
 #define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
 
 #include <linux/init_task.h>
@@ -621,7 +621,7 @@ static struct files_struct *alloc_files(void)
 	struct files_struct *newf;
 	struct fdtable *fdt;
 
-	newf = kmem_cache_alloc(files_cachep, SLAB_KERNEL);
+	newf = kmem_cache_alloc(files_cachep, GFP_KERNEL);
 	if (!newf)
 		goto out;
 

commit 7602bdf2fd14a40dd9b104e516fdc05e1bd17952
Author: Ashwin Chaugule <ashwin.chaugule@celunite.com>
Date:   Wed Dec 6 20:31:57 2006 -0800

    [PATCH] new scheme to preempt swap token
    
    The new swap token patches replace the current token traversal algo.  The old
    algo had a crude timeout parameter that was used to handover the token from
    one task to another.  This algo, transfers the token to the tasks that are in
    need of the token.  The urgency for the token is based on the number of times
    a task is required to swap-in pages.  Accordingly, the priority of a task is
    incremented if it has been badly affected due to swap-outs.  To ensure that
    the token doesnt bounce around rapidly, the token holders are given a priority
    boost.  The priority of tasks is also decremented, if their rate of swap-in's
    keeps reducing.  This way, the condition to check whether to pre-empt the swap
    token, is a matter of comparing two task's priority fields.
    
    [akpm@osdl.org: cleanups]
    Signed-off-by: Ashwin Chaugule <ashwin.chaugule@celunite.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8cdd3e72ba55..5678e6c61ef2 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -479,6 +479,10 @@ static struct mm_struct *dup_mm(struct task_struct *tsk)
 
 	memcpy(mm, oldmm, sizeof(*mm));
 
+	/* Initializing for Swap token stuff */
+	mm->token_priority = 0;
+	mm->last_interval = 0;
+
 	if (!mm_init(mm))
 		goto fail_nomem;
 
@@ -542,6 +546,10 @@ static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
 		goto fail_nomem;
 
 good_mm:
+	/* Initializing for Swap token stuff */
+	mm->token_priority = 0;
+	mm->last_interval = 0;
+
 	tsk->mm = mm;
 	tsk->active_mm = mm;
 	return 0;

commit f95d47caae5302a63d92be9a0292abc90e2a14e1
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Thu Dec 7 02:14:02 2006 +0100

    [PATCH] i386: Use %gs as the PDA base-segment in the kernel
    
    This patch is the meat of the PDA change.  This patch makes several related
    changes:
    
    1: Most significantly, %gs is now used in the kernel.  This means that on
       entry, the old value of %gs is saved away, and it is reloaded with
       __KERNEL_PDA.
    
    2: entry.S constructs the stack in the shape of struct pt_regs, and this
       is passed around the kernel so that the process's saved register
       state can be accessed.
    
       Unfortunately struct pt_regs doesn't currently have space for %gs
       (or %fs). This patch extends pt_regs to add space for gs (no space
       is allocated for %fs, since it won't be used, and it would just
       complicate the code in entry.S to work around the space).
    
    3: Because %gs is now saved on the stack like %ds, %es and the integer
       registers, there are a number of places where it no longer needs to
       be handled specially; namely context switch, and saving/restoring the
       register state in a signal context.
    
    4: And since kernel threads run in kernel space and call normal kernel
       code, they need to be created with their %gs == __KERNEL_PDA.
    
    Signed-off-by: Jeremy Fitzhardinge <jeremy@xensource.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Cc: Chuck Ebbert <76306.1226@compuserve.com>
    Cc: Zachary Amsden <zach@vmware.com>
    Cc: Jan Beulich <jbeulich@novell.com>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8cdd3e72ba55..fd22245e3881 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1303,7 +1303,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	return ERR_PTR(retval);
 }
 
-struct pt_regs * __devinit __attribute__((weak)) idle_regs(struct pt_regs *regs)
+noinline struct pt_regs * __devinit __attribute__((weak)) idle_regs(struct pt_regs *regs)
 {
 	memset(regs, 0, sizeof(struct pt_regs));
 	return regs;

commit 753ca4f312a4b26940e4731b4fa5dbbbbcc77e97
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Sat Nov 25 11:09:34 2006 -0800

    [PATCH] fix copy_process() error check
    
    The return value of copy_process() should be checked by IS_ERR().
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3da978eec791..8cdd3e72ba55 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1315,9 +1315,8 @@ struct task_struct * __devinit fork_idle(int cpu)
 	struct pt_regs regs;
 
 	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL, NULL, 0);
-	if (!task)
-		return ERR_PTR(-ENOMEM);
-	init_idle(task, cpu);
+	if (!IS_ERR(task))
+		init_idle(task, cpu);
 
 	return task;
 }

commit 9a3a04ac386f44175b6a4142eaeab3d4170a57f3
Author: Linus Torvalds <torvalds@woody.osdl.org>
Date:   Tue Nov 14 15:20:51 2006 -0800

    Revert "[PATCH] fix Data Acess error in dup_fd"
    
    This reverts commit 0130b0b32ee53dc7add773fcea984f6a26ef1da3.
    
    Sergey Vlasov points out (and Vadim Lobanov concurs) that the bug it was
    supposed to fix must be some unrelated memory corruption, and the "fix"
    actually causes more problems:
    
      "However, the new code does not look safe in all cases.  If some other
       task has opened more files while dup_fd() released oldf->file_lock, the
       new code will update open_files to the new larger value.  But newf was
       allocated with the old smaller value of open_files, therefore subsequent
       accesses to newf may try to write into unallocated memory."
    
    so revert it.
    
    Cc: Sharyathi Nagesh <sharyath@in.ibm.com>
    Cc: Sergey Vlasov <vsu@altlinux.ru>
    Cc: Vadim Lobanov <vlobanov@speakeasy.net>
    Cc: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4b4eab2a3161..3da978eec791 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -687,7 +687,6 @@ static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 		 * the latest pointer.
 		 */
 		spin_lock(&oldf->file_lock);
-		open_files = count_open_files(old_fdt);
 		old_fdt = files_fdtable(oldf);
 	}
 

commit 0130b0b32ee53dc7add773fcea984f6a26ef1da3
Author: Sharyathi Nagesh <sharyath@in.ibm.com>
Date:   Fri Nov 10 12:27:54 2006 -0800

    [PATCH] fix Data Acess error in dup_fd
    
    On running the Stress Test on machine for more than 72 hours following
    error message was observed.
    
    0:mon> e
    cpu 0x0: Vector: 300 (Data Access) at [c00000007ce2f7f0]
        pc: c000000000060d90: .dup_fd+0x240/0x39c
        lr: c000000000060d6c: .dup_fd+0x21c/0x39c
        sp: c00000007ce2fa70
       msr: 800000000000b032
       dar: ffffffff00000028
     dsisr: 40000000
      current = 0xc000000074950980
      paca    = 0xc000000000454500
        pid   = 27330, comm = bash
    
    0:mon> t
    [c00000007ce2fa70] c000000000060d28 .dup_fd+0x1d8/0x39c (unreliable)
    [c00000007ce2fb30] c000000000060f48 .copy_files+0x5c/0x88
    [c00000007ce2fbd0] c000000000061f5c .copy_process+0x574/0x1520
    [c00000007ce2fcd0] c000000000062f88 .do_fork+0x80/0x1c4
    [c00000007ce2fdc0] c000000000011790 .sys_clone+0x5c/0x74
    [c00000007ce2fe30] c000000000008950 .ppc_clone+0x8/0xc
    
    The problem is because of race window.  When if(expand) block is executed in
    dup_fd unlocking of oldf->file_lock give a window for fdtable in oldf to be
    modified.  So actual open_files in oldf may not match with open_files
    variable.
    
    Cc: Vadim Lobanov <vlobanov@speakeasy.net>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3da978eec791..4b4eab2a3161 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -687,6 +687,7 @@ static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 		 * the latest pointer.
 		 */
 		spin_lock(&oldf->file_lock);
+		open_files = count_open_files(old_fdt);
 		old_fdt = files_fdtable(oldf);
 	}
 

commit b8534d7bd89df0cd41cd47bcd6733a05ea9a691a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sat Oct 28 10:38:53 2006 -0700

    [PATCH] taskstats: kill ->taskstats_lock in favor of ->siglock
    
    signal_struct is (mostly) protected by ->sighand->siglock, I think we don't
    need ->taskstats_lock to protect ->stats.  This also allows us to simplify the
    locking in fill_tgid().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 213326609bac..3da978eec791 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -830,7 +830,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	if (clone_flags & CLONE_THREAD) {
 		atomic_inc(&current->signal->count);
 		atomic_inc(&current->signal->live);
-		taskstats_tgid_alloc(current->signal);
+		taskstats_tgid_alloc(current);
 		return 0;
 	}
 	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);

commit 093a8e8aecd77b2799934996a55a6838e1e2b8f3
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sat Oct 28 10:38:51 2006 -0700

    [PATCH] taskstats_tgid_free: fix usage
    
    taskstats_tgid_free() is called on copy_process's error path. This is wrong.
    
            IF (clone_flags & CLONE_THREAD)
                    We should not clear ->signal->taskstats, current uses it,
                    it probably has a valid accumulated info.
            ELSE
                    taskstats_tgid_init() set ->signal->taskstats = NULL,
                    there is nothing to free.
    
    Move the callsite to __exit_signal(). We don't need any locking, entire
    thread group is exiting, nobody should have a reference to soon to be
    released ->signal.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 29ebb30850ed..213326609bac 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -897,7 +897,6 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 void __cleanup_signal(struct signal_struct *sig)
 {
 	exit_thread_group_keys(sig);
-	taskstats_tgid_free(sig);
 	kmem_cache_free(signal_cachep, sig);
 }
 

commit bea493a031fe3337f4fe5479e8e865513828ea76
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 17 00:10:33 2006 -0700

    [PATCH] rt-mutex: fixup rt-mutex debug code
    
    BUG: warning at kernel/rtmutex-debug.c:125/rt_mutex_debug_task_free() (Not tainted)
     [<c04051e3>] show_trace_log_lvl+0x58/0x16a
     [<c04057f0>] show_trace+0xd/0x10
     [<c0405900>] dump_stack+0x19/0x1b
     [<c043f03d>] rt_mutex_debug_task_free+0x35/0x6a
     [<c04224c0>] free_task+0x15/0x24
     [<c042378c>] copy_process+0x12bd/0x1324
     [<c0423835>] do_fork+0x42/0x113
     [<c04021dd>] sys_fork+0x19/0x1b
     [<c0403fb7>] syscall_call+0x7/0xb
    
    In copy_process(), dup_task_struct() also duplicates the ->pi_lock,
    ->pi_waiters and ->pi_blocked_on members.  rt_mutex_debug_task_free()
    called from free_task() validates these members.  However free_task() can
    be invoked before these members are reset for the new task.
    
    Move the initialization code before the first bail that can hit free_task().
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7dc6140baac6..29ebb30850ed 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -984,6 +984,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	if (!p)
 		goto fork_out;
 
+	rt_mutex_init_task(p);
+
 #ifdef CONFIG_TRACE_IRQFLAGS
 	DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);
 	DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
@@ -1088,8 +1090,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->lockdep_recursion = 0;
 #endif
 
-	rt_mutex_init_task(p);
-
 #ifdef CONFIG_DEBUG_MUTEXES
 	p->blocked_on = NULL; /* not blocked yet */
 #endif

commit 73ea41302bab5e02c9e86ab15c509494a550f1db
Author: Kirill Korotaev <dev@openvz.org>
Date:   Mon Oct 2 02:18:20 2006 -0700

    [PATCH] IPC namespace - utils
    
    This patch adds basic IPC namespace functionality to
    IPC utils:
    - init_ipc_ns
    - copy/clone/unshare/free IPC ns
    - /proc preparations
    
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Signed-off-by: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d6cc56558507..7dc6140baac6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1589,6 +1589,16 @@ static int unshare_semundo(unsigned long unshare_flags, struct sem_undo_list **n
 	return 0;
 }
 
+#ifndef CONFIG_IPC_NS
+static inline int unshare_ipcs(unsigned long flags, struct ipc_namespace **ns)
+{
+	if (flags & CLONE_NEWIPC)
+		return -EINVAL;
+
+	return 0;
+}
+#endif
+
 /*
  * unshare allows a process to 'unshare' part of the process
  * context which was originally shared using clone.  copy_*

commit 25b21cb2f6d69b0475b134e0a3e8e269137270fa
Author: Kirill Korotaev <dev@openvz.org>
Date:   Mon Oct 2 02:18:19 2006 -0700

    [PATCH] IPC namespace core
    
    This patch set allows to unshare IPCs and have a private set of IPC objects
    (sem, shm, msg) inside namespace.  Basically, it is another building block of
    containers functionality.
    
    This patch implements core IPC namespace changes:
    - ipc_namespace structure
    - new config option CONFIG_IPC_NS
    - adds CLONE_NEWIPC flag
    - unshare support
    
    [clg@fr.ibm.com: small fix for unshare of ipc namespace]
    [akpm@osdl.org: build fix]
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Signed-off-by: Kirill Korotaev <dev@openvz.org>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 208dd99f13bc..d6cc56558507 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1608,13 +1608,15 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	struct sem_undo_list *new_ulist = NULL;
 	struct nsproxy *new_nsproxy = NULL, *old_nsproxy = NULL;
 	struct uts_namespace *uts, *new_uts = NULL;
+	struct ipc_namespace *ipc, *new_ipc = NULL;
 
 	check_unshare_flags(&unshare_flags);
 
 	/* Return -EINVAL for all unsupported flags */
 	err = -EINVAL;
 	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
-				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|CLONE_NEWUTS))
+				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|
+				CLONE_NEWUTS|CLONE_NEWIPC))
 		goto bad_unshare_out;
 
 	if ((err = unshare_thread(unshare_flags)))
@@ -1633,18 +1635,20 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		goto bad_unshare_cleanup_fd;
 	if ((err = unshare_utsname(unshare_flags, &new_uts)))
 		goto bad_unshare_cleanup_semundo;
+	if ((err = unshare_ipcs(unshare_flags, &new_ipc)))
+		goto bad_unshare_cleanup_uts;
 
-	if (new_ns || new_uts) {
+	if (new_ns || new_uts || new_ipc) {
 		old_nsproxy = current->nsproxy;
 		new_nsproxy = dup_namespaces(old_nsproxy);
 		if (!new_nsproxy) {
 			err = -ENOMEM;
-			goto bad_unshare_cleanup_uts;
+			goto bad_unshare_cleanup_ipc;
 		}
 	}
 
 	if (new_fs || new_ns || new_sigh || new_mm || new_fd || new_ulist ||
-				new_uts) {
+				new_uts || new_ipc) {
 
 		task_lock(current);
 
@@ -1692,12 +1696,22 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 			new_uts = uts;
 		}
 
+		if (new_ipc) {
+			ipc = current->nsproxy->ipc_ns;
+			current->nsproxy->ipc_ns = new_ipc;
+			new_ipc = ipc;
+		}
+
 		task_unlock(current);
 	}
 
 	if (new_nsproxy)
 		put_nsproxy(new_nsproxy);
 
+bad_unshare_cleanup_ipc:
+	if (new_ipc)
+		put_ipc_ns(new_ipc);
+
 bad_unshare_cleanup_uts:
 	if (new_uts)
 		put_uts_ns(new_uts);

commit c0b2fc316599d6cd875b6b8cafa67f03b9512b4d
Author: Serge Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:18 2006 -0700

    [PATCH] uts: copy nsproxy only when needed
    
    The nsproxy was being copied in unshare() when anything was being unshared,
    even if it was something not referenced from nsproxy.  This should end up
    in some cases with far more memory usage than necessary.
    
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c08cf05dd59b..208dd99f13bc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1606,7 +1606,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	struct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;
 	struct files_struct *fd, *new_fd = NULL;
 	struct sem_undo_list *new_ulist = NULL;
-	struct nsproxy *new_nsproxy, *old_nsproxy;
+	struct nsproxy *new_nsproxy = NULL, *old_nsproxy = NULL;
 	struct uts_namespace *uts, *new_uts = NULL;
 
 	check_unshare_flags(&unshare_flags);
@@ -1634,18 +1634,24 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	if ((err = unshare_utsname(unshare_flags, &new_uts)))
 		goto bad_unshare_cleanup_semundo;
 
-	if (new_fs || new_ns || new_sigh || new_mm || new_fd || new_ulist ||
-				new_uts) {
-
+	if (new_ns || new_uts) {
 		old_nsproxy = current->nsproxy;
 		new_nsproxy = dup_namespaces(old_nsproxy);
 		if (!new_nsproxy) {
 			err = -ENOMEM;
 			goto bad_unshare_cleanup_uts;
 		}
+	}
+
+	if (new_fs || new_ns || new_sigh || new_mm || new_fd || new_ulist ||
+				new_uts) {
 
 		task_lock(current);
-		current->nsproxy = new_nsproxy;
+
+		if (new_nsproxy) {
+			current->nsproxy = new_nsproxy;
+			new_nsproxy = old_nsproxy;
+		}
 
 		if (new_fs) {
 			fs = current->fs;
@@ -1687,9 +1693,11 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		}
 
 		task_unlock(current);
-		put_nsproxy(old_nsproxy);
 	}
 
+	if (new_nsproxy)
+		put_nsproxy(new_nsproxy);
+
 bad_unshare_cleanup_uts:
 	if (new_uts)
 		put_uts_ns(new_uts);

commit 071df104f808b8195c40643dcb4d060681742e29
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:17 2006 -0700

    [PATCH] namespaces: utsname: implement CLONE_NEWUTS flag
    
    Implement a CLONE_NEWUTS flag, and use it at clone and sys_unshare.
    
    [clg@fr.ibm.com: IPC unshare fix]
    [bunk@stusta.de: cleanup]
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 33fcf0733ca6..c08cf05dd59b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1607,13 +1607,14 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	struct files_struct *fd, *new_fd = NULL;
 	struct sem_undo_list *new_ulist = NULL;
 	struct nsproxy *new_nsproxy, *old_nsproxy;
+	struct uts_namespace *uts, *new_uts = NULL;
 
 	check_unshare_flags(&unshare_flags);
 
 	/* Return -EINVAL for all unsupported flags */
 	err = -EINVAL;
 	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
-				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM))
+				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM|CLONE_NEWUTS))
 		goto bad_unshare_out;
 
 	if ((err = unshare_thread(unshare_flags)))
@@ -1630,14 +1631,17 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		goto bad_unshare_cleanup_vm;
 	if ((err = unshare_semundo(unshare_flags, &new_ulist)))
 		goto bad_unshare_cleanup_fd;
+	if ((err = unshare_utsname(unshare_flags, &new_uts)))
+		goto bad_unshare_cleanup_semundo;
 
-	if (new_fs || new_ns || new_sigh || new_mm || new_fd || new_ulist) {
+	if (new_fs || new_ns || new_sigh || new_mm || new_fd || new_ulist ||
+				new_uts) {
 
 		old_nsproxy = current->nsproxy;
 		new_nsproxy = dup_namespaces(old_nsproxy);
 		if (!new_nsproxy) {
 			err = -ENOMEM;
-			goto bad_unshare_cleanup_semundo;
+			goto bad_unshare_cleanup_uts;
 		}
 
 		task_lock(current);
@@ -1676,10 +1680,20 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 			new_fd = fd;
 		}
 
+		if (new_uts) {
+			uts = current->nsproxy->uts_ns;
+			current->nsproxy->uts_ns = new_uts;
+			new_uts = uts;
+		}
+
 		task_unlock(current);
 		put_nsproxy(old_nsproxy);
 	}
 
+bad_unshare_cleanup_uts:
+	if (new_uts)
+		put_uts_ns(new_uts);
+
 bad_unshare_cleanup_semundo:
 bad_unshare_cleanup_fd:
 	if (new_fd)

commit 1651e14e28a2d9f446018ef522882e0709a2ce4f
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:08 2006 -0700

    [PATCH] namespaces: incorporate fs namespace into nsproxy
    
    This moves the mount namespace into the nsproxy.  The mount namespace count
    now refers to the number of nsproxies point to it, rather than the number of
    tasks.  As a result, the unshare_namespace() function in kernel/fork.c no
    longer checks whether it is being shared.
    
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c9e660ae47aa..33fcf0733ca6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1119,11 +1119,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_mm;
 	if ((retval = copy_namespaces(clone_flags, p)))
 		goto bad_fork_cleanup_keys;
-	if ((retval = copy_namespace(clone_flags, p)))
-		goto bad_fork_cleanup_namespaces;
 	retval = copy_thread(0, clone_flags, stack_start, stack_size, p, regs);
 	if (retval)
-		goto bad_fork_cleanup_namespace;
+		goto bad_fork_cleanup_namespaces;
 
 	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
 	/*
@@ -1215,7 +1213,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		spin_unlock(&current->sighand->siglock);
 		write_unlock_irq(&tasklist_lock);
 		retval = -ERESTARTNOINTR;
-		goto bad_fork_cleanup_namespace;
+		goto bad_fork_cleanup_namespaces;
 	}
 
 	if (clone_flags & CLONE_THREAD) {
@@ -1263,8 +1261,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	proc_fork_connector(p);
 	return p;
 
-bad_fork_cleanup_namespace:
-	exit_namespace(p);
 bad_fork_cleanup_namespaces:
 	exit_task_namespaces(p);
 bad_fork_cleanup_keys:
@@ -1519,10 +1515,9 @@ static int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)
  */
 static int unshare_namespace(unsigned long unshare_flags, struct namespace **new_nsp, struct fs_struct *new_fs)
 {
-	struct namespace *ns = current->namespace;
+	struct namespace *ns = current->nsproxy->namespace;
 
-	if ((unshare_flags & CLONE_NEWNS) &&
-	    (ns && atomic_read(&ns->count) > 1)) {
+	if ((unshare_flags & CLONE_NEWNS) && ns) {
 		if (!capable(CAP_SYS_ADMIN))
 			return -EPERM;
 
@@ -1655,8 +1650,8 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		}
 
 		if (new_ns) {
-			ns = current->namespace;
-			current->namespace = new_ns;
+			ns = current->nsproxy->namespace;
+			current->nsproxy->namespace = new_ns;
 			new_ns = ns;
 		}
 

commit ab516013ad9ca47f1d3a936fa81303bfbf734d52
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:06 2006 -0700

    [PATCH] namespaces: add nsproxy
    
    This patch adds a nsproxy structure to the task struct.  Later patches will
    move the fs namespace pointer into this structure, and introduce a new utsname
    namespace into the nsproxy.
    
    The vserver and openvz functionality, then, would be implemented in large part
    by virtualizing/isolating more and more resources into namespaces, each
    contained in the nsproxy.
    
    [akpm@osdl.org: build fix]
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 89f666491d1f..c9e660ae47aa 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -27,6 +27,7 @@
 #include <linux/binfmts.h>
 #include <linux/mman.h>
 #include <linux/fs.h>
+#include <linux/nsproxy.h>
 #include <linux/capability.h>
 #include <linux/cpu.h>
 #include <linux/cpuset.h>
@@ -1116,8 +1117,10 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_signal;
 	if ((retval = copy_keys(clone_flags, p)))
 		goto bad_fork_cleanup_mm;
-	if ((retval = copy_namespace(clone_flags, p)))
+	if ((retval = copy_namespaces(clone_flags, p)))
 		goto bad_fork_cleanup_keys;
+	if ((retval = copy_namespace(clone_flags, p)))
+		goto bad_fork_cleanup_namespaces;
 	retval = copy_thread(0, clone_flags, stack_start, stack_size, p, regs);
 	if (retval)
 		goto bad_fork_cleanup_namespace;
@@ -1262,6 +1265,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 bad_fork_cleanup_namespace:
 	exit_namespace(p);
+bad_fork_cleanup_namespaces:
+	exit_task_namespaces(p);
 bad_fork_cleanup_keys:
 	exit_keys(p);
 bad_fork_cleanup_mm:
@@ -1606,6 +1611,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 	struct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;
 	struct files_struct *fd, *new_fd = NULL;
 	struct sem_undo_list *new_ulist = NULL;
+	struct nsproxy *new_nsproxy, *old_nsproxy;
 
 	check_unshare_flags(&unshare_flags);
 
@@ -1632,7 +1638,15 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 
 	if (new_fs || new_ns || new_sigh || new_mm || new_fd || new_ulist) {
 
+		old_nsproxy = current->nsproxy;
+		new_nsproxy = dup_namespaces(old_nsproxy);
+		if (!new_nsproxy) {
+			err = -ENOMEM;
+			goto bad_unshare_cleanup_semundo;
+		}
+
 		task_lock(current);
+		current->nsproxy = new_nsproxy;
 
 		if (new_fs) {
 			fs = current->fs;
@@ -1668,8 +1682,10 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		}
 
 		task_unlock(current);
+		put_nsproxy(old_nsproxy);
 	}
 
+bad_unshare_cleanup_semundo:
 bad_unshare_cleanup_fd:
 	if (new_fd)
 		put_files_struct(new_fd);

commit 8f0ab5147951267134612570604cf8341901a80c
Author: Jay Lan <jlan@engr.sgi.com>
Date:   Sat Sep 30 23:28:59 2006 -0700

    [PATCH] csa: convert CONFIG tag for extended accounting routines
    
    There were a few accounting data/macros that are used in CSA but are #ifdef'ed
    inside CONFIG_BSD_PROCESS_ACCT.  This patch is to change those ifdef's from
    CONFIG_BSD_PROCESS_ACCT to CONFIG_TASK_XACCT.  A few defines are moved from
    kernel/acct.c and include/linux/acct.h to kernel/tsacct.c and
    include/linux/tsacct_kern.h.
    
    Signed-off-by: Jay Lan <jlan@sgi.com>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Chris Sturtivant <csturtiv@sgi.com>
    Cc: Tony Ernst <tee@sgi.com>
    Cc: Guillaume Thouvenin <guillaume.thouvenin@bull.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1c999f3e0b47..89f666491d1f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -42,6 +42,7 @@
 #include <linux/profile.h>
 #include <linux/rmap.h>
 #include <linux/acct.h>
+#include <linux/tsacct_kern.h>
 #include <linux/cn_proc.h>
 #include <linux/delayacct.h>
 #include <linux/taskstats_kern.h>

commit 5b160f5ecd2f1b6df2e0015dc1f319c8ef803d62
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Sep 29 02:00:52 2006 -0700

    [PATCH] copy_process: cosmetic ->ioprio tweak
    
    copy_process:
    // holds tasklist_lock + ->siglock
           /*
            * inherit ioprio
            */
           p->ioprio = current->ioprio;
    
    Why?  ->ioprio was already copied in dup_task_struct().  I guess this is
    needed to ensure that the child can't escape
    sys_ioprio_set(IOPRIO_WHO_{PGRP,USER}), yes?
    
    In that case we don't need ->siglock held, and the comment should be
    updated.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Jens Axboe <axboe@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index bca6ce6d3ded..1c999f3e0b47 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1150,7 +1150,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 
 	/* Our parent execution domain becomes current domain
 	   These must match for thread signalling to apply */
-	   
 	p->parent_exec_id = p->self_exec_id;
 
 	/* ok, now we should be set up.. */
@@ -1173,6 +1172,9 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	/* Need tasklist lock for parent etc handling! */
 	write_lock_irq(&tasklist_lock);
 
+	/* for sys_ioprio_set(IOPRIO_WHO_PGRP) */
+	p->ioprio = current->ioprio;
+
 	/*
 	 * The task hasn't been attached yet, so its cpus_allowed mask will
 	 * not be changed, nor will its assigned CPU.
@@ -1232,11 +1234,6 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		}
 	}
 
-	/*
-	 * inherit ioprio
-	 */
-	p->ioprio = current->ioprio;
-
 	if (likely(p->pid)) {
 		add_parent(p);
 		if (unlikely(p->ptrace & PT_PTRACED))

commit 6c5c934153513dc72e2d6464f39e8ef1f27c0a3e
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Sep 29 01:59:40 2006 -0700

    [PATCH] ifdef blktrace debugging fields
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 802b1cf0e63f..bca6ce6d3ded 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -183,7 +183,9 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	/* One for us, one for whoever does the "release_task()" (usually parent) */
 	atomic_set(&tsk->usage,2);
 	atomic_set(&tsk->fs_excl, 0);
+#ifdef CONFIG_BLK_DEV_IO_TRACE
 	tsk->btrace_seq = 0;
+#endif
 	tsk->splice_pipe = NULL;
 	return tsk;
 }

commit ebdea46fecae40c4d7effcd33f40918a37a1df4b
Merge: fecf3404f4ab 250d375d1da4
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Thu Sep 28 14:40:39 2006 -0700

    Merge branch 'devel' of master.kernel.org:/home/rmk/linux-2.6-arm
    
    * 'devel' of master.kernel.org:/home/rmk/linux-2.6-arm: (130 commits)
      [ARM] 3856/1: Add clocksource for Intel IXP4xx platforms
      [ARM] 3855/1: Add generic time support
      [ARM] 3873/1: S3C24XX: Add irq_chip names
      [ARM] 3872/1: S3C24XX: Apply consistant tabbing to irq_chips
      [ARM] 3871/1: S3C24XX: Fix ordering of EINT4..23
      [ARM] nommu: confirms the CR_V bit in nommu mode
      [ARM] nommu: abort handler fixup for !CPU_CP15_MMU cores.
      [ARM] 3870/1: AT91: Start removing static memory mappings
      [ARM] 3869/1: AT91: NAND support for DK and KB9202 boards
      [ARM] 3868/1: AT91 hardware header update
      [ARM] 3867/1: AT91 GPIO update
      [ARM] 3866/1: AT91 clock update
      [ARM] 3865/1: AT91RM9200 header updates
      [ARM] 3862/2: S3C2410 - add basic power management support for AML M5900 series
      [ARM] kthread: switch arch/arm/kernel/apm.c
      [ARM] Off-by-one in arch/arm/common/icst*
      [ARM] 3864/1: Refactore sharpsl_pm
      [ARM] 3863/1: Add Locomo SPI Device
      [ARM] 3847/2:  Convert LOMOMO to use struct device for GPIOs
      [ARM] Use CPU_CACHE_* where possible in asm/cacheflush.h
      ...

commit 0a4254058037eb172758961d0a5b94f4320a1425
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Tue Sep 26 10:52:38 2006 +0200

    [PATCH] Add the canary field to the PDA area and the task struct
    
    This patch adds the per thread cookie field to the task struct and the PDA.
    Also it makes sure that the PDA value gets the new cookie value at context
    switch, and that a new task gets a new cookie at task creation time.
    
    Signed-off-by: Arjan van Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andi Kleen <ak@suse.de>
    CC: Andi Kleen <ak@suse.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index f9b014e3e700..a0dad84567c9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -45,6 +45,7 @@
 #include <linux/cn_proc.h>
 #include <linux/delayacct.h>
 #include <linux/taskstats_kern.h>
+#include <linux/random.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -175,6 +176,10 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	tsk->thread_info = ti;
 	setup_thread_stack(tsk, orig);
 
+#ifdef CONFIG_CC_STACKPROTECTOR
+	tsk->stack_canary = get_random_int();
+#endif
+
 	/* One for us, one for whoever does the "release_task()" (usually parent) */
 	atomic_set(&tsk->usage,2);
 	atomic_set(&tsk->fs_excl, 0);

commit b36e4758dc1b9ff1f6d97e951edba22366230d11
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Aug 27 12:26:34 2006 +0100

    [ARM] Fix kernel/fork.c for lockdep on ARM
    
    ARM has interrupts enabled over context switches (iow, has
    __ARCH_WANT_INTERRUPTS_ON_CTXSW defined.)  The lockdep code in fork.c
     assumes that interrupts are always disabled.  Fix this wrong
    assumption by making the initialisation of 'p->hardirqs_enabled'
    depend on __ARCH_WANT_INTERRUPTS_ON_CTXSW.
    
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index f9b014e3e700..8f76adf1c6a6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1056,7 +1056,11 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 #endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	p->irq_events = 0;
+#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW
+	p->hardirqs_enabled = 1;
+#else
 	p->hardirqs_enabled = 0;
+#endif
 	p->hardirq_enable_ip = 0;
 	p->hardirq_enable_event = 0;
 	p->hardirq_disable_ip = _THIS_IP_;

commit 35df17c57cecb08f0120fb18926325f1093dc429
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Thu Aug 31 21:27:38 2006 -0700

    [PATCH] task delay accounting fixes
    
    Cleanup allocation and freeing of tsk->delays used by delay accounting.
    This solves two problems reported for delay accounting:
    
    1. oops in __delayacct_blkio_ticks
    http://www.uwsg.indiana.edu/hypermail/linux/kernel/0608.2/1844.html
    
    Currently tsk->delays is getting freed too early in task exit which can
    cause a NULL tsk->delays to get accessed via reading of /proc/<tgid>/stats.
     The patch fixes this problem by freeing tsk->delays closer to when
    task_struct itself is freed up.  As a result, it also eliminates the use of
    tsk->delays_lock which was only being used (inadequately) to safeguard
    access to tsk->delays while a task was exiting.
    
    2. Possible memory leak in kernel/delayacct.c
    http://www.uwsg.indiana.edu/hypermail/linux/kernel/0608.2/1389.html
    
    The patch cleans up tsk->delays allocations after a bad fork which was
    missing earlier.
    
    The patch has been tested to fix the problems listed above and stress
    tested with rapid calls to delay accounting's taskstats command interface
    (which is the other path that can access the same data, besides the /proc
    interface causing the oops above).
    
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index aa36c43783cc..f9b014e3e700 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -117,6 +117,7 @@ void __put_task_struct(struct task_struct *tsk)
 	security_task_free(tsk);
 	free_uid(tsk->user);
 	put_group_info(tsk->group_info);
+	delayacct_tsk_free(tsk);
 
 	if (!profile_handoff_task(tsk))
 		free_task(tsk);
@@ -1011,7 +1012,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	retval = -EFAULT;
 	if (clone_flags & CLONE_PARENT_SETTID)
 		if (put_user(p->pid, parent_tidptr))
-			goto bad_fork_cleanup;
+			goto bad_fork_cleanup_delays_binfmt;
 
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);
@@ -1277,7 +1278,8 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_cpuset:
 #endif
 	cpuset_exit(p);
-bad_fork_cleanup:
+bad_fork_cleanup_delays_binfmt:
+	delayacct_tsk_free(p);
 	if (p->binfmt)
 		module_put(p->binfmt->module);
 bad_fork_cleanup_put_domain:

commit 9f59ce5d0e0dd837853385927b150f5cef3a7f52
Author: Chuck Ebbert <76306.1226@compuserve.com>
Date:   Sat Aug 5 12:14:11 2006 -0700

    [PATCH] ptrace: make pid of child process available for PTRACE_EVENT_VFORK_DONE
    
    When delivering PTRACE_EVENT_VFORK_DONE, provide pid of the child process
    when tracer calls ptrace(PTRACE_GETEVENTMSG).  This is already
    (accidentally) available when the tracer is tracing VFORK in addition to
    VFORK_DONE.
    
    Signed-off-by: Chuck Ebbert <76306.1226@compuserve.com>
    Cc: Daniel Jacobowitz <dan@debian.org>
    Cc: Albert Cahalan <acahalan@gmail.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1b0f7b1e0881..aa36c43783cc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1387,8 +1387,10 @@ long do_fork(unsigned long clone_flags,
 
 		if (clone_flags & CLONE_VFORK) {
 			wait_for_completion(&vfork);
-			if (unlikely (current->ptrace & PT_TRACE_VFORK_DONE))
+			if (unlikely (current->ptrace & PT_TRACE_VFORK_DONE)) {
+				current->ptrace_message = nr;
 				ptrace_notify ((PTRACE_EVENT_VFORK_DONE << 8) | SIGTRAP);
+			}
 		}
 	} else {
 		free_pid(pid);

commit ad4ecbcba72855a2b5319b96e2a3a65ed1ca3bfd
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Fri Jul 14 00:24:44 2006 -0700

    [PATCH] delay accounting taskstats interface send tgid once
    
    Send per-tgid data only once during exit of a thread group instead of once
    with each member thread exit.
    
    Currently, when a thread exits, besides its per-tid data, the per-tgid data
    of its thread group is also sent out, if its thread group is non-empty.
    The per-tgid data sent consists of the sum of per-tid stats for all
    *remaining* threads of the thread group.
    
    This patch modifies this sending in two ways:
    
    - the per-tgid data is sent only when the last thread of a thread group
      exits.  This cuts down heavily on the overhead of sending/receiving
      per-tgid data, especially when other exploiters of the taskstats
      interface aren't interested in per-tgid stats
    
    - the semantics of the per-tgid data sent are changed.  Instead of being
      the sum of per-tid data for remaining threads, the value now sent is the
      true total accumalated statistics for all threads that are/were part of
      the thread group.
    
    The patch also addresses a minor issue where failure of one accounting
    subsystem to fill in the taskstats structure was causing the send of
    taskstats to not be sent at all.
    
    The patch has been tested for stability and run cerberus for over 4 hours
    on an SMP.
    
    [akpm@osdl.org: bugfixes]
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 451cfd35bf22..1b0f7b1e0881 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -44,6 +44,7 @@
 #include <linux/acct.h>
 #include <linux/cn_proc.h>
 #include <linux/delayacct.h>
+#include <linux/taskstats_kern.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -819,6 +820,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	if (clone_flags & CLONE_THREAD) {
 		atomic_inc(&current->signal->count);
 		atomic_inc(&current->signal->live);
+		taskstats_tgid_alloc(current->signal);
 		return 0;
 	}
 	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);
@@ -863,6 +865,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	INIT_LIST_HEAD(&sig->cpu_timers[0]);
 	INIT_LIST_HEAD(&sig->cpu_timers[1]);
 	INIT_LIST_HEAD(&sig->cpu_timers[2]);
+	taskstats_tgid_init(sig);
 
 	task_lock(current->group_leader);
 	memcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);
@@ -884,6 +887,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 void __cleanup_signal(struct signal_struct *sig)
 {
 	exit_thread_group_keys(sig);
+	taskstats_tgid_free(sig);
 	kmem_cache_free(signal_cachep, sig);
 }
 

commit ca74e92b4698276b6696f15a801759f50944f387
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Fri Jul 14 00:24:36 2006 -0700

    [PATCH] per-task-delay-accounting: setup
    
    Initialization code related to collection of per-task "delay" statistics which
    measure how long it had to wait for cpu, sync block io, swapping etc.  The
    collection of statistics and the interface are in other patches.  This patch
    sets up the data structures and allows the statistics collection to be
    disabled through a kernel boot parameter.
    
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Peter Chubb <peterc@gelato.unsw.edu.au>
    Cc: Erich Focht <efocht@ess.nec.de>
    Cc: Levent Serinol <lserinol@gmail.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 926e5a68ea9e..451cfd35bf22 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -43,6 +43,7 @@
 #include <linux/rmap.h>
 #include <linux/acct.h>
 #include <linux/cn_proc.h>
+#include <linux/delayacct.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -1000,6 +1001,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 		goto bad_fork_cleanup_put_domain;
 
 	p->did_exec = 0;
+	delayacct_tsk_init(p);	/* Must remain after dup_task_struct() */
 	copy_flags(clone_flags, p);
 	p->pid = pid;
 	retval = -EFAULT;

commit c59923a15c12d2b3597af913bf234a0ef264a38b
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jul 10 04:45:40 2006 -0700

    [PATCH] remove the tasklist_lock export
    
    As announced half a year ago this patch will remove the tasklist_lock
    export.  The previous two patches got rid of the remaining modular users.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 56e4e07e45f7..926e5a68ea9e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -61,9 +61,7 @@ int max_threads;		/* tunable limit on nr_threads */
 
 DEFINE_PER_CPU(unsigned long, process_counts) = 0;
 
- __cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
-
-EXPORT_SYMBOL(tasklist_lock);
+__cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
 
 int nr_processes(void)
 {

commit 36c8b586896f60cb91a4fd526233190b34316baf
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:25:41 2006 -0700

    [PATCH] sched: cleanup, remove task_t, convert to struct task_struct
    
    cleanup: remove task_t and convert all the uses to struct task_struct. I
    introduced it for the scheduler anno and it was a mistake.
    
    Conversion was mostly scripted, the result was reviewed and all
    secondary whitespace and style impact (if any) was fixed up by hand.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 54953d8a6f17..56e4e07e45f7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -933,13 +933,13 @@ static inline void rt_mutex_init_task(struct task_struct *p)
  * parts of the process environment (as per the clone
  * flags). The actual kick-off is left to the caller.
  */
-static task_t *copy_process(unsigned long clone_flags,
-				 unsigned long stack_start,
-				 struct pt_regs *regs,
-				 unsigned long stack_size,
-				 int __user *parent_tidptr,
-				 int __user *child_tidptr,
-				 int pid)
+static struct task_struct *copy_process(unsigned long clone_flags,
+					unsigned long stack_start,
+					struct pt_regs *regs,
+					unsigned long stack_size,
+					int __user *parent_tidptr,
+					int __user *child_tidptr,
+					int pid)
 {
 	int retval;
 	struct task_struct *p = NULL;
@@ -1294,9 +1294,9 @@ struct pt_regs * __devinit __attribute__((weak)) idle_regs(struct pt_regs *regs)
 	return regs;
 }
 
-task_t * __devinit fork_idle(int cpu)
+struct task_struct * __devinit fork_idle(int cpu)
 {
-	task_t *task;
+	struct task_struct *task;
 	struct pt_regs regs;
 
 	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL, NULL, 0);

commit ad33945175bed649ca5fe0881269db005bbb449a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:25:15 2006 -0700

    [PATCH] lockdep: annotate ->mmap_sem
    
    Teach special (recursive) locking code to the lock validator.  Has no effect
    on non-lockdep kernels.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7f48abdd7bb6..54953d8a6f17 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -193,7 +193,10 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 
 	down_write(&oldmm->mmap_sem);
 	flush_cache_mm(oldmm);
-	down_write(&mm->mmap_sem);
+	/*
+	 * Not linked in yet - no deadlock potential:
+	 */
+	down_write_nested(&mm->mmap_sem, SINGLE_DEPTH_NESTING);
 
 	mm->locked_vm = 0;
 	mm->mmap = NULL;

commit fbb9ce9530fd9b66096d5187fa6a115d16d9746c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:50 2006 -0700

    [PATCH] lockdep: core
    
    Do 'make oldconfig' and accept all the defaults for new config options -
    reboot into the kernel and if everything goes well it should boot up fine and
    you should have /proc/lockdep and /proc/lockdep_stats files.
    
    Typically if the lock validator finds some problem it will print out
    voluminous debug output that begins with "BUG: ..." and which syslog output
    can be used by kernel developers to figure out the precise locking scenario.
    
    What does the lock validator do?  It "observes" and maps all locking rules as
    they occur dynamically (as triggered by the kernel's natural use of spinlocks,
    rwlocks, mutexes and rwsems).  Whenever the lock validator subsystem detects a
    new locking scenario, it validates this new rule against the existing set of
    rules.  If this new rule is consistent with the existing set of rules then the
    new rule is added transparently and the kernel continues as normal.  If the
    new rule could create a deadlock scenario then this condition is printed out.
    
    When determining validity of locking, all possible "deadlock scenarios" are
    considered: assuming arbitrary number of CPUs, arbitrary irq context and task
    context constellations, running arbitrary combinations of all the existing
    locking scenarios.  In a typical system this means millions of separate
    scenarios.  This is why we call it a "locking correctness" validator - for all
    rules that are observed the lock validator proves it with mathematical
    certainty that a deadlock could not occur (assuming that the lock validator
    implementation itself is correct and its internal data structures are not
    corrupted by some other kernel subsystem).  [see more details and conditionals
    of this statement in include/linux/lockdep.h and
    Documentation/lockdep-design.txt]
    
    Furthermore, this "all possible scenarios" property of the validator also
    enables the finding of complex, highly unlikely multi-CPU multi-context races
    via single single-context rules, increasing the likelyhood of finding bugs
    drastically.  In practical terms: the lock validator already found a bug in
    the upstream kernel that could only occur on systems with 3 or more CPUs, and
    which needed 3 very unlikely code sequences to occur at once on the 3 CPUs.
    That bug was found and reported on a single-CPU system (!).  So in essence a
    race will be found "piecemail-wise", triggering all the necessary components
    for the race, without having to reproduce the race scenario itself!  In its
    short existence the lock validator found and reported many bugs before they
    actually caused a real deadlock.
    
    To further increase the efficiency of the validator, the mapping is not per
    "lock instance", but per "lock-class".  For example, all struct inode objects
    in the kernel have inode->inotify_mutex.  If there are 10,000 inodes cached,
    then there are 10,000 lock objects.  But ->inotify_mutex is a single "lock
    type", and all locking activities that occur against ->inotify_mutex are
    "unified" into this single lock-class.  The advantage of the lock-class
    approach is that all historical ->inotify_mutex uses are mapped into a single
    (and as narrow as possible) set of locking rules - regardless of how many
    different tasks or inode structures it took to build this set of rules.  The
    set of rules persist during the lifetime of the kernel.
    
    To see the rough magnitude of checking that the lock validator does, here's a
    portion of /proc/lockdep_stats, fresh after bootup:
    
     lock-classes:                            694 [max: 2048]
     direct dependencies:                  1598 [max: 8192]
     indirect dependencies:               17896
     all direct dependencies:             16206
     dependency chains:                    1910 [max: 8192]
     in-hardirq chains:                      17
     in-softirq chains:                     105
     in-process chains:                    1065
     stack-trace entries:                 38761 [max: 131072]
     combined max dependencies:         2033928
     hardirq-safe locks:                     24
     hardirq-unsafe locks:                  176
     softirq-safe locks:                     53
     softirq-unsafe locks:                  137
     irq-safe locks:                         59
     irq-unsafe locks:                      176
    
    The lock validator has observed 1598 actual single-thread locking patterns,
    and has validated all possible 2033928 distinct locking scenarios.
    
    More details about the design of the lock validator can be found in
    Documentation/lockdep-design.txt, which can also found at:
    
       http://redhat.com/~mingo/lockdep-patches/lockdep-design.txt
    
    [bunk@stusta.de: cleanups]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b7db7fb74f53..7f48abdd7bb6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1061,6 +1061,11 @@ static task_t *copy_process(unsigned long clone_flags,
 	p->hardirq_context = 0;
 	p->softirq_context = 0;
 #endif
+#ifdef CONFIG_LOCKDEP
+	p->lockdep_depth = 0; /* no locks held yet */
+	p->curr_chain_key = 0;
+	p->lockdep_recursion = 0;
+#endif
 
 	rt_mutex_init_task(p);
 

commit de30a2b355ea85350ca2f58f3b9bf4e5bc007986
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:42 2006 -0700

    [PATCH] lockdep: irqtrace subsystem, core
    
    Accurate hard-IRQ-flags and softirq-flags state tracing.
    
    This allows us to attach extra functionality to IRQ flags on/off
    events (such as trace-on/off).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1cd46a4fb0d3..b7db7fb74f53 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -968,6 +968,10 @@ static task_t *copy_process(unsigned long clone_flags,
 	if (!p)
 		goto fork_out;
 
+#ifdef CONFIG_TRACE_IRQFLAGS
+	DEBUG_LOCKS_WARN_ON(!p->hardirqs_enabled);
+	DEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);
+#endif
 	retval = -EAGAIN;
 	if (atomic_read(&p->user->processes) >=
 			p->signal->rlim[RLIMIT_NPROC].rlim_cur) {
@@ -1042,6 +1046,21 @@ static task_t *copy_process(unsigned long clone_flags,
  	}
 	mpol_fix_fork_child_flag(p);
 #endif
+#ifdef CONFIG_TRACE_IRQFLAGS
+	p->irq_events = 0;
+	p->hardirqs_enabled = 0;
+	p->hardirq_enable_ip = 0;
+	p->hardirq_enable_event = 0;
+	p->hardirq_disable_ip = _THIS_IP_;
+	p->hardirq_disable_event = 0;
+	p->softirqs_enabled = 1;
+	p->softirq_enable_ip = _THIS_IP_;
+	p->softirq_enable_event = 0;
+	p->softirq_disable_ip = 0;
+	p->softirq_disable_event = 0;
+	p->hardirq_context = 0;
+	p->softirq_context = 0;
+#endif
 
 	rt_mutex_init_task(p);
 

commit 9a11b49a805665e13a56aa067afaf81d43ec1514
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:33 2006 -0700

    [PATCH] lockdep: better lock debugging
    
    Generic lock debugging:
    
     - generalized lock debugging framework. For example, a bug in one lock
       subsystem turns off debugging in all lock subsystems.
    
     - got rid of the caller address passing (__IP__/__IP_DECL__/etc.) from
       the mutex/rtmutex debugging code: it caused way too much prototype
       hackery, and lockdep will give the same information anyway.
    
     - ability to do silent tests
    
     - check lock freeing in vfree too.
    
     - more finegrained debugging options, to allow distributions to
       turn off more expensive debugging features.
    
    There's no separate 'held mutexes' list anymore - but there's a 'held locks'
    stack within lockdep, which unifies deadlock detection across all lock
    classes.  (this is independent of the lockdep validation stuff - lockdep first
    checks whether we are holding a lock already)
    
    Here are the current debugging options:
    
    CONFIG_DEBUG_MUTEXES=y
    CONFIG_DEBUG_LOCK_ALLOC=y
    
    which do:
    
     config DEBUG_MUTEXES
              bool "Mutex debugging, basic checks"
    
     config DEBUG_LOCK_ALLOC
             bool "Detect incorrect freeing of live mutexes"
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9064bf9e131b..1cd46a4fb0d3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -919,10 +919,6 @@ static inline void rt_mutex_init_task(struct task_struct *p)
 	spin_lock_init(&p->pi_lock);
 	plist_head_init(&p->pi_waiters, &p->pi_lock);
 	p->pi_blocked_on = NULL;
-# ifdef CONFIG_DEBUG_RT_MUTEXES
-	spin_lock_init(&p->held_list_lock);
-	INIT_LIST_HEAD(&p->held_list_head);
-# endif
 #endif
 }
 

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jörn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jörn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 628198a4f28a..9064bf9e131b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -11,7 +11,6 @@
  * management can be a bitch. See 'mm/memory.c': 'copy_page_range()'
  */
 
-#include <linux/config.h>
 #include <linux/slab.h>
 #include <linux/init.h>
 #include <linux/unistd.h>

commit c87e2837be82df479a6bae9f155c43516d2feebc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 27 02:54:58 2006 -0700

    [PATCH] pi-futex: futex_lock_pi/futex_unlock_pi support
    
    This adds the actual pi-futex implementation, based on rt-mutexes.
    
    [dino@in.ibm.com: fix an oops-causing race]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Dinakar Guniguntala <dino@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b664a081fffa..628198a4f28a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1092,6 +1092,9 @@ static task_t *copy_process(unsigned long clone_flags,
 #ifdef CONFIG_COMPAT
 	p->compat_robust_list = NULL;
 #endif
+	INIT_LIST_HEAD(&p->pi_state_list);
+	p->pi_state_cache = NULL;
+
 	/*
 	 * sigaltstack should be cleared when sharing the same VM
 	 */

commit 23f78d4a03c53cbd75d87a795378ea540aa08c86
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 27 02:54:53 2006 -0700

    [PATCH] pi-futex: rt mutex core
    
    Core functions for the rt-mutex subsystem.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9b4e54ef0225..b664a081fffa 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -104,6 +104,7 @@ static kmem_cache_t *mm_cachep;
 void free_task(struct task_struct *tsk)
 {
 	free_thread_info(tsk->thread_info);
+	rt_mutex_debug_task_free(tsk);
 	free_task_struct(tsk);
 }
 EXPORT_SYMBOL(free_task);
@@ -913,6 +914,19 @@ asmlinkage long sys_set_tid_address(int __user *tidptr)
 	return current->pid;
 }
 
+static inline void rt_mutex_init_task(struct task_struct *p)
+{
+#ifdef CONFIG_RT_MUTEXES
+	spin_lock_init(&p->pi_lock);
+	plist_head_init(&p->pi_waiters, &p->pi_lock);
+	p->pi_blocked_on = NULL;
+# ifdef CONFIG_DEBUG_RT_MUTEXES
+	spin_lock_init(&p->held_list_lock);
+	INIT_LIST_HEAD(&p->held_list_head);
+# endif
+#endif
+}
+
 /*
  * This creates a new process as a copy of the old one,
  * but does not actually start it yet.
@@ -1034,6 +1048,8 @@ static task_t *copy_process(unsigned long clone_flags,
 	mpol_fix_fork_child_flag(p);
 #endif
 
+	rt_mutex_init_task(p);
+
 #ifdef CONFIG_DEBUG_MUTEXES
 	p->blocked_on = NULL; /* not blocked yet */
 #endif

commit cf2dfbfbf4c5cb489ea12defd85a484307b955b8
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Mon Jun 26 00:26:10 2006 -0700

    [PATCH] coredump: copy_process: don't check SIGNAL_GROUP_EXIT
    
    After the previous patch SIGNAL_GROUP_EXIT implies a pending SIGKILL, we
    can remove this check from copy_process() because we already checked
    !signal_pending().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 79e91046f36e..9b4e54ef0225 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1156,18 +1156,6 @@ static task_t *copy_process(unsigned long clone_flags,
 	}
 
 	if (clone_flags & CLONE_THREAD) {
-		/*
-		 * Important: if an exit-all has been started then
-		 * do not create this new thread - the whole thread
-		 * group is supposed to exit anyway.
-		 */
-		if (current->signal->flags & SIGNAL_GROUP_EXIT) {
-			spin_unlock(&current->sighand->siglock);
-			write_unlock_irq(&tasklist_lock);
-			retval = -EAGAIN;
-			goto bad_fork_cleanup_namespace;
-		}
-
 		p->group_leader = current->group_leader;
 		list_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);
 

commit 48e6484d49020dba3578ad117b461e8a391e8f0f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jun 26 00:25:48 2006 -0700

    [PATCH] proc: Rewrite the proc dentry flush on exit optimization
    
    To keep the dcache from filling up with dead /proc entries we flush them on
    process exit.  However over the years that code has gotten hairy with a
    dentry_pointer and a lock in task_struct and misdocumented as a correctness
    feature.
    
    I have rewritten this code to look and see if we have a corresponding entry in
    the dcache and if so flush it on process exit.  This removes the extra fields
    in the task_struct and allows me to trivially handle the case of a
    /proc/<tgid>/task/<pid> entry as well as the current /proc/<pid> entries.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index dfd10cb370c3..79e91046f36e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -993,13 +993,10 @@ static task_t *copy_process(unsigned long clone_flags,
 		if (put_user(p->pid, parent_tidptr))
 			goto bad_fork_cleanup;
 
-	p->proc_dentry = NULL;
-
 	INIT_LIST_HEAD(&p->children);
 	INIT_LIST_HEAD(&p->sibling);
 	p->vfork_done = NULL;
 	spin_lock_init(&p->alloc_lock);
-	spin_lock_init(&p->proc_lock);
 
 	clear_tsk_thread_flag(p, TIF_SIGPENDING);
 	init_sigpending(&p->pending);

commit 0e4648141af02331f21aabcd34940c70f09a2d04
Author: KaiGai Kohei <kaigai@ak.jp.nec.com>
Date:   Sun Jun 25 05:49:24 2006 -0700

    [PATCH] pacct: add pacct_struct to fix some pacct bugs.
    
    The pacct facility need an i/o operation when an accounting record is
    generated.  There is a possibility to wake OOM killer up.  If OOM killer is
    activated, it kills some processes to make them release process memory
    regions.
    
    But acct_process() is called in the killed processes context before calling
    exit_mm(), so those processes cannot release own memory.  In the results, any
    processes stop in this point and it finally cause a system stall.

diff --git a/kernel/fork.c b/kernel/fork.c
index 49adc0e8d47c..dfd10cb370c3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -874,6 +874,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 		tsk->it_prof_expires =
 			secs_to_cputime(sig->rlim[RLIMIT_CPU].rlim_cur);
 	}
+	acct_init_pacct(&sig->pacct);
 
 	return 0;
 }

commit 6e6672604773b9bae44d88d38afdf0763c104b1c
Author: Prasanna Meda <mlp@google.com>
Date:   Fri Jun 23 02:05:23 2006 -0700

    [PATCH] dup fd error fix
    
    Set errorp in dup_fd, it will be used in sys_unshare also.
    
    Signed-off-by: Prasanna Meda <mlp@google.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 195958a3a4d6..49adc0e8d47c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -625,6 +625,7 @@ static struct files_struct *alloc_files(void)
 /*
  * Allocate a new files structure and copy contents from the
  * passed in files structure.
+ * errorp will be valid only when the returned files_struct is NULL.
  */
 static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 {
@@ -633,6 +634,7 @@ static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 	int open_files, size, i, expand;
 	struct fdtable *old_fdt, *new_fdt;
 
+	*errorp = -ENOMEM;
 	newf = alloc_files();
 	if (!newf)
 		goto out;
@@ -746,7 +748,6 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 	 * break this.
 	 */
 	tsk->files = NULL;
-	error = -ENOMEM;
 	newf = dup_fd(oldf, &error);
 	if (!newf)
 		goto out;

commit 0ae26f1b3159f3acb21ae1e866c3c7e16edd450f
Author: Andrew Morton <akpm@osdl.org>
Date:   Fri Jun 23 02:05:15 2006 -0700

    [PATCH] mmput() might sleep
    
    exit_aio() and exit_mmap() can sleep.  But it's easy to accidentally call
    mmput() from inside locks.
    
    Cc: Dave Peterson <dsp@llnl.gov>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ac8100e3088a..195958a3a4d6 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -368,6 +368,8 @@ void fastcall __mmdrop(struct mm_struct *mm)
  */
 void mmput(struct mm_struct *mm)
 {
+	might_sleep();
+
 	if (atomic_dec_and_test(&mm->mm_users)) {
 		exit_aio(mm);
 		exit_mmap(mm);

commit fa84cb935d4ec601528f5e2f0d5d31e7876a5044
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Mar 29 20:30:19 2006 -0500

    [PATCH] move call of audit_free() into do_exit()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/fork.c b/kernel/fork.c
index d2fa57d480d4..ac8100e3088a 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -114,8 +114,6 @@ void __put_task_struct(struct task_struct *tsk)
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);
 
-	if (unlikely(tsk->audit_context))
-		audit_free(tsk);
 	security_task_free(tsk);
 	free_uid(tsk->user);
 	put_group_info(tsk->group_info);

commit a0aa7f68afeeb92f6274b395177c20e617c8ed2d
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Apr 20 13:05:33 2006 +0200

    [PATCH] Don't inherit ->splice_pipe across forks
    
    It's really task private, so clear that field on fork after copying
    task structure.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index 34515772611e..d2fa57d480d4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -180,6 +180,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	atomic_set(&tsk->usage,2);
 	atomic_set(&tsk->fs_excl, 0);
 	tsk->btrace_seq = 0;
+	tsk->splice_pipe = NULL;
 	return tsk;
 }
 

commit 5e85d4abe3f43bb5362f384bab0e20ef082ce0b5
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Apr 18 22:20:16 2006 -0700

    [PATCH] task: Make task list manipulations RCU safe
    
    While we can currently walk through thread groups, process groups, and
    sessions with just the rcu_read_lock, this opens the door to walking the
    entire task list.
    
    We already have all of the other RCU guarantees so there is no cost in
    doing this, this should be enough so that proc can stop taking the
    tasklist lock during readdir.
    
    prev_task was killed because it has no users, and using it will miss new
    tasks when doing an rcu traversal.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 54b15f8cda53..34515772611e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1204,7 +1204,7 @@ static task_t *copy_process(unsigned long clone_flags,
 			attach_pid(p, PIDTYPE_PGID, process_group(p));
 			attach_pid(p, PIDTYPE_SID, p->signal->session);
 
-			list_add_tail(&p->tasks, &init_task.tasks);
+			list_add_tail_rcu(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;
 		}
 		attach_pid(p, PIDTYPE_PID, p->pid);

commit 64541d19702cfdb7ea946fdc20faee849f6874b1
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Apr 14 12:43:15 2006 -0600

    [PATCH] kill unushed __put_task_struct_cb
    
    Somehow in the midst of dotting i's and crossing t's during
    the merge up to rc1 we wound up keeping __put_task_struct_cb
    when it should have been killed as it no longer has any users.
    Sorry I probably should have caught this while it was
    still in the -mm tree.
    
    Having the old code there gets confusing when reading
    through the code and trying to understand what is
    happening.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3384eb89cb1c..54b15f8cda53 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -124,12 +124,6 @@ void __put_task_struct(struct task_struct *tsk)
 		free_task(tsk);
 }
 
-void __put_task_struct_cb(struct rcu_head *rhp)
-{
-	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
-	__put_task_struct(tsk);
-}
-
 void __init fork_init(unsigned long mempages)
 {
 #ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR

commit 428622986858aebddc32d022af65e88b9d2ea8bb
Author: Kirill Korotaev <dev@openvz.org>
Date:   Fri Mar 31 17:58:46 2006 +0400

    [PATCH] wrong error path in dup_fd() leading to oopses in RCU
    
    Wrong error path in dup_fd() - it should return NULL on error,
    not an address of already freed memory :/
    
    Triggered by OpenVZ stress test suite.
    
    What is interesting is that it was causing different oopses in RCU like
    below:
    Call Trace:
       [<c013492c>] rcu_do_batch+0x2c/0x80
       [<c0134bdd>] rcu_process_callbacks+0x3d/0x70
       [<c0126cf3>] tasklet_action+0x73/0xe0
       [<c01269aa>] __do_softirq+0x10a/0x130
       [<c01058ff>] do_softirq+0x4f/0x60
       =======================
       [<c0113817>] smp_apic_timer_interrupt+0x77/0x110
       [<c0103b54>] apic_timer_interrupt+0x1c/0x24
      Code:  Bad EIP value.
       <0>Kernel panic - not syncing: Fatal exception in interrupt
    
    Signed-Off-By: Pavel Emelianov <xemul@sw.ru>
    Signed-Off-By: Dmitry Mishin <dim@openvz.org>
    Signed-Off-By: Kirill Korotaev <dev@openvz.org>
    Signed-Off-By: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 03975d0467f9..3384eb89cb1c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -725,7 +725,7 @@ static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 	free_fdset (new_fdt->open_fds, new_fdt->max_fdset);
 	free_fd_array(new_fdt->fd, new_fdt->max_fds);
 	kmem_cache_free(files_cachep, newf);
-	goto out;
+	return NULL;
 }
 
 static int copy_files(unsigned long clone_flags, struct task_struct * tsk)

commit 92476d7fc0326a409ab1d3864a04093a6be9aca7
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Mar 31 02:31:42 2006 -0800

    [PATCH] pidhash: Refactor the pid hash table
    
    Simplifies the code, reduces the need for 4 pid hash tables, and makes the
    code more capable.
    
    In the discussions I had with Oleg it was felt that to a large extent the
    cleanup itself justified the work.  With struct pid being dynamically
    allocated meant we could create the hash table entry when the pid was
    allocated and free the hash table entry when the pid was freed.  Instead of
    playing with the hash lists when ever a process would attach or detach to a
    process.
    
    For myself the fact that it gave what my previous task_ref patch gave for free
    with simpler code was a big win.  The problem is that if you hold a reference
    to struct task_struct you lock in 10K of low memory.  If you do that in a user
    controllable way like /proc does, with an unprivileged but hostile user space
    application with typical resource limits of 1000 fds and 100 processes I can
    trigger the OOM killer by consuming all of low memory with task structs, on a
    machine wight 1GB of low memory.
    
    If I instead hold a reference to struct pid which holds a pointer to my
    task_struct, I don't suffer from that problem because struct pid is 2 orders
    of magnitude smaller.  In fact struct pid is small enough that most other
    kernel data structures dwarf it, so simply limiting the number of referring
    data structures is enough to prevent exhaustion of low memory.
    
    This splits the current struct pid into two structures, struct pid and struct
    pid_link, and reduces our number of hash tables from PIDTYPE_MAX to just one.
    struct pid_link is the per process linkage into the hash tables and lives in
    struct task_struct.  struct pid is given an indepedent lifetime, and holds
    pointers to each of the pid types.
    
    The independent life of struct pid simplifies attach_pid, and detach_pid,
    because we are always manipulating the list of pids and not the hash table.
    In addition in giving struct pid an indpendent life it makes the concept much
    more powerful.
    
    Kernel data structures can now embed a struct pid * instead of a pid_t and
    not suffer from pid wrap around problems or from keeping unnecessarily
    large amounts of memory allocated.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b1341205be27..03975d0467f9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1315,17 +1315,19 @@ long do_fork(unsigned long clone_flags,
 {
 	struct task_struct *p;
 	int trace = 0;
-	long pid = alloc_pidmap();
+	struct pid *pid = alloc_pid();
+	long nr;
 
-	if (pid < 0)
+	if (!pid)
 		return -EAGAIN;
+	nr = pid->nr;
 	if (unlikely(current->ptrace)) {
 		trace = fork_traceflag (clone_flags);
 		if (trace)
 			clone_flags |= CLONE_PTRACE;
 	}
 
-	p = copy_process(clone_flags, stack_start, regs, stack_size, parent_tidptr, child_tidptr, pid);
+	p = copy_process(clone_flags, stack_start, regs, stack_size, parent_tidptr, child_tidptr, nr);
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.
@@ -1352,7 +1354,7 @@ long do_fork(unsigned long clone_flags,
 			p->state = TASK_STOPPED;
 
 		if (unlikely (trace)) {
-			current->ptrace_message = pid;
+			current->ptrace_message = nr;
 			ptrace_notify ((trace << 8) | SIGTRAP);
 		}
 
@@ -1362,10 +1364,10 @@ long do_fork(unsigned long clone_flags,
 				ptrace_notify ((PTRACE_EVENT_VFORK_DONE << 8) | SIGTRAP);
 		}
 	} else {
-		free_pidmap(pid);
-		pid = PTR_ERR(p);
+		free_pid(pid);
+		nr = PTR_ERR(p);
 	}
-	return pid;
+	return nr;
 }
 
 #ifndef ARCH_MIN_MMSTRUCT_ALIGN

commit 158d9ebd19280582da172626ad3edda1a626dace
Author: Andrew Morton <akpm@osdl.org>
Date:   Fri Mar 31 02:31:34 2006 -0800

    [PATCH] resurrect __put_task_struct
    
    This just got nuked in mainline.  Bring it back because Eric's patches use it.
    
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b3f7a1bb5e55..b1341205be27 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -108,10 +108,8 @@ void free_task(struct task_struct *tsk)
 }
 EXPORT_SYMBOL(free_task);
 
-void __put_task_struct_cb(struct rcu_head *rhp)
+void __put_task_struct(struct task_struct *tsk)
 {
-	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
-
 	WARN_ON(!(tsk->exit_state & (EXIT_DEAD | EXIT_ZOMBIE)));
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);
@@ -126,6 +124,12 @@ void __put_task_struct_cb(struct rcu_head *rhp)
 		free_task(tsk);
 }
 
+void __put_task_struct_cb(struct rcu_head *rhp)
+{
+	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
+	__put_task_struct(tsk);
+}
+
 void __init fork_init(unsigned long mempages)
 {
 #ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR

commit a7e5328a06a2beee3a2bbfaf87ce2a7bbe937de1
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:27 2006 -0800

    [PATCH] cleanup __exit_signal->cleanup_sighand path
    
    Move 'tsk->sighand = NULL' from cleanup_sighand() to __exit_signal().  This
    makes the exit path more understandable and allows us to do
    cleanup_sighand() outside of ->siglock protected section.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index aa50c848fae7..b3f7a1bb5e55 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -803,12 +803,8 @@ static inline int copy_sighand(unsigned long clone_flags, struct task_struct * t
 	return 0;
 }
 
-void cleanup_sighand(struct task_struct *tsk)
+void __cleanup_sighand(struct sighand_struct *sighand)
 {
-	struct sighand_struct * sighand = tsk->sighand;
-
-	/* Ok, we're done with the signal handlers */
-	tsk->sighand = NULL;
 	if (atomic_dec_and_test(&sighand->count))
 		kmem_cache_free(sighand_cachep, sighand);
 }
@@ -1233,7 +1229,7 @@ static task_t *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_signal:
 	cleanup_signal(p);
 bad_fork_cleanup_sighand:
-	cleanup_sighand(p);
+	__cleanup_sighand(p->sighand);
 bad_fork_cleanup_fs:
 	exit_fs(p); /* blocking */
 bad_fork_cleanup_files:

commit 4a2c7a7837da1b91468e50426066d988050e4d56
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:26 2006 -0800

    [PATCH] make fork() atomic wrt pgrp/session signals
    
    Eric W. Biederman wrote:
    >
    > Ok. SUSV3/Posix is clear, fork is atomic with respect
    > to signals.  Either a signal comes before or after a
    > fork but not during. (See the rationale section).
    > http://www.opengroup.org/onlinepubs/000095399/functions/fork.html
    >
    > The tasklist_lock does not stop forks from adding to a process
    > group. The forks stall while the tasklist_lock is held, but a fork
    > that began before we grabbed the tasklist_lock simply completes
    > afterwards, and the child does not receive the signal.
    
    This also means that SIGSTOP or sig_kernel_coredump() signal can't
    be delivered to pgrp/session reliably.
    
    With this patch copy_process() returns -ERESTARTNOINTR when it
    detects a pending signal, fork() will be restarted transparently
    after handling the signals.
    
    This patch also deletes now unneeded "group_stop_count > 0" check,
    copy_process() can no longer succeed while group stop in progress.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-By: Eric Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index bc551efb5fd4..aa50c848fae7 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1136,16 +1136,6 @@ static task_t *copy_process(unsigned long clone_flags,
 			!cpu_online(task_cpu(p))))
 		set_task_cpu(p, smp_processor_id());
 
-	/*
-	 * Check for pending SIGKILL! The new thread should not be allowed
-	 * to slip out of an OOM kill. (or normal SIGKILL.)
-	 */
-	if (sigismember(&current->pending.signal, SIGKILL)) {
-		write_unlock_irq(&tasklist_lock);
-		retval = -EINTR;
-		goto bad_fork_cleanup_namespace;
-	}
-
 	/* CLONE_PARENT re-uses the old parent */
 	if (clone_flags & (CLONE_PARENT|CLONE_THREAD))
 		p->real_parent = current->real_parent;
@@ -1154,6 +1144,23 @@ static task_t *copy_process(unsigned long clone_flags,
 	p->parent = p->real_parent;
 
 	spin_lock(&current->sighand->siglock);
+
+	/*
+	 * Process group and session signals need to be delivered to just the
+	 * parent before the fork or both the parent and the child after the
+	 * fork. Restart if a signal comes in before we add the new process to
+	 * it's process group.
+	 * A fatal signal pending means that current will exit, so the new
+	 * thread can't slip out of an OOM kill (or normal SIGKILL).
+ 	 */
+ 	recalc_sigpending();
+	if (signal_pending(current)) {
+		spin_unlock(&current->sighand->siglock);
+		write_unlock_irq(&tasklist_lock);
+		retval = -ERESTARTNOINTR;
+		goto bad_fork_cleanup_namespace;
+	}
+
 	if (clone_flags & CLONE_THREAD) {
 		/*
 		 * Important: if an exit-all has been started then
@@ -1170,16 +1177,6 @@ static task_t *copy_process(unsigned long clone_flags,
 		p->group_leader = current->group_leader;
 		list_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);
 
-		if (current->signal->group_stop_count > 0) {
-			/*
-			 * There is an all-stop in progress for the group.
-			 * We ourselves will stop as soon as we check signals.
-			 * Make the new thread part of that group stop too.
-			 */
-			current->signal->group_stop_count++;
-			set_tsk_thread_flag(p, TIF_SIGPENDING);
-		}
-
 		if (!cputime_eq(current->signal->it_virt_expires,
 				cputime_zero) ||
 		    !cputime_eq(current->signal->it_prof_expires,

commit 47e65328a7b1cdfc4e3102e50d60faf94ebba7d3
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:25 2006 -0800

    [PATCH] pids: kill PIDTYPE_TGID
    
    This patch kills PIDTYPE_TGID pid_type thus saving one hash table in
    kernel/pid.c and speeding up subthreads create/destroy a bit.  It is also a
    preparation for the further tref/pids rework.
    
    This patch adds 'struct list_head thread_group' to 'struct task_struct'
    instead.
    
    We don't detach group leader from PIDTYPE_PID namespace until another
    thread inherits it's ->pid == ->tgid, so we are safe wrt premature
    free_pidmap(->tgid) call.
    
    Currently there are no users of find_task_by_pid_type(PIDTYPE_TGID).
    Should the need arise, we can use find_task_by_pid()->group_leader.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-By: Eric Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 12cdd9fc9d02..bc551efb5fd4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1112,6 +1112,7 @@ static task_t *copy_process(unsigned long clone_flags,
 	 * We dont wake it up yet.
 	 */
 	p->group_leader = p;
+	INIT_LIST_HEAD(&p->thread_group);
 	INIT_LIST_HEAD(&p->ptrace_children);
 	INIT_LIST_HEAD(&p->ptrace_list);
 
@@ -1165,7 +1166,9 @@ static task_t *copy_process(unsigned long clone_flags,
 			retval = -EAGAIN;
 			goto bad_fork_cleanup_namespace;
 		}
+
 		p->group_leader = current->group_leader;
+		list_add_tail_rcu(&p->thread_group, &p->group_leader->thread_group);
 
 		if (current->signal->group_stop_count > 0) {
 			/*
@@ -1213,7 +1216,6 @@ static task_t *copy_process(unsigned long clone_flags,
 			list_add_tail(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;
 		}
-		attach_pid(p, PIDTYPE_TGID, p->tgid);
 		attach_pid(p, PIDTYPE_PID, p->pid);
 		nr_threads++;
 	}

commit c81addc9d3a0ebff2155e0cd86f90820ab97147e
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:17 2006 -0800

    [PATCH] rename __exit_sighand to cleanup_sighand
    
    Cosmetic, rename __exit_sighand to cleanup_sighand and move it close to
    copy_sighand().
    
    This matches copy_signal/cleanup_signal naming, and I think it is easier to
    follow.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: "Paul E. McKenney" <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0aff28cdbadd..12cdd9fc9d02 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -803,6 +803,16 @@ static inline int copy_sighand(unsigned long clone_flags, struct task_struct * t
 	return 0;
 }
 
+void cleanup_sighand(struct task_struct *tsk)
+{
+	struct sighand_struct * sighand = tsk->sighand;
+
+	/* Ok, we're done with the signal handlers */
+	tsk->sighand = NULL;
+	if (atomic_dec_and_test(&sighand->count))
+		kmem_cache_free(sighand_cachep, sighand);
+}
+
 static inline int copy_signal(unsigned long clone_flags, struct task_struct * tsk)
 {
 	struct signal_struct *sig;
@@ -1224,7 +1234,7 @@ static task_t *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_signal:
 	cleanup_signal(p);
 bad_fork_cleanup_sighand:
-	__exit_sighand(p);
+	cleanup_sighand(p);
 bad_fork_cleanup_fs:
 	exit_fs(p); /* blocking */
 bad_fork_cleanup_files:

commit 6b3934ef52712ece50605dfc72e55d00c580831a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:16 2006 -0800

    [PATCH] copy_process: cleanup bad_fork_cleanup_signal
    
    __exit_signal() does important cleanups atomically under ->siglock.  It is
    also called from copy_process's error path.  This is not good, for example we
    can't move __unhash_process() under ->siglock for that reason.
    
    We should not mix these 2 paths, just look at ugly 'if (p->sighand)' under
    'bad_fork_cleanup_sighand:' label.  For copy_process() case it is sufficient
    to just backout copy_signal(), nothing more.
    
    Again, nobody can see this task yet.  For CLONE_THREAD case we just decrement
    signal->count, otherwise nobody can see this ->signal and we can free it
    lockless.
    
    This patch assumes it is safe to do exit_thread_group_keys() without
    tasklist_lock.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8a46ad52be8f..0aff28cdbadd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -84,7 +84,7 @@ static kmem_cache_t *task_struct_cachep;
 #endif
 
 /* SLAB cache for signal_struct structures (tsk->signal) */
-kmem_cache_t *signal_cachep;
+static kmem_cache_t *signal_cachep;
 
 /* SLAB cache for sighand_struct structures (tsk->sighand) */
 kmem_cache_t *sighand_cachep;
@@ -872,6 +872,22 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	return 0;
 }
 
+void __cleanup_signal(struct signal_struct *sig)
+{
+	exit_thread_group_keys(sig);
+	kmem_cache_free(signal_cachep, sig);
+}
+
+static inline void cleanup_signal(struct task_struct *tsk)
+{
+	struct signal_struct *sig = tsk->signal;
+
+	atomic_dec(&sig->live);
+
+	if (atomic_dec_and_test(&sig->count))
+		__cleanup_signal(sig);
+}
+
 static inline void copy_flags(unsigned long clone_flags, struct task_struct *p)
 {
 	unsigned long new_flags = p->flags;
@@ -1206,10 +1222,9 @@ static task_t *copy_process(unsigned long clone_flags,
 	if (p->mm)
 		mmput(p->mm);
 bad_fork_cleanup_signal:
-	exit_signal(p);
+	cleanup_signal(p);
 bad_fork_cleanup_sighand:
-	if (p->sighand)
-		__exit_sighand(p);
+	__exit_sighand(p);
 bad_fork_cleanup_fs:
 	exit_fs(p); /* blocking */
 bad_fork_cleanup_files:

commit 7001510d0cbf51ad202dd2d0744f54104285cbb9
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:14 2006 -0800

    [PATCH] copy_process: cleanup bad_fork_cleanup_sighand
    
    The only caller of exit_sighand(tsk) is copy_process's error path.  We can
    call __exit_sighand() directly and kill exit_sighand().
    
    This 'tsk' was not yet registered in pid_hash[] or init_task.tasks, it has no
    external references, nobody can see it, and
    
            IF (clone_flags & CLONE_SIGHAND)
                    At least 'current' has a reference to ->sighand, this
                    means atomic_dec_and_test(sighand->count) can't be true.
    
            ELSE
                    Nobody can see this ->sighand, this means we can free it
                    without any locking.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: "Paul E. McKenney" <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 33ffb5bf0dbc..8a46ad52be8f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1208,7 +1208,8 @@ static task_t *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_signal:
 	exit_signal(p);
 bad_fork_cleanup_sighand:
-	exit_sighand(p);
+	if (p->sighand)
+		__exit_sighand(p);
 bad_fork_cleanup_fs:
 	exit_fs(p); /* blocking */
 bad_fork_cleanup_files:

commit aa1757f90bea3f598b6e5d04d922a6a60200f1da
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:12 2006 -0800

    [PATCH] convert sighand_cache to use SLAB_DESTROY_BY_RCU
    
    This patch borrows a clever Hugh's 'struct anon_vma' trick.
    
    Without tasklist_lock held we can't trust task->sighand until we locked it
    and re-checked that it is still the same.
    
    But this means we don't need to defer 'kmem_cache_free(sighand)'.  We can
    return the memory to slab immediately, all we need is to be sure that
    sighand->siglock can't dissapear inside rcu protected section.
    
    To do so we need to initialize ->siglock inside ctor function,
    SLAB_DESTROY_BY_RCU does the rest.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0c32e28cdc5f..33ffb5bf0dbc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -786,14 +786,6 @@ int unshare_files(void)
 
 EXPORT_SYMBOL(unshare_files);
 
-void sighand_free_cb(struct rcu_head *rhp)
-{
-	struct sighand_struct *sp;
-
-	sp = container_of(rhp, struct sighand_struct, rcu);
-	kmem_cache_free(sighand_cachep, sp);
-}
-
 static inline int copy_sighand(unsigned long clone_flags, struct task_struct * tsk)
 {
 	struct sighand_struct *sig;
@@ -806,7 +798,6 @@ static inline int copy_sighand(unsigned long clone_flags, struct task_struct * t
 	rcu_assign_pointer(tsk->sighand, sig);
 	if (!sig)
 		return -ENOMEM;
-	spin_lock_init(&sig->siglock);
 	atomic_set(&sig->count, 1);
 	memcpy(sig->action, current->sighand->action, sizeof(sig->action));
 	return 0;
@@ -1356,11 +1347,21 @@ long do_fork(unsigned long clone_flags,
 #define ARCH_MIN_MMSTRUCT_ALIGN 0
 #endif
 
+static void sighand_ctor(void *data, kmem_cache_t *cachep, unsigned long flags)
+{
+	struct sighand_struct *sighand = data;
+
+	if ((flags & (SLAB_CTOR_VERIFY | SLAB_CTOR_CONSTRUCTOR)) ==
+					SLAB_CTOR_CONSTRUCTOR)
+		spin_lock_init(&sighand->siglock);
+}
+
 void __init proc_caches_init(void)
 {
 	sighand_cachep = kmem_cache_create("sighand_cache",
 			sizeof(struct sighand_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_DESTROY_BY_RCU,
+			sighand_ctor, NULL);
 	signal_cachep = kmem_cache_create("signal_cache",
 			sizeof(struct signal_struct), 0,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);

commit 73b9ebfe126a4a886ee46cbab637374d7024668a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:07 2006 -0800

    [PATCH] pidhash: don't count idle threads
    
    fork_idle() does unhash_process() just after copy_process().  Contrary,
    boot_cpu's idle thread explicitely registers itself for each pid_type with nr
    = 0.
    
    copy_process() already checks p->pid != 0 before process_counts++, I think we
    can just skip attach_pid() calls and job control inits for idle threads and
    kill unhash_process().  We don't need to cleanup ->proc_dentry in fork_idle()
    because with this patch idle threads are never hashed in
    kernel/pid.c:pid_hash[].
    
    We don't need to hash pid == 0 in pidmap_init().  free_pidmap() is never
    called with pid == 0 arg, so it will never be reused.  So it is still possible
    to use pid == 0 in any PIDTYPE_xxx namespace from kernel/pid.c's POV.
    
    However with this patch we don't hash pid == 0 for PIDTYPE_PID case.  We still
    have have PIDTYPE_PGID/PIDTYPE_SID entries with pid == 0: /sbin/init and
    kernel threads which don't call daemonize().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 74c67629ee62..0c32e28cdc5f 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1181,25 +1181,26 @@ static task_t *copy_process(unsigned long clone_flags,
 	 */
 	p->ioprio = current->ioprio;
 
-	add_parent(p);
-	if (unlikely(p->ptrace & PT_PTRACED))
-		__ptrace_link(p, current->parent);
-
-	if (thread_group_leader(p)) {
-		p->signal->tty = current->signal->tty;
-		p->signal->pgrp = process_group(current);
-		p->signal->session = current->signal->session;
-		attach_pid(p, PIDTYPE_PGID, process_group(p));
-		attach_pid(p, PIDTYPE_SID, p->signal->session);
-
-		list_add_tail(&p->tasks, &init_task.tasks);
-		if (p->pid)
+	if (likely(p->pid)) {
+		add_parent(p);
+		if (unlikely(p->ptrace & PT_PTRACED))
+			__ptrace_link(p, current->parent);
+
+		if (thread_group_leader(p)) {
+			p->signal->tty = current->signal->tty;
+			p->signal->pgrp = process_group(current);
+			p->signal->session = current->signal->session;
+			attach_pid(p, PIDTYPE_PGID, process_group(p));
+			attach_pid(p, PIDTYPE_SID, p->signal->session);
+
+			list_add_tail(&p->tasks, &init_task.tasks);
 			__get_cpu_var(process_counts)++;
+		}
+		attach_pid(p, PIDTYPE_TGID, p->tgid);
+		attach_pid(p, PIDTYPE_PID, p->pid);
+		nr_threads++;
 	}
-	attach_pid(p, PIDTYPE_TGID, p->tgid);
-	attach_pid(p, PIDTYPE_PID, p->pid);
 
-	nr_threads++;
 	total_forks++;
 	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
@@ -1263,7 +1264,7 @@ task_t * __devinit fork_idle(int cpu)
 	if (!task)
 		return ERR_PTR(-ENOMEM);
 	init_idle(task, cpu);
-	unhash_process(task);
+
 	return task;
 }
 

commit c97d98931ac52ef110b62d9b75c6a6f2bfbc1898
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:06 2006 -0800

    [PATCH] kill SET_LINKS/REMOVE_LINKS
    
    Both SET_LINKS() and SET_LINKS/REMOVE_LINKS() have exactly one caller, and
    these callers already check thread_group_leader().
    
    This patch kills theese macros, they mix two different things: setting
    process's parent and registering it in init_task.tasks list.  Callers are
    updated to do these actions by hand.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c49bd193b058..74c67629ee62 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1181,7 +1181,7 @@ static task_t *copy_process(unsigned long clone_flags,
 	 */
 	p->ioprio = current->ioprio;
 
-	SET_LINKS(p);
+	add_parent(p);
 	if (unlikely(p->ptrace & PT_PTRACED))
 		__ptrace_link(p, current->parent);
 
@@ -1191,6 +1191,8 @@ static task_t *copy_process(unsigned long clone_flags,
 		p->signal->session = current->signal->session;
 		attach_pid(p, PIDTYPE_PGID, process_group(p));
 		attach_pid(p, PIDTYPE_SID, p->signal->session);
+
+		list_add_tail(&p->tasks, &init_task.tasks);
 		if (p->pid)
 			__get_cpu_var(process_counts)++;
 	}

commit 8f17d3a5049d32392b79925c73a0cf99ce6d5af0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 27 01:16:27 2006 -0800

    [PATCH] lightweight robust futexes updates
    
    - fix: initialize the robust list(s) to NULL in copy_process.
    
    - doc update
    
    - cleanup: rename _inuser to _inatomic
    
    - __user cleanups and other small cleanups
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Ulrich Drepper <drepper@redhat.com>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e0a2b449dea6..c49bd193b058 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1061,7 +1061,10 @@ static task_t *copy_process(unsigned long clone_flags,
 	 * Clear TID on mm_release()?
 	 */
 	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;
-
+	p->robust_list = NULL;
+#ifdef CONFIG_COMPAT
+	p->compat_robust_list = NULL;
+#endif
 	/*
 	 * sigaltstack should be cleared when sharing the same VM
 	 */

commit 9ae21d1bb376436285cd5346d3e4b3655d6dd1b9
Merge: f9b4192923fa e9415777b1cd
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Sun Mar 26 09:41:18 2006 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bunk/trivial
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/bunk/trivial:
      drivers/char/ftape/lowlevel/fdc-io.c: Correct a comment
      Kconfig help: MTD_JEDECPROBE already supports Intel
      Remove ugly debugging stuff
      do_mounts.c: Minor ROOT_DEV comment cleanup
      BUG_ON() Conversion in drivers/s390/block/dasd_devmap.c
      BUG_ON() Conversion in mm/mempool.c
      BUG_ON() Conversion in mm/memory.c
      BUG_ON() Conversion in kernel/fork.c
      BUG_ON() Conversion in ipc/sem.c
      BUG_ON() Conversion in fs/ext2/
      BUG_ON() Conversion in fs/hfs/
      BUG_ON() Conversion in fs/dcache.c
      BUG_ON() Conversion in fs/buffer.c
      BUG_ON() Conversion in input/serio/hp_sdc_mlc.c
      BUG_ON() Conversion in md/dm-table.c
      BUG_ON() Conversion in md/dm-path-selector.c
      BUG_ON() Conversion in drivers/isdn
      BUG_ON() Conversion in drivers/char
      BUG_ON() Conversion in drivers/mtd/

commit 05cfb614ddbf3181540ce09d44d96486f8ba8d6a
Author: Roman Zippel <zippel@linux-m68k.org>
Date:   Sun Mar 26 01:38:12 2006 -0800

    [PATCH] hrtimers: remove data field
    
    The nanosleep cleanup allows to remove the data field of hrtimer.  The
    callback function can use container_of() to get it's own data.  Since the
    hrtimer structure is anyway embedded in other structures, this adds no
    overhead.
    
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a02063903aaa..4bd6486aa67d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -848,7 +848,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_REL);
 	sig->it_real_incr.tv64 = 0;
 	sig->real_timer.function = it_real_fn;
-	sig->real_timer.data = tsk;
+	sig->tsk = tsk;
 
 	sig->it_virt_expires = cputime_zero;
 	sig->it_virt_incr = cputime_zero;

commit 910dea7fdda22f0ee83d26d459e460c79ed94557
Author: Eric Sesterhenn <snakebyte@gmx.de>
Date:   Sun Mar 26 18:29:26 2006 +0200

    BUG_ON() Conversion in kernel/fork.c
    
    this changes if() BUG(); constructs to BUG_ON() which is
    cleaner, contains unlikely() and can better optimized away.
    
    Signed-off-by: Eric Sesterhenn <snakebyte@gmx.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index a02063903aaa..d93ab2ba729c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -769,8 +769,7 @@ int unshare_files(void)
 	struct files_struct *files  = current->files;
 	int rc;
 
-	if(!files)
-		BUG();
+	BUG_ON(!files);
 
 	/* This can race but the race causes us to copy when we don't
 	   need to and drop the copy */

commit c61afb181c649754ea221f104e268cbacfc993e3
Author: Paul Jackson <pj@sgi.com>
Date:   Fri Mar 24 03:16:08 2006 -0800

    [PATCH] cpuset memory spread slab cache optimizations
    
    The hooks in the slab cache allocator code path for support of NUMA
    mempolicies and cpuset memory spreading are in an important code path.  Many
    systems will use neither feature.
    
    This patch optimizes those hooks down to a single check of some bits in the
    current tasks task_struct flags.  For non NUMA systems, this hook and related
    code is already ifdef'd out.
    
    The optimization is done by using another task flag, set if the task is using
    a non-default NUMA mempolicy.  Taking this flag bit along with the
    PF_SPREAD_PAGE and PF_SPREAD_SLAB flag bits added earlier in this 'cpuset
    memory spreading' patch set, one can check for the combination of any of these
    special case memory placement mechanisms with a single test of the current
    tasks task_struct flags.
    
    This patch also tightens up the code, to save a few bytes of kernel text
    space, and moves some of it out of line.  Due to the nested inlines called
    from multiple places, we were ending up with three copies of this code, which
    once we get off the main code path (for local node allocation) seems a bit
    wasteful of instruction memory.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index c21bae8c93b9..a02063903aaa 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1021,6 +1021,7 @@ static task_t *copy_process(unsigned long clone_flags,
  		p->mempolicy = NULL;
  		goto bad_fork_cleanup_cpuset;
  	}
+	mpol_fix_fork_child_flag(p);
 #endif
 
 #ifdef CONFIG_DEBUG_MUTEXES

commit 2056a782f8e7e65fd4bfd027506b4ce1c5e9ccd4
Author: Jens Axboe <axboe@suse.de>
Date:   Thu Mar 23 20:00:26 2006 +0100

    [PATCH] Block queue IO tracing support (blktrace) as of 2006-03-23
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/kernel/fork.c b/kernel/fork.c
index c79ae0b19a49..c21bae8c93b9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -181,6 +181,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 	/* One for us, one for whoever does the "release_task()" (usually parent) */
 	atomic_set(&tsk->usage,2);
 	atomic_set(&tsk->fs_excl, 0);
+	tsk->btrace_seq = 0;
 	return tsk;
 }
 

commit 0c9e63fd38a2fb2181668a0cdd622a3c23cfd567
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu Mar 23 03:00:12 2006 -0800

    [PATCH] Shrinks sizeof(files_struct) and better layout
    
    1) Reduce the size of (struct fdtable) to exactly 64 bytes on 32bits
       platforms, lowering kmalloc() allocated space by 50%.
    
    2) Reduce the size of (files_struct), using a special 32 bits (or
       64bits) embedded_fd_set, instead of a 1024 bits fd_set for the
       close_on_exec_init and open_fds_init fields.  This save some ram (248
       bytes per task) as most tasks dont open more than 32 files.  D-Cache
       footprint for such tasks is also reduced to the minimum.
    
    3) Reduce size of allocated fdset.  Currently two full pages are
       allocated, that is 32768 bits on x86 for example, and way too much.  The
       minimum is now L1_CACHE_BYTES.
    
    UP and SMP should benefit from this patch, because most tasks will touch
    only one cache line when open()/close() stdin/stdout/stderr (0/1/2),
    (next_fd, close_on_exec_init, open_fds_init, fd_array[0 ..  2] being in the
    same cache line)
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 9bd7b65ee418..c79ae0b19a49 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -607,12 +607,12 @@ static struct files_struct *alloc_files(void)
 	atomic_set(&newf->count, 1);
 
 	spin_lock_init(&newf->file_lock);
+	newf->next_fd = 0;
 	fdt = &newf->fdtab;
-	fdt->next_fd = 0;
 	fdt->max_fds = NR_OPEN_DEFAULT;
-	fdt->max_fdset = __FD_SETSIZE;
-	fdt->close_on_exec = &newf->close_on_exec_init;
-	fdt->open_fds = &newf->open_fds_init;
+	fdt->max_fdset = EMBEDDED_FD_SET_SIZE;
+	fdt->close_on_exec = (fd_set *)&newf->close_on_exec_init;
+	fdt->open_fds = (fd_set *)&newf->open_fds_init;
 	fdt->fd = &newf->fd_array[0];
 	INIT_RCU_HEAD(&fdt->rcu);
 	fdt->free_files = NULL;

commit 06f9d4f94a075285d25253edbf57f2cda07d4ff3
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Mar 22 00:07:40 2006 -0800

    [PATCH] unshare: Error if passed unsupported flags
    
    A bare bones trivial patch to ensure we always get -EINVAL on the
    unsupported cases for sys_unshare.  If this goes in before 2.6.16 it allows
    us to forward compatible with future applications using sys_unshare.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: JANAK DESAI <janak@us.ibm.com>
    Cc: <stable@kerenl.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b373322ca497..9bd7b65ee418 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1534,6 +1534,12 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 
 	check_unshare_flags(&unshare_flags);
 
+	/* Return -EINVAL for all unsupported flags */
+	err = -EINVAL;
+	if (unshare_flags & ~(CLONE_THREAD|CLONE_FS|CLONE_NEWNS|CLONE_SIGHAND|
+				CLONE_VM|CLONE_FILES|CLONE_SYSVSEM))
+		goto bad_unshare_out;
+
 	if ((err = unshare_thread(unshare_flags)))
 		goto bad_unshare_out;
 	if ((err = unshare_fs(unshare_flags, &new_fs)))

commit 2d61b86775a5676a8fba2ba2f0f869564e35c630
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sat Mar 18 20:41:10 2006 +0300

    [PATCH] disable unshare(CLONE_VM) for now
    
    sys_unshare() does mmput(new_mm).  This is not enough if we have
    mm->core_waiters.
    
    This patch is a temporary fix for soon to be released 2.6.16.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    [ Checked with Uli: "I'm not planning to use unshare(CLONE_VM).  It's
      not needed for any functionality planned so far.  What we (as in Red
      Hat) need unshare() for now is the filesystem side." ]
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 46060cb24af0..b373322ca497 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1478,9 +1478,7 @@ static int unshare_vm(unsigned long unshare_flags, struct mm_struct **new_mmp)
 
 	if ((unshare_flags & CLONE_VM) &&
 	    (mm && atomic_read(&mm->mm_users) > 1)) {
-		*new_mmp = dup_mm(current);
-		if (!*new_mmp)
-			return -ENOMEM;
+		return -EINVAL;
 	}
 
 	return 0;

commit e0e8eb54d8ae0c4cfd1d297f6351b08a7f635c5f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Mar 16 10:31:38 2006 -0700

    [PATCH] unshare: Use rcu_assign_pointer when setting sighand
    
    The sighand pointer only needs the rcu_read_lock on the
    read side.  So only depending on task_lock protection
    when setting this pointer is not enough.  We also need
    a memory barrier to ensure the initialization is seen first.
    
    Use rcu_assign_pointer as it does this for us, and clearly
    documents that we are setting an rcu readable pointer.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Acked-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ccdfbb16c86d..46060cb24af0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1569,7 +1569,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 
 		if (new_sigh) {
 			sigh = current->sighand;
-			current->sighand = new_sigh;
+			rcu_assign_pointer(current->sighand, new_sigh);
 			new_sigh = sigh;
 		}
 

commit f9a3879abf2f1a27c39915e6074b8ff15a24cb55
Author: GOTO Masanori <gotom@sanori.org>
Date:   Mon Mar 13 21:20:44 2006 -0800

    [PATCH] Fix sigaltstack corruption among cloned threads
    
    This patch fixes alternate signal stack corruption among cloned threads
    with CLONE_SIGHAND (and CLONE_VM) for linux-2.6.16-rc6.
    
    The value of alternate signal stack is currently inherited after a call of
    clone(...  CLONE_SIGHAND | CLONE_VM).  But if sigaltstack is set by a
    parent thread, and then if multiple cloned child threads (+ parent threads)
    call signal handler at the same time, some threads may be conflicted -
    because they share to use the same alternative signal stack region.
    Finally they get sigsegv.  It's an undesirable race condition.  Note that
    child threads created from NPTL pthread_create() also hit this conflict
    when the parent thread uses sigaltstack, without my patch.
    
    To fix this problem, this patch clears the child threads' sigaltstack
    information like exec().  This behavior follows the SUSv3 specification.
    In SUSv3, pthread_create() says "The alternate stack shall not be inherited
    (when new threads are initialized)".  It means that sigaltstack should be
    cleared when sigaltstack memory space is shared by cloned threads with
    CLONE_SIGHAND.
    
    Note that I chose "if (clone_flags & CLONE_SIGHAND)" line because:
      - If clone_flags line is not existed, fork() does not inherit sigaltstack.
      - CLONE_VM is another choice, but vfork() does not inherit sigaltstack.
      - CLONE_SIGHAND implies CLONE_VM, and it looks suitable.
      - CLONE_THREAD is another candidate, and includes CLONE_SIGHAND + CLONE_VM,
        but this flag has a bit different semantics.
    I decided to use CLONE_SIGHAND.
    
    [ Changed to test for CLONE_VM && !CLONE_VFORK after discussion --Linus ]
    
    Signed-off-by: GOTO Masanori <gotom@sanori.org>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Linus Torvalds <torvalds@osdl.org>
    Cc: Ulrich Drepper <drepper@redhat.com>
    Cc: Jakub Jelinek <jakub@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a8eab86de7f1..ccdfbb16c86d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1061,6 +1061,12 @@ static task_t *copy_process(unsigned long clone_flags,
 	 */
 	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;
 
+	/*
+	 * sigaltstack should be cleared when sharing the same VM
+	 */
+	if ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)
+		p->sas_ss_sp = p->sas_ss_size = 0;
+
 	/*
 	 * Syscall tracing should be turned off in the child regardless
 	 * of CLONE_PTRACE.

commit 7cd9013be6c22f3ff6f777354f766c8c0b955e17
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Mar 11 03:27:18 2006 -0800

    [PATCH] remove __put_task_struct_cb export again
    
    The patch '[PATCH] RCU signal handling' [1] added an export for
    __put_task_struct_cb, a put_task_struct helper newly introduced in that
    patch.  But the put_task_struct couldn't be used modular previously as
    __put_task_struct wasn't exported.  There are not callers of it in modular
    code, and it shouldn't be exported because we don't want drivers to hold
    references to task_structs.
    
    This patch removes the export and folds __put_task_struct into
    __put_task_struct_cb as there's no other caller.
    
    [1] http://www2.kernel.org/git/gitweb.cgi?p=linux/kernel/git/torvalds/linux-2.6.git;a=commit;h=e56d090310d7625ecb43a1eeebd479f04affb48b
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fbea12d7a943..a8eab86de7f1 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -108,8 +108,10 @@ void free_task(struct task_struct *tsk)
 }
 EXPORT_SYMBOL(free_task);
 
-void __put_task_struct(struct task_struct *tsk)
+void __put_task_struct_cb(struct rcu_head *rhp)
 {
+	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
+
 	WARN_ON(!(tsk->exit_state & (EXIT_DEAD | EXIT_ZOMBIE)));
 	WARN_ON(atomic_read(&tsk->usage));
 	WARN_ON(tsk == current);

commit dadac81b1b86196fcc48fb87620403c4a7174f06
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Feb 15 22:13:26 2006 +0300

    [PATCH] fix kill_proc_info() vs fork() theoretical race
    
    copy_process:
    
            attach_pid(p, PIDTYPE_PID, p->pid);
            attach_pid(p, PIDTYPE_TGID, p->tgid);
    
    What if kill_proc_info(p->pid) happens in between?
    
    copy_process() holds current->sighand.siglock, so we are safe
    in CLONE_THREAD case, because current->sighand == p->sighand.
    
    Otherwise, p->sighand is unlocked, the new process is already
    visible to the find_task_by_pid(), but have a copy of parent's
    'struct pid' in ->pids[PIDTYPE_TGID].
    
    This means that __group_complete_signal() may hang while doing
    
            do ... while (next_thread() != p)
    
    We can solve this problem if we reverse these 2 attach_pid()s:
    
            attach_pid() does wmb()
    
            group_send_sig_info() calls spin_lock(), which
            provides a read barrier. // Yes ?
    
    I don't think we can hit this race in practice, but still.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3683ce10f4a9..fbea12d7a943 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1173,8 +1173,6 @@ static task_t *copy_process(unsigned long clone_flags,
 	if (unlikely(p->ptrace & PT_PTRACED))
 		__ptrace_link(p, current->parent);
 
-	attach_pid(p, PIDTYPE_PID, p->pid);
-	attach_pid(p, PIDTYPE_TGID, p->tgid);
 	if (thread_group_leader(p)) {
 		p->signal->tty = current->signal->tty;
 		p->signal->pgrp = process_group(current);
@@ -1184,6 +1182,8 @@ static task_t *copy_process(unsigned long clone_flags,
 		if (p->pid)
 			__get_cpu_var(process_counts)++;
 	}
+	attach_pid(p, PIDTYPE_TGID, p->tgid);
+	attach_pid(p, PIDTYPE_PID, p->pid);
 
 	nr_threads++;
 	total_forks++;

commit 3f17da699431ec48540beabc55c54d4b5e66c8e7
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Feb 15 22:13:24 2006 +0300

    [PATCH] fix kill_proc_info() vs CLONE_THREAD race
    
    There is a window after copy_process() unlocks ->sighand.siglock
    and before it adds the new thread to the thread list.
    
    In that window __group_complete_signal(SIGKILL) will not see the
    new thread yet, so this thread will start running while the whole
    thread group was supposed to exit.
    
    I beleive we have another good reason to place attach_pid(PID/TGID)
    under ->sighand.siglock. We can do the same for
    
            release_task()->__unhash_process()
    
            de_thread()->switch_exec_pids()
    
    After that we don't need tasklist_lock to iterate over the thread
    list, and we can simplify things, see for example do_sigaction()
    or sys_times().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8e88b374cee9..3683ce10f4a9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1123,8 +1123,8 @@ static task_t *copy_process(unsigned long clone_flags,
 		p->real_parent = current;
 	p->parent = p->real_parent;
 
+	spin_lock(&current->sighand->siglock);
 	if (clone_flags & CLONE_THREAD) {
-		spin_lock(&current->sighand->siglock);
 		/*
 		 * Important: if an exit-all has been started then
 		 * do not create this new thread - the whole thread
@@ -1162,8 +1162,6 @@ static task_t *copy_process(unsigned long clone_flags,
 			 */
 			p->it_prof_expires = jiffies_to_cputime(1);
 		}
-
-		spin_unlock(&current->sighand->siglock);
 	}
 
 	/*
@@ -1189,6 +1187,7 @@ static task_t *copy_process(unsigned long clone_flags,
 
 	nr_threads++;
 	total_forks++;
+	spin_unlock(&current->sighand->siglock);
 	write_unlock_irq(&tasklist_lock);
 	proc_fork_connector(p);
 	return p;

commit a016f3389c06606dd80e687942ff3c71d41823c4
Author: JANAK DESAI <janak@us.ibm.com>
Date:   Tue Feb 7 12:59:02 2006 -0800

    [PATCH] unshare system call -v5: unshare files
    
    If the file descriptor structure is being shared, allocate a new one and copy
    information from the current, shared, structure.
    
    Signed-off-by: Janak Desai <janak@us.ibm.com>
    Cc: Al Viro <viro@ftp.linux.org.uk>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Michael Kerrisk <mtk-manpages@gmx.net>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d1aceaea3f33..8e88b374cee9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -620,32 +620,17 @@ static struct files_struct *alloc_files(void)
 	return newf;
 }
 
-static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
+/*
+ * Allocate a new files structure and copy contents from the
+ * passed in files structure.
+ */
+static struct files_struct *dup_fd(struct files_struct *oldf, int *errorp)
 {
-	struct files_struct *oldf, *newf;
+	struct files_struct *newf;
 	struct file **old_fds, **new_fds;
-	int open_files, size, i, error = 0, expand;
+	int open_files, size, i, expand;
 	struct fdtable *old_fdt, *new_fdt;
 
-	/*
-	 * A background process may not have any files ...
-	 */
-	oldf = current->files;
-	if (!oldf)
-		goto out;
-
-	if (clone_flags & CLONE_FILES) {
-		atomic_inc(&oldf->count);
-		goto out;
-	}
-
-	/*
-	 * Note: we may be using current for both targets (See exec.c)
-	 * This works because we cache current->files (old) as oldf. Don't
-	 * break this.
-	 */
-	tsk->files = NULL;
-	error = -ENOMEM;
 	newf = alloc_files();
 	if (!newf)
 		goto out;
@@ -674,9 +659,9 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 	if (expand) {
 		spin_unlock(&oldf->file_lock);
 		spin_lock(&newf->file_lock);
-		error = expand_files(newf, open_files-1);
+		*errorp = expand_files(newf, open_files-1);
 		spin_unlock(&newf->file_lock);
-		if (error < 0)
+		if (*errorp < 0)
 			goto out_release;
 		new_fdt = files_fdtable(newf);
 		/*
@@ -725,10 +710,8 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 		memset(&new_fdt->close_on_exec->fds_bits[start], 0, left);
 	}
 
-	tsk->files = newf;
-	error = 0;
 out:
-	return error;
+	return newf;
 
 out_release:
 	free_fdset (new_fdt->close_on_exec, new_fdt->max_fdset);
@@ -738,6 +721,40 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 	goto out;
 }
 
+static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
+{
+	struct files_struct *oldf, *newf;
+	int error = 0;
+
+	/*
+	 * A background process may not have any files ...
+	 */
+	oldf = current->files;
+	if (!oldf)
+		goto out;
+
+	if (clone_flags & CLONE_FILES) {
+		atomic_inc(&oldf->count);
+		goto out;
+	}
+
+	/*
+	 * Note: we may be using current for both targets (See exec.c)
+	 * This works because we cache current->files (old) as oldf. Don't
+	 * break this.
+	 */
+	tsk->files = NULL;
+	error = -ENOMEM;
+	newf = dup_fd(oldf, &error);
+	if (!newf)
+		goto out;
+
+	tsk->files = newf;
+	error = 0;
+out:
+	return error;
+}
+
 /*
  *	Helper to unshare the files of the current task.
  *	We don't want to expose copy_files internals to
@@ -1463,15 +1480,19 @@ static int unshare_vm(unsigned long unshare_flags, struct mm_struct **new_mmp)
 }
 
 /*
- * Unsharing of files for tasks created with CLONE_FILES is not supported yet
+ * Unshare file descriptor table if it is being shared
  */
 static int unshare_fd(unsigned long unshare_flags, struct files_struct **new_fdp)
 {
 	struct files_struct *fd = current->files;
+	int error = 0;
 
 	if ((unshare_flags & CLONE_FILES) &&
-	    (fd && atomic_read(&fd->count) > 1))
-		return -EINVAL;
+	    (fd && atomic_read(&fd->count) > 1)) {
+		*new_fdp = dup_fd(fd, &error);
+		if (!*new_fdp)
+			return error;
+	}
 
 	return 0;
 }

commit a0a7ec308f1be5957b20a1a535d21f683dfd83f0
Author: JANAK DESAI <janak@us.ibm.com>
Date:   Tue Feb 7 12:59:01 2006 -0800

    [PATCH] unshare system call -v5: unshare vm
    
    If vm structure is being shared, allocate a new one and copy information from
    the current, shared, structure.
    
    Signed-off-by: Janak Desai <janak@us.ibm.com>
    Cc: Al Viro <viro@ftp.linux.org.uk>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Michael Kerrisk <mtk-manpages@gmx.net>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 07dd241aa1e0..d1aceaea3f33 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -446,6 +446,55 @@ void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 	}
 }
 
+/*
+ * Allocate a new mm structure and copy contents from the
+ * mm structure of the passed in task structure.
+ */
+static struct mm_struct *dup_mm(struct task_struct *tsk)
+{
+	struct mm_struct *mm, *oldmm = current->mm;
+	int err;
+
+	if (!oldmm)
+		return NULL;
+
+	mm = allocate_mm();
+	if (!mm)
+		goto fail_nomem;
+
+	memcpy(mm, oldmm, sizeof(*mm));
+
+	if (!mm_init(mm))
+		goto fail_nomem;
+
+	if (init_new_context(tsk, mm))
+		goto fail_nocontext;
+
+	err = dup_mmap(mm, oldmm);
+	if (err)
+		goto free_pt;
+
+	mm->hiwater_rss = get_mm_rss(mm);
+	mm->hiwater_vm = mm->total_vm;
+
+	return mm;
+
+free_pt:
+	mmput(mm);
+
+fail_nomem:
+	return NULL;
+
+fail_nocontext:
+	/*
+	 * If init_new_context() failed, we cannot use mmput() to free the mm
+	 * because it calls destroy_context()
+	 */
+	mm_free_pgd(mm);
+	free_mm(mm);
+	return NULL;
+}
+
 static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
 {
 	struct mm_struct * mm, *oldmm;
@@ -473,43 +522,17 @@ static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
 	}
 
 	retval = -ENOMEM;
-	mm = allocate_mm();
+	mm = dup_mm(tsk);
 	if (!mm)
 		goto fail_nomem;
 
-	/* Copy the current MM stuff.. */
-	memcpy(mm, oldmm, sizeof(*mm));
-	if (!mm_init(mm))
-		goto fail_nomem;
-
-	if (init_new_context(tsk,mm))
-		goto fail_nocontext;
-
-	retval = dup_mmap(mm, oldmm);
-	if (retval)
-		goto free_pt;
-
-	mm->hiwater_rss = get_mm_rss(mm);
-	mm->hiwater_vm = mm->total_vm;
-
 good_mm:
 	tsk->mm = mm;
 	tsk->active_mm = mm;
 	return 0;
 
-free_pt:
-	mmput(mm);
 fail_nomem:
 	return retval;
-
-fail_nocontext:
-	/*
-	 * If init_new_context() failed, we cannot use mmput() to free the mm
-	 * because it calls destroy_context()
-	 */
-	mm_free_pgd(mm);
-	free_mm(mm);
-	return retval;
 }
 
 static inline struct fs_struct *__copy_fs_struct(struct fs_struct *old)
@@ -1423,18 +1446,20 @@ static int unshare_sighand(unsigned long unshare_flags, struct sighand_struct **
 }
 
 /*
- * Unsharing of vm for tasks created with CLONE_VM is not supported yet
+ * Unshare vm if it is being shared
  */
 static int unshare_vm(unsigned long unshare_flags, struct mm_struct **new_mmp)
 {
 	struct mm_struct *mm = current->mm;
 
 	if ((unshare_flags & CLONE_VM) &&
-	    (mm && atomic_read(&mm->mm_users) > 1))
-		return -EINVAL;
+	    (mm && atomic_read(&mm->mm_users) > 1)) {
+		*new_mmp = dup_mm(current);
+		if (!*new_mmp)
+			return -ENOMEM;
+	}
 
 	return 0;
-
 }
 
 /*

commit 741a295130606143edbf9fc740f633dbc1e6225f
Author: JANAK DESAI <janak@us.ibm.com>
Date:   Tue Feb 7 12:59:00 2006 -0800

    [PATCH] unshare system call -v5: unshare namespace
    
    If the namespace structure is being shared, allocate a new one and copy
    information from the current, shared, structure.
    
    Signed-off-by: Janak Desai <janak@us.ibm.com>
    Cc: Al Viro <viro@ftp.linux.org.uk>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Michael Kerrisk <mtk-manpages@gmx.net>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 598e5c27242c..07dd241aa1e0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1388,16 +1388,21 @@ static int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)
 }
 
 /*
- * Unsharing of namespace for tasks created without CLONE_NEWNS is not
- * supported yet
+ * Unshare the namespace structure if it is being shared
  */
-static int unshare_namespace(unsigned long unshare_flags, struct namespace **new_nsp)
+static int unshare_namespace(unsigned long unshare_flags, struct namespace **new_nsp, struct fs_struct *new_fs)
 {
 	struct namespace *ns = current->namespace;
 
 	if ((unshare_flags & CLONE_NEWNS) &&
-	    (ns && atomic_read(&ns->count) > 1))
-		return -EINVAL;
+	    (ns && atomic_read(&ns->count) > 1)) {
+		if (!capable(CAP_SYS_ADMIN))
+			return -EPERM;
+
+		*new_nsp = dup_namespace(current, new_fs ? new_fs : current->fs);
+		if (!*new_nsp)
+			return -ENOMEM;
+	}
 
 	return 0;
 }
@@ -1482,7 +1487,7 @@ asmlinkage long sys_unshare(unsigned long unshare_flags)
 		goto bad_unshare_out;
 	if ((err = unshare_fs(unshare_flags, &new_fs)))
 		goto bad_unshare_cleanup_thread;
-	if ((err = unshare_namespace(unshare_flags, &new_ns)))
+	if ((err = unshare_namespace(unshare_flags, &new_ns, new_fs)))
 		goto bad_unshare_cleanup_fs;
 	if ((err = unshare_sighand(unshare_flags, &new_sigh)))
 		goto bad_unshare_cleanup_ns;

commit 99d1419d96d7df9cfa56bc977810be831bd5ef64
Author: JANAK DESAI <janak@us.ibm.com>
Date:   Tue Feb 7 12:58:59 2006 -0800

    [PATCH] unshare system call -v5: unshare filesystem info
    
    If filesystem structure is being shared, allocate a new one and copy
    information from the current, shared, structure.
    
    Signed-off-by: Janak Desai <janak@us.ibm.com>
    Cc: Al Viro <viro@ftp.linux.org.uk>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Michael Kerrisk <mtk-manpages@gmx.net>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 6eb9362775f9..598e5c27242c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1371,15 +1371,18 @@ static int unshare_thread(unsigned long unshare_flags)
 }
 
 /*
- * Unsharing of fs info for tasks created with CLONE_FS is not supported yet
+ * Unshare the filesystem structure if it is being shared
  */
 static int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)
 {
 	struct fs_struct *fs = current->fs;
 
 	if ((unshare_flags & CLONE_FS) &&
-	    (fs && atomic_read(&fs->count) > 1))
-		return -EINVAL;
+	    (fs && atomic_read(&fs->count) > 1)) {
+		*new_fsp = __copy_fs_struct(current->fs);
+		if (!*new_fsp)
+			return -ENOMEM;
+	}
 
 	return 0;
 }

commit cf2e340f4249b781b3d2beb41e891d08581f0e10
Author: JANAK DESAI <janak@us.ibm.com>
Date:   Tue Feb 7 12:58:58 2006 -0800

    [PATCH] unshare system call -v5: system call handler function
    
    sys_unshare system call handler function accepts the same flags as clone
    system call, checks constraints on each of the flags and invokes corresponding
    unshare functions to disassociate respective process context if it was being
    shared with another task.
    
    Here is the link to a program for testing unshare system call.
    
    http://prdownloads.sourceforge.net/audit/unshare_test.c?download
    
    Please note that because of a problem in rmdir associated with bind mounts and
    clone with CLONE_NEWNS, the test fails while trying to remove temporary test
    directory.  You can remove that temporary directory by doing rmdir, twice,
    from the command line.  The first will fail with EBUSY, but the second will
    succeed.  I have reported the problem to Ram Pai and Al Viro with a small
    program which reproduces the problem.  Al told us yesterday that he will be
    looking at the problem soon.  I have tried multiple rmdirs from the
    unshare_test program itself, but for some reason that is not working.  Doing
    two rmdirs from command line does seem to remove the directory.
    
    Signed-off-by: Janak Desai <janak@us.ibm.com>
    Cc: Al Viro <viro@ftp.linux.org.uk>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Michael Kerrisk <mtk-manpages@gmx.net>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7f0ab5ee948c..6eb9362775f9 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1323,3 +1323,235 @@ void __init proc_caches_init(void)
 			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
 }
+
+
+/*
+ * Check constraints on flags passed to the unshare system call and
+ * force unsharing of additional process context as appropriate.
+ */
+static inline void check_unshare_flags(unsigned long *flags_ptr)
+{
+	/*
+	 * If unsharing a thread from a thread group, must also
+	 * unshare vm.
+	 */
+	if (*flags_ptr & CLONE_THREAD)
+		*flags_ptr |= CLONE_VM;
+
+	/*
+	 * If unsharing vm, must also unshare signal handlers.
+	 */
+	if (*flags_ptr & CLONE_VM)
+		*flags_ptr |= CLONE_SIGHAND;
+
+	/*
+	 * If unsharing signal handlers and the task was created
+	 * using CLONE_THREAD, then must unshare the thread
+	 */
+	if ((*flags_ptr & CLONE_SIGHAND) &&
+	    (atomic_read(&current->signal->count) > 1))
+		*flags_ptr |= CLONE_THREAD;
+
+	/*
+	 * If unsharing namespace, must also unshare filesystem information.
+	 */
+	if (*flags_ptr & CLONE_NEWNS)
+		*flags_ptr |= CLONE_FS;
+}
+
+/*
+ * Unsharing of tasks created with CLONE_THREAD is not supported yet
+ */
+static int unshare_thread(unsigned long unshare_flags)
+{
+	if (unshare_flags & CLONE_THREAD)
+		return -EINVAL;
+
+	return 0;
+}
+
+/*
+ * Unsharing of fs info for tasks created with CLONE_FS is not supported yet
+ */
+static int unshare_fs(unsigned long unshare_flags, struct fs_struct **new_fsp)
+{
+	struct fs_struct *fs = current->fs;
+
+	if ((unshare_flags & CLONE_FS) &&
+	    (fs && atomic_read(&fs->count) > 1))
+		return -EINVAL;
+
+	return 0;
+}
+
+/*
+ * Unsharing of namespace for tasks created without CLONE_NEWNS is not
+ * supported yet
+ */
+static int unshare_namespace(unsigned long unshare_flags, struct namespace **new_nsp)
+{
+	struct namespace *ns = current->namespace;
+
+	if ((unshare_flags & CLONE_NEWNS) &&
+	    (ns && atomic_read(&ns->count) > 1))
+		return -EINVAL;
+
+	return 0;
+}
+
+/*
+ * Unsharing of sighand for tasks created with CLONE_SIGHAND is not
+ * supported yet
+ */
+static int unshare_sighand(unsigned long unshare_flags, struct sighand_struct **new_sighp)
+{
+	struct sighand_struct *sigh = current->sighand;
+
+	if ((unshare_flags & CLONE_SIGHAND) &&
+	    (sigh && atomic_read(&sigh->count) > 1))
+		return -EINVAL;
+	else
+		return 0;
+}
+
+/*
+ * Unsharing of vm for tasks created with CLONE_VM is not supported yet
+ */
+static int unshare_vm(unsigned long unshare_flags, struct mm_struct **new_mmp)
+{
+	struct mm_struct *mm = current->mm;
+
+	if ((unshare_flags & CLONE_VM) &&
+	    (mm && atomic_read(&mm->mm_users) > 1))
+		return -EINVAL;
+
+	return 0;
+
+}
+
+/*
+ * Unsharing of files for tasks created with CLONE_FILES is not supported yet
+ */
+static int unshare_fd(unsigned long unshare_flags, struct files_struct **new_fdp)
+{
+	struct files_struct *fd = current->files;
+
+	if ((unshare_flags & CLONE_FILES) &&
+	    (fd && atomic_read(&fd->count) > 1))
+		return -EINVAL;
+
+	return 0;
+}
+
+/*
+ * Unsharing of semundo for tasks created with CLONE_SYSVSEM is not
+ * supported yet
+ */
+static int unshare_semundo(unsigned long unshare_flags, struct sem_undo_list **new_ulistp)
+{
+	if (unshare_flags & CLONE_SYSVSEM)
+		return -EINVAL;
+
+	return 0;
+}
+
+/*
+ * unshare allows a process to 'unshare' part of the process
+ * context which was originally shared using clone.  copy_*
+ * functions used by do_fork() cannot be used here directly
+ * because they modify an inactive task_struct that is being
+ * constructed. Here we are modifying the current, active,
+ * task_struct.
+ */
+asmlinkage long sys_unshare(unsigned long unshare_flags)
+{
+	int err = 0;
+	struct fs_struct *fs, *new_fs = NULL;
+	struct namespace *ns, *new_ns = NULL;
+	struct sighand_struct *sigh, *new_sigh = NULL;
+	struct mm_struct *mm, *new_mm = NULL, *active_mm = NULL;
+	struct files_struct *fd, *new_fd = NULL;
+	struct sem_undo_list *new_ulist = NULL;
+
+	check_unshare_flags(&unshare_flags);
+
+	if ((err = unshare_thread(unshare_flags)))
+		goto bad_unshare_out;
+	if ((err = unshare_fs(unshare_flags, &new_fs)))
+		goto bad_unshare_cleanup_thread;
+	if ((err = unshare_namespace(unshare_flags, &new_ns)))
+		goto bad_unshare_cleanup_fs;
+	if ((err = unshare_sighand(unshare_flags, &new_sigh)))
+		goto bad_unshare_cleanup_ns;
+	if ((err = unshare_vm(unshare_flags, &new_mm)))
+		goto bad_unshare_cleanup_sigh;
+	if ((err = unshare_fd(unshare_flags, &new_fd)))
+		goto bad_unshare_cleanup_vm;
+	if ((err = unshare_semundo(unshare_flags, &new_ulist)))
+		goto bad_unshare_cleanup_fd;
+
+	if (new_fs || new_ns || new_sigh || new_mm || new_fd || new_ulist) {
+
+		task_lock(current);
+
+		if (new_fs) {
+			fs = current->fs;
+			current->fs = new_fs;
+			new_fs = fs;
+		}
+
+		if (new_ns) {
+			ns = current->namespace;
+			current->namespace = new_ns;
+			new_ns = ns;
+		}
+
+		if (new_sigh) {
+			sigh = current->sighand;
+			current->sighand = new_sigh;
+			new_sigh = sigh;
+		}
+
+		if (new_mm) {
+			mm = current->mm;
+			active_mm = current->active_mm;
+			current->mm = new_mm;
+			current->active_mm = new_mm;
+			activate_mm(active_mm, new_mm);
+			new_mm = mm;
+		}
+
+		if (new_fd) {
+			fd = current->files;
+			current->files = new_fd;
+			new_fd = fd;
+		}
+
+		task_unlock(current);
+	}
+
+bad_unshare_cleanup_fd:
+	if (new_fd)
+		put_files_struct(new_fd);
+
+bad_unshare_cleanup_vm:
+	if (new_mm)
+		mmput(new_mm);
+
+bad_unshare_cleanup_sigh:
+	if (new_sigh)
+		if (atomic_dec_and_test(&new_sigh->count))
+			kmem_cache_free(sighand_cachep, new_sigh);
+
+bad_unshare_cleanup_ns:
+	if (new_ns)
+		put_namespace(new_ns);
+
+bad_unshare_cleanup_fs:
+	if (new_fs)
+		put_fs_struct(new_fs);
+
+bad_unshare_cleanup_thread:
+bad_unshare_out:
+	return err;
+}

commit 7978672c4d9a1e6a6081de3a9d9ba5e5b24904a0
Author: George Anzinger <george@wildturkeyranch.net>
Date:   Wed Feb 1 03:05:11 2006 -0800

    [PATCH] hrtimers: cleanups and simplifications
    
    Clean up the interface to hrtimers by changing the init code to pass the mode
    as well as the clock.  This allow the init code to select the correct base and
    eliminates extra timer re-init code in posix-timers.  We also simplify the
    restart interface nanosleep use.
    
    Signed-off-by: George Anzinger <george@mvista.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4ae8cfc1c89c..7f0ab5ee948c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -802,7 +802,7 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	init_sigpending(&sig->shared_pending);
 	INIT_LIST_HEAD(&sig->posix_timers);
 
-	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC);
+	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC, HRTIMER_REL);
 	sig->it_real_incr.tv64 = 0;
 	sig->real_timer.function = it_real_fn;
 	sig->real_timer.data = tsk;

commit 5fd63b308569060ffa40af52ed122d9734111bff
Author: Ravikiran G Thirumalai <kiran@scalex86.org>
Date:   Wed Jan 11 22:46:15 2006 +0100

    [PATCH] x86_64: Inclusion of ScaleMP vSMP architecture patches - vsmp_align
    
    vSMP specific alignment patch to
    1. Define INTERNODE_CACHE_SHIFT for vSMP
    2. Use this for alignment of critical structures
    3. Use INTERNODE_CACHE_SHIFT for ARCH_MIN_TASKALIGN,
       and let the slab align task_struct allocations to the internode cacheline size
    4. Introduce and use ARCH_MIN_MMSTRUCT_ALIGN for mm_struct slab allocations.
    
    Signed-off-by: Ravikiran Thirumalai <kiran@scalemp.com>
    Signed-off-by: Shai Fultheim <shai@scalemp.com>
    Signed-off-by: Andi Kleen <ak@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 16a776ec2c0b..4ae8cfc1c89c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1298,6 +1298,10 @@ long do_fork(unsigned long clone_flags,
 	return pid;
 }
 
+#ifndef ARCH_MIN_MMSTRUCT_ALIGN
+#define ARCH_MIN_MMSTRUCT_ALIGN 0
+#endif
+
 void __init proc_caches_init(void)
 {
 	sighand_cachep = kmem_cache_create("sighand_cache",
@@ -1316,6 +1320,6 @@ void __init proc_caches_init(void)
 			sizeof(struct vm_area_struct), 0,
 			SLAB_PANIC, NULL, NULL);
 	mm_cachep = kmem_cache_create("mm_struct",
-			sizeof(struct mm_struct), 0,
+			sizeof(struct mm_struct), ARCH_MIN_MMSTRUCT_ALIGN,
 			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
 }

commit c59ede7b78db329949d9cdcd7064e22d357560ef
Author: Randy.Dunlap <rdunlap@xenotime.net>
Date:   Wed Jan 11 12:17:46 2006 -0800

    [PATCH] move capable() to capability.h
    
    - Move capable() from sched.h to capability.h;
    
    - Use <linux/capability.h> where capable() is used
            (in include/, block/, ipc/, kernel/, a few drivers/,
            mm/, security/, & sound/;
            many more drivers/ to go)
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 3bdcab49998d..16a776ec2c0b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -28,6 +28,7 @@
 #include <linux/binfmts.h>
 #include <linux/mman.h>
 #include <linux/fs.h>
+#include <linux/capability.h>
 #include <linux/cpu.h>
 #include <linux/cpuset.h>
 #include <linux/security.h>

commit 2ff678b8da6478d861c1b0ecb3ac14575760e906
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jan 9 20:52:34 2006 -0800

    [PATCH] hrtimer: switch itimers to hrtimer
    
    switch itimers to a hrtimers-based implementation
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b18d64554feb..3bdcab49998d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -801,10 +801,10 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	init_sigpending(&sig->shared_pending);
 	INIT_LIST_HEAD(&sig->posix_timers);
 
-	sig->it_real_value = sig->it_real_incr = 0;
+	hrtimer_init(&sig->real_timer, CLOCK_MONOTONIC);
+	sig->it_real_incr.tv64 = 0;
 	sig->real_timer.function = it_real_fn;
-	sig->real_timer.data = (unsigned long) tsk;
-	init_timer(&sig->real_timer);
+	sig->real_timer.data = tsk;
 
 	sig->it_virt_expires = cputime_zero;
 	sig->it_virt_incr = cputime_zero;

commit 408894ee4dd4debfdedd472eb4d8414892fc90f6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jan 9 15:59:20 2006 -0800

    [PATCH] mutex subsystem, debugging code
    
    mutex implementation - add debugging code.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 72e3252c6763..b18d64554feb 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -979,6 +979,10 @@ static task_t *copy_process(unsigned long clone_flags,
  	}
 #endif
 
+#ifdef CONFIG_DEBUG_MUTEXES
+	p->blocked_on = NULL; /* not blocked yet */
+#endif
+
 	p->tgid = p->pid;
 	if (clone_flags & CLONE_THREAD)
 		p->tgid = current->tgid;

commit fe7d37d1fbf8ffe78abd72728b24fb0c64f7af55
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Jan 8 01:04:02 2006 -0800

    [PATCH] copy_process: error path cleanup
    
    This patch moves 'fork_out:' under 'bad_fork_free:', and removes now
    unneeded 'if (retval)' check.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 4bc0bd8ef176..72e3252c6763 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1146,11 +1146,6 @@ static task_t *copy_process(unsigned long clone_flags,
 	total_forks++;
 	write_unlock_irq(&tasklist_lock);
 	proc_fork_connector(p);
-	retval = 0;
-
-fork_out:
-	if (retval)
-		return ERR_PTR(retval);
 	return p;
 
 bad_fork_cleanup_namespace:
@@ -1191,7 +1186,8 @@ static task_t *copy_process(unsigned long clone_flags,
 	free_uid(p->user);
 bad_fork_free:
 	free_task(p);
-	goto fork_out;
+fork_out:
+	return ERR_PTR(retval);
 }
 
 struct pt_regs * __devinit __attribute__((weak)) idle_regs(struct pt_regs *regs)

commit 9a5d3023e626a0baf86ac6b892c983b3db13f22b
Author: Oren Laadan <orenl@cs.columbia.edu>
Date:   Sun Jan 8 01:03:51 2006 -0800

    [PATCH] fork: fix race in setting child's pgrp and tty
    
    In fork, child should recopy parent's pgrp/tty after it has tasklist_lock.
    Otherwise following a setpgid() on the parent, *after* copy_signal(), the
    child will own a stale pgrp (which may be reused); (eg.  if copy_mm()
    sleeps a long while due to memory pressure).  Similar issue for the tty.
    
    Signed-off-by: Oren Laadan <orenl@cs.columbia.edu>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7992ee759d89..4bc0bd8ef176 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -811,9 +811,6 @@ static inline int copy_signal(unsigned long clone_flags, struct task_struct * ts
 	sig->it_prof_expires = cputime_zero;
 	sig->it_prof_incr = cputime_zero;
 
-	sig->tty = current->signal->tty;
-	sig->pgrp = process_group(current);
-	sig->session = current->signal->session;
 	sig->leader = 0;	/* session leadership doesn't inherit */
 	sig->tty_old_pgrp = 0;
 
@@ -1136,15 +1133,15 @@ static task_t *copy_process(unsigned long clone_flags,
 	attach_pid(p, PIDTYPE_PID, p->pid);
 	attach_pid(p, PIDTYPE_TGID, p->tgid);
 	if (thread_group_leader(p)) {
+		p->signal->tty = current->signal->tty;
+		p->signal->pgrp = process_group(current);
+		p->signal->session = current->signal->session;
 		attach_pid(p, PIDTYPE_PGID, process_group(p));
 		attach_pid(p, PIDTYPE_SID, p->signal->session);
 		if (p->pid)
 			__get_cpu_var(process_counts)++;
 	}
 
-	if (!current->signal->tty && p->signal->tty)
-		p->signal->tty = NULL;
-
 	nr_threads++;
 	total_forks++;
 	write_unlock_irq(&tasklist_lock);

commit b4b2641843db124637fa3d2cb2101982035dcc82
Author: Paul Jackson <pj@sgi.com>
Date:   Sun Jan 8 01:01:53 2006 -0800

    [PATCH] cpuset: fork hook fix
    
    Fix obscure, never seen in real life, cpuset fork race.  The cpuset_fork()
    call in fork.c was setting up the correct task->cpuset pointer after the
    tasklist_lock was dropped, which briefly exposed the newly forked process with
    an unsafe (copied from parent without locks or usage counter increment) cpuset
    pointer.
    
    In theory, that exposed cpuset pointer could have been pointing at a cpuset
    that was already freed and removed, and in theory another task that had been
    sitting on the tasklist_lock waiting to scan the task list could have raced
    down the entire tasklist, found our new child at the far end, and dereferenced
    that bogus cpuset pointer.
    
    To fix, setup up the correct cpuset pointer in the new child by calling
    cpuset_fork() before the new task is linked into the tasklist, and with that,
    add a fork failure case, to dereference that cpuset, if the fork fails along
    the way, after cpuset_fork() was called.
    
    Had to remove a BUG_ON() from cpuset_exit(), because it was no longer valid -
    the call to cpuset_exit() from a failed fork would not have PF_EXITING set.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7fe3adfa65cb..7992ee759d89 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -972,12 +972,13 @@ static task_t *copy_process(unsigned long clone_flags,
 	p->io_context = NULL;
 	p->io_wait = NULL;
 	p->audit_context = NULL;
+	cpuset_fork(p);
 #ifdef CONFIG_NUMA
  	p->mempolicy = mpol_copy(p->mempolicy);
  	if (IS_ERR(p->mempolicy)) {
  		retval = PTR_ERR(p->mempolicy);
  		p->mempolicy = NULL;
- 		goto bad_fork_cleanup;
+ 		goto bad_fork_cleanup_cpuset;
  	}
 #endif
 
@@ -1148,7 +1149,6 @@ static task_t *copy_process(unsigned long clone_flags,
 	total_forks++;
 	write_unlock_irq(&tasklist_lock);
 	proc_fork_connector(p);
-	cpuset_fork(p);
 	retval = 0;
 
 fork_out:
@@ -1180,7 +1180,9 @@ static task_t *copy_process(unsigned long clone_flags,
 bad_fork_cleanup_policy:
 #ifdef CONFIG_NUMA
 	mpol_free(p->mempolicy);
+bad_fork_cleanup_cpuset:
 #endif
+	cpuset_exit(p);
 bad_fork_cleanup:
 	if (p->binfmt)
 		module_put(p->binfmt->module);

commit e56d090310d7625ecb43a1eeebd479f04affb48b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 8 01:01:37 2006 -0800

    [PATCH] RCU signal handling
    
    RCU tasklist_lock and RCU signal handling: send signals RCU-read-locked
    instead of tasklist_lock read-locked.  This is a scalability improvement on
    SMP and a preemption-latency improvement under PREEMPT_RCU.
    
    Signed-off-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: William Irwin <wli@holomorphy.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index fb8572a42297..7fe3adfa65cb 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -743,6 +743,14 @@ int unshare_files(void)
 
 EXPORT_SYMBOL(unshare_files);
 
+void sighand_free_cb(struct rcu_head *rhp)
+{
+	struct sighand_struct *sp;
+
+	sp = container_of(rhp, struct sighand_struct, rcu);
+	kmem_cache_free(sighand_cachep, sp);
+}
+
 static inline int copy_sighand(unsigned long clone_flags, struct task_struct * tsk)
 {
 	struct sighand_struct *sig;
@@ -752,7 +760,7 @@ static inline int copy_sighand(unsigned long clone_flags, struct task_struct * t
 		return 0;
 	}
 	sig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);
-	tsk->sighand = sig;
+	rcu_assign_pointer(tsk->sighand, sig);
 	if (!sig)
 		return -ENOMEM;
 	spin_lock_init(&sig->siglock);

commit 8c4b8add83c93306b07d78469fd351dc462e4b66
Author: Paul Jackson <pj@sgi.com>
Date:   Mon Nov 28 13:44:05 2005 -0800

    [PATCH] cpuset fork locking fix
    
    Move the cpuset_fork() call below the write_unlock_irq call in
    kernel/fork.c copy_process().
    
    Since the cpuset-dual-semaphore-locking-overhaul.patch, the cpuset_fork()
    routine acquires task_lock(), so cannot be called while holding the
    tasklist_lock for write.
    
    Signed-off-by: Paul Jackson <pj@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index d0d49879ab7c..fb8572a42297 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1124,8 +1124,6 @@ static task_t *copy_process(unsigned long clone_flags,
 	if (unlikely(p->ptrace & PT_PTRACED))
 		__ptrace_link(p, current->parent);
 
-	cpuset_fork(p);
-
 	attach_pid(p, PIDTYPE_PID, p->pid);
 	attach_pid(p, PIDTYPE_TGID, p->tgid);
 	if (thread_group_leader(p)) {
@@ -1142,6 +1140,7 @@ static task_t *copy_process(unsigned long clone_flags,
 	total_forks++;
 	write_unlock_irq(&tasklist_lock);
 	proc_fork_connector(p);
+	cpuset_fork(p);
 	retval = 0;
 
 fork_out:

commit c13cf856cbe16aec3007604dc013cbf3a16c6686
Author: Andrew Morton <akpm@osdl.org>
Date:   Mon Nov 28 13:43:48 2005 -0800

    [PATCH] fork.c: proc_fork_connector() called under write_lock()
    
    Don't do that - it does GFP_KERNEL allocations, for a start.
    
    (Reported by Guillaume Thouvenin <guillaume.thouvenin@bull.net>)
    
    Acked-by: Matt Helsley <matthltc@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 1c1cf8dc396b..d0d49879ab7c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1135,13 +1135,13 @@ static task_t *copy_process(unsigned long clone_flags,
 			__get_cpu_var(process_counts)++;
 	}
 
-	proc_fork_connector(p);
 	if (!current->signal->tty && p->signal->tty)
 		p->signal->tty = NULL;
 
 	nr_threads++;
 	total_forks++;
 	write_unlock_irq(&tasklist_lock);
+	proc_fork_connector(p);
 	retval = 0;
 
 fork_out:

commit 0b0db14c536debd92328819fe6c51a49717e8440
Author: Hugh Dickins <hugh@veritas.com>
Date:   Mon Nov 21 21:32:20 2005 -0800

    [PATCH] unpaged: copy_page_range vma
    
    For copy_one_pte's print_bad_pte to show the task correctly (instead of
    "???"), dup_mmap must pass down parent vma rather than child vma.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e0d0b77343f8..1c1cf8dc396b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -263,7 +263,7 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		rb_parent = &tmp->vm_rb;
 
 		mm->map_count++;
-		retval = copy_page_range(mm, oldmm, tmp);
+		retval = copy_page_range(mm, oldmm, mpnt);
 
 		if (tmp->vm_ops && tmp->vm_ops->open)
 			tmp->vm_ops->open(tmp);

commit 20dcae32439384b6863c626bb3b2a09bed65b33e
Author: Zach Brown <zach.brown@oracle.com>
Date:   Sun Nov 13 16:07:33 2005 -0800

    [PATCH] aio: remove kioctx from mm_struct
    
    Sync iocbs have a life cycle that don't need a kioctx.  Their retrying, if
    any, is done in the context of their owner who has allocated them on the
    stack.
    
    The sole user of a sync iocb's ctx reference was aio_complete() checking for
    an elevated iocb ref count that could never happen.  No path which grabs an
    iocb ref has access to sync iocbs.
    
    If we were to implement sync iocb cancelation it would be done by the owner of
    the iocb using its on-stack reference.
    
    Removing this chunk from aio_complete allows us to remove the entire kioctx
    instance from mm_struct, reducing its size by a third.  On a i386 testing box
    the slab size went from 768 to 504 bytes and from 5 to 8 per page.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>
    Acked-by: Benjamin LaHaise <bcrl@kvack.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2c70c9cdf5dc..e0d0b77343f8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -323,7 +323,6 @@ static struct mm_struct * mm_init(struct mm_struct * mm)
 	spin_lock_init(&mm->page_table_lock);
 	rwlock_init(&mm->ioctx_list_lock);
 	mm->ioctx_list = NULL;
-	mm->default_kioctx = (struct kioctx)INIT_KIOCTX(mm->default_kioctx, *mm);
 	mm->free_area_cache = TASK_UNMAPPED_BASE;
 	mm->cached_hole_size = ~0UL;
 

commit 10ebffde3d3916026974352b7900e44afe2b243f
Author: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
Date:   Sun Nov 13 16:06:56 2005 -0800

    [PATCH] m68k: introduce setup_thread_stack() and end_of_stack()
    
    encapsulates the rest of arch-dependent operations with thread_info access.
    Two new helpers - setup_thread_stack() and end_of_stack().  For normal case
    the former consists of copying thread_info of parent to new thread_info and
    the latter returns pointer immediately past the end of thread_info.
    
    Signed-off-by: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7ef352ce347b..2c70c9cdf5dc 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -171,10 +171,9 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 		return NULL;
 	}
 
-	*ti = *orig->thread_info;
 	*tsk = *orig;
 	tsk->thread_info = ti;
-	ti->task = tsk;
+	setup_thread_stack(tsk, orig);
 
 	/* One for us, one for whoever does the "release_task()" (usually parent) */
 	atomic_set(&tsk->usage,2);

commit a1261f54611ec4ad6a7ab7080f86747e3ac3685b
Author: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
Date:   Sun Nov 13 16:06:55 2005 -0800

    [PATCH] m68k: introduce task_thread_info
    
    new helper - task_thread_info(task).  On platforms that have thread_info
    allocated separately (i.e.  in default case) it simply returns
    task->thread_info.  m68k wants (and for good reasons) to embed its thread_info
    into task_struct.  So it will (in later patch) have task_thread_info() of its
    own.  For now we just add a macro for generic case and convert existing
    instances of its body in core kernel to uses of new macro.  Obviously safe -
    all normal architectures get the same preprocessor output they used to get.
    
    Signed-off-by: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 158710d22566..7ef352ce347b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -919,7 +919,7 @@ static task_t *copy_process(unsigned long clone_flags,
 	if (nr_threads >= max_threads)
 		goto bad_fork_cleanup_count;
 
-	if (!try_module_get(p->thread_info->exec_domain->module))
+	if (!try_module_get(task_thread_info(p)->exec_domain->module))
 		goto bad_fork_cleanup_count;
 
 	if (p->binfmt && !try_module_get(p->binfmt->module))
@@ -1180,7 +1180,7 @@ static task_t *copy_process(unsigned long clone_flags,
 	if (p->binfmt)
 		module_put(p->binfmt->module);
 bad_fork_cleanup_put_domain:
-	module_put(p->thread_info->exec_domain->module);
+	module_put(task_thread_info(p)->exec_domain->module);
 bad_fork_cleanup_count:
 	put_group_info(p->group_info);
 	atomic_dec(&p->user->processes);

commit dedeb0029b9c83420fc1337d4ee53daa7b2a0ad4
Author: Hugh Dickins <hugh@veritas.com>
Date:   Mon Nov 7 14:09:01 2005 -0800

    [SPARC64] mm: context switch ptlock
    
    sparc64 is unique among architectures in taking the page_table_lock in
    its context switch (well, cris does too, but erroneously, and it's not
    yet SMP anyway).
    
    This seems to be a private affair between switch_mm and activate_mm,
    using page_table_lock as a per-mm lock, without any relation to its uses
    elsewhere.  That's fine, but comment it as such; and unlock sooner in
    switch_mm, more like in activate_mm (preemption is disabled here).
    
    There is a block of "if (0)"ed code in smp_flush_tlb_pending which would
    have liked to rely on the page_table_lock, in switch_mm and elsewhere;
    but its comment explains how dup_mmap's flush_tlb_mm defeated it.  And
    though that could have been changed at any time over the past few years,
    now the chance vanishes as we push the page_table_lock downwards, and
    perhaps split it per page table page.  Just delete that block of code.
    
    Which leaves the mysterious spin_unlock_wait(&oldmm->page_table_lock)
    in kernel/fork.c copy_mm.  Textual analysis (supported by Nick Piggin)
    suggests that the comment was written by DaveM, and that it relates to
    the defeated approach in the sparc64 smp_flush_tlb_pending.  Just delete
    this block too.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/fork.c b/kernel/fork.c
index efac2c58ec7d..158710d22566 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -470,13 +470,6 @@ static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
 	if (clone_flags & CLONE_VM) {
 		atomic_inc(&oldmm->mm_users);
 		mm = oldmm;
-		/*
-		 * There are cases where the PTL is held to ensure no
-		 * new threads start up in user mode using an mm, which
-		 * allows optimizing out ipis; the tlb_gather_mmu code
-		 * is an example.
-		 */
-		spin_unlock_wait(&oldmm->page_table_lock);
 		goto good_mm;
 	}
 

commit 9f46080c41d5f3f7c00b4e169ba4b0b2865258bf
Author: Matt Helsley <matthltc@us.ibm.com>
Date:   Mon Nov 7 00:59:16 2005 -0800

    [PATCH] Process Events Connector
    
    This patch adds a connector that reports fork, exec, id change, and exit
    events for all processes to userspace.  It replaces the fork_advisor patch
    that ELSA is currently using.  Applications that may find these events
    useful include accounting/auditing (e.g.  ELSA), system activity monitoring
    (e.g.  top), security, and resource management (e.g.  CKRM).
    
    Signed-off-by: Matt Helsley <matthltc@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8a069612eac3..efac2c58ec7d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -42,6 +42,7 @@
 #include <linux/profile.h>
 #include <linux/rmap.h>
 #include <linux/acct.h>
+#include <linux/cn_proc.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -1143,6 +1144,7 @@ static task_t *copy_process(unsigned long clone_flags,
 			__get_cpu_var(process_counts)++;
 	}
 
+	proc_fork_connector(p);
 	if (!current->signal->tty && p->signal->tty)
 		p->signal->tty = NULL;
 

commit c74df32c724a1652ad8399b4891bb02c9d43743a
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:23 2005 -0700

    [PATCH] mm: ptd_alloc take ptlock
    
    Second step in pushing down the page_table_lock.  Remove the temporary
    bridging hack from __pud_alloc, __pmd_alloc, __pte_alloc: expect callers not
    to hold page_table_lock, whether it's on init_mm or a user mm; take
    page_table_lock internally to check if a racing task already allocated.
    
    Convert their callers from common code.  But avoid coming back to change them
    again later: instead of moving the spin_lock(&mm->page_table_lock) down,
    switch over to new macros pte_alloc_map_lock and pte_unmap_unlock, which
    encapsulate the mapping+locking and unlocking+unmapping together, and in the
    end may use alternatives to the mm page_table_lock itself.
    
    These callers all hold mmap_sem (some exclusively, some not), so at no level
    can a page table be whipped away from beneath them; and pte_alloc uses the
    "atomic" pmd_present to test whether it needs to allocate.  It appears that on
    all arches we can safely descend without page_table_lock.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2a587b3224e3..8a069612eac3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -255,7 +255,6 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		/*
 		 * Link in the new vma and copy the page table entries.
 		 */
-		spin_lock(&mm->page_table_lock);
 		*pprev = tmp;
 		pprev = &tmp->vm_next;
 
@@ -265,7 +264,6 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 
 		mm->map_count++;
 		retval = copy_page_range(mm, oldmm, tmp);
-		spin_unlock(&mm->page_table_lock);
 
 		if (tmp->vm_ops && tmp->vm_ops->open)
 			tmp->vm_ops->open(tmp);

commit 7ee78232501ea9de2b6c8f10d32c9a0fee541357
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:08 2005 -0700

    [PATCH] mm: dup_mmap down new mmap_sem
    
    One anomaly remains from when Andrea rationalized the responsibilities of
    mmap_sem and page_table_lock: in dup_mmap we add vmas to the child holding its
    page_table_lock, but not the mmap_sem which normally guards the vma list and
    rbtree.  Which could be an issue for unuse_mm: though since it just walks down
    the list (today with page_table_lock, tomorrow not), it's probably okay.  Will
    need a memory barrier?  Oh, keep it simple, Nick and I agreed, no harm in
    taking child's mmap_sem here.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 0e7fe4a8a8df..2a587b3224e3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -192,6 +192,8 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 
 	down_write(&oldmm->mmap_sem);
 	flush_cache_mm(oldmm);
+	down_write(&mm->mmap_sem);
+
 	mm->locked_vm = 0;
 	mm->mmap = NULL;
 	mm->mmap_cache = NULL;
@@ -251,10 +253,7 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 		}
 
 		/*
-		 * Link in the new vma and copy the page table entries:
-		 * link in first so that swapoff can see swap entries.
-		 * Note that, exceptionally, here the vma is inserted
-		 * without holding mm->mmap_sem.
+		 * Link in the new vma and copy the page table entries.
 		 */
 		spin_lock(&mm->page_table_lock);
 		*pprev = tmp;
@@ -275,8 +274,8 @@ static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 			goto out;
 	}
 	retval = 0;
-
 out:
+	up_write(&mm->mmap_sem);
 	flush_tlb_mm(oldmm);
 	up_write(&oldmm->mmap_sem);
 	return retval;

commit fd3e42fcc888a773572282575d2fdbf5cfd6216e
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:06 2005 -0700

    [PATCH] mm: dup_mmap use oldmm more
    
    Use the parent's oldmm throughout dup_mmap, instead of perversely going back
    to current->mm.  (Can you hear the sigh of relief from those mpnts?  Usually I
    squash them, but not today.)
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2048ed7b5872..0e7fe4a8a8df 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -182,16 +182,16 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 }
 
 #ifdef CONFIG_MMU
-static inline int dup_mmap(struct mm_struct * mm, struct mm_struct * oldmm)
+static inline int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)
 {
-	struct vm_area_struct * mpnt, *tmp, **pprev;
+	struct vm_area_struct *mpnt, *tmp, **pprev;
 	struct rb_node **rb_link, *rb_parent;
 	int retval;
 	unsigned long charge;
 	struct mempolicy *pol;
 
 	down_write(&oldmm->mmap_sem);
-	flush_cache_mm(current->mm);
+	flush_cache_mm(oldmm);
 	mm->locked_vm = 0;
 	mm->mmap = NULL;
 	mm->mmap_cache = NULL;
@@ -204,7 +204,7 @@ static inline int dup_mmap(struct mm_struct * mm, struct mm_struct * oldmm)
 	rb_parent = NULL;
 	pprev = &mm->mmap;
 
-	for (mpnt = current->mm->mmap ; mpnt ; mpnt = mpnt->vm_next) {
+	for (mpnt = oldmm->mmap; mpnt; mpnt = mpnt->vm_next) {
 		struct file *file;
 
 		if (mpnt->vm_flags & VM_DONTCOPY) {
@@ -265,7 +265,7 @@ static inline int dup_mmap(struct mm_struct * mm, struct mm_struct * oldmm)
 		rb_parent = &tmp->vm_rb;
 
 		mm->map_count++;
-		retval = copy_page_range(mm, current->mm, tmp);
+		retval = copy_page_range(mm, oldmm, tmp);
 		spin_unlock(&mm->page_table_lock);
 
 		if (tmp->vm_ops && tmp->vm_ops->open)
@@ -277,7 +277,7 @@ static inline int dup_mmap(struct mm_struct * mm, struct mm_struct * oldmm)
 	retval = 0;
 
 out:
-	flush_tlb_mm(current->mm);
+	flush_tlb_mm(oldmm);
 	up_write(&oldmm->mmap_sem);
 	return retval;
 fail_nomem_policy:

commit 4294621f41a85497019fae64341aa5351a1921b7
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:05 2005 -0700

    [PATCH] mm: rss = file_rss + anon_rss
    
    I was lazy when we added anon_rss, and chose to change as few places as
    possible.  So currently each anonymous page has to be counted twice, in rss
    and in anon_rss.  Which won't be so good if those are atomic counts in some
    configurations.
    
    Change that around: keep file_rss and anon_rss separately, and add them
    together (with get_mm_rss macro) when the total is needed - reading two
    atomics is much cheaper than updating two atomics.  And update anon_rss
    upfront, typically in memory.c, not tucked away in page_add_anon_rmap.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 25caa02e2eac..2048ed7b5872 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -321,7 +321,7 @@ static struct mm_struct * mm_init(struct mm_struct * mm)
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->core_waiters = 0;
 	mm->nr_ptes = 0;
-	set_mm_counter(mm, rss, 0);
+	set_mm_counter(mm, file_rss, 0);
 	set_mm_counter(mm, anon_rss, 0);
 	spin_lock_init(&mm->page_table_lock);
 	rwlock_init(&mm->ioctx_list_lock);
@@ -499,7 +499,7 @@ static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
 	if (retval)
 		goto free_pt;
 
-	mm->hiwater_rss = get_mm_counter(mm,rss);
+	mm->hiwater_rss = get_mm_rss(mm);
 	mm->hiwater_vm = mm->total_vm;
 
 good_mm:

commit 404351e67a9facb475abf1492245374a28d13e90
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:04 2005 -0700

    [PATCH] mm: mm_init set_mm_counters
    
    How is anon_rss initialized?  In dup_mmap, and by mm_alloc's memset; but
    that's not so good if an mm_counter_t is a special type.  And how is rss
    initialized?  By set_mm_counter, all over the place.  Come on, we just need to
    initialize them both at once by set_mm_counter in mm_init (which follows the
    memcpy when forking).
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index e2ff11f8c1b0..25caa02e2eac 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -198,8 +198,6 @@ static inline int dup_mmap(struct mm_struct * mm, struct mm_struct * oldmm)
 	mm->free_area_cache = oldmm->mmap_base;
 	mm->cached_hole_size = ~0UL;
 	mm->map_count = 0;
-	set_mm_counter(mm, rss, 0);
-	set_mm_counter(mm, anon_rss, 0);
 	cpus_clear(mm->cpu_vm_mask);
 	mm->mm_rb = RB_ROOT;
 	rb_link = &mm->mm_rb.rb_node;
@@ -323,6 +321,8 @@ static struct mm_struct * mm_init(struct mm_struct * mm)
 	INIT_LIST_HEAD(&mm->mmlist);
 	mm->core_waiters = 0;
 	mm->nr_ptes = 0;
+	set_mm_counter(mm, rss, 0);
+	set_mm_counter(mm, anon_rss, 0);
 	spin_lock_init(&mm->page_table_lock);
 	rwlock_init(&mm->ioctx_list_lock);
 	mm->ioctx_list = NULL;

commit ab50b8ed818016cfecd747d6d4bb9139986bc029
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:15:56 2005 -0700

    [PATCH] mm: vm_stat_account unshackled
    
    The original vm_stat_account has fallen into disuse, with only one user, and
    only one user of vm_stat_unaccount.  It's easier to keep track if we convert
    them all to __vm_stat_account, then free it from its __shackles.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 280bd44ac441..e2ff11f8c1b0 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -212,7 +212,7 @@ static inline int dup_mmap(struct mm_struct * mm, struct mm_struct * oldmm)
 		if (mpnt->vm_flags & VM_DONTCOPY) {
 			long pages = vma_pages(mpnt);
 			mm->total_vm -= pages;
-			__vm_stat_account(mm, mpnt->vm_flags, mpnt->vm_file,
+			vm_stat_account(mm, mpnt->vm_flags, mpnt->vm_file,
 								-pages);
 			continue;
 		}

commit d1209d049bbc3df66650f8417637be4f7b57b604
Author: Alan Stern <stern@rowland.harvard.edu>
Date:   Wed Oct 19 21:23:51 2005 -0700

    [PATCH] Threads shouldn't inherit PF_NOFREEZE
    
    The PF_NOFREEZE process flag should not be inherited when a thread is
    forked.  This patch (as585) removes the flag from the child.
    
    This problem is starting to show up more and more as drivers turn to the
    kthread API instead of using kernel_thread().  As a result, their kernel
    threads are now children of the kthread worker instead of modprobe, and
    they inherit the PF_NOFREEZE flag.  This can cause problems during system
    suspend; the kernel threads are not getting frozen as they ought to be.
    
    Signed-off-by: Alan Stern <stern@rowland.harvard.edu>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 533ce27f4b2c..280bd44ac441 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -848,7 +848,7 @@ static inline void copy_flags(unsigned long clone_flags, struct task_struct *p)
 {
 	unsigned long new_flags = p->flags;
 
-	new_flags &= ~PF_SUPERPRIV;
+	new_flags &= ~(PF_SUPERPRIV | PF_NOFREEZE);
 	new_flags |= PF_FORKNOEXEC;
 	if (!(clone_flags & CLONE_PTRACE))
 		p->ptrace = 0;

commit 26ff6ad9786abf6f40a6d3cbb89753b4fa50cb00
Author: Srivatsa Vaddagiri <vatsa@in.ibm.com>
Date:   Fri Sep 16 19:27:40 2005 -0700

    [PATCH] CPU hotplug breaks wake_up_new_task
    
    Fix a problem wherein a new-born task is added to a dead CPU.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Acked-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Acked-by: Shaohua Li <shaohua.li@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 8149f3602881..533ce27f4b2c 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1062,7 +1062,8 @@ static task_t *copy_process(unsigned long clone_flags,
 	 * parent's CPU). This avoids alot of nasty races.
 	 */
 	p->cpus_allowed = current->cpus_allowed;
-	if (unlikely(!cpu_isset(task_cpu(p), p->cpus_allowed)))
+	if (unlikely(!cpu_isset(task_cpu(p), p->cpus_allowed) ||
+			!cpu_online(task_cpu(p))))
 		set_task_cpu(p, smp_processor_id());
 
 	/*

commit ab2af1f5005069321c5d130f09cce577b03f43ef
Author: Dipankar Sarma <dipankar@in.ibm.com>
Date:   Fri Sep 9 13:04:13 2005 -0700

    [PATCH] files: files struct with RCU
    
    Patch to eliminate struct files_struct.file_lock spinlock on the reader side
    and use rcu refcounting rcuref_xxx api for the f_count refcounter.  The
    updates to the fdtable are done by allocating a new fdtable structure and
    setting files->fdt to point to the new structure.  The fdtable structure is
    protected by RCU thereby allowing lock-free lookup.  For fd arrays/sets that
    are vmalloced, we use keventd to free them since RCU callbacks can't sleep.  A
    global list of fdtable to be freed is not scalable, so we use a per-cpu list.
    If keventd is already handling the current cpu's work, we use a timer to defer
    queueing of that work.
    
    Since the last publication, this patch has been re-written to avoid using
    explicit memory barriers and use rcu_assign_pointer(), rcu_dereference()
    premitives instead.  This required that the fd information is kept in a
    separate structure (fdtable) and updated atomically.
    
    Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index ecc694debb50..8149f3602881 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -35,6 +35,7 @@
 #include <linux/syscalls.h>
 #include <linux/jiffies.h>
 #include <linux/futex.h>
+#include <linux/rcupdate.h>
 #include <linux/ptrace.h>
 #include <linux/mount.h>
 #include <linux/audit.h>
@@ -565,13 +566,12 @@ static inline int copy_fs(unsigned long clone_flags, struct task_struct * tsk)
 	return 0;
 }
 
-static int count_open_files(struct files_struct *files, int size)
+static int count_open_files(struct fdtable *fdt)
 {
+	int size = fdt->max_fdset;
 	int i;
-	struct fdtable *fdt;
 
 	/* Find the last open fd */
-	fdt = files_fdtable(files);
 	for (i = size/(8*sizeof(long)); i > 0; ) {
 		if (fdt->open_fds->fds_bits[--i])
 			break;
@@ -592,13 +592,17 @@ static struct files_struct *alloc_files(void)
 	atomic_set(&newf->count, 1);
 
 	spin_lock_init(&newf->file_lock);
-	fdt = files_fdtable(newf);
+	fdt = &newf->fdtab;
 	fdt->next_fd = 0;
 	fdt->max_fds = NR_OPEN_DEFAULT;
 	fdt->max_fdset = __FD_SETSIZE;
 	fdt->close_on_exec = &newf->close_on_exec_init;
 	fdt->open_fds = &newf->open_fds_init;
 	fdt->fd = &newf->fd_array[0];
+	INIT_RCU_HEAD(&fdt->rcu);
+	fdt->free_files = NULL;
+	fdt->next = NULL;
+	rcu_assign_pointer(newf->fdt, fdt);
 out:
 	return newf;
 }
@@ -637,7 +641,7 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 	old_fdt = files_fdtable(oldf);
 	new_fdt = files_fdtable(newf);
 	size = old_fdt->max_fdset;
-	open_files = count_open_files(oldf, old_fdt->max_fdset);
+	open_files = count_open_files(old_fdt);
 	expand = 0;
 
 	/*
@@ -661,7 +665,14 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 		spin_unlock(&newf->file_lock);
 		if (error < 0)
 			goto out_release;
+		new_fdt = files_fdtable(newf);
+		/*
+		 * Reacquire the oldf lock and a pointer to its fd table
+		 * who knows it may have a new bigger fd table. We need
+		 * the latest pointer.
+		 */
 		spin_lock(&oldf->file_lock);
+		old_fdt = files_fdtable(oldf);
 	}
 
 	old_fds = old_fdt->fd;
@@ -683,7 +694,7 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 			 */
 			FD_CLR(open_files - i, new_fdt->open_fds);
 		}
-		*new_fds++ = f;
+		rcu_assign_pointer(*new_fds++, f);
 	}
 	spin_unlock(&oldf->file_lock);
 

commit badf16621c1f9d1ac753be056fce11b43d6e0be5
Author: Dipankar Sarma <dipankar@in.ibm.com>
Date:   Fri Sep 9 13:04:10 2005 -0700

    [PATCH] files: break up files struct
    
    In order for the RCU to work, the file table array, sets and their sizes must
    be updated atomically.  Instead of ensuring this through too many memory
    barriers, we put the arrays and their sizes in a separate structure.  This
    patch takes the first step of putting the file table elements in a separate
    structure fdtable that is embedded withing files_struct.  It also changes all
    the users to refer to the file table using files_fdtable() macro.  Subsequent
    applciation of RCU becomes easier after this.
    
    Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-Off-By: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b25802065031..ecc694debb50 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -568,21 +568,47 @@ static inline int copy_fs(unsigned long clone_flags, struct task_struct * tsk)
 static int count_open_files(struct files_struct *files, int size)
 {
 	int i;
+	struct fdtable *fdt;
 
 	/* Find the last open fd */
+	fdt = files_fdtable(files);
 	for (i = size/(8*sizeof(long)); i > 0; ) {
-		if (files->open_fds->fds_bits[--i])
+		if (fdt->open_fds->fds_bits[--i])
 			break;
 	}
 	i = (i+1) * 8 * sizeof(long);
 	return i;
 }
 
+static struct files_struct *alloc_files(void)
+{
+	struct files_struct *newf;
+	struct fdtable *fdt;
+
+	newf = kmem_cache_alloc(files_cachep, SLAB_KERNEL);
+	if (!newf)
+		goto out;
+
+	atomic_set(&newf->count, 1);
+
+	spin_lock_init(&newf->file_lock);
+	fdt = files_fdtable(newf);
+	fdt->next_fd = 0;
+	fdt->max_fds = NR_OPEN_DEFAULT;
+	fdt->max_fdset = __FD_SETSIZE;
+	fdt->close_on_exec = &newf->close_on_exec_init;
+	fdt->open_fds = &newf->open_fds_init;
+	fdt->fd = &newf->fd_array[0];
+out:
+	return newf;
+}
+
 static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 {
 	struct files_struct *oldf, *newf;
 	struct file **old_fds, **new_fds;
 	int open_files, size, i, error = 0, expand;
+	struct fdtable *old_fdt, *new_fdt;
 
 	/*
 	 * A background process may not have any files ...
@@ -603,35 +629,27 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 	 */
 	tsk->files = NULL;
 	error = -ENOMEM;
-	newf = kmem_cache_alloc(files_cachep, SLAB_KERNEL);
-	if (!newf) 
+	newf = alloc_files();
+	if (!newf)
 		goto out;
 
-	atomic_set(&newf->count, 1);
-
-	spin_lock_init(&newf->file_lock);
-	newf->next_fd	    = 0;
-	newf->max_fds	    = NR_OPEN_DEFAULT;
-	newf->max_fdset	    = __FD_SETSIZE;
-	newf->close_on_exec = &newf->close_on_exec_init;
-	newf->open_fds	    = &newf->open_fds_init;
-	newf->fd	    = &newf->fd_array[0];
-
 	spin_lock(&oldf->file_lock);
-
-	open_files = count_open_files(oldf, oldf->max_fdset);
+	old_fdt = files_fdtable(oldf);
+	new_fdt = files_fdtable(newf);
+	size = old_fdt->max_fdset;
+	open_files = count_open_files(oldf, old_fdt->max_fdset);
 	expand = 0;
 
 	/*
 	 * Check whether we need to allocate a larger fd array or fd set.
 	 * Note: we're not a clone task, so the open count won't  change.
 	 */
-	if (open_files > newf->max_fdset) {
-		newf->max_fdset = 0;
+	if (open_files > new_fdt->max_fdset) {
+		new_fdt->max_fdset = 0;
 		expand = 1;
 	}
-	if (open_files > newf->max_fds) {
-		newf->max_fds = 0;
+	if (open_files > new_fdt->max_fds) {
+		new_fdt->max_fds = 0;
 		expand = 1;
 	}
 
@@ -646,11 +664,11 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 		spin_lock(&oldf->file_lock);
 	}
 
-	old_fds = oldf->fd;
-	new_fds = newf->fd;
+	old_fds = old_fdt->fd;
+	new_fds = new_fdt->fd;
 
-	memcpy(newf->open_fds->fds_bits, oldf->open_fds->fds_bits, open_files/8);
-	memcpy(newf->close_on_exec->fds_bits, oldf->close_on_exec->fds_bits, open_files/8);
+	memcpy(new_fdt->open_fds->fds_bits, old_fdt->open_fds->fds_bits, open_files/8);
+	memcpy(new_fdt->close_on_exec->fds_bits, old_fdt->close_on_exec->fds_bits, open_files/8);
 
 	for (i = open_files; i != 0; i--) {
 		struct file *f = *old_fds++;
@@ -663,24 +681,24 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 			 * is partway through open().  So make sure that this
 			 * fd is available to the new process.
 			 */
-			FD_CLR(open_files - i, newf->open_fds);
+			FD_CLR(open_files - i, new_fdt->open_fds);
 		}
 		*new_fds++ = f;
 	}
 	spin_unlock(&oldf->file_lock);
 
 	/* compute the remainder to be cleared */
-	size = (newf->max_fds - open_files) * sizeof(struct file *);
+	size = (new_fdt->max_fds - open_files) * sizeof(struct file *);
 
 	/* This is long word aligned thus could use a optimized version */ 
 	memset(new_fds, 0, size); 
 
-	if (newf->max_fdset > open_files) {
-		int left = (newf->max_fdset-open_files)/8;
+	if (new_fdt->max_fdset > open_files) {
+		int left = (new_fdt->max_fdset-open_files)/8;
 		int start = open_files / (8 * sizeof(unsigned long));
 
-		memset(&newf->open_fds->fds_bits[start], 0, left);
-		memset(&newf->close_on_exec->fds_bits[start], 0, left);
+		memset(&new_fdt->open_fds->fds_bits[start], 0, left);
+		memset(&new_fdt->close_on_exec->fds_bits[start], 0, left);
 	}
 
 	tsk->files = newf;
@@ -689,9 +707,9 @@ static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
 	return error;
 
 out_release:
-	free_fdset (newf->close_on_exec, newf->max_fdset);
-	free_fdset (newf->open_fds, newf->max_fdset);
-	free_fd_array(newf->fd, newf->max_fds);
+	free_fdset (new_fdt->close_on_exec, new_fdt->max_fdset);
+	free_fdset (new_fdt->open_fds, new_fdt->max_fdset);
+	free_fd_array(new_fdt->fd, new_fdt->max_fds);
 	kmem_cache_free(files_cachep, newf);
 	goto out;
 }

commit b0d62e6d5b3318b6b722121d945afa295f7201b5
Author: Jason Baron <jbaron@redhat.com>
Date:   Fri Sep 9 13:02:01 2005 -0700

    [PATCH] fix disassociate_ctty vs. fork race
    
    Race is as follows. Process A forks process B, both being part of the same
    session. Then, A calls disassociate_ctty while B forks C:
    
    A                               B
    ====                            ====
                                    fork()
                                      copy_signal()
    dissasociate_ctty()             ....
                                      attach_pid(p, PIDTYPE_SID, p->signal->session);
    
    Now, C can have current->signal->tty pointing to a freed tty structure, as
    it hasn't yet been added to the session group (to have its controlling tty
    cleared on the diassociate_ctty() call).
    
    This has shown up as an oops but could be even more serious.  I haven't
    tried to create a test case, but a customer has verified that the patch
    below resolves the issue, which was occuring quite frequently.  I'll try
    and post the test case if i can.
    
    The patch simply checks for a NULL tty *after* it has been attached to the
    proper session group and clears it as necessary.  Alternatively, we could
    simply do the tty assignment after the the process is added to the proper
    session group.
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index dfeadf466f18..b25802065031 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1116,6 +1116,9 @@ static task_t *copy_process(unsigned long clone_flags,
 			__get_cpu_var(process_counts)++;
 	}
 
+	if (!current->signal->tty && p->signal->tty)
+		p->signal->tty = NULL;
+
 	nr_threads++;
 	total_forks++;
 	write_unlock_irq(&tasklist_lock);

commit 4b5d37ac02954572e80e09255bb5737277aaee8e
Author: Giancarlo Formicuccia <giancarlo.formicuccia@gmail.com>
Date:   Fri Sep 9 13:01:22 2005 -0700

    [PATCH] Clear task_struct->fs_excl on fork()
    
    An oversight.  We don't want to carry the IO scheduler's "we hold exclusive fs
    resources" hint over to the child across fork().
    
    Acked-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 7e1ead9a6ba4..dfeadf466f18 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -176,6 +176,7 @@ static struct task_struct *dup_task_struct(struct task_struct *orig)
 
 	/* One for us, one for whoever does the "release_task()" (usually parent) */
 	atomic_set(&tsk->usage,2);
+	atomic_set(&tsk->fs_excl, 0);
 	return tsk;
 }
 

commit ed75e8d58010fdc06e2c3a81bfbebae92314c7e3
Author: Laurent Vivier <LaurentVivier@wanadoo.fr>
Date:   Sat Sep 3 15:57:18 2005 -0700

    [PATCH] UML Support - Ptrace: adds the host SYSEMU support, for UML and general usage
    
          Jeff Dike <jdike@addtoit.com>,
          Paolo 'Blaisorblade' Giarrusso <blaisorblade_spam@yahoo.it>,
          Bodo Stroesser <bstroesser@fujitsu-siemens.com>
    
    Adds a new ptrace(2) mode, called PTRACE_SYSEMU, resembling PTRACE_SYSCALL
    except that the kernel does not execute the requested syscall; this is useful
    to improve performance for virtual environments, like UML, which want to run
    the syscall on their own.
    
    In fact, using PTRACE_SYSCALL means stopping child execution twice, on entry
    and on exit, and each time you also have two context switches; with SYSEMU you
    avoid the 2nd stop and so save two context switches per syscall.
    
    Also, some architectures don't have support in the host for changing the
    syscall number via ptrace(), which is currently needed to skip syscall
    execution (UML turns any syscall into getpid() to avoid it being executed on
    the host).  Fixing that is hard, while SYSEMU is easier to implement.
    
    * This version of the patch includes some suggestions of Jeff Dike to avoid
      adding any instructions to the syscall fast path, plus some other little
      changes, by myself, to make it work even when the syscall is executed with
      SYSENTER (but I'm unsure about them). It has been widely tested for quite a
      lot of time.
    
    * Various fixed were included to handle the various switches between
      various states, i.e. when for instance a syscall entry is traced with one of
      PT_SYSCALL / _SYSEMU / _SINGLESTEP and another one is used on exit.
      Basically, this is done by remembering which one of them was used even after
      the call to ptrace_notify().
    
    * We're combining TIF_SYSCALL_EMU with TIF_SYSCALL_TRACE or TIF_SINGLESTEP
      to make do_syscall_trace() notice that the current syscall was started with
      SYSEMU on entry, so that no notification ought to be done in the exit path;
      this is a bit of a hack, so this problem is solved in another way in next
      patches.
    
    * Also, the effects of the patch:
    "Ptrace - i386: fix Syscall Audit interaction with singlestep"
    are cancelled; they are restored back in the last patch of this series.
    
    Detailed descriptions of the patches doing this kind of processing follow (but
    I've already summed everything up).
    
    * Fix behaviour when changing interception kind #1.
    
      In do_syscall_trace(), we check the status of the TIF_SYSCALL_EMU flag
      only after doing the debugger notification; but the debugger might have
      changed the status of this flag because he continued execution with
      PTRACE_SYSCALL, so this is wrong.  This patch fixes it by saving the flag
      status before calling ptrace_notify().
    
    * Fix behaviour when changing interception kind #2:
      avoid intercepting syscall on return when using SYSCALL again.
    
      A guest process switching from using PTRACE_SYSEMU to PTRACE_SYSCALL
      crashes.
    
      The problem is in arch/i386/kernel/entry.S.  The current SYSEMU patch
      inhibits the syscall-handler to be called, but does not prevent
      do_syscall_trace() to be called after this for syscall completion
      interception.
    
      The appended patch fixes this.  It reuses the flag TIF_SYSCALL_EMU to
      remember "we come from PTRACE_SYSEMU and now are in PTRACE_SYSCALL", since
      the flag is unused in the depicted situation.
    
    * Fix behaviour when changing interception kind #3:
      avoid intercepting syscall on return when using SINGLESTEP.
    
      When testing 2.6.9 and the skas3.v6 patch, with my latest patch and had
      problems with singlestepping on UML in SKAS with SYSEMU.  It looped
      receiving SIGTRAPs without moving forward.  EIP of the traced process was
      the same for all SIGTRAPs.
    
    What's missing is to handle switching from PTRACE_SYSCALL_EMU to
    PTRACE_SINGLESTEP in a way very similar to what is done for the change from
    PTRACE_SYSCALL_EMU to PTRACE_SYSCALL_TRACE.
    
    I.e., after calling ptrace(PTRACE_SYSEMU), on the return path, the debugger is
    notified and then wake ups the process; the syscall is executed (or skipped,
    when do_syscall_trace() returns 0, i.e.  when using PTRACE_SYSEMU), and
    do_syscall_trace() is called again.  Since we are on the return path of a
    SYSEMU'd syscall, if the wake up is performed through ptrace(PTRACE_SYSCALL),
    we must still avoid notifying the parent of the syscall exit.  Now, this
    behaviour is extended even to resuming with PTRACE_SINGLESTEP.
    
    Signed-off-by: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index b65187f0c74e..7e1ead9a6ba4 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -994,6 +994,9 @@ static task_t *copy_process(unsigned long clone_flags,
 	 * of CLONE_PTRACE.
 	 */
 	clear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);
+#ifdef TIF_SYSCALL_EMU
+	clear_tsk_thread_flag(p, TIF_SYSCALL_EMU);
+#endif
 
 	/* Our parent execution domain becomes current domain
 	   These must match for thread signalling to apply */

commit 3b6bfcdb116f2cc2cab921fcac6d39d4022952d2
Author: Hugh Dickins <hugh@veritas.com>
Date:   Tue Jul 12 13:58:09 2005 -0700

    [PATCH] lower VM_DONTCOPY total_vm
    
    dup_mmap of a VM_DONTCOPY vma forgot to lower the child's total_vm.  (But
    no way does this account for the recent report of total_vm seen too low.)
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index cdef6cea8900..b65187f0c74e 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -208,8 +208,10 @@ static inline int dup_mmap(struct mm_struct * mm, struct mm_struct * oldmm)
 		struct file *file;
 
 		if (mpnt->vm_flags & VM_DONTCOPY) {
+			long pages = vma_pages(mpnt);
+			mm->total_vm -= pages;
 			__vm_stat_account(mm, mpnt->vm_flags, mpnt->vm_file,
-							-vma_pages(mpnt));
+								-pages);
 			continue;
 		}
 		charge = 0;

commit 22e2c507c301c3dbbcf91b4948b88f78842ee6c9
Author: Jens Axboe <axboe@suse.de>
Date:   Mon Jun 27 10:55:12 2005 +0200

    [PATCH] Update cfq io scheduler to time sliced design
    
    This updates the CFQ io scheduler to the new time sliced design (cfq
    v3).  It provides full process fairness, while giving excellent
    aggregate system throughput even for many competing processes.  It
    supports io priorities, either inherited from the cpu nice value or set
    directly with the ioprio_get/set syscalls.  The latter closely mimic
    set/getpriority.
    
    This import is based on my latest from -mm.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 2c7806873bfd..cdef6cea8900 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1090,6 +1090,11 @@ static task_t *copy_process(unsigned long clone_flags,
 		spin_unlock(&current->sighand->siglock);
 	}
 
+	/*
+	 * inherit ioprio
+	 */
+	p->ioprio = current->ioprio;
+
 	SET_LINKS(p);
 	if (unlikely(p->ptrace & PT_PTRACED))
 		__ptrace_link(p, current->parent);

commit 476d139c218e44e045e4bc6d4cc02b010b343939
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Sat Jun 25 14:57:29 2005 -0700

    [PATCH] sched: consolidate sbe sbf
    
    Consolidate balance-on-exec with balance-on-fork.  This is made easy by the
    sched-domains RCU patches.
    
    As well as the general goodness of code reduction, this allows the runqueues
    to be unlocked during balance-on-fork.
    
    schedstats is a problem.  Maybe just have balance-on-event instead of
    distinguishing fork and exec?
    
    Signed-off-by: Nick Piggin <nickpiggin@yahoo.com.au>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index a28d11e10877..2c7806873bfd 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1003,9 +1003,6 @@ static task_t *copy_process(unsigned long clone_flags,
 	p->pdeath_signal = 0;
 	p->exit_state = 0;
 
-	/* Perform scheduler related setup */
-	sched_fork(p);
-
 	/*
 	 * Ok, make it visible to the rest of the system.
 	 * We dont wake it up yet.
@@ -1014,18 +1011,24 @@ static task_t *copy_process(unsigned long clone_flags,
 	INIT_LIST_HEAD(&p->ptrace_children);
 	INIT_LIST_HEAD(&p->ptrace_list);
 
+	/* Perform scheduler related setup. Assign this task to a CPU. */
+	sched_fork(p, clone_flags);
+
 	/* Need tasklist lock for parent etc handling! */
 	write_lock_irq(&tasklist_lock);
 
 	/*
-	 * The task hasn't been attached yet, so cpus_allowed mask cannot
-	 * have changed. The cpus_allowed mask of the parent may have
-	 * changed after it was copied first time, and it may then move to
-	 * another CPU - so we re-copy it here and set the child's CPU to
-	 * the parent's CPU. This avoids alot of nasty races.
+	 * The task hasn't been attached yet, so its cpus_allowed mask will
+	 * not be changed, nor will its assigned CPU.
+	 *
+	 * The cpus_allowed mask of the parent may have changed after it was
+	 * copied first time - so re-copy it here, then check the child's CPU
+	 * to ensure it is on a valid CPU (and if not, just force it back to
+	 * parent's CPU). This avoids alot of nasty races.
 	 */
 	p->cpus_allowed = current->cpus_allowed;
-	set_task_cpu(p, smp_processor_id());
+	if (unlikely(!cpu_isset(task_cpu(p), p->cpus_allowed)))
+		set_task_cpu(p, smp_processor_id());
 
 	/*
 	 * Check for pending SIGKILL! The new thread should not be allowed

commit 45918e1a8bfcabc1cb4570b8df276655020eac45
Author: Hugh Dickins <hugh@veritas.com>
Date:   Tue Jun 21 17:15:08 2005 -0700

    [PATCH] dup_mmap: update comment on new vma
    
    Remove part of comment on linking new vma in dup_mmap: since anon_vma rmap
    came in, try_to_unmap_one knows the vma without needing find_vma.  But add
    a comment to note that here vma is inserted without mmap_sem.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index 876b31cd822d..a28d11e10877 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -250,8 +250,9 @@ static inline int dup_mmap(struct mm_struct * mm, struct mm_struct * oldmm)
 
 		/*
 		 * Link in the new vma and copy the page table entries:
-		 * link in first so that swapoff can see swap entries,
-		 * and try_to_unmap_one's find_vma find the new vma.
+		 * link in first so that swapoff can see swap entries.
+		 * Note that, exceptionally, here the vma is inserted
+		 * without holding mm->mmap_sem.
 		 */
 		spin_lock(&mm->page_table_lock);
 		*pprev = tmp;

commit 1363c3cd8603a913a27e2995dccbd70d5312d8e6
Author: Wolfgang Wander <wwc@rentec.com>
Date:   Tue Jun 21 17:14:49 2005 -0700

    [PATCH] Avoiding mmap fragmentation
    
    Ingo recently introduced a great speedup for allocating new mmaps using the
    free_area_cache pointer which boosts the specweb SSL benchmark by 4-5% and
    causes huge performance increases in thread creation.
    
    The downside of this patch is that it does lead to fragmentation in the
    mmap-ed areas (visible via /proc/self/maps), such that some applications
    that work fine under 2.4 kernels quickly run out of memory on any 2.6
    kernel.
    
    The problem is twofold:
    
      1) the free_area_cache is used to continue a search for memory where
         the last search ended.  Before the change new areas were always
         searched from the base address on.
    
         So now new small areas are cluttering holes of all sizes
         throughout the whole mmap-able region whereas before small holes
         tended to close holes near the base leaving holes far from the base
         large and available for larger requests.
    
      2) the free_area_cache also is set to the location of the last
         munmap-ed area so in scenarios where we allocate e.g.  five regions of
         1K each, then free regions 4 2 3 in this order the next request for 1K
         will be placed in the position of the old region 3, whereas before we
         appended it to the still active region 1, placing it at the location
         of the old region 2.  Before we had 1 free region of 2K, now we only
         get two free regions of 1K -> fragmentation.
    
    The patch addresses thes issues by introducing yet another cache descriptor
    cached_hole_size that contains the largest known hole size below the
    current free_area_cache.  If a new request comes in the size is compared
    against the cached_hole_size and if the request can be filled with a hole
    below free_area_cache the search is started from the base instead.
    
    The results look promising: Whereas 2.6.12-rc4 fragments quickly and my
    (earlier posted) leakme.c test program terminates after 50000+ iterations
    with 96 distinct and fragmented maps in /proc/self/maps it performs nicely
    (as expected) with thread creation, Ingo's test_str02 with 20000 threads
    requires 0.7s system time.
    
    Taking out Ingo's patch (un-patch available per request) by basically
    deleting all mentions of free_area_cache from the kernel and starting the
    search for new memory always at the respective bases we observe: leakme
    terminates successfully with 11 distinctive hardly fragmented areas in
    /proc/self/maps but thread creating is gringdingly slow: 30+s(!) system
    time for Ingo's test_str02 with 20000 threads.
    
    Now - drumroll ;-) the appended patch works fine with leakme: it ends with
    only 7 distinct areas in /proc/self/maps and also thread creation seems
    sufficiently fast with 0.71s for 20000 threads.
    
    Signed-off-by: Wolfgang Wander <wwc@rentec.com>
    Credit-to: "Richard Purdie" <rpurdie@rpsys.net>
    Signed-off-by: Ken Chen <kenneth.w.chen@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu> (partly)
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/fork.c b/kernel/fork.c
index f42a17f88699..876b31cd822d 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -194,6 +194,7 @@ static inline int dup_mmap(struct mm_struct * mm, struct mm_struct * oldmm)
 	mm->mmap = NULL;
 	mm->mmap_cache = NULL;
 	mm->free_area_cache = oldmm->mmap_base;
+	mm->cached_hole_size = ~0UL;
 	mm->map_count = 0;
 	set_mm_counter(mm, rss, 0);
 	set_mm_counter(mm, anon_rss, 0);
@@ -322,6 +323,7 @@ static struct mm_struct * mm_init(struct mm_struct * mm)
 	mm->ioctx_list = NULL;
 	mm->default_kioctx = (struct kioctx)INIT_KIOCTX(mm->default_kioctx, *mm);
 	mm->free_area_cache = TASK_UNMAPPED_BASE;
+	mm->cached_hole_size = ~0UL;
 
 	if (likely(!mm_alloc_pgd(mm))) {
 		mm->def_flags = 0;

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/kernel/fork.c b/kernel/fork.c
new file mode 100644
index 000000000000..f42a17f88699
--- /dev/null
+++ b/kernel/fork.c
@@ -0,0 +1,1274 @@
+/*
+ *  linux/kernel/fork.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ */
+
+/*
+ *  'fork.c' contains the help-routines for the 'fork' system call
+ * (see also entry.S and others).
+ * Fork is rather simple, once you get the hang of it, but the memory
+ * management can be a bitch. See 'mm/memory.c': 'copy_page_range()'
+ */
+
+#include <linux/config.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/unistd.h>
+#include <linux/smp_lock.h>
+#include <linux/module.h>
+#include <linux/vmalloc.h>
+#include <linux/completion.h>
+#include <linux/namespace.h>
+#include <linux/personality.h>
+#include <linux/mempolicy.h>
+#include <linux/sem.h>
+#include <linux/file.h>
+#include <linux/key.h>
+#include <linux/binfmts.h>
+#include <linux/mman.h>
+#include <linux/fs.h>
+#include <linux/cpu.h>
+#include <linux/cpuset.h>
+#include <linux/security.h>
+#include <linux/swap.h>
+#include <linux/syscalls.h>
+#include <linux/jiffies.h>
+#include <linux/futex.h>
+#include <linux/ptrace.h>
+#include <linux/mount.h>
+#include <linux/audit.h>
+#include <linux/profile.h>
+#include <linux/rmap.h>
+#include <linux/acct.h>
+
+#include <asm/pgtable.h>
+#include <asm/pgalloc.h>
+#include <asm/uaccess.h>
+#include <asm/mmu_context.h>
+#include <asm/cacheflush.h>
+#include <asm/tlbflush.h>
+
+/*
+ * Protected counters by write_lock_irq(&tasklist_lock)
+ */
+unsigned long total_forks;	/* Handle normal Linux uptimes. */
+int nr_threads; 		/* The idle threads do not count.. */
+
+int max_threads;		/* tunable limit on nr_threads */
+
+DEFINE_PER_CPU(unsigned long, process_counts) = 0;
+
+ __cacheline_aligned DEFINE_RWLOCK(tasklist_lock);  /* outer */
+
+EXPORT_SYMBOL(tasklist_lock);
+
+int nr_processes(void)
+{
+	int cpu;
+	int total = 0;
+
+	for_each_online_cpu(cpu)
+		total += per_cpu(process_counts, cpu);
+
+	return total;
+}
+
+#ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
+# define alloc_task_struct()	kmem_cache_alloc(task_struct_cachep, GFP_KERNEL)
+# define free_task_struct(tsk)	kmem_cache_free(task_struct_cachep, (tsk))
+static kmem_cache_t *task_struct_cachep;
+#endif
+
+/* SLAB cache for signal_struct structures (tsk->signal) */
+kmem_cache_t *signal_cachep;
+
+/* SLAB cache for sighand_struct structures (tsk->sighand) */
+kmem_cache_t *sighand_cachep;
+
+/* SLAB cache for files_struct structures (tsk->files) */
+kmem_cache_t *files_cachep;
+
+/* SLAB cache for fs_struct structures (tsk->fs) */
+kmem_cache_t *fs_cachep;
+
+/* SLAB cache for vm_area_struct structures */
+kmem_cache_t *vm_area_cachep;
+
+/* SLAB cache for mm_struct structures (tsk->mm) */
+static kmem_cache_t *mm_cachep;
+
+void free_task(struct task_struct *tsk)
+{
+	free_thread_info(tsk->thread_info);
+	free_task_struct(tsk);
+}
+EXPORT_SYMBOL(free_task);
+
+void __put_task_struct(struct task_struct *tsk)
+{
+	WARN_ON(!(tsk->exit_state & (EXIT_DEAD | EXIT_ZOMBIE)));
+	WARN_ON(atomic_read(&tsk->usage));
+	WARN_ON(tsk == current);
+
+	if (unlikely(tsk->audit_context))
+		audit_free(tsk);
+	security_task_free(tsk);
+	free_uid(tsk->user);
+	put_group_info(tsk->group_info);
+
+	if (!profile_handoff_task(tsk))
+		free_task(tsk);
+}
+
+void __init fork_init(unsigned long mempages)
+{
+#ifndef __HAVE_ARCH_TASK_STRUCT_ALLOCATOR
+#ifndef ARCH_MIN_TASKALIGN
+#define ARCH_MIN_TASKALIGN	L1_CACHE_BYTES
+#endif
+	/* create a slab on which task_structs can be allocated */
+	task_struct_cachep =
+		kmem_cache_create("task_struct", sizeof(struct task_struct),
+			ARCH_MIN_TASKALIGN, SLAB_PANIC, NULL, NULL);
+#endif
+
+	/*
+	 * The default maximum number of threads is set to a safe
+	 * value: the thread structures can take up at most half
+	 * of memory.
+	 */
+	max_threads = mempages / (8 * THREAD_SIZE / PAGE_SIZE);
+
+	/*
+	 * we need to allow at least 20 threads to boot a system
+	 */
+	if(max_threads < 20)
+		max_threads = 20;
+
+	init_task.signal->rlim[RLIMIT_NPROC].rlim_cur = max_threads/2;
+	init_task.signal->rlim[RLIMIT_NPROC].rlim_max = max_threads/2;
+	init_task.signal->rlim[RLIMIT_SIGPENDING] =
+		init_task.signal->rlim[RLIMIT_NPROC];
+}
+
+static struct task_struct *dup_task_struct(struct task_struct *orig)
+{
+	struct task_struct *tsk;
+	struct thread_info *ti;
+
+	prepare_to_copy(orig);
+
+	tsk = alloc_task_struct();
+	if (!tsk)
+		return NULL;
+
+	ti = alloc_thread_info(tsk);
+	if (!ti) {
+		free_task_struct(tsk);
+		return NULL;
+	}
+
+	*ti = *orig->thread_info;
+	*tsk = *orig;
+	tsk->thread_info = ti;
+	ti->task = tsk;
+
+	/* One for us, one for whoever does the "release_task()" (usually parent) */
+	atomic_set(&tsk->usage,2);
+	return tsk;
+}
+
+#ifdef CONFIG_MMU
+static inline int dup_mmap(struct mm_struct * mm, struct mm_struct * oldmm)
+{
+	struct vm_area_struct * mpnt, *tmp, **pprev;
+	struct rb_node **rb_link, *rb_parent;
+	int retval;
+	unsigned long charge;
+	struct mempolicy *pol;
+
+	down_write(&oldmm->mmap_sem);
+	flush_cache_mm(current->mm);
+	mm->locked_vm = 0;
+	mm->mmap = NULL;
+	mm->mmap_cache = NULL;
+	mm->free_area_cache = oldmm->mmap_base;
+	mm->map_count = 0;
+	set_mm_counter(mm, rss, 0);
+	set_mm_counter(mm, anon_rss, 0);
+	cpus_clear(mm->cpu_vm_mask);
+	mm->mm_rb = RB_ROOT;
+	rb_link = &mm->mm_rb.rb_node;
+	rb_parent = NULL;
+	pprev = &mm->mmap;
+
+	for (mpnt = current->mm->mmap ; mpnt ; mpnt = mpnt->vm_next) {
+		struct file *file;
+
+		if (mpnt->vm_flags & VM_DONTCOPY) {
+			__vm_stat_account(mm, mpnt->vm_flags, mpnt->vm_file,
+							-vma_pages(mpnt));
+			continue;
+		}
+		charge = 0;
+		if (mpnt->vm_flags & VM_ACCOUNT) {
+			unsigned int len = (mpnt->vm_end - mpnt->vm_start) >> PAGE_SHIFT;
+			if (security_vm_enough_memory(len))
+				goto fail_nomem;
+			charge = len;
+		}
+		tmp = kmem_cache_alloc(vm_area_cachep, SLAB_KERNEL);
+		if (!tmp)
+			goto fail_nomem;
+		*tmp = *mpnt;
+		pol = mpol_copy(vma_policy(mpnt));
+		retval = PTR_ERR(pol);
+		if (IS_ERR(pol))
+			goto fail_nomem_policy;
+		vma_set_policy(tmp, pol);
+		tmp->vm_flags &= ~VM_LOCKED;
+		tmp->vm_mm = mm;
+		tmp->vm_next = NULL;
+		anon_vma_link(tmp);
+		file = tmp->vm_file;
+		if (file) {
+			struct inode *inode = file->f_dentry->d_inode;
+			get_file(file);
+			if (tmp->vm_flags & VM_DENYWRITE)
+				atomic_dec(&inode->i_writecount);
+      
+			/* insert tmp into the share list, just after mpnt */
+			spin_lock(&file->f_mapping->i_mmap_lock);
+			tmp->vm_truncate_count = mpnt->vm_truncate_count;
+			flush_dcache_mmap_lock(file->f_mapping);
+			vma_prio_tree_add(tmp, mpnt);
+			flush_dcache_mmap_unlock(file->f_mapping);
+			spin_unlock(&file->f_mapping->i_mmap_lock);
+		}
+
+		/*
+		 * Link in the new vma and copy the page table entries:
+		 * link in first so that swapoff can see swap entries,
+		 * and try_to_unmap_one's find_vma find the new vma.
+		 */
+		spin_lock(&mm->page_table_lock);
+		*pprev = tmp;
+		pprev = &tmp->vm_next;
+
+		__vma_link_rb(mm, tmp, rb_link, rb_parent);
+		rb_link = &tmp->vm_rb.rb_right;
+		rb_parent = &tmp->vm_rb;
+
+		mm->map_count++;
+		retval = copy_page_range(mm, current->mm, tmp);
+		spin_unlock(&mm->page_table_lock);
+
+		if (tmp->vm_ops && tmp->vm_ops->open)
+			tmp->vm_ops->open(tmp);
+
+		if (retval)
+			goto out;
+	}
+	retval = 0;
+
+out:
+	flush_tlb_mm(current->mm);
+	up_write(&oldmm->mmap_sem);
+	return retval;
+fail_nomem_policy:
+	kmem_cache_free(vm_area_cachep, tmp);
+fail_nomem:
+	retval = -ENOMEM;
+	vm_unacct_memory(charge);
+	goto out;
+}
+
+static inline int mm_alloc_pgd(struct mm_struct * mm)
+{
+	mm->pgd = pgd_alloc(mm);
+	if (unlikely(!mm->pgd))
+		return -ENOMEM;
+	return 0;
+}
+
+static inline void mm_free_pgd(struct mm_struct * mm)
+{
+	pgd_free(mm->pgd);
+}
+#else
+#define dup_mmap(mm, oldmm)	(0)
+#define mm_alloc_pgd(mm)	(0)
+#define mm_free_pgd(mm)
+#endif /* CONFIG_MMU */
+
+ __cacheline_aligned_in_smp DEFINE_SPINLOCK(mmlist_lock);
+
+#define allocate_mm()	(kmem_cache_alloc(mm_cachep, SLAB_KERNEL))
+#define free_mm(mm)	(kmem_cache_free(mm_cachep, (mm)))
+
+#include <linux/init_task.h>
+
+static struct mm_struct * mm_init(struct mm_struct * mm)
+{
+	atomic_set(&mm->mm_users, 1);
+	atomic_set(&mm->mm_count, 1);
+	init_rwsem(&mm->mmap_sem);
+	INIT_LIST_HEAD(&mm->mmlist);
+	mm->core_waiters = 0;
+	mm->nr_ptes = 0;
+	spin_lock_init(&mm->page_table_lock);
+	rwlock_init(&mm->ioctx_list_lock);
+	mm->ioctx_list = NULL;
+	mm->default_kioctx = (struct kioctx)INIT_KIOCTX(mm->default_kioctx, *mm);
+	mm->free_area_cache = TASK_UNMAPPED_BASE;
+
+	if (likely(!mm_alloc_pgd(mm))) {
+		mm->def_flags = 0;
+		return mm;
+	}
+	free_mm(mm);
+	return NULL;
+}
+
+/*
+ * Allocate and initialize an mm_struct.
+ */
+struct mm_struct * mm_alloc(void)
+{
+	struct mm_struct * mm;
+
+	mm = allocate_mm();
+	if (mm) {
+		memset(mm, 0, sizeof(*mm));
+		mm = mm_init(mm);
+	}
+	return mm;
+}
+
+/*
+ * Called when the last reference to the mm
+ * is dropped: either by a lazy thread or by
+ * mmput. Free the page directory and the mm.
+ */
+void fastcall __mmdrop(struct mm_struct *mm)
+{
+	BUG_ON(mm == &init_mm);
+	mm_free_pgd(mm);
+	destroy_context(mm);
+	free_mm(mm);
+}
+
+/*
+ * Decrement the use count and release all resources for an mm.
+ */
+void mmput(struct mm_struct *mm)
+{
+	if (atomic_dec_and_test(&mm->mm_users)) {
+		exit_aio(mm);
+		exit_mmap(mm);
+		if (!list_empty(&mm->mmlist)) {
+			spin_lock(&mmlist_lock);
+			list_del(&mm->mmlist);
+			spin_unlock(&mmlist_lock);
+		}
+		put_swap_token(mm);
+		mmdrop(mm);
+	}
+}
+EXPORT_SYMBOL_GPL(mmput);
+
+/**
+ * get_task_mm - acquire a reference to the task's mm
+ *
+ * Returns %NULL if the task has no mm.  Checks PF_BORROWED_MM (meaning
+ * this kernel workthread has transiently adopted a user mm with use_mm,
+ * to do its AIO) is not set and if so returns a reference to it, after
+ * bumping up the use count.  User must release the mm via mmput()
+ * after use.  Typically used by /proc and ptrace.
+ */
+struct mm_struct *get_task_mm(struct task_struct *task)
+{
+	struct mm_struct *mm;
+
+	task_lock(task);
+	mm = task->mm;
+	if (mm) {
+		if (task->flags & PF_BORROWED_MM)
+			mm = NULL;
+		else
+			atomic_inc(&mm->mm_users);
+	}
+	task_unlock(task);
+	return mm;
+}
+EXPORT_SYMBOL_GPL(get_task_mm);
+
+/* Please note the differences between mmput and mm_release.
+ * mmput is called whenever we stop holding onto a mm_struct,
+ * error success whatever.
+ *
+ * mm_release is called after a mm_struct has been removed
+ * from the current process.
+ *
+ * This difference is important for error handling, when we
+ * only half set up a mm_struct for a new process and need to restore
+ * the old one.  Because we mmput the new mm_struct before
+ * restoring the old one. . .
+ * Eric Biederman 10 January 1998
+ */
+void mm_release(struct task_struct *tsk, struct mm_struct *mm)
+{
+	struct completion *vfork_done = tsk->vfork_done;
+
+	/* Get rid of any cached register state */
+	deactivate_mm(tsk, mm);
+
+	/* notify parent sleeping on vfork() */
+	if (vfork_done) {
+		tsk->vfork_done = NULL;
+		complete(vfork_done);
+	}
+	if (tsk->clear_child_tid && atomic_read(&mm->mm_users) > 1) {
+		u32 __user * tidptr = tsk->clear_child_tid;
+		tsk->clear_child_tid = NULL;
+
+		/*
+		 * We don't check the error code - if userspace has
+		 * not set up a proper pointer then tough luck.
+		 */
+		put_user(0, tidptr);
+		sys_futex(tidptr, FUTEX_WAKE, 1, NULL, NULL, 0);
+	}
+}
+
+static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
+{
+	struct mm_struct * mm, *oldmm;
+	int retval;
+
+	tsk->min_flt = tsk->maj_flt = 0;
+	tsk->nvcsw = tsk->nivcsw = 0;
+
+	tsk->mm = NULL;
+	tsk->active_mm = NULL;
+
+	/*
+	 * Are we cloning a kernel thread?
+	 *
+	 * We need to steal a active VM for that..
+	 */
+	oldmm = current->mm;
+	if (!oldmm)
+		return 0;
+
+	if (clone_flags & CLONE_VM) {
+		atomic_inc(&oldmm->mm_users);
+		mm = oldmm;
+		/*
+		 * There are cases where the PTL is held to ensure no
+		 * new threads start up in user mode using an mm, which
+		 * allows optimizing out ipis; the tlb_gather_mmu code
+		 * is an example.
+		 */
+		spin_unlock_wait(&oldmm->page_table_lock);
+		goto good_mm;
+	}
+
+	retval = -ENOMEM;
+	mm = allocate_mm();
+	if (!mm)
+		goto fail_nomem;
+
+	/* Copy the current MM stuff.. */
+	memcpy(mm, oldmm, sizeof(*mm));
+	if (!mm_init(mm))
+		goto fail_nomem;
+
+	if (init_new_context(tsk,mm))
+		goto fail_nocontext;
+
+	retval = dup_mmap(mm, oldmm);
+	if (retval)
+		goto free_pt;
+
+	mm->hiwater_rss = get_mm_counter(mm,rss);
+	mm->hiwater_vm = mm->total_vm;
+
+good_mm:
+	tsk->mm = mm;
+	tsk->active_mm = mm;
+	return 0;
+
+free_pt:
+	mmput(mm);
+fail_nomem:
+	return retval;
+
+fail_nocontext:
+	/*
+	 * If init_new_context() failed, we cannot use mmput() to free the mm
+	 * because it calls destroy_context()
+	 */
+	mm_free_pgd(mm);
+	free_mm(mm);
+	return retval;
+}
+
+static inline struct fs_struct *__copy_fs_struct(struct fs_struct *old)
+{
+	struct fs_struct *fs = kmem_cache_alloc(fs_cachep, GFP_KERNEL);
+	/* We don't need to lock fs - think why ;-) */
+	if (fs) {
+		atomic_set(&fs->count, 1);
+		rwlock_init(&fs->lock);
+		fs->umask = old->umask;
+		read_lock(&old->lock);
+		fs->rootmnt = mntget(old->rootmnt);
+		fs->root = dget(old->root);
+		fs->pwdmnt = mntget(old->pwdmnt);
+		fs->pwd = dget(old->pwd);
+		if (old->altroot) {
+			fs->altrootmnt = mntget(old->altrootmnt);
+			fs->altroot = dget(old->altroot);
+		} else {
+			fs->altrootmnt = NULL;
+			fs->altroot = NULL;
+		}
+		read_unlock(&old->lock);
+	}
+	return fs;
+}
+
+struct fs_struct *copy_fs_struct(struct fs_struct *old)
+{
+	return __copy_fs_struct(old);
+}
+
+EXPORT_SYMBOL_GPL(copy_fs_struct);
+
+static inline int copy_fs(unsigned long clone_flags, struct task_struct * tsk)
+{
+	if (clone_flags & CLONE_FS) {
+		atomic_inc(&current->fs->count);
+		return 0;
+	}
+	tsk->fs = __copy_fs_struct(current->fs);
+	if (!tsk->fs)
+		return -ENOMEM;
+	return 0;
+}
+
+static int count_open_files(struct files_struct *files, int size)
+{
+	int i;
+
+	/* Find the last open fd */
+	for (i = size/(8*sizeof(long)); i > 0; ) {
+		if (files->open_fds->fds_bits[--i])
+			break;
+	}
+	i = (i+1) * 8 * sizeof(long);
+	return i;
+}
+
+static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
+{
+	struct files_struct *oldf, *newf;
+	struct file **old_fds, **new_fds;
+	int open_files, size, i, error = 0, expand;
+
+	/*
+	 * A background process may not have any files ...
+	 */
+	oldf = current->files;
+	if (!oldf)
+		goto out;
+
+	if (clone_flags & CLONE_FILES) {
+		atomic_inc(&oldf->count);
+		goto out;
+	}
+
+	/*
+	 * Note: we may be using current for both targets (See exec.c)
+	 * This works because we cache current->files (old) as oldf. Don't
+	 * break this.
+	 */
+	tsk->files = NULL;
+	error = -ENOMEM;
+	newf = kmem_cache_alloc(files_cachep, SLAB_KERNEL);
+	if (!newf) 
+		goto out;
+
+	atomic_set(&newf->count, 1);
+
+	spin_lock_init(&newf->file_lock);
+	newf->next_fd	    = 0;
+	newf->max_fds	    = NR_OPEN_DEFAULT;
+	newf->max_fdset	    = __FD_SETSIZE;
+	newf->close_on_exec = &newf->close_on_exec_init;
+	newf->open_fds	    = &newf->open_fds_init;
+	newf->fd	    = &newf->fd_array[0];
+
+	spin_lock(&oldf->file_lock);
+
+	open_files = count_open_files(oldf, oldf->max_fdset);
+	expand = 0;
+
+	/*
+	 * Check whether we need to allocate a larger fd array or fd set.
+	 * Note: we're not a clone task, so the open count won't  change.
+	 */
+	if (open_files > newf->max_fdset) {
+		newf->max_fdset = 0;
+		expand = 1;
+	}
+	if (open_files > newf->max_fds) {
+		newf->max_fds = 0;
+		expand = 1;
+	}
+
+	/* if the old fdset gets grown now, we'll only copy up to "size" fds */
+	if (expand) {
+		spin_unlock(&oldf->file_lock);
+		spin_lock(&newf->file_lock);
+		error = expand_files(newf, open_files-1);
+		spin_unlock(&newf->file_lock);
+		if (error < 0)
+			goto out_release;
+		spin_lock(&oldf->file_lock);
+	}
+
+	old_fds = oldf->fd;
+	new_fds = newf->fd;
+
+	memcpy(newf->open_fds->fds_bits, oldf->open_fds->fds_bits, open_files/8);
+	memcpy(newf->close_on_exec->fds_bits, oldf->close_on_exec->fds_bits, open_files/8);
+
+	for (i = open_files; i != 0; i--) {
+		struct file *f = *old_fds++;
+		if (f) {
+			get_file(f);
+		} else {
+			/*
+			 * The fd may be claimed in the fd bitmap but not yet
+			 * instantiated in the files array if a sibling thread
+			 * is partway through open().  So make sure that this
+			 * fd is available to the new process.
+			 */
+			FD_CLR(open_files - i, newf->open_fds);
+		}
+		*new_fds++ = f;
+	}
+	spin_unlock(&oldf->file_lock);
+
+	/* compute the remainder to be cleared */
+	size = (newf->max_fds - open_files) * sizeof(struct file *);
+
+	/* This is long word aligned thus could use a optimized version */ 
+	memset(new_fds, 0, size); 
+
+	if (newf->max_fdset > open_files) {
+		int left = (newf->max_fdset-open_files)/8;
+		int start = open_files / (8 * sizeof(unsigned long));
+
+		memset(&newf->open_fds->fds_bits[start], 0, left);
+		memset(&newf->close_on_exec->fds_bits[start], 0, left);
+	}
+
+	tsk->files = newf;
+	error = 0;
+out:
+	return error;
+
+out_release:
+	free_fdset (newf->close_on_exec, newf->max_fdset);
+	free_fdset (newf->open_fds, newf->max_fdset);
+	free_fd_array(newf->fd, newf->max_fds);
+	kmem_cache_free(files_cachep, newf);
+	goto out;
+}
+
+/*
+ *	Helper to unshare the files of the current task.
+ *	We don't want to expose copy_files internals to
+ *	the exec layer of the kernel.
+ */
+
+int unshare_files(void)
+{
+	struct files_struct *files  = current->files;
+	int rc;
+
+	if(!files)
+		BUG();
+
+	/* This can race but the race causes us to copy when we don't
+	   need to and drop the copy */
+	if(atomic_read(&files->count) == 1)
+	{
+		atomic_inc(&files->count);
+		return 0;
+	}
+	rc = copy_files(0, current);
+	if(rc)
+		current->files = files;
+	return rc;
+}
+
+EXPORT_SYMBOL(unshare_files);
+
+static inline int copy_sighand(unsigned long clone_flags, struct task_struct * tsk)
+{
+	struct sighand_struct *sig;
+
+	if (clone_flags & (CLONE_SIGHAND | CLONE_THREAD)) {
+		atomic_inc(&current->sighand->count);
+		return 0;
+	}
+	sig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);
+	tsk->sighand = sig;
+	if (!sig)
+		return -ENOMEM;
+	spin_lock_init(&sig->siglock);
+	atomic_set(&sig->count, 1);
+	memcpy(sig->action, current->sighand->action, sizeof(sig->action));
+	return 0;
+}
+
+static inline int copy_signal(unsigned long clone_flags, struct task_struct * tsk)
+{
+	struct signal_struct *sig;
+	int ret;
+
+	if (clone_flags & CLONE_THREAD) {
+		atomic_inc(&current->signal->count);
+		atomic_inc(&current->signal->live);
+		return 0;
+	}
+	sig = kmem_cache_alloc(signal_cachep, GFP_KERNEL);
+	tsk->signal = sig;
+	if (!sig)
+		return -ENOMEM;
+
+	ret = copy_thread_group_keys(tsk);
+	if (ret < 0) {
+		kmem_cache_free(signal_cachep, sig);
+		return ret;
+	}
+
+	atomic_set(&sig->count, 1);
+	atomic_set(&sig->live, 1);
+	init_waitqueue_head(&sig->wait_chldexit);
+	sig->flags = 0;
+	sig->group_exit_code = 0;
+	sig->group_exit_task = NULL;
+	sig->group_stop_count = 0;
+	sig->curr_target = NULL;
+	init_sigpending(&sig->shared_pending);
+	INIT_LIST_HEAD(&sig->posix_timers);
+
+	sig->it_real_value = sig->it_real_incr = 0;
+	sig->real_timer.function = it_real_fn;
+	sig->real_timer.data = (unsigned long) tsk;
+	init_timer(&sig->real_timer);
+
+	sig->it_virt_expires = cputime_zero;
+	sig->it_virt_incr = cputime_zero;
+	sig->it_prof_expires = cputime_zero;
+	sig->it_prof_incr = cputime_zero;
+
+	sig->tty = current->signal->tty;
+	sig->pgrp = process_group(current);
+	sig->session = current->signal->session;
+	sig->leader = 0;	/* session leadership doesn't inherit */
+	sig->tty_old_pgrp = 0;
+
+	sig->utime = sig->stime = sig->cutime = sig->cstime = cputime_zero;
+	sig->nvcsw = sig->nivcsw = sig->cnvcsw = sig->cnivcsw = 0;
+	sig->min_flt = sig->maj_flt = sig->cmin_flt = sig->cmaj_flt = 0;
+	sig->sched_time = 0;
+	INIT_LIST_HEAD(&sig->cpu_timers[0]);
+	INIT_LIST_HEAD(&sig->cpu_timers[1]);
+	INIT_LIST_HEAD(&sig->cpu_timers[2]);
+
+	task_lock(current->group_leader);
+	memcpy(sig->rlim, current->signal->rlim, sizeof sig->rlim);
+	task_unlock(current->group_leader);
+
+	if (sig->rlim[RLIMIT_CPU].rlim_cur != RLIM_INFINITY) {
+		/*
+		 * New sole thread in the process gets an expiry time
+		 * of the whole CPU time limit.
+		 */
+		tsk->it_prof_expires =
+			secs_to_cputime(sig->rlim[RLIMIT_CPU].rlim_cur);
+	}
+
+	return 0;
+}
+
+static inline void copy_flags(unsigned long clone_flags, struct task_struct *p)
+{
+	unsigned long new_flags = p->flags;
+
+	new_flags &= ~PF_SUPERPRIV;
+	new_flags |= PF_FORKNOEXEC;
+	if (!(clone_flags & CLONE_PTRACE))
+		p->ptrace = 0;
+	p->flags = new_flags;
+}
+
+asmlinkage long sys_set_tid_address(int __user *tidptr)
+{
+	current->clear_child_tid = tidptr;
+
+	return current->pid;
+}
+
+/*
+ * This creates a new process as a copy of the old one,
+ * but does not actually start it yet.
+ *
+ * It copies the registers, and all the appropriate
+ * parts of the process environment (as per the clone
+ * flags). The actual kick-off is left to the caller.
+ */
+static task_t *copy_process(unsigned long clone_flags,
+				 unsigned long stack_start,
+				 struct pt_regs *regs,
+				 unsigned long stack_size,
+				 int __user *parent_tidptr,
+				 int __user *child_tidptr,
+				 int pid)
+{
+	int retval;
+	struct task_struct *p = NULL;
+
+	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
+		return ERR_PTR(-EINVAL);
+
+	/*
+	 * Thread groups must share signals as well, and detached threads
+	 * can only be started up within the thread group.
+	 */
+	if ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))
+		return ERR_PTR(-EINVAL);
+
+	/*
+	 * Shared signal handlers imply shared VM. By way of the above,
+	 * thread groups also imply shared VM. Blocking this case allows
+	 * for various simplifications in other code.
+	 */
+	if ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))
+		return ERR_PTR(-EINVAL);
+
+	retval = security_task_create(clone_flags);
+	if (retval)
+		goto fork_out;
+
+	retval = -ENOMEM;
+	p = dup_task_struct(current);
+	if (!p)
+		goto fork_out;
+
+	retval = -EAGAIN;
+	if (atomic_read(&p->user->processes) >=
+			p->signal->rlim[RLIMIT_NPROC].rlim_cur) {
+		if (!capable(CAP_SYS_ADMIN) && !capable(CAP_SYS_RESOURCE) &&
+				p->user != &root_user)
+			goto bad_fork_free;
+	}
+
+	atomic_inc(&p->user->__count);
+	atomic_inc(&p->user->processes);
+	get_group_info(p->group_info);
+
+	/*
+	 * If multiple threads are within copy_process(), then this check
+	 * triggers too late. This doesn't hurt, the check is only there
+	 * to stop root fork bombs.
+	 */
+	if (nr_threads >= max_threads)
+		goto bad_fork_cleanup_count;
+
+	if (!try_module_get(p->thread_info->exec_domain->module))
+		goto bad_fork_cleanup_count;
+
+	if (p->binfmt && !try_module_get(p->binfmt->module))
+		goto bad_fork_cleanup_put_domain;
+
+	p->did_exec = 0;
+	copy_flags(clone_flags, p);
+	p->pid = pid;
+	retval = -EFAULT;
+	if (clone_flags & CLONE_PARENT_SETTID)
+		if (put_user(p->pid, parent_tidptr))
+			goto bad_fork_cleanup;
+
+	p->proc_dentry = NULL;
+
+	INIT_LIST_HEAD(&p->children);
+	INIT_LIST_HEAD(&p->sibling);
+	p->vfork_done = NULL;
+	spin_lock_init(&p->alloc_lock);
+	spin_lock_init(&p->proc_lock);
+
+	clear_tsk_thread_flag(p, TIF_SIGPENDING);
+	init_sigpending(&p->pending);
+
+	p->utime = cputime_zero;
+	p->stime = cputime_zero;
+ 	p->sched_time = 0;
+	p->rchar = 0;		/* I/O counter: bytes read */
+	p->wchar = 0;		/* I/O counter: bytes written */
+	p->syscr = 0;		/* I/O counter: read syscalls */
+	p->syscw = 0;		/* I/O counter: write syscalls */
+	acct_clear_integrals(p);
+
+ 	p->it_virt_expires = cputime_zero;
+	p->it_prof_expires = cputime_zero;
+ 	p->it_sched_expires = 0;
+ 	INIT_LIST_HEAD(&p->cpu_timers[0]);
+ 	INIT_LIST_HEAD(&p->cpu_timers[1]);
+ 	INIT_LIST_HEAD(&p->cpu_timers[2]);
+
+	p->lock_depth = -1;		/* -1 = no lock */
+	do_posix_clock_monotonic_gettime(&p->start_time);
+	p->security = NULL;
+	p->io_context = NULL;
+	p->io_wait = NULL;
+	p->audit_context = NULL;
+#ifdef CONFIG_NUMA
+ 	p->mempolicy = mpol_copy(p->mempolicy);
+ 	if (IS_ERR(p->mempolicy)) {
+ 		retval = PTR_ERR(p->mempolicy);
+ 		p->mempolicy = NULL;
+ 		goto bad_fork_cleanup;
+ 	}
+#endif
+
+	p->tgid = p->pid;
+	if (clone_flags & CLONE_THREAD)
+		p->tgid = current->tgid;
+
+	if ((retval = security_task_alloc(p)))
+		goto bad_fork_cleanup_policy;
+	if ((retval = audit_alloc(p)))
+		goto bad_fork_cleanup_security;
+	/* copy all the process information */
+	if ((retval = copy_semundo(clone_flags, p)))
+		goto bad_fork_cleanup_audit;
+	if ((retval = copy_files(clone_flags, p)))
+		goto bad_fork_cleanup_semundo;
+	if ((retval = copy_fs(clone_flags, p)))
+		goto bad_fork_cleanup_files;
+	if ((retval = copy_sighand(clone_flags, p)))
+		goto bad_fork_cleanup_fs;
+	if ((retval = copy_signal(clone_flags, p)))
+		goto bad_fork_cleanup_sighand;
+	if ((retval = copy_mm(clone_flags, p)))
+		goto bad_fork_cleanup_signal;
+	if ((retval = copy_keys(clone_flags, p)))
+		goto bad_fork_cleanup_mm;
+	if ((retval = copy_namespace(clone_flags, p)))
+		goto bad_fork_cleanup_keys;
+	retval = copy_thread(0, clone_flags, stack_start, stack_size, p, regs);
+	if (retval)
+		goto bad_fork_cleanup_namespace;
+
+	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
+	/*
+	 * Clear TID on mm_release()?
+	 */
+	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;
+
+	/*
+	 * Syscall tracing should be turned off in the child regardless
+	 * of CLONE_PTRACE.
+	 */
+	clear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);
+
+	/* Our parent execution domain becomes current domain
+	   These must match for thread signalling to apply */
+	   
+	p->parent_exec_id = p->self_exec_id;
+
+	/* ok, now we should be set up.. */
+	p->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);
+	p->pdeath_signal = 0;
+	p->exit_state = 0;
+
+	/* Perform scheduler related setup */
+	sched_fork(p);
+
+	/*
+	 * Ok, make it visible to the rest of the system.
+	 * We dont wake it up yet.
+	 */
+	p->group_leader = p;
+	INIT_LIST_HEAD(&p->ptrace_children);
+	INIT_LIST_HEAD(&p->ptrace_list);
+
+	/* Need tasklist lock for parent etc handling! */
+	write_lock_irq(&tasklist_lock);
+
+	/*
+	 * The task hasn't been attached yet, so cpus_allowed mask cannot
+	 * have changed. The cpus_allowed mask of the parent may have
+	 * changed after it was copied first time, and it may then move to
+	 * another CPU - so we re-copy it here and set the child's CPU to
+	 * the parent's CPU. This avoids alot of nasty races.
+	 */
+	p->cpus_allowed = current->cpus_allowed;
+	set_task_cpu(p, smp_processor_id());
+
+	/*
+	 * Check for pending SIGKILL! The new thread should not be allowed
+	 * to slip out of an OOM kill. (or normal SIGKILL.)
+	 */
+	if (sigismember(&current->pending.signal, SIGKILL)) {
+		write_unlock_irq(&tasklist_lock);
+		retval = -EINTR;
+		goto bad_fork_cleanup_namespace;
+	}
+
+	/* CLONE_PARENT re-uses the old parent */
+	if (clone_flags & (CLONE_PARENT|CLONE_THREAD))
+		p->real_parent = current->real_parent;
+	else
+		p->real_parent = current;
+	p->parent = p->real_parent;
+
+	if (clone_flags & CLONE_THREAD) {
+		spin_lock(&current->sighand->siglock);
+		/*
+		 * Important: if an exit-all has been started then
+		 * do not create this new thread - the whole thread
+		 * group is supposed to exit anyway.
+		 */
+		if (current->signal->flags & SIGNAL_GROUP_EXIT) {
+			spin_unlock(&current->sighand->siglock);
+			write_unlock_irq(&tasklist_lock);
+			retval = -EAGAIN;
+			goto bad_fork_cleanup_namespace;
+		}
+		p->group_leader = current->group_leader;
+
+		if (current->signal->group_stop_count > 0) {
+			/*
+			 * There is an all-stop in progress for the group.
+			 * We ourselves will stop as soon as we check signals.
+			 * Make the new thread part of that group stop too.
+			 */
+			current->signal->group_stop_count++;
+			set_tsk_thread_flag(p, TIF_SIGPENDING);
+		}
+
+		if (!cputime_eq(current->signal->it_virt_expires,
+				cputime_zero) ||
+		    !cputime_eq(current->signal->it_prof_expires,
+				cputime_zero) ||
+		    current->signal->rlim[RLIMIT_CPU].rlim_cur != RLIM_INFINITY ||
+		    !list_empty(&current->signal->cpu_timers[0]) ||
+		    !list_empty(&current->signal->cpu_timers[1]) ||
+		    !list_empty(&current->signal->cpu_timers[2])) {
+			/*
+			 * Have child wake up on its first tick to check
+			 * for process CPU timers.
+			 */
+			p->it_prof_expires = jiffies_to_cputime(1);
+		}
+
+		spin_unlock(&current->sighand->siglock);
+	}
+
+	SET_LINKS(p);
+	if (unlikely(p->ptrace & PT_PTRACED))
+		__ptrace_link(p, current->parent);
+
+	cpuset_fork(p);
+
+	attach_pid(p, PIDTYPE_PID, p->pid);
+	attach_pid(p, PIDTYPE_TGID, p->tgid);
+	if (thread_group_leader(p)) {
+		attach_pid(p, PIDTYPE_PGID, process_group(p));
+		attach_pid(p, PIDTYPE_SID, p->signal->session);
+		if (p->pid)
+			__get_cpu_var(process_counts)++;
+	}
+
+	nr_threads++;
+	total_forks++;
+	write_unlock_irq(&tasklist_lock);
+	retval = 0;
+
+fork_out:
+	if (retval)
+		return ERR_PTR(retval);
+	return p;
+
+bad_fork_cleanup_namespace:
+	exit_namespace(p);
+bad_fork_cleanup_keys:
+	exit_keys(p);
+bad_fork_cleanup_mm:
+	if (p->mm)
+		mmput(p->mm);
+bad_fork_cleanup_signal:
+	exit_signal(p);
+bad_fork_cleanup_sighand:
+	exit_sighand(p);
+bad_fork_cleanup_fs:
+	exit_fs(p); /* blocking */
+bad_fork_cleanup_files:
+	exit_files(p); /* blocking */
+bad_fork_cleanup_semundo:
+	exit_sem(p);
+bad_fork_cleanup_audit:
+	audit_free(p);
+bad_fork_cleanup_security:
+	security_task_free(p);
+bad_fork_cleanup_policy:
+#ifdef CONFIG_NUMA
+	mpol_free(p->mempolicy);
+#endif
+bad_fork_cleanup:
+	if (p->binfmt)
+		module_put(p->binfmt->module);
+bad_fork_cleanup_put_domain:
+	module_put(p->thread_info->exec_domain->module);
+bad_fork_cleanup_count:
+	put_group_info(p->group_info);
+	atomic_dec(&p->user->processes);
+	free_uid(p->user);
+bad_fork_free:
+	free_task(p);
+	goto fork_out;
+}
+
+struct pt_regs * __devinit __attribute__((weak)) idle_regs(struct pt_regs *regs)
+{
+	memset(regs, 0, sizeof(struct pt_regs));
+	return regs;
+}
+
+task_t * __devinit fork_idle(int cpu)
+{
+	task_t *task;
+	struct pt_regs regs;
+
+	task = copy_process(CLONE_VM, 0, idle_regs(&regs), 0, NULL, NULL, 0);
+	if (!task)
+		return ERR_PTR(-ENOMEM);
+	init_idle(task, cpu);
+	unhash_process(task);
+	return task;
+}
+
+static inline int fork_traceflag (unsigned clone_flags)
+{
+	if (clone_flags & CLONE_UNTRACED)
+		return 0;
+	else if (clone_flags & CLONE_VFORK) {
+		if (current->ptrace & PT_TRACE_VFORK)
+			return PTRACE_EVENT_VFORK;
+	} else if ((clone_flags & CSIGNAL) != SIGCHLD) {
+		if (current->ptrace & PT_TRACE_CLONE)
+			return PTRACE_EVENT_CLONE;
+	} else if (current->ptrace & PT_TRACE_FORK)
+		return PTRACE_EVENT_FORK;
+
+	return 0;
+}
+
+/*
+ *  Ok, this is the main fork-routine.
+ *
+ * It copies the process, and if successful kick-starts
+ * it and waits for it to finish using the VM if required.
+ */
+long do_fork(unsigned long clone_flags,
+	      unsigned long stack_start,
+	      struct pt_regs *regs,
+	      unsigned long stack_size,
+	      int __user *parent_tidptr,
+	      int __user *child_tidptr)
+{
+	struct task_struct *p;
+	int trace = 0;
+	long pid = alloc_pidmap();
+
+	if (pid < 0)
+		return -EAGAIN;
+	if (unlikely(current->ptrace)) {
+		trace = fork_traceflag (clone_flags);
+		if (trace)
+			clone_flags |= CLONE_PTRACE;
+	}
+
+	p = copy_process(clone_flags, stack_start, regs, stack_size, parent_tidptr, child_tidptr, pid);
+	/*
+	 * Do this prior waking up the new thread - the thread pointer
+	 * might get invalid after that point, if the thread exits quickly.
+	 */
+	if (!IS_ERR(p)) {
+		struct completion vfork;
+
+		if (clone_flags & CLONE_VFORK) {
+			p->vfork_done = &vfork;
+			init_completion(&vfork);
+		}
+
+		if ((p->ptrace & PT_PTRACED) || (clone_flags & CLONE_STOPPED)) {
+			/*
+			 * We'll start up with an immediate SIGSTOP.
+			 */
+			sigaddset(&p->pending.signal, SIGSTOP);
+			set_tsk_thread_flag(p, TIF_SIGPENDING);
+		}
+
+		if (!(clone_flags & CLONE_STOPPED))
+			wake_up_new_task(p, clone_flags);
+		else
+			p->state = TASK_STOPPED;
+
+		if (unlikely (trace)) {
+			current->ptrace_message = pid;
+			ptrace_notify ((trace << 8) | SIGTRAP);
+		}
+
+		if (clone_flags & CLONE_VFORK) {
+			wait_for_completion(&vfork);
+			if (unlikely (current->ptrace & PT_TRACE_VFORK_DONE))
+				ptrace_notify ((PTRACE_EVENT_VFORK_DONE << 8) | SIGTRAP);
+		}
+	} else {
+		free_pidmap(pid);
+		pid = PTR_ERR(p);
+	}
+	return pid;
+}
+
+void __init proc_caches_init(void)
+{
+	sighand_cachep = kmem_cache_create("sighand_cache",
+			sizeof(struct sighand_struct), 0,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
+	signal_cachep = kmem_cache_create("signal_cache",
+			sizeof(struct signal_struct), 0,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
+	files_cachep = kmem_cache_create("files_cache", 
+			sizeof(struct files_struct), 0,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
+	fs_cachep = kmem_cache_create("fs_cache", 
+			sizeof(struct fs_struct), 0,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
+	vm_area_cachep = kmem_cache_create("vm_area_struct",
+			sizeof(struct vm_area_struct), 0,
+			SLAB_PANIC, NULL, NULL);
+	mm_cachep = kmem_cache_create("mm_struct",
+			sizeof(struct mm_struct), 0,
+			SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
+}
