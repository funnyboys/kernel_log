commit c1e8d7c6a7a682e1405e3e242d32fc377fd196ff
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:54 2020 -0700

    mmap locking API: convert mmap_sem comments
    
    Convert comments that reference mmap_sem to reference mmap_lock instead.
    
    [akpm@linux-foundation.org: fix up linux-next leftovers]
    [akpm@linux-foundation.org: s/lockaphore/lock/, per Vlastimil]
    [akpm@linux-foundation.org: more linux-next fixups, per Michel]
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Laurent Dufour <ldufour@linux.ibm.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-13-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 36cbaa43ac80..727150f28103 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -440,7 +440,7 @@ static void exit_mm(void)
 	sync_mm_rss(mm);
 	/*
 	 * Serialize with any possible pending coredump.
-	 * We must hold mmap_sem around checking core_state
+	 * We must hold mmap_lock around checking core_state
 	 * and clearing tsk->mm.  The core-inducing thread
 	 * will increment ->nr_threads for each thread in the
 	 * group with ->mm != NULL.

commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1aecef938822..36cbaa43ac80 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -445,12 +445,12 @@ static void exit_mm(void)
 	 * will increment ->nr_threads for each thread in the
 	 * group with ->mm != NULL.
 	 */
-	down_read(&mm->mmap_sem);
+	mmap_read_lock(mm);
 	core_state = mm->core_state;
 	if (core_state) {
 		struct core_thread self;
 
-		up_read(&mm->mmap_sem);
+		mmap_read_unlock(mm);
 
 		self.task = current;
 		self.next = xchg(&core_state->dumper.next, &self);
@@ -468,14 +468,14 @@ static void exit_mm(void)
 			freezable_schedule();
 		}
 		__set_current_state(TASK_RUNNING);
-		down_read(&mm->mmap_sem);
+		mmap_read_lock(mm);
 	}
 	mmgrab(mm);
 	BUG_ON(mm != current->active_mm);
 	/* more a memory barrier than a real lock */
 	task_lock(current);
 	current->mm = NULL;
-	up_read(&mm->mmap_sem);
+	mmap_read_unlock(mm);
 	enter_lazy_tlb(mm, current);
 	task_unlock(current);
 	mm_update_next_owner(mm);

commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c300253a7b8e..1aecef938822 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -66,7 +66,6 @@
 
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
-#include <asm/pgtable.h>
 #include <asm/mmu_context.h>
 
 static void __unhash_process(struct task_struct *p, bool group_dead)

commit 039aeb9deb9291f3b19c375a8bc6fa7f768996cc
Merge: 6b2591c21273 13ffbd8db1dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:13:47 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - Move the arch-specific code into arch/arm64/kvm
    
       - Start the post-32bit cleanup
    
       - Cherry-pick a few non-invasive pre-NV patches
    
      x86:
       - Rework of TLB flushing
    
       - Rework of event injection, especially with respect to nested
         virtualization
    
       - Nested AMD event injection facelift, building on the rework of
         generic code and fixing a lot of corner cases
    
       - Nested AMD live migration support
    
       - Optimization for TSC deadline MSR writes and IPIs
    
       - Various cleanups
    
       - Asynchronous page fault cleanups (from tglx, common topic branch
         with tip tree)
    
       - Interrupt-based delivery of asynchronous "page ready" events (host
         side)
    
       - Hyper-V MSRs and hypercalls for guest debugging
    
       - VMX preemption timer fixes
    
      s390:
       - Cleanups
    
      Generic:
       - switch vCPU thread wakeup from swait to rcuwait
    
      The other architectures, and the guest side of the asynchronous page
      fault work, will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (256 commits)
      KVM: selftests: fix rdtsc() for vmx_tsc_adjust_test
      KVM: check userspace_addr for all memslots
      KVM: selftests: update hyperv_cpuid with SynDBG tests
      x86/kvm/hyper-v: Add support for synthetic debugger via hypercalls
      x86/kvm/hyper-v: enable hypercalls regardless of hypercall page
      x86/kvm/hyper-v: Add support for synthetic debugger interface
      x86/hyper-v: Add synthetic debugger definitions
      KVM: selftests: VMX preemption timer migration test
      KVM: nVMX: Fix VMX preemption timer migration
      x86/kvm/hyper-v: Explicitly align hcall param for kvm_hyperv_exit
      KVM: x86/pmu: Support full width counting
      KVM: x86/pmu: Tweak kvm_pmu_get_msr to pass 'struct msr_data' in
      KVM: x86: announce KVM_FEATURE_ASYNC_PF_INT
      KVM: x86: acknowledgment mechanism for async pf page ready notifications
      KVM: x86: interrupt based APF 'page ready' event delivery
      KVM: introduce kvm_read_guest_offset_cached()
      KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
      KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
      Revert "KVM: async_pf: Fix #DF due to inject "Page not Present" and "Page Ready" exceptions simultaneously"
      KVM: VMX: Replace zero-length array with flexible-array
      ...

commit d479c5a1919b4e569dcd3ae9c84ed74a675d0b94
Merge: f6aee505c71b 25de110d1486
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 13:06:42 2020 -0700

    Merge tag 'sched-core-2020-06-02' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The changes in this cycle are:
    
       - Optimize the task wakeup CPU selection logic, to improve
         scalability and reduce wakeup latency spikes
    
       - PELT enhancements
    
       - CFS bandwidth handling fixes
    
       - Optimize the wakeup path by remove rq->wake_list and replacing it
         with ->ttwu_pending
    
       - Optimize IPI cross-calls by making flush_smp_call_function_queue()
         process sync callbacks first.
    
       - Misc fixes and enhancements"
    
    * tag 'sched-core-2020-06-02' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (36 commits)
      irq_work: Define irq_work_single() on !CONFIG_IRQ_WORK too
      sched/headers: Split out open-coded prototypes into kernel/sched/smp.h
      sched: Replace rq::wake_list
      sched: Add rq::ttwu_pending
      irq_work, smp: Allow irq_work on call_single_queue
      smp: Optimize send_call_function_single_ipi()
      smp: Move irq_work_run() out of flush_smp_call_function_queue()
      smp: Optimize flush_smp_call_function_queue()
      sched: Fix smp_call_function_single_async() usage for ILB
      sched/core: Offload wakee task activation if it the wakee is descheduling
      sched/core: Optimize ttwu() spinning on p->on_cpu
      sched: Defend cfs and rt bandwidth quota against overflow
      sched/cpuacct: Fix charge cpuacct.usage_sys
      sched/fair: Replace zero-length array with flexible-array
      sched/pelt: Sync util/runnable_sum with PELT window when propagating
      sched/cpuacct: Use __this_cpu_add() instead of this_cpu_ptr()
      sched/fair: Optimize enqueue_task_fair()
      sched: Make scheduler_ipi inline
      sched: Clean up scheduler_ipi()
      sched/core: Simplify sched_init()
      ...

commit e148a8f948afc0b1eeb5c157b23b3d0a4d4517a5
Merge: e0cd9206878a 5fb1514164de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 16:11:38 2020 -0700

    Merge branch 'uaccess.readdir' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull uaccess/readdir updates from Al Viro:
     "Finishing the conversion of readdir.c to unsafe_... API.
    
      This includes the uaccess_{read,write}_begin series by Christophe
      Leroy"
    
    * 'uaccess.readdir' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      readdir.c: get rid of the last __put_user(), drop now-useless access_ok()
      readdir.c: get compat_filldir() more or less in sync with filldir()
      switch readdir(2) to unsafe_copy_dirent_name()
      drm/i915/gem: Replace user_access_begin by user_write_access_begin
      uaccess: Selectively open read or write user access
      uaccess: Add user_read_access_begin/end and user_write_access_begin/end

commit 9d5272f5e36155bcead69417fd12e98624e7faef
Merge: febd668d375c 3a7c8fafd1b4
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 20 03:40:09 2020 -0400

    Merge tag 'noinstr-x86-kvm-2020-05-16' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip into HEAD

commit 9d9a6ebfea329cadeda87544dceae01e9c27aaef
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Thu Apr 23 22:48:34 2020 -0700

    rcuwait: Let rcuwait_wake_up() return whether or not a task was awoken
    
    Propagating the return value of wake_up_process() back to the caller
    can come in handy for future users, such as for statistics or
    accounting purposes.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Message-Id: <20200424054837.5138-3-dave@stgolabs.net>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9f9015f3f6b0..f3beb637acf7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -227,8 +227,9 @@ void release_task(struct task_struct *p)
 		goto repeat;
 }
 
-void rcuwait_wake_up(struct rcuwait *w)
+int rcuwait_wake_up(struct rcuwait *w)
 {
+	int ret = 0;
 	struct task_struct *task;
 
 	rcu_read_lock();
@@ -248,8 +249,10 @@ void rcuwait_wake_up(struct rcuwait *w)
 
 	task = rcu_dereference(w->task);
 	if (task)
-		wake_up_process(task);
+		ret = wake_up_process(task);
 	rcu_read_unlock();
+
+	return ret;
 }
 EXPORT_SYMBOL_GPL(rcuwait_wake_up);
 

commit c9d64a1b2d0b9c81aef5e907bbeff549fd844d13
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Thu Apr 23 22:48:33 2020 -0700

    rcuwait: Fix stale wake call name in comment
    
    The 'trywake' name was renamed to simply 'wake', update the comment.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Message-Id: <20200424054837.5138-2-dave@stgolabs.net>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 389a88cb3081..9f9015f3f6b0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -236,7 +236,7 @@ void rcuwait_wake_up(struct rcuwait *w)
 	/*
 	 * Order condition vs @task, such that everything prior to the load
 	 * of @task is visible. This is the condition as to why the user called
-	 * rcuwait_trywake() in the first place. Pairs with set_current_state()
+	 * rcuwait_wake() in the first place. Pairs with set_current_state()
 	 * barrier (A) in rcuwait_wait_event().
 	 *
 	 *    WAIT                WAKE

commit 41cd780524674082b037e7c8461f90c5e42103f0
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 3 07:20:51 2020 +0000

    uaccess: Selectively open read or write user access
    
    When opening user access to only perform reads, only open read access.
    When opening user access to only perform writes, only open write
    access.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/2e73bc57125c2c6ab12a587586a4eed3a47105fc.1585898438.git.christophe.leroy@c-s.fr

diff --git a/kernel/exit.c b/kernel/exit.c
index 389a88cb3081..2d97cbba512d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1557,7 +1557,7 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	if (!infop)
 		return err;
 
-	if (!user_access_begin(infop, sizeof(*infop)))
+	if (!user_write_access_begin(infop, sizeof(*infop)))
 		return -EFAULT;
 
 	unsafe_put_user(signo, &infop->si_signo, Efault);
@@ -1566,10 +1566,10 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	unsafe_put_user(info.pid, &infop->si_pid, Efault);
 	unsafe_put_user(info.uid, &infop->si_uid, Efault);
 	unsafe_put_user(info.status, &infop->si_status, Efault);
-	user_access_end();
+	user_write_access_end();
 	return err;
 Efault:
-	user_access_end();
+	user_write_access_end();
 	return -EFAULT;
 }
 
@@ -1684,7 +1684,7 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 	if (!infop)
 		return err;
 
-	if (!user_access_begin(infop, sizeof(*infop)))
+	if (!user_write_access_begin(infop, sizeof(*infop)))
 		return -EFAULT;
 
 	unsafe_put_user(signo, &infop->si_signo, Efault);
@@ -1693,10 +1693,10 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 	unsafe_put_user(info.pid, &infop->si_pid, Efault);
 	unsafe_put_user(info.uid, &infop->si_uid, Efault);
 	unsafe_put_user(info.status, &infop->si_status, Efault);
-	user_access_end();
+	user_write_access_end();
 	return err;
 Efault:
-	user_access_end();
+	user_write_access_end();
 	return -EFAULT;
 }
 #endif

commit 586b58cac8b4683eb58a1446fbc399de18974e40
Author: Jann Horn <jannh@google.com>
Date:   Thu Mar 5 23:06:57 2020 +0100

    exit: Move preemption fixup up, move blocking operations down
    
    With CONFIG_DEBUG_ATOMIC_SLEEP=y and CONFIG_CGROUPS=y, kernel oopses in
    non-preemptible context look untidy; after the main oops, the kernel prints
    a "sleeping function called from invalid context" report because
    exit_signals() -> cgroup_threadgroup_change_begin() -> percpu_down_read()
    can sleep, and that happens before the preempt_count_set(PREEMPT_ENABLED)
    fixup.
    
    It looks like the same thing applies to profile_task_exit() and
    kcov_task_exit().
    
    Fix it by moving the preemption fixup up and the calls to
    profile_task_exit() and kcov_task_exit() down.
    
    Fixes: 1dc0fffc48af ("sched/core: Robustify preemption leak checks")
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200305220657.46800-1-jannh@google.com

diff --git a/kernel/exit.c b/kernel/exit.c
index ce2a75bc0ade..d56fe51bdf07 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -708,8 +708,12 @@ void __noreturn do_exit(long code)
 	struct task_struct *tsk = current;
 	int group_dead;
 
-	profile_task_exit(tsk);
-	kcov_task_exit(tsk);
+	/*
+	 * We can get here from a kernel oops, sometimes with preemption off.
+	 * Start by checking for critical errors.
+	 * Then fix up important state like USER_DS and preemption.
+	 * Then do everything else.
+	 */
 
 	WARN_ON(blk_needs_flush_plug(tsk));
 
@@ -727,6 +731,16 @@ void __noreturn do_exit(long code)
 	 */
 	set_fs(USER_DS);
 
+	if (unlikely(in_atomic())) {
+		pr_info("note: %s[%d] exited with preempt_count %d\n",
+			current->comm, task_pid_nr(current),
+			preempt_count());
+		preempt_count_set(PREEMPT_ENABLED);
+	}
+
+	profile_task_exit(tsk);
+	kcov_task_exit(tsk);
+
 	ptrace_event(PTRACE_EVENT_EXIT, code);
 
 	validate_creds_for_do_exit(tsk);
@@ -744,13 +758,6 @@ void __noreturn do_exit(long code)
 
 	exit_signals(tsk);  /* sets PF_EXITING */
 
-	if (unlikely(in_atomic())) {
-		pr_info("note: %s[%d] exited with preempt_count %d\n",
-			current->comm, task_pid_nr(current),
-			preempt_count());
-		preempt_count_set(PREEMPT_ENABLED);
-	}
-
 	/* sync mm's RSS info before statistics gathering */
 	if (tsk->mm)
 		sync_mm_rss(tsk->mm);

commit 6ade99ec6175ab2b54c227521e181e1c3c2bfc8a
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Apr 24 15:41:20 2020 -0500

    proc: Put thread_pid in release_task not proc_flush_pid
    
    Oleg pointed out that in the unlikely event the kernel is compiled
    with CONFIG_PROC_FS unset that release_task will now leak the pid.
    
    Move the put_pid out of proc_flush_pid into release_task to fix this
    and to guarantee I don't make that mistake again.
    
    When possible it makes sense to keep get and put in the same function
    so it can easily been seen how they pair up.
    
    Fixes: 7bc3e6e55acf ("proc: Use a list of inodes to flush from proc")
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 389a88cb3081..ce2a75bc0ade 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -219,6 +219,7 @@ void release_task(struct task_struct *p)
 
 	write_unlock_irq(&tasklist_lock);
 	proc_flush_pid(thread_pid);
+	put_pid(thread_pid);
 	release_thread(p);
 	put_task_struct_rcu_user(p);
 

commit d987ca1c6b7e22fbd30664111e85cec7aa66000d
Merge: 919dce24701f d1e7fd6462ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 2 11:22:17 2020 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull exec/proc updates from Eric Biederman:
     "This contains two significant pieces of work: the work to sort out
      proc_flush_task, and the work to solve a deadlock between strace and
      exec.
    
      Fixing proc_flush_task so that it no longer requires a persistent
      mount makes improvements to proc possible. The removal of the
      persistent mount solves an old regression that that caused the hidepid
      mount option to only work on remount not on mount. The regression was
      found and reported by the Android folks. This further allows Alexey
      Gladkov's work making proc mount options specific to an individual
      mount of proc to move forward.
    
      The work on exec starts solving a long standing issue with exec that
      it takes mutexes of blocking userspace applications, which makes exec
      extremely deadlock prone. For the moment this adds a second mutex with
      a narrower scope that handles all of the easy cases. Which makes the
      tricky cases easy to spot. With a little luck the code to solve those
      deadlocks will be ready by next merge window"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (25 commits)
      signal: Extend exec_id to 64bits
      pidfd: Use new infrastructure to fix deadlocks in execve
      perf: Use new infrastructure to fix deadlocks in execve
      proc: io_accounting: Use new infrastructure to fix deadlocks in execve
      proc: Use new infrastructure to fix deadlocks in execve
      kernel/kcmp.c: Use new infrastructure to fix deadlocks in execve
      kernel: doc: remove outdated comment cred.c
      mm: docs: Fix a comment in process_vm_rw_core
      selftests/ptrace: add test cases for dead-locks
      exec: Fix a deadlock in strace
      exec: Add exec_update_mutex to replace cred_guard_mutex
      exec: Move exec_mmap right after de_thread in flush_old_exec
      exec: Move cleanup of posix timers on exec out of de_thread
      exec: Factor unshare_sighand out of de_thread and call it separately
      exec: Only compute current once in flush_old_exec
      pid: Improve the comment about waiting in zap_pid_ns_processes
      proc: Remove the now unnecessary internal mount of proc
      uml: Create a private mount of proc for mconsole
      uml: Don't consult current to find the proc_mnt in mconsole_proc
      proc: Use a list of inodes to flush from proc
      ...

commit dbb381b619aa5242c9cb1a8fd54d71c4d79c91eb
Merge: 336622e9fce7 4479730e9263
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 18:51:47 2020 -0700

    Merge tag 'timers-core-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timekeeping and timer updates from Thomas Gleixner:
     "Core:
    
       - Consolidation of the vDSO build infrastructure to address the
         difficulties of cross-builds for ARM64 compat vDSO libraries by
         restricting the exposure of header content to the vDSO build.
    
         This is achieved by splitting out header content into separate
         headers. which contain only the minimaly required information which
         is necessary to build the vDSO. These new headers are included from
         the kernel headers and the vDSO specific files.
    
       - Enhancements to the generic vDSO library allowing more fine grained
         control over the compiled in code, further reducing architecture
         specific storage and preparing for adopting the generic library by
         PPC.
    
       - Cleanup and consolidation of the exit related code in posix CPU
         timers.
    
       - Small cleanups and enhancements here and there
    
      Drivers:
    
       - The obligatory new drivers: Ingenic JZ47xx and X1000 TCU support
    
       - Correct the clock rate of PIT64b global clock
    
       - setup_irq() cleanup
    
       - Preparation for PWM and suspend support for the TI DM timer
    
       - Expand the fttmr010 driver to support ast2600 systems
    
       - The usual small fixes, enhancements and cleanups all over the
         place"
    
    * tag 'timers-core-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (80 commits)
      Revert "clocksource/drivers/timer-probe: Avoid creating dead devices"
      vdso: Fix clocksource.h macro detection
      um: Fix header inclusion
      arm64: vdso32: Enable Clang Compilation
      lib/vdso: Enable common headers
      arm: vdso: Enable arm to use common headers
      x86/vdso: Enable x86 to use common headers
      mips: vdso: Enable mips to use common headers
      arm64: vdso32: Include common headers in the vdso library
      arm64: vdso: Include common headers in the vdso library
      arm64: Introduce asm/vdso/processor.h
      arm64: vdso32: Code clean up
      linux/elfnote.h: Replace elf.h with UAPI equivalent
      scripts: Fix the inclusion order in modpost
      common: Introduce processor.h
      linux/ktime.h: Extract common header for vDSO
      linux/jiffies.h: Extract common header for vDSO
      linux/time64.h: Extract common header for vDSO
      linux/time32.h: Extract common header for vDSO
      linux/time.h: Extract common header for vDSO
      ...

commit 4b9fd8a829a1eec7442e38afff21d610604de56a
Merge: a776c270a0b2 f1e67e355c2a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 16:17:15 2020 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Continued user-access cleanups in the futex code.
    
       - percpu-rwsem rewrite that uses its own waitqueue and atomic_t
         instead of an embedded rwsem. This addresses a couple of
         weaknesses, but the primary motivation was complications on the -rt
         kernel.
    
       - Introduce raw lock nesting detection on lockdep
         (CONFIG_PROVE_RAW_LOCK_NESTING=y), document the raw_lock vs. normal
         lock differences. This too originates from -rt.
    
       - Reuse lockdep zapped chain_hlocks entries, to conserve RAM
         footprint on distro-ish kernels running into the "BUG:
         MAX_LOCKDEP_CHAIN_HLOCKS too low!" depletion of the lockdep
         chain-entries pool.
    
       - Misc cleanups, smaller fixes and enhancements - see the changelog
         for details"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (55 commits)
      fs/buffer: Make BH_Uptodate_Lock bit_spin_lock a regular spinlock_t
      thermal/x86_pkg_temp: Make pkg_temp_lock a raw_spinlock_t
      Documentation/locking/locktypes: Minor copy editor fixes
      Documentation/locking/locktypes: Further clarifications and wordsmithing
      m68knommu: Remove mm.h include from uaccess_no.h
      x86: get rid of user_atomic_cmpxchg_inatomic()
      generic arch_futex_atomic_op_inuser() doesn't need access_ok()
      x86: don't reload after cmpxchg in unsafe_atomic_op2() loop
      x86: convert arch_futex_atomic_op_inuser() to user_access_begin/user_access_end()
      objtool: whitelist __sanitizer_cov_trace_switch()
      [parisc, s390, sparc64] no need for access_ok() in futex handling
      sh: no need of access_ok() in arch_futex_atomic_op_inuser()
      futex: arch_futex_atomic_op_inuser() calling conventions change
      completion: Use lockdep_assert_RT_in_threaded_ctx() in complete_all()
      lockdep: Add posixtimer context tracing bits
      lockdep: Annotate irq_work
      lockdep: Add hrtimer context tracing bits
      lockdep: Introduce wait-type checks
      completion: Use simple wait queues
      sched/swait: Prepare usage in completions
      ...

commit b95e31c07c5eb4f5c0769f12b38b0343d7961040
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Feb 28 11:15:03 2020 -0600

    posix-cpu-timers: Stop disabling timers on mt-exec
    
    The reasons why the extra posix_cpu_timers_exit_group() invocation has been
    added are not entirely clear from the commit message.  Today all that
    posix_cpu_timers_exit_group() does is stop timers that are tracking the
    task from firing.  Every other operation on those timers is still allowed.
    
    The practical implication of this is posix_cpu_timer_del() which could
    not get the siglock after the thread group leader has exited (because
    sighand == NULL) would be able to run successfully because the timer
    was already dequeued.
    
    With that locking issue fixed there is no point in disabling all of the
    timers.  So remove this ``tempoary'' hack.
    
    Fixes: e0a70217107e ("posix-cpu-timers: workaround to suppress the problems with mt exec")
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/87o8tityzs.fsf@x220.int.ebiederm.org

diff --git a/kernel/exit.c b/kernel/exit.c
index 2833ffb0c211..df546315f699 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -103,17 +103,8 @@ static void __exit_signal(struct task_struct *tsk)
 
 #ifdef CONFIG_POSIX_TIMERS
 	posix_cpu_timers_exit(tsk);
-	if (group_dead) {
+	if (group_dead)
 		posix_cpu_timers_exit_group(tsk);
-	} else {
-		/*
-		 * This can only happen if the caller is de_thread().
-		 * FIXME: this is the temporary hack, we should teach
-		 * posix-cpu-timers to handle this case correctly.
-		 */
-		if (unlikely(has_group_leader_pid(tsk)))
-			posix_cpu_timers_exit_group(tsk);
-	}
 #endif
 
 	if (group_dead) {

commit 22a34c6fe0ffc1d92ee26a25913fadf347258fd6
Author: Madhuparna Bhowmik <madhuparnabhowmik10@gmail.com>
Date:   Thu Jan 30 11:50:28 2020 +0530

    exit: Fix Sparse errors and warnings
    
    This patch fixes the following sparse error:
    kernel/exit.c:627:25: error: incompatible types in comparison expression
    
    And the following warning:
    kernel/exit.c:626:40: warning: incorrect type in assignment
    
    Signed-off-by: Madhuparna Bhowmik <madhuparnabhowmik10@gmail.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
    [christian.brauner@ubuntu.com: edit commit message]
    Link: https://lore.kernel.org/r/20200130062028.4870-1-madhuparnabhowmik10@gmail.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2833ffb0c211..0b81b26a872a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -619,8 +619,8 @@ static void forget_original_parent(struct task_struct *father,
 	reaper = find_new_reaper(father, reaper);
 	list_for_each_entry(p, &father->children, sibling) {
 		for_each_thread(p, t) {
-			t->real_parent = reaper;
-			BUG_ON((!t->ptrace) != (t->parent == father));
+			RCU_INIT_POINTER(t->real_parent, reaper);
+			BUG_ON((!t->ptrace) != (rcu_access_pointer(t->parent) == father));
 			if (likely(!t->ptrace))
 				t->parent = t->real_parent;
 			if (t->pdeath_signal)

commit 7bc3e6e55acf065500a24621f3b313e7e5998acf
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Feb 19 18:22:26 2020 -0600

    proc: Use a list of inodes to flush from proc
    
    Rework the flushing of proc to use a list of directory inodes that
    need to be flushed.
    
    The list is kept on struct pid not on struct task_struct, as there is
    a fixed connection between proc inodes and pids but at least for the
    case of de_thread the pid of a task_struct changes.
    
    This removes the dependency on proc_mnt which allows for different
    mounts of proc having different mount options even in the same pid
    namespace and this allows for the removal of proc_mnt which will
    trivially the first mount of proc to honor it's mount options.
    
    This flushing remains an optimization.  The functions
    pid_delete_dentry and pid_revalidate ensure that ordinary dcache
    management will not attempt to use dentries past the point their
    respective task has died.  When unused the shrinker will
    eventually be able to remove these dentries.
    
    There is a case in de_thread where proc_flush_pid can be
    called early for a given pid.  Which winds up being
    safe (if suboptimal) as this is just an optiimization.
    
    Only pid directories are put on the list as the other
    per pid files are children of those directories and
    d_invalidate on the directory will get them as well.
    
    So that the pid can be used during flushing it's reference count is
    taken in release_task and dropped in proc_flush_pid.  Further the call
    of proc_flush_pid is moved after the tasklist_lock is released in
    release_task so that it is certain that the pid has already been
    unhashed when flushing it taking place.  This removes a small race
    where a dentry could recreated.
    
    As struct pid is supposed to be small and I need a per pid lock
    I reuse the only lock that currently exists in struct pid the
    the wait_pidfd.lock.
    
    The net result is that this adds all of this functionality
    with just a little extra list management overhead and
    a single extra pointer in struct pid.
    
    v2: Initialize pid->inodes.  I somehow failed to get that
        initialization into the initial version of the patch.  A boot
        failure was reported by "kernel test robot <lkp@intel.com>", and
        failure to initialize that pid->inodes matches all of the reported
        symptoms.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2833ffb0c211..502b4995b688 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -191,6 +191,7 @@ void put_task_struct_rcu_user(struct task_struct *task)
 void release_task(struct task_struct *p)
 {
 	struct task_struct *leader;
+	struct pid *thread_pid;
 	int zap_leader;
 repeat:
 	/* don't need to get the RCU readlock here - the process is dead and
@@ -199,11 +200,11 @@ void release_task(struct task_struct *p)
 	atomic_dec(&__task_cred(p)->user->processes);
 	rcu_read_unlock();
 
-	proc_flush_task(p);
 	cgroup_release(p);
 
 	write_lock_irq(&tasklist_lock);
 	ptrace_release_task(p);
+	thread_pid = get_pid(p->thread_pid);
 	__exit_signal(p);
 
 	/*
@@ -226,6 +227,7 @@ void release_task(struct task_struct *p)
 	}
 
 	write_unlock_irq(&tasklist_lock);
+	proc_flush_pid(thread_pid);
 	release_thread(p);
 	put_task_struct_rcu_user(p);
 

commit ac8dec420970f5cbaf2f6eda39153a60ec5b257b
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Mon Nov 18 15:19:35 2019 -0800

    locking/percpu-rwsem: Fold __percpu_up_read()
    
    Now that __percpu_up_read() is only ever used from percpu_up_read()
    merge them, it's a small function.
    
    Signed-off-by: Davidlohr Bueso <dave@stgolabs.net>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Waiman Long <longman@redhat.com>
    Link: https://lkml.kernel.org/r/20200131151540.212415454@infradead.org

diff --git a/kernel/exit.c b/kernel/exit.c
index 2833ffb0c211..f64a8f9d412a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -258,6 +258,7 @@ void rcuwait_wake_up(struct rcuwait *w)
 		wake_up_process(task);
 	rcu_read_unlock();
 }
+EXPORT_SYMBOL_GPL(rcuwait_wake_up);
 
 /*
  * Determine if a process group is "orphaned", according to the POSIX

commit d9c82fd8c89766f2cf1e667f663e8e8c25c12aee
Merge: 6f2e9c3d2816 43cf75d96409
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 3 11:17:14 2020 -0800

    Merge tag 'for-linus-2020-01-03' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull thread fixes from Christian Brauner:
     "Here are two fixes:
    
       - Panic earlier when global init exits to generate useable coredumps.
    
         Currently, when global init and all threads in its thread-group
         have exited we panic via:
    
           do_exit()
           -> exit_notify()
              -> forget_original_parent()
                 -> find_child_reaper()
    
         This makes it hard to extract a useable coredump for global init
         from a kernel crashdump because by the time we panic exit_mm() will
         have already released global init's mm. We now panic slightly
         earlier. This has been a problem in certain environments such as
         Android.
    
       - Fix a race in assigning and reading taskstats for thread-groups
         with more than one thread.
    
         This patch has been waiting for quite a while since people
         disagreed on what the correct fix was at first"
    
    * tag 'for-linus-2020-01-03' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      exit: panic before exit_mm() on global init exit
      taskstats: fix data-race

commit 43cf75d96409a20ef06b756877a2e72b10a026fc
Author: chenqiwu <chenqiwu@xiaomi.com>
Date:   Thu Dec 19 14:29:53 2019 +0800

    exit: panic before exit_mm() on global init exit
    
    Currently, when global init and all threads in its thread-group have exited
    we panic via:
    do_exit()
    -> exit_notify()
       -> forget_original_parent()
          -> find_child_reaper()
    This makes it hard to extract a useable coredump for global init from a
    kernel crashdump because by the time we panic exit_mm() will have already
    released global init's mm.
    This patch moves the panic futher up before exit_mm() is called. As was the
    case previously, we only panic when global init and all its threads in the
    thread-group have exited.
    
    Signed-off-by: chenqiwu <chenqiwu@xiaomi.com>
    Acked-by: Christian Brauner <christian.brauner@ubuntu.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    [christian.brauner@ubuntu.com: fix typo, rewrite commit message]
    Link: https://lore.kernel.org/r/1576736993-10121-1-git-send-email-qiwuchen55@gmail.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index a46a50d67002..fc364272759d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -517,10 +517,6 @@ static struct task_struct *find_child_reaper(struct task_struct *father,
 	}
 
 	write_unlock_irq(&tasklist_lock);
-	if (unlikely(pid_ns == &init_pid_ns)) {
-		panic("Attempted to kill init! exitcode=0x%08x\n",
-			father->signal->group_exit_code ?: father->exit_code);
-	}
 
 	list_for_each_entry_safe(p, n, dead, ptrace_entry) {
 		list_del_init(&p->ptrace_entry);
@@ -786,6 +782,14 @@ void __noreturn do_exit(long code)
 	acct_update_integrals(tsk);
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
+		/*
+		 * If the last thread of global init has exited, panic
+		 * immediately to get a useable coredump.
+		 */
+		if (unlikely(is_global_init(tsk)))
+			panic("Attempted to kill init! exitcode=0x%08x\n",
+				tsk->signal->group_exit_code ?: (int)code);
+
 #ifdef CONFIG_POSIX_TIMERS
 		hrtimer_cancel(&tsk->signal->real_timer);
 		exit_itimers(tsk->signal);

commit 6a965666b7e7475c2f8c8e724703db58b8a8a445
Merge: 32ef9553635a 3c0edea9b29f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 30 14:12:13 2019 -0800

    Merge tag 'notifications-pipe-prep-20191115' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs
    
    Pull pipe rework from David Howells:
     "This is my set of preparatory patches for building a general
      notification queue on top of pipes. It makes a number of significant
      changes:
    
       - It removes the nr_exclusive argument from __wake_up_sync_key() as
         this is always 1. This prepares for the next step:
    
       - Adds wake_up_interruptible_sync_poll_locked() so that poll can be
         woken up from a function that's holding the poll waitqueue
         spinlock.
    
       - Change the pipe buffer ring to be managed in terms of unbounded
         head and tail indices rather than bounded index and length. This
         means that reading the pipe only needs to modify one index, not
         two.
    
       - A selection of helper functions are provided to query the state of
         the pipe buffer, plus a couple to apply updates to the pipe
         indices.
    
       - The pipe ring is allowed to have kernel-reserved slots. This allows
         many notification messages to be spliced in by the kernel without
         allowing userspace to pin too many pages if it writes to the same
         pipe.
    
       - Advance the head and tail indices inside the pipe waitqueue lock
         and use wake_up_interruptible_sync_poll_locked() to poke poll
         without having to take the lock twice.
    
       - Rearrange pipe_write() to preallocate the buffer it is going to
         write into and then drop the spinlock. This allows kernel
         notifications to then be added the ring whilst it is filling the
         buffer it allocated. The read side is stalled because the pipe
         mutex is still held.
    
       - Don't wake up readers on a pipe if there was already data in it
         when we added more.
    
       - Don't wake up writers on a pipe if the ring wasn't full before we
         removed a buffer"
    
    * tag 'notifications-pipe-prep-20191115' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs:
      pipe: Remove sync on wake_ups
      pipe: Increase the writer-wakeup threshold to reduce context-switch count
      pipe: Check for ring full inside of the spinlock in pipe_write()
      pipe: Remove redundant wakeup from pipe_write()
      pipe: Rearrange sequence in pipe_write() to preallocate slot
      pipe: Conditionalise wakeup in pipe_read()
      pipe: Advance tail pointer inside of wait spinlock in pipe_read()
      pipe: Allow pipes to have kernel-reserved slots
      pipe: Use head and tail pointers for the ring, not cursor and length
      Add wake_up_interruptible_sync_poll_locked()
      Remove the nr_exclusive argument from __wake_up_sync_key()
      pipe: Reduce #inclusion of pipe_fs_i.h

commit 168829ad09ca9cdfdc664b2110d0e3569932c12d
Merge: 1ae78780eda5 500543c53a54
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 16:02:40 2019 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - A comprehensive rewrite of the robust/PI futex code's exit handling
         to fix various exit races. (Thomas Gleixner et al)
    
       - Rework the generic REFCOUNT_FULL implementation using
         atomic_fetch_* operations so that the performance impact of the
         cmpxchg() loops is mitigated for common refcount operations.
    
         With these performance improvements the generic implementation of
         refcount_t should be good enough for everybody - and this got
         confirmed by performance testing, so remove ARCH_HAS_REFCOUNT and
         REFCOUNT_FULL entirely, leaving the generic implementation enabled
         unconditionally. (Will Deacon)
    
       - Other misc changes, fixes, cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      lkdtm: Remove references to CONFIG_REFCOUNT_FULL
      locking/refcount: Remove unused 'refcount_error_report()' function
      locking/refcount: Consolidate implementations of refcount_t
      locking/refcount: Consolidate REFCOUNT_{MAX,SATURATED} definitions
      locking/refcount: Move saturation warnings out of line
      locking/refcount: Improve performance of generic REFCOUNT_FULL code
      locking/refcount: Move the bulk of the REFCOUNT_FULL implementation into the <linux/refcount.h> header
      locking/refcount: Remove unused refcount_*_checked() variants
      locking/refcount: Ensure integer operands are treated as signed
      locking/refcount: Define constants for saturation and max refcount values
      futex: Prevent exit livelock
      futex: Provide distinct return value when owner is exiting
      futex: Add mutex around futex exit
      futex: Provide state handling for exec() as well
      futex: Sanitize exit state handling
      futex: Mark the begin of futex exit explicitly
      futex: Set task::futex_state to DEAD right after handling futex exit
      futex: Split futex_mm_release() for exit/exec
      exit/exec: Seperate mm_release()
      futex: Replace PF_EXITPIDONE with a state
      ...

commit 18f694385c4fd77a09851fd301236746ca83f3cb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 6 22:55:41 2019 +0100

    futex: Mark the begin of futex exit explicitly
    
    Instead of relying on PF_EXITING use an explicit state for the futex exit
    and set it in the futex exit function. This moves the smp barrier and the
    lock/unlock serialization into the futex code.
    
    As with the DEAD state this is restricted to the exit path as exec
    continues to use the same task struct.
    
    This allows to simplify that logic in a next step.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191106224556.539409004@linutronix.de

diff --git a/kernel/exit.c b/kernel/exit.c
index f3b8fa1b8945..d351fd09e739 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -746,23 +746,12 @@ void __noreturn do_exit(long code)
 	 */
 	if (unlikely(tsk->flags & PF_EXITING)) {
 		pr_alert("Fixing recursive fault but reboot is needed!\n");
-		futex_exit_done(tsk);
+		futex_exit_recursive(tsk);
 		set_current_state(TASK_UNINTERRUPTIBLE);
 		schedule();
 	}
 
 	exit_signals(tsk);  /* sets PF_EXITING */
-	/*
-	 * Ensure that all new tsk->pi_lock acquisitions must observe
-	 * PF_EXITING. Serializes against futex.c:attach_to_pi_owner().
-	 */
-	smp_mb();
-	/*
-	 * Ensure that we must observe the pi_state in exit_mm() ->
-	 * mm_release() -> exit_pi_state_list().
-	 */
-	raw_spin_lock_irq(&tsk->pi_lock);
-	raw_spin_unlock_irq(&tsk->pi_lock);
 
 	if (unlikely(in_atomic())) {
 		pr_info("note: %s[%d] exited with preempt_count %d\n",

commit f24f22435dcc11389acc87e5586239c1819d217c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 6 22:55:40 2019 +0100

    futex: Set task::futex_state to DEAD right after handling futex exit
    
    Setting task::futex_state in do_exit() is rather arbitrarily placed for no
    reason. Move it into the futex code.
    
    Note, this is only done for the exit cleanup as the exec cleanup cannot set
    the state to FUTEX_STATE_DEAD because the task struct is still in active
    use.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191106224556.439511191@linutronix.de

diff --git a/kernel/exit.c b/kernel/exit.c
index cd893b530902..f3b8fa1b8945 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -837,7 +837,6 @@ void __noreturn do_exit(long code)
 	 * Make sure we are holding no locks:
 	 */
 	debug_check_no_locks_held();
-	futex_exit_done(tsk);
 
 	if (tsk->io_context)
 		exit_io_context(tsk);

commit 4610ba7ad877fafc0a25a30c6c82015304120426
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 6 22:55:38 2019 +0100

    exit/exec: Seperate mm_release()
    
    mm_release() contains the futex exit handling. mm_release() is called from
    do_exit()->exit_mm() and from exec()->exec_mm().
    
    In the exit_mm() case PF_EXITING and the futex state is updated. In the
    exec_mm() case these states are not touched.
    
    As the futex exit code needs further protections against exit races, this
    needs to be split into two functions.
    
    Preparatory only, no functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191106224556.240518241@linutronix.de

diff --git a/kernel/exit.c b/kernel/exit.c
index d11bdcaac2e1..cd893b530902 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -437,7 +437,7 @@ static void exit_mm(void)
 	struct mm_struct *mm = current->mm;
 	struct core_state *core_state;
 
-	mm_release(current, mm);
+	exit_mm_release(current, mm);
 	if (!mm)
 		return;
 	sync_mm_rss(mm);

commit 3d4775df0a89240f671861c6ab6e8d59af8e9e41
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Nov 6 22:55:37 2019 +0100

    futex: Replace PF_EXITPIDONE with a state
    
    The futex exit handling relies on PF_ flags. That's suboptimal as it
    requires a smp_mb() and an ugly lock/unlock of the exiting tasks pi_lock in
    the middle of do_exit() to enforce the observability of PF_EXITING in the
    futex code.
    
    Add a futex_state member to task_struct and convert the PF_EXITPIDONE logic
    over to the new state. The PF_EXITING dependency will be cleaned up in a
    later step.
    
    This prepares for handling various futex exit issues later.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20191106224556.149449274@linutronix.de

diff --git a/kernel/exit.c b/kernel/exit.c
index a46a50d67002..d11bdcaac2e1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -746,16 +746,7 @@ void __noreturn do_exit(long code)
 	 */
 	if (unlikely(tsk->flags & PF_EXITING)) {
 		pr_alert("Fixing recursive fault but reboot is needed!\n");
-		/*
-		 * We can do this unlocked here. The futex code uses
-		 * this flag just to verify whether the pi state
-		 * cleanup has been done or not. In the worst case it
-		 * loops once more. We pretend that the cleanup was
-		 * done as there is no way to return. Either the
-		 * OWNER_DIED bit is set by now or we push the blocked
-		 * task into the wait for ever nirwana as well.
-		 */
-		tsk->flags |= PF_EXITPIDONE;
+		futex_exit_done(tsk);
 		set_current_state(TASK_UNINTERRUPTIBLE);
 		schedule();
 	}
@@ -846,12 +837,7 @@ void __noreturn do_exit(long code)
 	 * Make sure we are holding no locks:
 	 */
 	debug_check_no_locks_held();
-	/*
-	 * We can do this unlocked here. The futex code uses this flag
-	 * just to verify whether the pi state cleanup has been done
-	 * or not. In the worst case it loops once more.
-	 */
-	tsk->flags |= PF_EXITPIDONE;
+	futex_exit_done(tsk);
 
 	if (tsk->io_context)
 		exit_io_context(tsk);

commit ce4dd4429b3c7e4506870796f3b8b06d707d2928
Author: David Howells <dhowells@redhat.com>
Date:   Wed Oct 16 15:13:41 2019 +0100

    Remove the nr_exclusive argument from __wake_up_sync_key()
    
    Remove the nr_exclusive argument from __wake_up_sync_key() and derived
    functions as everything seems to set it to 1.  Note also that if it wasn't
    set to 1, it would clear WF_SYNC anyway.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a46a50d67002..a1ff25ef050e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1435,7 +1435,7 @@ static int child_wait_callback(wait_queue_entry_t *wait, unsigned mode,
 void __wake_up_parent(struct task_struct *p, struct task_struct *parent)
 {
 	__wake_up_sync_key(&parent->signal->wait_chldexit,
-				TASK_INTERRUPTIBLE, 1, p);
+			   TASK_INTERRUPTIBLE, p);
 }
 
 static long do_wait(struct wait_opts *wo)

commit 1722c14a2097634a7ba37000c0ec7d9409918b64
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Thu Oct 17 12:18:31 2019 +0200

    exit: use pid_has_task() in do_wait()
    
    Replace hlist_empty() with the new pid_has_task() helper which is more
    idiomatic, easier to grep for, and unifies how callers perform this check.
    
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Link: https://lore.kernel.org/r/20191017101832.5985-4-christian.brauner@ubuntu.com

diff --git a/kernel/exit.c b/kernel/exit.c
index a46a50d67002..f2d20ab74422 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1457,7 +1457,7 @@ static long do_wait(struct wait_opts *wo)
 	 */
 	wo->notask_error = -ECHILD;
 	if ((wo->wo_type < PIDTYPE_MAX) &&
-	   (!wo->wo_pid || hlist_empty(&wo->wo_pid->tasks[wo->wo_type])))
+	   (!wo->wo_pid || !pid_has_task(wo->wo_pid, wo->wo_type)))
 		goto notask;
 
 	set_current_state(TASK_INTERRUPTIBLE);

commit 154abafc68bfb7c2ef2ad5308a3b2de8968c3f61
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 14 07:34:30 2019 -0500

    tasks, sched/core: With a grace period after finish_task_switch(), remove unnecessary code
    
    Remove work arounds that were written before there was a grace period
    after tasks left the runqueue in finish_task_switch().
    
    In particular now that there tasks exiting the runqueue exprience
    a RCU grace period none of the work performed by task_rcu_dereference()
    excpet the rcu_dereference() is necessary so replace task_rcu_dereference()
    with rcu_dereference().
    
    Remove the code in rcuwait_wait_event() that checks to ensure the current
    task has not exited.  It is no longer necessary as it is guaranteed
    that any running task will experience a RCU grace period after it
    leaves the run queueue.
    
    Remove the comment in rcuwait_wake_up() as it is no longer relevant.
    
    Ref: 8f95c90ceb54 ("sched/wait, RCU: Introduce rcuwait machinery")
    Ref: 150593bf8693 ("sched/api: Introduce task_rcu_dereference() and try_get_task_struct()")
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King - ARM Linux admin <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/87lfurdpk9.fsf_-_@x220.int.ebiederm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3bcaec2ea3ba..a46a50d67002 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -234,69 +234,6 @@ void release_task(struct task_struct *p)
 		goto repeat;
 }
 
-/*
- * Note that if this function returns a valid task_struct pointer (!NULL)
- * task->usage must remain >0 for the duration of the RCU critical section.
- */
-struct task_struct *task_rcu_dereference(struct task_struct **ptask)
-{
-	struct sighand_struct *sighand;
-	struct task_struct *task;
-
-	/*
-	 * We need to verify that release_task() was not called and thus
-	 * delayed_put_task_struct() can't run and drop the last reference
-	 * before rcu_read_unlock(). We check task->sighand != NULL,
-	 * but we can read the already freed and reused memory.
-	 */
-retry:
-	task = rcu_dereference(*ptask);
-	if (!task)
-		return NULL;
-
-	probe_kernel_address(&task->sighand, sighand);
-
-	/*
-	 * Pairs with atomic_dec_and_test() in put_task_struct(). If this task
-	 * was already freed we can not miss the preceding update of this
-	 * pointer.
-	 */
-	smp_rmb();
-	if (unlikely(task != READ_ONCE(*ptask)))
-		goto retry;
-
-	/*
-	 * We've re-checked that "task == *ptask", now we have two different
-	 * cases:
-	 *
-	 * 1. This is actually the same task/task_struct. In this case
-	 *    sighand != NULL tells us it is still alive.
-	 *
-	 * 2. This is another task which got the same memory for task_struct.
-	 *    We can't know this of course, and we can not trust
-	 *    sighand != NULL.
-	 *
-	 *    In this case we actually return a random value, but this is
-	 *    correct.
-	 *
-	 *    If we return NULL - we can pretend that we actually noticed that
-	 *    *ptask was updated when the previous task has exited. Or pretend
-	 *    that probe_slab_address(&sighand) reads NULL.
-	 *
-	 *    If we return the new task (because sighand is not NULL for any
-	 *    reason) - this is fine too. This (new) task can't go away before
-	 *    another gp pass.
-	 *
-	 *    And note: We could even eliminate the false positive if re-read
-	 *    task->sighand once again to avoid the falsely NULL. But this case
-	 *    is very unlikely so we don't care.
-	 */
-	if (!sighand)
-		return NULL;
-
-	return task;
-}
-
 void rcuwait_wake_up(struct rcuwait *w)
 {
 	struct task_struct *task;
@@ -316,10 +253,6 @@ void rcuwait_wake_up(struct rcuwait *w)
 	 */
 	smp_mb(); /* (B) */
 
-	/*
-	 * Avoid using task_rcu_dereference() magic as long as we are careful,
-	 * see comment in rcuwait_wait_event() regarding ->exit_state.
-	 */
 	task = rcu_dereference(w->task);
 	if (task)
 		wake_up_process(task);

commit 3fbd7ee285b2bbc6eebd15a3c8786d9776a402a8
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 14 07:33:34 2019 -0500

    tasks: Add a count of task RCU users
    
    Add a count of the number of RCU users (currently 1) of the task
    struct so that we can later add the scheduler case and get rid of the
    very subtle task_rcu_dereference(), and just use rcu_dereference().
    
    As suggested by Oleg have the count overlap rcu_head so that no
    additional space in task_struct is required.
    
    Inspired-by: Linus Torvalds <torvalds@linux-foundation.org>
    Inspired-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King - ARM Linux admin <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/87woebdplt.fsf_-_@x220.int.ebiederm.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 22ab6a4bdc51..3bcaec2ea3ba 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -182,6 +182,11 @@ static void delayed_put_task_struct(struct rcu_head *rhp)
 	put_task_struct(tsk);
 }
 
+void put_task_struct_rcu_user(struct task_struct *task)
+{
+	if (refcount_dec_and_test(&task->rcu_users))
+		call_rcu(&task->rcu, delayed_put_task_struct);
+}
 
 void release_task(struct task_struct *p)
 {
@@ -222,7 +227,7 @@ void release_task(struct task_struct *p)
 
 	write_unlock_irq(&tasklist_lock);
 	release_thread(p);
-	call_rcu(&p->rcu, delayed_put_task_struct);
+	put_task_struct_rcu_user(p);
 
 	p = leader;
 	if (unlikely(zap_leader))

commit c17112a5c413f20188da276c138484e7127cdc84
Merge: 4d856f72c10e 821cc7b0b205
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 09:28:19 2019 -0700

    Merge tag 'core-process-v5.4' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux
    
    Pull pidfd/waitid updates from Christian Brauner:
     "This contains two features and various tests.
    
      First, it adds support for waiting on process through pidfds by adding
      the P_PIDFD type to the waitid() syscall. This completes the basic
      functionality of the pidfd api (cf. [1]). In the meantime we also have
      a new adition to the userspace projects that make use of the pidfd
      api. The qt project was nice enough to send a mail pointing out that
      they have a pr up to switch to the pidfd api (cf. [2]).
    
      Second, this tag contains an extension to the waitid() syscall to make
      it possible to wait on the current process group in a race free manner
      (even though the actual problem is very unlikely) by specifing 0
      together with the P_PGID type. This extension traces back to a
      discussion on the glibc development mailing list.
    
      There are also a range of tests for the features above. Additionally,
      the test-suite which detected the pidfd-polling race we fixed in [3]
      is included in this tag"
    
    [1] https://lwn.net/Articles/794707/
    [2] https://codereview.qt-project.org/c/qt/qtbase/+/108456
    [3] commit b191d6491be6 ("pidfd: fix a poll race when setting exit_state")
    
    * tag 'core-process-v5.4' of git://git.kernel.org/pub/scm/linux/kernel/git/brauner/linux:
      waitid: Add support for waiting for the current process group
      tests: add pidfd poll tests
      tests: move common definitions and functions into pidfd.h
      pidfd: add pidfd_wait tests
      pidfd: add P_PIDFD to waitid()

commit 821cc7b0b205c0df64cce59aacc330af251fa8f7
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Jul 23 07:44:46 2019 -0500

    waitid: Add support for waiting for the current process group
    
    It was recently discovered that the linux version of waitid is not a
    superset of the other wait functions because it does not include support
    for waiting for the current process group. This has two downsides:
    1. An extra system call is needed to get the current process group.
    2. After the current process group is received and before it is passed
       to waitid a signal could arrive causing the current process group to change.
       Inherent race-conditions as these make it impossible for userspace to
       emulate this functionaly and thus violate async-signal safety
       requirements for waitpid.
    
    Arguments can be made for using a different choice of idtype and id
    for this case but the BSDs already use this P_PGID and 0 to indicate
    waiting for the current process's process group.  So be nice to user
    space programmers and don't introduce an unnecessary incompatibility.
    
    Some people have noted that the posix description is that
    waitpid will wait for the current process group, and that in
    the presence of pthreads that process group can change.  To get
    clarity on this issue I looked at XNU, FreeBSD, and Luminos.  All of
    those flavors of unix waited for the current process group at the
    time of call and as written could not adapt to the process group
    changing after the call.
    
    At one point Linux did adapt to the current process group changing but
    that stopped in 161550d74c07 ("pid: sys_wait... fixes").  It has been
    over 11 years since Linux has that behavior, no programs that fail
    with the change in behavior have been reported, and I could not
    find any other unix that does this.  So I think it is safe to clarify
    the definition of current process group, to current process group
    at the time of the wait function.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Alistair Francis <alistair23@gmail.com>
    Cc: Zong Li <zongbox@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: Adhemerval Zanella <adhemerval.zanella@linaro.org>
    Cc: GNU C Library <libc-alpha@sourceware.org>
    Link: https://lore.kernel.org/r/20190814154400.6371-2-christian.brauner@ubuntu.com

diff --git a/kernel/exit.c b/kernel/exit.c
index 64bb6893a37d..d2d74a7b81d1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1596,10 +1596,13 @@ static long kernel_waitid(int which, pid_t upid, struct waitid_info *infop,
 		break;
 	case P_PGID:
 		type = PIDTYPE_PGID;
-		if (upid <= 0)
+		if (upid < 0)
 			return -EINVAL;
 
-		pid = find_get_pid(upid);
+		if (upid)
+			pid = find_get_pid(upid);
+		else
+			pid = get_task_pid(current, PIDTYPE_PGID);
 		break;
 	case P_PIDFD:
 		type = PIDTYPE_PID;

commit 3695eae5fee0605f316fbaad0b9e3de791d7dfaf
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Sun Jul 28 00:22:29 2019 +0200

    pidfd: add P_PIDFD to waitid()
    
    This adds the P_PIDFD type to waitid().
    One of the last remaining bits for the pidfd api is to make it possible
    to wait on pidfds. With P_PIDFD added to waitid() the parts of userspace
    that want to use the pidfd api to exclusively manage processes can do so
    now.
    
    One of the things this will unblock in the future is the ability to make
    it possible to retrieve the exit status via waitid(P_PIDFD) for
    non-parent processes if handed a _suitable_ pidfd that has this feature
    set. This is similar to what you can do on FreeBSD with kqueue(). It
    might even end up being possible to wait on a process as a non-parent if
    an appropriate property is enabled on the pidfd.
    
    With P_PIDFD no scoping of the process identified by the pidfd is
    possible, i.e. it explicitly blocks things such as wait4(-1), wait4(0),
    waitid(P_ALL), waitid(P_PGID) etc. It only allows for semantics
    equivalent to wait4(pid), waitid(P_PID). Users that need scoping should
    rely on pid-based wait*() syscalls for now.
    
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Joel Fernandes (Google) <joel@joelfernandes.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Andy Lutomirsky <luto@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aleksa Sarai <cyphar@cyphar.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Link: https://lore.kernel.org/r/20190727222229.6516-2-christian@brauner.io

diff --git a/kernel/exit.c b/kernel/exit.c
index a75b6a7f458a..64bb6893a37d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1552,6 +1552,23 @@ static long do_wait(struct wait_opts *wo)
 	return retval;
 }
 
+static struct pid *pidfd_get_pid(unsigned int fd)
+{
+	struct fd f;
+	struct pid *pid;
+
+	f = fdget(fd);
+	if (!f.file)
+		return ERR_PTR(-EBADF);
+
+	pid = pidfd_pid(f.file);
+	if (!IS_ERR(pid))
+		get_pid(pid);
+
+	fdput(f);
+	return pid;
+}
+
 static long kernel_waitid(int which, pid_t upid, struct waitid_info *infop,
 			  int options, struct rusage *ru)
 {
@@ -1574,19 +1591,29 @@ static long kernel_waitid(int which, pid_t upid, struct waitid_info *infop,
 		type = PIDTYPE_PID;
 		if (upid <= 0)
 			return -EINVAL;
+
+		pid = find_get_pid(upid);
 		break;
 	case P_PGID:
 		type = PIDTYPE_PGID;
 		if (upid <= 0)
 			return -EINVAL;
+
+		pid = find_get_pid(upid);
+		break;
+	case P_PIDFD:
+		type = PIDTYPE_PID;
+		if (upid < 0)
+			return -EINVAL;
+
+		pid = pidfd_get_pid(upid);
+		if (IS_ERR(pid))
+			return PTR_ERR(pid);
 		break;
 	default:
 		return -EINVAL;
 	}
 
-	if (type < PIDTYPE_MAX)
-		pid = find_get_pid(upid);
-
 	wo.wo_type	= type;
 	wo.wo_pid	= pid;
 	wo.wo_flags	= options;

commit 30b692d3b390c6fe78a5064be0c4bbd44a41be59
Author: Christian Brauner <christian@brauner.io>
Date:   Mon Jul 29 17:48:24 2019 +0200

    exit: make setting exit_state consistent
    
    Since commit b191d6491be6 ("pidfd: fix a poll race when setting exit_state")
    we unconditionally set exit_state to EXIT_ZOMBIE before calling into
    do_notify_parent(). This was done to eliminate a race when querying
    exit_state in do_notify_pidfd().
    Back then we decided to do the absolute minimal thing to fix this and
    not touch the rest of the exit_notify() function where exit_state is
    set.
    Since this fix has not caused any issues change the setting of
    exit_state to EXIT_DEAD in the autoreap case to account for the fact hat
    exit_state is set to EXIT_ZOMBIE unconditionally. This fix was planned
    but also explicitly requested in [1] and makes the whole code more
    consistent.
    
    /* References */
    [1]: https://lore.kernel.org/lkml/CAHk-=wigcxGFR2szue4wavJtH5cYTTeNES=toUBVGsmX0rzX+g@mail.gmail.com
    
    Signed-off-by: Christian Brauner <christian@brauner.io>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4436158a6d30..5b4a5dcce8f8 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -734,9 +734,10 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 		autoreap = true;
 	}
 
-	tsk->exit_state = autoreap ? EXIT_DEAD : EXIT_ZOMBIE;
-	if (tsk->exit_state == EXIT_DEAD)
+	if (autoreap) {
+		tsk->exit_state = EXIT_DEAD;
 		list_add(&tsk->ptrace_entry, &dead);
+	}
 
 	/* mt-exec, de_thread() is waiting for group leader */
 	if (unlikely(tsk->signal->notify_count < 0))

commit b191d6491be67cef2b3fa83015561caca1394ab9
Author: Suren Baghdasaryan <surenb@google.com>
Date:   Wed Jul 17 13:21:00 2019 -0400

    pidfd: fix a poll race when setting exit_state
    
    There is a race between reading task->exit_state in pidfd_poll and
    writing it after do_notify_parent calls do_notify_pidfd. Expected
    sequence of events is:
    
    CPU 0                            CPU 1
    ------------------------------------------------
    exit_notify
      do_notify_parent
        do_notify_pidfd
      tsk->exit_state = EXIT_DEAD
                                      pidfd_poll
                                         if (tsk->exit_state)
    
    However nothing prevents the following sequence:
    
    CPU 0                            CPU 1
    ------------------------------------------------
    exit_notify
      do_notify_parent
        do_notify_pidfd
                                       pidfd_poll
                                          if (tsk->exit_state)
      tsk->exit_state = EXIT_DEAD
    
    This causes a polling task to wait forever, since poll blocks because
    exit_state is 0 and the waiting task is not notified again. A stress
    test continuously doing pidfd poll and process exits uncovered this bug.
    
    To fix it, we make sure that the task's exit_state is always set before
    calling do_notify_pidfd.
    
    Fixes: b53b0b9d9a6 ("pidfd: add polling support")
    Cc: kernel-team@android.com
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Suren Baghdasaryan <surenb@google.com>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Link: https://lore.kernel.org/r/20190717172100.261204-1-joel@joelfernandes.org
    [christian@brauner.io: adapt commit message and drop unneeded changes from wait_task_zombie]
    Signed-off-by: Christian Brauner <christian@brauner.io>

diff --git a/kernel/exit.c b/kernel/exit.c
index a75b6a7f458a..4436158a6d30 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -720,6 +720,7 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	if (group_dead)
 		kill_orphaned_pgrp(tsk->group_leader, NULL);
 
+	tsk->exit_state = EXIT_ZOMBIE;
 	if (unlikely(tsk->ptrace)) {
 		int sig = thread_group_leader(tsk) &&
 				thread_group_empty(tsk) &&

commit 6b115bf58e6f013ca75e7115aabcbd56c20ff31d
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 31 10:38:57 2019 -0700

    cgroup: Call cgroup_release() before __exit_signal()
    
    cgroup_release() calls cgroup_subsys->release() which is used by the
    pids controller to uncharge its pid.  We want to use it to manage
    iteration of dying tasks which requires putting it before
    __unhash_process().  Move cgroup_release() above __exit_signal().
    While this makes it uncharge before the pid is freed, pid is RCU freed
    anyway and the window is very narrow.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1803efb2922f..a75b6a7f458a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -195,6 +195,7 @@ void release_task(struct task_struct *p)
 	rcu_read_unlock();
 
 	proc_flush_task(p);
+	cgroup_release(p);
 
 	write_lock_irq(&tasklist_lock);
 	ptrace_release_task(p);
@@ -220,7 +221,6 @@ void release_task(struct task_struct *p)
 	}
 
 	write_unlock_irq(&tasklist_lock);
-	cgroup_release(p);
 	release_thread(p);
 	call_rcu(&p->rcu, delayed_put_task_struct);
 

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8361a560cd1d..1803efb2922f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/kernel/exit.c
  *

commit 987717e5e016a0dd3011d3bb16546672713f94e2
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Tue May 14 15:40:50 2019 -0700

    mm: change mm_update_next_owner() to update mm->owner with WRITE_ONCE
    
    The RCU reader uses rcu_dereference() inside rcu_read_lock critical
    sections, so the writer shall use WRITE_ONCE.  Just a cleanup, we still
    rely on gcc to emit atomic writes in other places.
    
    Link: http://lkml.kernel.org/r/20190325225636.11635-3-aarcange@redhat.com
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: "Kirill A . Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Xu <peterx@redhat.com>
    Cc: zhong jiang <zhongjiang@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2166c2d92ddc..8361a560cd1d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -422,7 +422,7 @@ void mm_update_next_owner(struct mm_struct *mm)
 	 * freed task structure.
 	 */
 	if (atomic_read(&mm->mm_users) <= 1) {
-		mm->owner = NULL;
+		WRITE_ONCE(mm->owner, NULL);
 		return;
 	}
 
@@ -462,7 +462,7 @@ void mm_update_next_owner(struct mm_struct *mm)
 	 * most likely racing with swapoff (try_to_unuse()) or /proc or
 	 * ptrace or page migration (get_task_mm()).  Mark owner as NULL.
 	 */
-	mm->owner = NULL;
+	WRITE_ONCE(mm->owner, NULL);
 	return;
 
 assign_new_owner:
@@ -483,7 +483,7 @@ void mm_update_next_owner(struct mm_struct *mm)
 		put_task_struct(c);
 		goto retry;
 	}
-	mm->owner = c;
+	WRITE_ONCE(mm->owner, c);
 	task_unlock(c);
 	put_task_struct(c);
 }

commit 1fc1cd8399ab5541a488a7e47b2f21537dd76c2d
Merge: abf7c3d8ddea 6a613d24effc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 7 10:11:41 2019 -0800

    Merge branch 'for-5.1' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - Oleg's pids controller accounting update which gets rid of rcu delay
       in pids accounting updates
    
     - rstat (cgroup hierarchical stat collection mechanism) optimization
    
     - Doc updates
    
    * 'for-5.1' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cpuset: remove unused task_has_mempolicy()
      cgroup, rstat: Don't flush subtree root unless necessary
      cgroup: add documentation for pids.events file
      Documentation: cgroup-v2: eliminate markup warnings
      MAINTAINERS: Update cgroup entry
      cgroup/pids: turn cgroup_subsys->free() into cgroup_subsys->release() to fix the accounting

commit 8fb335e078378c8426fabeed1ebee1fbf915690c
Author: Andrei Vagin <avagin@gmail.com>
Date:   Fri Feb 1 14:20:24 2019 -0800

    kernel/exit.c: release ptraced tasks before zap_pid_ns_processes
    
    Currently, exit_ptrace() adds all ptraced tasks in a dead list, then
    zap_pid_ns_processes() waits on all tasks in a current pidns, and only
    then are tasks from the dead list released.
    
    zap_pid_ns_processes() can get stuck on waiting tasks from the dead
    list.  In this case, we will have one unkillable process with one or
    more dead children.
    
    Thanks to Oleg for the advice to release tasks in find_child_reaper().
    
    Link: http://lkml.kernel.org/r/20190110175200.12442-1-avagin@gmail.com
    Fixes: 7c8bd2322c7f ("exit: ptrace: shift "reap dead" code from exit_ptrace() to forget_original_parent()")
    Signed-off-by: Andrei Vagin <avagin@gmail.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3fb7be001964..2639a30a8aa5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -558,12 +558,14 @@ static struct task_struct *find_alive_thread(struct task_struct *p)
 	return NULL;
 }
 
-static struct task_struct *find_child_reaper(struct task_struct *father)
+static struct task_struct *find_child_reaper(struct task_struct *father,
+						struct list_head *dead)
 	__releases(&tasklist_lock)
 	__acquires(&tasklist_lock)
 {
 	struct pid_namespace *pid_ns = task_active_pid_ns(father);
 	struct task_struct *reaper = pid_ns->child_reaper;
+	struct task_struct *p, *n;
 
 	if (likely(reaper != father))
 		return reaper;
@@ -579,6 +581,12 @@ static struct task_struct *find_child_reaper(struct task_struct *father)
 		panic("Attempted to kill init! exitcode=0x%08x\n",
 			father->signal->group_exit_code ?: father->exit_code);
 	}
+
+	list_for_each_entry_safe(p, n, dead, ptrace_entry) {
+		list_del_init(&p->ptrace_entry);
+		release_task(p);
+	}
+
 	zap_pid_ns_processes(pid_ns);
 	write_lock_irq(&tasklist_lock);
 
@@ -668,7 +676,7 @@ static void forget_original_parent(struct task_struct *father,
 		exit_ptrace(father, dead);
 
 	/* Can drop and reacquire tasklist_lock */
-	reaper = find_child_reaper(father);
+	reaper = find_child_reaper(father, dead);
 	if (list_empty(&father->children))
 		return;
 

commit 51bee5abeab2058ea5813c5615d6197a23dbf041
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jan 28 17:00:13 2019 +0100

    cgroup/pids: turn cgroup_subsys->free() into cgroup_subsys->release() to fix the accounting
    
    The only user of cgroup_subsys->free() callback is pids_cgrp_subsys which
    needs pids_free() to uncharge the pid.
    
    However, ->free() is called from __put_task_struct()->cgroup_free() and this
    is too late. Even the trivial program which does
    
            for (;;) {
                    int pid = fork();
                    assert(pid >= 0);
                    if (pid)
                            wait(NULL);
                    else
                            exit(0);
            }
    
    can run out of limits because release_task()->call_rcu(delayed_put_task_struct)
    implies an RCU gp after the task/pid goes away and before the final put().
    
    Test-case:
    
            mkdir -p /tmp/CG
            mount -t cgroup2 none /tmp/CG
            echo '+pids' > /tmp/CG/cgroup.subtree_control
    
            mkdir /tmp/CG/PID
            echo 2 > /tmp/CG/PID/pids.max
    
            perl -e 'while ($p = fork) { wait; } $p // die "fork failed: $!\n"' &
            echo $! > /tmp/CG/PID/cgroup.procs
    
    Without this patch the forking process fails soon after migration.
    
    Rename cgroup_subsys->free() to cgroup_subsys->release() and move the callsite
    into the new helper, cgroup_release(), called by release_task() which actually
    frees the pid(s).
    
    Reported-by: Herton R. Krzesinski <hkrzesin@redhat.com>
    Reported-by: Jan Stancek <jstancek@redhat.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3fb7be001964..c2b8443f30b4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -219,6 +219,7 @@ void release_task(struct task_struct *p)
 	}
 
 	write_unlock_irq(&tasklist_lock);
+	cgroup_release(p);
 	release_thread(p);
 	call_rcu(&p->rcu, delayed_put_task_struct);
 

commit 6dc080eeb2ba01973bfff0d79844d7a59e12542e
Author: Prateek Sood <prsood@codeaurora.org>
Date:   Fri Nov 30 20:40:56 2018 +0530

    sched/wait: Fix rcuwait_wake_up() ordering
    
    For some peculiar reason rcuwait_wake_up() has the right barrier in
    the comment, but not in the code.
    
    This mistake has been observed to cause a deadlock in the following
    situation:
    
        P1                                  P2
    
        percpu_up_read()                    percpu_down_write()
          rcu_sync_is_idle() // false
                                              rcu_sync_enter()
                                              ...
          __percpu_up_read()
    
    [S] ,-  __this_cpu_dec(*sem->read_count)
        |   smp_rmb();
    [L] |   task = rcu_dereference(w->task) // NULL
        |
        |                               [S]     w->task = current
        |                                       smp_mb();
        |                               [L]     readers_active_check() // fail
        `-> <store happens here>
    
    Where the smp_rmb() (obviously) fails to constrain the store.
    
    [ peterz: Added changelog. ]
    
    Signed-off-by: Prateek Sood <prsood@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Acked-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 8f95c90ceb54 ("sched/wait, RCU: Introduce rcuwait machinery")
    Link: https://lkml.kernel.org/r/1543590656-7157-1-git-send-email-prsood@codeaurora.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 284f2fe9a293..3fb7be001964 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -307,7 +307,7 @@ void rcuwait_wake_up(struct rcuwait *w)
 	 *        MB (A)	      MB (B)
 	 *    [L] cond		  [L] tsk
 	 */
-	smp_rmb(); /* (B) */
+	smp_mb(); /* (B) */
 
 	/*
 	 * Avoid using task_rcu_dereference() magic as long as we are careful,

commit e8746440bf68212f19688f1454dad593c74abee1
Merge: fe76fc6aaf53 2f960bd05640
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 16 05:13:36 2019 +1200

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Pull networking fixes from David Miller:
    
     1) Fix regression in multi-SKB responses to RTM_GETADDR, from Arthur
        Gautier.
    
     2) Fix ipv6 frag parsing in openvswitch, from Yi-Hung Wei.
    
     3) Unbounded recursion in ipv4 and ipv6 GUE tunnels, from Stefano
        Brivio.
    
     4) Use after free in hns driver, from Yonglong Liu.
    
     5) icmp6_send() needs to handle the case of NULL skb, from Eric
        Dumazet.
    
     6) Missing rcu read lock in __inet6_bind() when operating on mapped
        addresses, from David Ahern.
    
     7) Memory leak in tipc-nl_compat_publ_dump(), from Gustavo A. R. Silva.
    
     8) Fix PHY vs r8169 module loading ordering issues, from Heiner
        Kallweit.
    
     9) Fix bridge vlan memory leak, from Ido Schimmel.
    
    10) Dev refcount leak in AF_PACKET, from Jason Gunthorpe.
    
    11) Infoleak in ipv6_local_error(), flow label isn't completely
        initialized. From Eric Dumazet.
    
    12) Handle mv88e6390 errata, from Andrew Lunn.
    
    13) Making vhost/vsock CID hashing consistent, from Zha Bin.
    
    14) Fix lack of UMH cleanup when it unexpectedly exits, from Taehee Yoo.
    
    15) Bridge forwarding must clear skb->tstamp, from Paolo Abeni.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net: (87 commits)
      bnxt_en: Fix context memory allocation.
      bnxt_en: Fix ring checking logic on 57500 chips.
      mISDN: hfcsusb: Use struct_size() in kzalloc()
      net: clear skb->tstamp in bridge forwarding path
      net: bpfilter: disallow to remove bpfilter module while being used
      net: bpfilter: restart bpfilter_umh when error occurred
      net: bpfilter: use cleanup callback to release umh_info
      umh: add exit routine for UMH process
      isdn: i4l: isdn_tty: Fix some concurrency double-free bugs
      vhost/vsock: fix vhost vsock cid hashing inconsistent
      net: stmmac: Prevent RX starvation in stmmac_napi_poll()
      net: stmmac: Fix the logic of checking if RX Watchdog must be enabled
      net: stmmac: Check if CBS is supported before configuring
      net: stmmac: dwxgmac2: Only clear interrupts that are active
      net: stmmac: Fix PCI module removal leak
      tools/bpf: fix bpftool map dump with bitfields
      tools/bpf: test btf bitfield with >=256 struct member offset
      bpf: fix bpffs bitfield pretty print
      net: ethernet: mediatek: fix warning in phy_start_aneg
      tcp: change txhash on SYN-data timeout
      ...

commit 73ab1cb2de9e3efe7f818d5453de271e5371df1d
Author: Taehee Yoo <ap420073@gmail.com>
Date:   Wed Jan 9 02:23:56 2019 +0900

    umh: add exit routine for UMH process
    
    A UMH process which is created by the fork_usermode_blob() such as
    bpfilter needs to release members of the umh_info when process is
    terminated.
    But the do_exit() does not release members of the umh_info. hence module
    which uses UMH needs own code to detect whether UMH process is
    terminated or not.
    But this implementation needs extra code for checking the status of
    UMH process. it eventually makes the code more complex.
    
    The new PF_UMH flag is added and it is used to identify UMH processes.
    The exit_umh() does not release members of the umh_info.
    Hence umh_info->cleanup callback should release both members of the
    umh_info and the private data.
    
    Suggested-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Taehee Yoo <ap420073@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8a01b671dc1f..dad70419195c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -866,6 +866,7 @@ void __noreturn do_exit(long code)
 	exit_task_namespaces(tsk);
 	exit_task_work(tsk);
 	exit_thread(tsk);
+	exit_umh(tsk);
 
 	/*
 	 * Flush inherited counters to the parent - before the parent

commit 594cc251fdd0d231d342d88b2fdff4bc42fb0690
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 4 12:56:09 2019 -0800

    make 'user_access_begin()' do 'access_ok()'
    
    Originally, the rule used to be that you'd have to do access_ok()
    separately, and then user_access_begin() before actually doing the
    direct (optimized) user access.
    
    But experience has shown that people then decide not to do access_ok()
    at all, and instead rely on it being implied by other operations or
    similar.  Which makes it very hard to verify that the access has
    actually been range-checked.
    
    If you use the unsafe direct user accesses, hardware features (either
    SMAP - Supervisor Mode Access Protection - on x86, or PAN - Privileged
    Access Never - on ARM) do force you to use user_access_begin().  But
    nothing really forces the range check.
    
    By putting the range check into user_access_begin(), we actually force
    people to do the right thing (tm), and the range check vill be visible
    near the actual accesses.  We have way too long a history of people
    trying to avoid them.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8a01b671dc1f..2d14979577ee 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1604,10 +1604,9 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	if (!infop)
 		return err;
 
-	if (!access_ok(infop, sizeof(*infop)))
+	if (!user_access_begin(infop, sizeof(*infop)))
 		return -EFAULT;
 
-	user_access_begin();
 	unsafe_put_user(signo, &infop->si_signo, Efault);
 	unsafe_put_user(0, &infop->si_errno, Efault);
 	unsafe_put_user(info.cause, &infop->si_code, Efault);
@@ -1732,10 +1731,9 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 	if (!infop)
 		return err;
 
-	if (!access_ok(infop, sizeof(*infop)))
+	if (!user_access_begin(infop, sizeof(*infop)))
 		return -EFAULT;
 
-	user_access_begin();
 	unsafe_put_user(signo, &infop->si_signo, Efault);
 	unsafe_put_user(0, &infop->si_errno, Efault);
 	unsafe_put_user(info.cause, &infop->si_code, Efault);

commit 96d4f267e40f9509e8a66e2b39e8b95655617693
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 3 18:57:57 2019 -0800

    Remove 'type' argument from access_ok() function
    
    Nobody has actually used the type (VERIFY_READ vs VERIFY_WRITE) argument
    of the user address range verification function since we got rid of the
    old racy i386-only code to walk page tables by hand.
    
    It existed because the original 80386 would not honor the write protect
    bit when in kernel mode, so you had to do COW by hand before doing any
    user access.  But we haven't supported that in a long time, and these
    days the 'type' argument is a purely historical artifact.
    
    A discussion about extending 'user_access_begin()' to do the range
    checking resulted this patch, because there is no way we're going to
    move the old VERIFY_xyz interface to that model.  And it's best done at
    the end of the merge window when I've done most of my merges, so let's
    just get this done once and for all.
    
    This patch was mostly done with a sed-script, with manual fix-ups for
    the cases that weren't of the trivial 'access_ok(VERIFY_xyz' form.
    
    There were a couple of notable cases:
    
     - csky still had the old "verify_area()" name as an alias.
    
     - the iter_iov code had magical hardcoded knowledge of the actual
       values of VERIFY_{READ,WRITE} (not that they mattered, since nothing
       really used it)
    
     - microblaze used the type argument for a debug printout
    
    but other than those oddities this should be a total no-op patch.
    
    I tried to fix up all architectures, did fairly extensive grepping for
    access_ok() uses, and the changes are trivial, but I may have missed
    something.  Any missed conversion should be trivially fixable, though.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 0e21e6d21f35..8a01b671dc1f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1604,7 +1604,7 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	if (!infop)
 		return err;
 
-	if (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))
+	if (!access_ok(infop, sizeof(*infop)))
 		return -EFAULT;
 
 	user_access_begin();
@@ -1732,7 +1732,7 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 	if (!infop)
 		return err;
 
-	if (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))
+	if (!access_ok(infop, sizeof(*infop)))
 		return -EFAULT;
 
 	user_access_begin();

commit 0102498083d58d8b17759642c602b525215e1a54
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Jul 13 18:40:57 2018 -0500

    signal: Pass pid type into group_send_sig_info
    
    This passes the information we already have at the call sight
    into group_send_sig_info.  Ultimatelly allowing for to better handle
    signals sent to a group of processes.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 25582b442955..0e21e6d21f35 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -681,7 +681,8 @@ static void forget_original_parent(struct task_struct *father,
 				t->parent = t->real_parent;
 			if (t->pdeath_signal)
 				group_send_sig_info(t->pdeath_signal,
-						    SEND_SIG_NOINFO, t);
+						    SEND_SIG_NOINFO, t,
+						    PIDTYPE_TGID);
 		}
 		/*
 		 * If this is a threaded reparent there is no need to

commit 6883f81aac6f44e7df70a6af189b3689ff52cbfb
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sun Jun 4 04:32:13 2017 -0500

    pid: Implement PIDTYPE_TGID
    
    Everywhere except in the pid array we distinguish between a tasks pid and
    a tasks tgid (thread group id).  Even in the enumeration we want that
    distinction sometimes so we have added __PIDTYPE_TGID.  With leader_pid
    we almost have an implementation of PIDTYPE_TGID in struct signal_struct.
    
    Add PIDTYPE_TGID as a first class member of the pid_type enumeration and
    into the pids array.  Then remove the __PIDTYPE_TGID special case and the
    leader_pid in signal_struct.
    
    The net size increase is just an extra pointer added to struct pid and
    an extra pair of pointers of an hlist_node added to task_struct.
    
    The effect on code maintenance is the removal of a number of special
    cases today and the potential to remove many more special cases as
    PIDTYPE_TGID gets used to it's fullest.  The long term potential
    is allowing zombie thread group leaders to exit, which will remove
    a lot more special cases in the code.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 16432428fc6c..25582b442955 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -73,6 +73,7 @@ static void __unhash_process(struct task_struct *p, bool group_dead)
 	nr_threads--;
 	detach_pid(p, PIDTYPE_PID);
 	if (group_dead) {
+		detach_pid(p, PIDTYPE_TGID);
 		detach_pid(p, PIDTYPE_PGID);
 		detach_pid(p, PIDTYPE_SID);
 

commit 1fb53567a3633740aac8761eb7023dc5671f0edb
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri May 5 13:45:14 2017 -0500

    pids: Move task_pid_type into sched/signal.h
    
    The function is general and inline so there is no need
    to hide it inside of exit.c
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index c3c7ac560114..16432428fc6c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1001,14 +1001,6 @@ struct wait_opts {
 	int			notask_error;
 };
 
-static inline
-struct pid *task_pid_type(struct task_struct *task, enum pid_type type)
-{
-	if (type != PIDTYPE_PID)
-		task = task->group_leader;
-	return task->pids[type].pid;
-}
-
 static int eligible_pid(struct wait_opts *wo, struct task_struct *p)
 {
 	return	wo->wo_type == PIDTYPE_MAX ||

commit d300b610812f3c10d146db4c18f98eba38834c70
Author: Dominik Brodowski <linux@dominikbrodowski.net>
Date:   Sun Mar 11 11:34:26 2018 +0100

    kernel: use kernel_wait4() instead of sys_wait4()
    
    All call sites of sys_wait4() set *rusage to NULL. Therefore, there is
    no need for the copy_to_user() handling of *rusage, and we can use
    kernel_wait4() directly.
    
    This patch is part of a series which removes in-kernel calls to syscalls.
    On this basis, the syscall entry path can be streamlined. For details, see
    http://lkml.kernel.org/r/20180325162527.GA17492@light.dominikbrodowski.net
    
    Acked-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dominik Brodowski <linux@dominikbrodowski.net>

diff --git a/kernel/exit.c b/kernel/exit.c
index 995453d9fb55..c3c7ac560114 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1691,7 +1691,7 @@ SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
  */
 SYSCALL_DEFINE3(waitpid, pid_t, pid, int __user *, stat_addr, int, options)
 {
-	return sys_wait4(pid, stat_addr, options, NULL);
+	return kernel_wait4(pid, stat_addr, options, NULL);
 }
 
 #endif

commit dc8635b78cd8669c37e230058d18c33af7451ab1
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu Jan 4 16:17:56 2018 -0800

    kernel/exit.c: export abort() to modules
    
    gcc -fisolate-erroneous-paths-dereference can generate calls to abort()
    from modular code too.
    
    [arnd@arndb.de: drop duplicate exports of abort()]
      Link: http://lkml.kernel.org/r/20180102103311.706364-1-arnd@arndb.de
    Reported-by: Vineet Gupta <Vineet.Gupta1@synopsys.com>
    Cc: Sudip Mukherjee <sudipm.mukherjee@gmail.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Alexey Brodkin <Alexey.Brodkin@synopsys.com>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Jose Abreu <Jose.Abreu@synopsys.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index df0c91d5606c..995453d9fb55 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1763,3 +1763,4 @@ __weak void abort(void)
 	/* if that doesn't kill us, halt */
 	panic("Oops failed to kill thread");
 }
+EXPORT_SYMBOL(abort);

commit 7c2c11b208be09c156573fc0076b7b3646e05219
Author: Sudip Mukherjee <sudipm.mukherjee@gmail.com>
Date:   Thu Dec 14 15:33:19 2017 -0800

    arch: define weak abort()
    
    gcc toggle -fisolate-erroneous-paths-dereference (default at -O2
    onwards) isolates faulty code paths such as null pointer access, divide
    by zero etc.  If gcc port doesnt implement __builtin_trap, an abort() is
    generated which causes kernel link error.
    
    In this case, gcc is generating abort due to 'divide by zero' in
    lib/mpi/mpih-div.c.
    
    Currently 'frv' and 'arc' are failing.  Previously other arch was also
    broken like m32r was fixed by commit d22e3d69ee1a ("m32r: fix build
    failure").
    
    Let's define this weak function which is common for all arch and fix the
    problem permanently.  We can even remove the arch specific 'abort' after
    this is done.
    
    Link: http://lkml.kernel.org/r/1513118956-8718-1-git-send-email-sudipm.mukherjee@gmail.com
    Signed-off-by: Sudip Mukherjee <sudipm.mukherjee@gmail.com>
    Cc: Alexey Brodkin <Alexey.Brodkin@synopsys.com>
    Cc: Vineet Gupta <Vineet.Gupta1@synopsys.com>
    Cc: Sudip Mukherjee <sudipm.mukherjee@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6b4298a41167..df0c91d5606c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1755,3 +1755,11 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 	return -EFAULT;
 }
 #endif
+
+__weak void abort(void)
+{
+	BUG();
+
+	/* if that doesn't kill us, halt */
+	panic("Oops failed to kill thread");
+}

commit 6aa7de059173a986114ac43b8f50b297a86f09a8
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Oct 23 14:07:29 2017 -0700

    locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE()
    
    Please do not apply this to mainline directly, instead please re-run the
    coccinelle script shown below and apply its output.
    
    For several reasons, it is desirable to use {READ,WRITE}_ONCE() in
    preference to ACCESS_ONCE(), and new code is expected to use one of the
    former. So far, there's been no reason to change most existing uses of
    ACCESS_ONCE(), as these aren't harmful, and changing them results in
    churn.
    
    However, for some features, the read/write distinction is critical to
    correct operation. To distinguish these cases, separate read/write
    accessors must be used. This patch migrates (most) remaining
    ACCESS_ONCE() instances to {READ,WRITE}_ONCE(), using the following
    coccinelle script:
    
    ----
    // Convert trivial ACCESS_ONCE() uses to equivalent READ_ONCE() and
    // WRITE_ONCE()
    
    // $ make coccicheck COCCI=/home/mark/once.cocci SPFLAGS="--include-headers" MODE=patch
    
    virtual patch
    
    @ depends on patch @
    expression E1, E2;
    @@
    
    - ACCESS_ONCE(E1) = E2
    + WRITE_ONCE(E1, E2)
    
    @ depends on patch @
    expression E;
    @@
    
    - ACCESS_ONCE(E)
    + READ_ONCE(E)
    ----
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: davem@davemloft.net
    Cc: linux-arch@vger.kernel.org
    Cc: mpe@ellerman.id.au
    Cc: shuah@kernel.org
    Cc: snitzer@redhat.com
    Cc: thor.thayer@linux.intel.com
    Cc: tj@kernel.org
    Cc: viro@zeniv.linux.org.uk
    Cc: will.deacon@arm.com
    Link: http://lkml.kernel.org/r/1508792849-3115-19-git-send-email-paulmck@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f6cad39f35df..6b4298a41167 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1339,7 +1339,7 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 	 * Ensure that EXIT_ZOMBIE -> EXIT_DEAD/EXIT_TRACE transition
 	 * can't confuse the checks below.
 	 */
-	int exit_state = ACCESS_ONCE(p->exit_state);
+	int exit_state = READ_ONCE(p->exit_state);
 	int ret;
 
 	if (unlikely(exit_state == EXIT_DEAD))

commit 1c9fec470b81ca5e89391c20a11ead31a1e9314b
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Oct 20 07:36:05 2017 -0700

    waitid(): Avoid unbalanced user_access_end() on access_ok() error
    
    As pointed out by Linus and David, the earlier waitid() fix resulted in
    a (currently harmless) unbalanced user_access_end() call.  This fixes it
    to just directly return EFAULT on access_ok() failure.
    
    Fixes: 96ca579a1ecc ("waitid(): Add missing access_ok() checks")
    Acked-by: David Daney <david.daney@cavium.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index cf28528842bc..f6cad39f35df 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1611,7 +1611,7 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 		return err;
 
 	if (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))
-		goto Efault;
+		return -EFAULT;
 
 	user_access_begin();
 	unsafe_put_user(signo, &infop->si_signo, Efault);
@@ -1739,7 +1739,7 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 		return err;
 
 	if (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))
-		goto Efault;
+		return -EFAULT;
 
 	user_access_begin();
 	unsafe_put_user(signo, &infop->si_signo, Efault);

commit 96ca579a1ecc943b75beba58bebb0356f6cc4b51
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Oct 9 11:36:52 2017 -0700

    waitid(): Add missing access_ok() checks
    
    Adds missing access_ok() checks.
    
    CVE-2017-5123
    
    Reported-by: Chris Salls <chrissalls5@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Al Viro <viro@zeniv.linux.org.uk>
    Fixes: 4c48abe91be0 ("waitid(): switch copyout of siginfo to unsafe_put_user()")
    Cc: stable@kernel.org # 4.13
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f2cd53e92147..cf28528842bc 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1610,6 +1610,9 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	if (!infop)
 		return err;
 
+	if (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))
+		goto Efault;
+
 	user_access_begin();
 	unsafe_put_user(signo, &infop->si_signo, Efault);
 	unsafe_put_user(0, &infop->si_errno, Efault);
@@ -1735,6 +1738,9 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 	if (!infop)
 		return err;
 
+	if (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))
+		goto Efault;
+
 	user_access_begin();
 	unsafe_put_user(signo, &infop->si_signo, Efault);
 	unsafe_put_user(0, &infop->si_errno, Efault);

commit 6c85501f2fabcfc4fc6ed976543d252c4eaf4be9
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Sep 29 13:43:15 2017 -0400

    fix infoleak in waitid(2)
    
    kernel_waitid() can return a PID, an error or 0.  rusage is filled in the first
    case and waitid(2) rusage should've been copied out exactly in that case, *not*
    whenever kernel_waitid() has not returned an error.  Compat variant shares that
    braino; none of kernel_wait4() callers do, so the below ought to fix it.
    
    Reported-and-tested-by: Alexander Potapenko <glider@google.com>
    Fixes: ce72a16fa705 ("wait4(2)/waitid(2): separate copying rusage to userland")
    Cc: stable@vger.kernel.org # v4.13
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3481ababd06a..f2cd53e92147 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1600,12 +1600,10 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	struct waitid_info info = {.status = 0};
 	long err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);
 	int signo = 0;
+
 	if (err > 0) {
 		signo = SIGCHLD;
 		err = 0;
-	}
-
-	if (!err) {
 		if (ru && copy_to_user(ru, &r, sizeof(struct rusage)))
 			return -EFAULT;
 	}
@@ -1723,16 +1721,15 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 	if (err > 0) {
 		signo = SIGCHLD;
 		err = 0;
-	}
-
-	if (!err && uru) {
-		/* kernel_waitid() overwrites everything in ru */
-		if (COMPAT_USE_64BIT_TIME)
-			err = copy_to_user(uru, &ru, sizeof(ru));
-		else
-			err = put_compat_rusage(&ru, uru);
-		if (err)
-			return -EFAULT;
+		if (uru) {
+			/* kernel_waitid() overwrites everything in ru */
+			if (COMPAT_USE_64BIT_TIME)
+				err = copy_to_user(uru, &ru, sizeof(ru));
+			else
+				err = put_compat_rusage(&ru, uru);
+			if (err)
+				return -EFAULT;
+		}
 	}
 
 	if (!infop)

commit dd198ce7141aa8dd9ffcc9549de422fb055508de
Merge: 89fd915c4021 076a9bcacfc7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 11 18:34:47 2017 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull namespace updates from Eric Biederman:
     "Life has been busy and I have not gotten half as much done this round
      as I would have liked. I delayed it so that a minor conflict
      resolution with the mips tree could spend a little time in linux-next
      before I sent this pull request.
    
      This includes two long delayed user namespace changes from Kirill
      Tkhai. It also includes a very useful change from Serge Hallyn that
      allows the security capability attribute to be used inside of user
      namespaces. The practical effect of this is people can now untar
      tarballs and install rpms in user namespaces. It had been suggested to
      generalize this and encode some of the namespace information
      information in the xattr name. Upon close inspection that makes the
      things that should be hard easy and the things that should be easy
      more expensive.
    
      Then there is my bugfix/cleanup for signal injection that removes the
      magic encoding of the siginfo union member from the kernel internal
      si_code. The mips folks reported the case where I had used FPE_FIXME
      me is impossible so I have remove FPE_FIXME from mips, while at the
      same time including a return statement in that case to keep gcc from
      complaining about unitialized variables.
    
      I almost finished the work to get make copy_siginfo_to_user a trivial
      copy to user. The code is available at:
    
         git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace.git neuter-copy_siginfo_to_user-v3
    
      But I did not have time/energy to get the code posted and reviewed
      before the merge window opened.
    
      I was able to see that the security excuse for just copying fields
      that we know are initialized doesn't work in practice there are buggy
      initializations that don't initialize the proper fields in siginfo. So
      we still sometimes copy unitialized data to userspace"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      Introduce v3 namespaced file capabilities
      mips/signal: In force_fcr31_sig return in the impossible case
      signal: Remove kernel interal si_code magic
      fcntl: Don't use ambiguous SIG_POLL si_codes
      prctl: Allow local CAP_SYS_ADMIN changing exe_file
      security: Use user_namespace::level to avoid redundant iterations in cap_capable()
      userns,pidns: Verify the userns for new pid namespaces
      signal/testing: Don't look for __SI_FAULT in userspace
      signal/mips: Document a conflict with SI_USER with SIGFPE
      signal/sparc: Document a conflict with SI_USER with SIGFPE
      signal/ia64: Document a conflict with SI_USER with SIGFPE
      signal/alpha: Document a conflict with SI_USER for SIGTRAP

commit 5f82e71a001d14824a7728ad9e49f6aea420f161
Merge: 6c51e67b64d1 edc2988c548d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 11:52:29 2017 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
    
     - Add 'cross-release' support to lockdep, which allows APIs like
       completions, where it's not the 'owner' who releases the lock, to be
       tracked. It's all activated automatically under
       CONFIG_PROVE_LOCKING=y.
    
     - Clean up (restructure) the x86 atomics op implementation to be more
       readable, in preparation of KASAN annotations. (Dmitry Vyukov)
    
     - Fix static keys (Paolo Bonzini)
    
     - Add killable versions of down_read() et al (Kirill Tkhai)
    
     - Rework and fix jump_label locking (Marc Zyngier, Paolo Bonzini)
    
     - Rework (and fix) tlb_flush_pending() barriers (Peter Zijlstra)
    
     - Remove smp_mb__before_spinlock() and convert its usages, introduce
       smp_mb__after_spinlock() (Peter Zijlstra)
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (56 commits)
      locking/lockdep/selftests: Fix mixed read-write ABBA tests
      sched/completion: Avoid unnecessary stack allocation for COMPLETION_INITIALIZER_ONSTACK()
      acpi/nfit: Fix COMPLETION_INITIALIZER_ONSTACK() abuse
      locking/pvqspinlock: Relax cmpxchg's to improve performance on some architectures
      smp: Avoid using two cache lines for struct call_single_data
      locking/lockdep: Untangle xhlock history save/restore from task independence
      locking/refcounts, x86/asm: Disable CONFIG_ARCH_HAS_REFCOUNT for the time being
      futex: Remove duplicated code and fix undefined behaviour
      Documentation/locking/atomic: Finish the document...
      locking/lockdep: Fix workqueue crossrelease annotation
      workqueue/lockdep: 'Fix' flush_work() annotation
      locking/lockdep/selftests: Add mixed read-write ABBA tests
      mm, locking/barriers: Clarify tlb_flush_pending() barriers
      locking/lockdep: Make CONFIG_LOCKDEP_CROSSRELEASE and CONFIG_LOCKDEP_COMPLETIONS truly non-interactive
      locking/lockdep: Explicitly initialize wq_barrier::done::map
      locking/lockdep: Rename CONFIG_LOCKDEP_COMPLETE to CONFIG_LOCKDEP_COMPLETIONS
      locking/lockdep: Reword title of LOCKDEP_CROSSRELEASE config
      locking/lockdep: Make CONFIG_LOCKDEP_CROSSRELEASE part of CONFIG_PROVE_LOCKING
      locking/refcounts, x86/asm: Implement fast refcount overflow protection
      locking/lockdep: Fix the rollback and overwrite detection logic in crossrelease
      ...

commit 656e7c0c0a2e8d899f87fd7f081ea7a711146604
Merge: 850bf6d59265 16c0b106070f 09efeeee173e 22e4ebb97582 952111d7db02 35732cf9dd38 f34c8585ed70
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 17 08:10:04 2017 -0700

    Merge branches 'doc.2017.08.17a', 'fixes.2017.08.17a', 'hotplug.2017.07.25b', 'misc.2017.08.17a', 'spin_unlock_wait_no.2017.08.17a', 'srcu.2017.07.27c' and 'torture.2017.07.24c' into HEAD
    
    doc.2017.08.17a: Documentation updates.
    fixes.2017.08.17a: RCU fixes.
    hotplug.2017.07.25b: CPU-hotplug updates.
    misc.2017.08.17a: Miscellaneous fixes outside of RCU (give or take conflicts).
    spin_unlock_wait_no.2017.08.17a: Remove spin_unlock_wait().
    srcu.2017.07.27c: SRCU updates.
    torture.2017.07.24c: Torture-test updates.

commit 8083f29349372d5b949dc022ae9a981edc89ac41
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 29 12:55:21 2017 -0700

    exit: Replace spin_unlock_wait() with lock/unlock pair
    
    There is no agreed-upon definition of spin_unlock_wait()'s semantics, and
    it appears that all callers could do just as well with a lock/unlock pair.
    This commit therefore replaces the spin_unlock_wait() call in do_exit()
    with spin_lock() followed immediately by spin_unlock().  This should be
    safe from a performance perspective because the lock is a per-task lock,
    and this is happening only at task-exit time.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Andrea Parri <parri.andrea@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c5548faa9f37..abfbcf66e5c0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -819,7 +819,8 @@ void __noreturn do_exit(long code)
 	 * Ensure that we must observe the pi_state in exit_mm() ->
 	 * mm_release() -> exit_pi_state_list().
 	 */
-	raw_spin_unlock_wait(&tsk->pi_lock);
+	raw_spin_lock_irq(&tsk->pi_lock);
+	raw_spin_unlock_irq(&tsk->pi_lock);
 
 	if (unlikely(in_atomic())) {
 		pr_info("note: %s[%d] exited with preempt_count %d\n",

commit ccdd29ffffa7246cb359b9408772858a15fc4ea5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu May 25 08:51:48 2017 -0700

    rcu: Create reasonable API for do_exit() TASKS_RCU processing
    
    Currently, the exit-time support for TASKS_RCU is open-coded in do_exit().
    This commit creates exit_tasks_rcu_start() and exit_tasks_rcu_finish()
    APIs for do_exit() use.  This has the benefit of confining the use of the
    tasks_rcu_exit_srcu variable to one file, allowing it to become static.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index c5548faa9f37..d297c525f188 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -764,7 +764,6 @@ void __noreturn do_exit(long code)
 {
 	struct task_struct *tsk = current;
 	int group_dead;
-	TASKS_RCU(int tasks_rcu_i);
 
 	profile_task_exit(tsk);
 	kcov_task_exit(tsk);
@@ -881,9 +880,7 @@ void __noreturn do_exit(long code)
 	 */
 	flush_ptrace_hw_breakpoint(tsk);
 
-	TASKS_RCU(preempt_disable());
-	TASKS_RCU(tasks_rcu_i = __srcu_read_lock(&tasks_rcu_exit_srcu));
-	TASKS_RCU(preempt_enable());
+	exit_tasks_rcu_start();
 	exit_notify(tsk, group_dead);
 	proc_exit_connector(tsk);
 	mpol_put_task_policy(tsk);
@@ -918,7 +915,7 @@ void __noreturn do_exit(long code)
 	if (tsk->nr_dirtied)
 		__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);
 	exit_rcu();
-	TASKS_RCU(__srcu_read_unlock(&tasks_rcu_exit_srcu, tasks_rcu_i));
+	exit_tasks_rcu_finish();
 
 	do_task_dead();
 }

commit b09be676e0ff25bd6d2e7637e26d349f9109ad75
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Aug 7 16:12:52 2017 +0900

    locking/lockdep: Implement the 'crossrelease' feature
    
    Lockdep is a runtime locking correctness validator that detects and
    reports a deadlock or its possibility by checking dependencies between
    locks. It's useful since it does not report just an actual deadlock but
    also the possibility of a deadlock that has not actually happened yet.
    That enables problems to be fixed before they affect real systems.
    
    However, this facility is only applicable to typical locks, such as
    spinlocks and mutexes, which are normally released within the context in
    which they were acquired. However, synchronization primitives like page
    locks or completions, which are allowed to be released in any context,
    also create dependencies and can cause a deadlock.
    
    So lockdep should track these locks to do a better job. The 'crossrelease'
    implementation makes these primitives also be tracked.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: boqun.feng@gmail.com
    Cc: kernel-team@lge.com
    Cc: kirill@shutemov.name
    Cc: npiggin@gmail.com
    Cc: walken@google.com
    Cc: willy@infradead.org
    Link: http://lkml.kernel.org/r/1502089981-21272-6-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c5548faa9f37..fa72d57db747 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -920,6 +920,7 @@ void __noreturn do_exit(long code)
 	exit_rcu();
 	TASKS_RCU(__srcu_read_unlock(&tasks_rcu_exit_srcu, tasks_rcu_i));
 
+	lockdep_free_task(tsk);
 	do_task_dead();
 }
 EXPORT_SYMBOL_GPL(do_exit);

commit cc731525f26af85a1c3537da41e0abd1d35e0bdb
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sun Jul 16 22:36:59 2017 -0500

    signal: Remove kernel interal si_code magic
    
    struct siginfo is a union and the kernel since 2.4 has been hiding a union
    tag in the high 16bits of si_code using the values:
    __SI_KILL
    __SI_TIMER
    __SI_POLL
    __SI_FAULT
    __SI_CHLD
    __SI_RT
    __SI_MESGQ
    __SI_SYS
    
    While this looks plausible on the surface, in practice this situation has
    not worked well.
    
    - Injected positive signals are not copied to user space properly
      unless they have these magic high bits set.
    
    - Injected positive signals are not reported properly by signalfd
      unless they have these magic high bits set.
    
    - These kernel internal values leaked to userspace via ptrace_peek_siginfo
    
    - It was possible to inject these kernel internal values and cause the
      the kernel to misbehave.
    
    - Kernel developers got confused and expected these kernel internal values
      in userspace in kernel self tests.
    
    - Kernel developers got confused and set si_code to __SI_FAULT which
      is SI_USER in userspace which causes userspace to think an ordinary user
      sent the signal and that it was not kernel generated.
    
    - The values make it impossible to reorganize the code to transform
      siginfo_copy_to_user into a plain copy_to_user.  As si_code must
      be massaged before being passed to userspace.
    
    So remove these kernel internal si codes and make the kernel code simpler
    and more maintainable.
    
    To replace these kernel internal magic si_codes introduce the helper
    function siginfo_layout, that takes a signal number and an si_code and
    computes which union member of siginfo is being used.  Have
    siginfo_layout return an enumeration so that gcc will have enough
    information to warn if a switch statement does not handle all of union
    members.
    
    A couple of architectures have a messed up ABI that defines signal
    specific duplications of SI_USER which causes more special cases in
    siginfo_layout than I would like.  The good news is only problem
    architectures pay the cost.
    
    Update all of the code that used the previous magic __SI_ values to
    use the new SIL_ values and to call siginfo_layout to get those
    values.  Escept where not all of the cases are handled remove the
    defaults in the switch statements so that if a new case is missed in
    the future the lack will show up at compile time.
    
    Modify the code that copies siginfo si_code to userspace to just copy
    the value and not cast si_code to a short first.  The high bits are no
    longer used to hold a magic union member.
    
    Fixup the siginfo header files to stop including the __SI_ values in
    their constants and for the headers that were missing it to properly
    update the number of si_codes for each signal type.
    
    The fixes to copy_siginfo_from_user32 implementations has the
    interesting property that several of them perviously should never have
    worked as the __SI_ values they depended up where kernel internal.
    With that dependency gone those implementations should work much
    better.
    
    The idea of not passing the __SI_ values out to userspace and then
    not reinserting them has been tested with criu and criu worked without
    changes.
    
    Ref: 2.4.0-test1
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index c5548faa9f37..c8f23613df5b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1616,7 +1616,7 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	user_access_begin();
 	unsafe_put_user(signo, &infop->si_signo, Efault);
 	unsafe_put_user(0, &infop->si_errno, Efault);
-	unsafe_put_user((short)info.cause, &infop->si_code, Efault);
+	unsafe_put_user(info.cause, &infop->si_code, Efault);
 	unsafe_put_user(info.pid, &infop->si_pid, Efault);
 	unsafe_put_user(info.uid, &infop->si_uid, Efault);
 	unsafe_put_user(info.status, &infop->si_status, Efault);
@@ -1742,7 +1742,7 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 	user_access_begin();
 	unsafe_put_user(signo, &infop->si_signo, Efault);
 	unsafe_put_user(0, &infop->si_errno, Efault);
-	unsafe_put_user((short)info.cause, &infop->si_code, Efault);
+	unsafe_put_user(info.cause, &infop->si_code, Efault);
 	unsafe_put_user(info.pid, &infop->si_pid, Efault);
 	unsafe_put_user(info.uid, &infop->si_uid, Efault);
 	unsafe_put_user(info.status, &infop->si_status, Efault);

commit dd83c161fbcc5d8be637ab159c0de015cbff5ba4
Author: zhongjiang <zhongjiang@huawei.com>
Date:   Mon Jul 10 15:53:01 2017 -0700

    kernel/exit.c: avoid undefined behaviour when calling wait4()
    
    wait4(-2147483648, 0x20, 0, 0xdd0000) triggers:
    UBSAN: Undefined behaviour in kernel/exit.c:1651:9
    
    The related calltrace is as follows:
    
      negation of -2147483648 cannot be represented in type 'int':
      CPU: 9 PID: 16482 Comm: zj Tainted: G    B          ---- -------   3.10.0-327.53.58.71.x86_64+ #66
      Hardware name: Huawei Technologies Co., Ltd. Tecal RH2285          /BC11BTSA              , BIOS CTSAV036 04/27/2011
      Call Trace:
        dump_stack+0x19/0x1b
        ubsan_epilogue+0xd/0x50
        __ubsan_handle_negate_overflow+0x109/0x14e
        SyS_wait4+0x1cb/0x1e0
        system_call_fastpath+0x16/0x1b
    
    Exclude the overflow to avoid the UBSAN warning.
    
    Link: http://lkml.kernel.org/r/1497264618-20212-1-git-send-email-zhongjiang@huawei.com
    Signed-off-by: zhongjiang <zhongjiang@huawei.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 608c9775a37b..c5548faa9f37 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1639,6 +1639,10 @@ long kernel_wait4(pid_t upid, int __user *stat_addr, int options,
 			__WNOTHREAD|__WCLONE|__WALL))
 		return -EINVAL;
 
+	/* -INT_MIN is not defined */
+	if (upid == INT_MIN)
+		return -ESRCH;
+
 	if (upid == -1)
 		type = PIDTYPE_MAX;
 	else if (upid < 0) {

commit 634a81609561f05266e1f625b6f2567c2e0b0419
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Jul 8 11:26:39 2017 -0400

    fix waitid(2) breakage
    
    We lose the distinction between "found a PID" and "nothing, but that's not
    an error" a bit too early in waitid().  Easily fixed, fortunately...
    
    Reported-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Fixes: 67d7ddded322 ("waitid(2): leave copyout of siginfo to syscall itself")
    Tested-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2bbc23273e2f..608c9775a37b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1590,9 +1590,6 @@ static long kernel_waitid(int which, pid_t upid, struct waitid_info *infop,
 	wo.wo_rusage	= ru;
 	ret = do_wait(&wo);
 
-	if (ret > 0)
-		ret = 0;
-
 	put_pid(pid);
 	return ret;
 }
@@ -1603,6 +1600,11 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	struct rusage r;
 	struct waitid_info info = {.status = 0};
 	long err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);
+	int signo = 0;
+	if (err > 0) {
+		signo = SIGCHLD;
+		err = 0;
+	}
 
 	if (!err) {
 		if (ru && copy_to_user(ru, &r, sizeof(struct rusage)))
@@ -1612,7 +1614,7 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 		return err;
 
 	user_access_begin();
-	unsafe_put_user(err ? 0 : SIGCHLD, &infop->si_signo, Efault);
+	unsafe_put_user(signo, &infop->si_signo, Efault);
 	unsafe_put_user(0, &infop->si_errno, Efault);
 	unsafe_put_user((short)info.cause, &infop->si_code, Efault);
 	unsafe_put_user(info.pid, &infop->si_pid, Efault);
@@ -1714,6 +1716,11 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 	struct rusage ru;
 	struct waitid_info info = {.status = 0};
 	long err = kernel_waitid(which, pid, &info, options, uru ? &ru : NULL);
+	int signo = 0;
+	if (err > 0) {
+		signo = SIGCHLD;
+		err = 0;
+	}
 
 	if (!err && uru) {
 		/* kernel_waitid() overwrites everything in ru */
@@ -1729,7 +1736,7 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 		return err;
 
 	user_access_begin();
-	unsafe_put_user(err ? 0 : SIGCHLD, &infop->si_signo, Efault);
+	unsafe_put_user(signo, &infop->si_signo, Efault);
 	unsafe_put_user(0, &infop->si_errno, Efault);
 	unsafe_put_user((short)info.cause, &infop->si_code, Efault);
 	unsafe_put_user(info.pid, &infop->si_pid, Efault);

commit 57ecbd3831ee3ad43914d5c9dddbff7ce30e3d42
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Thu Jul 6 15:38:32 2017 -0700

    kernel/exit.c: don't include unused userfaultfd_k.h
    
    Commit dd0db88d8094 ("userfaultfd: non-cooperative: rollback
    userfaultfd_exit") removed userfaultfd callback from exit() which makes
    the include of <linux/userfaultfd_k.h> unnecessary.
    
    Link: http://lkml.kernel.org/r/1494930907-3060-1-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b0cc86a2d00b..2bbc23273e2f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -51,7 +51,6 @@
 #include <linux/task_io_accounting_ops.h>
 #include <linux/tracehook.h>
 #include <linux/fs_struct.h>
-#include <linux/userfaultfd_k.h>
 #include <linux/init_task.h>
 #include <linux/perf_event.h>
 #include <trace/events/sched.h>

commit 4be95131bf3bca97b6a7db9c6fb63db2cb94da06
Merge: 3bad2f1c6765 92ebce5ac55d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 5 14:10:19 2017 -0700

    Merge branch 'work.sys_wait' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull wait syscall updates from Al Viro:
     "Consolidating sys_wait* and compat counterparts.
    
      Gets rid of set_fs()/double-copy mess, simplifies the whole thing
      (lifting the copyouts to the syscalls means less headache in the part
      that does actual work - fewer failure exits, to start with), gets rid
      of the overhead of field-by-field __put_user()"
    
    * 'work.sys_wait' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      osf_wait4: switch to kernel_wait4()
      waitid(): switch copyout of siginfo to unsafe_put_user()
      wait_task_zombie: consolidate info logics
      kill wait_noreap_copyout()
      lift getrusage() from wait_noreap_copyout()
      waitid(2): leave copyout of siginfo to syscall itself
      kernel_wait4()/kernel_waitid(): delay copying status to userland
      wait4(2)/waitid(2): separate copying rusage to userland
      move compat wait4 and waitid next to native variants

commit f11cc0760b8397e0d230122606421b6a96e9f869
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Wed Jun 14 19:37:30 2017 -0700

    sched/core: Drop the unused try_get_task_struct() helper function
    
    This function was introduced by:
    
      150593bf8693 ("sched/api: Introduce task_rcu_dereference() and try_get_task_struct()")
    
    ... to allow easier usage of task_rcu_dereference(), however no users
    were ever added. Drop the helper.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Link: http://lkml.kernel.org/r/20170615023730.22827-1-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7d694437ab44..c63226283aef 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -318,19 +318,6 @@ void rcuwait_wake_up(struct rcuwait *w)
 	rcu_read_unlock();
 }
 
-struct task_struct *try_get_task_struct(struct task_struct **ptask)
-{
-	struct task_struct *task;
-
-	rcu_read_lock();
-	task = task_rcu_dereference(ptask);
-	if (task)
-		get_task_struct(task);
-	rcu_read_unlock();
-
-	return task;
-}
-
 /*
  * Determine if a process group is "orphaned", according to the POSIX
  * definition in 2.2.2.52.  Orphaned process groups are not to be affected

commit ac6424b981bce1c4bc55675c6ce11bfe1bbfa64f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Jun 20 12:06:13 2017 +0200

    sched/wait: Rename wait_queue_t => wait_queue_entry_t
    
    Rename:
    
            wait_queue_t            =>      wait_queue_entry_t
    
    'wait_queue_t' was always a slight misnomer: its name implies that it's a "queue",
    but in reality it's a queue *entry*. The 'real' queue is the wait queue head,
    which had to carry the name.
    
    Start sorting this out by renaming it to 'wait_queue_entry_t'.
    
    This also allows the real structure name 'struct __wait_queue' to
    lose its double underscore and become 'struct wait_queue_entry',
    which is the more canonical nomenclature for such data types.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 516acdb0e0ec..7d694437ab44 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1004,7 +1004,7 @@ struct wait_opts {
 	int __user		*wo_stat;
 	struct rusage __user	*wo_rusage;
 
-	wait_queue_t		child_wait;
+	wait_queue_entry_t		child_wait;
 	int			notask_error;
 };
 
@@ -1541,7 +1541,7 @@ static int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)
 	return 0;
 }
 
-static int child_wait_callback(wait_queue_t *wait, unsigned mode,
+static int child_wait_callback(wait_queue_entry_t *wait, unsigned mode,
 				int sync, void *key)
 {
 	struct wait_opts *wo = container_of(wait, struct wait_opts,

commit 92ebce5ac55dba258c608248dddf59eca3f7f514
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun May 14 23:54:33 2017 -0400

    osf_wait4: switch to kernel_wait4()
    
    ... and sanitize copying rusage to userland
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index f3b8c3a87bc1..462fc25eec6e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1639,8 +1639,8 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	return -EFAULT;
 }
 
-static long kernel_wait4(pid_t upid, int __user *stat_addr,
-			int options, struct rusage *ru)
+long kernel_wait4(pid_t upid, int __user *stat_addr, int options,
+		  struct rusage *ru)
 {
 	struct wait_opts wo;
 	struct pid *pid = NULL;

commit 4c48abe91be03d191d0c20cc755877da2cb35622
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun May 14 19:27:32 2017 -0400

    waitid(): switch copyout of siginfo to unsafe_put_user()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 97db9ee03f90..f3b8c3a87bc1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1625,15 +1625,18 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	if (!infop)
 		return err;
 
-	if (put_user(err ? 0 : SIGCHLD, &infop->si_signo) ||
-	    put_user(0, &infop->si_errno) ||
-	    put_user((short)info.cause, &infop->si_code) ||
-	    put_user(info.pid, &infop->si_pid) ||
-	    put_user(info.uid, &infop->si_uid) ||
-	    put_user(info.status, &infop->si_status))
-		err = -EFAULT;
-
+	user_access_begin();
+	unsafe_put_user(err ? 0 : SIGCHLD, &infop->si_signo, Efault);
+	unsafe_put_user(0, &infop->si_errno, Efault);
+	unsafe_put_user((short)info.cause, &infop->si_code, Efault);
+	unsafe_put_user(info.pid, &infop->si_pid, Efault);
+	unsafe_put_user(info.uid, &infop->si_uid, Efault);
+	unsafe_put_user(info.status, &infop->si_status, Efault);
+	user_access_end();
 	return err;
+Efault:
+	user_access_end();
+	return -EFAULT;
 }
 
 static long kernel_wait4(pid_t upid, int __user *stat_addr,
@@ -1736,13 +1739,20 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 			return -EFAULT;
 	}
 
-	if (put_user(err ? 0 : SIGCHLD, &infop->si_signo) ||
-	    put_user(0, &infop->si_errno) ||
-	    put_user((short)info.cause, &infop->si_code) ||
-	    put_user(info.pid, &infop->si_pid) ||
-	    put_user(info.uid, &infop->si_uid) ||
-	    put_user(info.status, &infop->si_status))
-		err = -EFAULT;
+	if (!infop)
+		return err;
+
+	user_access_begin();
+	unsafe_put_user(err ? 0 : SIGCHLD, &infop->si_signo, Efault);
+	unsafe_put_user(0, &infop->si_errno, Efault);
+	unsafe_put_user((short)info.cause, &infop->si_code, Efault);
+	unsafe_put_user(info.pid, &infop->si_pid, Efault);
+	unsafe_put_user(info.uid, &infop->si_uid, Efault);
+	unsafe_put_user(info.status, &infop->si_status, Efault);
+	user_access_end();
 	return err;
+Efault:
+	user_access_end();
+	return -EFAULT;
 }
 #endif

commit 76d9871e1122aabc086e7aade5251b1e5124cbb9
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun May 14 21:38:26 2017 -0400

    wait_task_zombie: consolidate info logics
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index f01ebaab978a..97db9ee03f90 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1074,28 +1074,14 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		return 0;
 
 	if (unlikely(wo->wo_flags & WNOWAIT)) {
-		int exit_code = p->exit_code;
-
+		status = p->exit_code;
 		get_task_struct(p);
 		read_unlock(&tasklist_lock);
 		sched_annotate_sleep();
 		if (wo->wo_rusage)
 			getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
 		put_task_struct(p);
-
-		infop = wo->wo_info;
-		if (infop) {
-			if ((exit_code & 0x7f) == 0) {
-				infop->cause = CLD_EXITED;
-				infop->status = exit_code >> 8;
-			} else {
-				infop->cause = (exit_code & 0x80) ? CLD_DUMPED : CLD_KILLED;
-				infop->status = exit_code & 0x7f;
-			}
-			infop->pid = pid;
-			infop->uid = uid;
-		}
-		return pid;
+		goto out_info;
 	}
 	/*
 	 * Move the task's state to DEAD/TRACE, only one thread can do this.
@@ -1174,19 +1160,6 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		? p->signal->group_exit_code : p->exit_code;
 	wo->wo_stat = status;
 
-	infop = wo->wo_info;
-	if (infop) {
-		if ((status & 0x7f) == 0) {
-			infop->cause = CLD_EXITED;
-			infop->status = status >> 8;
-		} else {
-			infop->cause = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;
-			infop->status = status & 0x7f;
-		}
-		infop->pid = pid;
-		infop->uid = uid;
-	}
-
 	if (state == EXIT_TRACE) {
 		write_lock_irq(&tasklist_lock);
 		/* We dropped tasklist, ptracer could die and untrace */
@@ -1202,6 +1175,20 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	if (state == EXIT_DEAD)
 		release_task(p);
 
+out_info:
+	infop = wo->wo_info;
+	if (infop) {
+		if ((status & 0x7f) == 0) {
+			infop->cause = CLD_EXITED;
+			infop->status = status >> 8;
+		} else {
+			infop->cause = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;
+			infop->status = status & 0x7f;
+		}
+		infop->pid = pid;
+		infop->uid = uid;
+	}
+
 	return pid;
 }
 

commit bb380ec33a7d8ee048e722889627869d21a5d527
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun May 14 21:33:21 2017 -0400

    kill wait_noreap_copyout()
    
    folds into callers
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index d4f5097da85a..f01ebaab978a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1057,22 +1057,6 @@ eligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)
 	return 1;
 }
 
-static int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,
-				pid_t pid, uid_t uid, int why, int status)
-{
-	struct waitid_info *infop;
-
-	put_task_struct(p);
-	infop = wo->wo_info;
-	if (infop) {
-		infop->cause = why;
-		infop->pid = pid;
-		infop->uid = uid;
-		infop->status = status;
-	}
-	return pid;
-}
-
 /*
  * Handle sys_wait4 work for one task in state EXIT_ZOMBIE.  We hold
  * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold
@@ -1091,22 +1075,27 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 
 	if (unlikely(wo->wo_flags & WNOWAIT)) {
 		int exit_code = p->exit_code;
-		int why;
 
 		get_task_struct(p);
 		read_unlock(&tasklist_lock);
 		sched_annotate_sleep();
 		if (wo->wo_rusage)
 			getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
+		put_task_struct(p);
 
-		if ((exit_code & 0x7f) == 0) {
-			why = CLD_EXITED;
-			status = exit_code >> 8;
-		} else {
-			why = (exit_code & 0x80) ? CLD_DUMPED : CLD_KILLED;
-			status = exit_code & 0x7f;
+		infop = wo->wo_info;
+		if (infop) {
+			if ((exit_code & 0x7f) == 0) {
+				infop->cause = CLD_EXITED;
+				infop->status = exit_code >> 8;
+			} else {
+				infop->cause = (exit_code & 0x80) ? CLD_DUMPED : CLD_KILLED;
+				infop->status = exit_code & 0x7f;
+			}
+			infop->pid = pid;
+			infop->uid = uid;
 		}
-		return wait_noreap_copyout(wo, p, pid, uid, why, status);
+		return pid;
 	}
 	/*
 	 * Move the task's state to DEAD/TRACE, only one thread can do this.
@@ -1297,11 +1286,10 @@ static int wait_task_stopped(struct wait_opts *wo,
 	sched_annotate_sleep();
 	if (wo->wo_rusage)
 		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
+	put_task_struct(p);
 
-	if (unlikely(wo->wo_flags & WNOWAIT))
-		return wait_noreap_copyout(wo, p, pid, uid, why, exit_code);
-
-	wo->wo_stat = (exit_code << 8) | 0x7f;
+	if (likely(!(wo->wo_flags & WNOWAIT)))
+		wo->wo_stat = (exit_code << 8) | 0x7f;
 
 	infop = wo->wo_info;
 	if (infop) {
@@ -1310,9 +1298,6 @@ static int wait_task_stopped(struct wait_opts *wo,
 		infop->pid = pid;
 		infop->uid = uid;
 	}
-	put_task_struct(p);
-
-	BUG_ON(!pid);
 	return pid;
 }
 
@@ -1324,7 +1309,7 @@ static int wait_task_stopped(struct wait_opts *wo,
  */
 static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 {
-	int retval;
+	struct waitid_info *infop;
 	pid_t pid;
 	uid_t uid;
 
@@ -1351,18 +1336,18 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 	sched_annotate_sleep();
 	if (wo->wo_rusage)
 		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
+	put_task_struct(p);
 
-	if (!wo->wo_info) {
-		put_task_struct(p);
+	infop = wo->wo_info;
+	if (!infop) {
 		wo->wo_stat = 0xffff;
-		retval = pid;
 	} else {
-		retval = wait_noreap_copyout(wo, p, pid, uid,
-					     CLD_CONTINUED, SIGCONT);
-		BUG_ON(retval == 0);
+		infop->cause = CLD_CONTINUED;
+		infop->pid = pid;
+		infop->uid = uid;
+		infop->status = SIGCONT;
 	}
-
-	return retval;
+	return pid;
 }
 
 /*

commit e61a250229fbf0f003e93676bf4d8a555a8c9eec
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun May 14 21:25:03 2017 -0400

    lift getrusage() from wait_noreap_copyout()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 42f26480b3cc..d4f5097da85a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1062,9 +1062,6 @@ static int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,
 {
 	struct waitid_info *infop;
 
-	if (wo->wo_rusage)
-		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
-
 	put_task_struct(p);
 	infop = wo->wo_info;
 	if (infop) {
@@ -1099,6 +1096,8 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		get_task_struct(p);
 		read_unlock(&tasklist_lock);
 		sched_annotate_sleep();
+		if (wo->wo_rusage)
+			getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
 
 		if ((exit_code & 0x7f) == 0) {
 			why = CLD_EXITED;
@@ -1296,12 +1295,12 @@ static int wait_task_stopped(struct wait_opts *wo,
 	why = ptrace ? CLD_TRAPPED : CLD_STOPPED;
 	read_unlock(&tasklist_lock);
 	sched_annotate_sleep();
+	if (wo->wo_rusage)
+		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
 
 	if (unlikely(wo->wo_flags & WNOWAIT))
 		return wait_noreap_copyout(wo, p, pid, uid, why, exit_code);
 
-	if (wo->wo_rusage)
-		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
 	wo->wo_stat = (exit_code << 8) | 0x7f;
 
 	infop = wo->wo_info;
@@ -1350,10 +1349,10 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);
 	sched_annotate_sleep();
+	if (wo->wo_rusage)
+		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
 
 	if (!wo->wo_info) {
-		if (wo->wo_rusage)
-			getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
 		put_task_struct(p);
 		wo->wo_stat = 0xffff;
 		retval = pid;

commit 67d7ddded322db99f451a7959d56ed6c70a6c4aa
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun May 14 20:53:13 2017 -0400

    waitid(2): leave copyout of siginfo to syscall itself
    
    have kernel_waitid() collect the information needed for siginfo into
    a small structure (waitid_info) passed to it; deal with copyout in
    sys_waitid()/compat_sys_waitid().
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 94cdccf8e7e7..42f26480b3cc 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -996,12 +996,19 @@ SYSCALL_DEFINE1(exit_group, int, error_code)
 	return 0;
 }
 
+struct waitid_info {
+	pid_t pid;
+	uid_t uid;
+	int status;
+	int cause;
+};
+
 struct wait_opts {
 	enum pid_type		wo_type;
 	int			wo_flags;
 	struct pid		*wo_pid;
 
-	struct siginfo __user	*wo_info;
+	struct waitid_info	*wo_info;
 	int			wo_stat;
 	struct rusage		*wo_rusage;
 
@@ -1053,8 +1060,7 @@ eligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)
 static int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,
 				pid_t pid, uid_t uid, int why, int status)
 {
-	struct siginfo __user *infop;
-	int retval = 0;
+	struct waitid_info *infop;
 
 	if (wo->wo_rusage)
 		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
@@ -1062,22 +1068,12 @@ static int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,
 	put_task_struct(p);
 	infop = wo->wo_info;
 	if (infop) {
-		if (!retval)
-			retval = put_user(SIGCHLD, &infop->si_signo);
-		if (!retval)
-			retval = put_user(0, &infop->si_errno);
-		if (!retval)
-			retval = put_user((short)why, &infop->si_code);
-		if (!retval)
-			retval = put_user(pid, &infop->si_pid);
-		if (!retval)
-			retval = put_user(uid, &infop->si_uid);
-		if (!retval)
-			retval = put_user(status, &infop->si_status);
+		infop->cause = why;
+		infop->pid = pid;
+		infop->uid = uid;
+		infop->status = status;
 	}
-	if (!retval)
-		retval = pid;
-	return retval;
+	return pid;
 }
 
 /*
@@ -1088,10 +1084,10 @@ static int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,
  */
 static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 {
-	int state, retval, status;
+	int state, status;
 	pid_t pid = task_pid_vnr(p);
 	uid_t uid = from_kuid_munged(current_user_ns(), task_uid(p));
-	struct siginfo __user *infop;
+	struct waitid_info *infop;
 
 	if (!likely(wo->wo_flags & WEXITED))
 		return 0;
@@ -1186,36 +1182,22 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 
 	if (wo->wo_rusage)
 		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
-	retval = 0;
 	status = (p->signal->flags & SIGNAL_GROUP_EXIT)
 		? p->signal->group_exit_code : p->exit_code;
 	wo->wo_stat = status;
 
 	infop = wo->wo_info;
-	if (!retval && infop)
-		retval = put_user(SIGCHLD, &infop->si_signo);
-	if (!retval && infop)
-		retval = put_user(0, &infop->si_errno);
-	if (!retval && infop) {
-		int why;
-
+	if (infop) {
 		if ((status & 0x7f) == 0) {
-			why = CLD_EXITED;
-			status >>= 8;
+			infop->cause = CLD_EXITED;
+			infop->status = status >> 8;
 		} else {
-			why = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;
-			status &= 0x7f;
+			infop->cause = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;
+			infop->status = status & 0x7f;
 		}
-		retval = put_user((short)why, &infop->si_code);
-		if (!retval)
-			retval = put_user(status, &infop->si_status);
+		infop->pid = pid;
+		infop->uid = uid;
 	}
-	if (!retval && infop)
-		retval = put_user(pid, &infop->si_pid);
-	if (!retval && infop)
-		retval = put_user(uid, &infop->si_uid);
-	if (!retval)
-		retval = pid;
 
 	if (state == EXIT_TRACE) {
 		write_lock_irq(&tasklist_lock);
@@ -1232,7 +1214,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	if (state == EXIT_DEAD)
 		release_task(p);
 
-	return retval;
+	return pid;
 }
 
 static int *task_stopped_code(struct task_struct *p, bool ptrace)
@@ -1268,8 +1250,8 @@ static int *task_stopped_code(struct task_struct *p, bool ptrace)
 static int wait_task_stopped(struct wait_opts *wo,
 				int ptrace, struct task_struct *p)
 {
-	struct siginfo __user *infop;
-	int retval, exit_code, *p_code, why;
+	struct waitid_info *infop;
+	int exit_code, *p_code, why;
 	uid_t uid = 0; /* unneeded, required by compiler */
 	pid_t pid;
 
@@ -1320,28 +1302,19 @@ static int wait_task_stopped(struct wait_opts *wo,
 
 	if (wo->wo_rusage)
 		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
-	retval = 0;
 	wo->wo_stat = (exit_code << 8) | 0x7f;
 
 	infop = wo->wo_info;
-	if (!retval && infop)
-		retval = put_user(SIGCHLD, &infop->si_signo);
-	if (!retval && infop)
-		retval = put_user(0, &infop->si_errno);
-	if (!retval && infop)
-		retval = put_user((short)why, &infop->si_code);
-	if (!retval && infop)
-		retval = put_user(exit_code, &infop->si_status);
-	if (!retval && infop)
-		retval = put_user(pid, &infop->si_pid);
-	if (!retval && infop)
-		retval = put_user(uid, &infop->si_uid);
-	if (!retval)
-		retval = pid;
+	if (infop) {
+		infop->cause = why;
+		infop->status = exit_code;
+		infop->pid = pid;
+		infop->uid = uid;
+	}
 	put_task_struct(p);
 
-	BUG_ON(!retval);
-	return retval;
+	BUG_ON(!pid);
+	return pid;
 }
 
 /*
@@ -1618,7 +1591,7 @@ static long do_wait(struct wait_opts *wo)
 	return retval;
 }
 
-static long kernel_waitid(int which, pid_t upid, struct siginfo __user *infop,
+static long kernel_waitid(int which, pid_t upid, struct waitid_info *infop,
 			  int options, struct rusage *ru)
 {
 	struct wait_opts wo;
@@ -1660,27 +1633,8 @@ static long kernel_waitid(int which, pid_t upid, struct siginfo __user *infop,
 	wo.wo_rusage	= ru;
 	ret = do_wait(&wo);
 
-	if (ret > 0) {
+	if (ret > 0)
 		ret = 0;
-	} else if (infop) {
-		/*
-		 * For a WNOHANG return, clear out all the fields
-		 * we would set so the user can easily tell the
-		 * difference.
-		 */
-		if (!ret)
-			ret = put_user(0, &infop->si_signo);
-		if (!ret)
-			ret = put_user(0, &infop->si_errno);
-		if (!ret)
-			ret = put_user(0, &infop->si_code);
-		if (!ret)
-			ret = put_user(0, &infop->si_pid);
-		if (!ret)
-			ret = put_user(0, &infop->si_uid);
-		if (!ret)
-			ret = put_user(0, &infop->si_status);
-	}
 
 	put_pid(pid);
 	return ret;
@@ -1690,12 +1644,24 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 		infop, int, options, struct rusage __user *, ru)
 {
 	struct rusage r;
-	long err = kernel_waitid(which, upid, infop, options, ru ? &r : NULL);
+	struct waitid_info info = {.status = 0};
+	long err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);
 
 	if (!err) {
 		if (ru && copy_to_user(ru, &r, sizeof(struct rusage)))
 			return -EFAULT;
 	}
+	if (!infop)
+		return err;
+
+	if (put_user(err ? 0 : SIGCHLD, &infop->si_signo) ||
+	    put_user(0, &infop->si_errno) ||
+	    put_user((short)info.cause, &infop->si_code) ||
+	    put_user(info.pid, &infop->si_pid) ||
+	    put_user(info.uid, &infop->si_uid) ||
+	    put_user(info.status, &infop->si_status))
+		err = -EFAULT;
+
 	return err;
 }
 
@@ -1785,33 +1751,27 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 		struct compat_siginfo __user *, infop, int, options,
 		struct compat_rusage __user *, uru)
 {
-	siginfo_t info;
 	struct rusage ru;
-	long ret;
-	mm_segment_t old_fs = get_fs();
-
-	memset(&info, 0, sizeof(info));
+	struct waitid_info info = {.status = 0};
+	long err = kernel_waitid(which, pid, &info, options, uru ? &ru : NULL);
 
-	set_fs(KERNEL_DS);
-	ret = kernel_waitid(which, pid, (siginfo_t __user *)&info, options,
-			 uru ? &ru : NULL);
-	set_fs(old_fs);
-
-	if ((ret < 0) || (info.si_signo == 0))
-		return ret;
-
-	if (uru) {
-		/* sys_waitid() overwrites everything in ru */
+	if (!err && uru) {
+		/* kernel_waitid() overwrites everything in ru */
 		if (COMPAT_USE_64BIT_TIME)
-			ret = copy_to_user(uru, &ru, sizeof(ru));
+			err = copy_to_user(uru, &ru, sizeof(ru));
 		else
-			ret = put_compat_rusage(&ru, uru);
-		if (ret)
+			err = put_compat_rusage(&ru, uru);
+		if (err)
 			return -EFAULT;
 	}
 
-	BUG_ON(info.si_code & __SI_MASK);
-	info.si_code |= __SI_CHLD;
-	return copy_siginfo_to_user32(infop, &info);
+	if (put_user(err ? 0 : SIGCHLD, &infop->si_signo) ||
+	    put_user(0, &infop->si_errno) ||
+	    put_user((short)info.cause, &infop->si_code) ||
+	    put_user(info.pid, &infop->si_pid) ||
+	    put_user(info.uid, &infop->si_uid) ||
+	    put_user(info.status, &infop->si_status))
+		err = -EFAULT;
+	return err;
 }
 #endif

commit 359566faefa850504d146839d74496f0cf12d3b9
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun May 14 20:39:39 2017 -0400

    kernel_wait4()/kernel_waitid(): delay copying status to userland
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index d44f12948c5f..94cdccf8e7e7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1002,7 +1002,7 @@ struct wait_opts {
 	struct pid		*wo_pid;
 
 	struct siginfo __user	*wo_info;
-	int __user		*wo_stat;
+	int			wo_stat;
 	struct rusage		*wo_rusage;
 
 	wait_queue_t		child_wait;
@@ -1189,8 +1189,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	retval = 0;
 	status = (p->signal->flags & SIGNAL_GROUP_EXIT)
 		? p->signal->group_exit_code : p->exit_code;
-	if (!retval && wo->wo_stat)
-		retval = put_user(status, wo->wo_stat);
+	wo->wo_stat = status;
 
 	infop = wo->wo_info;
 	if (!retval && infop)
@@ -1322,8 +1321,7 @@ static int wait_task_stopped(struct wait_opts *wo,
 	if (wo->wo_rusage)
 		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
 	retval = 0;
-	if (!retval && wo->wo_stat)
-		retval = put_user((exit_code << 8) | 0x7f, wo->wo_stat);
+	wo->wo_stat = (exit_code << 8) | 0x7f;
 
 	infop = wo->wo_info;
 	if (!retval && infop)
@@ -1383,12 +1381,9 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 	if (!wo->wo_info) {
 		if (wo->wo_rusage)
 			getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
-		retval = 0;
 		put_task_struct(p);
-		if (!retval && wo->wo_stat)
-			retval = put_user(0xffff, wo->wo_stat);
-		if (!retval)
-			retval = pid;
+		wo->wo_stat = 0xffff;
+		retval = pid;
 	} else {
 		retval = wait_noreap_copyout(wo, p, pid, uid,
 					     CLD_CONTINUED, SIGCONT);
@@ -1662,7 +1657,6 @@ static long kernel_waitid(int which, pid_t upid, struct siginfo __user *infop,
 	wo.wo_pid	= pid;
 	wo.wo_flags	= options;
 	wo.wo_info	= infop;
-	wo.wo_stat	= NULL;
 	wo.wo_rusage	= ru;
 	ret = do_wait(&wo);
 
@@ -1734,10 +1728,12 @@ static long kernel_wait4(pid_t upid, int __user *stat_addr,
 	wo.wo_pid	= pid;
 	wo.wo_flags	= options | WEXITED;
 	wo.wo_info	= NULL;
-	wo.wo_stat	= stat_addr;
+	wo.wo_stat	= 0;
 	wo.wo_rusage	= ru;
 	ret = do_wait(&wo);
 	put_pid(pid);
+	if (ret > 0 && stat_addr && put_user(wo.wo_stat, stat_addr))
+		ret = -EFAULT;
 
 	return ret;
 }

commit ce72a16fa705f960ca2352e95a7c5f4801475e75
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun May 14 20:25:02 2017 -0400

    wait4(2)/waitid(2): separate copying rusage to userland
    
    New helpers: kernel_waitid() and kernel_wait4().  sys_waitid(),
    sys_wait4() and their compat variants switched to those.  Copying
    struct rusage to userland is left to syscall itself.  For
    compat_sys_wait4() that eliminates the use of set_fs() completely.
    For compat_sys_waitid() it's still needed (for siginfo handling);
    that will change shortly.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index f98782bd27b6..d44f12948c5f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1003,7 +1003,7 @@ struct wait_opts {
 
 	struct siginfo __user	*wo_info;
 	int __user		*wo_stat;
-	struct rusage __user	*wo_rusage;
+	struct rusage		*wo_rusage;
 
 	wait_queue_t		child_wait;
 	int			notask_error;
@@ -1054,8 +1054,10 @@ static int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,
 				pid_t pid, uid_t uid, int why, int status)
 {
 	struct siginfo __user *infop;
-	int retval = wo->wo_rusage
-		? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
+	int retval = 0;
+
+	if (wo->wo_rusage)
+		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
 
 	put_task_struct(p);
 	infop = wo->wo_info;
@@ -1182,8 +1184,9 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		spin_unlock_irq(&current->sighand->siglock);
 	}
 
-	retval = wo->wo_rusage
-		? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
+	if (wo->wo_rusage)
+		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
+	retval = 0;
 	status = (p->signal->flags & SIGNAL_GROUP_EXIT)
 		? p->signal->group_exit_code : p->exit_code;
 	if (!retval && wo->wo_stat)
@@ -1316,8 +1319,9 @@ static int wait_task_stopped(struct wait_opts *wo,
 	if (unlikely(wo->wo_flags & WNOWAIT))
 		return wait_noreap_copyout(wo, p, pid, uid, why, exit_code);
 
-	retval = wo->wo_rusage
-		? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
+	if (wo->wo_rusage)
+		getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
+	retval = 0;
 	if (!retval && wo->wo_stat)
 		retval = put_user((exit_code << 8) | 0x7f, wo->wo_stat);
 
@@ -1377,8 +1381,9 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 	sched_annotate_sleep();
 
 	if (!wo->wo_info) {
-		retval = wo->wo_rusage
-			? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
+		if (wo->wo_rusage)
+			getrusage(p, RUSAGE_BOTH, wo->wo_rusage);
+		retval = 0;
 		put_task_struct(p);
 		if (!retval && wo->wo_stat)
 			retval = put_user(0xffff, wo->wo_stat);
@@ -1618,8 +1623,8 @@ static long do_wait(struct wait_opts *wo)
 	return retval;
 }
 
-SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
-		infop, int, options, struct rusage __user *, ru)
+static long kernel_waitid(int which, pid_t upid, struct siginfo __user *infop,
+			  int options, struct rusage *ru)
 {
 	struct wait_opts wo;
 	struct pid *pid = NULL;
@@ -1687,8 +1692,21 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	return ret;
 }
 
-SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
-		int, options, struct rusage __user *, ru)
+SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
+		infop, int, options, struct rusage __user *, ru)
+{
+	struct rusage r;
+	long err = kernel_waitid(which, upid, infop, options, ru ? &r : NULL);
+
+	if (!err) {
+		if (ru && copy_to_user(ru, &r, sizeof(struct rusage)))
+			return -EFAULT;
+	}
+	return err;
+}
+
+static long kernel_wait4(pid_t upid, int __user *stat_addr,
+			int options, struct rusage *ru)
 {
 	struct wait_opts wo;
 	struct pid *pid = NULL;
@@ -1724,6 +1742,19 @@ SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
 	return ret;
 }
 
+SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
+		int, options, struct rusage __user *, ru)
+{
+	struct rusage r;
+	long err = kernel_wait4(upid, stat_addr, options, ru ? &r : NULL);
+
+	if (err > 0) {
+		if (ru && copy_to_user(ru, &r, sizeof(struct rusage)))
+			return -EFAULT;
+	}
+	return err;
+}
+
 #ifdef __ARCH_WANT_SYS_WAITPID
 
 /*
@@ -1744,29 +1775,13 @@ COMPAT_SYSCALL_DEFINE4(wait4,
 	int, options,
 	struct compat_rusage __user *, ru)
 {
-	if (!ru) {
-		return sys_wait4(pid, stat_addr, options, NULL);
-	} else {
-		struct rusage r;
-		int ret;
-		unsigned int status;
-		mm_segment_t old_fs = get_fs();
-
-		set_fs (KERNEL_DS);
-		ret = sys_wait4(pid,
-				(stat_addr ?
-				 (unsigned int __user *) &status : NULL),
-				options, (struct rusage __user *) &r);
-		set_fs (old_fs);
-
-		if (ret > 0) {
-			if (put_compat_rusage(&r, ru))
-				return -EFAULT;
-			if (stat_addr && put_user(status, stat_addr))
-				return -EFAULT;
-		}
-		return ret;
+	struct rusage r;
+	long err = kernel_wait4(pid, stat_addr, options, ru ? &r : NULL);
+	if (err > 0) {
+		if (ru && put_compat_rusage(&r, ru))
+			return -EFAULT;
 	}
+	return err;
 }
 
 COMPAT_SYSCALL_DEFINE5(waitid,
@@ -1782,8 +1797,8 @@ COMPAT_SYSCALL_DEFINE5(waitid,
 	memset(&info, 0, sizeof(info));
 
 	set_fs(KERNEL_DS);
-	ret = sys_waitid(which, pid, (siginfo_t __user *)&info, options,
-			 uru ? (struct rusage __user *)&ru : NULL);
+	ret = kernel_waitid(which, pid, (siginfo_t __user *)&info, options,
+			 uru ? &ru : NULL);
 	set_fs(old_fs);
 
 	if ((ret < 0) || (info.si_signo == 0))

commit 7e95a225901a5d2fd140f14b4302805cecc22da7
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun May 14 19:52:01 2017 -0400

    move compat wait4 and waitid next to native variants
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 516acdb0e0ec..f98782bd27b6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -62,6 +62,7 @@
 #include <linux/kcov.h>
 #include <linux/random.h>
 #include <linux/rcuwait.h>
+#include <linux/compat.h>
 
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
@@ -1735,3 +1736,71 @@ SYSCALL_DEFINE3(waitpid, pid_t, pid, int __user *, stat_addr, int, options)
 }
 
 #endif
+
+#ifdef CONFIG_COMPAT
+COMPAT_SYSCALL_DEFINE4(wait4,
+	compat_pid_t, pid,
+	compat_uint_t __user *, stat_addr,
+	int, options,
+	struct compat_rusage __user *, ru)
+{
+	if (!ru) {
+		return sys_wait4(pid, stat_addr, options, NULL);
+	} else {
+		struct rusage r;
+		int ret;
+		unsigned int status;
+		mm_segment_t old_fs = get_fs();
+
+		set_fs (KERNEL_DS);
+		ret = sys_wait4(pid,
+				(stat_addr ?
+				 (unsigned int __user *) &status : NULL),
+				options, (struct rusage __user *) &r);
+		set_fs (old_fs);
+
+		if (ret > 0) {
+			if (put_compat_rusage(&r, ru))
+				return -EFAULT;
+			if (stat_addr && put_user(status, stat_addr))
+				return -EFAULT;
+		}
+		return ret;
+	}
+}
+
+COMPAT_SYSCALL_DEFINE5(waitid,
+		int, which, compat_pid_t, pid,
+		struct compat_siginfo __user *, infop, int, options,
+		struct compat_rusage __user *, uru)
+{
+	siginfo_t info;
+	struct rusage ru;
+	long ret;
+	mm_segment_t old_fs = get_fs();
+
+	memset(&info, 0, sizeof(info));
+
+	set_fs(KERNEL_DS);
+	ret = sys_waitid(which, pid, (siginfo_t __user *)&info, options,
+			 uru ? (struct rusage __user *)&ru : NULL);
+	set_fs(old_fs);
+
+	if ((ret < 0) || (info.si_signo == 0))
+		return ret;
+
+	if (uru) {
+		/* sys_waitid() overwrites everything in ru */
+		if (COMPAT_USE_64BIT_TIME)
+			ret = copy_to_user(uru, &ru, sizeof(ru));
+		else
+			ret = put_compat_rusage(&ru, uru);
+		if (ret)
+			return -EFAULT;
+	}
+
+	BUG_ON(info.si_code & __SI_MASK);
+	info.si_code |= __SI_CHLD;
+	return copy_siginfo_to_user32(infop, &info);
+}
+#endif

commit dd0db88d8094a6d9d4d1fc5fcd56ab619f54ccf8
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Mar 9 16:16:49 2017 -0800

    userfaultfd: non-cooperative: rollback userfaultfd_exit
    
    Patch series "userfaultfd non-cooperative further update for 4.11 merge
    window".
    
    Unfortunately I noticed one relevant bug in userfaultfd_exit while doing
    more testing.  I've been doing testing before and this was also tested
    by kbuild bot and exercised by the selftest, but this bug never
    reproduced before.
    
    I dropped userfaultfd_exit as result.  I dropped it because of
    implementation difficulty in receiving signals in __mmput and because I
    think -ENOSPC as result from the background UFFDIO_COPY should be enough
    already.
    
    Before I decided to remove userfaultfd_exit, I noticed userfaultfd_exit
    wasn't exercised by the selftest and when I tried to exercise it, after
    moving it to a more correct place in __mmput where it would make more
    sense and where the vma list is stable, it resulted in the
    event_wait_completion in D state.  So then I added the second patch to
    be sure even if we call userfaultfd_event_wait_completion too late
    during task exit(), we won't risk to generate tasks in D state.  The
    same check exists in handle_userfault() for the same reason, except it
    makes a difference there, while here is just a robustness check and it's
    run under WARN_ON_ONCE.
    
    While looking at the userfaultfd_event_wait_completion() function I
    looked back at its callers too while at it and I think it's not ok to
    stop executing dup_fctx on the fcs list because we relay on
    userfaultfd_event_wait_completion to execute
    userfaultfd_ctx_put(fctx->orig) which is paired against
    userfaultfd_ctx_get(fctx->orig) in dup_userfault just before
    list_add(fcs).  This change only takes care of fctx->orig but this area
    also needs further review looking for similar problems in fctx->new.
    
    The only patch that is urgent is the first because it's an use after
    free during a SMP race condition that affects all processes if
    CONFIG_USERFAULTFD=y.  Very hard to reproduce though and probably
    impossible without SLUB poisoning enabled.
    
    This patch (of 3):
    
    I once reproduced this oops with the userfaultfd selftest, it's not
    easily reproducible and it requires SLUB poisoning to reproduce.
    
        general protection fault: 0000 [#1] SMP
        Modules linked in:
        CPU: 2 PID: 18421 Comm: userfaultfd Tainted: G               ------------ T 3.10.0+ #15
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.10.1-0-g8891697-prebuilt.qemu-project.org 04/01/2014
        task: ffff8801f83b9440 ti: ffff8801f833c000 task.ti: ffff8801f833c000
        RIP: 0010:[<ffffffff81451299>]  [<ffffffff81451299>] userfaultfd_exit+0x29/0xa0
        RSP: 0018:ffff8801f833fe80  EFLAGS: 00010202
        RAX: ffff8801f833ffd8 RBX: 6b6b6b6b6b6b6b6b RCX: ffff8801f83b9440
        RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffff8800baf18600
        RBP: ffff8801f833fee8 R08: 0000000000000000 R09: 0000000000000001
        R10: 0000000000000000 R11: ffffffff8127ceb3 R12: 0000000000000000
        R13: ffff8800baf186b0 R14: ffff8801f83b99f8 R15: 00007faed746c700
        FS:  0000000000000000(0000) GS:ffff88023fc80000(0000) knlGS:0000000000000000
        CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
        CR2: 00007faf0966f028 CR3: 0000000001bc6000 CR4: 00000000000006e0
        DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
        DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
        Call Trace:
          do_exit+0x297/0xd10
          SyS_exit+0x17/0x20
          tracesys+0xdd/0xe2
        Code: 00 00 66 66 66 66 90 55 48 89 e5 41 54 53 48 83 ec 58 48 8b 1f 48 85 db 75 11 eb 73 66 0f 1f 44 00 00 48 8b 5b 10 48 85 db 74 64 <4c> 8b a3 b8 00 00 00 4d 85 e4 74 eb 41 f6 84 24 2c 01 00 00 80
        RIP  [<ffffffff81451299>] userfaultfd_exit+0x29/0xa0
         RSP <ffff8801f833fe80>
        ---[ end trace 9fecd6dcb442846a ]---
    
    In the debugger I located the "mm" pointer in the stack and walking
    mm->mmap->vm_next through the end shows the vma->vm_next list is fully
    consistent and it is null terminated list as expected.  So this has to
    be an SMP race condition where userfaultfd_exit was running while the
    vma list was being modified by another CPU.
    
    When userfaultfd_exit() run one of the ->vm_next pointers pointed to
    SLAB_POISON (RBX is the vma pointer and is 0x6b6b..).
    
    The reason is that it's not running in __mmput but while there are still
    other threads running and it's not holding the mmap_sem (it can't as it
    has to wait the even to be received by the manager).  So this is an use
    after free that was happening for all processes.
    
    One more implementation problem aside from the race condition:
    userfaultfd_exit has really to check a flag in mm->flags before walking
    the vma or it's going to slowdown the exit() path for regular tasks.
    
    One more implementation problem: at that point signals can't be
    delivered so it would also create a task in D state if the manager
    doesn't read the event.
    
    The major design issue: it overall looks superfluous as the manager can
    check for -ENOSPC in the background transfer:
    
            if (mmget_not_zero(ctx->mm)) {
    [..]
            } else {
                    return -ENOSPC;
            }
    
    It's safer to roll it back and re-introduce it later if at all.
    
    [rppt@linux.vnet.ibm.com: documentation fixup after removal of UFFD_EVENT_EXIT]
      Link: http://lkml.kernel.org/r/1488345437-4364-1-git-send-email-rppt@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/20170224181957.19736-2-aarcange@redhat.com
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Pavel Emelyanov <xemul@parallels.com>
    Cc: Hillf Danton <hillf.zj@alibaba-inc.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e126ebf2400c..516acdb0e0ec 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -554,7 +554,6 @@ static void exit_mm(void)
 	enter_lazy_tlb(mm, current);
 	task_unlock(current);
 	mm_update_next_owner(mm);
-	userfaultfd_exit(mm);
 	mmput(mm);
 	if (test_thread_flag(TIF_MEMDIE))
 		exit_oom_victim();

commit 32ef5517c298042ed58408545f475df43afe1f24
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 11:48:36 2017 +0100

    sched/headers: Prepare to move cputime functionality from <linux/sched.h> into <linux/sched/cputime.h>
    
    Introduce a trivial, mostly empty <linux/sched/cputime.h> header
    to prepare for the moving of cputime functionality out of sched.h.
    
    Update all code that relies on these facilities.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 771c33fc9952..e126ebf2400c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -11,6 +11,7 @@
 #include <linux/sched/stat.h>
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
+#include <linux/sched/cputime.h>
 #include <linux/interrupt.h>
 #include <linux/module.h>
 #include <linux/capability.h>

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e53408d156df..771c33fc9952 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -10,6 +10,7 @@
 #include <linux/sched/mm.h>
 #include <linux/sched/stat.h>
 #include <linux/sched/task.h>
+#include <linux/sched/task_stack.h>
 #include <linux/interrupt.h>
 #include <linux/module.h>
 #include <linux/capability.h>

commit 299300258d1bc4e997b7db340a2e06636757fe2e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task.h>
    
    We are going to split <linux/sched/task.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5bad7dce1b7b..e53408d156df 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -9,6 +9,7 @@
 #include <linux/sched/autogroup.h>
 #include <linux/sched/mm.h>
 #include <linux/sched/stat.h>
+#include <linux/sched/task.h>
 #include <linux/interrupt.h>
 #include <linux/module.h>
 #include <linux/capability.h>

commit 03441a3482a31462c93509939a388877e3cd9261
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/stat.h>
    
    We are going to split <linux/sched/stat.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/stat.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 48649ccbb649..5bad7dce1b7b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -8,6 +8,7 @@
 #include <linux/slab.h>
 #include <linux/sched/autogroup.h>
 #include <linux/sched/mm.h>
+#include <linux/sched/stat.h>
 #include <linux/interrupt.h>
 #include <linux/module.h>
 #include <linux/capability.h>

commit 6e84f31522f931027bf695752087ece278c10d3f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:29 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/mm.h>
    
    We are going to split <linux/sched/mm.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/mm.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    The APIs that are going to be moved first are:
    
       mm_alloc()
       __mmdrop()
       mmdrop()
       mmdrop_async_fn()
       mmdrop_async()
       mmget_not_zero()
       mmput()
       mmput_async()
       get_task_mm()
       mm_access()
       mm_release()
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9c0b92833fbb..48649ccbb649 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -7,6 +7,7 @@
 #include <linux/mm.h>
 #include <linux/slab.h>
 #include <linux/sched/autogroup.h>
+#include <linux/sched/mm.h>
 #include <linux/interrupt.h>
 #include <linux/module.h>
 #include <linux/capability.h>

commit 4eb5aaa3af8a54e5e9bac90e2b42bbab1f1ee5a3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:29 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/autogroup.h>
    
    We are going to split <linux/sched/autogroup.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/autogroup.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8a768a3672a5..9c0b92833fbb 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -6,6 +6,7 @@
 
 #include <linux/mm.h>
 #include <linux/slab.h>
+#include <linux/sched/autogroup.h>
 #include <linux/interrupt.h>
 #include <linux/module.h>
 #include <linux/capability.h>

commit f1f1007644ffc8051a4c11427d58b1967ae7b75a
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Mon Feb 27 14:30:07 2017 -0800

    mm: add new mmgrab() helper
    
    Apart from adding the helper function itself, the rest of the kernel is
    converted mechanically using:
    
      git grep -l 'atomic_inc.*mm_count' | xargs sed -i 's/atomic_inc(&\(.*\)->mm_count);/mmgrab\(\1\);/'
      git grep -l 'atomic_inc.*mm_count' | xargs sed -i 's/atomic_inc(&\(.*\)\.mm_count);/mmgrab\(\&\1\);/'
    
    This is needed for a later patch that hooks into the helper, but might
    be a worthwhile cleanup on its own.
    
    (Michal Hocko provided most of the kerneldoc comment.)
    
    Link: http://lkml.kernel.org/r/20161218123229.22952-1-vegard.nossum@oracle.com
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 90b09ca35c84..8a768a3672a5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -539,7 +539,7 @@ static void exit_mm(void)
 		__set_current_state(TASK_RUNNING);
 		down_read(&mm->mmap_sem);
 	}
-	atomic_inc(&mm->mm_count);
+	mmgrab(mm);
 	BUG_ON(mm != current->active_mm);
 	/* more a memory barrier than a real lock */
 	task_lock(current);

commit ca49ca7114553587736fe78319e22f073b631380
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Fri Feb 24 14:58:25 2017 -0800

    userfaultfd: non-cooperative: add event for exit() notification
    
    Allow userfaultfd monitor track termination of the processes that have
    memory backed by the uffd.
    
    [rppt@linux.vnet.ibm.com: add comment]
      Link: http://lkml.kernel.org/r/20170202135448.GB19804@rapoport-lnxLink: http://lkml.kernel.org/r/1485542673-24387-4-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Pavel Emelyanov <xemul@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9960accbf2ab..90b09ca35c84 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -45,6 +45,7 @@
 #include <linux/task_io_accounting_ops.h>
 #include <linux/tracehook.h>
 #include <linux/fs_struct.h>
+#include <linux/userfaultfd_k.h>
 #include <linux/init_task.h>
 #include <linux/perf_event.h>
 #include <trace/events/sched.h>
@@ -547,6 +548,7 @@ static void exit_mm(void)
 	enter_lazy_tlb(mm, current);
 	task_unlock(current);
 	mm_update_next_owner(mm);
+	userfaultfd_exit(mm);
 	mmput(mm);
 	if (test_thread_flag(TIF_MEMDIE))
 		exit_oom_victim();

commit f1ef09fde17f9b77ca1435a5b53a28b203afb81c
Merge: ef96152e6a36 ace0c791e6c3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 23 20:33:51 2017 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull namespace updates from Eric Biederman:
     "There is a lot here. A lot of these changes result in subtle user
      visible differences in kernel behavior. I don't expect anything will
      care but I will revert/fix things immediately if any regressions show
      up.
    
      From Seth Forshee there is a continuation of the work to make the vfs
      ready for unpriviled mounts. We had thought the previous changes
      prevented the creation of files outside of s_user_ns of a filesystem,
      but it turns we missed the O_CREAT path. Ooops.
    
      Pavel Tikhomirov and Oleg Nesterov worked together to fix a long
      standing bug in the implemenation of PR_SET_CHILD_SUBREAPER where only
      children that are forked after the prctl are considered and not
      children forked before the prctl. The only known user of this prctl
      systemd forks all children after the prctl. So no userspace
      regressions will occur. Holding earlier forked children to the same
      rules as later forked children creates a semantic that is sane enough
      to allow checkpoing of processes that use this feature.
    
      There is a long delayed change by Nikolay Borisov to limit inotify
      instances inside a user namespace.
    
      Michael Kerrisk extends the API for files used to maniuplate
      namespaces with two new trivial ioctls to allow discovery of the
      hierachy and properties of namespaces.
    
      Konstantin Khlebnikov with the help of Al Viro adds code that when a
      network namespace exits purges it's sysctl entries from the dcache. As
      in some circumstances this could use a lot of memory.
    
      Vivek Goyal fixed a bug with stacked filesystems where the permissions
      on the wrong inode were being checked.
    
      I continue previous work on ptracing across exec. Allowing a file to
      be setuid across exec while being ptraced if the tracer has enough
      credentials in the user namespace, and if the process has CAP_SETUID
      in it's own namespace. Proc files for setuid or otherwise undumpable
      executables are now owned by the root in the user namespace of their
      mm. Allowing debugging of setuid applications in containers to work
      better.
    
      A bug I introduced with permission checking and automount is now
      fixed. The big change is to mark the mounts that the kernel initiates
      as a result of an automount. This allows the permission checks in sget
      to be safely suppressed for this kind of mount. As the permission
      check happened when the original filesystem was mounted.
    
      Finally a special case in the mount namespace is removed preventing
      unbounded chains in the mount hash table, and making the semantics
      simpler which benefits CRIU.
    
      The vfs fix along with related work in ima and evm I believe makes us
      ready to finish developing and merge fully unprivileged mounts of the
      fuse filesystem. The cleanups of the mount namespace makes discussing
      how to fix the worst case complexity of umount. The stacked filesystem
      fixes pave the way for adding multiple mappings for the filesystem
      uids so that efficient and safer containers can be implemented"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      proc/sysctl: Don't grab i_lock under sysctl_lock.
      vfs: Use upper filesystem inode in bprm_fill_uid()
      proc/sysctl: prune stale dentries during unregistering
      mnt: Tuck mounts under others instead of creating shadow/side mounts.
      prctl: propagate has_child_subreaper flag to every descendant
      introduce the walk_process_tree() helper
      nsfs: Add an ioctl() to return owner UID of a userns
      fs: Better permission checking for submounts
      exit: fix the setns() && PR_SET_CHILD_SUBREAPER interaction
      vfs: open() with O_CREAT should not create inodes with unknown ids
      nsfs: Add an ioctl() to return the namespace type
      proc: Better ownership of files for non-dumpable tasks in user namespaces
      exec: Remove LSM_UNSAFE_PTRACE_CAP
      exec: Test the ptracer's saved cred to see if the tracee can gain caps
      exec: Don't reset euid and egid when the tracee has CAP_SETUID
      inotify: Convert to using per-namespace limits

commit c9341ee0af4df0af8b727873ef851227345defed
Merge: 7a771ceac771 61841be6358c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 21 12:49:56 2017 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security layer updates from James Morris:
     "Highlights:
    
       - major AppArmor update: policy namespaces & lots of fixes
    
       - add /sys/kernel/security/lsm node for easy detection of loaded LSMs
    
       - SELinux cgroupfs labeling support
    
       - SELinux context mounts on tmpfs, ramfs, devpts within user
         namespaces
    
       - improved TPM 2.0 support"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (117 commits)
      tpm: declare tpm2_get_pcr_allocation() as static
      tpm: Fix expected number of response bytes of TPM1.2 PCR Extend
      tpm xen: drop unneeded chip variable
      tpm: fix misspelled "facilitate" in module parameter description
      tpm_tis: fix the error handling of init_tis()
      KEYS: Use memzero_explicit() for secret data
      KEYS: Fix an error code in request_master_key()
      sign-file: fix build error in sign-file.c with libressl
      selinux: allow changing labels for cgroupfs
      selinux: fix off-by-one in setprocattr
      tpm: silence an array overflow warning
      tpm: fix the type of owned field in cap_t
      tpm: add securityfs support for TPM 2.0 firmware event log
      tpm: enhance read_log_of() to support Physical TPM event log
      tpm: enhance TPM 2.0 PCR extend to support multiple banks
      tpm: implement TPM 2.0 capability to get active PCR banks
      tpm: fix RC value check in tpm2_seal_trusted
      tpm_tis: fix iTPM probe via probe_itpm() function
      tpm: Begin the process to deprecate user_read_timer
      tpm: remove tpm_read_index and tpm_write_index from tpm.h
      ...

commit 42e1b14b6e1455ece2ccbe474c25388d0230a590
Merge: 828cad8ea05d 95cb64c1fe61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 13:23:30 2017 -0800

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Implement wraparound-safe refcount_t and kref_t types based on
         generic atomic primitives (Peter Zijlstra)
    
       - Improve and fix the ww_mutex code (Nicolai Hhnle)
    
       - Add self-tests to the ww_mutex code (Chris Wilson)
    
       - Optimize percpu-rwsems with the 'rcuwait' mechanism (Davidlohr
         Bueso)
    
       - Micro-optimize the current-task logic all around the core kernel
         (Davidlohr Bueso)
    
       - Tidy up after recent optimizations: remove stale code and APIs,
         clean up the code (Waiman Long)
    
       - ... plus misc fixes, updates and cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (50 commits)
      fork: Fix task_struct alignment
      locking/spinlock/debug: Remove spinlock lockup detection code
      lockdep: Fix incorrect condition to print bug msgs for MAX_LOCKDEP_CHAIN_HLOCKS
      lkdtm: Convert to refcount_t testing
      kref: Implement 'struct kref' using refcount_t
      refcount_t: Introduce a special purpose refcount type
      sched/wake_q: Clarify queue reinit comment
      sched/wait, rcuwait: Fix typo in comment
      locking/mutex: Fix lockdep_assert_held() fail
      locking/rtmutex: Flip unlikely() branch to likely() in __rt_mutex_slowlock()
      locking/rwsem: Reinit wake_q after use
      locking/rwsem: Remove unnecessary atomic_long_t casts
      jump_labels: Move header guard #endif down where it belongs
      locking/atomic, kref: Implement kref_put_lock()
      locking/ww_mutex: Turn off __must_check for now
      locking/atomic, kref: Avoid more abuse
      locking/atomic, kref: Use kref_get_unless_zero() more
      locking/atomic, kref: Kill kref_sub()
      locking/atomic, kref: Add kref_read()
      locking/atomic, kref: Add KREF_INIT()
      ...

commit 5613fda9a503cd6137b120298902a34a1386b2c1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:23 2017 +0100

    sched/cputime: Convert task/group cputime to nsecs
    
    Now that most cputime readers use the transition API which return the
    task cputime in old style cputime_t, we can safely store the cputime in
    nsecs. This will eventually make cputime statistics less opaque and more
    granular. Back and forth convertions between cputime_t and nsecs in order
    to deal with cputime_t random granularity won't be needed anymore.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-8-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8f14b866f9f6..8e5e21338b3a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -86,7 +86,7 @@ static void __exit_signal(struct task_struct *tsk)
 	bool group_dead = thread_group_leader(tsk);
 	struct sighand_struct *sighand;
 	struct tty_struct *uninitialized_var(tty);
-	cputime_t utime, stime;
+	u64 utime, stime;
 
 	sighand = rcu_dereference_check(tsk->sighand,
 					lockdep_tasklist_lock_is_held());
@@ -1091,7 +1091,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		struct signal_struct *sig = p->signal;
 		struct signal_struct *psig = current->signal;
 		unsigned long maxrss;
-		cputime_t tgutime, tgstime;
+		u64 tgutime, tgstime;
 
 		/*
 		 * The resource counters for the group leader are in its

commit c6c70f4455d1eda91065e93cc4f7eddf4499b105
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jan 30 19:17:35 2017 +0100

    exit: fix the setns() && PR_SET_CHILD_SUBREAPER interaction
    
    find_new_reaper() checks same_thread_group(reaper, child_reaper) to
    prevent the cross-namespace reparenting but this is not enough if the
    exiting parent was injected by setns() + fork().
    
    Suppose we have a process P in the root namespace and some namespace X.
    P does setns() to enter the X namespace, and forks the child C.
    C forks a grandchild G and exits.
    
    The grandchild G should be re-parented to X->child_reaper, but in this
    case the ->real_parent chain does not lead to ->child_reaper, so it will
    be wrongly reparanted to P's sub-reaper or a global init.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8f14b866f9f6..5cfbd595f918 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -578,15 +578,18 @@ static struct task_struct *find_new_reaper(struct task_struct *father,
 		return thread;
 
 	if (father->signal->has_child_subreaper) {
+		unsigned int ns_level = task_pid(father)->level;
 		/*
 		 * Find the first ->is_child_subreaper ancestor in our pid_ns.
-		 * We start from father to ensure we can not look into another
-		 * namespace, this is safe because all its threads are dead.
+		 * We can't check reaper != child_reaper to ensure we do not
+		 * cross the namespaces, the exiting parent could be injected
+		 * by setns() + fork().
+		 * We check pid->level, this is slightly more efficient than
+		 * task_active_pid_ns(reaper) != task_active_pid_ns(father).
 		 */
-		for (reaper = father;
-		     !same_thread_group(reaper, child_reaper);
+		for (reaper = father->real_parent;
+		     task_pid(reaper)->level == ns_level;
 		     reaper = reaper->real_parent) {
-			/* call_usermodehelper() descendants need this check */
 			if (reaper == &init_task)
 				break;
 			if (!reaper->signal->is_child_subreaper)

commit 8f95c90ceb541a38ac16fec48c05142ef1450c25
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Wed Jan 11 07:22:25 2017 -0800

    sched/wait, RCU: Introduce rcuwait machinery
    
    rcuwait provides support for (single) RCU-safe task wait/wake functionality,
    with the caveat that it must not be called after exit_notify(), such that
    we avoid racing with rcu delayed_put_task_struct callbacks, task_struct
    being rcu unaware in this context -- for which we similarly have
    task_rcu_dereference() magic, but with different return semantics, which
    can conflict with the wakeup side.
    
    The interfaces are quite straightforward:
    
      rcuwait_wait_event()
      rcuwait_wake_up()
    
    More details are in the comments, but it's perhaps worth mentioning at least,
    that users must provide proper serialization when waiting on a condition, and
    avoid corrupting a concurrent waiter. Also care must be taken between the task
    and the condition for when calling the wakeup -- we cannot miss wakeups. When
    porting users, this is for example, a given when using waitqueues in that
    everything is done under the q->lock. As such, it can remove sources of non
    preemptable unbounded work for realtime.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Link: http://lkml.kernel.org/r/1484148146-14210-2-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 27c68653e2fc..a9441da69e29 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -55,6 +55,7 @@
 #include <linux/shm.h>
 #include <linux/kcov.h>
 #include <linux/random.h>
+#include <linux/rcuwait.h>
 
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
@@ -282,6 +283,35 @@ struct task_struct *task_rcu_dereference(struct task_struct **ptask)
 	return task;
 }
 
+void rcuwait_wake_up(struct rcuwait *w)
+{
+	struct task_struct *task;
+
+	rcu_read_lock();
+
+	/*
+	 * Order condition vs @task, such that everything prior to the load
+	 * of @task is visible. This is the condition as to why the user called
+	 * rcuwait_trywake() in the first place. Pairs with set_current_state()
+	 * barrier (A) in rcuwait_wait_event().
+	 *
+	 *    WAIT                WAKE
+	 *    [S] tsk = current	  [S] cond = true
+	 *        MB (A)	      MB (B)
+	 *    [L] cond		  [L] tsk
+	 */
+	smp_rmb(); /* (B) */
+
+	/*
+	 * Avoid using task_rcu_dereference() magic as long as we are careful,
+	 * see comment in rcuwait_wait_event() regarding ->exit_state.
+	 */
+	task = rcu_dereference(w->task);
+	if (task)
+		wake_up_process(task);
+	rcu_read_unlock();
+}
+
 struct task_struct *try_get_task_struct(struct task_struct **ptask)
 {
 	struct task_struct *task;

commit 642fa448ae6b3a4e5e8737054a094173405b7643
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Tue Jan 3 13:43:14 2017 -0800

    sched/core: Remove set_task_state()
    
    This is a nasty interface and setting the state of a foreign task must
    not be done. As of the following commit:
    
      be628be0956 ("bcache: Make gc wakeup sane, remove set_task_state()")
    
    ... everyone in the kernel calls set_task_state() with current, allowing
    the helper to be removed.
    
    However, as the comment indicates, it is still around for those archs
    where computing current is more expensive than using a pointer, at least
    in theory. An important arch that is affected is arm64, however this has
    been addressed now [1] and performance is up to par making no difference
    with either calls.
    
    Of all the callers, if any, it's the locking bits that would care most
    about this -- ie: we end up passing a tsk pointer to a lot of the lock
    slowpath, and setting ->state on that. The following numbers are based
    on two tests: a custom ad-hoc microbenchmark that just measures
    latencies (for ~65 million calls) between get_task_state() vs
    get_current_state().
    
    Secondly for a higher overview, an unlink microbenchmark was used,
    which pounds on a single file with open, close,unlink combos with
    increasing thread counts (up to 4x ncpus). While the workload is quite
    unrealistic, it does contend a lot on the inode mutex or now rwsem.
    
    [1] https://lkml.kernel.org/r/1483468021-8237-1-git-send-email-mark.rutland@arm.com
    
    == 1. x86-64 ==
    
    Avg runtime set_task_state():    601 msecs
    Avg runtime set_current_state(): 552 msecs
    
                                                vanilla                 dirty
    Hmean    unlink1-processes-2      36089.26 (  0.00%)    38977.33 (  8.00%)
    Hmean    unlink1-processes-5      28555.01 (  0.00%)    29832.55 (  4.28%)
    Hmean    unlink1-processes-8      37323.75 (  0.00%)    44974.57 ( 20.50%)
    Hmean    unlink1-processes-12     43571.88 (  0.00%)    44283.01 (  1.63%)
    Hmean    unlink1-processes-21     34431.52 (  0.00%)    38284.45 ( 11.19%)
    Hmean    unlink1-processes-30     34813.26 (  0.00%)    37975.17 (  9.08%)
    Hmean    unlink1-processes-48     37048.90 (  0.00%)    39862.78 (  7.59%)
    Hmean    unlink1-processes-79     35630.01 (  0.00%)    36855.30 (  3.44%)
    Hmean    unlink1-processes-110    36115.85 (  0.00%)    39843.91 ( 10.32%)
    Hmean    unlink1-processes-141    32546.96 (  0.00%)    35418.52 (  8.82%)
    Hmean    unlink1-processes-172    34674.79 (  0.00%)    36899.21 (  6.42%)
    Hmean    unlink1-processes-203    37303.11 (  0.00%)    36393.04 ( -2.44%)
    Hmean    unlink1-processes-224    35712.13 (  0.00%)    36685.96 (  2.73%)
    
    == 2. ppc64le ==
    
    Avg runtime set_task_state():  938 msecs
    Avg runtime set_current_state: 940 msecs
    
                                                vanilla                 dirty
    Hmean    unlink1-processes-2      19269.19 (  0.00%)    30704.50 ( 59.35%)
    Hmean    unlink1-processes-5      20106.15 (  0.00%)    21804.15 (  8.45%)
    Hmean    unlink1-processes-8      17496.97 (  0.00%)    17243.28 ( -1.45%)
    Hmean    unlink1-processes-12     14224.15 (  0.00%)    17240.21 ( 21.20%)
    Hmean    unlink1-processes-21     14155.66 (  0.00%)    15681.23 ( 10.78%)
    Hmean    unlink1-processes-30     14450.70 (  0.00%)    15995.83 ( 10.69%)
    Hmean    unlink1-processes-48     16945.57 (  0.00%)    16370.42 ( -3.39%)
    Hmean    unlink1-processes-79     15788.39 (  0.00%)    14639.27 ( -7.28%)
    Hmean    unlink1-processes-110    14268.48 (  0.00%)    14377.40 (  0.76%)
    Hmean    unlink1-processes-141    14023.65 (  0.00%)    16271.69 ( 16.03%)
    Hmean    unlink1-processes-172    13417.62 (  0.00%)    16067.55 ( 19.75%)
    Hmean    unlink1-processes-203    15293.08 (  0.00%)    15440.40 (  0.96%)
    Hmean    unlink1-processes-234    13719.32 (  0.00%)    16190.74 ( 18.01%)
    Hmean    unlink1-processes-265    16400.97 (  0.00%)    16115.22 ( -1.74%)
    Hmean    unlink1-processes-296    14388.60 (  0.00%)    16216.13 ( 12.70%)
    Hmean    unlink1-processes-320    15771.85 (  0.00%)    15905.96 (  0.85%)
    
    x86-64 (known to be fast for get_current()/this_cpu_read_stable() caching)
    and ppc64 (with paca) show similar improvements in the unlink microbenches.
    The small delta for ppc64 (2ms), does not represent the gains on the unlink
    runs. In the case of x86, there was a decent amount of variation in the
    latency runs, but always within a 20 to 50ms increase), ppc was more constant.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Cc: mark.rutland@arm.com
    Link: http://lkml.kernel.org/r/1483479794-14013-5-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2385d434a46e..27c68653e2fc 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -501,12 +501,12 @@ static void exit_mm(void)
 			complete(&core_state->startup);
 
 		for (;;) {
-			set_task_state(current, TASK_UNINTERRUPTIBLE);
+			set_current_state(TASK_UNINTERRUPTIBLE);
 			if (!self.task) /* see coredump_finish() */
 				break;
 			freezable_schedule();
 		}
-		__set_task_state(current, TASK_RUNNING);
+		__set_current_state(TASK_RUNNING);
 		down_read(&mm->mmap_sem);
 	}
 	atomic_inc(&mm->mm_count);

commit 0039962a1473f07fd5c8355bd8264be1eb87eb3e
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Tue Jan 3 13:43:11 2017 -0800

    kernel/exit: Compute 'current' directly
    
    This patch effectively replaces the tsk pointer dereference (which is
    obviously == current), to directly use get_current() macro. In this
    case, do_exit() always passes current to exit_mm(), hence we can
    simply get rid of the argument. This is also a performance win on some
    archs such as x86-64 and ppc64 -- arm64 is no longer an issue.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dave@stgolabs.net
    Cc: mark.rutland@arm.com
    Link: http://lkml.kernel.org/r/1483479794-14013-2-git-send-email-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8f14b866f9f6..2385d434a46e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -468,12 +468,12 @@ void mm_update_next_owner(struct mm_struct *mm)
  * Turn us into a lazy TLB process if we
  * aren't already..
  */
-static void exit_mm(struct task_struct *tsk)
+static void exit_mm(void)
 {
-	struct mm_struct *mm = tsk->mm;
+	struct mm_struct *mm = current->mm;
 	struct core_state *core_state;
 
-	mm_release(tsk, mm);
+	mm_release(current, mm);
 	if (!mm)
 		return;
 	sync_mm_rss(mm);
@@ -491,7 +491,7 @@ static void exit_mm(struct task_struct *tsk)
 
 		up_read(&mm->mmap_sem);
 
-		self.task = tsk;
+		self.task = current;
 		self.next = xchg(&core_state->dumper.next, &self);
 		/*
 		 * Implies mb(), the result of xchg() must be visible
@@ -501,22 +501,22 @@ static void exit_mm(struct task_struct *tsk)
 			complete(&core_state->startup);
 
 		for (;;) {
-			set_task_state(tsk, TASK_UNINTERRUPTIBLE);
+			set_task_state(current, TASK_UNINTERRUPTIBLE);
 			if (!self.task) /* see coredump_finish() */
 				break;
 			freezable_schedule();
 		}
-		__set_task_state(tsk, TASK_RUNNING);
+		__set_task_state(current, TASK_RUNNING);
 		down_read(&mm->mmap_sem);
 	}
 	atomic_inc(&mm->mm_count);
-	BUG_ON(mm != tsk->active_mm);
+	BUG_ON(mm != current->active_mm);
 	/* more a memory barrier than a real lock */
-	task_lock(tsk);
-	tsk->mm = NULL;
+	task_lock(current);
+	current->mm = NULL;
 	up_read(&mm->mmap_sem);
 	enter_lazy_tlb(mm, current);
-	task_unlock(tsk);
+	task_unlock(current);
 	mm_update_next_owner(mm);
 	mmput(mm);
 	if (test_thread_flag(TIF_MEMDIE))
@@ -823,7 +823,7 @@ void __noreturn do_exit(long code)
 	tsk->exit_code = code;
 	taskstats_exit(tsk, group_dead);
 
-	exit_mm(tsk);
+	exit_mm();
 
 	if (group_dead)
 		acct_process();

commit 3a2f5a59a695a73e0cde9a61e0feae5fa730e936
Author: Stephen Smalley <sds@tycho.nsa.gov>
Date:   Tue Jan 10 12:28:32 2017 -0500

    security,selinux,smack: kill security_task_wait hook
    
    As reported by yangshukui, a permission denial from security_task_wait()
    can lead to a soft lockup in zap_pid_ns_processes() since it only expects
    sys_wait4() to return 0 or -ECHILD. Further, security_task_wait() can
    in general lead to zombies; in the absence of some way to automatically
    reparent a child process upon a denial, the hook is not useful.  Remove
    the security hook and its implementations in SELinux and Smack.  Smack
    already removed its check from its hook.
    
    Reported-by: yangshukui <yangshukui@huawei.com>
    Signed-off-by: Stephen Smalley <sds@tycho.nsa.gov>
    Acked-by: Casey Schaufler <casey@schaufler-ca.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Paul Moore <paul@paul-moore.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8f14b866f9f6..60f245190571 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -14,7 +14,6 @@
 #include <linux/tty.h>
 #include <linux/iocontext.h>
 #include <linux/key.h>
-#include <linux/security.h>
 #include <linux/cpu.h>
 #include <linux/acct.h>
 #include <linux/tsacct_kern.h>
@@ -1360,7 +1359,7 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
  * Returns nonzero for a final return, when we have unlocked tasklist_lock.
  * Returns zero if the search for a child should continue;
  * then ->notask_error is 0 if @p is an eligible child,
- * or another error from security_task_wait(), or still -ECHILD.
+ * or still -ECHILD.
  */
 static int wait_consider_task(struct wait_opts *wo, int ptrace,
 				struct task_struct *p)
@@ -1380,20 +1379,6 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 	if (!ret)
 		return ret;
 
-	ret = security_task_wait(p);
-	if (unlikely(ret < 0)) {
-		/*
-		 * If we have not yet seen any eligible child,
-		 * then let this error code replace -ECHILD.
-		 * A permission error will give the user a clue
-		 * to look for security policy problems, rather
-		 * than for mysterious wait bugs.
-		 */
-		if (wo->notask_error)
-			wo->notask_error = ret;
-		return 0;
-	}
-
 	if (unlikely(exit_state == EXIT_TRACE)) {
 		/*
 		 * ptrace == 0 means we are the natural parent. In this case
@@ -1486,7 +1471,7 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
  * Returns nonzero for a final return, when we have unlocked tasklist_lock.
  * Returns zero if the search for a child should continue; then
  * ->notask_error is 0 if there were any eligible children,
- * or another error from security_task_wait(), or still -ECHILD.
+ * or still -ECHILD.
  */
 static int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)
 {

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index aacff8e2aec0..8f14b866f9f6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -56,7 +56,7 @@
 #include <linux/kcov.h>
 #include <linux/random.h>
 
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/unistd.h>
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>

commit 9465d9cc31fa732089cd8bec9f1bdfcdc174a5ce
Merge: e71c3978d6f9 c029a2bec66e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 12 19:56:15 2016 -0800

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "The time/timekeeping/timer folks deliver with this update:
    
       - Fix a reintroduced signed/unsigned issue and cleanup the whole
         signed/unsigned mess in the timekeeping core so this wont happen
         accidentaly again.
    
       - Add a new trace clock based on boot time
    
       - Prevent injection of random sleep times when PM tracing abuses the
         RTC for storage
    
       - Make posix timers configurable for real tiny systems
    
       - Add tracepoints for the alarm timer subsystem so timer based
         suspend wakeups can be instrumented
    
       - The usual pile of fixes and updates to core and drivers"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      timekeeping: Use mul_u64_u32_shr() instead of open coding it
      timekeeping: Get rid of pointless typecasts
      timekeeping: Make the conversion call chain consistently unsigned
      timekeeping_Force_unsigned_clocksource_to_nanoseconds_conversion
      alarmtimer: Add tracepoints for alarm timers
      trace: Update documentation for mono, mono_raw and boot clock
      trace: Add an option for boot clock as trace clock
      timekeeping: Add a fast and NMI safe boot clock
      timekeeping/clocksource_cyc2ns: Document intended range limitation
      timekeeping: Ignore the bogus sleep time if pm_trace is enabled
      selftests/timers: Fix spelling mistake "Asyncrhonous" -> "Asynchronous"
      clocksource/drivers/bcm2835_timer: Unmap region obtained by of_iomap
      clocksource/drivers/arm_arch_timer: Map frame with of_io_request_and_map()
      arm64: dts: rockchip: Arch counter doesn't tick in system suspend
      clocksource/drivers/arm_arch_timer: Don't assume clock runs in suspend
      posix-timers: Make them configurable
      posix_cpu_timers: Move the add_device_randomness() call to a proper place
      timer: Move sys_alarm from timer.c to itimer.c
      ptp_clock: Allow for it to be optional
      Kconfig: Regenerate *.c_shipped files after previous changes
      ...

commit 8e5bfa8c1f8471aa4a2d30be631ef2b50e10abaf
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Nov 14 19:46:12 2016 +0100

    sched/autogroup: Do not use autogroup->tg in zombie threads
    
    Exactly because for_each_thread() in autogroup_move_group() can't see it
    and update its ->sched_task_group before _put() and possibly free().
    
    So the exiting task needs another sched_move_task() before exit_notify()
    and we need to re-introduce the PF_EXITING (or similar) check removed by
    the previous change for another reason.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: hartsjc@redhat.com
    Cc: vbendel@redhat.com
    Cc: vlovejoy@redhat.com
    Link: http://lkml.kernel.org/r/20161114184612.GA15968@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9d68c45ebbe3..3076f3089919 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -836,6 +836,7 @@ void __noreturn do_exit(long code)
 	 */
 	perf_event_exit_task(tsk);
 
+	sched_autogroup_exit_task(tsk);
 	cgroup_exit(tsk);
 
 	/*

commit baa73d9e478ff32d62f3f9422822b59dd9a95a21
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Nov 11 00:10:10 2016 -0500

    posix-timers: Make them configurable
    
    Some embedded systems have no use for them.  This removes about
    25KB from the kernel binary size when configured out.
    
    Corresponding syscalls are routed to a stub logging the attempt to
    use those syscalls which should be enough of a clue if they were
    disabled without proper consideration. They are: timer_create,
    timer_gettime: timer_getoverrun, timer_settime, timer_delete,
    clock_adjtime, setitimer, getitimer, alarm.
    
    The clock_settime, clock_gettime, clock_getres and clock_nanosleep
    syscalls are replaced by simple wrappers compatible with CLOCK_REALTIME,
    CLOCK_MONOTONIC and CLOCK_BOOTTIME only which should cover the vast
    majority of use cases with very little code.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Richard Cochran <richardcochran@gmail.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>
    Cc: Paul Bolle <pebolle@tiscali.nl>
    Cc: linux-kbuild@vger.kernel.org
    Cc: netdev@vger.kernel.org
    Cc: Michal Marek <mmarek@suse.com>
    Cc: Edward Cree <ecree@solarflare.com>
    Link: http://lkml.kernel.org/r/1478841010-28605-7-git-send-email-nicolas.pitre@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/exit.c b/kernel/exit.c
index d16bcdd89dbe..684de019b674 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -92,11 +92,10 @@ static void __exit_signal(struct task_struct *tsk)
 					lockdep_tasklist_lock_is_held());
 	spin_lock(&sighand->siglock);
 
+#ifdef CONFIG_POSIX_TIMERS
 	posix_cpu_timers_exit(tsk);
 	if (group_dead) {
 		posix_cpu_timers_exit_group(tsk);
-		tty = sig->tty;
-		sig->tty = NULL;
 	} else {
 		/*
 		 * This can only happen if the caller is de_thread().
@@ -105,7 +104,13 @@ static void __exit_signal(struct task_struct *tsk)
 		 */
 		if (unlikely(has_group_leader_pid(tsk)))
 			posix_cpu_timers_exit_group(tsk);
+	}
+#endif
 
+	if (group_dead) {
+		tty = sig->tty;
+		sig->tty = NULL;
+	} else {
 		/*
 		 * If there is any task waiting for the group exit
 		 * then notify it:
@@ -803,8 +808,10 @@ void __noreturn do_exit(long code)
 	acct_update_integrals(tsk);
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
+#ifdef CONFIG_POSIX_TIMERS
 		hrtimer_cancel(&tsk->signal->real_timer);
 		exit_itimers(tsk->signal);
+#endif
 		if (tsk->mm)
 			setmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);
 	}

commit 53d3eaa31508222e445b489f3c3ac4c63542a4ef
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Nov 11 00:10:09 2016 -0500

    posix_cpu_timers: Move the add_device_randomness() call to a proper place
    
    There is no logical relation between add_device_randomness() and
    posix_cpu_timers_exit(). Let's move the former to where the later
    is called. This way, when posix-cpu-timers.c is compiled out, there
    is no need to worry about not losing a call to add_device_randomness().
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: John Stultz <john.stultz@linaro.org>
    Cc: Paul Bolle <pebolle@tiscali.nl>
    Cc: linux-kbuild@vger.kernel.org
    Cc: netdev@vger.kernel.org
    Cc: Richard Cochran <richardcochran@gmail.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Michal Marek <mmarek@suse.com>
    Cc: Edward Cree <ecree@solarflare.com>
    Link: http://lkml.kernel.org/r/1478841010-28605-6-git-send-email-nicolas.pitre@linaro.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9d68c45ebbe3..d16bcdd89dbe 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -54,6 +54,7 @@
 #include <linux/writeback.h>
 #include <linux/shm.h>
 #include <linux/kcov.h>
+#include <linux/random.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -116,6 +117,9 @@ static void __exit_signal(struct task_struct *tsk)
 			sig->curr_target = next_thread(tsk);
 	}
 
+	add_device_randomness((const void*) &tsk->se.sum_exec_runtime,
+			      sizeof(unsigned long long));
+
 	/*
 	 * Accumulate here the counters for all threads as they die. We could
 	 * skip the group leader because it is the last user of signal_struct,

commit 38531201c12144cd7d96abfdfe7449c2b01375e8
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Fri Oct 7 16:59:03 2016 -0700

    mm, oom: enforce exit_oom_victim on current task
    
    There are no users of exit_oom_victim on !current task anymore so enforce
    the API to always work on the current.
    
    Link: http://lkml.kernel.org/r/1472119394-11342-8-git-send-email-mhocko@kernel.org
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1e1d913914c0..9d68c45ebbe3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -511,7 +511,7 @@ static void exit_mm(struct task_struct *tsk)
 	mm_update_next_owner(mm);
 	mmput(mm);
 	if (test_thread_flag(TIF_MEMDIE))
-		exit_oom_victim(tsk);
+		exit_oom_victim();
 }
 
 static struct task_struct *find_alive_thread(struct task_struct *p)

commit 9af6528ee9b682df7f29dbee86fbba0b67eab944
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 13 18:37:29 2016 +0200

    sched/core: Optimize __schedule()
    
    Oleg noted that by making do_exit() use __schedule() for the TASK_DEAD
    context switch, we can avoid the TASK_DEAD special case currently in
    __schedule() because that avoids the extra preempt_disable() from
    schedule().
    
    In order to facilitate this, create a do_task_dead() helper which we
    place in the scheduler code, such that it can access __schedule().
    
    Also add some __noreturn annotations to the functions, there's no
    coming back from do_exit().
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Cheng Chao <cs.os.kernel@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: chris@chris-wilson.co.uk
    Cc: tj@kernel.org
    Link: http://lkml.kernel.org/r/20160913163729.GB5012@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 091a78be3b09..1e1d913914c0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -725,7 +725,7 @@ static void check_stack_usage(void)
 static inline void check_stack_usage(void) {}
 #endif
 
-void do_exit(long code)
+void __noreturn do_exit(long code)
 {
 	struct task_struct *tsk = current;
 	int group_dead;
@@ -882,29 +882,7 @@ void do_exit(long code)
 	exit_rcu();
 	TASKS_RCU(__srcu_read_unlock(&tasks_rcu_exit_srcu, tasks_rcu_i));
 
-	/*
-	 * The setting of TASK_RUNNING by try_to_wake_up() may be delayed
-	 * when the following two conditions become true.
-	 *   - There is race condition of mmap_sem (It is acquired by
-	 *     exit_mm()), and
-	 *   - SMI occurs before setting TASK_RUNINNG.
-	 *     (or hypervisor of virtual machine switches to other guest)
-	 *  As a result, we may become TASK_RUNNING after becoming TASK_DEAD
-	 *
-	 * To avoid it, we have to wait for releasing tsk->pi_lock which
-	 * is held by try_to_wake_up()
-	 */
-	smp_mb();
-	raw_spin_unlock_wait(&tsk->pi_lock);
-
-	/* causes final put_task_struct in finish_task_switch(). */
-	tsk->state = TASK_DEAD;
-	tsk->flags |= PF_NOFREEZE;	/* tell freezer to ignore us */
-	schedule();
-	BUG();
-	/* Avoid "noreturn function does return".  */
-	for (;;)
-		cpu_relax();	/* For when BUG is null */
+	do_task_dead();
 }
 EXPORT_SYMBOL_GPL(do_exit);
 

commit c11600e4fed67ae4cd6a8096936afd445410e8ed
Author: David Rientjes <rientjes@google.com>
Date:   Thu Sep 1 16:15:07 2016 -0700

    mm, mempolicy: task->mempolicy must be NULL before dropping final reference
    
    KASAN allocates memory from the page allocator as part of
    kmem_cache_free(), and that can reference current->mempolicy through any
    number of allocation functions.  It needs to be NULL'd out before the
    final reference is dropped to prevent a use-after-free bug:
    
            BUG: KASAN: use-after-free in alloc_pages_current+0x363/0x370 at addr ffff88010b48102c
            CPU: 0 PID: 15425 Comm: trinity-c2 Not tainted 4.8.0-rc2+ #140
            ...
            Call Trace:
                    dump_stack
                    kasan_object_err
                    kasan_report_error
                    __asan_report_load2_noabort
                    alloc_pages_current     <-- use after free
                    depot_save_stack
                    save_stack
                    kasan_slab_free
                    kmem_cache_free
                    __mpol_put              <-- free
                    do_exit
    
    This patch sets current->mempolicy to NULL before dropping the final
    reference.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1608301442180.63329@chino.kir.corp.google.com
    Fixes: cd11016e5f52 ("mm, kasan: stackdepot implementation. Enable stackdepot for SLAB")
    Signed-off-by: David Rientjes <rientjes@google.com>
    Reported-by: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: <stable@vger.kernel.org>    [4.6+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2f974ae042a6..091a78be3b09 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -848,12 +848,7 @@ void do_exit(long code)
 	TASKS_RCU(preempt_enable());
 	exit_notify(tsk, group_dead);
 	proc_exit_connector(tsk);
-#ifdef CONFIG_NUMA
-	task_lock(tsk);
-	mpol_put(tsk->mempolicy);
-	tsk->mempolicy = NULL;
-	task_unlock(tsk);
-#endif
+	mpol_put_task_policy(tsk);
 #ifdef CONFIG_FUTEX
 	if (unlikely(current->pi_state_cache))
 		kfree(current->pi_state_cache);

commit 627393d44860386e948bb63a8e5b53f2cc44d070
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Aug 2 14:05:40 2016 -0700

    kernel/exit.c: quieten greatest stack depth printk
    
    Many targets enable CONFIG_DEBUG_STACK_USAGE, and while the information
    is useful, it isn't worthy of pr_warn().  Reduce it to pr_info().
    
    Link: http://lkml.kernel.org/r/1466982072-29836-1-git-send-email-anton@ozlabs.org
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 84ae830234f8..2f974ae042a6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -715,7 +715,7 @@ static void check_stack_usage(void)
 
 	spin_lock(&low_water_lock);
 	if (free < lowest_to_date) {
-		pr_warn("%s (%d) used greatest stack depth: %lu bytes left\n",
+		pr_info("%s (%d) used greatest stack depth: %lu bytes left\n",
 			current->comm, task_pid_nr(current), free);
 		lowest_to_date = free;
 	}

commit cca08cd66ce6cc37812b6b36986ba7eaabd33e0b
Merge: 7e4dc77b2869 748c7201e622
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 13:59:34 2016 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - introduce and use task_rcu_dereference()/try_get_task_struct() to fix
       and generalize task_struct handling (Oleg Nesterov)
    
     - do various per entity load tracking (PELT) fixes and optimizations
       (Peter Zijlstra)
    
     - cputime virt-steal time accounting enhancements/fixes (Wanpeng Li)
    
     - introduce consolidated cputime output file cpuacct.usage_all and
       related refactorings (Zhao Lei)
    
     - ... plus misc fixes and enhancements
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/core: Panic on scheduling while atomic bugs if kernel.panic_on_warn is set
      sched/cpuacct: Introduce cpuacct.usage_all to show all CPU stats together
      sched/cpuacct: Use loop to consolidate code in cpuacct_stats_show()
      sched/cpuacct: Merge cpuacct_usage_index and cpuacct_stat_index enums
      sched/fair: Rework throttle_count sync
      sched/core: Fix sched_getaffinity() return value kerneldoc comment
      sched/fair: Reorder cgroup creation code
      sched/fair: Apply more PELT fixes
      sched/fair: Fix PELT integrity for new tasks
      sched/cgroup: Fix cpu_cgroup_fork() handling
      sched/fair: Fix PELT integrity for new groups
      sched/fair: Fix and optimize the fork() path
      sched/cputime: Add steal time support to full dynticks CPU time accounting
      sched/cputime: Fix prev steal time accouting during CPU hotplug
      KVM: Fix steal clock warp during guest CPU hotplug
      sched/debug: Always show 'nr_migrations'
      sched/fair: Use task_rcu_dereference()
      sched/api: Introduce task_rcu_dereference() and try_get_task_struct()
      sched/idle: Optimize the generic idle loop
      sched/fair: Fix the wrong throttled clock time for cfs_rq_clock_task()

commit be3e7844980352756de4261b276ee2ba5be7a26b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 24 14:45:21 2016 +0200

    locking/spinlock: Update spin_unlock_wait() users
    
    With the modified semantics of spin_unlock_wait() a number of
    explicit barriers can be removed. Also update the comment for the
    do_exit() usecase, as that was somewhat stale/obscure.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9e6e1356e6bb..0b40791b9e70 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -700,10 +700,14 @@ void do_exit(long code)
 
 	exit_signals(tsk);  /* sets PF_EXITING */
 	/*
-	 * tsk->flags are checked in the futex code to protect against
-	 * an exiting task cleaning up the robust pi futexes.
+	 * Ensure that all new tsk->pi_lock acquisitions must observe
+	 * PF_EXITING. Serializes against futex.c:attach_to_pi_owner().
 	 */
 	smp_mb();
+	/*
+	 * Ensure that we must observe the pi_state in exit_mm() ->
+	 * mm_release() -> exit_pi_state_list().
+	 */
 	raw_spin_unlock_wait(&tsk->pi_lock);
 
 	if (unlikely(in_atomic())) {

commit 150593bf869393d10a79f6bd3df2585ecc20a9bb
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 18 19:02:18 2016 +0200

    sched/api: Introduce task_rcu_dereference() and try_get_task_struct()
    
    Generally task_struct is only protected by RCU if it was found on a
    RCU protected list (say, for_each_process() or find_task_by_vpid()).
    
    As Kirill pointed out rq->curr isn't protected by RCU, the scheduler
    drops the (potentially) last reference without RCU gp, this means
    that we need to fix the code which uses foreign_rq->curr under
    rcu_read_lock().
    
    Add a new helper which can be used to dereference rq->curr or any
    other pointer to task_struct assuming that it should be cleared or
    updated before the final put_task_struct(). It returns non-NULL
    only if this task can't go away before rcu_read_unlock().
    
    ( Also add try_get_task_struct() to make it easier to use this API
      correctly. )
    
    Suggested-by: Kirill Tkhai <ktkhai@parallels.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    [ Updated comments; added try_get_task_struct()]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Link: http://lkml.kernel.org/r/20160518170218.GY3192@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9e6e1356e6bb..2fb4d44c51b1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -210,6 +210,82 @@ void release_task(struct task_struct *p)
 		goto repeat;
 }
 
+/*
+ * Note that if this function returns a valid task_struct pointer (!NULL)
+ * task->usage must remain >0 for the duration of the RCU critical section.
+ */
+struct task_struct *task_rcu_dereference(struct task_struct **ptask)
+{
+	struct sighand_struct *sighand;
+	struct task_struct *task;
+
+	/*
+	 * We need to verify that release_task() was not called and thus
+	 * delayed_put_task_struct() can't run and drop the last reference
+	 * before rcu_read_unlock(). We check task->sighand != NULL,
+	 * but we can read the already freed and reused memory.
+	 */
+retry:
+	task = rcu_dereference(*ptask);
+	if (!task)
+		return NULL;
+
+	probe_kernel_address(&task->sighand, sighand);
+
+	/*
+	 * Pairs with atomic_dec_and_test() in put_task_struct(). If this task
+	 * was already freed we can not miss the preceding update of this
+	 * pointer.
+	 */
+	smp_rmb();
+	if (unlikely(task != READ_ONCE(*ptask)))
+		goto retry;
+
+	/*
+	 * We've re-checked that "task == *ptask", now we have two different
+	 * cases:
+	 *
+	 * 1. This is actually the same task/task_struct. In this case
+	 *    sighand != NULL tells us it is still alive.
+	 *
+	 * 2. This is another task which got the same memory for task_struct.
+	 *    We can't know this of course, and we can not trust
+	 *    sighand != NULL.
+	 *
+	 *    In this case we actually return a random value, but this is
+	 *    correct.
+	 *
+	 *    If we return NULL - we can pretend that we actually noticed that
+	 *    *ptask was updated when the previous task has exited. Or pretend
+	 *    that probe_slab_address(&sighand) reads NULL.
+	 *
+	 *    If we return the new task (because sighand is not NULL for any
+	 *    reason) - this is fine too. This (new) task can't go away before
+	 *    another gp pass.
+	 *
+	 *    And note: We could even eliminate the false positive if re-read
+	 *    task->sighand once again to avoid the falsely NULL. But this case
+	 *    is very unlikely so we don't care.
+	 */
+	if (!sighand)
+		return NULL;
+
+	return task;
+}
+
+struct task_struct *try_get_task_struct(struct task_struct **ptask)
+{
+	struct task_struct *task;
+
+	rcu_read_lock();
+	task = task_rcu_dereference(ptask);
+	if (task)
+		get_task_struct(task);
+	rcu_read_unlock();
+
+	return task;
+}
+
 /*
  * Determine if a process group is "orphaned", according to the POSIX
  * definition in 2.2.2.52.  Orphaned process groups are not to be affected

commit 91c4e8ea8f05916df0c8a6f383508ac7c9e10dba
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon May 23 16:23:53 2016 -0700

    wait: allow sys_waitid() to accept __WNOTHREAD/__WCLONE/__WALL
    
    I see no reason why waitid() can't support other linux-specific flags
    allowed in sys_wait4().
    
    In particular this change can help if we reconsider the previous change
    ("wait/ptrace: assume __WALL if the child is traced") which adds the
    "automagical" __WALL for debugger.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: "Michael Kerrisk (man-pages)" <mtk.manpages@gmail.com>
    Cc: Pedro Alves <palves@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: <syzkaller@googlegroups.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 44fbe6edd7fe..9e6e1356e6bb 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1535,7 +1535,8 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	enum pid_type type;
 	long ret;
 
-	if (options & ~(WNOHANG|WNOWAIT|WEXITED|WSTOPPED|WCONTINUED))
+	if (options & ~(WNOHANG|WNOWAIT|WEXITED|WSTOPPED|WCONTINUED|
+			__WNOTHREAD|__WCLONE|__WALL))
 		return -EINVAL;
 	if (!(options & (WEXITED|WSTOPPED|WCONTINUED)))
 		return -EINVAL;

commit bf959931ddb88c4e4366e96dd22e68fa0db9527c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon May 23 16:23:50 2016 -0700

    wait/ptrace: assume __WALL if the child is traced
    
    The following program (simplified version of generated by syzkaller)
    
            #include <pthread.h>
            #include <unistd.h>
            #include <sys/ptrace.h>
            #include <stdio.h>
            #include <signal.h>
    
            void *thread_func(void *arg)
            {
                    ptrace(PTRACE_TRACEME, 0,0,0);
                    return 0;
            }
    
            int main(void)
            {
                    pthread_t thread;
    
                    if (fork())
                            return 0;
    
                    while (getppid() != 1)
                            ;
    
                    pthread_create(&thread, NULL, thread_func, NULL);
                    pthread_join(thread, NULL);
                    return 0;
            }
    
    creates an unreapable zombie if /sbin/init doesn't use __WALL.
    
    This is not a kernel bug, at least in a sense that everything works as
    expected: debugger should reap a traced sub-thread before it can reap the
    leader, but without __WALL/__WCLONE do_wait() ignores sub-threads.
    
    Unfortunately, it seems that /sbin/init in most (all?) distributions
    doesn't use it and we have to change the kernel to avoid the problem.
    Note also that most init's use sys_waitid() which doesn't allow __WALL, so
    the necessary user-space fix is not that trivial.
    
    This patch just adds the "ptrace" check into eligible_child().  To some
    degree this matches the "tsk->ptrace" in exit_notify(), ->exit_signal is
    mostly ignored when the tracee reports to debugger.  Or WSTOPPED, the
    tracer doesn't need to set this flag to wait for the stopped tracee.
    
    This obviously means the user-visible change: __WCLONE and __WALL no
    longer have any meaning for debugger.  And I can only hope that this won't
    break something, but at least strace/gdb won't suffer.
    
    We could make a more conservative change.  Say, we can take __WCLONE into
    account, or !thread_group_leader().  But it would be nice to not
    complicate these historical/confusing checks.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: "Michael Kerrisk (man-pages)" <mtk.manpages@gmail.com>
    Cc: Pedro Alves <palves@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: <syzkaller@googlegroups.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 75b34fe835b2..44fbe6edd7fe 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -918,17 +918,28 @@ static int eligible_pid(struct wait_opts *wo, struct task_struct *p)
 		task_pid_type(p, wo->wo_type) == wo->wo_pid;
 }
 
-static int eligible_child(struct wait_opts *wo, struct task_struct *p)
+static int
+eligible_child(struct wait_opts *wo, bool ptrace, struct task_struct *p)
 {
 	if (!eligible_pid(wo, p))
 		return 0;
-	/* Wait for all children (clone and not) if __WALL is set;
-	 * otherwise, wait for clone children *only* if __WCLONE is
-	 * set; otherwise, wait for non-clone children *only*.  (Note:
-	 * A "clone" child here is one that reports to its parent
-	 * using a signal other than SIGCHLD.) */
-	if (((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))
-	    && !(wo->wo_flags & __WALL))
+
+	/*
+	 * Wait for all children (clone and not) if __WALL is set or
+	 * if it is traced by us.
+	 */
+	if (ptrace || (wo->wo_flags & __WALL))
+		return 1;
+
+	/*
+	 * Otherwise, wait for clone children *only* if __WCLONE is set;
+	 * otherwise, wait for non-clone children *only*.
+	 *
+	 * Note: a "clone" child here is one that reports to its parent
+	 * using a signal other than SIGCHLD, or a non-leader thread which
+	 * we can only see if it is traced by us.
+	 */
+	if ((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))
 		return 0;
 
 	return 1;
@@ -1300,7 +1311,7 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 	if (unlikely(exit_state == EXIT_DEAD))
 		return 0;
 
-	ret = eligible_child(wo, p);
+	ret = eligible_child(wo, ptrace, p);
 	if (!ret)
 		return ret;
 

commit e64646946ed32902fd597fa6e514b1da84642de3
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 20 17:00:20 2016 -0700

    exit_thread: accept a task parameter to be exited
    
    We need to call exit_thread from copy_process in a fail path.  So make it
    accept task_struct as a parameter.
    
    [v2]
    * s390: exit_thread_runtime_instr doesn't make sense to be called for
      non-current tasks.
    * arm: fix the comment in vfp_thread_copy
    * change 'me' to 'tsk' for task_struct
    * now we can change only archs that actually have exit_thread
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index fd90195667e1..75b34fe835b2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -746,7 +746,7 @@ void do_exit(long code)
 		disassociate_ctty(1);
 	exit_task_namespaces(tsk);
 	exit_task_work(tsk);
-	exit_thread();
+	exit_thread(tsk);
 
 	/*
 	 * Flush inherited counters to the parent - before the parent

commit 36324a990cf578b57828c04cd85ac62cd25cf5a4
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Mar 25 14:20:27 2016 -0700

    oom: clear TIF_MEMDIE after oom_reaper managed to unmap the address space
    
    When oom_reaper manages to unmap all the eligible vmas there shouldn't
    be much of the freable memory held by the oom victim left anymore so it
    makes sense to clear the TIF_MEMDIE flag for the victim and allow the
    OOM killer to select another task.
    
    The lack of TIF_MEMDIE also means that the victim cannot access memory
    reserves anymore but that shouldn't be a problem because it would get
    the access again if it needs to allocate and hits the OOM killer again
    due to the fatal_signal_pending resp.  PF_EXITING check.  We can safely
    hide the task from the OOM killer because it is clearly not a good
    candidate anymore as everyhing reclaimable has been torn down already.
    
    This patch will allow to cap the time an OOM victim can keep TIF_MEMDIE
    and thus hold off further global OOM killer actions granted the oom
    reaper is able to take mmap_sem for the associated mm struct.  This is
    not guaranteed now but further steps should make sure that mmap_sem for
    write should be blocked killable which will help to reduce such a lock
    contention.  This is not done by this patch.
    
    Note that exit_oom_victim might be called on a remote task from
    __oom_reap_task now so we have to check and clear the flag atomically
    otherwise we might race and underflow oom_victims or wake up waiters too
    early.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Suggested-by: Johannes Weiner <hannes@cmpxchg.org>
    Suggested-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Andrea Argangeli <andrea@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 953d1a1c0387..fd90195667e1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -435,7 +435,7 @@ static void exit_mm(struct task_struct *tsk)
 	mm_update_next_owner(mm);
 	mmput(mm);
 	if (test_thread_flag(TIF_MEMDIE))
-		exit_oom_victim();
+		exit_oom_victim(tsk);
 }
 
 static struct task_struct *find_alive_thread(struct task_struct *p)

commit 5c9a8750a6409c63a0f01d51a9024861022f6593
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Tue Mar 22 14:27:30 2016 -0700

    kernel: add kcov code coverage
    
    kcov provides code coverage collection for coverage-guided fuzzing
    (randomized testing).  Coverage-guided fuzzing is a testing technique
    that uses coverage feedback to determine new interesting inputs to a
    system.  A notable user-space example is AFL
    (http://lcamtuf.coredump.cx/afl/).  However, this technique is not
    widely used for kernel testing due to missing compiler and kernel
    support.
    
    kcov does not aim to collect as much coverage as possible.  It aims to
    collect more or less stable coverage that is function of syscall inputs.
    To achieve this goal it does not collect coverage in soft/hard
    interrupts and instrumentation of some inherently non-deterministic or
    non-interesting parts of kernel is disbled (e.g.  scheduler, locking).
    
    Currently there is a single coverage collection mode (tracing), but the
    API anticipates additional collection modes.  Initially I also
    implemented a second mode which exposes coverage in a fixed-size hash
    table of counters (what Quentin used in his original patch).  I've
    dropped the second mode for simplicity.
    
    This patch adds the necessary support on kernel side.  The complimentary
    compiler support was added in gcc revision 231296.
    
    We've used this support to build syzkaller system call fuzzer, which has
    found 90 kernel bugs in just 2 months:
    
      https://github.com/google/syzkaller/wiki/Found-Bugs
    
    We've also found 30+ bugs in our internal systems with syzkaller.
    Another (yet unexplored) direction where kcov coverage would greatly
    help is more traditional "blob mutation".  For example, mounting a
    random blob as a filesystem, or receiving a random blob over wire.
    
    Why not gcov.  Typical fuzzing loop looks as follows: (1) reset
    coverage, (2) execute a bit of code, (3) collect coverage, repeat.  A
    typical coverage can be just a dozen of basic blocks (e.g.  an invalid
    input).  In such context gcov becomes prohibitively expensive as
    reset/collect coverage steps depend on total number of basic
    blocks/edges in program (in case of kernel it is about 2M).  Cost of
    kcov depends only on number of executed basic blocks/edges.  On top of
    that, kernel requires per-thread coverage because there are always
    background threads and unrelated processes that also produce coverage.
    With inlined gcov instrumentation per-thread coverage is not possible.
    
    kcov exposes kernel PCs and control flow to user-space which is
    insecure.  But debugfs should not be mapped as user accessible.
    
    Based on a patch by Quentin Casasnovas.
    
    [akpm@linux-foundation.org: make task_struct.kcov_mode have type `enum kcov_mode']
    [akpm@linux-foundation.org: unbreak allmodconfig]
    [akpm@linux-foundation.org: follow x86 Makefile layout standards]
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Cc: syzkaller <syzkaller@googlegroups.com>
    Cc: Vegard Nossum <vegard.nossum@oracle.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Tavis Ormandy <taviso@google.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Cc: Kostya Serebryany <kcc@google.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Kees Cook <keescook@google.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: David Drysdale <drysdale@google.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 10e088237fed..953d1a1c0387 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -53,6 +53,7 @@
 #include <linux/oom.h>
 #include <linux/writeback.h>
 #include <linux/shm.h>
+#include <linux/kcov.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -655,6 +656,7 @@ void do_exit(long code)
 	TASKS_RCU(int tasks_rcu_i);
 
 	profile_task_exit(tsk);
+	kcov_task_exit(tsk);
 
 	WARN_ON(blk_needs_flush_plug(tsk));
 

commit c428fbdbf3e9515bfe686881ffdba862dbd8cb6f
Author: Dmitry Safonov <0x7f454c46@gmail.com>
Date:   Wed Jan 20 15:00:10 2016 -0800

    exit: remove unneeded declaration of exit_mm()
    
    Signed-off-by: Dmitry Safonov <0x7f454c46@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b0eea830303c..10e088237fed 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -59,8 +59,6 @@
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
 
-static void exit_mm(struct task_struct *tsk);
-
 static void __unhash_process(struct task_struct *p, bool group_dead)
 {
 	nr_threads--;

commit 570ac9337b5c13dbf46ca6758c376e2e13e8956f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jan 20 14:59:58 2016 -0800

    ptrace: task_stopped_code(ptrace => true) can't see TASK_STOPPED task
    
    task_stopped_code()->task_is_stopped_or_traced() doesn't look right, the
    traced task must never be TASK_STOPPED.
    
    We can not add WARN_ON(task_is_stopped(p)), but this is only because
    do_wait() can race with PTRACE_ATTACH from another thread.
    
    [akpm@linux-foundation.org: teeny cleanup]
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Pedro Alves <palves@redhat.com>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 07110c6020a0..b0eea830303c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1120,8 +1120,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 static int *task_stopped_code(struct task_struct *p, bool ptrace)
 {
 	if (ptrace) {
-		if (task_is_stopped_or_traced(p) &&
-		    !(p->jobctl & JOBCTL_LISTENING))
+		if (task_is_traced(p) && !(p->jobctl & JOBCTL_LISTENING))
 			return &p->exit_code;
 	} else {
 		if (p->signal->flags & SIGNAL_STOP_STOPPED)

commit 53528695ff6d8b77011bc818407c13e30914a946
Merge: b831ef2cad97 e73e85f05938
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 3 18:03:50 2015 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - sched/fair load tracking fixes and cleanups (Byungchul Park)
    
       - Make load tracking frequency scale invariant (Dietmar Eggemann)
    
       - sched/deadline updates (Juri Lelli)
    
       - stop machine fixes, cleanups and enhancements for bugs triggered by
         CPU hotplug stress testing (Oleg Nesterov)
    
       - scheduler preemption code rework: remove PREEMPT_ACTIVE and related
         cleanups (Peter Zijlstra)
    
       - Rework the sched_info::run_delay code to fix races (Peter Zijlstra)
    
       - Optimize per entity utilization tracking (Peter Zijlstra)
    
       - ... misc other fixes, cleanups and smaller updates"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (57 commits)
      sched: Don't scan all-offline ->cpus_allowed twice if !CONFIG_CPUSETS
      sched: Move cpu_active() tests from stop_two_cpus() into migrate_swap_stop()
      sched: Start stopper early
      stop_machine: Kill cpu_stop_threads->setup() and cpu_stop_unpark()
      stop_machine: Kill smp_hotplug_thread->pre_unpark, introduce stop_machine_unpark()
      stop_machine: Change cpu_stop_queue_two_works() to rely on stopper->enabled
      stop_machine: Introduce __cpu_stop_queue_work() and cpu_stop_queue_two_works()
      stop_machine: Ensure that a queued callback will be called before cpu_stop_park()
      sched/x86: Fix typo in __switch_to() comments
      sched/core: Remove a parameter in the migrate_task_rq() function
      sched/core: Drop unlikely behind BUG_ON()
      sched/core: Fix task and run queue sched_info::run_delay inconsistencies
      sched/numa: Fix task_tick_fair() from disabling numa_balancing
      sched/core: Add preempt_count invariant check
      sched/core: More notrace annotations
      sched/core: Kill PREEMPT_ACTIVE
      sched/core, sched/x86: Kill thread_info::saved_preempt_count
      sched/core: Simplify preempt_count tests
      sched/core: Robustify preemption leak checks
      sched/core: Stop setting PREEMPT_ACTIVE
      ...

commit 49f5903b473c5f63f3b57856d1bd4593db0a2eef
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Sep 1 00:42:57 2015 -0700

    rcu: Move preemption disabling out of __srcu_read_lock()
    
    Currently, __srcu_read_lock() cannot be invoked from restricted
    environments because it contains calls to preempt_disable() and
    preempt_enable(), both of which can invoke lockdep, which is a bad
    idea in some restricted execution modes.  This commit therefore moves
    the preempt_disable() and preempt_enable() from __srcu_read_lock()
    to srcu_read_lock().  It also inserts the preempt_disable() and
    preempt_enable() around the call to __srcu_read_lock() in do_exit().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ea95ee1b5ef7..0e93b63bbc59 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -761,7 +761,9 @@ void do_exit(long code)
 	 */
 	flush_ptrace_hw_breakpoint(tsk);
 
+	TASKS_RCU(preempt_disable());
 	TASKS_RCU(tasks_rcu_i = __srcu_read_lock(&tasks_rcu_exit_srcu));
+	TASKS_RCU(preempt_enable());
 	exit_notify(tsk, group_dead);
 	proc_exit_connector(tsk);
 #ifdef CONFIG_NUMA

commit 1dc0fffc48af94513e621f95dff730ed4f7317ec
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Sep 28 17:57:39 2015 +0200

    sched/core: Robustify preemption leak checks
    
    When we warn about a preempt_count leak; reset the preempt_count to
    the known good value such that the problem does not ripple forward.
    
    This is most important on x86 which has a per cpu preempt_count that is
    not saved/restored (after this series). So if you schedule with an
    invalid (!2*PREEMPT_DISABLE_OFFSET) preempt_count the next task is
    messed up too.
    
    Enforcing this invariant limits the borkage to just the one task.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ea95ee1b5ef7..443677c8efe6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -706,10 +706,12 @@ void do_exit(long code)
 	smp_mb();
 	raw_spin_unlock_wait(&tsk->pi_lock);
 
-	if (unlikely(in_atomic()))
+	if (unlikely(in_atomic())) {
 		pr_info("note: %s[%d] exited with preempt_count %d\n",
 			current->comm, task_pid_nr(current),
 			preempt_count());
+		preempt_count_set(PREEMPT_ENABLED);
+	}
 
 	/* sync mm's RSS info before statistics gathering */
 	if (tsk->mm)

commit 3da56d1663158a23818a5b94721cd79f7e4342d7
Author: Frans Klaver <fransklaver@gmail.com>
Date:   Thu May 21 22:35:57 2015 +0200

    kernel: exit: fix typo in comment
    
    s,critiera,criteria,
    
    While at it, add a comma, because it makes sense grammatically.
    
    Signed-off-by: Frans Klaver <fransklaver@gmail.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 031325e9acf9..ea95ee1b5ef7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1471,7 +1471,7 @@ static long do_wait(struct wait_opts *wo)
 	add_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);
 repeat:
 	/*
-	 * If there is nothing that can match our critiera just get out.
+	 * If there is nothing that can match our criteria, just get out.
 	 * We will clear ->notask_error to zero if we see any child that
 	 * might later match our criteria, even if we are not able to reap
 	 * it yet.

commit 51229b495340bd7a02ce3622d1966829b67054ea
Author: Rik van Riel <riel@redhat.com>
Date:   Thu Jun 25 15:03:56 2015 -0700

    exit,stats: /* obey this comment */
    
    There is a helpful comment in do_exit() that states we sync the mm's RSS
    info before statistics gathering.
    
    The function that does the statistics gathering is called right above that
    comment.
    
    Change the code to obey the comment.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 185752a729f6..031325e9acf9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -711,10 +711,10 @@ void do_exit(long code)
 			current->comm, task_pid_nr(current),
 			preempt_count());
 
-	acct_update_integrals(tsk);
 	/* sync mm's RSS info before statistics gathering */
 	if (tsk->mm)
 		sync_mm_rss(tsk->mm);
+	acct_update_integrals(tsk);
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
 		hrtimer_cancel(&tsk->signal->real_timer);

commit 16e951966f05da5ccd650104176f6ba289f7fa20
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Wed Jun 24 16:57:07 2015 -0700

    mm: oom_kill: clean up victim marking and exiting interfaces
    
    Rename unmark_oom_victim() to exit_oom_victim().  Marking and unmarking
    are related in functionality, but the interface is not symmetrical at
    all: one is an internal OOM killer function used during the killing, the
    other is for an OOM victim to signal its own death on exit later on.
    This has locking implications, see follow-up changes.
    
    While at it, rename mark_tsk_oom_victim() to mark_oom_victim(), which
    is easier on the eye.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Cc: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 22fcc05dec40..185752a729f6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -436,7 +436,7 @@ static void exit_mm(struct task_struct *tsk)
 	mm_update_next_owner(mm);
 	mmput(mm);
 	if (test_thread_flag(TIF_MEMDIE))
-		unmark_oom_victim();
+		exit_oom_victim();
 }
 
 static struct task_struct *find_alive_thread(struct task_struct *p)

commit 973f911f55a0e510dd6db8bbb29cd82ff138d3c0
Author: Richard Weinberger <richard@nod.at>
Date:   Mon Mar 30 08:14:16 2015 +0200

    Remove execution domain support
    
    All users of exec_domain are gone, now we can get rid
    of that abandoned feature.
    To not break existing userspace we keep a dummy
    /proc/execdomains file which will always contain
    "0-0     Linux                   [kernel]".
    
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/kernel/exit.c b/kernel/exit.c
index feff10bbb307..22fcc05dec40 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -756,8 +756,6 @@ void do_exit(long code)
 
 	cgroup_exit(tsk);
 
-	module_put(task_thread_info(tsk)->exec_domain->module);
-
 	/*
 	 * FIXME: do that only when needed, using sched_exit tracepoint
 	 */

commit c32b3cbe0d067a9cfae85aa70ba1e97ceba0ced7
Author: Michal Hocko <mhocko@suse.cz>
Date:   Wed Feb 11 15:26:24 2015 -0800

    oom, PM: make OOM detection in the freezer path raceless
    
    Commit 5695be142e20 ("OOM, PM: OOM killed task shouldn't escape PM
    suspend") has left a race window when OOM killer manages to
    note_oom_kill after freeze_processes checks the counter.  The race
    window is quite small and really unlikely and partial solution deemed
    sufficient at the time of submission.
    
    Tejun wasn't happy about this partial solution though and insisted on a
    full solution.  That requires the full OOM and freezer's task freezing
    exclusion, though.  This is done by this patch which introduces oom_sem
    RW lock and turns oom_killer_disable() into a full OOM barrier.
    
    oom_killer_disabled check is moved from the allocation path to the OOM
    level and we take oom_sem for reading for both the check and the whole
    OOM invocation.
    
    oom_killer_disable() takes oom_sem for writing so it waits for all
    currently running OOM killer invocations.  Then it disable all the further
    OOMs by setting oom_killer_disabled and checks for any oom victims.
    Victims are counted via mark_tsk_oom_victim resp.  unmark_oom_victim.  The
    last victim wakes up all waiters enqueued by oom_killer_disable().
    Therefore this function acts as the full OOM barrier.
    
    The page fault path is covered now as well although it was assumed to be
    safe before.  As per Tejun, "We used to have freezing points deep in file
    system code which may be reacheable from page fault." so it would be
    better and more robust to not rely on freezing points here.  Same applies
    to the memcg OOM killer.
    
    out_of_memory tells the caller whether the OOM was allowed to trigger and
    the callers are supposed to handle the situation.  The page allocation
    path simply fails the allocation same as before.  The page fault path will
    retry the fault (more on that later) and Sysrq OOM trigger will simply
    complain to the log.
    
    Normally there wouldn't be any unfrozen user tasks after
    try_to_freeze_tasks so the function will not block. But if there was an
    OOM killer racing with try_to_freeze_tasks and the OOM victim didn't
    finish yet then we have to wait for it. This should complete in a finite
    time, though, because
    
            - the victim cannot loop in the page fault handler (it would die
              on the way out from the exception)
            - it cannot loop in the page allocator because all the further
              allocation would fail and __GFP_NOFAIL allocations are not
              acceptable at this stage
            - it shouldn't be blocked on any locks held by frozen tasks
              (try_to_freeze expects lockless context) and kernel threads and
              work queues are not frozen yet
    
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Suggested-by: Tejun Heo <tj@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 02b3d1ab2ec0..feff10bbb307 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -435,7 +435,8 @@ static void exit_mm(struct task_struct *tsk)
 	task_unlock(tsk);
 	mm_update_next_owner(mm);
 	mmput(mm);
-	unmark_oom_victim();
+	if (test_thread_flag(TIF_MEMDIE))
+		unmark_oom_victim();
 }
 
 static struct task_struct *find_alive_thread(struct task_struct *p)

commit 49550b605587924b3336386caae53200c68969d3
Author: Michal Hocko <mhocko@suse.cz>
Date:   Wed Feb 11 15:26:12 2015 -0800

    oom: add helpers for setting and clearing TIF_MEMDIE
    
    This patchset addresses a race which was described in the changelog for
    5695be142e20 ("OOM, PM: OOM killed task shouldn't escape PM suspend"):
    
    : PM freezer relies on having all tasks frozen by the time devices are
    : getting frozen so that no task will touch them while they are getting
    : frozen.  But OOM killer is allowed to kill an already frozen task in order
    : to handle OOM situtation.  In order to protect from late wake ups OOM
    : killer is disabled after all tasks are frozen.  This, however, still keeps
    : a window open when a killed task didn't manage to die by the time
    : freeze_processes finishes.
    
    The original patch hasn't closed the race window completely because that
    would require a more complex solution as it can be seen by this patchset.
    
    The primary motivation was to close the race condition between OOM killer
    and PM freezer _completely_.  As Tejun pointed out, even though the race
    condition is unlikely the harder it would be to debug weird bugs deep in
    the PM freezer when the debugging options are reduced considerably.  I can
    only speculate what might happen when a task is still runnable
    unexpectedly.
    
    On a plus side and as a side effect the oom enable/disable has a better
    (full barrier) semantic without polluting hot paths.
    
    I have tested the series in KVM with 100M RAM:
    - many small tasks (20M anon mmap) which are triggering OOM continually
    - s2ram which resumes automatically is triggered in a loop
            echo processors > /sys/power/pm_test
            while true
            do
                    echo mem > /sys/power/state
                    sleep 1s
            done
    - simple module which allocates and frees 20M in 8K chunks. If it sees
      freezing(current) then it tries another round of allocation before calling
      try_to_freeze
    - debugging messages of PM stages and OOM killer enable/disable/fail added
      and unmark_oom_victim is delayed by 1s after it clears TIF_MEMDIE and before
      it wakes up waiters.
    - rebased on top of the current mmotm which means some necessary updates
      in mm/oom_kill.c. mark_tsk_oom_victim is now called under task_lock but
      I think this should be OK because __thaw_task shouldn't interfere with any
      locking down wake_up_process. Oleg?
    
    As expected there are no OOM killed tasks after oom is disabled and
    allocations requested by the kernel thread are failing after all the tasks
    are frozen and OOM disabled.  I wasn't able to catch a race where
    oom_killer_disable would really have to wait but I kinda expected the race
    is really unlikely.
    
    [  242.609330] Killed process 2992 (mem_eater) total-vm:24412kB, anon-rss:2164kB, file-rss:4kB
    [  243.628071] Unmarking 2992 OOM victim. oom_victims: 1
    [  243.636072] (elapsed 2.837 seconds) done.
    [  243.641985] Trying to disable OOM killer
    [  243.643032] Waiting for concurent OOM victims
    [  243.644342] OOM killer disabled
    [  243.645447] Freezing remaining freezable tasks ... (elapsed 0.005 seconds) done.
    [  243.652983] Suspending console(s) (use no_console_suspend to debug)
    [  243.903299] kmem_eater: page allocation failure: order:1, mode:0x204010
    [...]
    [  243.992600] PM: suspend of devices complete after 336.667 msecs
    [  243.993264] PM: late suspend of devices complete after 0.660 msecs
    [  243.994713] PM: noirq suspend of devices complete after 1.446 msecs
    [  243.994717] ACPI: Preparing to enter system sleep state S3
    [  243.994795] PM: Saving platform NVS memory
    [  243.994796] Disabling non-boot CPUs ...
    
    The first 2 patches are simple cleanups for OOM.  They should go in
    regardless the rest IMO.
    
    Patches 3 and 4 are trivial printk -> pr_info conversion and they should
    go in ditto.
    
    The main patch is the last one and I would appreciate acks from Tejun and
    Rafael.  I think the OOM part should be OK (except for __thaw_task vs.
    task_lock where a look from Oleg would appreciated) but I am not so sure I
    haven't screwed anything in the freezer code.  I have found several
    surprises there.
    
    This patch (of 5):
    
    This patch is just a preparatory and it doesn't introduce any functional
    change.
    
    Note:
    I am utterly unhappy about lowmemory killer abusing TIF_MEMDIE just to
    wait for the oom victim and to prevent from new killing. This is
    just a side effect of the flag. The primary meaning is to give the oom
    victim access to the memory reserves and that shouldn't be necessary
    here.
    
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6806c55475ee..02b3d1ab2ec0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -435,7 +435,7 @@ static void exit_mm(struct task_struct *tsk)
 	task_unlock(tsk);
 	mm_update_next_owner(mm);
 	mmput(mm);
-	clear_thread_flag(TIF_MEMDIE);
+	unmark_oom_victim();
 }
 
 static struct task_struct *find_alive_thread(struct task_struct *p)

commit 3245d6acab981a2388ffb877c7ecc97e763c59d4
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jan 8 14:32:12 2015 -0800

    exit: fix race between wait_consider_task() and wait_task_zombie()
    
    wait_consider_task() checks EXIT_ZOMBIE after EXIT_DEAD/EXIT_TRACE and
    both checks can fail if we race with EXIT_ZOMBIE -> EXIT_DEAD/EXIT_TRACE
    change in between, gcc needs to reload p->exit_state after
    security_task_wait().  In this case ->notask_error will be wrongly
    cleared and do_wait() can hang forever if it was the last eligible
    child.
    
    Many thanks to Arne who carefully investigated the problem.
    
    Note: this bug is very old but it was pure theoretical until commit
    b3ab03160dfa ("wait: completely ignore the EXIT_DEAD tasks").  Before
    this commit "-O2" was probably enough to guarantee that compiler won't
    read ->exit_state twice.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reported-by: Arne Goedeke <el@laramies.com>
    Tested-by: Arne Goedeke <el@laramies.com>
    Cc: <stable@vger.kernel.org>    [3.15+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1ea4369890a3..6806c55475ee 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1287,9 +1287,15 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 static int wait_consider_task(struct wait_opts *wo, int ptrace,
 				struct task_struct *p)
 {
+	/*
+	 * We can race with wait_task_zombie() from another thread.
+	 * Ensure that EXIT_ZOMBIE -> EXIT_DEAD/EXIT_TRACE transition
+	 * can't confuse the checks below.
+	 */
+	int exit_state = ACCESS_ONCE(p->exit_state);
 	int ret;
 
-	if (unlikely(p->exit_state == EXIT_DEAD))
+	if (unlikely(exit_state == EXIT_DEAD))
 		return 0;
 
 	ret = eligible_child(wo, p);
@@ -1310,7 +1316,7 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		return 0;
 	}
 
-	if (unlikely(p->exit_state == EXIT_TRACE)) {
+	if (unlikely(exit_state == EXIT_TRACE)) {
 		/*
 		 * ptrace == 0 means we are the natural parent. In this case
 		 * we should clear notask_error, debugger will notify us.
@@ -1337,7 +1343,7 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 	}
 
 	/* slay zombie? */
-	if (p->exit_state == EXIT_ZOMBIE) {
+	if (exit_state == EXIT_ZOMBIE) {
 		/* we don't reap group leaders with subthreads */
 		if (!delay_group_leader(p)) {
 			/*

commit 37da7bbbe84fe9e8862940d3f9194fd27dce59bb
Merge: e7cf773d431a dd63af108f08
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 14 15:23:32 2014 -0800

    Merge tag 'tty-3.19-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/tty
    
    Pull tty/serial driver updates from Greg KH:
     "Here's the big tty/serial driver update for 3.19-rc1.
    
      There are a number of TTY core changes/fixes in here from Peter Hurley
      that have all been teted in linux-next for a long time now.  There are
      also the normal serial driver updates as well, full details in the
      changelog below"
    
    * tag 'tty-3.19-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/tty: (219 commits)
      serial: pxa: hold port.lock when reporting modem line changes
      tty-hvsi_lib: Deletion of an unnecessary check before the function call "tty_kref_put"
      tty: Deletion of unnecessary checks before two function calls
      n_tty: Fix read_buf race condition, increment read_head after pushing data
      serial: of-serial: add PM suspend/resume support
      Revert "serial: of-serial: add PM suspend/resume support"
      Revert "serial: of-serial: fix up PM ops on no_console_suspend and port type"
      serial: 8250: don't attempt a trylock if in sysrq
      serial: core: Add big-endian iotype
      serial: samsung: use port->fifosize instead of hardcoded values
      serial: samsung: prefer to use fifosize from driver data
      serial: samsung: fix style problems
      serial: samsung: wait for transfer completion before clock disable
      serial: icom: fix error return code
      serial: tegra: clean up tty-flag assignments
      serial: Fix io address assign flow with Fintek PCI-to-UART Product
      serial: mxs-auart: fix tx_empty against shift register
      serial: mxs-auart: fix gpio change detection on interrupt
      serial: mxs-auart: Fix mxs_auart_set_ldisc()
      serial: 8250_dw: Use 64-bit access for OCTEON.
      ...

commit 6c66e7dba3d4419c8b973505679635efcd6b311c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:55:23 2014 -0800

    exit: exit_notify: re-use "dead" list to autoreap current
    
    After the previous change we can add just the exiting EXIT_DEAD task to
    the "dead" list and remove another release_task(tsk).
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8061891ddd9b..8714e5ded8b4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -632,6 +632,8 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	}
 
 	tsk->exit_state = autoreap ? EXIT_DEAD : EXIT_ZOMBIE;
+	if (tsk->exit_state == EXIT_DEAD)
+		list_add(&tsk->ptrace_entry, &dead);
 
 	/* mt-exec, de_thread() is waiting for group leader */
 	if (unlikely(tsk->signal->notify_count < 0))
@@ -642,10 +644,6 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 		list_del_init(&p->ptrace_entry);
 		release_task(p);
 	}
-
-	/* If the process is dead, release it - nobody will wait for it */
-	if (autoreap)
-		release_task(tsk);
 }
 
 #ifdef CONFIG_DEBUG_STACK_USAGE

commit 482a3767e5087f6e6ad2486a6655aaa5f3d59301
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:55:20 2014 -0800

    exit: reparent: call forget_original_parent() under tasklist_lock
    
    Shift "release dead children" loop from forget_original_parent() to its
    caller, exit_notify().  It is safe to reap them even if our parent reaps
    us right after we drop tasklist_lock, those children no longer have any
    connection to the exiting task.
    
    And this allows us to avoid write_lock_irq(tasklist_lock) right after it
    was released by forget_original_parent(), we can simply call it with
    tasklist_lock held.
    
    While at it, move the comment about forget_original_parent() up to
    this function.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 063745699f7f..8061891ddd9b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -560,19 +560,26 @@ static void reparent_leader(struct task_struct *father, struct task_struct *p,
 	kill_orphaned_pgrp(p, father);
 }
 
-static void forget_original_parent(struct task_struct *father)
+/*
+ * This does two things:
+ *
+ * A.  Make init inherit all the child processes
+ * B.  Check to see if any process groups have become orphaned
+ *	as a result of our exiting, and if they have any stopped
+ *	jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
+ */
+static void forget_original_parent(struct task_struct *father,
+					struct list_head *dead)
 {
-	struct task_struct *p, *t, *n, *reaper;
-	LIST_HEAD(dead_children);
+	struct task_struct *p, *t, *reaper;
 
-	write_lock_irq(&tasklist_lock);
 	if (unlikely(!list_empty(&father->ptraced)))
-		exit_ptrace(father, &dead_children);
+		exit_ptrace(father, dead);
 
 	/* Can drop and reacquire tasklist_lock */
 	reaper = find_child_reaper(father);
 	if (list_empty(&father->children))
-		goto unlock;
+		return;
 
 	reaper = find_new_reaper(father, reaper);
 	list_for_each_entry(p, &father->children, sibling) {
@@ -590,16 +597,9 @@ static void forget_original_parent(struct task_struct *father)
 		 * notify anyone anything has happened.
 		 */
 		if (!same_thread_group(reaper, father))
-			reparent_leader(father, p, &dead_children);
+			reparent_leader(father, p, dead);
 	}
 	list_splice_tail_init(&father->children, &reaper->children);
- unlock:
-	write_unlock_irq(&tasklist_lock);
-
-	list_for_each_entry_safe(p, n, &dead_children, ptrace_entry) {
-		list_del_init(&p->ptrace_entry);
-		release_task(p);
-	}
 }
 
 /*
@@ -609,18 +609,12 @@ static void forget_original_parent(struct task_struct *father)
 static void exit_notify(struct task_struct *tsk, int group_dead)
 {
 	bool autoreap;
-
-	/*
-	 * This does two things:
-	 *
-	 * A.  Make init inherit all the child processes
-	 * B.  Check to see if any process groups have become orphaned
-	 *	as a result of our exiting, and if they have any stopped
-	 *	jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
-	 */
-	forget_original_parent(tsk);
+	struct task_struct *p, *n;
+	LIST_HEAD(dead);
 
 	write_lock_irq(&tasklist_lock);
+	forget_original_parent(tsk, &dead);
+
 	if (group_dead)
 		kill_orphaned_pgrp(tsk->group_leader, NULL);
 
@@ -644,6 +638,11 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 		wake_up_process(tsk->signal->group_exit_task);
 	write_unlock_irq(&tasklist_lock);
 
+	list_for_each_entry_safe(p, n, &dead, ptrace_entry) {
+		list_del_init(&p->ptrace_entry);
+		release_task(p);
+	}
+
 	/* If the process is dead, release it - nobody will wait for it */
 	if (autoreap)
 		release_task(tsk);

commit ad9e206aefa56788b676ebcd6329e828f40d2238
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:55:17 2014 -0800

    exit: reparent: avoid find_new_reaper() if no children
    
    Now that pid_ns logic was isolated we can change forget_original_parent()
    to return right after find_child_reaper() when father->children is empty,
    there is nothing to reparent in this case.
    
    In particular this avoids find_alive_thread() and this can help if the
    whole process exits and it has a lot of PF_EXITING threads at the start of
    the thread list, this can easily lead to O(nr_threads ** 2) iterations.
    
    Trivial test case (tested under KVM, 2 CPUs):
    
        static void *tfunc(void *arg)
        {
            pause();
            return NULL;
        }
    
        static int child(unsigned int nt)
        {
            pthread_t pt;
    
            while (nt--)
                assert(pthread_create(&pt, NULL, tfunc, NULL) == 0);
    
            pthread_kill(pt, SIGTRAP);
            pause();
            return 0;
        }
    
        int main(int argc, const char *argv[])
        {
            int stat;
            unsigned int nf = atoi(argv[1]);
            unsigned int nt = atoi(argv[2]);
    
            while (nf--) {
                if (!fork())
                    return child(nt);
    
                wait(&stat);
                assert(stat == SIGTRAP);
            }
    
            return 0;
        }
    
    $ time ./test 16 16536 shows:
    
                  real        user         sys
        -    5m37.628s    0m4.437s    8m5.560s
        +    0m50.032s    0m7.130s    1m4.927s
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b0f482f5daf9..063745699f7f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -571,6 +571,8 @@ static void forget_original_parent(struct task_struct *father)
 
 	/* Can drop and reacquire tasklist_lock */
 	reaper = find_child_reaper(father);
+	if (list_empty(&father->children))
+		goto unlock;
 
 	reaper = find_new_reaper(father, reaper);
 	list_for_each_entry(p, &father->children, sibling) {
@@ -591,6 +593,7 @@ static void forget_original_parent(struct task_struct *father)
 			reparent_leader(father, p, &dead_children);
 	}
 	list_splice_tail_init(&father->children, &reaper->children);
+ unlock:
 	write_unlock_irq(&tasklist_lock);
 
 	list_for_each_entry_safe(p, n, &dead_children, ptrace_entry) {

commit c9dc05bfdb3f7fd7c00f3cbd33816c99d2cb9029
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:55:14 2014 -0800

    exit: reparent: introduce find_alive_thread()
    
    Add the new simple helper to factor out the for_each_thread() code in
    find_child_reaper() and find_new_reaper().  It can also simplify the
    potential PF_EXITING -> exit_state change, plus perhaps we can change this
    code to take SIGNAL_GROUP_EXIT into account.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Kay Sievers <kay@vrfy.org>
    Cc: Lennart Poettering <lennart@poettering.net>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 930fbe1b5ee2..b0f482f5daf9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -459,6 +459,17 @@ static void exit_mm(struct task_struct *tsk)
 	clear_thread_flag(TIF_MEMDIE);
 }
 
+static struct task_struct *find_alive_thread(struct task_struct *p)
+{
+	struct task_struct *t;
+
+	for_each_thread(p, t) {
+		if (!(t->flags & PF_EXITING))
+			return t;
+	}
+	return NULL;
+}
+
 static struct task_struct *find_child_reaper(struct task_struct *father)
 	__releases(&tasklist_lock)
 	__acquires(&tasklist_lock)
@@ -469,9 +480,8 @@ static struct task_struct *find_child_reaper(struct task_struct *father)
 	if (likely(reaper != father))
 		return reaper;
 
-	for_each_thread(father, reaper) {
-		if (reaper->flags & PF_EXITING)
-			continue;
+	reaper = find_alive_thread(father);
+	if (reaper) {
 		pid_ns->child_reaper = reaper;
 		return reaper;
 	}
@@ -497,16 +507,13 @@ static struct task_struct *find_child_reaper(struct task_struct *father)
 static struct task_struct *find_new_reaper(struct task_struct *father,
 					   struct task_struct *child_reaper)
 {
-	struct task_struct *thread;
+	struct task_struct *thread, *reaper;
 
-	for_each_thread(father, thread) {
-		if (thread->flags & PF_EXITING)
-			continue;
+	thread = find_alive_thread(father);
+	if (thread)
 		return thread;
-	}
 
 	if (father->signal->has_child_subreaper) {
-		struct task_struct *reaper;
 		/*
 		 * Find the first ->is_child_subreaper ancestor in our pid_ns.
 		 * We start from father to ensure we can not look into another
@@ -520,10 +527,9 @@ static struct task_struct *find_new_reaper(struct task_struct *father,
 				break;
 			if (!reaper->signal->is_child_subreaper)
 				continue;
-			for_each_thread(reaper, thread) {
-				if (!(thread->flags & PF_EXITING))
-					return thread;
-			}
+			thread = find_alive_thread(reaper);
+			if (thread)
+				return thread;
 		}
 	}
 

commit 1109909c7df08f55ff9104276bb9db1ee2e6e53d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:55:11 2014 -0800

    exit: reparent: introduce find_child_reaper()
    
    find_new_reaper() does 2 completely different things.  Not only it finds a
    reaper, it also updates pid_ns->child_reaper or kills the whole namespace
    if the caller is ->child_reaper.
    
    Now that has_child_subreaper logic doesn't depend on child_reaper check we
    can move that pid_ns code into a separate helper.  IMHO this makes the
    code more clean, and this allows the next changes.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Kay Sievers <kay@vrfy.org>
    Cc: Lennart Poettering <lennart@poettering.net>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 576949ce5665..930fbe1b5ee2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -459,6 +459,34 @@ static void exit_mm(struct task_struct *tsk)
 	clear_thread_flag(TIF_MEMDIE);
 }
 
+static struct task_struct *find_child_reaper(struct task_struct *father)
+	__releases(&tasklist_lock)
+	__acquires(&tasklist_lock)
+{
+	struct pid_namespace *pid_ns = task_active_pid_ns(father);
+	struct task_struct *reaper = pid_ns->child_reaper;
+
+	if (likely(reaper != father))
+		return reaper;
+
+	for_each_thread(father, reaper) {
+		if (reaper->flags & PF_EXITING)
+			continue;
+		pid_ns->child_reaper = reaper;
+		return reaper;
+	}
+
+	write_unlock_irq(&tasklist_lock);
+	if (unlikely(pid_ns == &init_pid_ns)) {
+		panic("Attempted to kill init! exitcode=0x%08x\n",
+			father->signal->group_exit_code ?: father->exit_code);
+	}
+	zap_pid_ns_processes(pid_ns);
+	write_lock_irq(&tasklist_lock);
+
+	return father;
+}
+
 /*
  * When we die, we re-parent all our children, and try to:
  * 1. give them to another thread in our thread group, if such a member exists
@@ -466,33 +494,17 @@ static void exit_mm(struct task_struct *tsk)
  *    child_subreaper for its children (like a service manager)
  * 3. give it to the init process (PID 1) in our pid namespace
  */
-static struct task_struct *find_new_reaper(struct task_struct *father)
-	__releases(&tasklist_lock)
-	__acquires(&tasklist_lock)
+static struct task_struct *find_new_reaper(struct task_struct *father,
+					   struct task_struct *child_reaper)
 {
-	struct pid_namespace *pid_ns = task_active_pid_ns(father);
 	struct task_struct *thread;
 
 	for_each_thread(father, thread) {
 		if (thread->flags & PF_EXITING)
 			continue;
-		if (unlikely(pid_ns->child_reaper == father))
-			pid_ns->child_reaper = thread;
 		return thread;
 	}
 
-	if (unlikely(pid_ns->child_reaper == father)) {
-		write_unlock_irq(&tasklist_lock);
-		if (unlikely(pid_ns == &init_pid_ns)) {
-			panic("Attempted to kill init! exitcode=0x%08x\n",
-				father->signal->group_exit_code ?:
-					father->exit_code);
-		}
-
-		zap_pid_ns_processes(pid_ns);
-		write_lock_irq(&tasklist_lock);
-	}
-
 	if (father->signal->has_child_subreaper) {
 		struct task_struct *reaper;
 		/*
@@ -501,7 +513,7 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 		 * namespace, this is safe because all its threads are dead.
 		 */
 		for (reaper = father;
-		     !same_thread_group(reaper, pid_ns->child_reaper);
+		     !same_thread_group(reaper, child_reaper);
 		     reaper = reaper->real_parent) {
 			/* call_usermodehelper() descendants need this check */
 			if (reaper == &init_task)
@@ -515,7 +527,7 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 		}
 	}
 
-	return pid_ns->child_reaper;
+	return child_reaper;
 }
 
 /*
@@ -552,7 +564,9 @@ static void forget_original_parent(struct task_struct *father)
 		exit_ptrace(father, &dead_children);
 
 	/* Can drop and reacquire tasklist_lock */
-	reaper = find_new_reaper(father);
+	reaper = find_child_reaper(father);
+
+	reaper = find_new_reaper(father, reaper);
 	list_for_each_entry(p, &father->children, sibling) {
 		for_each_thread(p, t) {
 			t->real_parent = reaper;

commit 175aed3f8d38b87d3287bb765c794205f2b511de
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:55:08 2014 -0800

    exit: reparent: document the ->has_child_subreaper checks
    
    Swap the "init_task" and same_thread_group() checks.  This way it is more
    simple to document these checks and we can remove the link to the previous
    discussion on lkml.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Kay Sievers <kay@vrfy.org>
    Cc: Lennart Poettering <lennart@poettering.net>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a4204aaba8a2..576949ce5665 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -495,18 +495,16 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 
 	if (father->signal->has_child_subreaper) {
 		struct task_struct *reaper;
-
 		/*
-		 * Find the first ancestor marked as child_subreaper.
-		 * Note that the code below checks same_thread_group(reaper,
-		 * pid_ns->child_reaper).  This is what we need to DTRT in a
-		 * PID namespace. However we still need the check above, see
-		 * http://marc.info/?l=linux-kernel&m=131385460420380
+		 * Find the first ->is_child_subreaper ancestor in our pid_ns.
+		 * We start from father to ensure we can not look into another
+		 * namespace, this is safe because all its threads are dead.
 		 */
 		for (reaper = father;
-		     reaper != &init_task;
+		     !same_thread_group(reaper, pid_ns->child_reaper);
 		     reaper = reaper->real_parent) {
-			if (same_thread_group(reaper, pid_ns->child_reaper))
+			/* call_usermodehelper() descendants need this check */
+			if (reaper == &init_task)
 				break;
 			if (!reaper->signal->is_child_subreaper)
 				continue;

commit 3750ef979cfa1296630aa9f23e265c1bd721498a
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:55:05 2014 -0800

    exit: reparent: s/while_each_thread/for_each_thread/ in find_new_reaper()
    
    Change find_new_reaper() to use for_each_thread() instead of deprecated
    while_each_thread().  We do not bother to check "thread != father" in the
    1st loop, we can rely on PF_EXITING check.
    
    Note: this means the minor behavioural change: for_each_thread() starts
    from the group leader.  But this should be fine, nobody should make any
    assumption about do_wait(__WNOTHREAD) when it comes to reparented tasks.
    And this can avoid the pointless reparenting to a short-living thread
    While zombie leaders are not that common.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Kay Sievers <kay@vrfy.org>
    Cc: Lennart Poettering <lennart@poettering.net>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9babd47a36e2..a4204aaba8a2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -473,8 +473,7 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 	struct pid_namespace *pid_ns = task_active_pid_ns(father);
 	struct task_struct *thread;
 
-	thread = father;
-	while_each_thread(father, thread) {
+	for_each_thread(father, thread) {
 		if (thread->flags & PF_EXITING)
 			continue;
 		if (unlikely(pid_ns->child_reaper == father))
@@ -511,11 +510,10 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 				break;
 			if (!reaper->signal->is_child_subreaper)
 				continue;
-			thread = reaper;
-			do {
+			for_each_thread(reaper, thread) {
 				if (!(thread->flags & PF_EXITING))
 					return thread;
-			} while_each_thread(reaper, thread);
+			}
 		}
 	}
 

commit 7d24e2df52f596a1ea922e4f84a61f2fb24fbb70
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:55:02 2014 -0800

    exit: reparent: fix the cross-namespace PR_SET_CHILD_SUBREAPER reparenting
    
    find_new_reaper() assumes that "has_child_subreaper" logic is safe as
    long as we are not the exiting ->child_reaper and this is doubly wrong:
    
    1. In fact it is safe if "pid_ns->child_reaper == father"; there must
       be no children after zap_pid_ns_processes() returns, so it doesn't
       matter what we return in this case and even pid_ns->child_reaper is
       wrong otherwise: we can't reparent to ->child_reaper == current.
    
       This is not a bug, but this is confusing.
    
    2. It is not safe if we are not pid_ns->child_reaper but from the same
       thread group. We drop tasklist_lock before zap_pid_ns_processes(),
       so another thread can lock it and choose the new reaper from the
       upper namespace if has_child_subreaper == T, and this is obviously
       wrong.
    
       This is not that bad, zap_pid_ns_processes() won't return until the
       the new reaper reaps all zombies, but this should be fixed anyway.
    
    We could change for_each_thread() loop to use ->exit_state instead of
    PF_EXITING which we had to use until 8aac62706ada, or we could change
    copy_signal() to check CLONE_NEWPID before setting has_child_subreaper,
    but lets change this code so that it is clear we can't look outside of
    our namespace, otherwise same_thread_group(reaper, child_reaper) check
    will look wrong and confusing anyway.
    
    We can simply start from "father" and fix the problem. We can't wrongly
    return a thread from the same thread group if ->is_child_subreaper == T,
    we know that all threads have PF_EXITING set.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Kay Sievers <kay@vrfy.org>
    Cc: Lennart Poettering <lennart@poettering.net>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index fd38a8f04367..9babd47a36e2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -492,7 +492,9 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 
 		zap_pid_ns_processes(pid_ns);
 		write_lock_irq(&tasklist_lock);
-	} else if (father->signal->has_child_subreaper) {
+	}
+
+	if (father->signal->has_child_subreaper) {
 		struct task_struct *reaper;
 
 		/*
@@ -502,7 +504,7 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 		 * PID namespace. However we still need the check above, see
 		 * http://marc.info/?l=linux-kernel&m=131385460420380
 		 */
-		for (reaper = father->real_parent;
+		for (reaper = father;
 		     reaper != &init_task;
 		     reaper = reaper->real_parent) {
 			if (same_thread_group(reaper, pid_ns->child_reaper))

commit 8a1296aea4a319b33c3367ff3805835e949a229f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:54:59 2014 -0800

    exit: reparent: fix the dead-parent PR_SET_CHILD_SUBREAPER reparenting
    
    The ->has_child_subreaper code in find_new_reaper() finds alive "thread"
    but returns another "reaper" thread which can be dead.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Kay Sievers <kay@vrfy.org>
    Cc: Lennart Poettering <lennart@poettering.net>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9a65f10dc9ff..fd38a8f04367 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -512,7 +512,7 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 			thread = reaper;
 			do {
 				if (!(thread->flags & PF_EXITING))
-					return reaper;
+					return thread;
 			} while_each_thread(reaper, thread);
 		}
 	}

commit 26e75b5c3d2226cb995fde064744aa93f63849c4
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:54:54 2014 -0800

    exit: release_task: fix the comment about group leader accounting
    
    Contrary to what the comment in __exit_signal() says we do account the
    group leader. Fix this and explain why.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6297eb0f5bd2..9a65f10dc9ff 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -118,13 +118,10 @@ static void __exit_signal(struct task_struct *tsk)
 	}
 
 	/*
-	 * Accumulate here the counters for all threads but the group leader
-	 * as they die, so they can be added into the process-wide totals
-	 * when those are taken.  The group leader stays around as a zombie as
-	 * long as there are other threads.  When it gets reaped, the exit.c
-	 * code will add its counts into these totals.  We won't ever get here
-	 * for the group leader, since it will have been the last reference on
-	 * the signal_struct.
+	 * Accumulate here the counters for all threads as they die. We could
+	 * skip the group leader because it is the last user of signal_struct,
+	 * but we want to avoid the race with thread_group_cputime() which can
+	 * see the empty ->thread_head list.
 	 */
 	task_cputime(tsk, &utime, &stime);
 	write_seqlock(&sig->stats_lock);

commit 986094dfe161b4346831547136d4e5ed7f94310e
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:54:51 2014 -0800

    exit: wait: drop tasklist_lock before psig->c* accounting
    
    wait_task_zombie() no longer needs tasklist_lock to accumulate the
    psig->c* counters, we can drop it right after cmpxchg(exit_state).
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 457673d65934..6297eb0f5bd2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1005,6 +1005,11 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		EXIT_TRACE : EXIT_DEAD;
 	if (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)
 		return 0;
+	/*
+	 * We own this thread, nobody else can reap it.
+	 */
+	read_unlock(&tasklist_lock);
+	sched_annotate_sleep();
 
 	/*
 	 * Check thread_group_leader() to exclude the traced sub-threads.
@@ -1064,13 +1069,6 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		spin_unlock_irq(&current->sighand->siglock);
 	}
 
-	/*
-	 * Now we are sure this task is interesting, and no other
-	 * thread can reap it because we its state == DEAD/TRACE.
-	 */
-	read_unlock(&tasklist_lock);
-	sched_annotate_sleep();
-
 	retval = wo->wo_rusage
 		? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
 	status = (p->signal->flags & SIGNAL_GROUP_EXIT)

commit f953ccd00615140b5e722ffe2b920da22dfb4db9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:54:48 2014 -0800

    exit: wait: don't use zombie->real_parent
    
    1. wait_task_zombie() uses p->real_parent to get psig/siglock. This is
       correct but needs tasklist_lock, ->real_parent can exit.
    
       We can use "current" instead. This is our natural child, its parent
       must be our sub-thread.
    
    2. Read psig/sig outside of ->siglock, ->signal is no longer protected
       by this lock.
    
    3. Fix the outdated comments about tasklist_lock. We can not race with
       __exit_signal(), the whole thread group is dead, nobody but us can
       call it.
    
       Also clarify the usage of ->stats_lock and ->siglock.
    
    Note: thread_group_cputime_adjusted() is sub-optimal in this case, we
    probably want to export cputime_adjust() to avoid thread_group_cputime().
    The comment says "all threads" but there are no other threads.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 20875d6398ae..457673d65934 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1010,8 +1010,8 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	 * Check thread_group_leader() to exclude the traced sub-threads.
 	 */
 	if (state == EXIT_DEAD && thread_group_leader(p)) {
-		struct signal_struct *psig;
-		struct signal_struct *sig;
+		struct signal_struct *sig = p->signal;
+		struct signal_struct *psig = current->signal;
 		unsigned long maxrss;
 		cputime_t tgutime, tgstime;
 
@@ -1023,21 +1023,20 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		 * accumulate in the parent's signal_struct c* fields.
 		 *
 		 * We don't bother to take a lock here to protect these
-		 * p->signal fields, because they are only touched by
-		 * __exit_signal, which runs with tasklist_lock
-		 * write-locked anyway, and so is excluded here.  We do
-		 * need to protect the access to parent->signal fields,
-		 * as other threads in the parent group can be right
-		 * here reaping other children at the same time.
+		 * p->signal fields because the whole thread group is dead
+		 * and nobody can change them.
+		 *
+		 * psig->stats_lock also protects us from our sub-theads
+		 * which can reap other children at the same time. Until
+		 * we change k_getrusage()-like users to rely on this lock
+		 * we have to take ->siglock as well.
 		 *
 		 * We use thread_group_cputime_adjusted() to get times for
 		 * the thread group, which consolidates times for all threads
 		 * in the group including the group leader.
 		 */
 		thread_group_cputime_adjusted(p, &tgutime, &tgstime);
-		spin_lock_irq(&p->real_parent->sighand->siglock);
-		psig = p->real_parent->signal;
-		sig = p->signal;
+		spin_lock_irq(&current->sighand->siglock);
 		write_seqlock(&psig->stats_lock);
 		psig->cutime += tgutime + sig->cutime;
 		psig->cstime += tgstime + sig->cstime;
@@ -1062,7 +1061,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		task_io_accounting_add(&psig->ioac, &p->ioac);
 		task_io_accounting_add(&psig->ioac, &sig->ioac);
 		write_sequnlock(&psig->stats_lock);
-		spin_unlock_irq(&p->real_parent->sighand->siglock);
+		spin_unlock_irq(&current->sighand->siglock);
 	}
 
 	/*

commit f6507f83bccd4a5f7dc7091079bf58128dc56d66
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:54:45 2014 -0800

    exit: wait: cleanup the ptrace_reparented() checks
    
    Now that EXIT_DEAD is the terminal state we can kill "int traced"
    variable and check "state == EXIT_DEAD" instead to cleanup the code.  In
    particular, this way it is clear that the check obviously doesn't need
    tasklist_lock.
    
    Also fix the type of "unsigned long state", "long" was always wrong
    although this doesn't matter because cmpxchg/xchg uses typeof(*ptr).
    
    [akpm@linux-foundation.org: don't make me google the C Operator Precedence table]
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sterling Alexander <stalexan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9c9526d87276..20875d6398ae 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -973,8 +973,7 @@ static int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,
  */
 static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 {
-	unsigned long state;
-	int retval, status, traced;
+	int state, retval, status;
 	pid_t pid = task_pid_vnr(p);
 	uid_t uid = from_kuid_munged(current_user_ns(), task_uid(p));
 	struct siginfo __user *infop;
@@ -999,19 +998,18 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		}
 		return wait_noreap_copyout(wo, p, pid, uid, why, status);
 	}
-
-	traced = ptrace_reparented(p);
 	/*
 	 * Move the task's state to DEAD/TRACE, only one thread can do this.
 	 */
-	state = traced && thread_group_leader(p) ? EXIT_TRACE : EXIT_DEAD;
+	state = (ptrace_reparented(p) && thread_group_leader(p)) ?
+		EXIT_TRACE : EXIT_DEAD;
 	if (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)
 		return 0;
+
 	/*
-	 * It can be ptraced but not reparented, check
-	 * thread_group_leader() to filter out sub-threads.
+	 * Check thread_group_leader() to exclude the traced sub-threads.
 	 */
-	if (likely(!traced) && thread_group_leader(p)) {
+	if (state == EXIT_DEAD && thread_group_leader(p)) {
 		struct signal_struct *psig;
 		struct signal_struct *sig;
 		unsigned long maxrss;

commit 7c8bd2322c7fd973d089b27de55e29c92c667a06
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:45:33 2014 -0800

    exit: ptrace: shift "reap dead" code from exit_ptrace() to forget_original_parent()
    
    Now that forget_original_parent() uses ->ptrace_entry for EXIT_DEAD tasks,
    we can simply pass "dead_children" list to exit_ptrace() and remove
    another release_task() loop.  Plus this way we do not need to drop and
    reacquire tasklist_lock.
    
    Also shift the list_empty(ptraced) check, if we want this optimization it
    makes sense to eliminate the function call altogether.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>,
    Cc: Sterling Alexander <stalexan@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Roland McGrath <roland@hack.frob.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 772e9175735c..9c9526d87276 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -553,13 +553,11 @@ static void forget_original_parent(struct task_struct *father)
 	LIST_HEAD(dead_children);
 
 	write_lock_irq(&tasklist_lock);
-	/*
-	 * Note that exit_ptrace() and find_new_reaper() might
-	 * drop tasklist_lock and reacquire it.
-	 */
-	exit_ptrace(father);
-	reaper = find_new_reaper(father);
+	if (unlikely(!list_empty(&father->ptraced)))
+		exit_ptrace(father, &dead_children);
 
+	/* Can drop and reacquire tasklist_lock */
+	reaper = find_new_reaper(father);
 	list_for_each_entry(p, &father->children, sibling) {
 		for_each_thread(p, t) {
 			t->real_parent = reaper;

commit 2831096e21503897ee474c23131c3feb8db0ffb1
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:45:30 2014 -0800

    exit: reparent: cleanup the usage of reparent_leader()
    
    1. Now that reparent_leader() doesn't abuse ->sibling we can shift
       list_move_tail() from reparent_leader() to forget_original_parent()
       and turn it into a single list_splice_tail_init(). This also makes
       BUG_ON(!list_empty()) and list_for_each_entry_safe() unnecessary.
    
    2. This also allows to shift the same_thread_group() check, it looks
       a bit more clear in the caller.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>,
    Cc: Sterling Alexander <stalexan@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Roland McGrath <roland@hack.frob.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 464971e6923e..772e9175735c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -529,15 +529,7 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 static void reparent_leader(struct task_struct *father, struct task_struct *p,
 				struct list_head *dead)
 {
-	list_move_tail(&p->sibling, &p->real_parent->children);
-
-	if (p->exit_state == EXIT_DEAD)
-		return;
-	/*
-	 * If this is a threaded reparent there is no need to
-	 * notify anyone anything has happened.
-	 */
-	if (same_thread_group(p->real_parent, father))
+	if (unlikely(p->exit_state == EXIT_DEAD))
 		return;
 
 	/* We don't want people slaying init. */
@@ -568,7 +560,7 @@ static void forget_original_parent(struct task_struct *father)
 	exit_ptrace(father);
 	reaper = find_new_reaper(father);
 
-	list_for_each_entry_safe(p, n, &father->children, sibling) {
+	list_for_each_entry(p, &father->children, sibling) {
 		for_each_thread(p, t) {
 			t->real_parent = reaper;
 			BUG_ON((!t->ptrace) != (t->parent == father));
@@ -578,12 +570,16 @@ static void forget_original_parent(struct task_struct *father)
 				group_send_sig_info(t->pdeath_signal,
 						    SEND_SIG_NOINFO, t);
 		}
-		reparent_leader(father, p, &dead_children);
+		/*
+		 * If this is a threaded reparent there is no need to
+		 * notify anyone anything has happened.
+		 */
+		if (!same_thread_group(reaper, father))
+			reparent_leader(father, p, &dead_children);
 	}
+	list_splice_tail_init(&father->children, &reaper->children);
 	write_unlock_irq(&tasklist_lock);
 
-	BUG_ON(!list_empty(&father->children));
-
 	list_for_each_entry_safe(p, n, &dead_children, ptrace_entry) {
 		list_del_init(&p->ptrace_entry);
 		release_task(p);

commit 57a059187d5ba5592e36c6f23d046bc37616f346
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:45:27 2014 -0800

    exit: reparent: cleanup the changing of ->parent
    
    1. Cosmetic, but "if (t->parent == father)" looks a bit confusing.
       We need to change t->parent if and only if t is not traced.
    
    2. If we actually want this BUG_ON() to ensure that parent/ptrace
       match each other, then we should also take ptrace_reparented()
       case into account too.
    
    3. Change this code to use for_each_thread() instead of deprecated
       while_each_thread().
    
    [dan.carpenter@oracle.com: silence a bogus static checker warning]
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>,
    Cc: Sterling Alexander <stalexan@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Roland McGrath <roland@hack.frob.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 0272305bf855..464971e6923e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -557,7 +557,7 @@ static void reparent_leader(struct task_struct *father, struct task_struct *p,
 
 static void forget_original_parent(struct task_struct *father)
 {
-	struct task_struct *p, *n, *reaper;
+	struct task_struct *p, *t, *n, *reaper;
 	LIST_HEAD(dead_children);
 
 	write_lock_irq(&tasklist_lock);
@@ -569,18 +569,15 @@ static void forget_original_parent(struct task_struct *father)
 	reaper = find_new_reaper(father);
 
 	list_for_each_entry_safe(p, n, &father->children, sibling) {
-		struct task_struct *t = p;
-
-		do {
+		for_each_thread(p, t) {
 			t->real_parent = reaper;
-			if (t->parent == father) {
-				BUG_ON(t->ptrace);
+			BUG_ON((!t->ptrace) != (t->parent == father));
+			if (likely(!t->ptrace))
 				t->parent = t->real_parent;
-			}
 			if (t->pdeath_signal)
 				group_send_sig_info(t->pdeath_signal,
 						    SEND_SIG_NOINFO, t);
-		} while_each_thread(p, t);
+		}
 		reparent_leader(father, p, &dead_children);
 	}
 	write_unlock_irq(&tasklist_lock);

commit dc2fd4b00946751ebd222d366fc64550e4188dc2
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Dec 10 15:45:24 2014 -0800

    exit: reparent: use ->ptrace_entry rather than ->sibling for EXIT_DEAD tasks
    
    reparent_leader() reuses ->sibling as a list node to add an EXIT_DEAD task
    into dead_children list we are going to release.  This obviously removes
    the dead task from its real_parent->children list and this is even good;
    the parent can do nothing with the EXIT_DEAD reparented zombie, it only
    makes do_wait() slower.
    
    But, this also means that it can not be reparented once again, so if its
    new parent dies too nobody will update ->parent/real_parent, they can
    point to the freed memory even before release_task() we are going to call,
    this breaks the code which relies on pid_alive() to access
    ->real_parent/parent.
    
    Fortunately this is mostly theoretical, this can only happen if init or
    PR_SET_CHILD_SUBREAPER process ignores SIGCHLD and the new parent
    sub-thread exits right after we drop tasklist_lock.
    
    Change this code to use ->ptrace_entry instead, we know that the child is
    not traced so nobody can ever use this member.  This also allows to unify
    this logic with exit_ptrace(), see the next changes.
    
    Note: we really need to change release_task() to nullify real_parent/
    parent/group_leader pointers, but we need to change the current users
    first somehow.  And it would be better to reap this zombie immediately but
    release_task_locked() we need is complicated by proc_flush_task().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>,
    Cc: Sterling Alexander <stalexan@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Roland McGrath <roland@hack.frob.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 232c4bc8bcc9..0272305bf855 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -548,7 +548,7 @@ static void reparent_leader(struct task_struct *father, struct task_struct *p,
 	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {
 		if (do_notify_parent(p, p->exit_signal)) {
 			p->exit_state = EXIT_DEAD;
-			list_move_tail(&p->sibling, dead);
+			list_add(&p->ptrace_entry, dead);
 		}
 	}
 
@@ -587,8 +587,8 @@ static void forget_original_parent(struct task_struct *father)
 
 	BUG_ON(!list_empty(&father->children));
 
-	list_for_each_entry_safe(p, n, &dead_children, sibling) {
-		list_del_init(&p->sibling);
+	list_for_each_entry_safe(p, n, &dead_children, ptrace_entry) {
+		list_del_init(&p->ptrace_entry);
 		release_task(p);
 	}
 }

commit e1c2296c3485158304bfad5a80e89078463d70c8
Author: Peter Hurley <peter@hurleysoftware.com>
Date:   Thu Oct 16 14:59:48 2014 -0400

    tty: Move session_of_pgrp() and make static
    
    tiocspgrp() is the lone caller of session_of_pgrp(); relocate and
    limit to file scope.
    
    Signed-off-by: Peter Hurley <peter@hurleysoftware.com>
    Reviewed-by: Alan Cox <alan@linux.intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5d30019ff953..6a3e2e5004ba 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -214,27 +214,6 @@ void release_task(struct task_struct *p)
 		goto repeat;
 }
 
-/*
- * This checks not only the pgrp, but falls back on the pid if no
- * satisfactory pgrp is found. I dunno - gdb doesn't work correctly
- * without this...
- *
- * The caller must hold rcu lock or the tasklist lock.
- */
-struct pid *session_of_pgrp(struct pid *pgrp)
-{
-	struct task_struct *p;
-	struct pid *sid = NULL;
-
-	p = pid_task(pgrp, PIDTYPE_PGID);
-	if (p == NULL)
-		p = pid_task(pgrp, PIDTYPE_PID);
-	if (p != NULL)
-		sid = task_session(p);
-
-	return sid;
-}
-
 /*
  * Determine if a process group is "orphaned", according to the POSIX
  * definition in 2.2.2.52.  Orphaned process groups are not to be affected

commit 1029a2b52c09e479fd7b07275812ad97868c0fb0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 24 10:18:49 2014 +0200

    sched, exit: Deal with nested sleeps
    
    do_wait() is a big wait loop, but we set TASK_RUNNING too late; we end
    up calling potential sleeps before we reset it.
    
    Not strictly a bug since we're guaranteed to exit the loop and not
    call schedule(); put in annotations to quiet might_sleep().
    
     WARNING: CPU: 0 PID: 1 at ../kernel/sched/core.c:7123 __might_sleep+0x7e/0x90()
     do not call blocking ops when !TASK_RUNNING; state=1 set at [<ffffffff8109a788>] do_wait+0x88/0x270
    
     Call Trace:
      [<ffffffff81694991>] dump_stack+0x4e/0x7a
      [<ffffffff8109877c>] warn_slowpath_common+0x8c/0xc0
      [<ffffffff8109886c>] warn_slowpath_fmt+0x4c/0x50
      [<ffffffff810bca6e>] __might_sleep+0x7e/0x90
      [<ffffffff811a1c15>] might_fault+0x55/0xb0
      [<ffffffff8109a3fb>] wait_consider_task+0x90b/0xc10
      [<ffffffff8109a804>] do_wait+0x104/0x270
      [<ffffffff8109b837>] SyS_wait4+0x77/0x100
      [<ffffffff8169d692>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: tglx@linutronix.de
    Cc: umgwanakikbuti@gmail.com
    Cc: ilya.dryomov@inktank.com
    Cc: Alex Elder <alex.elder@linaro.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Axel Lin <axel.lin@ingics.com>
    Cc: Daniel Borkmann <dborkman@redhat.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Guillaume Morin <guillaume@morinfr.org>
    Cc: Ionut Alexa <ionut.m.alexa@gmail.com>
    Cc: Jason Baron <jbaron@akamai.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Michal Schmidt <mschmidt@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20140924082242.186408915@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5d30019ff953..232c4bc8bcc9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -997,6 +997,8 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 
 		get_task_struct(p);
 		read_unlock(&tasklist_lock);
+		sched_annotate_sleep();
+
 		if ((exit_code & 0x7f) == 0) {
 			why = CLD_EXITED;
 			status = exit_code >> 8;
@@ -1079,6 +1081,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	 * thread can reap it because we its state == DEAD/TRACE.
 	 */
 	read_unlock(&tasklist_lock);
+	sched_annotate_sleep();
 
 	retval = wo->wo_rusage
 		? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
@@ -1210,6 +1213,7 @@ static int wait_task_stopped(struct wait_opts *wo,
 	pid = task_pid_vnr(p);
 	why = ptrace ? CLD_TRAPPED : CLD_STOPPED;
 	read_unlock(&tasklist_lock);
+	sched_annotate_sleep();
 
 	if (unlikely(wo->wo_flags & WNOWAIT))
 		return wait_noreap_copyout(wo, p, pid, uid, why, exit_code);
@@ -1272,6 +1276,7 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 	pid = task_pid_vnr(p);
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);
+	sched_annotate_sleep();
 
 	if (!wo->wo_info) {
 		retval = wo->wo_rusage

commit faafcba3b5e15999cf75d5c5a513ac8e47e2545f
Merge: 13ead805c5a1 f10e00f4bf36
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 13 16:23:15 2014 +0200

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - Optimized support for Intel "Cluster-on-Die" (CoD) topologies (Dave
         Hansen)
    
       - Various sched/idle refinements for better idle handling (Nicolas
         Pitre, Daniel Lezcano, Chuansheng Liu, Vincent Guittot)
    
       - sched/numa updates and optimizations (Rik van Riel)
    
       - sysbench speedup (Vincent Guittot)
    
       - capacity calculation cleanups/refactoring (Vincent Guittot)
    
       - Various cleanups to thread group iteration (Oleg Nesterov)
    
       - Double-rq-lock removal optimization and various refactorings
         (Kirill Tkhai)
    
       - various sched/deadline fixes
    
      ... and lots of other changes"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (72 commits)
      sched/dl: Use dl_bw_of() under rcu_read_lock_sched()
      sched/fair: Delete resched_cpu() from idle_balance()
      sched, time: Fix build error with 64 bit cputime_t on 32 bit systems
      sched: Improve sysbench performance by fixing spurious active migration
      sched/x86: Fix up typo in topology detection
      x86, sched: Add new topology for multi-NUMA-node CPUs
      sched/rt: Use resched_curr() in task_tick_rt()
      sched: Use rq->rd in sched_setaffinity() under RCU read lock
      sched: cleanup: Rename 'out_unlock' to 'out_free_new_mask'
      sched: Use dl_bw_of() under RCU read lock
      sched/fair: Remove duplicate code from can_migrate_task()
      sched, mips, ia64: Remove __ARCH_WANT_UNLOCKED_CTXSW
      sched: print_rq(): Don't use tasklist_lock
      sched: normalize_rt_tasks(): Don't use _irqsave for tasklist_lock, use task_rq_lock()
      sched: Fix the task-group check in tg_has_rt_tasks()
      sched/fair: Leverage the idle state info when choosing the "idlest" cpu
      sched: Let the scheduler see CPU idle states
      sched/deadline: Fix inter- exclusive cpusets migrations
      sched/deadline: Clear dl_entity params when setscheduling to different class
      sched/numa: Kill the wrong/dead TASK_DEAD check in task_numa_fault()
      ...

commit e78c3496790ee8a36522a838b59b388e8a709e65
Author: Rik van Riel <riel@redhat.com>
Date:   Sat Aug 16 13:40:10 2014 -0400

    time, signal: Protect resource use statistics with seqlock
    
    Both times() and clock_gettime(CLOCK_PROCESS_CPUTIME_ID) have scalability
    issues on large systems, due to both functions being serialized with a
    lock.
    
    The lock protects against reporting a wrong value, due to a thread in the
    task group exiting, its statistics reporting up to the signal struct, and
    that exited task's statistics being counted twice (or not at all).
    
    Protecting that with a lock results in times() and clock_gettime() being
    completely serialized on large systems.
    
    This can be fixed by using a seqlock around the events that gather and
    propagate statistics. As an additional benefit, the protection code can
    be moved into thread_group_cputime(), slightly simplifying the calling
    functions.
    
    In the case of posix_cpu_clock_get_task() things can be simplified a
    lot, because the calling function already ensures that the task sticks
    around, and the rest is now taken care of in thread_group_cputime().
    
    This way the statistics reporting code can run lockless.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Daeseok Youn <daeseok.youn@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guillaume Morin <guillaume@morinfr.org>
    Cc: Ionut Alexa <ionut.m.alexa@gmail.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Michal Schmidt <mschmidt@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: umgwanakikbuti@gmail.com
    Cc: fweisbec@gmail.com
    Cc: srao@redhat.com
    Cc: lwoodman@redhat.com
    Cc: atheurer@redhat.com
    Link: http://lkml.kernel.org/r/20140816134010.26a9b572@annuminas.surriel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b93d46dab6fc..fa09b86609db 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -127,6 +127,7 @@ static void __exit_signal(struct task_struct *tsk)
 	 * the signal_struct.
 	 */
 	task_cputime(tsk, &utime, &stime);
+	write_seqlock(&sig->stats_lock);
 	sig->utime += utime;
 	sig->stime += stime;
 	sig->gtime += task_gtime(tsk);
@@ -140,6 +141,7 @@ static void __exit_signal(struct task_struct *tsk)
 	sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 	sig->nr_threads--;
 	__unhash_process(tsk, group_dead);
+	write_sequnlock(&sig->stats_lock);
 
 	/*
 	 * Do this under ->siglock, we can race with another thread
@@ -1042,6 +1044,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		spin_lock_irq(&p->real_parent->sighand->siglock);
 		psig = p->real_parent->signal;
 		sig = p->signal;
+		write_seqlock(&psig->stats_lock);
 		psig->cutime += tgutime + sig->cutime;
 		psig->cstime += tgstime + sig->cstime;
 		psig->cgtime += task_gtime(p) + sig->gtime + sig->cgtime;
@@ -1064,6 +1067,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 			psig->cmaxrss = maxrss;
 		task_io_accounting_add(&psig->ioac, &p->ioac);
 		task_io_accounting_add(&psig->ioac, &sig->ioac);
+		write_sequnlock(&psig->stats_lock);
 		spin_unlock_irq(&p->real_parent->sighand->siglock);
 	}
 

commit 90ed9cbe765ad358b3151a12b8bf889a3cbcd573
Author: Rik van Riel <riel@redhat.com>
Date:   Fri Aug 15 16:05:36 2014 -0400

    exit: Always reap resource stats in __exit_signal()
    
    Oleg pointed out that wait_task_zombie adds a task's usage statistics
    to the parent's signal struct, but the task's own signal struct should
    also propagate the statistics at exit time.
    
    This allows thread_group_cputime(reaped_zombie) to get the statistics
    after __unhash_process() has made the task invisible to for_each_thread,
    but before the thread has actually been rcu freed, making sure no
    non-monotonic results are returned inside that window.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Guillaume Morin <guillaume@morinfr.org>
    Cc: Ionut Alexa <ionut.m.alexa@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Michal Schmidt <mschmidt@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: umgwanakikbuti@gmail.com
    Cc: fweisbec@gmail.com
    Cc: srao@redhat.com
    Cc: lwoodman@redhat.com
    Cc: atheurer@redhat.com
    Link: http://lkml.kernel.org/r/1408133138-22048-2-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 32c58f7433a3..b93d46dab6fc 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -115,30 +115,29 @@ static void __exit_signal(struct task_struct *tsk)
 
 		if (tsk == sig->curr_target)
 			sig->curr_target = next_thread(tsk);
-		/*
-		 * Accumulate here the counters for all threads but the
-		 * group leader as they die, so they can be added into
-		 * the process-wide totals when those are taken.
-		 * The group leader stays around as a zombie as long
-		 * as there are other threads.  When it gets reaped,
-		 * the exit.c code will add its counts into these totals.
-		 * We won't ever get here for the group leader, since it
-		 * will have been the last reference on the signal_struct.
-		 */
-		task_cputime(tsk, &utime, &stime);
-		sig->utime += utime;
-		sig->stime += stime;
-		sig->gtime += task_gtime(tsk);
-		sig->min_flt += tsk->min_flt;
-		sig->maj_flt += tsk->maj_flt;
-		sig->nvcsw += tsk->nvcsw;
-		sig->nivcsw += tsk->nivcsw;
-		sig->inblock += task_io_get_inblock(tsk);
-		sig->oublock += task_io_get_oublock(tsk);
-		task_io_accounting_add(&sig->ioac, &tsk->ioac);
-		sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 	}
 
+	/*
+	 * Accumulate here the counters for all threads but the group leader
+	 * as they die, so they can be added into the process-wide totals
+	 * when those are taken.  The group leader stays around as a zombie as
+	 * long as there are other threads.  When it gets reaped, the exit.c
+	 * code will add its counts into these totals.  We won't ever get here
+	 * for the group leader, since it will have been the last reference on
+	 * the signal_struct.
+	 */
+	task_cputime(tsk, &utime, &stime);
+	sig->utime += utime;
+	sig->stime += stime;
+	sig->gtime += task_gtime(tsk);
+	sig->min_flt += tsk->min_flt;
+	sig->maj_flt += tsk->maj_flt;
+	sig->nvcsw += tsk->nvcsw;
+	sig->nivcsw += tsk->nivcsw;
+	sig->inblock += task_io_get_inblock(tsk);
+	sig->oublock += task_io_get_oublock(tsk);
+	task_io_accounting_add(&sig->ioac, &tsk->ioac);
+	sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 	sig->nr_threads--;
 	__unhash_process(tsk, group_dead);
 

commit 3f95aa81d265223fdb13ea2b59883766a05adbdf
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Aug 4 06:10:23 2014 -0700

    rcu: Make TASKS_RCU handle tasks that are almost done exiting
    
    Once a task has passed exit_notify() in the do_exit() code path, it
    is no longer on the task lists, and is therefore no longer visible
    to rcu_tasks_kthread().  This means that an almost-exited task might
    be preempted while within a trampoline, and this task won't be waited
    on by rcu_tasks_kthread().  This commit fixes this bug by adding an
    srcu_struct.  An exiting task does srcu_read_lock() just before calling
    exit_notify(), and does the corresponding srcu_read_unlock() after
    doing the final preempt_disable().  This means that rcu_tasks_kthread()
    can do synchronize_srcu() to wait for all mostly-exited tasks to reach
    their final preempt_disable() region, and then use synchronize_sched()
    to wait for those tasks to finish exiting.
    
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Suggested-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 32c58f7433a3..d13f2eec4bb8 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -667,6 +667,7 @@ void do_exit(long code)
 {
 	struct task_struct *tsk = current;
 	int group_dead;
+	TASKS_RCU(int tasks_rcu_i);
 
 	profile_task_exit(tsk);
 
@@ -775,6 +776,7 @@ void do_exit(long code)
 	 */
 	flush_ptrace_hw_breakpoint(tsk);
 
+	TASKS_RCU(tasks_rcu_i = __srcu_read_lock(&tasks_rcu_exit_srcu));
 	exit_notify(tsk, group_dead);
 	proc_exit_connector(tsk);
 #ifdef CONFIG_NUMA
@@ -814,6 +816,7 @@ void do_exit(long code)
 	if (tsk->nr_dirtied)
 		__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);
 	exit_rcu();
+	TASKS_RCU(__srcu_read_unlock(&tasks_rcu_exit_srcu, tasks_rcu_i));
 
 	/*
 	 * The setting of TASK_RUNNING by try_to_wake_up() may be delayed

commit a0be55dee71d437f7593c8c3673edd92962bafaf
Author: Ionut Alexa <ionut.m.alexa@gmail.com>
Date:   Fri Aug 8 14:21:18 2014 -0700

    kernel/exit.c: fix coding style warnings and errors
    
    Fixed coding style warnings and errors.
    
    Signed-off-by: Ionut Alexa <ionut.m.alexa@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 88c6b3e42583..32c58f7433a3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -59,7 +59,7 @@
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
 
-static void exit_mm(struct task_struct * tsk);
+static void exit_mm(struct task_struct *tsk);
 
 static void __unhash_process(struct task_struct *p, bool group_dead)
 {
@@ -151,7 +151,7 @@ static void __exit_signal(struct task_struct *tsk)
 	spin_unlock(&sighand->siglock);
 
 	__cleanup_sighand(sighand);
-	clear_tsk_thread_flag(tsk,TIF_SIGPENDING);
+	clear_tsk_thread_flag(tsk, TIF_SIGPENDING);
 	if (group_dead) {
 		flush_sigqueue(&sig->shared_pending);
 		tty_kref_put(tty);
@@ -168,7 +168,7 @@ static void delayed_put_task_struct(struct rcu_head *rhp)
 }
 
 
-void release_task(struct task_struct * p)
+void release_task(struct task_struct *p)
 {
 	struct task_struct *leader;
 	int zap_leader;
@@ -192,7 +192,8 @@ void release_task(struct task_struct * p)
 	 */
 	zap_leader = 0;
 	leader = p->group_leader;
-	if (leader != p && thread_group_empty(leader) && leader->exit_state == EXIT_ZOMBIE) {
+	if (leader != p && thread_group_empty(leader)
+			&& leader->exit_state == EXIT_ZOMBIE) {
 		/*
 		 * If we were the last child thread and the leader has
 		 * exited already, and the leader's parent ignores SIGCHLD,
@@ -241,7 +242,8 @@ struct pid *session_of_pgrp(struct pid *pgrp)
  *
  * "I ask you, have you ever known what it is to be an orphan?"
  */
-static int will_become_orphaned_pgrp(struct pid *pgrp, struct task_struct *ignored_task)
+static int will_become_orphaned_pgrp(struct pid *pgrp,
+					struct task_struct *ignored_task)
 {
 	struct task_struct *p;
 
@@ -294,9 +296,9 @@ kill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)
 	struct task_struct *ignored_task = tsk;
 
 	if (!parent)
-		 /* exit: our father is in a different pgrp than
-		  * we are and we were the only connection outside.
-		  */
+		/* exit: our father is in a different pgrp than
+		 * we are and we were the only connection outside.
+		 */
 		parent = tsk->real_parent;
 	else
 		/* reparent: our child is in a different pgrp than
@@ -405,7 +407,7 @@ void mm_update_next_owner(struct mm_struct *mm)
  * Turn us into a lazy TLB process if we
  * aren't already..
  */
-static void exit_mm(struct task_struct * tsk)
+static void exit_mm(struct task_struct *tsk)
 {
 	struct mm_struct *mm = tsk->mm;
 	struct core_state *core_state;
@@ -425,6 +427,7 @@ static void exit_mm(struct task_struct * tsk)
 	core_state = mm->core_state;
 	if (core_state) {
 		struct core_thread self;
+
 		up_read(&mm->mmap_sem);
 
 		self.task = tsk;
@@ -566,6 +569,7 @@ static void forget_original_parent(struct task_struct *father)
 
 	list_for_each_entry_safe(p, n, &father->children, sibling) {
 		struct task_struct *t = p;
+
 		do {
 			t->real_parent = reaper;
 			if (t->parent == father) {
@@ -599,7 +603,7 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	/*
 	 * This does two things:
 	 *
-  	 * A.  Make init inherit all the child processes
+	 * A.  Make init inherit all the child processes
 	 * B.  Check to see if any process groups have become orphaned
 	 *	as a result of our exiting, and if they have any stopped
 	 *	jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
@@ -649,9 +653,8 @@ static void check_stack_usage(void)
 
 	spin_lock(&low_water_lock);
 	if (free < lowest_to_date) {
-		printk(KERN_WARNING "%s (%d) used greatest stack depth: "
-				"%lu bytes left\n",
-				current->comm, task_pid_nr(current), free);
+		pr_warn("%s (%d) used greatest stack depth: %lu bytes left\n",
+			current->comm, task_pid_nr(current), free);
 		lowest_to_date = free;
 	}
 	spin_unlock(&low_water_lock);
@@ -692,8 +695,7 @@ void do_exit(long code)
 	 * leave this task alone and wait for reboot.
 	 */
 	if (unlikely(tsk->flags & PF_EXITING)) {
-		printk(KERN_ALERT
-			"Fixing recursive fault but reboot is needed!\n");
+		pr_alert("Fixing recursive fault but reboot is needed!\n");
 		/*
 		 * We can do this unlocked here. The futex code uses
 		 * this flag just to verify whether the pi state
@@ -717,9 +719,9 @@ void do_exit(long code)
 	raw_spin_unlock_wait(&tsk->pi_lock);
 
 	if (unlikely(in_atomic()))
-		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
-				current->comm, task_pid_nr(current),
-				preempt_count());
+		pr_info("note: %s[%d] exited with preempt_count %d\n",
+			current->comm, task_pid_nr(current),
+			preempt_count());
 
 	acct_update_integrals(tsk);
 	/* sync mm's RSS info before statistics gathering */
@@ -837,7 +839,6 @@ void do_exit(long code)
 	for (;;)
 		cpu_relax();	/* For when BUG is null */
 }
-
 EXPORT_SYMBOL_GPL(do_exit);
 
 void complete_and_exit(struct completion *comp, long code)
@@ -847,7 +848,6 @@ void complete_and_exit(struct completion *comp, long code)
 
 	do_exit(code);
 }
-
 EXPORT_SYMBOL(complete_and_exit);
 
 SYSCALL_DEFINE1(exit, int, error_code)
@@ -870,6 +870,7 @@ do_group_exit(int exit_code)
 		exit_code = sig->group_exit_code;
 	else if (!thread_group_empty(current)) {
 		struct sighand_struct *const sighand = current->sighand;
+
 		spin_lock_irq(&sighand->siglock);
 		if (signal_group_exit(sig))
 			/* Another thread got here before we took the lock.  */
@@ -1034,9 +1035,9 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		 * as other threads in the parent group can be right
 		 * here reaping other children at the same time.
 		 *
-		 * We use thread_group_cputime_adjusted() to get times for the thread
-		 * group, which consolidates times for all threads in the
-		 * group including the group leader.
+		 * We use thread_group_cputime_adjusted() to get times for
+		 * the thread group, which consolidates times for all threads
+		 * in the group including the group leader.
 		 */
 		thread_group_cputime_adjusted(p, &tgutime, &tgstime);
 		spin_lock_irq(&p->real_parent->sighand->siglock);
@@ -1418,6 +1419,7 @@ static int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)
 
 	list_for_each_entry(p, &tsk->children, sibling) {
 		int ret = wait_consider_task(wo, 0, p);
+
 		if (ret)
 			return ret;
 	}
@@ -1431,6 +1433,7 @@ static int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)
 
 	list_for_each_entry(p, &tsk->ptraced, ptrace_entry) {
 		int ret = wait_consider_task(wo, 1, p);
+
 		if (ret)
 			return ret;
 	}

commit fb794bcbb4e5552242f9a4c5e1ffe4c6da29a968
Author: David Rientjes <rientjes@google.com>
Date:   Wed Aug 6 16:07:58 2014 -0700

    mm, oom: remove unnecessary exit_state check
    
    The oom killer scans each process and determines whether it is eligible
    for oom kill or whether the oom killer should abort because of
    concurrent memory freeing.  It will abort when an eligible process is
    found to have TIF_MEMDIE set, meaning it has already been oom killed and
    we're waiting for it to exit.
    
    Processes with task->mm == NULL should not be considered because they
    are either kthreads or have already detached their memory and killing
    them would not lead to memory freeing.  That memory is only freed after
    exit_mm() has returned, however, and not when task->mm is first set to
    NULL.
    
    Clear TIF_MEMDIE after exit_mm()'s mmput() so that an oom killed process
    is no longer considered for oom kill, but only until exit_mm() has
    returned.  This was fragile in the past because it relied on
    exit_notify() to be reached before no longer considering TIF_MEMDIE
    processes.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e5c4668f1799..88c6b3e42583 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -455,6 +455,7 @@ static void exit_mm(struct task_struct * tsk)
 	task_unlock(tsk);
 	mm_update_next_owner(mm);
 	mmput(mm);
+	clear_thread_flag(TIF_MEMDIE);
 }
 
 /*

commit 0341729b4b832e753c5e745c6ba0e797f6198be0
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jun 6 14:36:53 2014 -0700

    signals: mv {dis,}allow_signal() from sched.h/exit.c to signal.[ch]
    
    Move the declaration/definition of allow_signal/disallow_signal to
    signal.h/signal.c.  The new place is more logical and allows to use the
    static helpers in signal.c (see the next changes).
    
    While at it, make them return void and remove the valid_signal() check.
    Nobody checks the returned value, and in-kernel users must not pass the
    wrong signal number.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 750c2e594617..e5c4668f1799 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -313,45 +313,6 @@ kill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)
 	}
 }
 
-/*
- * Let kernel threads use this to say that they allow a certain signal.
- * Must not be used if kthread was cloned with CLONE_SIGHAND.
- */
-int allow_signal(int sig)
-{
-	if (!valid_signal(sig) || sig < 1)
-		return -EINVAL;
-
-	spin_lock_irq(&current->sighand->siglock);
-	/* This is only needed for daemonize()'ed kthreads */
-	sigdelset(&current->blocked, sig);
-	/*
-	 * Kernel threads handle their own signals. Let the signal code
-	 * know it'll be handled, so that they don't get converted to
-	 * SIGKILL or just silently dropped.
-	 */
-	current->sighand->action[(sig)-1].sa.sa_handler = (void __user *)2;
-	recalc_sigpending();
-	spin_unlock_irq(&current->sighand->siglock);
-	return 0;
-}
-
-EXPORT_SYMBOL(allow_signal);
-
-int disallow_signal(int sig)
-{
-	if (!valid_signal(sig) || sig < 1)
-		return -EINVAL;
-
-	spin_lock_irq(&current->sighand->siglock);
-	current->sighand->action[(sig)-1].sa.sa_handler = SIG_IGN;
-	recalc_sigpending();
-	spin_unlock_irq(&current->sighand->siglock);
-	return 0;
-}
-
-EXPORT_SYMBOL(disallow_signal);
-
 #ifdef CONFIG_MEMCG
 /*
  * A task is exiting.   If it owned this mm, find a new owner for the mm.

commit 39af1765f1255b2bbadc3064e16270781abf24a1
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 4 16:07:54 2014 -0700

    memcg: optimize the "Search everything else" loop in mm_update_next_owner()
    
    for_each_process_thread() is sub-optimal. All threads share the same
    ->mm, we can swicth to the next process once we found a thread with
    ->mm != NULL and ->mm != mm.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Peter Chiang <pchiang@nvidia.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5ac3c19c245c..750c2e594617 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -397,9 +397,15 @@ void mm_update_next_owner(struct mm_struct *mm)
 	/*
 	 * Search through everything else, we should not get here often.
 	 */
-	for_each_process_thread(g, c) {
-		if (!(c->flags & PF_KTHREAD) && c->mm == mm)
-			goto assign_new_owner;
+	for_each_process(g) {
+		if (g->flags & PF_KTHREAD)
+			continue;
+		for_each_thread(g, c) {
+			if (c->mm == mm)
+				goto assign_new_owner;
+			if (c->mm)
+				break;
+		}
 	}
 	read_unlock(&tasklist_lock);
 	/*

commit f87fb599ae4d2a152a93f9821b94f3158146d097
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 4 16:07:52 2014 -0700

    memcg: mm_update_next_owner() should skip kthreads
    
    "Search through everything else" in mm_update_next_owner() can hit a
    kthread which adopted this "mm" via use_mm(), it should not be used as
    mm->owner.  Add the PF_KTHREAD check.
    
    While at it, change this code to use for_each_process_thread() instead
    of deprecated do_each_thread/while_each_thread.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Peter Chiang <pchiang@nvidia.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index da1b838de8a6..5ac3c19c245c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -395,14 +395,12 @@ void mm_update_next_owner(struct mm_struct *mm)
 	}
 
 	/*
-	 * Search through everything else. We should not get
-	 * here often
+	 * Search through everything else, we should not get here often.
 	 */
-	do_each_thread(g, c) {
-		if (c->mm == mm)
+	for_each_process_thread(g, c) {
+		if (!(c->flags & PF_KTHREAD) && c->mm == mm)
 			goto assign_new_owner;
-	} while_each_thread(g, c);
-
+	}
 	read_unlock(&tasklist_lock);
 	/*
 	 * We found no owner yet mm_users > 1: this implies that we are

commit f98bafa06a28fdfdd5c49f820f4d6560f636fc46
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 4 16:07:34 2014 -0700

    memcg: kill CONFIG_MM_OWNER
    
    CONFIG_MM_OWNER makes no sense.  It is not user-selectable, it is only
    selected by CONFIG_MEMCG automatically.  So we can kill this option in
    init/Kconfig and do s/CONFIG_MM_OWNER/CONFIG_MEMCG/ globally.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6ed6a1d552b5..da1b838de8a6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -352,7 +352,7 @@ int disallow_signal(int sig)
 
 EXPORT_SYMBOL(disallow_signal);
 
-#ifdef CONFIG_MM_OWNER
+#ifdef CONFIG_MEMCG
 /*
  * A task is exiting.   If it owned this mm, find a new owner for the mm.
  */
@@ -434,7 +434,7 @@ void mm_update_next_owner(struct mm_struct *mm)
 	task_unlock(c);
 	put_task_struct(c);
 }
-#endif /* CONFIG_MM_OWNER */
+#endif /* CONFIG_MEMCG */
 
 /*
  * Turn us into a lazy TLB process if we

commit 7c733eb3eac0e3d091aaf37c183d2175eeebfb2b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:49 2014 -0700

    wait: WSTOPPED|WCONTINUED doesn't work if a zombie leader is traced by another process
    
    Even if the main thread is dead the process still can stop/continue.
    However, if the leader is ptraced wait_consider_task(ptrace => false)
    always skips wait_task_stopped/wait_task_continued, so WSTOPPED or
    WCONTINUED can never work for the natural parent in this case.
    
    Move the "A zombie ptracee is only visible to its ptracer" check into the
    "if (!delay_group_leader(p))" block.  ->notask_error is cleared by the
    "fall through" code below.
    
    This depends on the previous change, wait_task_stopped/continued must be
    avoided if !delay_group_leader() and the tracer is ->real_parent.
    Otherwise WSTOPPED|WEXITED could wrongly report "stopped" when the child
    is already dead (single-threaded or not).  If it is traced by another task
    then the "stopped" state is fine until the debugger detaches and reveals a
    zombie state.
    
    Stupid test-case:
    
            void *tfunc(void *arg)
            {
                    sleep(1);       // wait for zombie leader
                    raise(SIGSTOP);
                    exit(0x13);
                    return NULL;
            }
    
            int run_child(void)
            {
                    pthread_t thread;
    
                    if (!fork()) {
                            int tracee = getppid();
    
                            assert(ptrace(PTRACE_ATTACH, tracee, 0,0) == 0);
                            do
                                    ptrace(PTRACE_CONT, tracee, 0,0);
                            while (wait(NULL) > 0);
    
                            return 0;
                    }
    
                    sleep(1);       // wait for PTRACE_ATTACH
                    assert(pthread_create(&thread, NULL, tfunc, NULL) == 0);
                    pthread_exit(NULL);
            }
    
            int main(void)
            {
                    int child, stat;
    
                    child = fork();
                    if (!child)
                            return run_child();
    
                    assert(child == waitpid(-1, &stat, WSTOPPED));
                    assert(stat == 0x137f);
    
                    kill(child, SIGCONT);
    
                    assert(child == waitpid(-1, &stat, WCONTINUED));
                    assert(stat == 0xffff);
    
                    assert(child == waitpid(-1, &stat, 0));
                    assert(stat == 0x1300);
    
                    return 0;
            }
    
    Without this patch it hangs in waitpid(WSTOPPED), wait_task_stopped() is
    never called.
    
    Note: this doesn't fix all problems with a zombie delay_group_leader(),
    WCONTINUED | WEXITED check is not exactly right.  debugger can't assume it
    will be notified if another thread reaps the whole thread group.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Cc: Michal Schmidt <mschmidt@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 92d38d4da4b1..6ed6a1d552b5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1380,20 +1380,16 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 
 	/* slay zombie? */
 	if (p->exit_state == EXIT_ZOMBIE) {
-		/*
-		 * A zombie ptracee is only visible to its ptracer.
-		 * Notification and reaping will be cascaded to the real
-		 * parent when the ptracer detaches.
-		 */
-		if (likely(!ptrace) && unlikely(p->ptrace)) {
-			/* it will become visible, clear notask_error */
-			wo->notask_error = 0;
-			return 0;
-		}
-
 		/* we don't reap group leaders with subthreads */
-		if (!delay_group_leader(p))
-			return wait_task_zombie(wo, p);
+		if (!delay_group_leader(p)) {
+			/*
+			 * A zombie ptracee is only visible to its ptracer.
+			 * Notification and reaping will be cascaded to the
+			 * real parent when the ptracer detaches.
+			 */
+			if (unlikely(ptrace) || likely(!p->ptrace))
+				return wait_task_zombie(wo, p);
+		}
 
 		/*
 		 * Allow access to stopped/continued state via zombie by

commit 377d75dafa07ee0da64223c9169f4e17b26c2b9a
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:47 2014 -0700

    wait: WSTOPPED|WCONTINUED hangs if a zombie child is traced by real_parent
    
    "A zombie is only visible to its ptracer" logic in wait_consider_task()
    is very wrong. Trivial test-case:
    
            #include <unistd.h>
            #include <sys/ptrace.h>
            #include <sys/wait.h>
            #include <assert.h>
    
            int main(void)
            {
                    int child = fork();
    
                    if (!child) {
                            assert(ptrace(PTRACE_TRACEME, 0,0,0) == 0);
                            return 0x23;
                    }
    
                    assert(waitid(P_ALL, child, NULL, WEXITED | WNOWAIT) == 0);
                    assert(waitid(P_ALL, 0, NULL, WSTOPPED) == -1);
                    return 0;
            }
    
    it hangs in waitpid(WSTOPPED) despite the fact it has a single zombie
    child.  This is because wait_consider_task(ptrace => 0) sees p->ptrace and
    cleares ->notask_error assuming that the debugger should detach and notify
    us.
    
    Change wait_consider_task(ptrace => 0) to pretend that ptrace == T if the
    child is traced by us.  This really simplifies the logic and allows us to
    do more fixes, see the next changes.  This also hides the unwanted group
    stop state automatically, we can remove another ptrace_reparented() check.
    
    Unfortunately, this adds the following behavioural changes:
    
            1. Before this patch wait(WEXITED | __WNOTHREAD) does not reap
               a natural child if it is traced by the caller's sub-thread.
    
               Hopefully nobody will ever notice this change, and I think
               that nobody should rely on this behaviour anyway.
    
            2. SIGNAL_STOP_CONTINUED is no longer hidden from debugger if
               it is real parent.
    
               While this change comes as a side effect, I think it is good
               by itself. The group continued state can not be consumed by
               another process in this case, it doesn't depend on ptrace,
               it doesn't make sense to hide it from real parent.
    
               Perhaps we should add the thread_group_leader() check before
               wait_task_continued()? May be, but this shouldn't depend on
               ptrace_reparented().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Cc: Michal Schmidt <mschmidt@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 33cf8dba0a61..92d38d4da4b1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1362,6 +1362,22 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		return 0;
 	}
 
+	if (likely(!ptrace) && unlikely(p->ptrace)) {
+		/*
+		 * If it is traced by its real parent's group, just pretend
+		 * the caller is ptrace_do_wait() and reap this child if it
+		 * is zombie.
+		 *
+		 * This also hides group stop state from real parent; otherwise
+		 * a single stop can be reported twice as group and ptrace stop.
+		 * If a ptracer wants to distinguish these two events for its
+		 * own children it should create a separate process which takes
+		 * the role of real parent.
+		 */
+		if (!ptrace_reparented(p))
+			ptrace = 1;
+	}
+
 	/* slay zombie? */
 	if (p->exit_state == EXIT_ZOMBIE) {
 		/*
@@ -1402,19 +1418,6 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		if (likely(!ptrace) || (wo->wo_flags & (WCONTINUED | WEXITED)))
 			wo->notask_error = 0;
 	} else {
-		/*
-		 * If @p is ptraced by a task in its real parent's group,
-		 * hide group stop/continued state when looking at @p as
-		 * the real parent; otherwise, a single stop can be
-		 * reported twice as group and ptrace stops.
-		 *
-		 * If a ptracer wants to distinguish the two events for its
-		 * own children, it should create a separate process which
-		 * takes the role of real parent.
-		 */
-		if (likely(!ptrace) && p->ptrace && !ptrace_reparented(p))
-			return 0;
-
 		/*
 		 * @p is alive and it's gonna stop, continue or exit, so
 		 * there always is something to wait for.

commit b3ab03160dfaf8ab78d476b670de319f4c1a5685
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:45 2014 -0700

    wait: completely ignore the EXIT_DEAD tasks
    
    Now that EXIT_DEAD is the terminal state it doesn't make sense to call
    eligible_child() or security_task_wait() if the task is really dead.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Tested-by: Michal Schmidt <mschmidt@redhat.com>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4773ed990907..33cf8dba0a61 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1329,7 +1329,12 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 static int wait_consider_task(struct wait_opts *wo, int ptrace,
 				struct task_struct *p)
 {
-	int ret = eligible_child(wo, p);
+	int ret;
+
+	if (unlikely(p->exit_state == EXIT_DEAD))
+		return 0;
+
+	ret = eligible_child(wo, p);
 	if (!ret)
 		return ret;
 
@@ -1347,10 +1352,6 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		return 0;
 	}
 
-	/* dead body doesn't have much to contribute */
-	if (unlikely(p->exit_state == EXIT_DEAD))
-		return 0;
-
 	if (unlikely(p->exit_state == EXIT_TRACE)) {
 		/*
 		 * ptrace == 0 means we are the natural parent. In this case

commit b436069059fede30ca31d4bf439cc86436ff5b1d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:43 2014 -0700

    wait: use EXIT_TRACE only if thread_group_leader(zombie)
    
    wait_task_zombie() always uses EXIT_TRACE/ptrace_unlink() if
    ptrace_reparented().  This is suboptimal and a bit confusing: we do not
    need do_notify_parent(p) if !thread_group_leader(p) and in this case we
    also do not need ptrace_unlink(), we can rely on ptrace_release_task().
    
    Change wait_task_zombie() to check thread_group_leader() along with
    ptrace_reparented() and simplify the final p->exit_state transition.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Tested-by: Michal Schmidt <mschmidt@redhat.com>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 022a0ff17318..4773ed990907 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1040,7 +1040,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	/*
 	 * Move the task's state to DEAD/TRACE, only one thread can do this.
 	 */
-	state = traced ? EXIT_TRACE : EXIT_DEAD;
+	state = traced && thread_group_leader(p) ? EXIT_TRACE : EXIT_DEAD;
 	if (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)
 		return 0;
 	/*
@@ -1140,18 +1140,15 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	if (!retval)
 		retval = pid;
 
-	if (traced) {
+	if (state == EXIT_TRACE) {
 		write_lock_irq(&tasklist_lock);
 		/* We dropped tasklist, ptracer could die and untrace */
 		ptrace_unlink(p);
-		/*
-		 * If this is not a sub-thread, notify the parent.
-		 * If parent wants a zombie, don't release it now.
-		 */
-		state = EXIT_DEAD;
-		if (thread_group_leader(p) &&
-		    !do_notify_parent(p, p->exit_signal))
-			state = EXIT_ZOMBIE;
+
+		/* If parent wants a zombie, don't release it now */
+		state = EXIT_ZOMBIE;
+		if (do_notify_parent(p, p->exit_signal))
+			state = EXIT_DEAD;
 		p->exit_state = state;
 		write_unlock_irq(&tasklist_lock);
 	}

commit abd50b39e783e1b6c75c7534c37f1eb2d94a89cd
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:42 2014 -0700

    wait: introduce EXIT_TRACE to avoid the racy EXIT_DEAD->EXIT_ZOMBIE transition
    
    wait_task_zombie() first does EXIT_ZOMBIE->EXIT_DEAD transition and
    drops tasklist_lock.  If this task is not the natural child and it is
    traced, we change its state back to EXIT_ZOMBIE for ->real_parent.
    
    The last transition is racy, this is even documented in 50b8d257486a
    "ptrace: partially fix the do_wait(WEXITED) vs EXIT_DEAD->EXIT_ZOMBIE
    race".  wait_consider_task() tries to detect this transition and clear
    ->notask_error but we can't rely on ptrace_reparented(), debugger can
    exit and do ptrace_unlink() before its sub-thread sets EXIT_ZOMBIE.
    
    And there is another problem which were missed before: this transition
    can also race with reparent_leader() which doesn't reset >exit_signal if
    EXIT_DEAD, assuming that this task must be reaped by someone else.  So
    the tracee can be re-parented with ->exit_signal != SIGCHLD, and if
    /sbin/init doesn't use __WALL it becomes unreapable.  This was fixed by
    the previous commit, but it was the temporary hack.
    
    1. Add the new exit_state, EXIT_TRACE. It means that the task is the
       traced zombie, debugger is going to detach and notify its natural
       parent.
    
       This new state is actually EXIT_ZOMBIE | EXIT_DEAD. This way we
       can avoid the changes in proc/kgdb code, get_task_state() still
       reports "X (dead)" in this case.
    
       Note: with or without this change userspace can see Z -> X -> Z
       transition. Not really bad, but probably makes sense to fix.
    
    2. Change wait_task_zombie() to use EXIT_TRACE instead of EXIT_DEAD
       if we need to notify the ->real_parent.
    
    3. Revert the previous hack in reparent_leader(), now that EXIT_DEAD
       is always the final state we can safely ignore such a task.
    
    4. Change wait_consider_task() to check EXIT_TRACE separately and kill
       the racy and no longer needed ptrace_reparented() case.
    
       If ptrace == T an EXIT_TRACE thread should be simply ignored, the
       owner of this state is going to ptrace_unlink() this task. We can
       pretend that it was already removed from ->ptraced list.
    
       Otherwise we should skip this thread too but clear ->notask_error,
       we must be the natural parent and debugger is going to untrace and
       notify us. IOW, this doesn't differ from "EXIT_ZOMBIE && p->ptrace"
       even if the task was already untraced.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reported-by: Jan Kratochvil <jan.kratochvil@redhat.com>
    Reported-by: Michal Schmidt <mschmidt@redhat.com>
    Tested-by: Michal Schmidt <mschmidt@redhat.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e354cbb13a9b..022a0ff17318 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -560,6 +560,9 @@ static void reparent_leader(struct task_struct *father, struct task_struct *p,
 				struct list_head *dead)
 {
 	list_move_tail(&p->sibling, &p->real_parent->children);
+
+	if (p->exit_state == EXIT_DEAD)
+		return;
 	/*
 	 * If this is a threaded reparent there is no need to
 	 * notify anyone anything has happened.
@@ -567,19 +570,9 @@ static void reparent_leader(struct task_struct *father, struct task_struct *p,
 	if (same_thread_group(p->real_parent, father))
 		return;
 
-	/*
-	 * We don't want people slaying init.
-	 *
-	 * Note: we do this even if it is EXIT_DEAD, wait_task_zombie()
-	 * can change ->exit_state to EXIT_ZOMBIE. If this is the final
-	 * state, do_notify_parent() was already called and ->exit_signal
-	 * doesn't matter.
-	 */
+	/* We don't want people slaying init. */
 	p->exit_signal = SIGCHLD;
 
-	if (p->exit_state == EXIT_DEAD)
-		return;
-
 	/* If it has exited notify the new parent about this child's death. */
 	if (!p->ptrace &&
 	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {
@@ -1043,17 +1036,13 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		return wait_noreap_copyout(wo, p, pid, uid, why, status);
 	}
 
+	traced = ptrace_reparented(p);
 	/*
-	 * Try to move the task's state to DEAD
-	 * only one thread is allowed to do this:
+	 * Move the task's state to DEAD/TRACE, only one thread can do this.
 	 */
-	state = xchg(&p->exit_state, EXIT_DEAD);
-	if (state != EXIT_ZOMBIE) {
-		BUG_ON(state != EXIT_DEAD);
+	state = traced ? EXIT_TRACE : EXIT_DEAD;
+	if (cmpxchg(&p->exit_state, EXIT_ZOMBIE, state) != EXIT_ZOMBIE)
 		return 0;
-	}
-
-	traced = ptrace_reparented(p);
 	/*
 	 * It can be ptraced but not reparented, check
 	 * thread_group_leader() to filter out sub-threads.
@@ -1114,7 +1103,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 
 	/*
 	 * Now we are sure this task is interesting, and no other
-	 * thread can reap it because we set its state to EXIT_DEAD.
+	 * thread can reap it because we its state == DEAD/TRACE.
 	 */
 	read_unlock(&tasklist_lock);
 
@@ -1159,14 +1148,14 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		 * If this is not a sub-thread, notify the parent.
 		 * If parent wants a zombie, don't release it now.
 		 */
+		state = EXIT_DEAD;
 		if (thread_group_leader(p) &&
-		    !do_notify_parent(p, p->exit_signal)) {
-			p->exit_state = EXIT_ZOMBIE;
-			p = NULL;
-		}
+		    !do_notify_parent(p, p->exit_signal))
+			state = EXIT_ZOMBIE;
+		p->exit_state = state;
 		write_unlock_irq(&tasklist_lock);
 	}
-	if (p != NULL)
+	if (state == EXIT_DEAD)
 		release_task(p);
 
 	return retval;
@@ -1362,12 +1351,15 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 	}
 
 	/* dead body doesn't have much to contribute */
-	if (unlikely(p->exit_state == EXIT_DEAD)) {
+	if (unlikely(p->exit_state == EXIT_DEAD))
+		return 0;
+
+	if (unlikely(p->exit_state == EXIT_TRACE)) {
 		/*
-		 * But do not ignore this task until the tracer does
-		 * wait_task_zombie()->do_notify_parent().
+		 * ptrace == 0 means we are the natural parent. In this case
+		 * we should clear notask_error, debugger will notify us.
 		 */
-		if (likely(!ptrace) && unlikely(ptrace_reparented(p)))
+		if (likely(!ptrace))
 			wo->notask_error = 0;
 		return 0;
 	}

commit dfccbb5e49a621c1b21a62527d61fc4305617aca
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:41 2014 -0700

    wait: fix reparent_leader() vs EXIT_DEAD->EXIT_ZOMBIE race
    
    wait_task_zombie() first does EXIT_ZOMBIE->EXIT_DEAD transition and
    drops tasklist_lock.  If this task is not the natural child and it is
    traced, we change its state back to EXIT_ZOMBIE for ->real_parent.
    
    The last transition is racy, this is even documented in 50b8d257486a
    "ptrace: partially fix the do_wait(WEXITED) vs EXIT_DEAD->EXIT_ZOMBIE
    race".  wait_consider_task() tries to detect this transition and clear
    ->notask_error but we can't rely on ptrace_reparented(), debugger can
    exit and do ptrace_unlink() before its sub-thread sets EXIT_ZOMBIE.
    
    And there is another problem which were missed before: this transition
    can also race with reparent_leader() which doesn't reset >exit_signal if
    EXIT_DEAD, assuming that this task must be reaped by someone else.  So
    the tracee can be re-parented with ->exit_signal != SIGCHLD, and if
    /sbin/init doesn't use __WALL it becomes unreapable.
    
    Change reparent_leader() to update ->exit_signal even if EXIT_DEAD.
    Note: this is the simple temporary hack for -stable, it doesn't try to
    solve all problems, it will be reverted by the next changes.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reported-by: Jan Kratochvil <jan.kratochvil@redhat.com>
    Reported-by: Michal Schmidt <mschmidt@redhat.com>
    Tested-by: Michal Schmidt <mschmidt@redhat.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Cc: Lennart Poettering <lpoetter@redhat.com>
    Cc: Roland McGrath <roland@hack.frob.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index decf648574f6..e354cbb13a9b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -560,9 +560,6 @@ static void reparent_leader(struct task_struct *father, struct task_struct *p,
 				struct list_head *dead)
 {
 	list_move_tail(&p->sibling, &p->real_parent->children);
-
-	if (p->exit_state == EXIT_DEAD)
-		return;
 	/*
 	 * If this is a threaded reparent there is no need to
 	 * notify anyone anything has happened.
@@ -570,9 +567,19 @@ static void reparent_leader(struct task_struct *father, struct task_struct *p,
 	if (same_thread_group(p->real_parent, father))
 		return;
 
-	/* We don't want people slaying init.  */
+	/*
+	 * We don't want people slaying init.
+	 *
+	 * Note: we do this even if it is EXIT_DEAD, wait_task_zombie()
+	 * can change ->exit_state to EXIT_ZOMBIE. If this is the final
+	 * state, do_notify_parent() was already called and ->exit_signal
+	 * doesn't matter.
+	 */
 	p->exit_signal = SIGCHLD;
 
+	if (p->exit_state == EXIT_DEAD)
+		return;
+
 	/* If it has exited notify the new parent about this child's death. */
 	if (!p->ptrace &&
 	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {

commit ef9823939e5acd5d323ff61fbc427ef998dd203e
Author: Guillaume Morin <guillaume@morinfr.org>
Date:   Mon Apr 7 15:38:31 2014 -0700

    kernel/exit.c: call proc_exit_connector() after exit_state is set
    
    The process events connector delivers a notification when a process
    exits.  This is really convenient for a process that spawns and wants to
    monitor its children through an epoll-able() interface.
    
    Unfortunately, there is a small window between when the event is
    delivered and the child become wait()-able.
    
    This is creates a race if the parent wants to make sure that it knows
    about the exit, e.g
    
    pid_t pid = fork();
    if (pid > 0) {
            register_interest_for_pid(pid);
            if (waitpid(pid, NULL, WNOHANG) > 0)
            {
              /* We might have raced with exit() */
            }
            return;
    }
    
    /* Child */
    execve(...)
    
    register_interest_for_pid() would be telling the the connector socket
    reader to pay attention to events related to pid.
    
    Though this is not a bug, I think it would make the connector a bit more
    usable if this race was closed by simply moving the call to
    proc_exit_connector() from just before exit_notify() to right after.
    
    Oleg said:
    
    : Even with this patch the code above is still "racy" if the child is
    : multi-threaded.  Plus it should obviously filter-out subthreads.  And
    : afaics there is no way to make it reliable, even if you change the code
    : above so that waitpid() is called only after the last thread exits WNOHANG
    : still can fail.
    
    Signed-off-by: Guillaume Morin <guillaume@morinfr.org>
    Cc: Matt Helsley <matt.helsley@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 171c9a9d7b00..decf648574f6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -802,13 +802,13 @@ void do_exit(long code)
 
 	module_put(task_thread_info(tsk)->exec_domain->module);
 
-	proc_exit_connector(tsk);
 	/*
 	 * FIXME: do that only when needed, using sched_exit tracepoint
 	 */
 	flush_ptrace_hw_breakpoint(tsk);
 
 	exit_notify(tsk, group_dead);
+	proc_exit_connector(tsk);
 #ifdef CONFIG_NUMA
 	task_lock(tsk);
 	mpol_put(tsk->mempolicy);

commit 4bcb8232cf4eb061b086c10f56b6808adcdb5a93
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:30 2014 -0700

    exit: move check_stack_usage() to the end of do_exit()
    
    It is not clear why check_stack_usage() is called so early and thus it
    never checks the stack usage in, say, exit_notify() or
    flush_ptrace_hw_breakpoint() or other functions which are only called by
    do_exit().
    
    Move the callsite down to the last preempt_disable/schedule.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 11f9e39a7368..171c9a9d7b00 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -788,7 +788,6 @@ void do_exit(long code)
 		disassociate_ctty(1);
 	exit_task_namespaces(tsk);
 	exit_task_work(tsk);
-	check_stack_usage();
 	exit_thread();
 
 	/*
@@ -842,6 +841,7 @@ void do_exit(long code)
 
 	validate_creds_for_do_exit(tsk);
 
+	check_stack_usage();
 	preempt_disable();
 	if (tsk->nr_dirtied)
 		__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);

commit c39df5fa37b0623589508c95515b4aa1531c524e
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 7 15:38:29 2014 -0700

    exit: call disassociate_ctty() before exit_task_namespaces()
    
    Commit 8aac62706ada ("move exit_task_namespaces() outside of
    exit_notify()") breaks pppd and the exiting service crashes the kernel:
    
        BUG: unable to handle kernel NULL pointer dereference at 0000000000000028
        IP: ppp_register_channel+0x13/0x20 [ppp_generic]
        Call Trace:
          ppp_asynctty_open+0x12b/0x170 [ppp_async]
          tty_ldisc_open.isra.2+0x27/0x60
          tty_ldisc_hangup+0x1e3/0x220
          __tty_hangup+0x2c4/0x440
          disassociate_ctty+0x61/0x270
          do_exit+0x7f2/0xa50
    
    ppp_register_channel() needs ->net_ns and current->nsproxy == NULL.
    
    Move disassociate_ctty() before exit_task_namespaces(), it doesn't make
    sense to delay it after perf_event_exit_task() or cgroup_exit().
    
    This also allows to use task_work_add() inside the (nontrivial) code
    paths in disassociate_ctty().
    
    Investigated by Peter Hurley.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reported-by: Sree Harsha Totakura <sreeharsha@totakura.in>
    Cc: Peter Hurley <peter@hurleysoftware.com>
    Cc: Sree Harsha Totakura <sreeharsha@totakura.in>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Andrey Vagin <avagin@openvz.org>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: <stable@vger.kernel.org>    [v3.10+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6480d1c85d7a..11f9e39a7368 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -784,6 +784,8 @@ void do_exit(long code)
 	exit_shm(tsk);
 	exit_files(tsk);
 	exit_fs(tsk);
+	if (group_dead)
+		disassociate_ctty(1);
 	exit_task_namespaces(tsk);
 	exit_task_work(tsk);
 	check_stack_usage();
@@ -799,13 +801,9 @@ void do_exit(long code)
 
 	cgroup_exit(tsk);
 
-	if (group_dead)
-		disassociate_ctty(1);
-
 	module_put(task_thread_info(tsk)->exec_domain->module);
 
 	proc_exit_connector(tsk);
-
 	/*
 	 * FIXME: do that only when needed, using sched_exit tracepoint
 	 */

commit 1ec41830e087cda1f62dda4182c2b62811eb0ffc
Author: Li Zefan <lizefan@huawei.com>
Date:   Fri Mar 28 15:22:19 2014 +0800

    cgroup: remove useless argument from cgroup_exit()
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1e77fc645317..6480d1c85d7a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -797,7 +797,7 @@ void do_exit(long code)
 	 */
 	perf_event_exit_task(tsk);
 
-	cgroup_exit(tsk, 1);
+	cgroup_exit(tsk);
 
 	if (group_dead)
 		disassociate_ctty(1);

commit 0c740d0afc3bff0a097ad03a1c8df92757516f5c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jan 21 15:49:56 2014 -0800

    introduce for_each_thread() to replace the buggy while_each_thread()
    
    while_each_thread() and next_thread() should die, almost every lockless
    usage is wrong.
    
    1. Unless g == current, the lockless while_each_thread() is not safe.
    
       while_each_thread(g, t) can loop forever if g exits, next_thread()
       can't reach the unhashed thread in this case. Note that this can
       happen even if g is the group leader, it can exec.
    
    2. Even if while_each_thread() itself was correct, people often use
       it wrongly.
    
       It was never safe to just take rcu_read_lock() and loop unless
       you verify that pid_alive(g) == T, even the first next_thread()
       can point to the already freed/reused memory.
    
    This patch adds signal_struct->thread_head and task->thread_node to
    create the normal rcu-safe list with the stable head.  The new
    for_each_thread(g, t) helper is always safe under rcu_read_lock() as
    long as this task_struct can't go away.
    
    Note: of course it is ugly to have both task_struct->thread_node and the
    old task_struct->thread_group, we will kill it later, after we change
    the users of while_each_thread() to use for_each_thread().
    
    Perhaps we can kill it even before we convert all users, we can
    reimplement next_thread(t) using the new thread_head/thread_node.  But
    we can't do this right now because this will lead to subtle behavioural
    changes.  For example, do/while_each_thread() always sees at least one
    task, while for_each_thread() can do nothing if the whole thread group
    has died.  Or thread_group_empty(), currently its semantics is not clear
    unless thread_group_leader(p) and we need to audit the callers before we
    can change it.
    
    So this patch adds the new interface which has to coexist with the old
    one for some time, hopefully the next changes will be more or less
    straightforward and the old one will go away soon.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Sergey Dyasly <dserrg@gmail.com>
    Tested-by: Sergey Dyasly <dserrg@gmail.com>
    Reviewed-by: Sameer Nanda <snanda@chromium.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mandeep Singh Baines <msb@chromium.org>
    Cc: "Ma, Xindong" <xindong.ma@intel.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: "Tu, Xiaobing" <xiaobing.tu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a949819055d5..1e77fc645317 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -74,6 +74,7 @@ static void __unhash_process(struct task_struct *p, bool group_dead)
 		__this_cpu_dec(process_counts);
 	}
 	list_del_rcu(&p->thread_group);
+	list_del_rcu(&p->thread_node);
 }
 
 /*

commit 7c8df28633bf0b7eb253f866029be0ac59ddb062
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jul 8 16:00:54 2013 -0700

    ptrace: revert "Prepare to fix racy accesses on task breakpoints"
    
    This reverts commit bf26c018490c ("Prepare to fix racy accesses on task
    breakpoints").
    
    The patch was fine but we can no longer race with SIGKILL after commit
    9899d11f6544 ("ptrace: ensure arch_ptrace/ptrace_request can never race
    with SIGKILL"), the __TASK_TRACED tracee can't be woken up and
    ->ptrace_bps[] can't go away.
    
    Now that ptrace_get_breakpoints/ptrace_put_breakpoints have no callers,
    we can kill them and remove task->ptrace_bp_refcnt.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index fafe75d9e6f6..a949819055d5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -808,7 +808,7 @@ void do_exit(long code)
 	/*
 	 * FIXME: do that only when needed, using sched_exit tracepoint
 	 */
-	ptrace_put_breakpoints(tsk);
+	flush_ptrace_hw_breakpoint(tsk);
 
 	exit_notify(tsk, group_dead);
 #ifdef CONFIG_NUMA

commit 7f0ef0267e20d62d45d527911a993b1e998f4968
Merge: 862f00125491 9307c2952450
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 3 17:12:13 2013 -0700

    Merge branch 'akpm' (updates from Andrew Morton)
    
    Merge first patch-bomb from Andrew Morton:
     - various misc bits
     - I'm been patchmonkeying ocfs2 for a while, as Joel and Mark have been
       distracted.  There has been quite a bit of activity.
     - About half the MM queue
     - Some backlight bits
     - Various lib/ updates
     - checkpatch updates
     - zillions more little rtc patches
     - ptrace
     - signals
     - exec
     - procfs
     - rapidio
     - nbd
     - aoe
     - pps
     - memstick
     - tools/testing/selftests updates
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (445 commits)
      tools/testing/selftests: don't assume the x bit is set on scripts
      selftests: add .gitignore for kcmp
      selftests: fix clean target in kcmp Makefile
      selftests: add .gitignore for vm
      selftests: add hugetlbfstest
      self-test: fix make clean
      selftests: exit 1 on failure
      kernel/resource.c: remove the unneeded assignment in function __find_resource
      aio: fix wrong comment in aio_complete()
      drivers/w1/slaves/w1_ds2408.c: add magic sequence to disable P0 test mode
      drivers/memstick/host/r592.c: convert to module_pci_driver
      drivers/memstick/host/jmb38x_ms: convert to module_pci_driver
      pps-gpio: add device-tree binding and support
      drivers/pps/clients/pps-gpio.c: convert to module_platform_driver
      drivers/pps/clients/pps-gpio.c: convert to devm_* helpers
      drivers/parport/share.c: use kzalloc
      Documentation/accounting/getdelays.c: avoid strncpy in accounting tool
      aoe: update internal version number to v83
      aoe: update copyright date
      aoe: perform I/O completions in parallel
      ...

commit 81dabb464139324c005159f5afba377104d8828d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jul 3 15:08:26 2013 -0700

    exit.c: unexport __set_special_pids()
    
    Move __set_special_pids() from exit.c to sys.c close to its single caller
    and make it static.
    
    And rename it to set_special_pids(), another helper with this name has
    gone away.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7bb73f9d09db..3a77cd9390a1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -312,17 +312,6 @@ kill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)
 	}
 }
 
-void __set_special_pids(struct pid *pid)
-{
-	struct task_struct *curr = current->group_leader;
-
-	if (task_session(curr) != pid)
-		change_pid(curr, PIDTYPE_SID, pid);
-
-	if (task_pgrp(curr) != pid)
-		change_pid(curr, PIDTYPE_PGID, pid);
-}
-
 /*
  * Let kernel threads use this to say that they allow a certain signal.
  * Must not be used if kthread was cloned with CLONE_SIGHAND.

commit 207bc1181b1c03ab6ecb55bca5b307606dd1d6bc
Merge: e8b6cb394743 2b15af6f9530
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Jun 28 13:00:53 2013 +0200

    Merge branch 'freezer'
    
    * freezer:
      af_unix: use freezable blocking calls in read
      sigtimedwait: use freezable blocking call
      nanosleep: use freezable blocking call
      futex: use freezable blocking call
      select: use freezable blocking call
      epoll: use freezable blocking call
      binder: use freezable blocking calls
      freezer: add new freezable helpers using freezer_do_not_count()
      freezer: convert freezable helpers to static inline where possible
      freezer: convert freezable helpers to freezer_do_not_count()
      freezer: skip waking up tasks with PF_FREEZER_SKIP set
      freezer: shorten freezer sleep time using exponential backoff
      lockdep: check that no locks held at freeze time
      lockdep: remove task argument from debug_check_no_locks_held
      freezer: add unsafe versions of freezable helpers for CIFS
      freezer: add unsafe versions of freezable helpers for NFS

commit 8aac62706adaaf0fab02c4327761561c8bda9448
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jun 14 21:09:49 2013 +0200

    move exit_task_namespaces() outside of exit_notify()
    
    exit_notify() does exit_task_namespaces() after
    forget_original_parent(). This was needed to ensure that ->nsproxy
    can't be cleared prematurely, an exiting child we are going to
    reparent can do do_notify_parent() and use the parent's (ours) pid_ns.
    
    However, after 32084504 "pidns: use task_active_pid_ns in
    do_notify_parent" ->nsproxy != NULL is no longer needed, we rely
    on task_active_pid_ns().
    
    Move exit_task_namespaces() from exit_notify() to do_exit(), after
    exit_fs() and before exit_task_work().
    
    This solves the problem reported by Andrey, free_ipc_ns()->shm_destroy()
    does fput() which needs task_work_add().
    
    Note: this particular problem can be fixed if we change fput(), and
    that change makes sense anyway. But there is another reason to move
    the callsite. The original reason for exit_task_namespaces() from
    the middle of exit_notify() was subtle and it has already gone away,
    now this looks confusing. And this allows us do simplify exit_notify(),
    we can avoid unlock/lock(tasklist) and we can use ->exit_state instead
    of PF_EXITING in forget_original_parent().
    
    Reported-by: Andrey Vagin <avagin@openvz.org>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Andrey Vagin <avagin@openvz.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index af2eb3cbd499..7bb73f9d09db 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -649,7 +649,6 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	 *	jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
 	 */
 	forget_original_parent(tsk);
-	exit_task_namespaces(tsk);
 
 	write_lock_irq(&tasklist_lock);
 	if (group_dead)
@@ -795,6 +794,7 @@ void do_exit(long code)
 	exit_shm(tsk);
 	exit_files(tsk);
 	exit_fs(tsk);
+	exit_task_namespaces(tsk);
 	exit_task_work(tsk);
 	check_stack_usage();
 	exit_thread();

commit 1b1d2fb4444231f25ddabc598aa2b5a9c0833fba
Author: Colin Cross <ccross@android.com>
Date:   Mon May 6 23:50:08 2013 +0000

    lockdep: remove task argument from debug_check_no_locks_held
    
    The only existing caller to debug_check_no_locks_held calls it
    with 'current' as the task, and the freezer needs to call
    debug_check_no_locks_held but doesn't already have a current
    task pointer, so remove the argument.  It is already assuming
    that the current task is relevant by dumping the current stack
    trace as part of the warning.
    
    This was originally part of 6aa9707099c (lockdep: check that
    no locks held at freeze time) which was reverted in
    dbf520a9d7d4.
    
    Original-author: Mandeep Singh Baines <msb@chromium.org>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index af2eb3cbd499..e59756275000 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -835,7 +835,7 @@ void do_exit(long code)
 	/*
 	 * Make sure we are holding no locks:
 	 */
-	debug_check_no_locks_held(tsk);
+	debug_check_no_locks_held();
 	/*
 	 * We can do this unlocked here. The futex code uses this flag
 	 * just to verify whether the pi state cleanup has been done

commit 20b4fb485227404329e41ad15588afad3df23050
Merge: b9394d8a657c ac3e3c5b1164
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 1 17:51:54 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull VFS updates from Al Viro,
    
    Misc cleanups all over the place, mainly wrt /proc interfaces (switch
    create_proc_entry to proc_create(), get rid of the deprecated
    create_proc_read_entry() in favor of using proc_create_data() and
    seq_file etc).
    
    7kloc removed.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (204 commits)
      don't bother with deferred freeing of fdtables
      proc: Move non-public stuff from linux/proc_fs.h to fs/proc/internal.h
      proc: Make the PROC_I() and PDE() macros internal to procfs
      proc: Supply a function to remove a proc entry by PDE
      take cgroup_open() and cpuset_open() to fs/proc/base.c
      ppc: Clean up scanlog
      ppc: Clean up rtas_flash driver somewhat
      hostap: proc: Use remove_proc_subtree()
      drm: proc: Use remove_proc_subtree()
      drm: proc: Use minor->index to label things, not PDE->name
      drm: Constify drm_proc_list[]
      zoran: Don't print proc_dir_entry data in debug
      reiserfs: Don't access the proc_dir_entry in r_open(), r_start() r_show()
      proc: Supply an accessor for getting the data from a PDE's parent
      airo: Use remove_proc_subtree()
      rtl8192u: Don't need to save device proc dir PDE
      rtl8187se: Use a dir under /proc/net/r8180/
      proc: Add proc_mkdir_data()
      proc: Move some bits from linux/proc_fs.h to linux/{of.h,signal.h,tty.h}
      proc: Move PDE_NET() to fs/proc/proc_net.c
      ...

commit 08d76760832993050ad8c25e63b56773ef2ca303
Merge: 5f56886521d6 99e621f796d7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 1 07:21:43 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull compat cleanup from Al Viro:
     "Mostly about syscall wrappers this time; there will be another pile
      with patches in the same general area from various people, but I'd
      rather push those after both that and vfs.git pile are in."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal:
      syscalls.h: slightly reduce the jungles of macros
      get rid of union semop in sys_semctl(2) arguments
      make do_mremap() static
      sparc: no need to sign-extend in sync_file_range() wrapper
      ppc compat wrappers for add_key(2) and request_key(2) are pointless
      x86: trim sys_ia32.h
      x86: sys32_kill and sys32_mprotect are pointless
      get rid of compat_sys_semctl() and friends in case of ARCH_WANT_OLD_COMPAT_IPC
      merge compat sys_ipc instances
      consolidate compat lookup_dcookie()
      convert vmsplice to COMPAT_SYSCALL_DEFINE
      switch getrusage() to COMPAT_SYSCALL_DEFINE
      switch epoll_pwait to COMPAT_SYSCALL_DEFINE
      convert sendfile{,64} to COMPAT_SYSCALL_DEFINE
      switch signalfd{,4}() to COMPAT_SYSCALL_DEFINE
      make SYSCALL_DEFINE<n>-generated wrappers do asmlinkage_protect
      make HAVE_SYSCALL_WRAPPERS unconditional
      consolidate cond_syscall and SYSCALL_ALIAS declarations
      teach SYSCALL_DEFINE<n> how to deal with long long/unsigned long long
      get rid of duplicate logics in __SC_....[1-6] definitions

commit 4b8a8f1e4f94fd87747e6e3acef74cf0b4dc0dae
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Mar 21 11:06:46 2013 -0400

    get rid of the last free_pipe_info() callers
    
    and rename __free_pipe_info() to free_pipe_info()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 51e485ca9935..cd9e9e799bd2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -847,7 +847,7 @@ void do_exit(long code)
 		exit_io_context(tsk);
 
 	if (tsk->splice_pipe)
-		__free_pipe_info(tsk->splice_pipe);
+		free_pipe_info(tsk->splice_pipe);
 
 	if (tsk->task_frag.page)
 		put_page(tsk->task_frag.page);

commit dbf520a9d7d4d5ba28d2947be11e34099a5e3e20
Author: Paul Walmsley <paul@pwsan.com>
Date:   Sun Mar 31 00:04:40 2013 +0000

    Revert "lockdep: check that no locks held at freeze time"
    
    This reverts commit 6aa9707099c4b25700940eb3d016f16c4434360d.
    
    Commit 6aa9707099c4 ("lockdep: check that no locks held at freeze time")
    causes problems with NFS root filesystems.  The failures were noticed on
    OMAP2 and 3 boards during kernel init:
    
      [ BUG: swapper/0/1 still has locks held! ]
      3.9.0-rc3-00344-ga937536 #1 Not tainted
      -------------------------------------
      1 lock held by swapper/0/1:
       #0:  (&type->s_umount_key#13/1){+.+.+.}, at: [<c011e84c>] sget+0x248/0x574
    
      stack backtrace:
        rpc_wait_bit_killable
        __wait_on_bit
        out_of_line_wait_on_bit
        __rpc_execute
        rpc_run_task
        rpc_call_sync
        nfs_proc_get_root
        nfs_get_root
        nfs_fs_mount_common
        nfs_try_mount
        nfs_fs_mount
        mount_fs
        vfs_kern_mount
        do_mount
        sys_mount
        do_mount_root
        mount_root
        prepare_namespace
        kernel_init_freeable
        kernel_init
    
    Although the rootfs mounts, the system is unstable.  Here's a transcript
    from a PM test:
    
      http://www.pwsan.com/omap/testlogs/test_v3.9-rc3/20130317194234/pm/37xxevm/37xxevm_log.txt
    
    Here's what the test log should look like:
    
      http://www.pwsan.com/omap/testlogs/test_v3.8/20130218214403/pm/37xxevm/37xxevm_log.txt
    
    Mailing list discussion is here:
    
      http://lkml.org/lkml/2013/3/4/221
    
    Deal with this for v3.9 by reverting the problem commit, until folks can
    figure out the right long-term course of action.
    
    Signed-off-by: Paul Walmsley <paul@pwsan.com>
    Cc: Mandeep Singh Baines <msb@chromium.org>
    Cc: Jeff Layton <jlayton@redhat.com>
    Cc: Shawn Guo <shawn.guo@linaro.org>
    Cc: <maciej.rutecki@gmail.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ben Chan <benchan@chromium.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 51e485ca9935..60bc027c61c3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -835,7 +835,7 @@ void do_exit(long code)
 	/*
 	 * Make sure we are holding no locks:
 	 */
-	debug_check_no_locks_held();
+	debug_check_no_locks_held(tsk);
 	/*
 	 * We can do this unlocked here. The futex code uses this flag
 	 * just to verify whether the pi state cleanup has been done

commit 2cf0966683430b6468f36ca20515a33ca7f2403c
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Jan 21 15:25:54 2013 -0500

    make SYSCALL_DEFINE<n>-generated wrappers do asmlinkage_protect
    
    ... and switch i386 to HAVE_SYSCALL_WRAPPERS, killing open-coded
    uses of asmlinkage_protect() in a bunch of syscalls.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 51e485ca9935..25d0108d7452 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1629,9 +1629,6 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	}
 
 	put_pid(pid);
-
-	/* avoid REGPARM breakage on x86: */
-	asmlinkage_protect(5, ret, which, upid, infop, options, ru);
 	return ret;
 }
 
@@ -1669,8 +1666,6 @@ SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
 	ret = do_wait(&wo);
 	put_pid(pid);
 
-	/* avoid REGPARM breakage on x86: */
-	asmlinkage_protect(4, ret, upid, stat_addr, options, ru);
 	return ret;
 }
 

commit 80d26af89a7249aa5475467000322163c60cdd72
Author: Mandeep Singh Baines <msb@chromium.org>
Date:   Wed Feb 27 17:03:20 2013 -0800

    coredump: use a freezable_schedule for the coredump_finish wait
    
    Prevents hung_task detector from panicing the machine. This is also
    needed to prevent this wait from blocking suspend.
    
    (It doesnt' currently block suspend but it would once the next
    patch in this series is applied.)
    
    [yongjun_wei@trendmicro.com.cn: kernel/exit.c: remove duplicated include]
    Signed-off-by: Mandeep Singh Baines <msb@chromium.org>
    Cc: Ben Chan <benchan@chromium.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index aaf97c122e8a..51e485ca9935 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -20,6 +20,7 @@
 #include <linux/tsacct_kern.h>
 #include <linux/file.h>
 #include <linux/fdtable.h>
+#include <linux/freezer.h>
 #include <linux/binfmts.h>
 #include <linux/nsproxy.h>
 #include <linux/pid_namespace.h>
@@ -31,7 +32,6 @@
 #include <linux/mempolicy.h>
 #include <linux/taskstats_kern.h>
 #include <linux/delayacct.h>
-#include <linux/freezer.h>
 #include <linux/cgroup.h>
 #include <linux/syscalls.h>
 #include <linux/signal.h>
@@ -485,7 +485,7 @@ static void exit_mm(struct task_struct * tsk)
 			set_task_state(tsk, TASK_UNINTERRUPTIBLE);
 			if (!self.task) /* see coredump_finish() */
 				break;
-			schedule();
+			freezable_schedule();
 		}
 		__set_task_state(tsk, TASK_RUNNING);
 		down_read(&mm->mmap_sem);

commit 6aa9707099c4b25700940eb3d016f16c4434360d
Author: Mandeep Singh Baines <msb@chromium.org>
Date:   Wed Feb 27 17:03:18 2013 -0800

    lockdep: check that no locks held at freeze time
    
    We shouldn't try_to_freeze if locks are held.  Holding a lock can cause a
    deadlock if the lock is later acquired in the suspend or hibernate path
    (e.g.  by dpm).  Holding a lock can also cause a deadlock in the case of
    cgroup_freezer if a lock is held inside a frozen cgroup that is later
    acquired by a process outside that group.
    
    [akpm@linux-foundation.org: export debug_check_no_locks_held]
    Signed-off-by: Mandeep Singh Baines <msb@chromium.org>
    Cc: Ben Chan <benchan@chromium.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7dd20408707c..aaf97c122e8a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -835,7 +835,7 @@ void do_exit(long code)
 	/*
 	 * Make sure we are holding no locks:
 	 */
-	debug_check_no_locks_held(tsk);
+	debug_check_no_locks_held();
 	/*
 	 * We can do this unlocked here. The futex code uses this flag
 	 * just to verify whether the pi state cleanup has been done

commit 6fac4829ce0ef9b7f24369086ce5f0e9f38d37bc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 13 14:20:55 2012 +0100

    cputime: Use accessors to read task cputime stats
    
    This is in preparation for the full dynticks feature. While
    remotely reading the cputime of a task running in a full
    dynticks CPU, we'll need to do some extra-computation. This
    way we can account the time it spent tickless in userspace
    since its last cputime snapshot.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/exit.c b/kernel/exit.c
index b4df21937216..7dd20408707c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -85,6 +85,7 @@ static void __exit_signal(struct task_struct *tsk)
 	bool group_dead = thread_group_leader(tsk);
 	struct sighand_struct *sighand;
 	struct tty_struct *uninitialized_var(tty);
+	cputime_t utime, stime;
 
 	sighand = rcu_dereference_check(tsk->sighand,
 					lockdep_tasklist_lock_is_held());
@@ -123,9 +124,10 @@ static void __exit_signal(struct task_struct *tsk)
 		 * We won't ever get here for the group leader, since it
 		 * will have been the last reference on the signal_struct.
 		 */
-		sig->utime += tsk->utime;
-		sig->stime += tsk->stime;
-		sig->gtime += tsk->gtime;
+		task_cputime(tsk, &utime, &stime);
+		sig->utime += utime;
+		sig->stime += stime;
+		sig->gtime += task_gtime(tsk);
 		sig->min_flt += tsk->min_flt;
 		sig->maj_flt += tsk->maj_flt;
 		sig->nvcsw += tsk->nvcsw;
@@ -1092,7 +1094,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		sig = p->signal;
 		psig->cutime += tgutime + sig->cutime;
 		psig->cstime += tgstime + sig->cstime;
-		psig->cgtime += p->gtime + sig->gtime + sig->cgtime;
+		psig->cgtime += task_gtime(p) + sig->gtime + sig->cgtime;
 		psig->cmin_flt +=
 			p->min_flt + sig->min_flt + sig->cmin_flt;
 		psig->cmaj_flt +=

commit 6a2b60b17b3e48a418695a94bd2420f6ab32e519
Merge: 9228ff90387e 98f842e675f9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 17 15:44:47 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull user namespace changes from Eric Biederman:
     "While small this set of changes is very significant with respect to
      containers in general and user namespaces in particular.  The user
      space interface is now complete.
    
      This set of changes adds support for unprivileged users to create user
      namespaces and as a user namespace root to create other namespaces.
      The tyranny of supporting suid root preventing unprivileged users from
      using cool new kernel features is broken.
    
      This set of changes completes the work on setns, adding support for
      the pid, user, mount namespaces.
    
      This set of changes includes a bunch of basic pid namespace
      cleanups/simplifications.  Of particular significance is the rework of
      the pid namespace cleanup so it no longer requires sending out
      tendrils into all kinds of unexpected cleanup paths for operation.  At
      least one case of broken error handling is fixed by this cleanup.
    
      The files under /proc/<pid>/ns/ have been converted from regular files
      to magic symlinks which prevents incorrect caching by the VFS,
      ensuring the files always refer to the namespace the process is
      currently using and ensuring that the ptrace_mayaccess permission
      checks are always applied.
    
      The files under /proc/<pid>/ns/ have been given stable inode numbers
      so it is now possible to see if different processes share the same
      namespaces.
    
      Through the David Miller's net tree are changes to relax many of the
      permission checks in the networking stack to allowing the user
      namespace root to usefully use the networking stack.  Similar changes
      for the mount namespace and the pid namespace are coming through my
      tree.
    
      Two small changes to add user namespace support were commited here adn
      in David Miller's -net tree so that I could complete the work on the
      /proc/<pid>/ns/ files in this tree.
    
      Work remains to make it safe to build user namespaces and 9p, afs,
      ceph, cifs, coda, gfs2, ncpfs, nfs, nfsd, ocfs2, and xfs so the
      Kconfig guard remains in place preventing that user namespaces from
      being built when any of those filesystems are enabled.
    
      Future design work remains to allow root users outside of the initial
      user namespace to mount more than just /proc and /sys."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (38 commits)
      proc: Usable inode numbers for the namespace file descriptors.
      proc: Fix the namespace inode permission checks.
      proc: Generalize proc inode allocation
      userns: Allow unprivilged mounts of proc and sysfs
      userns: For /proc/self/{uid,gid}_map derive the lower userns from the struct file
      procfs: Print task uids and gids in the userns that opened the proc file
      userns: Implement unshare of the user namespace
      userns: Implent proc namespace operations
      userns: Kill task_user_ns
      userns: Make create_new_namespaces take a user_ns parameter
      userns: Allow unprivileged use of setns.
      userns: Allow unprivileged users to create new namespaces
      userns: Allow setting a userns mapping to your current uid.
      userns: Allow chown and setgid preservation
      userns: Allow unprivileged users to create user namespaces.
      userns: Ignore suid and sgid on binaries if the uid or gid can not be mapped
      userns: fix return value on mntns_install() failure
      vfs: Allow unprivileged manipulation of the mount namespace.
      vfs: Only support slave subtrees across different user namespaces
      vfs: Add a user namespace reference from struct mnt_namespace
      ...

commit 9977d9b379cb77e0f67bd6f4563618106e58e11d
Merge: cf4af0122157 541880d9a2c7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 12 12:22:13 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull big execve/kernel_thread/fork unification series from Al Viro:
     "All architectures are converted to new model.  Quite a bit of that
      stuff is actually shared with architecture trees; in such cases it's
      literally shared branch pulled by both, not a cherry-pick.
    
      A lot of ugliness and black magic is gone (-3KLoC total in this one):
    
       - kernel_thread()/kernel_execve()/sys_execve() redesign.
    
         We don't do syscalls from kernel anymore for either kernel_thread()
         or kernel_execve():
    
         kernel_thread() is essentially clone(2) with callback run before we
         return to userland, the callbacks either never return or do
         successful do_execve() before returning.
    
         kernel_execve() is a wrapper for do_execve() - it doesn't need to
         do transition to user mode anymore.
    
         As a result kernel_thread() and kernel_execve() are
         arch-independent now - they live in kernel/fork.c and fs/exec.c
         resp.  sys_execve() is also in fs/exec.c and it's completely
         architecture-independent.
    
       - daemonize() is gone, along with its parts in fs/*.c
    
       - struct pt_regs * is no longer passed to do_fork/copy_process/
         copy_thread/do_execve/search_binary_handler/->load_binary/do_coredump.
    
       - sys_fork()/sys_vfork()/sys_clone() unified; some architectures
         still need wrappers (ones with callee-saved registers not saved in
         pt_regs on syscall entry), but the main part of those suckers is in
         kernel/fork.c now."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (113 commits)
      do_coredump(): get rid of pt_regs argument
      print_fatal_signal(): get rid of pt_regs argument
      ptrace_signal(): get rid of unused arguments
      get rid of ptrace_signal_deliver() arguments
      new helper: signal_pt_regs()
      unify default ptrace_signal_deliver
      flagday: kill pt_regs argument of do_fork()
      death to idle_regs()
      don't pass regs to copy_process()
      flagday: don't pass regs to copy_thread()
      bfin: switch to generic vfork, get rid of pointless wrappers
      xtensa: switch to generic clone()
      openrisc: switch to use of generic fork and clone
      unicore32: switch to generic clone(2)
      score: switch to generic fork/vfork/clone
      c6x: sanitize copy_thread(), get rid of clone(2) wrapper, switch to generic clone()
      take sys_fork/sys_vfork/sys_clone prototypes to linux/syscalls.h
      mn10300: switch to generic fork/vfork/clone
      h8300: switch to generic fork/vfork/clone
      tile: switch to generic clone()
      ...
    
    Conflicts:
            arch/microblaze/include/asm/Kbuild

commit c4144670fd9b34d6eae22c9f83751745898e8243
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Oct 2 16:34:38 2012 -0400

    kill daemonize()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 346616c0092c..f9275e2c7c2c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -322,43 +322,6 @@ kill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)
 	}
 }
 
-/**
- * reparent_to_kthreadd - Reparent the calling kernel thread to kthreadd
- *
- * If a kernel thread is launched as a result of a system call, or if
- * it ever exits, it should generally reparent itself to kthreadd so it
- * isn't in the way of other processes and is correctly cleaned up on exit.
- *
- * The various task state such as scheduling policy and priority may have
- * been inherited from a user process, so we reset them to sane values here.
- *
- * NOTE that reparent_to_kthreadd() gives the caller full capabilities.
- */
-static void reparent_to_kthreadd(void)
-{
-	write_lock_irq(&tasklist_lock);
-
-	ptrace_unlink(current);
-	/* Reparent to init */
-	current->real_parent = current->parent = kthreadd_task;
-	list_move_tail(&current->sibling, &current->real_parent->children);
-
-	/* Set the exit signal to SIGCHLD so we signal init on exit */
-	current->exit_signal = SIGCHLD;
-
-	if (task_nice(current) < 0)
-		set_user_nice(current, 0);
-	/* cpus_allowed? */
-	/* rt_priority? */
-	/* signals? */
-	memcpy(current->signal->rlim, init_task.signal->rlim,
-	       sizeof(current->signal->rlim));
-
-	atomic_inc(&init_cred.usage);
-	commit_creds(&init_cred);
-	write_unlock_irq(&tasklist_lock);
-}
-
 void __set_special_pids(struct pid *pid)
 {
 	struct task_struct *curr = current->group_leader;
@@ -370,13 +333,6 @@ void __set_special_pids(struct pid *pid)
 		change_pid(curr, PIDTYPE_PGID, pid);
 }
 
-static void set_special_pids(struct pid *pid)
-{
-	write_lock_irq(&tasklist_lock);
-	__set_special_pids(pid);
-	write_unlock_irq(&tasklist_lock);
-}
-
 /*
  * Let kernel threads use this to say that they allow a certain signal.
  * Must not be used if kthread was cloned with CLONE_SIGHAND.
@@ -416,54 +372,6 @@ int disallow_signal(int sig)
 
 EXPORT_SYMBOL(disallow_signal);
 
-/*
- *	Put all the gunge required to become a kernel thread without
- *	attached user resources in one place where it belongs.
- */
-
-void daemonize(const char *name, ...)
-{
-	va_list args;
-	sigset_t blocked;
-
-	va_start(args, name);
-	vsnprintf(current->comm, sizeof(current->comm), name, args);
-	va_end(args);
-
-	/*
-	 * If we were started as result of loading a module, close all of the
-	 * user space pages.  We don't need them, and if we didn't close them
-	 * they would be locked into memory.
-	 */
-	exit_mm(current);
-	/*
-	 * We don't want to get frozen, in case system-wide hibernation
-	 * or suspend transition begins right now.
-	 */
-	current->flags |= (PF_NOFREEZE | PF_KTHREAD);
-
-	if (current->nsproxy != &init_nsproxy) {
-		get_nsproxy(&init_nsproxy);
-		switch_task_namespaces(current, &init_nsproxy);
-	}
-	set_special_pids(&init_struct_pid);
-	proc_clear_tty(current);
-
-	/* Block and flush all signals */
-	sigfillset(&blocked);
-	sigprocmask(SIG_BLOCK, &blocked, NULL);
-	flush_signals(current);
-
-	/* Become as one with the init task */
-
-	daemonize_fs_struct();
-	daemonize_descriptors();
-
-	reparent_to_kthreadd();
-}
-
-EXPORT_SYMBOL(daemonize);
-
 #ifdef CONFIG_MM_OWNER
 /*
  * A task is exiting.   If it owned this mm, find a new owner for the mm.

commit e80d0a1ae8bb8fee0edd37427836f108b30f596b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 21 16:26:44 2012 +0100

    cputime: Rename thread_group_times to thread_group_cputime_adjusted
    
    We have thread_group_cputime() and thread_group_times(). The naming
    doesn't provide enough information about the difference between
    these two APIs.
    
    To lower the confusion, rename thread_group_times() to
    thread_group_cputime_adjusted(). This name better suggests that
    it's a version of thread_group_cputime() that does some stabilization
    on the raw cputime values. ie here: scale on top of CFS runtime
    stats and bound lower value for monotonicity.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 346616c0092c..618f7ee56003 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1186,11 +1186,11 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		 * as other threads in the parent group can be right
 		 * here reaping other children at the same time.
 		 *
-		 * We use thread_group_times() to get times for the thread
+		 * We use thread_group_cputime_adjusted() to get times for the thread
 		 * group, which consolidates times for all threads in the
 		 * group including the group leader.
 		 */
-		thread_group_times(p, &tgutime, &tgstime);
+		thread_group_cputime_adjusted(p, &tgutime, &tgstime);
 		spin_lock_irq(&p->real_parent->sighand->siglock);
 		psig = p->real_parent->signal;
 		sig = p->signal;

commit af4b8a83add95ef40716401395b44a1b579965f4
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Aug 1 15:03:42 2012 -0700

    pidns: Wait in zap_pid_ns_processes until pid_ns->nr_hashed == 1
    
    Looking at pid_ns->nr_hashed is a bit simpler and it works for
    disjoint process trees that an unshare or a join of a pid_namespace
    may create.
    
    Acked-by: "Serge E. Hallyn" <serge@hallyn.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 346616c0092c..d7fe58db4527 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -72,18 +72,6 @@ static void __unhash_process(struct task_struct *p, bool group_dead)
 		list_del_rcu(&p->tasks);
 		list_del_init(&p->sibling);
 		__this_cpu_dec(process_counts);
-		/*
-		 * If we are the last child process in a pid namespace to be
-		 * reaped, notify the reaper sleeping zap_pid_ns_processes().
-		 */
-		if (IS_ENABLED(CONFIG_PID_NS)) {
-			struct task_struct *parent = p->real_parent;
-
-			if ((task_active_pid_ns(parent)->child_reaper == parent) &&
-			    list_empty(&parent->children) &&
-			    (parent->flags & PF_EXITING))
-				wake_up_process(parent);
-		}
 	}
 	list_del_rcu(&p->thread_group);
 }

commit aab174f0df5d72d31caccf281af5f614fa254578
Merge: ca41cc96b281 2bd2c1941f14
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 20:25:04 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs update from Al Viro:
    
     - big one - consolidation of descriptor-related logics; almost all of
       that is moved to fs/file.c
    
       (BTW, I'm seriously tempted to rename the result to fd.c.  As it is,
       we have a situation when file_table.c is about handling of struct
       file and file.c is about handling of descriptor tables; the reasons
       are historical - file_table.c used to be about a static array of
       struct file we used to have way back).
    
       A lot of stray ends got cleaned up and converted to saner primitives,
       disgusting mess in android/binder.c is still disgusting, but at least
       doesn't poke so much in descriptor table guts anymore.  A bunch of
       relatively minor races got fixed in process, plus an ext4 struct file
       leak.
    
     - related thing - fget_light() partially unuglified; see fdget() in
       there (and yes, it generates the code as good as we used to have).
    
     - also related - bits of Cyrill's procfs stuff that got entangled into
       that work; _not_ all of it, just the initial move to fs/proc/fd.c and
       switch of fdinfo to seq_file.
    
     - Alex's fs/coredump.c spiltoff - the same story, had been easier to
       take that commit than mess with conflicts.  The rest is a separate
       pile, this was just a mechanical code movement.
    
     - a few misc patches all over the place.  Not all for this cycle,
       there'll be more (and quite a few currently sit in akpm's tree)."
    
    Fix up trivial conflicts in the android binder driver, and some fairly
    simple conflicts due to two different changes to the sock_alloc_file()
    interface ("take descriptor handling from sock_alloc_file() to callers"
    vs "net: Providing protocol type via system.sockprotoname xattr of
    /proc/PID/fd entries" adding a dentry name to the socket)
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (72 commits)
      MAX_LFS_FILESIZE should be a loff_t
      compat: fs: Generic compat_sys_sendfile implementation
      fs: push rcu_barrier() from deactivate_locked_super() to filesystems
      btrfs: reada_extent doesn't need kref for refcount
      coredump: move core dump functionality into its own file
      coredump: prevent double-free on an error path in core dumper
      usb/gadget: fix misannotations
      fcntl: fix misannotations
      ceph: don't abuse d_delete() on failure exits
      hypfs: ->d_parent is never NULL or negative
      vfs: delete surplus inode NULL check
      switch simple cases of fget_light to fdget
      new helpers: fdget()/fdput()
      switch o2hb_region_dev_write() to fget_light()
      proc_map_files_readdir(): don't bother with grabbing files
      make get_file() return its argument
      vhost_set_vring(): turn pollstart/pollstop into bool
      switch prctl_set_mm_exe_file() to fget_light()
      switch xfs_find_handle() to fget_light()
      switch xfs_swapext() to fget_light()
      ...

commit 864bdb3b6cbd9911222543fef1cfe36f88183f44
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Aug 22 18:42:10 2012 -0400

    new helper: daemonize_descriptors()
    
    descriptor-related parts of daemonize, done right.  As the
    result we simplify the locking rules for ->files - we
    hold task_lock in *all* cases when we modify ->files.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 20dfc7617c2e..095113321318 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -457,9 +457,7 @@ void daemonize(const char *name, ...)
 	/* Become as one with the init task */
 
 	daemonize_fs_struct();
-	exit_files(current);
-	current->files = init_task.files;
-	atomic_inc(&current->files->count);
+	daemonize_descriptors();
 
 	reparent_to_kthreadd();
 }

commit 7cf4dc3c8dbfdfde163d4636f621cf99a1f63bfb
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Aug 15 19:56:12 2012 -0400

    move files_struct-related bits from kernel/exit.c to fs/file.c
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index f65345f9e5bb..20dfc7617c2e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -466,99 +466,6 @@ void daemonize(const char *name, ...)
 
 EXPORT_SYMBOL(daemonize);
 
-static void close_files(struct files_struct * files)
-{
-	int i, j;
-	struct fdtable *fdt;
-
-	j = 0;
-
-	/*
-	 * It is safe to dereference the fd table without RCU or
-	 * ->file_lock because this is the last reference to the
-	 * files structure.  But use RCU to shut RCU-lockdep up.
-	 */
-	rcu_read_lock();
-	fdt = files_fdtable(files);
-	rcu_read_unlock();
-	for (;;) {
-		unsigned long set;
-		i = j * BITS_PER_LONG;
-		if (i >= fdt->max_fds)
-			break;
-		set = fdt->open_fds[j++];
-		while (set) {
-			if (set & 1) {
-				struct file * file = xchg(&fdt->fd[i], NULL);
-				if (file) {
-					filp_close(file, files);
-					cond_resched();
-				}
-			}
-			i++;
-			set >>= 1;
-		}
-	}
-}
-
-struct files_struct *get_files_struct(struct task_struct *task)
-{
-	struct files_struct *files;
-
-	task_lock(task);
-	files = task->files;
-	if (files)
-		atomic_inc(&files->count);
-	task_unlock(task);
-
-	return files;
-}
-
-void put_files_struct(struct files_struct *files)
-{
-	struct fdtable *fdt;
-
-	if (atomic_dec_and_test(&files->count)) {
-		close_files(files);
-		/*
-		 * Free the fd and fdset arrays if we expanded them.
-		 * If the fdtable was embedded, pass files for freeing
-		 * at the end of the RCU grace period. Otherwise,
-		 * you can free files immediately.
-		 */
-		rcu_read_lock();
-		fdt = files_fdtable(files);
-		if (fdt != &files->fdtab)
-			kmem_cache_free(files_cachep, files);
-		free_fdtable(fdt);
-		rcu_read_unlock();
-	}
-}
-
-void reset_files_struct(struct files_struct *files)
-{
-	struct task_struct *tsk = current;
-	struct files_struct *old;
-
-	old = tsk->files;
-	task_lock(tsk);
-	tsk->files = files;
-	task_unlock(tsk);
-	put_files_struct(old);
-}
-
-void exit_files(struct task_struct *tsk)
-{
-	struct files_struct * files = tsk->files;
-
-	if (files) {
-		task_lock(tsk);
-		tsk->files = NULL;
-		task_unlock(tsk);
-		put_files_struct(files);
-	}
-}
-
 #ifdef CONFIG_MM_OWNER
 /*
  * A task is exiting.   If it owned this mm, find a new owner for the mm.

commit 5640f7685831e088fe6c2e1f863a6805962f8e81
Author: Eric Dumazet <edumazet@google.com>
Date:   Sun Sep 23 23:04:42 2012 +0000

    net: use a per task frag allocator
    
    We currently use a per socket order-0 page cache for tcp_sendmsg()
    operations.
    
    This page is used to build fragments for skbs.
    
    Its done to increase probability of coalescing small write() into
    single segments in skbs still in write queue (not yet sent)
    
    But it wastes a lot of memory for applications handling many mostly
    idle sockets, since each socket holds one page in sk->sk_sndmsg_page
    
    Its also quite inefficient to build TSO 64KB packets, because we need
    about 16 pages per skb on arches where PAGE_SIZE = 4096, so we hit
    page allocator more than wanted.
    
    This patch adds a per task frag allocator and uses bigger pages,
    if available. An automatic fallback is done in case of memory pressure.
    
    (up to 32768 bytes per frag, thats order-3 pages on x86)
    
    This increases TCP stream performance by 20% on loopback device,
    but also benefits on other network devices, since 8x less frags are
    mapped on transmit and unmapped on tx completion. Alexander Duyck
    mentioned a probable performance win on systems with IOMMU enabled.
    
    Its possible some SG enabled hardware cant cope with bigger fragments,
    but their ndo_start_xmit() should already handle this, splitting a
    fragment in sub fragments, since some arches have PAGE_SIZE=65536
    
    Successfully tested on various ethernet devices.
    (ixgbe, igb, bnx2x, tg3, mellanox mlx4)
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Ben Hutchings <bhutchings@solarflare.com>
    Cc: Vijay Subramanian <subramanian.vijay@gmail.com>
    Cc: Alexander Duyck <alexander.h.duyck@intel.com>
    Tested-by: Vijay Subramanian <subramanian.vijay@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/exit.c b/kernel/exit.c
index f65345f9e5bb..42f25952edd9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1046,6 +1046,9 @@ void do_exit(long code)
 	if (tsk->splice_pipe)
 		__free_pipe_info(tsk->splice_pipe);
 
+	if (tsk->task_frag.page)
+		put_page(tsk->task_frag.page);
+
 	validate_creds_for_do_exit(tsk);
 
 	preempt_disable();

commit 8ded2bbc1845e19c771eb55209aab166ef011243
Author: Josh Boyer <jwboyer@redhat.com>
Date:   Wed Jul 25 10:40:34 2012 -0400

    posix_types.h: Cleanup stale __NFDBITS and related definitions
    
    Recently, glibc made a change to suppress sign-conversion warnings in
    FD_SET (glibc commit ceb9e56b3d1).  This uncovered an issue with the
    kernel's definition of __NFDBITS if applications #include
    <linux/types.h> after including <sys/select.h>.  A build failure would
    be seen when passing the -Werror=sign-compare and -D_FORTIFY_SOURCE=2
    flags to gcc.
    
    It was suggested that the kernel should either match the glibc
    definition of __NFDBITS or remove that entirely.  The current in-kernel
    uses of __NFDBITS can be replaced with BITS_PER_LONG, and there are no
    uses of the related __FDELT and __FDMASK defines.  Given that, we'll
    continue the cleanup that was started with commit 8b3d1cda4f5f
    ("posix_types: Remove fd_set macros") and drop the remaining unused
    macros.
    
    Additionally, linux/time.h has similar macros defined that expand to
    nothing so we'll remove those at the same time.
    
    Reported-by: Jeff Law <law@redhat.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    CC: <stable@vger.kernel.org>
    Signed-off-by: Josh Boyer <jwboyer@redhat.com>
    [ .. and fix up whitespace as per akpm ]
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index d17f6c4ddfa9..f65345f9e5bb 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -483,7 +483,7 @@ static void close_files(struct files_struct * files)
 	rcu_read_unlock();
 	for (;;) {
 		unsigned long set;
-		i = j * __NFDBITS;
+		i = j * BITS_PER_LONG;
 		if (i >= fdt->max_fds)
 			break;
 		set = fdt->open_fds[j++];

commit ed3e694d78cc75fa79bf29698631b146fd27aa35
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jun 27 11:31:24 2012 +0400

    move exit_task_work() past exit_files() et.al.
    
    ... and get rid of PF_EXITING check in task_work_add().
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2f59cc334516..d17f6c4ddfa9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -953,14 +953,11 @@ void do_exit(long code)
 	exit_signals(tsk);  /* sets PF_EXITING */
 	/*
 	 * tsk->flags are checked in the futex code to protect against
-	 * an exiting task cleaning up the robust pi futexes, and in
-	 * task_work_add() to avoid the race with exit_task_work().
+	 * an exiting task cleaning up the robust pi futexes.
 	 */
 	smp_mb();
 	raw_spin_unlock_wait(&tsk->pi_lock);
 
-	exit_task_work(tsk);
-
 	if (unlikely(in_atomic()))
 		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
 				current->comm, task_pid_nr(current),
@@ -995,6 +992,7 @@ void do_exit(long code)
 	exit_shm(tsk);
 	exit_files(tsk);
 	exit_fs(tsk);
+	exit_task_work(tsk);
 	check_stack_usage();
 	exit_thread();
 

commit 50d75f8daead8a1f850c40a3b6c6575ab19b48cf
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 20 12:53:04 2012 -0700

    pidns: find_new_reaper() can no longer switch to init_pid_ns.child_reaper
    
    find_new_reaper() changes pid_ns->child_reaper, see add0d4df ("pid_ns:
    zap_pid_ns_processes: fix the ->child_reaper changing").
    
    The original reason has gone away after the previous patch, ->children
    list must be empty after zap_pid_ns_processes().
    
    However now we can not switch to init_pid_ns.child_reaper.
    __unhash_process() relies on the "->child_reaper == parent" check, but
    this check does not work if the last exiting task is also the child
    reaper.
    
    As Eric sugested, we can change __unhash_process() to use the parent's
    pid_ns and remove this code.
    
    Also, with this change we can move detach_pid(PIDTYPE_PID) back, where it
    was before the previous fix.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Louis Rilling <louis.rilling@kerlabs.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Acked-by: Pavel Emelyanov <xemul@parallels.com>
    Tested-by: Andrew Wagin <avagin@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a85efd2348bd..2f59cc334516 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -64,6 +64,7 @@ static void exit_mm(struct task_struct * tsk);
 static void __unhash_process(struct task_struct *p, bool group_dead)
 {
 	nr_threads--;
+	detach_pid(p, PIDTYPE_PID);
 	if (group_dead) {
 		detach_pid(p, PIDTYPE_PGID);
 		detach_pid(p, PIDTYPE_SID);
@@ -78,13 +79,12 @@ static void __unhash_process(struct task_struct *p, bool group_dead)
 		if (IS_ENABLED(CONFIG_PID_NS)) {
 			struct task_struct *parent = p->real_parent;
 
-			if ((task_active_pid_ns(p)->child_reaper == parent) &&
+			if ((task_active_pid_ns(parent)->child_reaper == parent) &&
 			    list_empty(&parent->children) &&
 			    (parent->flags & PF_EXITING))
 				wake_up_process(parent);
 		}
 	}
-	detach_pid(p, PIDTYPE_PID);
 	list_del_rcu(&p->thread_group);
 }
 
@@ -732,12 +732,6 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 
 		zap_pid_ns_processes(pid_ns);
 		write_lock_irq(&tasklist_lock);
-		/*
-		 * We can not clear ->child_reaper or leave it alone.
-		 * There may by stealth EXIT_DEAD tasks on ->children,
-		 * forget_original_parent() must move them somewhere.
-		 */
-		pid_ns->child_reaper = init_pid_ns.child_reaper;
 	} else if (father->signal->has_child_subreaper) {
 		struct task_struct *reaper;
 

commit 6347e90091041e34bea625370794c92f4ce71228
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Jun 20 12:53:03 2012 -0700

    pidns: guarantee that the pidns init will be the last pidns process reaped
    
    Today we have a twofold bug.  Sometimes release_task on pid == 1 in a pid
    namespace can run before other processes in a pid namespace have had
    release task called.  With the result that pid_ns_release_proc can be
    called before the last proc_flus_task() is done using upid->ns->proc_mnt,
    resulting in the use of a stale pointer.  This same set of circumstances
    can lead to waitpid(...) returning for a processes started with
    clone(CLONE_NEWPID) before the every process in the pid namespace has
    actually exited.
    
    To fix this modify zap_pid_ns_processess wait until all other processes in
    the pid namespace have exited, even EXIT_DEAD zombies.
    
    The delay_group_leader and related tests ensure that the thread gruop
    leader will be the last thread of a process group to be reaped, or to
    become EXIT_DEAD and self reap.  With the change to zap_pid_ns_processes
    we get the guarantee that pid == 1 in a pid namespace will be the last
    task that release_task is called on.
    
    With pid == 1 being the last task to pass through release_task
    pid_ns_release_proc can no longer be called too early nor can wait return
    before all of the EXIT_DEAD tasks in a pid namespace have exited.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Louis Rilling <louis.rilling@kerlabs.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Acked-by: Pavel Emelyanov <xemul@parallels.com>
    Tested-by: Andrew Wagin <avagin@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c0277d3f1aaa..a85efd2348bd 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -64,7 +64,6 @@ static void exit_mm(struct task_struct * tsk);
 static void __unhash_process(struct task_struct *p, bool group_dead)
 {
 	nr_threads--;
-	detach_pid(p, PIDTYPE_PID);
 	if (group_dead) {
 		detach_pid(p, PIDTYPE_PGID);
 		detach_pid(p, PIDTYPE_SID);
@@ -72,7 +71,20 @@ static void __unhash_process(struct task_struct *p, bool group_dead)
 		list_del_rcu(&p->tasks);
 		list_del_init(&p->sibling);
 		__this_cpu_dec(process_counts);
+		/*
+		 * If we are the last child process in a pid namespace to be
+		 * reaped, notify the reaper sleeping zap_pid_ns_processes().
+		 */
+		if (IS_ENABLED(CONFIG_PID_NS)) {
+			struct task_struct *parent = p->real_parent;
+
+			if ((task_active_pid_ns(p)->child_reaper == parent) &&
+			    list_empty(&parent->children) &&
+			    (parent->flags & PF_EXITING))
+				wake_up_process(parent);
+		}
 	}
+	detach_pid(p, PIDTYPE_PID);
 	list_del_rcu(&p->thread_group);
 }
 

commit 4fe7efdbdfb1c7e7a7f31decfd831c0f31d37091
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Wed Jun 20 12:53:01 2012 -0700

    mm: correctly synchronize rss-counters at exit/exec
    
    do_exit() and exec_mmap() call sync_mm_rss() before mm_release() does
    put_user(clear_child_tid) which can update task->rss_stat and thus make
    mm->rss_stat inconsistent.  This triggers the "BUG:" printk in check_mm().
    
    Let's fix this bug in the safest way, and optimize/cleanup this later.
    
    Reported-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 34867cc5b42a..c0277d3f1aaa 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -643,6 +643,7 @@ static void exit_mm(struct task_struct * tsk)
 	mm_release(tsk, mm);
 	if (!mm)
 		return;
+	sync_mm_rss(mm);
 	/*
 	 * Serialize with any possible pending coredump.
 	 * We must hold mmap_sem around checking core_state

commit 48d212a2eecaca2e1875925837ad27b2f43f48a3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 7 17:54:07 2012 -0700

    Revert "mm: correctly synchronize rss-counters at exit/exec"
    
    This reverts commit 40af1bbdca47e5c8a2044039bb78ca8fd8b20f94.
    
    It's horribly and utterly broken for at least the following reasons:
    
     - calling sync_mm_rss() from mmput() is fundamentally wrong, because
       there's absolutely no reason to believe that the task that does the
       mmput() always does it on its own VM.  Example: fork, ptrace, /proc -
       you name it.
    
     - calling it *after* having done mmdrop() on it is doubly insane, since
       the mm struct may well be gone now.
    
     - testing mm against NULL before you call it is insane too, since a
    NULL mm there would have caused oopses long before.
    
    .. and those are just the three bugs I found before I decided to give up
    looking for me and revert it asap.  I should have caught it before I
    even took it, but I trusted Andrew too much.
    
    Cc: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Cc: Markus Trippelsdorf <markus@trippelsdorf.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 804fb6bb8161..34867cc5b42a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -423,7 +423,6 @@ void daemonize(const char *name, ...)
 	 * user space pages.  We don't need them, and if we didn't close them
 	 * they would be locked into memory.
 	 */
-	mm_release(current, current->mm);
 	exit_mm(current);
 	/*
 	 * We don't want to get frozen, in case system-wide hibernation
@@ -641,6 +640,7 @@ static void exit_mm(struct task_struct * tsk)
 	struct mm_struct *mm = tsk->mm;
 	struct core_state *core_state;
 
+	mm_release(tsk, mm);
 	if (!mm)
 		return;
 	/*
@@ -960,13 +960,9 @@ void do_exit(long code)
 				preempt_count());
 
 	acct_update_integrals(tsk);
-
-	/* Set exit_code before complete_vfork_done() in mm_release() */
-	tsk->exit_code = code;
-
-	/* Release mm and sync mm's RSS info before statistics gathering */
-	mm_release(tsk, tsk->mm);
-
+	/* sync mm's RSS info before statistics gathering */
+	if (tsk->mm)
+		sync_mm_rss(tsk->mm);
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
 		hrtimer_cancel(&tsk->signal->real_timer);
@@ -979,6 +975,7 @@ void do_exit(long code)
 		tty_audit_exit();
 	audit_free(tsk);
 
+	tsk->exit_code = code;
 	taskstats_exit(tsk, group_dead);
 
 	exit_mm(tsk);

commit 40af1bbdca47e5c8a2044039bb78ca8fd8b20f94
Author: Konstantin Khlebnikov <khlebnikov@openvz.org>
Date:   Thu Jun 7 14:21:14 2012 -0700

    mm: correctly synchronize rss-counters at exit/exec
    
    mm->rss_stat counters have per-task delta: task->rss_stat.  Before
    changing task->mm pointer the kernel must flush this delta with
    sync_mm_rss().
    
    do_exit() already calls sync_mm_rss() to flush the rss-counters before
    committing the rss statistics into task->signal->maxrss, taskstats,
    audit and other stuff.  Unfortunately the kernel does this before
    calling mm_release(), which can call put_user() for processing
    task->clear_child_tid.  So at this point we can trigger page-faults and
    task->rss_stat becomes non-zero again.  As a result mm->rss_stat becomes
    inconsistent and check_mm() will print something like this:
    
    | BUG: Bad rss-counter state mm:ffff88020813c380 idx:1 val:-1
    | BUG: Bad rss-counter state mm:ffff88020813c380 idx:2 val:1
    
    This patch moves sync_mm_rss() into mm_release(), and moves mm_release()
    out of do_exit() and calls it earlier.  After mm_release() there should
    be no pagefaults.
    
    [akpm@linux-foundation.org: tweak comment]
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@openvz.org>
    Reported-by: Markus Trippelsdorf <markus@trippelsdorf.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: <stable@vger.kernel.org>            [3.4.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 34867cc5b42a..804fb6bb8161 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -423,6 +423,7 @@ void daemonize(const char *name, ...)
 	 * user space pages.  We don't need them, and if we didn't close them
 	 * they would be locked into memory.
 	 */
+	mm_release(current, current->mm);
 	exit_mm(current);
 	/*
 	 * We don't want to get frozen, in case system-wide hibernation
@@ -640,7 +641,6 @@ static void exit_mm(struct task_struct * tsk)
 	struct mm_struct *mm = tsk->mm;
 	struct core_state *core_state;
 
-	mm_release(tsk, mm);
 	if (!mm)
 		return;
 	/*
@@ -960,9 +960,13 @@ void do_exit(long code)
 				preempt_count());
 
 	acct_update_integrals(tsk);
-	/* sync mm's RSS info before statistics gathering */
-	if (tsk->mm)
-		sync_mm_rss(tsk->mm);
+
+	/* Set exit_code before complete_vfork_done() in mm_release() */
+	tsk->exit_code = code;
+
+	/* Release mm and sync mm's RSS info before statistics gathering */
+	mm_release(tsk, tsk->mm);
+
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
 		hrtimer_cancel(&tsk->signal->real_timer);
@@ -975,7 +979,6 @@ void do_exit(long code)
 		tty_audit_exit();
 	audit_free(tsk);
 
-	tsk->exit_code = code;
 	taskstats_exit(tsk, group_dead);
 
 	exit_mm(tsk);

commit fb21affa49204acd409328415b49bfe90136653c
Merge: a00b6151a2ae f23ca335462e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 31 18:47:30 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull second pile of signal handling patches from Al Viro:
     "This one is just task_work_add() series + remaining prereqs for it.
    
      There probably will be another pull request from that tree this
      cycle - at least for helpers, to get them out of the way for per-arch
      fixes remaining in the tree."
    
    Fix trivial conflict in kernel/irq/manage.c: the merge of Andrew's pile
    had brought in commit 97fd75b7b8e0 ("kernel/irq/manage.c: use the
    pr_foo() infrastructure to prefix printks") which changed one of the
    pr_err() calls that this merge moves around.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal:
      keys: kill task_struct->replacement_session_keyring
      keys: kill the dummy key_replace_session_keyring()
      keys: change keyctl_session_to_parent() to use task_work_add()
      genirq: reimplement exit_irq_thread() hook via task_work_add()
      task_work_add: generic process-context callbacks
      avr32: missed _TIF_NOTIFY_RESUME on one of do_notify_resume callers
      parisc: need to check NOTIFY_RESUME when exiting from syscall
      move key_repace_session_keyring() into tracehook_notify_resume()
      TIF_NOTIFY_RESUME is defined on all targets now

commit 168eeccbc956d2ec083c3a513f7706784ee0dc5f
Author: Tim Bird <tim.bird@am.sony.com>
Date:   Thu May 31 16:26:16 2012 -0700

    stack usage: add pid to warning printk in check_stack_usage
    
    In embedded systems, sometimes the same program (busybox) is the cause of
    multiple warnings.  Outputting the pid with the program name in the
    warning printk helps distinguish which instances of a program are using
    the stack most.
    
    This is a small patch, but useful.
    
    Signed-off-by: Tim Bird <tim.bird@am.sony.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3281493ce7ad..6d85655353e9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -884,9 +884,9 @@ static void check_stack_usage(void)
 
 	spin_lock(&low_water_lock);
 	if (free < lowest_to_date) {
-		printk(KERN_WARNING "%s used greatest stack depth: %lu bytes "
-				"left\n",
-				current->comm, free);
+		printk(KERN_WARNING "%s (%d) used greatest stack depth: "
+				"%lu bytes left\n",
+				current->comm, task_pid_nr(current), free);
 		lowest_to_date = free;
 	}
 	spin_unlock(&low_water_lock);

commit 43e13cc107cf6cd3c15fbe1cef849435c2223d50
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu May 31 16:26:16 2012 -0700

    cred: remove task_is_dead() from __task_cred() validation
    
    Commit 8f92054e7ca1 ("CRED: Fix __task_cred()'s lockdep check and banner
    comment"):
    
        add the following validation condition:
    
            task->exit_state >= 0
    
        to permit the access if the target task is dead and therefore
        unable to change its own credentials.
    
    OK, but afaics currently this can only help wait_task_zombie() which calls
    __task_cred() without rcu lock.
    
    Remove this validation and change wait_task_zombie() to use task_uid()
    instead.  This means we do rcu_read_lock() only to shut up the lockdep,
    but we already do the same in, say, wait_task_stopped().
    
    task_is_dead() should die, task->exit_state != 0 means that this task has
    passed exit_notify(), only do_wait-like code paths should use this.
    
    Unfortunately, we can't kill task_is_dead() right now, it has already
    acquired buggy users in drivers/staging.  The fix already exists.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: James Morris <jmorris@namei.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 910a0716e17a..3281493ce7ad 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1214,7 +1214,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	unsigned long state;
 	int retval, status, traced;
 	pid_t pid = task_pid_vnr(p);
-	uid_t uid = from_kuid_munged(current_user_ns(), __task_cred(p)->uid);
+	uid_t uid = from_kuid_munged(current_user_ns(), task_uid(p));
 	struct siginfo __user *infop;
 
 	if (!likely(wo->wo_flags & WEXITED))

commit 4d1d61a6b203d957777d73fcebf19d90b038b5b2
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri May 11 10:59:08 2012 +1000

    genirq: reimplement exit_irq_thread() hook via task_work_add()
    
    exit_irq_thread() and task->irq_thread are needed to handle the unexpected
    (and unlikely) exit of irq-thread.
    
    We can use task_work instead and make this all private to
    kernel/irq/manage.c, cleanup plus micro-optimization.
    
    1. rename exit_irq_thread() to irq_thread_dtor(), make it
       static, and move it up before irq_thread().
    
    2. change irq_thread() to do task_work_add(irq_thread_dtor)
       at the start and task_work_cancel() before return.
    
       tracehook_notify_resume() can never play with kthreads,
       only do_exit()->exit_task_work() can call the callback
       and this is what we want.
    
    3. remove task_struct->irq_thread and the special hook
       in do_exit().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Smith <dsmith@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3d93325e0b1a..3ecd096e5d4d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -954,8 +954,6 @@ void do_exit(long code)
 
 	exit_task_work(tsk);
 
-	exit_irq_thread();
-
 	if (unlikely(in_atomic()))
 		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
 				current->comm, task_pid_nr(current),

commit e73f8959af0439d114847eab5a8a5ce48f1217c4
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri May 11 10:59:07 2012 +1000

    task_work_add: generic process-context callbacks
    
    Provide a simple mechanism that allows running code in the (nonatomic)
    context of the arbitrary task.
    
    The caller does task_work_add(task, task_work) and this task executes
    task_work->func() either from do_notify_resume() or from do_exit().  The
    callback can rely on PF_EXITING to detect the latter case.
    
    "struct task_work" can be embedded in another struct, still it has "void
    *data" to handle the most common/simple case.
    
    This allows us to kill the ->replacement_session_keyring hack, and
    potentially this can have more users.
    
    Performance-wise, this adds 2 "unlikely(!hlist_empty())" checks into
    tracehook_notify_resume() and do_exit().  But at the same time we can
    remove the "replacement_session_keyring != NULL" checks from
    arch/*/signal.c and exit_creds().
    
    Note: task_work_add/task_work_run abuses ->pi_lock.  This is only because
    this lock is already used by lookup_pi_state() to synchronize with
    do_exit() setting PF_EXITING.  Fortunately the scope of this lock in
    task_work.c is really tiny, and the code is unlikely anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Alexander Gordeev <agordeev@redhat.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Smith <dsmith@redhat.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 910a0716e17a..3d93325e0b1a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -946,11 +946,14 @@ void do_exit(long code)
 	exit_signals(tsk);  /* sets PF_EXITING */
 	/*
 	 * tsk->flags are checked in the futex code to protect against
-	 * an exiting task cleaning up the robust pi futexes.
+	 * an exiting task cleaning up the robust pi futexes, and in
+	 * task_work_add() to avoid the race with exit_task_work().
 	 */
 	smp_mb();
 	raw_spin_unlock_wait(&tsk->pi_lock);
 
+	exit_task_work(tsk);
+
 	exit_irq_thread();
 
 	if (unlikely(in_atomic()))

commit 8ca937a668d4be0b0c89c4fc9a912a5885ac06fe
Author: Sasha Levin <levinsasha928@gmail.com>
Date:   Thu May 17 23:31:39 2012 +0200

    cred: use correct cred accessor with regards to rcu read lock
    
    Commit "userns: Convert setting and getting uid and gid system calls to use
    kuid and kgid has modified the accessors in wait_task_continued() and
    wait_task_stopped() to use __task_cred() instead of task_uid().
    
    __task_cred() assumes that we're inside a rcu read lock, which is untrue
    for these two functions.
    
    Modify it to use task_uid() instead.
    
    Signed-off-by: Sasha Levin <levinsasha928@gmail.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 789e3c5777f7..910a0716e17a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1427,7 +1427,7 @@ static int wait_task_stopped(struct wait_opts *wo,
 	if (!unlikely(wo->wo_flags & WNOWAIT))
 		*p_code = 0;
 
-	uid = from_kuid_munged(current_user_ns(), __task_cred(p)->uid);
+	uid = from_kuid_munged(current_user_ns(), task_uid(p));
 unlock_sig:
 	spin_unlock_irq(&p->sighand->siglock);
 	if (!exit_code)
@@ -1500,7 +1500,7 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 	}
 	if (!unlikely(wo->wo_flags & WNOWAIT))
 		p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
-	uid = from_kuid_munged(current_user_ns(), __task_cred(p)->uid);
+	uid = from_kuid_munged(current_user_ns(), task_uid(p));
 	spin_unlock_irq(&p->sighand->siglock);
 
 	pid = task_pid_vnr(p);

commit a29c33f4e506e1dae7e0985b6328046535becbf8
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Feb 7 18:51:01 2012 -0800

    userns: Convert setting and getting uid and gid system calls to use kuid and kgid
    
    Convert setregid, setgid, setreuid, setuid,
    setresuid, getresuid, setresgid, getresgid, setfsuid, setfsgid,
    getuid, geteuid, getgid, getegid,
    waitpid, waitid, wait4.
    
    Convert userspace uids and gids into kuids and kgids before
    being placed on struct cred.  Convert struct cred kuids and
    kgids into userspace uids and gids when returning them.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index d8bd3b425fa7..789e3c5777f7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1214,7 +1214,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	unsigned long state;
 	int retval, status, traced;
 	pid_t pid = task_pid_vnr(p);
-	uid_t uid = __task_cred(p)->uid;
+	uid_t uid = from_kuid_munged(current_user_ns(), __task_cred(p)->uid);
 	struct siginfo __user *infop;
 
 	if (!likely(wo->wo_flags & WEXITED))
@@ -1427,7 +1427,7 @@ static int wait_task_stopped(struct wait_opts *wo,
 	if (!unlikely(wo->wo_flags & WNOWAIT))
 		*p_code = 0;
 
-	uid = task_uid(p);
+	uid = from_kuid_munged(current_user_ns(), __task_cred(p)->uid);
 unlock_sig:
 	spin_unlock_irq(&p->sighand->siglock);
 	if (!exit_code)
@@ -1500,7 +1500,7 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 	}
 	if (!unlikely(wo->wo_flags & WNOWAIT))
 		p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
-	uid = task_uid(p);
+	uid = from_kuid_munged(current_user_ns(), __task_cred(p)->uid);
 	spin_unlock_irq(&p->sighand->siglock);
 
 	pid = task_pid_vnr(p);

commit a591afc01d9e48affbacb365558a31e53c85af45
Merge: 820d41cf0cd0 31796ac4e8f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 18:12:23 2012 -0700

    Merge branch 'x86-x32-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x32 support for x86-64 from Ingo Molnar:
     "This tree introduces the X32 binary format and execution mode for x86:
      32-bit data space binaries using 64-bit instructions and 64-bit kernel
      syscalls.
    
      This allows applications whose working set fits into a 32 bits address
      space to make use of 64-bit instructions while using a 32-bit address
      space with shorter pointers, more compressed data structures, etc."
    
    Fix up trivial context conflicts in arch/x86/{Kconfig,vdso/vma.c}
    
    * 'x86-x32-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (71 commits)
      x32: Fix alignment fail in struct compat_siginfo
      x32: Fix stupid ia32/x32 inversion in the siginfo format
      x32: Add ptrace for x32
      x32: Switch to a 64-bit clock_t
      x32: Provide separate is_ia32_task() and is_x32_task() predicates
      x86, mtrr: Use explicit sizing and padding for the 64-bit ioctls
      x86/x32: Fix the binutils auto-detect
      x32: Warn and disable rather than error if binutils too old
      x32: Only clear TIF_X32 flag once
      x32: Make sure TS_COMPAT is cleared for x32 tasks
      fs: Remove missed ->fds_bits from cessation use of fd_set structs internally
      fs: Fix close_on_exec pointer in alloc_fdtable
      x32: Drop non-__vdso weak symbols from the x32 VDSO
      x32: Fix coding style violations in the x32 VDSO code
      x32: Add x32 VDSO support
      x32: Allow x32 to be configured
      x32: If configured, add x32 system calls to system call tables
      x32: Handle process creation
      x32: Signal-related system calls
      x86: Add #ifdef CONFIG_COMPAT to <asm/sys_ia32.h>
      ...

commit 397a21f24d455982a8a6f9bc11b5f3326ce3c6ef
Author: Denys Vlasenko <vda.linux@googlemail.com>
Date:   Fri Mar 23 15:01:54 2012 -0700

    kernel/exit.c: if init dies, log a signal which killed it, if any
    
    I just received another user's pleas for help when their init
    mysteriously died.  I again explained that they need to check whether it
    died because of bad instruction, a segv, or something else.  Which was
    an annoying detour into writing a trivial C program to spawn his init
    and print its exit code:
    
      http://lists.busybox.net/pipermail/busybox/2012-January/077172.html
    
    I hear you saying "just test it under /bin/sh".  Well, the crashing init
    _was_ /bin/sh.
    
    Which prompted me to make kernel do this first step automatically.  We can
    print exit code, which makes it possible to see that death was from e.g.
    SIGILL without writing test programs.
    
    [akpm@linux-foundation.org: add 0x to hex number output]
    Signed-off-by: Denys Vlasenko <vda.linux@googlemail.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 456329fd4ea3..3db1909faed9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -711,8 +711,11 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 
 	if (unlikely(pid_ns->child_reaper == father)) {
 		write_unlock_irq(&tasklist_lock);
-		if (unlikely(pid_ns == &init_pid_ns))
-			panic("Attempted to kill init!");
+		if (unlikely(pid_ns == &init_pid_ns)) {
+			panic("Attempted to kill init! exitcode=0x%08x\n",
+				father->signal->group_exit_code ?:
+					father->exit_code);
+		}
 
 		zap_pid_ns_processes(pid_ns);
 		write_lock_irq(&tasklist_lock);

commit ebec18a6d3aa1e7d84aab16225e87fd25170ec2b
Author: Lennart Poettering <lennart@poettering.net>
Date:   Fri Mar 23 15:01:54 2012 -0700

    prctl: add PR_{SET,GET}_CHILD_SUBREAPER to allow simple process supervision
    
    Userspace service managers/supervisors need to track their started
    services.  Many services daemonize by double-forking and get implicitly
    re-parented to PID 1.  The service manager will no longer be able to
    receive the SIGCHLD signals for them, and is no longer in charge of
    reaping the children with wait().  All information about the children is
    lost at the moment PID 1 cleans up the re-parented processes.
    
    With this prctl, a service manager process can mark itself as a sort of
    'sub-init', able to stay as the parent for all orphaned processes
    created by the started services.  All SIGCHLD signals will be delivered
    to the service manager.
    
    Receiving SIGCHLD and doing wait() is in cases of a service-manager much
    preferred over any possible asynchronous notification about specific
    PIDs, because the service manager has full access to the child process
    data in /proc and the PID can not be re-used until the wait(), the
    service-manager itself is in charge of, has happened.
    
    As a side effect, the relevant parent PID information does not get lost
    by a double-fork, which results in a more elaborate process tree and
    'ps' output:
    
    before:
      # ps afx
      253 ?        Ss     0:00 /bin/dbus-daemon --system --nofork
      294 ?        Sl     0:00 /usr/libexec/polkit-1/polkitd
      328 ?        S      0:00 /usr/sbin/modem-manager
      608 ?        Sl     0:00 /usr/libexec/colord
      658 ?        Sl     0:00 /usr/libexec/upowerd
      819 ?        Sl     0:00 /usr/libexec/imsettings-daemon
      916 ?        Sl     0:00 /usr/libexec/udisks-daemon
      917 ?        S      0:00  \_ udisks-daemon: not polling any devices
    
    after:
      # ps afx
      294 ?        Ss     0:00 /bin/dbus-daemon --system --nofork
      426 ?        Sl     0:00  \_ /usr/libexec/polkit-1/polkitd
      449 ?        S      0:00  \_ /usr/sbin/modem-manager
      635 ?        Sl     0:00  \_ /usr/libexec/colord
      705 ?        Sl     0:00  \_ /usr/libexec/upowerd
      959 ?        Sl     0:00  \_ /usr/libexec/udisks-daemon
      960 ?        S      0:00  |   \_ udisks-daemon: not polling any devices
      977 ?        Sl     0:00  \_ /usr/libexec/packagekitd
    
    This prctl is orthogonal to PID namespaces.  PID namespaces are isolated
    from each other, while a service management process usually requires the
    services to live in the same namespace, to be able to talk to each
    other.
    
    Users of this will be the systemd per-user instance, which provides
    init-like functionality for the user's login session and D-Bus, which
    activates bus services on-demand.  Both need init-like capabilities to
    be able to properly keep track of the services they start.
    
    Many thanks to Oleg for several rounds of review and insights.
    
    [akpm@linux-foundation.org: fix comment layout and spelling]
    [akpm@linux-foundation.org: add lengthy code comment from Oleg]
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Lennart Poettering <lennart@poettering.net>
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Acked-by: Valdis Kletnieks <Valdis.Kletnieks@vt.edu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 16b07bfac224..456329fd4ea3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -687,11 +687,11 @@ static void exit_mm(struct task_struct * tsk)
 }
 
 /*
- * When we die, we re-parent all our children.
- * Try to give them to another thread in our thread
- * group, and if no such member exists, give it to
- * the child reaper process (ie "init") in our pid
- * space.
+ * When we die, we re-parent all our children, and try to:
+ * 1. give them to another thread in our thread group, if such a member exists
+ * 2. give it to the first ancestor process which prctl'd itself as a
+ *    child_subreaper for its children (like a service manager)
+ * 3. give it to the init process (PID 1) in our pid namespace
  */
 static struct task_struct *find_new_reaper(struct task_struct *father)
 	__releases(&tasklist_lock)
@@ -722,6 +722,29 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 		 * forget_original_parent() must move them somewhere.
 		 */
 		pid_ns->child_reaper = init_pid_ns.child_reaper;
+	} else if (father->signal->has_child_subreaper) {
+		struct task_struct *reaper;
+
+		/*
+		 * Find the first ancestor marked as child_subreaper.
+		 * Note that the code below checks same_thread_group(reaper,
+		 * pid_ns->child_reaper).  This is what we need to DTRT in a
+		 * PID namespace. However we still need the check above, see
+		 * http://marc.info/?l=linux-kernel&m=131385460420380
+		 */
+		for (reaper = father->real_parent;
+		     reaper != &init_task;
+		     reaper = reaper->real_parent) {
+			if (same_thread_group(reaper, pid_ns->child_reaper))
+				break;
+			if (!reaper->signal->is_child_subreaper)
+				continue;
+			thread = reaper;
+			do {
+				if (!(thread->flags & PF_EXITING))
+					return reaper;
+			} while_each_thread(reaper, thread);
+		}
 	}
 
 	return pid_ns->child_reaper;

commit 95211279c5ad00a317c98221d7e4365e02f20836
Merge: 5375871d432a 12724850e806
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 22 09:04:48 2012 -0700

    Merge branch 'akpm' (Andrew's patch-bomb)
    
    Merge first batch of patches from Andrew Morton:
     "A few misc things and all the MM queue"
    
    * emailed from Andrew Morton <akpm@linux-foundation.org>: (92 commits)
      memcg: avoid THP split in task migration
      thp: add HPAGE_PMD_* definitions for !CONFIG_TRANSPARENT_HUGEPAGE
      memcg: clean up existing move charge code
      mm/memcontrol.c: remove unnecessary 'break' in mem_cgroup_read()
      mm/memcontrol.c: remove redundant BUG_ON() in mem_cgroup_usage_unregister_event()
      mm/memcontrol.c: s/stealed/stolen/
      memcg: fix performance of mem_cgroup_begin_update_page_stat()
      memcg: remove PCG_FILE_MAPPED
      memcg: use new logic for page stat accounting
      memcg: remove PCG_MOVE_LOCK flag from page_cgroup
      memcg: simplify move_account() check
      memcg: remove EXPORT_SYMBOL(mem_cgroup_update_page_stat)
      memcg: kill dead prev_priority stubs
      memcg: remove PCG_CACHE page_cgroup flag
      memcg: let css_get_next() rely upon rcu_read_lock()
      cgroup: revert ss_id_lock to spinlock
      idr: make idr_get_next() good for rcu_read_lock()
      memcg: remove unnecessary thp check in page stat accounting
      memcg: remove redundant returns
      memcg: enum lru_list lru
      ...

commit 05af2e104a0c282dcd9303431e1360750ba76de6
Author: David Rientjes <rientjes@google.com>
Date:   Wed Mar 21 16:34:13 2012 -0700

    mm, counters: remove task argument to sync_mm_rss() and __sync_task_rss_stat()
    
    sync_mm_rss() can only be used for current to avoid race conditions in
    iterating and clearing its per-task counters.  Remove the task argument
    for it and its helper function, __sync_task_rss_stat(), to avoid thinking
    it can be used safely for anything other than current.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 0ed15fed579f..d26acd3c1e2e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -934,7 +934,7 @@ void do_exit(long code)
 	acct_update_integrals(tsk);
 	/* sync mm's RSS info before statistics gathering */
 	if (tsk->mm)
-		sync_mm_rss(tsk, tsk->mm);
+		sync_mm_rss(tsk->mm);
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
 		hrtimer_cancel(&tsk->signal->real_timer);

commit 3556485f1595e3964ba539e39ea682acbb835cee
Merge: b8716614a7cc 09f61cdbb32a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 21 13:25:04 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates for 3.4 from James Morris:
     "The main addition here is the new Yama security module from Kees Cook,
      which was discussed at the Linux Security Summit last year.  Its
      purpose is to collect miscellaneous DAC security enhancements in one
      place.  This also marks a departure in policy for LSM modules, which
      were previously limited to being standalone access control systems.
      Chromium OS is using Yama, and I believe there are plans for Ubuntu,
      at least.
    
      This patchset also includes maintenance updates for AppArmor, TOMOYO
      and others."
    
    Fix trivial conflict in <net/sock.h> due to the jumo_label->static_key
    rename.
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (38 commits)
      AppArmor: Fix location of const qualifier on generated string tables
      TOMOYO: Return error if fails to delete a domain
      AppArmor: add const qualifiers to string arrays
      AppArmor: Add ability to load extended policy
      TOMOYO: Return appropriate value to poll().
      AppArmor: Move path failure information into aa_get_name and rename
      AppArmor: Update dfa matching routines.
      AppArmor: Minor cleanup of d_namespace_path to consolidate error handling
      AppArmor: Retrieve the dentry_path for error reporting when path lookup fails
      AppArmor: Add const qualifiers to generated string tables
      AppArmor: Fix oops in policy unpack auditing
      AppArmor: Fix error returned when a path lookup is disconnected
      KEYS: testing wrong bit for KEY_FLAG_REVOKED
      TOMOYO: Fix mount flags checking order.
      security: fix ima kconfig warning
      AppArmor: Fix the error case for chroot relative path name lookup
      AppArmor: fix mapping of META_READ to audit and quiet flags
      AppArmor: Fix underflow in xindex calculation
      AppArmor: Fix dropping of allowed operations that are force audited
      AppArmor: Add mising end of structure test to caps unpacking
      ...

commit c7c66c0cb0c77b1a8edf09bca57d922312d58030
Merge: 9f3938346a5c 98e8bdafeb47
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 21 10:15:51 2012 -0700

    Merge tag 'pm-for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates for 3.4 from Rafael Wysocki:
     "Assorted extensions and fixes including:
    
      * Introduction of early/late suspend/hibernation device callbacks.
      * Generic PM domains extensions and fixes.
      * devfreq updates from Axel Lin and MyungJoo Ham.
      * Device PM QoS updates.
      * Fixes of concurrency problems with wakeup sources.
      * System suspend and hibernation fixes."
    
    * tag 'pm-for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (43 commits)
      PM / Domains: Check domain status during hibernation restore of devices
      PM / devfreq: add relation of recommended frequency.
      PM / shmobile: Make MTU2 driver use pm_genpd_dev_always_on()
      PM / shmobile: Make CMT driver use pm_genpd_dev_always_on()
      PM / shmobile: Make TMU driver use pm_genpd_dev_always_on()
      PM / Domains: Introduce "always on" device flag
      PM / Domains: Fix hibernation restore of devices, v2
      PM / Domains: Fix handling of wakeup devices during system resume
      sh_mmcif / PM: Use PM QoS latency constraint
      tmio_mmc / PM: Use PM QoS latency constraint
      PM / QoS: Make it possible to expose PM QoS latency constraints
      PM / Sleep: JBD and JBD2 missing set_freezable()
      PM / Domains: Fix include for PM_GENERIC_DOMAINS=n case
      PM / Freezer: Remove references to TIF_FREEZE in comments
      PM / Sleep: Add more wakeup source initialization routines
      PM / Hibernate: Enable usermodehelpers in hibernate() error path
      PM / Sleep: Make __pm_stay_awake() delete wakeup source timers
      PM / Sleep: Fix race conditions related to wakeup source timer function
      PM / Sleep: Fix possible infinite loop during wakeup source destruction
      PM / Hibernate: print physical addresses consistently with other parts of kernel
      ...

commit b6e238dceed36891cc633167afe7151f1f3d83c5
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 19 17:03:41 2012 +0100

    exit_signal: fix the "parent has changed security domain" logic
    
    exit_notify() changes ->exit_signal if the parent already did exec.
    This doesn't really work, we are not going to send the signal now
    if there is another live thread or the exiting task is traced. The
    parent can exec before the last dies or the tracer detaches.
    
    Move this check into do_notify_parent() which actually sends the
    signal.
    
    The user-visible change is that we do not change ->exit_signal,
    and thus the exiting task is still "clone children" for
    do_wait()->eligible_child(__WCLONE). Hopefully this is fine, the
    current logic is racy anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 51ac4ced1313..ce5f758f40bd 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -818,20 +818,6 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	if (group_dead)
 		kill_orphaned_pgrp(tsk->group_leader, NULL);
 
-	/* Let father know we died
-	 *
-	 * Thread signals are configurable, but you aren't going to use
-	 * that to send signals to arbitrary processes.
-	 * That stops right now.
-	 *
-	 * If the parent exec id doesn't match the exec id we saved
-	 * when we started then we know the parent has changed security
-	 * domain.
-	 */
-	if (thread_group_leader(tsk) && tsk->exit_signal != SIGCHLD &&
-	    tsk->parent_exec_id != tsk->real_parent->self_exec_id)
-		tsk->exit_signal = SIGCHLD;
-
 	if (unlikely(tsk->ptrace)) {
 		int sig = thread_group_leader(tsk) &&
 				thread_group_empty(tsk) &&

commit e636825346b36a07ccfc8e30946d52855e21f681
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Mar 19 17:03:22 2012 +0100

    exit_signal: simplify the "we have changed execution domain" logic
    
    exit_notify() checks "tsk->self_exec_id != tsk->parent_exec_id"
    to handle the "we have changed execution domain" case.
    
    We can change do_thread() to always set ->exit_signal = SIGCHLD
    and remove this check to simplify the code.
    
    We could change setup_new_exec() instead, this looks more logical
    because it increments ->self_exec_id. But note that de_thread()
    already resets ->exit_signal if it changes the leader, let's keep
    both changes close to each other.
    
    Note that we change ->exit_signal lockless, this changes the rules.
    Thereafter ->exit_signal is not stable under tasklist but this is
    fine, the only possible change is OLDSIG -> SIGCHLD. This can race
    with eligible_child() but the race is harmless. We can race with
    reparent_leader() which changes our ->exit_signal in parallel, but
    it does the same change to SIGCHLD.
    
    The noticeable user-visible change is that the execing task is not
    "visible" to do_wait()->eligible_child(__WCLONE) right after exec.
    To me this looks more logical, and this is consistent with mt case.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 752d2c0abd19..51ac4ced1313 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -827,14 +827,9 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	 * If the parent exec id doesn't match the exec id we saved
 	 * when we started then we know the parent has changed security
 	 * domain.
-	 *
-	 * If our self_exec id doesn't match our parent_exec_id then
-	 * we have changed execution domain as these two values started
-	 * the same after a fork.
 	 */
 	if (thread_group_leader(tsk) && tsk->exit_signal != SIGCHLD &&
-	    (tsk->parent_exec_id != tsk->real_parent->self_exec_id ||
-	     tsk->self_exec_id != tsk->parent_exec_id))
+	    tsk->parent_exec_id != tsk->real_parent->self_exec_id)
 		tsk->exit_signal = SIGCHLD;
 
 	if (unlikely(tsk->ptrace)) {

commit 5234ffb9f74cfa8993d174782bc861dd9b7b5bfb
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Fri Mar 9 14:59:59 2012 +0100

    genirq: Get rid of unnecessary IRQTF_DIED flag
    
    Currently IRQTF_DIED flag is set when a IRQ thread handler calls do_exit()
    But also PF_EXITING per process flag gets set when a thread exits. This
    fix eliminates the duplicate by using PF_EXITING flag.
    
    Also, there is a race condition in exit_irq_thread(). In case a thread's
    bit is cleared in desc->threads_oneshot (and the IRQ line gets unmasked),
    but before IRQTF_DIED flag is set, a new interrupt might come in and set
    just cleared bit again, this time forever. This fix throws IRQTF_DIED flag
    away, eliminating the race as a result.
    
    [ tglx: Test THREAD_EXITING first as suggested by Oleg ]
    
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Link: http://lkml.kernel.org/r/20120309135958.GD2114@dhcp-26-207.brq.redhat.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4b4042f9bc6a..752d2c0abd19 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -935,8 +935,6 @@ void do_exit(long code)
 		schedule();
 	}
 
-	exit_irq_thread();
-
 	exit_signals(tsk);  /* sets PF_EXITING */
 	/*
 	 * tsk->flags are checked in the futex code to protect against
@@ -945,6 +943,8 @@ void do_exit(long code)
 	smp_mb();
 	raw_spin_unlock_wait(&tsk->pi_lock);
 
+	exit_irq_thread();
+
 	if (unlikely(in_atomic()))
 		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
 				current->comm, task_pid_nr(current),

commit 643161ace2a7624fd0106ede12ae43bcbbfc1de0
Merge: 743c5bc210f4 37f08be11be9
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Sun Mar 4 23:11:14 2012 +0100

    Merge branch 'pm-sleep'
    
    * pm-sleep:
      PM / Freezer: Remove references to TIF_FREEZE in comments
      PM / Sleep: Add more wakeup source initialization routines
      PM / Hibernate: Enable usermodehelpers in hibernate() error path
      PM / Sleep: Make __pm_stay_awake() delete wakeup source timers
      PM / Sleep: Fix race conditions related to wakeup source timer function
      PM / Sleep: Fix possible infinite loop during wakeup source destruction
      PM / Hibernate: print physical addresses consistently with other parts of kernel
      PM: Add comment describing relationships between PM callbacks to pm.h
      PM / Sleep: Drop suspend_stats_update()
      PM / Sleep: Make enter_state() in kernel/power/suspend.c static
      PM / Sleep: Unify kerneldoc comments in kernel/power/suspend.c
      PM / Sleep: Remove unnecessary label from suspend_freeze_processes()
      PM / Sleep: Do not check wakeup too often in try_to_freeze_tasks()
      PM / Sleep: Initialize wakeup source locks in wakeup_source_add()
      PM / Hibernate: Refactor and simplify freezer_test_done
      PM / Hibernate: Thaw kernel threads in hibernation_snapshot() in error/test path
      PM / Freezer / Docs: Document the beauty of freeze/thaw semantics
      PM / Suspend: Avoid code duplication in suspend statistics update
      PM / Sleep: Introduce generic callbacks for new device PM phases
      PM / Sleep: Introduce "late suspend" and "early resume" of devices

commit 37f08be11be9a7d9351fb1b9b408259519a126f3
Author: Marcos Paulo de Souza <marcos.mage@gmail.com>
Date:   Tue Feb 21 23:57:47 2012 +0100

    PM / Freezer: Remove references to TIF_FREEZE in comments
    
    This patch removes all the references in the code about the TIF_FREEZE
    flag removed by commit a3201227f803ad7fd43180c5195dbe5a2bf998aa
    
        freezer: make freezing() test freeze conditions in effect instead of TIF_FREEZE
    
    There still are some references to TIF_FREEZE in
    Documentation/power/freezing-of-tasks.txt, but it looks like that
    documentation needs more thorough work to reflect how the new
    freezer works, and hence merely removing the references to TIF_FREEZE
    won't really help. So I have not touched that part in this patch.
    
    Suggested-by: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Signed-off-by: Marcos Paulo de Souza <marcos.mage@gmail.com>
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>

diff --git a/kernel/exit.c b/kernel/exit.c
index 294b1709170d..fd0af05e0639 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -424,7 +424,7 @@ void daemonize(const char *name, ...)
 	 */
 	exit_mm(current);
 	/*
-	 * We don't want to have TIF_FREEZE set if the system-wide hibernation
+	 * We don't want to get frozen, in case system-wide hibernation
 	 * or suspend transition begins right now.
 	 */
 	current->flags |= (PF_NOFREEZE | PF_KTHREAD);

commit 1fd36adcd98c14d2fd97f545293c488775cb2823
Author: David Howells <dhowells@redhat.com>
Date:   Thu Feb 16 17:49:54 2012 +0000

    Replace the fd_sets in struct fdtable with an array of unsigned longs
    
    Replace the fd_sets in struct fdtable with an array of unsigned longs and then
    use the standard non-atomic bit operations rather than the FD_* macros.
    
    This:
    
     (1) Removes the abuses of struct fd_set:
    
         (a) Since we don't want to allocate a full fd_set the vast majority of the
             time, we actually, in effect, just allocate a just-big-enough array of
             unsigned longs and cast it to an fd_set type - so why bother with the
             fd_set at all?
    
         (b) Some places outside of the core fdtable handling code (such as
             SELinux) want to look inside the array of unsigned longs hidden inside
             the fd_set struct for more efficient iteration over the entire set.
    
     (2) Eliminates the use of FD_*() macros in the kernel completely.
    
     (3) Permits the __FD_*() macros to be deleted entirely where not exposed to
         userspace.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Link: http://lkml.kernel.org/r/20120216174954.23314.48147.stgit@warthog.procyon.org.uk
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4b4042f9bc6a..4db020015f14 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -473,7 +473,7 @@ static void close_files(struct files_struct * files)
 		i = j * __NFDBITS;
 		if (i >= fdt->max_fds)
 			break;
-		set = fdt->open_fds->fds_bits[j++];
+		set = fdt->open_fds[j++];
 		while (set) {
 			if (set & 1) {
 				struct file * file = xchg(&fdt->fd[i], NULL);

commit 4040153087478993cbf0809f444400a3c808074c
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Mon Feb 13 03:58:52 2012 +0000

    security: trim security.h
    
    Trim security.h
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4b4042f9bc6a..5ad867a3685e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -52,6 +52,7 @@
 #include <linux/hw_breakpoint.h>
 #include <linux/oom.h>
 #include <linux/writeback.h>
+#include <linux/shm.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>

commit b5740f4b2cb3503b436925eb2242bc3d75cd3dfe
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Tue Jan 17 17:40:31 2012 +0900

    sched: Fix ancient race in do_exit()
    
    try_to_wake_up() has a problem which may change status from TASK_DEAD to
    TASK_RUNNING in race condition with SMI or guest environment of virtual
    machine. As a result, exited task is scheduled() again and panic occurs.
    
    Here is the sequence how it occurs:
    
     ----------------------------------+-----------------------------
                                       |
                CPU A                  |             CPU B
     ----------------------------------+-----------------------------
    
    TASK A calls exit()....
    
    do_exit()
    
      exit_mm()
        down_read(mm->mmap_sem);
    
        rwsem_down_failed_common()
    
          set TASK_UNINTERRUPTIBLE
          set waiter.task <= task A
          list_add to sem->wait_list
               :
          raw_spin_unlock_irq()
          (I/O interruption occured)
    
                                          __rwsem_do_wake(mmap_sem)
    
                                            list_del(&waiter->list);
                                            waiter->task = NULL
                                            wake_up_process(task A)
                                              try_to_wake_up()
                                                 (task is still
                                                   TASK_UNINTERRUPTIBLE)
                                                  p->on_rq is still 1.)
    
                                                  ttwu_do_wakeup()
                                                     (*A)
                                                       :
         (I/O interruption handler finished)
    
          if (!waiter.task)
              schedule() is not called
              due to waiter.task is NULL.
    
          tsk->state = TASK_RUNNING
    
              :
                                                  check_preempt_curr();
                                                      :
      task->state = TASK_DEAD
                                                  (*B)
                                            <---    set TASK_RUNNING (*C)
    
         schedule()
         (exit task is running again)
         BUG_ON() is called!
     --------------------------------------------------------
    
    The execution time between (*A) and (*B) is usually very short,
    because the interruption is disabled, and setting TASK_RUNNING at (*C)
    must be executed before setting TASK_DEAD.
    
    HOWEVER, if SMI is interrupted between (*A) and (*B),
    (*C) is able to execute AFTER setting TASK_DEAD!
    Then, exited task is scheduled again, and BUG_ON() is called....
    
    If the system works on guest system of virtual machine, the time
    between (*A) and (*B) may be also long due to scheduling of hypervisor,
    and same phenomenon can occur.
    
    By this patch, do_exit() waits for releasing task->pi_lock which is used
    in try_to_wake_up(). It guarantees the task becomes TASK_DEAD after
    waking up.
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20120117174031.3118.E1E9C6FF@jp.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 294b1709170d..4b4042f9bc6a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1038,6 +1038,22 @@ void do_exit(long code)
 	if (tsk->nr_dirtied)
 		__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);
 	exit_rcu();
+
+	/*
+	 * The setting of TASK_RUNNING by try_to_wake_up() may be delayed
+	 * when the following two conditions become true.
+	 *   - There is race condition of mmap_sem (It is acquired by
+	 *     exit_mm()), and
+	 *   - SMI occurs before setting TASK_RUNINNG.
+	 *     (or hypervisor of virtual machine switches to other guest)
+	 *  As a result, we may become TASK_RUNNING after becoming TASK_DEAD
+	 *
+	 * To avoid it, we have to wait for releasing tsk->pi_lock which
+	 * is held by try_to_wake_up()
+	 */
+	smp_mb();
+	raw_spin_unlock_wait(&tsk->pi_lock);
+
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;
 	tsk->flags |= PF_NOFREEZE;	/* tell freezer to ignore us */

commit f429ee3b808118591d1f3cdf3c0d0793911a5677
Merge: 22b4eb5e3174 c158a35c8a68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 17 16:06:51 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/audit
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/audit: (29 commits)
      audit: no leading space in audit_log_d_path prefix
      audit: treat s_id as an untrusted string
      audit: fix signedness bug in audit_log_execve_info()
      audit: comparison on interprocess fields
      audit: implement all object interfield comparisons
      audit: allow interfield comparison between gid and ogid
      audit: complex interfield comparison helper
      audit: allow interfield comparison in audit rules
      Kernel: Audit Support For The ARM Platform
      audit: do not call audit_getname on error
      audit: only allow tasks to set their loginuid if it is -1
      audit: remove task argument to audit_set_loginuid
      audit: allow audit matching on inode gid
      audit: allow matching on obj_uid
      audit: remove audit_finish_fork as it can't be called
      audit: reject entry,always rules
      audit: inline audit_free to simplify the look of generic code
      audit: drop audit_set_macxattr as it doesn't do anything
      audit: inline checks for not needing to collect aux records
      audit: drop some potentially inadvisable likely notations
      ...
    
    Use evil merge to fix up grammar mistakes in Kconfig file.
    
    Bad speling and horrible grammar (and copious swearing) is to be
    expected, but let's keep it to commit messages and comments, rather than
    expose it to users in config help texts or printouts.

commit a4ff8dba7d8ce5ceb43fb27df66292251cc73bdc
Author: Eric Paris <eparis@redhat.com>
Date:   Tue Jan 3 14:23:07 2012 -0500

    audit: inline audit_free to simplify the look of generic code
    
    make the conditional a static inline instead of doing it in generic code.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 94ed6e20bb53..88dcbbc446f7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -964,8 +964,7 @@ NORET_TYPE void do_exit(long code)
 	acct_collect(code, group_dead);
 	if (group_dead)
 		tty_audit_exit();
-	if (unlikely(tsk->audit_context))
-		audit_free(tsk);
+	audit_free(tsk);
 
 	tsk->exit_code = code;
 	taskstats_exit(tsk, group_dead);

commit 9402c95f34a66e81eba473a2f7267bbae5a1dee2
Author: Joe Perches <joe@perches.com>
Date:   Thu Jan 12 17:17:17 2012 -0800

    treewide: remove useless NORET_TYPE macro and uses
    
    It's a very old and now unused prototype marking so just delete it.
    
    Neaten panic pointer argument style to keep checkpatch quiet.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 94ed6e20bb53..c44738267be7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -887,7 +887,7 @@ static void check_stack_usage(void)
 static inline void check_stack_usage(void) {}
 #endif
 
-NORET_TYPE void do_exit(long code)
+void do_exit(long code)
 {
 	struct task_struct *tsk = current;
 	int group_dead;
@@ -1051,7 +1051,7 @@ NORET_TYPE void do_exit(long code)
 
 EXPORT_SYMBOL_GPL(do_exit);
 
-NORET_TYPE void complete_and_exit(struct completion *comp, long code)
+void complete_and_exit(struct completion *comp, long code)
 {
 	if (comp)
 		complete(comp);
@@ -1070,7 +1070,7 @@ SYSCALL_DEFINE1(exit, int, error_code)
  * Take down every thread in the group.  This is called by fatal signals
  * as well as by sys_exit_group (below).
  */
-NORET_TYPE void
+void
 do_group_exit(int exit_code)
 {
 	struct signal_struct *sig = current->signal;

commit 001a541ea9163ace5e8243ee0e907ad80a4c0ec2
Merge: 40ba587923ae bc31b86a5923
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 10 16:59:59 2012 -0800

    Merge branch 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux
    
    * 'writeback-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux:
      writeback: move MIN_WRITEBACK_PAGES to fs-writeback.c
      writeback: balanced_rate cannot exceed write bandwidth
      writeback: do strict bdi dirty_exceeded
      writeback: avoid tiny dirty poll intervals
      writeback: max, min and target dirty pause time
      writeback: dirty ratelimit - think time compensation
      btrfs: fix dirtied pages accounting on sub-page writes
      writeback: fix dirtied pages accounting on redirty
      writeback: fix dirtied pages accounting on sub-page writes
      writeback: charge leaked page dirties to active tasks
      writeback: Include all dirty inodes in background writeback

commit eb59c505f8a5906ad2e053d14fab50eb8574fd6f
Merge: 1619ed8f6095 c233523b3d39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 8 13:10:57 2012 -0800

    Merge branch 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    * 'pm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (76 commits)
      PM / Hibernate: Implement compat_ioctl for /dev/snapshot
      PM / Freezer: fix return value of freezable_schedule_timeout_killable()
      PM / shmobile: Allow the A4R domain to be turned off at run time
      PM / input / touchscreen: Make st1232 use device PM QoS constraints
      PM / QoS: Introduce dev_pm_qos_add_ancestor_request()
      PM / shmobile: Remove the stay_on flag from SH7372's PM domains
      PM / shmobile: Don't include SH7372's INTCS in syscore suspend/resume
      PM / shmobile: Add support for the sh7372 A4S power domain / sleep mode
      PM: Drop generic_subsys_pm_ops
      PM / Sleep: Remove forward-only callbacks from AMBA bus type
      PM / Sleep: Remove forward-only callbacks from platform bus type
      PM: Run the driver callback directly if the subsystem one is not there
      PM / Sleep: Make pm_op() and pm_noirq_op() return callback pointers
      PM/Devfreq: Add Exynos4-bus device DVFS driver for Exynos4210/4212/4412.
      PM / Sleep: Merge internal functions in generic_ops.c
      PM / Sleep: Simplify generic system suspend callbacks
      PM / Hibernate: Remove deprecated hibernation snapshot ioctls
      PM / Sleep: Fix freezer failures due to racy usermodehelper_is_disabled()
      ARM: S3C64XX: Implement basic power domain support
      PM / shmobile: Use common always on power domain governor
      ...
    
    Fix up trivial conflict in fs/xfs/xfs_buf.c due to removal of unused
    XBT_FORCE_SLEEP bit

commit 0db49b72bce26341274b74fd968501489a361ae3
Merge: 35b740e4662e 1ac9bc6943ed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 6 08:33:28 2012 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (40 commits)
      sched/tracing: Add a new tracepoint for sleeptime
      sched: Disable scheduler warnings during oopses
      sched: Fix cgroup movement of waking process
      sched: Fix cgroup movement of newly created process
      sched: Fix cgroup movement of forking process
      sched: Remove cfs bandwidth period check in tg_set_cfs_period()
      sched: Fix load-balance lock-breaking
      sched: Replace all_pinned with a generic flags field
      sched: Only queue remote wakeups when crossing cache boundaries
      sched: Add missing rcu_dereference() around ->real_parent usage
      [S390] fix cputime overflow in uptime_proc_show
      [S390] cputime: add sparse checking and cleanup
      sched: Mark parent and real_parent as __rcu
      sched, nohz: Fix missing RCU read lock
      sched, nohz: Set the NOHZ_BALANCE_KICK flag for idle load balancer
      sched, nohz: Fix the idle cpu check in nohz_idle_balance
      sched: Use jump_labels for sched_feat
      sched/accounting: Fix parameter passing in task_group_account_field
      sched/accounting: Fix user/system tick double accounting
      sched/accounting: Re-use scheduler statistics for the root cgroup
      ...
    
    Fix up conflicts in
     - arch/ia64/include/asm/cputime.h, include/asm-generic/cputime.h
            usecs_to_cputime64() vs the sparse cleanups
     - kernel/sched/fair.c, kernel/time/tick-sched.c
            scheduler changes in multiple branches

commit 50b8d257486a45cba7b65ca978986ed216bbcc10
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jan 4 17:29:02 2012 +0100

    ptrace: partially fix the do_wait(WEXITED) vs EXIT_DEAD->EXIT_ZOMBIE race
    
    Test-case:
    
            int main(void)
            {
                    int pid, status;
    
                    pid = fork();
                    if (!pid) {
                            for (;;) {
                                    if (!fork())
                                            return 0;
                                    if (waitpid(-1, &status, 0) < 0) {
                                            printf("ERR!! wait: %m\n");
                                            return 0;
                                    }
                            }
                    }
    
                    assert(ptrace(PTRACE_ATTACH, pid, 0,0) == 0);
                    assert(waitpid(-1, NULL, 0) == pid);
    
                    assert(ptrace(PTRACE_SETOPTIONS, pid, 0,
                                            PTRACE_O_TRACEFORK) == 0);
    
                    do {
                            ptrace(PTRACE_CONT, pid, 0, 0);
                            pid = waitpid(-1, NULL, 0);
                    } while (pid > 0);
    
                    return 1;
            }
    
    It fails because ->real_parent sees its child in EXIT_DEAD state
    while the tracer is going to change the state back to EXIT_ZOMBIE
    in wait_task_zombie().
    
    The offending commit is 823b018e which moved the EXIT_DEAD check,
    but in fact we should not blame it. The original code was not
    correct as well because it didn't take ptrace_reparented() into
    account and because we can't really trust ->ptrace.
    
    This patch adds the additional check to close this particular
    race but it doesn't solve the whole problem. We simply can't
    rely on ->ptrace in this case, it can be cleared if the tracer
    is multithreaded by the exiting ->parent.
    
    I think we should kill EXIT_DEAD altogether, we should always
    remove the soon-to-be-reaped child from ->children or at least
    we should never do the DEAD->ZOMBIE transition. But this is too
    complex for 3.2.
    
    Reported-and-tested-by: Denys Vlasenko <vda.linux@googlemail.com>
    Tested-by: Lukasz Michalik <lmi@ift.uni.wroc.pl>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: <stable@kernel.org>         [3.0+]
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index d0b7d988f873..e6e01b959a0e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1540,8 +1540,15 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 	}
 
 	/* dead body doesn't have much to contribute */
-	if (p->exit_state == EXIT_DEAD)
+	if (unlikely(p->exit_state == EXIT_DEAD)) {
+		/*
+		 * But do not ignore this task until the tracer does
+		 * wait_task_zombie()->do_notify_parent().
+		 */
+		if (likely(!ptrace) && unlikely(ptrace_reparented(p)))
+			wo->notask_error = 0;
 		return 0;
+	}
 
 	/* slay zombie? */
 	if (p->exit_state == EXIT_ZOMBIE) {

commit 54848d73f9f254631303d6eab9b976855988b266
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Tue Apr 5 13:21:19 2011 -0600

    writeback: charge leaked page dirties to active tasks
    
    It's a years long problem that a large number of short-lived dirtiers
    (eg. gcc instances in a fast kernel build) may starve long-run dirtiers
    (eg. dd) as well as pushing the dirty pages to the global hard limit.
    
    The solution is to charge the pages dirtied by the exited gcc to the
    other random dirtying tasks. It sounds not perfect, however should
    behave good enough in practice, seeing as that throttled tasks aren't
    actually running so those that are running are more likely to pick it up
    and get throttled, therefore promoting an equal spread.
    
    Randy: fix compile error: 'dirty_throttle_leaks' undeclared in exit.c
    
    Acked-by: Jan Kara <jack@suse.cz>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index d0b7d988f873..d4aac24cc469 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -51,6 +51,7 @@
 #include <trace/events/sched.h>
 #include <linux/hw_breakpoint.h>
 #include <linux/oom.h>
+#include <linux/writeback.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -1037,6 +1038,8 @@ NORET_TYPE void do_exit(long code)
 	validate_creds_for_do_exit(tsk);
 
 	preempt_disable();
+	if (tsk->nr_dirtied)
+		__this_cpu_add(dirty_throttle_leaks, tsk->nr_dirtied);
 	exit_rcu();
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;

commit 648616343cdbe904c585a6c12e323d3b3c72e46f
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Thu Dec 15 14:56:09 2011 +0100

    [S390] cputime: add sparse checking and cleanup
    
    Make cputime_t and cputime64_t nocast to enable sparse checking to
    detect incorrect use of cputime. Drop the cputime macros for simple
    scalar operations. The conversion macros are still needed.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index d0b7d988f873..5e0d1f4c696e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -121,9 +121,9 @@ static void __exit_signal(struct task_struct *tsk)
 		 * We won't ever get here for the group leader, since it
 		 * will have been the last reference on the signal_struct.
 		 */
-		sig->utime = cputime_add(sig->utime, tsk->utime);
-		sig->stime = cputime_add(sig->stime, tsk->stime);
-		sig->gtime = cputime_add(sig->gtime, tsk->gtime);
+		sig->utime += tsk->utime;
+		sig->stime += tsk->stime;
+		sig->gtime += tsk->gtime;
 		sig->min_flt += tsk->min_flt;
 		sig->maj_flt += tsk->maj_flt;
 		sig->nvcsw += tsk->nvcsw;
@@ -1255,19 +1255,9 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		spin_lock_irq(&p->real_parent->sighand->siglock);
 		psig = p->real_parent->signal;
 		sig = p->signal;
-		psig->cutime =
-			cputime_add(psig->cutime,
-			cputime_add(tgutime,
-				    sig->cutime));
-		psig->cstime =
-			cputime_add(psig->cstime,
-			cputime_add(tgstime,
-				    sig->cstime));
-		psig->cgtime =
-			cputime_add(psig->cgtime,
-			cputime_add(p->gtime,
-			cputime_add(sig->gtime,
-				    sig->cgtime)));
+		psig->cutime += tgutime + sig->cutime;
+		psig->cstime += tgstime + sig->cstime;
+		psig->cgtime += p->gtime + sig->gtime + sig->cgtime;
 		psig->cmin_flt +=
 			p->min_flt + sig->min_flt + sig->cmin_flt;
 		psig->cmaj_flt +=

commit a585042f7b933539a0b6bc63650c2d49ffb2e55d
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Nov 21 12:32:23 2011 -0800

    freezer: remove racy clear_freeze_flag() and set PF_NOFREEZE on dead tasks
    
    clear_freeze_flag() in exit_mm() is racy.  Freezing can start
    afterwards.  Remove it.  Skipping freezer for exiting task will be
    properly implemented later.
    
    Also, freezable() was testing exit_state directly to make system
    freezer ignore dead tasks.  Let the exiting task set PF_NOFREEZE after
    entering TASK_DEAD instead.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index d0b7d988f873..95a4141d07e7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -679,8 +679,6 @@ static void exit_mm(struct task_struct * tsk)
 	tsk->mm = NULL;
 	up_read(&mm->mmap_sem);
 	enter_lazy_tlb(mm, current);
-	/* We don't want this task to be frozen prematurely */
-	clear_freeze_flag(tsk);
 	task_unlock(tsk);
 	mm_update_next_owner(mm);
 	mmput(mm);
@@ -1040,6 +1038,7 @@ NORET_TYPE void do_exit(long code)
 	exit_rcu();
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;
+	tsk->flags |= PF_NOFREEZE;	/* tell freezer to ignore us */
 	schedule();
 	BUG();
 	/* Avoid "noreturn function does return".  */

commit c9f01245b6a7d77d17deaa71af10f6aca14fa24e
Author: David Rientjes <rientjes@google.com>
Date:   Mon Oct 31 17:07:15 2011 -0700

    oom: remove oom_disable_count
    
    This removes mm->oom_disable_count entirely since it's unnecessary and
    currently buggy.  The counter was intended to be per-process but it's
    currently decremented in the exit path for each thread that exits, causing
    it to underflow.
    
    The count was originally intended to prevent oom killing threads that
    share memory with threads that cannot be killed since it doesn't lead to
    future memory freeing.  The counter could be fixed to represent all
    threads sharing the same mm, but it's better to remove the count since:
    
     - it is possible that the OOM_DISABLE thread sharing memory with the
       victim is waiting on that thread to exit and will actually cause
       future memory freeing, and
    
     - there is no guarantee that a thread is disabled from oom killing just
       because another thread sharing its mm is oom disabled.
    
    Signed-off-by: David Rientjes <rientjes@google.com>
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Ying Han <yinghan@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2913b3509d42..d0b7d988f873 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -681,8 +681,6 @@ static void exit_mm(struct task_struct * tsk)
 	enter_lazy_tlb(mm, current);
 	/* We don't want this task to be frozen prematurely */
 	clear_freeze_flag(tsk);
-	if (tsk->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)
-		atomic_dec(&mm->oom_disable_count);
 	task_unlock(tsk);
 	mm_update_next_owner(mm);
 	mmput(mm);

commit b34a6b1da371ed8af1221459a18c67970f7e3d53
Author: Vasiliy Kulikov <segoon@openwall.com>
Date:   Tue Jul 26 16:08:48 2011 -0700

    ipc: introduce shm_rmid_forced sysctl
    
    Add support for the shm_rmid_forced sysctl.  If set to 1, all shared
    memory objects in current ipc namespace will be automatically forced to
    use IPC_RMID.
    
    The POSIX way of handling shmem allows one to create shm objects and
    call shmdt(), leaving shm object associated with no process, thus
    consuming memory not counted via rlimits.
    
    With shm_rmid_forced=1 the shared memory object is counted at least for
    one process, so OOM killer may effectively kill the fat process holding
    the shared memory.
    
    It obviously breaks POSIX - some programs relying on the feature would
    stop working.  So set shm_rmid_forced=1 only if you're sure nobody uses
    "orphaned" memory.  Use shm_rmid_forced=0 by default for compatability
    reasons.
    
    The feature was previously impemented in -ow as a configure option.
    
    [akpm@linux-foundation.org: fix documentation, per Randy]
    [akpm@linux-foundation.org: fix warning]
    [akpm@linux-foundation.org: readability/conventionality tweaks]
    [akpm@linux-foundation.org: fix shm_rmid_forced/shm_forced_rmid confusion, use standard comment layout]
    Signed-off-by: Vasiliy Kulikov <segoon@openwall.com>
    Cc: Randy Dunlap <rdunlap@xenotime.net>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "Serge E. Hallyn" <serge.hallyn@canonical.com>
    Cc: Daniel Lezcano <daniel.lezcano@free.fr>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Solar Designer <solar@openwall.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9ee58bb9e60f..2913b3509d42 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -980,6 +980,7 @@ NORET_TYPE void do_exit(long code)
 	trace_sched_process_exit(tsk);
 
 	exit_sem(tsk);
+	exit_shm(tsk);
 	exit_files(tsk);
 	exit_fs(tsk);
 	check_stack_usage();

commit d3ec4844d449cf7af9e749f73ba2052fb7b72fc2
Merge: 0003230e8200 df2e301fee3c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 13:56:39 2011 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (43 commits)
      fs: Merge split strings
      treewide: fix potentially dangerous trailing ';' in #defined values/expressions
      uwb: Fix misspelling of neighbourhood in comment
      net, netfilter: Remove redundant goto in ebt_ulog_packet
      trivial: don't touch files that are removed in the staging tree
      lib/vsprintf: replace link to Draft by final RFC number
      doc: Kconfig: `to be' -> `be'
      doc: Kconfig: Typo: square -> squared
      doc: Konfig: Documentation/power/{pm => apm-acpi}.txt
      drivers/net: static should be at beginning of declaration
      drivers/media: static should be at beginning of declaration
      drivers/i2c: static should be at beginning of declaration
      XTENSA: static should be at beginning of declaration
      SH: static should be at beginning of declaration
      MIPS: static should be at beginning of declaration
      ARM: static should be at beginning of declaration
      rcu: treewide: Do not use rcu_read_lock_held when calling rcu_dereference_check
      Update my e-mail address
      PCIe ASPM: forcedly -> forcibly
      gma500: push through device driver tree
      ...
    
    Fix up trivial conflicts:
     - arch/arm/mach-ep93xx/dma-m2p.c (deleted)
     - drivers/gpio/gpio-ep93xx.c (renamed and context nearby)
     - drivers/net/r8169.c (just context changes)

commit 096a705bbc080a4041636d07514560da8d78acbe
Merge: fea80311a939 5757a6d76cdf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 10:33:36 2011 -0700

    Merge branch 'for-3.1/core' of git://git.kernel.dk/linux-block
    
    * 'for-3.1/core' of git://git.kernel.dk/linux-block: (24 commits)
      block: strict rq_affinity
      backing-dev: use synchronize_rcu_expedited instead of synchronize_rcu
      block: fix patch import error in max_discard_sectors check
      block: reorder request_queue to remove 64 bit alignment padding
      CFQ: add think time check for group
      CFQ: add think time check for service tree
      CFQ: move think time check variables to a separate struct
      fixlet: Remove fs_excl from struct task.
      cfq: Remove special treatment for metadata rqs.
      block: document blk_plug list access
      block: avoid building too big plug list
      compat_ioctl: fix make headers_check regression
      block: eliminate potential for infinite loop in blkdev_issue_discard
      compat_ioctl: fix warning caused by qemu
      block: flush MEDIA_CHANGE from drivers on close(2)
      blk-throttle: Make total_nr_queued unsigned
      block: Add __attribute__((format(printf...) and fix fallout
      fs/partitions/check.c: make local symbols static
      block:remove some spare spaces in genhd.c
      block:fix the comment error in blkdev.h
      ...

commit 8209f53d79444747782a28520187abaf689761f2
Merge: 22a3b9771117 eac1b5e57d7a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 22 15:06:50 2011 -0700

    Merge branch 'ptrace' of git://git.kernel.org/pub/scm/linux/kernel/git/oleg/misc
    
    * 'ptrace' of git://git.kernel.org/pub/scm/linux/kernel/git/oleg/misc: (39 commits)
      ptrace: do_wait(traced_leader_killed_by_mt_exec) can block forever
      ptrace: fix ptrace_signal() && STOP_DEQUEUED interaction
      connector: add an event for monitoring process tracers
      ptrace: dont send SIGSTOP on auto-attach if PT_SEIZED
      ptrace: mv send-SIGSTOP from do_fork() to ptrace_init_task()
      ptrace_init_task: initialize child->jobctl explicitly
      has_stopped_jobs: s/task_is_stopped/SIGNAL_STOP_STOPPED/
      ptrace: make former thread ID available via PTRACE_GETEVENTMSG after PTRACE_EVENT_EXEC stop
      ptrace: wait_consider_task: s/same_thread_group/ptrace_reparented/
      ptrace: kill real_parent_is_ptracer() in in favor of ptrace_reparented()
      ptrace: ptrace_reparented() should check same_thread_group()
      redefine thread_group_leader() as exit_signal >= 0
      do not change dead_task->exit_signal
      kill task_detached()
      reparent_leader: check EXIT_DEAD instead of task_detached()
      make do_notify_parent() __must_check, update the callers
      __ptrace_detach: avoid task_detached(), check do_notify_parent()
      kill tracehook_notify_death()
      make do_notify_parent() return bool
      ptrace: s/tracehook_tracer_task()/ptrace_parent()/
      ...

commit 961c4675c75112717705fa5c0c53cb9664051479
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jul 7 21:33:54 2011 +0200

    has_stopped_jobs: s/task_is_stopped/SIGNAL_STOP_STOPPED/
    
    has_stopped_jobs() naively checks task_is_stopped(group_leader). This
    was always wrong even without ptrace, group_leader can be dead. And
    given that ptrace can change the state to TRACED this is wrong even
    in the single-threaded case.
    
    Change the code to check SIGNAL_STOP_STOPPED and simplify the code,
    retval + break/continue doesn't make this trivial code more readable.
    
    We could probably add the usual "|| signal->group_stop_count" check
    but I don't think this makes sense, the task can start the group-stop
    right after the check anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b8d3b47bb881..6c7fbbe7d86f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -266,18 +266,16 @@ int is_current_pgrp_orphaned(void)
 	return retval;
 }
 
-static int has_stopped_jobs(struct pid *pgrp)
+static bool has_stopped_jobs(struct pid *pgrp)
 {
-	int retval = 0;
 	struct task_struct *p;
 
 	do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
-		if (!task_is_stopped(p))
-			continue;
-		retval = 1;
-		break;
+		if (p->signal->flags & SIGNAL_STOP_STOPPED)
+			return true;
 	} while_each_pid_task(pgrp, PIDTYPE_PGID, p);
-	return retval;
+
+	return false;
 }
 
 /*

commit 4aede84b33d6beb401136a3deca0651ae07c5e99
Author: Justin TerAvest <teravest@google.com>
Date:   Tue Jul 12 08:31:45 2011 +0200

    fixlet: Remove fs_excl from struct task.
    
    fs_excl is a poor man's priority inheritance for filesystems to hint to
    the block layer that an operation is important. It was never clearly
    specified, not widely adopted, and will not prevent starvation in many
    cases (like across cgroups).
    
    fs_excl was introduced with the time sliced CFQ IO scheduler, to
    indicate when a process held FS exclusive resources and thus needed
    a boost.
    
    It doesn't cover all file systems, and it was never fully complete.
    Lets kill it.
    
    Signed-off-by: Justin TerAvest <teravest@google.com>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index f2b321bae440..b412df45ea6c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -906,7 +906,6 @@ NORET_TYPE void do_exit(long code)
 
 	profile_task_exit(tsk);
 
-	WARN_ON(atomic_read(&tsk->fs_excl));
 	WARN_ON(blk_needs_flush_plug(tsk));
 
 	if (unlikely(in_interrupt()))

commit b7e9c223be8ce335e30f2cf6ba588e6a4092275c
Merge: c172d82500a6 e3bbfa78bab1
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Jul 11 14:15:48 2011 +0200

    Merge branch 'master' into for-next
    
    Sync with Linus' tree to be able to apply pending patches that
    are based on newer code already present upstream.

commit d8bf4ca9ca9576548628344c9725edd3786e90b1
Author: Michal Hocko <mhocko@suse.cz>
Date:   Fri Jul 8 14:39:41 2011 +0200

    rcu: treewide: Do not use rcu_read_lock_held when calling rcu_dereference_check
    
    Since ca5ecddf (rcu: define __rcu address space modifier for sparse)
    rcu_dereference_check use rcu_read_lock_held as a part of condition
    automatically so callers do not have to do that as well.
    
    Signed-off-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/exit.c b/kernel/exit.c
index 20a406471525..07dc154fc799 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -85,7 +85,6 @@ static void __exit_signal(struct task_struct *tsk)
 	struct tty_struct *uninitialized_var(tty);
 
 	sighand = rcu_dereference_check(tsk->sighand,
-					rcu_read_lock_held() ||
 					lockdep_tasklist_lock_is_held());
 	spin_lock(&sighand->siglock);
 

commit 479bf98c1c29b40d86e40a4e6e4944c2f03d9493
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jun 24 17:34:39 2011 +0200

    ptrace: wait_consider_task: s/same_thread_group/ptrace_reparented/
    
    wait_consider_task() checks same_thread_group(parent, real_parent),
    this is the open-coded ptrace_reparented().
    
    __ptrace_detach() remains the only function which has to check this by
    hand, although we could reorganize the code to delay __ptrace_unlink.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9fa99702645d..b8d3b47bb881 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1599,8 +1599,7 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		 * own children, it should create a separate process which
 		 * takes the role of real parent.
 		 */
-		if (likely(!ptrace) && p->ptrace &&
-		    same_thread_group(p->parent, p->real_parent))
+		if (likely(!ptrace) && p->ptrace && !ptrace_reparented(p))
 			return 0;
 
 		/*

commit e550f14dc6322e794d4e70825f63c9c99177ae8b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 22 23:09:54 2011 +0200

    kill task_detached()
    
    Upadate the last user of task_detached(), wait_task_zombie(), to
    use thread_group_leader() and kill task_detached().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2b1ba8048a14..9fa99702645d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -189,7 +189,6 @@ void release_task(struct task_struct * p)
 	zap_leader = 0;
 	leader = p->group_leader;
 	if (leader != p && thread_group_empty(leader) && leader->exit_state == EXIT_ZOMBIE) {
-		BUG_ON(task_detached(leader));
 		/*
 		 * If we were the last child thread and the leader has
 		 * exited already, and the leader's parent ignores SIGCHLD,
@@ -1231,9 +1230,9 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	traced = ptrace_reparented(p);
 	/*
 	 * It can be ptraced but not reparented, check
-	 * !task_detached() to filter out sub-threads.
+	 * thread_group_leader() to filter out sub-threads.
 	 */
-	if (likely(!traced) && likely(!task_detached(p))) {
+	if (likely(!traced) && thread_group_leader(p)) {
 		struct signal_struct *psig;
 		struct signal_struct *sig;
 		unsigned long maxrss;

commit 0976a03e5ce8ec346e985f21046d7a75bb7fdffd
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 22 23:09:39 2011 +0200

    reparent_leader: check EXIT_DEAD instead of task_detached()
    
    Change reparent_leader() to check ->exit_state instead of ->exit_signal,
    this matches the similar EXIT_DEAD check in wait_consider_task() and
    allows us to cleanup the do_notify_parent/task_detached logic.
    
    task_detached() was really needed during reparenting before 9cd80bbb
    "do_wait() optimization: do not place sub-threads on ->children list"
    to filter out the sub-threads. After this change task_detached(p) can
    only be true if p is the dead group_leader and its parent ignores
    SIGCHLD, in this case the caller of do_notify_parent() is going to
    reap this task and it should set EXIT_DEAD.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f68d137ffeb4..2b1ba8048a14 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -742,7 +742,7 @@ static void reparent_leader(struct task_struct *father, struct task_struct *p,
 {
 	list_move_tail(&p->sibling, &p->real_parent->children);
 
-	if (task_detached(p))
+	if (p->exit_state == EXIT_DEAD)
 		return;
 	/*
 	 * If this is a threaded reparent there is no need to

commit 8677347378044ab564470bced2275520efb3670d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 22 23:09:09 2011 +0200

    make do_notify_parent() __must_check, update the callers
    
    Change other callers of do_notify_parent() to check the value it
    returns, this makes the subsequent task_detached() unnecessary.
    Mark do_notify_parent() as __must_check.
    
    Use thread_group_leader() instead of !task_detached() to check
    if we need to notify the real parent in wait_task_zombie().
    
    Remove the stale comment in release_task(). "just for sanity" is
    no longer true, we have to set EXIT_DEAD to avoid the races with
    do_wait().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index bb08e938ca74..f68d137ffeb4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -190,21 +190,12 @@ void release_task(struct task_struct * p)
 	leader = p->group_leader;
 	if (leader != p && thread_group_empty(leader) && leader->exit_state == EXIT_ZOMBIE) {
 		BUG_ON(task_detached(leader));
-		do_notify_parent(leader, leader->exit_signal);
 		/*
 		 * If we were the last child thread and the leader has
 		 * exited already, and the leader's parent ignores SIGCHLD,
 		 * then we are the one who should release the leader.
-		 *
-		 * do_notify_parent() will have marked it self-reaping in
-		 * that case.
-		 */
-		zap_leader = task_detached(leader);
-
-		/*
-		 * This maintains the invariant that release_task()
-		 * only runs on a task in EXIT_DEAD, just for sanity.
 		 */
+		zap_leader = do_notify_parent(leader, leader->exit_signal);
 		if (zap_leader)
 			leader->exit_state = EXIT_DEAD;
 	}
@@ -766,8 +757,7 @@ static void reparent_leader(struct task_struct *father, struct task_struct *p,
 	/* If it has exited notify the new parent about this child's death. */
 	if (!p->ptrace &&
 	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {
-		do_notify_parent(p, p->exit_signal);
-		if (task_detached(p)) {
+		if (do_notify_parent(p, p->exit_signal)) {
 			p->exit_state = EXIT_DEAD;
 			list_move_tail(&p->sibling, dead);
 		}
@@ -1351,16 +1341,13 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		/* We dropped tasklist, ptracer could die and untrace */
 		ptrace_unlink(p);
 		/*
-		 * If this is not a detached task, notify the parent.
-		 * If it's still not detached after that, don't release
-		 * it now.
+		 * If this is not a sub-thread, notify the parent.
+		 * If parent wants a zombie, don't release it now.
 		 */
-		if (!task_detached(p)) {
-			do_notify_parent(p, p->exit_signal);
-			if (!task_detached(p)) {
-				p->exit_state = EXIT_ZOMBIE;
-				p = NULL;
-			}
+		if (thread_group_leader(p) &&
+		    !do_notify_parent(p, p->exit_signal)) {
+			p->exit_state = EXIT_ZOMBIE;
+			p = NULL;
 		}
 		write_unlock_irq(&tasklist_lock);
 	}

commit 45cdf5cc0703c537194588c63d53bad1f2539d36
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jun 23 19:06:50 2011 +0200

    kill tracehook_notify_death()
    
    Kill tracehook_notify_death(), reimplement the logic in its caller,
    exit_notify().
    
    Also, change the exec_id's check to use thread_group_leader() instead
    of task_detached(), this is more clear. This logic only applies to
    the exiting leader, a sub-thread must never change its exit_signal.
    
    Note: when the traced group leader exits the exit_signal-or-SIGCHLD
    logic looks really strange:
    
            - we notify the tracer even if !thread_group_empty() but
               do_wait(WEXITED) can't work until all threads exit
    
            - if the tracer is real_parent, it is not clear why can't
              we use ->exit_signal event if !thread_group_empty()
    
    -v2: do not try to fix the 2nd oddity to avoid the subtle behavior
         change mixed with reorganization, suggested by Tejun.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 34d135f4fccc..bb08e938ca74 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -819,9 +819,7 @@ static void forget_original_parent(struct task_struct *father)
  */
 static void exit_notify(struct task_struct *tsk, int group_dead)
 {
-	int signal;
 	bool autoreap;
-	void *cookie;
 
 	/*
 	 * This does two things:
@@ -852,16 +850,23 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	 * we have changed execution domain as these two values started
 	 * the same after a fork.
 	 */
-	if (tsk->exit_signal != SIGCHLD && !task_detached(tsk) &&
+	if (thread_group_leader(tsk) && tsk->exit_signal != SIGCHLD &&
 	    (tsk->parent_exec_id != tsk->real_parent->self_exec_id ||
 	     tsk->self_exec_id != tsk->parent_exec_id))
 		tsk->exit_signal = SIGCHLD;
 
-	signal = tracehook_notify_death(tsk, &cookie, group_dead);
-	if (signal >= 0)
-		autoreap = do_notify_parent(tsk, signal);
-	else
-		autoreap = (signal == DEATH_REAP);
+	if (unlikely(tsk->ptrace)) {
+		int sig = thread_group_leader(tsk) &&
+				thread_group_empty(tsk) &&
+				!ptrace_reparented(tsk) ?
+			tsk->exit_signal : SIGCHLD;
+		autoreap = do_notify_parent(tsk, sig);
+	} else if (thread_group_leader(tsk)) {
+		autoreap = thread_group_empty(tsk) &&
+			do_notify_parent(tsk, tsk->exit_signal);
+	} else {
+		autoreap = true;
+	}
 
 	tsk->exit_state = autoreap ? EXIT_DEAD : EXIT_ZOMBIE;
 

commit 53c8f9f199b239668e6b1a907735ee323a0d1ccd
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 22 23:08:18 2011 +0200

    make do_notify_parent() return bool
    
    - change do_notify_parent() to return a boolean, true if the task should
      be reaped because its parent ignores SIGCHLD.
    
    - update the only caller which checks the returned value, exit_notify().
    
    This temporary uglifies exit_notify() even more, will be cleanuped by
    the next change.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index d49134a7f250..34d135f4fccc 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -820,6 +820,7 @@ static void forget_original_parent(struct task_struct *father)
 static void exit_notify(struct task_struct *tsk, int group_dead)
 {
 	int signal;
+	bool autoreap;
 	void *cookie;
 
 	/*
@@ -858,9 +859,11 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 
 	signal = tracehook_notify_death(tsk, &cookie, group_dead);
 	if (signal >= 0)
-		signal = do_notify_parent(tsk, signal);
+		autoreap = do_notify_parent(tsk, signal);
+	else
+		autoreap = (signal == DEATH_REAP);
 
-	tsk->exit_state = signal == DEATH_REAP ? EXIT_DEAD : EXIT_ZOMBIE;
+	tsk->exit_state = autoreap ? EXIT_DEAD : EXIT_ZOMBIE;
 
 	/* mt-exec, de_thread() is waiting for group leader */
 	if (unlikely(tsk->signal->notify_count < 0))
@@ -868,7 +871,7 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	write_unlock_irq(&tasklist_lock);
 
 	/* If the process is dead, release it - nobody will wait for it */
-	if (signal == DEATH_REAP)
+	if (autoreap)
 		release_task(tsk);
 }
 

commit a288eecce5253cc1565d400a52b9b476a157e040
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jun 17 16:50:37 2011 +0200

    ptrace: kill trivial tracehooks
    
    At this point, tracehooks aren't useful to mainline kernel and mostly
    just add an extra layer of obfuscation.  Although they have comments,
    without actual in-kernel users, it is difficult to tell what are their
    assumptions and they're actually trying to achieve.  To mainline
    kernel, they just aren't worth keeping around.
    
    This patch kills the following trivial tracehooks.
    
    * Ones testing whether task is ptraced.  Replace with ->ptrace test.
    
            tracehook_expect_breakpoints()
            tracehook_consider_ignored_signal()
            tracehook_consider_fatal_signal()
    
    * ptrace_event() wrappers.  Call directly.
    
            tracehook_report_exec()
            tracehook_report_exit()
            tracehook_report_vfork_done()
    
    * ptrace_release_task() wrapper.  Call directly.
    
            tracehook_finish_release_task()
    
    * noop
    
            tracehook_prepare_release_task()
            tracehook_report_death()
    
    This doesn't introduce any behavior change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index e5cc05644609..d49134a7f250 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -169,7 +169,6 @@ void release_task(struct task_struct * p)
 	struct task_struct *leader;
 	int zap_leader;
 repeat:
-	tracehook_prepare_release_task(p);
 	/* don't need to get the RCU readlock here - the process is dead and
 	 * can't be modifying its own credentials. But shut RCU-lockdep up */
 	rcu_read_lock();
@@ -179,7 +178,7 @@ void release_task(struct task_struct * p)
 	proc_flush_task(p);
 
 	write_lock_irq(&tasklist_lock);
-	tracehook_finish_release_task(p);
+	ptrace_release_task(p);
 	__exit_signal(p);
 
 	/*
@@ -868,8 +867,6 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 		wake_up_process(tsk->signal->group_exit_task);
 	write_unlock_irq(&tasklist_lock);
 
-	tracehook_report_death(tsk, signal, cookie, group_dead);
-
 	/* If the process is dead, release it - nobody will wait for it */
 	if (signal == DEATH_REAP)
 		release_task(tsk);
@@ -924,7 +921,7 @@ NORET_TYPE void do_exit(long code)
 	 */
 	set_fs(USER_DS);
 
-	tracehook_report_exit(&code);
+	ptrace_event(PTRACE_EVENT_EXIT, code);
 
 	validate_creds_for_do_exit(tsk);
 

commit d21142ece414ce1088cfcae760689aa60d6fee80
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Jun 17 16:50:34 2011 +0200

    ptrace: kill task_ptrace()
    
    task_ptrace(task) simply dereferences task->ptrace and isn't even used
    consistently only adding confusion.  Kill it and directly access
    ->ptrace instead.
    
    This doesn't introduce any behavior change.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 289f59d686bf..e5cc05644609 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -765,7 +765,7 @@ static void reparent_leader(struct task_struct *father, struct task_struct *p,
 	p->exit_signal = SIGCHLD;
 
 	/* If it has exited notify the new parent about this child's death. */
-	if (!task_ptrace(p) &&
+	if (!p->ptrace &&
 	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {
 		do_notify_parent(p, p->exit_signal);
 		if (task_detached(p)) {
@@ -795,7 +795,7 @@ static void forget_original_parent(struct task_struct *father)
 		do {
 			t->real_parent = reaper;
 			if (t->parent == father) {
-				BUG_ON(task_ptrace(t));
+				BUG_ON(t->ptrace);
 				t->parent = t->real_parent;
 			}
 			if (t->pdeath_signal)
@@ -1565,7 +1565,7 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		 * Notification and reaping will be cascaded to the real
 		 * parent when the ptracer detaches.
 		 */
-		if (likely(!ptrace) && unlikely(task_ptrace(p))) {
+		if (likely(!ptrace) && unlikely(p->ptrace)) {
 			/* it will become visible, clear notask_error */
 			wo->notask_error = 0;
 			return 0;
@@ -1608,7 +1608,7 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		 * own children, it should create a separate process which
 		 * takes the role of real parent.
 		 */
-		if (likely(!ptrace) && task_ptrace(p) &&
+		if (likely(!ptrace) && p->ptrace &&
 		    same_thread_group(p->parent, p->real_parent))
 			return 0;
 

commit 544b2c91a9f14f9565af1972203438b7f49afd48
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jun 14 11:20:18 2011 +0200

    ptrace: implement PTRACE_LISTEN
    
    The previous patch implemented async notification for ptrace but it
    only worked while trace is running.  This patch introduces
    PTRACE_LISTEN which is suggested by Oleg Nestrov.
    
    It's allowed iff tracee is in STOP trap and puts tracee into
    quasi-running state - tracee never really runs but wait(2) and
    ptrace(2) consider it to be running.  While ptracer is listening,
    tracee is allowed to re-enter STOP to notify an async event.
    Listening state is cleared on the first notification.  Ptracer can
    also clear it by issuing INTERRUPT - tracee will re-trap into STOP
    with listening state cleared.
    
    This allows ptracer to monitor group stop state without running tracee
    - use INTERRUPT to put tracee into STOP trap, issue LISTEN and then
    wait(2) to wait for the next group stop event.  When it happens,
    PTRACE_GETSIGINFO provides information to determine the current state.
    
    Test program follows.
    
      #define PTRACE_SEIZE          0x4206
      #define PTRACE_INTERRUPT      0x4207
      #define PTRACE_LISTEN         0x4208
    
      #define PTRACE_SEIZE_DEVEL    0x80000000
    
      static const struct timespec ts1s = { .tv_sec = 1 };
    
      int main(int argc, char **argv)
      {
              pid_t tracee, tracer;
              int i;
    
              tracee = fork();
              if (!tracee)
                      while (1)
                              pause();
    
              tracer = fork();
              if (!tracer) {
                      siginfo_t si;
    
                      ptrace(PTRACE_SEIZE, tracee, NULL,
                             (void *)(unsigned long)PTRACE_SEIZE_DEVEL);
                      ptrace(PTRACE_INTERRUPT, tracee, NULL, NULL);
              repeat:
                      waitid(P_PID, tracee, NULL, WSTOPPED);
    
                      ptrace(PTRACE_GETSIGINFO, tracee, NULL, &si);
                      if (!si.si_code) {
                              printf("tracer: SIG %d\n", si.si_signo);
                              ptrace(PTRACE_CONT, tracee, NULL,
                                     (void *)(unsigned long)si.si_signo);
                              goto repeat;
                      }
                      printf("tracer: stopped=%d signo=%d\n",
                             si.si_signo != SIGTRAP, si.si_signo);
                      if (si.si_signo != SIGTRAP)
                              ptrace(PTRACE_LISTEN, tracee, NULL, NULL);
                      else
                              ptrace(PTRACE_CONT, tracee, NULL, NULL);
                      goto repeat;
              }
    
              for (i = 0; i < 3; i++) {
                      nanosleep(&ts1s, NULL);
                      printf("mother: SIGSTOP\n");
                      kill(tracee, SIGSTOP);
                      nanosleep(&ts1s, NULL);
                      printf("mother: SIGCONT\n");
                      kill(tracee, SIGCONT);
              }
              nanosleep(&ts1s, NULL);
    
              kill(tracer, SIGKILL);
              kill(tracee, SIGKILL);
              return 0;
      }
    
    This is identical to the program to test TRAP_NOTIFY except that
    tracee is PTRACE_LISTEN'd instead of PTRACE_CONT'd when group stopped.
    This allows ptracer to monitor when group stop ends without running
    tracee.
    
      # ./test-listen
      tracer: stopped=0 signo=5
      mother: SIGSTOP
      tracer: SIG 19
      tracer: stopped=1 signo=19
      mother: SIGCONT
      tracer: stopped=0 signo=5
      tracer: SIG 18
      mother: SIGSTOP
      tracer: SIG 19
      tracer: stopped=1 signo=19
      mother: SIGCONT
      tracer: stopped=0 signo=5
      tracer: SIG 18
      mother: SIGSTOP
      tracer: SIG 19
      tracer: stopped=1 signo=19
      mother: SIGCONT
      tracer: stopped=0 signo=5
      tracer: SIG 18
    
    -v2: Moved JOBCTL_LISTENING check in wait_task_stopped() into
         task_stopped_code() as suggested by Oleg.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 20a406471525..289f59d686bf 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1368,7 +1368,8 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 static int *task_stopped_code(struct task_struct *p, bool ptrace)
 {
 	if (ptrace) {
-		if (task_is_stopped_or_traced(p))
+		if (task_is_stopped_or_traced(p) &&
+		    !(p->jobctl & JOBCTL_LISTENING))
 			return &p->exit_code;
 	} else {
 		if (p->signal->flags & SIGNAL_STOP_STOPPED)

commit 733eda7ac316cd4e550fa096e4ed42356dc546e7
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Wed Jun 15 15:08:43 2011 -0700

    memcg: clear mm->owner when last possible owner leaves
    
    The following crash was reported:
    
    > Call Trace:
    > [<ffffffff81139792>] mem_cgroup_from_task+0x15/0x17
    > [<ffffffff8113a75a>] __mem_cgroup_try_charge+0x148/0x4b4
    > [<ffffffff810493f3>] ? need_resched+0x23/0x2d
    > [<ffffffff814cbf43>] ? preempt_schedule+0x46/0x4f
    > [<ffffffff8113afe8>] mem_cgroup_charge_common+0x9a/0xce
    > [<ffffffff8113b6d1>] mem_cgroup_newpage_charge+0x5d/0x5f
    > [<ffffffff81134024>] khugepaged+0x5da/0xfaf
    > [<ffffffff81078ea0>] ? __init_waitqueue_head+0x4b/0x4b
    > [<ffffffff81133a4a>] ? add_mm_counter.constprop.5+0x13/0x13
    > [<ffffffff81078625>] kthread+0xa8/0xb0
    > [<ffffffff814d13e8>] ? sub_preempt_count+0xa1/0xb4
    > [<ffffffff814d5664>] kernel_thread_helper+0x4/0x10
    > [<ffffffff814ce858>] ? retint_restore_args+0x13/0x13
    > [<ffffffff8107857d>] ? __init_kthread_worker+0x5a/0x5a
    
    What happens is that khugepaged tries to charge a huge page against an mm
    whose last possible owner has already exited, and the memory controller
    crashes when the stale mm->owner is used to look up the cgroup to charge.
    
    mm->owner has never been set to NULL with the last owner going away, but
    nobody cared until khugepaged came along.
    
    Even then it wasn't a problem because the final mmput() on an mm was
    forced to acquire and release mmap_sem in write-mode, preventing an
    exiting owner to go away while the mmap_sem was held, and until "692e0b3
    mm: thp: optimize memcg charge in khugepaged", the memory cgroup charge
    was protected by mmap_sem in read-mode.
    
    Instead of going back to relying on the mmap_sem to enforce lifetime of a
    task, this patch ensures that mm->owner is properly set to NULL when the
    last possible owner is exiting, which the memory controller can handle
    just fine.
    
    [akpm@linux-foundation.org: tweak comments]
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reported-by: Hugh Dickins <hughd@google.com>
    Reported-by: Dave Jones <davej@redhat.com>
    Reviewed-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 20a406471525..f2b321bae440 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -561,29 +561,28 @@ void exit_files(struct task_struct *tsk)
 
 #ifdef CONFIG_MM_OWNER
 /*
- * Task p is exiting and it owned mm, lets find a new owner for it
+ * A task is exiting.   If it owned this mm, find a new owner for the mm.
  */
-static inline int
-mm_need_new_owner(struct mm_struct *mm, struct task_struct *p)
-{
-	/*
-	 * If there are other users of the mm and the owner (us) is exiting
-	 * we need to find a new owner to take on the responsibility.
-	 */
-	if (atomic_read(&mm->mm_users) <= 1)
-		return 0;
-	if (mm->owner != p)
-		return 0;
-	return 1;
-}
-
 void mm_update_next_owner(struct mm_struct *mm)
 {
 	struct task_struct *c, *g, *p = current;
 
 retry:
-	if (!mm_need_new_owner(mm, p))
+	/*
+	 * If the exiting or execing task is not the owner, it's
+	 * someone else's problem.
+	 */
+	if (mm->owner != p)
 		return;
+	/*
+	 * The current owner is exiting/execing and there are no other
+	 * candidates.  Do not leave the mm pointing to a possibly
+	 * freed task structure.
+	 */
+	if (atomic_read(&mm->mm_users) <= 1) {
+		mm->owner = NULL;
+		return;
+	}
 
 	read_lock(&tasklist_lock);
 	/*

commit 3ed4c0583daa34dedb568b26ff99e5a7b58db612
Merge: ad9471752eba bd715d9a4f13
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 20 13:33:21 2011 -0700

    Merge branch 'ptrace' of git://git.kernel.org/pub/scm/linux/kernel/git/oleg/misc
    
    * 'ptrace' of git://git.kernel.org/pub/scm/linux/kernel/git/oleg/misc: (41 commits)
      signal: trivial, fix the "timespec declared inside parameter list" warning
      job control: reorganize wait_task_stopped()
      ptrace: fix signal->wait_chldexit usage in task_clear_group_stop_trapping()
      signal: sys_sigprocmask() needs retarget_shared_pending()
      signal: cleanup sys_sigprocmask()
      signal: rename signandsets() to sigandnsets()
      signal: do_sigtimedwait() needs retarget_shared_pending()
      signal: introduce do_sigtimedwait() to factor out compat/native code
      signal: sys_rt_sigtimedwait: simplify the timeout logic
      signal: cleanup sys_rt_sigprocmask()
      x86: signal: sys_rt_sigreturn() should use set_current_blocked()
      x86: signal: handle_signal() should use set_current_blocked()
      signal: sigprocmask() should do retarget_shared_pending()
      signal: sigprocmask: narrow the scope of ->siglock
      signal: retarget_shared_pending: optimize while_each_thread() loop
      signal: retarget_shared_pending: consider shared/unblocked signals only
      signal: introduce retarget_shared_pending()
      ptrace: ptrace_check_attach() should not do s/STOPPED/TRACED/
      signal: Turn SIGNAL_STOP_DEQUEUED into GROUP_STOP_DEQUEUED
      signal: do_signal_stop: Remove the unneeded task_clear_group_stop_pending()
      ...

commit 19e274630c9e23a84d5940af83cf5db35103f968
Author: Tejun Heo <tj@kernel.org>
Date:   Thu May 12 10:47:23 2011 +0200

    job control: reorganize wait_task_stopped()
    
    wait_task_stopped() tested task_stopped_code() without acquiring
    siglock and, if stop condition existed, called wait_task_stopped() and
    directly returned the result.  This patch moves the initial
    task_stopped_code() testing into wait_task_stopped() and make
    wait_consider_task() fall through to wait_task_continue() on 0 return.
    
    This is for the following two reasons.
    
    * Because the initial task_stopped_code() test is done without
      acquiring siglock, it may race against SIGCONT generation.  The
      stopped condition might have been replaced by continued state by the
      time wait_task_stopped() acquired siglock.  This may lead to
      unexpected failure of WNOHANG waits.
    
      This reorganization addresses this single race case but there are
      other cases - TASK_RUNNING -> TASK_STOPPED transition and EXIT_*
      transitions.
    
    * Scheduled ptrace updates require changes to the initial test which
      would fit better inside wait_task_stopped().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5cbc83e83a5d..33837936b98c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1377,11 +1377,23 @@ static int *task_stopped_code(struct task_struct *p, bool ptrace)
 	return NULL;
 }
 
-/*
- * Handle sys_wait4 work for one task in state TASK_STOPPED.  We hold
- * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold
- * the lock and this task is uninteresting.  If we return nonzero, we have
- * released the lock and the system call should return.
+/**
+ * wait_task_stopped - Wait for %TASK_STOPPED or %TASK_TRACED
+ * @wo: wait options
+ * @ptrace: is the wait for ptrace
+ * @p: task to wait for
+ *
+ * Handle sys_wait4() work for %p in state %TASK_STOPPED or %TASK_TRACED.
+ *
+ * CONTEXT:
+ * read_lock(&tasklist_lock), which is released if return value is
+ * non-zero.  Also, grabs and releases @p->sighand->siglock.
+ *
+ * RETURNS:
+ * 0 if wait condition didn't exist and search for other wait conditions
+ * should continue.  Non-zero return, -errno on failure and @p's pid on
+ * success, implies that tasklist_lock is released and wait condition
+ * search should terminate.
  */
 static int wait_task_stopped(struct wait_opts *wo,
 				int ptrace, struct task_struct *p)
@@ -1397,6 +1409,9 @@ static int wait_task_stopped(struct wait_opts *wo,
 	if (!ptrace && !(wo->wo_flags & WUNTRACED))
 		return 0;
 
+	if (!task_stopped_code(p, ptrace))
+		return 0;
+
 	exit_code = 0;
 	spin_lock_irq(&p->sighand->siglock);
 
@@ -1607,8 +1622,9 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 	 * Wait for stopped.  Depending on @ptrace, different stopped state
 	 * is used and the two don't interact with each other.
 	 */
-	if (task_stopped_code(p, ptrace))
-		return wait_task_stopped(wo, ptrace, p);
+	ret = wait_task_stopped(wo, ptrace, p);
+	if (ret)
+		return ret;
 
 	/*
 	 * Wait for continued.  There's only one continued state and the

commit bf26c018490c2fce7fe9b629083b96ce0e6ad019
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Apr 7 16:53:20 2011 +0200

    ptrace: Prepare to fix racy accesses on task breakpoints
    
    When a task is traced and is in a stopped state, the tracer
    may execute a ptrace request to examine the tracee state and
    get its task struct. Right after, the tracee can be killed
    and thus its breakpoints released.
    This can happen concurrently when the tracer is in the middle
    of reading or modifying these breakpoints, leading to dereferencing
    a freed pointer.
    
    Hence, to prepare the fix, create a generic breakpoint reference
    holding API. When a reference on the breakpoints of a task is
    held, the breakpoints won't be released until the last reference
    is dropped. After that, no more ptrace request on the task's
    breakpoints can be serviced for the tracer.
    
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: v2.6.33.. <stable@kernel.org>
    Link: http://lkml.kernel.org/r/1302284067-7860-2-git-send-email-fweisbec@gmail.com

diff --git a/kernel/exit.c b/kernel/exit.c
index f5d2f63bae0b..8dd874181542 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1016,7 +1016,7 @@ NORET_TYPE void do_exit(long code)
 	/*
 	 * FIXME: do that only when needed, using sched_exit tracepoint
 	 */
-	flush_ptrace_hw_breakpoint(tsk);
+	ptrace_put_breakpoints(tsk);
 
 	exit_notify(tsk, group_dead);
 #ifdef CONFIG_NUMA

commit e46bc9b6fd65bc9f406a4211fbf95683cc9c2937
Merge: 2b9accbee563 321fb561971b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 7 20:44:11 2011 +0200

    Merge branch 'ptrace' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/misc into ptrace

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6a488ad2dce5..f5d2f63bae0b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -841,7 +841,7 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	/* Let father know we died
 	 *
 	 * Thread signals are configurable, but you aren't going to use
-	 * that to send signals to arbitary processes.
+	 * that to send signals to arbitrary processes.
 	 * That stops right now.
 	 *
 	 * If the parent exec id doesn't match the exec id we saved

commit 45cb24a1da53beb70f09efccc0373f6a47a9efe0
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 23 10:37:01 2011 +0100

    job control: Allow access to job control events through ptracees
    
    Currently a real parent can't access job control stopped/continued
    events through a ptraced child.  This utterly breaks job control when
    the children are ptraced.
    
    For example, if a program is run from an interactive shell and then
    strace(1) attaches to it, pressing ^Z would send SIGTSTP and strace(1)
    would notice it but the shell has no way to tell whether the child
    entered job control stop and thus can't tell when to take over the
    terminal - leading to awkward lone ^Z on the terminal.
    
    Because the job control and ptrace stopped states are independent,
    there is no reason to prevent real parents from accessing the stopped
    state regardless of ptrace.  The continued state isn't separate but
    ptracers don't have any use for them as ptracees can never resume
    without explicit command from their ptracers, so as long as ptracers
    don't consume it, it should be fine.
    
    Although this is a behavior change, because the previous behavior is
    utterly broken when viewed from real parents and the change is only
    visible to real parents, I don't think it's necessary to make this
    behavior optional.
    
    One situation to be careful about is when a task from the real
    parent's group is ptracing.  The parent group is the recipient of both
    ptrace and job control stop events and one stop can be reported as
    both job control and ptrace stops.  As this can break the current
    ptrace users, suppress job control stopped events for these cases.
    
    If a real parent ptracer wants to know about both job control and
    ptrace stops, it can create a separate process to serve the role of
    real parent.
    
    Note that this only updates wait(2) side of things.  The real parent
    can access the states via wait(2) but still is not properly notified
    (woken up and delivered signal).  Test case polls wait(2) with WNOHANG
    to work around.  Notification will be updated by future patches.
    
    Test case follows.
    
      #include <stdio.h>
      #include <unistd.h>
      #include <time.h>
      #include <errno.h>
      #include <sys/types.h>
      #include <sys/ptrace.h>
      #include <sys/wait.h>
    
      int main(void)
      {
              const struct timespec ts100ms = { .tv_nsec = 100000000 };
              pid_t tracee, tracer;
              siginfo_t si;
              int i;
    
              tracee = fork();
              if (tracee == 0) {
                      while (1) {
                              printf("tracee: SIGSTOP\n");
                              raise(SIGSTOP);
                              nanosleep(&ts100ms, NULL);
                              printf("tracee: SIGCONT\n");
                              raise(SIGCONT);
                              nanosleep(&ts100ms, NULL);
                      }
              }
    
              waitid(P_PID, tracee, &si, WSTOPPED | WNOHANG | WNOWAIT);
    
              tracer = fork();
              if (tracer == 0) {
                      nanosleep(&ts100ms, NULL);
                      ptrace(PTRACE_ATTACH, tracee, NULL, NULL);
    
                      for (i = 0; i < 11; i++) {
                              si.si_pid = 0;
                              waitid(P_PID, tracee, &si, WSTOPPED);
                              if (si.si_pid && si.si_code == CLD_TRAPPED)
                                      ptrace(PTRACE_CONT, tracee, NULL,
                                             (void *)(long)si.si_status);
                      }
                      printf("tracer: EXITING\n");
                      return 0;
              }
    
              while (1) {
                      si.si_pid = 0;
                      waitid(P_PID, tracee, &si,
                             WSTOPPED | WCONTINUED | WEXITED | WNOHANG);
                      if (si.si_pid)
                              printf("mommy : WAIT status=%02d code=%02d\n",
                                     si.si_status, si.si_code);
                      nanosleep(&ts100ms, NULL);
              }
              return 0;
      }
    
    Before the patch, while ptraced, the parent can't see any job control
    events.
    
      tracee: SIGSTOP
      mommy : WAIT status=19 code=05
      tracee: SIGCONT
      tracee: SIGSTOP
      tracee: SIGCONT
      tracee: SIGSTOP
      tracee: SIGCONT
      tracee: SIGSTOP
      tracer: EXITING
      mommy : WAIT status=19 code=05
      ^C
    
    After the patch,
    
      tracee: SIGSTOP
      mommy : WAIT status=19 code=05
      tracee: SIGCONT
      mommy : WAIT status=18 code=06
      tracee: SIGSTOP
      mommy : WAIT status=19 code=05
      tracee: SIGCONT
      mommy : WAIT status=18 code=06
      tracee: SIGSTOP
      mommy : WAIT status=19 code=05
      tracee: SIGCONT
      mommy : WAIT status=18 code=06
      tracee: SIGSTOP
      tracer: EXITING
      mommy : WAIT status=19 code=05
      ^C
    
    -v2: Oleg pointed out that wait(2) should be suppressed for the real
         parent's group instead of only the real parent task itself.
         Updated accordingly.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 84d13d6bb30b..1a0f10f0a4db 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1541,17 +1541,19 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 	if (p->exit_state == EXIT_DEAD)
 		return 0;
 
-	if (likely(!ptrace) && unlikely(task_ptrace(p))) {
+	/* slay zombie? */
+	if (p->exit_state == EXIT_ZOMBIE) {
 		/*
-		 * This child is hidden by ptrace.
-		 * We aren't allowed to see it now, but eventually we will.
+		 * A zombie ptracee is only visible to its ptracer.
+		 * Notification and reaping will be cascaded to the real
+		 * parent when the ptracer detaches.
 		 */
-		wo->notask_error = 0;
-		return 0;
-	}
+		if (likely(!ptrace) && unlikely(task_ptrace(p))) {
+			/* it will become visible, clear notask_error */
+			wo->notask_error = 0;
+			return 0;
+		}
 
-	/* slay zombie? */
-	if (p->exit_state == EXIT_ZOMBIE) {
 		/* we don't reap group leaders with subthreads */
 		if (!delay_group_leader(p))
 			return wait_task_zombie(wo, p);
@@ -1579,6 +1581,20 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		if (likely(!ptrace) || (wo->wo_flags & (WCONTINUED | WEXITED)))
 			wo->notask_error = 0;
 	} else {
+		/*
+		 * If @p is ptraced by a task in its real parent's group,
+		 * hide group stop/continued state when looking at @p as
+		 * the real parent; otherwise, a single stop can be
+		 * reported twice as group and ptrace stops.
+		 *
+		 * If a ptracer wants to distinguish the two events for its
+		 * own children, it should create a separate process which
+		 * takes the role of real parent.
+		 */
+		if (likely(!ptrace) && task_ptrace(p) &&
+		    same_thread_group(p->parent, p->real_parent))
+			return 0;
+
 		/*
 		 * @p is alive and it's gonna stop, continue or exit, so
 		 * there always is something to wait for.
@@ -1586,9 +1602,18 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		wo->notask_error = 0;
 	}
 
+	/*
+	 * Wait for stopped.  Depending on @ptrace, different stopped state
+	 * is used and the two don't interact with each other.
+	 */
 	if (task_stopped_code(p, ptrace))
 		return wait_task_stopped(wo, ptrace, p);
 
+	/*
+	 * Wait for continued.  There's only one continued state and the
+	 * ptracer can consume it which can confuse the real parent.  Don't
+	 * use WCONTINUED from ptracer.  You don't need or want it.
+	 */
 	return wait_task_continued(wo, p);
 }
 

commit 9b84cca2564b9a5b2d064fb44d2a55a5b44473a0
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 23 10:37:01 2011 +0100

    job control: Fix ptracer wait(2) hang and explain notask_error clearing
    
    wait(2) and friends allow access to stopped/continued states through
    zombies, which is required as the states are process-wide and should
    be accessible whether the leader task is alive or undead.
    wait_consider_task() implements this by always clearing notask_error
    and going through wait_task_stopped/continued() for unreaped zombies.
    
    However, while ptraced, the stopped state is per-task and as such if
    the ptracee became a zombie, there's no further stopped event to
    listen to and wait(2) and friends should return -ECHILD on the tracee.
    
    Fix it by clearing notask_error only if WCONTINUED | WEXITED is set
    for ptraced zombies.  While at it, document why clearing notask_error
    is safe for each case.
    
    Test case follows.
    
      #include <stdio.h>
      #include <unistd.h>
      #include <pthread.h>
      #include <time.h>
      #include <sys/types.h>
      #include <sys/ptrace.h>
      #include <sys/wait.h>
    
      static void *nooper(void *arg)
      {
              pause();
              return NULL;
      }
    
      int main(void)
      {
              const struct timespec ts1s = { .tv_sec = 1 };
              pid_t tracee, tracer;
              siginfo_t si;
    
              tracee = fork();
              if (tracee == 0) {
                      pthread_t thr;
    
                      pthread_create(&thr, NULL, nooper, NULL);
                      nanosleep(&ts1s, NULL);
                      printf("tracee exiting\n");
                      pthread_exit(NULL);   /* let subthread run */
              }
    
              tracer = fork();
              if (tracer == 0) {
                      ptrace(PTRACE_ATTACH, tracee, NULL, NULL);
                      while (1) {
                              if (waitid(P_PID, tracee, &si, WSTOPPED) < 0) {
                                      perror("waitid");
                                      break;
                              }
                              ptrace(PTRACE_CONT, tracee, NULL,
                                     (void *)(long)si.si_status);
                      }
                      return 0;
              }
    
              waitid(P_PID, tracer, &si, WEXITED);
              kill(tracee, SIGKILL);
              return 0;
      }
    
    Before the patch, after the tracee becomes a zombie, the tracer's
    waitid(WSTOPPED) never returns and the program doesn't terminate.
    
      tracee exiting
      ^C
    
    After the patch, tracee exiting triggers waitid() to fail.
    
      tracee exiting
      waitid: No child processes
    
    -v2: Oleg pointed out that exited in addition to continued can happen
         for ptraced dead group leader.  Clear notask_error for ptraced
         child on WEXITED too.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index b4a935c72159..84d13d6bb30b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1550,17 +1550,41 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		return 0;
 	}
 
-	/*
-	 * We don't reap group leaders with subthreads.
-	 */
-	if (p->exit_state == EXIT_ZOMBIE && !delay_group_leader(p))
-		return wait_task_zombie(wo, p);
+	/* slay zombie? */
+	if (p->exit_state == EXIT_ZOMBIE) {
+		/* we don't reap group leaders with subthreads */
+		if (!delay_group_leader(p))
+			return wait_task_zombie(wo, p);
 
-	/*
-	 * It's stopped or running now, so it might
-	 * later continue, exit, or stop again.
-	 */
-	wo->notask_error = 0;
+		/*
+		 * Allow access to stopped/continued state via zombie by
+		 * falling through.  Clearing of notask_error is complex.
+		 *
+		 * When !@ptrace:
+		 *
+		 * If WEXITED is set, notask_error should naturally be
+		 * cleared.  If not, subset of WSTOPPED|WCONTINUED is set,
+		 * so, if there are live subthreads, there are events to
+		 * wait for.  If all subthreads are dead, it's still safe
+		 * to clear - this function will be called again in finite
+		 * amount time once all the subthreads are released and
+		 * will then return without clearing.
+		 *
+		 * When @ptrace:
+		 *
+		 * Stopped state is per-task and thus can't change once the
+		 * target task dies.  Only continued and exited can happen.
+		 * Clear notask_error if WCONTINUED | WEXITED.
+		 */
+		if (likely(!ptrace) || (wo->wo_flags & (WCONTINUED | WEXITED)))
+			wo->notask_error = 0;
+	} else {
+		/*
+		 * @p is alive and it's gonna stop, continue or exit, so
+		 * there always is something to wait for.
+		 */
+		wo->notask_error = 0;
+	}
 
 	if (task_stopped_code(p, ptrace))
 		return wait_task_stopped(wo, ptrace, p);

commit 823b018e5b1196d810790559357447948f644548
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 23 10:37:01 2011 +0100

    job control: Small reorganization of wait_consider_task()
    
    Move EXIT_DEAD test in wait_consider_task() above ptrace check.  As
    ptraced tasks can't be EXIT_DEAD, this change doesn't cause any
    behavior change.  This is to prepare for further changes.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index f9a45ebcc7b1..b4a935c72159 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1537,6 +1537,10 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		return 0;
 	}
 
+	/* dead body doesn't have much to contribute */
+	if (p->exit_state == EXIT_DEAD)
+		return 0;
+
 	if (likely(!ptrace) && unlikely(task_ptrace(p))) {
 		/*
 		 * This child is hidden by ptrace.
@@ -1546,9 +1550,6 @@ static int wait_consider_task(struct wait_opts *wo, int ptrace,
 		return 0;
 	}
 
-	if (p->exit_state == EXIT_DEAD)
-		return 0;
-
 	/*
 	 * We don't reap group leaders with subthreads.
 	 */

commit 73c101011926c5832e6e141682180c4debe2cf45
Author: Jens Axboe <jaxboe@fusionio.com>
Date:   Tue Mar 8 13:19:51 2011 +0100

    block: initial patch for on-stack per-task plugging
    
    This patch adds support for creating a queuing context outside
    of the queue itself. This enables us to batch up pieces of IO
    before grabbing the block device queue lock and submitting them to
    the IO scheduler.
    
    The context is created on the stack of the process and assigned in
    the task structure, so that we can auto-unplug it if we hit a schedule
    event.
    
    The current queue plugging happens implicitly if IO is submitted to
    an empty device, yet callers have to remember to unplug that IO when
    they are going to wait for it. This is an ugly API and has caused bugs
    in the past. Additionally, it requires hacks in the vm (->sync_page()
    callback) to handle that logic. By switching to an explicit plugging
    scheme we make the API a lot nicer and can get rid of the ->sync_page()
    hack in the vm.
    
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index f9a45ebcc7b1..6a488ad2dce5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -908,6 +908,7 @@ NORET_TYPE void do_exit(long code)
 	profile_task_exit(tsk);
 
 	WARN_ON(atomic_read(&tsk->fs_excl));
+	WARN_ON(blk_needs_flush_plug(tsk));
 
 	if (unlikely(in_interrupt()))
 		panic("Aiee, killing interrupt handler!");

commit 42776163e13a56ea3096edff7a5df95408e80eb4
Merge: edb2877f4a62 3d03e2ea7410
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 11 11:02:13 2011 -0800

    Merge branch 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (28 commits)
      perf session: Fix infinite loop in __perf_session__process_events
      perf evsel: Support perf_evsel__open(cpus > 1 && threads > 1)
      perf sched: Use PTHREAD_STACK_MIN to avoid pthread_attr_setstacksize() fail
      perf tools: Emit clearer message for sys_perf_event_open ENOENT return
      perf stat: better error message for unsupported events
      perf sched: Fix allocation result check
      perf, x86: P4 PMU - Fix unflagged overflows handling
      dynamic debug: Fix build issue with older gcc
      tracing: Fix TRACE_EVENT power tracepoint creation
      tracing: Fix preempt count leak
      tracepoint: Add __rcu annotation
      tracing: remove duplicate null-pointer check in skb tracepoint
      tracing/trivial: Add missing comma in TRACE_EVENT comment
      tracing: Include module.h in define_trace.h
      x86: Save rbp in pt_regs on irq entry
      x86, dumpstack: Fix unused variable warning
      x86, NMI: Clean-up default_do_nmi()
      x86, NMI: Allow NMI reason io port (0x61) to be processed on any CPU
      x86, NMI: Remove DIE_NMI_IPI
      x86, NMI: Add priorities to handlers
      ...

commit 0b3fcf178deefd7b64154c2c0760a2c63df0b74f
Author: Stephane Eranian <eranian@google.com>
Date:   Mon Jan 3 18:20:01 2011 +0200

    perf_events: Move code around to prepare for cgroup
    
    In particular this patch move perf_event_exit_task() before
    cgroup_exit() to allow for cgroup support. The cgroup_exit()
    function detaches the cgroups attached to a task.
    
    Other movements include hoisting some definitions and inlines
    at the top of perf_event.c
    
    Signed-off-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4d22058b.cdace30a.4657.ffff95b1@mx.google.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 676149a4ac5f..8cb89045ecf3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -994,6 +994,15 @@ NORET_TYPE void do_exit(long code)
 	exit_fs(tsk);
 	check_stack_usage();
 	exit_thread();
+
+	/*
+	 * Flush inherited counters to the parent - before the parent
+	 * gets woken up by child-exit notifications.
+	 *
+	 * because of cgroup mode, must be called before cgroup_exit()
+	 */
+	perf_event_exit_task(tsk);
+
 	cgroup_exit(tsk, 1);
 
 	if (group_dead)
@@ -1007,11 +1016,6 @@ NORET_TYPE void do_exit(long code)
 	 * FIXME: do that only when needed, using sched_exit tracepoint
 	 */
 	flush_ptrace_hw_breakpoint(tsk);
-	/*
-	 * Flush inherited counters to the parent - before the parent
-	 * gets woken up by child-exit notifications.
-	 */
-	perf_event_exit_task(tsk);
 
 	exit_notify(tsk, group_dead);
 #ifdef CONFIG_NUMA

commit 909ea96468096b07fbb41aaf69be060d92bd9271
Author: Christoph Lameter <cl@linux.com>
Date:   Wed Dec 8 16:22:55 2010 +0100

    core: Replace __get_cpu_var with __this_cpu_read if not used for an address.
    
    __get_cpu_var() can be replaced with this_cpu_read and will then use a
    single read instruction with implied address calculation to access the
    correct per cpu instance.
    
    However, the address of a per cpu variable passed to __this_cpu_read()
    cannot be determined (since it's an implied address conversion through
    segment prefixes).  Therefore apply this only to uses of __get_cpu_var
    where the address of the variable is not used.
    
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 676149a4ac5f..89c74861a3da 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -69,7 +69,7 @@ static void __unhash_process(struct task_struct *p, bool group_dead)
 
 		list_del_rcu(&p->tasks);
 		list_del_init(&p->sibling);
-		__get_cpu_var(process_counts)--;
+		__this_cpu_dec(process_counts);
 	}
 	list_del_rcu(&p->thread_group);
 }

commit 33dd94ae1ccbfb7bf0fb6c692bc3d1c4269e6177
Author: Nelson Elhage <nelhage@ksplice.com>
Date:   Thu Dec 2 14:31:21 2010 -0800

    do_exit(): make sure that we run with get_fs() == USER_DS
    
    If a user manages to trigger an oops with fs set to KERNEL_DS, fs is not
    otherwise reset before do_exit().  do_exit may later (via mm_release in
    fork.c) do a put_user to a user-controlled address, potentially allowing
    a user to leverage an oops into a controlled write into kernel memory.
    
    This is only triggerable in the presence of another bug, but this
    potentially turns a lot of DoS bugs into privilege escalations, so it's
    worth fixing.  I have proof-of-concept code which uses this bug along
    with CVE-2010-3849 to write a zero to an arbitrary kernel address, so
    I've tested that this is not theoretical.
    
    A more logical place to put this fix might be when we know an oops has
    occurred, before we call do_exit(), but that would involve changing
    every architecture, in multiple places.
    
    Let's just stick it in do_exit instead.
    
    [akpm@linux-foundation.org: update code comment]
    Signed-off-by: Nelson Elhage <nelhage@ksplice.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 21aa7b3001fb..676149a4ac5f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -914,6 +914,15 @@ NORET_TYPE void do_exit(long code)
 	if (unlikely(!tsk->pid))
 		panic("Attempted to kill the idle task!");
 
+	/*
+	 * If do_exit is called because this processes oopsed, it's possible
+	 * that get_fs() was left as KERNEL_DS, so reset it to USER_DS before
+	 * continuing. Amongst other possible reasons, this is to prevent
+	 * mm_release()->clear_child_tid() from writing to a user-controlled
+	 * kernel address.
+	 */
+	set_fs(USER_DS);
+
 	tracehook_report_exit(&code);
 
 	validate_creds_for_do_exit(tsk);

commit e0a70217107e6f9844628120412cb27bb4cea194
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Nov 5 16:53:42 2010 +0100

    posix-cpu-timers: workaround to suppress the problems with mt exec
    
    posix-cpu-timers.c correctly assumes that the dying process does
    posix_cpu_timers_exit_group() and removes all !CPUCLOCK_PERTHREAD
    timers from signal->cpu_timers list.
    
    But, it also assumes that timer->it.cpu.task is always the group
    leader, and thus the dead ->task means the dead thread group.
    
    This is obviously not true after de_thread() changes the leader.
    After that almost every posix_cpu_timer_ method has problems.
    
    It is not simple to fix this bug correctly. First of all, I think
    that timer->it.cpu should use struct pid instead of task_struct.
    Also, the locking should be reworked completely. In particular,
    tasklist_lock should not be used at all. This all needs a lot of
    nontrivial and hard-to-test changes.
    
    Change __exit_signal() to do posix_cpu_timers_exit_group() when
    the old leader dies during exec. This is not the fix, just the
    temporary hack to hide the problem for 2.6.37 and stable. IOW,
    this is obviously wrong but this is what we currently have anyway:
    cpu timers do not work after mt exec.
    
    In theory this change adds another race. The exiting leader can
    detach the timers which were attached to the new leader. However,
    the window between de_thread() and release_task() is small, we
    can pretend that sys_timer_create() was called before de_thread().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b194febf5799..21aa7b3001fb 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -95,6 +95,14 @@ static void __exit_signal(struct task_struct *tsk)
 		tty = sig->tty;
 		sig->tty = NULL;
 	} else {
+		/*
+		 * This can only happen if the caller is de_thread().
+		 * FIXME: this is the temporary hack, we should teach
+		 * posix-cpu-timers to handle this case correctly.
+		 */
+		if (unlikely(has_group_leader_pid(tsk)))
+			posix_cpu_timers_exit_group(tsk);
+
 		/*
 		 * If there is any task waiting for the group exit
 		 * then notify it:

commit d16e15f5b029fc7d03540ba0e5fb23b0abb0ebe0
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Wed Oct 27 15:34:10 2010 -0700

    exit: add lock context annotation on find_new_reaper()
    
    find_new_reaper() releases and regrabs tasklist_lock but was missing
    proper annotations.  Add it.  This remove following sparse warning:
    
     warning: context imbalance in 'find_new_reaper' - unexpected unlock
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 894179a32ec1..b194febf5799 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -703,6 +703,8 @@ static void exit_mm(struct task_struct * tsk)
  * space.
  */
 static struct task_struct *find_new_reaper(struct task_struct *father)
+	__releases(&tasklist_lock)
+	__acquires(&tasklist_lock)
 {
 	struct pid_namespace *pid_ns = task_active_pid_ns(father);
 	struct task_struct *thread;

commit 3d5992d2ac7dc09aed8ab537cba074589f0f0a52
Author: Ying Han <yinghan@google.com>
Date:   Tue Oct 26 14:21:23 2010 -0700

    oom: add per-mm oom disable count
    
    It's pointless to kill a task if another thread sharing its mm cannot be
    killed to allow future memory freeing.  A subsequent patch will prevent
    kills in such cases, but first it's necessary to have a way to flag a task
    that shares memory with an OOM_DISABLE task that doesn't incur an
    additional tasklist scan, which would make select_bad_process() an O(n^2)
    function.
    
    This patch adds an atomic counter to struct mm_struct that follows how
    many threads attached to it have an oom_score_adj of OOM_SCORE_ADJ_MIN.
    They cannot be killed by the kernel, so their memory cannot be freed in
    oom conditions.
    
    This only requires task_lock() on the task that we're operating on, it
    does not require mm->mmap_sem since task_lock() pins the mm and the
    operation is atomic.
    
    [rientjes@google.com: changelog and sys_unshare() code]
    [rientjes@google.com: protect oom_disable_count with task_lock in fork]
    [rientjes@google.com: use old_mm for oom_disable_count in exec]
    Signed-off-by: Ying Han <yinghan@google.com>
    Signed-off-by: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e2bdf37f9fde..894179a32ec1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -50,6 +50,7 @@
 #include <linux/perf_event.h>
 #include <trace/events/sched.h>
 #include <linux/hw_breakpoint.h>
+#include <linux/oom.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -687,6 +688,8 @@ static void exit_mm(struct task_struct * tsk)
 	enter_lazy_tlb(mm, current);
 	/* We don't want this task to be frozen prematurely */
 	clear_freeze_flag(tsk);
+	if (tsk->signal->oom_score_adj == OOM_SCORE_ADJ_MIN)
+		atomic_dec(&mm->oom_disable_count);
 	task_unlock(tsk);
 	mm_update_next_owner(mm);
 	mmput(mm);

commit 4e231c7962ce711c7d8c2a4dc23ecd1e8fc28363
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Sep 9 21:01:59 2010 +0200

    perf: Fix up delayed_put_task_struct()
    
    I missed a perf_event_ctxp user when converting it to an array. Pull this
    last user into perf_event.c as well and fix it up.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 03120229db28..e2bdf37f9fde 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -149,9 +149,7 @@ static void delayed_put_task_struct(struct rcu_head *rhp)
 {
 	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
 
-#ifdef CONFIG_PERF_EVENTS
-	WARN_ON_ONCE(tsk->perf_event_ctxp);
-#endif
+	perf_event_delayed_put(tsk);
 	trace_sched_process_free(tsk);
 	put_task_struct(tsk);
 }

commit f362b73244fb16ea4ae127ced1467dd8adaa7733
Author: Daniel J Blueman <daniel.blueman@gmail.com>
Date:   Tue Aug 17 23:56:55 2010 +0100

    Fix unprotected access to task credentials in waitid()
    
    Using a program like the following:
    
            #include <stdlib.h>
            #include <unistd.h>
            #include <sys/types.h>
            #include <sys/wait.h>
    
            int main() {
                    id_t id;
                    siginfo_t infop;
                    pid_t res;
    
                    id = fork();
                    if (id == 0) { sleep(1); exit(0); }
                    kill(id, SIGSTOP);
                    alarm(1);
                    waitid(P_PID, id, &infop, WCONTINUED);
                    return 0;
            }
    
    to call waitid() on a stopped process results in access to the child task's
    credentials without the RCU read lock being held - which may be replaced in the
    meantime - eliciting the following warning:
    
            ===================================================
            [ INFO: suspicious rcu_dereference_check() usage. ]
            ---------------------------------------------------
            kernel/exit.c:1460 invoked rcu_dereference_check() without protection!
    
            other info that might help us debug this:
    
            rcu_scheduler_active = 1, debug_locks = 1
            2 locks held by waitid02/22252:
             #0:  (tasklist_lock){.?.?..}, at: [<ffffffff81061ce5>] do_wait+0xc5/0x310
             #1:  (&(&sighand->siglock)->rlock){-.-...}, at: [<ffffffff810611da>]
            wait_consider_task+0x19a/0xbe0
    
            stack backtrace:
            Pid: 22252, comm: waitid02 Not tainted 2.6.35-323cd+ #3
            Call Trace:
             [<ffffffff81095da4>] lockdep_rcu_dereference+0xa4/0xc0
             [<ffffffff81061b31>] wait_consider_task+0xaf1/0xbe0
             [<ffffffff81061d15>] do_wait+0xf5/0x310
             [<ffffffff810620b6>] sys_waitid+0x86/0x1f0
             [<ffffffff8105fce0>] ? child_wait_callback+0x0/0x70
             [<ffffffff81003282>] system_call_fastpath+0x16/0x1b
    
    This is fixed by holding the RCU read lock in wait_task_continued() to ensure
    that the task's current credentials aren't destroyed between us reading the
    cred pointer and us reading the UID from those credentials.
    
    Furthermore, protect wait_task_stopped() in the same way.
    
    We don't need to keep holding the RCU read lock once we've read the UID from
    the credentials as holding the RCU read lock doesn't stop the target task from
    changing its creds under us - so the credentials may be outdated immediately
    after we've read the pointer, lock or no lock.
    
    Signed-off-by: Daniel J Blueman <daniel.blueman@gmail.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 671ed56e0a49..03120229db28 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1386,8 +1386,7 @@ static int wait_task_stopped(struct wait_opts *wo,
 	if (!unlikely(wo->wo_flags & WNOWAIT))
 		*p_code = 0;
 
-	/* don't need the RCU readlock here as we're holding a spinlock */
-	uid = __task_cred(p)->uid;
+	uid = task_uid(p);
 unlock_sig:
 	spin_unlock_irq(&p->sighand->siglock);
 	if (!exit_code)
@@ -1460,7 +1459,7 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 	}
 	if (!unlikely(wo->wo_flags & WNOWAIT))
 		p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
-	uid = __task_cred(p)->uid;
+	uid = task_uid(p);
 	spin_unlock_irq(&p->sighand->siglock);
 
 	pid = task_pid_vnr(p);

commit c7e49c1488ab20342eaaf38f1ca35a207f4c051d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Aug 10 18:03:07 2010 -0700

    ptrace: optimize exit_ptrace() for the likely case
    
    exit_ptrace() takes tasklist_lock unconditionally.  We need this lock to
    avoid the race with ptrace_traceme(), it acts as a barrier.
    
    Change its caller, forget_original_parent(), to call exit_ptrace() under
    tasklist_lock.  Change exit_ptrace() to drop and reacquire this lock if
    needed.
    
    This allows us to add the fastpath list_empty(ptraced) check.  In the
    likely no-tracees case exit_ptrace() just returns and we avoid the lock()
    + unlock() sequence.
    
    "Zhang, Yanmin" <yanmin_zhang@linux.intel.com> suggested to add this
    check, and he reports that this change adds about 11% improvement in some
    tests.
    
    Suggested-and-tested-by: "Zhang, Yanmin" <yanmin_zhang@linux.intel.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ceffc67b564a..671ed56e0a49 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -771,9 +771,12 @@ static void forget_original_parent(struct task_struct *father)
 	struct task_struct *p, *n, *reaper;
 	LIST_HEAD(dead_children);
 
-	exit_ptrace(father);
-
 	write_lock_irq(&tasklist_lock);
+	/*
+	 * Note that exit_ptrace() and find_new_reaper() might
+	 * drop tasklist_lock and reacquire it.
+	 */
+	exit_ptrace(father);
 	reaper = find_new_reaper(father);
 
 	list_for_each_entry_safe(p, n, &father->children, sibling) {

commit b3ac022cb9dc5883505a88b159d1b240ad1ef405
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:24 2010 -0700

    proc: turn signal_struct->count into "int nr_threads"
    
    No functional changes, just s/atomic_t count/int nr_threads/.
    
    With the recent changes this counter has a single user, get_nr_threads()
    And, none of its callers need the really accurate number of threads, not
    to mention each caller obviously races with fork/exit.  It is only used to
    report this value to the user-space, except first_tid() uses it to avoid
    the unnecessary while_each_thread() loop in the unlikely case.
    
    It is a bit sad we need a word in struct signal_struct for this, perhaps
    we can change get_nr_threads() to approximate the number of threads using
    signal->live and kill ->nr_threads later.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 357d443d5a00..ceffc67b564a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -83,14 +83,10 @@ static void __exit_signal(struct task_struct *tsk)
 	struct sighand_struct *sighand;
 	struct tty_struct *uninitialized_var(tty);
 
-	BUG_ON(!sig);
-	BUG_ON(!atomic_read(&sig->count));
-
 	sighand = rcu_dereference_check(tsk->sighand,
 					rcu_read_lock_held() ||
 					lockdep_tasklist_lock_is_held());
 	spin_lock(&sighand->siglock);
-	atomic_dec(&sig->count);
 
 	posix_cpu_timers_exit(tsk);
 	if (group_dead) {
@@ -130,6 +126,7 @@ static void __exit_signal(struct task_struct *tsk)
 		sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 	}
 
+	sig->nr_threads--;
 	__unhash_process(tsk, group_dead);
 
 	/*

commit 97101eb41d0d3c97543878ce40e0b8a8b2747ed7
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:20 2010 -0700

    exit: move taskstats_tgid_free() from __exit_signal() to free_signal_struct()
    
    Move taskstats_tgid_free() from __exit_signal() to free_signal_struct().
    
    This way signal->stats never points to nowhere and we can read ->stats
    lockless.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3602f468e3a0..357d443d5a00 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -144,7 +144,6 @@ static void __exit_signal(struct task_struct *tsk)
 	clear_tsk_thread_flag(tsk,TIF_SIGPENDING);
 	if (group_dead) {
 		flush_sigqueue(&sig->shared_pending);
-		taskstats_tgid_free(sig);
 		tty_kref_put(tty);
 	}
 }

commit d40e48e02f3785b9342ee4eb3d7cc9f12981b7f5
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:19 2010 -0700

    exit: __exit_signal: use thread_group_leader() consistently
    
    Cleanup:
    
    - Add the boolean, group_dead = thread_group_leader(), for clarity.
    
    - Do not test/set sig == NULL to detect the all-dead case, use this
      boolean.
    
    - Pass this boolen to __unhash_process() and use it instead of another
      thread_group_leader() call which needs ->group_leader.
    
      This can be considered as microoptimization, but hopefully this also
      allows us do do other cleanups later.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index bbc790646502..3602f468e3a0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -58,11 +58,11 @@
 
 static void exit_mm(struct task_struct * tsk);
 
-static void __unhash_process(struct task_struct *p)
+static void __unhash_process(struct task_struct *p, bool group_dead)
 {
 	nr_threads--;
 	detach_pid(p, PIDTYPE_PID);
-	if (thread_group_leader(p)) {
+	if (group_dead) {
 		detach_pid(p, PIDTYPE_PGID);
 		detach_pid(p, PIDTYPE_SID);
 
@@ -79,6 +79,7 @@ static void __unhash_process(struct task_struct *p)
 static void __exit_signal(struct task_struct *tsk)
 {
 	struct signal_struct *sig = tsk->signal;
+	bool group_dead = thread_group_leader(tsk);
 	struct sighand_struct *sighand;
 	struct tty_struct *uninitialized_var(tty);
 
@@ -92,7 +93,7 @@ static void __exit_signal(struct task_struct *tsk)
 	atomic_dec(&sig->count);
 
 	posix_cpu_timers_exit(tsk);
-	if (thread_group_leader(tsk)) {
+	if (group_dead) {
 		posix_cpu_timers_exit_group(tsk);
 		tty = sig->tty;
 		sig->tty = NULL;
@@ -127,10 +128,9 @@ static void __exit_signal(struct task_struct *tsk)
 		sig->oublock += task_io_get_oublock(tsk);
 		task_io_accounting_add(&sig->ioac, &tsk->ioac);
 		sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
-		sig = NULL; /* Marker for below. */
 	}
 
-	__unhash_process(tsk);
+	__unhash_process(tsk, group_dead);
 
 	/*
 	 * Do this under ->siglock, we can race with another thread
@@ -142,7 +142,7 @@ static void __exit_signal(struct task_struct *tsk)
 
 	__cleanup_sighand(sighand);
 	clear_tsk_thread_flag(tsk,TIF_SIGPENDING);
-	if (sig) {
+	if (group_dead) {
 		flush_sigqueue(&sig->shared_pending);
 		taskstats_tgid_free(sig);
 		tty_kref_put(tty);

commit b7b8ff6373d4b910af081f76888395e6df53249d
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:18 2010 -0700

    signals: kill the awful task_rq_unlock_wait() hack
    
    Now that task->signal can't go away we can revert the horrible hack added
    by ad474caca3e2a0550b7ce0706527ad5ab389a4d4 ("fix for
    account_group_exec_runtime(), make sure ->signal can't be freed under
    rq->lock").
    
    And we can do more cleanups sched_stats.h/posix-cpu-timers.c later.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 356d91fa095f..bbc790646502 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -145,11 +145,6 @@ static void __exit_signal(struct task_struct *tsk)
 	if (sig) {
 		flush_sigqueue(&sig->shared_pending);
 		taskstats_tgid_free(sig);
-		/*
-		 * Make sure ->signal can't go away under rq->lock,
-		 * see account_group_exec_runtime().
-		 */
-		task_rq_unlock_wait(tsk);
 		tty_kref_put(tty);
 	}
 }

commit 4ada856fb0ee62f6fe3aac3de726deac0640d929
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:17 2010 -0700

    signals: clear signal->tty when the last thread exits
    
    When the last thread exits signal->tty is freed, but the pointer is not
    cleared and points to nowhere.
    
    This is OK.  Nobody should use signal->tty lockless, and it is no longer
    possible to take ->siglock.  However this looks wrong even if correct, and
    the nice OOPS is better than subtle and hard to find bugs.
    
    Change __exit_signal() to clear signal->tty under ->siglock.
    
    Note: __exit_signal() needs more cleanups.  It should not check "sig !=
    NULL" to detect the all-dead case and we have the same issues with
    signal->stats.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 92af5cde9bbe..356d91fa095f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -80,6 +80,7 @@ static void __exit_signal(struct task_struct *tsk)
 {
 	struct signal_struct *sig = tsk->signal;
 	struct sighand_struct *sighand;
+	struct tty_struct *uninitialized_var(tty);
 
 	BUG_ON(!sig);
 	BUG_ON(!atomic_read(&sig->count));
@@ -93,6 +94,8 @@ static void __exit_signal(struct task_struct *tsk)
 	posix_cpu_timers_exit(tsk);
 	if (thread_group_leader(tsk)) {
 		posix_cpu_timers_exit_group(tsk);
+		tty = sig->tty;
+		sig->tty = NULL;
 	} else {
 		/*
 		 * If there is any task waiting for the group exit
@@ -147,7 +150,7 @@ static void __exit_signal(struct task_struct *tsk)
 		 * see account_group_exec_runtime().
 		 */
 		task_rq_unlock_wait(tsk);
-		tty_kref_put(sig->tty);
+		tty_kref_put(tty);
 	}
 }
 

commit ea6d290ca34c4fd91b7348338c0cc7bdeff94a35
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:16 2010 -0700

    signals: make task_struct->signal immutable/refcountable
    
    We have a lot of problems with accessing task_struct->signal, it can
    "disappear" at any moment.  Even current can't use its ->signal safely
    after exit_notify().  ->siglock helps, but it is not convenient, not
    always possible, and sometimes it makes sense to use task->signal even
    after this task has already dead.
    
    This patch adds the reference counter, sigcnt, into signal_struct.  This
    reference is owned by task_struct and it is dropped in
    __put_task_struct().  Perhaps it makes sense to export
    get/put_signal_struct() later, but currently I don't see the immediate
    reason.
    
    Rename __cleanup_signal() to free_signal_struct() and unexport it.  With
    the previous changes it does nothing except kmem_cache_free().
    
    Change __exit_signal() to not clear/free ->signal, it will be freed when
    the last reference to any thread in the thread group goes away.
    
    Note:
            - when the last thead exits signal->tty can point to nowhere, see
              the next patch.
    
            - with or without this patch signal_struct->count should go away,
              or at least it should be "int nr_threads" for fs/proc. This will
              be addressed later.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Alan Cox <alan@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4a72f1753edb..92af5cde9bbe 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -134,8 +134,6 @@ static void __exit_signal(struct task_struct *tsk)
 	 * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.
 	 */
 	flush_sigqueue(&tsk->pending);
-
-	tsk->signal = NULL;
 	tsk->sighand = NULL;
 	spin_unlock(&sighand->siglock);
 
@@ -150,7 +148,6 @@ static void __exit_signal(struct task_struct *tsk)
 		 */
 		task_rq_unlock_wait(tsk);
 		tty_kref_put(sig->tty);
-		__cleanup_signal(sig);
 	}
 }
 

commit 4dec2a91fd7e8815d730afbfdcf085cbf53433ac
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:15 2010 -0700

    fork/exit: move tty_kref_put() outside of __cleanup_signal()
    
    tty_kref_put() has two callsites in copy_process() paths,
    
            1. if copy_process() suceeds it is called before we copy
               signal->tty from parent
    
            2. otherwise it is called from __cleanup_signal() under
               bad_fork_cleanup_signal: label
    
    In both cases tty_kref_put() is not right and unneeded because we don't
    have the balancing tty_kref_get().  Fortunately, this is harmless because
    this can only happen without CLONE_THREAD, and in this case signal->tty
    must be NULL.
    
    Remove tty_kref_put() from copy_process() and __cleanup_signal(), and
    change another caller of __cleanup_signal(), __exit_signal(), to call
    tty_kref_put() by hand.
    
    I hope this change makes sense by itself, but it is also needed to make
    ->signal refcountable.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Alan Cox <alan@linux.intel.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Greg KH <greg@kroah.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4c70c377d21f..4a72f1753edb 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -149,6 +149,7 @@ static void __exit_signal(struct task_struct *tsk)
 		 * see account_group_exec_runtime().
 		 */
 		task_rq_unlock_wait(tsk);
+		tty_kref_put(sig->tty);
 		__cleanup_signal(sig);
 	}
 }

commit 4a5999429739844367d0f77a65efdd7db8202779
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:12 2010 -0700

    exit: avoid sig->count in __exit_signal() to detect the group-dead case
    
    Change __exit_signal() to check thread_group_leader() instead of
    atomic_dec_and_test(&sig->count).  This must be equivalent, the group
    leader must be released only after all other threads have exited and
    passed __exit_signal().
    
    Henceforth sig->count is not actually used, except in fs/proc for
    get_nr_threads/etc.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9220967f4256..4c70c377d21f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -88,11 +88,12 @@ static void __exit_signal(struct task_struct *tsk)
 					rcu_read_lock_held() ||
 					lockdep_tasklist_lock_is_held());
 	spin_lock(&sighand->siglock);
+	atomic_dec(&sig->count);
 
 	posix_cpu_timers_exit(tsk);
-	if (atomic_dec_and_test(&sig->count))
+	if (thread_group_leader(tsk)) {
 		posix_cpu_timers_exit_group(tsk);
-	else {
+	} else {
 		/*
 		 * If there is any task waiting for the group exit
 		 * then notify it:

commit d344193a05da89c97e965da2c5cbf687d7385eae
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:11 2010 -0700

    exit: avoid sig->count in de_thread/__exit_signal synchronization
    
    de_thread() and __exit_signal() use signal_struct->count/notify_count for
    synchronization.  We can simplify the code and use ->notify_count only.
    Instead of comparing these two counters, we can change de_thread() to set
    ->notify_count = nr_of_sub_threads, then change __exit_signal() to
    dec-and-test this counter and notify group_exit_task.
    
    Note that __exit_signal() checks "notify_count > 0" just for symmetry with
    exit_notify(), we could just check it is != 0.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 59a104c673f7..9220967f4256 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -97,7 +97,7 @@ static void __exit_signal(struct task_struct *tsk)
 		 * If there is any task waiting for the group exit
 		 * then notify it:
 		 */
-		if (sig->group_exit_task && atomic_read(&sig->count) == sig->notify_count)
+		if (sig->notify_count > 0 && !--sig->notify_count)
 			wake_up_process(sig->group_exit_task);
 
 		if (tsk == sig->curr_target)

commit 9c3391684415c9dca239130d9e433a60a4edf04b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed May 26 14:43:10 2010 -0700

    exit: exit_notify() can trust signal->notify_count < 0
    
    signal_struct->count in its current form must die.
    
    - it has no reasons to be atomic_t
    
    - it looks like a reference counter, but it is not
    
    - otoh, we really need to make task->signal refcountable, just look at
      the extremely ugly task_rq_unlock_wait() called from __exit_signals().
    
    - we should change the lifetime rules for task->signal, it should be
      pinned to task_struct.  We have a lot of code which can be simplified
      after that.
    
    - it is not needed!  while the code is correct, any usage of this
      counter is artificial, except fs/proc uses it correctly to show the
      number of threads.
    
    This series removes the usage of sig->count from exit pathes.
    
    This patch:
    
    Now that Veaceslav changed copy_signal() to use zalloc(), exit_notify()
    can just check notify_count < 0 to ensure the execing sub-threads needs
    the notification from us.  No need to do other checks, notify_count != 0
    must always mean ->group_exit_task != NULL is waiting for us.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Veaceslav Falico <vfalico@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 019a2843bf95..59a104c673f7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -856,12 +856,9 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 
 	tsk->exit_state = signal == DEATH_REAP ? EXIT_DEAD : EXIT_ZOMBIE;
 
-	/* mt-exec, de_thread() is waiting for us */
-	if (thread_group_leader(tsk) &&
-	    tsk->signal->group_exit_task &&
-	    tsk->signal->notify_count < 0)
+	/* mt-exec, de_thread() is waiting for group leader */
+	if (unlikely(tsk->signal->notify_count < 0))
 		wake_up_process(tsk->signal->group_exit_task);
-
 	write_unlock_irq(&tasklist_lock);
 
 	tracehook_report_death(tsk, signal, cookie, group_dead);

commit c0ff7453bb5c7c98e0885fb94279f2571946f280
Author: Miao Xie <miaox@cn.fujitsu.com>
Date:   Mon May 24 14:32:08 2010 -0700

    cpuset,mm: fix no node to alloc memory when changing cpuset's mems
    
    Before applying this patch, cpuset updates task->mems_allowed and
    mempolicy by setting all new bits in the nodemask first, and clearing all
    old unallowed bits later.  But in the way, the allocator may find that
    there is no node to alloc memory.
    
    The reason is that cpuset rebinds the task's mempolicy, it cleans the
    nodes which the allocater can alloc pages on, for example:
    
    (mpol: mempolicy)
            task1                   task1's mpol    task2
            alloc page              1
              alloc on node0? NO    1
                                    1               change mems from 1 to 0
                                    1               rebind task1's mpol
                                    0-1               set new bits
                                    0                 clear disallowed bits
              alloc on node1? NO    0
              ...
            can't alloc page
              goto oom
    
    This patch fixes this problem by expanding the nodes range first(set newly
    allowed bits) and shrink it lazily(clear newly disallowed bits).  So we
    use a variable to tell the write-side task that read-side task is reading
    nodemask, and the write-side task clears newly disallowed nodes after
    read-side task ends the current memory allocation.
    
    [akpm@linux-foundation.org: fix spello]
    Signed-off-by: Miao Xie <miaox@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Paul Menage <menage@google.com>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Ravikiran Thirumalai <kiran@scalex86.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index eabca5a73a85..019a2843bf95 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1002,8 +1002,10 @@ NORET_TYPE void do_exit(long code)
 
 	exit_notify(tsk, group_dead);
 #ifdef CONFIG_NUMA
+	task_lock(tsk);
 	mpol_put(tsk->mempolicy);
 	tsk->mempolicy = NULL;
+	task_unlock(tsk);
 #endif
 #ifdef CONFIG_FUTEX
 	if (unlikely(current->pi_state_cache))

commit b257c14ceb1194a6181144210056d38f22127189
Merge: 371fd7e7a56a 2ba3abd8186f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Apr 15 09:35:24 2010 +0200

    Merge branch 'linus' into sched/core
    
    Merge reason: merge the latest fixes, update to -rc4.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a3a2e76c77fa22b114e421ac11dec0c56c3503fb
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Apr 6 14:34:42 2010 -0700

    mm: avoid null-pointer deref in sync_mm_rss()
    
    - We weren't zeroing p->rss_stat[] at fork()
    
    - Consequently sync_mm_rss() was dereferencing tsk->mm for kernel
      threads and was oopsing.
    
    - Make __sync_task_rss_stat() static, too.
    
    Addresses https://bugzilla.kernel.org/show_bug.cgi?id=15648
    
    [akpm@linux-foundation.org: remove the BUG_ON(!mm->rss)]
    Reported-by: Troels Liebe Bentsen <tlb@rapanden.dk>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    "Michael S. Tsirkin" <mst@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index cce59cb5ee6a..7f2683a10ac4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -953,7 +953,8 @@ NORET_TYPE void do_exit(long code)
 
 	acct_update_integrals(tsk);
 	/* sync mm's RSS info before statistics gathering */
-	sync_mm_rss(tsk, tsk->mm);
+	if (tsk->mm)
+		sync_mm_rss(tsk, tsk->mm);
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
 		hrtimer_cancel(&tsk->signal->real_timer);

commit 32bd7eb5a7f4596c8440dd9440322fe9e686634d
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Mar 24 13:17:19 2010 +0800

    sched: Remove remaining USER_SCHED code
    
    This is left over from commit 7c9414385e ("sched: Remove USER_SCHED"")
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Dhaval Giani <dhaval.giani@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: David Howells <dhowells@redhat.com>
    LKML-Reference: <4BA9A05F.7010407@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index cce59cb5ee6a..84dc4b294e47 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -55,7 +55,6 @@
 #include <asm/unistd.h>
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
-#include "cred-internals.h"
 
 static void exit_mm(struct task_struct * tsk);
 

commit 4e3eaddd142e2142c048c5052a0a9d2604fccfc6
Merge: 8655e7e3ddec b97c4bc16734
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 13 14:43:01 2010 -0800

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      locking: Make sparse work with inline spinlocks and rwlocks
      x86/mce: Fix RCU lockdep splats
      rcu: Increase RCU CPU stall timeouts if PROVE_RCU
      ftrace: Replace read_barrier_depends() with rcu_dereference_raw()
      rcu: Suppress RCU lockdep warnings during early boot
      rcu, ftrace: Fix RCU lockdep splat in ftrace_perf_buf_prepare()
      rcu: Suppress __mpol_dup() false positive from RCU lockdep
      rcu: Make rcu_read_lock_sched_held() handle !PREEMPT
      rcu: Add control variables to lockdep_rcu_dereference() diagnostics
      rcu, cgroup: Relax the check in task_subsys_state() as early boot is now handled by lockdep-RCU
      rcu: Use wrapper function instead of exporting tasklist_lock
      sched, rcu: Fix rcu_dereference() for RCU-lockdep
      rcu: Make task_subsys_state() RCU-lockdep checks handle boot-time use
      rcu: Fix holdoff for accelerated GPs for last non-dynticked CPU
      x86/gart: Unexport gart_iommu_aperture
    
    Fix trivial conflicts in kernel/trace/ftrace.c

commit f3abd4f9531becb71626bd206955d47d5ea54f06
Author: Thiago Farina <tfransosi@gmail.com>
Date:   Fri Mar 5 13:42:52 2010 -0800

    kernel/exit.c: fix shadows sparse warning
    
    kernel/exit.c:1183:26: warning: symbol 'status' shadows an earlier one
    kernel/exit.c:1173:21: originally declared here
    
    Signed-off-by: Thiago Farina <tfransosi@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 10d3c5d5ae44..ce1e48c2d93d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1189,7 +1189,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 
 	if (unlikely(wo->wo_flags & WNOWAIT)) {
 		int exit_code = p->exit_code;
-		int why, status;
+		int why;
 
 		get_task_struct(p);
 		read_unlock(&tasklist_lock);

commit 34e55232e59f7b19050267a05ff1226e5cd122a5
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Fri Mar 5 13:41:40 2010 -0800

    mm: avoid false sharing of mm_counter
    
    Considering the nature of per mm stats, it's the shared object among
    threads and can be a cache-miss point in the page fault path.
    
    This patch adds per-thread cache for mm_counter.  RSS value will be
    counted into a struct in task_struct and synchronized with mm's one at
    events.
    
    Now, in this patch, the event is the number of calls to handle_mm_fault.
    Per-thread value is added to mm at each 64 calls.
    
     rough estimation with small benchmark on parallel thread (2threads) shows
     [before]
         4.5 cache-miss/faults
     [after]
         4.0 cache-miss/faults
     Anyway, the most contended object is mmap_sem if the number of threads grows.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: Christoph Lameter <cl@linux-foundation.org>
    Cc: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 45ed043b8bf5..10d3c5d5ae44 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -952,7 +952,8 @@ NORET_TYPE void do_exit(long code)
 				preempt_count());
 
 	acct_update_integrals(tsk);
-
+	/* sync mm's RSS info before statistics gathering */
+	sync_mm_rss(tsk, tsk->mm);
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
 		hrtimer_cancel(&tsk->signal->real_timer);

commit db1466b3e1bd1727375cdbfcbea4bcce2f860f61
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Mar 3 07:46:56 2010 -0800

    rcu: Use wrapper function instead of exporting tasklist_lock
    
    Lockdep-RCU commit d11c563d exported tasklist_lock, which is not
    a good thing.  This patch instead exports a function that uses
    lockdep to check whether tasklist_lock is held.
    
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    Cc: Christoph Hellwig <hch@lst.de>
    LKML-Reference: <1267631219-8713-1-git-send-email-paulmck@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 45ed043b8bf5..fed3a4db6f04 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -87,7 +87,7 @@ static void __exit_signal(struct task_struct *tsk)
 
 	sighand = rcu_dereference_check(tsk->sighand,
 					rcu_read_lock_held() ||
-					lockdep_is_held(&tasklist_lock));
+					lockdep_tasklist_lock_is_held());
 	spin_lock(&sighand->siglock);
 
 	posix_cpu_timers_exit(tsk);

commit d11c563dd20ff35da5652c3e1c989d9e10e1d6d0
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Feb 22 17:04:50 2010 -0800

    sched: Use lockdep-based checking on rcu_dereference()
    
    Update the rcu_dereference() usages to take advantage of the new
    lockdep-based checking.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josh@joshtriplett.org
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    Cc: Valdis.Kletnieks@vt.edu
    Cc: dhowells@redhat.com
    LKML-Reference: <1266887105-1528-6-git-send-email-paulmck@linux.vnet.ibm.com>
    [ -v2: fix allmodconfig missing symbol export build failure on x86 ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 546774a31a66..45ed043b8bf5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -85,7 +85,9 @@ static void __exit_signal(struct task_struct *tsk)
 	BUG_ON(!sig);
 	BUG_ON(!atomic_read(&sig->count));
 
-	sighand = rcu_dereference(tsk->sighand);
+	sighand = rcu_dereference_check(tsk->sighand,
+					rcu_read_lock_held() ||
+					lockdep_is_held(&tasklist_lock));
 	spin_lock(&sighand->siglock);
 
 	posix_cpu_timers_exit(tsk);
@@ -170,8 +172,10 @@ void release_task(struct task_struct * p)
 repeat:
 	tracehook_prepare_release_task(p);
 	/* don't need to get the RCU readlock here - the process is dead and
-	 * can't be modifying its own credentials */
+	 * can't be modifying its own credentials. But shut RCU-lockdep up */
+	rcu_read_lock();
 	atomic_dec(&__task_cred(p)->user->processes);
+	rcu_read_unlock();
 
 	proc_flush_task(p);
 
@@ -473,9 +477,11 @@ static void close_files(struct files_struct * files)
 	/*
 	 * It is safe to dereference the fd table without RCU or
 	 * ->file_lock because this is the last reference to the
-	 * files structure.
+	 * files structure.  But use RCU to shut RCU-lockdep up.
 	 */
+	rcu_read_lock();
 	fdt = files_fdtable(files);
+	rcu_read_unlock();
 	for (;;) {
 		unsigned long set;
 		i = j * __NFDBITS;
@@ -521,10 +527,12 @@ void put_files_struct(struct files_struct *files)
 		 * at the end of the RCU grace period. Otherwise,
 		 * you can free files immediately.
 		 */
+		rcu_read_lock();
 		fdt = files_fdtable(files);
 		if (fdt != &files->fdtab)
 			kmem_cache_free(files_cachep, files);
 		free_fdtable(fdt);
+		rcu_read_unlock();
 	}
 }
 

commit 9cd80bbb07fcd6d4d037fad4297496d3b132ac6b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Dec 17 15:27:15 2009 -0800

    do_wait() optimization: do not place sub-threads on task_struct->children list
    
    Thanks to Roland who pointed out de_thread() issues.
    
    Currently we add sub-threads to ->real_parent->children list.  This buys
    nothing but slows down do_wait().
    
    With this patch ->children contains only main threads (group leaders).
    The only complication is that forget_original_parent() should iterate over
    sub-threads by hand, and de_thread() needs another list_replace() when it
    changes ->group_leader.
    
    Henceforth do_wait_thread() can never see task_detached() && !EXIT_DEAD
    tasks, we can remove this check (and we can unify do_wait_thread() and
    ptrace_do_wait()).
    
    This change can confuse the optimistic search in mm_update_next_owner(),
    but this is fixable and minor.
    
    Perhaps badness() and oom_kill_process() should be updated, but they
    should be fixed in any case.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Ratan Nalumasu <rnalumasu@gmail.com>
    Cc: Vitaly Mayatskikh <vmayatsk@redhat.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5962d7ccf243..546774a31a66 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -68,10 +68,10 @@ static void __unhash_process(struct task_struct *p)
 		detach_pid(p, PIDTYPE_SID);
 
 		list_del_rcu(&p->tasks);
+		list_del_init(&p->sibling);
 		__get_cpu_var(process_counts)--;
 	}
 	list_del_rcu(&p->thread_group);
-	list_del_init(&p->sibling);
 }
 
 /*
@@ -736,12 +736,9 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 /*
 * Any that need to be release_task'd are put on the @dead list.
  */
-static void reparent_thread(struct task_struct *father, struct task_struct *p,
+static void reparent_leader(struct task_struct *father, struct task_struct *p,
 				struct list_head *dead)
 {
-	if (p->pdeath_signal)
-		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
-
 	list_move_tail(&p->sibling, &p->real_parent->children);
 
 	if (task_detached(p))
@@ -780,12 +777,18 @@ static void forget_original_parent(struct task_struct *father)
 	reaper = find_new_reaper(father);
 
 	list_for_each_entry_safe(p, n, &father->children, sibling) {
-		p->real_parent = reaper;
-		if (p->parent == father) {
-			BUG_ON(task_ptrace(p));
-			p->parent = p->real_parent;
-		}
-		reparent_thread(father, p, &dead_children);
+		struct task_struct *t = p;
+		do {
+			t->real_parent = reaper;
+			if (t->parent == father) {
+				BUG_ON(task_ptrace(t));
+				t->parent = t->real_parent;
+			}
+			if (t->pdeath_signal)
+				group_send_sig_info(t->pdeath_signal,
+						    SEND_SIG_NOINFO, t);
+		} while_each_thread(p, t);
+		reparent_leader(father, p, &dead_children);
 	}
 	write_unlock_irq(&tasklist_lock);
 
@@ -1551,14 +1554,9 @@ static int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)
 	struct task_struct *p;
 
 	list_for_each_entry(p, &tsk->children, sibling) {
-		/*
-		 * Do not consider detached threads.
-		 */
-		if (!task_detached(p)) {
-			int ret = wait_consider_task(wo, 0, p);
-			if (ret)
-				return ret;
-		}
+		int ret = wait_consider_task(wo, 0, p);
+		if (ret)
+			return ret;
 	}
 
 	return 0;

commit 1d615482547584b9a8bb6316a58fed6ce90dd9ff
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Nov 17 14:54:03 2009 +0100

    sched: Convert pi_lock to raw_spinlock
    
    Convert locks which cannot be sleeping locks in preempt-rt to
    raw_spinlocks.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6f50ef55a6f3..5962d7ccf243 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -933,7 +933,7 @@ NORET_TYPE void do_exit(long code)
 	 * an exiting task cleaning up the robust pi futexes.
 	 */
 	smp_mb();
-	spin_unlock_wait(&tsk->pi_lock);
+	raw_spin_unlock_wait(&tsk->pi_lock);
 
 	if (unlikely(in_atomic()))
 		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",

commit 5ec93d1154fd1e269162398f8e70efc7e004485d
Author: Alan Cox <alan@linux.intel.com>
Date:   Mon Nov 30 13:18:45 2009 +0000

    tty: Move the leader test in disassociate
    
    There are two call points, both want to check that tty->signal->leader is
    set. Move the test into disassociate_ctty() as that will make locking
    changes easier in a bit
    
    Signed-off-by: Alan Cox <alan@linux.intel.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1143012951e9..6f50ef55a6f3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -971,7 +971,7 @@ NORET_TYPE void do_exit(long code)
 	exit_thread();
 	cgroup_exit(tsk, 1);
 
-	if (group_dead && tsk->signal->leader)
+	if (group_dead)
 		disassociate_ctty(1);
 
 	module_put(task_thread_info(tsk)->exec_domain->module);

commit 6035ccd8e9e40bb654fbfdef325902ab531679a5
Merge: 23eb3b64b5e4 878eaddd05d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 8 08:19:16 2009 -0800

    Merge branch 'for-2.6.33' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.33' of git://git.kernel.dk/linux-2.6-block: (113 commits)
      cfq-iosched: Do not access cfqq after freeing it
      block: include linux/err.h to use ERR_PTR
      cfq-iosched: use call_rcu() instead of doing grace period stall on queue exit
      blkio: Allow CFQ group IO scheduling even when CFQ is a module
      blkio: Implement dynamic io controlling policy registration
      blkio: Export some symbols from blkio as its user CFQ can be a module
      block: Fix io_context leak after failure of clone with CLONE_IO
      block: Fix io_context leak after clone with CLONE_IO
      cfq-iosched: make nonrot check logic consistent
      io controller: quick fix for blk-cgroup and modular CFQ
      cfq-iosched: move IO controller declerations to a header file
      cfq-iosched: fix compile problem with !CONFIG_CGROUP
      blkio: Documentation
      blkio: Wait on sync-noidle queue even if rq_noidle = 1
      blkio: Implement group_isolation tunable
      blkio: Determine async workload length based on total number of queues
      blkio: Wait for cfq queue to get backlogged if group is empty
      blkio: Propagate cgroup weight updation to cfq groups
      blkio: Drop the reference to queue once the task changes cgroup
      blkio: Provide some isolation between groups
      ...

commit 897e81bea1fcfcd2c5cdb720c9efdb25da9ff374
Merge: c3fa27d1367f 0cf55e1ec08b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 5 15:30:49 2009 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (35 commits)
      sched, cputime: Introduce thread_group_times()
      sched, cputime: Cleanups related to task_times()
      Revert "sched, x86: Optimize branch hint in __switch_to()"
      sched: Fix isolcpus boot option
      sched: Revert 498657a478c60be092208422fefa9c7b248729c2
      sched, time: Define nsecs_to_jiffies()
      sched: Remove task_{u,s,g}time()
      sched: Introduce task_times() to replace task_{u,s}time() pair
      sched: Limit the number of scheduler debug messages
      sched.c: Call debug_show_all_locks() when dumping all tasks
      sched, x86: Optimize branch hint in __switch_to()
      sched: Optimize branch hint in context_switch()
      sched: Optimize branch hint in pick_next_task_fair()
      sched_feat_write(): Update ppos instead of file->f_pos
      sched: Sched_rt_periodic_timer vs cpu hotplug
      sched, kvm: Fix race condition involving sched_in_preempt_notifers
      sched: More generic WAKE_AFFINE vs select_idle_sibling()
      sched: Cleanup select_task_rq_fair()
      sched: Fix granularity of task_u/stime()
      sched: Fix/add missing update_rq_clock() calls
      ...

commit b69f2292063d2caf37ca9aec7d63ded203701bf3
Author: Louis Rilling <louis.rilling@kerlabs.com>
Date:   Fri Dec 4 14:52:42 2009 +0100

    block: Fix io_context leak after failure of clone with CLONE_IO
    
    With CLONE_IO, parent's io_context->nr_tasks is incremented, but never
    decremented whenever copy_process() fails afterwards, which prevents
    exit_io_context() from calling IO schedulers exit functions.
    
    Give a task_struct to exit_io_context(), and call exit_io_context() instead of
    put_io_context() in copy_process() cleanup path.
    
    Signed-off-by: Louis Rilling <louis.rilling@kerlabs.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index f7864ac2ecc1..2544000125d9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1004,7 +1004,7 @@ NORET_TYPE void do_exit(long code)
 	tsk->flags |= PF_EXITPIDONE;
 
 	if (tsk->io_context)
-		exit_io_context();
+		exit_io_context(tsk);
 
 	if (tsk->splice_pipe)
 		__free_pipe_info(tsk->splice_pipe);

commit 0cf55e1ec08bb5a22e068309e2d8ba1180ab4239
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Wed Dec 2 17:28:07 2009 +0900

    sched, cputime: Introduce thread_group_times()
    
    This is a real fix for problem of utime/stime values decreasing
    described in the thread:
    
       http://lkml.org/lkml/2009/11/3/522
    
    Now cputime is accounted in the following way:
    
     - {u,s}time in task_struct are increased every time when the thread
       is interrupted by a tick (timer interrupt).
    
     - When a thread exits, its {u,s}time are added to signal->{u,s}time,
       after adjusted by task_times().
    
     - When all threads in a thread_group exits, accumulated {u,s}time
       (and also c{u,s}time) in signal struct are added to c{u,s}time
       in signal struct of the group's parent.
    
    So {u,s}time in task struct are "raw" tick count, while
    {u,s}time and c{u,s}time in signal struct are "adjusted" values.
    
    And accounted values are used by:
    
     - task_times(), to get cputime of a thread:
       This function returns adjusted values that originates from raw
       {u,s}time and scaled by sum_exec_runtime that accounted by CFS.
    
     - thread_group_cputime(), to get cputime of a thread group:
       This function returns sum of all {u,s}time of living threads in
       the group, plus {u,s}time in the signal struct that is sum of
       adjusted cputimes of all exited threads belonged to the group.
    
    The problem is the return value of thread_group_cputime(),
    because it is mixed sum of "raw" value and "adjusted" value:
    
      group's {u,s}time = foreach(thread){{u,s}time} + exited({u,s}time)
    
    This misbehavior can break {u,s}time monotonicity.
    Assume that if there is a thread that have raw values greater
    than adjusted values (e.g. interrupted by 1000Hz ticks 50 times
    but only runs 45ms) and if it exits, cputime will decrease (e.g.
    -5ms).
    
    To fix this, we could do:
    
      group's {u,s}time = foreach(t){task_times(t)} + exited({u,s}time)
    
    But task_times() contains hard divisions, so applying it for
    every thread should be avoided.
    
    This patch fixes the above problem in the following way:
    
     - Modify thread's exit (= __exit_signal()) not to use task_times().
       It means {u,s}time in signal struct accumulates raw values instead
       of adjusted values.  As the result it makes thread_group_cputime()
       to return pure sum of "raw" values.
    
     - Introduce a new function thread_group_times(*task, *utime, *stime)
       that converts "raw" values of thread_group_cputime() to "adjusted"
       values, in same calculation procedure as task_times().
    
     - Modify group's exit (= wait_task_zombie()) to use this introduced
       thread_group_times().  It make c{u,s}time in signal struct to
       have adjusted values like before this patch.
    
     - Replace some thread_group_cputime() by thread_group_times().
       This replacements are only applied where conveys the "adjusted"
       cputime to users, and where already uses task_times() near by it.
       (i.e. sys_times(), getrusage(), and /proc/<PID>/stat.)
    
    This patch have a positive side effect:
    
     - Before this patch, if a group contains many short-life threads
       (e.g. runs 0.9ms and not interrupted by ticks), the group's
       cputime could be invisible since thread's cputime was accumulated
       after adjusted: imagine adjustment function as adj(ticks, runtime),
         {adj(0, 0.9) + adj(0, 0.9) + ....} = {0 + 0 + ....} = 0.
       After this patch it will not happen because the adjustment is
       applied after accumulated.
    
    v2:
     - remove if()s, put new variables into signal_struct.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Spencer Candland <spencer@bluehost.com>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    LKML-Reference: <4B162517.8040909@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2eaf68b634e3..b221ad65fd20 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -91,8 +91,6 @@ static void __exit_signal(struct task_struct *tsk)
 	if (atomic_dec_and_test(&sig->count))
 		posix_cpu_timers_exit_group(tsk);
 	else {
-		cputime_t utime, stime;
-
 		/*
 		 * If there is any task waiting for the group exit
 		 * then notify it:
@@ -112,9 +110,8 @@ static void __exit_signal(struct task_struct *tsk)
 		 * We won't ever get here for the group leader, since it
 		 * will have been the last reference on the signal_struct.
 		 */
-		task_times(tsk, &utime, &stime);
-		sig->utime = cputime_add(sig->utime, utime);
-		sig->stime = cputime_add(sig->stime, stime);
+		sig->utime = cputime_add(sig->utime, tsk->utime);
+		sig->stime = cputime_add(sig->stime, tsk->stime);
 		sig->gtime = cputime_add(sig->gtime, tsk->gtime);
 		sig->min_flt += tsk->min_flt;
 		sig->maj_flt += tsk->maj_flt;
@@ -1208,6 +1205,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		struct signal_struct *psig;
 		struct signal_struct *sig;
 		unsigned long maxrss;
+		cputime_t tgutime, tgstime;
 
 		/*
 		 * The resource counters for the group leader are in its
@@ -1223,20 +1221,23 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		 * need to protect the access to parent->signal fields,
 		 * as other threads in the parent group can be right
 		 * here reaping other children at the same time.
+		 *
+		 * We use thread_group_times() to get times for the thread
+		 * group, which consolidates times for all threads in the
+		 * group including the group leader.
 		 */
+		thread_group_times(p, &tgutime, &tgstime);
 		spin_lock_irq(&p->real_parent->sighand->siglock);
 		psig = p->real_parent->signal;
 		sig = p->signal;
 		psig->cutime =
 			cputime_add(psig->cutime,
-			cputime_add(p->utime,
-			cputime_add(sig->utime,
-				    sig->cutime)));
+			cputime_add(tgutime,
+				    sig->cutime));
 		psig->cstime =
 			cputime_add(psig->cstime,
-			cputime_add(p->stime,
-			cputime_add(sig->stime,
-				    sig->cstime)));
+			cputime_add(tgstime,
+				    sig->cstime));
 		psig->cgtime =
 			cputime_add(psig->cgtime,
 			cputime_add(p->gtime,

commit d5b7c78e975302a1bab28263266c39ecb71acad4
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Thu Nov 26 14:49:05 2009 +0900

    sched: Remove task_{u,s,g}time()
    
    Now all task_{u,s}time() pairs are replaced by task_times().
    And task_gtime() is too simple to be an inline function.
    
    Cleanup them all.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Spencer Candland <spencer@bluehost.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    LKML-Reference: <4B0E16D1.70902@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 29068ab2670a..2eaf68b634e3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -115,7 +115,7 @@ static void __exit_signal(struct task_struct *tsk)
 		task_times(tsk, &utime, &stime);
 		sig->utime = cputime_add(sig->utime, utime);
 		sig->stime = cputime_add(sig->stime, stime);
-		sig->gtime = cputime_add(sig->gtime, task_gtime(tsk));
+		sig->gtime = cputime_add(sig->gtime, tsk->gtime);
 		sig->min_flt += tsk->min_flt;
 		sig->maj_flt += tsk->maj_flt;
 		sig->nvcsw += tsk->nvcsw;

commit d180c5bccec02612256fd8076ff3c1fac3429553
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Thu Nov 26 14:48:30 2009 +0900

    sched: Introduce task_times() to replace task_{u,s}time() pair
    
    Functions task_{u,s}time() are called in pair in almost all
    cases.  However task_stime() is implemented to call task_utime()
    from its inside, so such paired calls run task_utime() twice.
    
    It means we do heavy divisions (div_u64 + do_div) twice to get
    utime and stime which can be obtained at same time by one set
    of divisions.
    
    This patch introduces a function task_times(*tsk, *utime,
    *stime) to retrieve utime and stime at once in better, optimized
    way.
    
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Spencer Candland <spencer@bluehost.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Americo Wang <xiyou.wangcong@gmail.com>
    LKML-Reference: <4B0E16AE.906@jp.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index f7864ac2ecc1..29068ab2670a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -91,6 +91,8 @@ static void __exit_signal(struct task_struct *tsk)
 	if (atomic_dec_and_test(&sig->count))
 		posix_cpu_timers_exit_group(tsk);
 	else {
+		cputime_t utime, stime;
+
 		/*
 		 * If there is any task waiting for the group exit
 		 * then notify it:
@@ -110,8 +112,9 @@ static void __exit_signal(struct task_struct *tsk)
 		 * We won't ever get here for the group leader, since it
 		 * will have been the last reference on the signal_struct.
 		 */
-		sig->utime = cputime_add(sig->utime, task_utime(tsk));
-		sig->stime = cputime_add(sig->stime, task_stime(tsk));
+		task_times(tsk, &utime, &stime);
+		sig->utime = cputime_add(sig->utime, utime);
+		sig->stime = cputime_add(sig->stime, stime);
 		sig->gtime = cputime_add(sig->gtime, task_gtime(tsk));
 		sig->min_flt += tsk->min_flt;
 		sig->maj_flt += tsk->maj_flt;

commit 96200591a34f8ecb98481c626125df43a2463b55
Merge: 7031281e02bf 68efa37df779
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Nov 21 14:07:23 2009 +0100

    Merge branch 'tracing/hw-breakpoints' into perf/core
    
    Conflicts:
            arch/x86/kernel/kprobes.c
            kernel/trace/Makefile
    
    Merge reason: hw-breakpoints perf integration is looking
                  good in testing and in reviews, plus conflicts
                  are mounting up - so merge & resolve.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 24f1e32c60c45c89a997c73395b69c8af6f0a84e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 9 19:22:48 2009 +0200

    hw-breakpoints: Rewrite the hw-breakpoints layer on top of perf events
    
    This patch rebase the implementation of the breakpoints API on top of
    perf events instances.
    
    Each breakpoints are now perf events that handle the
    register scheduling, thread/cpu attachment, etc..
    
    The new layering is now made as follows:
    
           ptrace       kgdb      ftrace   perf syscall
              \          |          /         /
               \         |         /         /
                                            /
                Core breakpoint API        /
                                          /
                         |               /
                         |              /
    
                  Breakpoints perf events
    
                         |
                         |
    
                   Breakpoints PMU ---- Debug Register constraints handling
                                        (Part of core breakpoint API)
                         |
                         |
    
                 Hardware debug registers
    
    Reasons of this rewrite:
    
    - Use the centralized/optimized pmu registers scheduling,
      implying an easier arch integration
    - More powerful register handling: perf attributes (pinned/flexible
      events, exclusive/non-exclusive, tunable period, etc...)
    
    Impact:
    
    - New perf ABI: the hardware breakpoints counters
    - Ptrace breakpoints setting remains tricky and still needs some per
      thread breakpoints references.
    
    Todo (in the order):
    
    - Support breakpoints perf counter events for perf tools (ie: implement
      perf_bpcounter_event())
    - Support from perf tools
    
    Changes in v2:
    
    - Follow the perf "event " rename
    - The ptrace regression have been fixed (ptrace breakpoint perf events
      weren't released when a task ended)
    - Drop the struct hw_breakpoint and store generic fields in
      perf_event_attr.
    - Separate core and arch specific headers, drop
      asm-generic/hw_breakpoint.h and create linux/hw_breakpoint.h
    - Use new generic len/type for breakpoint
    - Handle off case: when breakpoints api is not supported by an arch
    
    Changes in v3:
    
    - Fix broken CONFIG_KVM, we need to propagate the breakpoint api
      changes to kvm when we exit the guest and restore the bp registers
      to the host.
    
    Changes in v4:
    
    - Drop the hw_breakpoint_restore() stub as it is only used by KVM
    - EXPORT_SYMBOL_GPL hw_breakpoint_restore() as KVM can be built as a
      module
    - Restore the breakpoints unconditionally on kvm guest exit:
      TIF_DEBUG_THREAD doesn't anymore cover every cases of running
      breakpoints and vcpu->arch.switch_db_regs might not always be
      set when the guest used debug registers.
      (Waiting for a reliable optimization)
    
    Changes in v5:
    
    - Split-up the asm-generic/hw-breakpoint.h moving to
      linux/hw_breakpoint.h into a separate patch
    - Optimize the breakpoints restoring while switching from kvm guest
      to host. We only want to restore the state if we have active
      breakpoints to the host, otherwise we don't care about messed-up
      address registers.
    - Add asm/hw_breakpoint.h to Kbuild
    - Fix bad breakpoint type in trace_selftest.c
    
    Changes in v6:
    
    - Fix wrong header inclusion in trace.h (triggered a build
      error with CONFIG_FTRACE_SELFTEST
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jan Kiszka <jan.kiszka@web.de>
    Cc: Jiri Slaby <jirislaby@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Paul Mundt <lethal@linux-sh.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e61891f80123..266f8920628a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -49,6 +49,7 @@
 #include <linux/init_task.h>
 #include <linux/perf_event.h>
 #include <trace/events/sched.h>
+#include <linux/hw_breakpoint.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -979,6 +980,10 @@ NORET_TYPE void do_exit(long code)
 
 	proc_exit_connector(tsk);
 
+	/*
+	 * FIXME: do that only when needed, using sched_exit tracepoint
+	 */
+	flush_ptrace_hw_breakpoint(tsk);
 	/*
 	 * Flush inherited counters to the parent - before the parent
 	 * gets woken up by child-exit notifications.

commit 0d0df599f1f11f12d589318bacb59a50fb5c0310
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Oct 26 16:49:34 2009 -0700

    connector: fix regression introduced by sid connector
    
    Since commit 02b51df1b07b4e9ca823c89284e704cadb323cd1 (proc connector: add
    event for process becoming session leader) we have the following warning:
    
    Badness at kernel/softirq.c:143
    [...]
    Krnl PSW : 0404c00180000000 00000000001481d4 (local_bh_enable+0xb0/0xe0)
    [...]
    Call Trace:
    ([<000000013fe04100>] 0x13fe04100)
     [<000000000048a946>] sk_filter+0x9a/0xd0
     [<000000000049d938>] netlink_broadcast+0x2c0/0x53c
     [<00000000003ba9ae>] cn_netlink_send+0x272/0x2b0
     [<00000000003baef0>] proc_sid_connector+0xc4/0xd4
     [<0000000000142604>] __set_special_pids+0x58/0x90
     [<0000000000159938>] sys_setsid+0xb4/0xd8
     [<00000000001187fe>] sysc_noemu+0x10/0x16
     [<00000041616cb266>] 0x41616cb266
    
    The warning is
    --->    WARN_ON_ONCE(in_irq() || irqs_disabled());
    
    The network code must not be called with disabled interrupts but
    sys_setsid holds the tasklist_lock with spinlock_irq while calling the
    connector.
    
    After a discussion we agreed that we can move proc_sid_connector from
    __set_special_pids to sys_setsid.
    
    We also agreed that it is sufficient to change the check from
    task_session(curr) != pid into err > 0, since if we don't change the
    session, this means we were already the leader and return -EPERM.
    
    One last thing:
    There is also daemonize(), and some people might want to get a
    notification in that case. Since daemonize() is only needed if a user
    space does kernel_thread this does not look important (and there seems
    to be no consensus if this connector should be called in daemonize). If
    we really want this, we can add proc_sid_connector to daemonize() in an
    additional patch (Scott?)
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Scott James Remnant <scott@ubuntu.com>
    Cc: Matt Helsley <matthltc@us.ibm.com>
    Cc: David S. Miller <davem@davemloft.net>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Evgeniy Polyakov <zbr@ioremap.net>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e61891f80123..f7864ac2ecc1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -359,10 +359,8 @@ void __set_special_pids(struct pid *pid)
 {
 	struct task_struct *curr = current->group_leader;
 
-	if (task_session(curr) != pid) {
+	if (task_session(curr) != pid)
 		change_pid(curr, PIDTYPE_SID, pid);
-		proc_sid_connector(curr);
-	}
 
 	if (task_pgrp(curr) != pid)
 		change_pid(curr, PIDTYPE_PGID, pid);

commit f579bbcd9bb8b688df03191b92c56ab8af4d6322
Merge: e80fb7e52fd3 da085681014f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 8 12:16:35 2009 -0700

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      futex: fix requeue_pi key imbalance
      futex: Fix typo in FUTEX_WAIT/WAKE_BITSET_PRIVATE definitions
      rcu: Place root rcu_node structure in separate lockdep class
      rcu: Make hot-unplugged CPU relinquish its own RCU callbacks
      rcu: Move rcu_barrier() to rcutree
      futex: Move exit_pi_state() call to release_mm()
      futex: Nullify robust lists after cleanup
      futex: Fix locking imbalance
      panic: Fix panic message visibility by calling bust_spinlocks(0) before dying
      rcu: Replace the rcu_barrier enum with pointer to call_rcu*() function
      rcu: Clean up code based on review feedback from Josh Triplett, part 4
      rcu: Clean up code based on review feedback from Josh Triplett, part 3
      rcu: Fix rcu_lock_map build failure on CONFIG_PROVE_LOCKING=y
      rcu: Clean up code to address Ingo's checkpatch feedback
      rcu: Clean up code based on review feedback from Josh Triplett, part 2
      rcu: Clean up code based on review feedback from Josh Triplett

commit 322a2c100a8998158445599ea437fb556aa95b11
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Oct 5 18:18:03 2009 +0200

    futex: Move exit_pi_state() call to release_mm()
    
    exit_pi_state() is called from do_exit() but not from do_execve().
    Move it to release_mm() so it gets called from do_execve() as well.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    LKML-Reference: <new-submission>
    Cc: stable@kernel.org
    Cc: Anirban Sinha <ani@anirban.org>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ae5d8660ddff..bc2b1fdfc354 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -989,8 +989,6 @@ NORET_TYPE void do_exit(long code)
 	tsk->mempolicy = NULL;
 #endif
 #ifdef CONFIG_FUTEX
-	if (unlikely(!list_empty(&tsk->pi_state_list)))
-		exit_pi_state_list(tsk);
 	if (unlikely(current->pi_state_cache))
 		kfree(current->pi_state_cache);
 #endif

commit 801460d0cf5c5288153b722565773059b0f44348
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Wed Sep 23 15:57:41 2009 -0700

    task_struct cleanup: move binfmt field to mm_struct
    
    Because the binfmt is not different between threads in the same process,
    it can be moved from task_struct to mm_struct.  And binfmt moudle is
    handled per mm_struct instead of task_struct.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6c75ff83a8fe..5859f598c951 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -976,8 +976,6 @@ NORET_TYPE void do_exit(long code)
 		disassociate_ctty(1);
 
 	module_put(task_thread_info(tsk)->exec_domain->module);
-	if (tsk->binfmt)
-		module_put(tsk->binfmt->module);
 
 	proc_exit_connector(tsk);
 

commit b6fe2d117e98805ee76352e6468f87d494a97292
Author: Vitaly Mayatskikh <v.mayatskih@gmail.com>
Date:   Wed Sep 23 15:56:52 2009 -0700

    wait_noreap_copyout(): check for ->wo_info != NULL
    
    Current behaviour of sys_waitid() looks odd.  If user passes infop ==
    NULL, sys_waitid() returns success.  When user additionally specifies flag
    WNOWAIT, sys_waitid() returns -EFAULT on the same conditions.  When user
    combines WNOWAIT with WCONTINUED, sys_waitid() again returns success.
    
    This patch adds check for ->wo_info in wait_noreap_copyout().
    
    User-visible change: starting from this commit, sys_waitid() always checks
    infop != NULL and does not fail if it is NULL.
    
    Signed-off-by: Vitaly Mayatskikh <v.mayatskih@gmail.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2cc69eb8db2a..6c75ff83a8fe 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1140,18 +1140,20 @@ static int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,
 
 	put_task_struct(p);
 	infop = wo->wo_info;
-	if (!retval)
-		retval = put_user(SIGCHLD, &infop->si_signo);
-	if (!retval)
-		retval = put_user(0, &infop->si_errno);
-	if (!retval)
-		retval = put_user((short)why, &infop->si_code);
-	if (!retval)
-		retval = put_user(pid, &infop->si_pid);
-	if (!retval)
-		retval = put_user(uid, &infop->si_uid);
-	if (!retval)
-		retval = put_user(status, &infop->si_status);
+	if (infop) {
+		if (!retval)
+			retval = put_user(SIGCHLD, &infop->si_signo);
+		if (!retval)
+			retval = put_user(0, &infop->si_errno);
+		if (!retval)
+			retval = put_user((short)why, &infop->si_code);
+		if (!retval)
+			retval = put_user(pid, &infop->si_pid);
+		if (!retval)
+			retval = put_user(uid, &infop->si_uid);
+		if (!retval)
+			retval = put_user(status, &infop->si_status);
+	}
 	if (!retval)
 		retval = pid;
 	return retval;

commit dfe16dfa4ac178d9a10b489a73d535c6976e48d2
Author: Vitaly Mayatskikh <v.mayatskih@gmail.com>
Date:   Wed Sep 23 15:56:51 2009 -0700

    do_wait: fix sys_waitid()-specific behaviour
    
    do_wait() checks ->wo_info to figure out who is the caller.  If it's not
    NULL the caller should be sys_waitid(), in that case do_wait() fixes up
    the retval or zeros ->wo_info, depending on retval from underlying
    function.
    
    This is bug: user can pass ->wo_info == NULL and sys_waitid() will return
    incorrect value.
    
    man 2 waitid says:
    
            waitid(): returns 0 on success
    
    Test-case:
    
            int main(void)
            {
                    if (fork())
                            assert(waitid(P_ALL, 0, NULL, WEXITED) == 0);
    
                    return 0;
            }
    
    Result:
    
            Assertion `waitid(P_ALL, 0, ((void *)0), 4) == 0' failed.
    
    Move that code to sys_waitid().
    
    User-visible change: sys_waitid() will return 0 on success, either
    infop is set or not.
    
    Note, there's another bug in wait_noreap_copyout() which affects
    return value of sys_waitid(). It will be fixed in next patch.
    
    Signed-off-by: Vitaly Mayatskikh <v.mayatskih@gmail.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1daa7f46bccd..2cc69eb8db2a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1645,32 +1645,6 @@ static long do_wait(struct wait_opts *wo)
 end:
 	__set_current_state(TASK_RUNNING);
 	remove_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);
-
-	if (wo->wo_info) {
-		struct siginfo __user *infop = wo->wo_info;
-
-		if (retval > 0)
-			retval = 0;
-		else {
-			/*
-			 * For a WNOHANG return, clear out all the fields
-			 * we would set so the user can easily tell the
-			 * difference.
-			 */
-			if (!retval)
-				retval = put_user(0, &infop->si_signo);
-			if (!retval)
-				retval = put_user(0, &infop->si_errno);
-			if (!retval)
-				retval = put_user(0, &infop->si_code);
-			if (!retval)
-				retval = put_user(0, &infop->si_pid);
-			if (!retval)
-				retval = put_user(0, &infop->si_uid);
-			if (!retval)
-				retval = put_user(0, &infop->si_status);
-		}
-	}
 	return retval;
 }
 
@@ -1715,6 +1689,29 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 	wo.wo_stat	= NULL;
 	wo.wo_rusage	= ru;
 	ret = do_wait(&wo);
+
+	if (ret > 0) {
+		ret = 0;
+	} else if (infop) {
+		/*
+		 * For a WNOHANG return, clear out all the fields
+		 * we would set so the user can easily tell the
+		 * difference.
+		 */
+		if (!ret)
+			ret = put_user(0, &infop->si_signo);
+		if (!ret)
+			ret = put_user(0, &infop->si_errno);
+		if (!ret)
+			ret = put_user(0, &infop->si_code);
+		if (!ret)
+			ret = put_user(0, &infop->si_pid);
+		if (!ret)
+			ret = put_user(0, &infop->si_uid);
+		if (!ret)
+			ret = put_user(0, &infop->si_status);
+	}
+
 	put_pid(pid);
 
 	/* avoid REGPARM breakage on x86: */

commit b6e763f07fba6243d2a553ed9a4f3e10a789932a
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 23 15:56:50 2009 -0700

    wait_consider_task: kill "parent" argument
    
    Kill the unused "parent" argument in wait_consider_task(), it was never used.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Ratan Nalumasu <rnalumasu@gmail.com>
    Cc: Vitaly Mayatskikh <vmayatsk@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 650c1d1a55d0..1daa7f46bccd 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1481,8 +1481,8 @@ static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
  * then ->notask_error is 0 if @p is an eligible child,
  * or another error from security_task_wait(), or still -ECHILD.
  */
-static int wait_consider_task(struct wait_opts *wo, struct task_struct *parent,
-				int ptrace, struct task_struct *p)
+static int wait_consider_task(struct wait_opts *wo, int ptrace,
+				struct task_struct *p)
 {
 	int ret = eligible_child(wo, p);
 	if (!ret)
@@ -1550,7 +1550,7 @@ static int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)
 		 * Do not consider detached threads.
 		 */
 		if (!task_detached(p)) {
-			int ret = wait_consider_task(wo, tsk, 0, p);
+			int ret = wait_consider_task(wo, 0, p);
 			if (ret)
 				return ret;
 		}
@@ -1564,7 +1564,7 @@ static int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)
 	struct task_struct *p;
 
 	list_for_each_entry(p, &tsk->ptraced, ptrace_entry) {
-		int ret = wait_consider_task(wo, tsk, 1, p);
+		int ret = wait_consider_task(wo, 1, p);
 		if (ret)
 			return ret;
 	}

commit 989264f4645c183331a1279d513f4b1ddc06e1f5
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 23 15:56:49 2009 -0700

    do_wait-wakeup-optimization: simplify task_pid_type()
    
    task_pid_type() is only used by eligible_pid() which has to check wo_type
    != PIDTYPE_MAX anyway.  Remove this check from task_pid_type() and factor
    out ->pids[type] access, this shrinks .text a bit and simplifies the code.
    
    The matches the behaviour of other similar helpers, say get_task_pid().
    The caller must ensure that pid_type is valid, not the callee.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3fb9a77863d5..650c1d1a55d0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1101,17 +1101,15 @@ struct wait_opts {
 	int			notask_error;
 };
 
-static struct pid *task_pid_type(struct task_struct *task, enum pid_type type)
+static inline
+struct pid *task_pid_type(struct task_struct *task, enum pid_type type)
 {
-	struct pid *pid = NULL;
-	if (type == PIDTYPE_PID)
-		pid = task->pids[type].pid;
-	else if (type < PIDTYPE_MAX)
-		pid = task->group_leader->pids[type].pid;
-	return pid;
+	if (type != PIDTYPE_PID)
+		task = task->group_leader;
+	return task->pids[type].pid;
 }
 
-static inline int eligible_pid(struct wait_opts *wo, struct task_struct *p)
+static int eligible_pid(struct wait_opts *wo, struct task_struct *p)
 {
 	return	wo->wo_type == PIDTYPE_MAX ||
 		task_pid_type(p, wo->wo_type) == wo->wo_pid;

commit 5c01ba49e6647d86bc7576105f82027200d1f303
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 23 15:56:48 2009 -0700

    do_wait-wakeup-optimization: fix child_wait_callback()->eligible_child() usage
    
    child_wait_callback()->eligible_child() is not right, we can miss the
    wakeup if the task was detached before __wake_up_parent() and the caller
    of do_wait() didn't use __WALL.
    
    Move ->wo_pid checks from eligible_child() to the new helper,
    eligible_pid(), and change child_wait_callback() to use it instead of
    eligible_child().
    
    Note: actually I think it would be better to fix the __WCLONE check in
    eligible_child(), it doesn't look exactly right.  But it is not clear what
    is the supposed behaviour, and any change is user-visible.
    
    Reported-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Tested-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 270a68b7f22f..3fb9a77863d5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1111,13 +1111,16 @@ static struct pid *task_pid_type(struct task_struct *task, enum pid_type type)
 	return pid;
 }
 
-static int eligible_child(struct wait_opts *wo, struct task_struct *p)
+static inline int eligible_pid(struct wait_opts *wo, struct task_struct *p)
 {
-	if (wo->wo_type < PIDTYPE_MAX) {
-		if (task_pid_type(p, wo->wo_type) != wo->wo_pid)
-			return 0;
-	}
+	return	wo->wo_type == PIDTYPE_MAX ||
+		task_pid_type(p, wo->wo_type) == wo->wo_pid;
+}
 
+static int eligible_child(struct wait_opts *wo, struct task_struct *p)
+{
+	if (!eligible_pid(wo, p))
+		return 0;
 	/* Wait for all children (clone and not) if __WALL is set;
 	 * otherwise, wait for clone children *only* if __WCLONE is
 	 * set; otherwise, wait for non-clone children *only*.  (Note:
@@ -1578,7 +1581,7 @@ static int child_wait_callback(wait_queue_t *wait, unsigned mode,
 						child_wait);
 	struct task_struct *p = key;
 
-	if (!eligible_child(wo, p))
+	if (!eligible_pid(wo, p))
 		return 0;
 
 	if ((wo->wo_flags & __WNOTHREAD) && wait->private != p->parent)

commit b4fe51823d797d6959b2eee7868023e61606daa9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 23 15:56:47 2009 -0700

    do_wait() wakeup optimization: child_wait_callback: check __WNOTHREAD case
    
    Suggested by Roland.
    
    do_wait(__WNOTHREAD) can only succeed if the caller is either ptracer, or
    it is ->real_parent and the child is not traced. IOW, caller == p->parent
    otherwise we should not wake up.
    
    Change child_wait_callback() to check this. Ratan reports the workload with
    CPU load >99% caused by unnecessary wakeups, should be fixed by this patch.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Ratan Nalumasu <rnalumasu@gmail.com>
    Cc: Vitaly Mayatskikh <vmayatsk@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7838b4d68774..270a68b7f22f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1581,6 +1581,9 @@ static int child_wait_callback(wait_queue_t *wait, unsigned mode,
 	if (!eligible_child(wo, p))
 		return 0;
 
+	if ((wo->wo_flags & __WNOTHREAD) && wait->private != p->parent)
+		return 0;
+
 	return default_wake_function(wait, mode, sync, key);
 }
 

commit 0b7570e77f7c3abd43107dabc47ea89daf9a1cba
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 23 15:56:46 2009 -0700

    do_wait() wakeup optimization: change __wake_up_parent() to use filtered wakeup
    
    Ratan Nalumasu reported that in a process with many threads doing
    unnecessary wakeups.  Every waiting thread in the process wakes up to loop
    through the children and see that the only ones it cares about are still
    not ready.
    
    Now that we have struct wait_opts we can change do_wait/__wake_up_parent
    to use filtered wakeups.
    
    We can make child_wait_callback() more clever later, right now it only
    checks eligible_child().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Ratan Nalumasu <rnalumasu@gmail.com>
    Cc: Vitaly Mayatskikh <vmayatsk@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Tested-by: Valdis Kletnieks <valdis.kletnieks@vt.edu>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ef2dfa818bf1..7838b4d68774 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1097,6 +1097,7 @@ struct wait_opts {
 	int __user		*wo_stat;
 	struct rusage __user	*wo_rusage;
 
+	wait_queue_t		child_wait;
 	int			notask_error;
 };
 
@@ -1570,20 +1571,35 @@ static int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)
 	return 0;
 }
 
+static int child_wait_callback(wait_queue_t *wait, unsigned mode,
+				int sync, void *key)
+{
+	struct wait_opts *wo = container_of(wait, struct wait_opts,
+						child_wait);
+	struct task_struct *p = key;
+
+	if (!eligible_child(wo, p))
+		return 0;
+
+	return default_wake_function(wait, mode, sync, key);
+}
+
 void __wake_up_parent(struct task_struct *p, struct task_struct *parent)
 {
-	wake_up_interruptible_sync(&parent->signal->wait_chldexit);
+	__wake_up_sync_key(&parent->signal->wait_chldexit,
+				TASK_INTERRUPTIBLE, 1, p);
 }
 
 static long do_wait(struct wait_opts *wo)
 {
-	DECLARE_WAITQUEUE(wait, current);
 	struct task_struct *tsk;
 	int retval;
 
 	trace_sched_process_wait(wo->wo_pid);
 
-	add_wait_queue(&current->signal->wait_chldexit,&wait);
+	init_waitqueue_func_entry(&wo->child_wait, child_wait_callback);
+	wo->child_wait.private = current;
+	add_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);
 repeat:
 	/*
 	 * If there is nothing that can match our critiera just get out.
@@ -1624,7 +1640,8 @@ static long do_wait(struct wait_opts *wo)
 	}
 end:
 	__set_current_state(TASK_RUNNING);
-	remove_wait_queue(&current->signal->wait_chldexit,&wait);
+	remove_wait_queue(&current->signal->wait_chldexit, &wo->child_wait);
+
 	if (wo->wo_info) {
 		struct siginfo __user *infop = wo->wo_info;
 

commit a2322e1d272938d192d8c24cdacf57c0c7a2683f
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 23 15:56:45 2009 -0700

    do_wait() wakeup optimization: shift security_task_wait() from eligible_child() to wait_consider_task()
    
    Preparation, no functional changes.
    
    eligible_child() has a single caller, wait_consider_task(). We can move
    security_task_wait() out from eligible_child(), this allows us to use it
    for filtered wake_up().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Ratan Nalumasu <rnalumasu@gmail.com>
    Cc: Vitaly Mayatskikh <vmayatsk@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 782b2e1f7ca2..ef2dfa818bf1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1112,8 +1112,6 @@ static struct pid *task_pid_type(struct task_struct *task, enum pid_type type)
 
 static int eligible_child(struct wait_opts *wo, struct task_struct *p)
 {
-	int err;
-
 	if (wo->wo_type < PIDTYPE_MAX) {
 		if (task_pid_type(p, wo->wo_type) != wo->wo_pid)
 			return 0;
@@ -1128,10 +1126,6 @@ static int eligible_child(struct wait_opts *wo, struct task_struct *p)
 	    && !(wo->wo_flags & __WALL))
 		return 0;
 
-	err = security_task_wait(p);
-	if (err)
-		return err;
-
 	return 1;
 }
 
@@ -1492,6 +1486,7 @@ static int wait_consider_task(struct wait_opts *wo, struct task_struct *parent,
 	if (!ret)
 		return ret;
 
+	ret = security_task_wait(p);
 	if (unlikely(ret < 0)) {
 		/*
 		 * If we have not yet seen any eligible child,

commit a7f0765edfd53aed09cb7b0e15863688b39447de
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Sep 23 15:56:44 2009 -0700

    ptrace: __ptrace_detach: do __wake_up_parent() if we reap the tracee
    
    The bug is old, it wasn't cause by recent changes.
    
    Test case:
    
            static void *tfunc(void *arg)
            {
                    int pid = (long)arg;
    
                    assert(ptrace(PTRACE_ATTACH, pid, NULL, NULL) == 0);
                    kill(pid, SIGKILL);
    
                    sleep(1);
                    return NULL;
            }
    
            int main(void)
            {
                    pthread_t th;
                    long pid = fork();
    
                    if (!pid)
                            pause();
    
                    signal(SIGCHLD, SIG_IGN);
                    assert(pthread_create(&th, NULL, tfunc, (void*)pid) == 0);
    
                    int r = waitpid(-1, NULL, __WNOTHREAD);
                    printf("waitpid: %d %m\n", r);
    
                    return 0;
            }
    
    Before the patch this program hangs, after this patch waitpid() correctly
    fails with errno == -ECHILD.
    
    The problem is, __ptrace_detach() reaps the EXIT_ZOMBIE tracee if its
    ->real_parent is our sub-thread and we ignore SIGCHLD.  But in this case
    we should wake up other threads which can sleep in do_wait().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Vitaly Mayatskikh <vmayatsk@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 60d6fdcc9265..782b2e1f7ca2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1575,6 +1575,11 @@ static int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)
 	return 0;
 }
 
+void __wake_up_parent(struct task_struct *p, struct task_struct *parent)
+{
+	wake_up_interruptible_sync(&parent->signal->wait_chldexit);
+}
+
 static long do_wait(struct wait_opts *wo)
 {
 	DECLARE_WAITQUEUE(wait, current);

commit 1f10206cf8e945220f7220a809d8bfc15c21f9a5
Author: Jiri Pirko <jpirko@redhat.com>
Date:   Tue Sep 22 16:44:10 2009 -0700

    getrusage: fill ru_maxrss value
    
    Make ->ru_maxrss value in struct rusage filled accordingly to rss hiwater
    mark.  This struct is filled as a parameter to getrusage syscall.
    ->ru_maxrss value is set to KBs which is the way it is done in BSD
    systems.  /usr/bin/time (gnu time) application converts ->ru_maxrss to KBs
    which seems to be incorrect behavior.  Maintainer of this util was
    notified by me with the patch which corrects it and cc'ed.
    
    To make this happen we extend struct signal_struct by two fields.  The
    first one is ->maxrss which we use to store rss hiwater of the task.  The
    second one is ->cmaxrss which we use to store highest rss hiwater of all
    task childs.  These values are used in k_getrusage() to actually fill
    ->ru_maxrss.  k_getrusage() uses current rss hiwater value directly if mm
    struct exists.
    
    Note:
    exec() clear mm->hiwater_rss, but doesn't clear sig->maxrss.
    it is intetionally behavior. *BSD getrusage have exec() inheriting.
    
    test programs
    ========================================================
    
    getrusage.c
    ===========
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
     #include <sys/types.h>
     #include <sys/time.h>
     #include <sys/resource.h>
     #include <sys/types.h>
     #include <sys/wait.h>
     #include <unistd.h>
     #include <signal.h>
     #include <sys/mman.h>
    
     #include "common.h"
    
     #define err(str) perror(str), exit(1)
    
    int main(int argc, char** argv)
    {
            int status;
    
            printf("allocate 100MB\n");
            consume(100);
    
            printf("testcase1: fork inherit? \n");
            printf("  expect: initial.self ~= child.self\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
            } else {
                    show_rusage("fork child");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase2: fork inherit? (cont.) \n");
            printf("  expect: initial.children ~= 100MB, but child.children = 0\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
            } else {
                    show_rusage("child");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase3: fork + malloc \n");
            printf("  expect: child.self ~= initial.self + 50MB\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
            } else {
                    printf("allocate +50MB\n");
                    consume(50);
                    show_rusage("fork child");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase4: grandchild maxrss\n");
            printf("  expect: post_wait.children ~= 300MB\n");
            show_rusage("initial");
            if (__fork()) {
                    wait(&status);
                    show_rusage("post_wait");
            } else {
                    system("./child -n 0 -g 300");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase5: zombie\n");
            printf("  expect: pre_wait ~= initial, IOW the zombie process is not accounted.\n");
            printf("          post_wait ~= 400MB, IOW wait() collect child's max_rss. \n");
            show_rusage("initial");
            if (__fork()) {
                    sleep(1); /* children become zombie */
                    show_rusage("pre_wait");
                    wait(&status);
                    show_rusage("post_wait");
            } else {
                    system("./child -n 400");
                    _exit(0);
            }
            printf("\n");
    
            printf("testcase6: SIG_IGN\n");
            printf("  expect: initial ~= after_zombie (child's 500MB alloc should be ignored).\n");
            show_rusage("initial");
            signal(SIGCHLD, SIG_IGN);
            if (__fork()) {
                    sleep(1); /* children become zombie */
                    show_rusage("after_zombie");
            } else {
                    system("./child -n 500");
                    _exit(0);
            }
            printf("\n");
            signal(SIGCHLD, SIG_DFL);
    
            printf("testcase7: exec (without fork) \n");
            printf("  expect: initial ~= exec \n");
            show_rusage("initial");
            execl("./child", "child", "-v", NULL);
    
            return 0;
    }
    
    child.c
    =======
     #include <sys/types.h>
     #include <unistd.h>
     #include <sys/types.h>
     #include <sys/wait.h>
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
     #include <sys/types.h>
     #include <sys/time.h>
     #include <sys/resource.h>
    
     #include "common.h"
    
    int main(int argc, char** argv)
    {
            int status;
            int c;
            long consume_size = 0;
            long grandchild_consume_size = 0;
            int show = 0;
    
            while ((c = getopt(argc, argv, "n:g:v")) != -1) {
                    switch (c) {
                    case 'n':
                            consume_size = atol(optarg);
                            break;
                    case 'v':
                            show = 1;
                            break;
                    case 'g':
    
                            grandchild_consume_size = atol(optarg);
                            break;
                    default:
                            break;
                    }
            }
    
            if (show)
                    show_rusage("exec");
    
            if (consume_size) {
                    printf("child alloc %ldMB\n", consume_size);
                    consume(consume_size);
            }
    
            if (grandchild_consume_size) {
                    if (fork()) {
                            wait(&status);
                    } else {
                            printf("grandchild alloc %ldMB\n", grandchild_consume_size);
                            consume(grandchild_consume_size);
    
                            exit(0);
                    }
            }
    
            return 0;
    }
    
    common.c
    ========
     #include <stdio.h>
     #include <stdlib.h>
     #include <string.h>
     #include <sys/types.h>
     #include <sys/time.h>
     #include <sys/resource.h>
     #include <sys/types.h>
     #include <sys/wait.h>
     #include <unistd.h>
     #include <signal.h>
     #include <sys/mman.h>
    
     #include "common.h"
     #define err(str) perror(str), exit(1)
    
    void show_rusage(char *prefix)
    {
            int err, err2;
            struct rusage rusage_self;
            struct rusage rusage_children;
    
            printf("%s: ", prefix);
            err = getrusage(RUSAGE_SELF, &rusage_self);
            if (!err)
                    printf("self %ld ", rusage_self.ru_maxrss);
            err2 = getrusage(RUSAGE_CHILDREN, &rusage_children);
            if (!err2)
                    printf("children %ld ", rusage_children.ru_maxrss);
    
            printf("\n");
    }
    
    /* Some buggy OS need this worthless CPU waste. */
    void make_pagefault(void)
    {
            void *addr;
            int size = getpagesize();
            int i;
    
            for (i=0; i<1000; i++) {
                    addr = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANON, -1, 0);
                    if (addr == MAP_FAILED)
                            err("make_pagefault");
                    memset(addr, 0, size);
                    munmap(addr, size);
            }
    }
    
    void consume(int mega)
    {
            size_t sz = mega * 1024 * 1024;
            void *ptr;
    
            ptr = malloc(sz);
            memset(ptr, 0, sz);
            make_pagefault();
    }
    
    pid_t __fork(void)
    {
            pid_t pid;
    
            pid = fork();
            make_pagefault();
    
            return pid;
    }
    
    common.h
    ========
    void show_rusage(char *prefix);
    void make_pagefault(void);
    void consume(int mega);
    pid_t __fork(void);
    
    FreeBSD result (expected result)
    ========================================================
    allocate 100MB
    testcase1: fork inherit?
      expect: initial.self ~= child.self
    initial: self 103492 children 0
    fork child: self 103540 children 0
    
    testcase2: fork inherit? (cont.)
      expect: initial.children ~= 100MB, but child.children = 0
    initial: self 103540 children 103540
    child: self 103564 children 0
    
    testcase3: fork + malloc
      expect: child.self ~= initial.self + 50MB
    initial: self 103564 children 103564
    allocate +50MB
    fork child: self 154860 children 0
    
    testcase4: grandchild maxrss
      expect: post_wait.children ~= 300MB
    initial: self 103564 children 154860
    grandchild alloc 300MB
    post_wait: self 103564 children 308720
    
    testcase5: zombie
      expect: pre_wait ~= initial, IOW the zombie process is not accounted.
              post_wait ~= 400MB, IOW wait() collect child's max_rss.
    initial: self 103564 children 308720
    child alloc 400MB
    pre_wait: self 103564 children 308720
    post_wait: self 103564 children 411312
    
    testcase6: SIG_IGN
      expect: initial ~= after_zombie (child's 500MB alloc should be ignored).
    initial: self 103564 children 411312
    child alloc 500MB
    after_zombie: self 103624 children 411312
    
    testcase7: exec (without fork)
      expect: initial ~= exec
    initial: self 103624 children 411312
    exec: self 103624 children 411312
    
    Linux result (actual test result)
    ========================================================
    allocate 100MB
    testcase1: fork inherit?
      expect: initial.self ~= child.self
    initial: self 102848 children 0
    fork child: self 102572 children 0
    
    testcase2: fork inherit? (cont.)
      expect: initial.children ~= 100MB, but child.children = 0
    initial: self 102876 children 102644
    child: self 102572 children 0
    
    testcase3: fork + malloc
      expect: child.self ~= initial.self + 50MB
    initial: self 102876 children 102644
    allocate +50MB
    fork child: self 153804 children 0
    
    testcase4: grandchild maxrss
      expect: post_wait.children ~= 300MB
    initial: self 102876 children 153864
    grandchild alloc 300MB
    post_wait: self 102876 children 307536
    
    testcase5: zombie
      expect: pre_wait ~= initial, IOW the zombie process is not accounted.
              post_wait ~= 400MB, IOW wait() collect child's max_rss.
    initial: self 102876 children 307536
    child alloc 400MB
    pre_wait: self 102876 children 307536
    post_wait: self 102876 children 410076
    
    testcase6: SIG_IGN
      expect: initial ~= after_zombie (child's 500MB alloc should be ignored).
    initial: self 102876 children 410076
    child alloc 500MB
    after_zombie: self 102880 children 410076
    
    testcase7: exec (without fork)
      expect: initial ~= exec
    initial: self 102880 children 410076
    exec: self 102880 children 410076
    
    Signed-off-by: Jiri Pirko <jpirko@redhat.com>
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Hugh Dickins <hugh.dickins@tiscali.co.uk>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 61bb1761c7b8..60d6fdcc9265 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -947,6 +947,8 @@ NORET_TYPE void do_exit(long code)
 	if (group_dead) {
 		hrtimer_cancel(&tsk->signal->real_timer);
 		exit_itimers(tsk->signal);
+		if (tsk->mm)
+			setmax_mm_hiwater_rss(&tsk->signal->maxrss, tsk->mm);
 	}
 	acct_collect(code, group_dead);
 	if (group_dead)
@@ -1210,6 +1212,7 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	if (likely(!traced) && likely(!task_detached(p))) {
 		struct signal_struct *psig;
 		struct signal_struct *sig;
+		unsigned long maxrss;
 
 		/*
 		 * The resource counters for the group leader are in its
@@ -1258,6 +1261,9 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 		psig->coublock +=
 			task_io_get_oublock(p) +
 			sig->oublock + sig->coublock;
+		maxrss = max(sig->maxrss, sig->cmaxrss);
+		if (psig->cmaxrss < maxrss)
+			psig->cmaxrss = maxrss;
 		task_io_accounting_add(&psig->ioac, &p->ioac);
 		task_io_accounting_add(&psig->ioac, &sig->ioac);
 		spin_unlock_irq(&p->real_parent->sighand->siglock);

commit 02b51df1b07b4e9ca823c89284e704cadb323cd1
Author: Scott James Remnant <scott@ubuntu.com>
Date:   Tue Sep 22 16:43:44 2009 -0700

    proc connector: add event for process becoming session leader
    
    The act of a process becoming a session leader is a useful signal to a
    supervising init daemon such as Upstart.
    
    While a daemon will normally do this as part of the process of becoming a
    daemon, it is rare for its children to do so.  When the children do, it is
    nearly always a sign that the child should be considered detached from the
    parent and not supervised along with it.
    
    The poster-child example is OpenSSH; the per-login children call setsid()
    so that they may control the pty connected to them.  If the primary daemon
    dies or is restarted, we do not want to consider the per-login children
    and want to respawn the primary daemon without killing the children.
    
    This patch adds a new PROC_SID_EVENT and associated structure to the
    proc_event event_data union, it arranges for this to be emitted when the
    special PIDTYPE_SID pid is set.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Scott James Remnant <scott@ubuntu.com>
    Acked-by: Matt Helsley <matthltc@us.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Evgeniy Polyakov <johnpol@2ka.mipt.ru>
    Acked-by: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e47ee8a06135..61bb1761c7b8 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -359,8 +359,10 @@ void __set_special_pids(struct pid *pid)
 {
 	struct task_struct *curr = current->group_leader;
 
-	if (task_session(curr) != pid)
+	if (task_session(curr) != pid) {
 		change_pid(curr, PIDTYPE_SID, pid);
+		proc_sid_connector(curr);
+	}
 
 	if (task_pgrp(curr) != pid)
 		change_pid(curr, PIDTYPE_PGID, pid);

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index ae5d8660ddff..e47ee8a06135 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -47,7 +47,7 @@
 #include <linux/tracehook.h>
 #include <linux/fs_struct.h>
 #include <linux/init_task.h>
-#include <linux/perf_counter.h>
+#include <linux/perf_event.h>
 #include <trace/events/sched.h>
 
 #include <asm/uaccess.h>
@@ -154,8 +154,8 @@ static void delayed_put_task_struct(struct rcu_head *rhp)
 {
 	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
 
-#ifdef CONFIG_PERF_COUNTERS
-	WARN_ON_ONCE(tsk->perf_counter_ctxp);
+#ifdef CONFIG_PERF_EVENTS
+	WARN_ON_ONCE(tsk->perf_event_ctxp);
 #endif
 	trace_sched_process_free(tsk);
 	put_task_struct(tsk);
@@ -981,7 +981,7 @@ NORET_TYPE void do_exit(long code)
 	 * Flush inherited counters to the parent - before the parent
 	 * gets woken up by child-exit notifications.
 	 */
-	perf_counter_exit_task(tsk);
+	perf_event_exit_task(tsk);
 
 	exit_notify(tsk, group_dead);
 #ifdef CONFIG_NUMA

commit eee2775d9924b22643bd89b2e568cc5eed7e8a04
Merge: 53e16fbd3000 7db905e636f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 11 13:20:18 2009 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (28 commits)
      rcu: Move end of special early-boot RCU operation earlier
      rcu: Changes from reviews: avoid casts, fix/add warnings, improve comments
      rcu: Create rcutree plugins to handle hotplug CPU for multi-level trees
      rcu: Remove lockdep annotations from RCU's _notrace() API members
      rcu: Add #ifdef to suppress __rcu_offline_cpu() warning in !HOTPLUG_CPU builds
      rcu: Add CPU-offline processing for single-node configurations
      rcu: Add "notrace" to RCU function headers used by ftrace
      rcu: Remove CONFIG_PREEMPT_RCU
      rcu: Merge preemptable-RCU functionality into hierarchical RCU
      rcu: Simplify rcu_pending()/rcu_check_callbacks() API
      rcu: Use debugfs_remove_recursive() simplify code.
      rcu: Merge per-RCU-flavor initialization into pre-existing macro
      rcu: Fix online/offline indication for rcudata.csv trace file
      rcu: Consolidate sparse and lockdep declarations in include/linux/rcupdate.h
      rcu: Renamings to increase RCU clarity
      rcu: Move private definitions from include/linux/rcutree.h to kernel/rcutree.h
      rcu: Expunge lingering references to CONFIG_CLASSIC_RCU, optimize on !SMP
      rcu: Delay rcu_barrier() wait until beginning of next CPU-hotunplug operation.
      rcu: Fix typo in rcu_irq_exit() comment header
      rcu: Make rcupreempt_trace.c look at offline CPUs
      ...

commit e0e817392b9acf2c98d3be80c233dddb1b52003d
Author: David Howells <dhowells@redhat.com>
Date:   Wed Sep 2 09:13:40 2009 +0100

    CRED: Add some configurable debugging [try #6]
    
    Add a config option (CONFIG_DEBUG_CREDENTIALS) to turn on some debug checking
    for credential management.  The additional code keeps track of the number of
    pointers from task_structs to any given cred struct, and checks to see that
    this number never exceeds the usage count of the cred struct (which includes
    all references, not just those from task_structs).
    
    Furthermore, if SELinux is enabled, the code also checks that the security
    pointer in the cred struct is never seen to be invalid.
    
    This attempts to catch the bug whereby inode_has_perm() faults in an nfsd
    kernel thread on seeing cred->security be a NULL pointer (it appears that the
    credential struct has been previously released):
    
            http://www.kerneloops.org/oops.php?number=252883
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 869dc221733e..c98ff7a8025f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -901,6 +901,8 @@ NORET_TYPE void do_exit(long code)
 
 	tracehook_report_exit(&code);
 
+	validate_creds_for_do_exit(tsk);
+
 	/*
 	 * We're taking recursive faults here in do_exit. Safest is to just
 	 * leave this task alone and wait for reboot.
@@ -1009,6 +1011,8 @@ NORET_TYPE void do_exit(long code)
 	if (tsk->splice_pipe)
 		__free_pipe_info(tsk->splice_pipe);
 
+	validate_creds_for_do_exit(tsk);
+
 	preempt_disable();
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;

commit f41d911f8c49a5d65c86504c19e8204bb605c4fd
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Aug 22 13:56:52 2009 -0700

    rcu: Merge preemptable-RCU functionality into hierarchical RCU
    
    Create a kernel/rcutree_plugin.h file that contains definitions
    for preemptable RCU (or, under the #else branch of the #ifdef,
    empty definitions for the classic non-preemptable semantics).
    These definitions fit into plugins defined in kernel/rcutree.c
    for this purpose.
    
    This variant of preemptable RCU uses a new algorithm whose
    read-side expense is roughly that of classic hierarchical RCU
    under CONFIG_PREEMPT. This new algorithm's update-side expense
    is similar to that of classic hierarchical RCU, and, in absence
    of read-side preemption or blocking, is exactly that of classic
    hierarchical RCU.  Perhaps more important, this new algorithm
    has a much simpler implementation, saving well over 1,000 lines
    of code compared to mainline's implementation of preemptable
    RCU, which will hopefully be retired in favor of this new
    algorithm.
    
    The simplifications are obtained by maintaining per-task
    nesting state for running tasks, and using a simple
    lock-protected algorithm to handle accounting when tasks block
    within RCU read-side critical sections, making use of lessons
    learned while creating numerous user-level RCU implementations
    over the past 18 months.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: laijs@cn.fujitsu.com
    Cc: dipankar@in.ibm.com
    Cc: akpm@linux-foundation.org
    Cc: mathieu.desnoyers@polymtl.ca
    Cc: josht@linux.vnet.ibm.com
    Cc: dvhltc@us.ibm.com
    Cc: niv@us.ibm.com
    Cc: peterz@infradead.org
    Cc: rostedt@goodmis.org
    LKML-Reference: <12509746134003-git-send-email->
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 869dc221733e..263f95ed7201 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1010,6 +1010,7 @@ NORET_TYPE void do_exit(long code)
 		__free_pipe_info(tsk->splice_pipe);
 
 	preempt_disable();
+	exit_rcu();
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;
 	schedule();

commit b43f3cbd21ffbd719fd4fa6642bfe6af255ded34
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Jul 8 01:54:37 2009 +0400

    headers: mnt_namespace.h redux
    
    Fix various silly problems wrt mnt_namespace.h:
    
     - exit_mnt_ns() isn't used, remove it
     - done that, sched.h and nsproxy.h inclusions aren't needed
     - mount.h inclusion was need for vfsmount_lock, but no longer
     - remove mnt_namespace.h inclusion from files which don't use anything
       from mnt_namespace.h
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 628d41f0dd54..869dc221733e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -12,7 +12,6 @@
 #include <linux/completion.h>
 #include <linux/personality.h>
 #include <linux/tty.h>
-#include <linux/mnt_namespace.h>
 #include <linux/iocontext.h>
 #include <linux/key.h>
 #include <linux/security.h>

commit befca96779b0259ac8fad0183e748a62935c39cb
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jun 18 16:49:11 2009 -0700

    ptrace: wait_task_zombie: do not account traced sub-threads
    
    The bug is ancient.
    
    If we trace the sub-thread of our natural child and this sub-thread exits,
    we update parent->signal->cxxx fields.  But we should not do this until
    the whole thread-group exits, otherwise we account this thread (and all
    other live threads) twice.
    
    Add the task_detached() check.  No need to check thread_group_empty(),
    wait_consider_task()->delay_group_leader() already did this.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Vitaly Mayatskikh <vmayatsk@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 13ae64001fec..628d41f0dd54 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1197,8 +1197,11 @@ static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 	}
 
 	traced = ptrace_reparented(p);
-
-	if (likely(!traced)) {
+	/*
+	 * It can be ptraced but not reparented, check
+	 * !task_detached() to filter out sub-threads.
+	 */
+	if (likely(!traced) && likely(!task_detached(p))) {
 		struct signal_struct *psig;
 		struct signal_struct *sig;
 

commit e1eb1ebcca871673c76caf63335c4237680040f1
Author: Richard Kennedy <richard@rsk.demon.co.uk>
Date:   Wed Jun 17 16:27:42 2009 -0700

    mm: exit.c reorder wait_opts to remove padding on 64 bit builds
    
    Reorder struct wait_opts to remove 8 bytes of alignment padding on 64 bit
    builds.
    
    Signed-off-by: Richard Kennedy <richard@rsk.demon.co.uk>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7ef355dd3dca..13ae64001fec 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1082,8 +1082,8 @@ SYSCALL_DEFINE1(exit_group, int, error_code)
 
 struct wait_opts {
 	enum pid_type		wo_type;
-	struct pid		*wo_pid;
 	int			wo_flags;
+	struct pid		*wo_pid;
 
 	struct siginfo __user	*wo_info;
 	int __user		*wo_stat;

commit f95d39d10fe7d47336e65172f52bf64e0096f983
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:42 2009 -0700

    do_wait: fix the theoretical race with stop/trace/cont
    
    do_wait:
    
            current->state = TASK_INTERRUPTIBLE;
    
            read_lock(&tasklist_lock);
            ... search for the task to reap ...
    
    In theory, the ->state changing can leak into the critical section.  Since
    the child can change its status under read_lock(tasklist) in parallel
    (finish_stop/ptrace_stop), we can miss the wakeup if __wake_up_parent()
    sees us in TASK_RUNNING state.  Add the barrier.
    
    Also, use __set_current_state() to set TASK_RUNNING.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index dd83c8419101..7ef355dd3dca 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1581,7 +1581,7 @@ static long do_wait(struct wait_opts *wo)
 	   (!wo->wo_pid || hlist_empty(&wo->wo_pid->tasks[wo->wo_type])))
 		goto notask;
 
-	current->state = TASK_INTERRUPTIBLE;
+	set_current_state(TASK_INTERRUPTIBLE);
 	read_lock(&tasklist_lock);
 	tsk = current;
 	do {
@@ -1608,7 +1608,7 @@ static long do_wait(struct wait_opts *wo)
 		}
 	}
 end:
-	current->state = TASK_RUNNING;
+	__set_current_state(TASK_RUNNING);
 	remove_wait_queue(&current->signal->wait_chldexit,&wait);
 	if (wo->wo_info) {
 		struct siginfo __user *infop = wo->wo_info;

commit a3f6dfb7295facb0505b5beca5a7ce48b0612379
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:41 2009 -0700

    do_wait: kill the old BUG_ON, use while_each_thread()
    
    do_wait() does BUG_ON(tsk->signal != current->signal), this looks like a
    raher obsolete check.  At least, I don't think do_wait() is the best place
    to verify that all threads have the same ->signal.  Remove it.
    
    Also, change the code to use while_each_thread().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9c6881a0a8b4..dd83c8419101 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1595,9 +1595,7 @@ static long do_wait(struct wait_opts *wo)
 
 		if (wo->wo_flags & __WNOTHREAD)
 			break;
-		tsk = next_thread(tsk);
-		BUG_ON(tsk->signal != current->signal);
-	} while (tsk != current);
+	} while_each_thread(current, tsk);
 	read_unlock(&tasklist_lock);
 
 notask:

commit 64a16caf5e3417ee32f670debcb5857b02a9e08e
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:40 2009 -0700

    do_wait: simplify retval/tsk_result/notask_error mess
    
    Now that we don't pass &retval down to other helpers we can simplify
    the code more.
    
    - kill tsk_result, just use retval
    
    - add the "notask" label right after the main loop, and
      s/got end/goto notask/ after the fastpath pid check.
    
      This way we don't need to initialize retval before this
      check and the code becomes a bit more clean, if this pid
      has no attached tasks we should just skip the list search.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 29622e468b7f..9c6881a0a8b4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1576,27 +1576,22 @@ static long do_wait(struct wait_opts *wo)
 	 * might later match our criteria, even if we are not able to reap
 	 * it yet.
 	 */
-	retval = wo->notask_error = -ECHILD;
+	wo->notask_error = -ECHILD;
 	if ((wo->wo_type < PIDTYPE_MAX) &&
 	   (!wo->wo_pid || hlist_empty(&wo->wo_pid->tasks[wo->wo_type])))
-		goto end;
+		goto notask;
 
 	current->state = TASK_INTERRUPTIBLE;
 	read_lock(&tasklist_lock);
 	tsk = current;
 	do {
-		int tsk_result = do_wait_thread(wo, tsk);
-
-		if (!tsk_result)
-			tsk_result = ptrace_do_wait(wo, tsk);
+		retval = do_wait_thread(wo, tsk);
+		if (retval)
+			goto end;
 
-		if (tsk_result) {
-			/*
-			 * tasklist_lock is unlocked and we have a final result.
-			 */
-			retval = tsk_result;
+		retval = ptrace_do_wait(wo, tsk);
+		if (retval)
 			goto end;
-		}
 
 		if (wo->wo_flags & __WNOTHREAD)
 			break;
@@ -1605,6 +1600,7 @@ static long do_wait(struct wait_opts *wo)
 	} while (tsk != current);
 	read_unlock(&tasklist_lock);
 
+notask:
 	retval = wo->notask_error;
 	if (!retval && !(wo->wo_flags & WNOHANG)) {
 		retval = -ERESTARTSYS;

commit 9e8ae01d1c86dcaa6443c897662545d088036e4c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:39 2009 -0700

    introduce "struct wait_opts" to simplify do_wait() patches
    
    Introduce "struct wait_opts" which holds the parameters for misc helpers
    in do_wait() pathes.
    
    This adds 13 lines to kernel/exit.c, but saves 256 bytes from .o and imho
    makes the code much more readable.
    
    This patch temporary uglifies rusage/siginfo code a little bit, will be
    addressed by further cleanups.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index fd781b56401d..29622e468b7f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1080,6 +1080,18 @@ SYSCALL_DEFINE1(exit_group, int, error_code)
 	return 0;
 }
 
+struct wait_opts {
+	enum pid_type		wo_type;
+	struct pid		*wo_pid;
+	int			wo_flags;
+
+	struct siginfo __user	*wo_info;
+	int __user		*wo_stat;
+	struct rusage __user	*wo_rusage;
+
+	int			notask_error;
+};
+
 static struct pid *task_pid_type(struct task_struct *task, enum pid_type type)
 {
 	struct pid *pid = NULL;
@@ -1090,13 +1102,12 @@ static struct pid *task_pid_type(struct task_struct *task, enum pid_type type)
 	return pid;
 }
 
-static int eligible_child(enum pid_type type, struct pid *pid, int options,
-			  struct task_struct *p)
+static int eligible_child(struct wait_opts *wo, struct task_struct *p)
 {
 	int err;
 
-	if (type < PIDTYPE_MAX) {
-		if (task_pid_type(p, type) != pid)
+	if (wo->wo_type < PIDTYPE_MAX) {
+		if (task_pid_type(p, wo->wo_type) != wo->wo_pid)
 			return 0;
 	}
 
@@ -1105,8 +1116,8 @@ static int eligible_child(enum pid_type type, struct pid *pid, int options,
 	 * set; otherwise, wait for non-clone children *only*.  (Note:
 	 * A "clone" child here is one that reports to its parent
 	 * using a signal other than SIGCHLD.) */
-	if (((p->exit_signal != SIGCHLD) ^ ((options & __WCLONE) != 0))
-	    && !(options & __WALL))
+	if (((p->exit_signal != SIGCHLD) ^ !!(wo->wo_flags & __WCLONE))
+	    && !(wo->wo_flags & __WALL))
 		return 0;
 
 	err = security_task_wait(p);
@@ -1116,14 +1127,15 @@ static int eligible_child(enum pid_type type, struct pid *pid, int options,
 	return 1;
 }
 
-static int wait_noreap_copyout(struct task_struct *p, pid_t pid, uid_t uid,
-			       int why, int status,
-			       struct siginfo __user *infop,
-			       struct rusage __user *rusagep)
+static int wait_noreap_copyout(struct wait_opts *wo, struct task_struct *p,
+				pid_t pid, uid_t uid, int why, int status)
 {
-	int retval = rusagep ? getrusage(p, RUSAGE_BOTH, rusagep) : 0;
+	struct siginfo __user *infop;
+	int retval = wo->wo_rusage
+		? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
 
 	put_task_struct(p);
+	infop = wo->wo_info;
 	if (!retval)
 		retval = put_user(SIGCHLD, &infop->si_signo);
 	if (!retval)
@@ -1147,19 +1159,18 @@ static int wait_noreap_copyout(struct task_struct *p, pid_t pid, uid_t uid,
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_zombie(struct task_struct *p, int options,
-			    struct siginfo __user *infop,
-			    int __user *stat_addr, struct rusage __user *ru)
+static int wait_task_zombie(struct wait_opts *wo, struct task_struct *p)
 {
 	unsigned long state;
 	int retval, status, traced;
 	pid_t pid = task_pid_vnr(p);
 	uid_t uid = __task_cred(p)->uid;
+	struct siginfo __user *infop;
 
-	if (!likely(options & WEXITED))
+	if (!likely(wo->wo_flags & WEXITED))
 		return 0;
 
-	if (unlikely(options & WNOWAIT)) {
+	if (unlikely(wo->wo_flags & WNOWAIT)) {
 		int exit_code = p->exit_code;
 		int why, status;
 
@@ -1172,8 +1183,7 @@ static int wait_task_zombie(struct task_struct *p, int options,
 			why = (exit_code & 0x80) ? CLD_DUMPED : CLD_KILLED;
 			status = exit_code & 0x7f;
 		}
-		return wait_noreap_copyout(p, pid, uid, why,
-					   status, infop, ru);
+		return wait_noreap_copyout(wo, p, pid, uid, why, status);
 	}
 
 	/*
@@ -1250,11 +1260,14 @@ static int wait_task_zombie(struct task_struct *p, int options,
 	 */
 	read_unlock(&tasklist_lock);
 
-	retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
+	retval = wo->wo_rusage
+		? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
 	status = (p->signal->flags & SIGNAL_GROUP_EXIT)
 		? p->signal->group_exit_code : p->exit_code;
-	if (!retval && stat_addr)
-		retval = put_user(status, stat_addr);
+	if (!retval && wo->wo_stat)
+		retval = put_user(status, wo->wo_stat);
+
+	infop = wo->wo_info;
 	if (!retval && infop)
 		retval = put_user(SIGCHLD, &infop->si_signo);
 	if (!retval && infop)
@@ -1322,10 +1335,10 @@ static int *task_stopped_code(struct task_struct *p, bool ptrace)
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_stopped(int ptrace, struct task_struct *p,
-			     int options, struct siginfo __user *infop,
-			     int __user *stat_addr, struct rusage __user *ru)
+static int wait_task_stopped(struct wait_opts *wo,
+				int ptrace, struct task_struct *p)
 {
+	struct siginfo __user *infop;
 	int retval, exit_code, *p_code, why;
 	uid_t uid = 0; /* unneeded, required by compiler */
 	pid_t pid;
@@ -1333,7 +1346,7 @@ static int wait_task_stopped(int ptrace, struct task_struct *p,
 	/*
 	 * Traditionally we see ptrace'd stopped tasks regardless of options.
 	 */
-	if (!ptrace && !(options & WUNTRACED))
+	if (!ptrace && !(wo->wo_flags & WUNTRACED))
 		return 0;
 
 	exit_code = 0;
@@ -1347,7 +1360,7 @@ static int wait_task_stopped(int ptrace, struct task_struct *p,
 	if (!exit_code)
 		goto unlock_sig;
 
-	if (!unlikely(options & WNOWAIT))
+	if (!unlikely(wo->wo_flags & WNOWAIT))
 		*p_code = 0;
 
 	/* don't need the RCU readlock here as we're holding a spinlock */
@@ -1369,14 +1382,15 @@ static int wait_task_stopped(int ptrace, struct task_struct *p,
 	why = ptrace ? CLD_TRAPPED : CLD_STOPPED;
 	read_unlock(&tasklist_lock);
 
-	if (unlikely(options & WNOWAIT))
-		return wait_noreap_copyout(p, pid, uid,
-					   why, exit_code,
-					   infop, ru);
+	if (unlikely(wo->wo_flags & WNOWAIT))
+		return wait_noreap_copyout(wo, p, pid, uid, why, exit_code);
+
+	retval = wo->wo_rusage
+		? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
+	if (!retval && wo->wo_stat)
+		retval = put_user((exit_code << 8) | 0x7f, wo->wo_stat);
 
-	retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
-	if (!retval && stat_addr)
-		retval = put_user((exit_code << 8) | 0x7f, stat_addr);
+	infop = wo->wo_info;
 	if (!retval && infop)
 		retval = put_user(SIGCHLD, &infop->si_signo);
 	if (!retval && infop)
@@ -1403,15 +1417,13 @@ static int wait_task_stopped(int ptrace, struct task_struct *p,
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_continued(struct task_struct *p, int options,
-			       struct siginfo __user *infop,
-			       int __user *stat_addr, struct rusage __user *ru)
+static int wait_task_continued(struct wait_opts *wo, struct task_struct *p)
 {
 	int retval;
 	pid_t pid;
 	uid_t uid;
 
-	if (!unlikely(options & WCONTINUED))
+	if (!unlikely(wo->wo_flags & WCONTINUED))
 		return 0;
 
 	if (!(p->signal->flags & SIGNAL_STOP_CONTINUED))
@@ -1423,7 +1435,7 @@ static int wait_task_continued(struct task_struct *p, int options,
 		spin_unlock_irq(&p->sighand->siglock);
 		return 0;
 	}
-	if (!unlikely(options & WNOWAIT))
+	if (!unlikely(wo->wo_flags & WNOWAIT))
 		p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
 	uid = __task_cred(p)->uid;
 	spin_unlock_irq(&p->sighand->siglock);
@@ -1432,17 +1444,17 @@ static int wait_task_continued(struct task_struct *p, int options,
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);
 
-	if (!infop) {
-		retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
+	if (!wo->wo_info) {
+		retval = wo->wo_rusage
+			? getrusage(p, RUSAGE_BOTH, wo->wo_rusage) : 0;
 		put_task_struct(p);
-		if (!retval && stat_addr)
-			retval = put_user(0xffff, stat_addr);
+		if (!retval && wo->wo_stat)
+			retval = put_user(0xffff, wo->wo_stat);
 		if (!retval)
 			retval = pid;
 	} else {
-		retval = wait_noreap_copyout(p, pid, uid,
-					     CLD_CONTINUED, SIGCONT,
-					     infop, ru);
+		retval = wait_noreap_copyout(wo, p, pid, uid,
+					     CLD_CONTINUED, SIGCONT);
 		BUG_ON(retval == 0);
 	}
 
@@ -1452,19 +1464,16 @@ static int wait_task_continued(struct task_struct *p, int options,
 /*
  * Consider @p for a wait by @parent.
  *
- * -ECHILD should be in *@notask_error before the first call.
+ * -ECHILD should be in ->notask_error before the first call.
  * Returns nonzero for a final return, when we have unlocked tasklist_lock.
  * Returns zero if the search for a child should continue;
- * then *@notask_error is 0 if @p is an eligible child,
+ * then ->notask_error is 0 if @p is an eligible child,
  * or another error from security_task_wait(), or still -ECHILD.
  */
-static int wait_consider_task(struct task_struct *parent, int ptrace,
-			      struct task_struct *p, int *notask_error,
-			      enum pid_type type, struct pid *pid, int options,
-			      struct siginfo __user *infop,
-			      int __user *stat_addr, struct rusage __user *ru)
+static int wait_consider_task(struct wait_opts *wo, struct task_struct *parent,
+				int ptrace, struct task_struct *p)
 {
-	int ret = eligible_child(type, pid, options, p);
+	int ret = eligible_child(wo, p);
 	if (!ret)
 		return ret;
 
@@ -1476,8 +1485,8 @@ static int wait_consider_task(struct task_struct *parent, int ptrace,
 		 * to look for security policy problems, rather
 		 * than for mysterious wait bugs.
 		 */
-		if (*notask_error)
-			*notask_error = ret;
+		if (wo->notask_error)
+			wo->notask_error = ret;
 		return 0;
 	}
 
@@ -1486,7 +1495,7 @@ static int wait_consider_task(struct task_struct *parent, int ptrace,
 		 * This child is hidden by ptrace.
 		 * We aren't allowed to see it now, but eventually we will.
 		 */
-		*notask_error = 0;
+		wo->notask_error = 0;
 		return 0;
 	}
 
@@ -1497,34 +1506,30 @@ static int wait_consider_task(struct task_struct *parent, int ptrace,
 	 * We don't reap group leaders with subthreads.
 	 */
 	if (p->exit_state == EXIT_ZOMBIE && !delay_group_leader(p))
-		return wait_task_zombie(p, options, infop, stat_addr, ru);
+		return wait_task_zombie(wo, p);
 
 	/*
 	 * It's stopped or running now, so it might
 	 * later continue, exit, or stop again.
 	 */
-	*notask_error = 0;
+	wo->notask_error = 0;
 
 	if (task_stopped_code(p, ptrace))
-		return wait_task_stopped(ptrace, p, options,
-					 infop, stat_addr, ru);
+		return wait_task_stopped(wo, ptrace, p);
 
-	return wait_task_continued(p, options, infop, stat_addr, ru);
+	return wait_task_continued(wo, p);
 }
 
 /*
  * Do the work of do_wait() for one thread in the group, @tsk.
  *
- * -ECHILD should be in *@notask_error before the first call.
+ * -ECHILD should be in ->notask_error before the first call.
  * Returns nonzero for a final return, when we have unlocked tasklist_lock.
  * Returns zero if the search for a child should continue; then
- * *@notask_error is 0 if there were any eligible children,
+ * ->notask_error is 0 if there were any eligible children,
  * or another error from security_task_wait(), or still -ECHILD.
  */
-static int do_wait_thread(struct task_struct *tsk, int *notask_error,
-			  enum pid_type type, struct pid *pid, int options,
-			  struct siginfo __user *infop, int __user *stat_addr,
-			  struct rusage __user *ru)
+static int do_wait_thread(struct wait_opts *wo, struct task_struct *tsk)
 {
 	struct task_struct *p;
 
@@ -1533,9 +1538,7 @@ static int do_wait_thread(struct task_struct *tsk, int *notask_error,
 		 * Do not consider detached threads.
 		 */
 		if (!task_detached(p)) {
-			int ret = wait_consider_task(tsk, 0, p, notask_error,
-						     type, pid, options,
-						     infop, stat_addr, ru);
+			int ret = wait_consider_task(wo, tsk, 0, p);
 			if (ret)
 				return ret;
 		}
@@ -1544,17 +1547,12 @@ static int do_wait_thread(struct task_struct *tsk, int *notask_error,
 	return 0;
 }
 
-static int ptrace_do_wait(struct task_struct *tsk, int *notask_error,
-			  enum pid_type type, struct pid *pid, int options,
-			  struct siginfo __user *infop, int __user *stat_addr,
-			  struct rusage __user *ru)
+static int ptrace_do_wait(struct wait_opts *wo, struct task_struct *tsk)
 {
 	struct task_struct *p;
 
 	list_for_each_entry(p, &tsk->ptraced, ptrace_entry) {
-		int ret = wait_consider_task(tsk, 1, p, notask_error,
-					     type, pid, options,
-					     infop, stat_addr, ru);
+		int ret = wait_consider_task(wo, tsk, 1, p);
 		if (ret)
 			return ret;
 	}
@@ -1562,38 +1560,36 @@ static int ptrace_do_wait(struct task_struct *tsk, int *notask_error,
 	return 0;
 }
 
-static long do_wait(enum pid_type type, struct pid *pid, int options,
-		    struct siginfo __user *infop, int __user *stat_addr,
-		    struct rusage __user *ru)
+static long do_wait(struct wait_opts *wo)
 {
 	DECLARE_WAITQUEUE(wait, current);
 	struct task_struct *tsk;
 	int retval;
 
-	trace_sched_process_wait(pid);
+	trace_sched_process_wait(wo->wo_pid);
 
 	add_wait_queue(&current->signal->wait_chldexit,&wait);
 repeat:
 	/*
 	 * If there is nothing that can match our critiera just get out.
-	 * We will clear @retval to zero if we see any child that might later
-	 * match our criteria, even if we are not able to reap it yet.
+	 * We will clear ->notask_error to zero if we see any child that
+	 * might later match our criteria, even if we are not able to reap
+	 * it yet.
 	 */
-	retval = -ECHILD;
-	if ((type < PIDTYPE_MAX) && (!pid || hlist_empty(&pid->tasks[type])))
+	retval = wo->notask_error = -ECHILD;
+	if ((wo->wo_type < PIDTYPE_MAX) &&
+	   (!wo->wo_pid || hlist_empty(&wo->wo_pid->tasks[wo->wo_type])))
 		goto end;
 
 	current->state = TASK_INTERRUPTIBLE;
 	read_lock(&tasklist_lock);
 	tsk = current;
 	do {
-		int tsk_result = do_wait_thread(tsk, &retval,
-						type, pid, options,
-						infop, stat_addr, ru);
+		int tsk_result = do_wait_thread(wo, tsk);
+
 		if (!tsk_result)
-			tsk_result = ptrace_do_wait(tsk, &retval,
-						    type, pid, options,
-						    infop, stat_addr, ru);
+			tsk_result = ptrace_do_wait(wo, tsk);
+
 		if (tsk_result) {
 			/*
 			 * tasklist_lock is unlocked and we have a final result.
@@ -1602,25 +1598,27 @@ static long do_wait(enum pid_type type, struct pid *pid, int options,
 			goto end;
 		}
 
-		if (options & __WNOTHREAD)
+		if (wo->wo_flags & __WNOTHREAD)
 			break;
 		tsk = next_thread(tsk);
 		BUG_ON(tsk->signal != current->signal);
 	} while (tsk != current);
 	read_unlock(&tasklist_lock);
 
-	if (!retval && !(options & WNOHANG)) {
+	retval = wo->notask_error;
+	if (!retval && !(wo->wo_flags & WNOHANG)) {
 		retval = -ERESTARTSYS;
 		if (!signal_pending(current)) {
 			schedule();
 			goto repeat;
 		}
 	}
-
 end:
 	current->state = TASK_RUNNING;
 	remove_wait_queue(&current->signal->wait_chldexit,&wait);
-	if (infop) {
+	if (wo->wo_info) {
+		struct siginfo __user *infop = wo->wo_info;
+
 		if (retval > 0)
 			retval = 0;
 		else {
@@ -1649,6 +1647,7 @@ static long do_wait(enum pid_type type, struct pid *pid, int options,
 SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 		infop, int, options, struct rusage __user *, ru)
 {
+	struct wait_opts wo;
 	struct pid *pid = NULL;
 	enum pid_type type;
 	long ret;
@@ -1678,7 +1677,14 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 
 	if (type < PIDTYPE_MAX)
 		pid = find_get_pid(upid);
-	ret = do_wait(type, pid, options, infop, NULL, ru);
+
+	wo.wo_type	= type;
+	wo.wo_pid	= pid;
+	wo.wo_flags	= options;
+	wo.wo_info	= infop;
+	wo.wo_stat	= NULL;
+	wo.wo_rusage	= ru;
+	ret = do_wait(&wo);
 	put_pid(pid);
 
 	/* avoid REGPARM breakage on x86: */
@@ -1689,6 +1695,7 @@ SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
 SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
 		int, options, struct rusage __user *, ru)
 {
+	struct wait_opts wo;
 	struct pid *pid = NULL;
 	enum pid_type type;
 	long ret;
@@ -1710,7 +1717,13 @@ SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
 		pid = find_get_pid(upid);
 	}
 
-	ret = do_wait(type, pid, options | WEXITED, NULL, stat_addr, ru);
+	wo.wo_type	= type;
+	wo.wo_pid	= pid;
+	wo.wo_flags	= options | WEXITED;
+	wo.wo_info	= NULL;
+	wo.wo_stat	= stat_addr;
+	wo.wo_rusage	= ru;
+	ret = do_wait(&wo);
 	put_pid(pid);
 
 	/* avoid REGPARM breakage on x86: */

commit 47918025efdabd34e96b13b26eb2cf2fd6fd1f7c
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:39 2009 -0700

    shift "ptrace implies WUNTRACED" from ptrace_do_wait() to wait_task_stopped()
    
    No functional changes, preparation for the next patch.
    
    ptrace_do_wait() adds WUNTRACED to options for wait_task_stopped() which
    should always accept the stopped tracee, even if do_wait() was called
    without WUNTRACED.
    
    Change wait_task_stopped() to check "ptrace || WUNTRACED" instead.  This
    makes the code more explicit, and "int options" argument becomes const in
    do_wait() pathes.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 94a9992e6fd9..fd781b56401d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1330,7 +1330,10 @@ static int wait_task_stopped(int ptrace, struct task_struct *p,
 	uid_t uid = 0; /* unneeded, required by compiler */
 	pid_t pid;
 
-	if (!(options & WUNTRACED))
+	/*
+	 * Traditionally we see ptrace'd stopped tasks regardless of options.
+	 */
+	if (!ptrace && !(options & WUNTRACED))
 		return 0;
 
 	exit_code = 0;
@@ -1548,11 +1551,6 @@ static int ptrace_do_wait(struct task_struct *tsk, int *notask_error,
 {
 	struct task_struct *p;
 
-	/*
-	 * Traditionally we see ptrace'd stopped tasks regardless of options.
-	 */
-	options |= WUNTRACED;
-
 	list_for_each_entry(p, &tsk->ptraced, ptrace_entry) {
 		int ret = wait_consider_task(tsk, 1, p, notask_error,
 					     type, pid, options,

commit 77d1ef79568b337f599b75795acc8f78a87ba9ba
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:36 2009 -0700

    wait_task_zombie: do not use thread_group_cputime()
    
    There is no reason for thread_group_cputime() in wait_task_zombie(), there
    must be no other threads.
    
    This call was previously needed to collect the per-cpu data which we do
    not have any longer.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Vitaly Mayatskikh <vmayatsk@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 826e1dc8168b..94a9992e6fd9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1191,7 +1191,6 @@ static int wait_task_zombie(struct task_struct *p, int options,
 	if (likely(!traced)) {
 		struct signal_struct *psig;
 		struct signal_struct *sig;
-		struct task_cputime cputime;
 
 		/*
 		 * The resource counters for the group leader are in its
@@ -1207,23 +1206,20 @@ static int wait_task_zombie(struct task_struct *p, int options,
 		 * need to protect the access to parent->signal fields,
 		 * as other threads in the parent group can be right
 		 * here reaping other children at the same time.
-		 *
-		 * We use thread_group_cputime() to get times for the thread
-		 * group, which consolidates times for all threads in the
-		 * group including the group leader.
 		 */
-		thread_group_cputime(p, &cputime);
 		spin_lock_irq(&p->real_parent->sighand->siglock);
 		psig = p->real_parent->signal;
 		sig = p->signal;
 		psig->cutime =
 			cputime_add(psig->cutime,
-			cputime_add(cputime.utime,
-				    sig->cutime));
+			cputime_add(p->utime,
+			cputime_add(sig->utime,
+				    sig->cutime)));
 		psig->cstime =
 			cputime_add(psig->cstime,
-			cputime_add(cputime.stime,
-				    sig->cstime));
+			cputime_add(p->stime,
+			cputime_add(sig->stime,
+				    sig->cstime)));
 		psig->cgtime =
 			cputime_add(psig->cgtime,
 			cputime_add(p->gtime,

commit d1e98f429aa10132b3010ba3b0be47552a2eb14b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:34 2009 -0700

    ptrace: wait_task_zombie: s/->parent/->real_parent/
    
    Change wait_task_zombie() to use ->real_parent instead of ->parent.  We
    could even use current afaics, but ->real_parent is more clean.
    
    We know that the child is not ptrace_reparented() and thus they are equal.
     But we should avoid using task_struct->parent, we are going to remove it.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 938cceebb9ad..826e1dc8168b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1204,7 +1204,7 @@ static int wait_task_zombie(struct task_struct *p, int options,
 		 * p->signal fields, because they are only touched by
 		 * __exit_signal, which runs with tasklist_lock
 		 * write-locked anyway, and so is excluded here.  We do
-		 * need to protect the access to p->parent->signal fields,
+		 * need to protect the access to parent->signal fields,
 		 * as other threads in the parent group can be right
 		 * here reaping other children at the same time.
 		 *
@@ -1213,8 +1213,8 @@ static int wait_task_zombie(struct task_struct *p, int options,
 		 * group including the group leader.
 		 */
 		thread_group_cputime(p, &cputime);
-		spin_lock_irq(&p->parent->sighand->siglock);
-		psig = p->parent->signal;
+		spin_lock_irq(&p->real_parent->sighand->siglock);
+		psig = p->real_parent->signal;
 		sig = p->signal;
 		psig->cutime =
 			cputime_add(psig->cutime,
@@ -1245,7 +1245,7 @@ static int wait_task_zombie(struct task_struct *p, int options,
 			sig->oublock + sig->coublock;
 		task_io_accounting_add(&psig->ioac, &p->ioac);
 		task_io_accounting_add(&psig->ioac, &sig->ioac);
-		spin_unlock_irq(&p->parent->sighand->siglock);
+		spin_unlock_irq(&p->real_parent->sighand->siglock);
 	}
 
 	/*

commit 5cb11446892833e50970fb2277a9f7563b0a8bd3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:30 2009 -0700

    ptrace: do not use task->ptrace directly in core kernel
    
    No functional changes.
    
    - Nobody except ptrace.c & co should use ptrace flags directly, we have
      task_ptrace() for that.
    
    - No need to specially check PT_PTRACED, we must not have other PT_ bits
      set without PT_PTRACED. And no need to know this flag exists.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 213f906f5e16..938cceebb9ad 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -757,7 +757,7 @@ static void reparent_thread(struct task_struct *father, struct task_struct *p,
 	p->exit_signal = SIGCHLD;
 
 	/* If it has exited notify the new parent about this child's death. */
-	if (!p->ptrace &&
+	if (!task_ptrace(p) &&
 	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {
 		do_notify_parent(p, p->exit_signal);
 		if (task_detached(p)) {
@@ -782,7 +782,7 @@ static void forget_original_parent(struct task_struct *father)
 	list_for_each_entry_safe(p, n, &father->children, sibling) {
 		p->real_parent = reaper;
 		if (p->parent == father) {
-			BUG_ON(p->ptrace);
+			BUG_ON(task_ptrace(p));
 			p->parent = p->real_parent;
 		}
 		reparent_thread(father, p, &dead_children);
@@ -1482,7 +1482,7 @@ static int wait_consider_task(struct task_struct *parent, int ptrace,
 		return 0;
 	}
 
-	if (likely(!ptrace) && unlikely(p->ptrace)) {
+	if (likely(!ptrace) && unlikely(task_ptrace(p))) {
 		/*
 		 * This child is hidden by ptrace.
 		 * We aren't allowed to see it now, but eventually we will.

commit dea33cfd99022d82d923a0c6a3bd895fb6683fb2
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:29 2009 -0700

    ptrace: mm_need_new_owner: use ->real_parent to search in the siblings
    
    "Search in the siblings" should use ->real_parent, not ->parent.  If the
    task is traced then ->parent == tracer, while the task's parent is always
    ->real_parent.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 533e5f85669a..213f906f5e16 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -590,7 +590,7 @@ void mm_update_next_owner(struct mm_struct *mm)
 	/*
 	 * Search in the siblings
 	 */
-	list_for_each_entry(c, &p->parent->children, sibling) {
+	list_for_each_entry(c, &p->real_parent->children, sibling) {
 		if (c->mm == mm)
 			goto assign_new_owner;
 	}

commit 87245135d5057edd5a8037131f81eeffd76d4fef
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jun 17 16:27:23 2009 -0700

    allow_signal: kill the bogus ->mm check, add a note about CLONE_SIGHAND
    
    allow_signal() checks ->mm == NULL.  Not sure why.  Perhaps to make sure
    current is the kernel thread.  But this helper must not be used unless we
    are the kernel thread, kill this check.
    
    Also, document the fact that the CLONE_SIGHAND kthread must not use
    allow_signal(), unless the caller really wants to change the parent's
    ->sighand->action as well.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b6c90b5ef509..533e5f85669a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -375,9 +375,8 @@ static void set_special_pids(struct pid *pid)
 }
 
 /*
- * Let kernel threads use this to say that they
- * allow a certain signal (since daemonize() will
- * have disabled all of them by default).
+ * Let kernel threads use this to say that they allow a certain signal.
+ * Must not be used if kthread was cloned with CLONE_SIGHAND.
  */
 int allow_signal(int sig)
 {
@@ -385,14 +384,14 @@ int allow_signal(int sig)
 		return -EINVAL;
 
 	spin_lock_irq(&current->sighand->siglock);
+	/* This is only needed for daemonize()'ed kthreads */
 	sigdelset(&current->blocked, sig);
-	if (!current->mm) {
-		/* Kernel threads handle their own signals.
-		   Let the signal code know it'll be handled, so
-		   that they don't get converted to SIGKILL or
-		   just silently dropped */
-		current->sighand->action[(sig)-1].sa.sa_handler = (void __user *)2;
-	}
+	/*
+	 * Kernel threads handle their own signals. Let the signal code
+	 * know it'll be handled, so that they don't get converted to
+	 * SIGKILL or just silently dropped.
+	 */
+	current->sighand->action[(sig)-1].sa.sa_handler = (void __user *)2;
 	recalc_sigpending();
 	spin_unlock_irq(&current->sighand->siglock);
 	return 0;

commit 8a1ca8cedd108c8e76a6ab34079d0bbb4f244799
Merge: b640f042faa2 940010c5a314
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 14:01:07 2009 -0700

    Merge branch 'perfcounters-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perfcounters-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (574 commits)
      perf_counter: Turn off by default
      perf_counter: Add counter->id to the throttle event
      perf_counter: Better align code
      perf_counter: Rename L2 to LL cache
      perf_counter: Standardize event names
      perf_counter: Rename enums
      perf_counter tools: Clean up u64 usage
      perf_counter: Rename perf_counter_limit sysctl
      perf_counter: More paranoia settings
      perf_counter: powerpc: Implement generalized cache events for POWER processors
      perf_counters: powerpc: Add support for POWER7 processors
      perf_counter: Accurate period data
      perf_counter: Introduce struct for sample data
      perf_counter tools: Normalize data using per sample period data
      perf_counter: Annotate exit ctx recursion
      perf_counter tools: Propagate signals properly
      perf_counter tools: Small frequency related fixes
      perf_counter: More aggressive frequency adjustment
      perf_counter/x86: Fix the model number of Intel Core2 processors
      perf_counter, x86: Correct some event and umask values for Intel processors
      ...

commit 3296ca27f50ecbd71db1d808c7a72d311027f919
Merge: e893123c7378 73fbad283cfb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 11 10:01:41 2009 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/security-testing-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/security-testing-2.6: (44 commits)
      nommu: Provide mmap_min_addr definition.
      TOMOYO: Add description of lists and structures.
      TOMOYO: Remove unused field.
      integrity: ima audit dentry_open failure
      TOMOYO: Remove unused parameter.
      security: use mmap_min_addr indepedently of security models
      TOMOYO: Simplify policy reader.
      TOMOYO: Remove redundant markers.
      SELinux: define audit permissions for audit tree netlink messages
      TOMOYO: Remove unused mutex.
      tomoyo: avoid get+put of task_struct
      smack: Remove redundant initialization.
      integrity: nfsd imbalance bug fix
      rootplug: Remove redundant initialization.
      smack: do not beyond ARRAY_SIZE of data
      integrity: move ima_counts_get
      integrity: path_check update
      IMA: Add __init notation to ima functions
      IMA: Minimal IMA policy and boot param for TCB IMA policy
      selinux: remove obsolete read buffer limit from sel_read_bool
      ...

commit 940010c5a314a7bd9b498593bc6ba1718ac5aec5
Merge: 8dc8e5e8bc0c 991ec02cdca3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jun 11 17:55:42 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/irqinit.c
            arch/x86/kernel/irqinit_64.c
            arch/x86/kernel/traps.c
            arch/x86/mm/fault.c
            include/linux/sched.h
            kernel/exit.c

commit a63eaf34ae60bdb067a354cc8def2e8f4a01f5f4
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri May 22 14:17:31 2009 +1000

    perf_counter: Dynamically allocate tasks' perf_counter_context struct
    
    This replaces the struct perf_counter_context in the task_struct with
    a pointer to a dynamically allocated perf_counter_context struct.  The
    main reason for doing is this is to allow us to transfer a
    perf_counter_context from one task to another when we do lazy PMU
    switching in a later patch.
    
    This has a few side-benefits: the task_struct becomes a little smaller,
    we save some memory because only tasks that have perf_counters attached
    get a perf_counter_context allocated for them, and we can remove the
    inclusion of <linux/perf_counter.h> in sched.h, meaning that we don't
    end up recompiling nearly everything whenever perf_counter.h changes.
    
    The perf_counter_context structures are reference-counted and freed
    when the last reference is dropped.  A context can have references
    from its task and the counters on its task.  Counters can outlive the
    task so it is possible that a context will be freed well after its
    task has exited.
    
    Contexts are allocated on fork if the parent had a context, or
    otherwise the first time that a per-task counter is created on a task.
    In the latter case, we set the context pointer in the task struct
    locklessly using an atomic compare-and-exchange operation in case we
    raced with some other task in creating a context for the subject task.
    
    This also removes the task pointer from the perf_counter struct.  The
    task pointer was not used anywhere and would make it harder to move a
    context from one task to another.  Anything that needed to know which
    task a counter was attached to was already using counter->ctx->task.
    
    The __perf_counter_init_context function moves up in perf_counter.c
    so that it can be called from find_get_context, and now initializes
    the refcount, but is otherwise unchanged.
    
    We were potentially calling list_del_counter twice: once from
    __perf_counter_exit_task when the task exits and once from
    __perf_counter_remove_from_context when the counter's fd gets closed.
    This adds a check in list_del_counter so it doesn't do anything if
    the counter has already been removed from the lists.
    
    Since perf_counter_task_sched_in doesn't do anything if the task doesn't
    have a context, and leaves cpuctx->task_ctx = NULL, this adds code to
    __perf_install_in_context to set cpuctx->task_ctx if necessary, i.e. in
    the case where the current task adds the first counter to itself and
    thus creates a context for itself.
    
    This also adds similar code to __perf_counter_enable to handle a
    similar situation which can arise when the counters have been disabled
    using prctl; that also leaves cpuctx->task_ctx = NULL.
    
    [ Impact: refactor counter context management to prepare for new feature ]
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <18966.10075.781053.231153@cargo.ozlabs.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index f9dfedd94af0..99ad4063ee4a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -48,6 +48,7 @@
 #include <linux/tracehook.h>
 #include <linux/fs_struct.h>
 #include <linux/init_task.h>
+#include <linux/perf_counter.h>
 #include <trace/sched.h>
 
 #include <asm/uaccess.h>
@@ -159,7 +160,7 @@ static void delayed_put_task_struct(struct rcu_head *rhp)
 	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
 
 #ifdef CONFIG_PERF_COUNTERS
-	WARN_ON_ONCE(!list_empty(&tsk->perf_counter_ctx.counter_list));
+	WARN_ON_ONCE(tsk->perf_counter_ctxp);
 #endif
 	trace_sched_process_free(tsk);
 	put_task_struct(tsk);

commit 33b2fb303fe7f6b08bbb32f708e67b96eaa94a7a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun May 17 11:08:41 2009 +0200

    perf_counter: fix counter freeing logic
    
    Fix counter lifetime bugs which explain the crashes reported by
    Marcelo Tosatti and Arnaldo Carvalho de Melo.
    
    The new rule is: flushing + freeing is only done for a task's
    own counters, never for other tasks.
    
    [ Impact: fix crashes/lockups with inherited counters ]
    
    Reported-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Reported-by: Marcelo Tosatti <mtosatti@redhat.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 73affd35e76d..f9dfedd94af0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -178,12 +178,6 @@ void release_task(struct task_struct * p)
 
 	proc_flush_task(p);
 
-	/*
-	 * Flush inherited counters to the parent - before the parent
-	 * gets woken up by child-exit notifications.
-	 */
-	perf_counter_exit_task(p);
-
 	write_lock_irq(&tasklist_lock);
 	tracehook_finish_release_task(p);
 	__exit_signal(p);
@@ -985,6 +979,13 @@ NORET_TYPE void do_exit(long code)
 		module_put(tsk->binfmt->module);
 
 	proc_exit_connector(tsk);
+
+	/*
+	 * Flush inherited counters to the parent - before the parent
+	 * gets woken up by child-exit notifications.
+	 */
+	perf_counter_exit_task(tsk);
+
 	exit_notify(tsk, group_dead);
 #ifdef CONFIG_NUMA
 	mpol_put(tsk->mempolicy);
@@ -1257,12 +1258,6 @@ static int wait_task_zombie(struct task_struct *p, int options,
 	 */
 	read_unlock(&tasklist_lock);
 
-	/*
-	 * Flush inherited counters to the parent - before the parent
-	 * gets woken up by child-exit notifications.
-	 */
-	perf_counter_exit_task(p);
-
 	retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
 	status = (p->signal->flags & SIGNAL_GROUP_EXIT)
 		? p->signal->group_exit_code : p->exit_code;

commit 0203026b58b4299ba7281c0b4b417207c1f05d0e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun May 17 11:24:08 2009 +0200

    perf_counter: fix threaded task exit
    
    Flushing counters in __exit_signal() with irqs disabled is not
    a good idea as perf_counter_exit_task() acquires mutexes. So
    flush it before acquiring the tasklist lock.
    
    (Note, we still need a fix for when the PID has been unhashed.)
    
    [ Impact: fix crash with inherited counters ]
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 16d74f13a3e7..73affd35e76d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -128,12 +128,6 @@ static void __exit_signal(struct task_struct *tsk)
 		sig = NULL; /* Marker for below. */
 	}
 
-	/*
-	 * Flush inherited counters to the parent - before the parent
-	 * gets woken up by child-exit notifications.
-	 */
-	perf_counter_exit_task(tsk);
-
 	__unhash_process(tsk);
 
 	/*
@@ -183,6 +177,13 @@ void release_task(struct task_struct * p)
 	atomic_dec(&__task_cred(p)->user->processes);
 
 	proc_flush_task(p);
+
+	/*
+	 * Flush inherited counters to the parent - before the parent
+	 * gets woken up by child-exit notifications.
+	 */
+	perf_counter_exit_task(p);
+
 	write_lock_irq(&tasklist_lock);
 	tracehook_finish_release_task(p);
 	__exit_signal(p);

commit 856d56b9e5de650a64a6c41c17aaed702b55d578
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 15 20:45:59 2009 +0200

    perf_counter: Fix counter inheritance
    
    Srivatsa Vaddagiri reported that a Java workload triggers this
    warning in kernel/exit.c:
    
       WARN_ON_ONCE(!list_empty(&tsk->perf_counter_ctx.counter_list));
    
    Add the inherited counter propagation on self-detach, this could
    cause counter leaks and incomplete stats in threaded code like
    the below:
    
      #include <pthread.h>
      #include <unistd.h>
    
      void *thread(void *arg)
      {
              sleep(5);
              return NULL;
      }
    
      void main(void)
      {
              pthread_t thr;
              pthread_create(&thr, NULL, thread, NULL);
      }
    
    Reported-by: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4741376c8dec..16d74f13a3e7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -128,6 +128,12 @@ static void __exit_signal(struct task_struct *tsk)
 		sig = NULL; /* Marker for below. */
 	}
 
+	/*
+	 * Flush inherited counters to the parent - before the parent
+	 * gets woken up by child-exit notifications.
+	 */
+	perf_counter_exit_task(tsk);
+
 	__unhash_process(tsk);
 
 	/*

commit d254117099d711f215e62427f55dfb8ebd5ad011
Merge: 07ff7a0b187f 8c9ed899b44c
Author: James Morris <jmorris@namei.org>
Date:   Fri May 8 17:56:47 2009 +1000

    Merge branch 'master' into next

commit 78a3d9d5654a7fd99cf8b2ab06b9497b9c7aad64
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Apr 29 18:01:23 2009 +0200

    do_wait: do take security_task_wait() into account
    
    I was never able to understand what should we actually do when
    security_task_wait() fails, but the current code doesn't look right.
    
    If ->task_wait() returns the error, we update *notask_error correctly.
    But then we either reap the child (despite the fact this was forbidden)
    or clear *notask_error (and hide the securiy policy problems).
    
    This patch assumes that "stolen by ptrace" doesn't matter. If selinux
    denies the child we should ignore it but make sure we report -EACCESS
    instead of -ECHLD if there are no other eligible children.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 167e1e3ad7c6..d2e8239ea187 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1582,6 +1582,7 @@ static int wait_consider_task(struct task_struct *parent, int ptrace,
 		 */
 		if (*notask_error)
 			*notask_error = ret;
+		return 0;
 	}
 
 	if (likely(!ptrace) && unlikely(p->ptrace)) {

commit ad8d75fff811a6a230f7f43b05a6483099349533
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Apr 14 19:39:12 2009 -0400

    tracing/events: move trace point headers into include/trace/events
    
    Impact: clean up
    
    Create a sub directory in include/trace called events to keep the
    trace point headers in their own separate directory. Only headers that
    declare trace points should be defined in this directory.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Neil Horman <nhorman@tuxdriver.com>
    Cc: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2fe9d2c7eeee..cab535c427b8 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -48,7 +48,7 @@
 #include <linux/tracehook.h>
 #include <linux/fs_struct.h>
 #include <linux/init_task.h>
-#include <trace/sched.h>
+#include <trace/events/sched.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>

commit a8d154b009168337494fbf345671bab74d3e4b8b
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 10 09:36:00 2009 -0400

    tracing: create automated trace defines
    
    This patch lowers the number of places a developer must modify to add
    new tracepoints. The current method to add a new tracepoint
    into an existing system is to write the trace point macro in the
    trace header with one of the macros TRACE_EVENT, TRACE_FORMAT or
    DECLARE_TRACE, then they must add the same named item into the C file
    with the macro DEFINE_TRACE(name) and then add the trace point.
    
    This change cuts out the needing to add the DEFINE_TRACE(name).
    Every file that uses the tracepoint must still include the trace/<type>.h
    file, but the one C file must also add a define before the including
    of that file.
    
     #define CREATE_TRACE_POINTS
     #include <trace/mytrace.h>
    
    This will cause the trace/mytrace.h file to also produce the C code
    necessary to implement the trace point.
    
    Note, if more than one trace/<type>.h is used to create the C code
    it is best to list them all together.
    
     #define CREATE_TRACE_POINTS
     #include <trace/foo.h>
     #include <trace/bar.h>
     #include <trace/fido.h>
    
    Thanks to Mathieu Desnoyers and Christoph Hellwig for coming up with
    the cleaner solution of the define above the includes over my first
    design to have the C code include a "special" header.
    
    This patch converts sched, irq and lockdep and skb to use this new
    method.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Neil Horman <nhorman@tuxdriver.com>
    Cc: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index abf9cf3b95c6..2fe9d2c7eeee 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -56,10 +56,6 @@
 #include <asm/mmu_context.h>
 #include "cred-internals.h"
 
-DEFINE_TRACE(sched_process_free);
-DEFINE_TRACE(sched_process_exit);
-DEFINE_TRACE(sched_process_wait);
-
 static void exit_mm(struct task_struct * tsk);
 
 static void __unhash_process(struct task_struct *p)

commit 5ea472a77f8e4811ceee3f44a9deda6ad6e8b789
Merge: 6c009ecef8cc 577c9c456f0e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Apr 8 10:35:30 2009 +0200

    Merge commit 'v2.6.30-rc1' into perfcounters/core
    
    Conflicts:
            arch/powerpc/include/asm/systbl.h
            arch/powerpc/include/asm/unistd.h
            include/linux/init_task.h
    
    Merge reason: the conflicts are non-trivial: PowerPC placement
                  of sys_perf_counter_open has to be mixed with the
                  new preadv/pwrite syscalls.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit c61b79b6ef266890954213a701d8f6021d8c1289
Merge: 2b2ec7554cf7 9efe21cb82b5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 7 14:07:52 2009 -0700

    Merge branch 'irq/threaded' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'irq/threaded' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      genirq: fix devres.o build for GENERIC_HARDIRQS=n
      genirq: provide old request_irq() for CONFIG_GENERIC_HARDIRQ=n
      genirq: threaded irq handlers review fixups
      genirq: add support for threaded interrupts to devres
      genirq: add threaded interrupt handler support

commit 6c009ecef8cca28c7c09eb16d0802e37915a76e1
Merge: 98c2aaf8be5b d508afb437da
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Apr 7 12:05:21 2009 +0200

    Merge branch 'linus' into perfcounters/core
    
    Merge reason: need the upstream facility added by:
    
      7f1e2ca: hrtimer: fix rq->lock inversion (again)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 432870dab85a2f69dc417022646cb9a70acf7f94
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Apr 6 16:16:02 2009 +0200

    exit_notify: kill the wrong capable(CAP_KILL) check
    
    The CAP_KILL check in exit_notify() looks just wrong, kill it.
    
    Whatever logic we have to reset ->exit_signal, the malicious user
    can bypass it if it execs the setuid application before exiting.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6686ed1e4aa3..32cbf2607cb0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -837,8 +837,7 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	 */
 	if (tsk->exit_signal != SIGCHLD && !task_detached(tsk) &&
 	    (tsk->parent_exec_id != tsk->real_parent->self_exec_id ||
-	     tsk->self_exec_id != tsk->parent_exec_id) &&
-	    !capable(CAP_KILL))
+	     tsk->self_exec_id != tsk->parent_exec_id))
 		tsk->exit_signal = SIGCHLD;
 
 	signal = tracehook_notify_death(tsk, &cookie, group_dead);

commit f541ae326fa120fa5c57433e4d9a133df212ce41
Merge: e255357764f9 0221c81b1b8e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 6 09:02:57 2009 +0200

    Merge branch 'linus' into perfcounters/core-v2
    
    Merge reason: we have gathered quite a few conflicts, need to merge upstream
    
    Conflicts:
            arch/powerpc/kernel/Makefile
            arch/x86/ia32/ia32entry.S
            arch/x86/include/asm/hardirq.h
            arch/x86/include/asm/unistd_32.h
            arch/x86/include/asm/unistd_64.h
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/irq.c
            arch/x86/kernel/syscall_table_32.S
            arch/x86/mm/iomap_32.c
            include/linux/sched.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 9efe21cb82b5dbe3b0b2ae4de4eccc64ecb94e95
Merge: de18836e447c 0221c81b1b8e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 6 01:41:22 2009 +0200

    Merge branch 'linus' into irq/threaded
    
    Conflicts:
            include/linux/irq.h
            kernel/irq/handle.c

commit 8fe74cf053de7ad2124a894996f84fa890a81093
Merge: c2eb2fa6d2b6 ced117c73edc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 2 21:09:10 2009 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs-2.6
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs-2.6:
      Remove two unneeded exports and make two symbols static in fs/mpage.c
      Cleanup after commit 585d3bc06f4ca57f975a5a1f698f65a45ea66225
      Trim includes of fdtable.h
      Don't crap into descriptor table in binfmt_som
      Trim includes in binfmt_elf
      Don't mess with descriptor table in load_elf_binary()
      Get rid of indirect include of fs_struct.h
      New helper - current_umask()
      check_unsafe_exec() doesn't care about signal handlers sharing
      New locking/refcounting for fs_struct
      Take fs_struct handling to new file (fs/fs_struct.c)
      Get rid of bumping fs_struct refcount in pivot_root(2)
      Kill unsharing fs_struct in __set_personality()

commit 1b0f7ffd0ea27cd3a0b9ca04e3df9522048c32a3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:39 2009 -0700

    pids: kill signal_struct-> __pgrp/__session and friends
    
    We are wasting 2 words in signal_struct without any reason to implement
    task_pgrp_nr() and task_session_nr().
    
    task_session_nr() has no callers since
    2e2ba22ea4fd4bb85f0fa37c521066db6775cbef, we can remove it.
    
    task_pgrp_nr() is still (I believe wrongly) used in fs/autofsX and
    fs/coda.
    
    This patch reimplements task_pgrp_nr() via task_pgrp_nr_ns(), and kills
    __pgrp/__session and the related helpers.
    
    The change in drivers/char/tty_io.c is cosmetic, but hopefully makes sense
    anyway.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Alan Cox <number6@the-village.bc.nu>          [tty parts]
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 384f09caf2ef..3bec141c82f6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -357,16 +357,12 @@ static void reparent_to_kthreadd(void)
 void __set_special_pids(struct pid *pid)
 {
 	struct task_struct *curr = current->group_leader;
-	pid_t nr = pid_nr(pid);
 
-	if (task_session(curr) != pid) {
+	if (task_session(curr) != pid)
 		change_pid(curr, PIDTYPE_SID, pid);
-		set_task_session(curr, nr);
-	}
-	if (task_pgrp(curr) != pid) {
+
+	if (task_pgrp(curr) != pid)
 		change_pid(curr, PIDTYPE_PGID, pid);
-		set_task_pgrp(curr, nr);
-	}
 }
 
 static void set_special_pids(struct pid *pid)

commit 2ae448efc87df6d328f5835969076c7f9fce59c3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:36 2009 -0700

    pids: improve get_task_pid() to fix the unsafe sys_wait4()->task_pgrp()
    
    sys_wait4() does get_pid(task_pgrp(current)), this is not safe.  We can
    add rcu lock/unlock around, but we already have get_task_pid() which can
    be improved to handle the special pids in more reliable manner.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Louis Rilling <Louis.Rilling@kerlabs.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 029415d9f82e..384f09caf2ef 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1737,7 +1737,7 @@ SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
 		pid = find_get_pid(-upid);
 	} else if (upid == 0) {
 		type = PIDTYPE_PGID;
-		pid = get_pid(task_pgrp(current));
+		pid = get_task_pid(current, PIDTYPE_PGID);
 	} else /* upid > 0 */ {
 		type = PIDTYPE_PID;
 		pid = find_get_pid(upid);

commit 5dfc80be73dd0c212d2e6dd8dbf5afa07e680bbe
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:19 2009 -0700

    forget_original_parent: do not abuse child->ptrace_entry
    
    By discussion with Roland.
    
    - Use ->sibling instead of ->ptrace_entry to chain the need to be
      release_task'd childs. Nobody else can use ->sibling, this task
      is EXIT_DEAD and nobody can find it on its own list.
    
    - rename ptrace_dead to dead_childs.
    
    - Now that we don't have the "parallel" untrace code, change back
      reparent_thread() to return void, pass dead_childs as an argument.
    
    Actually, I don't understand why do we notify /sbin/init when we
    reparent a zombie, probably it is better to reap it unconditionally.
    
    [akpm@linux-foundation.org: s/childs/children/]
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "Metzger, Markus T" <markus.t.metzger@intel.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 506693dfdd4e..029415d9f82e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -726,46 +726,6 @@ static void exit_mm(struct task_struct * tsk)
 	mmput(mm);
 }
 
-/* Returns nonzero if the child should be released. */
-static int reparent_thread(struct task_struct *p, struct task_struct *father)
-{
-	int dead;
-
-	if (p->pdeath_signal)
-		/* We already hold the tasklist_lock here.  */
-		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
-
-	list_move_tail(&p->sibling, &p->real_parent->children);
-
-	if (task_detached(p))
-		return 0;
-	/* If this is a threaded reparent there is no need to
-	 * notify anyone anything has happened.
-	 */
-	if (same_thread_group(p->real_parent, father))
-		return 0;
-
-	/* We don't want people slaying init.  */
-	p->exit_signal = SIGCHLD;
-
-	/* If we'd notified the old parent about this child's death,
-	 * also notify the new parent.
-	 */
-	dead = 0;
-	if (!p->ptrace &&
-	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {
-		do_notify_parent(p, p->exit_signal);
-		if (task_detached(p)) {
-			p->exit_state = EXIT_DEAD;
-			dead = 1;
-		}
-	}
-
-	kill_orphaned_pgrp(p, father);
-
-	return dead;
-}
-
 /*
  * When we die, we re-parent all our children.
  * Try to give them to another thread in our thread
@@ -805,10 +765,46 @@ static struct task_struct *find_new_reaper(struct task_struct *father)
 	return pid_ns->child_reaper;
 }
 
+/*
+* Any that need to be release_task'd are put on the @dead list.
+ */
+static void reparent_thread(struct task_struct *father, struct task_struct *p,
+				struct list_head *dead)
+{
+	if (p->pdeath_signal)
+		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
+
+	list_move_tail(&p->sibling, &p->real_parent->children);
+
+	if (task_detached(p))
+		return;
+	/*
+	 * If this is a threaded reparent there is no need to
+	 * notify anyone anything has happened.
+	 */
+	if (same_thread_group(p->real_parent, father))
+		return;
+
+	/* We don't want people slaying init.  */
+	p->exit_signal = SIGCHLD;
+
+	/* If it has exited notify the new parent about this child's death. */
+	if (!p->ptrace &&
+	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {
+		do_notify_parent(p, p->exit_signal);
+		if (task_detached(p)) {
+			p->exit_state = EXIT_DEAD;
+			list_move_tail(&p->sibling, dead);
+		}
+	}
+
+	kill_orphaned_pgrp(p, father);
+}
+
 static void forget_original_parent(struct task_struct *father)
 {
 	struct task_struct *p, *n, *reaper;
-	LIST_HEAD(ptrace_dead);
+	LIST_HEAD(dead_children);
 
 	exit_ptrace(father);
 
@@ -821,15 +817,14 @@ static void forget_original_parent(struct task_struct *father)
 			BUG_ON(p->ptrace);
 			p->parent = p->real_parent;
 		}
-		if (reparent_thread(p, father))
-			list_add(&p->ptrace_entry, &ptrace_dead);;
+		reparent_thread(father, p, &dead_children);
 	}
-
 	write_unlock_irq(&tasklist_lock);
+
 	BUG_ON(!list_empty(&father->children));
 
-	list_for_each_entry_safe(p, n, &ptrace_dead, ptrace_entry) {
-		list_del_init(&p->ptrace_entry);
+	list_for_each_entry_safe(p, n, &dead_children, sibling) {
+		list_del_init(&p->sibling);
 		release_task(p);
 	}
 }

commit 39c626ae47c469abdfd30c6e42eff884931380d6
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:18 2009 -0700

    forget_original_parent: split out the un-ptrace part
    
    By discussion with Roland.
    
    - Rename ptrace_exit() to exit_ptrace(), and change it to do all the
      necessary work with ->ptraced list by its own.
    
    - Move this code from exit.c to ptrace.c
    
    - Update the comment in ptrace_detach() to explain the rechecking of
      the child->ptrace.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: "Metzger, Markus T" <markus.t.metzger@intel.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3e09b7cb3b20..506693dfdd4e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -61,11 +61,6 @@ DEFINE_TRACE(sched_process_wait);
 
 static void exit_mm(struct task_struct * tsk);
 
-static inline int task_detached(struct task_struct *p)
-{
-	return p->exit_signal == -1;
-}
-
 static void __unhash_process(struct task_struct *p)
 {
 	nr_threads--;
@@ -731,85 +726,6 @@ static void exit_mm(struct task_struct * tsk)
 	mmput(mm);
 }
 
-/*
- * Called with irqs disabled, returns true if childs should reap themselves.
- */
-static int ignoring_children(struct sighand_struct *sigh)
-{
-	int ret;
-	spin_lock(&sigh->siglock);
-	ret = (sigh->action[SIGCHLD-1].sa.sa_handler == SIG_IGN) ||
-	      (sigh->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT);
-	spin_unlock(&sigh->siglock);
-	return ret;
-}
-
-/* Returns nonzero if the tracee should be released. */
-int __ptrace_detach(struct task_struct *tracer, struct task_struct *p)
-{
-	__ptrace_unlink(p);
-
-	if (p->exit_state != EXIT_ZOMBIE)
-		return 0;
-	/*
-	 * If it's a zombie, our attachedness prevented normal
-	 * parent notification or self-reaping.  Do notification
-	 * now if it would have happened earlier.  If it should
-	 * reap itself we return true.
-	 *
-	 * If it's our own child, there is no notification to do.
-	 * But if our normal children self-reap, then this child
-	 * was prevented by ptrace and we must reap it now.
-	 */
-	if (!task_detached(p) && thread_group_empty(p)) {
-		if (!same_thread_group(p->real_parent, tracer))
-			do_notify_parent(p, p->exit_signal);
-		else if (ignoring_children(tracer->sighand))
-			p->exit_signal = -1;
-	}
-
-	if (!task_detached(p))
-		return 0;
-
-	/* Mark it as in the process of being reaped. */
-	p->exit_state = EXIT_DEAD;
-	return 1;
-}
-
-/*
- * Detach all tasks we were using ptrace on.
- * Any that need to be release_task'd are put on the @dead list.
- *
- * Called with write_lock(&tasklist_lock) held.
- */
-static void ptrace_exit(struct task_struct *parent, struct list_head *dead)
-{
-	struct task_struct *p, *n;
-
-	list_for_each_entry_safe(p, n, &parent->ptraced, ptrace_entry) {
-		if (__ptrace_detach(parent, p))
-			list_add(&p->ptrace_entry, dead);
-	}
-}
-
-/*
- * Finish up exit-time ptrace cleanup.
- *
- * Called without locks.
- */
-static void ptrace_exit_finish(struct task_struct *parent,
-			       struct list_head *dead)
-{
-	struct task_struct *p, *n;
-
-	BUG_ON(!list_empty(&parent->ptraced));
-
-	list_for_each_entry_safe(p, n, dead, ptrace_entry) {
-		list_del_init(&p->ptrace_entry);
-		release_task(p);
-	}
-}
-
 /* Returns nonzero if the child should be released. */
 static int reparent_thread(struct task_struct *p, struct task_struct *father)
 {
@@ -894,12 +810,10 @@ static void forget_original_parent(struct task_struct *father)
 	struct task_struct *p, *n, *reaper;
 	LIST_HEAD(ptrace_dead);
 
+	exit_ptrace(father);
+
 	write_lock_irq(&tasklist_lock);
 	reaper = find_new_reaper(father);
-	/*
-	 * First clean up ptrace if we were using it.
-	 */
-	ptrace_exit(father, &ptrace_dead);
 
 	list_for_each_entry_safe(p, n, &father->children, sibling) {
 		p->real_parent = reaper;
@@ -914,7 +828,10 @@ static void forget_original_parent(struct task_struct *father)
 	write_unlock_irq(&tasklist_lock);
 	BUG_ON(!list_empty(&father->children));
 
-	ptrace_exit_finish(father, &ptrace_dead);
+	list_for_each_entry_safe(p, n, &ptrace_dead, ptrace_entry) {
+		list_del_init(&p->ptrace_entry);
+		release_task(p);
+	}
 }
 
 /*

commit 7f5d3652d469cdf9eb2365dfea7ce3fb9e1409cc
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:17 2009 -0700

    reparent_thread: fix a zombie leak if /sbin/init ignores SIGCHLD
    
    If /sbin/init ignores SIGCHLD and we re-parent a zombie, it is leaked.
    reparent_thread() does do_notify_parent() which sets ->exit_signal = -1 in
    this case.  This means that nobody except us can reap it, the detached
    task is not visible to do_wait().
    
    Change reparent_thread() to return a boolean (like __pthread_detach) to
    indicate that the thread is dead and must be released.  Also change
    forget_original_parent() to add the child to ptrace_dead list in this
    case.
    
    The naming becomes insane, the next patch does the cleanup.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5be0a406faeb..3e09b7cb3b20 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -810,8 +810,11 @@ static void ptrace_exit_finish(struct task_struct *parent,
 	}
 }
 
-static void reparent_thread(struct task_struct *p, struct task_struct *father)
+/* Returns nonzero if the child should be released. */
+static int reparent_thread(struct task_struct *p, struct task_struct *father)
 {
+	int dead;
+
 	if (p->pdeath_signal)
 		/* We already hold the tasklist_lock here.  */
 		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
@@ -819,12 +822,12 @@ static void reparent_thread(struct task_struct *p, struct task_struct *father)
 	list_move_tail(&p->sibling, &p->real_parent->children);
 
 	if (task_detached(p))
-		return;
+		return 0;
 	/* If this is a threaded reparent there is no need to
 	 * notify anyone anything has happened.
 	 */
 	if (same_thread_group(p->real_parent, father))
-		return;
+		return 0;
 
 	/* We don't want people slaying init.  */
 	p->exit_signal = SIGCHLD;
@@ -832,11 +835,19 @@ static void reparent_thread(struct task_struct *p, struct task_struct *father)
 	/* If we'd notified the old parent about this child's death,
 	 * also notify the new parent.
 	 */
+	dead = 0;
 	if (!p->ptrace &&
-	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p))
+	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p)) {
 		do_notify_parent(p, p->exit_signal);
+		if (task_detached(p)) {
+			p->exit_state = EXIT_DEAD;
+			dead = 1;
+		}
+	}
 
 	kill_orphaned_pgrp(p, father);
+
+	return dead;
 }
 
 /*
@@ -896,7 +907,8 @@ static void forget_original_parent(struct task_struct *father)
 			BUG_ON(p->ptrace);
 			p->parent = p->real_parent;
 		}
-		reparent_thread(p, father);
+		if (reparent_thread(p, father))
+			list_add(&p->ptrace_entry, &ptrace_dead);;
 	}
 
 	write_unlock_irq(&tasklist_lock);

commit b1442b055c154699a6a2c436f3352f71b6beede3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:16 2009 -0700

    reparent_thread: fix the "is it traced" check
    
    reparent_thread() uses ptrace_reparented() to check whether this thread is
    ptraced, in that case we should not notify the new parent.
    
    But ptrace_reparented() is not exactly correct when the reparented thread
    is traced by /sbin/init, because forget_original_parent() has already
    changed ->real_parent.
    
    Currently, the only problem is the false notification.  But with the next
    patch the kernel crash in this (yes, pathological) case.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 405e6877168b..5be0a406faeb 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -832,7 +832,7 @@ static void reparent_thread(struct task_struct *p, struct task_struct *father)
 	/* If we'd notified the old parent about this child's death,
 	 * also notify the new parent.
 	 */
-	if (!ptrace_reparented(p) &&
+	if (!p->ptrace &&
 	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p))
 		do_notify_parent(p, p->exit_signal);
 

commit 0a967a044a777e8b9c739120927114ddc0094298
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:15 2009 -0700

    reparent_thread: don't call kill_orphaned_pgrp() if task_detached()
    
    If task_detached(p) == T, then either
    
      a) p is not the main thread, we will find the group leader on the
         ->children list.
    
    or
    
      b) p is the group leader but its ->exit_state = EXIT_DEAD.  This
         can only happen when the last sub-thread has died, but in that case
         that thread has already called kill_orphaned_pgrp() from
         exit_notify().
    
    In both cases kill_orphaned_pgrp() looks bogus.
    
    Move the task_detached() check up and simplify the code, this is also
    right from the "common sense" pov: we should do nothing with the detached
    childs, except move them to the new parent's ->children list.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 576eae233b53..405e6877168b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -818,6 +818,8 @@ static void reparent_thread(struct task_struct *p, struct task_struct *father)
 
 	list_move_tail(&p->sibling, &p->real_parent->children);
 
+	if (task_detached(p))
+		return;
 	/* If this is a threaded reparent there is no need to
 	 * notify anyone anything has happened.
 	 */
@@ -825,15 +827,13 @@ static void reparent_thread(struct task_struct *p, struct task_struct *father)
 		return;
 
 	/* We don't want people slaying init.  */
-	if (!task_detached(p))
-		p->exit_signal = SIGCHLD;
+	p->exit_signal = SIGCHLD;
 
 	/* If we'd notified the old parent about this child's death,
 	 * also notify the new parent.
 	 */
 	if (!ptrace_reparented(p) &&
-	    p->exit_state == EXIT_ZOMBIE &&
-	    !task_detached(p) && thread_group_empty(p))
+	    p->exit_state == EXIT_ZOMBIE && thread_group_empty(p))
 		do_notify_parent(p, p->exit_signal);
 
 	kill_orphaned_pgrp(p, father);

commit b1b4c6799fb59e710454bfe0ab477cb8523a8667
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:13 2009 -0700

    ptrace: reintroduce __ptrace_detach() as a callee of ptrace_exit()
    
    No functional changes, preparation for the next patch.
    
    Move the "should we release this child" logic into the separate handler,
    __ptrace_detach().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7a8311422930..576eae233b53 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -744,6 +744,38 @@ static int ignoring_children(struct sighand_struct *sigh)
 	return ret;
 }
 
+/* Returns nonzero if the tracee should be released. */
+int __ptrace_detach(struct task_struct *tracer, struct task_struct *p)
+{
+	__ptrace_unlink(p);
+
+	if (p->exit_state != EXIT_ZOMBIE)
+		return 0;
+	/*
+	 * If it's a zombie, our attachedness prevented normal
+	 * parent notification or self-reaping.  Do notification
+	 * now if it would have happened earlier.  If it should
+	 * reap itself we return true.
+	 *
+	 * If it's our own child, there is no notification to do.
+	 * But if our normal children self-reap, then this child
+	 * was prevented by ptrace and we must reap it now.
+	 */
+	if (!task_detached(p) && thread_group_empty(p)) {
+		if (!same_thread_group(p->real_parent, tracer))
+			do_notify_parent(p, p->exit_signal);
+		else if (ignoring_children(tracer->sighand))
+			p->exit_signal = -1;
+	}
+
+	if (!task_detached(p))
+		return 0;
+
+	/* Mark it as in the process of being reaped. */
+	p->exit_state = EXIT_DEAD;
+	return 1;
+}
+
 /*
  * Detach all tasks we were using ptrace on.
  * Any that need to be release_task'd are put on the @dead list.
@@ -755,36 +787,8 @@ static void ptrace_exit(struct task_struct *parent, struct list_head *dead)
 	struct task_struct *p, *n;
 
 	list_for_each_entry_safe(p, n, &parent->ptraced, ptrace_entry) {
-		__ptrace_unlink(p);
-
-		if (p->exit_state != EXIT_ZOMBIE)
-			continue;
-
-		/*
-		 * If it's a zombie, our attachedness prevented normal
-		 * parent notification or self-reaping.  Do notification
-		 * now if it would have happened earlier.  If it should
-		 * reap itself, add it to the @dead list.  We can't call
-		 * release_task() here because we already hold tasklist_lock.
-		 *
-		 * If it's our own child, there is no notification to do.
-		 * But if our normal children self-reap, then this child
-		 * was prevented by ptrace and we must reap it now.
-		 */
-		if (!task_detached(p) && thread_group_empty(p)) {
-			if (!same_thread_group(p->real_parent, parent))
-				do_notify_parent(p, p->exit_signal);
-			else if (ignoring_children(parent->sighand))
-				p->exit_signal = -1;
-		}
-
-		if (task_detached(p)) {
-			/*
-			 * Mark it as in the process of being reaped.
-			 */
-			p->exit_state = EXIT_DEAD;
+		if (__ptrace_detach(parent, p))
 			list_add(&p->ptrace_entry, dead);
-		}
 	}
 }
 

commit 6d69cb87f05eef3b02370b2f7bae608ad2301a00
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:58:12 2009 -0700

    ptrace: simplify ptrace_exit()->ignoring_children() path
    
    ignoring_children() takes parent->sighand->siglock and checks
    k_sigaction[SIGCHLD] atomically.  But this buys nothing, we can't get the
    "really" wrong result even if we race with sigaction(SIGCHLD).  If we read
    the "stale" sa_handler/sa_flags we can pretend it was changed right after
    the check.
    
    Remove spin_lock(->siglock), and kill "int ign" which caches the result of
    ignoring_children() which becomes rather trivial.
    
    Perhaps it makes sense to export this helper, do_notify_parent() can use
    it too.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 0c06b9efae3b..7a8311422930 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -732,19 +732,15 @@ static void exit_mm(struct task_struct * tsk)
 }
 
 /*
- * Return nonzero if @parent's children should reap themselves.
- *
- * Called with write_lock_irq(&tasklist_lock) held.
+ * Called with irqs disabled, returns true if childs should reap themselves.
  */
-static int ignoring_children(struct task_struct *parent)
+static int ignoring_children(struct sighand_struct *sigh)
 {
 	int ret;
-	struct sighand_struct *psig = parent->sighand;
-	unsigned long flags;
-	spin_lock_irqsave(&psig->siglock, flags);
-	ret = (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||
-	       (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT));
-	spin_unlock_irqrestore(&psig->siglock, flags);
+	spin_lock(&sigh->siglock);
+	ret = (sigh->action[SIGCHLD-1].sa.sa_handler == SIG_IGN) ||
+	      (sigh->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT);
+	spin_unlock(&sigh->siglock);
 	return ret;
 }
 
@@ -757,7 +753,6 @@ static int ignoring_children(struct task_struct *parent)
 static void ptrace_exit(struct task_struct *parent, struct list_head *dead)
 {
 	struct task_struct *p, *n;
-	int ign = -1;
 
 	list_for_each_entry_safe(p, n, &parent->ptraced, ptrace_entry) {
 		__ptrace_unlink(p);
@@ -779,12 +774,8 @@ static void ptrace_exit(struct task_struct *parent, struct list_head *dead)
 		if (!task_detached(p) && thread_group_empty(p)) {
 			if (!same_thread_group(p->real_parent, parent))
 				do_notify_parent(p, p->exit_signal);
-			else {
-				if (ign < 0)
-					ign = ignoring_children(parent);
-				if (ign)
-					p->exit_signal = -1;
-			}
+			else if (ignoring_children(parent->sighand))
+				p->exit_signal = -1;
 		}
 
 		if (task_detached(p)) {

commit 90bc8d8b1a38f1ab131a2399a202e1889db95de8
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Apr 2 16:57:58 2009 -0700

    do_wait: fix waiting for the group stop with the dead leader
    
    do_wait(WSTOPPED) assumes that p->state must be == TASK_STOPPED, this is
    not true if the leader is already dead.  Check SIGNAL_STOP_STOPPED instead
    and use signal->group_exit_code.
    
    Trivial test-case:
    
            void *tfunc(void *arg)
            {
                    pause();
                    return NULL;
            }
    
            int main(void)
            {
                    pthread_t thr;
                    pthread_create(&thr, NULL, tfunc, NULL);
                    pthread_exit(NULL);
                    return 0;
            }
    
    It doesn't react to ^Z (and then to ^C or ^\). The task is stopped, but
    bash can't see this.
    
    The bug is very old, and it was reported multiple times. This patch was sent
    more than a year ago (http://marc.info/?t=119713920000003) but it was ignored.
    
    This change also fixes other oddities (but not all) in this area.  For
    example, before this patch:
    
            $ sleep 100
            ^Z
            [1]+  Stopped                 sleep 100
            $ strace -p `pidof sleep`
            Process 11442 attached - interrupt to quit
    
    strace hangs in do_wait(), because ->exit_code was already consumed by
    bash.  After this patch, strace happily proceeds:
    
            --- SIGTSTP (Stopped) @ 0 (0) ---
            restart_syscall(<... resuming interrupted call ...>
    
    To me, this looks much more "natural" and correct.
    
    Another example.  Let's suppose we have the main thread M and sub-thread
    T, the process is stopped, and its parent did wait(WSTOPPED).  Now we can
    ptrace T but not M.  This looks at least strange to me.
    
    Imho, do_wait() should not confuse the per-thread ptrace stops with the
    per-process job control stops.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Jan Kratochvil <jan.kratochvil@redhat.com>
    Cc: Kaz Kylheku <kkylheku@gmail.com>
    Cc: Michael Kerrisk <mtk.manpages@googlemail.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Ulrich Drepper <drepper@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 167e1e3ad7c6..0c06b9efae3b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1417,6 +1417,18 @@ static int wait_task_zombie(struct task_struct *p, int options,
 	return retval;
 }
 
+static int *task_stopped_code(struct task_struct *p, bool ptrace)
+{
+	if (ptrace) {
+		if (task_is_stopped_or_traced(p))
+			return &p->exit_code;
+	} else {
+		if (p->signal->flags & SIGNAL_STOP_STOPPED)
+			return &p->signal->group_exit_code;
+	}
+	return NULL;
+}
+
 /*
  * Handle sys_wait4 work for one task in state TASK_STOPPED.  We hold
  * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold
@@ -1427,7 +1439,7 @@ static int wait_task_stopped(int ptrace, struct task_struct *p,
 			     int options, struct siginfo __user *infop,
 			     int __user *stat_addr, struct rusage __user *ru)
 {
-	int retval, exit_code, why;
+	int retval, exit_code, *p_code, why;
 	uid_t uid = 0; /* unneeded, required by compiler */
 	pid_t pid;
 
@@ -1437,22 +1449,16 @@ static int wait_task_stopped(int ptrace, struct task_struct *p,
 	exit_code = 0;
 	spin_lock_irq(&p->sighand->siglock);
 
-	if (unlikely(!task_is_stopped_or_traced(p)))
-		goto unlock_sig;
-
-	if (!ptrace && p->signal->group_stop_count > 0)
-		/*
-		 * A group stop is in progress and this is the group leader.
-		 * We won't report until all threads have stopped.
-		 */
+	p_code = task_stopped_code(p, ptrace);
+	if (unlikely(!p_code))
 		goto unlock_sig;
 
-	exit_code = p->exit_code;
+	exit_code = *p_code;
 	if (!exit_code)
 		goto unlock_sig;
 
 	if (!unlikely(options & WNOWAIT))
-		p->exit_code = 0;
+		*p_code = 0;
 
 	/* don't need the RCU readlock here as we're holding a spinlock */
 	uid = __task_cred(p)->uid;
@@ -1608,7 +1614,7 @@ static int wait_consider_task(struct task_struct *parent, int ptrace,
 	 */
 	*notask_error = 0;
 
-	if (task_is_stopped_or_traced(p))
+	if (task_stopped_code(p, ptrace))
 		return wait_task_stopped(ptrace, p, options,
 					 infop, stat_addr, ru);
 

commit 5ad4e53bd5406ee214ddc5a41f03f779b8b2d526
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Mar 29 19:50:06 2009 -0400

    Get rid of indirect include of fs_struct.h
    
    Don't pull it in sched.h; very few files actually need it and those
    can include directly.  sched.h itself only needs forward declaration
    of struct fs_struct;
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index ad8375758a79..b5d656845c90 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -46,6 +46,7 @@
 #include <linux/blkdev.h>
 #include <linux/task_io_accounting_ops.h>
 #include <linux/tracehook.h>
+#include <linux/fs_struct.h>
 #include <linux/init_task.h>
 #include <trace/sched.h>
 

commit 3e93cd671813e204c258f1e6c797959920cf7772
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Mar 29 19:00:13 2009 -0400

    Take fs_struct handling to new file (fs/fs_struct.c)
    
    Pure code move; two new helper functions for nfsd and daemonize
    (unshare_fs_struct() and daemonize_fs_struct() resp.; for now -
    the same code as used to be in callers).  unshare_fs_struct()
    exported (for nfsd, as copy_fs_struct()/exit_fs() used to be),
    copy_fs_struct() and exit_fs() don't need exports anymore.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 167e1e3ad7c6..ad8375758a79 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -429,7 +429,6 @@ EXPORT_SYMBOL(disallow_signal);
 void daemonize(const char *name, ...)
 {
 	va_list args;
-	struct fs_struct *fs;
 	sigset_t blocked;
 
 	va_start(args, name);
@@ -462,11 +461,7 @@ void daemonize(const char *name, ...)
 
 	/* Become as one with the init task */
 
-	exit_fs(current);	/* current->fs->count--; */
-	fs = init_task.fs;
-	current->fs = fs;
-	atomic_inc(&fs->count);
-
+	daemonize_fs_struct();
 	exit_files(current);
 	current->files = init_task.files;
 	atomic_inc(&current->files->count);
@@ -565,30 +560,6 @@ void exit_files(struct task_struct *tsk)
 	}
 }
 
-void put_fs_struct(struct fs_struct *fs)
-{
-	/* No need to hold fs->lock if we are killing it */
-	if (atomic_dec_and_test(&fs->count)) {
-		path_put(&fs->root);
-		path_put(&fs->pwd);
-		kmem_cache_free(fs_cachep, fs);
-	}
-}
-
-void exit_fs(struct task_struct *tsk)
-{
-	struct fs_struct * fs = tsk->fs;
-
-	if (fs) {
-		task_lock(tsk);
-		tsk->fs = NULL;
-		task_unlock(tsk);
-		put_fs_struct(fs);
-	}
-}
-
-EXPORT_SYMBOL_GPL(exit_fs);
-
 #ifdef CONFIG_MM_OWNER
 /*
  * Task p is exiting and it owned mm, lets find a new owner for it

commit 3aa551c9b4c40018f0e261a178e3d25478dc04a9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Mar 23 18:28:15 2009 +0100

    genirq: add threaded interrupt handler support
    
    Add support for threaded interrupt handlers:
    
    A device driver can request that its main interrupt handler runs in a
    thread. To achive this the device driver requests the interrupt with
    request_threaded_irq() and provides additionally to the handler a
    thread function. The handler function is called in hard interrupt
    context and needs to check whether the interrupt originated from the
    device. If the interrupt originated from the device then the handler
    can either return IRQ_HANDLED or IRQ_WAKE_THREAD. IRQ_HANDLED is
    returned when no further action is required. IRQ_WAKE_THREAD causes
    the genirq code to invoke the threaded (main) handler. When
    IRQ_WAKE_THREAD is returned handler must have disabled the interrupt
    on the device level. This is mandatory for shared interrupt handlers,
    but we need to do it as well for obscure x86 hardware where disabling
    an interrupt on the IO_APIC level redirects the interrupt to the
    legacy PIC interrupt lines.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 167e1e3ad7c6..ca0b3488c4a9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1037,6 +1037,8 @@ NORET_TYPE void do_exit(long code)
 		schedule();
 	}
 
+	exit_irq_thread();
+
 	exit_signals(tsk);  /* sets PF_EXITING */
 	/*
 	 * tsk->flags are checked in the futex code to protect against

commit f8a6b2b9cee298a9663cbe38ce1eb5240987cb62
Merge: ba1511bf7fbd 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:44:22 2009 +0100

    Merge branch 'linus' into x86/apic
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c
            arch/x86/mm/fault.c

commit e9c4ffb11f0b19005b5b9dc8481687a3637e5887
Merge: 4bcf349a0f90 071a0bc2ceac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 13 09:34:07 2009 +0100

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            arch/x86/kernel/acpi/boot.c

commit 32bd671d6cbeda60dc73be77fa2b9037d9a9bfa0
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Feb 5 12:24:15 2009 +0100

    signal: re-add dead task accumulation stats.
    
    We're going to split the process wide cpu accounting into two parts:
    
     - clocks; which can take all the time they want since they run
               from user context.
    
     - timers; which need constant time tracing but can affort the overhead
               because they're default off -- and rare.
    
    The clock readout will go back to a full sum of the thread group, for this
    we need to re-add the exit stats that were removed in the initial itimer
    rework (f06febc9: timers: fix itimer/many thread hang).
    
    Furthermore, since that full sum can be rather slow for large thread groups
    and we have the complete dead task stats, revert the do_notify_parent time
    computation.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index f80dec3f1875..efd30ccf3858 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -118,6 +118,8 @@ static void __exit_signal(struct task_struct *tsk)
 		 * We won't ever get here for the group leader, since it
 		 * will have been the last reference on the signal_struct.
 		 */
+		sig->utime = cputime_add(sig->utime, task_utime(tsk));
+		sig->stime = cputime_add(sig->stime, task_stime(tsk));
 		sig->gtime = cputime_add(sig->gtime, task_gtime(tsk));
 		sig->min_flt += tsk->min_flt;
 		sig->maj_flt += tsk->maj_flt;
@@ -126,6 +128,7 @@ static void __exit_signal(struct task_struct *tsk)
 		sig->inblock += task_io_get_inblock(tsk);
 		sig->oublock += task_io_get_oublock(tsk);
 		task_io_accounting_add(&sig->ioac, &tsk->ioac);
+		sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 		sig = NULL; /* Marker for below. */
 	}
 

commit bfe2a3c3b5bf479788d5d5c5561346be6b169043
Merge: 77835492ed48 35d266a24796
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jan 23 10:20:15 2009 +0100

    Merge branch 'core/percpu' into perfcounters/core
    
    Conflicts:
            arch/x86/include/asm/hardirq_32.h
            arch/x86/include/asm/hardirq_64.h
    
    Semantic merge:
            arch/x86/include/asm/hardirq.h
            [ added apic_perf_irqs field. ]
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 77835492ed489c0b870f82f4c50687bd267acc0a
Merge: af37501c7921 1de9e8e70f5a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 21 16:37:27 2009 +0100

    Merge commit 'v2.6.29-rc2' into perfcounters/core
    
    Conflicts:
            include/linux/syscalls.h

commit 198030782cedf25391e67e7c88b04f87a5eb6563
Merge: 4ec71fa2d2c3 92181f190b64
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 21 10:39:51 2009 +0100

    Merge branch 'x86/mm' into core/percpu
    
    Conflicts:
            arch/x86/mm/fault.c

commit b2b062b8163391c42b3219d466ca1ac9742b9c7b
Merge: a9de18eb761f 99937d6455ce
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 18 18:37:14 2009 +0100

    Merge branch 'core/percpu' into stackprotector
    
    Conflicts:
            arch/x86/include/asm/pda.h
            arch/x86/include/asm/system.h
    
    Also, moved include/asm-x86/stackprotector.h to arch/x86/include/asm.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 17da2bd90abf428523de0fb98f7075e00e3ed42e
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 14 14:14:10 2009 +0100

    [CVE-2009-0029] System call wrappers part 08
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 08895df0eab3..f80dec3f1875 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1754,9 +1754,8 @@ static long do_wait(enum pid_type type, struct pid *pid, int options,
 	return retval;
 }
 
-asmlinkage long sys_waitid(int which, pid_t upid,
-			   struct siginfo __user *infop, int options,
-			   struct rusage __user *ru)
+SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,
+		infop, int, options, struct rusage __user *, ru)
 {
 	struct pid *pid = NULL;
 	enum pid_type type;
@@ -1833,7 +1832,7 @@ SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
  * sys_waitpid() remains for compatibility. waitpid() should be
  * implemented by calling sys_wait4() from libc.a.
  */
-asmlinkage long sys_waitpid(pid_t pid, int __user *stat_addr, int options)
+SYSCALL_DEFINE3(waitpid, pid_t, pid, int __user *, stat_addr, int, options)
 {
 	return sys_wait4(pid, stat_addr, options, NULL);
 }

commit 754fe8d297bfae7b77f7ce866e2fb0c5fb186506
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 14 14:14:09 2009 +0100

    [CVE-2009-0029] System call wrappers part 07
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index fac9b040af2c..08895df0eab3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1141,7 +1141,7 @@ NORET_TYPE void complete_and_exit(struct completion *comp, long code)
 
 EXPORT_SYMBOL(complete_and_exit);
 
-asmlinkage long sys_exit(int error_code)
+SYSCALL_DEFINE1(exit, int, error_code)
 {
 	do_exit((error_code&0xff)<<8);
 }
@@ -1182,7 +1182,7 @@ do_group_exit(int exit_code)
  * wait4()-ing process will get the correct exit code - even if this
  * thread is not the thread group leader.
  */
-asmlinkage long sys_exit_group(int error_code)
+SYSCALL_DEFINE1(exit_group, int, error_code)
 {
 	do_group_exit((error_code & 0xff) << 8);
 	/* NOTREACHED */
@@ -1795,8 +1795,8 @@ asmlinkage long sys_waitid(int which, pid_t upid,
 	return ret;
 }
 
-asmlinkage long sys_wait4(pid_t upid, int __user *stat_addr,
-			  int options, struct rusage __user *ru)
+SYSCALL_DEFINE4(wait4, pid_t, upid, int __user *, stat_addr,
+		int, options, struct rusage __user *, ru)
 {
 	struct pid *pid = NULL;
 	enum pid_type type;

commit 2ed7c03ec17779afb4fcfa3b8c61df61bd4879ba
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 14 14:13:54 2009 +0100

    [CVE-2009-0029] Convert all system calls to return a long
    
    Convert all system calls to return a long. This should be a NOP since all
    converted types should have the same size anyway.
    With the exception of sys_exit_group which returned void. But that doesn't
    matter since the system call doesn't return.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index c7740fa3252c..fac9b040af2c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1182,9 +1182,11 @@ do_group_exit(int exit_code)
  * wait4()-ing process will get the correct exit code - even if this
  * thread is not the thread group leader.
  */
-asmlinkage void sys_exit_group(int error_code)
+asmlinkage long sys_exit_group(int error_code)
 {
 	do_group_exit((error_code & 0xff) << 8);
+	/* NOTREACHED */
+	return 0;
 }
 
 static struct pid *task_pid_type(struct task_struct *task, enum pid_type type)

commit 506c10f26c481b7f8ef27c1c79290f68989b2e9e
Merge: e1df957670ae c59765042f53
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 11 02:42:53 2009 +0100

    Merge commit 'v2.6.29-rc1' into perfcounters/core
    
    Conflicts:
            include/linux/kernel_stat.h

commit 901608d9045146aec6f14a7777ea4b1501c379f0
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jan 6 14:40:29 2009 -0800

    mm: introduce get_mm_hiwater_xxx(), fix taskstats->hiwater_xxx accounting
    
    xacct_add_tsk() relies on do_exit()->update_hiwater_xxx() and uses
    mm->hiwater_xxx directly, this leads to 2 problems:
    
    - taskstats_user_cmd() can call fill_pid()->xacct_add_tsk() at any
      moment before the task exits, so we should check the current values of
      rss/vm anyway.
    
    - do_exit()->update_hiwater_xxx() calls are racy.  An exiting thread can
      be preempted right before mm->hiwater_xxx = new_val, and another thread
      can use A_LOT of memory and exit in between.  When the first thread
      resumes it can be the last thread in the thread group, in that case we
      report the wrong hiwater_xxx values which do not take A_LOT into
      account.
    
    Introduce get_mm_hiwater_rss() and get_mm_hiwater_vm() helpers and change
    xacct_add_tsk() to use them.  The first helper will also be used by
    rusage->ru_maxrss accounting.
    
    Kill do_exit()->update_hiwater_xxx() calls.  Unless we are going to
    decrease rss/vm there is no point to update mm->hiwater_xxx, and nobody
    can look at this mm_struct when exit_mmap() actually unmaps the memory.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Hugh Dickins <hugh@veritas.com>
    Reviewed-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f923724ab3c9..c7740fa3252c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1051,10 +1051,7 @@ NORET_TYPE void do_exit(long code)
 				preempt_count());
 
 	acct_update_integrals(tsk);
-	if (tsk->mm) {
-		update_hiwater_rss(tsk->mm);
-		update_hiwater_vm(tsk->mm);
-	}
+
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
 		hrtimer_cancel(&tsk->signal->real_timer);

commit e5991371ee0d1c0ce19e133c6f9075b49c5b4ae8
Author: Hugh Dickins <hugh@veritas.com>
Date:   Tue Jan 6 14:39:22 2009 -0800

    mm: remove cgroup_mm_owner_callbacks
    
    cgroup_mm_owner_callbacks() was brought in to support the memrlimit
    controller, but sneaked into mainline ahead of it.  That controller has
    now been shelved, and the mm_owner_changed() args were inadequate for it
    anyway (they needed an mm pointer instead of a task pointer).
    
    Remove the dead code, and restore mm_update_next_owner() locking to how it
    was before: taking mmap_sem there does nothing for memcontrol.c, now the
    only user of mm->owner.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Cc: Paul Menage <menage@google.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c9e5a1c14e08..f923724ab3c9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -642,35 +642,31 @@ void mm_update_next_owner(struct mm_struct *mm)
 	/*
 	 * We found no owner yet mm_users > 1: this implies that we are
 	 * most likely racing with swapoff (try_to_unuse()) or /proc or
-	 * ptrace or page migration (get_task_mm()).  Mark owner as NULL,
-	 * so that subsystems can understand the callback and take action.
+	 * ptrace or page migration (get_task_mm()).  Mark owner as NULL.
 	 */
-	down_write(&mm->mmap_sem);
-	cgroup_mm_owner_callbacks(mm->owner, NULL);
 	mm->owner = NULL;
-	up_write(&mm->mmap_sem);
 	return;
 
 assign_new_owner:
 	BUG_ON(c == p);
 	get_task_struct(c);
-	read_unlock(&tasklist_lock);
-	down_write(&mm->mmap_sem);
 	/*
 	 * The task_lock protects c->mm from changing.
 	 * We always want mm->owner->mm == mm
 	 */
 	task_lock(c);
+	/*
+	 * Delay read_unlock() till we have the task_lock()
+	 * to ensure that c does not slip away underneath us
+	 */
+	read_unlock(&tasklist_lock);
 	if (c->mm != mm) {
 		task_unlock(c);
-		up_write(&mm->mmap_sem);
 		put_task_struct(c);
 		goto retry;
 	}
-	cgroup_mm_owner_callbacks(mm->owner, c);
 	mm->owner = c;
 	task_unlock(c);
-	up_write(&mm->mmap_sem);
 	put_task_struct(c);
 }
 #endif /* CONFIG_MM_OWNER */

commit a9de18eb761f7c1c860964b2e5addc1a35c7e861
Merge: b2aaf8f74cdc 6a94cb73064c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 31 08:31:57 2008 +0100

    Merge branch 'linus' into stackprotector
    
    Conflicts:
            arch/x86/include/asm/pda.h
            kernel/fork.c

commit 1dff81f20cd55ffa5a8ee984da70ce0b99d29606
Merge: 179475a3b46f d3f761104b09
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 30 17:20:05 2008 -0800

    Merge branch 'for-2.6.29' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.29' of git://git.kernel.dk/linux-2.6-block: (43 commits)
      bio: get rid of bio_vec clearing
      bounce: don't rely on a zeroed bio_vec list
      cciss: simplify parameters to deregister_disk function
      cfq-iosched: fix race between exiting queue and exiting task
      loop: Do not call loop_unplug for not configured loop device.
      loop: Flush possible running bios when loop device is released.
      alpha: remove dead BIO_VMERGE_BOUNDARY
      Get rid of CONFIG_LSF
      block: make blk_softirq_init() static
      block: use min_not_zero in blk_queue_stack_limits
      block: add one-hit cache for disk partition lookup
      cfq-iosched: remove limit of dispatch depth of max 4 times quantum
      nbd: tell the block layer that it is not a rotational device
      block: get rid of elevator_t typedef
      aio: make the lookup_ioctx() lockless
      bio: add support for inlining a number of bio_vecs inside the bio
      bio: allow individual slabs in the bio_set
      bio: move the slab pointer inside the bio_set
      bio: only mempool back the largest bio_vec slab cache
      block: don't use plugging on SSD devices
      ...

commit 5f34fe1cfc1bdd8b4711bbe37421fba4ed0d1ed4
Merge: eca1bf5b4fab 6638101c1124
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 30 16:10:19 2008 -0800

    Merge branch 'core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (63 commits)
      stacktrace: provide save_stack_trace_tsk() weak alias
      rcu: provide RCU options on non-preempt architectures too
      printk: fix discarding message when recursion_bug
      futex: clean up futex_(un)lock_pi fault handling
      "Tree RCU": scalable classic RCU implementation
      futex: rename field in futex_q to clarify single waiter semantics
      x86/swiotlb: add default swiotlb_arch_range_needs_mapping
      x86/swiotlb: add default phys<->bus conversion
      x86: unify pci iommu setup and allow swiotlb to compile for 32 bit
      x86: add swiotlb allocation functions
      swiotlb: consolidate swiotlb info message printing
      swiotlb: support bouncing of HighMem pages
      swiotlb: factor out copy to/from device
      swiotlb: add arch hook to force mapping
      swiotlb: allow architectures to override phys<->bus<->phys conversions
      swiotlb: add comment where we handle the overflow of a dma mask on 32 bit
      rcu: fix rcutorture behavior during reboot
      resources: skip sanity check of busy resources
      swiotlb: move some definitions to header
      swiotlb: allow architectures to override swiotlb pool allocation
      ...
    
    Fix up trivial conflicts in
      arch/x86/kernel/Makefile
      arch/x86/mm/init_32.c
      include/linux/hardirq.h
    as per Ingo's suggestions.

commit e1df957670aef74ffd9a4ad93e6d2c90bf6b4845
Merge: 2b583d8bc8d7 3c92ec8ae91e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Dec 29 09:45:15 2008 +0100

    Merge branch 'linus' into perfcounters/core
    
    Conflicts:
            fs/exec.c
            include/linux/init_task.h
    
    Simple context conflicts.

commit 7c0990c7ee988aa193abbb7da3faeb9279146dbf
Author: Nikanth Karthikesan <knikanth@suse.de>
Date:   Wed Nov 19 10:20:23 2008 +0100

    Do not free io context when taking recursive faults in do_exit
    
    When taking recursive faults in do_exit, if the io_context is not null,
    exit_io_context() is being called. But it might decrement the refcount
    more than once. It is better to leave this task alone.
    
    Signed-off-by: Nikanth Karthikesan <knikanth@suse.de>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index c7422ca92038..9a213474f54a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1037,8 +1037,6 @@ NORET_TYPE void do_exit(long code)
 		 * task into the wait for ever nirwana as well.
 		 */
 		tsk->flags |= PF_EXITPIDONE;
-		if (tsk->io_context)
-			exit_io_context();
 		set_current_state(TASK_UNINTERRUPTIBLE);
 		schedule();
 	}

commit b0f4b285d7ed174804658539129a834270f4829a
Merge: be9c5ae4eeec 5250d329e38c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 28 12:21:10 2008 -0800

    Merge branch 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (241 commits)
      sched, trace: update trace_sched_wakeup()
      tracing/ftrace: don't trace on early stage of a secondary cpu boot, v3
      Revert "x86: disable X86_PTRACE_BTS"
      ring-buffer: prevent false positive warning
      ring-buffer: fix dangling commit race
      ftrace: enable format arguments checking
      x86, bts: memory accounting
      x86, bts: add fork and exit handling
      ftrace: introduce tracing_reset_online_cpus() helper
      tracing: fix warnings in kernel/trace/trace_sched_switch.c
      tracing: fix warning in kernel/trace/trace.c
      tracing/ring-buffer: remove unused ring_buffer size
      trace: fix task state printout
      ftrace: add not to regex on filtering functions
      trace: better use of stack_trace_enabled for boot up code
      trace: add a way to enable or disable the stack tracer
      x86: entry_64 - introduce FTRACE_ frame macro v2
      tracing/ftrace: add the printk-msg-only option
      tracing/ftrace: use preempt_enable_no_resched_notrace in ring_buffer_time_stamp()
      x86, bts: correctly report invalid bts records
      ...
    
    Fixed up trivial conflict in scripts/recordmcount.pl due to SH bits
    being already partly merged by the SH merge.

commit eef6cbf5844c620d9db9be99e4908cdf92492fb9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 19 10:20:42 2008 +0100

    perfcounters: pull inherited counters
    
    Change counter inheritance from a 'push' to a 'pull' model: instead of
    child tasks pushing their final counts to the parent, reuse the wait4
    infrastructure to pull counters as child tasks are exit-processed,
    much like how cutime/cstime is collected.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 244edfd96865..101b7eeff44c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -153,6 +153,9 @@ static void delayed_put_task_struct(struct rcu_head *rhp)
 {
 	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
 
+#ifdef CONFIG_PERF_COUNTERS
+	WARN_ON_ONCE(!list_empty(&tsk->perf_counter_ctx.counter_list));
+#endif
 	trace_sched_process_free(tsk);
 	put_task_struct(tsk);
 }
@@ -922,12 +925,6 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	forget_original_parent(tsk);
 	exit_task_namespaces(tsk);
 
-	/*
-	 * Flush inherited counters to the parent - before the parent
-	 * gets woken up by child-exit notifications.
-	 */
-	perf_counter_exit_task(tsk);
-
 	write_lock_irq(&tasklist_lock);
 	if (group_dead)
 		kill_orphaned_pgrp(tsk->group_leader, NULL);
@@ -1122,12 +1119,6 @@ NORET_TYPE void do_exit(long code)
 	if (tsk->splice_pipe)
 		__free_pipe_info(tsk->splice_pipe);
 
-	/*
-	 * These must happen late, after the PID is not
-	 * hashed anymore, but still at a point that may sleep:
-	 */
-	perf_counter_exit_task(tsk);
-
 	preempt_disable();
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;
@@ -1371,6 +1362,12 @@ static int wait_task_zombie(struct task_struct *p, int options,
 	 */
 	read_unlock(&tasklist_lock);
 
+	/*
+	 * Flush inherited counters to the parent - before the parent
+	 * gets woken up by child-exit notifications.
+	 */
+	perf_counter_exit_task(p);
+
 	retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
 	status = (p->signal->flags & SIGNAL_GROUP_EXIT)
 		? p->signal->group_exit_code : p->exit_code;

commit aa9c4c0f967fdb482ea95e8473ec3d201e6e0781
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Dec 17 14:10:57 2008 +0100

    perfcounters: fix task clock counter
    
    Impact: fix per task clock counter precision
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index d336c90a5f13..244edfd96865 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -922,6 +922,12 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	forget_original_parent(tsk);
 	exit_task_namespaces(tsk);
 
+	/*
+	 * Flush inherited counters to the parent - before the parent
+	 * gets woken up by child-exit notifications.
+	 */
+	perf_counter_exit_task(tsk);
+
 	write_lock_irq(&tasklist_lock);
 	if (group_dead)
 		kill_orphaned_pgrp(tsk->group_leader, NULL);
@@ -1093,11 +1099,6 @@ NORET_TYPE void do_exit(long code)
 	mpol_put(tsk->mempolicy);
 	tsk->mempolicy = NULL;
 #endif
-	/*
-	 * These must happen late, after the PID is not
-	 * hashed anymore, but still at a point that may sleep:
-	 */
-	perf_counter_exit_task(tsk);
 #ifdef CONFIG_FUTEX
 	if (unlikely(!list_empty(&tsk->pi_state_list)))
 		exit_pi_state_list(tsk);
@@ -1121,6 +1122,12 @@ NORET_TYPE void do_exit(long code)
 	if (tsk->splice_pipe)
 		__free_pipe_info(tsk->splice_pipe);
 
+	/*
+	 * These must happen late, after the PID is not
+	 * hashed anymore, but still at a point that may sleep:
+	 */
+	perf_counter_exit_task(tsk);
+
 	preempt_disable();
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;

commit 9b51f66dcb09ac5eb6bc68fc111d5c7a1e0131d6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 12 13:49:45 2008 +0100

    perfcounters: implement "counter inheritance"
    
    Impact: implement new performance feature
    
    Counter inheritance can be used to run performance counters in a workload,
    transparently - and pipe back the counter results to the parent counter.
    
    Inheritance for performance counters works the following way: when creating
    a counter it can be marked with the .inherit=1 flag. Such counters are then
    'inherited' by all child tasks (be they fork()-ed or clone()-ed). These
    counters get inherited through exec() boundaries as well (except through
    setuid boundaries).
    
    The counter values get added back to the parent counter(s) when the child
    task(s) exit - much like stime/utime statistics are gathered. So inherited
    counters are ideal to gather summary statistics about an application's
    behavior via shell commands, without having to modify that application.
    
    The timec.c command utilizes counter inheritance:
    
      http://redhat.com/~mingo/perfcounters/timec.c
    
    Sample output:
    
       $ ./timec -e 1 -e 3 -e 5 ls -lR /usr/include/ >/dev/null
    
       Performance counter stats for 'ls':
    
               163516953 instructions
                    2295 cache-misses
                 2855182 branch-misses
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2d8be7ebb0f7..d336c90a5f13 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1093,11 +1093,12 @@ NORET_TYPE void do_exit(long code)
 	mpol_put(tsk->mempolicy);
 	tsk->mempolicy = NULL;
 #endif
-#ifdef CONFIG_FUTEX
 	/*
-	 * This must happen late, after the PID is not
-	 * hashed anymore:
+	 * These must happen late, after the PID is not
+	 * hashed anymore, but still at a point that may sleep:
 	 */
+	perf_counter_exit_task(tsk);
+#ifdef CONFIG_FUTEX
 	if (unlikely(!list_empty(&tsk->pi_state_list)))
 		exit_pi_state_list(tsk);
 	if (unlikely(current->pi_state_cache))

commit b19b3c74c7bbec45a848631b8f970ac110665a01
Merge: ed313489bade 6003ab0bad4c 42569c39917a 7918baa55514 29cbda77a67c 2b5fe6de5827 b0788caf7af7 8dd2337470d2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Nov 24 17:44:55 2008 +0100

    Merge branches 'core/debug', 'core/futexes', 'core/locking', 'core/rcu', 'core/signal', 'core/urgent' and 'core/xen' into core/core

commit 65afa5e603d507014580ead016ec887b49e1afa6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 23 18:43:39 2008 +0100

    tracing/function-return-tracer: free the return stack on free_task()
    
    Impact: avoid losing some traces when a task is freed
    
    do_exit() is not the last function called when a task finishes.
    There are still some functions which are to be called such as
    ree_task().  So we delay the freeing of the return stack to the
    last moment.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index ef04d03b3286..e5ae36ebe8af 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -47,7 +47,6 @@
 #include <linux/task_io_accounting_ops.h>
 #include <linux/tracehook.h>
 #include <trace/sched.h>
-#include <linux/ftrace.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -1128,7 +1127,6 @@ NORET_TYPE void do_exit(long code)
 	preempt_disable();
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;
-	ftrace_retfunc_exit_task(tsk);
 	schedule();
 	BUG();
 	/* Avoid "noreturn function does return".  */

commit 82f60f0bc854aada696f27d863c03bef91f1509d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Nov 23 09:18:56 2008 +0100

    tracing/function-return-tracer: clean up task start/exit callbacks
    
    Impact: cleanup
    
    Eliminate #ifdefs in core code by using empty inline functions.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index b9d446329da1..ef04d03b3286 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1128,9 +1128,7 @@ NORET_TYPE void do_exit(long code)
 	preempt_disable();
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;
-#ifdef CONFIG_FUNCTION_RET_TRACER
 	ftrace_retfunc_exit_task(tsk);
-#endif
 	schedule();
 	BUG();
 	/* Avoid "noreturn function does return".  */

commit f201ae2356c74bcae130b2177b3dca903ea98071
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 23 06:22:56 2008 +0100

    tracing/function-return-tracer: store return stack into task_struct and allocate it dynamically
    
    Impact: use deeper function tracing depth safely
    
    Some tests showed that function return tracing needed a more deeper depth
    of function calls. But it could be unsafe to store these return addresses
    to the stack.
    
    So these arrays will now be allocated dynamically into task_struct of current
    only when the tracer is activated.
    
    Typical scheme when tracer is activated:
    - allocate a return stack for each task in global list.
    - fork: allocate the return stack for the newly created task
    - exit: free return stack of current
    - idle init: same as fork
    
    I chose a default depth of 50. I don't have overruns anymore.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 35c8ec2ba03a..b9d446329da1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -47,6 +47,7 @@
 #include <linux/task_io_accounting_ops.h>
 #include <linux/tracehook.h>
 #include <trace/sched.h>
+#include <linux/ftrace.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -1127,7 +1128,9 @@ NORET_TYPE void do_exit(long code)
 	preempt_disable();
 	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = TASK_DEAD;
-
+#ifdef CONFIG_FUNCTION_RET_TRACER
+	ftrace_retfunc_exit_task(tsk);
+#endif
 	schedule();
 	BUG();
 	/* Avoid "noreturn function does return".  */

commit 9676e73a9e0cbdc521e1ebf4e13e6e5aada34247
Merge: 5a209c2d58e7 86fa2f606745 6d5b43a67acc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Nov 19 10:04:25 2008 +0100

    Merge branches 'tracing/ftrace' and 'tracing/urgent' into tracing/core
    
    Conflicts:
            kernel/trace/ftrace.c
    
    [ We conflicted here because we backported a few fixes to
      tracing/urgent - which has different internal APIs. ]

commit f3a5c547012a09f38f7c27b17a8e3150b69cd259
Merge: e50a906e0200 4e14e833ac3b
Author: James Morris <jmorris@namei.org>
Date:   Tue Nov 18 18:52:37 2008 +1100

    Merge branch 'master' into next
    
    Conflicts:
            fs/cifs/misc.c
    
    Merge to resolve above, per the patch below.
    
    Signed-off-by: James Morris <jmorris@namei.org>
    
    diff --cc fs/cifs/misc.c
    index ec36410,addd1dc..0000000
    --- a/fs/cifs/misc.c
    +++ b/fs/cifs/misc.c
    @@@ -347,13 -338,13 +338,13 @@@ header_assemble(struct smb_hdr *buffer
                    /*  BB Add support for establishing new tCon and SMB Session  */
                    /*      with userid/password pairs found on the smb session   */
                    /*      for other target tcp/ip addresses               BB    */
     -                              if (current->fsuid != treeCon->ses->linux_uid) {
     +                              if (current_fsuid() != treeCon->ses->linux_uid) {
                                            cFYI(1, ("Multiuser mode and UID "
                                                     "did not match tcon uid"));
    -                                       read_lock(&GlobalSMBSeslock);
    -                                       list_for_each(temp_item, &GlobalSMBSessionList) {
    -                                               ses = list_entry(temp_item, struct cifsSesInfo, cifsSessionList);
    +                                       read_lock(&cifs_tcp_ses_lock);
    +                                       list_for_each(temp_item, &treeCon->ses->server->smb_ses_list) {
    +                                               ses = list_entry(temp_item, struct cifsSesInfo, smb_ses_list);
     -                                              if (ses->linux_uid == current->fsuid) {
     +                                              if (ses->linux_uid == current_fsuid()) {
                                                            if (ses->server == treeCon->ses->server) {
                                                                    cFYI(1, ("found matching uid substitute right smb_uid"));
                                                                    buffer->Uid = ses->Suid;

commit 2b5fe6de58276d0b5a7c884d5dbfc300ca47db78
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Nov 17 15:40:08 2008 +0100

    thread_group_cputime: move a couple of callsites outside of ->siglock
    
    Impact: relax the locking of cpu-time accounting calls
    
    ->siglock buys nothing for thread_group_cputime() in do_sys_times() and
    wait_task_zombie() (which btw takes the unrelated parent's ->siglock).
    
    Actually I think do_sys_times() doesn't need ->siglock at all.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index ae2b92be5fae..b9c4d8bb72e5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1330,10 +1330,10 @@ static int wait_task_zombie(struct task_struct *p, int options,
 		 * group, which consolidates times for all threads in the
 		 * group including the group leader.
 		 */
+		thread_group_cputime(p, &cputime);
 		spin_lock_irq(&p->parent->sighand->siglock);
 		psig = p->parent->signal;
 		sig = p->signal;
-		thread_group_cputime(p, &cputime);
 		psig->cutime =
 			cputime_add(psig->cutime,
 			cputime_add(cputime.utime,

commit 7e066fb870fcd1025ec3ba7bbde5d541094f4ce1
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Fri Nov 14 17:47:47 2008 -0500

    tracepoints: add DECLARE_TRACE() and DEFINE_TRACE()
    
    Impact: API *CHANGE*. Must update all tracepoint users.
    
    Add DEFINE_TRACE() to tracepoints to let them declare the tracepoint
    structure in a single spot for all the kernel. It helps reducing memory
    consumption, especially when declaring a lot of tracepoints, e.g. for
    kmalloc tracing.
    
    *API CHANGE WARNING*: now, DECLARE_TRACE() must be used in headers for
    tracepoint declarations rather than DEFINE_TRACE(). This is the sane way
    to do it. The name previously used was misleading.
    
    Updates scheduler instrumentation to follow this API change.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index ae2b92be5fae..f995d2418668 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -54,6 +54,10 @@
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
 
+DEFINE_TRACE(sched_process_free);
+DEFINE_TRACE(sched_process_exit);
+DEFINE_TRACE(sched_process_wait);
+
 static void exit_mm(struct task_struct * tsk);
 
 static inline int task_detached(struct task_struct *p)

commit 8141c7f3e7aee618312fa1c15109e1219de784a7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 15 10:20:36 2008 -0800

    Move "exit_robust_list" into mm_release()
    
    We don't want to get rid of the futexes just at exit() time, we want to
    drop them when doing an execve() too, since that gets rid of the
    previous VM image too.
    
    Doing it at mm_release() time means that we automatically always do it
    when we disassociate a VM map from the task.
    
    Reported-by: pageexec@freemail.hu
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Nick Piggin <npiggin@suse.de>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Brad Spengler <spender@grsecurity.net>
    Cc: Alex Efros <powerman@powerman.name>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ae2b92be5fae..2d8be7ebb0f7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -40,7 +40,6 @@
 #include <linux/cn_proc.h>
 #include <linux/mutex.h>
 #include <linux/futex.h>
-#include <linux/compat.h>
 #include <linux/pipe_fs_i.h>
 #include <linux/audit.h> /* for audit_free() */
 #include <linux/resource.h>
@@ -1059,14 +1058,6 @@ NORET_TYPE void do_exit(long code)
 		exit_itimers(tsk->signal);
 	}
 	acct_collect(code, group_dead);
-#ifdef CONFIG_FUTEX
-	if (unlikely(tsk->robust_list))
-		exit_robust_list(tsk);
-#ifdef CONFIG_COMPAT
-	if (unlikely(tsk->compat_robust_list))
-		compat_exit_robust_list(tsk);
-#endif
-#endif
 	if (group_dead)
 		tty_audit_exit();
 	if (unlikely(tsk->audit_context))

commit 2b828925652340277a889cbc11b2d0637f7cdaf7
Merge: 3a3b7ce93369 58e20d8d344b
Author: James Morris <jmorris@namei.org>
Date:   Fri Nov 14 11:29:12 2008 +1100

    Merge branch 'master' into next
    
    Conflicts:
            security/keys/internal.h
            security/keys/process_keys.c
            security/keys/request_key.c
    
    Fixed conflicts above by using the non 'tsk' versions.
    
    Signed-off-by: James Morris <jmorris@namei.org>

commit d84f4f992cbd76e8f39c488cf0c5d123843923b1
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:23 2008 +1100

    CRED: Inaugurate COW credentials
    
    Inaugurate copy-on-write credentials management.  This uses RCU to manage the
    credentials pointer in the task_struct with respect to accesses by other tasks.
    A process may only modify its own credentials, and so does not need locking to
    access or modify its own credentials.
    
    A mutex (cred_replace_mutex) is added to the task_struct to control the effect
    of PTRACE_ATTACHED on credential calculations, particularly with respect to
    execve().
    
    With this patch, the contents of an active credentials struct may not be
    changed directly; rather a new set of credentials must be prepared, modified
    and committed using something like the following sequence of events:
    
            struct cred *new = prepare_creds();
            int ret = blah(new);
            if (ret < 0) {
                    abort_creds(new);
                    return ret;
            }
            return commit_creds(new);
    
    There are some exceptions to this rule: the keyrings pointed to by the active
    credentials may be instantiated - keyrings violate the COW rule as managing
    COW keyrings is tricky, given that it is possible for a task to directly alter
    the keys in a keyring in use by another task.
    
    To help enforce this, various pointers to sets of credentials, such as those in
    the task_struct, are declared const.  The purpose of this is compile-time
    discouragement of altering credentials through those pointers.  Once a set of
    credentials has been made public through one of these pointers, it may not be
    modified, except under special circumstances:
    
      (1) Its reference count may incremented and decremented.
    
      (2) The keyrings to which it points may be modified, but not replaced.
    
    The only safe way to modify anything else is to create a replacement and commit
    using the functions described in Documentation/credentials.txt (which will be
    added by a later patch).
    
    This patch and the preceding patches have been tested with the LTP SELinux
    testsuite.
    
    This patch makes several logical sets of alteration:
    
     (1) execve().
    
         This now prepares and commits credentials in various places in the
         security code rather than altering the current creds directly.
    
     (2) Temporary credential overrides.
    
         do_coredump() and sys_faccessat() now prepare their own credentials and
         temporarily override the ones currently on the acting thread, whilst
         preventing interference from other threads by holding cred_replace_mutex
         on the thread being dumped.
    
         This will be replaced in a future patch by something that hands down the
         credentials directly to the functions being called, rather than altering
         the task's objective credentials.
    
     (3) LSM interface.
    
         A number of functions have been changed, added or removed:
    
         (*) security_capset_check(), ->capset_check()
         (*) security_capset_set(), ->capset_set()
    
             Removed in favour of security_capset().
    
         (*) security_capset(), ->capset()
    
             New.  This is passed a pointer to the new creds, a pointer to the old
             creds and the proposed capability sets.  It should fill in the new
             creds or return an error.  All pointers, barring the pointer to the
             new creds, are now const.
    
         (*) security_bprm_apply_creds(), ->bprm_apply_creds()
    
             Changed; now returns a value, which will cause the process to be
             killed if it's an error.
    
         (*) security_task_alloc(), ->task_alloc_security()
    
             Removed in favour of security_prepare_creds().
    
         (*) security_cred_free(), ->cred_free()
    
             New.  Free security data attached to cred->security.
    
         (*) security_prepare_creds(), ->cred_prepare()
    
             New. Duplicate any security data attached to cred->security.
    
         (*) security_commit_creds(), ->cred_commit()
    
             New. Apply any security effects for the upcoming installation of new
             security by commit_creds().
    
         (*) security_task_post_setuid(), ->task_post_setuid()
    
             Removed in favour of security_task_fix_setuid().
    
         (*) security_task_fix_setuid(), ->task_fix_setuid()
    
             Fix up the proposed new credentials for setuid().  This is used by
             cap_set_fix_setuid() to implicitly adjust capabilities in line with
             setuid() changes.  Changes are made to the new credentials, rather
             than the task itself as in security_task_post_setuid().
    
         (*) security_task_reparent_to_init(), ->task_reparent_to_init()
    
             Removed.  Instead the task being reparented to init is referred
             directly to init's credentials.
    
             NOTE!  This results in the loss of some state: SELinux's osid no
             longer records the sid of the thread that forked it.
    
         (*) security_key_alloc(), ->key_alloc()
         (*) security_key_permission(), ->key_permission()
    
             Changed.  These now take cred pointers rather than task pointers to
             refer to the security context.
    
     (4) sys_capset().
    
         This has been simplified and uses less locking.  The LSM functions it
         calls have been merged.
    
     (5) reparent_to_kthreadd().
    
         This gives the current thread the same credentials as init by simply using
         commit_thread() to point that way.
    
     (6) __sigqueue_alloc() and switch_uid()
    
         __sigqueue_alloc() can't stop the target task from changing its creds
         beneath it, so this function gets a reference to the currently applicable
         user_struct which it then passes into the sigqueue struct it returns if
         successful.
    
         switch_uid() is now called from commit_creds(), and possibly should be
         folded into that.  commit_creds() should take care of protecting
         __sigqueue_alloc().
    
     (7) [sg]et[ug]id() and co and [sg]et_current_groups.
    
         The set functions now all use prepare_creds(), commit_creds() and
         abort_creds() to build and check a new set of credentials before applying
         it.
    
         security_task_set[ug]id() is called inside the prepared section.  This
         guarantees that nothing else will affect the creds until we've finished.
    
         The calling of set_dumpable() has been moved into commit_creds().
    
         Much of the functionality of set_user() has been moved into
         commit_creds().
    
         The get functions all simply access the data directly.
    
     (8) security_task_prctl() and cap_task_prctl().
    
         security_task_prctl() has been modified to return -ENOSYS if it doesn't
         want to handle a function, or otherwise return the return value directly
         rather than through an argument.
    
         Additionally, cap_task_prctl() now prepares a new set of credentials, even
         if it doesn't end up using it.
    
     (9) Keyrings.
    
         A number of changes have been made to the keyrings code:
    
         (a) switch_uid_keyring(), copy_keys(), exit_keys() and suid_keys() have
             all been dropped and built in to the credentials functions directly.
             They may want separating out again later.
    
         (b) key_alloc() and search_process_keyrings() now take a cred pointer
             rather than a task pointer to specify the security context.
    
         (c) copy_creds() gives a new thread within the same thread group a new
             thread keyring if its parent had one, otherwise it discards the thread
             keyring.
    
         (d) The authorisation key now points directly to the credentials to extend
             the search into rather pointing to the task that carries them.
    
         (e) Installing thread, process or session keyrings causes a new set of
             credentials to be created, even though it's not strictly necessary for
             process or session keyrings (they're shared).
    
    (10) Usermode helper.
    
         The usermode helper code now carries a cred struct pointer in its
         subprocess_info struct instead of a new session keyring pointer.  This set
         of credentials is derived from init_cred and installed on the new process
         after it has been cloned.
    
         call_usermodehelper_setup() allocates the new credentials and
         call_usermodehelper_freeinfo() discards them if they haven't been used.  A
         special cred function (prepare_usermodeinfo_creds()) is provided
         specifically for call_usermodehelper_setup() to call.
    
         call_usermodehelper_setkeys() adjusts the credentials to sport the
         supplied keyring as the new session keyring.
    
    (11) SELinux.
    
         SELinux has a number of changes, in addition to those to support the LSM
         interface changes mentioned above:
    
         (a) selinux_setprocattr() no longer does its check for whether the
             current ptracer can access processes with the new SID inside the lock
             that covers getting the ptracer's SID.  Whilst this lock ensures that
             the check is done with the ptracer pinned, the result is only valid
             until the lock is released, so there's no point doing it inside the
             lock.
    
    (12) is_single_threaded().
    
         This function has been extracted from selinux_setprocattr() and put into
         a file of its own in the lib/ directory as join_session_keyring() now
         wants to use it too.
    
         The code in SELinux just checked to see whether a task shared mm_structs
         with other tasks (CLONE_VM), but that isn't good enough.  We really want
         to know if they're part of the same thread group (CLONE_THREAD).
    
    (13) nfsd.
    
         The NFS server daemon now has to use the COW credentials to set the
         credentials it is going to use.  It really needs to pass the credentials
         down to the functions it calls, but it can't do that until other patches
         in this series have been applied.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index bbc22530f2c1..c0711da15486 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -47,12 +47,14 @@
 #include <linux/blkdev.h>
 #include <linux/task_io_accounting_ops.h>
 #include <linux/tracehook.h>
+#include <linux/init_task.h>
 #include <trace/sched.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
+#include "cred-internals.h"
 
 static void exit_mm(struct task_struct * tsk);
 
@@ -338,12 +340,12 @@ static void reparent_to_kthreadd(void)
 	/* cpus_allowed? */
 	/* rt_priority? */
 	/* signals? */
-	security_task_reparent_to_init(current);
 	memcpy(current->signal->rlim, init_task.signal->rlim,
 	       sizeof(current->signal->rlim));
-	atomic_inc(&(INIT_USER->__count));
+
+	atomic_inc(&init_cred.usage);
+	commit_creds(&init_cred);
 	write_unlock_irq(&tasklist_lock);
-	switch_uid(INIT_USER);
 }
 
 void __set_special_pids(struct pid *pid)
@@ -1085,7 +1087,6 @@ NORET_TYPE void do_exit(long code)
 	check_stack_usage();
 	exit_thread();
 	cgroup_exit(tsk, 1);
-	exit_keys(tsk);
 
 	if (group_dead && tsk->signal->leader)
 		disassociate_ctty(1);

commit c69e8d9c01db2adc503464993c358901c9af9de4
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:19 2008 +1100

    CRED: Use RCU to access another task's creds and to release a task's own creds
    
    Use RCU to access another task's creds and to release a task's own creds.
    This means that it will be possible for the credentials of a task to be
    replaced without another task (a) requiring a full lock to read them, and (b)
    seeing deallocated memory.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e0f6e1892fb9..bbc22530f2c1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -160,7 +160,10 @@ void release_task(struct task_struct * p)
 	int zap_leader;
 repeat:
 	tracehook_prepare_release_task(p);
-	atomic_dec(&p->cred->user->processes);
+	/* don't need to get the RCU readlock here - the process is dead and
+	 * can't be modifying its own credentials */
+	atomic_dec(&__task_cred(p)->user->processes);
+
 	proc_flush_task(p);
 	write_lock_irq(&tasklist_lock);
 	tracehook_finish_release_task(p);
@@ -1267,12 +1270,12 @@ static int wait_task_zombie(struct task_struct *p, int options,
 	unsigned long state;
 	int retval, status, traced;
 	pid_t pid = task_pid_vnr(p);
+	uid_t uid = __task_cred(p)->uid;
 
 	if (!likely(options & WEXITED))
 		return 0;
 
 	if (unlikely(options & WNOWAIT)) {
-		uid_t uid = p->cred->uid;
 		int exit_code = p->exit_code;
 		int why, status;
 
@@ -1393,7 +1396,7 @@ static int wait_task_zombie(struct task_struct *p, int options,
 	if (!retval && infop)
 		retval = put_user(pid, &infop->si_pid);
 	if (!retval && infop)
-		retval = put_user(p->cred->uid, &infop->si_uid);
+		retval = put_user(uid, &infop->si_uid);
 	if (!retval)
 		retval = pid;
 
@@ -1458,7 +1461,8 @@ static int wait_task_stopped(int ptrace, struct task_struct *p,
 	if (!unlikely(options & WNOWAIT))
 		p->exit_code = 0;
 
-	uid = p->cred->uid;
+	/* don't need the RCU readlock here as we're holding a spinlock */
+	uid = __task_cred(p)->uid;
 unlock_sig:
 	spin_unlock_irq(&p->sighand->siglock);
 	if (!exit_code)
@@ -1532,10 +1536,10 @@ static int wait_task_continued(struct task_struct *p, int options,
 	}
 	if (!unlikely(options & WNOWAIT))
 		p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
+	uid = __task_cred(p)->uid;
 	spin_unlock_irq(&p->sighand->siglock);
 
 	pid = task_pid_vnr(p);
-	uid = p->cred->uid;
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);
 

commit b6dff3ec5e116e3af6f537d4caedcad6b9e5082a
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:16 2008 +1100

    CRED: Separate task security context from task_struct
    
    Separate the task security context from task_struct.  At this point, the
    security data is temporarily embedded in the task_struct with two pointers
    pointing to it.
    
    Note that the Alpha arch is altered as it refers to (E)UID and (E)GID in
    entry.S via asm-offsets.
    
    With comment fixes Signed-off-by: Marc Dionne <marc.c.dionne@gmail.com>
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 80137a5d9467..e0f6e1892fb9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -160,7 +160,7 @@ void release_task(struct task_struct * p)
 	int zap_leader;
 repeat:
 	tracehook_prepare_release_task(p);
-	atomic_dec(&p->user->processes);
+	atomic_dec(&p->cred->user->processes);
 	proc_flush_task(p);
 	write_lock_irq(&tasklist_lock);
 	tracehook_finish_release_task(p);
@@ -1272,7 +1272,7 @@ static int wait_task_zombie(struct task_struct *p, int options,
 		return 0;
 
 	if (unlikely(options & WNOWAIT)) {
-		uid_t uid = p->uid;
+		uid_t uid = p->cred->uid;
 		int exit_code = p->exit_code;
 		int why, status;
 
@@ -1393,7 +1393,7 @@ static int wait_task_zombie(struct task_struct *p, int options,
 	if (!retval && infop)
 		retval = put_user(pid, &infop->si_pid);
 	if (!retval && infop)
-		retval = put_user(p->uid, &infop->si_uid);
+		retval = put_user(p->cred->uid, &infop->si_uid);
 	if (!retval)
 		retval = pid;
 
@@ -1458,7 +1458,7 @@ static int wait_task_stopped(int ptrace, struct task_struct *p,
 	if (!unlikely(options & WNOWAIT))
 		p->exit_code = 0;
 
-	uid = p->uid;
+	uid = p->cred->uid;
 unlock_sig:
 	spin_unlock_irq(&p->sighand->siglock);
 	if (!exit_code)
@@ -1535,7 +1535,7 @@ static int wait_task_continued(struct task_struct *p, int options,
 	spin_unlock_irq(&p->sighand->siglock);
 
 	pid = task_pid_vnr(p);
-	uid = p->uid;
+	uid = p->cred->uid;
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);
 

commit ad474caca3e2a0550b7ce0706527ad5ab389a4d4
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Nov 10 15:39:30 2008 +0100

    fix for account_group_exec_runtime(), make sure ->signal can't be freed under rq->lock
    
    Impact: fix hang/crash on ia64 under high load
    
    This is ugly, but the simplest patch by far.
    
    Unlike other similar routines, account_group_exec_runtime() could be
    called "implicitly" from within scheduler after exit_notify(). This
    means we can race with the parent doing release_task(), we can't just
    check ->signal != NULL.
    
    Change __exit_signal() to do spin_unlock_wait(&task_rq(tsk)->lock)
    before __cleanup_signal() to make sure ->signal can't be freed under
    task_rq(tsk)->lock. Note that task_rq_unlock_wait() doesn't care
    about the case when tsk changes cpu/rq under us, this should be OK.
    
    Thanks to Ingo who nacked my previous buggy patch.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Reported-by: Doug Chapman <doug.chapman@hp.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 80137a5d9467..ae2b92be5fae 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -141,6 +141,11 @@ static void __exit_signal(struct task_struct *tsk)
 	if (sig) {
 		flush_sigqueue(&sig->shared_pending);
 		taskstats_tgid_free(sig);
+		/*
+		 * Make sure ->signal can't go away under rq->lock,
+		 * see account_group_exec_runtime().
+		 */
+		task_rq_unlock_wait(tsk);
 		__cleanup_signal(sig);
 	}
 }

commit 92b29b86fe2e183d44eb467e5e74a5f718ef2e43
Merge: b9d7ccf56be1 98d9c66ab074
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 20 13:35:07 2008 -0700

    Merge branch 'tracing-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-v28-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (131 commits)
      tracing/fastboot: improve help text
      tracing/stacktrace: improve help text
      tracing/fastboot: fix initcalls disposition in bootgraph.pl
      tracing/fastboot: fix bootgraph.pl initcall name regexp
      tracing/fastboot: fix issues and improve output of bootgraph.pl
      tracepoints: synchronize unregister static inline
      tracepoints: tracepoint_synchronize_unregister()
      ftrace: make ftrace_test_p6nop disassembler-friendly
      markers: fix synchronize marker unregister static inline
      tracing/fastboot: add better resolution to initcall debug/tracing
      trace: add build-time check to avoid overrunning hex buffer
      ftrace: fix hex output mode of ftrace
      tracing/fastboot: fix initcalls disposition in bootgraph.pl
      tracing/fastboot: fix printk format typo in boot tracer
      ftrace: return an error when setting a nonexistent tracer
      ftrace: make some tracers reentrant
      ring-buffer: make reentrant
      ring-buffer: move page indexes into page headers
      tracing/fastboot: only trace non-module initcalls
      ftrace: move pc counter in irqtrace
      ...
    
    Manually fix conflicts:
     - init/main.c: initcall tracing
     - kernel/module.c: verbose level vs tracepoints
     - scripts/bootgraph.pl: fallout from cherry-picking commits.

commit c465a76af658b443075d6efee1c3131257643020
Merge: 2d42244ae71d 1b02469088ac fb02fbc14d17 d40e944c25fb 1508487e7f16 322acf6585f3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Oct 20 13:14:06 2008 +0200

    Merge branches 'timers/clocksource', 'timers/hrtimers', 'timers/nohz', 'timers/ntp', 'timers/posixtimers' and 'timers/debug' into v28-timers-for-linus

commit 9363b9f23c9cc36cc8ef6c05fdf879ee4a96ae92
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Wed Oct 15 22:01:05 2008 -0700

    memrlimit: cgroup mm owner callback changes to add task info
    
    This patch adds an additional field to the mm_owner callbacks. This field
    is required to get to the mm that changed. Hold mmap_sem in write mode
    before calling the mm_owner_changed callback
    
    [hugh@veritas.com: fix mmap_sem deadlock]
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Sudhir Kumar <skumar@linux.vnet.ibm.com>
    Cc: YAMAMOTO Takashi <yamamoto@valinux.co.jp>
    Cc: Paul Menage <menage@google.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Pavel Emelianov <xemul@openvz.org>
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vivek Goyal <vgoyal@redhat.com>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 85a83c831856..0ef4673e351b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -640,24 +640,23 @@ void mm_update_next_owner(struct mm_struct *mm)
 assign_new_owner:
 	BUG_ON(c == p);
 	get_task_struct(c);
+	read_unlock(&tasklist_lock);
+	down_write(&mm->mmap_sem);
 	/*
 	 * The task_lock protects c->mm from changing.
 	 * We always want mm->owner->mm == mm
 	 */
 	task_lock(c);
-	/*
-	 * Delay read_unlock() till we have the task_lock()
-	 * to ensure that c does not slip away underneath us
-	 */
-	read_unlock(&tasklist_lock);
 	if (c->mm != mm) {
 		task_unlock(c);
+		up_write(&mm->mmap_sem);
 		put_task_struct(c);
 		goto retry;
 	}
 	cgroup_mm_owner_callbacks(mm->owner, c);
 	mm->owner = c;
 	task_unlock(c);
+	up_write(&mm->mmap_sem);
 	put_task_struct(c);
 }
 #endif /* CONFIG_MM_OWNER */

commit b2aaf8f74cdc84a9182f6cabf198b7763bcb9d40
Merge: 4f962d4d6592 278429cff880
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Oct 15 13:46:29 2008 +0200

    Merge branch 'linus' into stackprotector
    
    Conflicts:
            arch/x86/kernel/Makefile
            include/asm-x86/pda.h

commit 0a16b6075843325dc402edf80c1662838b929aff
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Fri Jul 18 12:16:17 2008 -0400

    tracing, sched: LTTng instrumentation - scheduler
    
    Instrument the scheduler activity (sched_switch, migration, wakeups,
    wait for a task, signal delivery) and process/thread
    creation/destruction (fork, exit, kthread stop). Actually, kthread
    creation is not instrumented in this patch because it is architecture
    dependent. It allows to connect tracers such as ftrace which detects
    scheduling latencies, good/bad scheduler decisions. Tools like LTTng can
    export this scheduler information along with instrumentation of the rest
    of the kernel activity to perform post-mortem analysis on the scheduler
    activity.
    
    About the performance impact of tracepoints (which is comparable to
    markers), even without immediate values optimizations, tests done by
    Hideo Aoki on ia64 show no regression. His test case was using hackbench
    on a kernel where scheduler instrumentation (about 5 events in code
    scheduler code) was added. See the "Tracepoints" patch header for
    performance result detail.
    
    Changelog :
    
    - Change instrumentation location and parameter to match ftrace
      instrumentation, previously done with kernel markers.
    
    [ mingo@elte.hu: conflict resolutions ]
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Acked-by: 'Peter Zijlstra' <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 85a83c831856..7b71f87f1207 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -47,6 +47,7 @@
 #include <linux/blkdev.h>
 #include <linux/task_io_accounting_ops.h>
 #include <linux/tracehook.h>
+#include <trace/sched.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -149,7 +150,10 @@ static void __exit_signal(struct task_struct *tsk)
 
 static void delayed_put_task_struct(struct rcu_head *rhp)
 {
-	put_task_struct(container_of(rhp, struct task_struct, rcu));
+	struct task_struct *tsk = container_of(rhp, struct task_struct, rcu);
+
+	trace_sched_process_free(tsk);
+	put_task_struct(tsk);
 }
 
 
@@ -1074,6 +1078,8 @@ NORET_TYPE void do_exit(long code)
 
 	if (group_dead)
 		acct_process();
+	trace_sched_process_exit(tsk);
+
 	exit_sem(tsk);
 	exit_files(tsk);
 	exit_fs(tsk);
@@ -1675,6 +1681,8 @@ static long do_wait(enum pid_type type, struct pid *pid, int options,
 	struct task_struct *tsk;
 	int retval;
 
+	trace_sched_process_wait(pid);
+
 	add_wait_queue(&current->signal->wait_chldexit,&wait);
 repeat:
 	/*

commit 31a78f23bac0069004e69f98808b6988baccb6b6
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Sun Sep 28 23:09:31 2008 +0100

    mm owner: fix race between swapoff and exit
    
    There's a race between mm->owner assignment and swapoff, more easily
    seen when task slab poisoning is turned on.  The condition occurs when
    try_to_unuse() runs in parallel with an exiting task.  A similar race
    can occur with callers of get_task_mm(), such as /proc/<pid>/<mmstats>
    or ptrace or page migration.
    
    CPU0                                    CPU1
                                            try_to_unuse
                                            looks at mm = task0->mm
                                            increments mm->mm_users
    task 0 exits
    mm->owner needs to be updated, but no
    new owner is found (mm_users > 1, but
    no other task has task->mm = task0->mm)
    mm_update_next_owner() leaves
                                            mmput(mm) decrements mm->mm_users
    task0 freed
                                            dereferencing mm->owner fails
    
    The fix is to notify the subsystem via mm_owner_changed callback(),
    if no new owner is found, by specifying the new task as NULL.
    
    Jiri Slaby:
    mm->owner was set to NULL prior to calling cgroup_mm_owner_callbacks(), but
    must be set after that, so as not to pass NULL as old owner causing oops.
    
    Daisuke Nishimura:
    mm_update_next_owner() may set mm->owner to NULL, but mem_cgroup_from_task()
    and its callers need to take account of this situation to avoid oops.
    
    Hugh Dickins:
    Lockdep warning and hang below exec_mmap() when testing these patches.
    exit_mm() up_reads mmap_sem before calling mm_update_next_owner(),
    so exec_mmap() now needs to do the same.  And with that repositioning,
    there's now no point in mm_need_new_owner() allowing for NULL mm.
    
    Reported-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Jiri Slaby <jirislaby@gmail.com>
    Signed-off-by: Daisuke Nishimura <nishimura@mxp.nes.nec.co.jp>
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Paul Menage <menage@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 16395644a98f..85a83c831856 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -583,8 +583,6 @@ mm_need_new_owner(struct mm_struct *mm, struct task_struct *p)
 	 * If there are other users of the mm and the owner (us) is exiting
 	 * we need to find a new owner to take on the responsibility.
 	 */
-	if (!mm)
-		return 0;
 	if (atomic_read(&mm->mm_users) <= 1)
 		return 0;
 	if (mm->owner != p)
@@ -627,6 +625,16 @@ void mm_update_next_owner(struct mm_struct *mm)
 	} while_each_thread(g, c);
 
 	read_unlock(&tasklist_lock);
+	/*
+	 * We found no owner yet mm_users > 1: this implies that we are
+	 * most likely racing with swapoff (try_to_unuse()) or /proc or
+	 * ptrace or page migration (get_task_mm()).  Mark owner as NULL,
+	 * so that subsystems can understand the callback and take action.
+	 */
+	down_write(&mm->mmap_sem);
+	cgroup_mm_owner_callbacks(mm->owner, NULL);
+	mm->owner = NULL;
+	up_write(&mm->mmap_sem);
 	return;
 
 assign_new_owner:

commit f06febc96ba8e0af80bcc3eaec0a109e88275fac
Author: Frank Mayhar <fmayhar@google.com>
Date:   Fri Sep 12 09:54:39 2008 -0700

    timers: fix itimer/many thread hang
    
    Overview
    
    This patch reworks the handling of POSIX CPU timers, including the
    ITIMER_PROF, ITIMER_VIRT timers and rlimit handling.  It was put together
    with the help of Roland McGrath, the owner and original writer of this code.
    
    The problem we ran into, and the reason for this rework, has to do with using
    a profiling timer in a process with a large number of threads.  It appears
    that the performance of the old implementation of run_posix_cpu_timers() was
    at least O(n*3) (where "n" is the number of threads in a process) or worse.
    Everything is fine with an increasing number of threads until the time taken
    for that routine to run becomes the same as or greater than the tick time, at
    which point things degrade rather quickly.
    
    This patch fixes bug 9906, "Weird hang with NPTL and SIGPROF."
    
    Code Changes
    
    This rework corrects the implementation of run_posix_cpu_timers() to make it
    run in constant time for a particular machine.  (Performance may vary between
    one machine and another depending upon whether the kernel is built as single-
    or multiprocessor and, in the latter case, depending upon the number of
    running processors.)  To do this, at each tick we now update fields in
    signal_struct as well as task_struct.  The run_posix_cpu_timers() function
    uses those fields to make its decisions.
    
    We define a new structure, "task_cputime," to contain user, system and
    scheduler times and use these in appropriate places:
    
    struct task_cputime {
            cputime_t utime;
            cputime_t stime;
            unsigned long long sum_exec_runtime;
    };
    
    This is included in the structure "thread_group_cputime," which is a new
    substructure of signal_struct and which varies for uniprocessor versus
    multiprocessor kernels.  For uniprocessor kernels, it uses "task_cputime" as
    a simple substructure, while for multiprocessor kernels it is a pointer:
    
    struct thread_group_cputime {
            struct task_cputime totals;
    };
    
    struct thread_group_cputime {
            struct task_cputime *totals;
    };
    
    We also add a new task_cputime substructure directly to signal_struct, to
    cache the earliest expiration of process-wide timers, and task_cputime also
    replaces the it_*_expires fields of task_struct (used for earliest expiration
    of thread timers).  The "thread_group_cputime" structure contains process-wide
    timers that are updated via account_user_time() and friends.  In the non-SMP
    case the structure is a simple aggregator; unfortunately in the SMP case that
    simplicity was not achievable due to cache-line contention between CPUs (in
    one measured case performance was actually _worse_ on a 16-cpu system than
    the same test on a 4-cpu system, due to this contention).  For SMP, the
    thread_group_cputime counters are maintained as a per-cpu structure allocated
    using alloc_percpu().  The timer functions update only the timer field in
    the structure corresponding to the running CPU, obtained using per_cpu_ptr().
    
    We define a set of inline functions in sched.h that we use to maintain the
    thread_group_cputime structure and hide the differences between UP and SMP
    implementations from the rest of the kernel.  The thread_group_cputime_init()
    function initializes the thread_group_cputime structure for the given task.
    The thread_group_cputime_alloc() is a no-op for UP; for SMP it calls the
    out-of-line function thread_group_cputime_alloc_smp() to allocate and fill
    in the per-cpu structures and fields.  The thread_group_cputime_free()
    function, also a no-op for UP, in SMP frees the per-cpu structures.  The
    thread_group_cputime_clone_thread() function (also a UP no-op) for SMP calls
    thread_group_cputime_alloc() if the per-cpu structures haven't yet been
    allocated.  The thread_group_cputime() function fills the task_cputime
    structure it is passed with the contents of the thread_group_cputime fields;
    in UP it's that simple but in SMP it must also safely check that tsk->signal
    is non-NULL (if it is it just uses the appropriate fields of task_struct) and,
    if so, sums the per-cpu values for each online CPU.  Finally, the three
    functions account_group_user_time(), account_group_system_time() and
    account_group_exec_runtime() are used by timer functions to update the
    respective fields of the thread_group_cputime structure.
    
    Non-SMP operation is trivial and will not be mentioned further.
    
    The per-cpu structure is always allocated when a task creates its first new
    thread, via a call to thread_group_cputime_clone_thread() from copy_signal().
    It is freed at process exit via a call to thread_group_cputime_free() from
    cleanup_signal().
    
    All functions that formerly summed utime/stime/sum_sched_runtime values from
    from all threads in the thread group now use thread_group_cputime() to
    snapshot the values in the thread_group_cputime structure or the values in
    the task structure itself if the per-cpu structure hasn't been allocated.
    
    Finally, the code in kernel/posix-cpu-timers.c has changed quite a bit.
    The run_posix_cpu_timers() function has been split into a fast path and a
    slow path; the former safely checks whether there are any expired thread
    timers and, if not, just returns, while the slow path does the heavy lifting.
    With the dedicated thread group fields, timers are no longer "rebalanced" and
    the process_timer_rebalance() function and related code has gone away.  All
    summing loops are gone and all code that used them now uses the
    thread_group_cputime() inline.  When process-wide timers are set, the new
    task_cputime structure in signal_struct is used to cache the earliest
    expiration; this is checked in the fast path.
    
    Performance
    
    The fix appears not to add significant overhead to existing operations.  It
    generally performs the same as the current code except in two cases, one in
    which it performs slightly worse (Case 5 below) and one in which it performs
    very significantly better (Case 2 below).  Overall it's a wash except in those
    two cases.
    
    I've since done somewhat more involved testing on a dual-core Opteron system.
    
    Case 1: With no itimer running, for a test with 100,000 threads, the fixed
            kernel took 1428.5 seconds, 513 seconds more than the unfixed system,
            all of which was spent in the system.  There were twice as many
            voluntary context switches with the fix as without it.
    
    Case 2: With an itimer running at .01 second ticks and 4000 threads (the most
            an unmodified kernel can handle), the fixed kernel ran the test in
            eight percent of the time (5.8 seconds as opposed to 70 seconds) and
            had better tick accuracy (.012 seconds per tick as opposed to .023
            seconds per tick).
    
    Case 3: A 4000-thread test with an initial timer tick of .01 second and an
            interval of 10,000 seconds (i.e. a timer that ticks only once) had
            very nearly the same performance in both cases:  6.3 seconds elapsed
            for the fixed kernel versus 5.5 seconds for the unfixed kernel.
    
    With fewer threads (eight in these tests), the Case 1 test ran in essentially
    the same time on both the modified and unmodified kernels (5.2 seconds versus
    5.8 seconds).  The Case 2 test ran in about the same time as well, 5.9 seconds
    versus 5.4 seconds but again with much better tick accuracy, .013 seconds per
    tick versus .025 seconds per tick for the unmodified kernel.
    
    Since the fix affected the rlimit code, I also tested soft and hard CPU limits.
    
    Case 4: With a hard CPU limit of 20 seconds and eight threads (and an itimer
            running), the modified kernel was very slightly favored in that while
            it killed the process in 19.997 seconds of CPU time (5.002 seconds of
            wall time), only .003 seconds of that was system time, the rest was
            user time.  The unmodified kernel killed the process in 20.001 seconds
            of CPU (5.014 seconds of wall time) of which .016 seconds was system
            time.  Really, though, the results were too close to call.  The results
            were essentially the same with no itimer running.
    
    Case 5: With a soft limit of 20 seconds and a hard limit of 2000 seconds
            (where the hard limit would never be reached) and an itimer running,
            the modified kernel exhibited worse tick accuracy than the unmodified
            kernel: .050 seconds/tick versus .028 seconds/tick.  Otherwise,
            performance was almost indistinguishable.  With no itimer running this
            test exhibited virtually identical behavior and times in both cases.
    
    In times past I did some limited performance testing.  those results are below.
    
    On a four-cpu Opteron system without this fix, a sixteen-thread test executed
    in 3569.991 seconds, of which user was 3568.435s and system was 1.556s.  On
    the same system with the fix, user and elapsed time were about the same, but
    system time dropped to 0.007 seconds.  Performance with eight, four and one
    thread were comparable.  Interestingly, the timer ticks with the fix seemed
    more accurate:  The sixteen-thread test with the fix received 149543 ticks
    for 0.024 seconds per tick, while the same test without the fix received 58720
    for 0.061 seconds per tick.  Both cases were configured for an interval of
    0.01 seconds.  Again, the other tests were comparable.  Each thread in this
    test computed the primes up to 25,000,000.
    
    I also did a test with a large number of threads, 100,000 threads, which is
    impossible without the fix.  In this case each thread computed the primes only
    up to 10,000 (to make the runtime manageable).  System time dominated, at
    1546.968 seconds out of a total 2176.906 seconds (giving a user time of
    629.938s).  It received 147651 ticks for 0.015 seconds per tick, still quite
    accurate.  There is obviously no comparable test without the fix.
    
    Signed-off-by: Frank Mayhar <fmayhar@google.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 16395644a98f..40036ac04271 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -112,8 +112,6 @@ static void __exit_signal(struct task_struct *tsk)
 		 * We won't ever get here for the group leader, since it
 		 * will have been the last reference on the signal_struct.
 		 */
-		sig->utime = cputime_add(sig->utime, task_utime(tsk));
-		sig->stime = cputime_add(sig->stime, task_stime(tsk));
 		sig->gtime = cputime_add(sig->gtime, task_gtime(tsk));
 		sig->min_flt += tsk->min_flt;
 		sig->maj_flt += tsk->maj_flt;
@@ -122,7 +120,6 @@ static void __exit_signal(struct task_struct *tsk)
 		sig->inblock += task_io_get_inblock(tsk);
 		sig->oublock += task_io_get_oublock(tsk);
 		task_io_accounting_add(&sig->ioac, &tsk->ioac);
-		sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 		sig = NULL; /* Marker for below. */
 	}
 
@@ -1294,6 +1291,7 @@ static int wait_task_zombie(struct task_struct *p, int options,
 	if (likely(!traced)) {
 		struct signal_struct *psig;
 		struct signal_struct *sig;
+		struct task_cputime cputime;
 
 		/*
 		 * The resource counters for the group leader are in its
@@ -1309,20 +1307,23 @@ static int wait_task_zombie(struct task_struct *p, int options,
 		 * need to protect the access to p->parent->signal fields,
 		 * as other threads in the parent group can be right
 		 * here reaping other children at the same time.
+		 *
+		 * We use thread_group_cputime() to get times for the thread
+		 * group, which consolidates times for all threads in the
+		 * group including the group leader.
 		 */
 		spin_lock_irq(&p->parent->sighand->siglock);
 		psig = p->parent->signal;
 		sig = p->signal;
+		thread_group_cputime(p, &cputime);
 		psig->cutime =
 			cputime_add(psig->cutime,
-			cputime_add(p->utime,
-			cputime_add(sig->utime,
-				    sig->cutime)));
+			cputime_add(cputime.utime,
+				    sig->cutime));
 		psig->cstime =
 			cputime_add(psig->cstime,
-			cputime_add(p->stime,
-			cputime_add(sig->stime,
-				    sig->cstime)));
+			cputime_add(cputime.stime,
+				    sig->cstime));
 		psig->cgtime =
 			cputime_add(psig->cgtime,
 			cputime_add(p->gtime,

commit 49048622eae698e5c4ae61f7e71200f265ccc529
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Fri Sep 5 18:12:23 2008 +0200

    sched: fix process time monotonicity
    
    Spencer reported a problem where utime and stime were going negative despite
    the fixes in commit b27f03d4bdc145a09fb7b0c0e004b29f1ee555fa. The suspected
    reason for the problem is that signal_struct maintains it's own utime and
    stime (of exited tasks), these are not updated using the new task_utime()
    routine, hence sig->utime can go backwards and cause the same problem
    to occur (sig->utime, adds tsk->utime and not task_utime()). This patch
    fixes the problem
    
    TODO: using max(task->prev_utime, derived utime) works for now, but a more
    generic solution is to implement cputime_max() and use the cputime_gt()
    function for comparison.
    
    Reported-by: spencer@bluehost.com
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 25ed2ad986df..16395644a98f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -112,9 +112,9 @@ static void __exit_signal(struct task_struct *tsk)
 		 * We won't ever get here for the group leader, since it
 		 * will have been the last reference on the signal_struct.
 		 */
-		sig->utime = cputime_add(sig->utime, tsk->utime);
-		sig->stime = cputime_add(sig->stime, tsk->stime);
-		sig->gtime = cputime_add(sig->gtime, tsk->gtime);
+		sig->utime = cputime_add(sig->utime, task_utime(tsk));
+		sig->stime = cputime_add(sig->stime, task_stime(tsk));
+		sig->gtime = cputime_add(sig->gtime, task_gtime(tsk));
 		sig->min_flt += tsk->min_flt;
 		sig->maj_flt += tsk->maj_flt;
 		sig->nvcsw += tsk->nvcsw;

commit 950bbabb5a804690a0201190de5c22837f72f83f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Sep 2 14:35:49 2008 -0700

    pid_ns: (BUG 11391) change ->child_reaper when init->group_leader exits
    
    We don't change pid_ns->child_reaper when the main thread of the
    subnamespace init exits.  As Robert Rex <robert.rex@exasol.com> pointed
    out this is wrong.
    
    Yes, the re-parenting itself works correctly, but if the reparented task
    exits it needs ->parent->nsproxy->pid_ns in do_notify_parent(), and if the
    main thread is zombie its ->nsproxy was already cleared by
    exit_task_namespaces().
    
    Introduce the new function, find_new_reaper(), which finds the new
    ->parent for the re-parenting and changes ->child_reaper if needed.  Kill
    the now unneeded exit_child_reaper().
    
    Also move the changing of ->child_reaper from zap_pid_ns_processes() to
    find_new_reaper(), this consolidates the games with ->child_reaper and
    makes it stable under tasklist_lock.
    
    Addresses http://bugzilla.kernel.org/show_bug.cgi?id=11391
    
    Reported-by: Robert Rex <robert.rex@exasol.com>
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Acked-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 75c647387639..25ed2ad986df 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -831,26 +831,50 @@ static void reparent_thread(struct task_struct *p, struct task_struct *father)
  * the child reaper process (ie "init") in our pid
  * space.
  */
+static struct task_struct *find_new_reaper(struct task_struct *father)
+{
+	struct pid_namespace *pid_ns = task_active_pid_ns(father);
+	struct task_struct *thread;
+
+	thread = father;
+	while_each_thread(father, thread) {
+		if (thread->flags & PF_EXITING)
+			continue;
+		if (unlikely(pid_ns->child_reaper == father))
+			pid_ns->child_reaper = thread;
+		return thread;
+	}
+
+	if (unlikely(pid_ns->child_reaper == father)) {
+		write_unlock_irq(&tasklist_lock);
+		if (unlikely(pid_ns == &init_pid_ns))
+			panic("Attempted to kill init!");
+
+		zap_pid_ns_processes(pid_ns);
+		write_lock_irq(&tasklist_lock);
+		/*
+		 * We can not clear ->child_reaper or leave it alone.
+		 * There may by stealth EXIT_DEAD tasks on ->children,
+		 * forget_original_parent() must move them somewhere.
+		 */
+		pid_ns->child_reaper = init_pid_ns.child_reaper;
+	}
+
+	return pid_ns->child_reaper;
+}
+
 static void forget_original_parent(struct task_struct *father)
 {
-	struct task_struct *p, *n, *reaper = father;
+	struct task_struct *p, *n, *reaper;
 	LIST_HEAD(ptrace_dead);
 
 	write_lock_irq(&tasklist_lock);
-
+	reaper = find_new_reaper(father);
 	/*
 	 * First clean up ptrace if we were using it.
 	 */
 	ptrace_exit(father, &ptrace_dead);
 
-	do {
-		reaper = next_thread(reaper);
-		if (reaper == father) {
-			reaper = task_child_reaper(father);
-			break;
-		}
-	} while (reaper->flags & PF_EXITING);
-
 	list_for_each_entry_safe(p, n, &father->children, sibling) {
 		p->real_parent = reaper;
 		if (p->parent == father) {
@@ -959,39 +983,6 @@ static void check_stack_usage(void)
 static inline void check_stack_usage(void) {}
 #endif
 
-static inline void exit_child_reaper(struct task_struct *tsk)
-{
-	if (likely(tsk->group_leader != task_child_reaper(tsk)))
-		return;
-
-	if (tsk->nsproxy->pid_ns == &init_pid_ns)
-		panic("Attempted to kill init!");
-
-	/*
-	 * @tsk is the last thread in the 'cgroup-init' and is exiting.
-	 * Terminate all remaining processes in the namespace and reap them
-	 * before exiting @tsk.
-	 *
-	 * Note that @tsk (last thread of cgroup-init) may not necessarily
-	 * be the child-reaper (i.e main thread of cgroup-init) of the
-	 * namespace i.e the child_reaper may have already exited.
-	 *
-	 * Even after a child_reaper exits, we let it inherit orphaned children,
-	 * because, pid_ns->child_reaper remains valid as long as there is
-	 * at least one living sub-thread in the cgroup init.
-
-	 * This living sub-thread of the cgroup-init will be notified when
-	 * a child inherited by the 'child-reaper' exits (do_notify_parent()
-	 * uses __group_send_sig_info()). Further, when reaping child processes,
-	 * do_wait() iterates over children of all living sub threads.
-
-	 * i.e even though 'child_reaper' thread is listed as the parent of the
-	 * orphaned children, any living sub-thread in the cgroup-init can
-	 * perform the role of the child_reaper.
-	 */
-	zap_pid_ns_processes(tsk->nsproxy->pid_ns);
-}
-
 NORET_TYPE void do_exit(long code)
 {
 	struct task_struct *tsk = current;
@@ -1051,7 +1042,6 @@ NORET_TYPE void do_exit(long code)
 	}
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
-		exit_child_reaper(tsk);
 		hrtimer_cancel(&tsk->signal->real_timer);
 		exit_itimers(tsk->signal);
 	}

commit 2633f0e57b1127f4060d70bf460140dc9bb19386
Author: Steve VanDeBogart <vandebo-lkml@NerdBox.Net>
Date:   Tue Aug 26 15:14:36 2008 -0700

    exit signals: use of uninitialized field notify_count
    
    task->signal->notify_count is only initialized if
    task->signal->group_exit_task is not NULL.  Reorder a conditional so
    that uninitialised memory is not used.  Found by Valgrind.
    
    Signed-off-by: Steve VanDeBogart <vandebo-lkml@nerdbox.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 38ec40630149..75c647387639 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -918,8 +918,8 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 
 	/* mt-exec, de_thread() is waiting for us */
 	if (thread_group_leader(tsk) &&
-	    tsk->signal->notify_count < 0 &&
-	    tsk->signal->group_exit_task)
+	    tsk->signal->group_exit_task &&
+	    tsk->signal->notify_count < 0)
 		wake_up_process(tsk->signal->group_exit_task);
 
 	write_unlock_irq(&tasklist_lock);

commit 5c7edcd7ee6b77b88252fe4096dce1a46a60c829
Author: Roland McGrath <roland@redhat.com>
Date:   Thu Jul 31 02:04:09 2008 -0700

    tracehook: fix exit_signal=0 case
    
    My commit 2b2a1ff64afbadac842bbc58c5166962cf4f7664 introduced a regression
    (sorry about that) for the odd case of exit_signal=0 (e.g. clone_flags=0).
    This is not a normal use, but it's used by a case in the glibc test suite.
    
    Dying with exit_signal=0 sends no signal, but it's supposed to wake up a
    parent's blocked wait*() calls (unlike the delayed_group_leader case).
    This fixes tracehook_notify_death() and its caller to distinguish a
    "signal 0" wakeup from the delayed_group_leader case (with no wakeup).
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Tested-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index eb4d6470d1d0..38ec40630149 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -911,10 +911,10 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 		tsk->exit_signal = SIGCHLD;
 
 	signal = tracehook_notify_death(tsk, &cookie, group_dead);
-	if (signal > 0)
+	if (signal >= 0)
 		signal = do_notify_parent(tsk, signal);
 
-	tsk->exit_state = signal < 0 ? EXIT_DEAD : EXIT_ZOMBIE;
+	tsk->exit_state = signal == DEATH_REAP ? EXIT_DEAD : EXIT_ZOMBIE;
 
 	/* mt-exec, de_thread() is waiting for us */
 	if (thread_group_leader(tsk) &&
@@ -927,7 +927,7 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	tracehook_report_death(tsk, signal, cookie, group_dead);
 
 	/* If the process is dead, release it - nobody will wait for it */
-	if (signal < 0)
+	if (signal == DEATH_REAP)
 		release_task(tsk);
 }
 

commit 5995477ab7f3522c497c9c4a1c55373e9d655574
Author: Andrea Righi <righi.andrea@gmail.com>
Date:   Sun Jul 27 17:29:15 2008 +0200

    task IO accounting: improve code readability
    
    Put all i/o statistics in struct proc_io_accounting and use inline functions to
    initialize and increment statistics, removing a lot of single variable
    assignments.
    
    This also reduces the kernel size as following (with CONFIG_TASK_XACCT=y and
    CONFIG_TASK_IO_ACCOUNTING=y).
    
        text    data     bss     dec     hex filename
       11651       0       0   11651    2d83 kernel/exit.o.before
       11619       0       0   11619    2d63 kernel/exit.o.after
       10886     132     136   11154    2b92 kernel/fork.o.before
       10758     132     136   11026    2b12 kernel/fork.o.after
    
     3082029  807968 4818600 8708597  84e1f5 vmlinux.o.before
     3081869  807968 4818600 8708437  84e155 vmlinux.o.after
    
    Signed-off-by: Andrea Righi <righi.andrea@gmail.com>
    Acked-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 0caf590548a0..eb4d6470d1d0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -121,18 +121,7 @@ static void __exit_signal(struct task_struct *tsk)
 		sig->nivcsw += tsk->nivcsw;
 		sig->inblock += task_io_get_inblock(tsk);
 		sig->oublock += task_io_get_oublock(tsk);
-#ifdef CONFIG_TASK_XACCT
-		sig->rchar += tsk->rchar;
-		sig->wchar += tsk->wchar;
-		sig->syscr += tsk->syscr;
-		sig->syscw += tsk->syscw;
-#endif /* CONFIG_TASK_XACCT */
-#ifdef CONFIG_TASK_IO_ACCOUNTING
-		sig->ioac.read_bytes += tsk->ioac.read_bytes;
-		sig->ioac.write_bytes += tsk->ioac.write_bytes;
-		sig->ioac.cancelled_write_bytes +=
-					tsk->ioac.cancelled_write_bytes;
-#endif /* CONFIG_TASK_IO_ACCOUNTING */
+		task_io_accounting_add(&sig->ioac, &tsk->ioac);
 		sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 		sig = NULL; /* Marker for below. */
 	}
@@ -1363,21 +1352,8 @@ static int wait_task_zombie(struct task_struct *p, int options,
 		psig->coublock +=
 			task_io_get_oublock(p) +
 			sig->oublock + sig->coublock;
-#ifdef CONFIG_TASK_XACCT
-		psig->rchar += p->rchar + sig->rchar;
-		psig->wchar += p->wchar + sig->wchar;
-		psig->syscr += p->syscr + sig->syscr;
-		psig->syscw += p->syscw + sig->syscw;
-#endif /* CONFIG_TASK_XACCT */
-#ifdef CONFIG_TASK_IO_ACCOUNTING
-		psig->ioac.read_bytes +=
-			p->ioac.read_bytes + sig->ioac.read_bytes;
-		psig->ioac.write_bytes +=
-			p->ioac.write_bytes + sig->ioac.write_bytes;
-		psig->ioac.cancelled_write_bytes +=
-				p->ioac.cancelled_write_bytes +
-				sig->ioac.cancelled_write_bytes;
-#endif /* CONFIG_TASK_IO_ACCOUNTING */
+		task_io_accounting_add(&psig->ioac, &p->ioac);
+		task_io_accounting_add(&psig->ioac, &sig->ioac);
 		spin_unlock_irq(&p->parent->sighand->siglock);
 	}
 

commit 7f2da1e7d0330395e5e9e350b879b98a1ea495df
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat May 10 20:44:54 2008 -0400

    [PATCH] kill altroot
    
    long overdue...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6cdf60712bd2..0caf590548a0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -565,8 +565,6 @@ void put_fs_struct(struct fs_struct *fs)
 	if (atomic_dec_and_test(&fs->count)) {
 		path_put(&fs->root);
 		path_put(&fs->pwd);
-		if (fs->altroot.dentry)
-			path_put(&fs->altroot);
 		kmem_cache_free(fs_cachep, fs);
 	}
 }

commit 2b2a1ff64afbadac842bbc58c5166962cf4f7664
Author: Roland McGrath <roland@redhat.com>
Date:   Fri Jul 25 19:45:54 2008 -0700

    tracehook: death
    
    This moves the ptrace logic in task death (exit_notify) into tracehook.h
    inlines.  Some code is rearranged slightly to make things nicer.  There is
    no change, only cleanup.
    
    There is one hook called with the tasklist_lock write-locked, as ptrace
    needs.  There is also a new hook called after exit_state changes and
    without locks.  This is a better place for tracing work to be in the
    future, since it doesn't delay the whole system with locking.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index da28745f7c38..6cdf60712bd2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -885,7 +885,8 @@ static void forget_original_parent(struct task_struct *father)
  */
 static void exit_notify(struct task_struct *tsk, int group_dead)
 {
-	int state;
+	int signal;
+	void *cookie;
 
 	/*
 	 * This does two things:
@@ -922,22 +923,11 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	    !capable(CAP_KILL))
 		tsk->exit_signal = SIGCHLD;
 
-	/* If something other than our normal parent is ptracing us, then
-	 * send it a SIGCHLD instead of honoring exit_signal.  exit_signal
-	 * only has special meaning to our real parent.
-	 */
-	if (!task_detached(tsk) && thread_group_empty(tsk)) {
-		int signal = ptrace_reparented(tsk) ?
-				SIGCHLD : tsk->exit_signal;
-		do_notify_parent(tsk, signal);
-	} else if (tsk->ptrace) {
-		do_notify_parent(tsk, SIGCHLD);
-	}
+	signal = tracehook_notify_death(tsk, &cookie, group_dead);
+	if (signal > 0)
+		signal = do_notify_parent(tsk, signal);
 
-	state = EXIT_ZOMBIE;
-	if (task_detached(tsk) && likely(!tsk->ptrace))
-		state = EXIT_DEAD;
-	tsk->exit_state = state;
+	tsk->exit_state = signal < 0 ? EXIT_DEAD : EXIT_ZOMBIE;
 
 	/* mt-exec, de_thread() is waiting for us */
 	if (thread_group_leader(tsk) &&
@@ -947,8 +937,10 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 
 	write_unlock_irq(&tasklist_lock);
 
+	tracehook_report_death(tsk, signal, cookie, group_dead);
+
 	/* If the process is dead, release it - nobody will wait for it */
-	if (state == EXIT_DEAD)
+	if (signal < 0)
 		release_task(tsk);
 }
 

commit dae33574dcf5211e1f43c7e45fa29f73ba3e00cb
Author: Roland McGrath <roland@redhat.com>
Date:   Fri Jul 25 19:45:48 2008 -0700

    tracehook: release_task
    
    This moves the ptrace-related logic from release_task into tracehook.h and
    ptrace.h inlines.  It provides clean hooks both before and after locking
    tasklist_lock, for future tracing logic to do more cleanup without the
    lock.
    
    This also changes release_task() itself in the rare "zap_leader" case to
    set the leader to EXIT_DEAD before iterating.  This maintains the
    invariant that release_task() only ever handles a task in EXIT_DEAD.  This
    is a common-sense invariant that is already always true except in this one
    arcane case of zombie leader whose parent ignores SIGCHLD.
    
    This change is harmless and only costs one store in this one rare case.
    It keeps the expected state more consisently sane, which is nicer when
    debugging weirdness in release_task().  It also lets some future code in
    the tracehook entry points rely on this invariant for bookkeeping.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c3691cbc220a..da28745f7c38 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -163,27 +163,17 @@ static void delayed_put_task_struct(struct rcu_head *rhp)
 	put_task_struct(container_of(rhp, struct task_struct, rcu));
 }
 
-/*
- * Do final ptrace-related cleanup of a zombie being reaped.
- *
- * Called with write_lock(&tasklist_lock) held.
- */
-static void ptrace_release_task(struct task_struct *p)
-{
-	BUG_ON(!list_empty(&p->ptraced));
-	ptrace_unlink(p);
-	BUG_ON(!list_empty(&p->ptrace_entry));
-}
 
 void release_task(struct task_struct * p)
 {
 	struct task_struct *leader;
 	int zap_leader;
 repeat:
+	tracehook_prepare_release_task(p);
 	atomic_dec(&p->user->processes);
 	proc_flush_task(p);
 	write_lock_irq(&tasklist_lock);
-	ptrace_release_task(p);
+	tracehook_finish_release_task(p);
 	__exit_signal(p);
 
 	/*
@@ -205,6 +195,13 @@ void release_task(struct task_struct * p)
 		 * that case.
 		 */
 		zap_leader = task_detached(leader);
+
+		/*
+		 * This maintains the invariant that release_task()
+		 * only runs on a task in EXIT_DEAD, just for sanity.
+		 */
+		if (zap_leader)
+			leader->exit_state = EXIT_DEAD;
 	}
 
 	write_unlock_irq(&tasklist_lock);

commit 30199f5a46aee204bf437a4f5b0740f3efe448b7
Author: Roland McGrath <roland@redhat.com>
Date:   Fri Jul 25 19:45:46 2008 -0700

    tracehook: exit
    
    This moves the PTRACE_EVENT_EXIT tracing into a tracehook.h inline,
    tracehook_report_exec().  The change has no effect, just clean-up.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Reviewed-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ad933bb29ec7..c3691cbc220a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -46,6 +46,7 @@
 #include <linux/resource.h>
 #include <linux/blkdev.h>
 #include <linux/task_io_accounting_ops.h>
+#include <linux/tracehook.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -1029,10 +1030,7 @@ NORET_TYPE void do_exit(long code)
 	if (unlikely(!tsk->pid))
 		panic("Attempted to kill the idle task!");
 
-	if (unlikely(current->ptrace & PT_TRACE_EXIT)) {
-		current->ptrace_message = code;
-		ptrace_notify((PTRACE_EVENT_EXIT << 8) | SIGTRAP);
-	}
+	tracehook_report_exit(&code);
 
 	/*
 	 * We're taking recursive faults here in do_exit. Safest is to just

commit 297c5d92634c809cef23d73e7b2556f2528ff7e2
Author: Andrea Righi <righi.andrea@gmail.com>
Date:   Fri Jul 25 01:48:49 2008 -0700

    task IO accounting: provide distinct tgid/tid I/O statistics
    
    Report per-thread I/O statistics in /proc/pid/task/tid/io and aggregate
    parent I/O statistics in /proc/pid/io.  This approach follows the same
    model used to account per-process and per-thread CPU times.
    
    As a practial application, this allows for example to quickly find the top
    I/O consumer when a process spawns many child threads that perform the
    actual I/O work, because the aggregated I/O statistics can always be found
    in /proc/pid/io.
    
    [ Oleg Nesterov points out that we should check that the task is still
      alive before we iterate over the threads, but also says that we can do
      that fixup on top of this later.  - Linus ]
    
    Acked-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Signed-off-by: Andrea Righi <righi.andrea@gmail.com>
    Cc: Matt Heaton <matt@hostmonster.com>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Acked-by-with-comments: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8a4d4d12e294..ad933bb29ec7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -120,6 +120,18 @@ static void __exit_signal(struct task_struct *tsk)
 		sig->nivcsw += tsk->nivcsw;
 		sig->inblock += task_io_get_inblock(tsk);
 		sig->oublock += task_io_get_oublock(tsk);
+#ifdef CONFIG_TASK_XACCT
+		sig->rchar += tsk->rchar;
+		sig->wchar += tsk->wchar;
+		sig->syscr += tsk->syscr;
+		sig->syscw += tsk->syscw;
+#endif /* CONFIG_TASK_XACCT */
+#ifdef CONFIG_TASK_IO_ACCOUNTING
+		sig->ioac.read_bytes += tsk->ioac.read_bytes;
+		sig->ioac.write_bytes += tsk->ioac.write_bytes;
+		sig->ioac.cancelled_write_bytes +=
+					tsk->ioac.cancelled_write_bytes;
+#endif /* CONFIG_TASK_IO_ACCOUNTING */
 		sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 		sig = NULL; /* Marker for below. */
 	}
@@ -1366,6 +1378,21 @@ static int wait_task_zombie(struct task_struct *p, int options,
 		psig->coublock +=
 			task_io_get_oublock(p) +
 			sig->oublock + sig->coublock;
+#ifdef CONFIG_TASK_XACCT
+		psig->rchar += p->rchar + sig->rchar;
+		psig->wchar += p->wchar + sig->wchar;
+		psig->syscr += p->syscr + sig->syscr;
+		psig->syscw += p->syscw + sig->syscw;
+#endif /* CONFIG_TASK_XACCT */
+#ifdef CONFIG_TASK_IO_ACCOUNTING
+		psig->ioac.read_bytes +=
+			p->ioac.read_bytes + sig->ioac.read_bytes;
+		psig->ioac.write_bytes +=
+			p->ioac.write_bytes + sig->ioac.write_bytes;
+		psig->ioac.cancelled_write_bytes +=
+				p->ioac.cancelled_write_bytes +
+				sig->ioac.cancelled_write_bytes;
+#endif /* CONFIG_TASK_IO_ACCOUNTING */
 		spin_unlock_irq(&p->parent->sighand->siglock);
 	}
 

commit a94e2d408eaedbd85aae259621d46fafc10479a2
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:46 2008 -0700

    coredump: kill mm->core_done
    
    Now that we have core_state->dumper list we can use it to wake up the
    sub-threads waiting for the coredump completion.
    
    This uglifies the code and .text grows by 47 bytes, but otoh mm_struct
    lessens by sizeof(struct completion).  Also, with this change we can
    decouple exit_mm() from the coredumping code.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b66f0d55c791..8a4d4d12e294 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -691,7 +691,13 @@ static void exit_mm(struct task_struct * tsk)
 		if (atomic_dec_and_test(&core_state->nr_threads))
 			complete(&core_state->startup);
 
-		wait_for_completion(&mm->core_done);
+		for (;;) {
+			set_task_state(tsk, TASK_UNINTERRUPTIBLE);
+			if (!self.task) /* see coredump_finish() */
+				break;
+			schedule();
+		}
+		__set_task_state(tsk, TASK_RUNNING);
 		down_read(&mm->mmap_sem);
 	}
 	atomic_inc(&mm->mm_count);

commit b564daf806d492dd4f7afe9b6c83b8d35d137669
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:44 2008 -0700

    coredump: construct the list of coredumping threads at startup time
    
    binfmt->core_dump() has to iterate over the all threads in system in order
    to find the coredumping threads and construct the list using the
    GFP_ATOMIC allocations.
    
    With this patch each thread allocates the list node on exit_mm()'s stack and
    adds itself to the list.
    
    This allows us to do further changes:
    
            - simplify ->core_dump()
    
            - change exit_mm() to clear ->mm first, then wait for ->core_done.
              this makes the coredumping process visible to oom_kill
    
            - kill mm->core_done
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 63d82957baae..b66f0d55c791 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -664,6 +664,7 @@ void mm_update_next_owner(struct mm_struct *mm)
 static void exit_mm(struct task_struct * tsk)
 {
 	struct mm_struct *mm = tsk->mm;
+	struct core_state *core_state;
 
 	mm_release(tsk, mm);
 	if (!mm)
@@ -676,11 +677,19 @@ static void exit_mm(struct task_struct * tsk)
 	 * group with ->mm != NULL.
 	 */
 	down_read(&mm->mmap_sem);
-	if (mm->core_state) {
+	core_state = mm->core_state;
+	if (core_state) {
+		struct core_thread self;
 		up_read(&mm->mmap_sem);
 
-		if (atomic_dec_and_test(&mm->core_state->nr_threads))
-			complete(&mm->core_state->startup);
+		self.task = tsk;
+		self.next = xchg(&core_state->dumper.next, &self);
+		/*
+		 * Implies mb(), the result of xchg() must be visible
+		 * to core_state->dumper.
+		 */
+		if (atomic_dec_and_test(&core_state->nr_threads))
+			complete(&core_state->startup);
 
 		wait_for_completion(&mm->core_done);
 		down_read(&mm->mmap_sem);

commit c5f1cc8c1828486a61ab3e575da6e2c62b34d399
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:42 2008 -0700

    coredump: turn core_state->nr_threads into atomic_t
    
    Turn core_state->nr_threads into atomic_t and kill now unneeded
    down_write(&mm->mmap_sem) in exit_mm().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 988e232254e9..63d82957baae 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -678,10 +678,9 @@ static void exit_mm(struct task_struct * tsk)
 	down_read(&mm->mmap_sem);
 	if (mm->core_state) {
 		up_read(&mm->mmap_sem);
-		down_write(&mm->mmap_sem);
-		if (!--mm->core_state->nr_threads)
+
+		if (atomic_dec_and_test(&mm->core_state->nr_threads))
 			complete(&mm->core_state->startup);
-		up_write(&mm->mmap_sem);
 
 		wait_for_completion(&mm->core_done);
 		down_read(&mm->mmap_sem);

commit 999d9fc1670bc082928b93b11d1f2e0e417d973c
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:41 2008 -0700

    coredump: move mm->core_waiters into struct core_state
    
    Move mm->core_waiters into "struct core_state" allocated on stack.  This
    shrinks mm_struct a little bit and allows further changes.
    
    This patch mostly does s/core_waiters/core_state.  The only essential
    change is that coredump_wait() must clear mm->core_state before return.
    
    The coredump_wait()'s path is uglified and .text grows by 30 bytes, this
    is fixed by the next patch.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f7fa21dbced4..988e232254e9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -670,16 +670,16 @@ static void exit_mm(struct task_struct * tsk)
 		return;
 	/*
 	 * Serialize with any possible pending coredump.
-	 * We must hold mmap_sem around checking core_waiters
+	 * We must hold mmap_sem around checking core_state
 	 * and clearing tsk->mm.  The core-inducing thread
-	 * will increment core_waiters for each thread in the
+	 * will increment ->nr_threads for each thread in the
 	 * group with ->mm != NULL.
 	 */
 	down_read(&mm->mmap_sem);
-	if (mm->core_waiters) {
+	if (mm->core_state) {
 		up_read(&mm->mmap_sem);
 		down_write(&mm->mmap_sem);
-		if (!--mm->core_waiters)
+		if (!--mm->core_state->nr_threads)
 			complete(&mm->core_state->startup);
 		up_write(&mm->mmap_sem);
 

commit 32ecb1f26dd50eeaac4e3f4dea4541c97848e459
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:41 2008 -0700

    coredump: turn mm->core_startup_done into the pointer to struct core_state
    
    mm->core_startup_done points to "struct completion startup_done" allocated
    on the coredump_wait()'s stack.  Introduce the new structure, core_state,
    which holds this "struct completion".  This way we can add more info
    visible to the threads participating in coredump without enlarging
    mm_struct.
    
    No changes in affected .o files.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 28a44a2612dc..f7fa21dbced4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -680,7 +680,7 @@ static void exit_mm(struct task_struct * tsk)
 		up_read(&mm->mmap_sem);
 		down_write(&mm->mmap_sem);
 		if (!--mm->core_waiters)
-			complete(mm->core_startup_done);
+			complete(&mm->core_state->startup);
 		up_write(&mm->mmap_sem);
 
 		wait_for_completion(&mm->core_done);

commit 7b34e4283c685f5cc6ba6d30e939906eee0d4bcf
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:37 2008 -0700

    introduce PF_KTHREAD flag
    
    Introduce the new PF_KTHREAD flag to mark the kernel threads.  It is set
    by INIT_TASK() and copied to the forked childs (we could set it in
    kthreadd() along with PF_NOFREEZE instead).
    
    daemonize() was changed as well.  In that case testing of PF_KTHREAD is
    racy, but daemonize() is hopeless anyway.
    
    This flag is cleared in do_execve(), before search_binary_handler().
    Probably not the best place, we can do this in exec_mmap() or in
    start_thread(), or clear it along with PF_FORKNOEXEC.  But I think this
    doesn't matter in practice, and if do_execve() fails kthread should die
    soon.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a7799d8a6404..28a44a2612dc 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -430,7 +430,7 @@ void daemonize(const char *name, ...)
 	 * We don't want to have TIF_FREEZE set if the system-wide hibernation
 	 * or suspend transition begins right now.
 	 */
-	current->flags |= PF_NOFREEZE;
+	current->flags |= (PF_NOFREEZE | PF_KTHREAD);
 
 	if (current->nsproxy != &init_nsproxy) {
 		get_nsproxy(&init_nsproxy);

commit 3854a771821c970065e3203a0b40ddc4101538cc
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jul 25 01:47:29 2008 -0700

    __exit_signal: don't take rcu lock
    
    There is no reason for rcu_read_lock() in __exit_signal().  tsk->sighand
    can only be changed if tsk does exec, obviously this is not possible.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 93d2711b9381..a7799d8a6404 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -85,7 +85,6 @@ static void __exit_signal(struct task_struct *tsk)
 	BUG_ON(!sig);
 	BUG_ON(!atomic_read(&sig->count));
 
-	rcu_read_lock();
 	sighand = rcu_dereference(tsk->sighand);
 	spin_lock(&sighand->siglock);
 
@@ -136,7 +135,6 @@ static void __exit_signal(struct task_struct *tsk)
 	tsk->signal = NULL;
 	tsk->sighand = NULL;
 	spin_unlock(&sighand->siglock);
-	rcu_read_unlock();
 
 	__cleanup_sighand(sighand);
 	clear_tsk_thread_flag(tsk,TIF_SIGPENDING);

commit 666f164f4fbfa78bd00fb4b74788b42a39842c64
Author: Roland McGrath <roland@redhat.com>
Date:   Tue Apr 8 23:12:30 2008 -0700

    fix dangling zombie when new parent ignores children
    
    This fixes an arcane bug that we think was a regression introduced
    by commit b2b2cbc4b2a2f389442549399a993a8306420baf.  When a parent
    ignores SIGCHLD (or uses SA_NOCLDWAIT), its children would self-reap
    but they don't if it's using ptrace on them.  When the parent thread
    later exits and ceases to ptrace a child but leaves other live
    threads in the parent's thread group, any zombie children are left
    dangling.  The fix makes them self-reap then, as they would have
    done earlier if ptrace had not been in use.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index a2af6cac823c..93d2711b9381 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -702,6 +702,23 @@ static void exit_mm(struct task_struct * tsk)
 	mmput(mm);
 }
 
+/*
+ * Return nonzero if @parent's children should reap themselves.
+ *
+ * Called with write_lock_irq(&tasklist_lock) held.
+ */
+static int ignoring_children(struct task_struct *parent)
+{
+	int ret;
+	struct sighand_struct *psig = parent->sighand;
+	unsigned long flags;
+	spin_lock_irqsave(&psig->siglock, flags);
+	ret = (psig->action[SIGCHLD-1].sa.sa_handler == SIG_IGN ||
+	       (psig->action[SIGCHLD-1].sa.sa_flags & SA_NOCLDWAIT));
+	spin_unlock_irqrestore(&psig->siglock, flags);
+	return ret;
+}
+
 /*
  * Detach all tasks we were using ptrace on.
  * Any that need to be release_task'd are put on the @dead list.
@@ -711,6 +728,7 @@ static void exit_mm(struct task_struct * tsk)
 static void ptrace_exit(struct task_struct *parent, struct list_head *dead)
 {
 	struct task_struct *p, *n;
+	int ign = -1;
 
 	list_for_each_entry_safe(p, n, &parent->ptraced, ptrace_entry) {
 		__ptrace_unlink(p);
@@ -726,10 +744,18 @@ static void ptrace_exit(struct task_struct *parent, struct list_head *dead)
 		 * release_task() here because we already hold tasklist_lock.
 		 *
 		 * If it's our own child, there is no notification to do.
+		 * But if our normal children self-reap, then this child
+		 * was prevented by ptrace and we must reap it now.
 		 */
 		if (!task_detached(p) && thread_group_empty(p)) {
 			if (!same_thread_group(p->real_parent, parent))
 				do_notify_parent(p, p->exit_signal);
+			else {
+				if (ign < 0)
+					ign = ignoring_children(parent);
+				if (ign)
+					p->exit_signal = -1;
+			}
 		}
 
 		if (task_detached(p)) {

commit 14dd0b81414a58caf0296dbeace016bb0a5d11ab
Author: Roland McGrath <roland@redhat.com>
Date:   Sun Mar 30 18:41:25 2008 -0700

    do_wait: return security_task_wait() error code in place of -ECHILD
    
    This reverts the effect of commit f2cc3eb133baa2e9dc8efd40f417106b2ee520f3
    "do_wait: fix security checks".  That change reverted the effect of commit
    73243284463a761e04d69d22c7516b2be7de096c.  The rationale for the original
    commit still stands.  The inconsistent treatment of children hidden by
    ptrace was an unintended omission in the original change and in no way
    invalidates its purpose.
    
    This makes do_wait return the error returned by security_task_wait()
    (usually -EACCES) in place of -ECHILD when there are some children the
    caller would be able to wait for if not for the permission failure.  A
    permission error will give the user a clue to look for security policy
    problems, rather than for mysterious wait bugs.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1e909826a804..a2af6cac823c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1199,14 +1199,10 @@ static int eligible_child(enum pid_type type, struct pid *pid, int options,
 		return 0;
 
 	err = security_task_wait(p);
-	if (likely(!err))
-		return 1;
+	if (err)
+		return err;
 
-	if (type != PIDTYPE_PID)
-		return 0;
-	/* This child was explicitly requested, abort */
-	read_unlock(&tasklist_lock);
-	return err;
+	return 1;
 }
 
 static int wait_noreap_copyout(struct task_struct *p, pid_t pid, uid_t uid,
@@ -1536,7 +1532,8 @@ static int wait_task_continued(struct task_struct *p, int options,
  * -ECHILD should be in *@notask_error before the first call.
  * Returns nonzero for a final return, when we have unlocked tasklist_lock.
  * Returns zero if the search for a child should continue;
- * then *@notask_error is 0 if @p is an eligible child, or still -ECHILD.
+ * then *@notask_error is 0 if @p is an eligible child,
+ * or another error from security_task_wait(), or still -ECHILD.
  */
 static int wait_consider_task(struct task_struct *parent, int ptrace,
 			      struct task_struct *p, int *notask_error,
@@ -1545,9 +1542,21 @@ static int wait_consider_task(struct task_struct *parent, int ptrace,
 			      int __user *stat_addr, struct rusage __user *ru)
 {
 	int ret = eligible_child(type, pid, options, p);
-	if (ret <= 0)
+	if (!ret)
 		return ret;
 
+	if (unlikely(ret < 0)) {
+		/*
+		 * If we have not yet seen any eligible child,
+		 * then let this error code replace -ECHILD.
+		 * A permission error will give the user a clue
+		 * to look for security policy problems, rather
+		 * than for mysterious wait bugs.
+		 */
+		if (*notask_error)
+			*notask_error = ret;
+	}
+
 	if (likely(!ptrace) && unlikely(p->ptrace)) {
 		/*
 		 * This child is hidden by ptrace.
@@ -1585,7 +1594,8 @@ static int wait_consider_task(struct task_struct *parent, int ptrace,
  * -ECHILD should be in *@notask_error before the first call.
  * Returns nonzero for a final return, when we have unlocked tasklist_lock.
  * Returns zero if the search for a child should continue; then
- * *@notask_error is 0 if there were any eligible children, or still -ECHILD.
+ * *@notask_error is 0 if there were any eligible children,
+ * or another error from security_task_wait(), or still -ECHILD.
  */
 static int do_wait_thread(struct task_struct *tsk, int *notask_error,
 			  enum pid_type type, struct pid *pid, int options,

commit f470021adb9190819c03d6d8c5c860a17480aa6d
Author: Roland McGrath <roland@redhat.com>
Date:   Mon Mar 24 18:36:23 2008 -0700

    ptrace children revamp
    
    ptrace no longer fiddles with the children/sibling links, and the
    old ptrace_children list is gone.  Now ptrace, whether of one's own
    children or another's via PTRACE_ATTACH, just uses the new ptraced
    list instead.
    
    There should be no user-visible difference that matters.  The only
    change is the order in which do_wait() sees multiple stopped
    children and stopped ptrace attachees.  Since wait_task_stopped()
    was changed earlier so it no longer reorders the children list, we
    already know this won't cause any new problems.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7453356a961f..1e909826a804 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -71,7 +71,7 @@ static void __unhash_process(struct task_struct *p)
 		__get_cpu_var(process_counts)--;
 	}
 	list_del_rcu(&p->thread_group);
-	remove_parent(p);
+	list_del_init(&p->sibling);
 }
 
 /*
@@ -152,6 +152,18 @@ static void delayed_put_task_struct(struct rcu_head *rhp)
 	put_task_struct(container_of(rhp, struct task_struct, rcu));
 }
 
+/*
+ * Do final ptrace-related cleanup of a zombie being reaped.
+ *
+ * Called with write_lock(&tasklist_lock) held.
+ */
+static void ptrace_release_task(struct task_struct *p)
+{
+	BUG_ON(!list_empty(&p->ptraced));
+	ptrace_unlink(p);
+	BUG_ON(!list_empty(&p->ptrace_entry));
+}
+
 void release_task(struct task_struct * p)
 {
 	struct task_struct *leader;
@@ -160,8 +172,7 @@ void release_task(struct task_struct * p)
 	atomic_dec(&p->user->processes);
 	proc_flush_task(p);
 	write_lock_irq(&tasklist_lock);
-	ptrace_unlink(p);
-	BUG_ON(!list_empty(&p->ptrace_list) || !list_empty(&p->ptrace_children));
+	ptrace_release_task(p);
 	__exit_signal(p);
 
 	/*
@@ -315,9 +326,8 @@ static void reparent_to_kthreadd(void)
 
 	ptrace_unlink(current);
 	/* Reparent to init */
-	remove_parent(current);
 	current->real_parent = current->parent = kthreadd_task;
-	add_parent(current);
+	list_move_tail(&current->sibling, &current->real_parent->children);
 
 	/* Set the exit signal to SIGCHLD so we signal init on exit */
 	current->exit_signal = SIGCHLD;
@@ -692,37 +702,71 @@ static void exit_mm(struct task_struct * tsk)
 	mmput(mm);
 }
 
-static void
-reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
+/*
+ * Detach all tasks we were using ptrace on.
+ * Any that need to be release_task'd are put on the @dead list.
+ *
+ * Called with write_lock(&tasklist_lock) held.
+ */
+static void ptrace_exit(struct task_struct *parent, struct list_head *dead)
 {
-	if (p->pdeath_signal)
-		/* We already hold the tasklist_lock here.  */
-		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
+	struct task_struct *p, *n;
 
-	/* Move the child from its dying parent to the new one.  */
-	if (unlikely(traced)) {
-		/* Preserve ptrace links if someone else is tracing this child.  */
-		list_del_init(&p->ptrace_list);
-		if (ptrace_reparented(p))
-			list_add(&p->ptrace_list, &p->real_parent->ptrace_children);
-	} else {
-		/* If this child is being traced, then we're the one tracing it
-		 * anyway, so let go of it.
+	list_for_each_entry_safe(p, n, &parent->ptraced, ptrace_entry) {
+		__ptrace_unlink(p);
+
+		if (p->exit_state != EXIT_ZOMBIE)
+			continue;
+
+		/*
+		 * If it's a zombie, our attachedness prevented normal
+		 * parent notification or self-reaping.  Do notification
+		 * now if it would have happened earlier.  If it should
+		 * reap itself, add it to the @dead list.  We can't call
+		 * release_task() here because we already hold tasklist_lock.
+		 *
+		 * If it's our own child, there is no notification to do.
 		 */
-		p->ptrace = 0;
-		remove_parent(p);
-		p->parent = p->real_parent;
-		add_parent(p);
+		if (!task_detached(p) && thread_group_empty(p)) {
+			if (!same_thread_group(p->real_parent, parent))
+				do_notify_parent(p, p->exit_signal);
+		}
 
-		if (task_is_traced(p)) {
+		if (task_detached(p)) {
 			/*
-			 * If it was at a trace stop, turn it into
-			 * a normal stop since it's no longer being
-			 * traced.
+			 * Mark it as in the process of being reaped.
 			 */
-			ptrace_untrace(p);
+			p->exit_state = EXIT_DEAD;
+			list_add(&p->ptrace_entry, dead);
 		}
 	}
+}
+
+/*
+ * Finish up exit-time ptrace cleanup.
+ *
+ * Called without locks.
+ */
+static void ptrace_exit_finish(struct task_struct *parent,
+			       struct list_head *dead)
+{
+	struct task_struct *p, *n;
+
+	BUG_ON(!list_empty(&parent->ptraced));
+
+	list_for_each_entry_safe(p, n, dead, ptrace_entry) {
+		list_del_init(&p->ptrace_entry);
+		release_task(p);
+	}
+}
+
+static void reparent_thread(struct task_struct *p, struct task_struct *father)
+{
+	if (p->pdeath_signal)
+		/* We already hold the tasklist_lock here.  */
+		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
+
+	list_move_tail(&p->sibling, &p->real_parent->children);
 
 	/* If this is a threaded reparent there is no need to
 	 * notify anyone anything has happened.
@@ -737,7 +781,8 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 	/* If we'd notified the old parent about this child's death,
 	 * also notify the new parent.
 	 */
-	if (!traced && p->exit_state == EXIT_ZOMBIE &&
+	if (!ptrace_reparented(p) &&
+	    p->exit_state == EXIT_ZOMBIE &&
 	    !task_detached(p) && thread_group_empty(p))
 		do_notify_parent(p, p->exit_signal);
 
@@ -754,12 +799,15 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 static void forget_original_parent(struct task_struct *father)
 {
 	struct task_struct *p, *n, *reaper = father;
-	struct list_head ptrace_dead;
-
-	INIT_LIST_HEAD(&ptrace_dead);
+	LIST_HEAD(ptrace_dead);
 
 	write_lock_irq(&tasklist_lock);
 
+	/*
+	 * First clean up ptrace if we were using it.
+	 */
+	ptrace_exit(father, &ptrace_dead);
+
 	do {
 		reaper = next_thread(reaper);
 		if (reaper == father) {
@@ -768,58 +816,19 @@ static void forget_original_parent(struct task_struct *father)
 		}
 	} while (reaper->flags & PF_EXITING);
 
-	/*
-	 * There are only two places where our children can be:
-	 *
-	 * - in our child list
-	 * - in our ptraced child list
-	 *
-	 * Search them and reparent children.
-	 */
 	list_for_each_entry_safe(p, n, &father->children, sibling) {
-		int ptrace;
-
-		ptrace = p->ptrace;
-
-		/* if father isn't the real parent, then ptrace must be enabled */
-		BUG_ON(father != p->real_parent && !ptrace);
-
-		if (father == p->real_parent) {
-			/* reparent with a reaper, real father it's us */
-			p->real_parent = reaper;
-			reparent_thread(p, father, 0);
-		} else {
-			/* reparent ptraced task to its real parent */
-			__ptrace_unlink (p);
-			if (p->exit_state == EXIT_ZOMBIE && !task_detached(p) &&
-			    thread_group_empty(p))
-				do_notify_parent(p, p->exit_signal);
-		}
-
-		/*
-		 * if the ptraced child is a detached zombie we must collect
-		 * it before we exit, or it will remain zombie forever since
-		 * we prevented it from self-reap itself while it was being
-		 * traced by us, to be able to see it in wait4.
-		 */
-		if (unlikely(ptrace && p->exit_state == EXIT_ZOMBIE && task_detached(p)))
-			list_add(&p->ptrace_list, &ptrace_dead);
-	}
-
-	list_for_each_entry_safe(p, n, &father->ptrace_children, ptrace_list) {
 		p->real_parent = reaper;
-		reparent_thread(p, father, 1);
+		if (p->parent == father) {
+			BUG_ON(p->ptrace);
+			p->parent = p->real_parent;
+		}
+		reparent_thread(p, father);
 	}
 
 	write_unlock_irq(&tasklist_lock);
 	BUG_ON(!list_empty(&father->children));
-	BUG_ON(!list_empty(&father->ptrace_children));
-
-	list_for_each_entry_safe(p, n, &ptrace_dead, ptrace_list) {
-		list_del_init(&p->ptrace_list);
-		release_task(p);
-	}
 
+	ptrace_exit_finish(father, &ptrace_dead);
 }
 
 /*
@@ -1180,13 +1189,6 @@ static int eligible_child(enum pid_type type, struct pid *pid, int options,
 			return 0;
 	}
 
-	/*
-	 * Do not consider detached threads that are
-	 * not ptraced:
-	 */
-	if (task_detached(p) && !p->ptrace)
-		return 0;
-
 	/* Wait for all children (clone and not) if __WALL is set;
 	 * otherwise, wait for clone children *only* if __WCLONE is
 	 * set; otherwise, wait for non-clone children *only*.  (Note:
@@ -1399,7 +1401,7 @@ static int wait_task_zombie(struct task_struct *p, int options,
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_stopped(struct task_struct *p,
+static int wait_task_stopped(int ptrace, struct task_struct *p,
 			     int options, struct siginfo __user *infop,
 			     int __user *stat_addr, struct rusage __user *ru)
 {
@@ -1407,7 +1409,7 @@ static int wait_task_stopped(struct task_struct *p,
 	uid_t uid = 0; /* unneeded, required by compiler */
 	pid_t pid;
 
-	if (!(p->ptrace & PT_PTRACED) && !(options & WUNTRACED))
+	if (!(options & WUNTRACED))
 		return 0;
 
 	exit_code = 0;
@@ -1416,7 +1418,7 @@ static int wait_task_stopped(struct task_struct *p,
 	if (unlikely(!task_is_stopped_or_traced(p)))
 		goto unlock_sig;
 
-	if (!(p->ptrace & PT_PTRACED) && p->signal->group_stop_count > 0)
+	if (!ptrace && p->signal->group_stop_count > 0)
 		/*
 		 * A group stop is in progress and this is the group leader.
 		 * We won't report until all threads have stopped.
@@ -1445,7 +1447,7 @@ static int wait_task_stopped(struct task_struct *p,
 	 */
 	get_task_struct(p);
 	pid = task_pid_vnr(p);
-	why = (p->ptrace & PT_PTRACED) ? CLD_TRAPPED : CLD_STOPPED;
+	why = ptrace ? CLD_TRAPPED : CLD_STOPPED;
 	read_unlock(&tasklist_lock);
 
 	if (unlikely(options & WNOWAIT))
@@ -1536,7 +1538,7 @@ static int wait_task_continued(struct task_struct *p, int options,
  * Returns zero if the search for a child should continue;
  * then *@notask_error is 0 if @p is an eligible child, or still -ECHILD.
  */
-static int wait_consider_task(struct task_struct *parent,
+static int wait_consider_task(struct task_struct *parent, int ptrace,
 			      struct task_struct *p, int *notask_error,
 			      enum pid_type type, struct pid *pid, int options,
 			      struct siginfo __user *infop,
@@ -1546,6 +1548,15 @@ static int wait_consider_task(struct task_struct *parent,
 	if (ret <= 0)
 		return ret;
 
+	if (likely(!ptrace) && unlikely(p->ptrace)) {
+		/*
+		 * This child is hidden by ptrace.
+		 * We aren't allowed to see it now, but eventually we will.
+		 */
+		*notask_error = 0;
+		return 0;
+	}
+
 	if (p->exit_state == EXIT_DEAD)
 		return 0;
 
@@ -1562,7 +1573,8 @@ static int wait_consider_task(struct task_struct *parent,
 	*notask_error = 0;
 
 	if (task_is_stopped_or_traced(p))
-		return wait_task_stopped(p, options, infop, stat_addr, ru);
+		return wait_task_stopped(ptrace, p, options,
+					 infop, stat_addr, ru);
 
 	return wait_task_continued(p, options, infop, stat_addr, ru);
 }
@@ -1583,11 +1595,16 @@ static int do_wait_thread(struct task_struct *tsk, int *notask_error,
 	struct task_struct *p;
 
 	list_for_each_entry(p, &tsk->children, sibling) {
-		int ret = wait_consider_task(tsk, p, notask_error,
-					     type, pid, options,
-					     infop, stat_addr, ru);
-		if (ret)
-			return ret;
+		/*
+		 * Do not consider detached threads.
+		 */
+		if (!task_detached(p)) {
+			int ret = wait_consider_task(tsk, 0, p, notask_error,
+						     type, pid, options,
+						     infop, stat_addr, ru);
+			if (ret)
+				return ret;
+		}
 	}
 
 	return 0;
@@ -1601,21 +1618,16 @@ static int ptrace_do_wait(struct task_struct *tsk, int *notask_error,
 	struct task_struct *p;
 
 	/*
-	 * If we never saw an eligile child, check for children stolen by
-	 * ptrace.  We don't leave -ECHILD in *@notask_error if there are any,
-	 * because we will eventually be allowed to wait for them again.
+	 * Traditionally we see ptrace'd stopped tasks regardless of options.
 	 */
-	if (!*notask_error)
-		return 0;
+	options |= WUNTRACED;
 
-	list_for_each_entry(p, &tsk->ptrace_children, ptrace_list) {
-		int ret = eligible_child(type, pid, options, p);
-		if (unlikely(ret < 0))
+	list_for_each_entry(p, &tsk->ptraced, ptrace_entry) {
+		int ret = wait_consider_task(tsk, 1, p, notask_error,
+					     type, pid, options,
+					     infop, stat_addr, ru);
+		if (ret)
 			return ret;
-		if (ret) {
-			*notask_error = 0;
-			return 0;
-		}
 	}
 
 	return 0;

commit 98abed02007b19bbfd68b6d06a5485afc3eeb01b
Author: Roland McGrath <roland@redhat.com>
Date:   Wed Mar 19 19:24:59 2008 -0700

    do_wait reorganization
    
    This breaks out the guts of do_wait into three subfunctions.
    The control flow is less nonobvious without so much goto.
    do_wait_thread and ptrace_do_wait contain the main work of the outer loop.
    wait_consider_task contains the main work of the inner loop.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index ceb258782835..7453356a961f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1238,7 +1238,7 @@ static int wait_noreap_copyout(struct task_struct *p, pid_t pid, uid_t uid,
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_zombie(struct task_struct *p, int noreap,
+static int wait_task_zombie(struct task_struct *p, int options,
 			    struct siginfo __user *infop,
 			    int __user *stat_addr, struct rusage __user *ru)
 {
@@ -1246,7 +1246,10 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 	int retval, status, traced;
 	pid_t pid = task_pid_vnr(p);
 
-	if (unlikely(noreap)) {
+	if (!likely(options & WEXITED))
+		return 0;
+
+	if (unlikely(options & WNOWAIT)) {
 		uid_t uid = p->uid;
 		int exit_code = p->exit_code;
 		int why, status;
@@ -1397,13 +1400,16 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
  * released the lock and the system call should return.
  */
 static int wait_task_stopped(struct task_struct *p,
-			     int noreap, struct siginfo __user *infop,
+			     int options, struct siginfo __user *infop,
 			     int __user *stat_addr, struct rusage __user *ru)
 {
 	int retval, exit_code, why;
 	uid_t uid = 0; /* unneeded, required by compiler */
 	pid_t pid;
 
+	if (!(p->ptrace & PT_PTRACED) && !(options & WUNTRACED))
+		return 0;
+
 	exit_code = 0;
 	spin_lock_irq(&p->sighand->siglock);
 
@@ -1421,7 +1427,7 @@ static int wait_task_stopped(struct task_struct *p,
 	if (!exit_code)
 		goto unlock_sig;
 
-	if (!noreap)
+	if (!unlikely(options & WNOWAIT))
 		p->exit_code = 0;
 
 	uid = p->uid;
@@ -1442,7 +1448,7 @@ static int wait_task_stopped(struct task_struct *p,
 	why = (p->ptrace & PT_PTRACED) ? CLD_TRAPPED : CLD_STOPPED;
 	read_unlock(&tasklist_lock);
 
-	if (unlikely(noreap))
+	if (unlikely(options & WNOWAIT))
 		return wait_noreap_copyout(p, pid, uid,
 					   why, exit_code,
 					   infop, ru);
@@ -1476,7 +1482,7 @@ static int wait_task_stopped(struct task_struct *p,
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_continued(struct task_struct *p, int noreap,
+static int wait_task_continued(struct task_struct *p, int options,
 			       struct siginfo __user *infop,
 			       int __user *stat_addr, struct rusage __user *ru)
 {
@@ -1484,6 +1490,9 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 	pid_t pid;
 	uid_t uid;
 
+	if (!unlikely(options & WCONTINUED))
+		return 0;
+
 	if (!(p->signal->flags & SIGNAL_STOP_CONTINUED))
 		return 0;
 
@@ -1493,7 +1502,7 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 		spin_unlock_irq(&p->sighand->siglock);
 		return 0;
 	}
-	if (!noreap)
+	if (!unlikely(options & WNOWAIT))
 		p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
 	spin_unlock_irq(&p->sighand->siglock);
 
@@ -1519,89 +1528,137 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 	return retval;
 }
 
+/*
+ * Consider @p for a wait by @parent.
+ *
+ * -ECHILD should be in *@notask_error before the first call.
+ * Returns nonzero for a final return, when we have unlocked tasklist_lock.
+ * Returns zero if the search for a child should continue;
+ * then *@notask_error is 0 if @p is an eligible child, or still -ECHILD.
+ */
+static int wait_consider_task(struct task_struct *parent,
+			      struct task_struct *p, int *notask_error,
+			      enum pid_type type, struct pid *pid, int options,
+			      struct siginfo __user *infop,
+			      int __user *stat_addr, struct rusage __user *ru)
+{
+	int ret = eligible_child(type, pid, options, p);
+	if (ret <= 0)
+		return ret;
+
+	if (p->exit_state == EXIT_DEAD)
+		return 0;
+
+	/*
+	 * We don't reap group leaders with subthreads.
+	 */
+	if (p->exit_state == EXIT_ZOMBIE && !delay_group_leader(p))
+		return wait_task_zombie(p, options, infop, stat_addr, ru);
+
+	/*
+	 * It's stopped or running now, so it might
+	 * later continue, exit, or stop again.
+	 */
+	*notask_error = 0;
+
+	if (task_is_stopped_or_traced(p))
+		return wait_task_stopped(p, options, infop, stat_addr, ru);
+
+	return wait_task_continued(p, options, infop, stat_addr, ru);
+}
+
+/*
+ * Do the work of do_wait() for one thread in the group, @tsk.
+ *
+ * -ECHILD should be in *@notask_error before the first call.
+ * Returns nonzero for a final return, when we have unlocked tasklist_lock.
+ * Returns zero if the search for a child should continue; then
+ * *@notask_error is 0 if there were any eligible children, or still -ECHILD.
+ */
+static int do_wait_thread(struct task_struct *tsk, int *notask_error,
+			  enum pid_type type, struct pid *pid, int options,
+			  struct siginfo __user *infop, int __user *stat_addr,
+			  struct rusage __user *ru)
+{
+	struct task_struct *p;
+
+	list_for_each_entry(p, &tsk->children, sibling) {
+		int ret = wait_consider_task(tsk, p, notask_error,
+					     type, pid, options,
+					     infop, stat_addr, ru);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+static int ptrace_do_wait(struct task_struct *tsk, int *notask_error,
+			  enum pid_type type, struct pid *pid, int options,
+			  struct siginfo __user *infop, int __user *stat_addr,
+			  struct rusage __user *ru)
+{
+	struct task_struct *p;
+
+	/*
+	 * If we never saw an eligile child, check for children stolen by
+	 * ptrace.  We don't leave -ECHILD in *@notask_error if there are any,
+	 * because we will eventually be allowed to wait for them again.
+	 */
+	if (!*notask_error)
+		return 0;
+
+	list_for_each_entry(p, &tsk->ptrace_children, ptrace_list) {
+		int ret = eligible_child(type, pid, options, p);
+		if (unlikely(ret < 0))
+			return ret;
+		if (ret) {
+			*notask_error = 0;
+			return 0;
+		}
+	}
+
+	return 0;
+}
+
 static long do_wait(enum pid_type type, struct pid *pid, int options,
 		    struct siginfo __user *infop, int __user *stat_addr,
 		    struct rusage __user *ru)
 {
 	DECLARE_WAITQUEUE(wait, current);
 	struct task_struct *tsk;
-	int flag, retval;
+	int retval;
 
 	add_wait_queue(&current->signal->wait_chldexit,&wait);
 repeat:
-	/* If there is nothing that can match our critier just get out */
+	/*
+	 * If there is nothing that can match our critiera just get out.
+	 * We will clear @retval to zero if we see any child that might later
+	 * match our criteria, even if we are not able to reap it yet.
+	 */
 	retval = -ECHILD;
 	if ((type < PIDTYPE_MAX) && (!pid || hlist_empty(&pid->tasks[type])))
 		goto end;
 
-	/*
-	 * We will set this flag if we see any child that might later
-	 * match our criteria, even if we are not able to reap it yet.
-	 */
-	flag = retval = 0;
 	current->state = TASK_INTERRUPTIBLE;
 	read_lock(&tasklist_lock);
 	tsk = current;
 	do {
-		struct task_struct *p;
-
-		list_for_each_entry(p, &tsk->children, sibling) {
-			int ret = eligible_child(type, pid, options, p);
-			if (!ret)
-				continue;
-
-			if (unlikely(ret < 0)) {
-				retval = ret;
-			} else if (task_is_stopped_or_traced(p)) {
-				/*
-				 * It's stopped now, so it might later
-				 * continue, exit, or stop again.
-				 */
-				flag = 1;
-				if (!(p->ptrace & PT_PTRACED) &&
-				    !(options & WUNTRACED))
-					continue;
-
-				retval = wait_task_stopped(p,
-						(options & WNOWAIT), infop,
-						stat_addr, ru);
-			} else if (p->exit_state == EXIT_ZOMBIE &&
-					!delay_group_leader(p)) {
-				/*
-				 * We don't reap group leaders with subthreads.
-				 */
-				if (!likely(options & WEXITED))
-					continue;
-				retval = wait_task_zombie(p,
-						(options & WNOWAIT), infop,
-						stat_addr, ru);
-			} else if (p->exit_state != EXIT_DEAD) {
-				/*
-				 * It's running now, so it might later
-				 * exit, stop, or stop and then continue.
-				 */
-				flag = 1;
-				if (!unlikely(options & WCONTINUED))
-					continue;
-				retval = wait_task_continued(p,
-						(options & WNOWAIT), infop,
-						stat_addr, ru);
-			}
-			if (retval != 0) /* tasklist_lock released */
-				goto end;
-		}
-		if (!flag) {
-			list_for_each_entry(p, &tsk->ptrace_children,
-								ptrace_list) {
-				flag = eligible_child(type, pid, options, p);
-				if (!flag)
-					continue;
-				if (likely(flag > 0))
-					break;
-				retval = flag;
-				goto end;
-			}
+		int tsk_result = do_wait_thread(tsk, &retval,
+						type, pid, options,
+						infop, stat_addr, ru);
+		if (!tsk_result)
+			tsk_result = ptrace_do_wait(tsk, &retval,
+						    type, pid, options,
+						    infop, stat_addr, ru);
+		if (tsk_result) {
+			/*
+			 * tasklist_lock is unlocked and we have a final result.
+			 */
+			retval = tsk_result;
+			goto end;
 		}
+
 		if (options & __WNOTHREAD)
 			break;
 		tsk = next_thread(tsk);
@@ -1609,16 +1666,14 @@ static long do_wait(enum pid_type type, struct pid *pid, int options,
 	} while (tsk != current);
 	read_unlock(&tasklist_lock);
 
-	if (flag) {
-		if (options & WNOHANG)
-			goto end;
+	if (!retval && !(options & WNOHANG)) {
 		retval = -ERESTARTSYS;
-		if (signal_pending(current))
-			goto end;
-		schedule();
-		goto repeat;
+		if (!signal_pending(current)) {
+			schedule();
+			goto repeat;
+		}
 	}
-	retval = -ECHILD;
+
 end:
 	current->state = TASK_RUNNING;
 	remove_wait_queue(&current->signal->wait_chldexit,&wait);

commit da9cbc87395308a21465bd25441297bbba0477e1
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Jun 30 20:42:08 2008 +0200

    block: blkdev.h cleanup, move iocontext stuff to iocontext.h
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8f6185e69b69..ceb258782835 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -13,6 +13,7 @@
 #include <linux/personality.h>
 #include <linux/tty.h>
 #include <linux/mnt_namespace.h>
+#include <linux/iocontext.h>
 #include <linux/key.h>
 #include <linux/security.h>
 #include <linux/cpu.h>

commit 7c9f8861e6c9c839f913e49b98c3854daca18f27
Author: Eric Sandeen <sandeen@sandeen.net>
Date:   Tue Apr 22 16:38:23 2008 -0500

    stackprotector: use canary at end of stack to indicate overruns at oops time
    
    (Updated with a common max-stack-used checker that knows about
    the canary, as suggested by Joe Perches)
    
    Use a canary at the end of the stack to clearly indicate
    at oops time whether the stack has ever overflowed.
    
    This is a very simple implementation with a couple of
    drawbacks:
    
    1) a thread may legitimately use exactly up to the last
       word on the stack
    
     -- but the chances of doing this and then oopsing later seem slim
    
    2) it's possible that the stack usage isn't dense enough
       that the canary location could get skipped over
    
     -- but the worst that happens is that we don't flag the overrun
     -- though this happens fairly often in my testing :(
    
    With the code in place, an intentionally-bloated stack oops might
    do:
    
    BUG: unable to handle kernel paging request at ffff8103f84cc680
    IP: [<ffffffff810253df>] update_curr+0x9a/0xa8
    PGD 8063 PUD 0
    Thread overran stack or stack corrupted
    Oops: 0000 [1] SMP
    CPU 0
    ...
    
    ... unless the stack overrun is so bad that it corrupts some other
    thread.
    
    Signed-off-by: Eric Sandeen <sandeen@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8f6185e69b69..fb8de6cbf2c7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -899,12 +899,9 @@ static void check_stack_usage(void)
 {
 	static DEFINE_SPINLOCK(low_water_lock);
 	static int lowest_to_date = THREAD_SIZE;
-	unsigned long *n = end_of_stack(current);
 	unsigned long free;
 
-	while (*n == 0)
-		n++;
-	free = (unsigned long)n - (unsigned long)end_of_stack(current);
+	free = stack_not_used(current);
 
 	if (free >= lowest_to_date)
 		return;

commit da7978b0348d497688541e2d2f5739aa2a2c334f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri May 23 13:04:41 2008 -0700

    signals: fix sigqueue_free() vs __exit_signal() race
    
    __exit_signal() does flush_sigqueue(tsk->pending) outside of ->siglock.
    This can race with another thread doing sigqueue_free(), we can free the
    same SIGQUEUE_PREALLOC sigqueue twice or corrupt the pending->list.
    
    Note that even sys_exit_group() can trigger this race, not only
    sys_timer_delete().
    
    Move the callsite of flush_sigqueue(tsk->pending) under ->siglock.
    
    This patch doesn't touch flush_sigqueue(->shared_pending) below, it is
    called when there are no other threads which can play with signals, and
    sigqueue_free() can't be used outside of our thread group.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1510f78a0ffa..8f6185e69b69 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -126,6 +126,12 @@ static void __exit_signal(struct task_struct *tsk)
 
 	__unhash_process(tsk);
 
+	/*
+	 * Do this under ->siglock, we can race with another thread
+	 * doing sigqueue_free() if we have SIGQUEUE_PREALLOC signals.
+	 */
+	flush_sigqueue(&tsk->pending);
+
 	tsk->signal = NULL;
 	tsk->sighand = NULL;
 	spin_unlock(&sighand->siglock);
@@ -133,7 +139,6 @@ static void __exit_signal(struct task_struct *tsk)
 
 	__cleanup_sighand(sighand);
 	clear_tsk_thread_flag(tsk,TIF_SIGPENDING);
-	flush_sigqueue(&tsk->pending);
 	if (sig) {
 		flush_sigqueue(&sig->shared_pending);
 		taskstats_tgid_free(sig);

commit 9f3acc3140444a900ab280de942291959f0f615d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Apr 24 07:44:08 2008 -0400

    [PATCH] split linux/file.h
    
    Initial splitoff of the low-level stuff; taken to fdtable.h
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index d3ad54677f9c..1510f78a0ffa 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -19,6 +19,7 @@
 #include <linux/acct.h>
 #include <linux/tsacct_kern.h>
 #include <linux/file.h>
+#include <linux/fdtable.h>
 #include <linux/binfmts.h>
 #include <linux/nsproxy.h>
 #include <linux/pid_namespace.h>

commit 7d8da0962eaee30b4a380ded177349bfbdd6ac46
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Apr 30 00:54:27 2008 -0700

    pids: __set_special_pids: use change_pid() helper
    
    Use change_pid() instead of detach_pid() + attach_pid() in
    __set_special_pids().
    
    This way task_session() is not NULL in between.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc:  "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 0da2921b1e7f..d3ad54677f9c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -334,13 +334,11 @@ void __set_special_pids(struct pid *pid)
 	pid_t nr = pid_nr(pid);
 
 	if (task_session(curr) != pid) {
-		detach_pid(curr, PIDTYPE_SID);
-		attach_pid(curr, PIDTYPE_SID, pid);
+		change_pid(curr, PIDTYPE_SID, pid);
 		set_task_session(curr, nr);
 	}
 	if (task_pgrp(curr) != pid) {
-		detach_pid(curr, PIDTYPE_PGID);
-		attach_pid(curr, PIDTYPE_PGID, pid);
+		change_pid(curr, PIDTYPE_PGID, pid);
 		set_task_pgrp(curr, nr);
 	}
 }

commit 53b6f9fbd3b63af14b4f6268e8b5b80d178d05bc
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Apr 30 00:53:13 2008 -0700

    ptrace: introduce ptrace_reparented() helper
    
    Add another trivial helper for the sake of grep.  It also auto-documents the
    fact that ->parent != real_parent implies ->ptrace.
    
    No functional changes.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 879ed6e1c883..0da2921b1e7f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -698,7 +698,7 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 	if (unlikely(traced)) {
 		/* Preserve ptrace links if someone else is tracing this child.  */
 		list_del_init(&p->ptrace_list);
-		if (p->parent != p->real_parent)
+		if (ptrace_reparented(p))
 			list_add(&p->ptrace_list, &p->real_parent->ptrace_children);
 	} else {
 		/* If this child is being traced, then we're the one tracing it
@@ -865,8 +865,8 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	 * only has special meaning to our real parent.
 	 */
 	if (!task_detached(tsk) && thread_group_empty(tsk)) {
-		int signal = (tsk->parent == tsk->real_parent)
-				? tsk->exit_signal : SIGCHLD;
+		int signal = ptrace_reparented(tsk) ?
+				SIGCHLD : tsk->exit_signal;
 		do_notify_parent(tsk, signal);
 	} else if (tsk->ptrace) {
 		do_notify_parent(tsk, SIGCHLD);
@@ -1269,8 +1269,7 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 		return 0;
 	}
 
-	/* traced means p->ptrace, but not vice versa */
-	traced = (p->real_parent != p->parent);
+	traced = ptrace_reparented(p);
 
 	if (likely(!traced)) {
 		struct signal_struct *psig;

commit 2800d8d19e51414403df8144eaa214bb03400b87
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Apr 30 00:53:12 2008 -0700

    document de_thread() with exit_notify() connection
    
    Add a couple of small comments, it is not easy to see what this code does.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 413c81ec858e..879ed6e1c883 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -877,6 +877,7 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 		state = EXIT_DEAD;
 	tsk->exit_state = state;
 
+	/* mt-exec, de_thread() is waiting for us */
 	if (thread_group_leader(tsk) &&
 	    tsk->signal->notify_count < 0 &&
 	    tsk->signal->group_exit_task)

commit 376e1d2531860358c8a79fecf5f4f42994d03c4d
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Apr 30 00:53:12 2008 -0700

    reparent_thread: use same_thread_group()
    
    Trivial, use same_thread_group() in reparent_thread().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4035d391a0d3..413c81ec858e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -722,7 +722,7 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 	/* If this is a threaded reparent there is no need to
 	 * notify anyone anything has happened.
 	 */
-	if (p->real_parent->group_leader == father->group_leader)
+	if (same_thread_group(p->real_parent, father))
 		return;
 
 	/* We don't want people slaying init.  */

commit d839fd4d2e95a5fbc4d50aa9d17eed6a5f2094e6
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Apr 30 00:53:11 2008 -0700

    ptrace: introduce task_detached() helper
    
    exit.c has numerous "->exit_signal == -1" comparisons, this check is subtle
    and deserves a helper.  Imho makes the code more parseable for humans.  At
    least it's surely more greppable.
    
    Also, a couple of whitespace cleanups. No functional changes.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6d019aa8522e..4035d391a0d3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -52,6 +52,11 @@
 
 static void exit_mm(struct task_struct * tsk);
 
+static inline int task_detached(struct task_struct *p)
+{
+	return p->exit_signal == -1;
+}
+
 static void __unhash_process(struct task_struct *p)
 {
 	nr_threads--;
@@ -160,7 +165,7 @@ void release_task(struct task_struct * p)
 	zap_leader = 0;
 	leader = p->group_leader;
 	if (leader != p && thread_group_empty(leader) && leader->exit_state == EXIT_ZOMBIE) {
-		BUG_ON(leader->exit_signal == -1);
+		BUG_ON(task_detached(leader));
 		do_notify_parent(leader, leader->exit_signal);
 		/*
 		 * If we were the last child thread and the leader has
@@ -170,7 +175,7 @@ void release_task(struct task_struct * p)
 		 * do_notify_parent() will have marked it self-reaping in
 		 * that case.
 		 */
-		zap_leader = (leader->exit_signal == -1);
+		zap_leader = task_detached(leader);
 	}
 
 	write_unlock_irq(&tasklist_lock);
@@ -721,14 +726,14 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 		return;
 
 	/* We don't want people slaying init.  */
-	if (p->exit_signal != -1)
+	if (!task_detached(p))
 		p->exit_signal = SIGCHLD;
 
 	/* If we'd notified the old parent about this child's death,
 	 * also notify the new parent.
 	 */
 	if (!traced && p->exit_state == EXIT_ZOMBIE &&
-	    p->exit_signal != -1 && thread_group_empty(p))
+	    !task_detached(p) && thread_group_empty(p))
 		do_notify_parent(p, p->exit_signal);
 
 	kill_orphaned_pgrp(p, father);
@@ -781,18 +786,18 @@ static void forget_original_parent(struct task_struct *father)
 		} else {
 			/* reparent ptraced task to its real parent */
 			__ptrace_unlink (p);
-			if (p->exit_state == EXIT_ZOMBIE && p->exit_signal != -1 &&
+			if (p->exit_state == EXIT_ZOMBIE && !task_detached(p) &&
 			    thread_group_empty(p))
 				do_notify_parent(p, p->exit_signal);
 		}
 
 		/*
-		 * if the ptraced child is a zombie with exit_signal == -1
-		 * we must collect it before we exit, or it will remain
-		 * zombie forever since we prevented it from self-reap itself
-		 * while it was being traced by us, to be able to see it in wait4.
+		 * if the ptraced child is a detached zombie we must collect
+		 * it before we exit, or it will remain zombie forever since
+		 * we prevented it from self-reap itself while it was being
+		 * traced by us, to be able to see it in wait4.
 		 */
-		if (unlikely(ptrace && p->exit_state == EXIT_ZOMBIE && p->exit_signal == -1))
+		if (unlikely(ptrace && p->exit_state == EXIT_ZOMBIE && task_detached(p)))
 			list_add(&p->ptrace_list, &ptrace_dead);
 	}
 
@@ -849,26 +854,26 @@ static void exit_notify(struct task_struct *tsk, int group_dead)
 	 * we have changed execution domain as these two values started
 	 * the same after a fork.
 	 */
-	if (tsk->exit_signal != SIGCHLD && tsk->exit_signal != -1 &&
+	if (tsk->exit_signal != SIGCHLD && !task_detached(tsk) &&
 	    (tsk->parent_exec_id != tsk->real_parent->self_exec_id ||
-	     tsk->self_exec_id != tsk->parent_exec_id)
-	    && !capable(CAP_KILL))
+	     tsk->self_exec_id != tsk->parent_exec_id) &&
+	    !capable(CAP_KILL))
 		tsk->exit_signal = SIGCHLD;
 
-
 	/* If something other than our normal parent is ptracing us, then
 	 * send it a SIGCHLD instead of honoring exit_signal.  exit_signal
 	 * only has special meaning to our real parent.
 	 */
-	if (tsk->exit_signal != -1 && thread_group_empty(tsk)) {
-		int signal = tsk->parent == tsk->real_parent ? tsk->exit_signal : SIGCHLD;
+	if (!task_detached(tsk) && thread_group_empty(tsk)) {
+		int signal = (tsk->parent == tsk->real_parent)
+				? tsk->exit_signal : SIGCHLD;
 		do_notify_parent(tsk, signal);
 	} else if (tsk->ptrace) {
 		do_notify_parent(tsk, SIGCHLD);
 	}
 
 	state = EXIT_ZOMBIE;
-	if (tsk->exit_signal == -1 && likely(!tsk->ptrace))
+	if (task_detached(tsk) && likely(!tsk->ptrace))
 		state = EXIT_DEAD;
 	tsk->exit_state = state;
 
@@ -1173,7 +1178,7 @@ static int eligible_child(enum pid_type type, struct pid *pid, int options,
 	 * Do not consider detached threads that are
 	 * not ptraced:
 	 */
-	if (p->exit_signal == -1 && !p->ptrace)
+	if (task_detached(p) && !p->ptrace)
 		return 0;
 
 	/* Wait for all children (clone and not) if __WALL is set;
@@ -1365,9 +1370,9 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 		 * If it's still not detached after that, don't release
 		 * it now.
 		 */
-		if (p->exit_signal != -1) {
+		if (!task_detached(p)) {
 			do_notify_parent(p, p->exit_signal);
-			if (p->exit_signal != -1) {
+			if (!task_detached(p)) {
 				p->exit_state = EXIT_ZOMBIE;
 				p = NULL;
 			}

commit bfc4b0890af566940de6e7aeb4b5faf46d3c3513
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Apr 30 00:52:36 2008 -0700

    signals: do_group_exit(): use signal_group_exit() more consistently
    
    do_group_exit() checks SIGNAL_GROUP_EXIT to avoid taking sighand->siglock.
    Since ed5d2cac114202fe2978a9cbcab8f5032796d538 exec() doesn't set this
    flag, we should use signal_group_exit().
    
    This is not needed for correctness, but can speedup the multithreaded exec
    and makes the code more consistent.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ae0f2c4e452b..6d019aa8522e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1115,12 +1115,13 @@ asmlinkage long sys_exit(int error_code)
 NORET_TYPE void
 do_group_exit(int exit_code)
 {
+	struct signal_struct *sig = current->signal;
+
 	BUG_ON(exit_code & 0x80); /* core dumps don't get here */
 
-	if (current->signal->flags & SIGNAL_GROUP_EXIT)
-		exit_code = current->signal->group_exit_code;
+	if (signal_group_exit(sig))
+		exit_code = sig->group_exit_code;
 	else if (!thread_group_empty(current)) {
-		struct signal_struct *const sig = current->signal;
 		struct sighand_struct *const sighand = current->sighand;
 		spin_lock_irq(&sighand->siglock);
 		if (signal_group_exit(sig))

commit cf475ad28ac35cc9ba612d67158f29b73b38b05d
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Tue Apr 29 01:00:16 2008 -0700

    cgroups: add an owner to the mm_struct
    
    Remove the mem_cgroup member from mm_struct and instead adds an owner.
    
    This approach was suggested by Paul Menage.  The advantage of this approach
    is that, once the mm->owner is known, using the subsystem id, the cgroup
    can be determined.  It also allows several control groups that are
    virtually grouped by mm_struct, to exist independent of the memory
    controller i.e., without adding mem_cgroup's for each controller, to
    mm_struct.
    
    A new config option CONFIG_MM_OWNER is added and the memory resource
    controller selects this config option.
    
    This patch also adds cgroup callbacks to notify subsystems when mm->owner
    changes.  The mm_cgroup_changed callback is called with the task_lock() of
    the new task held and is called just prior to changing the mm->owner.
    
    I am indebted to Paul Menage for the several reviews of this patchset and
    helping me make it lighter and simpler.
    
    This patch was tested on a powerpc box, it was compiled with both the
    MM_OWNER config turned on and off.
    
    After the thread group leader exits, it's moved to init_css_state by
    cgroup_exit(), thus all future charges from runnings threads would be
    redirected to the init_css_set's subsystem.
    
    Signed-off-by: Balbir Singh <balbir@linux.vnet.ibm.com>
    Cc: Pavel Emelianov <xemul@openvz.org>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: Sudhir Kumar <skumar@linux.vnet.ibm.com>
    Cc: YAMAMOTO Takashi <yamamoto@valinux.co.jp>
    Cc: Hirokazu Takahashi <taka@valinux.co.jp>
    Cc: David Rientjes <rientjes@google.com>,
    Cc: Balbir Singh <balbir@linux.vnet.ibm.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Reviewed-by: Paul Menage <menage@google.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2a9d98c641ac..ae0f2c4e452b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -557,6 +557,88 @@ void exit_fs(struct task_struct *tsk)
 
 EXPORT_SYMBOL_GPL(exit_fs);
 
+#ifdef CONFIG_MM_OWNER
+/*
+ * Task p is exiting and it owned mm, lets find a new owner for it
+ */
+static inline int
+mm_need_new_owner(struct mm_struct *mm, struct task_struct *p)
+{
+	/*
+	 * If there are other users of the mm and the owner (us) is exiting
+	 * we need to find a new owner to take on the responsibility.
+	 */
+	if (!mm)
+		return 0;
+	if (atomic_read(&mm->mm_users) <= 1)
+		return 0;
+	if (mm->owner != p)
+		return 0;
+	return 1;
+}
+
+void mm_update_next_owner(struct mm_struct *mm)
+{
+	struct task_struct *c, *g, *p = current;
+
+retry:
+	if (!mm_need_new_owner(mm, p))
+		return;
+
+	read_lock(&tasklist_lock);
+	/*
+	 * Search in the children
+	 */
+	list_for_each_entry(c, &p->children, sibling) {
+		if (c->mm == mm)
+			goto assign_new_owner;
+	}
+
+	/*
+	 * Search in the siblings
+	 */
+	list_for_each_entry(c, &p->parent->children, sibling) {
+		if (c->mm == mm)
+			goto assign_new_owner;
+	}
+
+	/*
+	 * Search through everything else. We should not get
+	 * here often
+	 */
+	do_each_thread(g, c) {
+		if (c->mm == mm)
+			goto assign_new_owner;
+	} while_each_thread(g, c);
+
+	read_unlock(&tasklist_lock);
+	return;
+
+assign_new_owner:
+	BUG_ON(c == p);
+	get_task_struct(c);
+	/*
+	 * The task_lock protects c->mm from changing.
+	 * We always want mm->owner->mm == mm
+	 */
+	task_lock(c);
+	/*
+	 * Delay read_unlock() till we have the task_lock()
+	 * to ensure that c does not slip away underneath us
+	 */
+	read_unlock(&tasklist_lock);
+	if (c->mm != mm) {
+		task_unlock(c);
+		put_task_struct(c);
+		goto retry;
+	}
+	cgroup_mm_owner_callbacks(mm->owner, c);
+	mm->owner = c;
+	task_unlock(c);
+	put_task_struct(c);
+}
+#endif /* CONFIG_MM_OWNER */
+
 /*
  * Turn us into a lazy TLB process if we
  * aren't already..
@@ -596,6 +678,7 @@ static void exit_mm(struct task_struct * tsk)
 	/* We don't want this task to be frozen prematurely */
 	clear_freeze_flag(tsk);
 	task_unlock(tsk);
+	mm_update_next_owner(mm);
 	mmput(mm);
 }
 

commit f0be3d32b05d3fea2fcdbbb81a39dac2a7163169
Author: Lee Schermerhorn <lee.schermerhorn@hp.com>
Date:   Mon Apr 28 02:13:08 2008 -0700

    mempolicy: rename mpol_free to mpol_put
    
    This is a change that was requested some time ago by Mel Gorman.  Makes sense
    to me, so here it is.
    
    Note: I retain the name "mpol_free_shared_policy()" because it actually does
    free the shared_policy, which is NOT a reference counted object.  However, ...
    
    The mempolicy object[s] referenced by the shared_policy are reference counted,
    so mpol_put() is used to release the reference held by the shared_policy.  The
    mempolicy might not be freed at this time, because some task attached to the
    shared object associated with the shared policy may be in the process of
    allocating a page based on the mempolicy.  In that case, the task performing
    the allocation will hold a reference on the mempolicy, obtained via
    mpol_shared_policy_lookup().  The mempolicy will be freed when all tasks
    holding such a reference have called mpol_put() for the mempolicy.
    
    Signed-off-by: Lee Schermerhorn <lee.schermerhorn@hp.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Andi Kleen <ak@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 97f609f574b1..2a9d98c641ac 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -967,7 +967,7 @@ NORET_TYPE void do_exit(long code)
 	proc_exit_connector(tsk);
 	exit_notify(tsk, group_dead);
 #ifdef CONFIG_NUMA
-	mpol_free(tsk->mempolicy);
+	mpol_put(tsk->mempolicy);
 	tsk->mempolicy = NULL;
 #endif
 #ifdef CONFIG_FUTEX

commit 3b1253880b7a9e6db54b943b2d40bcf2202f58ab
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Apr 22 05:31:30 2008 -0400

    [PATCH] sanitize unshare_files/reset_files_struct
    
    * let unshare_files() give caller the displaced files_struct
    * don't bother with grabbing reference only to drop it in the
      caller if it hadn't been shared in the first place
    * in that form unshare_files() is trivially implemented via
      unshare_fd(), so we eliminate the duplicate logics in fork.c
    * reset_files_struct() is not just only called for current;
      it will break the system if somebody ever calls it for anything
      else (we can't modify ->files of somebody else).  Lose the
      task_struct * argument.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3d320003cc03..97f609f574b1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -507,8 +507,9 @@ void put_files_struct(struct files_struct *files)
 	}
 }
 
-void reset_files_struct(struct task_struct *tsk, struct files_struct *files)
+void reset_files_struct(struct files_struct *files)
 {
+	struct task_struct *tsk = current;
 	struct files_struct *old;
 
 	old = tsk->files;

commit fd8328be874f4190a811c58cd4778ec2c74d2c05
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Apr 22 05:11:59 2008 -0400

    [PATCH] sanitize handling of shared descriptor tables in failing execve()
    
    * unshare_files() can fail; doing it after irreversible actions is wrong
      and de_thread() is certainly irreversible.
    * since we do it unconditionally anyway, we might as well do it in do_execve()
      and save ourselves the PITA in binfmt handlers, etc.
    * while we are at it, binfmt_som actually leaked files_struct on failure.
    
    As a side benefit, unshare_files(), put_files_struct() and reset_files_struct()
    become unexported.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index cece89f80ab4..3d320003cc03 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -507,8 +507,6 @@ void put_files_struct(struct files_struct *files)
 	}
 }
 
-EXPORT_SYMBOL(put_files_struct);
-
 void reset_files_struct(struct task_struct *tsk, struct files_struct *files)
 {
 	struct files_struct *old;
@@ -519,7 +517,6 @@ void reset_files_struct(struct task_struct *tsk, struct files_struct *files)
 	task_unlock(tsk);
 	put_files_struct(old);
 }
-EXPORT_SYMBOL(reset_files_struct);
 
 void exit_files(struct task_struct *tsk)
 {

commit 1ec7f1ddbe5ba49f7b10c3b129d6d5c90c43526c
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Apr 22 05:35:42 2008 -0400

    [PATCH] get rid of __exit_files(), __exit_fs() and __put_fs_struct()
    
    The only reason to have separated __...() for those was to keep them inlined
    for local users in exit.c.  Since Alexey removed the inline on those, there's
    no reason whatsoever to keep them around; just collapse with normal variants.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 073005b1cfb2..cece89f80ab4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -521,7 +521,7 @@ void reset_files_struct(struct task_struct *tsk, struct files_struct *files)
 }
 EXPORT_SYMBOL(reset_files_struct);
 
-static void __exit_files(struct task_struct *tsk)
+void exit_files(struct task_struct *tsk)
 {
 	struct files_struct * files = tsk->files;
 
@@ -533,12 +533,7 @@ static void __exit_files(struct task_struct *tsk)
 	}
 }
 
-void exit_files(struct task_struct *tsk)
-{
-	__exit_files(tsk);
-}
-
-static void __put_fs_struct(struct fs_struct *fs)
+void put_fs_struct(struct fs_struct *fs)
 {
 	/* No need to hold fs->lock if we are killing it */
 	if (atomic_dec_and_test(&fs->count)) {
@@ -550,12 +545,7 @@ static void __put_fs_struct(struct fs_struct *fs)
 	}
 }
 
-void put_fs_struct(struct fs_struct *fs)
-{
-	__put_fs_struct(fs);
-}
-
-static void __exit_fs(struct task_struct *tsk)
+void exit_fs(struct task_struct *tsk)
 {
 	struct fs_struct * fs = tsk->fs;
 
@@ -563,15 +553,10 @@ static void __exit_fs(struct task_struct *tsk)
 		task_lock(tsk);
 		tsk->fs = NULL;
 		task_unlock(tsk);
-		__put_fs_struct(fs);
+		put_fs_struct(fs);
 	}
 }
 
-void exit_fs(struct task_struct *tsk)
-{
-	__exit_fs(tsk);
-}
-
 EXPORT_SYMBOL_GPL(exit_fs);
 
 /*
@@ -967,8 +952,8 @@ NORET_TYPE void do_exit(long code)
 	if (group_dead)
 		acct_process();
 	exit_sem(tsk);
-	__exit_files(tsk);
-	__exit_fs(tsk);
+	exit_files(tsk);
+	exit_fs(tsk);
 	check_stack_usage();
 	exit_thread();
 	cgroup_exit(tsk, 1);

commit 54a015104136974262afa4b8ddd943ea70dec8a2
Author: Roland McGrath <roland@redhat.com>
Date:   Thu Apr 10 15:37:38 2008 -0700

    asmlinkage_protect replaces prevent_tail_call
    
    The prevent_tail_call() macro works around the problem of the compiler
    clobbering argument words on the stack, which for asmlinkage functions
    is the caller's (user's) struct pt_regs.  The tail/sibling-call
    optimization is not the only way that the compiler can decide to use
    stack argument words as scratch space, which we have to prevent.
    Other optimizations can do it too.
    
    Until we have new compiler support to make "asmlinkage" binding on the
    compiler's own use of the stack argument frame, we have work around all
    the manifestations of this issue that crop up.
    
    More cases seem to be prevented by also keeping the incoming argument
    variables live at the end of the function.  This makes their original
    stack slots attractive places to leave those variables, so the compiler
    tends not clobber them for something else.  It's still no guarantee, but
    it handles some observed cases that prevent_tail_call() did not.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 53872bf993fa..073005b1cfb2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1608,7 +1608,7 @@ asmlinkage long sys_waitid(int which, pid_t upid,
 	put_pid(pid);
 
 	/* avoid REGPARM breakage on x86: */
-	prevent_tail_call(ret);
+	asmlinkage_protect(5, ret, which, upid, infop, options, ru);
 	return ret;
 }
 
@@ -1640,7 +1640,7 @@ asmlinkage long sys_wait4(pid_t upid, int __user *stat_addr,
 	put_pid(pid);
 
 	/* avoid REGPARM breakage on x86: */
-	prevent_tail_call(ret);
+	asmlinkage_protect(4, ret, upid, stat_addr, options, ru);
 	return ret;
 }
 

commit 6efcae460186c0c1c94afff58a92784e1fc0d10b
Author: Roland McGrath <roland@redhat.com>
Date:   Sat Mar 8 11:41:22 2008 -0800

    Fix waitid si_code regression
    
    In commit ee7c82da830ea860b1f9274f1f0cdf99f206e7c2 ("wait_task_stopped:
    simplify and fix races with SIGCONT/SIGKILL/untrace"), the magic (short)
    cast when storing si_code was lost in wait_task_stopped.  This leaks the
    in-kernel CLD_* values that do not match what userland expects.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index cd20bf07e9e3..53872bf993fa 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1378,7 +1378,7 @@ static int wait_task_stopped(struct task_struct *p,
 	if (!retval && infop)
 		retval = put_user(0, &infop->si_errno);
 	if (!retval && infop)
-		retval = put_user(why, &infop->si_code);
+		retval = put_user((short)why, &infop->si_code);
 	if (!retval && infop)
 		retval = put_user(exit_code, &infop->si_status);
 	if (!retval && infop)

commit 821c7de7194e77afee1a69d50830a329a6d9af9f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Mar 2 21:44:44 2008 +0300

    exit_notify: fix kill_orphaned_pgrp() usage with mt exit
    
    1. exit_notify() always calls kill_orphaned_pgrp(). This is wrong, we
       should do this only when the whole process exits.
    
    2. exit_notify() uses "current" as "ignored_task", obviously wrong.
       Use ->group_leader instead.
    
    Test case:
    
            void hup(int sig)
            {
                    printf("HUP received\n");
            }
    
            void *tfunc(void *arg)
            {
                    sleep(2);
                    printf("sub-thread exited\n");
                    return NULL;
            }
    
            int main(int argc, char *argv[])
            {
                    if (!fork()) {
                            signal(SIGHUP, hup);
                            kill(getpid(), SIGSTOP);
                            exit(0);
                    }
    
                    pthread_t thr;
                    pthread_create(&thr, NULL, tfunc, NULL);
    
                    sleep(1);
                    printf("main thread exited\n");
                    syscall(__NR_exit, 0);
    
                    return 0;
            }
    
    output:
    
            main thread exited
            HUP received
            Hangup
    
    With this patch the output is:
    
            main thread exited
            sub-thread exited
            HUP received
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 41c1edace97a..cd20bf07e9e3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -750,7 +750,7 @@ static void forget_original_parent(struct task_struct *father)
  * Send signals to all our closest relatives so that they know
  * to properly mourn us..
  */
-static void exit_notify(struct task_struct *tsk)
+static void exit_notify(struct task_struct *tsk, int group_dead)
 {
 	int state;
 
@@ -766,7 +766,8 @@ static void exit_notify(struct task_struct *tsk)
 	exit_task_namespaces(tsk);
 
 	write_lock_irq(&tasklist_lock);
-	kill_orphaned_pgrp(tsk, NULL);
+	if (group_dead)
+		kill_orphaned_pgrp(tsk->group_leader, NULL);
 
 	/* Let father know we died
 	 *
@@ -981,7 +982,7 @@ NORET_TYPE void do_exit(long code)
 		module_put(tsk->binfmt->module);
 
 	proc_exit_connector(tsk);
-	exit_notify(tsk);
+	exit_notify(tsk, group_dead);
 #ifdef CONFIG_NUMA
 	mpol_free(tsk->mempolicy);
 	tsk->mempolicy = NULL;

commit 05e83df624fe682bb8571cdb2c6d5284a99c3066
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Mar 2 21:44:42 2008 +0300

    will_become_orphaned_pgrp: partially fix insufficient ->exit_state check
    
    p->exit_state != 0 doesn't mean this process is dead, it may have
    sub-threads.  Change the code to use "p->exit_state && thread_group_empty(p)"
    instead.
    
    Without this patch, ^Z doesn't deliver SIGTSTP to the foreground process
    if the main thread has exited.
    
    However, the new check is not perfect either.  There is a window when
    exit_notify() drops tasklist and before release_task().  Suppose that
    the last (non-leader) thread exits.  This means that entire group exits,
    but thread_group_empty() is not true yet.
    
    As Eric pointed out, is_global_init() is wrong as well, but I did not
    dare to do other changes.
    
    Just for the record, has_stopped_jobs() is absolutely wrong too.  But we
    can't fix it now, we should first fix SIGNAL_STOP_STOPPED issues.
    
    Even with this patch ^Z doesn't play well with the dead main thread.
    The task is stopped correctly but do_wait(WSTOPPED) won't see it.  This
    is another unrelated issue, will be (hopefully) fixed separately.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 11fcce760151..41c1edace97a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -214,20 +214,19 @@ struct pid *session_of_pgrp(struct pid *pgrp)
 static int will_become_orphaned_pgrp(struct pid *pgrp, struct task_struct *ignored_task)
 {
 	struct task_struct *p;
-	int ret = 1;
 
 	do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
-		if (p == ignored_task
-				|| p->exit_state
-				|| is_global_init(p->real_parent))
+		if ((p == ignored_task) ||
+		    (p->exit_state && thread_group_empty(p)) ||
+		    is_global_init(p->real_parent))
 			continue;
+
 		if (task_pgrp(p->real_parent) != pgrp &&
-		    task_session(p->real_parent) == task_session(p)) {
-			ret = 0;
-			break;
-		}
+		    task_session(p->real_parent) == task_session(p))
+			return 0;
 	} while_each_pid_task(pgrp, PIDTYPE_PGID, p);
-	return ret;	/* (sighing) "Often!" */
+
+	return 1;
 }
 
 int is_current_pgrp_orphaned(void)

commit f49ee505b1ecb5960984880740f09aba87f870dc
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Mar 2 21:44:40 2008 +0300

    introduce kill_orphaned_pgrp() helper
    
    Factor out the common code in reparent_thread() and exit_notify().
    
    No functional changes.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 506a957b665a..11fcce760151 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -255,6 +255,37 @@ static int has_stopped_jobs(struct pid *pgrp)
 	return retval;
 }
 
+/*
+ * Check to see if any process groups have become orphaned as
+ * a result of our exiting, and if they have any stopped jobs,
+ * send them a SIGHUP and then a SIGCONT. (POSIX 3.2.2.2)
+ */
+static void
+kill_orphaned_pgrp(struct task_struct *tsk, struct task_struct *parent)
+{
+	struct pid *pgrp = task_pgrp(tsk);
+	struct task_struct *ignored_task = tsk;
+
+	if (!parent)
+		 /* exit: our father is in a different pgrp than
+		  * we are and we were the only connection outside.
+		  */
+		parent = tsk->real_parent;
+	else
+		/* reparent: our child is in a different pgrp than
+		 * we are, and it was the only connection outside.
+		 */
+		ignored_task = NULL;
+
+	if (task_pgrp(parent) != pgrp &&
+	    task_session(parent) == task_session(tsk) &&
+	    will_become_orphaned_pgrp(pgrp, ignored_task) &&
+	    has_stopped_jobs(pgrp)) {
+		__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);
+		__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);
+	}
+}
+
 /**
  * reparent_to_kthreadd - Reparent the calling kernel thread to kthreadd
  *
@@ -635,22 +666,7 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 	    p->exit_signal != -1 && thread_group_empty(p))
 		do_notify_parent(p, p->exit_signal);
 
-	/*
-	 * process group orphan check
-	 * Case ii: Our child is in a different pgrp
-	 * than we are, and it was the only connection
-	 * outside, so the child pgrp is now orphaned.
-	 */
-	if ((task_pgrp(p) != task_pgrp(father)) &&
-	    (task_session(p) == task_session(father))) {
-		struct pid *pgrp = task_pgrp(p);
-
-		if (will_become_orphaned_pgrp(pgrp, NULL) &&
-		    has_stopped_jobs(pgrp)) {
-			__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);
-			__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);
-		}
-	}
+	kill_orphaned_pgrp(p, father);
 }
 
 /*
@@ -738,8 +754,6 @@ static void forget_original_parent(struct task_struct *father)
 static void exit_notify(struct task_struct *tsk)
 {
 	int state;
-	struct task_struct *t;
-	struct pid *pgrp;
 
 	/*
 	 * This does two things:
@@ -753,25 +767,7 @@ static void exit_notify(struct task_struct *tsk)
 	exit_task_namespaces(tsk);
 
 	write_lock_irq(&tasklist_lock);
-	/*
-	 * Check to see if any process groups have become orphaned
-	 * as a result of our exiting, and if they have any stopped
-	 * jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
-	 *
-	 * Case i: Our father is in a different pgrp than we are
-	 * and we were the only connection outside, so our pgrp
-	 * is about to become orphaned.
-	 */
-	t = tsk->real_parent;
-
-	pgrp = task_pgrp(tsk);
-	if ((task_pgrp(t) != pgrp) &&
-	    (task_session(t) == task_session(tsk)) &&
-	    will_become_orphaned_pgrp(pgrp, tsk) &&
-	    has_stopped_jobs(pgrp)) {
-		__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);
-		__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);
-	}
+	kill_orphaned_pgrp(tsk, NULL);
 
 	/* Let father know we died
 	 *
@@ -788,8 +784,8 @@ static void exit_notify(struct task_struct *tsk)
 	 * the same after a fork.
 	 */
 	if (tsk->exit_signal != SIGCHLD && tsk->exit_signal != -1 &&
-	    ( tsk->parent_exec_id != t->self_exec_id  ||
-	      tsk->self_exec_id != tsk->parent_exec_id)
+	    (tsk->parent_exec_id != tsk->real_parent->self_exec_id ||
+	     tsk->self_exec_id != tsk->parent_exec_id)
 	    && !capable(CAP_KILL))
 		tsk->exit_signal = SIGCHLD;
 

commit 6ac08c39a16f72c2d3e845cb6849a1392fa03e80
Author: Jan Blunck <jblunck@suse.de>
Date:   Thu Feb 14 19:34:38 2008 -0800

    Use struct path in fs_struct
    
    * Use struct path in fs_struct.
    
    Signed-off-by: Andreas Gruenbacher <agruen@suse.de>
    Signed-off-by: Jan Blunck <jblunck@suse.de>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3b893e78ce61..506a957b665a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -512,14 +512,10 @@ static void __put_fs_struct(struct fs_struct *fs)
 {
 	/* No need to hold fs->lock if we are killing it */
 	if (atomic_dec_and_test(&fs->count)) {
-		dput(fs->root);
-		mntput(fs->rootmnt);
-		dput(fs->pwd);
-		mntput(fs->pwdmnt);
-		if (fs->altroot) {
-			dput(fs->altroot);
-			mntput(fs->altrootmnt);
-		}
+		path_put(&fs->root);
+		path_put(&fs->pwd);
+		if (fs->altroot.dentry)
+			path_put(&fs->altroot);
 		kmem_cache_free(fs_cachep, fs);
 	}
 }

commit 7ad5b3a505e68cfdc342933d6e0fc0eaa5e0a4f7
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Fri Feb 8 04:19:53 2008 -0800

    kernel: remove fastcall in kernel/*
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 81345ba4b253..3b893e78ce61 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -458,7 +458,7 @@ struct files_struct *get_files_struct(struct task_struct *task)
 	return files;
 }
 
-void fastcall put_files_struct(struct files_struct *files)
+void put_files_struct(struct files_struct *files)
 {
 	struct fdtable *fdt;
 
@@ -887,7 +887,7 @@ static inline void exit_child_reaper(struct task_struct *tsk)
 	zap_pid_ns_processes(tsk->nsproxy->pid_ns);
 }
 
-fastcall NORET_TYPE void do_exit(long code)
+NORET_TYPE void do_exit(long code)
 {
 	struct task_struct *tsk = current;
 	int group_dead;

commit 6c5f3e7b43300508fe3947ff3cfff0f86043bb57
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Feb 8 04:19:20 2008 -0800

    Pidns: make full use of xxx_vnr() calls
    
    Some time ago the xxx_vnr() calls (e.g.  pid_vnr or find_task_by_vpid) were
    _all_ converted to operate on the current pid namespace.  After this each call
    like xxx_nr_ns(foo, current->nsproxy->pid_ns) is nothing but a xxx_vnr(foo)
    one.
    
    Switch all the xxx_nr_ns() callers to use the xxx_vnr() calls where
    appropriate.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Reviewed-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2567de3487bd..81345ba4b253 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1174,7 +1174,7 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 {
 	unsigned long state;
 	int retval, status, traced;
-	pid_t pid = task_pid_nr_ns(p, current->nsproxy->pid_ns);
+	pid_t pid = task_pid_vnr(p);
 
 	if (unlikely(noreap)) {
 		uid_t uid = p->uid;
@@ -1369,7 +1369,7 @@ static int wait_task_stopped(struct task_struct *p,
 	 * possibly take page faults for user memory.
 	 */
 	get_task_struct(p);
-	pid = task_pid_nr_ns(p, current->nsproxy->pid_ns);
+	pid = task_pid_vnr(p);
 	why = (p->ptrace & PT_PTRACED) ? CLD_TRAPPED : CLD_STOPPED;
 	read_unlock(&tasklist_lock);
 
@@ -1428,7 +1428,7 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 		p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
 	spin_unlock_irq(&p->sighand->siglock);
 
-	pid = task_pid_nr_ns(p, current->nsproxy->pid_ns);
+	pid = task_pid_vnr(p);
 	uid = p->uid;
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);

commit 161550d74c07303ffa6187ba776f62df5a906a21
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Feb 8 04:19:14 2008 -0800

    pid: sys_wait... fixes
    
    This modifies do_wait and eligible child to take a pair of enum pid_type
    and struct pid *pid to precisely specify what set of processes are eligible
    to be waited for, instead of the raw pid_t value from sys_wait4.
    
    This fixes a bug in sys_waitid where you could not wait for children in
    just process group 1.
    
    This fixes a pid namespace crossing case in eligible_child.  Allowing us to
    wait for a processes in our current process group even if our current
    process group == 0.
    
    This allows the no child with this pid case to be optimized.  This allows
    us to optimize the pid membership test in eligible child to be optimized.
    
    This even closes a theoretical pid wraparound race where in a threaded
    parent if two threads are waiting for the same child and one thread picks
    up the child and the pid numbers wrap around and generate another child
    with that same pid before the other thread is scheduled (teribly insanely
    unlikely) we could end up waiting on the second child with the same pid#
    and not discover that the specific child we were waiting for has exited.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2b332d170327..2567de3487bd 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1090,20 +1090,23 @@ asmlinkage void sys_exit_group(int error_code)
 	do_group_exit((error_code & 0xff) << 8);
 }
 
-static int eligible_child(pid_t pid, int options, struct task_struct *p)
+static struct pid *task_pid_type(struct task_struct *task, enum pid_type type)
+{
+	struct pid *pid = NULL;
+	if (type == PIDTYPE_PID)
+		pid = task->pids[type].pid;
+	else if (type < PIDTYPE_MAX)
+		pid = task->group_leader->pids[type].pid;
+	return pid;
+}
+
+static int eligible_child(enum pid_type type, struct pid *pid, int options,
+			  struct task_struct *p)
 {
 	int err;
-	struct pid_namespace *ns;
 
-	ns = current->nsproxy->pid_ns;
-	if (pid > 0) {
-		if (task_pid_nr_ns(p, ns) != pid)
-			return 0;
-	} else if (!pid) {
-		if (task_pgrp_nr_ns(p, ns) != task_pgrp_vnr(current))
-			return 0;
-	} else if (pid != -1) {
-		if (task_pgrp_nr_ns(p, ns) != -pid)
+	if (type < PIDTYPE_MAX) {
+		if (task_pid_type(p, type) != pid)
 			return 0;
 	}
 
@@ -1127,7 +1130,7 @@ static int eligible_child(pid_t pid, int options, struct task_struct *p)
 	if (likely(!err))
 		return 1;
 
-	if (pid <= 0)
+	if (type != PIDTYPE_PID)
 		return 0;
 	/* This child was explicitly requested, abort */
 	read_unlock(&tasklist_lock);
@@ -1447,8 +1450,9 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 	return retval;
 }
 
-static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
-		    int __user *stat_addr, struct rusage __user *ru)
+static long do_wait(enum pid_type type, struct pid *pid, int options,
+		    struct siginfo __user *infop, int __user *stat_addr,
+		    struct rusage __user *ru)
 {
 	DECLARE_WAITQUEUE(wait, current);
 	struct task_struct *tsk;
@@ -1456,6 +1460,11 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 
 	add_wait_queue(&current->signal->wait_chldexit,&wait);
 repeat:
+	/* If there is nothing that can match our critier just get out */
+	retval = -ECHILD;
+	if ((type < PIDTYPE_MAX) && (!pid || hlist_empty(&pid->tasks[type])))
+		goto end;
+
 	/*
 	 * We will set this flag if we see any child that might later
 	 * match our criteria, even if we are not able to reap it yet.
@@ -1468,7 +1477,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 		struct task_struct *p;
 
 		list_for_each_entry(p, &tsk->children, sibling) {
-			int ret = eligible_child(pid, options, p);
+			int ret = eligible_child(type, pid, options, p);
 			if (!ret)
 				continue;
 
@@ -1515,7 +1524,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 		if (!flag) {
 			list_for_each_entry(p, &tsk->ptrace_children,
 								ptrace_list) {
-				flag = eligible_child(pid, options, p);
+				flag = eligible_child(type, pid, options, p);
 				if (!flag)
 					continue;
 				if (likely(flag > 0))
@@ -1570,10 +1579,12 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 	return retval;
 }
 
-asmlinkage long sys_waitid(int which, pid_t pid,
+asmlinkage long sys_waitid(int which, pid_t upid,
 			   struct siginfo __user *infop, int options,
 			   struct rusage __user *ru)
 {
+	struct pid *pid = NULL;
+	enum pid_type type;
 	long ret;
 
 	if (options & ~(WNOHANG|WNOWAIT|WEXITED|WSTOPPED|WCONTINUED))
@@ -1583,37 +1594,58 @@ asmlinkage long sys_waitid(int which, pid_t pid,
 
 	switch (which) {
 	case P_ALL:
-		pid = -1;
+		type = PIDTYPE_MAX;
 		break;
 	case P_PID:
-		if (pid <= 0)
+		type = PIDTYPE_PID;
+		if (upid <= 0)
 			return -EINVAL;
 		break;
 	case P_PGID:
-		if (pid <= 0)
+		type = PIDTYPE_PGID;
+		if (upid <= 0)
 			return -EINVAL;
-		pid = -pid;
 		break;
 	default:
 		return -EINVAL;
 	}
 
-	ret = do_wait(pid, options, infop, NULL, ru);
+	if (type < PIDTYPE_MAX)
+		pid = find_get_pid(upid);
+	ret = do_wait(type, pid, options, infop, NULL, ru);
+	put_pid(pid);
 
 	/* avoid REGPARM breakage on x86: */
 	prevent_tail_call(ret);
 	return ret;
 }
 
-asmlinkage long sys_wait4(pid_t pid, int __user *stat_addr,
+asmlinkage long sys_wait4(pid_t upid, int __user *stat_addr,
 			  int options, struct rusage __user *ru)
 {
+	struct pid *pid = NULL;
+	enum pid_type type;
 	long ret;
 
 	if (options & ~(WNOHANG|WUNTRACED|WCONTINUED|
 			__WNOTHREAD|__WCLONE|__WALL))
 		return -EINVAL;
-	ret = do_wait(pid, options | WEXITED, NULL, stat_addr, ru);
+
+	if (upid == -1)
+		type = PIDTYPE_MAX;
+	else if (upid < 0) {
+		type = PIDTYPE_PGID;
+		pid = find_get_pid(-upid);
+	} else if (upid == 0) {
+		type = PIDTYPE_PGID;
+		pid = get_pid(task_pgrp(current));
+	} else /* upid > 0 */ {
+		type = PIDTYPE_PID;
+		pid = find_get_pid(upid);
+	}
+
+	ret = do_wait(type, pid, options | WEXITED, NULL, stat_addr, ru);
+	put_pid(pid);
 
 	/* avoid REGPARM breakage on x86: */
 	prevent_tail_call(ret);

commit 5dee1707dfbfc55eb7569b9ae5abaf932bd4c377
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:13 2008 -0800

    move the related code from exit_notify() to exit_signals()
    
    The previous bugfix was not optimal, we shouldn't care about group stop
    when we are the only thread or the group stop is in progress.  In that case
    nothing special is needed, just set PF_EXITING and return.
    
    Also, take the related "TIF_SIGPENDING re-targeting" code from exit_notify().
    
    So, from the performance POV the only difference is that we don't trust
    !signal_pending() until we take ->siglock.  But this in fact fixes another
    ___pure___ theoretical minor race.  __group_complete_signal() finds the
    task without PF_EXITING and chooses it as the target for signal_wake_up().
    But nothing prevents this task from exiting in between without noticing the
    pending signal and thus unpredictably delaying the actual delivery.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Davide Libenzi <davidel@xmailserver.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8f3bf53a5b4d..2b332d170327 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -745,24 +745,6 @@ static void exit_notify(struct task_struct *tsk)
 	struct task_struct *t;
 	struct pid *pgrp;
 
-	if (signal_pending(tsk) && !(tsk->signal->flags & SIGNAL_GROUP_EXIT)
-	    && !thread_group_empty(tsk)) {
-		/*
-		 * This occurs when there was a race between our exit
-		 * syscall and a group signal choosing us as the one to
-		 * wake up.  It could be that we are the only thread
-		 * alerted to check for pending signals, but another thread
-		 * should be woken now to take the signal since we will not.
-		 * Now we'll wake all the threads in the group just to make
-		 * sure someone gets all the pending signals.
-		 */
-		spin_lock_irq(&tsk->sighand->siglock);
-		for (t = next_thread(tsk); t != tsk; t = next_thread(t))
-			if (!signal_pending(t) && !(t->flags & PF_EXITING))
-				recalc_sigpending_and_wake(t);
-		spin_unlock_irq(&tsk->sighand->siglock);
-	}
-
 	/*
 	 * This does two things:
 	 *

commit d12619b5ff5664623524aef796514d1946ea3b4a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:12 2008 -0800

    fix group stop with exit race
    
    do_signal_stop() counts all sub-thread and sets ->group_stop_count
    accordingly.  Every thread should decrement ->group_stop_count and stop,
    the last one should notify the parent.
    
    However a sub-thread can exit before it notices the signal_pending(), or it
    may be somewhere in do_exit() already.  In that case the group stop never
    finishes properly.
    
    Note: this is a minimal fix, we can add some optimizations later.  Say we
    can return quickly if thread_group_empty().  Also, we can move some signal
    related code from exit_notify() to exit_signals().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Davide Libenzi <davidel@xmailserver.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index d7815f570882..8f3bf53a5b4d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -947,7 +947,7 @@ fastcall NORET_TYPE void do_exit(long code)
 		schedule();
 	}
 
-	tsk->flags |= PF_EXITING;
+	exit_signals(tsk);  /* sets PF_EXITING */
 	/*
 	 * tsk->flags are checked in the futex code to protect against
 	 * an exiting task cleaning up the robust pi futexes.

commit 297bd42b15daed02453ff59ce6d31216a58b0398
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:10 2008 -0800

    move daemonized kernel threads into the swapper's session
    
    Daemonized kernel threads run in the init's session. This doesn't match the
    behaviour of kthread_create()'ed threads, and this is one of the 2 reasons
    why we need a special hack in sys_setsid().
    
    Now that set_special_pids() was changed to use struct pid, not pid_t, we can
    use init_struct_pid and set 0,0 special pids.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 96716fd22373..d7815f570882 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -388,7 +388,7 @@ void daemonize(const char *name, ...)
 		get_nsproxy(&init_nsproxy);
 		switch_task_namespaces(current, &init_nsproxy);
 	}
-	set_special_pids(find_pid(1));
+	set_special_pids(&init_struct_pid);
 	proc_clear_tty(current);
 
 	/* Block and flush all signals */

commit 8520d7c7f8611216e3b270becec95bb35b6899d4
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:09 2008 -0800

    teach set_special_pids() to use struct pid
    
    Change set_special_pids() to work with struct pid, not pid_t from global name
    space. This again speedups and imho cleanups the code, also a preparation for
    the next patch.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 42a8713b2050..96716fd22373 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -293,26 +293,27 @@ static void reparent_to_kthreadd(void)
 	switch_uid(INIT_USER);
 }
 
-void __set_special_pids(pid_t session, pid_t pgrp)
+void __set_special_pids(struct pid *pid)
 {
 	struct task_struct *curr = current->group_leader;
+	pid_t nr = pid_nr(pid);
 
-	if (task_session_nr(curr) != session) {
+	if (task_session(curr) != pid) {
 		detach_pid(curr, PIDTYPE_SID);
-		set_task_session(curr, session);
-		attach_pid(curr, PIDTYPE_SID, find_pid(session));
+		attach_pid(curr, PIDTYPE_SID, pid);
+		set_task_session(curr, nr);
 	}
-	if (task_pgrp_nr(curr) != pgrp) {
+	if (task_pgrp(curr) != pid) {
 		detach_pid(curr, PIDTYPE_PGID);
-		set_task_pgrp(curr, pgrp);
-		attach_pid(curr, PIDTYPE_PGID, find_pid(pgrp));
+		attach_pid(curr, PIDTYPE_PGID, pid);
+		set_task_pgrp(curr, nr);
 	}
 }
 
-static void set_special_pids(pid_t session, pid_t pgrp)
+static void set_special_pids(struct pid *pid)
 {
 	write_lock_irq(&tasklist_lock);
-	__set_special_pids(session, pgrp);
+	__set_special_pids(pid);
 	write_unlock_irq(&tasklist_lock);
 }
 
@@ -383,7 +384,11 @@ void daemonize(const char *name, ...)
 	 */
 	current->flags |= PF_NOFREEZE;
 
-	set_special_pids(1, 1);
+	if (current->nsproxy != &init_nsproxy) {
+		get_nsproxy(&init_nsproxy);
+		switch_task_namespaces(current, &init_nsproxy);
+	}
+	set_special_pids(find_pid(1));
 	proc_clear_tty(current);
 
 	/* Block and flush all signals */
@@ -398,11 +403,6 @@ void daemonize(const char *name, ...)
 	current->fs = fs;
 	atomic_inc(&fs->count);
 
-	if (current->nsproxy != init_task.nsproxy) {
-		get_nsproxy(init_task.nsproxy);
-		switch_task_namespaces(current, init_task.nsproxy);
-	}
-
 	exit_files(current);
 	current->files = init_task.files;
 	atomic_inc(&current->files->count);

commit c543f1ee08ea6c2176dbdc47df0d0f6357c88713
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:07 2008 -0800

    wait_task_zombie: remove ->exit_state/exit_signal checks for WNOWAIT
    
    The first "p->exit_state != EXIT_ZOMBIE" check doesn't make too much sense.
    The exit_state was EXIT_ZOMBIE when the function was called, and another
    thread can change it to EXIT_DEAD right after the check.
    
    The second condition is not possible, detached non-traced threads were already
    filtered out by eligible_child(), we didn't drop tasklist since then.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index dee8b4d63403..42a8713b2050 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1196,10 +1196,6 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 		int exit_code = p->exit_code;
 		int why, status;
 
-		if (unlikely(p->exit_state != EXIT_ZOMBIE))
-			return 0;
-		if (unlikely(p->exit_signal == -1 && p->ptrace == 0))
-			return 0;
 		get_task_struct(p);
 		read_unlock(&tasklist_lock);
 		if ((exit_code & 0x7f) == 0) {

commit 3a515e4a62dbf7e4c213740268a5267faa69e5b2
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:07 2008 -0800

    wait_task_continued/zombie: don't use task_pid_nr_ns() lockless
    
    Surprise, the other two wait_task_*() functions also abuse the
    task_pid_nr_ns() function, and may cause read-after-free or report nr == 0
    in wait_task_continued().  wait_task_zombie() doesn't have this problem,
    but it is still better to cache pid_t rather than call task_pid_nr_ns()
    three times on the saved pid_namespace.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ee607720ae58..dee8b4d63403 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1189,12 +1189,9 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 {
 	unsigned long state;
 	int retval, status, traced;
-	struct pid_namespace *ns;
-
-	ns = current->nsproxy->pid_ns;
+	pid_t pid = task_pid_nr_ns(p, current->nsproxy->pid_ns);
 
 	if (unlikely(noreap)) {
-		pid_t pid = task_pid_nr_ns(p, ns);
 		uid_t uid = p->uid;
 		int exit_code = p->exit_code;
 		int why, status;
@@ -1313,11 +1310,11 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 			retval = put_user(status, &infop->si_status);
 	}
 	if (!retval && infop)
-		retval = put_user(task_pid_nr_ns(p, ns), &infop->si_pid);
+		retval = put_user(pid, &infop->si_pid);
 	if (!retval && infop)
 		retval = put_user(p->uid, &infop->si_uid);
 	if (!retval)
-		retval = task_pid_nr_ns(p, ns);
+		retval = pid;
 
 	if (traced) {
 		write_lock_irq(&tasklist_lock);
@@ -1436,7 +1433,6 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 	int retval;
 	pid_t pid;
 	uid_t uid;
-	struct pid_namespace *ns;
 
 	if (!(p->signal->flags & SIGNAL_STOP_CONTINUED))
 		return 0;
@@ -1451,8 +1447,7 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 		p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
 	spin_unlock_irq(&p->sighand->siglock);
 
-	ns = current->nsproxy->pid_ns;
-	pid = task_pid_nr_ns(p, ns);
+	pid = task_pid_nr_ns(p, current->nsproxy->pid_ns);
 	uid = p->uid;
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);
@@ -1463,7 +1458,7 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 		if (!retval && stat_addr)
 			retval = put_user(0xffff, stat_addr);
 		if (!retval)
-			retval = task_pid_nr_ns(p, ns);
+			retval = pid;
 	} else {
 		retval = wait_noreap_copyout(p, pid, uid,
 					     CLD_CONTINUED, SIGCONT,

commit f2cc3eb133baa2e9dc8efd40f417106b2ee520f3
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:06 2008 -0800

    do_wait: fix security checks
    
    Imho, the current usage of security_task_wait() is not logical.
    
    Suppose we have the single child p, and security_task_wait(p) return
    -EANY.  In that case waitpid(-1) returns this error.  Why? Isn't it
    better to return ECHLD? We don't really have reapable children.
    
    Now suppose that child was stolen by gdb.  In that case we find this
    child on ->ptrace_children and set flag = 1, but we don't check that the
    child was denied.  So, do_wait(..., WNOHANG) returns 0, this doesn't
    match the behaviour above.  Without WNOHANG do_wait() blocks only to
    return the error later, when the child will be untraced.  Inho, really
    strange.
    
    I think eligible_child() should return the error only if the child's pid
    was requested explicitly, otherwise we should silently ignore the tasks
    which were nacked by security_task_wait().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9ee229ea97e4..ee607720ae58 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1142,10 +1142,14 @@ static int eligible_child(pid_t pid, int options, struct task_struct *p)
 		return 0;
 
 	err = security_task_wait(p);
-	if (err)
-		return err;
+	if (likely(!err))
+		return 1;
 
-	return 1;
+	if (pid <= 0)
+		return 0;
+	/* This child was explicitly requested, abort */
+	read_unlock(&tasklist_lock);
+	return err;
 }
 
 static int wait_noreap_copyout(struct task_struct *p, pid_t pid, uid_t uid,
@@ -1476,7 +1480,6 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 	DECLARE_WAITQUEUE(wait, current);
 	struct task_struct *tsk;
 	int flag, retval;
-	int allowed, denied;
 
 	add_wait_queue(&current->signal->wait_chldexit,&wait);
 repeat:
@@ -1484,8 +1487,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 	 * We will set this flag if we see any child that might later
 	 * match our criteria, even if we are not able to reap it yet.
 	 */
-	flag = 0;
-	allowed = denied = 0;
+	flag = retval = 0;
 	current->state = TASK_INTERRUPTIBLE;
 	read_lock(&tasklist_lock);
 	tsk = current;
@@ -1498,13 +1500,8 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 				continue;
 
 			if (unlikely(ret < 0)) {
-				denied = ret;
-				continue;
-			}
-			allowed = 1;
-
-			retval = 0;
-			if (task_is_stopped_or_traced(p)) {
+				retval = ret;
+			} else if (task_is_stopped_or_traced(p)) {
 				/*
 				 * It's stopped now, so it might later
 				 * continue, exit, or stop again.
@@ -1544,11 +1541,14 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 		}
 		if (!flag) {
 			list_for_each_entry(p, &tsk->ptrace_children,
-					    ptrace_list) {
-				if (!eligible_child(pid, options, p))
+								ptrace_list) {
+				flag = eligible_child(pid, options, p);
+				if (!flag)
 					continue;
-				flag = 1;
-				break;
+				if (likely(flag > 0))
+					break;
+				retval = flag;
+				goto end;
 			}
 		}
 		if (options & __WNOTHREAD)
@@ -1556,10 +1556,9 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 		tsk = next_thread(tsk);
 		BUG_ON(tsk->signal != current->signal);
 	} while (tsk != current);
-
 	read_unlock(&tasklist_lock);
+
 	if (flag) {
-		retval = 0;
 		if (options & WNOHANG)
 			goto end;
 		retval = -ERESTARTSYS;
@@ -1569,8 +1568,6 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 		goto repeat;
 	}
 	retval = -ECHILD;
-	if (unlikely(denied) && !allowed)
-		retval = denied;
 end:
 	current->state = TASK_RUNNING;
 	remove_wait_queue(&current->signal->wait_chldexit,&wait);

commit 96fabbf55ae79826f2e8a86f4066d7e8834315ae
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:04 2008 -0800

    do_wait: cleanup delay_group_leader() usage
    
    eligible_child() == 2 means delay_group_leader().  With the previous patch
    this only matters for EXIT_ZOMBIE task, we can move that special check to
    the only place it is really needed.
    
    Also, with this patch we don't skip security_task_wait() for the group
    leaders in a non-empty thread group.  I don't really understand the exact
    semantics of security_task_wait(), but imho this change is a bugfix.
    
    Also rearrange the code a bit to kill an ugly "check_continued" backdoor.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Eric Paris <eparis@redhat.com>
    Cc: James Morris <jmorris@namei.org>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 190a4cdcdb4d..9ee229ea97e4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1140,12 +1140,6 @@ static int eligible_child(pid_t pid, int options, struct task_struct *p)
 	if (((p->exit_signal != SIGCHLD) ^ ((options & __WCLONE) != 0))
 	    && !(options & __WALL))
 		return 0;
-	/*
-	 * Do not consider thread group leaders that are
-	 * in a non-empty thread group:
-	 */
-	if (delay_group_leader(p))
-		return 2;
 
 	err = security_task_wait(p);
 	if (err)
@@ -1497,10 +1491,9 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 	tsk = current;
 	do {
 		struct task_struct *p;
-		int ret;
 
 		list_for_each_entry(p, &tsk->children, sibling) {
-			ret = eligible_child(pid, options, p);
+			int ret = eligible_child(pid, options, p);
 			if (!ret)
 				continue;
 
@@ -1524,19 +1517,17 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 				retval = wait_task_stopped(p,
 						(options & WNOWAIT), infop,
 						stat_addr, ru);
-			} else if (p->exit_state == EXIT_ZOMBIE) {
+			} else if (p->exit_state == EXIT_ZOMBIE &&
+					!delay_group_leader(p)) {
 				/*
-				 * Eligible but we cannot release it yet:
+				 * We don't reap group leaders with subthreads.
 				 */
-				if (ret == 2)
-					goto check_continued;
 				if (!likely(options & WEXITED))
 					continue;
 				retval = wait_task_zombie(p,
 						(options & WNOWAIT), infop,
 						stat_addr, ru);
 			} else if (p->exit_state != EXIT_DEAD) {
-check_continued:
 				/*
 				 * It's running now, so it might later
 				 * exit, stop, or stop and then continue.

commit 1bad95c3bee183719e15eebffef66afc3fb3f8b0
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:03 2008 -0800

    wait_task_stopped(): remove unneeded delay_group_leader check
    
    wait_task_stopped() doesn't need the "delay_group_leader" parameter.  If
    the child is not traced it must be a group leader.  With or without
    subthreads ->group_stop_count == 0 when the whole task is stopped.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Mika Penttila <mika.penttila@kolumbus.fi>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 723a69b69fa1..190a4cdcdb4d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1351,7 +1351,7 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
+static int wait_task_stopped(struct task_struct *p,
 			     int noreap, struct siginfo __user *infop,
 			     int __user *stat_addr, struct rusage __user *ru)
 {
@@ -1365,8 +1365,7 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 	if (unlikely(!task_is_stopped_or_traced(p)))
 		goto unlock_sig;
 
-	if (delayed_group_leader && !(p->ptrace & PT_PTRACED) &&
-	    p->signal->group_stop_count > 0)
+	if (!(p->ptrace & PT_PTRACED) && p->signal->group_stop_count > 0)
 		/*
 		 * A group stop is in progress and this is the group leader.
 		 * We won't report until all threads have stopped.
@@ -1522,7 +1521,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 				    !(options & WUNTRACED))
 					continue;
 
-				retval = wait_task_stopped(p, ret == 2,
+				retval = wait_task_stopped(p,
 						(options & WNOWAIT), infop,
 						stat_addr, ru);
 			} else if (p->exit_state == EXIT_ZOMBIE) {

commit 9cbab8100538efdd93aeae6fc37787d986f2f558
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:02 2008 -0800

    do_wait: factor out "retval != 0" checks
    
    Every branch if the main "if" statement does the same code at the end.  Move
    it down.  Also, fix the indentation.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index da293ac7e379..723a69b69fa1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1511,6 +1511,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 			}
 			allowed = 1;
 
+			retval = 0;
 			if (task_is_stopped_or_traced(p)) {
 				/*
 				 * It's stopped now, so it might later
@@ -1524,8 +1525,6 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 				retval = wait_task_stopped(p, ret == 2,
 						(options & WNOWAIT), infop,
 						stat_addr, ru);
-				if (retval != 0) /* He released the lock.  */
-					goto end;
 			} else if (p->exit_state == EXIT_ZOMBIE) {
 				/*
 				 * Eligible but we cannot release it yet:
@@ -1537,9 +1536,6 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 				retval = wait_task_zombie(p,
 						(options & WNOWAIT), infop,
 						stat_addr, ru);
-				/* He released the lock.  */
-				if (retval != 0)
-					goto end;
 			} else if (p->exit_state != EXIT_DEAD) {
 check_continued:
 				/*
@@ -1552,9 +1548,9 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 				retval = wait_task_continued(p,
 						(options & WNOWAIT), infop,
 						stat_addr, ru);
-				if (retval != 0) /* He released the lock.  */
-					goto end;
 			}
+			if (retval != 0) /* tasklist_lock released */
+				goto end;
 		}
 		if (!flag) {
 			list_for_each_entry(p, &tsk->ptrace_children,
@@ -1590,7 +1586,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 	remove_wait_queue(&current->signal->wait_chldexit,&wait);
 	if (infop) {
 		if (retval > 0)
-		retval = 0;
+			retval = 0;
 		else {
 			/*
 			 * For a WNOHANG return, clear out all the fields

commit ee7c82da830ea860b1f9274f1f0cdf99f206e7c2
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:19:01 2008 -0800

    wait_task_stopped: simplify and fix races with SIGCONT/SIGKILL/untrace
    
    wait_task_stopped() has multiple races with SIGCONT/SIGKILL.  tasklist_lock
    does not pin the child in TASK_TRACED/TASK_STOPPED stated, almost all info
    reported (including exit_code) may be wrong.
    
    In fact, the code under write_lock_irq(tasklist_lock) is not safe.  The child
    may be PTRACE_DETACH'ed at this time by another subthread, in that case it is
    possible we are no longer its ->parent.
    
    Change wait_task_stopped() to take ->siglock before inspecting the task.  This
    guarantees that the child can't resume and (for example) clear its
    ->exit_code, so we don't need to use xchg(&p->exit_code) and re-check.  The
    only exception is ptrace_stop() which changes ->state and ->exit_code without
    ->siglock held during abort.  But this can only happen if both the tracer and
    the tracee are dying (coredump is in progress), we don't care.
    
    With this patch wait_task_stopped() doesn't move the child to the end of
    the ->parent list on success.  This optimization could be restored, but
    in that case we have to take write_lock(tasklist) and do some nasty
    checks.
    
    Also change the do_wait() since we don't return EAGAIN any longer.
    
    [akpm@linux-foundation.org: fix up after Willy renamed everything]
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b5ff2b121093..da293ac7e379 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1355,17 +1355,35 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 			     int noreap, struct siginfo __user *infop,
 			     int __user *stat_addr, struct rusage __user *ru)
 {
-	int retval, exit_code;
+	int retval, exit_code, why;
+	uid_t uid = 0; /* unneeded, required by compiler */
 	pid_t pid;
 
-	if (!p->exit_code)
-		return 0;
+	exit_code = 0;
+	spin_lock_irq(&p->sighand->siglock);
+
+	if (unlikely(!task_is_stopped_or_traced(p)))
+		goto unlock_sig;
+
 	if (delayed_group_leader && !(p->ptrace & PT_PTRACED) &&
 	    p->signal->group_stop_count > 0)
 		/*
 		 * A group stop is in progress and this is the group leader.
 		 * We won't report until all threads have stopped.
 		 */
+		goto unlock_sig;
+
+	exit_code = p->exit_code;
+	if (!exit_code)
+		goto unlock_sig;
+
+	if (!noreap)
+		p->exit_code = 0;
+
+	uid = p->uid;
+unlock_sig:
+	spin_unlock_irq(&p->sighand->siglock);
+	if (!exit_code)
 		return 0;
 
 	/*
@@ -1375,65 +1393,15 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 	 * keep holding onto the tasklist_lock while we call getrusage and
 	 * possibly take page faults for user memory.
 	 */
-	pid = task_pid_nr_ns(p, current->nsproxy->pid_ns);
 	get_task_struct(p);
+	pid = task_pid_nr_ns(p, current->nsproxy->pid_ns);
+	why = (p->ptrace & PT_PTRACED) ? CLD_TRAPPED : CLD_STOPPED;
 	read_unlock(&tasklist_lock);
 
-	if (unlikely(noreap)) {
-		uid_t uid = p->uid;
-		int why = (p->ptrace & PT_PTRACED) ? CLD_TRAPPED : CLD_STOPPED;
-
-		exit_code = p->exit_code;
-		if (unlikely(!exit_code) || unlikely(p->exit_state))
-			goto bail_ref;
+	if (unlikely(noreap))
 		return wait_noreap_copyout(p, pid, uid,
 					   why, exit_code,
 					   infop, ru);
-	}
-
-	write_lock_irq(&tasklist_lock);
-
-	/*
-	 * This uses xchg to be atomic with the thread resuming and setting
-	 * it.  It must also be done with the write lock held to prevent a
-	 * race with the EXIT_ZOMBIE case.
-	 */
-	exit_code = xchg(&p->exit_code, 0);
-	if (unlikely(p->exit_state)) {
-		/*
-		 * The task resumed and then died.  Let the next iteration
-		 * catch it in EXIT_ZOMBIE.  Note that exit_code might
-		 * already be zero here if it resumed and did _exit(0).
-		 * The task itself is dead and won't touch exit_code again;
-		 * other processors in this function are locked out.
-		 */
-		p->exit_code = exit_code;
-		exit_code = 0;
-	}
-	if (unlikely(exit_code == 0)) {
-		/*
-		 * Another thread in this function got to it first, or it
-		 * resumed, or it resumed and then died.
-		 */
-		write_unlock_irq(&tasklist_lock);
-bail_ref:
-		put_task_struct(p);
-		/*
-		 * We are returning to the wait loop without having successfully
-		 * removed the process and having released the lock. We cannot
-		 * continue, since the "p" task pointer is potentially stale.
-		 *
-		 * Return -EAGAIN, and do_wait() will restart the loop from the
-		 * beginning. Do _not_ re-acquire the lock.
-		 */
-		return -EAGAIN;
-	}
-
-	/* move to end of parent's list to avoid starvation */
-	remove_parent(p);
-	add_parent(p);
-
-	write_unlock_irq(&tasklist_lock);
 
 	retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
 	if (!retval && stat_addr)
@@ -1443,15 +1411,13 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 	if (!retval && infop)
 		retval = put_user(0, &infop->si_errno);
 	if (!retval && infop)
-		retval = put_user((short)((p->ptrace & PT_PTRACED)
-					  ? CLD_TRAPPED : CLD_STOPPED),
-				  &infop->si_code);
+		retval = put_user(why, &infop->si_code);
 	if (!retval && infop)
 		retval = put_user(exit_code, &infop->si_status);
 	if (!retval && infop)
 		retval = put_user(pid, &infop->si_pid);
 	if (!retval && infop)
-		retval = put_user(p->uid, &infop->si_uid);
+		retval = put_user(uid, &infop->si_uid);
 	if (!retval)
 		retval = pid;
 	put_task_struct(p);
@@ -1558,8 +1524,6 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 				retval = wait_task_stopped(p, ret == 2,
 						(options & WNOWAIT), infop,
 						stat_addr, ru);
-				if (retval == -EAGAIN)
-					goto repeat;
 				if (retval != 0) /* He released the lock.  */
 					goto end;
 			} else if (p->exit_state == EXIT_ZOMBIE) {

commit 34a1738f7da0b3d28d4b066d03a78f46b8cab68f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:18:59 2008 -0800

    kill my_ptrace_child()
    
    Now that my_ptrace_child() is trivial we can use the "p->ptrace & PT_PTRACED"
    inline and simplify the corresponding logic in do_wait: we can't find the
    child in TASK_TRACED state without PT_PTRACED flag set, ptrace_untrace()
    either sets TASK_STOPPED or wakes up the tracee.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1f2c15297f2d..b5ff2b121093 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1511,12 +1511,6 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 	return retval;
 }
 
-
-static inline int my_ptrace_child(struct task_struct *p)
-{
-	return p->ptrace & PT_PTRACED;
-}
-
 static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 		    int __user *stat_addr, struct rusage __user *ru)
 {
@@ -1555,22 +1549,11 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 				/*
 				 * It's stopped now, so it might later
 				 * continue, exit, or stop again.
-				 *
-				 * When we hit the race with PTRACE_ATTACH, we
-				 * will not report this child.  But the race
-				 * means it has not yet been moved to our
-				 * ptrace_children list, so we need to set the
-				 * flag here to avoid a spurious ECHILD when
-				 * the race happens with the only child.
 				 */
 				flag = 1;
-
-				if (!my_ptrace_child(p)) {
-					if (task_is_traced(p))
-						continue;
-					if (!(options & WUNTRACED))
-						continue;
-				}
+				if (!(p->ptrace & PT_PTRACED) &&
+				    !(options & WUNTRACED))
+					continue;
 
 				retval = wait_task_stopped(p, ret == 2,
 						(options & WNOWAIT), infop,

commit 6b39c7bfbd1436836c0fb34c5b437fda1a7a3dd4
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Feb 8 04:18:58 2008 -0800

    kill PT_ATTACHED
    
    Since the patch
    
            "Fix ptrace_attach()/ptrace_traceme()/de_thread() race"
            commit f5b40e363ad6041a96e3da32281d8faa191597b9
    
    we set PT_ATTACHED and change child->parent "atomically" wrt task_list lock.
    
    This means we can remove the checks like "PT_ATTACHED && ->parent != ptracer"
    which were needed to catch the "ptrace attach is in progress" case.  We can
    also remove the flag itself since nobody else uses it.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Roland McGrath <roland@redhat.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index eb9934a82fc1..1f2c15297f2d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1514,18 +1514,7 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 
 static inline int my_ptrace_child(struct task_struct *p)
 {
-	if (!(p->ptrace & PT_PTRACED))
-		return 0;
-	if (!(p->ptrace & PT_ATTACHED))
-		return 1;
-	/*
-	 * This child was PTRACE_ATTACH'd.  We should be seeing it only if
-	 * we are the attacher.  If we are the real parent, this is a race
-	 * inside ptrace_attach.  It is waiting for the tasklist_lock,
-	 * which we have to switch the parent links, but has already set
-	 * the flags in p->ptrace.
-	 */
-	return (p->parent != p->real_parent);
+	return p->ptrace & PT_PTRACED;
 }
 
 static long do_wait(pid_t pid, int options, struct siginfo __user *infop,

commit 0a76fe8e50ee93a9d4a1badb1ec995852a6bcaf1
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Feb 6 01:37:06 2008 -0800

    do_wait: remove one "else if" branch
    
    Minor cleanup. We can remove one "else if" branch.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9d3d0f0b27d9..eb9934a82fc1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1590,8 +1590,6 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 					goto repeat;
 				if (retval != 0) /* He released the lock.  */
 					goto end;
-			} else if (p->exit_state == EXIT_DEAD) {
-				continue;
 			} else if (p->exit_state == EXIT_ZOMBIE) {
 				/*
 				 * Eligible but we cannot release it yet:
@@ -1606,7 +1604,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 				/* He released the lock.  */
 				if (retval != 0)
 					goto end;
-			} else {
+			} else if (p->exit_state != EXIT_DEAD) {
 check_continued:
 				/*
 				 * It's running now, so it might later

commit ed5d2cac114202fe2978a9cbcab8f5032796d538
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Mon Feb 4 22:27:24 2008 -0800

    exec: rework the group exit and fix the race with kill
    
    As Roland pointed out, we have the very old problem with exec.  de_thread()
    sets SIGNAL_GROUP_EXIT, kills other threads, changes ->group_leader and then
    clears signal->flags.  All signals (even fatal ones) sent in this window
    (which is not too small) will be lost.
    
    With this patch exec doesn't abuse SIGNAL_GROUP_EXIT.  signal_group_exit(),
    the new helper, should be used to detect exit_group() or exec() in progress.
    It can have more users, but this patch does only strictly necessary changes.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Davide Libenzi <davidel@xmailserver.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Robin Holt <holt@sgi.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9e459fefda77..9d3d0f0b27d9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1083,11 +1083,12 @@ do_group_exit(int exit_code)
 		struct signal_struct *const sig = current->signal;
 		struct sighand_struct *const sighand = current->sighand;
 		spin_lock_irq(&sighand->siglock);
-		if (sig->flags & SIGNAL_GROUP_EXIT)
+		if (signal_group_exit(sig))
 			/* Another thread got here before we took the lock.  */
 			exit_code = sig->group_exit_code;
 		else {
 			sig->group_exit_code = exit_code;
+			sig->flags = SIGNAL_GROUP_EXIT;
 			zap_other_threads(current);
 		}
 		spin_unlock_irq(&sighand->siglock);

commit 06c93e875747f3020d997220b3e7c98083acc7c3
Author: Pierre Peiffer <pierre.peiffer@bull.net>
Date:   Sun Feb 3 16:22:12 2008 +0200

    Remove one useless extern declaration
    
    The file exit.c contains one useless extern declaration of sem_exit().
    Moreover, it refers to nothing.
    
    This trivial patch removes it.
    
    Signed-off-by: Pierre Peiffer <pierre.peiffer@bull.net>
    Signed-off-by: Adrian Bunk <bunk@kernel.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index bfb1c0e940e8..9e459fefda77 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -50,8 +50,6 @@
 #include <asm/pgtable.h>
 #include <asm/mmu_context.h>
 
-extern void sem_exit (void);
-
 static void exit_mm(struct task_struct * tsk);
 
 static void __unhash_process(struct task_struct *p)

commit 338077e54e17e656da470a84724b773816207316
Author: Matthew Wilcox <matthew@wil.cx>
Date:   Thu Dec 6 11:09:35 2007 -0500

    exit: Use task_is_*
    
    Also restructure the loop in do_wait()
    
    Signed-off-by: Matthew Wilcox <willy@linux.intel.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 549c0558ba68..bfb1c0e940e8 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -249,7 +249,7 @@ static int has_stopped_jobs(struct pid *pgrp)
 	struct task_struct *p;
 
 	do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
-		if (p->state != TASK_STOPPED)
+		if (!task_is_stopped(p))
 			continue;
 		retval = 1;
 		break;
@@ -614,7 +614,7 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 		p->parent = p->real_parent;
 		add_parent(p);
 
-		if (p->state == TASK_TRACED) {
+		if (task_is_traced(p)) {
 			/*
 			 * If it was at a trace stop, turn it into
 			 * a normal stop since it's no longer being
@@ -1563,60 +1563,51 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 			}
 			allowed = 1;
 
-			switch (p->state) {
-			case TASK_TRACED:
-				/*
-				 * When we hit the race with PTRACE_ATTACH,
-				 * we will not report this child.  But the
-				 * race means it has not yet been moved to
-				 * our ptrace_children list, so we need to
-				 * set the flag here to avoid a spurious ECHILD
-				 * when the race happens with the only child.
-				 */
-				flag = 1;
-				if (!my_ptrace_child(p))
-					continue;
-				/*FALLTHROUGH*/
-			case TASK_STOPPED:
+			if (task_is_stopped_or_traced(p)) {
 				/*
 				 * It's stopped now, so it might later
 				 * continue, exit, or stop again.
+				 *
+				 * When we hit the race with PTRACE_ATTACH, we
+				 * will not report this child.  But the race
+				 * means it has not yet been moved to our
+				 * ptrace_children list, so we need to set the
+				 * flag here to avoid a spurious ECHILD when
+				 * the race happens with the only child.
 				 */
 				flag = 1;
-				if (!(options & WUNTRACED) &&
-				    !my_ptrace_child(p))
-					continue;
+
+				if (!my_ptrace_child(p)) {
+					if (task_is_traced(p))
+						continue;
+					if (!(options & WUNTRACED))
+						continue;
+				}
+
 				retval = wait_task_stopped(p, ret == 2,
-							   (options & WNOWAIT),
-							   infop,
-							   stat_addr, ru);
+						(options & WNOWAIT), infop,
+						stat_addr, ru);
 				if (retval == -EAGAIN)
 					goto repeat;
 				if (retval != 0) /* He released the lock.  */
 					goto end;
-				break;
-			default:
-			// case EXIT_DEAD:
-				if (p->exit_state == EXIT_DEAD)
+			} else if (p->exit_state == EXIT_DEAD) {
+				continue;
+			} else if (p->exit_state == EXIT_ZOMBIE) {
+				/*
+				 * Eligible but we cannot release it yet:
+				 */
+				if (ret == 2)
+					goto check_continued;
+				if (!likely(options & WEXITED))
 					continue;
-			// case EXIT_ZOMBIE:
-				if (p->exit_state == EXIT_ZOMBIE) {
-					/*
-					 * Eligible but we cannot release
-					 * it yet:
-					 */
-					if (ret == 2)
-						goto check_continued;
-					if (!likely(options & WEXITED))
-						continue;
-					retval = wait_task_zombie(
-						p, (options & WNOWAIT),
-						infop, stat_addr, ru);
-					/* He released the lock.  */
-					if (retval != 0)
-						goto end;
-					break;
-				}
+				retval = wait_task_zombie(p,
+						(options & WNOWAIT), infop,
+						stat_addr, ru);
+				/* He released the lock.  */
+				if (retval != 0)
+					goto end;
+			} else {
 check_continued:
 				/*
 				 * It's running now, so it might later
@@ -1625,12 +1616,11 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 				flag = 1;
 				if (!unlikely(options & WCONTINUED))
 					continue;
-				retval = wait_task_continued(
-					p, (options & WNOWAIT),
-					infop, stat_addr, ru);
+				retval = wait_task_continued(p,
+						(options & WNOWAIT), infop,
+						stat_addr, ru);
 				if (retval != 0) /* He released the lock.  */
 					goto end;
-				break;
 			}
 		}
 		if (!flag) {

commit e6ceb32aa25fc33f21af84cc7a32fe289b3e860c
Author: Scott James Remnant <scott@ubuntu.com>
Date:   Wed Nov 28 16:22:07 2007 -0800

    wait_task_stopped(): pass correct exit_code to wait_noreap_copyout()
    
    In wait_task_stopped() exit_code already contains the right value for the
    si_status member of siginfo, and this is simply set in the non WNOWAIT
    case.
    
    If you call waitid() with a stopped or traced process, you'll get the signal
    in siginfo.si_status as expected -- however if you call waitid(WNOWAIT) at the
    same time, you'll get the signal << 8 | 0x7f
    
    Pass it unchanged to wait_noreap_copyout(); we would only need to shift it
    and add 0x7f if we were returning it in the user status field and that
    isn't used for any function that permits WNOWAIT.
    
    Signed-off-by: Scott James Remnant <scott@ubuntu.com>
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 0a4a382ecf23..549c0558ba68 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1388,7 +1388,7 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 		if (unlikely(!exit_code) || unlikely(p->exit_state))
 			goto bail_ref;
 		return wait_noreap_copyout(p, pid, uid,
-					   why, (exit_code << 8) | 0x7f,
+					   why, exit_code,
 					   infop, ru);
 	}
 

commit c895078355b6b6e05c60aa205892526dd3390f0a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Nov 28 16:21:24 2007 -0800

    wait_task_stopped(): don't use task_pid_nr_ns() lockless
    
    wait_task_stopped(WNOWAIT) does task_pid_nr_ns() without tasklist/rcu lock,
    we can read an already freed memory.  Use the cached pid_t value.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Looks-good-to: Roland McGrath <roland@redhat.com>
    Acked-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index cd0f1d4137a7..0a4a382ecf23 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1357,7 +1357,7 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 			     int __user *stat_addr, struct rusage __user *ru)
 {
 	int retval, exit_code;
-	struct pid_namespace *ns;
+	pid_t pid;
 
 	if (!p->exit_code)
 		return 0;
@@ -1376,12 +1376,11 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 	 * keep holding onto the tasklist_lock while we call getrusage and
 	 * possibly take page faults for user memory.
 	 */
-	ns = current->nsproxy->pid_ns;
+	pid = task_pid_nr_ns(p, current->nsproxy->pid_ns);
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);
 
 	if (unlikely(noreap)) {
-		pid_t pid = task_pid_nr_ns(p, ns);
 		uid_t uid = p->uid;
 		int why = (p->ptrace & PT_PTRACED) ? CLD_TRAPPED : CLD_STOPPED;
 
@@ -1451,11 +1450,11 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 	if (!retval && infop)
 		retval = put_user(exit_code, &infop->si_status);
 	if (!retval && infop)
-		retval = put_user(task_pid_nr_ns(p, ns), &infop->si_pid);
+		retval = put_user(pid, &infop->si_pid);
 	if (!retval && infop)
 		retval = put_user(p->uid, &infop->si_uid);
 	if (!retval)
-		retval = task_pid_nr_ns(p, ns);
+		retval = pid;
 	put_task_struct(p);
 
 	BUG_ON(!retval);

commit a3474224e6a01924be40a8255636ea5522c1023a
Author: Roland McGrath <roland@redhat.com>
Date:   Tue Nov 13 22:11:50 2007 -0800

    wait_task_stopped: Check p->exit_state instead of TASK_TRACED
    
    The original meaning of the old test (p->state > TASK_STOPPED) was
    "not dead", since it was before TASK_TRACED existed and before the
    state/exit_state split.  It was a wrong correction in commit
    14bf01bb0599c89fc7f426d20353b76e12555308 to make this test for
    TASK_TRACED instead.  It should have been changed when TASK_TRACED
    was introducted and again when exit_state was introduced.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Alexey Dobriyan <adobriyan@sw.ru>
    Cc: Kees Cook <kees@ubuntu.com>
    Acked-by: Scott James Remnant <scott@ubuntu.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f1aec27f1df0..cd0f1d4137a7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1386,8 +1386,7 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 		int why = (p->ptrace & PT_PTRACED) ? CLD_TRAPPED : CLD_STOPPED;
 
 		exit_code = p->exit_code;
-		if (unlikely(!exit_code) ||
-		    unlikely(p->state & TASK_TRACED))
+		if (unlikely(!exit_code) || unlikely(p->exit_state))
 			goto bail_ref;
 		return wait_noreap_copyout(p, pid, uid,
 					   why, (exit_code << 8) | 0x7f,

commit a39bc51691a0c8880b7d10fa7c2f034f3ba9a037
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Thu Oct 18 23:41:10 2007 -0700

    Uninline fork.c/exit.c
    
    Save ~650 bytes here.
    
    add/remove: 4/0 grow/shrink: 0/7 up/down: 430/-1088 (-658)
    function                                     old     new   delta
    __copy_fs_struct                               -     202    +202
    __put_fs_struct                                -     112    +112
    __exit_fs                                      -      58     +58
    __exit_files                                   -      58     +58
    exit_files                                    58       2     -56
    put_fs_struct                                112       5    -107
    exit_fs                                      161       2    -159
    sys_unshare                                  774     590    -184
    copy_process                                4031    3840    -191
    do_exit                                     1791    1597    -194
    copy_fs_struct                               202       5    -197
    
    No difference in lmbench lat_proc tests on 2-way Opteron 246.
    Smaaaal degradation on UP P4 (within errors).
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7dab2defec63..f1aec27f1df0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -493,7 +493,7 @@ void reset_files_struct(struct task_struct *tsk, struct files_struct *files)
 }
 EXPORT_SYMBOL(reset_files_struct);
 
-static inline void __exit_files(struct task_struct *tsk)
+static void __exit_files(struct task_struct *tsk)
 {
 	struct files_struct * files = tsk->files;
 
@@ -510,7 +510,7 @@ void exit_files(struct task_struct *tsk)
 	__exit_files(tsk);
 }
 
-static inline void __put_fs_struct(struct fs_struct *fs)
+static void __put_fs_struct(struct fs_struct *fs)
 {
 	/* No need to hold fs->lock if we are killing it */
 	if (atomic_dec_and_test(&fs->count)) {
@@ -531,7 +531,7 @@ void put_fs_struct(struct fs_struct *fs)
 	__put_fs_struct(fs);
 }
 
-static inline void __exit_fs(struct task_struct *tsk)
+static void __exit_fs(struct task_struct *tsk)
 {
 	struct fs_struct * fs = tsk->fs;
 

commit ba25f9dcc4ea6e30839fcab5a5516f2176d5bfed
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:40 2007 -0700

    Use helpers to obtain task pid in printks
    
    The task_struct->pid member is going to be deprecated, so start
    using the helpers (task_pid_nr/task_pid_vnr/task_pid_nr_ns) in
    the kernel.
    
    The first thing to start with is the pid, printed to dmesg - in
    this case we may safely use task_pid_nr(). Besides, printks produce
    more (much more) than a half of all the explicit pid usage.
    
    [akpm@linux-foundation.org: git-drm went and changed lots of stuff]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Dave Airlie <airlied@linux.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6838d4d77e05..7dab2defec63 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -959,7 +959,7 @@ fastcall NORET_TYPE void do_exit(long code)
 
 	if (unlikely(in_atomic()))
 		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
-				current->comm, current->pid,
+				current->comm, task_pid_nr(current),
 				preempt_count());
 
 	acct_update_integrals(tsk);

commit 9a2e70572e94e21e7ec4186702d045415422bda0
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:39 2007 -0700

    Isolate the explicit usage of signal->pgrp
    
    The pgrp field is not used widely around the kernel so it is now marked as
    deprecated with appropriate comment.
    
    The initialization of INIT_SIGNALS is trimmed because
    a) they are set to 0 automatically;
    b) gcc cannot properly initialize two anonymous (the second one
       is the one with the session) unions. In this particular case
       to make it compile we'd have to add some field initialized
       right before the .pgrp.
    
    This is the same patch as the 1ec320afdc9552c92191d5f89fcd1ebe588334ca one
    (from Cedric), but for the pgrp field.
    
    Some progress report:
    
    We have to deprecate the pid, tgid, session and pgrp fields on struct
    task_struct and struct signal_struct.  The session and pgrp are already
    deprecated.  The tgid value is close to being such - the worst known usage
    in in fs/locks.c and audit code.  The pid field deprecation is mainly
    blocked by numerous printk-s around the kernel that print the tsk->pid to
    log.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 68d27039ef7d..6838d4d77e05 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -306,7 +306,7 @@ void __set_special_pids(pid_t session, pid_t pgrp)
 	}
 	if (task_pgrp_nr(curr) != pgrp) {
 		detach_pid(curr, PIDTYPE_PGID);
-		curr->signal->pgrp = pgrp;
+		set_task_pgrp(curr, pgrp);
 		attach_pid(curr, PIDTYPE_PGID, find_pid(pgrp));
 	}
 }

commit b488893a390edfe027bae7a46e9af8083e740668
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:14 2007 -0700

    pid namespaces: changes to show virtual ids to user
    
    This is the largest patch in the set. Make all (I hope) the places where
    the pid is shown to or get from user operate on the virtual pids.
    
    The idea is:
     - all in-kernel data structures must store either struct pid itself
       or the pid's global nr, obtained with pid_nr() call;
     - when seeking the task from kernel code with the stored id one
       should use find_task_by_pid() call that works with global pids;
     - when showing pid's numerical value to the user the virtual one
       should be used, but however when one shows task's pid outside this
       task's namespace the global one is to be used;
     - when getting the pid from userspace one need to consider this as
       the virtual one and use appropriate task/pid-searching functions.
    
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: nuther build fix]
    [akpm@linux-foundation.org: yet nuther build fix]
    [akpm@linux-foundation.org: remove unneeded casts]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Alexey Dobriyan <adobriyan@openvz.org>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 567909fd6be4..68d27039ef7d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1112,15 +1112,17 @@ asmlinkage void sys_exit_group(int error_code)
 static int eligible_child(pid_t pid, int options, struct task_struct *p)
 {
 	int err;
+	struct pid_namespace *ns;
 
+	ns = current->nsproxy->pid_ns;
 	if (pid > 0) {
-		if (p->pid != pid)
+		if (task_pid_nr_ns(p, ns) != pid)
 			return 0;
 	} else if (!pid) {
-		if (task_pgrp_nr(p) != task_pgrp_nr(current))
+		if (task_pgrp_nr_ns(p, ns) != task_pgrp_vnr(current))
 			return 0;
 	} else if (pid != -1) {
-		if (task_pgrp_nr(p) != -pid)
+		if (task_pgrp_nr_ns(p, ns) != -pid)
 			return 0;
 	}
 
@@ -1190,9 +1192,12 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 {
 	unsigned long state;
 	int retval, status, traced;
+	struct pid_namespace *ns;
+
+	ns = current->nsproxy->pid_ns;
 
 	if (unlikely(noreap)) {
-		pid_t pid = p->pid;
+		pid_t pid = task_pid_nr_ns(p, ns);
 		uid_t uid = p->uid;
 		int exit_code = p->exit_code;
 		int why, status;
@@ -1311,11 +1316,11 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 			retval = put_user(status, &infop->si_status);
 	}
 	if (!retval && infop)
-		retval = put_user(p->pid, &infop->si_pid);
+		retval = put_user(task_pid_nr_ns(p, ns), &infop->si_pid);
 	if (!retval && infop)
 		retval = put_user(p->uid, &infop->si_uid);
 	if (!retval)
-		retval = p->pid;
+		retval = task_pid_nr_ns(p, ns);
 
 	if (traced) {
 		write_lock_irq(&tasklist_lock);
@@ -1352,6 +1357,7 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 			     int __user *stat_addr, struct rusage __user *ru)
 {
 	int retval, exit_code;
+	struct pid_namespace *ns;
 
 	if (!p->exit_code)
 		return 0;
@@ -1370,11 +1376,12 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 	 * keep holding onto the tasklist_lock while we call getrusage and
 	 * possibly take page faults for user memory.
 	 */
+	ns = current->nsproxy->pid_ns;
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);
 
 	if (unlikely(noreap)) {
-		pid_t pid = p->pid;
+		pid_t pid = task_pid_nr_ns(p, ns);
 		uid_t uid = p->uid;
 		int why = (p->ptrace & PT_PTRACED) ? CLD_TRAPPED : CLD_STOPPED;
 
@@ -1445,11 +1452,11 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 	if (!retval && infop)
 		retval = put_user(exit_code, &infop->si_status);
 	if (!retval && infop)
-		retval = put_user(p->pid, &infop->si_pid);
+		retval = put_user(task_pid_nr_ns(p, ns), &infop->si_pid);
 	if (!retval && infop)
 		retval = put_user(p->uid, &infop->si_uid);
 	if (!retval)
-		retval = p->pid;
+		retval = task_pid_nr_ns(p, ns);
 	put_task_struct(p);
 
 	BUG_ON(!retval);
@@ -1469,6 +1476,7 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 	int retval;
 	pid_t pid;
 	uid_t uid;
+	struct pid_namespace *ns;
 
 	if (!(p->signal->flags & SIGNAL_STOP_CONTINUED))
 		return 0;
@@ -1483,7 +1491,8 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 		p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
 	spin_unlock_irq(&p->sighand->siglock);
 
-	pid = p->pid;
+	ns = current->nsproxy->pid_ns;
+	pid = task_pid_nr_ns(p, ns);
 	uid = p->uid;
 	get_task_struct(p);
 	read_unlock(&tasklist_lock);
@@ -1494,7 +1503,7 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 		if (!retval && stat_addr)
 			retval = put_user(0xffff, stat_addr);
 		if (!retval)
-			retval = p->pid;
+			retval = task_pid_nr_ns(p, ns);
 	} else {
 		retval = wait_noreap_copyout(p, pid, uid,
 					     CLD_CONTINUED, SIGCONT,

commit 3eb07c8c8adb6f0572baba844ba2d9e501654316
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Thu Oct 18 23:40:13 2007 -0700

    pid namespaces: destroy pid namespace on init's death
    
    Terminate all processes in a namespace when the reaper of the namespace is
    exiting.  We do this by walking the pidmap of the namespace and sending
    SIGKILL to all processes.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Acked-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index d9e8e5ee9d7f..567909fd6be4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -879,7 +879,32 @@ static inline void exit_child_reaper(struct task_struct *tsk)
 	if (likely(tsk->group_leader != task_child_reaper(tsk)))
 		return;
 
-	panic("Attempted to kill init!");
+	if (tsk->nsproxy->pid_ns == &init_pid_ns)
+		panic("Attempted to kill init!");
+
+	/*
+	 * @tsk is the last thread in the 'cgroup-init' and is exiting.
+	 * Terminate all remaining processes in the namespace and reap them
+	 * before exiting @tsk.
+	 *
+	 * Note that @tsk (last thread of cgroup-init) may not necessarily
+	 * be the child-reaper (i.e main thread of cgroup-init) of the
+	 * namespace i.e the child_reaper may have already exited.
+	 *
+	 * Even after a child_reaper exits, we let it inherit orphaned children,
+	 * because, pid_ns->child_reaper remains valid as long as there is
+	 * at least one living sub-thread in the cgroup init.
+
+	 * This living sub-thread of the cgroup-init will be notified when
+	 * a child inherited by the 'child-reaper' exits (do_notify_parent()
+	 * uses __group_send_sig_info()). Further, when reaping child processes,
+	 * do_wait() iterates over children of all living sub threads.
+
+	 * i.e even though 'child_reaper' thread is listed as the parent of the
+	 * orphaned children, any living sub-thread in the cgroup-init can
+	 * perform the role of the child_reaper.
+	 */
+	zap_pid_ns_processes(tsk->nsproxy->pid_ns);
 }
 
 fastcall NORET_TYPE void do_exit(long code)

commit 60347f6716aa49831ac311e04d77ccdc50dc024a
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:03 2007 -0700

    pid namespaces: prepare proc_flust_task() to flush entries from multiple proc trees
    
    The first part is trivial - we just make the proc_flush_task() to operate on
    arbitrary vfsmount with arbitrary ids and pass the pid and global proc_mnt to
    it.
    
    The other change is more tricky: I moved the proc_flush_task() call in
    release_task() higher to address the following problem.
    
    When flushing task from many proc trees we need to know the set of ids (not
    just one pid) to find the dentries' names to flush.  Thus we need to pass the
    task's pid to proc_flush_task() as struct pid is the only object that can
    provide all the pid numbers.  But after __exit_signal() task has detached all
    his pids and this information is lost.
    
    This creates a tiny gap for proc_pid_lookup() to bring some dentries back to
    tree and keep them in hash (since pids are still alive before __exit_signal())
    till the next shrink, but since proc_flush_task() does not provide a 100%
    guarantee that the dentries will be flushed, this is OK to do so.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index df2eee9c68ce..d9e8e5ee9d7f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -148,6 +148,7 @@ void release_task(struct task_struct * p)
 	int zap_leader;
 repeat:
 	atomic_dec(&p->user->processes);
+	proc_flush_task(p);
 	write_lock_irq(&tasklist_lock);
 	ptrace_unlink(p);
 	BUG_ON(!list_empty(&p->ptrace_list) || !list_empty(&p->ptrace_children));
@@ -175,7 +176,6 @@ void release_task(struct task_struct * p)
 	}
 
 	write_unlock_irq(&tasklist_lock);
-	proc_flush_task(p);
 	release_thread(p);
 	call_rcu(&p->rcu, delayed_put_task_struct);
 

commit 2e4a707269a409950c3f315010c20f9719c594e2
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:40:01 2007 -0700

    pid namespaces: move exit_task_namespaces()
    
    Make task release its namespaces after it has reparented all his children to
    child_reaper, but before it notifies its parent about its death.
    
    The reason to release namespaces after reparenting is that when task exits it
    may send a signal to its parent (SIGCHLD), but if the parent has already
    exited its namespaces there will be no way to decide what pid to dever to him
    - parent can be from different namespace.
    
    The reason to release namespace before notifying the parent it that when task
    sends a SIGCHLD to parent it can call wait() on this taks and release it.  But
    releasing the mnt namespace implies dropping of all the mounts in the mnt
    namespace and NFS expects the task to have valid sighand pointer.
    
    Thanks to Oleg for pointing out some races that can apear and helping with
    patches and fixes.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3f2182ccf187..df2eee9c68ce 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -774,6 +774,7 @@ static void exit_notify(struct task_struct *tsk)
 	 *	jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
 	 */
 	forget_original_parent(tsk);
+	exit_task_namespaces(tsk);
 
 	write_lock_irq(&tasklist_lock);
 	/*
@@ -984,7 +985,6 @@ fastcall NORET_TYPE void do_exit(long code)
 		module_put(tsk->binfmt->module);
 
 	proc_exit_connector(tsk);
-	exit_task_namespaces(tsk);
 	exit_notify(tsk);
 #ifdef CONFIG_NUMA
 	mpol_free(tsk->mempolicy);

commit 762a24beed3f3ab93224bd447710e6c36fcf1968
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Thu Oct 18 23:40:00 2007 -0700

    pid namespaces: rework forget_original_parent()
    
    A pid namespace is a "view" of a particular set of tasks on the system.  They
    work in a similar way to filesystem namespaces.  A file (or a process) can be
    accessed in multiple namespaces, but it may have a different name in each.  In
    a filesystem, this name might be /etc/passwd in one namespace, but
    /chroot/etc/passwd in another.
    
    For processes, a process may have pid 1234 in one namespace, but be pid 1 in
    another.  This allows new pid namespaces to have basically arbitrary pids, and
    not have to worry about what pids exist in other namespaces.  This is
    essential for checkpoint/restart where a restarted process's pid might collide
    with an existing process on the system's pid.
    
    In this particular implementation, pid namespaces have a parent-child
    relationship, just like processes.  A process in a pid namespace may see all
    of the processes in the same namespace, as well as all of the processes in all
    of the namespaces which are children of its namespace.  Processes may not,
    however, see others which are in their parent's namespace, but not in their
    own.  The same goes for sibling namespaces.
    
    The know issue to be solved in the nearest future is signal handling in the
    namespace boundary.  That is, currently the namespace's init is treated like
    an ordinary task that can be killed from within an namespace.  Ideally, the
    signal handling by the namespace's init should have two sides: when signaling
    the init from its namespace, the init should look like a real init task, i.e.
    receive only those signals, that is explicitly wants to; when signaling the
    init from one of the parent namespaces, init should look like an ordinary
    task, i.e.  receive any signal, only taking the general permissions into
    account.
    
    The pid namespace was developed by Pavel Emlyanov and Sukadev Bhattiprolu and
    we eventually came to almost the same implementation, which differed in some
    details.  This set is based on Pavel's patches, but it includes comments and
    patches that from Sukadev.
    
    Many thanks to Oleg, who reviewed the patches, pointed out many BUGs and made
    valuable advises on how to make this set cleaner.
    
    This patch:
    
    We have to call exit_task_namespaces() only after the exiting task has
    reparented all his children and is sure that no other threads will reparent
    theirs for it.  Why this is needed is explained in appropriate patch.  This
    one only reworks the forget_original_parent() so that after calling this a
    task cannot be/become parent of any other task.
    
    We check PF_EXITING instead of ->exit_state while choosing the new parent.
    Note that tasklits_lock acts as a barrier, everyone who takes tasklist after
    us (when forget_original_parent() drops it) must see PF_EXITING.
    
    The other changes are just cleanups.  They just move some code from
    exit_notify to forget_original_parent().  It is a bit silly to declare
    ptrace_dead in exit_notify(), take tasklist, pass ptrace_dead to
    forget_original_parent(), unlock-lock-unlock tasklist, and then use
    ptrace_dead.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Paul Menage <menage@google.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 179ac74bf911..3f2182ccf187 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -666,10 +666,14 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
  * the child reaper process (ie "init") in our pid
  * space.
  */
-static void
-forget_original_parent(struct task_struct *father, struct list_head *to_release)
+static void forget_original_parent(struct task_struct *father)
 {
 	struct task_struct *p, *n, *reaper = father;
+	struct list_head ptrace_dead;
+
+	INIT_LIST_HEAD(&ptrace_dead);
+
+	write_lock_irq(&tasklist_lock);
 
 	do {
 		reaper = next_thread(reaper);
@@ -677,7 +681,7 @@ forget_original_parent(struct task_struct *father, struct list_head *to_release)
 			reaper = task_child_reaper(father);
 			break;
 		}
-	} while (reaper->exit_state);
+	} while (reaper->flags & PF_EXITING);
 
 	/*
 	 * There are only two places where our children can be:
@@ -714,12 +718,23 @@ forget_original_parent(struct task_struct *father, struct list_head *to_release)
 		 * while it was being traced by us, to be able to see it in wait4.
 		 */
 		if (unlikely(ptrace && p->exit_state == EXIT_ZOMBIE && p->exit_signal == -1))
-			list_add(&p->ptrace_list, to_release);
+			list_add(&p->ptrace_list, &ptrace_dead);
 	}
+
 	list_for_each_entry_safe(p, n, &father->ptrace_children, ptrace_list) {
 		p->real_parent = reaper;
 		reparent_thread(p, father, 1);
 	}
+
+	write_unlock_irq(&tasklist_lock);
+	BUG_ON(!list_empty(&father->children));
+	BUG_ON(!list_empty(&father->ptrace_children));
+
+	list_for_each_entry_safe(p, n, &ptrace_dead, ptrace_list) {
+		list_del_init(&p->ptrace_list);
+		release_task(p);
+	}
+
 }
 
 /*
@@ -730,7 +745,6 @@ static void exit_notify(struct task_struct *tsk)
 {
 	int state;
 	struct task_struct *t;
-	struct list_head ptrace_dead, *_p, *_n;
 	struct pid *pgrp;
 
 	if (signal_pending(tsk) && !(tsk->signal->flags & SIGNAL_GROUP_EXIT)
@@ -751,8 +765,6 @@ static void exit_notify(struct task_struct *tsk)
 		spin_unlock_irq(&tsk->sighand->siglock);
 	}
 
-	write_lock_irq(&tasklist_lock);
-
 	/*
 	 * This does two things:
 	 *
@@ -761,12 +773,9 @@ static void exit_notify(struct task_struct *tsk)
 	 *	as a result of our exiting, and if they have any stopped
 	 *	jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
 	 */
+	forget_original_parent(tsk);
 
-	INIT_LIST_HEAD(&ptrace_dead);
-	forget_original_parent(tsk, &ptrace_dead);
-	BUG_ON(!list_empty(&tsk->children));
-	BUG_ON(!list_empty(&tsk->ptrace_children));
-
+	write_lock_irq(&tasklist_lock);
 	/*
 	 * Check to see if any process groups have become orphaned
 	 * as a result of our exiting, and if they have any stopped
@@ -831,12 +840,6 @@ static void exit_notify(struct task_struct *tsk)
 
 	write_unlock_irq(&tasklist_lock);
 
-	list_for_each_safe(_p, _n, &ptrace_dead) {
-		list_del_init(_p);
-		t = list_entry(_p, struct task_struct, ptrace_list);
-		release_task(t);
-	}
-
 	/* If the process is dead, release it - nobody will wait for it */
 	if (state == EXIT_DEAD)
 		release_task(tsk);

commit d4c5e41f3f1b0c19448fcf2d259bdab1ede75e2e
Author: Daniel Walker <dwalker@mvista.com>
Date:   Thu Oct 18 23:39:59 2007 -0700

    whitespace fixes: task exit handling
    
    Signed-off-by: Daniel Walker <dwalker@mvista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c5d97af12159..179ac74bf911 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -405,7 +405,7 @@ void daemonize(const char *name, ...)
 		switch_task_namespaces(current, init_task.nsproxy);
 	}
 
- 	exit_files(current);
+	exit_files(current);
 	current->files = init_task.files;
 	atomic_inc(&current->files->count);
 
@@ -790,7 +790,7 @@ static void exit_notify(struct task_struct *tsk)
 	/* Let father know we died
 	 *
 	 * Thread signals are configurable, but you aren't going to use
-	 * that to send signals to arbitary processes. 
+	 * that to send signals to arbitary processes.
 	 * That stops right now.
 	 *
 	 * If the parent exec id doesn't match the exec id we saved

commit 03ff17979c58a0b63b0fe30a373f41b719731bd2
Author: Matthias Kaehlcke <matthias.kaehlcke@gmail.com>
Date:   Thu Oct 18 23:39:57 2007 -0700

    kernel/exit.c: Use list_for_each_entry(_safe) instead of list_for_each(_safe)
    
    kernel/exit.c: Convert list_for_each(_safe) to
    list_for_each_entry(_safe) in forget_original_parent(), exit_notify()
    and do_wait()
    
    Signed-off-by: Matthias Kaehlcke <matthias.kaehlcke@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index db9764186d5a..c5d97af12159 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -669,8 +669,7 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 static void
 forget_original_parent(struct task_struct *father, struct list_head *to_release)
 {
-	struct task_struct *p, *reaper = father;
-	struct list_head *_p, *_n;
+	struct task_struct *p, *n, *reaper = father;
 
 	do {
 		reaper = next_thread(reaper);
@@ -688,9 +687,8 @@ forget_original_parent(struct task_struct *father, struct list_head *to_release)
 	 *
 	 * Search them and reparent children.
 	 */
-	list_for_each_safe(_p, _n, &father->children) {
+	list_for_each_entry_safe(p, n, &father->children, sibling) {
 		int ptrace;
-		p = list_entry(_p, struct task_struct, sibling);
 
 		ptrace = p->ptrace;
 
@@ -718,8 +716,7 @@ forget_original_parent(struct task_struct *father, struct list_head *to_release)
 		if (unlikely(ptrace && p->exit_state == EXIT_ZOMBIE && p->exit_signal == -1))
 			list_add(&p->ptrace_list, to_release);
 	}
-	list_for_each_safe(_p, _n, &father->ptrace_children) {
-		p = list_entry(_p, struct task_struct, ptrace_list);
+	list_for_each_entry_safe(p, n, &father->ptrace_children, ptrace_list) {
 		p->real_parent = reaper;
 		reparent_thread(p, father, 1);
 	}
@@ -1518,12 +1515,9 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 	tsk = current;
 	do {
 		struct task_struct *p;
-		struct list_head *_p;
 		int ret;
 
-		list_for_each(_p,&tsk->children) {
-			p = list_entry(_p, struct task_struct, sibling);
-
+		list_for_each_entry(p, &tsk->children, sibling) {
 			ret = eligible_child(pid, options, p);
 			if (!ret)
 				continue;
@@ -1605,9 +1599,8 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 			}
 		}
 		if (!flag) {
-			list_for_each(_p, &tsk->ptrace_children) {
-				p = list_entry(_p, struct task_struct,
-						ptrace_list);
+			list_for_each_entry(p, &tsk->ptrace_children,
+					    ptrace_list) {
 				if (!eligible_child(pid, options, p))
 					continue;
 				flag = 1;

commit cf7b708c8d1d7a27736771bcf4c457b332b0f818
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Thu Oct 18 23:39:54 2007 -0700

    Make access to task's nsproxy lighter
    
    When someone wants to deal with some other taks's namespaces it has to lock
    the task and then to get the desired namespace if the one exists.  This is
    slow on read-only paths and may be impossible in some cases.
    
    E.g.  Oleg recently noticed a race between unshare() and the (sent for
    review in cgroups) pid namespaces - when the task notifies the parent it
    has to know the parent's namespace, but taking the task_lock() is
    impossible there - the code is under write locked tasklist lock.
    
    On the other hand switching the namespace on task (daemonize) and releasing
    the namespace (after the last task exit) is rather rare operation and we
    can sacrifice its speed to solve the issues above.
    
    The access to other task namespaces is proposed to be performed
    like this:
    
         rcu_read_lock();
         nsproxy = task_nsproxy(tsk);
         if (nsproxy != NULL) {
                 / *
                   * work with the namespaces here
                   * e.g. get the reference on one of them
                   * /
         } / *
             * NULL task_nsproxy() means that this task is
             * almost dead (zombie)
             * /
         rcu_read_unlock();
    
    This patch has passed the review by Eric and Oleg :) and,
    of course, tested.
    
    [clg@fr.ibm.com: fix unshare()]
    [ebiederm@xmission.com: Update get_net_ns_by_pid]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index d22aefabb129..db9764186d5a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -400,9 +400,10 @@ void daemonize(const char *name, ...)
 	current->fs = fs;
 	atomic_inc(&fs->count);
 
-	exit_task_namespaces(current);
-	current->nsproxy = init_task.nsproxy;
-	get_task_namespaces(current);
+	if (current->nsproxy != init_task.nsproxy) {
+		get_nsproxy(init_task.nsproxy);
+		switch_task_namespaces(current, init_task.nsproxy);
+	}
 
  	exit_files(current);
 	current->files = init_task.files;

commit b460cbc581a53cc088ceba80608021dd49c63c43
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Thu Oct 18 23:39:52 2007 -0700

    pid namespaces: define is_global_init() and is_container_init()
    
    is_init() is an ambiguous name for the pid==1 check.  Split it into
    is_global_init() and is_container_init().
    
    A cgroup init has it's tsk->pid == 1.
    
    A global init also has it's tsk->pid == 1 and it's active pid namespace
    is the init_pid_ns.  But rather than check the active pid namespace,
    compare the task structure with 'init_pid_ns.child_reaper', which is
    initialized during boot to the /sbin/init process and never changes.
    
    Changelog:
    
            2.6.22-rc4-mm2-pidns1:
            - Use 'init_pid_ns.child_reaper' to determine if a given task is the
              global init (/sbin/init) process. This would improve performance
              and remove dependence on the task_pid().
    
            2.6.21-mm2-pidns2:
    
            - [Sukadev Bhattiprolu] Changed is_container_init() calls in {powerpc,
              ppc,avr32}/traps.c for the _exception() call to is_global_init().
              This way, we kill only the cgroup if the cgroup's init has a
              bug rather than force a kernel panic.
    
    [akpm@linux-foundation.org: fix comment]
    [sukadev@us.ibm.com: Use is_global_init() in arch/m32r/mm/fault.c]
    [bunk@stusta.de: kernel/pid.c: remove unused exports]
    [sukadev@us.ibm.com: Fix capability.c to work with threaded init]
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Acked-by: Pavel Emelianov <xemul@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Herbert Poetzel <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index d1eddc753fe3..d22aefabb129 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -221,7 +221,7 @@ static int will_become_orphaned_pgrp(struct pid *pgrp, struct task_struct *ignor
 	do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
 		if (p == ignored_task
 				|| p->exit_state
-				|| is_init(p->real_parent))
+				|| is_global_init(p->real_parent))
 			continue;
 		if (task_pgrp(p->real_parent) != pgrp &&
 		    task_session(p->real_parent) == task_session(p)) {

commit 88f21d818255bc61c002478d21caf52f8a9b8def
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Thu Oct 18 23:39:50 2007 -0700

    pid namespaces: rename child_reaper() function
    
    Rename the child_reaper() function to task_child_reaper() to be similar to
    other task_* functions and to distinguish the function from 'struct
    pid_namspace.child_reaper'.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Pavel Emelianov <xemul@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Herbert Poetzel <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6e6ec300330f..d1eddc753fe3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -674,7 +674,7 @@ forget_original_parent(struct task_struct *father, struct list_head *to_release)
 	do {
 		reaper = next_thread(reaper);
 		if (reaper == father) {
-			reaper = child_reaper(father);
+			reaper = task_child_reaper(father);
 			break;
 		}
 	} while (reaper->exit_state);
@@ -874,7 +874,7 @@ static inline void check_stack_usage(void) {}
 
 static inline void exit_child_reaper(struct task_struct *tsk)
 {
-	if (likely(tsk->group_leader != child_reaper(tsk)))
+	if (likely(tsk->group_leader != task_child_reaper(tsk)))
 		return;
 
 	panic("Attempted to kill init!");

commit a47afb0f9d794d525a372c8d69902147cc88222a
Author: Pavel Emelianov <xemul@openvz.org>
Date:   Thu Oct 18 23:39:46 2007 -0700

    pid namespaces: round up the API
    
    The set of functions process_session, task_session, process_group and
    task_pgrp is confusing, as the names can be mixed with each other when looking
    at the code for a long time.
    
    The proposals are to
    * equip the functions that return the integer with _nr suffix to
      represent that fact,
    * and to make all functions work with task (not process) by making
      the common prefix of the same name.
    
    For monotony the routines signal_session() and set_signal_session() are
    replaced with task_session_nr() and set_task_session(), especially since they
    are only used with the explicit task->signal dereference.
    
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Acked-by: Serge E. Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index cf03a52c3a9a..6e6ec300330f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -299,12 +299,12 @@ void __set_special_pids(pid_t session, pid_t pgrp)
 {
 	struct task_struct *curr = current->group_leader;
 
-	if (process_session(curr) != session) {
+	if (task_session_nr(curr) != session) {
 		detach_pid(curr, PIDTYPE_SID);
-		set_signal_session(curr->signal, session);
+		set_task_session(curr, session);
 		attach_pid(curr, PIDTYPE_SID, find_pid(session));
 	}
-	if (process_group(curr) != pgrp) {
+	if (task_pgrp_nr(curr) != pgrp) {
 		detach_pid(curr, PIDTYPE_PGID);
 		curr->signal->pgrp = pgrp;
 		attach_pid(curr, PIDTYPE_PGID, find_pid(pgrp));
@@ -1091,10 +1091,10 @@ static int eligible_child(pid_t pid, int options, struct task_struct *p)
 		if (p->pid != pid)
 			return 0;
 	} else if (!pid) {
-		if (process_group(p) != process_group(current))
+		if (task_pgrp_nr(p) != task_pgrp_nr(current))
 			return 0;
 	} else if (pid != -1) {
-		if (process_group(p) != -pid)
+		if (task_pgrp_nr(p) != -pid)
 			return 0;
 	}
 

commit 8793d854edbc2774943a4b0de3304dc73991159a
Author: Paul Menage <menage@google.com>
Date:   Thu Oct 18 23:39:39 2007 -0700

    Task Control Groups: make cpusets a client of cgroups
    
    Remove the filesystem support logic from the cpusets system and makes cpusets
    a cgroup subsystem
    
    The "cpuset" filesystem becomes a dummy filesystem; attempts to mount it get
    passed through to the cgroup filesystem with the appropriate options to
    emulate the old cpuset filesystem behaviour.
    
    Signed-off-by: Paul Menage <menage@google.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 44ff6147556a..cf03a52c3a9a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -31,7 +31,6 @@
 #include <linux/taskstats_kern.h>
 #include <linux/delayacct.h>
 #include <linux/freezer.h>
-#include <linux/cpuset.h>
 #include <linux/cgroup.h>
 #include <linux/syscalls.h>
 #include <linux/signal.h>
@@ -973,7 +972,6 @@ fastcall NORET_TYPE void do_exit(long code)
 	__exit_fs(tsk);
 	check_stack_usage();
 	exit_thread();
-	cpuset_exit(tsk);
 	cgroup_exit(tsk, 1);
 	exit_keys(tsk);
 

commit b4f48b6363c81ca743ef46943ef23fd72e60f679
Author: Paul Menage <menage@google.com>
Date:   Thu Oct 18 23:39:33 2007 -0700

    Task Control Groups: add fork()/exit() hooks
    
    This adds the necessary hooks to the fork() and exit() paths to ensure
    that new children inherit their parent's cgroup assignments, and that
    exiting processes release reference counts on their cgroups.
    
    Signed-off-by: Paul Menage <menage@google.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Paul Jackson <pj@sgi.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Srivatsa Vaddagiri <vatsa@in.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2c704c86edb3..44ff6147556a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -32,6 +32,7 @@
 #include <linux/delayacct.h>
 #include <linux/freezer.h>
 #include <linux/cpuset.h>
+#include <linux/cgroup.h>
 #include <linux/syscalls.h>
 #include <linux/signal.h>
 #include <linux/posix-timers.h>
@@ -973,6 +974,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	check_stack_usage();
 	exit_thread();
 	cpuset_exit(tsk);
+	cgroup_exit(tsk, 1);
 	exit_keys(tsk);
 
 	if (group_dead && tsk->signal->leader)

commit 42b2dd0a02c512cf59c96f5c227bf54bfe5bbf08
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Tue Oct 16 23:27:30 2007 -0700

    Shrink task_struct if CONFIG_FUTEX=n
    
    robust_list, compat_robust_list, pi_state_list, pi_state_cache are
    really used if futexes are on.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4c108df88a37..2c704c86edb3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -947,11 +947,13 @@ fastcall NORET_TYPE void do_exit(long code)
 		exit_itimers(tsk->signal);
 	}
 	acct_collect(code, group_dead);
+#ifdef CONFIG_FUTEX
 	if (unlikely(tsk->robust_list))
 		exit_robust_list(tsk);
-#if defined(CONFIG_FUTEX) && defined(CONFIG_COMPAT)
+#ifdef CONFIG_COMPAT
 	if (unlikely(tsk->compat_robust_list))
 		compat_exit_robust_list(tsk);
+#endif
 #endif
 	if (group_dead)
 		tty_audit_exit();
@@ -987,6 +989,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	mpol_free(tsk->mempolicy);
 	tsk->mempolicy = NULL;
 #endif
+#ifdef CONFIG_FUTEX
 	/*
 	 * This must happen late, after the PID is not
 	 * hashed anymore:
@@ -995,6 +998,7 @@ fastcall NORET_TYPE void do_exit(long code)
 		exit_pi_state_list(tsk);
 	if (unlikely(current->pi_state_cache))
 		kfree(current->pi_state_cache);
+#endif
 	/*
 	 * Make sure we are holding no locks:
 	 */

commit 6db840fa7887980ef68a649640d506fe069eef0c
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:27:23 2007 -0700

    exec: RT sub-thread can livelock and monopolize CPU on exec
    
    de_thread() yields waiting for ->group_leader to be a zombie. This deadlocks
    if an rt-prio execer shares the same cpu with ->group_leader. Change the code
    to use ->group_exit_task/notify_count mechanics.
    
    This patch certainly uglifies the code, perhaps someone can suggest something
    better.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 25f6805be5fe..4c108df88a37 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -92,10 +92,9 @@ static void __exit_signal(struct task_struct *tsk)
 		 * If there is any task waiting for the group exit
 		 * then notify it:
 		 */
-		if (sig->group_exit_task && atomic_read(&sig->count) == sig->notify_count) {
+		if (sig->group_exit_task && atomic_read(&sig->count) == sig->notify_count)
 			wake_up_process(sig->group_exit_task);
-			sig->group_exit_task = NULL;
-		}
+
 		if (tsk == sig->curr_target)
 			sig->curr_target = next_thread(tsk);
 		/*
@@ -827,6 +826,11 @@ static void exit_notify(struct task_struct *tsk)
 		state = EXIT_DEAD;
 	tsk->exit_state = state;
 
+	if (thread_group_leader(tsk) &&
+	    tsk->signal->notify_count < 0 &&
+	    tsk->signal->group_exit_task)
+		wake_up_process(tsk->signal->group_exit_task);
+
 	write_unlock_irq(&tasklist_lock);
 
 	list_for_each_safe(_p, _n, &ptrace_dead) {

commit 715015e8da37c4d13e234def054bcbea116297e9
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:27:00 2007 -0700

    wait_task_stopped/continued: remove unneeded p->signal != NULL check
    
    The child was found on ->children list under tasklist_lock, it must have a
    valid ->signal. __exit_signal() both removes the task from parent->children
    and clears ->signal "atomically" under write_lock(tasklist).
    
    Remove unneeded checks.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b27a3dcde671..25f6805be5fe 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1322,7 +1322,7 @@ static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
 	if (!p->exit_code)
 		return 0;
 	if (delayed_group_leader && !(p->ptrace & PT_PTRACED) &&
-	    p->signal && p->signal->group_stop_count > 0)
+	    p->signal->group_stop_count > 0)
 		/*
 		 * A group stop is in progress and this is the group leader.
 		 * We won't report until all threads have stopped.
@@ -1436,9 +1436,6 @@ static int wait_task_continued(struct task_struct *p, int noreap,
 	pid_t pid;
 	uid_t uid;
 
-	if (unlikely(!p->signal))
-		return 0;
-
 	if (!(p->signal->flags & SIGNAL_STOP_CONTINUED))
 		return 0;
 

commit 442a10cf9e1c350b4de4dd6f22c72618a0b13d7f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:26:59 2007 -0700

    wait_task_zombie: don't fight with non-existing race with a dying ptracee
    
    The "p->exit_signal == -1 && p->ptrace == 0" check and the comment are
    bogus.  We already did exactly the same check in eligible_child(), we did
    not drop tasklist_lock since then, and both variables need
    write_lock(tasklist) to be changed.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 28144b94e55c..b27a3dcde671 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1189,13 +1189,6 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 		BUG_ON(state != EXIT_DEAD);
 		return 0;
 	}
-	if (unlikely(p->exit_signal == -1 && p->ptrace == 0)) {
-		/*
-		 * This can only happen in a race with a ptraced thread
-		 * dying on another processor.
-		 */
-		return 0;
-	}
 
 	/* traced means p->ptrace, but not vice versa */
 	traced = (p->real_parent != p->parent);

commit 3ae4cbadf4bf97ee137b921a1b928d2a5dcd26ca
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:26:58 2007 -0700

    exit_notify: don't take tasklist for TIF_SIGPENDING re-targeting
    
    ->siglock provides enough protection to iterate over the thread group.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b4d569675d4b..28144b94e55c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -747,13 +747,11 @@ static void exit_notify(struct task_struct *tsk)
 		 * Now we'll wake all the threads in the group just to make
 		 * sure someone gets all the pending signals.
 		 */
-		read_lock(&tasklist_lock);
 		spin_lock_irq(&tsk->sighand->siglock);
 		for (t = next_thread(tsk); t != tsk; t = next_thread(t))
 			if (!signal_pending(t) && !(t->flags & PF_EXITING))
 				recalc_sigpending_and_wake(t);
 		spin_unlock_irq(&tsk->sighand->siglock);
-		read_unlock(&tasklist_lock);
 	}
 
 	write_lock_irq(&tasklist_lock);
@@ -781,9 +779,8 @@ static void exit_notify(struct task_struct *tsk)
 	 * and we were the only connection outside, so our pgrp
 	 * is about to become orphaned.
 	 */
-	 
 	t = tsk->real_parent;
-	
+
 	pgrp = task_pgrp(tsk);
 	if ((task_pgrp(t) != pgrp) &&
 	    (task_session(t) == task_session(tsk)) &&

commit 2f4e6e2a814eb1305a873a045401708d73f870bc
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:26:58 2007 -0700

    wait_task_zombie: fix 2/3 races vs forget_original_parent()
    
    Two threads, T1 and T2.  T2 ptraces P, and P is not a child of ptracer's
    thread group.  P exits and goes to TASK_ZOMBIE.
    
    T1 does wait_task_zombie(P):
    
            P->exit_state = TASK_DEAD;
            ...
            read_unlock(&tasklist_lock);
    
                                            T2 does exit(), takes tasklist,
                                            forget_original_parent() does
                                            __ptrace_unlink(P) but doesn't
                                            call do_notify_parent(P) because
                                            p->exit_state == EXIT_DEAD.
    
    Now, P is not visible to our process: __ptrace_unlink() removed it from
    ->children. We should send notification to P->parent and release P if and
    only if SIGCHLD is ignored.
    
    And we have 3 bugs:
    
    1. P->parent does do_wait() and gets -ECHILD (P is on ->parent->children,
       but its state is TASK_DEAD).
    
    2. // wait_task_zombie() continues
    
            if (put_user(...)) {
                    // TODO: is this safe?
                    p->exit_state = EXIT_ZOMBIE;
                    return;
            }
    
       we return without notification/release, task_struct leaked.
    
       Solution: ignore -EFAULT and proceed. It is an application's bug if
       we can't fill infop/stat_addr (in case of VM_FAULT_OOM we have much
       more problems).
    
    3. // wait_task_zombie() continues
    
            if (p->real_parent != p->parent) {
                    // Not taken, it was untraced'ed
                    ...
            }
    
            release_task(p);
    
       we released the task which we shouldn't.
    
       Solution: check ->real_parent != ->parent before, under tasklist_lock,
       but use ptrace_unlink() instead of __ptrace_unlink() to check ->ptrace.
    
    This patch hopefully solves 2 and 3, the 1st bug will be fixed later, we need
    some cleanups in forget_original_parent/reparent_thread.
    
    However, the first race is very unlikely and not critical, so I hope it makes
    sense to fix 1 and 2 for now.
    
    4. Small cleanup: don't "restore" EXIT_ZOMBIE unless we know we are not going
       to realease the child.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9d6e0897a447..b4d569675d4b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1158,8 +1158,7 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 			    int __user *stat_addr, struct rusage __user *ru)
 {
 	unsigned long state;
-	int retval;
-	int status;
+	int retval, status, traced;
 
 	if (unlikely(noreap)) {
 		pid_t pid = p->pid;
@@ -1201,7 +1200,10 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 		return 0;
 	}
 
-	if (likely(p->real_parent == p->parent)) {
+	/* traced means p->ptrace, but not vice versa */
+	traced = (p->real_parent != p->parent);
+
+	if (likely(!traced)) {
 		struct signal_struct *psig;
 		struct signal_struct *sig;
 
@@ -1288,35 +1290,30 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 		retval = put_user(p->pid, &infop->si_pid);
 	if (!retval && infop)
 		retval = put_user(p->uid, &infop->si_uid);
-	if (retval) {
-		// TODO: is this safe?
-		p->exit_state = EXIT_ZOMBIE;
-		return retval;
-	}
-	retval = p->pid;
-	if (p->real_parent != p->parent) {
+	if (!retval)
+		retval = p->pid;
+
+	if (traced) {
 		write_lock_irq(&tasklist_lock);
-		/* Double-check with lock held.  */
-		if (p->real_parent != p->parent) {
-			__ptrace_unlink(p);
-			// TODO: is this safe?
-			p->exit_state = EXIT_ZOMBIE;
-			/*
-			 * If this is not a detached task, notify the parent.
-			 * If it's still not detached after that, don't release
-			 * it now.
-			 */
+		/* We dropped tasklist, ptracer could die and untrace */
+		ptrace_unlink(p);
+		/*
+		 * If this is not a detached task, notify the parent.
+		 * If it's still not detached after that, don't release
+		 * it now.
+		 */
+		if (p->exit_signal != -1) {
+			do_notify_parent(p, p->exit_signal);
 			if (p->exit_signal != -1) {
-				do_notify_parent(p, p->exit_signal);
-				if (p->exit_signal != -1)
-					p = NULL;
+				p->exit_state = EXIT_ZOMBIE;
+				p = NULL;
 			}
 		}
 		write_unlock_irq(&tasklist_lock);
 	}
 	if (p != NULL)
 		release_task(p);
-	BUG_ON(!retval);
+
 	return retval;
 }
 

commit 407af46a967ffd2f208f0a5fb3f1ff954801494a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:26:57 2007 -0700

    wait_task_zombie: remove unneeded child->signal check
    
    A zombie must have a valid ->signal, we are going to release it and
    __exit_signal() starts with BUG_ON(!sig).
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8b64c0371ae9..9d6e0897a447 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1201,7 +1201,7 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 		return 0;
 	}
 
-	if (likely(p->real_parent == p->parent) && likely(p->signal)) {
+	if (likely(p->real_parent == p->parent)) {
 		struct signal_struct *psig;
 		struct signal_struct *sig;
 

commit 84eb646b6eabcd82ec563f30d2d9d40c2054a9c9
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:26:49 2007 -0700

    handle the multi-threaded init's exit() properly
    
    With or without this patch, multi-threaded init's are not fully supported,
    but do_exit() is completely wrong.  This becomes a real problem when we
    support pid namespaces.
    
    1. do_exit() panics when the main thread of /sbin/init exits. It should not
       until the whole thread group exits. Move the code below, under the
       "if (group_dead)" check.
    
       Note: this means that forget_original_parent() can use an already dead
       child_reaper()'s task_struct. This is OK for /sbin/init because
    
            - do_wait() from alive sub-thread still can reap a zombie, we iterate
              over all sub-thread's ->children lists
    
            - do_notify_parent() will wakeup some alive sub-thread because it sends
              the group-wide signal
    
       However, we should remove choose_new_parent()->BUG_ON(reaper->exit_state)
       for this.
    
    2. We are playing games with ->nsproxy->pid_ns. This code is bogus today, and
       it has to be changed anyway when we really support pid namespaces, just
       remove it.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Roland McGrath <roland@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3b8dfffd9329..8b64c0371ae9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -592,17 +592,6 @@ static void exit_mm(struct task_struct * tsk)
 	mmput(mm);
 }
 
-static inline void
-choose_new_parent(struct task_struct *p, struct task_struct *reaper)
-{
-	/*
-	 * Make sure we're not reparenting to ourselves and that
-	 * the parent is not a zombie.
-	 */
-	BUG_ON(p == reaper || reaper->exit_state);
-	p->real_parent = reaper;
-}
-
 static void
 reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 {
@@ -710,7 +699,7 @@ forget_original_parent(struct task_struct *father, struct list_head *to_release)
 
 		if (father == p->real_parent) {
 			/* reparent with a reaper, real father it's us */
-			choose_new_parent(p, reaper);
+			p->real_parent = reaper;
 			reparent_thread(p, father, 0);
 		} else {
 			/* reparent ptraced task to its real parent */
@@ -731,7 +720,7 @@ forget_original_parent(struct task_struct *father, struct list_head *to_release)
 	}
 	list_for_each_safe(_p, _n, &father->ptrace_children) {
 		p = list_entry(_p, struct task_struct, ptrace_list);
-		choose_new_parent(p, reaper);
+		p->real_parent = reaper;
 		reparent_thread(p, father, 1);
 	}
 }
@@ -882,6 +871,14 @@ static void check_stack_usage(void)
 static inline void check_stack_usage(void) {}
 #endif
 
+static inline void exit_child_reaper(struct task_struct *tsk)
+{
+	if (likely(tsk->group_leader != child_reaper(tsk)))
+		return;
+
+	panic("Attempted to kill init!");
+}
+
 fastcall NORET_TYPE void do_exit(long code)
 {
 	struct task_struct *tsk = current;
@@ -895,13 +892,6 @@ fastcall NORET_TYPE void do_exit(long code)
 		panic("Aiee, killing interrupt handler!");
 	if (unlikely(!tsk->pid))
 		panic("Attempted to kill the idle task!");
-	if (unlikely(tsk == child_reaper(tsk))) {
-		if (tsk->nsproxy->pid_ns != &init_pid_ns)
-			tsk->nsproxy->pid_ns->child_reaper = init_pid_ns.child_reaper;
-		else
-			panic("Attempted to kill init!");
-	}
-
 
 	if (unlikely(current->ptrace & PT_TRACE_EXIT)) {
 		current->ptrace_message = code;
@@ -951,6 +941,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	}
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
+		exit_child_reaper(tsk);
 		hrtimer_cancel(&tsk->signal->real_timer);
 		exit_itimers(tsk->signal);
 	}

commit d2ee7198cc2414aade234a3cebc69e6cbff35d9b
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Oct 16 23:26:47 2007 -0700

    pi-futex: set PF_EXITING without taking ->pi_lock
    
    It is a bit annoying that do_exit() takes ->pi_lock to set PF_EXITING.  All
    we need is to synchronize with lookup_pi_state() which saw this task
    without PF_EXITING under ->pi_lock.
    
    Change do_exit() to use spin_unlock_wait().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6ca1e4666e9f..3b8dfffd9329 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -931,13 +931,13 @@ fastcall NORET_TYPE void do_exit(long code)
 		schedule();
 	}
 
+	tsk->flags |= PF_EXITING;
 	/*
 	 * tsk->flags are checked in the futex code to protect against
 	 * an exiting task cleaning up the robust pi futexes.
 	 */
-	spin_lock_irq(&tsk->pi_lock);
-	tsk->flags |= PF_EXITING;
-	spin_unlock_irq(&tsk->pi_lock);
+	smp_mb();
+	spin_unlock_wait(&tsk->pi_lock);
 
 	if (unlikely(in_atomic()))
 		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",

commit a9022e9cb9e919e31d5bc15fcef5c7186740645e
Author: Jesper Juhl <jesper.juhl@gmail.com>
Date:   Tue Oct 16 23:26:23 2007 -0700

    Clean up duplicate includes in kernel/
    
    This patch cleans up duplicate includes in
            kernel/
    
    Signed-off-by: Jesper Juhl <jesper.juhl@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Satyam Sharma <ssatyam@cse.iitk.ac.in>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7f7959de4a87..6ca1e4666e9f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -44,7 +44,6 @@
 #include <linux/resource.h>
 #include <linux/blkdev.h>
 #include <linux/task_io_accounting_ops.h>
-#include <linux/freezer.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>

commit 9ac52315d4cf5f561f36dabaf0720c00d3553162
Author: Laurent Vivier <Laurent.Vivier@bull.net>
Date:   Mon Oct 15 17:00:19 2007 +0200

    sched: guest CPU accounting: add guest-CPU /proc/<pid>/stat fields
    
    like for cpustat, introduce the "gtime" (guest time of the task) and
    "cgtime" (guest time of the task children) fields for the
    tasks. Modify signal_struct and task_struct.
    
    Modify /proc/<pid>/stat to display these new fields.
    
    Signed-off-by: Laurent Vivier <Laurent.Vivier@bull.net>
    Acked-by: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 993369ee94d1..7f7959de4a87 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -111,6 +111,7 @@ static void __exit_signal(struct task_struct *tsk)
 		 */
 		sig->utime = cputime_add(sig->utime, tsk->utime);
 		sig->stime = cputime_add(sig->stime, tsk->stime);
+		sig->gtime = cputime_add(sig->gtime, tsk->gtime);
 		sig->min_flt += tsk->min_flt;
 		sig->maj_flt += tsk->maj_flt;
 		sig->nvcsw += tsk->nvcsw;
@@ -1242,6 +1243,11 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 			cputime_add(p->stime,
 			cputime_add(sig->stime,
 				    sig->cstime)));
+		psig->cgtime =
+			cputime_add(psig->cgtime,
+			cputime_add(p->gtime,
+			cputime_add(sig->gtime,
+				    sig->cgtime)));
 		psig->cmin_flt +=
 			p->min_flt + sig->min_flt + sig->cmin_flt;
 		psig->cmaj_flt +=

commit b8fceee17a310f189188599a8fa5e9beaff57eb0
Author: Davide Libenzi <davidel@xmailserver.org>
Date:   Thu Sep 20 12:40:16 2007 -0700

    signalfd simplification
    
    This simplifies signalfd code, by avoiding it to remain attached to the
    sighand during its lifetime.
    
    In this way, the signalfd remain attached to the sighand only during
    poll(2) (and select and epoll) and read(2).  This also allows to remove
    all the custom "tsk == current" checks in kernel/signal.c, since
    dequeue_signal() will only be called by "current".
    
    I think this is also what Ben was suggesting time ago.
    
    The external effect of this, is that a thread can extract only its own
    private signals and the group ones.  I think this is an acceptable
    behaviour, in that those are the signals the thread would be able to
    fetch w/out signalfd.
    
    Signed-off-by: Davide Libenzi <davidel@xmailserver.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 06b24b3aa370..993369ee94d1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -24,7 +24,6 @@
 #include <linux/pid_namespace.h>
 #include <linux/ptrace.h>
 #include <linux/profile.h>
-#include <linux/signalfd.h>
 #include <linux/mount.h>
 #include <linux/proc_fs.h>
 #include <linux/kthread.h>
@@ -86,14 +85,6 @@ static void __exit_signal(struct task_struct *tsk)
 	sighand = rcu_dereference(tsk->sighand);
 	spin_lock(&sighand->siglock);
 
-	/*
-	 * Notify that this sighand has been detached. This must
-	 * be called with the tsk->sighand lock held. Also, this
-	 * access tsk->sighand internally, so it must be called
-	 * before tsk->sighand is reset.
-	 */
-	signalfd_detach_locked(tsk);
-
 	posix_cpu_timers_exit(tsk);
 	if (atomic_dec_and_test(&sig->count))
 		posix_cpu_timers_exit_group(tsk);

commit f2ab6d8889422c1f5354f014e8bef337b1d1bade
Author: Jonathan Lim <jlim@sgi.com>
Date:   Thu Aug 30 23:56:23 2007 -0700

    Assign task_struct.exit_code before taskstats_exit()
    
    taskstats.ac_exitcode is assigned to task_struct.exit_code in bacct_add_tsk()
    through the following kernel function calls:
    
      do_exit()
        taskstats_exit()
          fill_pid()
            bacct_add_tsk()
    
    The problem is that in do_exit(), task_struct.exit_code is set to 'code' only
    after taskstats_exit() has been called.  So we need to move the assignment
    before taskstats_exit().
    
    Signed-off-by: Jonathan Lim <jlim@sgi.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9578c1ae19ca..06b24b3aa370 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -975,6 +975,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (unlikely(tsk->audit_context))
 		audit_free(tsk);
 
+	tsk->exit_code = code;
 	taskstats_exit(tsk, group_dead);
 
 	exit_mm(tsk);
@@ -996,7 +997,6 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (tsk->binfmt)
 		module_put(tsk->binfmt->module);
 
-	tsk->exit_code = code;
 	proc_exit_connector(tsk);
 	exit_task_namespaces(tsk);
 	exit_notify(tsk);

commit 247284481ca40288bd120cf0707681c3bdbee78f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sat Aug 4 01:04:41 2007 +0400

    Kill some obsolete sub-thread-ptrace stuff
    
    There is a couple of subtle checks which were needed to handle ptracing from
    the same thread group. This was deprecated a long ago, imho this code just
    complicates the understanding.
    
    And, the "->parent->signal->flags & SIGNAL_GROUP_EXIT" check in exit_notify()
    is not right. SIGNAL_GROUP_EXIT can mean exec(), not exit_group(). This means
    ptracer can lose a ptraced zombie on exec(). Minor problem, but still the bug.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 464c2b172f07..9578c1ae19ca 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -813,7 +813,7 @@ static void exit_notify(struct task_struct *tsk)
 		__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);
 	}
 
-	/* Let father know we died 
+	/* Let father know we died
 	 *
 	 * Thread signals are configurable, but you aren't going to use
 	 * that to send signals to arbitary processes. 
@@ -826,9 +826,7 @@ static void exit_notify(struct task_struct *tsk)
 	 * If our self_exec id doesn't match our parent_exec_id then
 	 * we have changed execution domain as these two values started
 	 * the same after a fork.
-	 *	
 	 */
-	
 	if (tsk->exit_signal != SIGCHLD && tsk->exit_signal != -1 &&
 	    ( tsk->parent_exec_id != t->self_exec_id  ||
 	      tsk->self_exec_id != tsk->parent_exec_id)
@@ -848,9 +846,7 @@ static void exit_notify(struct task_struct *tsk)
 	}
 
 	state = EXIT_ZOMBIE;
-	if (tsk->exit_signal == -1 &&
-	    (likely(tsk->ptrace == 0) ||
-	     unlikely(tsk->parent->signal->flags & SIGNAL_GROUP_EXIT)))
+	if (tsk->exit_signal == -1 && likely(!tsk->ptrace))
 		state = EXIT_DEAD;
 	tsk->exit_state = state;
 

commit 0c1eecfb345401629aa57c9d3b077273e56c45a7
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Thu Jul 19 01:47:33 2007 -0700

    Freezer: avoid freezing kernel threads prematurely
    
    Kernel threads should not have TIF_FREEZE set when user space processes are
    being frozen, since otherwise some of them might be frozen prematurely.
    To prevent this from happening we can (1) make exit_mm() unset TIF_FREEZE
    unconditionally just after clearing tsk->mm and (2) make try_to_freeze_tasks()
    check if p->mm is different from zero and PF_BORROWED_MM is unset in p->flags
    when user space processes are to be frozen.
    
    Namely, when user space processes are being frozen, we only should set
    TIF_FREEZE for tasks that have p->mm different from NULL and don't have
    PF_BORROWED_MM set in p->flags.  For this reason task_lock() must be used to
    prevent try_to_freeze_tasks() from racing with use_mm()/unuse_mm(), in which
    p->mm and p->flags.PF_BORROWED_MM are changed under task_lock(p).  Also, we
    need to prevent the following scenario from happening:
    
    * daemonize() is called by a task spawned from a user space code path
    * freezer checks if the task has p->mm set and the result is positive
    * task enters exit_mm() and clears its TIF_FREEZE
    * freezer sets TIF_FREEZE for the task
    * task calls try_to_freeze() and goes to the refrigerator, which is wrong at
      that point
    
    This requires us to acquire task_lock(p) before p->flags.PF_BORROWED_MM and
    p->mm are examined and release it after TIF_FREEZE is set for p (or it turns
    out that TIF_FREEZE should not be set).
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Nigel Cunningham <nigel@nigel.suspend2.net>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e8af8d0c2483..464c2b172f07 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -45,6 +45,7 @@
 #include <linux/resource.h>
 #include <linux/blkdev.h>
 #include <linux/task_io_accounting_ops.h>
+#include <linux/freezer.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -594,6 +595,8 @@ static void exit_mm(struct task_struct * tsk)
 	tsk->mm = NULL;
 	up_read(&mm->mmap_sem);
 	enter_lazy_tlb(mm, current);
+	/* We don't want this task to be frozen prematurely */
+	clear_freeze_flag(tsk);
 	task_unlock(tsk);
 	mmput(mm);
 }

commit 831441862956fffa17b9801db37e6ea1650b0f69
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Tue Jul 17 04:03:35 2007 -0700

    Freezer: make kernel threads nonfreezable by default
    
    Currently, the freezer treats all tasks as freezable, except for the kernel
    threads that explicitly set the PF_NOFREEZE flag for themselves.  This
    approach is problematic, since it requires every kernel thread to either
    set PF_NOFREEZE explicitly, or call try_to_freeze(), even if it doesn't
    care for the freezing of tasks at all.
    
    It seems better to only require the kernel threads that want to or need to
    be frozen to use some freezer-related code and to remove any
    freezer-related code from the other (nonfreezable) kernel threads, which is
    done in this patch.
    
    The patch causes all kernel threads to be nonfreezable by default (ie.  to
    have PF_NOFREEZE set by default) and introduces the set_freezable()
    function that should be called by the freezable kernel threads in order to
    unset PF_NOFREEZE.  It also makes all of the currently freezable kernel
    threads call set_freezable(), so it shouldn't cause any (intentional)
    change of behaviour to appear.  Additionally, it updates documentation to
    describe the freezing of tasks more accurately.
    
    [akpm@linux-foundation.org: build fixes]
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Nigel Cunningham <nigel@nigel.suspend2.net>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 57626692cd90..e8af8d0c2483 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -31,6 +31,7 @@
 #include <linux/mempolicy.h>
 #include <linux/taskstats_kern.h>
 #include <linux/delayacct.h>
+#include <linux/freezer.h>
 #include <linux/cpuset.h>
 #include <linux/syscalls.h>
 #include <linux/signal.h>
@@ -387,6 +388,11 @@ void daemonize(const char *name, ...)
 	 * they would be locked into memory.
 	 */
 	exit_mm(current);
+	/*
+	 * We don't want to have TIF_FREEZE set if the system-wide hibernation
+	 * or suspend transition begins right now.
+	 */
+	current->flags |= PF_NOFREEZE;
 
 	set_special_pids(1, 1);
 	proc_clear_tty(current);

commit 522ed7767e800cff6c650ec64b0ee0677303119c
Author: Miloslav Trmac <mitr@redhat.com>
Date:   Sun Jul 15 23:40:56 2007 -0700

    Audit: add TTY input auditing
    
    Add TTY input auditing, used to audit system administrator's actions.  This is
    required by various security standards such as DCID 6/3 and PCI to provide
    non-repudiation of administrator's actions and to allow a review of past
    actions if the administrator seems to overstep their duties or if the system
    becomes misconfigured for unknown reasons.  These requirements do not make it
    necessary to audit TTY output as well.
    
    Compared to an user-space keylogger, this approach records TTY input using the
    audit subsystem, correlated with other audit events, and it is completely
    transparent to the user-space application (e.g.  the console ioctls still
    work).
    
    TTY input auditing works on a higher level than auditing all system calls
    within the session, which would produce an overwhelming amount of mostly
    useless audit events.
    
    Add an "audit_tty" attribute, inherited across fork ().  Data read from TTYs
    by process with the attribute is sent to the audit subsystem by the kernel.
    The audit netlink interface is extended to allow modifying the audit_tty
    attribute, and to allow sending explanatory audit events from user-space (for
    example, a shell might send an event containing the final command, after the
    interactive command-line editing and history expansion is performed, which
    might be difficult to decipher from the TTY input alone).
    
    Because the "audit_tty" attribute is inherited across fork (), it would be set
    e.g.  for sshd restarted within an audited session.  To prevent this, the
    audit_tty attribute is cleared when a process with no open TTY file
    descriptors (e.g.  after daemon startup) opens a TTY.
    
    See https://www.redhat.com/archives/linux-audit/2007-June/msg00000.html for a
    more detailed rationale document for an older version of this patch.
    
    [akpm@linux-foundation.org: build fix]
    Signed-off-by: Miloslav Trmac <mitr@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Paul Fulghum <paulkf@microgate.com>
    Cc: Casey Schaufler <casey@schaufler-ca.com>
    Cc: Steve Grubb <sgrubb@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 64a5263c8c7b..57626692cd90 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -965,6 +965,8 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (unlikely(tsk->compat_robust_list))
 		compat_exit_robust_list(tsk);
 #endif
+	if (group_dead)
+		tty_audit_exit();
 	if (unlikely(tsk->audit_context))
 		audit_free(tsk);
 

commit e18eecb8b35703a5eea73ee2b45324262029e62c
Author: Jeff Dike <jdike@addtoit.com>
Date:   Sun Jul 15 23:38:48 2007 -0700

    Add generic exit-time stack-depth checking to CONFIG_DEBUG_STACK_USAGE
    
    Add generic exit-time stack-depth checking to CONFIG_DEBUG_STACK_USAGE.
    
    This also adds UML support.
    
    Tested on UML and i386.
    
    [akpm@linux-foundation.org: cleanups, speedups, tweaks]
    Signed-off-by: Jeff Dike <jdike@linux.intel.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ca6a11b73023..64a5263c8c7b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -858,6 +858,34 @@ static void exit_notify(struct task_struct *tsk)
 		release_task(tsk);
 }
 
+#ifdef CONFIG_DEBUG_STACK_USAGE
+static void check_stack_usage(void)
+{
+	static DEFINE_SPINLOCK(low_water_lock);
+	static int lowest_to_date = THREAD_SIZE;
+	unsigned long *n = end_of_stack(current);
+	unsigned long free;
+
+	while (*n == 0)
+		n++;
+	free = (unsigned long)n - (unsigned long)end_of_stack(current);
+
+	if (free >= lowest_to_date)
+		return;
+
+	spin_lock(&low_water_lock);
+	if (free < lowest_to_date) {
+		printk(KERN_WARNING "%s used greatest stack depth: %lu bytes "
+				"left\n",
+				current->comm, free);
+		lowest_to_date = free;
+	}
+	spin_unlock(&low_water_lock);
+}
+#else
+static inline void check_stack_usage(void) {}
+#endif
+
 fastcall NORET_TYPE void do_exit(long code)
 {
 	struct task_struct *tsk = current;
@@ -949,6 +977,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	exit_sem(tsk);
 	__exit_files(tsk);
 	__exit_fs(tsk);
+	check_stack_usage();
 	exit_thread();
 	cpuset_exit(tsk);
 	exit_keys(tsk);

commit 172ba844a8851c3edd13c0a979cdf46bd5e3cc1a
Author: Balbir Singh <balbir@linux.vnet.ibm.com>
Date:   Mon Jul 9 18:52:00 2007 +0200

    sched: update delay-accounting to use CFS's precise stats
    
    update delay-accounting to use CFS's precise stats.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8fd7acd7bbd0..ca6a11b73023 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -122,9 +122,9 @@ static void __exit_signal(struct task_struct *tsk)
 		sig->maj_flt += tsk->maj_flt;
 		sig->nvcsw += tsk->nvcsw;
 		sig->nivcsw += tsk->nivcsw;
-		sig->sched_time += tsk->sched_time;
 		sig->inblock += task_io_get_inblock(tsk);
 		sig->oublock += task_io_get_oublock(tsk);
+		sig->sum_sched_runtime += tsk->se.sum_exec_runtime;
 		sig = NULL; /* Marker for below. */
 	}
 

commit e05606d3301525aa67b081ad9fccade2b31ab35a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:59 2007 +0200

    sched: clean up the rt priority macros
    
    clean up the rt priority macros, pointed out by Andrew Morton.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6c7699240327..8fd7acd7bbd0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -290,7 +290,7 @@ static void reparent_to_kthreadd(void)
 	/* Set the exit signal to SIGCHLD so we signal init on exit */
 	current->exit_signal = SIGCHLD;
 
-	if (!has_rt_policy(current) && (task_nice(current) < 0))
+	if (task_nice(current) < 0)
 		set_user_nice(current, 0);
 	/* cpus_allowed? */
 	/* rt_priority? */

commit f64f61145a38f7039e4f1c0b50dcc3fbe70ec28e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:58 2007 +0200

    sched: remove sched_exit()
    
    remove sched_exit(): the elaborate dance of us trying to recover
    timeslices given to child tasks never really worked.
    
    CFS does not need it either.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5c8ecbaa19a5..6c7699240327 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -182,7 +182,6 @@ void release_task(struct task_struct * p)
 		zap_leader = (leader->exit_signal == -1);
 	}
 
-	sched_exit(p);
 	write_unlock_irq(&tasklist_lock);
 	proc_flush_task(p);
 	release_thread(p);

commit 778e9a9c3e7193ea9f434f382947155ffb59c755
Author: Alexey Kuznetsov <kuznet@ms2.inr.ac.ru>
Date:   Fri Jun 8 13:47:00 2007 -0700

    pi-futex: fix exit races and locking problems
    
    1. New entries can be added to tsk->pi_state_list after task completed
       exit_pi_state_list(). The result is memory leakage and deadlocks.
    
    2. handle_mm_fault() is called under spinlock. The result is obvious.
    
    3. results in self-inflicted deadlock inside glibc.
       Sometimes futex_lock_pi returns -ESRCH, when it is not expected
       and glibc enters to for(;;) sleep() to simulate deadlock. This problem
       is quite obvious and I think the patch is right. Though it looks like
       each "if" in futex_lock_pi() got some stupid special case "else if". :-)
    
    4. sometimes futex_lock_pi() returns -EDEADLK,
       when nobody has the lock. The reason is also obvious (see comment
       in the patch), but correct fix is far beyond my comprehension.
       I guess someone already saw this, the chunk:
    
                            if (rt_mutex_trylock(&q.pi_state->pi_mutex))
                                    ret = 0;
    
       is obviously from the same opera. But it does not work, because the
       rtmutex is really taken at this point: wake_futex_pi() of previous
       owner reassigned it to us. My fix works. But it looks very stupid.
       I would think about removal of shift of ownership in wake_futex_pi()
       and making all the work in context of process taking lock.
    
    From: Thomas Gleixner <tglx@linutronix.de>
    
    Fix 1) Avoid the tasklist lock variant of the exit race fix by adding
        an additional state transition to the exit code.
    
        This fixes also the issue, when a task with recursive segfaults
        is not able to release the futexes.
    
    Fix 2) Cleanup the lookup_pi_state() failure path and solve the -ESRCH
        problem finally.
    
    Fix 3) Solve the fixup_pi_state_owner() problem which needs to do the fixup
        in the lock protected section by using the in_atomic userspace access
        functions.
    
        This removes also the ugly lock drop / unqueue inside of fixup_pi_state()
    
    Fix 4) Fix a stale lock in the error path of futex_wake_pi()
    
    Added some error checks for verification.
    
    The -EDEADLK problem is solved by the rtmutex fixups.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ulrich Drepper <drepper@redhat.com>
    Cc: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5b888c24e43e..5c8ecbaa19a5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -892,13 +892,29 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (unlikely(tsk->flags & PF_EXITING)) {
 		printk(KERN_ALERT
 			"Fixing recursive fault but reboot is needed!\n");
+		/*
+		 * We can do this unlocked here. The futex code uses
+		 * this flag just to verify whether the pi state
+		 * cleanup has been done or not. In the worst case it
+		 * loops once more. We pretend that the cleanup was
+		 * done as there is no way to return. Either the
+		 * OWNER_DIED bit is set by now or we push the blocked
+		 * task into the wait for ever nirwana as well.
+		 */
+		tsk->flags |= PF_EXITPIDONE;
 		if (tsk->io_context)
 			exit_io_context();
 		set_current_state(TASK_UNINTERRUPTIBLE);
 		schedule();
 	}
 
+	/*
+	 * tsk->flags are checked in the futex code to protect against
+	 * an exiting task cleaning up the robust pi futexes.
+	 */
+	spin_lock_irq(&tsk->pi_lock);
 	tsk->flags |= PF_EXITING;
+	spin_unlock_irq(&tsk->pi_lock);
 
 	if (unlikely(in_atomic()))
 		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
@@ -912,7 +928,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	}
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
- 		hrtimer_cancel(&tsk->signal->real_timer);
+		hrtimer_cancel(&tsk->signal->real_timer);
 		exit_itimers(tsk->signal);
 	}
 	acct_collect(code, group_dead);
@@ -965,6 +981,12 @@ fastcall NORET_TYPE void do_exit(long code)
 	 * Make sure we are holding no locks:
 	 */
 	debug_check_no_locks_held(tsk);
+	/*
+	 * We can do this unlocked here. The futex code uses this flag
+	 * just to verify whether the pi state cleanup has been done
+	 * or not. In the worst case it loops once more.
+	 */
+	tsk->flags |= PF_EXITPIDONE;
 
 	if (tsk->io_context)
 		exit_io_context();

commit 7bb44adef39ad3bda2be40bb34686bc56bd563a5
Author: Roland McGrath <roland@redhat.com>
Date:   Wed May 23 13:57:44 2007 -0700

    recalc_sigpending_tsk fixes
    
    Steve Hawkes discovered a problem where recalc_sigpending_tsk was called in
    do_sigaction but no signal_wake_up call was made, preventing later signals
    from waking up blocked threads with TIF_SIGPENDING already set.
    
    In fact, the few other calls to recalc_sigpending_tsk outside the signals
    code are also subject to this problem in other race conditions.
    
    This change makes recalc_sigpending_tsk private to the signals code.  It
    changes the outside calls, as well as do_sigaction, to use the new
    recalc_sigpending_and_wake instead.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: <Steve.Hawkes@motorola.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c6d14b8008dd..5b888c24e43e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -762,11 +762,8 @@ static void exit_notify(struct task_struct *tsk)
 		read_lock(&tasklist_lock);
 		spin_lock_irq(&tsk->sighand->siglock);
 		for (t = next_thread(tsk); t != tsk; t = next_thread(t))
-			if (!signal_pending(t) && !(t->flags & PF_EXITING)) {
-				recalc_sigpending_tsk(t);
-				if (signal_pending(t))
-					signal_wake_up(t, 0);
-			}
+			if (!signal_pending(t) && !(t->flags & PF_EXITING))
+				recalc_sigpending_and_wake(t);
 		spin_unlock_irq(&tsk->sighand->siglock);
 		read_unlock(&tasklist_lock);
 	}

commit fba2afaaec790dc5ab4ae8827972f342211bbb86
Author: Davide Libenzi <davidel@xmailserver.org>
Date:   Thu May 10 22:23:13 2007 -0700

    signal/timer/event: signalfd core
    
    This patch series implements the new signalfd() system call.
    
    I took part of the original Linus code (and you know how badly it can be
    broken :), and I added even more breakage ;) Signals are fetched from the same
    signal queue used by the process, so signalfd will compete with standard
    kernel delivery in dequeue_signal().  If you want to reliably fetch signals on
    the signalfd file, you need to block them with sigprocmask(SIG_BLOCK).  This
    seems to be working fine on my Dual Opteron machine.  I made a quick test
    program for it:
    
    http://www.xmailserver.org/signafd-test.c
    
    The signalfd() system call implements signal delivery into a file descriptor
    receiver.  The signalfd file descriptor if created with the following API:
    
    int signalfd(int ufd, const sigset_t *mask, size_t masksize);
    
    The "ufd" parameter allows to change an existing signalfd sigmask, w/out going
    to close/create cycle (Linus idea).  Use "ufd" == -1 if you want a brand new
    signalfd file.
    
    The "mask" allows to specify the signal mask of signals that we are interested
    in.  The "masksize" parameter is the size of "mask".
    
    The signalfd fd supports the poll(2) and read(2) system calls.  The poll(2)
    will return POLLIN when signals are available to be dequeued.  As a direct
    consequence of supporting the Linux poll subsystem, the signalfd fd can use
    used together with epoll(2) too.
    
    The read(2) system call will return a "struct signalfd_siginfo" structure in
    the userspace supplied buffer.  The return value is the number of bytes copied
    in the supplied buffer, or -1 in case of error.  The read(2) call can also
    return 0, in case the sighand structure to which the signalfd was attached,
    has been orphaned.  The O_NONBLOCK flag is also supported, and read(2) will
    return -EAGAIN in case no signal is available.
    
    If the size of the buffer passed to read(2) is lower than sizeof(struct
    signalfd_siginfo), -EINVAL is returned.  A read from the signalfd can also
    return -ERESTARTSYS in case a signal hits the process.  The format of the
    struct signalfd_siginfo is, and the valid fields depends of the (->code &
    __SI_MASK) value, in the same way a struct siginfo would:
    
    struct signalfd_siginfo {
            __u32 signo;    /* si_signo */
            __s32 err;      /* si_errno */
            __s32 code;     /* si_code */
            __u32 pid;      /* si_pid */
            __u32 uid;      /* si_uid */
            __s32 fd;       /* si_fd */
            __u32 tid;      /* si_fd */
            __u32 band;     /* si_band */
            __u32 overrun;  /* si_overrun */
            __u32 trapno;   /* si_trapno */
            __s32 status;   /* si_status */
            __s32 svint;    /* si_int */
            __u64 svptr;    /* si_ptr */
            __u64 utime;    /* si_utime */
            __u64 stime;    /* si_stime */
            __u64 addr;     /* si_addr */
    };
    
    [akpm@linux-foundation.org: fix signalfd_copyinfo() on i386]
    Signed-off-by: Davide Libenzi <davidel@xmailserver.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e93691e9b325..c6d14b8008dd 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -24,6 +24,7 @@
 #include <linux/pid_namespace.h>
 #include <linux/ptrace.h>
 #include <linux/profile.h>
+#include <linux/signalfd.h>
 #include <linux/mount.h>
 #include <linux/proc_fs.h>
 #include <linux/kthread.h>
@@ -83,6 +84,14 @@ static void __exit_signal(struct task_struct *tsk)
 	sighand = rcu_dereference(tsk->sighand);
 	spin_lock(&sighand->siglock);
 
+	/*
+	 * Notify that this sighand has been detached. This must
+	 * be called with the tsk->sighand lock held. Also, this
+	 * access tsk->sighand internally, so it must be called
+	 * before tsk->sighand is reset.
+	 */
+	signalfd_detach_locked(tsk);
+
 	posix_cpu_timers_exit(tsk);
 	if (atomic_dec_and_test(&sig->count))
 		posix_cpu_timers_exit_group(tsk);

commit e713d0dab21a68500720e222fa02567fc7dfb14b
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Thu May 10 22:22:58 2007 -0700

    attach_pid() with struct pid parameter
    
    attach_pid() currently takes a pid_t and then uses find_pid() to find the
    corresponding struct pid.  Sometimes we already have the struct pid.  We can
    then skip find_pid() if attach_pid() were to take a struct pid parameter.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: <containers@lists.osdl.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7a5fd77f8fb0..e93691e9b325 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -302,12 +302,12 @@ void __set_special_pids(pid_t session, pid_t pgrp)
 	if (process_session(curr) != session) {
 		detach_pid(curr, PIDTYPE_SID);
 		set_signal_session(curr->signal, session);
-		attach_pid(curr, PIDTYPE_SID, session);
+		attach_pid(curr, PIDTYPE_SID, find_pid(session));
 	}
 	if (process_group(curr) != pgrp) {
 		detach_pid(curr, PIDTYPE_PGID);
 		curr->signal->pgrp = pgrp;
-		attach_pid(curr, PIDTYPE_PGID, pgrp);
+		attach_pid(curr, PIDTYPE_PGID, find_pid(pgrp));
 	}
 }
 

commit 6eaeeaba39e5fa3d52a0bb8de15e995516ae251a
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Thu May 10 22:22:37 2007 -0700

    getrusage(): fill ru_inblock and ru_oublock fields if possible
    
    If CONFIG_TASK_IO_ACCOUNTING is defined, we update io accounting counters for
    each task.
    
    This patch permits reporting of values using the well known getrusage()
    syscall, filling ru_inblock and ru_oublock instead of null values.
    
    As TASK_IO_ACCOUNTING currently counts bytes counts, we approximate blocks
    count doing : nr_blocks = nr_bytes / 512
    
    Example of use :
    ----------------------
    After patch is applied, /usr/bin/time command can now give a good
    approximation of IO that the process had to do.
    
    $ /usr/bin/time grep tototo /usr/include/*
    Command exited with non-zero status 1
    0.00user 0.02system 0:02.11elapsed 1%CPU (0avgtext+0avgdata 0maxresident)k
    24288inputs+0outputs (0major+259minor)pagefaults 0swaps
    
    $ /usr/bin/time dd if=/dev/zero of=/tmp/testfile count=1000
    1000+0 enregistrements lus
    1000+0 enregistrements crits
    512000 octets (512 kB) copis, 0,00326601 seconde, 157 MB/s
    0.00user 0.00system 0:00.00elapsed 80%CPU (0avgtext+0avgdata 0maxresident)k
    0inputs+3000outputs (0major+299minor)pagefaults 0swaps
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b0c6f0c3a2df..7a5fd77f8fb0 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -42,6 +42,7 @@
 #include <linux/audit.h> /* for audit_free() */
 #include <linux/resource.h>
 #include <linux/blkdev.h>
+#include <linux/task_io_accounting_ops.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -113,6 +114,8 @@ static void __exit_signal(struct task_struct *tsk)
 		sig->nvcsw += tsk->nvcsw;
 		sig->nivcsw += tsk->nivcsw;
 		sig->sched_time += tsk->sched_time;
+		sig->inblock += task_io_get_inblock(tsk);
+		sig->oublock += task_io_get_oublock(tsk);
 		sig = NULL; /* Marker for below. */
 	}
 
@@ -1193,6 +1196,12 @@ static int wait_task_zombie(struct task_struct *p, int noreap,
 			p->nvcsw + sig->nvcsw + sig->cnvcsw;
 		psig->cnivcsw +=
 			p->nivcsw + sig->nivcsw + sig->cnivcsw;
+		psig->cinblock +=
+			task_io_get_inblock(p) +
+			sig->inblock + sig->cinblock;
+		psig->coublock +=
+			task_io_get_oublock(p) +
+			sig->oublock + sig->coublock;
 		spin_unlock_irq(&p->parent->sighand->siglock);
 	}
 

commit 10ab825bdef8df510f99c703a5a2d9b13a4e31a5
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed May 9 02:34:37 2007 -0700

    change kernel threads to ignore signals instead of blocking them
    
    Currently kernel threads use sigprocmask(SIG_BLOCK) to protect against
    signals.  This doesn't prevent the signal delivery, this only blocks
    signal_wake_up().  Every "killall -33 kthreadd" means a "struct siginfo"
    leak.
    
    Change kthreadd_setup() to set all handlers to SIG_IGN instead of blocking
    them (make a new helper ignore_signals() for that).  If the kernel thread
    needs some signal, it should use allow_signal() anyway, and in that case it
    should not use CLONE_SIGHAND.
    
    Note that we can't change daemonize() (should die!) in the same way,
    because it can be used along with CLONE_SIGHAND.  This means that
    allow_signal() still should unblock the signal to work correctly with
    daemonize()ed threads.
    
    However, disallow_signal() doesn't block the signal any longer but ignores
    it.
    
    NOTE: with or without this patch the kernel threads are not protected from
    handle_stop_signal(), this seems harmless, but not good.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index bc982cd72743..b0c6f0c3a2df 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -347,7 +347,7 @@ int disallow_signal(int sig)
 		return -EINVAL;
 
 	spin_lock_irq(&current->sighand->siglock);
-	sigaddset(&current->blocked, sig);
+	current->sighand->action[(sig)-1].sa.sa_handler = SIG_IGN;
 	recalc_sigpending();
 	spin_unlock_irq(&current->sighand->siglock);
 	return 0;

commit 49d769d52e16efabd3ad47b7995522fff771371d
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed May 9 02:34:33 2007 -0700

    Change reparent_to_init to reparent_to_kthreadd
    
    When a kernel thread calls daemonize, instead of reparenting the thread to
    init reparent the thread to kthreadd next to the threads created by
    kthread_create.
    
    This is really just a stop gap until daemonize goes away, but it does
    ensure no kernel threads are under init and they are all in one place that
    is easy to find.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f5a7abb621f3..bc982cd72743 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -26,6 +26,7 @@
 #include <linux/profile.h>
 #include <linux/mount.h>
 #include <linux/proc_fs.h>
+#include <linux/kthread.h>
 #include <linux/mempolicy.h>
 #include <linux/taskstats_kern.h>
 #include <linux/delayacct.h>
@@ -254,26 +255,25 @@ static int has_stopped_jobs(struct pid *pgrp)
 }
 
 /**
- * reparent_to_init - Reparent the calling kernel thread to the init task of the pid space that the thread belongs to.
+ * reparent_to_kthreadd - Reparent the calling kernel thread to kthreadd
  *
  * If a kernel thread is launched as a result of a system call, or if
- * it ever exits, it should generally reparent itself to init so that
- * it is correctly cleaned up on exit.
+ * it ever exits, it should generally reparent itself to kthreadd so it
+ * isn't in the way of other processes and is correctly cleaned up on exit.
  *
  * The various task state such as scheduling policy and priority may have
  * been inherited from a user process, so we reset them to sane values here.
  *
- * NOTE that reparent_to_init() gives the caller full capabilities.
+ * NOTE that reparent_to_kthreadd() gives the caller full capabilities.
  */
-static void reparent_to_init(void)
+static void reparent_to_kthreadd(void)
 {
 	write_lock_irq(&tasklist_lock);
 
 	ptrace_unlink(current);
 	/* Reparent to init */
 	remove_parent(current);
-	current->parent = child_reaper(current);
-	current->real_parent = child_reaper(current);
+	current->real_parent = current->parent = kthreadd_task;
 	add_parent(current);
 
 	/* Set the exit signal to SIGCHLD so we signal init on exit */
@@ -400,7 +400,7 @@ void daemonize(const char *name, ...)
 	current->files = init_task.files;
 	atomic_inc(&current->files->count);
 
-	reparent_to_init();
+	reparent_to_kthreadd();
 }
 
 EXPORT_SYMBOL(daemonize);

commit e63340ae6b6205fef26b40a75673d1c9c0c8bb90
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Tue May 8 00:28:08 2007 -0700

    header cleaning: don't include smp_lock.h when not used
    
    Remove includes of <linux/smp_lock.h> where it is not used/needed.
    Suggested by Al Viro.
    
    Builds cleanly on x86_64, i386, alpha, ia64, powerpc, sparc,
    sparc64, and arm (all 59 defconfigs).
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 92369240d91d..f5a7abb621f3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -7,7 +7,6 @@
 #include <linux/mm.h>
 #include <linux/slab.h>
 #include <linux/interrupt.h>
-#include <linux/smp_lock.h>
 #include <linux/module.h>
 #include <linux/capability.h>
 #include <linux/completion.h>

commit 73243284463a761e04d69d22c7516b2be7de096c
Author: Roland McGrath <roland@redhat.com>
Date:   Sun May 6 14:50:20 2007 -0700

    Return EPERM not ECHILD on security_task_wait failure
    
    wait* syscalls return -ECHILD even when an individual PID of a live child
    was requested explicitly, when security_task_wait denies the operation.
    This means that something like a broken SELinux policy can produce an
    unexpected failure that looks just like a bug with wait or ptrace or
    something.
    
    This patch makes do_wait return -EACCES (or other appropriate error returned
    from security_task_wait() instead of -ECHILD if some children were ruled out
    solely because security_task_wait failed.
    
    [jmorris@namei.org: switch error code to EACCES]
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Acked-by: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: James Morris <jmorris@namei.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b55ed4cc9104..92369240d91d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1033,6 +1033,8 @@ asmlinkage void sys_exit_group(int error_code)
 
 static int eligible_child(pid_t pid, int options, struct task_struct *p)
 {
+	int err;
+
 	if (pid > 0) {
 		if (p->pid != pid)
 			return 0;
@@ -1066,8 +1068,9 @@ static int eligible_child(pid_t pid, int options, struct task_struct *p)
 	if (delay_group_leader(p))
 		return 2;
 
-	if (security_task_wait(p))
-		return 0;
+	err = security_task_wait(p);
+	if (err)
+		return err;
 
 	return 1;
 }
@@ -1449,6 +1452,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 	DECLARE_WAITQUEUE(wait, current);
 	struct task_struct *tsk;
 	int flag, retval;
+	int allowed, denied;
 
 	add_wait_queue(&current->signal->wait_chldexit,&wait);
 repeat:
@@ -1457,6 +1461,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 	 * match our criteria, even if we are not able to reap it yet.
 	 */
 	flag = 0;
+	allowed = denied = 0;
 	current->state = TASK_INTERRUPTIBLE;
 	read_lock(&tasklist_lock);
 	tsk = current;
@@ -1472,6 +1477,12 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 			if (!ret)
 				continue;
 
+			if (unlikely(ret < 0)) {
+				denied = ret;
+				continue;
+			}
+			allowed = 1;
+
 			switch (p->state) {
 			case TASK_TRACED:
 				/*
@@ -1570,6 +1581,8 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 		goto repeat;
 	}
 	retval = -ECHILD;
+	if (unlikely(denied) && !allowed)
+		retval = denied;
 end:
 	current->state = TASK_RUNNING;
 	remove_wait_queue(&current->signal->wait_chldexit,&wait);

commit 14e9d5730adfca26452b3a2838a80af6950556f5
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Mar 28 23:38:16 2007 -0600

    [PATCH] pid: Properly detect orphaned process groups in exit_notify
    
    In commit 0475ac0845f9295bc5f69af45f58dff2c104c8d1 when converting the
    orphaned process group handling to use struct pid I made a small
    mistake.  I accidentally replaced an == with a !=.
    
    Besides just being a dumb thing to do apparently this has a bad side
    effect.  The improper orphaned process group detection causes kwin to
    die after a suspend/resume cycle.
    
    I'm amazed this patch has been around as long as it has without anyone
    else noticing something funny going on.
    
    And the following people deserve credit for spotting and helping
    to reproduce this.
    
    Thanks to: Sid Boyce <g3vbv@blueyonder.co.uk>
    Thanks to: "Michael Wu"
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f132349c0325..b55ed4cc9104 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -790,7 +790,7 @@ static void exit_notify(struct task_struct *tsk)
 	
 	pgrp = task_pgrp(tsk);
 	if ((task_pgrp(t) != pgrp) &&
-	    (task_session(t) != task_session(tsk)) &&
+	    (task_session(t) == task_session(tsk)) &&
 	    will_become_orphaned_pgrp(pgrp, tsk) &&
 	    has_stopped_jobs(pgrp)) {
 		__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);

commit 3e7cd6c413c9e6fbb5e1ee2acdadb4ababd2d474
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Feb 12 00:52:58 2007 -0800

    [PATCH] pid: replace is_orphaned_pgrp with is_current_pgrp_orphaned
    
    Every call to is_orphaned_pgrp passed in process_group(current) which is racy
    with respect to another thread changing our process group.  It didn't bite us
    because we were dealing with integers and the worse we would get would be a
    stale answer.
    
    In switching the checks to use struct pid to be a little more efficient and
    prepare the way for pid namespaces this race became apparent.
    
    So I simplified the calls to the more specialized is_current_pgrp_orphaned so
    I didn't have to worry about making logic changes to avoid the race.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 407b80aaefda..f132349c0325 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -229,12 +229,12 @@ static int will_become_orphaned_pgrp(struct pid *pgrp, struct task_struct *ignor
 	return ret;	/* (sighing) "Often!" */
 }
 
-int is_orphaned_pgrp(int pgrp)
+int is_current_pgrp_orphaned(void)
 {
 	int retval;
 
 	read_lock(&tasklist_lock);
-	retval = will_become_orphaned_pgrp(find_pid(pgrp), NULL);
+	retval = will_become_orphaned_pgrp(task_pgrp(current), NULL);
 	read_unlock(&tasklist_lock);
 
 	return retval;

commit 0475ac0845f9295bc5f69af45f58dff2c104c8d1
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Feb 12 00:52:57 2007 -0800

    [PATCH] pid: use struct pid for talking about process groups in exitc
    
    Modify has_stopped_jobs and will_become_orphan_pgrp to use struct pid based
    process groups.  This reduces the number of hash tables looks ups and paves
    the way for multiple pid spaces.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3ac6a7a6f857..407b80aaefda 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -210,22 +210,22 @@ struct pid *session_of_pgrp(struct pid *pgrp)
  *
  * "I ask you, have you ever known what it is to be an orphan?"
  */
-static int will_become_orphaned_pgrp(int pgrp, struct task_struct *ignored_task)
+static int will_become_orphaned_pgrp(struct pid *pgrp, struct task_struct *ignored_task)
 {
 	struct task_struct *p;
 	int ret = 1;
 
-	do_each_task_pid(pgrp, PIDTYPE_PGID, p) {
+	do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
 		if (p == ignored_task
 				|| p->exit_state
 				|| is_init(p->real_parent))
 			continue;
-		if (process_group(p->real_parent) != pgrp &&
-		    process_session(p->real_parent) == process_session(p)) {
+		if (task_pgrp(p->real_parent) != pgrp &&
+		    task_session(p->real_parent) == task_session(p)) {
 			ret = 0;
 			break;
 		}
-	} while_each_task_pid(pgrp, PIDTYPE_PGID, p);
+	} while_each_pid_task(pgrp, PIDTYPE_PGID, p);
 	return ret;	/* (sighing) "Often!" */
 }
 
@@ -234,23 +234,23 @@ int is_orphaned_pgrp(int pgrp)
 	int retval;
 
 	read_lock(&tasklist_lock);
-	retval = will_become_orphaned_pgrp(pgrp, NULL);
+	retval = will_become_orphaned_pgrp(find_pid(pgrp), NULL);
 	read_unlock(&tasklist_lock);
 
 	return retval;
 }
 
-static int has_stopped_jobs(int pgrp)
+static int has_stopped_jobs(struct pid *pgrp)
 {
 	int retval = 0;
 	struct task_struct *p;
 
-	do_each_task_pid(pgrp, PIDTYPE_PGID, p) {
+	do_each_pid_task(pgrp, PIDTYPE_PGID, p) {
 		if (p->state != TASK_STOPPED)
 			continue;
 		retval = 1;
 		break;
-	} while_each_task_pid(pgrp, PIDTYPE_PGID, p);
+	} while_each_pid_task(pgrp, PIDTYPE_PGID, p);
 	return retval;
 }
 
@@ -648,14 +648,14 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 	 * than we are, and it was the only connection
 	 * outside, so the child pgrp is now orphaned.
 	 */
-	if ((process_group(p) != process_group(father)) &&
-	    (process_session(p) == process_session(father))) {
-		int pgrp = process_group(p);
+	if ((task_pgrp(p) != task_pgrp(father)) &&
+	    (task_session(p) == task_session(father))) {
+		struct pid *pgrp = task_pgrp(p);
 
 		if (will_become_orphaned_pgrp(pgrp, NULL) &&
 		    has_stopped_jobs(pgrp)) {
-			__kill_pg_info(SIGHUP, SEND_SIG_PRIV, pgrp);
-			__kill_pg_info(SIGCONT, SEND_SIG_PRIV, pgrp);
+			__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);
+			__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);
 		}
 	}
 }
@@ -735,6 +735,7 @@ static void exit_notify(struct task_struct *tsk)
 	int state;
 	struct task_struct *t;
 	struct list_head ptrace_dead, *_p, *_n;
+	struct pid *pgrp;
 
 	if (signal_pending(tsk) && !(tsk->signal->flags & SIGNAL_GROUP_EXIT)
 	    && !thread_group_empty(tsk)) {
@@ -787,12 +788,13 @@ static void exit_notify(struct task_struct *tsk)
 	 
 	t = tsk->real_parent;
 	
-	if ((process_group(t) != process_group(tsk)) &&
-	    (process_session(t) == process_session(tsk)) &&
-	    will_become_orphaned_pgrp(process_group(tsk), tsk) &&
-	    has_stopped_jobs(process_group(tsk))) {
-		__kill_pg_info(SIGHUP, SEND_SIG_PRIV, process_group(tsk));
-		__kill_pg_info(SIGCONT, SEND_SIG_PRIV, process_group(tsk));
+	pgrp = task_pgrp(tsk);
+	if ((task_pgrp(t) != pgrp) &&
+	    (task_session(t) != task_session(tsk)) &&
+	    will_become_orphaned_pgrp(pgrp, tsk) &&
+	    has_stopped_jobs(pgrp)) {
+		__kill_pgrp_info(SIGHUP, SEND_SIG_PRIV, pgrp);
+		__kill_pgrp_info(SIGCONT, SEND_SIG_PRIV, pgrp);
 	}
 
 	/* Let father know we died 

commit 04a2e6a5cbf84e85fe86de0a18f6509b147e1d89
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Feb 12 00:52:56 2007 -0800

    [PATCH] pid: make session_of_pgrp use struct pid instead of pid_t
    
    To properly implement a pid namespace I need to deal exclusively in terms of
    struct pid, because pid_t values become ambiguous.
    
    To this end session_of_pgrp is transformed to take and return a struct pid
    pointer.  To avoid the need to worry about reference counting I now require my
    caller to hold the appropriate locks.  Leaving callers repsonsible for
    increasing the reference count if they need access to the result outside of
    the locks.
    
    Since session_of_pgrp currently only has one caller and that caller simply
    uses only test the result for equality with another process group, the locking
    change means I don't actually have to acquire the tasklist_lock at all.
    
    tiocspgrp is also modified to take and release the lock.  The logic there is a
    little more complicated but nothing I won't need when I convert pgrp of a tty
    to a struct pid pointer.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 14f17033f563..3ac6a7a6f857 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -185,21 +185,19 @@ void release_task(struct task_struct * p)
  * This checks not only the pgrp, but falls back on the pid if no
  * satisfactory pgrp is found. I dunno - gdb doesn't work correctly
  * without this...
+ *
+ * The caller must hold rcu lock or the tasklist lock.
  */
-int session_of_pgrp(int pgrp)
+struct pid *session_of_pgrp(struct pid *pgrp)
 {
 	struct task_struct *p;
-	int sid = 0;
-
-	read_lock(&tasklist_lock);
+	struct pid *sid = NULL;
 
-	p = find_task_by_pid_type(PIDTYPE_PGID, pgrp);
+	p = pid_task(pgrp, PIDTYPE_PGID);
 	if (p == NULL)
-		p = find_task_by_pid(pgrp);
+		p = pid_task(pgrp, PIDTYPE_PID);
 	if (p != NULL)
-		sid = process_session(p);
-
-	read_unlock(&tasklist_lock);
+		sid = task_session(p);
 
 	return sid;
 }

commit 944be0b224724fcbf63c3a3fe3a5478c325a6547
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 12 00:52:26 2007 -0800

    [PATCH] close_files(): add scheduling point
    
    close_files() can sometimes take long enough to trigger the soft lockup
    detector.
    
    Cc: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index bc71fdfcd8a7..14f17033f563 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -430,8 +430,10 @@ static void close_files(struct files_struct * files)
 		while (set) {
 			if (set & 1) {
 				struct file * file = xchg(&fdt->fd[i], NULL);
-				if (file)
+				if (file) {
 					filp_close(file, files);
+					cond_resched();
+				}
 			}
 			i++;
 			set >>= 1;

commit 72fd4a35a824331d7a0f4168d7576502d95d34b3
Author: Robert P. J. Day <rpjday@mindspring.com>
Date:   Sat Feb 10 01:45:59 2007 -0800

    [PATCH] Numerous fixes to kernel-doc info in source files.
    
    A variety of (mostly) innocuous fixes to the embedded kernel-doc content in
    source files, including:
    
      * make multi-line initial descriptions single line
      * denote some function names, constants and structs as such
      * change erroneous opening '/*' to '/**' in a few places
      * reword some text for clarity
    
    Signed-off-by: Robert P. J. Day <rpjday@mindspring.com>
    Cc: "Randy.Dunlap" <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index fec12eb12471..bc71fdfcd8a7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -257,8 +257,7 @@ static int has_stopped_jobs(int pgrp)
 }
 
 /**
- * reparent_to_init - Reparent the calling kernel thread to the init task
- * of the pid space that the thread belongs to.
+ * reparent_to_init - Reparent the calling kernel thread to the init task of the pid space that the thread belongs to.
  *
  * If a kernel thread is launched as a result of a system call, or if
  * it ever exits, it should generally reparent itself to init so that

commit 0f2452855d86901ba3766826ccb5606ea4e15ab9
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Tue Jan 30 15:28:23 2007 -0600

    [PATCH] namespaces: fix task exit disaster
    
    This is based on a patch by Eric W.  Biederman, who pointed out that pid
    namespaces are still fake, and we only have one ever active.
    
    So for the time being, we can modify any code which could access
    tsk->nsproxy->pid_ns during task exit to just use &init_pid_ns instead,
    and move the exit_task_namespaces call in do_exit() back above
    exit_notify(), so that an exiting nfs server has a valid tsk->sighand to
    work with.
    
    Long term, pulling pid_ns out of nsproxy might be the cleanest solution.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    
    [ Eric's patch fixed to take care of free_pid() too ]
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 35401720635b..fec12eb12471 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -938,8 +938,8 @@ fastcall NORET_TYPE void do_exit(long code)
 
 	tsk->exit_code = code;
 	proc_exit_connector(tsk);
-	exit_notify(tsk);
 	exit_task_namespaces(tsk);
+	exit_notify(tsk);
 #ifdef CONFIG_NUMA
 	mpol_free(tsk->mempolicy);
 	tsk->mempolicy = NULL;

commit 444f378b237a0f728f5c4aba752c08d13c209344
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Tue Jan 30 13:35:18 2007 -0800

    Revert "[PATCH] namespaces: fix exit race by splitting exit"
    
    This reverts commit 7a238fcba0629b6f2edbcd37458bae56fcf36be5 in
    preparation for a better and simpler fix proposed by Eric Biederman
    (and fixed up by Serge Hallyn)
    
    Acked-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a5bf5329ff97..35401720635b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -396,7 +396,7 @@ void daemonize(const char *name, ...)
 	current->fs = fs;
 	atomic_inc(&fs->count);
 
-	put_and_finalize_nsproxy(current->nsproxy);
+	exit_task_namespaces(current);
 	current->nsproxy = init_task.nsproxy;
 	get_task_namespaces(current);
 
@@ -853,7 +853,6 @@ static void exit_notify(struct task_struct *tsk)
 fastcall NORET_TYPE void do_exit(long code)
 {
 	struct task_struct *tsk = current;
-	struct nsproxy *ns;
 	int group_dead;
 
 	profile_task_exit(tsk);
@@ -939,9 +938,8 @@ fastcall NORET_TYPE void do_exit(long code)
 
 	tsk->exit_code = code;
 	proc_exit_connector(tsk);
-	ns = preexit_task_namespaces(tsk);
 	exit_notify(tsk);
-	exit_task_namespaces(tsk, ns);
+	exit_task_namespaces(tsk);
 #ifdef CONFIG_NUMA
 	mpol_free(tsk->mempolicy);
 	tsk->mempolicy = NULL;

commit 7a238fcba0629b6f2edbcd37458bae56fcf36be5
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Jan 29 13:19:40 2007 -0800

    [PATCH] namespaces: fix exit race by splitting exit
    
    Fix exit race by splitting the nsproxy putting into two pieces.  First
    piece reduces the nsproxy refcount.  If we dropped the last reference, then
    it puts the mnt_ns, and returns the nsproxy as a hint to the caller.  Else
    it returns NULL.  The second piece of exiting task namespaces sets
    tsk->nsproxy to NULL, and drops the references to other namespaces and
    frees the nsproxy only if an nsproxy was passed in.
    
    A little awkward and should probably be reworked, but hopefully it fixes
    the NFS oops.
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Daniel Hokka Zakrisson <daniel@hozac.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 35401720635b..a5bf5329ff97 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -396,7 +396,7 @@ void daemonize(const char *name, ...)
 	current->fs = fs;
 	atomic_inc(&fs->count);
 
-	exit_task_namespaces(current);
+	put_and_finalize_nsproxy(current->nsproxy);
 	current->nsproxy = init_task.nsproxy;
 	get_task_namespaces(current);
 
@@ -853,6 +853,7 @@ static void exit_notify(struct task_struct *tsk)
 fastcall NORET_TYPE void do_exit(long code)
 {
 	struct task_struct *tsk = current;
+	struct nsproxy *ns;
 	int group_dead;
 
 	profile_task_exit(tsk);
@@ -938,8 +939,9 @@ fastcall NORET_TYPE void do_exit(long code)
 
 	tsk->exit_code = code;
 	proc_exit_connector(tsk);
+	ns = preexit_task_namespaces(tsk);
 	exit_notify(tsk);
-	exit_task_namespaces(tsk);
+	exit_task_namespaces(tsk, ns);
 #ifdef CONFIG_NUMA
 	mpol_free(tsk->mempolicy);
 	tsk->mempolicy = NULL;

commit 241ceee0b442c69226fb882d61d9b9785743898f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Dec 24 23:30:44 2006 +0300

    [PATCH] restore ->pdeath_signal behaviour
    
    Commit b2b2cbc4b2a2f389442549399a993a8306420baf introduced a user-
    visible change: ->pdeath_signal is sent only when the entire thread
    group exits.
    
    While this change is imho good, it may break things.  So restore the
    old behaviour for now.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    To: Albert Cahalan <acahalan@gmail.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Andrew Morton <akpm@osdl.org>
    Cc: Linus Torvalds <torvalds@osdl.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Qi Yong <qiyong@fc-cn.com>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 46cf6b681460..35401720635b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -597,6 +597,10 @@ choose_new_parent(struct task_struct *p, struct task_struct *reaper)
 static void
 reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 {
+	if (p->pdeath_signal)
+		/* We already hold the tasklist_lock here.  */
+		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
+
 	/* Move the child from its dying parent to the new one.  */
 	if (unlikely(traced)) {
 		/* Preserve ptrace links if someone else is tracing this child.  */
@@ -631,10 +635,6 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 	/* We don't want people slaying init.  */
 	if (p->exit_signal != -1)
 		p->exit_signal = SIGCHLD;
-		
-	if (p->pdeath_signal)
-		/* We already hold the tasklist_lock here.  */
-		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
 
 	/* If we'd notified the old parent about this child's death,
 	 * also notify the new parent.

commit b2b2cbc4b2a2f389442549399a993a8306420baf
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Dec 21 21:28:40 2006 -0700

    [PATCH] Fix reparenting to the same thread group. (take 2)
    
    This patch fixes the case when we reparent to a different thread in the
    same thread group.  This modifies the code so that we do not send
    signals and do not change the signal to send to SIGCHLD unless we have
    change the thread group of our parents.  It also suppresses sending
    pdeath_sig in this cas as well since the result of geppid doesn't
    change.
    
    Thanks to Oleg for spotting my bug of only fixing this for non-ptraced
    tasks.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Albert Cahalan <acahalan@gmail.com>
    Cc: Andrew Morton <akpm@osdl.org>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Coywolf Qi Hunt <qiyong@fc-cn.com>
    Acked-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 85917c2bf065..46cf6b681460 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -597,14 +597,6 @@ choose_new_parent(struct task_struct *p, struct task_struct *reaper)
 static void
 reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 {
-	/* We don't want people slaying init.  */
-	if (p->exit_signal != -1)
-		p->exit_signal = SIGCHLD;
-
-	if (p->pdeath_signal)
-		/* We already hold the tasklist_lock here.  */
-		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
-
 	/* Move the child from its dying parent to the new one.  */
 	if (unlikely(traced)) {
 		/* Preserve ptrace links if someone else is tracing this child.  */
@@ -620,13 +612,7 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 		p->parent = p->real_parent;
 		add_parent(p);
 
-		/* If we'd notified the old parent about this child's death,
-		 * also notify the new parent.
-		 */
-		if (p->exit_state == EXIT_ZOMBIE && p->exit_signal != -1 &&
-		    thread_group_empty(p))
-			do_notify_parent(p, p->exit_signal);
-		else if (p->state == TASK_TRACED) {
+		if (p->state == TASK_TRACED) {
 			/*
 			 * If it was at a trace stop, turn it into
 			 * a normal stop since it's no longer being
@@ -636,6 +622,27 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 		}
 	}
 
+	/* If this is a threaded reparent there is no need to
+	 * notify anyone anything has happened.
+	 */
+	if (p->real_parent->group_leader == father->group_leader)
+		return;
+
+	/* We don't want people slaying init.  */
+	if (p->exit_signal != -1)
+		p->exit_signal = SIGCHLD;
+		
+	if (p->pdeath_signal)
+		/* We already hold the tasklist_lock here.  */
+		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
+
+	/* If we'd notified the old parent about this child's death,
+	 * also notify the new parent.
+	 */
+	if (!traced && p->exit_state == EXIT_ZOMBIE &&
+	    p->exit_signal != -1 && thread_group_empty(p))
+		do_notify_parent(p, p->exit_signal);
+
 	/*
 	 * process group orphan check
 	 * Case ii: Our child is in a different pgrp

commit 01b2d93ca4c495f056471189ac6c4e6ac4cbbccb
Author: Vadim Lobanov <vlobanov@speakeasy.net>
Date:   Fri Dec 22 01:10:43 2006 -0800

    [PATCH] fdtable: Provide free_fdtable() wrapper
    
    Christoph Hellwig has expressed concerns that the recent fdtable changes
    expose the details of the RCU methodology used to release no-longer-used
    fdtable structures to the rest of the kernel.  The trivial patch below
    addresses these concerns by introducing the appropriate free_fdtable()
    calls, which simply wrap the release RCU usage.  Since free_fdtable() is a
    one-liner, it makes sense to promote it to an inline helper.
    
    Signed-off-by: Vadim Lobanov <vlobanov@speakeasy.net>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 122fadb972fc..85917c2bf065 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -468,7 +468,7 @@ void fastcall put_files_struct(struct files_struct *files)
 		fdt = files_fdtable(files);
 		if (fdt != &files->fdtab)
 			kmem_cache_free(files_cachep, files);
-		call_rcu(&fdt->rcu, free_fdtable_rcu);
+		free_fdtable(fdt);
 	}
 }
 

commit 4fd45812cbe875a620c86a096a5d46c742694b7e
Author: Vadim Lobanov <vlobanov@speakeasy.net>
Date:   Sun Dec 10 02:21:17 2006 -0800

    [PATCH] fdtable: Remove the free_files field
    
    An fdtable can either be embedded inside a files_struct or standalone (after
    being expanded).  When an fdtable is being discarded after all RCU references
    to it have expired, we must either free it directly, in the standalone case,
    or free the files_struct it is contained within, in the embedded case.
    
    Currently the free_files field controls this behavior, but we can get rid of
    it entirely, as all the necessary information is already recorded.  We can
    distinguish embedded and standalone fdtables using max_fds, and if it is
    embedded we can divine the relevant files_struct using container_of().
    
    Signed-off-by: Vadim Lobanov <vlobanov@speakeasy.net>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5f77e76b4f97..122fadb972fc 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -466,11 +466,9 @@ void fastcall put_files_struct(struct files_struct *files)
 		 * you can free files immediately.
 		 */
 		fdt = files_fdtable(files);
-		if (fdt == &files->fdtab)
-			fdt->free_files = files;
-		else
+		if (fdt != &files->fdtab)
 			kmem_cache_free(files_cachep, files);
-		free_fdtable(fdt);
+		call_rcu(&fdt->rcu, free_fdtable_rcu);
 	}
 }
 

commit bbea9f69668a3d0cf9feba15a724cd02896f8675
Author: Vadim Lobanov <vlobanov@speakeasy.net>
Date:   Sun Dec 10 02:21:12 2006 -0800

    [PATCH] fdtable: Make fdarray and fdsets equal in size
    
    Currently, each fdtable supports three dynamically-sized arrays of data: the
    fdarray and two fdsets.  The code allows the number of fds supported by the
    fdarray (fdtable->max_fds) to differ from the number of fds supported by each
    of the fdsets (fdtable->max_fdset).
    
    In practice, it is wasteful for these two sizes to differ: whenever we hit a
    limit on the smaller-capacity structure, we will reallocate the entire fdtable
    and all the dynamic arrays within it, so any delta in the memory used by the
    larger-capacity structure will never be touched at all.
    
    Rather than hogging this excess, we shouldn't even allocate it in the first
    place, and keep the capacities of the fdarray and the fdsets equal.  This
    patch removes fdtable->max_fdset.  As an added bonus, most of the supporting
    code becomes simpler.
    
    Signed-off-by: Vadim Lobanov <vlobanov@speakeasy.net>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 03e64fe4a14a..5f77e76b4f97 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -425,7 +425,7 @@ static void close_files(struct files_struct * files)
 	for (;;) {
 		unsigned long set;
 		i = j * __NFDBITS;
-		if (i >= fdt->max_fdset || i >= fdt->max_fds)
+		if (i >= fdt->max_fds)
 			break;
 		set = fdt->open_fds->fds_bits[j++];
 		while (set) {

commit 62dfb5541a025b47df9405ff0219c7829a97d83b
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Dec 8 02:38:03 2006 -0800

    [PATCH] session_of_pgrp: kill unnecessary do_each_task_pid(PIDTYPE_PGID)
    
    All members of the process group have the same sid and it can't be == 0.
    
    NOTE: this code (and a similar one in sys_setpgid) was needed because it
    was possibe to have ->session == 0. It's not possible any longer since
    
            [PATCH] pidhash: don't use zero pids
            Commit: c7c6464117a02b0d54feb4ebeca4db70fa493678
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index fd0e067952ab..03e64fe4a14a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -189,21 +189,18 @@ void release_task(struct task_struct * p)
 int session_of_pgrp(int pgrp)
 {
 	struct task_struct *p;
-	int sid = -1;
+	int sid = 0;
 
 	read_lock(&tasklist_lock);
-	do_each_task_pid(pgrp, PIDTYPE_PGID, p) {
-		if (process_session(p) > 0) {
-			sid = process_session(p);
-			goto out;
-		}
-	} while_each_task_pid(pgrp, PIDTYPE_PGID, p);
-	p = find_task_by_pid(pgrp);
-	if (p)
+
+	p = find_task_by_pid_type(PIDTYPE_PGID, pgrp);
+	if (p == NULL)
+		p = find_task_by_pid(pgrp);
+	if (p != NULL)
 		sid = process_session(p);
-out:
+
 	read_unlock(&tasklist_lock);
-	
+
 	return sid;
 }
 

commit 84d737866e2babdeab0c6b18ea155c6a649663b8
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Fri Dec 8 02:38:01 2006 -0800

    [PATCH] add child reaper to pid_namespace
    
    Add a per pid_namespace child-reaper.  This is needed so processes are reaped
    within the same pid space and do not spill over to the parent pid space.  Its
    also needed so containers preserve existing semantic that pid == 1 would reap
    orphaned children.
    
    This is based on Eric Biederman's patch: http://lkml.org/lkml/2006/2/6/285
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 28d9feedfd27..fd0e067952ab 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -22,6 +22,7 @@
 #include <linux/file.h>
 #include <linux/binfmts.h>
 #include <linux/nsproxy.h>
+#include <linux/pid_namespace.h>
 #include <linux/ptrace.h>
 #include <linux/profile.h>
 #include <linux/mount.h>
@@ -48,7 +49,6 @@
 #include <asm/mmu_context.h>
 
 extern void sem_exit (void);
-extern struct task_struct *child_reaper;
 
 static void exit_mm(struct task_struct * tsk);
 
@@ -260,7 +260,8 @@ static int has_stopped_jobs(int pgrp)
 }
 
 /**
- * reparent_to_init - Reparent the calling kernel thread to the init task.
+ * reparent_to_init - Reparent the calling kernel thread to the init task
+ * of the pid space that the thread belongs to.
  *
  * If a kernel thread is launched as a result of a system call, or if
  * it ever exits, it should generally reparent itself to init so that
@@ -278,8 +279,8 @@ static void reparent_to_init(void)
 	ptrace_unlink(current);
 	/* Reparent to init */
 	remove_parent(current);
-	current->parent = child_reaper;
-	current->real_parent = child_reaper;
+	current->parent = child_reaper(current);
+	current->real_parent = child_reaper(current);
 	add_parent(current);
 
 	/* Set the exit signal to SIGCHLD so we signal init on exit */
@@ -662,7 +663,8 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
  * When we die, we re-parent all our children.
  * Try to give them to another thread in our thread
  * group, and if no such member exists, give it to
- * the global child reaper process (ie "init")
+ * the child reaper process (ie "init") in our pid
+ * space.
  */
 static void
 forget_original_parent(struct task_struct *father, struct list_head *to_release)
@@ -673,7 +675,7 @@ forget_original_parent(struct task_struct *father, struct list_head *to_release)
 	do {
 		reaper = next_thread(reaper);
 		if (reaper == father) {
-			reaper = child_reaper;
+			reaper = child_reaper(father);
 			break;
 		}
 	} while (reaper->exit_state);
@@ -859,8 +861,13 @@ fastcall NORET_TYPE void do_exit(long code)
 		panic("Aiee, killing interrupt handler!");
 	if (unlikely(!tsk->pid))
 		panic("Attempted to kill the idle task!");
-	if (unlikely(tsk == child_reaper))
-		panic("Attempted to kill init!");
+	if (unlikely(tsk == child_reaper(tsk))) {
+		if (tsk->nsproxy->pid_ns != &init_pid_ns)
+			tsk->nsproxy->pid_ns->child_reaper = init_pid_ns.child_reaper;
+		else
+			panic("Attempted to kill init!");
+	}
+
 
 	if (unlikely(current->ptrace & PT_TRACE_EXIT)) {
 		current->ptrace_message = code;

commit 6b3286ed1169d74fea401367d6d4d6c6ec758a81
Author: Kirill Korotaev <dev@sw.ru>
Date:   Fri Dec 8 02:37:56 2006 -0800

    [PATCH] rename struct namespace to struct mnt_namespace
    
    Rename 'struct namespace' to 'struct mnt_namespace' to avoid confusion with
    other namespaces being developped for the containers : pid, uts, ipc, etc.
    'namespace' variables and attributes are also renamed to 'mnt_ns'
    
    Signed-off-by: Kirill Korotaev <dev@sw.ru>
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6267a6cc6113..28d9feedfd27 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -13,7 +13,7 @@
 #include <linux/completion.h>
 #include <linux/personality.h>
 #include <linux/tty.h>
-#include <linux/namespace.h>
+#include <linux/mnt_namespace.h>
 #include <linux/key.h>
 #include <linux/security.h>
 #include <linux/cpu.h>

commit 1ec320afdc9552c92191d5f89fcd1ebe588334ca
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Fri Dec 8 02:37:55 2006 -0800

    [PATCH] add process_session() helper routine: deprecate old field
    
    Add an anonymous union and ((deprecated)) to catch direct usage of the
    session field.
    
    [akpm@osdl.org: fix various missed conversions]
    [jdike@addtoit.com: fix UML bug]
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8d289bfc13d1..6267a6cc6113 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -304,7 +304,7 @@ void __set_special_pids(pid_t session, pid_t pgrp)
 
 	if (process_session(curr) != session) {
 		detach_pid(curr, PIDTYPE_SID);
-		curr->signal->session = session;
+		set_signal_session(curr->signal, session);
 		attach_pid(curr, PIDTYPE_SID, session);
 	}
 	if (process_group(curr) != pgrp) {

commit 937949d9edbf4049bd41af6c9f92c26280584564
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Fri Dec 8 02:37:54 2006 -0800

    [PATCH] add process_session() helper routine
    
    Replace occurences of task->signal->session by a new process_session() helper
    routine.
    
    It will be useful for pid namespaces to abstract the session pid number.
    
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index fa0495e167e9..8d289bfc13d1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -193,14 +193,14 @@ int session_of_pgrp(int pgrp)
 
 	read_lock(&tasklist_lock);
 	do_each_task_pid(pgrp, PIDTYPE_PGID, p) {
-		if (p->signal->session > 0) {
-			sid = p->signal->session;
+		if (process_session(p) > 0) {
+			sid = process_session(p);
 			goto out;
 		}
 	} while_each_task_pid(pgrp, PIDTYPE_PGID, p);
 	p = find_task_by_pid(pgrp);
 	if (p)
-		sid = p->signal->session;
+		sid = process_session(p);
 out:
 	read_unlock(&tasklist_lock);
 	
@@ -225,8 +225,8 @@ static int will_become_orphaned_pgrp(int pgrp, struct task_struct *ignored_task)
 				|| p->exit_state
 				|| is_init(p->real_parent))
 			continue;
-		if (process_group(p->real_parent) != pgrp
-			    && p->real_parent->signal->session == p->signal->session) {
+		if (process_group(p->real_parent) != pgrp &&
+		    process_session(p->real_parent) == process_session(p)) {
 			ret = 0;
 			break;
 		}
@@ -302,7 +302,7 @@ void __set_special_pids(pid_t session, pid_t pgrp)
 {
 	struct task_struct *curr = current->group_leader;
 
-	if (curr->signal->session != session) {
+	if (process_session(curr) != session) {
 		detach_pid(curr, PIDTYPE_SID);
 		curr->signal->session = session;
 		attach_pid(curr, PIDTYPE_SID, session);
@@ -647,10 +647,11 @@ reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 	 * outside, so the child pgrp is now orphaned.
 	 */
 	if ((process_group(p) != process_group(father)) &&
-	    (p->signal->session == father->signal->session)) {
+	    (process_session(p) == process_session(father))) {
 		int pgrp = process_group(p);
 
-		if (will_become_orphaned_pgrp(pgrp, NULL) && has_stopped_jobs(pgrp)) {
+		if (will_become_orphaned_pgrp(pgrp, NULL) &&
+		    has_stopped_jobs(pgrp)) {
 			__kill_pg_info(SIGHUP, SEND_SIG_PRIV, pgrp);
 			__kill_pg_info(SIGCONT, SEND_SIG_PRIV, pgrp);
 		}
@@ -784,7 +785,7 @@ static void exit_notify(struct task_struct *tsk)
 	t = tsk->real_parent;
 	
 	if ((process_group(t) != process_group(tsk)) &&
-	    (t->signal->session == tsk->signal->session) &&
+	    (process_session(t) == process_session(tsk)) &&
 	    will_become_orphaned_pgrp(process_group(tsk), tsk) &&
 	    has_stopped_jobs(process_group(tsk))) {
 		__kill_pg_info(SIGHUP, SEND_SIG_PRIV, process_group(tsk));

commit ae424ae4b5bcd820ad6ee6f0b986c4e14ed4d6cf
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Dec 8 02:36:08 2006 -0800

    [PATCH] make set_special_pids() static
    
    Make set_special_pids() static, the only caller is daemonize().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index fa235779b6a3..fa0495e167e9 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -314,7 +314,7 @@ void __set_special_pids(pid_t session, pid_t pgrp)
 	}
 }
 
-void set_special_pids(pid_t session, pid_t pgrp)
+static void set_special_pids(pid_t session, pid_t pgrp)
 {
 	write_lock_irq(&tasklist_lock);
 	__set_special_pids(session, pgrp);

commit 24ec839c431eb79bb8f6abc00c4e1eb3b8c4d517
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Dec 8 02:36:04 2006 -0800

    [PATCH] tty: ->signal->tty locking
    
    Fix the locking of signal->tty.
    
    Use ->sighand->siglock to protect ->signal->tty; this lock is already used
    by most other members of ->signal/->sighand.  And unless we are 'current'
    or the tasklist_lock is held we need ->siglock to access ->signal anyway.
    
    (NOTE: sys_unshare() is broken wrt ->sighand locking rules)
    
    Note that tty_mutex is held over tty destruction, so while holding
    tty_mutex any tty pointer remains valid.  Otherwise the lifetime of ttys
    are governed by their open file handles.  This leaves some holes for tty
    access from signal->tty (or any other non file related tty access).
    
    It solves the tty SLAB scribbles we were seeing.
    
    (NOTE: the change from group_send_sig_info to __group_send_sig_info needs to
           be examined by someone familiar with the security framework, I think
           it is safe given the SEND_SIG_PRIV from other __group_send_sig_info
           invocations)
    
    [schwidefsky@de.ibm.com: 3270 fix]
    [akpm@osdl.org: various post-viro fixes]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Alan Cox <alan@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Jan Kara <jack@ucw.cz>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4e3f919edc48..fa235779b6a3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -384,9 +384,7 @@ void daemonize(const char *name, ...)
 	exit_mm(current);
 
 	set_special_pids(1, 1);
-	mutex_lock(&tty_mutex);
-	current->signal->tty = NULL;
-	mutex_unlock(&tty_mutex);
+	proc_clear_tty(current);
 
 	/* Block and flush all signals */
 	sigfillset(&blocked);

commit 115085ea0794c0f339be8f9d25505c7f9861d824
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Wed Dec 6 20:36:51 2006 -0800

    [PATCH] taskstats: cleanup do_exit() path
    
    do_exit:
            taskstats_exit_alloc()
            ...
            taskstats_exit_send()
            taskstats_exit_free()
    
    I think this is not good, let it be a single function exported to the core
    kernel, taskstats_exit(), which does alloc + send + free itself.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 06de6c4e8ca3..4e3f919edc48 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -850,9 +850,7 @@ static void exit_notify(struct task_struct *tsk)
 fastcall NORET_TYPE void do_exit(long code)
 {
 	struct task_struct *tsk = current;
-	struct taskstats *tidstats;
 	int group_dead;
-	unsigned int mycpu;
 
 	profile_task_exit(tsk);
 
@@ -890,8 +888,6 @@ fastcall NORET_TYPE void do_exit(long code)
 				current->comm, current->pid,
 				preempt_count());
 
-	taskstats_exit_alloc(&tidstats, &mycpu);
-
 	acct_update_integrals(tsk);
 	if (tsk->mm) {
 		update_hiwater_rss(tsk->mm);
@@ -911,8 +907,8 @@ fastcall NORET_TYPE void do_exit(long code)
 #endif
 	if (unlikely(tsk->audit_context))
 		audit_free(tsk);
-	taskstats_exit_send(tsk, tidstats, group_dead, mycpu);
-	taskstats_exit_free(tidstats);
+
+	taskstats_exit(tsk, group_dead);
 
 	exit_mm(tsk);
 

commit 093a8e8aecd77b2799934996a55a6838e1e2b8f3
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sat Oct 28 10:38:51 2006 -0700

    [PATCH] taskstats_tgid_free: fix usage
    
    taskstats_tgid_free() is called on copy_process's error path. This is wrong.
    
            IF (clone_flags & CLONE_THREAD)
                    We should not clear ->signal->taskstats, current uses it,
                    it probably has a valid accumulated info.
            ELSE
                    taskstats_tgid_init() set ->signal->taskstats = NULL,
                    there is nothing to free.
    
    Move the callsite to __exit_signal(). We don't need any locking, entire
    thread group is exiting, nobody should have a reference to soon to be
    released ->signal.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f250a5e3e281..06de6c4e8ca3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -128,6 +128,7 @@ static void __exit_signal(struct task_struct *tsk)
 	flush_sigqueue(&tsk->pending);
 	if (sig) {
 		flush_sigqueue(&sig->shared_pending);
+		taskstats_tgid_free(sig);
 		__cleanup_signal(sig);
 	}
 }

commit fab413a334a7b3dd2688c5cd5d4718476e430ea4
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Mon Oct 2 02:18:09 2006 -0700

    [PATCH] namespaces: exit_task_namespaces() invalidates nsproxy
    
    exit_task_namespaces() has replaced the former exit_namespace().  It
    invalidates task->nsproxy and associated namespaces.  This is an issue for
    the (futur) pid namespace which is required to be valid in exit_notify().
    
    This patch moves exit_task_namespaces() after exit_notify() to keep nsproxy
    valid.
    
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Cc: Serge E. Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 741bbe42dfe8..f250a5e3e281 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -920,7 +920,6 @@ fastcall NORET_TYPE void do_exit(long code)
 	exit_sem(tsk);
 	__exit_files(tsk);
 	__exit_fs(tsk);
-	exit_task_namespaces(tsk);
 	exit_thread();
 	cpuset_exit(tsk);
 	exit_keys(tsk);
@@ -935,6 +934,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	tsk->exit_code = code;
 	proc_exit_connector(tsk);
 	exit_notify(tsk);
+	exit_task_namespaces(tsk);
 #ifdef CONFIG_NUMA
 	mpol_free(tsk->mempolicy);
 	tsk->mempolicy = NULL;

commit 1651e14e28a2d9f446018ef522882e0709a2ce4f
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:08 2006 -0700

    [PATCH] namespaces: incorporate fs namespace into nsproxy
    
    This moves the mount namespace into the nsproxy.  The mount namespace count
    now refers to the number of nsproxies point to it, rather than the number of
    tasks.  As a result, the unshare_namespace() function in kernel/fork.c no
    longer checks whether it is being shared.
    
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1d0e9ea1fa05..741bbe42dfe8 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -399,11 +399,8 @@ void daemonize(const char *name, ...)
 	current->fs = fs;
 	atomic_inc(&fs->count);
 
-	exit_namespace(current);
 	exit_task_namespaces(current);
-	current->namespace = init_task.namespace;
 	current->nsproxy = init_task.nsproxy;
-	get_namespace(current->namespace);
 	get_task_namespaces(current);
 
  	exit_files(current);
@@ -923,7 +920,6 @@ fastcall NORET_TYPE void do_exit(long code)
 	exit_sem(tsk);
 	__exit_files(tsk);
 	__exit_fs(tsk);
-	exit_namespace(tsk);
 	exit_task_namespaces(tsk);
 	exit_thread();
 	cpuset_exit(tsk);

commit ab516013ad9ca47f1d3a936fa81303bfbf734d52
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:06 2006 -0700

    [PATCH] namespaces: add nsproxy
    
    This patch adds a nsproxy structure to the task struct.  Later patches will
    move the fs namespace pointer into this structure, and introduce a new utsname
    namespace into the nsproxy.
    
    The vserver and openvz functionality, then, would be implemented in large part
    by virtualizing/isolating more and more resources into namespaces, each
    contained in the nsproxy.
    
    [akpm@osdl.org: build fix]
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3b47f26985f2..1d0e9ea1fa05 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -21,6 +21,7 @@
 #include <linux/tsacct_kern.h>
 #include <linux/file.h>
 #include <linux/binfmts.h>
+#include <linux/nsproxy.h>
 #include <linux/ptrace.h>
 #include <linux/profile.h>
 #include <linux/mount.h>
@@ -397,9 +398,14 @@ void daemonize(const char *name, ...)
 	fs = init_task.fs;
 	current->fs = fs;
 	atomic_inc(&fs->count);
+
 	exit_namespace(current);
+	exit_task_namespaces(current);
 	current->namespace = init_task.namespace;
+	current->nsproxy = init_task.nsproxy;
 	get_namespace(current->namespace);
+	get_task_namespaces(current);
+
  	exit_files(current);
 	current->files = init_task.files;
 	atomic_inc(&current->files->count);
@@ -918,6 +924,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	__exit_files(tsk);
 	__exit_fs(tsk);
 	exit_namespace(tsk);
+	exit_task_namespaces(tsk);
 	exit_thread();
 	cpuset_exit(tsk);
 	exit_keys(tsk);

commit 8f0ab5147951267134612570604cf8341901a80c
Author: Jay Lan <jlan@engr.sgi.com>
Date:   Sat Sep 30 23:28:59 2006 -0700

    [PATCH] csa: convert CONFIG tag for extended accounting routines
    
    There were a few accounting data/macros that are used in CSA but are #ifdef'ed
    inside CONFIG_BSD_PROCESS_ACCT.  This patch is to change those ifdef's from
    CONFIG_BSD_PROCESS_ACCT to CONFIG_TASK_XACCT.  A few defines are moved from
    kernel/acct.c and include/linux/acct.h to kernel/tsacct.c and
    include/linux/tsacct_kern.h.
    
    Signed-off-by: Jay Lan <jlan@sgi.com>
    Cc: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Chris Sturtivant <csturtiv@sgi.com>
    Cc: Tony Ernst <tee@sgi.com>
    Cc: Guillaume Thouvenin <guillaume.thouvenin@bull.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c189de2927ab..3b47f26985f2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -18,6 +18,7 @@
 #include <linux/security.h>
 #include <linux/cpu.h>
 #include <linux/acct.h>
+#include <linux/tsacct_kern.h>
 #include <linux/file.h>
 #include <linux/binfmts.h>
 #include <linux/ptrace.h>

commit 0d67a46df0125e20d14f12dbd3646f1f1bf23e8c
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 29 19:05:56 2006 +0100

    [PATCH] BLOCK: Remove duplicate declaration of exit_io_context() [try #6]
    
    Remove the duplicate declaration of exit_io_context() from linux/sched.h.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2e4c13cba95a..c189de2927ab 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -38,6 +38,7 @@
 #include <linux/pipe_fs_i.h>
 #include <linux/audit.h> /* for audit_free() */
 #include <linux/resource.h>
+#include <linux/blkdev.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>

commit c394cc9fbb367f87faa2228ec2eabacd2d4701c6
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Sep 29 02:01:11 2006 -0700

    [PATCH] introduce TASK_DEAD state
    
    I am not sure about this patch, I am asking Ingo to take a decision.
    
    task_struct->state == EXIT_DEAD is a very special case, to avoid a confusion
    it makes sense to introduce a new state, TASK_DEAD, while EXIT_DEAD should
    live only in ->exit_state as documented in sched.h.
    
    Note that this state is not visible to user-space, get_task_state() masks off
    unsuitable states.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9dd5f1336da2..2e4c13cba95a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -955,7 +955,7 @@ fastcall NORET_TYPE void do_exit(long code)
 
 	preempt_disable();
 	/* causes final put_task_struct in finish_task_switch(). */
-	tsk->state = EXIT_DEAD;
+	tsk->state = TASK_DEAD;
 
 	schedule();
 	BUG();

commit 55a101f8f71a3d3dbda7b5c77083ffe47552f831
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Sep 29 02:01:10 2006 -0700

    [PATCH] kill PF_DEAD flag
    
    After the previous change (->flags & PF_DEAD) <=> (->state == EXIT_DEAD), we
    don't need PF_DEAD any longer.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3d759c98fb11..9dd5f1336da2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -953,10 +953,8 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (tsk->splice_pipe)
 		__free_pipe_info(tsk->splice_pipe);
 
-	/* PF_DEAD causes final put_task_struct after we schedule. */
 	preempt_disable();
-	BUG_ON(tsk->flags & PF_DEAD);
-	tsk->flags |= PF_DEAD;
+	/* causes final put_task_struct in finish_task_switch(). */
 	tsk->state = EXIT_DEAD;
 
 	schedule();
@@ -972,7 +970,7 @@ NORET_TYPE void complete_and_exit(struct completion *comp, long code)
 {
 	if (comp)
 		complete(comp);
-	
+
 	do_exit(code);
 }
 

commit 29b884921634e1e01cbd276e1c9b8fc07a7e4a90
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Sep 29 02:01:09 2006 -0700

    [PATCH] set EXIT_DEAD state in do_exit(), not in schedule()
    
    schedule() checks PF_DEAD on every context switch and sets ->state = EXIT_DEAD
    to ensure that the exiting task will be deactivated.  Note that this EXIT_DEAD
    is in fact a "random" value, we can use any bit except normal TASK_XXX values.
    
    It is better to set this state in do_exit() along with PF_DEAD flag and remove
    that check in schedule().
    
    We are safe wrt concurrent try_to_wake_up() (for example ptrace, tkill), it
    can not change task's ->state: the 'state' argument of try_to_wake_up() can't
    have EXIT_DEAD bit.  And in case when try_to_wake_up() sees a stale value of
    ->state == TASK_RUNNING it will do nothing.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4a280856acd2..3d759c98fb11 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -957,6 +957,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	preempt_disable();
 	BUG_ON(tsk->flags & PF_DEAD);
 	tsk->flags |= PF_DEAD;
+	tsk->state = EXIT_DEAD;
 
 	schedule();
 	BUG();

commit 1c573afebc6213e4372e0d8352034c23d5262e1f
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Sep 29 02:00:51 2006 -0700

    [PATCH] reparent_to_init(): use has_rt_policy()
    
    Remove open-coded has_rt_policy(), no changes in kernel/exit.o
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Nick Piggin <nickpiggin@yahoo.com.au>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a51b347ae67b..4a280856acd2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -281,9 +281,7 @@ static void reparent_to_init(void)
 	/* Set the exit signal to SIGCHLD so we signal init on exit */
 	current->exit_signal = SIGCHLD;
 
-	if ((current->policy == SCHED_NORMAL ||
-			current->policy == SCHED_BATCH)
-				&& (task_nice(current) < 0))
+	if (!has_rt_policy(current) && (task_nice(current) < 0))
 		set_user_nice(current, 0);
 	/* cpus_allowed? */
 	/* rt_priority? */

commit 54306cf04c0ea0a8c432603dbc657ab62a438668
Author: Alan Cox <alan@lxorguk.ukuu.org.uk>
Date:   Fri Sep 29 02:00:42 2006 -0700

    [PATCH] exit: fix crash case
    
    If we are going to BUG() not panic() here then we should cover the case of
    the BUG being compiled out
    
    Signed-off-by: Alan Cox <alan@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3ec7b10eae38..a51b347ae67b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -963,7 +963,8 @@ fastcall NORET_TYPE void do_exit(long code)
 	schedule();
 	BUG();
 	/* Avoid "noreturn function does return".  */
-	for (;;) ;
+	for (;;)
+		cpu_relax();	/* For when BUG is null */
 }
 
 EXPORT_SYMBOL_GPL(do_exit);

commit b9ecb2bd5d3ab8904752685696cb76aac1f3fef2
Author: Roland McGrath <roland@redhat.com>
Date:   Fri Sep 29 02:00:31 2006 -0700

    [PATCH] has_stopped_jobs() cleanup
    
    This check has been obsolete since the introduction of TASK_TRACED.  Now
    TASK_STOPPED always means job control stop.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9961192d6055..3ec7b10eae38 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -249,17 +249,6 @@ static int has_stopped_jobs(int pgrp)
 	do_each_task_pid(pgrp, PIDTYPE_PGID, p) {
 		if (p->state != TASK_STOPPED)
 			continue;
-
-		/* If p is stopped by a debugger on a signal that won't
-		   stop it, then don't count p as stopped.  This isn't
-		   perfect but it's a good approximation.  */
-		if (unlikely (p->ptrace)
-		    && p->exit_code != SIGSTOP
-		    && p->exit_code != SIGTSTP
-		    && p->exit_code != SIGTTOU
-		    && p->exit_code != SIGTTIN)
-			continue;
-
 		retval = 1;
 		break;
 	} while_each_task_pid(pgrp, PIDTYPE_PGID, p);

commit f400e198b2ed26ce55b22a1412ded0896e7516ac
Author: Sukadev Bhattiprolu <sukadev@us.ibm.com>
Date:   Fri Sep 29 02:00:07 2006 -0700

    [PATCH] pidspace: is_init()
    
    This is an updated version of Eric Biederman's is_init() patch.
    (http://lkml.org/lkml/2006/2/6/280).  It applies cleanly to 2.6.18-rc3 and
    replaces a few more instances of ->pid == 1 with is_init().
    
    Further, is_init() checks pid and thus removes dependency on Eric's other
    patches for now.
    
    Eric's original description:
    
            There are a lot of places in the kernel where we test for init
            because we give it special properties.  Most  significantly init
            must not die.  This results in code all over the kernel test
            ->pid == 1.
    
            Introduce is_init to capture this case.
    
            With multiple pid spaces for all of the cases affected we are
            looking for only the first process on the system, not some other
            process that has pid == 1.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: <lxc-devel@lists.sourceforge.net>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4b6fb054b25d..9961192d6055 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -219,7 +219,7 @@ static int will_become_orphaned_pgrp(int pgrp, struct task_struct *ignored_task)
 	do_each_task_pid(pgrp, PIDTYPE_PGID, p) {
 		if (p == ignored_task
 				|| p->exit_state
-				|| p->real_parent->pid == 1)
+				|| is_init(p->real_parent))
 			continue;
 		if (process_group(p->real_parent) != pgrp
 			    && p->real_parent->signal->session == p->signal->session) {

commit 3b9b8ab65d8eed784b9164d03807cb2bda7b5cd6
Author: Kirill Korotaev <dev@sw.ru>
Date:   Fri Sep 29 02:00:05 2006 -0700

    [PATCH] Fix unserialized task->files changing
    
    Fixed race on put_files_struct on exec with proc.  Restoring files on
    current on error path may lead to proc having a pointer to already kfree-d
    files_struct.
    
    ->files changing at exit.c and khtread.c are safe as exit_files() makes all
    things under lock.
    
    Found during OpenVZ stress testing.
    
    [akpm@osdl.org: add export]
    Signed-off-by: Pavel Emelianov <xemul@openvz.org>
    Signed-off-by: Kirill Korotaev <dev@openvz.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index d891883420f7..4b6fb054b25d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -487,6 +487,18 @@ void fastcall put_files_struct(struct files_struct *files)
 
 EXPORT_SYMBOL(put_files_struct);
 
+void reset_files_struct(struct task_struct *tsk, struct files_struct *files)
+{
+	struct files_struct *old;
+
+	old = tsk->files;
+	task_lock(tsk);
+	tsk->files = files;
+	task_unlock(tsk);
+	put_files_struct(old);
+}
+EXPORT_SYMBOL(reset_files_struct);
+
 static inline void __exit_files(struct task_struct *tsk)
 {
 	struct files_struct * files = tsk->files;

commit 3b6362b833b9f7a9d4222cf1bb35f99c411abb31
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sat Sep 2 21:22:16 2006 +0400

    [PATCH] eligible_child: remove an obsolete ->tgid check
    
    It is not possible to find a sub-thread in ->children/->ptrace_children
    lists, ptrace_attach() does not allow to attach to sub-threads.
    
    Even if it was possible to ptrace the task from the same thread group,
    we can't allow to release ->group_leader while there are others (ptracer)
    threads in the same group.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a4c19a52ce46..d891883420f7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1053,7 +1053,7 @@ static int eligible_child(pid_t pid, int options, struct task_struct *p)
 	 * Do not consider thread group leaders that are
 	 * in a non-empty thread group:
 	 */
-	if (current->tgid != p->tgid && delay_group_leader(p))
+	if (delay_group_leader(p))
 		return 2;
 
 	if (security_task_wait(p))

commit 35df17c57cecb08f0120fb18926325f1093dc429
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Thu Aug 31 21:27:38 2006 -0700

    [PATCH] task delay accounting fixes
    
    Cleanup allocation and freeing of tsk->delays used by delay accounting.
    This solves two problems reported for delay accounting:
    
    1. oops in __delayacct_blkio_ticks
    http://www.uwsg.indiana.edu/hypermail/linux/kernel/0608.2/1844.html
    
    Currently tsk->delays is getting freed too early in task exit which can
    cause a NULL tsk->delays to get accessed via reading of /proc/<tgid>/stats.
     The patch fixes this problem by freeing tsk->delays closer to when
    task_struct itself is freed up.  As a result, it also eliminates the use of
    tsk->delays_lock which was only being used (inadequately) to safeguard
    access to tsk->delays while a task was exiting.
    
    2. Possible memory leak in kernel/delayacct.c
    http://www.uwsg.indiana.edu/hypermail/linux/kernel/0608.2/1389.html
    
    The patch cleans up tsk->delays allocations after a bad fork which was
    missing earlier.
    
    The patch has been tested to fix the problems listed above and stress
    tested with rapid calls to delay accounting's taskstats command interface
    (which is the other path that can access the same data, besides the /proc
    interface causing the oops above).
    
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index dba194a8d416..a4c19a52ce46 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -908,7 +908,6 @@ fastcall NORET_TYPE void do_exit(long code)
 		audit_free(tsk);
 	taskstats_exit_send(tsk, tidstats, group_dead, mycpu);
 	taskstats_exit_free(tidstats);
-	delayacct_tsk_exit(tsk);
 
 	exit_mm(tsk);
 

commit f9fd8914c1acca0d98b69d831b128d5b52f03c51
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Fri Jul 14 00:24:47 2006 -0700

    [PATCH] per-task delay accounting taskstats interface: control exit data through cpumasks
    
    On systems with a large number of cpus, with even a modest rate of tasks
    exiting per cpu, the volume of taskstats data sent on thread exit can
    overflow a userspace listener's buffers.
    
    One approach to avoiding overflow is to allow listeners to get data for a
    limited and specific set of cpus.  By scaling the number of listeners
    and/or the cpus they monitor, userspace can handle the statistical data
    overload more gracefully.
    
    In this patch, each listener registers to listen to a specific set of cpus
    by specifying a cpumask.  The interest is recorded per-cpu.  When a task
    exits on a cpu, its taskstats data is unicast to each listener interested
    in that cpu.
    
    Thanks to Andrew Morton for pointing out the various scalability and
    general concerns of previous attempts and for suggesting this design.
    
    [akpm@osdl.org: build fix]
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Signed-off-by: Chandra Seetharaman <sekharan@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 67c1e9a4f812..dba194a8d416 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -847,6 +847,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	struct task_struct *tsk = current;
 	struct taskstats *tidstats;
 	int group_dead;
+	unsigned int mycpu;
 
 	profile_task_exit(tsk);
 
@@ -884,7 +885,7 @@ fastcall NORET_TYPE void do_exit(long code)
 				current->comm, current->pid,
 				preempt_count());
 
-	taskstats_exit_alloc(&tidstats);
+	taskstats_exit_alloc(&tidstats, &mycpu);
 
 	acct_update_integrals(tsk);
 	if (tsk->mm) {
@@ -905,7 +906,7 @@ fastcall NORET_TYPE void do_exit(long code)
 #endif
 	if (unlikely(tsk->audit_context))
 		audit_free(tsk);
-	taskstats_exit_send(tsk, tidstats, group_dead);
+	taskstats_exit_send(tsk, tidstats, group_dead, mycpu);
 	taskstats_exit_free(tidstats);
 	delayacct_tsk_exit(tsk);
 

commit ad4ecbcba72855a2b5319b96e2a3a65ed1ca3bfd
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Fri Jul 14 00:24:44 2006 -0700

    [PATCH] delay accounting taskstats interface send tgid once
    
    Send per-tgid data only once during exit of a thread group instead of once
    with each member thread exit.
    
    Currently, when a thread exits, besides its per-tid data, the per-tgid data
    of its thread group is also sent out, if its thread group is non-empty.
    The per-tgid data sent consists of the sum of per-tid stats for all
    *remaining* threads of the thread group.
    
    This patch modifies this sending in two ways:
    
    - the per-tgid data is sent only when the last thread of a thread group
      exits.  This cuts down heavily on the overhead of sending/receiving
      per-tgid data, especially when other exploiters of the taskstats
      interface aren't interested in per-tgid stats
    
    - the semantics of the per-tgid data sent are changed.  Instead of being
      the sum of per-tid data for remaining threads, the value now sent is the
      true total accumalated statistics for all threads that are/were part of
      the thread group.
    
    The patch also addresses a minor issue where failure of one accounting
    subsystem to fill in the taskstats structure was causing the send of
    taskstats to not be sent at all.
    
    The patch has been tested for stability and run cerberus for over 4 hours
    on an SMP.
    
    [akpm@osdl.org: bugfixes]
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9852ed8c2988..67c1e9a4f812 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -845,7 +845,7 @@ static void exit_notify(struct task_struct *tsk)
 fastcall NORET_TYPE void do_exit(long code)
 {
 	struct task_struct *tsk = current;
-	struct taskstats *tidstats, *tgidstats;
+	struct taskstats *tidstats;
 	int group_dead;
 
 	profile_task_exit(tsk);
@@ -884,7 +884,7 @@ fastcall NORET_TYPE void do_exit(long code)
 				current->comm, current->pid,
 				preempt_count());
 
-	taskstats_exit_alloc(&tidstats, &tgidstats);
+	taskstats_exit_alloc(&tidstats);
 
 	acct_update_integrals(tsk);
 	if (tsk->mm) {
@@ -905,8 +905,8 @@ fastcall NORET_TYPE void do_exit(long code)
 #endif
 	if (unlikely(tsk->audit_context))
 		audit_free(tsk);
-	taskstats_exit_send(tsk, tidstats, tgidstats);
-	taskstats_exit_free(tidstats, tgidstats);
+	taskstats_exit_send(tsk, tidstats, group_dead);
+	taskstats_exit_free(tidstats);
 	delayacct_tsk_exit(tsk);
 
 	exit_mm(tsk);

commit c757249af152c59fd74b85e52e8c090acb33d9c0
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Fri Jul 14 00:24:40 2006 -0700

    [PATCH] per-task-delay-accounting: taskstats interface
    
    Create a "taskstats" interface based on generic netlink (NETLINK_GENERIC
    family), for getting statistics of tasks and thread groups during their
    lifetime and when they exit.  The interface is intended for use by multiple
    accounting packages though it is being created in the context of delay
    accounting.
    
    This patch creates the interface without populating the fields of the data
    that is sent to the user in response to a command or upon the exit of a task.
    Each accounting package interested in using taskstats has to provide an
    additional patch to add its stats to the common structure.
    
    [akpm@osdl.org: cleanups, Kconfig fix]
    Signed-off-by: Shailabh Nagar <nagar@us.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Peter Chubb <peterc@gelato.unsw.edu.au>
    Cc: Erich Focht <efocht@ess.nec.de>
    Cc: Levent Serinol <lserinol@gmail.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3c2cf91defa7..9852ed8c2988 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -25,6 +25,7 @@
 #include <linux/mount.h>
 #include <linux/proc_fs.h>
 #include <linux/mempolicy.h>
+#include <linux/taskstats_kern.h>
 #include <linux/delayacct.h>
 #include <linux/cpuset.h>
 #include <linux/syscalls.h>
@@ -844,6 +845,7 @@ static void exit_notify(struct task_struct *tsk)
 fastcall NORET_TYPE void do_exit(long code)
 {
 	struct task_struct *tsk = current;
+	struct taskstats *tidstats, *tgidstats;
 	int group_dead;
 
 	profile_task_exit(tsk);
@@ -882,6 +884,8 @@ fastcall NORET_TYPE void do_exit(long code)
 				current->comm, current->pid,
 				preempt_count());
 
+	taskstats_exit_alloc(&tidstats, &tgidstats);
+
 	acct_update_integrals(tsk);
 	if (tsk->mm) {
 		update_hiwater_rss(tsk->mm);
@@ -901,7 +905,10 @@ fastcall NORET_TYPE void do_exit(long code)
 #endif
 	if (unlikely(tsk->audit_context))
 		audit_free(tsk);
+	taskstats_exit_send(tsk, tidstats, tgidstats);
+	taskstats_exit_free(tidstats, tgidstats);
 	delayacct_tsk_exit(tsk);
+
 	exit_mm(tsk);
 
 	if (group_dead)

commit ca74e92b4698276b6696f15a801759f50944f387
Author: Shailabh Nagar <nagar@watson.ibm.com>
Date:   Fri Jul 14 00:24:36 2006 -0700

    [PATCH] per-task-delay-accounting: setup
    
    Initialization code related to collection of per-task "delay" statistics which
    measure how long it had to wait for cpu, sync block io, swapping etc.  The
    collection of statistics and the interface are in other patches.  This patch
    sets up the data structures and allows the statistics collection to be
    disabled through a kernel boot parameter.
    
    Signed-off-by: Shailabh Nagar <nagar@watson.ibm.com>
    Signed-off-by: Balbir Singh <balbir@in.ibm.com>
    Cc: Jes Sorensen <jes@sgi.com>
    Cc: Peter Chubb <peterc@gelato.unsw.edu.au>
    Cc: Erich Focht <efocht@ess.nec.de>
    Cc: Levent Serinol <lserinol@gmail.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6664c084783d..3c2cf91defa7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -25,6 +25,7 @@
 #include <linux/mount.h>
 #include <linux/proc_fs.h>
 #include <linux/mempolicy.h>
+#include <linux/delayacct.h>
 #include <linux/cpuset.h>
 #include <linux/syscalls.h>
 #include <linux/signal.h>
@@ -900,6 +901,7 @@ fastcall NORET_TYPE void do_exit(long code)
 #endif
 	if (unlikely(tsk->audit_context))
 		audit_free(tsk);
+	delayacct_tsk_exit(tsk);
 	exit_mm(tsk);
 
 	if (group_dead)

commit 36c8b586896f60cb91a4fd526233190b34316baf
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:25:41 2006 -0700

    [PATCH] sched: cleanup, remove task_t, convert to struct task_struct
    
    cleanup: remove task_t and convert all the uses to struct task_struct. I
    introduced it for the scheduler anno and it was a mistake.
    
    Conversion was mostly scripted, the result was reviewed and all
    secondary whitespace and style impact (if any) was fixed up by hand.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c595db14cf25..6664c084783d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -134,8 +134,8 @@ static void delayed_put_task_struct(struct rcu_head *rhp)
 
 void release_task(struct task_struct * p)
 {
+	struct task_struct *leader;
 	int zap_leader;
-	task_t *leader;
 repeat:
 	atomic_dec(&p->user->processes);
 	write_lock_irq(&tasklist_lock);
@@ -209,7 +209,7 @@ int session_of_pgrp(int pgrp)
  *
  * "I ask you, have you ever known what it is to be an orphan?"
  */
-static int will_become_orphaned_pgrp(int pgrp, task_t *ignored_task)
+static int will_become_orphaned_pgrp(int pgrp, struct task_struct *ignored_task)
 {
 	struct task_struct *p;
 	int ret = 1;
@@ -582,7 +582,8 @@ static void exit_mm(struct task_struct * tsk)
 	mmput(mm);
 }
 
-static inline void choose_new_parent(task_t *p, task_t *reaper)
+static inline void
+choose_new_parent(struct task_struct *p, struct task_struct *reaper)
 {
 	/*
 	 * Make sure we're not reparenting to ourselves and that
@@ -592,7 +593,8 @@ static inline void choose_new_parent(task_t *p, task_t *reaper)
 	p->real_parent = reaper;
 }
 
-static void reparent_thread(task_t *p, task_t *father, int traced)
+static void
+reparent_thread(struct task_struct *p, struct task_struct *father, int traced)
 {
 	/* We don't want people slaying init.  */
 	if (p->exit_signal != -1)
@@ -656,8 +658,8 @@ static void reparent_thread(task_t *p, task_t *father, int traced)
  * group, and if no such member exists, give it to
  * the global child reaper process (ie "init")
  */
-static void forget_original_parent(struct task_struct * father,
-					  struct list_head *to_release)
+static void
+forget_original_parent(struct task_struct *father, struct list_head *to_release)
 {
 	struct task_struct *p, *reaper = father;
 	struct list_head *_p, *_n;
@@ -680,7 +682,7 @@ static void forget_original_parent(struct task_struct * father,
 	 */
 	list_for_each_safe(_p, _n, &father->children) {
 		int ptrace;
-		p = list_entry(_p,struct task_struct,sibling);
+		p = list_entry(_p, struct task_struct, sibling);
 
 		ptrace = p->ptrace;
 
@@ -709,7 +711,7 @@ static void forget_original_parent(struct task_struct * father,
 			list_add(&p->ptrace_list, to_release);
 	}
 	list_for_each_safe(_p, _n, &father->ptrace_children) {
-		p = list_entry(_p,struct task_struct,ptrace_list);
+		p = list_entry(_p, struct task_struct, ptrace_list);
 		choose_new_parent(p, reaper);
 		reparent_thread(p, father, 1);
 	}
@@ -829,7 +831,7 @@ static void exit_notify(struct task_struct *tsk)
 
 	list_for_each_safe(_p, _n, &ptrace_dead) {
 		list_del_init(_p);
-		t = list_entry(_p,struct task_struct,ptrace_list);
+		t = list_entry(_p, struct task_struct, ptrace_list);
 		release_task(t);
 	}
 
@@ -1010,7 +1012,7 @@ asmlinkage void sys_exit_group(int error_code)
 	do_group_exit((error_code & 0xff) << 8);
 }
 
-static int eligible_child(pid_t pid, int options, task_t *p)
+static int eligible_child(pid_t pid, int options, struct task_struct *p)
 {
 	if (pid > 0) {
 		if (p->pid != pid)
@@ -1051,12 +1053,13 @@ static int eligible_child(pid_t pid, int options, task_t *p)
 	return 1;
 }
 
-static int wait_noreap_copyout(task_t *p, pid_t pid, uid_t uid,
+static int wait_noreap_copyout(struct task_struct *p, pid_t pid, uid_t uid,
 			       int why, int status,
 			       struct siginfo __user *infop,
 			       struct rusage __user *rusagep)
 {
 	int retval = rusagep ? getrusage(p, RUSAGE_BOTH, rusagep) : 0;
+
 	put_task_struct(p);
 	if (!retval)
 		retval = put_user(SIGCHLD, &infop->si_signo);
@@ -1081,7 +1084,7 @@ static int wait_noreap_copyout(task_t *p, pid_t pid, uid_t uid,
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_zombie(task_t *p, int noreap,
+static int wait_task_zombie(struct task_struct *p, int noreap,
 			    struct siginfo __user *infop,
 			    int __user *stat_addr, struct rusage __user *ru)
 {
@@ -1243,8 +1246,8 @@ static int wait_task_zombie(task_t *p, int noreap,
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_stopped(task_t *p, int delayed_group_leader, int noreap,
-			     struct siginfo __user *infop,
+static int wait_task_stopped(struct task_struct *p, int delayed_group_leader,
+			     int noreap, struct siginfo __user *infop,
 			     int __user *stat_addr, struct rusage __user *ru)
 {
 	int retval, exit_code;
@@ -1358,7 +1361,7 @@ static int wait_task_stopped(task_t *p, int delayed_group_leader, int noreap,
  * the lock and this task is uninteresting.  If we return nonzero, we have
  * released the lock and the system call should return.
  */
-static int wait_task_continued(task_t *p, int noreap,
+static int wait_task_continued(struct task_struct *p, int noreap,
 			       struct siginfo __user *infop,
 			       int __user *stat_addr, struct rusage __user *ru)
 {
@@ -1444,7 +1447,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 		int ret;
 
 		list_for_each(_p,&tsk->children) {
-			p = list_entry(_p,struct task_struct,sibling);
+			p = list_entry(_p, struct task_struct, sibling);
 
 			ret = eligible_child(pid, options, p);
 			if (!ret)

commit 9a11b49a805665e13a56aa067afaf81d43ec1514
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 3 00:24:33 2006 -0700

    [PATCH] lockdep: better lock debugging
    
    Generic lock debugging:
    
     - generalized lock debugging framework. For example, a bug in one lock
       subsystem turns off debugging in all lock subsystems.
    
     - got rid of the caller address passing (__IP__/__IP_DECL__/etc.) from
       the mutex/rtmutex debugging code: it caused way too much prototype
       hackery, and lockdep will give the same information anyway.
    
     - ability to do silent tests
    
     - check lock freeing in vfree too.
    
     - more finegrained debugging options, to allow distributions to
       turn off more expensive debugging features.
    
    There's no separate 'held mutexes' list anymore - but there's a 'held locks'
    stack within lockdep, which unifies deadlock detection across all lock
    classes.  (this is independent of the lockdep validation stuff - lockdep first
    checks whether we are holding a lock already)
    
    Here are the current debugging options:
    
    CONFIG_DEBUG_MUTEXES=y
    CONFIG_DEBUG_LOCK_ALLOC=y
    
    which do:
    
     config DEBUG_MUTEXES
              bool "Mutex debugging, basic checks"
    
     config DEBUG_LOCK_ALLOC
             bool "Detect incorrect freeing of live mutexes"
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7f7ef2258553..c595db14cf25 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -933,10 +933,9 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (unlikely(current->pi_state_cache))
 		kfree(current->pi_state_cache);
 	/*
-	 * If DEBUG_MUTEXES is on, make sure we are holding no locks:
+	 * Make sure we are holding no locks:
 	 */
-	mutex_debug_check_no_locks_held(tsk);
-	rt_mutex_debug_check_no_locks_held(tsk);
+	debug_check_no_locks_held(tsk);
 
 	if (tsk->io_context)
 		exit_io_context();

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jrn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jrn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/kernel/exit.c b/kernel/exit.c
index ab06b9f88f64..7f7ef2258553 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -4,7 +4,6 @@
  *  Copyright (C) 1991, 1992  Linus Torvalds
  */
 
-#include <linux/config.h>
 #include <linux/mm.h>
 #include <linux/slab.h>
 #include <linux/interrupt.h>

commit c87e2837be82df479a6bae9f155c43516d2feebc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 27 02:54:58 2006 -0700

    [PATCH] pi-futex: futex_lock_pi/futex_unlock_pi support
    
    This adds the actual pi-futex implementation, based on rt-mutexes.
    
    [dino@in.ibm.com: fix an oops-causing race]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Dinakar Guniguntala <dino@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3e8a0282e9a5..ab06b9f88f64 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -925,6 +925,14 @@ fastcall NORET_TYPE void do_exit(long code)
 	mpol_free(tsk->mempolicy);
 	tsk->mempolicy = NULL;
 #endif
+	/*
+	 * This must happen late, after the PID is not
+	 * hashed anymore:
+	 */
+	if (unlikely(!list_empty(&tsk->pi_state_list)))
+		exit_pi_state_list(tsk);
+	if (unlikely(current->pi_state_cache))
+		kfree(current->pi_state_cache);
 	/*
 	 * If DEBUG_MUTEXES is on, make sure we are holding no locks:
 	 */

commit e7eebaf6a81b956c989f184ee4b27277c88f8afe
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jun 27 02:54:55 2006 -0700

    [PATCH] pi-futex: rt mutex debug
    
    Runtime debugging functionality for rt-mutexes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 304ef637be6c..3e8a0282e9a5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -929,6 +929,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	 * If DEBUG_MUTEXES is on, make sure we are holding no locks:
 	 */
 	mutex_debug_check_no_locks_held(tsk);
+	rt_mutex_debug_check_no_locks_held(tsk);
 
 	if (tsk->io_context)
 		exit_io_context();

commit 48e6484d49020dba3578ad117b461e8a391e8f0f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jun 26 00:25:48 2006 -0700

    [PATCH] proc: Rewrite the proc dentry flush on exit optimization
    
    To keep the dcache from filling up with dead /proc entries we flush them on
    process exit.  However over the years that code has gotten hairy with a
    dentry_pointer and a lock in task_struct and misdocumented as a correctness
    feature.
    
    I have rewritten this code to look and see if we have a corresponding entry in
    the dcache and if so flush it on process exit.  This removes the extra fields
    in the task_struct and allows me to trivially handle the case of a
    /proc/<tgid>/task/<pid> entry as well as the current /proc/<pid> entries.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e76bd02e930e..304ef637be6c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -137,12 +137,8 @@ void release_task(struct task_struct * p)
 {
 	int zap_leader;
 	task_t *leader;
-	struct dentry *proc_dentry;
-
 repeat:
 	atomic_dec(&p->user->processes);
-	spin_lock(&p->proc_lock);
-	proc_dentry = proc_pid_unhash(p);
 	write_lock_irq(&tasklist_lock);
 	ptrace_unlink(p);
 	BUG_ON(!list_empty(&p->ptrace_list) || !list_empty(&p->ptrace_children));
@@ -171,8 +167,7 @@ void release_task(struct task_struct * p)
 
 	sched_exit(p);
 	write_unlock_irq(&tasklist_lock);
-	spin_unlock(&p->proc_lock);
-	proc_pid_flush(proc_dentry);
+	proc_flush_task(p);
 	release_thread(p);
 	call_rcu(&p->rcu, delayed_put_task_struct);
 

commit f6ec29a42d7ac3b309a9cef179b686d23986ab98
Author: KaiGai Kohei <kaigai@ak.jp.nec.com>
Date:   Sun Jun 25 05:49:25 2006 -0700

    [PATCH] pacct: avoidance to refer the last thread as a representation of the process
    
    When pacct facility generate an 'ac_flag' field in accounting record, it
    refers a task_struct of the thread which died last in the process.  But any
    other task_structs are ignored.
    
    Therefore, pacct facility drops ASU flag even if root-privilege operations are
    used by any other threads except the last one.  In addition, AFORK flag is
    always set when the thread of group-leader didn't die last, although this
    process has called execve() after fork().
    
    We have a same matter in ac_exitcode.  The recorded ac_exitcode is an exit
    code of the last thread in the process.  There is a possibility this exitcode
    is not the group leader's one.

diff --git a/kernel/exit.c b/kernel/exit.c
index 819d82c2efba..e76bd02e930e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -894,8 +894,8 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (group_dead) {
  		hrtimer_cancel(&tsk->signal->real_timer);
 		exit_itimers(tsk->signal);
-		acct_collect();
 	}
+	acct_collect(code, group_dead);
 	if (unlikely(tsk->robust_list))
 		exit_robust_list(tsk);
 #if defined(CONFIG_FUTEX) && defined(CONFIG_COMPAT)
@@ -907,7 +907,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	exit_mm(tsk);
 
 	if (group_dead)
-		acct_process(code);
+		acct_process();
 	exit_sem(tsk);
 	__exit_files(tsk);
 	__exit_fs(tsk);

commit 0e4648141af02331f21aabcd34940c70f09a2d04
Author: KaiGai Kohei <kaigai@ak.jp.nec.com>
Date:   Sun Jun 25 05:49:24 2006 -0700

    [PATCH] pacct: add pacct_struct to fix some pacct bugs.
    
    The pacct facility need an i/o operation when an accounting record is
    generated.  There is a possibility to wake OOM killer up.  If OOM killer is
    activated, it kills some processes to make them release process memory
    regions.
    
    But acct_process() is called in the killed processes context before calling
    exit_mm(), so those processes cannot release own memory.  In the results, any
    processes stop in this point and it finally cause a system stall.

diff --git a/kernel/exit.c b/kernel/exit.c
index 601263c0806f..819d82c2efba 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -894,7 +894,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (group_dead) {
  		hrtimer_cancel(&tsk->signal->real_timer);
 		exit_itimers(tsk->signal);
-		acct_process(code);
+		acct_collect();
 	}
 	if (unlikely(tsk->robust_list))
 		exit_robust_list(tsk);
@@ -906,6 +906,8 @@ fastcall NORET_TYPE void do_exit(long code)
 		audit_free(tsk);
 	exit_mm(tsk);
 
+	if (group_dead)
+		acct_process(code);
 	exit_sem(tsk);
 	__exit_files(tsk);
 	__exit_fs(tsk);

commit 2aa92581fb13e04e1440e5041b412cc06c782e0e
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Jun 25 05:48:44 2006 -0700

    [PATCH] Link error when futexes are disabled on 64bit architectures
    
    If futexes are disabled we fail to link on ppc64.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index b12a4706f73f..601263c0806f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -898,7 +898,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	}
 	if (unlikely(tsk->robust_list))
 		exit_robust_list(tsk);
-#ifdef CONFIG_COMPAT
+#if defined(CONFIG_FUTEX) && defined(CONFIG_COMPAT)
 	if (unlikely(tsk->compat_robust_list))
 		compat_exit_robust_list(tsk);
 #endif

commit 83cc5ed3c4c65fc4c3729a5cec2111ede1ebf85e
Author: Adrian Bunk <bunk@stusta.de>
Date:   Sun Jun 25 05:47:41 2006 -0700

    [PATCH] kernel/sys.c: cleanups
    
    - proper prototypes for the following functions:
      - ctrl_alt_del()  (in include/linux/reboot.h)
      - getrusage()     (in include/linux/resource.h)
    - make the following needlessly global functions static:
      - kernel_restart_prepare()
      - kernel_kexec()
    
    [akpm@osdl.org: compile fix]
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a3baf92462bd..b12a4706f73f 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -36,6 +36,7 @@
 #include <linux/compat.h>
 #include <linux/pipe_fs_i.h>
 #include <linux/audit.h> /* for audit_free() */
+#include <linux/resource.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -45,8 +46,6 @@
 extern void sem_exit (void);
 extern struct task_struct *child_reaper;
 
-int getrusage(struct task_struct *, int, struct rusage __user *);
-
 static void exit_mm(struct task_struct * tsk);
 
 static void __unhash_process(struct task_struct *p)

commit 125e18745f16685f69a34fd6130d47598fc4bf54
Author: Eric Sesterhenn <snakebyte@gmx.de>
Date:   Fri Jun 23 02:06:06 2006 -0700

    [PATCH] More BUG_ON conversion
    
    Signed-off-by: Eric Sesterhenn <snakebyte@gmx.de>
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Bartlomiej Zolnierkiewicz <B.Zolnierkiewicz@elka.pw.edu.pl>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: James Bottomley <James.Bottomley@steeleye.com>
    Acked-by: "Salyzyn, Mark" <mark_salyzyn@adaptec.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e06d0c10a24e..a3baf92462bd 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -579,7 +579,7 @@ static void exit_mm(struct task_struct * tsk)
 		down_read(&mm->mmap_sem);
 	}
 	atomic_inc(&mm->mm_count);
-	if (mm != tsk->active_mm) BUG();
+	BUG_ON(mm != tsk->active_mm);
 	/* more a memory barrier than a real lock */
 	task_lock(tsk);
 	tsk->mm = NULL;
@@ -1530,8 +1530,7 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 		if (options & __WNOTHREAD)
 			break;
 		tsk = next_thread(tsk);
-		if (tsk->signal != current->signal)
-			BUG();
+		BUG_ON(tsk->signal != current->signal);
 	} while (tsk != current);
 
 	read_unlock(&tasklist_lock);

commit 30f1e3dd8c72abda343bcf415f7d8894a02b4290
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Thu Jun 15 20:11:43 2006 +0400

    [PATCH] run_posix_cpu_timers: remove a bogus BUG_ON()
    
    do_exit() clears ->it_##clock##_expires, but nothing prevents
    another cpu to attach the timer to exiting process after that.
    arm_timer() tries to protect against this race, but the check
    is racy.
    
    After exit_notify() does 'write_unlock_irq(&tasklist_lock)' and
    before do_exit() calls 'schedule() local timer interrupt can find
    tsk->exit_state != 0. If that state was EXIT_DEAD (or another cpu
    does sys_wait4) interrupted task has ->signal == NULL.
    
    At this moment exiting task has no pending cpu timers, they were
    cleanuped in __exit_signal()->posix_cpu_timers_exit{,_group}(),
    so we can just return from irq.
    
    John Stultz recently confirmed this bug, see
    
            http://marc.theaimsgroup.com/?l=linux-kernel&m=115015841413687
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e95b93282210..e06d0c10a24e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -881,14 +881,6 @@ fastcall NORET_TYPE void do_exit(long code)
 
 	tsk->flags |= PF_EXITING;
 
-	/*
-	 * Make sure we don't try to process any timer firings
-	 * while we are already exiting.
-	 */
- 	tsk->it_virt_expires = cputime_zero;
- 	tsk->it_prof_expires = cputime_zero;
-	tsk->it_sched_expires = 0;
-
 	if (unlikely(in_atomic()))
 		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
 				current->comm, current->pid,

commit fa84cb935d4ec601528f5e2f0d5d31e7876a5044
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Mar 29 20:30:19 2006 -0500

    [PATCH] move call of audit_free() into do_exit()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index f86434d7b3d1..e95b93282210 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -35,6 +35,7 @@
 #include <linux/futex.h>
 #include <linux/compat.h>
 #include <linux/pipe_fs_i.h>
+#include <linux/audit.h> /* for audit_free() */
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -910,6 +911,8 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (unlikely(tsk->compat_robust_list))
 		compat_exit_robust_list(tsk);
 #endif
+	if (unlikely(tsk->audit_context))
+		audit_free(tsk);
 	exit_mm(tsk);
 
 	exit_sem(tsk);

commit 5e85d4abe3f43bb5362f384bab0e20ef082ce0b5
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Apr 18 22:20:16 2006 -0700

    [PATCH] task: Make task list manipulations RCU safe
    
    While we can currently walk through thread groups, process groups, and
    sessions with just the rcu_read_lock, this opens the door to walking the
    entire task list.
    
    We already have all of the other RCU guarantees so there is no cost in
    doing this, this should be enough so that proc can stop taking the
    tasklist lock during readdir.
    
    prev_task was killed because it has no users, and using it will miss new
    tasks when doing an rcu traversal.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 1a9787ac6173..f86434d7b3d1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -56,7 +56,7 @@ static void __unhash_process(struct task_struct *p)
 		detach_pid(p, PIDTYPE_PGID);
 		detach_pid(p, PIDTYPE_SID);
 
-		list_del_init(&p->tasks);
+		list_del_rcu(&p->tasks);
 		__get_cpu_var(process_counts)--;
 	}
 	list_del_rcu(&p->thread_group);

commit b92ce55893745e011edae70830b8bc863be881f9
Author: Jens Axboe <axboe@suse.de>
Date:   Tue Apr 11 13:52:07 2006 +0200

    [PATCH] splice: add direct fd <-> fd splicing support
    
    It's more efficient for sendfile() emulation. Basically we cache an
    internal private pipe and just use that as the intermediate area for
    pages. Direct splicing is not available from sys_splice(), it is only
    meant to be used for sendfile() emulation.
    
    Additional patch from Ingo Molnar to avoid the PIPE_BUFFERS loop at
    exit for the normal fast path.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6c2eeb8f6390..1a9787ac6173 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -34,6 +34,7 @@
 #include <linux/mutex.h>
 #include <linux/futex.h>
 #include <linux/compat.h>
+#include <linux/pipe_fs_i.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -941,6 +942,9 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (tsk->io_context)
 		exit_io_context();
 
+	if (tsk->splice_pipe)
+		__free_pipe_info(tsk->splice_pipe);
+
 	/* PF_DEAD causes final put_task_struct after we schedule. */
 	preempt_disable();
 	BUG_ON(tsk->flags & PF_DEAD);

commit 8c7904a00b06d2ee51149794b619e07369fcf9d4
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Mar 31 02:31:37 2006 -0800

    [PATCH] task: RCU protect task->usage
    
    A big problem with rcu protected data structures that are also reference
    counted is that you must jump through several hoops to increase the reference
    count.  I think someone finally implemented atomic_inc_not_zero(&count) to
    automate the common case.  Unfortunately this means you must special case the
    rcu access case.
    
    When data structures are only visible via rcu in a manner that is not
    determined by the reference count on the object (i.e.  tasks are visible until
    their zombies are reaped) there is a much simpler technique we can employ.
    Simply delaying the decrement of the reference count until the rcu interval is
    over.
    
    What that means is that the proc code that looks up a task and later
    wants to sleep can now do:
    
    rcu_read_lock();
    task = find_task_by_pid(some_pid);
    if (task) {
            get_task_struct(task);
    }
    rcu_read_unlock();
    
    The effect on the rest of the kernel is that put_task_struct becomes cheaper
    and immediate, and in the case where the task has been reaped it frees the
    task immediate instead of unnecessarily waiting an until the rcu interval is
    over.
    
    Cleanup of task_struct does not happen when its reference count drops to
    zero, instead cleanup happens when release_task is called.  Tasks can only
    be looked up via rcu before release_task is called.  All rcu protected
    members of task_struct are freed by release_task.
    
    Therefore we can move call_rcu from put_task_struct into release_task.  And
    we can modify release_task to not immediately release the reference count
    but instead have it call put_task_struct from the function it gives to
    call_rcu.
    
    The end result:
    
    - get_task_struct is safe in an rcu context where we have just looked
      up the task.
    
    - put_task_struct() simplifies into its old pre rcu self.
    
    This reorganization also makes put_task_struct uncallable from modules as
    it is not exported but it does not appear to be called from any modules so
    this should not be an issue, and is trivially fixed.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index bc0ec674d3f4..6c2eeb8f6390 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -127,6 +127,11 @@ static void __exit_signal(struct task_struct *tsk)
 	}
 }
 
+static void delayed_put_task_struct(struct rcu_head *rhp)
+{
+	put_task_struct(container_of(rhp, struct task_struct, rcu));
+}
+
 void release_task(struct task_struct * p)
 {
 	int zap_leader;
@@ -168,7 +173,7 @@ void release_task(struct task_struct * p)
 	spin_unlock(&p->proc_lock);
 	proc_pid_flush(proc_dentry);
 	release_thread(p);
-	put_task_struct(p);
+	call_rcu(&p->rcu, delayed_put_task_struct);
 
 	p = leader;
 	if (unlikely(zap_leader))

commit a7e5328a06a2beee3a2bbfaf87ce2a7bbe937de1
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:27 2006 -0800

    [PATCH] cleanup __exit_signal->cleanup_sighand path
    
    Move 'tsk->sighand = NULL' from cleanup_sighand() to __exit_signal().  This
    makes the exit path more understandable and allows us to do
    cleanup_sighand() outside of ->siglock protected section.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 22399caf7574..bc0ec674d3f4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -114,10 +114,11 @@ static void __exit_signal(struct task_struct *tsk)
 	__unhash_process(tsk);
 
 	tsk->signal = NULL;
-	cleanup_sighand(tsk);
+	tsk->sighand = NULL;
 	spin_unlock(&sighand->siglock);
 	rcu_read_unlock();
 
+	__cleanup_sighand(sighand);
 	clear_tsk_thread_flag(tsk,TIF_SIGPENDING);
 	flush_sigqueue(&tsk->pending);
 	if (sig) {

commit 47e65328a7b1cdfc4e3102e50d60faf94ebba7d3
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:25 2006 -0800

    [PATCH] pids: kill PIDTYPE_TGID
    
    This patch kills PIDTYPE_TGID pid_type thus saving one hash table in
    kernel/pid.c and speeding up subthreads create/destroy a bit.  It is also a
    preparation for the further tref/pids rework.
    
    This patch adds 'struct list_head thread_group' to 'struct task_struct'
    instead.
    
    We don't detach group leader from PIDTYPE_PID namespace until another
    thread inherits it's ->pid == ->tgid, so we are safe wrt premature
    free_pidmap(->tgid) call.
    
    Currently there are no users of find_task_by_pid_type(PIDTYPE_TGID).
    Should the need arise, we can use find_task_by_pid()->group_leader.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-By: Eric Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index aea23e713cf4..22399caf7574 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -51,7 +51,6 @@ static void __unhash_process(struct task_struct *p)
 {
 	nr_threads--;
 	detach_pid(p, PIDTYPE_PID);
-	detach_pid(p, PIDTYPE_TGID);
 	if (thread_group_leader(p)) {
 		detach_pid(p, PIDTYPE_PGID);
 		detach_pid(p, PIDTYPE_SID);
@@ -59,7 +58,7 @@ static void __unhash_process(struct task_struct *p)
 		list_del_init(&p->tasks);
 		__get_cpu_var(process_counts)--;
 	}
-
+	list_del_rcu(&p->thread_group);
 	remove_parent(p);
 }
 
@@ -964,13 +963,6 @@ asmlinkage long sys_exit(int error_code)
 	do_exit((error_code&0xff)<<8);
 }
 
-task_t fastcall *next_thread(const task_t *p)
-{
-	return pid_task(p->pids[PIDTYPE_TGID].pid_list.next, PIDTYPE_TGID);
-}
-
-EXPORT_SYMBOL(next_thread);
-
 /*
  * Take down every thread in the group.  This is called by fatal signals
  * as well as by sys_exit_group (below).

commit aacc90944d4b1f2fcec73a8127eb60a3a701ce1c
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:23 2006 -0800

    [PATCH] do_group_exit: don't take tasklist_lock
    
    do_group_exit() takes tasklist_lock for zap_other_threads(), this is unneeded
    now.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 44d6c6e3896d..aea23e713cf4 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -985,7 +985,6 @@ do_group_exit(int exit_code)
 	else if (!thread_group_empty(current)) {
 		struct signal_struct *const sig = current->signal;
 		struct sighand_struct *const sighand = current->sighand;
-		read_lock(&tasklist_lock);
 		spin_lock_irq(&sighand->siglock);
 		if (sig->flags & SIGNAL_GROUP_EXIT)
 			/* Another thread got here before we took the lock.  */
@@ -995,7 +994,6 @@ do_group_exit(int exit_code)
 			zap_other_threads(current);
 		}
 		spin_unlock_irq(&sighand->siglock);
-		read_unlock(&tasklist_lock);
 	}
 
 	do_exit(exit_code);

commit 5876700cd399112ecfa70df36203c8c6660d84f8
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:20 2006 -0800

    [PATCH] do __unhash_process() under ->siglock
    
    This patch moves __unhash_process() call from realease_task() to
    __exit_signal(), so __detach_pid() is called with ->siglock held.
    
    This means we don't need tasklist_lock to iterate over thread group anymore:
    
            copy_process() was already changed to do attach_pid()
            under ->siglock.
    
            Eric's "pidhash-kill-switch_exec_pids.patch" from -mm
            changed de_thread() so it doesn't touch PIDTYPE_TGID.
    
    NOTE: de_thread() still needs some attention.  It still changes task->pid
    lockless.  Taking ->sighand.siglock here allows to do more tasklist_lock
    removals.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6b2e4cf3e140..44d6c6e3896d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -112,6 +112,8 @@ static void __exit_signal(struct task_struct *tsk)
 		sig = NULL; /* Marker for below. */
 	}
 
+	__unhash_process(tsk);
+
 	tsk->signal = NULL;
 	cleanup_sighand(tsk);
 	spin_unlock(&sighand->siglock);
@@ -140,8 +142,6 @@ void release_task(struct task_struct * p)
 	BUG_ON(!list_empty(&p->ptrace_list) || !list_empty(&p->ptrace_children));
 	__exit_signal(p);
 
-	__unhash_process(p);
-
 	/*
 	 * If we are the last non-leader member of the thread
 	 * group, and the leader is zombie, then notify the

commit 35f5cad8c4bab94ecc5acdc4055df5ea12dc76f8
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:19 2006 -0800

    [PATCH] revert "Optimize sys_times for a single thread process"
    
    This patch reverts 'CONFIG_SMP && thread_group_empty()' optimization in
    sys_times().  The reason is that the next patch breaks memory ordering which
    is needed for that optimization.
    
    tasklist_lock in sys_times() will be eliminated completely by further patch.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3823ec89d7b8..6b2e4cf3e140 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -139,11 +139,7 @@ void release_task(struct task_struct * p)
 	ptrace_unlink(p);
 	BUG_ON(!list_empty(&p->ptrace_list) || !list_empty(&p->ptrace_children));
 	__exit_signal(p);
-	/*
-	 * Note that the fastpath in sys_times depends on __exit_signal having
-	 * updated the counters before a task is removed from the tasklist of
-	 * the process by __unhash_process.
-	 */
+
 	__unhash_process(p);
 
 	/*

commit 6a14c5c9da0b4c34b5be783403c54f0396fcfe77
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:18 2006 -0800

    [PATCH] move __exit_signal() to kernel/exit.c
    
    __exit_signal() is private to release_task() now.  I think it is better to
    make it static in kernel/exit.c and export flush_sigqueue() instead - this
    function is much more simple and straightforward.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 77c35efad88c..3823ec89d7b8 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -29,6 +29,7 @@
 #include <linux/cpuset.h>
 #include <linux/syscalls.h>
 #include <linux/signal.h>
+#include <linux/posix-timers.h>
 #include <linux/cn_proc.h>
 #include <linux/mutex.h>
 #include <linux/futex.h>
@@ -62,6 +63,68 @@ static void __unhash_process(struct task_struct *p)
 	remove_parent(p);
 }
 
+/*
+ * This function expects the tasklist_lock write-locked.
+ */
+static void __exit_signal(struct task_struct *tsk)
+{
+	struct signal_struct *sig = tsk->signal;
+	struct sighand_struct *sighand;
+
+	BUG_ON(!sig);
+	BUG_ON(!atomic_read(&sig->count));
+
+	rcu_read_lock();
+	sighand = rcu_dereference(tsk->sighand);
+	spin_lock(&sighand->siglock);
+
+	posix_cpu_timers_exit(tsk);
+	if (atomic_dec_and_test(&sig->count))
+		posix_cpu_timers_exit_group(tsk);
+	else {
+		/*
+		 * If there is any task waiting for the group exit
+		 * then notify it:
+		 */
+		if (sig->group_exit_task && atomic_read(&sig->count) == sig->notify_count) {
+			wake_up_process(sig->group_exit_task);
+			sig->group_exit_task = NULL;
+		}
+		if (tsk == sig->curr_target)
+			sig->curr_target = next_thread(tsk);
+		/*
+		 * Accumulate here the counters for all threads but the
+		 * group leader as they die, so they can be added into
+		 * the process-wide totals when those are taken.
+		 * The group leader stays around as a zombie as long
+		 * as there are other threads.  When it gets reaped,
+		 * the exit.c code will add its counts into these totals.
+		 * We won't ever get here for the group leader, since it
+		 * will have been the last reference on the signal_struct.
+		 */
+		sig->utime = cputime_add(sig->utime, tsk->utime);
+		sig->stime = cputime_add(sig->stime, tsk->stime);
+		sig->min_flt += tsk->min_flt;
+		sig->maj_flt += tsk->maj_flt;
+		sig->nvcsw += tsk->nvcsw;
+		sig->nivcsw += tsk->nivcsw;
+		sig->sched_time += tsk->sched_time;
+		sig = NULL; /* Marker for below. */
+	}
+
+	tsk->signal = NULL;
+	cleanup_sighand(tsk);
+	spin_unlock(&sighand->siglock);
+	rcu_read_unlock();
+
+	clear_tsk_thread_flag(tsk,TIF_SIGPENDING);
+	flush_sigqueue(&tsk->pending);
+	if (sig) {
+		flush_sigqueue(&sig->shared_pending);
+		__cleanup_signal(sig);
+	}
+}
+
 void release_task(struct task_struct * p)
 {
 	int zap_leader;

commit 1f09f9749cdde4e69f95d62d96d2e03f50b3353c
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:11 2006 -0800

    [PATCH] release_task: replace open-coded ptrace_unlink()
    
    Use ptrace_unlink() instead of open-coding.  No changes in kernel/exit.o
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 98eec590ecbd..77c35efad88c 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -68,13 +68,12 @@ void release_task(struct task_struct * p)
 	task_t *leader;
 	struct dentry *proc_dentry;
 
-repeat: 
+repeat:
 	atomic_dec(&p->user->processes);
 	spin_lock(&p->proc_lock);
 	proc_dentry = proc_pid_unhash(p);
 	write_lock_irq(&tasklist_lock);
-	if (unlikely(p->ptrace))
-		__ptrace_unlink(p);
+	ptrace_unlink(p);
 	BUG_ON(!list_empty(&p->ptrace_list) || !list_empty(&p->ptrace_children));
 	__exit_signal(p);
 	/*

commit 6ac781b11ade6e3451f6b460991c8b2b87e58280
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:09 2006 -0800

    [PATCH] reparent_thread: use remove_parent/add_parent
    
    Use remove_parent/add_parent instead of open coding.
    
    No changes in kernel/exit.o
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a94e1c31131b..98eec590ecbd 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -555,9 +555,9 @@ static void reparent_thread(task_t *p, task_t *father, int traced)
 		 * anyway, so let go of it.
 		 */
 		p->ptrace = 0;
-		list_del_init(&p->sibling);
+		remove_parent(p);
 		p->parent = p->real_parent;
-		list_add_tail(&p->sibling, &p->parent->children);
+		add_parent(p);
 
 		/* If we'd notified the old parent about this child's death,
 		 * also notify the new parent.

commit 73b9ebfe126a4a886ee46cbab637374d7024668a
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:07 2006 -0800

    [PATCH] pidhash: don't count idle threads
    
    fork_idle() does unhash_process() just after copy_process().  Contrary,
    boot_cpu's idle thread explicitely registers itself for each pid_type with nr
    = 0.
    
    copy_process() already checks p->pid != 0 before process_counts++, I think we
    can just skip attach_pid() calls and job control inits for idle threads and
    kill unhash_process().  We don't need to cleanup ->proc_dentry in fork_idle()
    because with this patch idle threads are never hashed in
    kernel/pid.c:pid_hash[].
    
    We don't need to hash pid == 0 in pidmap_init().  free_pidmap() is never
    called with pid == 0 arg, so it will never be reused.  So it is still possible
    to use pid == 0 in any PIDTYPE_xxx namespace from kernel/pid.c's POV.
    
    However with this patch we don't hash pid == 0 for PIDTYPE_PID case.  We still
    have have PIDTYPE_PGID/PIDTYPE_SID entries with pid == 0: /sbin/init and
    kernel threads which don't call daemonize().
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f436a6bd3fb7..a94e1c31131b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -56,8 +56,7 @@ static void __unhash_process(struct task_struct *p)
 		detach_pid(p, PIDTYPE_SID);
 
 		list_del_init(&p->tasks);
-		if (p->pid)
-			__get_cpu_var(process_counts)--;
+		__get_cpu_var(process_counts)--;
 	}
 
 	remove_parent(p);
@@ -118,21 +117,6 @@ void release_task(struct task_struct * p)
 		goto repeat;
 }
 
-/* we are using it only for SMP init */
-
-void unhash_process(struct task_struct *p)
-{
-	struct dentry *proc_dentry;
-
-	spin_lock(&p->proc_lock);
-	proc_dentry = proc_pid_unhash(p);
-	write_lock_irq(&tasklist_lock);
-	__unhash_process(p);
-	write_unlock_irq(&tasklist_lock);
-	spin_unlock(&p->proc_lock);
-	proc_pid_flush(proc_dentry);
-}
-
 /*
  * This checks not only the pgrp, but falls back on the pid if no
  * satisfactory pgrp is found. I dunno - gdb doesn't work correctly

commit c97d98931ac52ef110b62d9b75c6a6f2bfbc1898
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:06 2006 -0800

    [PATCH] kill SET_LINKS/REMOVE_LINKS
    
    Both SET_LINKS() and SET_LINKS/REMOVE_LINKS() have exactly one caller, and
    these callers already check thread_group_leader().
    
    This patch kills theese macros, they mix two different things: setting
    process's parent and registering it in init_task.tasks list.  Callers are
    updated to do these actions by hand.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5b5e8b67680e..f436a6bd3fb7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -54,11 +54,13 @@ static void __unhash_process(struct task_struct *p)
 	if (thread_group_leader(p)) {
 		detach_pid(p, PIDTYPE_PGID);
 		detach_pid(p, PIDTYPE_SID);
+
+		list_del_init(&p->tasks);
 		if (p->pid)
 			__get_cpu_var(process_counts)--;
 	}
 
-	REMOVE_LINKS(p);
+	remove_parent(p);
 }
 
 void release_task(struct task_struct * p)

commit 9b678ece42893b53aae5ed7cb8d7cb261cacb72c
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:05 2006 -0800

    [PATCH] don't use REMOVE_LINKS/SET_LINKS for reparenting
    
    There are places where kernel uses REMOVE_LINKS/SET_LINKS while changing
    process's ->parent.  Use add_parent/remove_parent instead, they don't abuse
    of global process list.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index df26c33037d2..5b5e8b67680e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -238,10 +238,10 @@ static void reparent_to_init(void)
 
 	ptrace_unlink(current);
 	/* Reparent to init */
-	REMOVE_LINKS(current);
+	remove_parent(current);
 	current->parent = child_reaper;
 	current->real_parent = child_reaper;
-	SET_LINKS(current);
+	add_parent(current);
 
 	/* Set the exit signal to SIGCHLD so we signal init on exit */
 	current->exit_signal = SIGCHLD;

commit 8fafabd86f1b75ed3cc6a6ffbe6c3e53e3d8457d
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:05 2006 -0800

    [PATCH] remove add_parent()'s parent argument
    
    add_parent(p, parent) is always called with parent == p->parent, and it makes
    no sense to do it differently.  This patch removes this argument.
    
    No changes in affected .o files.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e04a59405e99..df26c33037d2 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1281,7 +1281,7 @@ static int wait_task_stopped(task_t *p, int delayed_group_leader, int noreap,
 
 	/* move to end of parent's list to avoid starvation */
 	remove_parent(p);
-	add_parent(p, p->parent);
+	add_parent(p);
 
 	write_unlock_irq(&tasklist_lock);
 

commit d799f03597cabc6112acb518fc8ab4487aa4f953
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Tue Mar 28 16:11:04 2006 -0800

    [PATCH] choose_new_parent: remove unused arg, sanitize exit_state check
    
    'child_reaper' arg is not used in choose_new_parent().
    
    "->exit_state >= EXIT_ZOMBIE" check is a leftover, was
    valid when EXIT_ZOMBIE lived in ->state var.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: Eric Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 223a8802b665..e04a59405e99 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -538,13 +538,13 @@ static void exit_mm(struct task_struct * tsk)
 	mmput(mm);
 }
 
-static inline void choose_new_parent(task_t *p, task_t *reaper, task_t *child_reaper)
+static inline void choose_new_parent(task_t *p, task_t *reaper)
 {
 	/*
 	 * Make sure we're not reparenting to ourselves and that
 	 * the parent is not a zombie.
 	 */
-	BUG_ON(p == reaper || reaper->exit_state >= EXIT_ZOMBIE);
+	BUG_ON(p == reaper || reaper->exit_state);
 	p->real_parent = reaper;
 }
 
@@ -645,7 +645,7 @@ static void forget_original_parent(struct task_struct * father,
 
 		if (father == p->real_parent) {
 			/* reparent with a reaper, real father it's us */
-			choose_new_parent(p, reaper, child_reaper);
+			choose_new_parent(p, reaper);
 			reparent_thread(p, father, 0);
 		} else {
 			/* reparent ptraced task to its real parent */
@@ -666,7 +666,7 @@ static void forget_original_parent(struct task_struct * father,
 	}
 	list_for_each_safe(_p, _n, &father->ptrace_children) {
 		p = list_entry(_p,struct task_struct,ptrace_list);
-		choose_new_parent(p, reaper, child_reaper);
+		choose_new_parent(p, reaper);
 		reparent_thread(p, father, 1);
 	}
 }

commit fef23e7fbb11a0a78cd61935f7056bc2b237995a
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Mar 28 16:10:58 2006 -0800

    [PATCH] exec: allow init to exec from any thread.
    
    After looking at the problem of init calling exec some more I figured out
    an easy way to make the code work.
    
    The actual symptom without out this patch is that all threads will die
    except pid == 1, and the thread calling exec.  The thread calling exec will
    wait forever for pid == 1 to die.
    
    Since pid == 1 does not install a handler for SIGKILL it will never die.
    
    This modifies the tests for init from current->pid == 1 to the equivalent
    current == child_reaper.  And then it causes exec in the ugly case to
    modify child_reaper.
    
    The only weird symptom is that you wind up with an init process that
    doesn't have the oldest start time on the box.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a8c7efc7a681..223a8802b665 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -807,7 +807,7 @@ fastcall NORET_TYPE void do_exit(long code)
 		panic("Aiee, killing interrupt handler!");
 	if (unlikely(!tsk->pid))
 		panic("Attempted to kill the idle task!");
-	if (unlikely(tsk->pid == 1))
+	if (unlikely(tsk == child_reaper))
 		panic("Attempted to kill init!");
 
 	if (unlikely(current->ptrace & PT_TRACE_EXIT)) {

commit 34f192c6527f20c47ccec239e7d51a27691b93fc
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 27 01:16:24 2006 -0800

    [PATCH] lightweight robust futexes: compat
    
    32-bit syscall compatibility support.  (This patch also moves all futex
    related compat functionality into kernel/futex_compat.c.)
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Acked-by: Ulrich Drepper <drepper@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index aecb48ca7370..a8c7efc7a681 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -32,6 +32,7 @@
 #include <linux/cn_proc.h>
 #include <linux/mutex.h>
 #include <linux/futex.h>
+#include <linux/compat.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -855,6 +856,10 @@ fastcall NORET_TYPE void do_exit(long code)
 	}
 	if (unlikely(tsk->robust_list))
 		exit_robust_list(tsk);
+#ifdef CONFIG_COMPAT
+	if (unlikely(tsk->compat_robust_list))
+		compat_exit_robust_list(tsk);
+#endif
 	exit_mm(tsk);
 
 	exit_sem(tsk);

commit 0771dfefc9e538f077d0b43b6dec19a5a67d0e70
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 27 01:16:22 2006 -0800

    [PATCH] lightweight robust futexes: core
    
    Add the core infrastructure for robust futexes: structure definitions, the new
    syscalls and the do_exit() based cleanup mechanism.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Acked-by: Ulrich Drepper <drepper@redhat.com>
    Cc: Michael Kerrisk <mtk-manpages@gmx.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 8037405e136e..aecb48ca7370 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -31,6 +31,7 @@
 #include <linux/signal.h>
 #include <linux/cn_proc.h>
 #include <linux/mutex.h>
+#include <linux/futex.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -852,6 +853,8 @@ fastcall NORET_TYPE void do_exit(long code)
 		exit_itimers(tsk->signal);
 		acct_process(code);
 	}
+	if (unlikely(tsk->robust_list))
+		exit_robust_list(tsk);
 	exit_mm(tsk);
 
 	exit_sem(tsk);

commit 70522e121a521aa09bd0f4e62e1aa68708b798e1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 23 03:00:31 2006 -0800

    [PATCH] sem2mutex: tty
    
    Semaphore to mutex conversion.
    
    The conversion was generated via scripts, and the result was validated
    automatically via a script as well.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Alan Cox <alan@lxorguk.ukuu.org.uk>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index d1e8d500a7e1..8037405e136e 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -345,9 +345,9 @@ void daemonize(const char *name, ...)
 	exit_mm(current);
 
 	set_special_pids(1, 1);
-	down(&tty_sem);
+	mutex_lock(&tty_mutex);
 	current->signal->tty = NULL;
-	up(&tty_sem);
+	mutex_unlock(&tty_mutex);
 
 	/* Block and flush all signals */
 	sigfillset(&blocked);

commit afc847b7ddcf636e524cf5b0de644bd3a9419a8c
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Feb 28 12:51:55 2006 -0500

    [PATCH] don't do exit_io_context() until we know we won't be doing any IO
    
    testcase:
    
    mount /dev/sdb10 /mnt
    touch /mnt/tmp/b
    umount /mnt
    mount /dev/sdb10 /mnt
    rm /mnt/tmp/b </mnt/tmp/b
    umount /mnt
    
    and watch blkdev_ioc line in /proc/slabinfo.  Vanilla kernel leaks.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/exit.c b/kernel/exit.c
index 531aadca5530..d1e8d500a7e1 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -807,8 +807,6 @@ fastcall NORET_TYPE void do_exit(long code)
 		panic("Attempted to kill the idle task!");
 	if (unlikely(tsk->pid == 1))
 		panic("Attempted to kill init!");
-	if (tsk->io_context)
-		exit_io_context();
 
 	if (unlikely(current->ptrace & PT_TRACE_EXIT)) {
 		current->ptrace_message = code;
@@ -822,6 +820,8 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (unlikely(tsk->flags & PF_EXITING)) {
 		printk(KERN_ALERT
 			"Fixing recursive fault but reboot is needed!\n");
+		if (tsk->io_context)
+			exit_io_context();
 		set_current_state(TASK_UNINTERRUPTIBLE);
 		schedule();
 	}
@@ -881,6 +881,9 @@ fastcall NORET_TYPE void do_exit(long code)
 	 */
 	mutex_debug_check_no_locks_held(tsk);
 
+	if (tsk->io_context)
+		exit_io_context();
+
 	/* PF_DEAD causes final put_task_struct after we schedule. */
 	preempt_disable();
 	BUG_ON(tsk->flags & PF_DEAD);

commit 5914811acf36c3ff091f860a6964808f668f27d0
Author: Bjrn Steinbrink <B.Steinbrink@gmx.de>
Date:   Sat Feb 18 18:12:43 2006 +0100

    [PATCH] kjournald keeps reference to namespace
    
    In daemonize() a new thread gets cleaned up and 'merged' with init_task.
    The current fs_struct is handled there, but not the current namespace.
    
    This adds the namespace part.
    
    [ Eric Biederman pointed out the namespace wrappers, and also notes that
      we can't ever count on using our parents namespace because we already
      have called exit_fs(), which is the only way to the namespace from a
      process. ]
    
    Signed-off-by: Bjrn Steinbrink <B.Steinbrink@gmx.de>
    Acked-by: Eric Biederman <ebiederm@xmission.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 93cee3671332..531aadca5530 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -360,6 +360,9 @@ void daemonize(const char *name, ...)
 	fs = init_task.fs;
 	current->fs = fs;
 	atomic_inc(&fs->count);
+	exit_namespace(current);
+	current->namespace = init_task.namespace;
+	get_namespace(current->namespace);
  	exit_files(current);
 	current->files = init_task.files;
 	atomic_inc(&current->files->count);

commit 858119e159384308a5dde67776691a2ebf70df0f
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Sat Jan 14 13:20:43 2006 -0800

    [PATCH] Unlinline a bunch of other functions
    
    Remove the "inline" keyword from a bunch of big functions in the kernel with
    the goal of shrinking it by 30kb to 40kb
    
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Jeff Garzik <jgarzik@pobox.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7fb541cb8d69..93cee3671332 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -193,7 +193,7 @@ int is_orphaned_pgrp(int pgrp)
 	return retval;
 }
 
-static inline int has_stopped_jobs(int pgrp)
+static int has_stopped_jobs(int pgrp)
 {
 	int retval = 0;
 	struct task_struct *p;
@@ -230,7 +230,7 @@ static inline int has_stopped_jobs(int pgrp)
  *
  * NOTE that reparent_to_init() gives the caller full capabilities.
  */
-static inline void reparent_to_init(void)
+static void reparent_to_init(void)
 {
 	write_lock_irq(&tasklist_lock);
 
@@ -369,7 +369,7 @@ void daemonize(const char *name, ...)
 
 EXPORT_SYMBOL(daemonize);
 
-static inline void close_files(struct files_struct * files)
+static void close_files(struct files_struct * files)
 {
 	int i, j;
 	struct fdtable *fdt;
@@ -543,7 +543,7 @@ static inline void choose_new_parent(task_t *p, task_t *reaper, task_t *child_re
 	p->real_parent = reaper;
 }
 
-static inline void reparent_thread(task_t *p, task_t *father, int traced)
+static void reparent_thread(task_t *p, task_t *father, int traced)
 {
 	/* We don't want people slaying init.  */
 	if (p->exit_signal != -1)
@@ -607,7 +607,7 @@ static inline void reparent_thread(task_t *p, task_t *father, int traced)
  * group, and if no such member exists, give it to
  * the global child reaper process (ie "init")
  */
-static inline void forget_original_parent(struct task_struct * father,
+static void forget_original_parent(struct task_struct * father,
 					  struct list_head *to_release)
 {
 	struct task_struct *p, *reaper = father;

commit b0a9499c3dd50d333e2aedb7e894873c58da3785
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jan 14 13:20:41 2006 -0800

    [PATCH] sched: add new SCHED_BATCH policy
    
    Add a new SCHED_BATCH (3) scheduling policy: such tasks are presumed
    CPU-intensive, and will acquire a constant +5 priority level penalty.  Such
    policy is nice for workloads that are non-interactive, but which do not
    want to give up their nice levels.  The policy is also useful for workloads
    that want a deterministic scheduling policy without interactivity causing
    extra preemptions (between that workload's tasks).
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Cc: Michael Kerrisk <mtk-manpages@gmx.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index f8e609ff1893..7fb541cb8d69 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -244,7 +244,9 @@ static inline void reparent_to_init(void)
 	/* Set the exit signal to SIGCHLD so we signal init on exit */
 	current->exit_signal = SIGCHLD;
 
-	if ((current->policy == SCHED_NORMAL) && (task_nice(current) < 0))
+	if ((current->policy == SCHED_NORMAL ||
+			current->policy == SCHED_BATCH)
+				&& (task_nice(current) < 0))
 		set_user_nice(current, 0);
 	/* cpus_allowed? */
 	/* rt_priority? */

commit c59ede7b78db329949d9cdcd7064e22d357560ef
Author: Randy.Dunlap <rdunlap@xenotime.net>
Date:   Wed Jan 11 12:17:46 2006 -0800

    [PATCH] move capable() to capability.h
    
    - Move capable() from sched.h to capability.h;
    
    - Use <linux/capability.h> where capable() is used
            (in include/, block/, ipc/, kernel/, a few drivers/,
            mm/, security/, & sound/;
            many more drivers/ to go)
    
    Signed-off-by: Randy Dunlap <rdunlap@xenotime.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 802722814925..f8e609ff1893 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -10,6 +10,7 @@
 #include <linux/interrupt.h>
 #include <linux/smp_lock.h>
 #include <linux/module.h>
+#include <linux/capability.h>
 #include <linux/completion.h>
 #include <linux/personality.h>
 #include <linux/tty.h>

commit 3795e1616f16905889761536cdc266ebc51855e5
Author: Jesper Juhl <jesper.juhl@gmail.com>
Date:   Mon Jan 9 20:54:39 2006 -0800

    [PATCH] Decrease number of pointer derefs in exit.c
    
    Decrease the number of pointer derefs in kernel/exit.c
    
    Benefits of the patch:
     - Fewer pointer dereferences should make the code slightly faster.
     - Size of generated code is smaller
     - improved readability
    
    Signed-off-by: Jesper Juhl <jesper.juhl@gmail.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index e75a51f33768..802722814925 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1071,6 +1071,9 @@ static int wait_task_zombie(task_t *p, int noreap,
 	}
 
 	if (likely(p->real_parent == p->parent) && likely(p->signal)) {
+		struct signal_struct *psig;
+		struct signal_struct *sig;
+
 		/*
 		 * The resource counters for the group leader are in its
 		 * own task_struct.  Those for dead threads in the group
@@ -1087,24 +1090,26 @@ static int wait_task_zombie(task_t *p, int noreap,
 		 * here reaping other children at the same time.
 		 */
 		spin_lock_irq(&p->parent->sighand->siglock);
-		p->parent->signal->cutime =
-			cputime_add(p->parent->signal->cutime,
+		psig = p->parent->signal;
+		sig = p->signal;
+		psig->cutime =
+			cputime_add(psig->cutime,
 			cputime_add(p->utime,
-			cputime_add(p->signal->utime,
-				    p->signal->cutime)));
-		p->parent->signal->cstime =
-			cputime_add(p->parent->signal->cstime,
+			cputime_add(sig->utime,
+				    sig->cutime)));
+		psig->cstime =
+			cputime_add(psig->cstime,
 			cputime_add(p->stime,
-			cputime_add(p->signal->stime,
-				    p->signal->cstime)));
-		p->parent->signal->cmin_flt +=
-			p->min_flt + p->signal->min_flt + p->signal->cmin_flt;
-		p->parent->signal->cmaj_flt +=
-			p->maj_flt + p->signal->maj_flt + p->signal->cmaj_flt;
-		p->parent->signal->cnvcsw +=
-			p->nvcsw + p->signal->nvcsw + p->signal->cnvcsw;
-		p->parent->signal->cnivcsw +=
-			p->nivcsw + p->signal->nivcsw + p->signal->cnivcsw;
+			cputime_add(sig->stime,
+				    sig->cstime)));
+		psig->cmin_flt +=
+			p->min_flt + sig->min_flt + sig->cmin_flt;
+		psig->cmaj_flt +=
+			p->maj_flt + sig->maj_flt + sig->cmaj_flt;
+		psig->cnvcsw +=
+			p->nvcsw + sig->nvcsw + sig->cnvcsw;
+		psig->cnivcsw +=
+			p->nivcsw + sig->nivcsw + sig->cnivcsw;
 		spin_unlock_irq(&p->parent->sighand->siglock);
 	}
 

commit 2ff678b8da6478d861c1b0ecb3ac14575760e906
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jan 9 20:52:34 2006 -0800

    [PATCH] hrtimer: switch itimers to hrtimer
    
    switch itimers to a hrtimers-based implementation
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 309a46fa16f8..e75a51f33768 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -842,7 +842,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	}
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
- 		del_timer_sync(&tsk->signal->real_timer);
+ 		hrtimer_cancel(&tsk->signal->real_timer);
 		exit_itimers(tsk->signal);
 		acct_process(code);
 	}

commit de5097c2e73f826302cd8957c225b3725e0c7553
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jan 9 15:59:21 2006 -0800

    [PATCH] mutex subsystem, more debugging code
    
    more mutex debugging: check for held locks during memory freeing,
    task exit, enable sysrq printouts, etc.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Arjan van de Ven <arjan@infradead.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index caceabf3f230..309a46fa16f8 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -29,6 +29,7 @@
 #include <linux/syscalls.h>
 #include <linux/signal.h>
 #include <linux/cn_proc.h>
+#include <linux/mutex.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -869,6 +870,10 @@ fastcall NORET_TYPE void do_exit(long code)
 	mpol_free(tsk->mempolicy);
 	tsk->mempolicy = NULL;
 #endif
+	/*
+	 * If DEBUG_MUTEXES is on, make sure we are holding no locks:
+	 */
+	mutex_debug_check_no_locks_held(tsk);
 
 	/* PF_DEAD causes final put_task_struct after we schedule. */
 	preempt_disable();

commit e19f247a3dbd85485ec13174817ae9c2478fe541
Author: Oren Laadan <orenl@cs.columbia.edu>
Date:   Sun Jan 8 01:03:58 2006 -0800

    [PATCH] setpgid: should work for sub-threads
    
    setsid() does not work unless the calling process is a
    thread_group_leader().
    
    'man setpgid' does not tell anything about that, so I consider this
    behaviour is a bug.
    
    Signed-off-by: Oren Laadan <orenl@cs.columbia.edu>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index a80824f6108b..caceabf3f230 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -257,7 +257,7 @@ static inline void reparent_to_init(void)
 
 void __set_special_pids(pid_t session, pid_t pgrp)
 {
-	struct task_struct *curr = current;
+	struct task_struct *curr = current->group_leader;
 
 	if (curr->signal->session != session) {
 		detach_pid(curr, PIDTYPE_SID);

commit 485a6435abc3897934ce0dc530e31db93e9296a6
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Jan 8 01:03:14 2006 -0800

    [PATCH] little do_group_exit() cleanup
    
    zap_other_threads() sets SIGNAL_GROUP_EXIT at the very start,
    do_group_exit() doesn't need to do it.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c73a7eb26de3..a80824f6108b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -925,7 +925,6 @@ do_group_exit(int exit_code)
 			/* Another thread got here before we took the lock.  */
 			exit_code = sig->group_exit_code;
 		else {
-			sig->flags = SIGNAL_GROUP_EXIT;
 			sig->group_exit_code = exit_code;
 			zap_other_threads(current);
 		}

commit e56d090310d7625ecb43a1eeebd479f04affb48b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 8 01:01:37 2006 -0800

    [PATCH] RCU signal handling
    
    RCU tasklist_lock and RCU signal handling: send signals RCU-read-locked
    instead of tasklist_lock read-locked.  This is a scalability improvement on
    SMP and a preemption-latency improvement under PREEMPT_RCU.
    
    Signed-off-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: William Irwin <wli@holomorphy.com>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ee515683b92d..c73a7eb26de3 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -72,7 +72,6 @@ void release_task(struct task_struct * p)
 		__ptrace_unlink(p);
 	BUG_ON(!list_empty(&p->ptrace_list) || !list_empty(&p->ptrace_children));
 	__exit_signal(p);
-	__exit_sighand(p);
 	/*
 	 * Note that the fastpath in sys_times depends on __exit_signal having
 	 * updated the counters before a task is removed from the tasklist of

commit a1261f54611ec4ad6a7ab7080f86747e3ac3685b
Author: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
Date:   Sun Nov 13 16:06:55 2005 -0800

    [PATCH] m68k: introduce task_thread_info
    
    new helper - task_thread_info(task).  On platforms that have thread_info
    allocated separately (i.e.  in default case) it simply returns
    task->thread_info.  m68k wants (and for good reasons) to embed its thread_info
    into task_struct.  So it will (in later patch) have task_thread_info() of its
    own.  For now we just add a macro for generic case and convert existing
    instances of its body in core kernel to uses of new macro.  Obviously safe -
    all normal architectures get the same preprocessor output they used to get.
    
    Signed-off-by: Al Viro <viro@parcelfarce.linux.theplanet.co.uk>
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 452a1d116178..ee515683b92d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -859,7 +859,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	if (group_dead && tsk->signal->leader)
 		disassociate_ctty(1);
 
-	module_put(tsk->thread_info->exec_domain->module);
+	module_put(task_thread_info(tsk)->exec_domain->module);
 	if (tsk->binfmt)
 		module_put(tsk->binfmt->module);
 

commit 9f46080c41d5f3f7c00b4e169ba4b0b2865258bf
Author: Matt Helsley <matthltc@us.ibm.com>
Date:   Mon Nov 7 00:59:16 2005 -0800

    [PATCH] Process Events Connector
    
    This patch adds a connector that reports fork, exec, id change, and exit
    events for all processes to userspace.  It replaces the fork_advisor patch
    that ELSA is currently using.  Applications that may find these events
    useful include accounting/auditing (e.g.  ELSA), system activity monitoring
    (e.g.  top), security, and resource management (e.g.  CKRM).
    
    Signed-off-by: Matt Helsley <matthltc@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 537394b25e8d..452a1d116178 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -28,6 +28,7 @@
 #include <linux/cpuset.h>
 #include <linux/syscalls.h>
 #include <linux/signal.h>
+#include <linux/cn_proc.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -863,6 +864,7 @@ fastcall NORET_TYPE void do_exit(long code)
 		module_put(tsk->binfmt->module);
 
 	tsk->exit_code = code;
+	proc_exit_connector(tsk);
 	exit_notify(tsk);
 #ifdef CONFIG_NUMA
 	mpol_free(tsk->mempolicy);

commit b67a1b9e4bf878aa5d4b6b44cb5a251a2f425f0d
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Sun Oct 30 15:03:44 2005 -0800

    [PATCH] remove hardcoded SEND_SIG_xxx constants
    
    This patch replaces hardcoded SEND_SIG_xxx constants with
    their symbolic names.
    
    No changes in affected .o files.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2d39ccc367e6..537394b25e8d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -547,7 +547,7 @@ static inline void reparent_thread(task_t *p, task_t *father, int traced)
 
 	if (p->pdeath_signal)
 		/* We already hold the tasklist_lock here.  */
-		group_send_sig_info(p->pdeath_signal, (void *) 0, p);
+		group_send_sig_info(p->pdeath_signal, SEND_SIG_NOINFO, p);
 
 	/* Move the child from its dying parent to the new one.  */
 	if (unlikely(traced)) {
@@ -591,8 +591,8 @@ static inline void reparent_thread(task_t *p, task_t *father, int traced)
 		int pgrp = process_group(p);
 
 		if (will_become_orphaned_pgrp(pgrp, NULL) && has_stopped_jobs(pgrp)) {
-			__kill_pg_info(SIGHUP, (void *)1, pgrp);
-			__kill_pg_info(SIGCONT, (void *)1, pgrp);
+			__kill_pg_info(SIGHUP, SEND_SIG_PRIV, pgrp);
+			__kill_pg_info(SIGCONT, SEND_SIG_PRIV, pgrp);
 		}
 	}
 }
@@ -727,8 +727,8 @@ static void exit_notify(struct task_struct *tsk)
 	    (t->signal->session == tsk->signal->session) &&
 	    will_become_orphaned_pgrp(process_group(tsk), tsk) &&
 	    has_stopped_jobs(process_group(tsk))) {
-		__kill_pg_info(SIGHUP, (void *)1, process_group(tsk));
-		__kill_pg_info(SIGCONT, (void *)1, process_group(tsk));
+		__kill_pg_info(SIGHUP, SEND_SIG_PRIV, process_group(tsk));
+		__kill_pg_info(SIGCONT, SEND_SIG_PRIV, process_group(tsk));
 	}
 
 	/* Let father know we died 

commit 7f2a52555998c699a7e89f24636c909d6fc08a60
Author: Roland McGrath <roland@redhat.com>
Date:   Sun Oct 30 15:02:50 2005 -0800

    [PATCH] wait4 PTRACE_ATTACH race fix
    
    Back about a year ago when I last fiddled heavily with the do_wait code, I
    was thinking too hard about the wrong thing and I now think I introduced a
    bug whose inverse thought I was fixing.
    
    Apparently noone was looking too hard over much shoulder, so as to cite my
    bogus reasoning at the time.  In the race condition when PTRACE_ATTACH is
    about to steal a child and then the child hits a tracing event (what
    my_ptrace_child checks for), the real parent does need to set its flag
    noting it has some eligible live children.  Otherwise a spurious ECHILD
    error is possible, since the child in question is not yet on the
    ptrace_children list.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6ef8f7356a74..2d39ccc367e6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1383,6 +1383,15 @@ static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
 
 			switch (p->state) {
 			case TASK_TRACED:
+				/*
+				 * When we hit the race with PTRACE_ATTACH,
+				 * we will not report this child.  But the
+				 * race means it has not yet been moved to
+				 * our ptrace_children list, so we need to
+				 * set the flag here to avoid a spurious ECHILD
+				 * when the race happens with the only child.
+				 */
+				flag = 1;
 				if (!my_ptrace_child(p))
 					continue;
 				/*FALLTHROUGH*/

commit 7407251a0e2ed099e4b12b742b635503e981507c
Author: Coywolf Qi Hunt <qiyong@fc-cn.com>
Date:   Sun Oct 30 15:02:47 2005 -0800

    [PATCH] PF_DEAD cleanup
    
    The PF_DEAD setting doesn't belong to exit_notify(), move it to a proper
    place.
    
    Signed-off-by: Coywolf Qi Hunt <qiyong@fc-cn.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 79f52b85d6ed..6ef8f7356a74 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -783,10 +783,6 @@ static void exit_notify(struct task_struct *tsk)
 	/* If the process is dead, release it - nobody will wait for it */
 	if (state == EXIT_DEAD)
 		release_task(tsk);
-
-	/* PF_DEAD causes final put_task_struct after we schedule. */
-	preempt_disable();
-	tsk->flags |= PF_DEAD;
 }
 
 fastcall NORET_TYPE void do_exit(long code)
@@ -873,7 +869,11 @@ fastcall NORET_TYPE void do_exit(long code)
 	tsk->mempolicy = NULL;
 #endif
 
-	BUG_ON(!(current->flags & PF_DEAD));
+	/* PF_DEAD causes final put_task_struct after we schedule. */
+	preempt_disable();
+	BUG_ON(tsk->flags & PF_DEAD);
+	tsk->flags |= PF_DEAD;
+
 	schedule();
 	BUG();
 	/* Avoid "noreturn function does return".  */

commit 365e9c87a982c03d0af3886e29d877f581b59611
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:18 2005 -0700

    [PATCH] mm: update_hiwaters just in time
    
    update_mem_hiwater has attracted various criticisms, in particular from those
    concerned with mm scalability.  Originally it was called whenever rss or
    total_vm got raised.  Then many of those callsites were replaced by a timer
    tick call from account_system_time.  Now Frank van Maarseveen reports that to
    be found inadequate.  How about this?  Works for Frank.
    
    Replace update_mem_hiwater, a poor combination of two unrelated ops, by macros
    update_hiwater_rss and update_hiwater_vm.  Don't attempt to keep
    mm->hiwater_rss up to date at timer tick, nor every time we raise rss (usually
    by 1): those are hot paths.  Do the opposite, update only when about to lower
    rss (usually by many), or just before final accounting in do_exit.  Handle
    mm->hiwater_vm in the same way, though it's much less of an issue.  Demand
    that whoever collects these hiwater statistics do the work of taking the
    maximum with rss or total_vm.
    
    And there has been no collector of these hiwater statistics in the tree.  The
    new convention needs an example, so match Frank's usage by adding a VmPeak
    line above VmSize to /proc/<pid>/status, and also a VmHWM line above VmRSS
    (High-Water-Mark or High-Water-Memory).
    
    There was a particular anomaly during mremap move, that hiwater_vm might be
    captured too high.  A fleeting such anomaly remains, but it's quickly
    corrected now, whereas before it would stick.
    
    What locking?  None: if the app is racy then these statistics will be racy,
    it's not worth any overhead to make them exact.  But whenever it suits,
    hiwater_vm is updated under exclusive mmap_sem, and hiwater_rss under
    page_table_lock (for now) or with preemption disabled (later on): without
    going to any trouble, minimize the time between reading current values and
    updating, to minimize those occasions when a racing thread bumps a count up
    and back down in between.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3b25b182d2be..79f52b85d6ed 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -839,7 +839,10 @@ fastcall NORET_TYPE void do_exit(long code)
 				preempt_count());
 
 	acct_update_integrals(tsk);
-	update_mem_hiwater(tsk);
+	if (tsk->mm) {
+		update_hiwater_rss(tsk->mm);
+		update_hiwater_vm(tsk->mm);
+	}
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
  		del_timer_sync(&tsk->signal->real_timer);

commit a362f463a6d316d14daed0f817e151835ce97ff7
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Thu Oct 27 09:07:33 2005 -0700

    Revert "remove false BUG_ON() from run_posix_cpu_timers()"
    
    This reverts commit 3de463c7d9d58f8cf3395268230cb20a4c15bffa.
    
    Roland has another patch that allows us to leave the BUG_ON() in place
    by just making sure that the condition it tests for really is always
    true.
    
    That goes in next.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 4897977a1f4b..3b25b182d2be 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -825,6 +825,14 @@ fastcall NORET_TYPE void do_exit(long code)
 
 	tsk->flags |= PF_EXITING;
 
+	/*
+	 * Make sure we don't try to process any timer firings
+	 * while we are already exiting.
+	 */
+ 	tsk->it_virt_expires = cputime_zero;
+ 	tsk->it_prof_expires = cputime_zero;
+	tsk->it_sched_expires = 0;
+
 	if (unlikely(in_atomic()))
 		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
 				current->comm, current->pid,

commit 3de463c7d9d58f8cf3395268230cb20a4c15bffa
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Mon Oct 24 14:34:03 2005 +0400

    [PATCH] posix-timers: remove false BUG_ON() from run_posix_cpu_timers()
    
    do_exit() clears ->it_##clock##_expires, but nothing prevents
    another cpu to attach the timer to exiting process after that.
    
    After exit_notify() does 'write_unlock_irq(&tasklist_lock)' and
    before do_exit() calls 'schedule() local timer interrupt can find
    tsk->exit_state != 0. If that state was EXIT_DEAD (or another cpu
    does sys_wait4) interrupted task has ->signal == NULL.
    
    At this moment exiting task has no pending cpu timers, they were cleaned
    up in __exit_signal()->posix_cpu_timers_exit{,_group}(), so we can just
    return from irq.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3b25b182d2be..4897977a1f4b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -825,14 +825,6 @@ fastcall NORET_TYPE void do_exit(long code)
 
 	tsk->flags |= PF_EXITING;
 
-	/*
-	 * Make sure we don't try to process any timer firings
-	 * while we are already exiting.
-	 */
- 	tsk->it_virt_expires = cputime_zero;
- 	tsk->it_prof_expires = cputime_zero;
-	tsk->it_sched_expires = 0;
-
 	if (unlikely(in_atomic()))
 		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
 				current->comm, current->pid,

commit 25f407f0b668f5e4ebd5d13e1fb4306ba6427ead
Author: Roland McGrath <roland@redhat.com>
Date:   Fri Oct 21 15:03:29 2005 -0700

    [PATCH] Call exit_itimers from do_exit, not __exit_signal
    
    When I originally moved exit_itimers into __exit_signal, that was the only
    place where we could reliably know it was the last thread in the group
    dying, without races.  Since then we've gotten the signal_struct.live
    counter, and do_exit can reliably do group-wide cleanup work.
    
    This patch moves the call to do_exit, where it's made without locks.  This
    avoids the deadlock issues that the old __exit_signal code's comment talks
    about, and the one that Oleg found recently with process CPU timers.
    
    [ This replaces e03d13e985d48ac4885382c9e3b1510c78bd047f, which is why
      it was just reverted. ]
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 43077732619b..3b25b182d2be 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -843,6 +843,7 @@ fastcall NORET_TYPE void do_exit(long code)
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
 	if (group_dead) {
  		del_timer_sync(&tsk->signal->real_timer);
+		exit_itimers(tsk->signal);
 		acct_process(code);
 	}
 	exit_mm(tsk);

commit 14bf01bb0599c89fc7f426d20353b76e12555308
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Sat Oct 1 11:04:18 2005 -0700

    Fix inequality comparison against "task->state"
    
    We should always use bitmask ops, rather than depend on some ordering of
    the different states.  With the TASK_NONINTERACTIVE flag, the inequality
    doesn't really work.
    
    Oleg Nesterov argues (likely correctly) that this test is unnecessary in
    the first place.  However, the minimal fix for now is to at least make
    it work in the presense of TASK_NONINTERACTIVE.  Waiting for consensus
    from Roland & co on potential bigger cleanups.
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index ee6d8b8abef5..43077732619b 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -1203,7 +1203,7 @@ static int wait_task_stopped(task_t *p, int delayed_group_leader, int noreap,
 
 		exit_code = p->exit_code;
 		if (unlikely(!exit_code) ||
-		    unlikely(p->state > TASK_STOPPED))
+		    unlikely(p->state & TASK_TRACED))
 			goto bail_ref;
 		return wait_noreap_copyout(p, pid, uid,
 					   why, (exit_code << 8) | 0x7f,

commit 4fb3a53860cee2aaaf81186c451b7da0b95b45c1
Author: Dipankar Sarma <dipankar@in.ibm.com>
Date:   Fri Sep 16 19:28:13 2005 -0700

    [PATCH] files: fix preemption issues
    
    With the new fdtable locking rules, you have to protect fdtable with either
    ->file_lock or rcu_read_lock/unlock().  There are some places where we
    aren't doing either.  This patch fixes those places.
    
    Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6d2089a1bce7..ee6d8b8abef5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -371,6 +371,12 @@ static inline void close_files(struct files_struct * files)
 	struct fdtable *fdt;
 
 	j = 0;
+
+	/*
+	 * It is safe to dereference the fd table without RCU or
+	 * ->file_lock because this is the last reference to the
+	 * files structure.
+	 */
 	fdt = files_fdtable(files);
 	for (;;) {
 		unsigned long set;

commit ab2af1f5005069321c5d130f09cce577b03f43ef
Author: Dipankar Sarma <dipankar@in.ibm.com>
Date:   Fri Sep 9 13:04:13 2005 -0700

    [PATCH] files: files struct with RCU
    
    Patch to eliminate struct files_struct.file_lock spinlock on the reader side
    and use rcu refcounting rcuref_xxx api for the f_count refcounter.  The
    updates to the fdtable are done by allocating a new fdtable structure and
    setting files->fdt to point to the new structure.  The fdtable structure is
    protected by RCU thereby allowing lock-free lookup.  For fd arrays/sets that
    are vmalloced, we use keventd to free them since RCU callbacks can't sleep.  A
    global list of fdtable to be freed is not scalable, so we use a per-cpu list.
    If keventd is already handling the current cpu's work, we use a timer to defer
    queueing of that work.
    
    Since the last publication, this patch has been re-written to avoid using
    explicit memory barriers and use rcu_assign_pointer(), rcu_dereference()
    premitives instead.  This required that the fd information is kept in a
    separate structure (fdtable) and updated atomically.
    
    Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 83beb1e93b18..6d2089a1bce7 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -411,15 +411,16 @@ void fastcall put_files_struct(struct files_struct *files)
 		close_files(files);
 		/*
 		 * Free the fd and fdset arrays if we expanded them.
+		 * If the fdtable was embedded, pass files for freeing
+		 * at the end of the RCU grace period. Otherwise,
+		 * you can free files immediately.
 		 */
 		fdt = files_fdtable(files);
-		if (fdt->fd != &files->fd_array[0])
-			free_fd_array(fdt->fd, fdt->max_fds);
-		if (fdt->max_fdset > __FD_SETSIZE) {
-			free_fdset(fdt->open_fds, fdt->max_fdset);
-			free_fdset(fdt->close_on_exec, fdt->max_fdset);
-		}
-		kmem_cache_free(files_cachep, files);
+		if (fdt == &files->fdtab)
+			fdt->free_files = files;
+		else
+			kmem_cache_free(files_cachep, files);
+		free_fdtable(fdt);
 	}
 }
 

commit badf16621c1f9d1ac753be056fce11b43d6e0be5
Author: Dipankar Sarma <dipankar@in.ibm.com>
Date:   Fri Sep 9 13:04:10 2005 -0700

    [PATCH] files: break up files struct
    
    In order for the RCU to work, the file table array, sets and their sizes must
    be updated atomically.  Instead of ensuring this through too many memory
    barriers, we put the arrays and their sizes in a separate structure.  This
    patch takes the first step of putting the file table elements in a separate
    structure fdtable that is embedded withing files_struct.  It also changes all
    the users to refer to the file table using files_fdtable() macro.  Subsequent
    applciation of RCU becomes easier after this.
    
    Signed-off-by: Dipankar Sarma <dipankar@in.ibm.com>
    Signed-Off-By: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 5b0fb9f09f21..83beb1e93b18 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -368,17 +368,19 @@ EXPORT_SYMBOL(daemonize);
 static inline void close_files(struct files_struct * files)
 {
 	int i, j;
+	struct fdtable *fdt;
 
 	j = 0;
+	fdt = files_fdtable(files);
 	for (;;) {
 		unsigned long set;
 		i = j * __NFDBITS;
-		if (i >= files->max_fdset || i >= files->max_fds)
+		if (i >= fdt->max_fdset || i >= fdt->max_fds)
 			break;
-		set = files->open_fds->fds_bits[j++];
+		set = fdt->open_fds->fds_bits[j++];
 		while (set) {
 			if (set & 1) {
-				struct file * file = xchg(&files->fd[i], NULL);
+				struct file * file = xchg(&fdt->fd[i], NULL);
 				if (file)
 					filp_close(file, files);
 			}
@@ -403,16 +405,19 @@ struct files_struct *get_files_struct(struct task_struct *task)
 
 void fastcall put_files_struct(struct files_struct *files)
 {
+	struct fdtable *fdt;
+
 	if (atomic_dec_and_test(&files->count)) {
 		close_files(files);
 		/*
 		 * Free the fd and fdset arrays if we expanded them.
 		 */
-		if (files->fd != &files->fd_array[0])
-			free_fd_array(files->fd, files->max_fds);
-		if (files->max_fdset > __FD_SETSIZE) {
-			free_fdset(files->open_fds, files->max_fdset);
-			free_fdset(files->close_on_exec, files->max_fdset);
+		fdt = files_fdtable(files);
+		if (fdt->fd != &files->fd_array[0])
+			free_fd_array(fdt->fd, fdt->max_fds);
+		if (fdt->max_fdset > __FD_SETSIZE) {
+			free_fdset(fdt->open_fds, fdt->max_fdset);
+			free_fdset(fdt->close_on_exec, fdt->max_fdset);
 		}
 		kmem_cache_free(files_cachep, files);
 	}

commit c306895167c8384b88bc02945a0d226a04218fa5
Author: Andrew Morton <akpm@osdl.org>
Date:   Thu Aug 4 16:49:32 2005 -0700

    [PATCH] revert "timer exit cleanup"
    
    Revert this June 17 patch: it broke persistence of timers across execve().
    
    Cc: Roland McGrath <roland@redhat.com>
    Cc: george anzinger <george@mvista.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 9d1b10ed0135..5b0fb9f09f21 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -829,8 +829,10 @@ fastcall NORET_TYPE void do_exit(long code)
 	acct_update_integrals(tsk);
 	update_mem_hiwater(tsk);
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
-	if (group_dead)
+	if (group_dead) {
+ 		del_timer_sync(&tsk->signal->real_timer);
 		acct_process(code);
+	}
 	exit_mm(tsk);
 
 	exit_sem(tsk);

commit 22e2c507c301c3dbbcf91b4948b88f78842ee6c9
Author: Jens Axboe <axboe@suse.de>
Date:   Mon Jun 27 10:55:12 2005 +0200

    [PATCH] Update cfq io scheduler to time sliced design
    
    This updates the CFQ io scheduler to the new time sliced design (cfq
    v3).  It provides full process fairness, while giving excellent
    aggregate system throughput even for many competing processes.  It
    supports io priorities, either inherited from the cpu nice value or set
    directly with the ioprio_get/set syscalls.  The latter closely mimic
    set/getpriority.
    
    This import is based on my latest from -mm.
    
    Signed-off-by: Jens Axboe <axboe@suse.de>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 3ebcd60a19c6..9d1b10ed0135 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -784,6 +784,8 @@ fastcall NORET_TYPE void do_exit(long code)
 
 	profile_task_exit(tsk);
 
+	WARN_ON(atomic_read(&tsk->fs_excl));
+
 	if (unlikely(in_interrupt()))
 		panic("Aiee, killing interrupt handler!");
 	if (unlikely(!tsk->pid))

commit 71a2224d7d1cefc23a1ac80bba421cc069cc3257
Author: Christoph Lameter <christoph@lameter.com>
Date:   Thu Jun 23 00:10:05 2005 -0700

    [PATCH] Optimize sys_times for a single thread process
    
    Avoid taking the tasklist_lock in sys_times if the process is single
    threaded.  In a NUMA system taking the tasklist_lock may cause a bouncing
    cacheline if multiple independent processes continually call sys_times to
    measure their performance.
    
    Signed-off-by: Christoph Lameter <christoph@lameter.com>
    Signed-off-by: Shai Fultheim <shai@scalex86.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index c2bdf6fb61a5..3ebcd60a19c6 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -72,6 +72,11 @@ void release_task(struct task_struct * p)
 	BUG_ON(!list_empty(&p->ptrace_list) || !list_empty(&p->ptrace_children));
 	__exit_signal(p);
 	__exit_sighand(p);
+	/*
+	 * Note that the fastpath in sys_times depends on __exit_signal having
+	 * updated the counters before a task is removed from the tasklist of
+	 * the process by __unhash_process.
+	 */
 	__unhash_process(p);
 
 	/*

commit df164db5fd16888ddbe2a63a47b2f6dda9a428b5
Author: Alexander Nyberg <alexn@dsv.su.se>
Date:   Thu Jun 23 00:09:13 2005 -0700

    [PATCH] avoid resursive oopses
    
    Prevent recursive faults in do_exit() by leaving the task alone and wait
    for reboot.  This may allow a more graceful shutdown and possibly save the
    original oops.
    
    Signed-off-by: Alexander Nyberg <alexn@telia.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 2ef2ad540201..c2bdf6fb61a5 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -793,6 +793,17 @@ fastcall NORET_TYPE void do_exit(long code)
 		ptrace_notify((PTRACE_EVENT_EXIT << 8) | SIGTRAP);
 	}
 
+	/*
+	 * We're taking recursive faults here in do_exit. Safest is to just
+	 * leave this task alone and wait for reboot.
+	 */
+	if (unlikely(tsk->flags & PF_EXITING)) {
+		printk(KERN_ALERT
+			"Fixing recursive fault but reboot is needed!\n");
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		schedule();
+	}
+
 	tsk->flags |= PF_EXITING;
 
 	/*

commit caf2857ac6e0ba2651e722f05d5f7d3ec8ef2615
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jun 17 11:36:36 2005 +0200

    [PATCH] timer exit cleanup
    
    Do all timer zapping in exit_itimers.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index edaa50b5bbfa..2ef2ad540201 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -811,10 +811,8 @@ fastcall NORET_TYPE void do_exit(long code)
 	acct_update_integrals(tsk);
 	update_mem_hiwater(tsk);
 	group_dead = atomic_dec_and_test(&tsk->signal->live);
-	if (group_dead) {
- 		del_timer_sync(&tsk->signal->real_timer);
+	if (group_dead)
 		acct_process(code);
-	}
 	exit_mm(tsk);
 
 	exit_sem(tsk);

commit 012914dad25bd5cacf88af4429eecda62a06020d
Author: Russ Anderson <(rja@sgi.com)>
Date:   Sat Apr 23 00:08:00 2005 -0700

    [patch] MCA recovery module undefined symbol fix
    
    The patch "MCA recovery improvements" added do_exit to mca_drv.c.
    That's fine when the mca recovery code is built in the kernel
    (CONFIG_IA64_MCA_RECOVERY=y) but breaks building the mca recovery
    code as a module (CONFIG_IA64_MCA_RECOVERY=m).
    
    Most users are currently building this as a module, as loading
    and unloading the module provides a very convenient way to turn
    on/off error recovery.
    
    This patch exports do_exit, so mca_drv.c can build as a module.
    
    Signed-off-by: Russ Anderson (rja@sgi.com)
    Signed-off-by: Tony Luck <tony.luck@intel.com>

diff --git a/kernel/exit.c b/kernel/exit.c
index 7be283d98983..edaa50b5bbfa 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -846,6 +846,8 @@ fastcall NORET_TYPE void do_exit(long code)
 	for (;;) ;
 }
 
+EXPORT_SYMBOL_GPL(do_exit);
+
 NORET_TYPE void complete_and_exit(struct completion *comp, long code)
 {
 	if (comp)

commit 408b664a7d394a5e4315fbd14aca49b042cb2b08
Author: Adrian Bunk <bunk@stusta.de>
Date:   Sun May 1 08:59:29 2005 -0700

    [PATCH] make lots of things static
    
    Another large rollup of various patches from Adrian which make things static
    where they were needlessly exported.
    
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 419d9d3c4c48..7be283d98983 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -39,6 +39,8 @@ extern struct task_struct *child_reaper;
 
 int getrusage(struct task_struct *, int, struct rusage __user *);
 
+static void exit_mm(struct task_struct * tsk);
+
 static void __unhash_process(struct task_struct *p)
 {
 	nr_threads--;
@@ -474,7 +476,7 @@ EXPORT_SYMBOL_GPL(exit_fs);
  * Turn us into a lazy TLB process if we
  * aren't already..
  */
-void exit_mm(struct task_struct * tsk)
+static void exit_mm(struct task_struct * tsk)
 {
 	struct mm_struct *mm = tsk->mm;
 

commit 4dc3b16ba18c0f967ad100c52fa65b01a4f76ff0
Author: Pavel Pisa <pisa@cmp.felk.cvut.cz>
Date:   Sun May 1 08:59:25 2005 -0700

    [PATCH] DocBook: changes and extensions to the kernel documentation
    
    I have recompiled Linux kernel 2.6.11.5 documentation for me and our
    university students again.  The documentation could be extended for more
    sources which are equipped by structured comments for recent 2.6 kernels.  I
    have tried to proceed with that task.  I have done that more times from 2.6.0
    time and it gets boring to do same changes again and again.  Linux kernel
    compiles after changes for i386 and ARM targets.  I have added references to
    some more files into kernel-api book, I have added some section names as well.
     So please, check that changes do not break something and that categories are
    not too much skewed.
    
    I have changed kernel-doc to accept "fastcall" and "asmlinkage" words reserved
    by kernel convention.  Most of the other changes are modifications in the
    comments to make kernel-doc happy, accept some parameters description and do
    not bail out on errors.  Changed <pid> to @pid in the description, moved some
    #ifdef before comments to correct function to comments bindings, etc.
    
    You can see result of the modified documentation build at
      http://cmp.felk.cvut.cz/~pisa/linux/lkdb-2.6.11.tar.gz
    
    Some more sources are ready to be included into kernel-doc generated
    documentation.  Sources has been added into kernel-api for now.  Some more
    section names added and probably some more chaos introduced as result of quick
    cleanup work.
    
    Signed-off-by: Pavel Pisa <pisa@cmp.felk.cvut.cz>
    Signed-off-by: Martin Waitz <tali@admingilde.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index eb8da36e13df..419d9d3c4c48 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -210,7 +210,7 @@ static inline int has_stopped_jobs(int pgrp)
 }
 
 /**
- * reparent_to_init() - Reparent the calling kernel thread to the init task.
+ * reparent_to_init - Reparent the calling kernel thread to the init task.
  *
  * If a kernel thread is launched as a result of a system call, or if
  * it ever exits, it should generally reparent itself to init so that

commit 7ed20e1ad521b5f5df61bf6559ae60738e393741
Author: Jesper Juhl <juhl-lkml@dif.dk>
Date:   Sun May 1 08:59:14 2005 -0700

    [PATCH] convert that currently tests _NSIG directly to use valid_signal()
    
    Convert most of the current code that uses _NSIG directly to instead use
    valid_signal().  This avoids gcc -W warnings and off-by-one errors.
    
    Signed-off-by: Jesper Juhl <juhl-lkml@dif.dk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 93851bcd9584..eb8da36e13df 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -27,6 +27,7 @@
 #include <linux/mempolicy.h>
 #include <linux/cpuset.h>
 #include <linux/syscalls.h>
+#include <linux/signal.h>
 
 #include <asm/uaccess.h>
 #include <asm/unistd.h>
@@ -277,7 +278,7 @@ void set_special_pids(pid_t session, pid_t pgrp)
  */
 int allow_signal(int sig)
 {
-	if (sig < 1 || sig > _NSIG)
+	if (!valid_signal(sig) || sig < 1)
 		return -EINVAL;
 
 	spin_lock_irq(&current->sighand->siglock);
@@ -298,7 +299,7 @@ EXPORT_SYMBOL(allow_signal);
 
 int disallow_signal(int sig)
 {
-	if (sig < 1 || sig > _NSIG)
+	if (!valid_signal(sig) || sig < 1)
 		return -EINVAL;
 
 	spin_lock_irq(&current->sighand->siglock);

commit c06fec5022ebe014af876da2df4a0eee836e97c8
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Fri Apr 29 09:37:07 2005 -0700

    Remove bogus BUG() in kernel/exit.c
    
    It's old sanity checking that may have been useful for debugging, but
    is just bogus these days.
    
    Noticed by Mattia Belletti.

diff --git a/kernel/exit.c b/kernel/exit.c
index 39d35935b371..93851bcd9584 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -517,8 +517,6 @@ static inline void choose_new_parent(task_t *p, task_t *reaper, task_t *child_re
 	 */
 	BUG_ON(p == reaper || reaper->exit_state >= EXIT_ZOMBIE);
 	p->real_parent = reaper;
-	if (p->parent == p->real_parent)
-		BUG();
 }
 
 static inline void reparent_thread(task_t *p, task_t *father, int traced)

commit 6c46ada700568897165409e618ed584683838b49
Author: Coywolf Qi Hunt <coywolf@lovecn.org>
Date:   Sat Apr 16 15:26:01 2005 -0700

    [PATCH] reparent_to_init cleanup
    
    This patch hides reparent_to_init().  reparent_to_init() should only be
    called by daemonize().
    
    Signed-off-by: Coywolf Qi Hunt <coywolf@lovecn.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/exit.c b/kernel/exit.c
index 6dd4ebe1dd90..39d35935b371 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -220,7 +220,7 @@ static inline int has_stopped_jobs(int pgrp)
  *
  * NOTE that reparent_to_init() gives the caller full capabilities.
  */
-void reparent_to_init(void)
+static inline void reparent_to_init(void)
 {
 	write_lock_irq(&tasklist_lock);
 

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/kernel/exit.c b/kernel/exit.c
new file mode 100644
index 000000000000..6dd4ebe1dd90
--- /dev/null
+++ b/kernel/exit.c
@@ -0,0 +1,1527 @@
+/*
+ *  linux/kernel/exit.c
+ *
+ *  Copyright (C) 1991, 1992  Linus Torvalds
+ */
+
+#include <linux/config.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/smp_lock.h>
+#include <linux/module.h>
+#include <linux/completion.h>
+#include <linux/personality.h>
+#include <linux/tty.h>
+#include <linux/namespace.h>
+#include <linux/key.h>
+#include <linux/security.h>
+#include <linux/cpu.h>
+#include <linux/acct.h>
+#include <linux/file.h>
+#include <linux/binfmts.h>
+#include <linux/ptrace.h>
+#include <linux/profile.h>
+#include <linux/mount.h>
+#include <linux/proc_fs.h>
+#include <linux/mempolicy.h>
+#include <linux/cpuset.h>
+#include <linux/syscalls.h>
+
+#include <asm/uaccess.h>
+#include <asm/unistd.h>
+#include <asm/pgtable.h>
+#include <asm/mmu_context.h>
+
+extern void sem_exit (void);
+extern struct task_struct *child_reaper;
+
+int getrusage(struct task_struct *, int, struct rusage __user *);
+
+static void __unhash_process(struct task_struct *p)
+{
+	nr_threads--;
+	detach_pid(p, PIDTYPE_PID);
+	detach_pid(p, PIDTYPE_TGID);
+	if (thread_group_leader(p)) {
+		detach_pid(p, PIDTYPE_PGID);
+		detach_pid(p, PIDTYPE_SID);
+		if (p->pid)
+			__get_cpu_var(process_counts)--;
+	}
+
+	REMOVE_LINKS(p);
+}
+
+void release_task(struct task_struct * p)
+{
+	int zap_leader;
+	task_t *leader;
+	struct dentry *proc_dentry;
+
+repeat: 
+	atomic_dec(&p->user->processes);
+	spin_lock(&p->proc_lock);
+	proc_dentry = proc_pid_unhash(p);
+	write_lock_irq(&tasklist_lock);
+	if (unlikely(p->ptrace))
+		__ptrace_unlink(p);
+	BUG_ON(!list_empty(&p->ptrace_list) || !list_empty(&p->ptrace_children));
+	__exit_signal(p);
+	__exit_sighand(p);
+	__unhash_process(p);
+
+	/*
+	 * If we are the last non-leader member of the thread
+	 * group, and the leader is zombie, then notify the
+	 * group leader's parent process. (if it wants notification.)
+	 */
+	zap_leader = 0;
+	leader = p->group_leader;
+	if (leader != p && thread_group_empty(leader) && leader->exit_state == EXIT_ZOMBIE) {
+		BUG_ON(leader->exit_signal == -1);
+		do_notify_parent(leader, leader->exit_signal);
+		/*
+		 * If we were the last child thread and the leader has
+		 * exited already, and the leader's parent ignores SIGCHLD,
+		 * then we are the one who should release the leader.
+		 *
+		 * do_notify_parent() will have marked it self-reaping in
+		 * that case.
+		 */
+		zap_leader = (leader->exit_signal == -1);
+	}
+
+	sched_exit(p);
+	write_unlock_irq(&tasklist_lock);
+	spin_unlock(&p->proc_lock);
+	proc_pid_flush(proc_dentry);
+	release_thread(p);
+	put_task_struct(p);
+
+	p = leader;
+	if (unlikely(zap_leader))
+		goto repeat;
+}
+
+/* we are using it only for SMP init */
+
+void unhash_process(struct task_struct *p)
+{
+	struct dentry *proc_dentry;
+
+	spin_lock(&p->proc_lock);
+	proc_dentry = proc_pid_unhash(p);
+	write_lock_irq(&tasklist_lock);
+	__unhash_process(p);
+	write_unlock_irq(&tasklist_lock);
+	spin_unlock(&p->proc_lock);
+	proc_pid_flush(proc_dentry);
+}
+
+/*
+ * This checks not only the pgrp, but falls back on the pid if no
+ * satisfactory pgrp is found. I dunno - gdb doesn't work correctly
+ * without this...
+ */
+int session_of_pgrp(int pgrp)
+{
+	struct task_struct *p;
+	int sid = -1;
+
+	read_lock(&tasklist_lock);
+	do_each_task_pid(pgrp, PIDTYPE_PGID, p) {
+		if (p->signal->session > 0) {
+			sid = p->signal->session;
+			goto out;
+		}
+	} while_each_task_pid(pgrp, PIDTYPE_PGID, p);
+	p = find_task_by_pid(pgrp);
+	if (p)
+		sid = p->signal->session;
+out:
+	read_unlock(&tasklist_lock);
+	
+	return sid;
+}
+
+/*
+ * Determine if a process group is "orphaned", according to the POSIX
+ * definition in 2.2.2.52.  Orphaned process groups are not to be affected
+ * by terminal-generated stop signals.  Newly orphaned process groups are
+ * to receive a SIGHUP and a SIGCONT.
+ *
+ * "I ask you, have you ever known what it is to be an orphan?"
+ */
+static int will_become_orphaned_pgrp(int pgrp, task_t *ignored_task)
+{
+	struct task_struct *p;
+	int ret = 1;
+
+	do_each_task_pid(pgrp, PIDTYPE_PGID, p) {
+		if (p == ignored_task
+				|| p->exit_state
+				|| p->real_parent->pid == 1)
+			continue;
+		if (process_group(p->real_parent) != pgrp
+			    && p->real_parent->signal->session == p->signal->session) {
+			ret = 0;
+			break;
+		}
+	} while_each_task_pid(pgrp, PIDTYPE_PGID, p);
+	return ret;	/* (sighing) "Often!" */
+}
+
+int is_orphaned_pgrp(int pgrp)
+{
+	int retval;
+
+	read_lock(&tasklist_lock);
+	retval = will_become_orphaned_pgrp(pgrp, NULL);
+	read_unlock(&tasklist_lock);
+
+	return retval;
+}
+
+static inline int has_stopped_jobs(int pgrp)
+{
+	int retval = 0;
+	struct task_struct *p;
+
+	do_each_task_pid(pgrp, PIDTYPE_PGID, p) {
+		if (p->state != TASK_STOPPED)
+			continue;
+
+		/* If p is stopped by a debugger on a signal that won't
+		   stop it, then don't count p as stopped.  This isn't
+		   perfect but it's a good approximation.  */
+		if (unlikely (p->ptrace)
+		    && p->exit_code != SIGSTOP
+		    && p->exit_code != SIGTSTP
+		    && p->exit_code != SIGTTOU
+		    && p->exit_code != SIGTTIN)
+			continue;
+
+		retval = 1;
+		break;
+	} while_each_task_pid(pgrp, PIDTYPE_PGID, p);
+	return retval;
+}
+
+/**
+ * reparent_to_init() - Reparent the calling kernel thread to the init task.
+ *
+ * If a kernel thread is launched as a result of a system call, or if
+ * it ever exits, it should generally reparent itself to init so that
+ * it is correctly cleaned up on exit.
+ *
+ * The various task state such as scheduling policy and priority may have
+ * been inherited from a user process, so we reset them to sane values here.
+ *
+ * NOTE that reparent_to_init() gives the caller full capabilities.
+ */
+void reparent_to_init(void)
+{
+	write_lock_irq(&tasklist_lock);
+
+	ptrace_unlink(current);
+	/* Reparent to init */
+	REMOVE_LINKS(current);
+	current->parent = child_reaper;
+	current->real_parent = child_reaper;
+	SET_LINKS(current);
+
+	/* Set the exit signal to SIGCHLD so we signal init on exit */
+	current->exit_signal = SIGCHLD;
+
+	if ((current->policy == SCHED_NORMAL) && (task_nice(current) < 0))
+		set_user_nice(current, 0);
+	/* cpus_allowed? */
+	/* rt_priority? */
+	/* signals? */
+	security_task_reparent_to_init(current);
+	memcpy(current->signal->rlim, init_task.signal->rlim,
+	       sizeof(current->signal->rlim));
+	atomic_inc(&(INIT_USER->__count));
+	write_unlock_irq(&tasklist_lock);
+	switch_uid(INIT_USER);
+}
+
+void __set_special_pids(pid_t session, pid_t pgrp)
+{
+	struct task_struct *curr = current;
+
+	if (curr->signal->session != session) {
+		detach_pid(curr, PIDTYPE_SID);
+		curr->signal->session = session;
+		attach_pid(curr, PIDTYPE_SID, session);
+	}
+	if (process_group(curr) != pgrp) {
+		detach_pid(curr, PIDTYPE_PGID);
+		curr->signal->pgrp = pgrp;
+		attach_pid(curr, PIDTYPE_PGID, pgrp);
+	}
+}
+
+void set_special_pids(pid_t session, pid_t pgrp)
+{
+	write_lock_irq(&tasklist_lock);
+	__set_special_pids(session, pgrp);
+	write_unlock_irq(&tasklist_lock);
+}
+
+/*
+ * Let kernel threads use this to say that they
+ * allow a certain signal (since daemonize() will
+ * have disabled all of them by default).
+ */
+int allow_signal(int sig)
+{
+	if (sig < 1 || sig > _NSIG)
+		return -EINVAL;
+
+	spin_lock_irq(&current->sighand->siglock);
+	sigdelset(&current->blocked, sig);
+	if (!current->mm) {
+		/* Kernel threads handle their own signals.
+		   Let the signal code know it'll be handled, so
+		   that they don't get converted to SIGKILL or
+		   just silently dropped */
+		current->sighand->action[(sig)-1].sa.sa_handler = (void __user *)2;
+	}
+	recalc_sigpending();
+	spin_unlock_irq(&current->sighand->siglock);
+	return 0;
+}
+
+EXPORT_SYMBOL(allow_signal);
+
+int disallow_signal(int sig)
+{
+	if (sig < 1 || sig > _NSIG)
+		return -EINVAL;
+
+	spin_lock_irq(&current->sighand->siglock);
+	sigaddset(&current->blocked, sig);
+	recalc_sigpending();
+	spin_unlock_irq(&current->sighand->siglock);
+	return 0;
+}
+
+EXPORT_SYMBOL(disallow_signal);
+
+/*
+ *	Put all the gunge required to become a kernel thread without
+ *	attached user resources in one place where it belongs.
+ */
+
+void daemonize(const char *name, ...)
+{
+	va_list args;
+	struct fs_struct *fs;
+	sigset_t blocked;
+
+	va_start(args, name);
+	vsnprintf(current->comm, sizeof(current->comm), name, args);
+	va_end(args);
+
+	/*
+	 * If we were started as result of loading a module, close all of the
+	 * user space pages.  We don't need them, and if we didn't close them
+	 * they would be locked into memory.
+	 */
+	exit_mm(current);
+
+	set_special_pids(1, 1);
+	down(&tty_sem);
+	current->signal->tty = NULL;
+	up(&tty_sem);
+
+	/* Block and flush all signals */
+	sigfillset(&blocked);
+	sigprocmask(SIG_BLOCK, &blocked, NULL);
+	flush_signals(current);
+
+	/* Become as one with the init task */
+
+	exit_fs(current);	/* current->fs->count--; */
+	fs = init_task.fs;
+	current->fs = fs;
+	atomic_inc(&fs->count);
+ 	exit_files(current);
+	current->files = init_task.files;
+	atomic_inc(&current->files->count);
+
+	reparent_to_init();
+}
+
+EXPORT_SYMBOL(daemonize);
+
+static inline void close_files(struct files_struct * files)
+{
+	int i, j;
+
+	j = 0;
+	for (;;) {
+		unsigned long set;
+		i = j * __NFDBITS;
+		if (i >= files->max_fdset || i >= files->max_fds)
+			break;
+		set = files->open_fds->fds_bits[j++];
+		while (set) {
+			if (set & 1) {
+				struct file * file = xchg(&files->fd[i], NULL);
+				if (file)
+					filp_close(file, files);
+			}
+			i++;
+			set >>= 1;
+		}
+	}
+}
+
+struct files_struct *get_files_struct(struct task_struct *task)
+{
+	struct files_struct *files;
+
+	task_lock(task);
+	files = task->files;
+	if (files)
+		atomic_inc(&files->count);
+	task_unlock(task);
+
+	return files;
+}
+
+void fastcall put_files_struct(struct files_struct *files)
+{
+	if (atomic_dec_and_test(&files->count)) {
+		close_files(files);
+		/*
+		 * Free the fd and fdset arrays if we expanded them.
+		 */
+		if (files->fd != &files->fd_array[0])
+			free_fd_array(files->fd, files->max_fds);
+		if (files->max_fdset > __FD_SETSIZE) {
+			free_fdset(files->open_fds, files->max_fdset);
+			free_fdset(files->close_on_exec, files->max_fdset);
+		}
+		kmem_cache_free(files_cachep, files);
+	}
+}
+
+EXPORT_SYMBOL(put_files_struct);
+
+static inline void __exit_files(struct task_struct *tsk)
+{
+	struct files_struct * files = tsk->files;
+
+	if (files) {
+		task_lock(tsk);
+		tsk->files = NULL;
+		task_unlock(tsk);
+		put_files_struct(files);
+	}
+}
+
+void exit_files(struct task_struct *tsk)
+{
+	__exit_files(tsk);
+}
+
+static inline void __put_fs_struct(struct fs_struct *fs)
+{
+	/* No need to hold fs->lock if we are killing it */
+	if (atomic_dec_and_test(&fs->count)) {
+		dput(fs->root);
+		mntput(fs->rootmnt);
+		dput(fs->pwd);
+		mntput(fs->pwdmnt);
+		if (fs->altroot) {
+			dput(fs->altroot);
+			mntput(fs->altrootmnt);
+		}
+		kmem_cache_free(fs_cachep, fs);
+	}
+}
+
+void put_fs_struct(struct fs_struct *fs)
+{
+	__put_fs_struct(fs);
+}
+
+static inline void __exit_fs(struct task_struct *tsk)
+{
+	struct fs_struct * fs = tsk->fs;
+
+	if (fs) {
+		task_lock(tsk);
+		tsk->fs = NULL;
+		task_unlock(tsk);
+		__put_fs_struct(fs);
+	}
+}
+
+void exit_fs(struct task_struct *tsk)
+{
+	__exit_fs(tsk);
+}
+
+EXPORT_SYMBOL_GPL(exit_fs);
+
+/*
+ * Turn us into a lazy TLB process if we
+ * aren't already..
+ */
+void exit_mm(struct task_struct * tsk)
+{
+	struct mm_struct *mm = tsk->mm;
+
+	mm_release(tsk, mm);
+	if (!mm)
+		return;
+	/*
+	 * Serialize with any possible pending coredump.
+	 * We must hold mmap_sem around checking core_waiters
+	 * and clearing tsk->mm.  The core-inducing thread
+	 * will increment core_waiters for each thread in the
+	 * group with ->mm != NULL.
+	 */
+	down_read(&mm->mmap_sem);
+	if (mm->core_waiters) {
+		up_read(&mm->mmap_sem);
+		down_write(&mm->mmap_sem);
+		if (!--mm->core_waiters)
+			complete(mm->core_startup_done);
+		up_write(&mm->mmap_sem);
+
+		wait_for_completion(&mm->core_done);
+		down_read(&mm->mmap_sem);
+	}
+	atomic_inc(&mm->mm_count);
+	if (mm != tsk->active_mm) BUG();
+	/* more a memory barrier than a real lock */
+	task_lock(tsk);
+	tsk->mm = NULL;
+	up_read(&mm->mmap_sem);
+	enter_lazy_tlb(mm, current);
+	task_unlock(tsk);
+	mmput(mm);
+}
+
+static inline void choose_new_parent(task_t *p, task_t *reaper, task_t *child_reaper)
+{
+	/*
+	 * Make sure we're not reparenting to ourselves and that
+	 * the parent is not a zombie.
+	 */
+	BUG_ON(p == reaper || reaper->exit_state >= EXIT_ZOMBIE);
+	p->real_parent = reaper;
+	if (p->parent == p->real_parent)
+		BUG();
+}
+
+static inline void reparent_thread(task_t *p, task_t *father, int traced)
+{
+	/* We don't want people slaying init.  */
+	if (p->exit_signal != -1)
+		p->exit_signal = SIGCHLD;
+
+	if (p->pdeath_signal)
+		/* We already hold the tasklist_lock here.  */
+		group_send_sig_info(p->pdeath_signal, (void *) 0, p);
+
+	/* Move the child from its dying parent to the new one.  */
+	if (unlikely(traced)) {
+		/* Preserve ptrace links if someone else is tracing this child.  */
+		list_del_init(&p->ptrace_list);
+		if (p->parent != p->real_parent)
+			list_add(&p->ptrace_list, &p->real_parent->ptrace_children);
+	} else {
+		/* If this child is being traced, then we're the one tracing it
+		 * anyway, so let go of it.
+		 */
+		p->ptrace = 0;
+		list_del_init(&p->sibling);
+		p->parent = p->real_parent;
+		list_add_tail(&p->sibling, &p->parent->children);
+
+		/* If we'd notified the old parent about this child's death,
+		 * also notify the new parent.
+		 */
+		if (p->exit_state == EXIT_ZOMBIE && p->exit_signal != -1 &&
+		    thread_group_empty(p))
+			do_notify_parent(p, p->exit_signal);
+		else if (p->state == TASK_TRACED) {
+			/*
+			 * If it was at a trace stop, turn it into
+			 * a normal stop since it's no longer being
+			 * traced.
+			 */
+			ptrace_untrace(p);
+		}
+	}
+
+	/*
+	 * process group orphan check
+	 * Case ii: Our child is in a different pgrp
+	 * than we are, and it was the only connection
+	 * outside, so the child pgrp is now orphaned.
+	 */
+	if ((process_group(p) != process_group(father)) &&
+	    (p->signal->session == father->signal->session)) {
+		int pgrp = process_group(p);
+
+		if (will_become_orphaned_pgrp(pgrp, NULL) && has_stopped_jobs(pgrp)) {
+			__kill_pg_info(SIGHUP, (void *)1, pgrp);
+			__kill_pg_info(SIGCONT, (void *)1, pgrp);
+		}
+	}
+}
+
+/*
+ * When we die, we re-parent all our children.
+ * Try to give them to another thread in our thread
+ * group, and if no such member exists, give it to
+ * the global child reaper process (ie "init")
+ */
+static inline void forget_original_parent(struct task_struct * father,
+					  struct list_head *to_release)
+{
+	struct task_struct *p, *reaper = father;
+	struct list_head *_p, *_n;
+
+	do {
+		reaper = next_thread(reaper);
+		if (reaper == father) {
+			reaper = child_reaper;
+			break;
+		}
+	} while (reaper->exit_state);
+
+	/*
+	 * There are only two places where our children can be:
+	 *
+	 * - in our child list
+	 * - in our ptraced child list
+	 *
+	 * Search them and reparent children.
+	 */
+	list_for_each_safe(_p, _n, &father->children) {
+		int ptrace;
+		p = list_entry(_p,struct task_struct,sibling);
+
+		ptrace = p->ptrace;
+
+		/* if father isn't the real parent, then ptrace must be enabled */
+		BUG_ON(father != p->real_parent && !ptrace);
+
+		if (father == p->real_parent) {
+			/* reparent with a reaper, real father it's us */
+			choose_new_parent(p, reaper, child_reaper);
+			reparent_thread(p, father, 0);
+		} else {
+			/* reparent ptraced task to its real parent */
+			__ptrace_unlink (p);
+			if (p->exit_state == EXIT_ZOMBIE && p->exit_signal != -1 &&
+			    thread_group_empty(p))
+				do_notify_parent(p, p->exit_signal);
+		}
+
+		/*
+		 * if the ptraced child is a zombie with exit_signal == -1
+		 * we must collect it before we exit, or it will remain
+		 * zombie forever since we prevented it from self-reap itself
+		 * while it was being traced by us, to be able to see it in wait4.
+		 */
+		if (unlikely(ptrace && p->exit_state == EXIT_ZOMBIE && p->exit_signal == -1))
+			list_add(&p->ptrace_list, to_release);
+	}
+	list_for_each_safe(_p, _n, &father->ptrace_children) {
+		p = list_entry(_p,struct task_struct,ptrace_list);
+		choose_new_parent(p, reaper, child_reaper);
+		reparent_thread(p, father, 1);
+	}
+}
+
+/*
+ * Send signals to all our closest relatives so that they know
+ * to properly mourn us..
+ */
+static void exit_notify(struct task_struct *tsk)
+{
+	int state;
+	struct task_struct *t;
+	struct list_head ptrace_dead, *_p, *_n;
+
+	if (signal_pending(tsk) && !(tsk->signal->flags & SIGNAL_GROUP_EXIT)
+	    && !thread_group_empty(tsk)) {
+		/*
+		 * This occurs when there was a race between our exit
+		 * syscall and a group signal choosing us as the one to
+		 * wake up.  It could be that we are the only thread
+		 * alerted to check for pending signals, but another thread
+		 * should be woken now to take the signal since we will not.
+		 * Now we'll wake all the threads in the group just to make
+		 * sure someone gets all the pending signals.
+		 */
+		read_lock(&tasklist_lock);
+		spin_lock_irq(&tsk->sighand->siglock);
+		for (t = next_thread(tsk); t != tsk; t = next_thread(t))
+			if (!signal_pending(t) && !(t->flags & PF_EXITING)) {
+				recalc_sigpending_tsk(t);
+				if (signal_pending(t))
+					signal_wake_up(t, 0);
+			}
+		spin_unlock_irq(&tsk->sighand->siglock);
+		read_unlock(&tasklist_lock);
+	}
+
+	write_lock_irq(&tasklist_lock);
+
+	/*
+	 * This does two things:
+	 *
+  	 * A.  Make init inherit all the child processes
+	 * B.  Check to see if any process groups have become orphaned
+	 *	as a result of our exiting, and if they have any stopped
+	 *	jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
+	 */
+
+	INIT_LIST_HEAD(&ptrace_dead);
+	forget_original_parent(tsk, &ptrace_dead);
+	BUG_ON(!list_empty(&tsk->children));
+	BUG_ON(!list_empty(&tsk->ptrace_children));
+
+	/*
+	 * Check to see if any process groups have become orphaned
+	 * as a result of our exiting, and if they have any stopped
+	 * jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
+	 *
+	 * Case i: Our father is in a different pgrp than we are
+	 * and we were the only connection outside, so our pgrp
+	 * is about to become orphaned.
+	 */
+	 
+	t = tsk->real_parent;
+	
+	if ((process_group(t) != process_group(tsk)) &&
+	    (t->signal->session == tsk->signal->session) &&
+	    will_become_orphaned_pgrp(process_group(tsk), tsk) &&
+	    has_stopped_jobs(process_group(tsk))) {
+		__kill_pg_info(SIGHUP, (void *)1, process_group(tsk));
+		__kill_pg_info(SIGCONT, (void *)1, process_group(tsk));
+	}
+
+	/* Let father know we died 
+	 *
+	 * Thread signals are configurable, but you aren't going to use
+	 * that to send signals to arbitary processes. 
+	 * That stops right now.
+	 *
+	 * If the parent exec id doesn't match the exec id we saved
+	 * when we started then we know the parent has changed security
+	 * domain.
+	 *
+	 * If our self_exec id doesn't match our parent_exec_id then
+	 * we have changed execution domain as these two values started
+	 * the same after a fork.
+	 *	
+	 */
+	
+	if (tsk->exit_signal != SIGCHLD && tsk->exit_signal != -1 &&
+	    ( tsk->parent_exec_id != t->self_exec_id  ||
+	      tsk->self_exec_id != tsk->parent_exec_id)
+	    && !capable(CAP_KILL))
+		tsk->exit_signal = SIGCHLD;
+
+
+	/* If something other than our normal parent is ptracing us, then
+	 * send it a SIGCHLD instead of honoring exit_signal.  exit_signal
+	 * only has special meaning to our real parent.
+	 */
+	if (tsk->exit_signal != -1 && thread_group_empty(tsk)) {
+		int signal = tsk->parent == tsk->real_parent ? tsk->exit_signal : SIGCHLD;
+		do_notify_parent(tsk, signal);
+	} else if (tsk->ptrace) {
+		do_notify_parent(tsk, SIGCHLD);
+	}
+
+	state = EXIT_ZOMBIE;
+	if (tsk->exit_signal == -1 &&
+	    (likely(tsk->ptrace == 0) ||
+	     unlikely(tsk->parent->signal->flags & SIGNAL_GROUP_EXIT)))
+		state = EXIT_DEAD;
+	tsk->exit_state = state;
+
+	write_unlock_irq(&tasklist_lock);
+
+	list_for_each_safe(_p, _n, &ptrace_dead) {
+		list_del_init(_p);
+		t = list_entry(_p,struct task_struct,ptrace_list);
+		release_task(t);
+	}
+
+	/* If the process is dead, release it - nobody will wait for it */
+	if (state == EXIT_DEAD)
+		release_task(tsk);
+
+	/* PF_DEAD causes final put_task_struct after we schedule. */
+	preempt_disable();
+	tsk->flags |= PF_DEAD;
+}
+
+fastcall NORET_TYPE void do_exit(long code)
+{
+	struct task_struct *tsk = current;
+	int group_dead;
+
+	profile_task_exit(tsk);
+
+	if (unlikely(in_interrupt()))
+		panic("Aiee, killing interrupt handler!");
+	if (unlikely(!tsk->pid))
+		panic("Attempted to kill the idle task!");
+	if (unlikely(tsk->pid == 1))
+		panic("Attempted to kill init!");
+	if (tsk->io_context)
+		exit_io_context();
+
+	if (unlikely(current->ptrace & PT_TRACE_EXIT)) {
+		current->ptrace_message = code;
+		ptrace_notify((PTRACE_EVENT_EXIT << 8) | SIGTRAP);
+	}
+
+	tsk->flags |= PF_EXITING;
+
+	/*
+	 * Make sure we don't try to process any timer firings
+	 * while we are already exiting.
+	 */
+ 	tsk->it_virt_expires = cputime_zero;
+ 	tsk->it_prof_expires = cputime_zero;
+	tsk->it_sched_expires = 0;
+
+	if (unlikely(in_atomic()))
+		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
+				current->comm, current->pid,
+				preempt_count());
+
+	acct_update_integrals(tsk);
+	update_mem_hiwater(tsk);
+	group_dead = atomic_dec_and_test(&tsk->signal->live);
+	if (group_dead) {
+ 		del_timer_sync(&tsk->signal->real_timer);
+		acct_process(code);
+	}
+	exit_mm(tsk);
+
+	exit_sem(tsk);
+	__exit_files(tsk);
+	__exit_fs(tsk);
+	exit_namespace(tsk);
+	exit_thread();
+	cpuset_exit(tsk);
+	exit_keys(tsk);
+
+	if (group_dead && tsk->signal->leader)
+		disassociate_ctty(1);
+
+	module_put(tsk->thread_info->exec_domain->module);
+	if (tsk->binfmt)
+		module_put(tsk->binfmt->module);
+
+	tsk->exit_code = code;
+	exit_notify(tsk);
+#ifdef CONFIG_NUMA
+	mpol_free(tsk->mempolicy);
+	tsk->mempolicy = NULL;
+#endif
+
+	BUG_ON(!(current->flags & PF_DEAD));
+	schedule();
+	BUG();
+	/* Avoid "noreturn function does return".  */
+	for (;;) ;
+}
+
+NORET_TYPE void complete_and_exit(struct completion *comp, long code)
+{
+	if (comp)
+		complete(comp);
+	
+	do_exit(code);
+}
+
+EXPORT_SYMBOL(complete_and_exit);
+
+asmlinkage long sys_exit(int error_code)
+{
+	do_exit((error_code&0xff)<<8);
+}
+
+task_t fastcall *next_thread(const task_t *p)
+{
+	return pid_task(p->pids[PIDTYPE_TGID].pid_list.next, PIDTYPE_TGID);
+}
+
+EXPORT_SYMBOL(next_thread);
+
+/*
+ * Take down every thread in the group.  This is called by fatal signals
+ * as well as by sys_exit_group (below).
+ */
+NORET_TYPE void
+do_group_exit(int exit_code)
+{
+	BUG_ON(exit_code & 0x80); /* core dumps don't get here */
+
+	if (current->signal->flags & SIGNAL_GROUP_EXIT)
+		exit_code = current->signal->group_exit_code;
+	else if (!thread_group_empty(current)) {
+		struct signal_struct *const sig = current->signal;
+		struct sighand_struct *const sighand = current->sighand;
+		read_lock(&tasklist_lock);
+		spin_lock_irq(&sighand->siglock);
+		if (sig->flags & SIGNAL_GROUP_EXIT)
+			/* Another thread got here before we took the lock.  */
+			exit_code = sig->group_exit_code;
+		else {
+			sig->flags = SIGNAL_GROUP_EXIT;
+			sig->group_exit_code = exit_code;
+			zap_other_threads(current);
+		}
+		spin_unlock_irq(&sighand->siglock);
+		read_unlock(&tasklist_lock);
+	}
+
+	do_exit(exit_code);
+	/* NOTREACHED */
+}
+
+/*
+ * this kills every thread in the thread group. Note that any externally
+ * wait4()-ing process will get the correct exit code - even if this
+ * thread is not the thread group leader.
+ */
+asmlinkage void sys_exit_group(int error_code)
+{
+	do_group_exit((error_code & 0xff) << 8);
+}
+
+static int eligible_child(pid_t pid, int options, task_t *p)
+{
+	if (pid > 0) {
+		if (p->pid != pid)
+			return 0;
+	} else if (!pid) {
+		if (process_group(p) != process_group(current))
+			return 0;
+	} else if (pid != -1) {
+		if (process_group(p) != -pid)
+			return 0;
+	}
+
+	/*
+	 * Do not consider detached threads that are
+	 * not ptraced:
+	 */
+	if (p->exit_signal == -1 && !p->ptrace)
+		return 0;
+
+	/* Wait for all children (clone and not) if __WALL is set;
+	 * otherwise, wait for clone children *only* if __WCLONE is
+	 * set; otherwise, wait for non-clone children *only*.  (Note:
+	 * A "clone" child here is one that reports to its parent
+	 * using a signal other than SIGCHLD.) */
+	if (((p->exit_signal != SIGCHLD) ^ ((options & __WCLONE) != 0))
+	    && !(options & __WALL))
+		return 0;
+	/*
+	 * Do not consider thread group leaders that are
+	 * in a non-empty thread group:
+	 */
+	if (current->tgid != p->tgid && delay_group_leader(p))
+		return 2;
+
+	if (security_task_wait(p))
+		return 0;
+
+	return 1;
+}
+
+static int wait_noreap_copyout(task_t *p, pid_t pid, uid_t uid,
+			       int why, int status,
+			       struct siginfo __user *infop,
+			       struct rusage __user *rusagep)
+{
+	int retval = rusagep ? getrusage(p, RUSAGE_BOTH, rusagep) : 0;
+	put_task_struct(p);
+	if (!retval)
+		retval = put_user(SIGCHLD, &infop->si_signo);
+	if (!retval)
+		retval = put_user(0, &infop->si_errno);
+	if (!retval)
+		retval = put_user((short)why, &infop->si_code);
+	if (!retval)
+		retval = put_user(pid, &infop->si_pid);
+	if (!retval)
+		retval = put_user(uid, &infop->si_uid);
+	if (!retval)
+		retval = put_user(status, &infop->si_status);
+	if (!retval)
+		retval = pid;
+	return retval;
+}
+
+/*
+ * Handle sys_wait4 work for one task in state EXIT_ZOMBIE.  We hold
+ * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold
+ * the lock and this task is uninteresting.  If we return nonzero, we have
+ * released the lock and the system call should return.
+ */
+static int wait_task_zombie(task_t *p, int noreap,
+			    struct siginfo __user *infop,
+			    int __user *stat_addr, struct rusage __user *ru)
+{
+	unsigned long state;
+	int retval;
+	int status;
+
+	if (unlikely(noreap)) {
+		pid_t pid = p->pid;
+		uid_t uid = p->uid;
+		int exit_code = p->exit_code;
+		int why, status;
+
+		if (unlikely(p->exit_state != EXIT_ZOMBIE))
+			return 0;
+		if (unlikely(p->exit_signal == -1 && p->ptrace == 0))
+			return 0;
+		get_task_struct(p);
+		read_unlock(&tasklist_lock);
+		if ((exit_code & 0x7f) == 0) {
+			why = CLD_EXITED;
+			status = exit_code >> 8;
+		} else {
+			why = (exit_code & 0x80) ? CLD_DUMPED : CLD_KILLED;
+			status = exit_code & 0x7f;
+		}
+		return wait_noreap_copyout(p, pid, uid, why,
+					   status, infop, ru);
+	}
+
+	/*
+	 * Try to move the task's state to DEAD
+	 * only one thread is allowed to do this:
+	 */
+	state = xchg(&p->exit_state, EXIT_DEAD);
+	if (state != EXIT_ZOMBIE) {
+		BUG_ON(state != EXIT_DEAD);
+		return 0;
+	}
+	if (unlikely(p->exit_signal == -1 && p->ptrace == 0)) {
+		/*
+		 * This can only happen in a race with a ptraced thread
+		 * dying on another processor.
+		 */
+		return 0;
+	}
+
+	if (likely(p->real_parent == p->parent) && likely(p->signal)) {
+		/*
+		 * The resource counters for the group leader are in its
+		 * own task_struct.  Those for dead threads in the group
+		 * are in its signal_struct, as are those for the child
+		 * processes it has previously reaped.  All these
+		 * accumulate in the parent's signal_struct c* fields.
+		 *
+		 * We don't bother to take a lock here to protect these
+		 * p->signal fields, because they are only touched by
+		 * __exit_signal, which runs with tasklist_lock
+		 * write-locked anyway, and so is excluded here.  We do
+		 * need to protect the access to p->parent->signal fields,
+		 * as other threads in the parent group can be right
+		 * here reaping other children at the same time.
+		 */
+		spin_lock_irq(&p->parent->sighand->siglock);
+		p->parent->signal->cutime =
+			cputime_add(p->parent->signal->cutime,
+			cputime_add(p->utime,
+			cputime_add(p->signal->utime,
+				    p->signal->cutime)));
+		p->parent->signal->cstime =
+			cputime_add(p->parent->signal->cstime,
+			cputime_add(p->stime,
+			cputime_add(p->signal->stime,
+				    p->signal->cstime)));
+		p->parent->signal->cmin_flt +=
+			p->min_flt + p->signal->min_flt + p->signal->cmin_flt;
+		p->parent->signal->cmaj_flt +=
+			p->maj_flt + p->signal->maj_flt + p->signal->cmaj_flt;
+		p->parent->signal->cnvcsw +=
+			p->nvcsw + p->signal->nvcsw + p->signal->cnvcsw;
+		p->parent->signal->cnivcsw +=
+			p->nivcsw + p->signal->nivcsw + p->signal->cnivcsw;
+		spin_unlock_irq(&p->parent->sighand->siglock);
+	}
+
+	/*
+	 * Now we are sure this task is interesting, and no other
+	 * thread can reap it because we set its state to EXIT_DEAD.
+	 */
+	read_unlock(&tasklist_lock);
+
+	retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
+	status = (p->signal->flags & SIGNAL_GROUP_EXIT)
+		? p->signal->group_exit_code : p->exit_code;
+	if (!retval && stat_addr)
+		retval = put_user(status, stat_addr);
+	if (!retval && infop)
+		retval = put_user(SIGCHLD, &infop->si_signo);
+	if (!retval && infop)
+		retval = put_user(0, &infop->si_errno);
+	if (!retval && infop) {
+		int why;
+
+		if ((status & 0x7f) == 0) {
+			why = CLD_EXITED;
+			status >>= 8;
+		} else {
+			why = (status & 0x80) ? CLD_DUMPED : CLD_KILLED;
+			status &= 0x7f;
+		}
+		retval = put_user((short)why, &infop->si_code);
+		if (!retval)
+			retval = put_user(status, &infop->si_status);
+	}
+	if (!retval && infop)
+		retval = put_user(p->pid, &infop->si_pid);
+	if (!retval && infop)
+		retval = put_user(p->uid, &infop->si_uid);
+	if (retval) {
+		// TODO: is this safe?
+		p->exit_state = EXIT_ZOMBIE;
+		return retval;
+	}
+	retval = p->pid;
+	if (p->real_parent != p->parent) {
+		write_lock_irq(&tasklist_lock);
+		/* Double-check with lock held.  */
+		if (p->real_parent != p->parent) {
+			__ptrace_unlink(p);
+			// TODO: is this safe?
+			p->exit_state = EXIT_ZOMBIE;
+			/*
+			 * If this is not a detached task, notify the parent.
+			 * If it's still not detached after that, don't release
+			 * it now.
+			 */
+			if (p->exit_signal != -1) {
+				do_notify_parent(p, p->exit_signal);
+				if (p->exit_signal != -1)
+					p = NULL;
+			}
+		}
+		write_unlock_irq(&tasklist_lock);
+	}
+	if (p != NULL)
+		release_task(p);
+	BUG_ON(!retval);
+	return retval;
+}
+
+/*
+ * Handle sys_wait4 work for one task in state TASK_STOPPED.  We hold
+ * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold
+ * the lock and this task is uninteresting.  If we return nonzero, we have
+ * released the lock and the system call should return.
+ */
+static int wait_task_stopped(task_t *p, int delayed_group_leader, int noreap,
+			     struct siginfo __user *infop,
+			     int __user *stat_addr, struct rusage __user *ru)
+{
+	int retval, exit_code;
+
+	if (!p->exit_code)
+		return 0;
+	if (delayed_group_leader && !(p->ptrace & PT_PTRACED) &&
+	    p->signal && p->signal->group_stop_count > 0)
+		/*
+		 * A group stop is in progress and this is the group leader.
+		 * We won't report until all threads have stopped.
+		 */
+		return 0;
+
+	/*
+	 * Now we are pretty sure this task is interesting.
+	 * Make sure it doesn't get reaped out from under us while we
+	 * give up the lock and then examine it below.  We don't want to
+	 * keep holding onto the tasklist_lock while we call getrusage and
+	 * possibly take page faults for user memory.
+	 */
+	get_task_struct(p);
+	read_unlock(&tasklist_lock);
+
+	if (unlikely(noreap)) {
+		pid_t pid = p->pid;
+		uid_t uid = p->uid;
+		int why = (p->ptrace & PT_PTRACED) ? CLD_TRAPPED : CLD_STOPPED;
+
+		exit_code = p->exit_code;
+		if (unlikely(!exit_code) ||
+		    unlikely(p->state > TASK_STOPPED))
+			goto bail_ref;
+		return wait_noreap_copyout(p, pid, uid,
+					   why, (exit_code << 8) | 0x7f,
+					   infop, ru);
+	}
+
+	write_lock_irq(&tasklist_lock);
+
+	/*
+	 * This uses xchg to be atomic with the thread resuming and setting
+	 * it.  It must also be done with the write lock held to prevent a
+	 * race with the EXIT_ZOMBIE case.
+	 */
+	exit_code = xchg(&p->exit_code, 0);
+	if (unlikely(p->exit_state)) {
+		/*
+		 * The task resumed and then died.  Let the next iteration
+		 * catch it in EXIT_ZOMBIE.  Note that exit_code might
+		 * already be zero here if it resumed and did _exit(0).
+		 * The task itself is dead and won't touch exit_code again;
+		 * other processors in this function are locked out.
+		 */
+		p->exit_code = exit_code;
+		exit_code = 0;
+	}
+	if (unlikely(exit_code == 0)) {
+		/*
+		 * Another thread in this function got to it first, or it
+		 * resumed, or it resumed and then died.
+		 */
+		write_unlock_irq(&tasklist_lock);
+bail_ref:
+		put_task_struct(p);
+		/*
+		 * We are returning to the wait loop without having successfully
+		 * removed the process and having released the lock. We cannot
+		 * continue, since the "p" task pointer is potentially stale.
+		 *
+		 * Return -EAGAIN, and do_wait() will restart the loop from the
+		 * beginning. Do _not_ re-acquire the lock.
+		 */
+		return -EAGAIN;
+	}
+
+	/* move to end of parent's list to avoid starvation */
+	remove_parent(p);
+	add_parent(p, p->parent);
+
+	write_unlock_irq(&tasklist_lock);
+
+	retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
+	if (!retval && stat_addr)
+		retval = put_user((exit_code << 8) | 0x7f, stat_addr);
+	if (!retval && infop)
+		retval = put_user(SIGCHLD, &infop->si_signo);
+	if (!retval && infop)
+		retval = put_user(0, &infop->si_errno);
+	if (!retval && infop)
+		retval = put_user((short)((p->ptrace & PT_PTRACED)
+					  ? CLD_TRAPPED : CLD_STOPPED),
+				  &infop->si_code);
+	if (!retval && infop)
+		retval = put_user(exit_code, &infop->si_status);
+	if (!retval && infop)
+		retval = put_user(p->pid, &infop->si_pid);
+	if (!retval && infop)
+		retval = put_user(p->uid, &infop->si_uid);
+	if (!retval)
+		retval = p->pid;
+	put_task_struct(p);
+
+	BUG_ON(!retval);
+	return retval;
+}
+
+/*
+ * Handle do_wait work for one task in a live, non-stopped state.
+ * read_lock(&tasklist_lock) on entry.  If we return zero, we still hold
+ * the lock and this task is uninteresting.  If we return nonzero, we have
+ * released the lock and the system call should return.
+ */
+static int wait_task_continued(task_t *p, int noreap,
+			       struct siginfo __user *infop,
+			       int __user *stat_addr, struct rusage __user *ru)
+{
+	int retval;
+	pid_t pid;
+	uid_t uid;
+
+	if (unlikely(!p->signal))
+		return 0;
+
+	if (!(p->signal->flags & SIGNAL_STOP_CONTINUED))
+		return 0;
+
+	spin_lock_irq(&p->sighand->siglock);
+	/* Re-check with the lock held.  */
+	if (!(p->signal->flags & SIGNAL_STOP_CONTINUED)) {
+		spin_unlock_irq(&p->sighand->siglock);
+		return 0;
+	}
+	if (!noreap)
+		p->signal->flags &= ~SIGNAL_STOP_CONTINUED;
+	spin_unlock_irq(&p->sighand->siglock);
+
+	pid = p->pid;
+	uid = p->uid;
+	get_task_struct(p);
+	read_unlock(&tasklist_lock);
+
+	if (!infop) {
+		retval = ru ? getrusage(p, RUSAGE_BOTH, ru) : 0;
+		put_task_struct(p);
+		if (!retval && stat_addr)
+			retval = put_user(0xffff, stat_addr);
+		if (!retval)
+			retval = p->pid;
+	} else {
+		retval = wait_noreap_copyout(p, pid, uid,
+					     CLD_CONTINUED, SIGCONT,
+					     infop, ru);
+		BUG_ON(retval == 0);
+	}
+
+	return retval;
+}
+
+
+static inline int my_ptrace_child(struct task_struct *p)
+{
+	if (!(p->ptrace & PT_PTRACED))
+		return 0;
+	if (!(p->ptrace & PT_ATTACHED))
+		return 1;
+	/*
+	 * This child was PTRACE_ATTACH'd.  We should be seeing it only if
+	 * we are the attacher.  If we are the real parent, this is a race
+	 * inside ptrace_attach.  It is waiting for the tasklist_lock,
+	 * which we have to switch the parent links, but has already set
+	 * the flags in p->ptrace.
+	 */
+	return (p->parent != p->real_parent);
+}
+
+static long do_wait(pid_t pid, int options, struct siginfo __user *infop,
+		    int __user *stat_addr, struct rusage __user *ru)
+{
+	DECLARE_WAITQUEUE(wait, current);
+	struct task_struct *tsk;
+	int flag, retval;
+
+	add_wait_queue(&current->signal->wait_chldexit,&wait);
+repeat:
+	/*
+	 * We will set this flag if we see any child that might later
+	 * match our criteria, even if we are not able to reap it yet.
+	 */
+	flag = 0;
+	current->state = TASK_INTERRUPTIBLE;
+	read_lock(&tasklist_lock);
+	tsk = current;
+	do {
+		struct task_struct *p;
+		struct list_head *_p;
+		int ret;
+
+		list_for_each(_p,&tsk->children) {
+			p = list_entry(_p,struct task_struct,sibling);
+
+			ret = eligible_child(pid, options, p);
+			if (!ret)
+				continue;
+
+			switch (p->state) {
+			case TASK_TRACED:
+				if (!my_ptrace_child(p))
+					continue;
+				/*FALLTHROUGH*/
+			case TASK_STOPPED:
+				/*
+				 * It's stopped now, so it might later
+				 * continue, exit, or stop again.
+				 */
+				flag = 1;
+				if (!(options & WUNTRACED) &&
+				    !my_ptrace_child(p))
+					continue;
+				retval = wait_task_stopped(p, ret == 2,
+							   (options & WNOWAIT),
+							   infop,
+							   stat_addr, ru);
+				if (retval == -EAGAIN)
+					goto repeat;
+				if (retval != 0) /* He released the lock.  */
+					goto end;
+				break;
+			default:
+			// case EXIT_DEAD:
+				if (p->exit_state == EXIT_DEAD)
+					continue;
+			// case EXIT_ZOMBIE:
+				if (p->exit_state == EXIT_ZOMBIE) {
+					/*
+					 * Eligible but we cannot release
+					 * it yet:
+					 */
+					if (ret == 2)
+						goto check_continued;
+					if (!likely(options & WEXITED))
+						continue;
+					retval = wait_task_zombie(
+						p, (options & WNOWAIT),
+						infop, stat_addr, ru);
+					/* He released the lock.  */
+					if (retval != 0)
+						goto end;
+					break;
+				}
+check_continued:
+				/*
+				 * It's running now, so it might later
+				 * exit, stop, or stop and then continue.
+				 */
+				flag = 1;
+				if (!unlikely(options & WCONTINUED))
+					continue;
+				retval = wait_task_continued(
+					p, (options & WNOWAIT),
+					infop, stat_addr, ru);
+				if (retval != 0) /* He released the lock.  */
+					goto end;
+				break;
+			}
+		}
+		if (!flag) {
+			list_for_each(_p, &tsk->ptrace_children) {
+				p = list_entry(_p, struct task_struct,
+						ptrace_list);
+				if (!eligible_child(pid, options, p))
+					continue;
+				flag = 1;
+				break;
+			}
+		}
+		if (options & __WNOTHREAD)
+			break;
+		tsk = next_thread(tsk);
+		if (tsk->signal != current->signal)
+			BUG();
+	} while (tsk != current);
+
+	read_unlock(&tasklist_lock);
+	if (flag) {
+		retval = 0;
+		if (options & WNOHANG)
+			goto end;
+		retval = -ERESTARTSYS;
+		if (signal_pending(current))
+			goto end;
+		schedule();
+		goto repeat;
+	}
+	retval = -ECHILD;
+end:
+	current->state = TASK_RUNNING;
+	remove_wait_queue(&current->signal->wait_chldexit,&wait);
+	if (infop) {
+		if (retval > 0)
+		retval = 0;
+		else {
+			/*
+			 * For a WNOHANG return, clear out all the fields
+			 * we would set so the user can easily tell the
+			 * difference.
+			 */
+			if (!retval)
+				retval = put_user(0, &infop->si_signo);
+			if (!retval)
+				retval = put_user(0, &infop->si_errno);
+			if (!retval)
+				retval = put_user(0, &infop->si_code);
+			if (!retval)
+				retval = put_user(0, &infop->si_pid);
+			if (!retval)
+				retval = put_user(0, &infop->si_uid);
+			if (!retval)
+				retval = put_user(0, &infop->si_status);
+		}
+	}
+	return retval;
+}
+
+asmlinkage long sys_waitid(int which, pid_t pid,
+			   struct siginfo __user *infop, int options,
+			   struct rusage __user *ru)
+{
+	long ret;
+
+	if (options & ~(WNOHANG|WNOWAIT|WEXITED|WSTOPPED|WCONTINUED))
+		return -EINVAL;
+	if (!(options & (WEXITED|WSTOPPED|WCONTINUED)))
+		return -EINVAL;
+
+	switch (which) {
+	case P_ALL:
+		pid = -1;
+		break;
+	case P_PID:
+		if (pid <= 0)
+			return -EINVAL;
+		break;
+	case P_PGID:
+		if (pid <= 0)
+			return -EINVAL;
+		pid = -pid;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	ret = do_wait(pid, options, infop, NULL, ru);
+
+	/* avoid REGPARM breakage on x86: */
+	prevent_tail_call(ret);
+	return ret;
+}
+
+asmlinkage long sys_wait4(pid_t pid, int __user *stat_addr,
+			  int options, struct rusage __user *ru)
+{
+	long ret;
+
+	if (options & ~(WNOHANG|WUNTRACED|WCONTINUED|
+			__WNOTHREAD|__WCLONE|__WALL))
+		return -EINVAL;
+	ret = do_wait(pid, options | WEXITED, NULL, stat_addr, ru);
+
+	/* avoid REGPARM breakage on x86: */
+	prevent_tail_call(ret);
+	return ret;
+}
+
+#ifdef __ARCH_WANT_SYS_WAITPID
+
+/*
+ * sys_waitpid() remains for compatibility. waitpid() should be
+ * implemented by calling sys_wait4() from libc.a.
+ */
+asmlinkage long sys_waitpid(pid_t pid, int __user *stat_addr, int options)
+{
+	return sys_wait4(pid, stat_addr, options, NULL);
+}
+
+#endif
