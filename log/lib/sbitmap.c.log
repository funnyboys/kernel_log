commit df034c93f15ee71df231ff9fe311d27ff08a2a52
Author: David Jeffery <djeffery@redhat.com>
Date:   Tue Dec 17 11:00:24 2019 -0500

    sbitmap: only queue kyber's wait callback if not already active
    
    Under heavy loads where the kyber I/O scheduler hits the token limits for
    its scheduling domains, kyber can become stuck.  When active requests
    complete, kyber may not be woken up leaving the I/O requests in kyber
    stuck.
    
    This stuck state is due to a race condition with kyber and the sbitmap
    functions it uses to run a callback when enough requests have completed.
    The running of a sbt_wait callback can race with the attempt to insert the
    sbt_wait.  Since sbitmap_del_wait_queue removes the sbt_wait from the list
    first then sets the sbq field to NULL, kyber can see the item as not on a
    list but the call to sbitmap_add_wait_queue will see sbq as non-NULL. This
    results in the sbt_wait being inserted onto the wait list but ws_active
    doesn't get incremented.  So the sbitmap queue does not know there is a
    waiter on a wait list.
    
    Since sbitmap doesn't think there is a waiter, kyber may never be
    informed that there are domain tokens available and the I/O never advances.
    With the sbt_wait on a wait list, kyber believes it has an active waiter
    so cannot insert a new waiter when reaching the domain's full state.
    
    This race can be fixed by only adding the sbt_wait to the queue if the
    sbq field is NULL.  If sbq is not NULL, there is already an action active
    which will trigger the re-running of kyber.  Let it run and add the
    sbt_wait to the wait list if still needing to wait.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: David Jeffery <djeffery@redhat.com>
    Reported-by: John Pittman <jpittman@redhat.com>
    Tested-by: John Pittman <jpittman@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 33feec8989f1..af88d1346dd7 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -650,8 +650,8 @@ void sbitmap_add_wait_queue(struct sbitmap_queue *sbq,
 	if (!sbq_wait->sbq) {
 		sbq_wait->sbq = sbq;
 		atomic_inc(&sbq->ws_active);
+		add_wait_queue(&ws->wait, &sbq_wait->wait);
 	}
-	add_wait_queue(&ws->wait, &sbq_wait->wait);
 }
 EXPORT_SYMBOL_GPL(sbitmap_add_wait_queue);
 

commit 708edafa883186f55b24fa0c380242b5282f9105
Author: John Garry <john.garry@huawei.com>
Date:   Thu Nov 14 01:27:22 2019 +0800

    sbitmap: Delete sbitmap_any_bit_clear()
    
    Since the only caller of this function has been deleted, delete this one
    also.
    
    Signed-off-by: John Garry <john.garry@huawei.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 969e5400a615..33feec8989f1 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -236,23 +236,6 @@ bool sbitmap_any_bit_set(const struct sbitmap *sb)
 }
 EXPORT_SYMBOL_GPL(sbitmap_any_bit_set);
 
-bool sbitmap_any_bit_clear(const struct sbitmap *sb)
-{
-	unsigned int i;
-
-	for (i = 0; i < sb->map_nr; i++) {
-		const struct sbitmap_word *word = &sb->map[i];
-		unsigned long mask = word->word & ~word->cleared;
-		unsigned long ret;
-
-		ret = find_first_zero_bit(&mask, word->depth);
-		if (ret < word->depth)
-			return true;
-	}
-	return false;
-}
-EXPORT_SYMBOL_GPL(sbitmap_any_bit_clear);
-
 static unsigned int __sbitmap_weight(const struct sbitmap *sb, bool set)
 {
 	unsigned int i, weight = 0;

commit 417232880c8a646739dbf4666a231505a1917fcb
Author: Pavel Begunkov <asml.silence@gmail.com>
Date:   Thu May 23 18:39:16 2019 +0300

    sbitmap: Replace cmpxchg with xchg
    
    cmpxchg() with an immediate value could be replaced with less expensive
    xchg(). The same true if new value don't _depend_ on the old one.
    
    In the second block, atomic_cmpxchg() return value isn't checked, so
    after atomic_cmpxchg() ->  atomic_xchg() conversion it could be replaced
    with atomic_set(). Comparison with atomic_read() in the second chunk was
    left as an optimisation (if that was the initial intention).
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Pavel Begunkov <asml.silence@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 54f57cd117c6..969e5400a615 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -26,9 +26,7 @@ static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
 	/*
 	 * First get a stable cleared mask, setting the old mask to 0.
 	 */
-	do {
-		mask = sb->map[index].cleared;
-	} while (cmpxchg(&sb->map[index].cleared, mask, 0) != mask);
+	mask = xchg(&sb->map[index].cleared, 0);
 
 	/*
 	 * Now clear the masked bits in our free word
@@ -516,10 +514,8 @@ static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 		struct sbq_wait_state *ws = &sbq->ws[wake_index];
 
 		if (waitqueue_active(&ws->wait)) {
-			int o = atomic_read(&sbq->wake_index);
-
-			if (wake_index != o)
-				atomic_cmpxchg(&sbq->wake_index, o, wake_index);
+			if (wake_index != atomic_read(&sbq->wake_index))
+				atomic_set(&sbq->wake_index, wake_index);
 			return ws;
 		}
 

commit 0fc479b1ad6358d2440faf79a43d422065b77dc0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 29 16:57:42 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 328
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license v2 as published
      by the free software foundation this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details you
      should have received a copy of the gnu general public license along
      with this program if not see https www gnu org licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 2 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190530000435.923873561@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 4a7fc4915dfc..54f57cd117c6 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -1,18 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Copyright (C) 2016 Facebook
  * Copyright (C) 2013-2014 Jens Axboe
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public
- * License v2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <https://www.gnu.org/licenses/>.
  */
 
 #include <linux/sched.h>

commit a0934fd2b1208458e55fc4b48f55889809fce666
Author: Andrea Parri <andrea.parri@amarulasolutions.com>
Date:   Mon May 20 19:23:57 2019 +0200

    sbitmap: fix improper use of smp_mb__before_atomic()
    
    This barrier only applies to the read-modify-write operations; in
    particular, it does not apply to the atomic_set() primitive.
    
    Replace the barrier with an smp_mb().
    
    Fixes: 6c0ca7ae292ad ("sbitmap: fix wakeup hang after sbq resize")
    Cc: stable@vger.kernel.org
    Reported-by: "Paul E. McKenney" <paulmck@linux.ibm.com>
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Reviewed-by: Ming Lei <ming.lei@redhat.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Omar Sandoval <osandov@fb.com>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: linux-block@vger.kernel.org
    Cc: "Paul E. McKenney" <paulmck@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 155fe38756ec..4a7fc4915dfc 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -435,7 +435,7 @@ static void sbitmap_queue_update_wake_batch(struct sbitmap_queue *sbq,
 		 * to ensure that the batch size is updated before the wait
 		 * counts.
 		 */
-		smp_mb__before_atomic();
+		smp_mb();
 		for (i = 0; i < SBQ_WAIT_QUEUES; i++)
 			atomic_set(&sbq->ws[i].wait_cnt, 1);
 	}

commit e6d1fa584e0dd9bfebaf345e9feea588cf75ead2
Author: Ming Lei <ming.lei@redhat.com>
Date:   Fri Mar 22 09:13:51 2019 +0800

    sbitmap: order READ/WRITE freed instance and setting clear bit
    
    Inside sbitmap_queue_clear(), once the clear bit is set, it will be
    visiable to allocation path immediately. Meantime READ/WRITE on old
    associated instance(such as request in case of blk-mq) may be
    out-of-order with the setting clear bit, so race with re-allocation
    may be triggered.
    
    Adds one memory barrier for ordering READ/WRITE of the freed associated
    instance with setting clear bit for avoiding race with re-allocation.
    
    The following kernel oops triggerd by block/006 on aarch64 may be fixed:
    
    [  142.330954] Unable to handle kernel NULL pointer dereference at virtual address 0000000000000330
    [  142.338794] Mem abort info:
    [  142.341554]   ESR = 0x96000005
    [  142.344632]   Exception class = DABT (current EL), IL = 32 bits
    [  142.350500]   SET = 0, FnV = 0
    [  142.353544]   EA = 0, S1PTW = 0
    [  142.356678] Data abort info:
    [  142.359528]   ISV = 0, ISS = 0x00000005
    [  142.363343]   CM = 0, WnR = 0
    [  142.366305] user pgtable: 64k pages, 48-bit VAs, pgdp = 000000002a3c51c0
    [  142.372983] [0000000000000330] pgd=0000000000000000, pud=0000000000000000
    [  142.379777] Internal error: Oops: 96000005 [#1] SMP
    [  142.384613] Modules linked in: null_blk ib_isert iscsi_target_mod ib_srpt target_core_mod ib_srp scsi_transport_srp vfat fat rpcrdma sunrpc rdma_ucm ib_iser rdma_cm iw_cm libiscsi ib_umad scsi_transport_iscsi ib_ipoib ib_cm mlx5_ib ib_uverbs ib_core sbsa_gwdt crct10dif_ce ghash_ce ipmi_ssif sha2_ce ipmi_devintf sha256_arm64 sg sha1_ce ipmi_msghandler ip_tables xfs libcrc32c mlx5_core sdhci_acpi mlxfw ahci_platform at803x sdhci libahci_platform qcom_emac mmc_core hdma hdma_mgmt i2c_dev [last unloaded: null_blk]
    [  142.429753] CPU: 7 PID: 1983 Comm: fio Not tainted 5.0.0.cki #2
    [  142.449458] pstate: 00400005 (nzcv daif +PAN -UAO)
    [  142.454239] pc : __blk_mq_free_request+0x4c/0xa8
    [  142.458830] lr : blk_mq_free_request+0xec/0x118
    [  142.463344] sp : ffff00003360f6a0
    [  142.466646] x29: ffff00003360f6a0 x28: ffff000010e70000
    [  142.471941] x27: ffff801729a50048 x26: 0000000000010000
    [  142.477232] x25: ffff00003360f954 x24: ffff7bdfff021440
    [  142.482529] x23: 0000000000000000 x22: 00000000ffffffff
    [  142.487830] x21: ffff801729810000 x20: 0000000000000000
    [  142.493123] x19: ffff801729a50000 x18: 0000000000000000
    [  142.498413] x17: 0000000000000000 x16: 0000000000000001
    [  142.503709] x15: 00000000000000ff x14: ffff7fe000000000
    [  142.509003] x13: ffff8017dcde09a0 x12: 0000000000000000
    [  142.514308] x11: 0000000000000001 x10: 0000000000000008
    [  142.519597] x9 : ffff8017dcde09a0 x8 : 0000000000002000
    [  142.524889] x7 : ffff8017dcde0a00 x6 : 000000015388f9be
    [  142.530187] x5 : 0000000000000001 x4 : 0000000000000000
    [  142.535478] x3 : 0000000000000000 x2 : 0000000000000000
    [  142.540777] x1 : 0000000000000001 x0 : ffff00001041b194
    [  142.546071] Process fio (pid: 1983, stack limit = 0x000000006460a0ea)
    [  142.552500] Call trace:
    [  142.554926]  __blk_mq_free_request+0x4c/0xa8
    [  142.559181]  blk_mq_free_request+0xec/0x118
    [  142.563352]  blk_mq_end_request+0xfc/0x120
    [  142.567444]  end_cmd+0x3c/0xa8 [null_blk]
    [  142.571434]  null_complete_rq+0x20/0x30 [null_blk]
    [  142.576194]  blk_mq_complete_request+0x108/0x148
    [  142.580797]  null_handle_cmd+0x1d4/0x718 [null_blk]
    [  142.585662]  null_queue_rq+0x60/0xa8 [null_blk]
    [  142.590171]  blk_mq_try_issue_directly+0x148/0x280
    [  142.594949]  blk_mq_try_issue_list_directly+0x9c/0x108
    [  142.600064]  blk_mq_sched_insert_requests+0xb0/0xd0
    [  142.604926]  blk_mq_flush_plug_list+0x16c/0x2a0
    [  142.609441]  blk_flush_plug_list+0xec/0x118
    [  142.613608]  blk_finish_plug+0x3c/0x4c
    [  142.617348]  blkdev_direct_IO+0x3b4/0x428
    [  142.621336]  generic_file_read_iter+0x84/0x180
    [  142.625761]  blkdev_read_iter+0x50/0x78
    [  142.629579]  aio_read.isra.6+0xf8/0x190
    [  142.633409]  __io_submit_one.isra.8+0x148/0x738
    [  142.637912]  io_submit_one.isra.9+0x88/0xb8
    [  142.642078]  __arm64_sys_io_submit+0xe0/0x238
    [  142.646428]  el0_svc_handler+0xa0/0x128
    [  142.650238]  el0_svc+0x8/0xc
    [  142.653104] Code: b9402a63 f9000a7f 3100047f 540000a0 (f9419a81)
    [  142.659202] ---[ end trace 467586bc175eb09d ]---
    
    Fixes: ea86ea2cdced20057da ("sbitmap: ammortize cost of clearing bits")
    Reported-and-bisected_and_tested-by: Yi Zhang <yi.zhang@redhat.com>
    Cc: Yi Zhang <yi.zhang@redhat.com>
    Cc: "jianchao.wang" <jianchao.w.wang@oracle.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 5b382c1244ed..155fe38756ec 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -591,6 +591,17 @@ EXPORT_SYMBOL_GPL(sbitmap_queue_wake_up);
 void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 			 unsigned int cpu)
 {
+	/*
+	 * Once the clear bit is set, the bit may be allocated out.
+	 *
+	 * Orders READ/WRITE on the asssociated instance(such as request
+	 * of blk_mq) by this bit for avoiding race with re-allocation,
+	 * and its pair is the memory barrier implied in __sbitmap_get_word.
+	 *
+	 * One invariant is that the clear bit has to be zero when the bit
+	 * is in use.
+	 */
+	smp_mb__before_atomic();
 	sbitmap_deferred_clear_bit(&sbq->sb, nr);
 
 	/*

commit fe76fc6aaf538df27708ffa3e5d549a6c8e16142
Author: Ming Lei <ming.lei@redhat.com>
Date:   Tue Jan 15 11:59:52 2019 +0800

    sbitmap: Protect swap_lock from hardirq
    
    Because we may call blk_mq_get_driver_tag() directly from
    blk_mq_dispatch_rq_list() without holding any lock, then HARDIRQ may
    come and the above DEADLOCK is triggered.
    
    Commit ab53dcfb3e7b ("sbitmap: Protect swap_lock from hardirq") tries to
    fix this issue by using 'spin_lock_bh', which isn't enough because we
    complete request from hardirq context direclty in case of multiqueue.
    
    Cc: Clark Williams <williams@redhat.com>
    Fixes: ab53dcfb3e7b ("sbitmap: Protect swap_lock from hardirq")
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 864354000e04..5b382c1244ed 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -27,8 +27,9 @@ static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
 {
 	unsigned long mask, val;
 	bool ret = false;
+	unsigned long flags;
 
-	spin_lock_bh(&sb->map[index].swap_lock);
+	spin_lock_irqsave(&sb->map[index].swap_lock, flags);
 
 	if (!sb->map[index].cleared)
 		goto out_unlock;
@@ -49,7 +50,7 @@ static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
 
 	ret = true;
 out_unlock:
-	spin_unlock_bh(&sb->map[index].swap_lock);
+	spin_unlock_irqrestore(&sb->map[index].swap_lock, flags);
 	return ret;
 }
 

commit 3719876809e745b9db5293d418600c194bbf5c23
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jan 14 12:25:40 2019 -0500

    sbitmap: Protect swap_lock from softirqs
    
    The swap_lock used by sbitmap has a chain with locks taken from softirq,
    but the swap_lock is not protected from being preempted by softirqs.
    
    A chain exists of:
    
     sbq->ws[i].wait -> dispatch_wait_lock -> swap_lock
    
    Where the sbq->ws[i].wait lock can be taken from softirq context, which
    means all locks below it in the chain must also be protected from
    softirqs.
    
    Reported-by: Clark Williams <williams@redhat.com>
    Fixes: 58ab5e32e6fd ("sbitmap: silence bogus lockdep IRQ warning")
    Fixes: ea86ea2cdced ("sbitmap: amortize cost of clearing bits")
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Ming Lei <ming.lei@redhat.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 65c2d06250a6..864354000e04 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -26,14 +26,9 @@
 static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
 {
 	unsigned long mask, val;
-	unsigned long __maybe_unused flags;
 	bool ret = false;
 
-	/* Silence bogus lockdep warning */
-#if defined(CONFIG_LOCKDEP)
-	local_irq_save(flags);
-#endif
-	spin_lock(&sb->map[index].swap_lock);
+	spin_lock_bh(&sb->map[index].swap_lock);
 
 	if (!sb->map[index].cleared)
 		goto out_unlock;
@@ -54,10 +49,7 @@ static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
 
 	ret = true;
 out_unlock:
-	spin_unlock(&sb->map[index].swap_lock);
-#if defined(CONFIG_LOCKDEP)
-	local_irq_restore(flags);
-#endif
+	spin_unlock_bh(&sb->map[index].swap_lock);
 	return ret;
 }
 

commit 9f6b7ef6c3ebe35be77b0ae3cf12e4d25ae80420
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Dec 20 08:49:00 2018 -0700

    sbitmap: add helpers for add/del wait queue handling
    
    After commit 5d2ee7122c73, users of sbitmap that need wait queue
    handling must use the provided helpers. But we only added
    prepare_to_wait()/finish_wait() style helpers, add the equivalent
    add_wait_queue/list_del wrappers as we..
    
    This is needed to ensure kyber plays by the sbitmap waitqueue
    rules.
    
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 5b3e56d68dab..65c2d06250a6 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -671,13 +671,35 @@ void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_show);
 
+void sbitmap_add_wait_queue(struct sbitmap_queue *sbq,
+			    struct sbq_wait_state *ws,
+			    struct sbq_wait *sbq_wait)
+{
+	if (!sbq_wait->sbq) {
+		sbq_wait->sbq = sbq;
+		atomic_inc(&sbq->ws_active);
+	}
+	add_wait_queue(&ws->wait, &sbq_wait->wait);
+}
+EXPORT_SYMBOL_GPL(sbitmap_add_wait_queue);
+
+void sbitmap_del_wait_queue(struct sbq_wait *sbq_wait)
+{
+	list_del_init(&sbq_wait->wait.entry);
+	if (sbq_wait->sbq) {
+		atomic_dec(&sbq_wait->sbq->ws_active);
+		sbq_wait->sbq = NULL;
+	}
+}
+EXPORT_SYMBOL_GPL(sbitmap_del_wait_queue);
+
 void sbitmap_prepare_to_wait(struct sbitmap_queue *sbq,
 			     struct sbq_wait_state *ws,
 			     struct sbq_wait *sbq_wait, int state)
 {
-	if (!sbq_wait->accounted) {
+	if (!sbq_wait->sbq) {
 		atomic_inc(&sbq->ws_active);
-		sbq_wait->accounted = 1;
+		sbq_wait->sbq = sbq;
 	}
 	prepare_to_wait_exclusive(&ws->wait, &sbq_wait->wait, state);
 }
@@ -687,9 +709,9 @@ void sbitmap_finish_wait(struct sbitmap_queue *sbq, struct sbq_wait_state *ws,
 			 struct sbq_wait *sbq_wait)
 {
 	finish_wait(&ws->wait, &sbq_wait->wait);
-	if (sbq_wait->accounted) {
+	if (sbq_wait->sbq) {
 		atomic_dec(&sbq->ws_active);
-		sbq_wait->accounted = 0;
+		sbq_wait->sbq = NULL;
 	}
 }
 EXPORT_SYMBOL_GPL(sbitmap_finish_wait);

commit b2dbff1bb893d5dfdf501231ff5505ca10cdede3
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Dec 11 18:39:41 2018 -0700

    sbitmap: flush deferred clears for resize and shallow gets
    
    We're missing a deferred clear off the shallow get, which can cause
    a hang. Additionally, when we resize the sbitmap, we should also
    flush deferred clears for good measure.
    
    Ensure we have full coverage on batch clears, even for paths where
    we would not be doing deferred clear. This makes it less error
    prone for future additions.
    
    Reported-by: Bart Van Assche <bvanassche@acm.org>
    Tested-by: Ming Lei <ming.lei@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 2261136ae067..5b3e56d68dab 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -20,6 +20,47 @@
 #include <linux/sbitmap.h>
 #include <linux/seq_file.h>
 
+/*
+ * See if we have deferred clears that we can batch move
+ */
+static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
+{
+	unsigned long mask, val;
+	unsigned long __maybe_unused flags;
+	bool ret = false;
+
+	/* Silence bogus lockdep warning */
+#if defined(CONFIG_LOCKDEP)
+	local_irq_save(flags);
+#endif
+	spin_lock(&sb->map[index].swap_lock);
+
+	if (!sb->map[index].cleared)
+		goto out_unlock;
+
+	/*
+	 * First get a stable cleared mask, setting the old mask to 0.
+	 */
+	do {
+		mask = sb->map[index].cleared;
+	} while (cmpxchg(&sb->map[index].cleared, mask, 0) != mask);
+
+	/*
+	 * Now clear the masked bits in our free word
+	 */
+	do {
+		val = sb->map[index].word;
+	} while (cmpxchg(&sb->map[index].word, val, val & ~mask) != val);
+
+	ret = true;
+out_unlock:
+	spin_unlock(&sb->map[index].swap_lock);
+#if defined(CONFIG_LOCKDEP)
+	local_irq_restore(flags);
+#endif
+	return ret;
+}
+
 int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
 		      gfp_t flags, int node)
 {
@@ -70,6 +111,9 @@ void sbitmap_resize(struct sbitmap *sb, unsigned int depth)
 	unsigned int bits_per_word = 1U << sb->shift;
 	unsigned int i;
 
+	for (i = 0; i < sb->map_nr; i++)
+		sbitmap_deferred_clear(sb, i);
+
 	sb->depth = depth;
 	sb->map_nr = DIV_ROUND_UP(sb->depth, bits_per_word);
 
@@ -112,47 +156,6 @@ static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
 	return nr;
 }
 
-/*
- * See if we have deferred clears that we can batch move
- */
-static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
-{
-	unsigned long mask, val;
-	unsigned long __maybe_unused flags;
-	bool ret = false;
-
-	/* Silence bogus lockdep warning */
-#if defined(CONFIG_LOCKDEP)
-	local_irq_save(flags);
-#endif
-	spin_lock(&sb->map[index].swap_lock);
-
-	if (!sb->map[index].cleared)
-		goto out_unlock;
-
-	/*
-	 * First get a stable cleared mask, setting the old mask to 0.
-	 */
-	do {
-		mask = sb->map[index].cleared;
-	} while (cmpxchg(&sb->map[index].cleared, mask, 0) != mask);
-
-	/*
-	 * Now clear the masked bits in our free word
-	 */
-	do {
-		val = sb->map[index].word;
-	} while (cmpxchg(&sb->map[index].word, val, val & ~mask) != val);
-
-	ret = true;
-out_unlock:
-	spin_unlock(&sb->map[index].swap_lock);
-#if defined(CONFIG_LOCKDEP)
-	local_irq_restore(flags);
-#endif
-	return ret;
-}
-
 static int sbitmap_find_bit_in_index(struct sbitmap *sb, int index,
 				     unsigned int alloc_hint, bool round_robin)
 {
@@ -215,6 +218,7 @@ int sbitmap_get_shallow(struct sbitmap *sb, unsigned int alloc_hint,
 	index = SB_NR_TO_INDEX(sb, alloc_hint);
 
 	for (i = 0; i < sb->map_nr; i++) {
+again:
 		nr = __sbitmap_get_word(&sb->map[index].word,
 					min(sb->map[index].depth, shallow_depth),
 					SB_NR_TO_BIT(sb, alloc_hint), true);
@@ -223,6 +227,9 @@ int sbitmap_get_shallow(struct sbitmap *sb, unsigned int alloc_hint,
 			break;
 		}
 
+		if (sbitmap_deferred_clear(sb, index))
+			goto again;
+
 		/* Jump to next index. */
 		index++;
 		alloc_hint = index << sb->shift;
@@ -242,7 +249,7 @@ bool sbitmap_any_bit_set(const struct sbitmap *sb)
 	unsigned int i;
 
 	for (i = 0; i < sb->map_nr; i++) {
-		if (sb->map[i].word)
+		if (sb->map[i].word & ~sb->map[i].cleared)
 			return true;
 	}
 	return false;
@@ -255,9 +262,10 @@ bool sbitmap_any_bit_clear(const struct sbitmap *sb)
 
 	for (i = 0; i < sb->map_nr; i++) {
 		const struct sbitmap_word *word = &sb->map[i];
+		unsigned long mask = word->word & ~word->cleared;
 		unsigned long ret;
 
-		ret = find_first_zero_bit(&word->word, word->depth);
+		ret = find_first_zero_bit(&mask, word->depth);
 		if (ret < word->depth)
 			return true;
 	}

commit 58ab5e32e6fd83e33943614e7257f2ac5823824a
Author: Jens Axboe <axboe@kernel.dk>
Date:   Sun Dec 9 17:43:20 2018 -0700

    sbitmap: silence bogus lockdep IRQ warning
    
    Ming reports that lockdep spews the following trace. What this
    essentially says is that the sbitmap swap_lock was used inconsistently
    in IRQ enabled and disabled context, and that is usually indicative of a
    bug that will cause a deadlock.
    
    For this case, it's a false positive. The swap_lock is used from process
    context only, when we swap the bits in the word and cleared mask. We
    also end up doing that when we are getting a driver tag, from the
    blk_mq_mark_tag_wait(), and from there we hold the waitqueue lock with
    IRQs disabled. However, this isn't from an actual IRQ, it's still
    process context.
    
    In lieu of a better way to fix this, simply always disable interrupts
    when grabbing the swap_lock if lockdep is enabled.
    
    [  100.967642] ================start test sanity/001================
    [  101.238280] null: module loaded
    [  106.093735]
    [  106.094012] =====================================================
    [  106.094854] WARNING: SOFTIRQ-safe -> SOFTIRQ-unsafe lock order detected
    [  106.095759] 4.20.0-rc3_5d2ee7122c73_for-next+ #1 Not tainted
    [  106.096551] -----------------------------------------------------
    [  106.097386] fio/1043 [HC0[0]:SC0[0]:HE0:SE1] is trying to acquire:
    [  106.098231] 000000004c43fa71
    (&(&sb->map[i].swap_lock)->rlock){+.+.}, at: sbitmap_get+0xd5/0x22c
    [  106.099431]
    [  106.099431] and this task is already holding:
    [  106.100229] 000000007eec8b2f
    (&(&hctx->dispatch_wait_lock)->rlock){....}, at:
    blk_mq_dispatch_rq_list+0x4c1/0xd7c
    [  106.101630] which would create a new lock dependency:
    [  106.102326]  (&(&hctx->dispatch_wait_lock)->rlock){....} ->
    (&(&sb->map[i].swap_lock)->rlock){+.+.}
    [  106.103553]
    [  106.103553] but this new dependency connects a SOFTIRQ-irq-safe lock:
    [  106.104580]  (&sbq->ws[i].wait){..-.}
    [  106.104582]
    [  106.104582] ... which became SOFTIRQ-irq-safe at:
    [  106.105751]   _raw_spin_lock_irqsave+0x4b/0x82
    [  106.106284]   __wake_up_common_lock+0x119/0x1b9
    [  106.106825]   sbitmap_queue_wake_up+0x33f/0x383
    [  106.107456]   sbitmap_queue_clear+0x4c/0x9a
    [  106.108046]   __blk_mq_free_request+0x188/0x1d3
    [  106.108581]   blk_mq_free_request+0x23b/0x26b
    [  106.109102]   scsi_end_request+0x345/0x5d7
    [  106.109587]   scsi_io_completion+0x4b5/0x8f0
    [  106.110099]   scsi_finish_command+0x412/0x456
    [  106.110615]   scsi_softirq_done+0x23f/0x29b
    [  106.111115]   blk_done_softirq+0x2a7/0x2e6
    [  106.111608]   __do_softirq+0x360/0x6ad
    [  106.112062]   run_ksoftirqd+0x2f/0x5b
    [  106.112499]   smpboot_thread_fn+0x3a5/0x3db
    [  106.113000]   kthread+0x1d4/0x1e4
    [  106.113457]   ret_from_fork+0x3a/0x50
    [  106.113969]
    [  106.113969] to a SOFTIRQ-irq-unsafe lock:
    [  106.114672]  (&(&sb->map[i].swap_lock)->rlock){+.+.}
    [  106.114674]
    [  106.114674] ... which became SOFTIRQ-irq-unsafe at:
    [  106.116000] ...
    [  106.116003]   _raw_spin_lock+0x33/0x64
    [  106.116676]   sbitmap_get+0xd5/0x22c
    [  106.117134]   __sbitmap_queue_get+0xe8/0x177
    [  106.117731]   __blk_mq_get_tag+0x1e6/0x22d
    [  106.118286]   blk_mq_get_tag+0x1db/0x6e4
    [  106.118756]   blk_mq_get_driver_tag+0x161/0x258
    [  106.119383]   blk_mq_dispatch_rq_list+0x28e/0xd7c
    [  106.120043]   blk_mq_do_dispatch_sched+0x23a/0x287
    [  106.120607]   blk_mq_sched_dispatch_requests+0x379/0x3fc
    [  106.121234]   __blk_mq_run_hw_queue+0x137/0x17e
    [  106.121781]   __blk_mq_delay_run_hw_queue+0x80/0x25f
    [  106.122366]   blk_mq_run_hw_queue+0x151/0x187
    [  106.122887]   blk_mq_sched_insert_requests+0x13f/0x175
    [  106.123492]   blk_mq_flush_plug_list+0x7d6/0x81b
    [  106.124042]   blk_flush_plug_list+0x392/0x3d7
    [  106.124557]   blk_finish_plug+0x37/0x4f
    [  106.125019]   read_pages+0x3ef/0x430
    [  106.125446]   __do_page_cache_readahead+0x18e/0x2fc
    [  106.126027]   force_page_cache_readahead+0x121/0x133
    [  106.126621]   page_cache_sync_readahead+0x35f/0x3bb
    [  106.127229]   generic_file_buffered_read+0x410/0x1860
    [  106.127932]   __vfs_read+0x319/0x38f
    [  106.128415]   vfs_read+0xd2/0x19a
    [  106.128817]   ksys_read+0xb9/0x135
    [  106.129225]   do_syscall_64+0x140/0x385
    [  106.129684]   entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  106.130292]
    [  106.130292] other info that might help us debug this:
    [  106.130292]
    [  106.131226] Chain exists of:
    [  106.131226]   &sbq->ws[i].wait -->
    &(&hctx->dispatch_wait_lock)->rlock -->
    &(&sb->map[i].swap_lock)->rlock
    [  106.131226]
    [  106.132865]  Possible interrupt unsafe locking scenario:
    [  106.132865]
    [  106.133659]        CPU0                    CPU1
    [  106.134194]        ----                    ----
    [  106.134733]   lock(&(&sb->map[i].swap_lock)->rlock);
    [  106.135318]                                local_irq_disable();
    [  106.136014]                                lock(&sbq->ws[i].wait);
    [  106.136747]
    lock(&(&hctx->dispatch_wait_lock)->rlock);
    [  106.137742]   <Interrupt>
    [  106.138110]     lock(&sbq->ws[i].wait);
    [  106.138625]
    [  106.138625]  *** DEADLOCK ***
    [  106.138625]
    [  106.139430] 3 locks held by fio/1043:
    [  106.139947]  #0: 0000000076ff0fd9 (rcu_read_lock){....}, at:
    hctx_lock+0x29/0xe8
    [  106.140813]  #1: 000000002feb1016 (&sbq->ws[i].wait){..-.}, at:
    blk_mq_dispatch_rq_list+0x4ad/0xd7c
    [  106.141877]  #2: 000000007eec8b2f
    (&(&hctx->dispatch_wait_lock)->rlock){....}, at:
    blk_mq_dispatch_rq_list+0x4c1/0xd7c
    [  106.143267]
    [  106.143267] the dependencies between SOFTIRQ-irq-safe lock and the
    holding lock:
    [  106.144351]  -> (&sbq->ws[i].wait){..-.} ops: 82 {
    [  106.144926]     IN-SOFTIRQ-W at:
    [  106.145314]                       _raw_spin_lock_irqsave+0x4b/0x82
    [  106.146042]                       __wake_up_common_lock+0x119/0x1b9
    [  106.146785]                       sbitmap_queue_wake_up+0x33f/0x383
    [  106.147567]                       sbitmap_queue_clear+0x4c/0x9a
    [  106.148379]                       __blk_mq_free_request+0x188/0x1d3
    [  106.149148]                       blk_mq_free_request+0x23b/0x26b
    [  106.149864]                       scsi_end_request+0x345/0x5d7
    [  106.150546]                       scsi_io_completion+0x4b5/0x8f0
    [  106.151367]                       scsi_finish_command+0x412/0x456
    [  106.152157]                       scsi_softirq_done+0x23f/0x29b
    [  106.152855]                       blk_done_softirq+0x2a7/0x2e6
    [  106.153537]                       __do_softirq+0x360/0x6ad
    [  106.154280]                       run_ksoftirqd+0x2f/0x5b
    [  106.155020]                       smpboot_thread_fn+0x3a5/0x3db
    [  106.155828]                       kthread+0x1d4/0x1e4
    [  106.156526]                       ret_from_fork+0x3a/0x50
    [  106.157267]     INITIAL USE at:
    [  106.157713]                      _raw_spin_lock_irqsave+0x4b/0x82
    [  106.158542]                      prepare_to_wait_exclusive+0xa8/0x215
    [  106.159421]                      blk_mq_get_tag+0x34f/0x6e4
    [  106.160186]                      blk_mq_get_request+0x48e/0xaef
    [  106.160997]                      blk_mq_make_request+0x27e/0xbd2
    [  106.161828]                      generic_make_request+0x4d1/0x873
    [  106.162661]                      submit_bio+0x20c/0x253
    [  106.163379]                      mpage_bio_submit+0x44/0x4b
    [  106.164142]                      mpage_readpages+0x3c2/0x407
    [  106.164919]                      read_pages+0x13a/0x430
    [  106.165633]                      __do_page_cache_readahead+0x18e/0x2fc
    [  106.166530]                      force_page_cache_readahead+0x121/0x133
    [  106.167439]                      page_cache_sync_readahead+0x35f/0x3bb
    [  106.168337]                      generic_file_buffered_read+0x410/0x1860
    [  106.169255]                      __vfs_read+0x319/0x38f
    [  106.169977]                      vfs_read+0xd2/0x19a
    [  106.170662]                      ksys_read+0xb9/0x135
    [  106.171356]                      do_syscall_64+0x140/0x385
    [  106.172120]                      entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  106.173051]   }
    [  106.173308]   ... key      at: [<ffffffff85094600>] __key.26481+0x0/0x40
    [  106.174219]   ... acquired at:
    [  106.174646]    _raw_spin_lock+0x33/0x64
    [  106.175183]    blk_mq_dispatch_rq_list+0x4c1/0xd7c
    [  106.175843]    blk_mq_do_dispatch_sched+0x23a/0x287
    [  106.176518]    blk_mq_sched_dispatch_requests+0x379/0x3fc
    [  106.177262]    __blk_mq_run_hw_queue+0x137/0x17e
    [  106.177900]    __blk_mq_delay_run_hw_queue+0x80/0x25f
    [  106.178591]    blk_mq_run_hw_queue+0x151/0x187
    [  106.179207]    blk_mq_sched_insert_requests+0x13f/0x175
    [  106.179926]    blk_mq_flush_plug_list+0x7d6/0x81b
    [  106.180571]    blk_flush_plug_list+0x392/0x3d7
    [  106.181187]    blk_finish_plug+0x37/0x4f
    [  106.181737]    __se_sys_io_submit+0x171/0x304
    [  106.182346]    do_syscall_64+0x140/0x385
    [  106.182895]    entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  106.183607]
    [  106.183830] -> (&(&hctx->dispatch_wait_lock)->rlock){....} ops: 1 {
    [  106.184691]    INITIAL USE at:
    [  106.185119]                    _raw_spin_lock+0x33/0x64
    [  106.185838]                    blk_mq_dispatch_rq_list+0x4c1/0xd7c
    [  106.186697]                    blk_mq_do_dispatch_sched+0x23a/0x287
    [  106.187551]                    blk_mq_sched_dispatch_requests+0x379/0x3fc
    [  106.188481]                    __blk_mq_run_hw_queue+0x137/0x17e
    [  106.189307]                    __blk_mq_delay_run_hw_queue+0x80/0x25f
    [  106.190189]                    blk_mq_run_hw_queue+0x151/0x187
    [  106.190989]                    blk_mq_sched_insert_requests+0x13f/0x175
    [  106.191902]                    blk_mq_flush_plug_list+0x7d6/0x81b
    [  106.192739]                    blk_flush_plug_list+0x392/0x3d7
    [  106.193535]                    blk_finish_plug+0x37/0x4f
    [  106.194269]                    __se_sys_io_submit+0x171/0x304
    [  106.195059]                    do_syscall_64+0x140/0x385
    [  106.195794]                    entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  106.196705]  }
    [  106.196950]  ... key      at: [<ffffffff84880620>] __key.51231+0x0/0x40
    [  106.197853]  ... acquired at:
    [  106.198270]    lock_acquire+0x280/0x2f3
    [  106.198806]    _raw_spin_lock+0x33/0x64
    [  106.199337]    sbitmap_get+0xd5/0x22c
    [  106.199850]    __sbitmap_queue_get+0xe8/0x177
    [  106.200450]    __blk_mq_get_tag+0x1e6/0x22d
    [  106.201035]    blk_mq_get_tag+0x1db/0x6e4
    [  106.201589]    blk_mq_get_driver_tag+0x161/0x258
    [  106.202237]    blk_mq_dispatch_rq_list+0x5b9/0xd7c
    [  106.202902]    blk_mq_do_dispatch_sched+0x23a/0x287
    [  106.203572]    blk_mq_sched_dispatch_requests+0x379/0x3fc
    [  106.204316]    __blk_mq_run_hw_queue+0x137/0x17e
    [  106.204956]    __blk_mq_delay_run_hw_queue+0x80/0x25f
    [  106.205649]    blk_mq_run_hw_queue+0x151/0x187
    [  106.206269]    blk_mq_sched_insert_requests+0x13f/0x175
    [  106.206997]    blk_mq_flush_plug_list+0x7d6/0x81b
    [  106.207644]    blk_flush_plug_list+0x392/0x3d7
    [  106.208264]    blk_finish_plug+0x37/0x4f
    [  106.208814]    __se_sys_io_submit+0x171/0x304
    [  106.209415]    do_syscall_64+0x140/0x385
    [  106.209965]    entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  106.210684]
    [  106.210904]
    [  106.210904] the dependencies between the lock to be acquired
    [  106.210905]  and SOFTIRQ-irq-unsafe lock:
    [  106.212541] -> (&(&sb->map[i].swap_lock)->rlock){+.+.} ops: 1969 {
    [  106.213393]    HARDIRQ-ON-W at:
    [  106.213840]                     _raw_spin_lock+0x33/0x64
    [  106.214570]                     sbitmap_get+0xd5/0x22c
    [  106.215282]                     __sbitmap_queue_get+0xe8/0x177
    [  106.216086]                     __blk_mq_get_tag+0x1e6/0x22d
    [  106.216876]                     blk_mq_get_tag+0x1db/0x6e4
    [  106.217627]                     blk_mq_get_driver_tag+0x161/0x258
    [  106.218465]                     blk_mq_dispatch_rq_list+0x28e/0xd7c
    [  106.219326]                     blk_mq_do_dispatch_sched+0x23a/0x287
    [  106.220198]                     blk_mq_sched_dispatch_requests+0x379/0x3fc
    [  106.221138]                     __blk_mq_run_hw_queue+0x137/0x17e
    [  106.221975]                     __blk_mq_delay_run_hw_queue+0x80/0x25f
    [  106.222874]                     blk_mq_run_hw_queue+0x151/0x187
    [  106.223686]                     blk_mq_sched_insert_requests+0x13f/0x175
    [  106.224597]                     blk_mq_flush_plug_list+0x7d6/0x81b
    [  106.225444]                     blk_flush_plug_list+0x392/0x3d7
    [  106.226255]                     blk_finish_plug+0x37/0x4f
    [  106.227006]                     read_pages+0x3ef/0x430
    [  106.227717]                     __do_page_cache_readahead+0x18e/0x2fc
    [  106.228595]                     force_page_cache_readahead+0x121/0x133
    [  106.229491]                     page_cache_sync_readahead+0x35f/0x3bb
    [  106.230373]                     generic_file_buffered_read+0x410/0x1860
    [  106.231277]                     __vfs_read+0x319/0x38f
    [  106.231986]                     vfs_read+0xd2/0x19a
    [  106.232666]                     ksys_read+0xb9/0x135
    [  106.233350]                     do_syscall_64+0x140/0x385
    [  106.234097]                     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  106.235012]    SOFTIRQ-ON-W at:
    [  106.235460]                     _raw_spin_lock+0x33/0x64
    [  106.236195]                     sbitmap_get+0xd5/0x22c
    [  106.236913]                     __sbitmap_queue_get+0xe8/0x177
    [  106.237715]                     __blk_mq_get_tag+0x1e6/0x22d
    [  106.238488]                     blk_mq_get_tag+0x1db/0x6e4
    [  106.239244]                     blk_mq_get_driver_tag+0x161/0x258
    [  106.240079]                     blk_mq_dispatch_rq_list+0x28e/0xd7c
    [  106.240937]                     blk_mq_do_dispatch_sched+0x23a/0x287
    [  106.241806]                     blk_mq_sched_dispatch_requests+0x379/0x3fc
    [  106.242751]                     __blk_mq_run_hw_queue+0x137/0x17e
    [  106.243579]                     __blk_mq_delay_run_hw_queue+0x80/0x25f
    [  106.244469]                     blk_mq_run_hw_queue+0x151/0x187
    [  106.245277]                     blk_mq_sched_insert_requests+0x13f/0x175
    [  106.246191]                     blk_mq_flush_plug_list+0x7d6/0x81b
    [  106.247044]                     blk_flush_plug_list+0x392/0x3d7
    [  106.247859]                     blk_finish_plug+0x37/0x4f
    [  106.248749]                     read_pages+0x3ef/0x430
    [  106.249463]                     __do_page_cache_readahead+0x18e/0x2fc
    [  106.250357]                     force_page_cache_readahead+0x121/0x133
    [  106.251263]                     page_cache_sync_readahead+0x35f/0x3bb
    [  106.252157]                     generic_file_buffered_read+0x410/0x1860
    [  106.253084]                     __vfs_read+0x319/0x38f
    [  106.253808]                     vfs_read+0xd2/0x19a
    [  106.254488]                     ksys_read+0xb9/0x135
    [  106.255186]                     do_syscall_64+0x140/0x385
    [  106.255943]                     entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  106.256867]    INITIAL USE at:
    [  106.257300]                    _raw_spin_lock+0x33/0x64
    [  106.258033]                    sbitmap_get+0xd5/0x22c
    [  106.258747]                    __sbitmap_queue_get+0xe8/0x177
    [  106.259542]                    __blk_mq_get_tag+0x1e6/0x22d
    [  106.260320]                    blk_mq_get_tag+0x1db/0x6e4
    [  106.261072]                    blk_mq_get_driver_tag+0x161/0x258
    [  106.261902]                    blk_mq_dispatch_rq_list+0x28e/0xd7c
    [  106.262762]                    blk_mq_do_dispatch_sched+0x23a/0x287
    [  106.263626]                    blk_mq_sched_dispatch_requests+0x379/0x3fc
    [  106.264571]                    __blk_mq_run_hw_queue+0x137/0x17e
    [  106.265409]                    __blk_mq_delay_run_hw_queue+0x80/0x25f
    [  106.266302]                    blk_mq_run_hw_queue+0x151/0x187
    [  106.267111]                    blk_mq_sched_insert_requests+0x13f/0x175
    [  106.268028]                    blk_mq_flush_plug_list+0x7d6/0x81b
    [  106.268878]                    blk_flush_plug_list+0x392/0x3d7
    [  106.269694]                    blk_finish_plug+0x37/0x4f
    [  106.270432]                    read_pages+0x3ef/0x430
    [  106.271139]                    __do_page_cache_readahead+0x18e/0x2fc
    [  106.272040]                    force_page_cache_readahead+0x121/0x133
    [  106.272932]                    page_cache_sync_readahead+0x35f/0x3bb
    [  106.273811]                    generic_file_buffered_read+0x410/0x1860
    [  106.274709]                    __vfs_read+0x319/0x38f
    [  106.275407]                    vfs_read+0xd2/0x19a
    [  106.276074]                    ksys_read+0xb9/0x135
    [  106.276764]                    do_syscall_64+0x140/0x385
    [  106.277500]                    entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [  106.278417]  }
    [  106.278676]  ... key      at: [<ffffffff85094640>] __key.26212+0x0/0x40
    [  106.279586]  ... acquired at:
    [  106.280026]    lock_acquire+0x280/0x2f3
    [  106.280559]    _raw_spin_lock+0x33/0x64
    [  106.281101]    sbitmap_get+0xd5/0x22c
    [  106.281610]    __sbitmap_queue_get+0xe8/0x177
    [  106.282221]    __blk_mq_get_tag+0x1e6/0x22d
    [  106.282809]    blk_mq_get_tag+0x1db/0x6e4
    [  106.283368]    blk_mq_get_driver_tag+0x161/0x258
    [  106.284018]    blk_mq_dispatch_rq_list+0x5b9/0xd7c
    [  106.284685]    blk_mq_do_dispatch_sched+0x23a/0x287
    [  106.285371]    blk_mq_sched_dispatch_requests+0x379/0x3fc
    [  106.286135]    __blk_mq_run_hw_queue+0x137/0x17e
    [  106.286806]    __blk_mq_delay_run_hw_queue+0x80/0x25f
    [  106.287515]    blk_mq_run_hw_queue+0x151/0x187
    [  106.288149]    blk_mq_sched_insert_requests+0x13f/0x175
    [  106.289041]    blk_mq_flush_plug_list+0x7d6/0x81b
    [  106.289912]    blk_flush_plug_list+0x392/0x3d7
    [  106.290590]    blk_finish_plug+0x37/0x4f
    [  106.291238]    __se_sys_io_submit+0x171/0x304
    [  106.291864]    do_syscall_64+0x140/0x385
    [  106.292534]    entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    Reported-by: Ming Lei <ming.lei@redhat.com>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index a89fbe7cf6ca..2261136ae067 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -118,8 +118,13 @@ static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
 static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
 {
 	unsigned long mask, val;
+	unsigned long __maybe_unused flags;
 	bool ret = false;
 
+	/* Silence bogus lockdep warning */
+#if defined(CONFIG_LOCKDEP)
+	local_irq_save(flags);
+#endif
 	spin_lock(&sb->map[index].swap_lock);
 
 	if (!sb->map[index].cleared)
@@ -142,6 +147,9 @@ static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
 	ret = true;
 out_unlock:
 	spin_unlock(&sb->map[index].swap_lock);
+#if defined(CONFIG_LOCKDEP)
+	local_irq_restore(flags);
+#endif
 	return ret;
 }
 

commit 5d2ee7122c73be6a3b6bfe90d237e8aed737cfaa
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Nov 29 17:36:41 2018 -0700

    sbitmap: optimize wakeup check
    
    Even if we have no waiters on any of the sbitmap_queue wait states, we
    still have to loop every entry to check. We do this for every IO, so
    the cost adds up.
    
    Shift a bit of the cost to the slow path, when we actually have waiters.
    Wrap prepare_to_wait_exclusive() and finish_wait(), so we can maintain
    an internal count of how many are currently active. Then we can simply
    check this count in sbq_wake_ptr() and not have to loop if we don't
    have any sleepers.
    
    Convert the two users of sbitmap with waiting, blk-mq-tag and iSCSI.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index f99382e59314..a89fbe7cf6ca 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -394,6 +394,7 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 	sbq->min_shallow_depth = UINT_MAX;
 	sbq->wake_batch = sbq_calc_wake_batch(sbq, depth);
 	atomic_set(&sbq->wake_index, 0);
+	atomic_set(&sbq->ws_active, 0);
 
 	sbq->ws = kzalloc_node(SBQ_WAIT_QUEUES * sizeof(*sbq->ws), flags, node);
 	if (!sbq->ws) {
@@ -509,6 +510,9 @@ static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
 
+	if (!atomic_read(&sbq->ws_active))
+		return NULL;
+
 	wake_index = atomic_read(&sbq->wake_index);
 	for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
 		struct sbq_wait_state *ws = &sbq->ws[wake_index];
@@ -634,6 +638,7 @@ void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
 
 	seq_printf(m, "wake_batch=%u\n", sbq->wake_batch);
 	seq_printf(m, "wake_index=%d\n", atomic_read(&sbq->wake_index));
+	seq_printf(m, "ws_active=%d\n", atomic_read(&sbq->ws_active));
 
 	seq_puts(m, "ws={\n");
 	for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
@@ -649,3 +654,26 @@ void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
 	seq_printf(m, "min_shallow_depth=%u\n", sbq->min_shallow_depth);
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_show);
+
+void sbitmap_prepare_to_wait(struct sbitmap_queue *sbq,
+			     struct sbq_wait_state *ws,
+			     struct sbq_wait *sbq_wait, int state)
+{
+	if (!sbq_wait->accounted) {
+		atomic_inc(&sbq->ws_active);
+		sbq_wait->accounted = 1;
+	}
+	prepare_to_wait_exclusive(&ws->wait, &sbq_wait->wait, state);
+}
+EXPORT_SYMBOL_GPL(sbitmap_prepare_to_wait);
+
+void sbitmap_finish_wait(struct sbitmap_queue *sbq, struct sbq_wait_state *ws,
+			 struct sbq_wait *sbq_wait)
+{
+	finish_wait(&ws->wait, &sbq_wait->wait);
+	if (sbq_wait->accounted) {
+		atomic_dec(&sbq->ws_active);
+		sbq_wait->accounted = 0;
+	}
+}
+EXPORT_SYMBOL_GPL(sbitmap_finish_wait);

commit ea86ea2cdced20057da4d2c32965c1219c238197
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Nov 30 13:18:06 2018 -0700

    sbitmap: ammortize cost of clearing bits
    
    sbitmap maintains a set of words that we use to set and clear bits, with
    each bit representing a tag for blk-mq. Even though we spread the bits
    out and maintain a hint cache, one particular bit allocated will end up
    being cleared in the exact same spot.
    
    This introduces batched clearing of bits. Instead of clearing a given
    bit, the same bit is set in a cleared/free mask instead. If we fail
    allocating a bit from a given word, then we check the free mask, and
    batch move those cleared bits at that time. This trades 64 atomic bitops
    for 2 cmpxchg().
    
    In a threaded poll test case, half the overhead of getting and clearing
    tags is removed with this change. On another poll test case with a
    single thread, performance is unchanged.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 45cab6bbc1c7..f99382e59314 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -59,6 +59,7 @@ int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
 	for (i = 0; i < sb->map_nr; i++) {
 		sb->map[i].depth = min(depth, bits_per_word);
 		depth -= sb->map[i].depth;
+		spin_lock_init(&sb->map[i].swap_lock);
 	}
 	return 0;
 }
@@ -111,6 +112,57 @@ static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
 	return nr;
 }
 
+/*
+ * See if we have deferred clears that we can batch move
+ */
+static inline bool sbitmap_deferred_clear(struct sbitmap *sb, int index)
+{
+	unsigned long mask, val;
+	bool ret = false;
+
+	spin_lock(&sb->map[index].swap_lock);
+
+	if (!sb->map[index].cleared)
+		goto out_unlock;
+
+	/*
+	 * First get a stable cleared mask, setting the old mask to 0.
+	 */
+	do {
+		mask = sb->map[index].cleared;
+	} while (cmpxchg(&sb->map[index].cleared, mask, 0) != mask);
+
+	/*
+	 * Now clear the masked bits in our free word
+	 */
+	do {
+		val = sb->map[index].word;
+	} while (cmpxchg(&sb->map[index].word, val, val & ~mask) != val);
+
+	ret = true;
+out_unlock:
+	spin_unlock(&sb->map[index].swap_lock);
+	return ret;
+}
+
+static int sbitmap_find_bit_in_index(struct sbitmap *sb, int index,
+				     unsigned int alloc_hint, bool round_robin)
+{
+	int nr;
+
+	do {
+		nr = __sbitmap_get_word(&sb->map[index].word,
+					sb->map[index].depth, alloc_hint,
+					!round_robin);
+		if (nr != -1)
+			break;
+		if (!sbitmap_deferred_clear(sb, index))
+			break;
+	} while (1);
+
+	return nr;
+}
+
 int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 {
 	unsigned int i, index;
@@ -129,9 +181,8 @@ int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 		alloc_hint = 0;
 
 	for (i = 0; i < sb->map_nr; i++) {
-		nr = __sbitmap_get_word(&sb->map[index].word,
-					sb->map[index].depth, alloc_hint,
-					!round_robin);
+		nr = sbitmap_find_bit_in_index(sb, index, alloc_hint,
+						round_robin);
 		if (nr != -1) {
 			nr += index << sb->shift;
 			break;
@@ -206,23 +257,36 @@ bool sbitmap_any_bit_clear(const struct sbitmap *sb)
 }
 EXPORT_SYMBOL_GPL(sbitmap_any_bit_clear);
 
-unsigned int sbitmap_weight(const struct sbitmap *sb)
+static unsigned int __sbitmap_weight(const struct sbitmap *sb, bool set)
 {
 	unsigned int i, weight = 0;
 
 	for (i = 0; i < sb->map_nr; i++) {
 		const struct sbitmap_word *word = &sb->map[i];
 
-		weight += bitmap_weight(&word->word, word->depth);
+		if (set)
+			weight += bitmap_weight(&word->word, word->depth);
+		else
+			weight += bitmap_weight(&word->cleared, word->depth);
 	}
 	return weight;
 }
-EXPORT_SYMBOL_GPL(sbitmap_weight);
+
+static unsigned int sbitmap_weight(const struct sbitmap *sb)
+{
+	return __sbitmap_weight(sb, true);
+}
+
+static unsigned int sbitmap_cleared(const struct sbitmap *sb)
+{
+	return __sbitmap_weight(sb, false);
+}
 
 void sbitmap_show(struct sbitmap *sb, struct seq_file *m)
 {
 	seq_printf(m, "depth=%u\n", sb->depth);
-	seq_printf(m, "busy=%u\n", sbitmap_weight(sb));
+	seq_printf(m, "busy=%u\n", sbitmap_weight(sb) - sbitmap_cleared(sb));
+	seq_printf(m, "cleared=%u\n", sbitmap_cleared(sb));
 	seq_printf(m, "bits_per_word=%u\n", 1U << sb->shift);
 	seq_printf(m, "map_nr=%u\n", sb->map_nr);
 }
@@ -514,7 +578,8 @@ EXPORT_SYMBOL_GPL(sbitmap_queue_wake_up);
 void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 			 unsigned int cpu)
 {
-	sbitmap_clear_bit_unlock(&sbq->sb, nr);
+	sbitmap_deferred_clear_bit(&sbq->sb, nr);
+
 	/*
 	 * Pairs with the memory barrier in set_current_state() to ensure the
 	 * proper ordering of clear_bit_unlock()/waitqueue_active() in the waker

commit 27fae429acee1e9418059e7fa545438075af5256
Author: Jens Axboe <axboe@kernel.dk>
Date:   Thu Nov 29 12:35:16 2018 -0700

    sbitmap: don't loop for find_next_zero_bit() for !round_robin
    
    If we aren't forced to do round robin tag allocation, just use the
    allocation hint to find the index for the tag word, don't use it for the
    offset inside the word. This avoids a potential extra round trip in the
    bit looping, and since we're fetching this cacheline, we may as well
    check the whole word from the start.
    
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index fdd1b8aa8ac6..45cab6bbc1c7 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -118,10 +118,19 @@ int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 
 	index = SB_NR_TO_INDEX(sb, alloc_hint);
 
+	/*
+	 * Unless we're doing round robin tag allocation, just use the
+	 * alloc_hint to find the right word index. No point in looping
+	 * twice in find_next_zero_bit() for that case.
+	 */
+	if (round_robin)
+		alloc_hint = SB_NR_TO_BIT(sb, alloc_hint);
+	else
+		alloc_hint = 0;
+
 	for (i = 0; i < sb->map_nr; i++) {
 		nr = __sbitmap_get_word(&sb->map[index].word,
-					sb->map[index].depth,
-					SB_NR_TO_BIT(sb, alloc_hint),
+					sb->map[index].depth, alloc_hint,
 					!round_robin);
 		if (nr != -1) {
 			nr += index << sb->shift;
@@ -129,13 +138,9 @@ int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 		}
 
 		/* Jump to next index. */
-		index++;
-		alloc_hint = index << sb->shift;
-
-		if (index >= sb->map_nr) {
+		alloc_hint = 0;
+		if (++index >= sb->map_nr)
 			index = 0;
-			alloc_hint = 0;
-		}
 	}
 
 	return nr;

commit 590b5b7d8671e011d1a8e1ab20c60addb249d015
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Jun 12 14:04:20 2018 -0700

    treewide: kzalloc_node() -> kcalloc_node()
    
    The kzalloc_node() function has a 2-factor argument form, kcalloc_node(). This
    patch replaces cases of:
    
            kzalloc_node(a * b, gfp, node)
    
    with:
            kcalloc_node(a * b, gfp, node)
    
    as well as handling cases of:
    
            kzalloc_node(a * b * c, gfp, node)
    
    with:
    
            kzalloc_node(array3_size(a, b, c), gfp, node)
    
    as it's slightly less ugly than:
    
            kcalloc_node(array_size(a, b), c, gfp, node)
    
    This does, however, attempt to ignore constant size factors like:
    
            kzalloc_node(4 * 1024, gfp, node)
    
    though any constants defined via macros get caught up in the conversion.
    
    Any factors with a sizeof() of "unsigned char", "char", and "u8" were
    dropped, since they're redundant.
    
    The Coccinelle script used for this was:
    
    // Fix redundant parens around sizeof().
    @@
    type TYPE;
    expression THING, E;
    @@
    
    (
      kzalloc_node(
    -       (sizeof(TYPE)) * E
    +       sizeof(TYPE) * E
      , ...)
    |
      kzalloc_node(
    -       (sizeof(THING)) * E
    +       sizeof(THING) * E
      , ...)
    )
    
    // Drop single-byte sizes and redundant parens.
    @@
    expression COUNT;
    typedef u8;
    typedef __u8;
    @@
    
    (
      kzalloc_node(
    -       sizeof(u8) * (COUNT)
    +       COUNT
      , ...)
    |
      kzalloc_node(
    -       sizeof(__u8) * (COUNT)
    +       COUNT
      , ...)
    |
      kzalloc_node(
    -       sizeof(char) * (COUNT)
    +       COUNT
      , ...)
    |
      kzalloc_node(
    -       sizeof(unsigned char) * (COUNT)
    +       COUNT
      , ...)
    |
      kzalloc_node(
    -       sizeof(u8) * COUNT
    +       COUNT
      , ...)
    |
      kzalloc_node(
    -       sizeof(__u8) * COUNT
    +       COUNT
      , ...)
    |
      kzalloc_node(
    -       sizeof(char) * COUNT
    +       COUNT
      , ...)
    |
      kzalloc_node(
    -       sizeof(unsigned char) * COUNT
    +       COUNT
      , ...)
    )
    
    // 2-factor product with sizeof(type/expression) and identifier or constant.
    @@
    type TYPE;
    expression THING;
    identifier COUNT_ID;
    constant COUNT_CONST;
    @@
    
    (
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(TYPE) * (COUNT_ID)
    +       COUNT_ID, sizeof(TYPE)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(TYPE) * COUNT_ID
    +       COUNT_ID, sizeof(TYPE)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(TYPE) * (COUNT_CONST)
    +       COUNT_CONST, sizeof(TYPE)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(TYPE) * COUNT_CONST
    +       COUNT_CONST, sizeof(TYPE)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(THING) * (COUNT_ID)
    +       COUNT_ID, sizeof(THING)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(THING) * COUNT_ID
    +       COUNT_ID, sizeof(THING)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(THING) * (COUNT_CONST)
    +       COUNT_CONST, sizeof(THING)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(THING) * COUNT_CONST
    +       COUNT_CONST, sizeof(THING)
      , ...)
    )
    
    // 2-factor product, only identifiers.
    @@
    identifier SIZE, COUNT;
    @@
    
    - kzalloc_node
    + kcalloc_node
      (
    -       SIZE * COUNT
    +       COUNT, SIZE
      , ...)
    
    // 3-factor product with 1 sizeof(type) or sizeof(expression), with
    // redundant parens removed.
    @@
    expression THING;
    identifier STRIDE, COUNT;
    type TYPE;
    @@
    
    (
      kzalloc_node(
    -       sizeof(TYPE) * (COUNT) * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kzalloc_node(
    -       sizeof(TYPE) * (COUNT) * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kzalloc_node(
    -       sizeof(TYPE) * COUNT * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kzalloc_node(
    -       sizeof(TYPE) * COUNT * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kzalloc_node(
    -       sizeof(THING) * (COUNT) * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      kzalloc_node(
    -       sizeof(THING) * (COUNT) * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      kzalloc_node(
    -       sizeof(THING) * COUNT * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      kzalloc_node(
    -       sizeof(THING) * COUNT * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    )
    
    // 3-factor product with 2 sizeof(variable), with redundant parens removed.
    @@
    expression THING1, THING2;
    identifier COUNT;
    type TYPE1, TYPE2;
    @@
    
    (
      kzalloc_node(
    -       sizeof(TYPE1) * sizeof(TYPE2) * COUNT
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
      , ...)
    |
      kzalloc_node(
    -       sizeof(TYPE1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
      , ...)
    |
      kzalloc_node(
    -       sizeof(THING1) * sizeof(THING2) * COUNT
    +       array3_size(COUNT, sizeof(THING1), sizeof(THING2))
      , ...)
    |
      kzalloc_node(
    -       sizeof(THING1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(THING1), sizeof(THING2))
      , ...)
    |
      kzalloc_node(
    -       sizeof(TYPE1) * sizeof(THING2) * COUNT
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
      , ...)
    |
      kzalloc_node(
    -       sizeof(TYPE1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
      , ...)
    )
    
    // 3-factor product, only identifiers, with redundant parens removed.
    @@
    identifier STRIDE, SIZE, COUNT;
    @@
    
    (
      kzalloc_node(
    -       (COUNT) * STRIDE * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc_node(
    -       COUNT * (STRIDE) * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc_node(
    -       COUNT * STRIDE * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc_node(
    -       (COUNT) * (STRIDE) * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc_node(
    -       COUNT * (STRIDE) * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc_node(
    -       (COUNT) * STRIDE * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc_node(
    -       (COUNT) * (STRIDE) * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kzalloc_node(
    -       COUNT * STRIDE * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    )
    
    // Any remaining multi-factor products, first at least 3-factor products,
    // when they're not all constants...
    @@
    expression E1, E2, E3;
    constant C1, C2, C3;
    @@
    
    (
      kzalloc_node(C1 * C2 * C3, ...)
    |
      kzalloc_node(
    -       (E1) * E2 * E3
    +       array3_size(E1, E2, E3)
      , ...)
    |
      kzalloc_node(
    -       (E1) * (E2) * E3
    +       array3_size(E1, E2, E3)
      , ...)
    |
      kzalloc_node(
    -       (E1) * (E2) * (E3)
    +       array3_size(E1, E2, E3)
      , ...)
    |
      kzalloc_node(
    -       E1 * E2 * E3
    +       array3_size(E1, E2, E3)
      , ...)
    )
    
    // And then all remaining 2 factors products when they're not all constants,
    // keeping sizeof() as the second factor argument.
    @@
    expression THING, E1, E2;
    type TYPE;
    constant C1, C2, C3;
    @@
    
    (
      kzalloc_node(sizeof(THING) * C2, ...)
    |
      kzalloc_node(sizeof(TYPE) * C2, ...)
    |
      kzalloc_node(C1 * C2 * C3, ...)
    |
      kzalloc_node(C1 * C2, ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(TYPE) * (E2)
    +       E2, sizeof(TYPE)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(TYPE) * E2
    +       E2, sizeof(TYPE)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(THING) * (E2)
    +       E2, sizeof(THING)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       sizeof(THING) * E2
    +       E2, sizeof(THING)
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       (E1) * E2
    +       E1, E2
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       (E1) * (E2)
    +       E1, E2
      , ...)
    |
    - kzalloc_node
    + kcalloc_node
      (
    -       E1 * E2
    +       E1, E2
      , ...)
    )
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 6fdc6267f4a8..fdd1b8aa8ac6 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -52,7 +52,7 @@ int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
 		return 0;
 	}
 
-	sb->map = kzalloc_node(sb->map_nr * sizeof(*sb->map), flags, node);
+	sb->map = kcalloc_node(sb->map_nr, sizeof(*sb->map), flags, node);
 	if (!sb->map)
 		return -ENOMEM;
 

commit e6fc46498784e799d3eb95d83079180e413c4e7d
Author: Ming Lei <ming.lei@redhat.com>
Date:   Thu May 24 11:00:39 2018 -0600

    blk-mq: avoid starving tag allocation after allocating process migrates
    
    When the allocation process is scheduled back and the mapped hw queue is
    changed, fake one extra wake up on previous queue for compensating wake
    up miss, so other allocations on the previous queue won't be starved.
    
    This patch fixes one request allocation hang issue, which can be
    triggered easily in case of very low nr_request.
    
    The race is as follows:
    
    1) 2 hw queues, nr_requests are 2, and wake_batch is one
    
    2) there are 3 waiters on hw queue 0
    
    3) two in-flight requests in hw queue 0 are completed, and only two
       waiters of 3 are waken up because of wake_batch, but both the two
       waiters can be scheduled to another CPU and cause to switch to hw
       queue 1
    
    4) then the 3rd waiter will wait for ever, since no in-flight request
       is in hw queue 0 any more.
    
    5) this patch fixes it by the fake wakeup when waiter is scheduled to
       another hw queue
    
    Cc: <stable@vger.kernel.org>
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    
    Modified commit message to make it clearer, and make it apply on
    top of the 4.18 branch.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index e6d7d610778d..6fdc6267f4a8 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -352,8 +352,9 @@ static void sbitmap_queue_update_wake_batch(struct sbitmap_queue *sbq,
 	if (sbq->wake_batch != wake_batch) {
 		WRITE_ONCE(sbq->wake_batch, wake_batch);
 		/*
-		 * Pairs with the memory barrier in sbq_wake_up() to ensure that
-		 * the batch size is updated before the wait counts.
+		 * Pairs with the memory barrier in sbitmap_queue_wake_up()
+		 * to ensure that the batch size is updated before the wait
+		 * counts.
 		 */
 		smp_mb__before_atomic();
 		for (i = 0; i < SBQ_WAIT_QUEUES; i++)
@@ -463,15 +464,6 @@ static bool __sbq_wake_up(struct sbitmap_queue *sbq)
 	unsigned int wake_batch;
 	int wait_cnt;
 
-	/*
-	 * Pairs with the memory barrier in set_current_state() to ensure the
-	 * proper ordering of clear_bit()/waitqueue_active() in the waker and
-	 * test_and_set_bit_lock()/prepare_to_wait()/finish_wait() in the
-	 * waiter. See the comment on waitqueue_active(). This is __after_atomic
-	 * because we just did clear_bit_unlock() in the caller.
-	 */
-	smp_mb__after_atomic();
-
 	ws = sbq_wake_ptr(sbq);
 	if (!ws)
 		return false;
@@ -507,17 +499,26 @@ static bool __sbq_wake_up(struct sbitmap_queue *sbq)
 	return false;
 }
 
-static void sbq_wake_up(struct sbitmap_queue *sbq)
+void sbitmap_queue_wake_up(struct sbitmap_queue *sbq)
 {
 	while (__sbq_wake_up(sbq))
 		;
 }
+EXPORT_SYMBOL_GPL(sbitmap_queue_wake_up);
 
 void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 			 unsigned int cpu)
 {
 	sbitmap_clear_bit_unlock(&sbq->sb, nr);
-	sbq_wake_up(sbq);
+	/*
+	 * Pairs with the memory barrier in set_current_state() to ensure the
+	 * proper ordering of clear_bit_unlock()/waitqueue_active() in the waker
+	 * and test_and_set_bit_lock()/prepare_to_wait()/finish_wait() in the
+	 * waiter. See the comment on waitqueue_active().
+	 */
+	smp_mb__after_atomic();
+	sbitmap_queue_wake_up(sbq);
+
 	if (likely(!sbq->round_robin && nr < sbq->sb.depth))
 		*per_cpu_ptr(sbq->alloc_hint, cpu) = nr;
 }
@@ -529,7 +530,7 @@ void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 
 	/*
 	 * Pairs with the memory barrier in set_current_state() like in
-	 * sbq_wake_up().
+	 * sbitmap_queue_wake_up().
 	 */
 	smp_mb();
 	wake_index = atomic_read(&sbq->wake_index);

commit c854ab5773be1c1a0d3cef0c3a3261f2c48ab7f8
Author: Jens Axboe <axboe@kernel.dk>
Date:   Mon May 14 12:17:31 2018 -0600

    sbitmap: fix race in wait batch accounting
    
    If we have multiple callers of sbq_wake_up(), we can end up in a
    situation where the wait_cnt will continually go more and more
    negative. Consider the case where our wake batch is 1, hence
    wait_cnt will start out as 1.
    
    wait_cnt == 1
    
    CPU0                            CPU1
    atomic_dec_return(), cnt == 0
                                    atomic_dec_return(), cnt == -1
                                    cmpxchg(-1, 0) (succeeds)
                                    [wait_cnt now 0]
    cmpxchg(0, 1) (fails)
    
    This ends up with wait_cnt being 0, we'll wakeup immediately
    next time. Going through the same loop as above again, and
    we'll have wait_cnt -1.
    
    For the case where we have a larger wake batch, the only
    difference is that the starting point will be higher. We'll
    still end up with continually smaller batch wakeups, which
    defeats the purpose of the rolling wakeups.
    
    Always reset the wait_cnt to the batch value. Then it doesn't
    matter who wins the race. But ensure that whomever does win
    the race is the one that increments the ws index and wakes up
    our batch count, loser gets to call __sbq_wake_up() again to
    account his wakeups towards the next active wait state index.
    
    Fixes: 6c0ca7ae292a ("sbitmap: fix wakeup hang after sbq resize")
    Reviewed-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 8f0950fbaa5c..e6d7d610778d 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -457,7 +457,7 @@ static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 	return NULL;
 }
 
-static void sbq_wake_up(struct sbitmap_queue *sbq)
+static bool __sbq_wake_up(struct sbitmap_queue *sbq)
 {
 	struct sbq_wait_state *ws;
 	unsigned int wake_batch;
@@ -474,28 +474,43 @@ static void sbq_wake_up(struct sbitmap_queue *sbq)
 
 	ws = sbq_wake_ptr(sbq);
 	if (!ws)
-		return;
+		return false;
 
 	wait_cnt = atomic_dec_return(&ws->wait_cnt);
 	if (wait_cnt <= 0) {
+		int ret;
+
 		wake_batch = READ_ONCE(sbq->wake_batch);
+
 		/*
 		 * Pairs with the memory barrier in sbitmap_queue_resize() to
 		 * ensure that we see the batch size update before the wait
 		 * count is reset.
 		 */
 		smp_mb__before_atomic();
+
 		/*
-		 * If there are concurrent callers to sbq_wake_up(), the last
-		 * one to decrement the wait count below zero will bump it back
-		 * up. If there is a concurrent resize, the count reset will
-		 * either cause the cmpxchg to fail or overwrite after the
-		 * cmpxchg.
+		 * For concurrent callers of this, the one that failed the
+		 * atomic_cmpxhcg() race should call this function again
+		 * to wakeup a new batch on a different 'ws'.
 		 */
-		atomic_cmpxchg(&ws->wait_cnt, wait_cnt, wait_cnt + wake_batch);
-		sbq_index_atomic_inc(&sbq->wake_index);
-		wake_up_nr(&ws->wait, wake_batch);
+		ret = atomic_cmpxchg(&ws->wait_cnt, wait_cnt, wake_batch);
+		if (ret == wait_cnt) {
+			sbq_index_atomic_inc(&sbq->wake_index);
+			wake_up_nr(&ws->wait, wake_batch);
+			return false;
+		}
+
+		return true;
 	}
+
+	return false;
+}
+
+static void sbq_wake_up(struct sbitmap_queue *sbq)
+{
+	while (__sbq_wake_up(sbq))
+		;
 }
 
 void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,

commit 61445b56d031bc12feafb477848cf4ef9a725fc9
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed May 9 17:29:24 2018 -0700

    sbitmap: warn if using smaller shallow depth than was setup
    
    Make sure the user passed the right value to
    sbitmap_queue_min_shallow_depth().
    
    Acked-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index d21473b42465..8f0950fbaa5c 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -402,6 +402,8 @@ int __sbitmap_queue_get_shallow(struct sbitmap_queue *sbq,
 	unsigned int hint, depth;
 	int nr;
 
+	WARN_ON_ONCE(shallow_depth < sbq->min_shallow_depth);
+
 	hint = this_cpu_read(*sbq->alloc_hint);
 	depth = READ_ONCE(sbq->sb.depth);
 	if (unlikely(hint >= depth)) {

commit a327553965dede92587e6ccbe7df98dba36edcea
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed May 9 17:16:31 2018 -0700

    sbitmap: fix missed wakeups caused by sbitmap_queue_get_shallow()
    
    The sbitmap queue wake batch is calculated such that once allocations
    start blocking, all of the bits which are already allocated must be
    enough to fulfill the batch counters of all of the waitqueues. However,
    the shallow allocation depth can break this invariant, since we block
    before our full depth is being utilized. Add
    sbitmap_queue_min_shallow_depth(), which saves the minimum shallow depth
    the sbq will use, and update sbq_calc_wake_batch() to take it into
    account.
    
    Acked-by: Paolo Valente <paolo.valente@linaro.org>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index e6a9c06ec70c..d21473b42465 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -270,18 +270,33 @@ void sbitmap_bitmap_show(struct sbitmap *sb, struct seq_file *m)
 }
 EXPORT_SYMBOL_GPL(sbitmap_bitmap_show);
 
-static unsigned int sbq_calc_wake_batch(unsigned int depth)
+static unsigned int sbq_calc_wake_batch(struct sbitmap_queue *sbq,
+					unsigned int depth)
 {
 	unsigned int wake_batch;
+	unsigned int shallow_depth;
 
 	/*
 	 * For each batch, we wake up one queue. We need to make sure that our
-	 * batch size is small enough that the full depth of the bitmap is
-	 * enough to wake up all of the queues.
+	 * batch size is small enough that the full depth of the bitmap,
+	 * potentially limited by a shallow depth, is enough to wake up all of
+	 * the queues.
+	 *
+	 * Each full word of the bitmap has bits_per_word bits, and there might
+	 * be a partial word. There are depth / bits_per_word full words and
+	 * depth % bits_per_word bits left over. In bitwise arithmetic:
+	 *
+	 * bits_per_word = 1 << shift
+	 * depth / bits_per_word = depth >> shift
+	 * depth % bits_per_word = depth & ((1 << shift) - 1)
+	 *
+	 * Each word can be limited to sbq->min_shallow_depth bits.
 	 */
-	wake_batch = SBQ_WAKE_BATCH;
-	if (wake_batch > depth / SBQ_WAIT_QUEUES)
-		wake_batch = max(1U, depth / SBQ_WAIT_QUEUES);
+	shallow_depth = min(1U << sbq->sb.shift, sbq->min_shallow_depth);
+	depth = ((depth >> sbq->sb.shift) * shallow_depth +
+		 min(depth & ((1U << sbq->sb.shift) - 1), shallow_depth));
+	wake_batch = clamp_t(unsigned int, depth / SBQ_WAIT_QUEUES, 1,
+			     SBQ_WAKE_BATCH);
 
 	return wake_batch;
 }
@@ -307,7 +322,8 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 			*per_cpu_ptr(sbq->alloc_hint, i) = prandom_u32() % depth;
 	}
 
-	sbq->wake_batch = sbq_calc_wake_batch(depth);
+	sbq->min_shallow_depth = UINT_MAX;
+	sbq->wake_batch = sbq_calc_wake_batch(sbq, depth);
 	atomic_set(&sbq->wake_index, 0);
 
 	sbq->ws = kzalloc_node(SBQ_WAIT_QUEUES * sizeof(*sbq->ws), flags, node);
@@ -327,9 +343,10 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_init_node);
 
-void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
+static void sbitmap_queue_update_wake_batch(struct sbitmap_queue *sbq,
+					    unsigned int depth)
 {
-	unsigned int wake_batch = sbq_calc_wake_batch(depth);
+	unsigned int wake_batch = sbq_calc_wake_batch(sbq, depth);
 	int i;
 
 	if (sbq->wake_batch != wake_batch) {
@@ -342,6 +359,11 @@ void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
 		for (i = 0; i < SBQ_WAIT_QUEUES; i++)
 			atomic_set(&sbq->ws[i].wait_cnt, 1);
 	}
+}
+
+void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
+{
+	sbitmap_queue_update_wake_batch(sbq, depth);
 	sbitmap_resize(&sbq->sb, depth);
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
@@ -403,6 +425,14 @@ int __sbitmap_queue_get_shallow(struct sbitmap_queue *sbq,
 }
 EXPORT_SYMBOL_GPL(__sbitmap_queue_get_shallow);
 
+void sbitmap_queue_min_shallow_depth(struct sbitmap_queue *sbq,
+				     unsigned int min_shallow_depth)
+{
+	sbq->min_shallow_depth = min_shallow_depth;
+	sbitmap_queue_update_wake_batch(sbq, sbq->sb.depth);
+}
+EXPORT_SYMBOL_GPL(sbitmap_queue_min_shallow_depth);
+
 static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
@@ -528,5 +558,6 @@ void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
 	seq_puts(m, "}\n");
 
 	seq_printf(m, "round_robin=%d\n", sbq->round_robin);
+	seq_printf(m, "min_shallow_depth=%u\n", sbq->min_shallow_depth);
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_show);

commit 4ace53f1ed40a5cfee4bdd7614c8a8b2798227ad
Author: Omar Sandoval <osandov@fb.com>
Date:   Tue Feb 27 16:56:43 2018 -0800

    sbitmap: use test_and_set_bit_lock()/clear_bit_unlock()
    
    sbitmap_queue_get()/sbitmap_queue_clear() are used for
    allocating/freeing a resource, so they should provide acquire/release
    barrier semantics, respectively. sbitmap_get() currently contains a full
    barrier, which is unnecessary, so use test_and_set_bit_lock() instead of
    test_and_set_bit() (these are equivalent on x86_64). sbitmap_clear_bit()
    does not imply any barriers, which is incorrect, as accesses of the
    resource (e.g., request) could potentially get reordered to after the
    clear_bit(). Introduce sbitmap_clear_bit_unlock() and use it for
    sbitmap_queue_clear() (this only adds a compiler barrier on x86_64). The
    other existing user of sbitmap_clear_bit() (the blk-mq software queue
    pending map) is serialized through a spinlock and does not need this.
    
    Reported-by: Tejun Heo <tj@kernel.org>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 42b5ca0acf93..e6a9c06ec70c 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -100,7 +100,7 @@ static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
 			return -1;
 		}
 
-		if (!test_and_set_bit(nr, word))
+		if (!test_and_set_bit_lock(nr, word))
 			break;
 
 		hint = nr + 1;
@@ -434,9 +434,9 @@ static void sbq_wake_up(struct sbitmap_queue *sbq)
 	/*
 	 * Pairs with the memory barrier in set_current_state() to ensure the
 	 * proper ordering of clear_bit()/waitqueue_active() in the waker and
-	 * test_and_set_bit()/prepare_to_wait()/finish_wait() in the waiter. See
-	 * the comment on waitqueue_active(). This is __after_atomic because we
-	 * just did clear_bit() in the caller.
+	 * test_and_set_bit_lock()/prepare_to_wait()/finish_wait() in the
+	 * waiter. See the comment on waitqueue_active(). This is __after_atomic
+	 * because we just did clear_bit_unlock() in the caller.
 	 */
 	smp_mb__after_atomic();
 
@@ -469,7 +469,7 @@ static void sbq_wake_up(struct sbitmap_queue *sbq)
 void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 			 unsigned int cpu)
 {
-	sbitmap_clear_bit(&sbq->sb, nr);
+	sbitmap_clear_bit_unlock(&sbq->sb, nr);
 	sbq_wake_up(sbq);
 	if (likely(!sbq->round_robin && nr < sbq->sb.depth))
 		*per_cpu_ptr(sbq->alloc_hint, cpu) = nr;

commit 4e5dff41be7b5201c1c47ceb3a2a8d698516bc2b
Author: Jens Axboe <axboe@kernel.dk>
Date:   Tue Nov 14 10:24:58 2017 -0700

    blk-mq: improve heavily contended tag case
    
    Even with a number of waitqueues, we can get into a situation where we
    are heavily contended on the waitqueue lock. I got a report on spc1
    where we're spending seconds doing this. Arguably the use case is nasty,
    I reproduce it with one device and 1000 threads banging on the device.
    But that doesn't mean we shouldn't be handling it better.
    
    What ends up happening is that a thread will fail to get a tag, add
    itself to the waitqueue, and subsequently get woken up when a tag is
    freed - only to find itself going back to sleep on the waitqueue.
    
    Instead of waking all threads, use an exclusive wait and wake up our
    sbitmap batch count instead. This seems to work well for me (massive
    improvement for this use case), and it survives basic testing. But I
    haven't fully verified it yet.
    
    An additional improvement is running the queue and checking for a new
    tag BEFORE needing to add ourselves to the waitqueue.
    
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 80aa8d5463fa..42b5ca0acf93 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -462,7 +462,7 @@ static void sbq_wake_up(struct sbitmap_queue *sbq)
 		 */
 		atomic_cmpxchg(&ws->wait_cnt, wait_cnt, wait_cnt + wake_batch);
 		sbq_index_atomic_inc(&sbq->wake_index);
-		wake_up(&ws->wait);
+		wake_up_nr(&ws->wait, wake_batch);
 	}
 }
 

commit c05e66733788118377c21a913c1bc7b64bccc167
Author: Omar Sandoval <osandov@fb.com>
Date:   Fri Apr 14 00:59:58 2017 -0700

    sbitmap: add sbitmap_get_shallow() operation
    
    This operation supports the use case of limiting the number of bits that
    can be allocated for a given operation. Rather than setting aside some
    bits at the end of the bitmap, we can set aside bits in each word of the
    bitmap. This means we can keep the allocation hints spread out and
    support sbitmap_resize() nicely at the cost of lower granularity for the
    allowed depth.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 60e800e0b5a0..80aa8d5463fa 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -79,15 +79,15 @@ void sbitmap_resize(struct sbitmap *sb, unsigned int depth)
 }
 EXPORT_SYMBOL_GPL(sbitmap_resize);
 
-static int __sbitmap_get_word(struct sbitmap_word *word, unsigned int hint,
-			      bool wrap)
+static int __sbitmap_get_word(unsigned long *word, unsigned long depth,
+			      unsigned int hint, bool wrap)
 {
 	unsigned int orig_hint = hint;
 	int nr;
 
 	while (1) {
-		nr = find_next_zero_bit(&word->word, word->depth, hint);
-		if (unlikely(nr >= word->depth)) {
+		nr = find_next_zero_bit(word, depth, hint);
+		if (unlikely(nr >= depth)) {
 			/*
 			 * We started with an offset, and we didn't reset the
 			 * offset to 0 in a failure case, so start from 0 to
@@ -100,11 +100,11 @@ static int __sbitmap_get_word(struct sbitmap_word *word, unsigned int hint,
 			return -1;
 		}
 
-		if (!test_and_set_bit(nr, &word->word))
+		if (!test_and_set_bit(nr, word))
 			break;
 
 		hint = nr + 1;
-		if (hint >= word->depth - 1)
+		if (hint >= depth - 1)
 			hint = 0;
 	}
 
@@ -119,7 +119,8 @@ int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 	index = SB_NR_TO_INDEX(sb, alloc_hint);
 
 	for (i = 0; i < sb->map_nr; i++) {
-		nr = __sbitmap_get_word(&sb->map[index],
+		nr = __sbitmap_get_word(&sb->map[index].word,
+					sb->map[index].depth,
 					SB_NR_TO_BIT(sb, alloc_hint),
 					!round_robin);
 		if (nr != -1) {
@@ -141,6 +142,37 @@ int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
 }
 EXPORT_SYMBOL_GPL(sbitmap_get);
 
+int sbitmap_get_shallow(struct sbitmap *sb, unsigned int alloc_hint,
+			unsigned long shallow_depth)
+{
+	unsigned int i, index;
+	int nr = -1;
+
+	index = SB_NR_TO_INDEX(sb, alloc_hint);
+
+	for (i = 0; i < sb->map_nr; i++) {
+		nr = __sbitmap_get_word(&sb->map[index].word,
+					min(sb->map[index].depth, shallow_depth),
+					SB_NR_TO_BIT(sb, alloc_hint), true);
+		if (nr != -1) {
+			nr += index << sb->shift;
+			break;
+		}
+
+		/* Jump to next index. */
+		index++;
+		alloc_hint = index << sb->shift;
+
+		if (index >= sb->map_nr) {
+			index = 0;
+			alloc_hint = 0;
+		}
+	}
+
+	return nr;
+}
+EXPORT_SYMBOL_GPL(sbitmap_get_shallow);
+
 bool sbitmap_any_bit_set(const struct sbitmap *sb)
 {
 	unsigned int i;
@@ -342,6 +374,35 @@ int __sbitmap_queue_get(struct sbitmap_queue *sbq)
 }
 EXPORT_SYMBOL_GPL(__sbitmap_queue_get);
 
+int __sbitmap_queue_get_shallow(struct sbitmap_queue *sbq,
+				unsigned int shallow_depth)
+{
+	unsigned int hint, depth;
+	int nr;
+
+	hint = this_cpu_read(*sbq->alloc_hint);
+	depth = READ_ONCE(sbq->sb.depth);
+	if (unlikely(hint >= depth)) {
+		hint = depth ? prandom_u32() % depth : 0;
+		this_cpu_write(*sbq->alloc_hint, hint);
+	}
+	nr = sbitmap_get_shallow(&sbq->sb, hint, shallow_depth);
+
+	if (nr == -1) {
+		/* If the map is full, a hint won't do us much good. */
+		this_cpu_write(*sbq->alloc_hint, 0);
+	} else if (nr == hint || unlikely(sbq->round_robin)) {
+		/* Only update the hint if we used it. */
+		hint = nr + 1;
+		if (hint >= depth - 1)
+			hint = 0;
+		this_cpu_write(*sbq->alloc_hint, hint);
+	}
+
+	return nr;
+}
+EXPORT_SYMBOL_GPL(__sbitmap_queue_get_shallow);
+
 static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;

commit af8601ad420f6afa6445c927ad9f36d9700d96d6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 09:57:00 2017 +0100

    kasan, sched/headers: Uninline kasan_enable/disable_current()
    
    <linux/kasan.h> is a low level header that is included early
    in affected kernel headers. But it includes <linux/sched.h>
    which complicates the cleanup of sched.h dependencies.
    
    But kasan.h has almost no need for sched.h: its only use of
    scheduler functionality is in two inline functions which are
    not used very frequently - so uninline kasan_enable_current()
    and kasan_disable_current().
    
    Also add a <linux/sched.h> dependency to a .c file that depended
    on kasan.h including it.
    
    This paves the way to remove the <linux/sched.h> include from kasan.h.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 55e11c4b2f3b..60e800e0b5a0 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -15,6 +15,7 @@
  * along with this program.  If not, see <https://www.gnu.org/licenses/>.
  */
 
+#include <linux/sched.h>
 #include <linux/random.h>
 #include <linux/sbitmap.h>
 #include <linux/seq_file.h>

commit 24af1ccfe12adddbe17d11801e1689791a4cc282
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed Jan 25 14:32:13 2017 -0800

    sbitmap: add helpers for dumping to a seq_file
    
    This is useful debugging information that will be used in the blk-mq
    debugfs directory.
    
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    
    Changed 'weight' to 'busy'.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 8f5c3b268c77..55e11c4b2f3b 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -17,6 +17,7 @@
 
 #include <linux/random.h>
 #include <linux/sbitmap.h>
+#include <linux/seq_file.h>
 
 int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
 		      gfp_t flags, int node)
@@ -180,6 +181,62 @@ unsigned int sbitmap_weight(const struct sbitmap *sb)
 }
 EXPORT_SYMBOL_GPL(sbitmap_weight);
 
+void sbitmap_show(struct sbitmap *sb, struct seq_file *m)
+{
+	seq_printf(m, "depth=%u\n", sb->depth);
+	seq_printf(m, "busy=%u\n", sbitmap_weight(sb));
+	seq_printf(m, "bits_per_word=%u\n", 1U << sb->shift);
+	seq_printf(m, "map_nr=%u\n", sb->map_nr);
+}
+EXPORT_SYMBOL_GPL(sbitmap_show);
+
+static inline void emit_byte(struct seq_file *m, unsigned int offset, u8 byte)
+{
+	if ((offset & 0xf) == 0) {
+		if (offset != 0)
+			seq_putc(m, '\n');
+		seq_printf(m, "%08x:", offset);
+	}
+	if ((offset & 0x1) == 0)
+		seq_putc(m, ' ');
+	seq_printf(m, "%02x", byte);
+}
+
+void sbitmap_bitmap_show(struct sbitmap *sb, struct seq_file *m)
+{
+	u8 byte = 0;
+	unsigned int byte_bits = 0;
+	unsigned int offset = 0;
+	int i;
+
+	for (i = 0; i < sb->map_nr; i++) {
+		unsigned long word = READ_ONCE(sb->map[i].word);
+		unsigned int word_bits = READ_ONCE(sb->map[i].depth);
+
+		while (word_bits > 0) {
+			unsigned int bits = min(8 - byte_bits, word_bits);
+
+			byte |= (word & (BIT(bits) - 1)) << byte_bits;
+			byte_bits += bits;
+			if (byte_bits == 8) {
+				emit_byte(m, offset, byte);
+				byte = 0;
+				byte_bits = 0;
+				offset++;
+			}
+			word >>= bits;
+			word_bits -= bits;
+		}
+	}
+	if (byte_bits) {
+		emit_byte(m, offset, byte);
+		offset++;
+	}
+	if (offset)
+		seq_putc(m, '\n');
+}
+EXPORT_SYMBOL_GPL(sbitmap_bitmap_show);
+
 static unsigned int sbq_calc_wake_batch(unsigned int depth)
 {
 	unsigned int wake_batch;
@@ -377,3 +434,37 @@ void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 	}
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_wake_all);
+
+void sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m)
+{
+	bool first;
+	int i;
+
+	sbitmap_show(&sbq->sb, m);
+
+	seq_puts(m, "alloc_hint={");
+	first = true;
+	for_each_possible_cpu(i) {
+		if (!first)
+			seq_puts(m, ", ");
+		first = false;
+		seq_printf(m, "%u", *per_cpu_ptr(sbq->alloc_hint, i));
+	}
+	seq_puts(m, "}\n");
+
+	seq_printf(m, "wake_batch=%u\n", sbq->wake_batch);
+	seq_printf(m, "wake_index=%d\n", atomic_read(&sbq->wake_index));
+
+	seq_puts(m, "ws={\n");
+	for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
+		struct sbq_wait_state *ws = &sbq->ws[i];
+
+		seq_printf(m, "\t{.wait_cnt=%d, .wait=%s},\n",
+			   atomic_read(&ws->wait_cnt),
+			   waitqueue_active(&ws->wait) ? "active" : "inactive");
+	}
+	seq_puts(m, "}\n");
+
+	seq_printf(m, "round_robin=%d\n", sbq->round_robin);
+}
+EXPORT_SYMBOL_GPL(sbitmap_queue_show);

commit 6c0ca7ae292adea09b8bdd33a524bb9326c3e989
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed Jan 18 11:55:22 2017 -0800

    sbitmap: fix wakeup hang after sbq resize
    
    When we resize a struct sbitmap_queue, we update the wakeup batch size,
    but we don't update the wait count in the struct sbq_wait_states. If we
    resized down from a size which could use a bigger batch size, these
    counts could be too large and cause us to miss necessary wakeups. To fix
    this, update the wait counts when we resize (ensuring some careful
    memory ordering so that it's safe w.r.t. concurrent clears).
    
    This also fixes a theoretical issue where two threads could end up
    bumping the wait count up by the batch size, which could also
    potentially lead to hangs.
    
    Reported-by: Martin Raiber <martin@urbackup.org>
    Fixes: e3a2b3f931f5 ("blk-mq: allow changing of queue depth through sysfs")
    Fixes: 2971c35f3588 ("blk-mq: bitmap tag: fix race on blk_mq_bitmap_tags::wake_cnt")
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index df4e472df8a3..8f5c3b268c77 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -239,7 +239,19 @@ EXPORT_SYMBOL_GPL(sbitmap_queue_init_node);
 
 void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
 {
-	sbq->wake_batch = sbq_calc_wake_batch(depth);
+	unsigned int wake_batch = sbq_calc_wake_batch(depth);
+	int i;
+
+	if (sbq->wake_batch != wake_batch) {
+		WRITE_ONCE(sbq->wake_batch, wake_batch);
+		/*
+		 * Pairs with the memory barrier in sbq_wake_up() to ensure that
+		 * the batch size is updated before the wait counts.
+		 */
+		smp_mb__before_atomic();
+		for (i = 0; i < SBQ_WAIT_QUEUES; i++)
+			atomic_set(&sbq->ws[i].wait_cnt, 1);
+	}
 	sbitmap_resize(&sbq->sb, depth);
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
@@ -297,6 +309,7 @@ static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 static void sbq_wake_up(struct sbitmap_queue *sbq)
 {
 	struct sbq_wait_state *ws;
+	unsigned int wake_batch;
 	int wait_cnt;
 
 	/*
@@ -313,10 +326,22 @@ static void sbq_wake_up(struct sbitmap_queue *sbq)
 		return;
 
 	wait_cnt = atomic_dec_return(&ws->wait_cnt);
-	if (unlikely(wait_cnt < 0))
-		wait_cnt = atomic_inc_return(&ws->wait_cnt);
-	if (wait_cnt == 0) {
-		atomic_add(sbq->wake_batch, &ws->wait_cnt);
+	if (wait_cnt <= 0) {
+		wake_batch = READ_ONCE(sbq->wake_batch);
+		/*
+		 * Pairs with the memory barrier in sbitmap_queue_resize() to
+		 * ensure that we see the batch size update before the wait
+		 * count is reset.
+		 */
+		smp_mb__before_atomic();
+		/*
+		 * If there are concurrent callers to sbq_wake_up(), the last
+		 * one to decrement the wait count below zero will bump it back
+		 * up. If there is a concurrent resize, the count reset will
+		 * either cause the cmpxchg to fail or overwrite after the
+		 * cmpxchg.
+		 */
+		atomic_cmpxchg(&ws->wait_cnt, wait_cnt, wait_cnt + wake_batch);
 		sbq_index_atomic_inc(&sbq->wake_index);
 		wake_up(&ws->wait);
 	}

commit f66227de5924ed0fde1823f5cbc4d8b8f45faaa2
Author: Omar Sandoval <osandov@fb.com>
Date:   Wed Jan 18 11:55:21 2017 -0800

    sbitmap: use smp_mb__after_atomic() in sbq_wake_up()
    
    We always do an atomic clear_bit() right before we call sbq_wake_up(),
    so we can use smp_mb__after_atomic(). While we're here, comment the
    memory barriers in here a little more.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 2cecf05c82fd..df4e472df8a3 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -299,8 +299,14 @@ static void sbq_wake_up(struct sbitmap_queue *sbq)
 	struct sbq_wait_state *ws;
 	int wait_cnt;
 
-	/* Ensure that the wait list checks occur after clear_bit(). */
-	smp_mb();
+	/*
+	 * Pairs with the memory barrier in set_current_state() to ensure the
+	 * proper ordering of clear_bit()/waitqueue_active() in the waker and
+	 * test_and_set_bit()/prepare_to_wait()/finish_wait() in the waiter. See
+	 * the comment on waitqueue_active(). This is __after_atomic because we
+	 * just did clear_bit() in the caller.
+	 */
+	smp_mb__after_atomic();
 
 	ws = sbq_wake_ptr(sbq);
 	if (!ws)
@@ -331,7 +337,8 @@ void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
 	int i, wake_index;
 
 	/*
-	 * Make sure all changes prior to this are visible from other CPUs.
+	 * Pairs with the memory barrier in set_current_state() like in
+	 * sbq_wake_up().
 	 */
 	smp_mb();
 	wake_index = atomic_read(&sbq->wake_index);

commit 60658e0dc1df058607990278fdf9d831e0c2c71a
Author: Colin Ian King <colin.king@canonical.com>
Date:   Mon Sep 19 14:34:08 2016 +0100

    sbitmap: initialize weight to zero
    
    Variable weight is not being initialized to zero before it is
    used to compute the weight sum. Ensure it is initialized to zero.
    
    Found with static analysis with cppcheck:
    [lib/sbitmap.c:177]: (error) Uninitialized variable: weight
    
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index e40808921544..2cecf05c82fd 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -169,7 +169,7 @@ EXPORT_SYMBOL_GPL(sbitmap_any_bit_clear);
 
 unsigned int sbitmap_weight(const struct sbitmap *sb)
 {
-	unsigned int i, weight;
+	unsigned int i, weight = 0;
 
 	for (i = 0; i < sb->map_nr; i++) {
 		const struct sbitmap_word *word = &sb->map[i];

commit 5c64a8df0ca88c79c9cb74674c2481e5f7ede511
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 12:20:54 2016 -0700

    sbitmap: don't update the allocation hint on clear after resize
    
    If we have a bunch of high-numbered bits allocated and then we resize
    the struct sbitmap_queue, when those bits get cleared, we'll update the
    hint and then have to re-randomize it repeatedly. Avoid that by checking
    that the cleared bit is still a valid hint. No measurable performance
    difference in the common case.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index f736c52a712c..e40808921544 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -321,7 +321,7 @@ void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
 {
 	sbitmap_clear_bit(&sbq->sb, nr);
 	sbq_wake_up(sbq);
-	if (likely(!sbq->round_robin))
+	if (likely(!sbq->round_robin && nr < sbq->sb.depth))
 		*per_cpu_ptr(sbq->alloc_hint, cpu) = nr;
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);

commit 05fd095d53b979878f016c3a7080d3683cc89d72
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 01:28:26 2016 -0700

    sbitmap: re-initialize allocation hints after resize
    
    After a struct sbitmap_queue is resized smaller, the allocation hints
    may still be set to bits beyond the new depth of the bitmap. This means
    that, for example, if the number of blk-mq tags is reduced through
    sysfs, more requests than the nominal queue depth may be in flight.
    
    It's tempting to fix this at resize time by doing a one-time
    reinitialization of the hints, but this can race with
    __sbitmap_queue_get() updating the hint. Instead, check the hint before
    we use it. This caused no measurable performance difference in my
    synthetic benchmarks.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 928b82a733f2..f736c52a712c 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -246,10 +246,15 @@ EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
 
 int __sbitmap_queue_get(struct sbitmap_queue *sbq)
 {
-	unsigned int hint;
+	unsigned int hint, depth;
 	int nr;
 
 	hint = this_cpu_read(*sbq->alloc_hint);
+	depth = READ_ONCE(sbq->sb.depth);
+	if (unlikely(hint >= depth)) {
+		hint = depth ? prandom_u32() % depth : 0;
+		this_cpu_write(*sbq->alloc_hint, hint);
+	}
 	nr = sbitmap_get(&sbq->sb, hint, sbq->round_robin);
 
 	if (nr == -1) {
@@ -258,7 +263,7 @@ int __sbitmap_queue_get(struct sbitmap_queue *sbq)
 	} else if (nr == hint || unlikely(sbq->round_robin)) {
 		/* Only update the hint if we used it. */
 		hint = nr + 1;
-		if (hint >= sbq->sb.depth - 1)
+		if (hint >= depth - 1)
 			hint = 0;
 		this_cpu_write(*sbq->alloc_hint, hint);
 	}

commit 98d95416dbfaf4910caadfb4ddc75e4aacbdff8c
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 01:28:25 2016 -0700

    sbitmap: randomize initial alloc_hint values
    
    In order to get good cache behavior from a sbitmap, we want each CPU to
    stick to its own cacheline(s) as much as possible. This might happen
    naturally as the bitmap gets filled up and the alloc_hint values spread
    out, but we really want this behavior from the start. blk-mq apparently
    intended to do this, but the code to do this was never wired up. Get rid
    of the dead code and make it part of the sbitmap library.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index be55f744b771..928b82a733f2 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -15,6 +15,7 @@
  * along with this program.  If not, see <https://www.gnu.org/licenses/>.
  */
 
+#include <linux/random.h>
 #include <linux/sbitmap.h>
 
 int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
@@ -211,6 +212,11 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 		return -ENOMEM;
 	}
 
+	if (depth && !round_robin) {
+		for_each_possible_cpu(i)
+			*per_cpu_ptr(sbq->alloc_hint, i) = prandom_u32() % depth;
+	}
+
 	sbq->wake_batch = sbq_calc_wake_batch(depth);
 	atomic_set(&sbq->wake_index, 0);
 

commit f4a644db86669d938c71f19560aebf69d4720d63
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 01:28:24 2016 -0700

    sbitmap: push alloc policy into sbitmap_queue
    
    Again, there's no point in passing this in every time. Make it part of
    struct sbitmap_queue and clean up the API.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 1651ad9d5530..be55f744b771 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -196,7 +196,7 @@ static unsigned int sbq_calc_wake_batch(unsigned int depth)
 }
 
 int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
-			    int shift, gfp_t flags, int node)
+			    int shift, bool round_robin, gfp_t flags, int node)
 {
 	int ret;
 	int i;
@@ -225,6 +225,8 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 		init_waitqueue_head(&sbq->ws[i].wait);
 		atomic_set(&sbq->ws[i].wait_cnt, sbq->wake_batch);
 	}
+
+	sbq->round_robin = round_robin;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_init_node);
@@ -236,18 +238,18 @@ void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
 
-int __sbitmap_queue_get(struct sbitmap_queue *sbq, bool round_robin)
+int __sbitmap_queue_get(struct sbitmap_queue *sbq)
 {
 	unsigned int hint;
 	int nr;
 
 	hint = this_cpu_read(*sbq->alloc_hint);
-	nr = sbitmap_get(&sbq->sb, hint, round_robin);
+	nr = sbitmap_get(&sbq->sb, hint, sbq->round_robin);
 
 	if (nr == -1) {
 		/* If the map is full, a hint won't do us much good. */
 		this_cpu_write(*sbq->alloc_hint, 0);
-	} else if (nr == hint || unlikely(round_robin)) {
+	} else if (nr == hint || unlikely(sbq->round_robin)) {
 		/* Only update the hint if we used it. */
 		hint = nr + 1;
 		if (hint >= sbq->sb.depth - 1)
@@ -304,11 +306,11 @@ static void sbq_wake_up(struct sbitmap_queue *sbq)
 }
 
 void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
-			 bool round_robin, unsigned int cpu)
+			 unsigned int cpu)
 {
 	sbitmap_clear_bit(&sbq->sb, nr);
 	sbq_wake_up(sbq);
-	if (likely(!round_robin))
+	if (likely(!sbq->round_robin))
 		*per_cpu_ptr(sbq->alloc_hint, cpu) = nr;
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);

commit 40aabb67464d5aad9ca3d2a5fedee56e2ff45aa0
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 01:28:23 2016 -0700

    sbitmap: push per-cpu last_tag into sbitmap_queue
    
    Allocating your own per-cpu allocation hint separately makes for an
    awkward API. Instead, allocate the per-cpu hint as part of the struct
    sbitmap_queue. There's no point for a struct sbitmap_queue without the
    cache, but you can still use a bare struct sbitmap.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index 4d8e97e470ee..1651ad9d5530 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -205,11 +205,18 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 	if (ret)
 		return ret;
 
+	sbq->alloc_hint = alloc_percpu_gfp(unsigned int, flags);
+	if (!sbq->alloc_hint) {
+		sbitmap_free(&sbq->sb);
+		return -ENOMEM;
+	}
+
 	sbq->wake_batch = sbq_calc_wake_batch(depth);
 	atomic_set(&sbq->wake_index, 0);
 
 	sbq->ws = kzalloc_node(SBQ_WAIT_QUEUES * sizeof(*sbq->ws), flags, node);
 	if (!sbq->ws) {
+		free_percpu(sbq->alloc_hint);
 		sbitmap_free(&sbq->sb);
 		return -ENOMEM;
 	}
@@ -229,6 +236,29 @@ void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
 
+int __sbitmap_queue_get(struct sbitmap_queue *sbq, bool round_robin)
+{
+	unsigned int hint;
+	int nr;
+
+	hint = this_cpu_read(*sbq->alloc_hint);
+	nr = sbitmap_get(&sbq->sb, hint, round_robin);
+
+	if (nr == -1) {
+		/* If the map is full, a hint won't do us much good. */
+		this_cpu_write(*sbq->alloc_hint, 0);
+	} else if (nr == hint || unlikely(round_robin)) {
+		/* Only update the hint if we used it. */
+		hint = nr + 1;
+		if (hint >= sbq->sb.depth - 1)
+			hint = 0;
+		this_cpu_write(*sbq->alloc_hint, hint);
+	}
+
+	return nr;
+}
+EXPORT_SYMBOL_GPL(__sbitmap_queue_get);
+
 static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
 {
 	int i, wake_index;
@@ -273,10 +303,13 @@ static void sbq_wake_up(struct sbitmap_queue *sbq)
 	}
 }
 
-void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr)
+void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,
+			 bool round_robin, unsigned int cpu)
 {
 	sbitmap_clear_bit(&sbq->sb, nr);
 	sbq_wake_up(sbq);
+	if (likely(!round_robin))
+		*per_cpu_ptr(sbq->alloc_hint, cpu) = nr;
 }
 EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
 

commit 48e28166a7b608e19a6aea3acadd81cdfe660f6b
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 01:28:22 2016 -0700

    sbitmap: allocate wait queues on a specific node
    
    The original bt_alloc() we converted from was using kzalloc(), not
    kzalloc_node(), to allocate the wait queues. This was probably an
    oversight, so fix it for sbitmap_queue_init_node().
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
index dfc084ac6937..4d8e97e470ee 100644
--- a/lib/sbitmap.c
+++ b/lib/sbitmap.c
@@ -208,7 +208,7 @@ int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
 	sbq->wake_batch = sbq_calc_wake_batch(depth);
 	atomic_set(&sbq->wake_index, 0);
 
-	sbq->ws = kzalloc(SBQ_WAIT_QUEUES * sizeof(*sbq->ws), flags);
+	sbq->ws = kzalloc_node(SBQ_WAIT_QUEUES * sizeof(*sbq->ws), flags, node);
 	if (!sbq->ws) {
 		sbitmap_free(&sbq->sb);
 		return -ENOMEM;

commit 88459642cba452630326b9cab1c651e09577d4e4
Author: Omar Sandoval <osandov@fb.com>
Date:   Sat Sep 17 08:38:44 2016 -0600

    blk-mq: abstract tag allocation out into sbitmap library
    
    This is a generally useful data structure, so make it available to
    anyone else who might want to use it. It's also a nice cleanup
    separating the allocation logic from the rest of the tag handling logic.
    
    The code is behind a new Kconfig option, CONFIG_SBITMAP, which is only
    selected by CONFIG_BLOCK for now.
    
    This should be a complete noop functionality-wise.
    
    Signed-off-by: Omar Sandoval <osandov@fb.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/sbitmap.c b/lib/sbitmap.c
new file mode 100644
index 000000000000..dfc084ac6937
--- /dev/null
+++ b/lib/sbitmap.c
@@ -0,0 +1,301 @@
+/*
+ * Copyright (C) 2016 Facebook
+ * Copyright (C) 2013-2014 Jens Axboe
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License v2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <https://www.gnu.org/licenses/>.
+ */
+
+#include <linux/sbitmap.h>
+
+int sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,
+		      gfp_t flags, int node)
+{
+	unsigned int bits_per_word;
+	unsigned int i;
+
+	if (shift < 0) {
+		shift = ilog2(BITS_PER_LONG);
+		/*
+		 * If the bitmap is small, shrink the number of bits per word so
+		 * we spread over a few cachelines, at least. If less than 4
+		 * bits, just forget about it, it's not going to work optimally
+		 * anyway.
+		 */
+		if (depth >= 4) {
+			while ((4U << shift) > depth)
+				shift--;
+		}
+	}
+	bits_per_word = 1U << shift;
+	if (bits_per_word > BITS_PER_LONG)
+		return -EINVAL;
+
+	sb->shift = shift;
+	sb->depth = depth;
+	sb->map_nr = DIV_ROUND_UP(sb->depth, bits_per_word);
+
+	if (depth == 0) {
+		sb->map = NULL;
+		return 0;
+	}
+
+	sb->map = kzalloc_node(sb->map_nr * sizeof(*sb->map), flags, node);
+	if (!sb->map)
+		return -ENOMEM;
+
+	for (i = 0; i < sb->map_nr; i++) {
+		sb->map[i].depth = min(depth, bits_per_word);
+		depth -= sb->map[i].depth;
+	}
+	return 0;
+}
+EXPORT_SYMBOL_GPL(sbitmap_init_node);
+
+void sbitmap_resize(struct sbitmap *sb, unsigned int depth)
+{
+	unsigned int bits_per_word = 1U << sb->shift;
+	unsigned int i;
+
+	sb->depth = depth;
+	sb->map_nr = DIV_ROUND_UP(sb->depth, bits_per_word);
+
+	for (i = 0; i < sb->map_nr; i++) {
+		sb->map[i].depth = min(depth, bits_per_word);
+		depth -= sb->map[i].depth;
+	}
+}
+EXPORT_SYMBOL_GPL(sbitmap_resize);
+
+static int __sbitmap_get_word(struct sbitmap_word *word, unsigned int hint,
+			      bool wrap)
+{
+	unsigned int orig_hint = hint;
+	int nr;
+
+	while (1) {
+		nr = find_next_zero_bit(&word->word, word->depth, hint);
+		if (unlikely(nr >= word->depth)) {
+			/*
+			 * We started with an offset, and we didn't reset the
+			 * offset to 0 in a failure case, so start from 0 to
+			 * exhaust the map.
+			 */
+			if (orig_hint && hint && wrap) {
+				hint = orig_hint = 0;
+				continue;
+			}
+			return -1;
+		}
+
+		if (!test_and_set_bit(nr, &word->word))
+			break;
+
+		hint = nr + 1;
+		if (hint >= word->depth - 1)
+			hint = 0;
+	}
+
+	return nr;
+}
+
+int sbitmap_get(struct sbitmap *sb, unsigned int alloc_hint, bool round_robin)
+{
+	unsigned int i, index;
+	int nr = -1;
+
+	index = SB_NR_TO_INDEX(sb, alloc_hint);
+
+	for (i = 0; i < sb->map_nr; i++) {
+		nr = __sbitmap_get_word(&sb->map[index],
+					SB_NR_TO_BIT(sb, alloc_hint),
+					!round_robin);
+		if (nr != -1) {
+			nr += index << sb->shift;
+			break;
+		}
+
+		/* Jump to next index. */
+		index++;
+		alloc_hint = index << sb->shift;
+
+		if (index >= sb->map_nr) {
+			index = 0;
+			alloc_hint = 0;
+		}
+	}
+
+	return nr;
+}
+EXPORT_SYMBOL_GPL(sbitmap_get);
+
+bool sbitmap_any_bit_set(const struct sbitmap *sb)
+{
+	unsigned int i;
+
+	for (i = 0; i < sb->map_nr; i++) {
+		if (sb->map[i].word)
+			return true;
+	}
+	return false;
+}
+EXPORT_SYMBOL_GPL(sbitmap_any_bit_set);
+
+bool sbitmap_any_bit_clear(const struct sbitmap *sb)
+{
+	unsigned int i;
+
+	for (i = 0; i < sb->map_nr; i++) {
+		const struct sbitmap_word *word = &sb->map[i];
+		unsigned long ret;
+
+		ret = find_first_zero_bit(&word->word, word->depth);
+		if (ret < word->depth)
+			return true;
+	}
+	return false;
+}
+EXPORT_SYMBOL_GPL(sbitmap_any_bit_clear);
+
+unsigned int sbitmap_weight(const struct sbitmap *sb)
+{
+	unsigned int i, weight;
+
+	for (i = 0; i < sb->map_nr; i++) {
+		const struct sbitmap_word *word = &sb->map[i];
+
+		weight += bitmap_weight(&word->word, word->depth);
+	}
+	return weight;
+}
+EXPORT_SYMBOL_GPL(sbitmap_weight);
+
+static unsigned int sbq_calc_wake_batch(unsigned int depth)
+{
+	unsigned int wake_batch;
+
+	/*
+	 * For each batch, we wake up one queue. We need to make sure that our
+	 * batch size is small enough that the full depth of the bitmap is
+	 * enough to wake up all of the queues.
+	 */
+	wake_batch = SBQ_WAKE_BATCH;
+	if (wake_batch > depth / SBQ_WAIT_QUEUES)
+		wake_batch = max(1U, depth / SBQ_WAIT_QUEUES);
+
+	return wake_batch;
+}
+
+int sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,
+			    int shift, gfp_t flags, int node)
+{
+	int ret;
+	int i;
+
+	ret = sbitmap_init_node(&sbq->sb, depth, shift, flags, node);
+	if (ret)
+		return ret;
+
+	sbq->wake_batch = sbq_calc_wake_batch(depth);
+	atomic_set(&sbq->wake_index, 0);
+
+	sbq->ws = kzalloc(SBQ_WAIT_QUEUES * sizeof(*sbq->ws), flags);
+	if (!sbq->ws) {
+		sbitmap_free(&sbq->sb);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
+		init_waitqueue_head(&sbq->ws[i].wait);
+		atomic_set(&sbq->ws[i].wait_cnt, sbq->wake_batch);
+	}
+	return 0;
+}
+EXPORT_SYMBOL_GPL(sbitmap_queue_init_node);
+
+void sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth)
+{
+	sbq->wake_batch = sbq_calc_wake_batch(depth);
+	sbitmap_resize(&sbq->sb, depth);
+}
+EXPORT_SYMBOL_GPL(sbitmap_queue_resize);
+
+static struct sbq_wait_state *sbq_wake_ptr(struct sbitmap_queue *sbq)
+{
+	int i, wake_index;
+
+	wake_index = atomic_read(&sbq->wake_index);
+	for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
+		struct sbq_wait_state *ws = &sbq->ws[wake_index];
+
+		if (waitqueue_active(&ws->wait)) {
+			int o = atomic_read(&sbq->wake_index);
+
+			if (wake_index != o)
+				atomic_cmpxchg(&sbq->wake_index, o, wake_index);
+			return ws;
+		}
+
+		wake_index = sbq_index_inc(wake_index);
+	}
+
+	return NULL;
+}
+
+static void sbq_wake_up(struct sbitmap_queue *sbq)
+{
+	struct sbq_wait_state *ws;
+	int wait_cnt;
+
+	/* Ensure that the wait list checks occur after clear_bit(). */
+	smp_mb();
+
+	ws = sbq_wake_ptr(sbq);
+	if (!ws)
+		return;
+
+	wait_cnt = atomic_dec_return(&ws->wait_cnt);
+	if (unlikely(wait_cnt < 0))
+		wait_cnt = atomic_inc_return(&ws->wait_cnt);
+	if (wait_cnt == 0) {
+		atomic_add(sbq->wake_batch, &ws->wait_cnt);
+		sbq_index_atomic_inc(&sbq->wake_index);
+		wake_up(&ws->wait);
+	}
+}
+
+void sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr)
+{
+	sbitmap_clear_bit(&sbq->sb, nr);
+	sbq_wake_up(sbq);
+}
+EXPORT_SYMBOL_GPL(sbitmap_queue_clear);
+
+void sbitmap_queue_wake_all(struct sbitmap_queue *sbq)
+{
+	int i, wake_index;
+
+	/*
+	 * Make sure all changes prior to this are visible from other CPUs.
+	 */
+	smp_mb();
+	wake_index = atomic_read(&sbq->wake_index);
+	for (i = 0; i < SBQ_WAIT_QUEUES; i++) {
+		struct sbq_wait_state *ws = &sbq->ws[wake_index];
+
+		if (waitqueue_active(&ws->wait))
+			wake_up(&ws->wait);
+
+		wake_index = sbq_index_inc(wake_index);
+	}
+}
+EXPORT_SYMBOL_GPL(sbitmap_queue_wake_all);
