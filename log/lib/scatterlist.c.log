commit 6e85318521c001b6af82830774a8a7d05eec6adf
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Mon Apr 6 20:10:09 2020 -0700

    lib/scatterlist: fix sg_copy_buffer() kerneldoc
    
    Add the missing closing parenthesis to the description for the to_buffer
    parameter of sg_copy_buffer().
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Akinobu Mita <akinobu.mita@gmail.com
    Link: http://lkml.kernel.org/r/20200212084241.8778-1-geert+renesas@glider.be
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 5813072bc589..5d63a8857f36 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -832,7 +832,7 @@ EXPORT_SYMBOL(sg_miter_stop);
  * @buflen:		 The number of bytes to copy
  * @skip:		 Number of bytes to skip before copying
  * @to_buffer:		 transfer direction (true == from an sg list to a
- *			 buffer, false == from a buffer to an sg list
+ *			 buffer, false == from a buffer to an sg list)
  *
  * Returns the number of copied bytes.
  *

commit 4e456fee215677584cafa7f67298a76917e89c64
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Thu Jan 30 22:16:37 2020 -0800

    lib/scatterlist.c: adjust indentation in __sg_alloc_table
    
    Clang warns:
    
      ../lib/scatterlist.c:314:5: warning: misleading indentation; statement
      is not part of the previous 'if' [-Wmisleading-indentation]
                              return -ENOMEM;
                              ^
      ../lib/scatterlist.c:311:4: note: previous statement is here
                              if (prv)
                              ^
      1 warning generated.
    
    This warning occurs because there is a space before the tab on this
    line.  Remove it so that the indentation is consistent with the Linux
    kernel coding style and clang no longer warns.
    
    Link: http://lkml.kernel.org/r/20191218033606.11942-1-natechancellor@gmail.com
    Link: https://github.com/ClangBuiltLinux/linux/issues/830
    Fixes: edce6820a9fd ("scatterlist: prevent invalid free when alloc fails")
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index c2cf2c311b7d..5813072bc589 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -311,7 +311,7 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 			if (prv)
 				table->nents = ++table->orig_nents;
 
- 			return -ENOMEM;
+			return -ENOMEM;
 		}
 
 		sg_init_table(sg, alloc_size);

commit 1f7563f743d7081710a9d186a8b203997d09f383
Merge: ba6d10ab8014 3e99b3b13a1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 11 15:17:41 2019 -0700

    Merge tag 'scsi-sg' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi
    
    Pull SCSI scatter-gather list updates from James Bottomley:
     "This topic branch covers a fundamental change in how our sg lists are
      allocated to make mq more efficient by reducing the size of the
      preallocated sg list.
    
      This necessitates a large number of driver changes because the
      previous guarantee that if a driver specified SG_ALL as the size of
      its scatter list, it would get a non-chained list and didn't need to
      bother with scatterlist iterators is now broken and every driver
      *must* use scatterlist iterators.
    
      This was broken out as a separate topic because we need to convert all
      the drivers before pulling the trigger and unconverted drivers kept
      being found, necessitating a rebase"
    
    * tag 'scsi-sg' of git://git.kernel.org/pub/scm/linux/kernel/git/jejb/scsi: (21 commits)
      scsi: core: don't preallocate small SGL in case of NO_SG_CHAIN
      scsi: lib/sg_pool.c: clear 'first_chunk' in case of no preallocation
      scsi: core: avoid preallocating big SGL for data
      scsi: core: avoid preallocating big SGL for protection information
      scsi: lib/sg_pool.c: improve APIs for allocating sg pool
      scsi: esp: use sg helper to iterate over scatterlist
      scsi: NCR5380: use sg helper to iterate over scatterlist
      scsi: wd33c93: use sg helper to iterate over scatterlist
      scsi: ppa: use sg helper to iterate over scatterlist
      scsi: pcmcia: nsp_cs: use sg helper to iterate over scatterlist
      scsi: imm: use sg helper to iterate over scatterlist
      scsi: aha152x: use sg helper to iterate over scatterlist
      scsi: s390: zfcp_fc: use sg helper to iterate over scatterlist
      scsi: staging: unisys: visorhba: use sg helper to iterate over scatterlist
      scsi: usb: image: microtek: use sg helper to iterate over scatterlist
      scsi: pmcraid: use sg helper to iterate over scatterlist
      scsi: ipr: use sg helper to iterate over scatterlist
      scsi: mvumi: use sg helper to iterate over scatterlist
      scsi: lpfc: use sg helper to iterate over scatterlist
      scsi: advansys: use sg helper to iterate over scatterlist
      ...

commit 4d2fa8b44b891f0da5ceda3e5a1402ccf0ab6f26
Merge: 8b68150883ca f3880a23564e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 20:57:08 2019 -0700

    Merge branch 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6
    
    Pull crypto updates from Herbert Xu:
     "Here is the crypto update for 5.3:
    
      API:
       - Test shash interface directly in testmgr
       - cra_driver_name is now mandatory
    
      Algorithms:
       - Replace arc4 crypto_cipher with library helper
       - Implement 5 way interleave for ECB, CBC and CTR on arm64
       - Add xxhash
       - Add continuous self-test on noise source to drbg
       - Update jitter RNG
    
      Drivers:
       - Add support for SHA204A random number generator
       - Add support for 7211 in iproc-rng200
       - Fix fuzz test failures in inside-secure
       - Fix fuzz test failures in talitos
       - Fix fuzz test failures in qat"
    
    * 'linus' of git://git.kernel.org/pub/scm/linux/kernel/git/herbert/crypto-2.6: (143 commits)
      crypto: stm32/hash - remove interruptible condition for dma
      crypto: stm32/hash - Fix hmac issue more than 256 bytes
      crypto: stm32/crc32 - rename driver file
      crypto: amcc - remove memset after dma_alloc_coherent
      crypto: ccp - Switch to SPDX license identifiers
      crypto: ccp - Validate the the error value used to index error messages
      crypto: doc - Fix formatting of new crypto engine content
      crypto: doc - Add parameter documentation
      crypto: arm64/aes-ce - implement 5 way interleave for ECB, CBC and CTR
      crypto: arm64/aes-ce - add 5 way interleave routines
      crypto: talitos - drop icv_ool
      crypto: talitos - fix hash on SEC1.
      crypto: talitos - move struct talitos_edesc into talitos.h
      lib/scatterlist: Fix mapping iterator when sg->offset is greater than PAGE_SIZE
      crypto/NX: Set receive window credits to max number of CRBs in RxFIFO
      crypto: asymmetric_keys - select CRYPTO_HASH where needed
      crypto: serpent - mark __serpent_setkey_sbox noinline
      crypto: testmgr - dynamically allocate crypto_shash
      crypto: testmgr - dynamically allocate testvec_config
      crypto: talitos - eliminate unneeded 'done' functions at build time
      ...

commit aeb87246537a83c2aff482f3f34a2e0991e02cbc
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Jun 24 07:20:14 2019 +0000

    lib/scatterlist: Fix mapping iterator when sg->offset is greater than PAGE_SIZE
    
    All mapping iterator logic is based on the assumption that sg->offset
    is always lower than PAGE_SIZE.
    
    But there are situations where sg->offset is such that the SG item
    is on the second page. In that case sg_copy_to_buffer() fails
    properly copying the data into the buffer. One of the reason is
    that the data will be outside the kmapped area used to access that
    data.
    
    This patch fixes the issue by adjusting the mapping iterator
    offset and pgoffset fields such that offset is always lower than
    PAGE_SIZE.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Fixes: 4225fc8555a9 ("lib/scatterlist: use page iterator in the mapping iterator")
    Cc: stable@vger.kernel.org
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 739dc9fe2c55..f0757a67affe 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -678,17 +678,18 @@ static bool sg_miter_get_next_page(struct sg_mapping_iter *miter)
 {
 	if (!miter->__remaining) {
 		struct scatterlist *sg;
-		unsigned long pgoffset;
 
 		if (!__sg_page_iter_next(&miter->piter))
 			return false;
 
 		sg = miter->piter.sg;
-		pgoffset = miter->piter.sg_pgoffset;
 
-		miter->__offset = pgoffset ? 0 : sg->offset;
+		miter->__offset = miter->piter.sg_pgoffset ? 0 : sg->offset;
+		miter->piter.sg_pgoffset += miter->__offset >> PAGE_SHIFT;
+		miter->__offset &= PAGE_SIZE - 1;
 		miter->__remaining = sg->offset + sg->length -
-				(pgoffset << PAGE_SHIFT) - miter->__offset;
+				     (miter->piter.sg_pgoffset << PAGE_SHIFT) -
+				     miter->__offset;
 		miter->__remaining = min_t(unsigned long, miter->__remaining,
 					   PAGE_SIZE - miter->__offset);
 	}

commit 4635873c561ac57b66adfcc2487c38106b1c916c
Author: Ming Lei <ming.lei@redhat.com>
Date:   Sun Apr 28 15:39:30 2019 +0800

    scsi: lib/sg_pool.c: improve APIs for allocating sg pool
    
    sg_alloc_table_chained() currently allows the caller to provide one
    preallocated SGL and returns if the requested number isn't bigger than
    size of that SGL. This is used to inline an SGL for an IO request.
    
    However, scattergather code only allows that size of the 1st preallocated
    SGL to be SG_CHUNK_SIZE(128). This means a substantial amount of memory
    (4KB) is claimed for the SGL for each IO request. If the I/O is small, it
    would be prudent to allocate a smaller SGL.
    
    Introduce an extra parameter to sg_alloc_table_chained() and
    sg_free_table_chained() for specifying size of the preallocated SGL.
    
    Both __sg_free_table() and __sg_alloc_table() assume that each SGL has the
    same size except for the last one.  Change the code to allow both functions
    to accept a variable size for the 1st preallocated SGL.
    
    [mkp: attempted to clarify commit desc]
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Bart Van Assche <bvanassche@acm.org>
    Cc: Ewan D. Milne <emilne@redhat.com>
    Cc: Hannes Reinecke <hare@suse.com>
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Cc: Chuck Lever <chuck.lever@oracle.com>
    Cc: netdev@vger.kernel.org
    Cc: linux-nvme@lists.infradead.org
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ming Lei <ming.lei@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Martin K. Petersen <martin.petersen@oracle.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 739dc9fe2c55..77ec8eec3fd0 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -181,7 +181,8 @@ static void sg_kfree(struct scatterlist *sg, unsigned int nents)
  * __sg_free_table - Free a previously mapped sg table
  * @table:	The sg table header to use
  * @max_ents:	The maximum number of entries per single scatterlist
- * @skip_first_chunk: don't free the (preallocated) first scatterlist chunk
+ * @nents_first_chunk: Number of entries int the (preallocated) first
+ * 	scatterlist chunk, 0 means no such preallocated first chunk
  * @free_fn:	Free function
  *
  *  Description:
@@ -191,9 +192,10 @@ static void sg_kfree(struct scatterlist *sg, unsigned int nents)
  *
  **/
 void __sg_free_table(struct sg_table *table, unsigned int max_ents,
-		     bool skip_first_chunk, sg_free_fn *free_fn)
+		     unsigned int nents_first_chunk, sg_free_fn *free_fn)
 {
 	struct scatterlist *sgl, *next;
+	unsigned curr_max_ents = nents_first_chunk ?: max_ents;
 
 	if (unlikely(!table->sgl))
 		return;
@@ -209,9 +211,9 @@ void __sg_free_table(struct sg_table *table, unsigned int max_ents,
 		 * sg_size is then one less than alloc size, since the last
 		 * element is the chain pointer.
 		 */
-		if (alloc_size > max_ents) {
-			next = sg_chain_ptr(&sgl[max_ents - 1]);
-			alloc_size = max_ents;
+		if (alloc_size > curr_max_ents) {
+			next = sg_chain_ptr(&sgl[curr_max_ents - 1]);
+			alloc_size = curr_max_ents;
 			sg_size = alloc_size - 1;
 		} else {
 			sg_size = alloc_size;
@@ -219,11 +221,12 @@ void __sg_free_table(struct sg_table *table, unsigned int max_ents,
 		}
 
 		table->orig_nents -= sg_size;
-		if (skip_first_chunk)
-			skip_first_chunk = false;
+		if (nents_first_chunk)
+			nents_first_chunk = 0;
 		else
 			free_fn(sgl, alloc_size);
 		sgl = next;
+		curr_max_ents = max_ents;
 	}
 
 	table->sgl = NULL;
@@ -246,6 +249,8 @@ EXPORT_SYMBOL(sg_free_table);
  * @table:	The sg table header to use
  * @nents:	Number of entries in sg list
  * @max_ents:	The maximum number of entries the allocator returns per call
+ * @nents_first_chunk: Number of entries int the (preallocated) first
+ * 	scatterlist chunk, 0 means no such preallocated chunk provided by user
  * @gfp_mask:	GFP allocation mask
  * @alloc_fn:	Allocator to use
  *
@@ -262,10 +267,13 @@ EXPORT_SYMBOL(sg_free_table);
  **/
 int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 		     unsigned int max_ents, struct scatterlist *first_chunk,
-		     gfp_t gfp_mask, sg_alloc_fn *alloc_fn)
+		     unsigned int nents_first_chunk, gfp_t gfp_mask,
+		     sg_alloc_fn *alloc_fn)
 {
 	struct scatterlist *sg, *prv;
 	unsigned int left;
+	unsigned curr_max_ents = nents_first_chunk ?: max_ents;
+	unsigned prv_max_ents;
 
 	memset(table, 0, sizeof(*table));
 
@@ -281,8 +289,8 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 	do {
 		unsigned int sg_size, alloc_size = left;
 
-		if (alloc_size > max_ents) {
-			alloc_size = max_ents;
+		if (alloc_size > curr_max_ents) {
+			alloc_size = curr_max_ents;
 			sg_size = alloc_size - 1;
 		} else
 			sg_size = alloc_size;
@@ -316,7 +324,7 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 		 * If this is not the first mapping, chain previous part.
 		 */
 		if (prv)
-			sg_chain(prv, max_ents, sg);
+			sg_chain(prv, prv_max_ents, sg);
 		else
 			table->sgl = sg;
 
@@ -327,6 +335,8 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 			sg_mark_end(&sg[sg_size - 1]);
 
 		prv = sg;
+		prv_max_ents = curr_max_ents;
+		curr_max_ents = max_ents;
 	} while (left);
 
 	return 0;
@@ -349,9 +359,9 @@ int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
 	int ret;
 
 	ret = __sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,
-			       NULL, gfp_mask, sg_kmalloc);
+			       NULL, 0, gfp_mask, sg_kmalloc);
 	if (unlikely(ret))
-		__sg_free_table(table, SG_MAX_SINGLE_ALLOC, false, sg_kfree);
+		__sg_free_table(table, SG_MAX_SINGLE_ALLOC, 0, sg_kfree);
 
 	return ret;
 }

commit 40b0b3f8fb2d8f55d13ceed41593d46689a6b496
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:46 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 230
    
    Based on 2 normalized pattern(s):
    
      this source code is licensed under the gnu general public license
      version 2 see the file copying for more details
    
      this source code is licensed under general public license version 2
      see
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 52 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.449021192@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 739dc9fe2c55..2882d9ba6607 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -1,10 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Copyright (C) 2007 Jens Axboe <jens.axboe@oracle.com>
  *
  * Scatterlist handling helpers.
- *
- * This source code is licensed under the GNU General Public License,
- * Version 2. See the file COPYING for more details.
  */
 #include <linux/export.h>
 #include <linux/slab.h>

commit d901b2760dc6cd5fbbf2eac31d71d94baa6c4aef
Author: Jason Gunthorpe <jgg@mellanox.com>
Date:   Fri Jan 4 11:40:21 2019 -0700

    lib/scatterlist: Provide a DMA page iterator
    
    Commit 2db76d7c3c6d ("lib/scatterlist: sg_page_iter: support sg lists w/o
    backing pages") introduced the sg_page_iter_dma_address() function without
    providing a way to use it in the general case. If the sg_dma_len() is not
    equal to the sg length callers cannot safely use the
    for_each_sg_page/sg_page_iter_dma_address combination.
    
    Resolve this API mistake by providing a DMA specific iterator,
    for_each_sg_dma_page(), that uses the right length so
    sg_page_iter_dma_address() works as expected with all sglists.
    
    A new iterator type is introduced to provide compile-time safety against
    wrongly mixing accessors and iterators.
    
    Acked-by: Christoph Hellwig <hch@lst.de> (for scatterlist)
    Acked-by: Thomas Hellstrom <thellstrom@vmware.com>
    Acked-by: Sakari Ailus <sakari.ailus@linux.intel.com> (ipu3-cio2)
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 9ba349e775ef..739dc9fe2c55 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -625,6 +625,32 @@ bool __sg_page_iter_next(struct sg_page_iter *piter)
 }
 EXPORT_SYMBOL(__sg_page_iter_next);
 
+static int sg_dma_page_count(struct scatterlist *sg)
+{
+	return PAGE_ALIGN(sg->offset + sg_dma_len(sg)) >> PAGE_SHIFT;
+}
+
+bool __sg_page_iter_dma_next(struct sg_dma_page_iter *dma_iter)
+{
+	struct sg_page_iter *piter = &dma_iter->base;
+
+	if (!piter->__nents || !piter->sg)
+		return false;
+
+	piter->sg_pgoffset += piter->__pg_advance;
+	piter->__pg_advance = 1;
+
+	while (piter->sg_pgoffset >= sg_dma_page_count(piter->sg)) {
+		piter->sg_pgoffset -= sg_dma_page_count(piter->sg);
+		piter->sg = sg_next(piter->sg);
+		if (!--piter->__nents || !piter->sg)
+			return false;
+	}
+
+	return true;
+}
+EXPORT_SYMBOL(__sg_page_iter_dma_next);
+
 /**
  * sg_miter_start - start mapping iteration over a sg list
  * @miter: sg mapping iter to be started

commit 7c703e54cc71df5baa962e24a5663d88173bba5c
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Nov 9 09:51:00 2018 +0100

    arch: switch the default on ARCH_HAS_SG_CHAIN
    
    These days architectures are mostly out of the business of dealing with
    struct scatterlist at all, unless they have architecture specific iommu
    drivers.  Replace the ARCH_HAS_SG_CHAIN symbol with a ARCH_NO_SG_CHAIN
    one only enabled for architectures with horrible legacy iommu drivers
    like alpha and parisc, and conditionally for arm which wants to keep it
    disable for legacy platforms.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Palmer Dabbelt <palmer@sifive.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 7c6096a71704..9ba349e775ef 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -271,7 +271,7 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 
 	if (nents == 0)
 		return -EINVAL;
-#ifndef CONFIG_ARCH_HAS_SG_CHAIN
+#ifdef CONFIG_ARCH_NO_SG_CHAIN
 	if (WARN_ON_ONCE(nents > max_ents))
 		return -EINVAL;
 #endif

commit e6e5bec43c0d5dec97355ebf9f6c9bbf4d4c29d5
Merge: 1904148a361a 9544bc534720
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 30 10:47:46 2018 -0700

    Merge tag 'for-linus-20180629' of git://git.kernel.dk/linux-block
    
    Pull block fixes from Jens Axboe:
     "Small set of fixes for this series. Mostly just minor fixes, the only
      oddball in here is the sg change.
    
      The sg change came out of the stall fix for NVMe, where we added a
      mempool and limited us to a single page allocation. CONFIG_SG_DEBUG
      sort-of ruins that, since we'd need to account for that. That's
      actually a generic problem, since lots of drivers need to allocate SG
      lists. So this just removes support for CONFIG_SG_DEBUG, which I added
      back in 2007 and to my knowledge it was never useful.
    
      Anyway, outside of that, this pull contains:
    
       - clone of request with special payload fix (Bart)
    
       - drbd discard handling fix (Bart)
    
       - SATA blk-mq stall fix (me)
    
       - chunk size fix (Keith)
    
       - double free nvme rdma fix (Sagi)"
    
    * tag 'for-linus-20180629' of git://git.kernel.dk/linux-block:
      sg: remove ->sg_magic member
      drbd: Fix drbd_request_prepare() discard handling
      blk-mq: don't queue more if we get a busy return
      block: Fix cloning of requests with a special payload
      nvme-rdma: fix possible double free of controller async event buffer
      block: Fix transfer when chunk sectors exceeds max

commit 9544bc5347207a68eb308cc8aaaed6c3a687cabd
Author: Jens Axboe <axboe@kernel.dk>
Date:   Fri Jun 29 08:48:06 2018 -0600

    sg: remove ->sg_magic member
    
    This was introduced more than a decade ago when sg chaining was
    added, but we never really caught anything with it. The scatterlist
    entry size can be critical, since drivers allocate it, so remove
    the magic member. Recently it's been triggering allocation stalls
    and failures in NVMe.
    
    Tested-by: Jordan Glover <Golden_Miller83@protonmail.ch>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 06dad7a072fd..d4ae67d6cd1e 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -24,9 +24,6 @@
  **/
 struct scatterlist *sg_next(struct scatterlist *sg)
 {
-#ifdef CONFIG_DEBUG_SG
-	BUG_ON(sg->sg_magic != SG_MAGIC);
-#endif
 	if (sg_is_last(sg))
 		return NULL;
 
@@ -111,10 +108,7 @@ struct scatterlist *sg_last(struct scatterlist *sgl, unsigned int nents)
 	for_each_sg(sgl, sg, nents, i)
 		ret = sg;
 
-#ifdef CONFIG_DEBUG_SG
-	BUG_ON(sgl[0].sg_magic != SG_MAGIC);
 	BUG_ON(!sg_is_last(ret));
-#endif
 	return ret;
 }
 EXPORT_SYMBOL(sg_last);

commit 6da2ec56059c3c7a7e5f729e6349e74ace1e5c57
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Jun 12 13:55:00 2018 -0700

    treewide: kmalloc() -> kmalloc_array()
    
    The kmalloc() function has a 2-factor argument form, kmalloc_array(). This
    patch replaces cases of:
    
            kmalloc(a * b, gfp)
    
    with:
            kmalloc_array(a * b, gfp)
    
    as well as handling cases of:
    
            kmalloc(a * b * c, gfp)
    
    with:
    
            kmalloc(array3_size(a, b, c), gfp)
    
    as it's slightly less ugly than:
    
            kmalloc_array(array_size(a, b), c, gfp)
    
    This does, however, attempt to ignore constant size factors like:
    
            kmalloc(4 * 1024, gfp)
    
    though any constants defined via macros get caught up in the conversion.
    
    Any factors with a sizeof() of "unsigned char", "char", and "u8" were
    dropped, since they're redundant.
    
    The tools/ directory was manually excluded, since it has its own
    implementation of kmalloc().
    
    The Coccinelle script used for this was:
    
    // Fix redundant parens around sizeof().
    @@
    type TYPE;
    expression THING, E;
    @@
    
    (
      kmalloc(
    -       (sizeof(TYPE)) * E
    +       sizeof(TYPE) * E
      , ...)
    |
      kmalloc(
    -       (sizeof(THING)) * E
    +       sizeof(THING) * E
      , ...)
    )
    
    // Drop single-byte sizes and redundant parens.
    @@
    expression COUNT;
    typedef u8;
    typedef __u8;
    @@
    
    (
      kmalloc(
    -       sizeof(u8) * (COUNT)
    +       COUNT
      , ...)
    |
      kmalloc(
    -       sizeof(__u8) * (COUNT)
    +       COUNT
      , ...)
    |
      kmalloc(
    -       sizeof(char) * (COUNT)
    +       COUNT
      , ...)
    |
      kmalloc(
    -       sizeof(unsigned char) * (COUNT)
    +       COUNT
      , ...)
    |
      kmalloc(
    -       sizeof(u8) * COUNT
    +       COUNT
      , ...)
    |
      kmalloc(
    -       sizeof(__u8) * COUNT
    +       COUNT
      , ...)
    |
      kmalloc(
    -       sizeof(char) * COUNT
    +       COUNT
      , ...)
    |
      kmalloc(
    -       sizeof(unsigned char) * COUNT
    +       COUNT
      , ...)
    )
    
    // 2-factor product with sizeof(type/expression) and identifier or constant.
    @@
    type TYPE;
    expression THING;
    identifier COUNT_ID;
    constant COUNT_CONST;
    @@
    
    (
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(TYPE) * (COUNT_ID)
    +       COUNT_ID, sizeof(TYPE)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(TYPE) * COUNT_ID
    +       COUNT_ID, sizeof(TYPE)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(TYPE) * (COUNT_CONST)
    +       COUNT_CONST, sizeof(TYPE)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(TYPE) * COUNT_CONST
    +       COUNT_CONST, sizeof(TYPE)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(THING) * (COUNT_ID)
    +       COUNT_ID, sizeof(THING)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(THING) * COUNT_ID
    +       COUNT_ID, sizeof(THING)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(THING) * (COUNT_CONST)
    +       COUNT_CONST, sizeof(THING)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(THING) * COUNT_CONST
    +       COUNT_CONST, sizeof(THING)
      , ...)
    )
    
    // 2-factor product, only identifiers.
    @@
    identifier SIZE, COUNT;
    @@
    
    - kmalloc
    + kmalloc_array
      (
    -       SIZE * COUNT
    +       COUNT, SIZE
      , ...)
    
    // 3-factor product with 1 sizeof(type) or sizeof(expression), with
    // redundant parens removed.
    @@
    expression THING;
    identifier STRIDE, COUNT;
    type TYPE;
    @@
    
    (
      kmalloc(
    -       sizeof(TYPE) * (COUNT) * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kmalloc(
    -       sizeof(TYPE) * (COUNT) * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kmalloc(
    -       sizeof(TYPE) * COUNT * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kmalloc(
    -       sizeof(TYPE) * COUNT * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(TYPE))
      , ...)
    |
      kmalloc(
    -       sizeof(THING) * (COUNT) * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      kmalloc(
    -       sizeof(THING) * (COUNT) * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      kmalloc(
    -       sizeof(THING) * COUNT * (STRIDE)
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    |
      kmalloc(
    -       sizeof(THING) * COUNT * STRIDE
    +       array3_size(COUNT, STRIDE, sizeof(THING))
      , ...)
    )
    
    // 3-factor product with 2 sizeof(variable), with redundant parens removed.
    @@
    expression THING1, THING2;
    identifier COUNT;
    type TYPE1, TYPE2;
    @@
    
    (
      kmalloc(
    -       sizeof(TYPE1) * sizeof(TYPE2) * COUNT
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
      , ...)
    |
      kmalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(TYPE2))
      , ...)
    |
      kmalloc(
    -       sizeof(THING1) * sizeof(THING2) * COUNT
    +       array3_size(COUNT, sizeof(THING1), sizeof(THING2))
      , ...)
    |
      kmalloc(
    -       sizeof(THING1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(THING1), sizeof(THING2))
      , ...)
    |
      kmalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * COUNT
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
      , ...)
    |
      kmalloc(
    -       sizeof(TYPE1) * sizeof(THING2) * (COUNT)
    +       array3_size(COUNT, sizeof(TYPE1), sizeof(THING2))
      , ...)
    )
    
    // 3-factor product, only identifiers, with redundant parens removed.
    @@
    identifier STRIDE, SIZE, COUNT;
    @@
    
    (
      kmalloc(
    -       (COUNT) * STRIDE * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kmalloc(
    -       COUNT * (STRIDE) * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kmalloc(
    -       COUNT * STRIDE * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kmalloc(
    -       (COUNT) * (STRIDE) * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kmalloc(
    -       COUNT * (STRIDE) * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kmalloc(
    -       (COUNT) * STRIDE * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kmalloc(
    -       (COUNT) * (STRIDE) * (SIZE)
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    |
      kmalloc(
    -       COUNT * STRIDE * SIZE
    +       array3_size(COUNT, STRIDE, SIZE)
      , ...)
    )
    
    // Any remaining multi-factor products, first at least 3-factor products,
    // when they're not all constants...
    @@
    expression E1, E2, E3;
    constant C1, C2, C3;
    @@
    
    (
      kmalloc(C1 * C2 * C3, ...)
    |
      kmalloc(
    -       (E1) * E2 * E3
    +       array3_size(E1, E2, E3)
      , ...)
    |
      kmalloc(
    -       (E1) * (E2) * E3
    +       array3_size(E1, E2, E3)
      , ...)
    |
      kmalloc(
    -       (E1) * (E2) * (E3)
    +       array3_size(E1, E2, E3)
      , ...)
    |
      kmalloc(
    -       E1 * E2 * E3
    +       array3_size(E1, E2, E3)
      , ...)
    )
    
    // And then all remaining 2 factors products when they're not all constants,
    // keeping sizeof() as the second factor argument.
    @@
    expression THING, E1, E2;
    type TYPE;
    constant C1, C2, C3;
    @@
    
    (
      kmalloc(sizeof(THING) * C2, ...)
    |
      kmalloc(sizeof(TYPE) * C2, ...)
    |
      kmalloc(C1 * C2 * C3, ...)
    |
      kmalloc(C1 * C2, ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(TYPE) * (E2)
    +       E2, sizeof(TYPE)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(TYPE) * E2
    +       E2, sizeof(TYPE)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(THING) * (E2)
    +       E2, sizeof(THING)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       sizeof(THING) * E2
    +       E2, sizeof(THING)
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       (E1) * E2
    +       E1, E2
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       (E1) * (E2)
    +       E1, E2
      , ...)
    |
    - kmalloc
    + kmalloc_array
      (
    -       E1 * E2
    +       E1, E2
      , ...)
    )
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 06dad7a072fd..1642fd507a96 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -170,7 +170,8 @@ static struct scatterlist *sg_kmalloc(unsigned int nents, gfp_t gfp_mask)
 		kmemleak_alloc(ptr, PAGE_SIZE, 1, gfp_mask);
 		return ptr;
 	} else
-		return kmalloc(nents * sizeof(struct scatterlist), gfp_mask);
+		return kmalloc_array(nents, sizeof(struct scatterlist),
+				     gfp_mask);
 }
 
 static void sg_kfree(struct scatterlist *sg, unsigned int nents)

commit f385178679b6561d2e717567d12e07c7f927ee59
Author: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
Date:   Fri Mar 30 09:20:59 2018 +0900

    lib/scatterlist: add sg_init_marker() helper
    
    sg_init_marker initializes sg_magic in the sg table and calls
    sg_mark_end() on the last entry of the table. This can be useful to
    avoid memset in sg_init_table() when scatterlist is already zeroed out
    
    For example: when scatterlist is embedded inside other struct and that
    container struct is zeroed out
    
    Suggested-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
    Acked-by: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 53728d391d3a..06dad7a072fd 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -132,14 +132,7 @@ EXPORT_SYMBOL(sg_last);
 void sg_init_table(struct scatterlist *sgl, unsigned int nents)
 {
 	memset(sgl, 0, sizeof(*sgl) * nents);
-#ifdef CONFIG_DEBUG_SG
-	{
-		unsigned int i;
-		for (i = 0; i < nents; i++)
-			sgl[i].sg_magic = SG_MAGIC;
-	}
-#endif
-	sg_mark_end(&sgl[nents - 1]);
+	sg_init_marker(sgl, nents);
 }
 EXPORT_SYMBOL(sg_init_table);
 

commit 8c7a8d1c4b9c30a2be3b31a2e6af1cefd45574eb
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Fri Jan 19 11:00:54 2018 -0800

    lib/scatterlist: Fix chaining support in sgl_alloc_order()
    
    This patch avoids that workloads with large block sizes (megabytes)
    can trigger the following call stack with the ib_srpt driver (that
    driver is the only driver that chains scatterlists allocated by
    sgl_alloc_order()):
    
    BUG: Bad page state in process kworker/0:1H  pfn:2423a78
    page:fffffb03d08e9e00 count:-3 mapcount:0 mapping:          (null) index:0x0
    flags: 0x57ffffc0000000()
    raw: 0057ffffc0000000 0000000000000000 0000000000000000 fffffffdffffffff
    raw: dead000000000100 dead000000000200 0000000000000000 0000000000000000
    page dumped because: nonzero _count
    CPU: 0 PID: 733 Comm: kworker/0:1H Tainted: G          I      4.15.0-rc7.bart+ #1
    Hardware name: HP ProLiant DL380 G7, BIOS P67 08/16/2015
    Workqueue: ib-comp-wq ib_cq_poll_work [ib_core]
    Call Trace:
     dump_stack+0x5c/0x83
     bad_page+0xf5/0x10f
     get_page_from_freelist+0xa46/0x11b0
     __alloc_pages_nodemask+0x103/0x290
     sgl_alloc_order+0x101/0x180
     target_alloc_sgl+0x2c/0x40 [target_core_mod]
     srpt_alloc_rw_ctxs+0x173/0x2d0 [ib_srpt]
     srpt_handle_new_iu+0x61e/0x7f0 [ib_srpt]
     __ib_process_cq+0x55/0xa0 [ib_core]
     ib_cq_poll_work+0x1b/0x60 [ib_core]
     process_one_work+0x141/0x340
     worker_thread+0x47/0x3e0
     kthread+0xf5/0x130
     ret_from_fork+0x1f/0x30
    
    Fixes: e80a0af4759a ("lib/scatterlist: Introduce sgl_alloc() and sgl_free()")
    Reported-by: Laurence Oberman <loberman@redhat.com>
    Tested-by: Laurence Oberman <loberman@redhat.com>
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Cc: Nicholas A. Bellinger <nab@linux-iscsi.org>
    Cc: Laurence Oberman <loberman@redhat.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 9afc9b432083..53728d391d3a 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -512,7 +512,7 @@ struct scatterlist *sgl_alloc_order(unsigned long long length,
 	if (!sgl)
 		return NULL;
 
-	sg_init_table(sgl, nent);
+	sg_init_table(sgl, nalloc);
 	sg = sgl;
 	while (length) {
 		elem_len = min_t(u64, length, PAGE_SIZE << order);
@@ -526,7 +526,7 @@ struct scatterlist *sgl_alloc_order(unsigned long long length,
 		length -= elem_len;
 		sg = sg_next(sg);
 	}
-	WARN_ON_ONCE(sg);
+	WARN_ONCE(length, "length = %lld\n", length);
 	if (nent_p)
 		*nent_p = nent;
 	return sgl;
@@ -549,22 +549,44 @@ struct scatterlist *sgl_alloc(unsigned long long length, gfp_t gfp,
 EXPORT_SYMBOL(sgl_alloc);
 
 /**
- * sgl_free_order - free a scatterlist and its pages
+ * sgl_free_n_order - free a scatterlist and its pages
  * @sgl: Scatterlist with one or more elements
+ * @nents: Maximum number of elements to free
  * @order: Second argument for __free_pages()
+ *
+ * Notes:
+ * - If several scatterlists have been chained and each chain element is
+ *   freed separately then it's essential to set nents correctly to avoid that a
+ *   page would get freed twice.
+ * - All pages in a chained scatterlist can be freed at once by setting @nents
+ *   to a high number.
  */
-void sgl_free_order(struct scatterlist *sgl, int order)
+void sgl_free_n_order(struct scatterlist *sgl, int nents, int order)
 {
 	struct scatterlist *sg;
 	struct page *page;
+	int i;
 
-	for (sg = sgl; sg; sg = sg_next(sg)) {
+	for_each_sg(sgl, sg, nents, i) {
+		if (!sg)
+			break;
 		page = sg_page(sg);
 		if (page)
 			__free_pages(page, order);
 	}
 	kfree(sgl);
 }
+EXPORT_SYMBOL(sgl_free_n_order);
+
+/**
+ * sgl_free_order - free a scatterlist and its pages
+ * @sgl: Scatterlist with one or more elements
+ * @order: Second argument for __free_pages()
+ */
+void sgl_free_order(struct scatterlist *sgl, int order)
+{
+	sgl_free_n_order(sgl, INT_MAX, order);
+}
 EXPORT_SYMBOL(sgl_free_order);
 
 /**

commit e80a0af4759a164214f02da157a3800753ce135f
Author: Bart Van Assche <bart.vanassche@wdc.com>
Date:   Fri Jan 5 08:26:46 2018 -0800

    lib/scatterlist: Introduce sgl_alloc() and sgl_free()
    
    Many kernel drivers contain code that allocates and frees both a
    scatterlist and the pages that populate that scatterlist.
    Introduce functions in lib/scatterlist.c that perform these tasks
    instead of duplicating this functionality in multiple drivers.
    Only include these functions in the build if CONFIG_SGL_ALLOC=y
    to avoid that the kernel size increases if this functionality is
    not used.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@wdc.com>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 7c1c55f7daaa..9afc9b432083 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -474,6 +474,111 @@ int sg_alloc_table_from_pages(struct sg_table *sgt, struct page **pages,
 }
 EXPORT_SYMBOL(sg_alloc_table_from_pages);
 
+#ifdef CONFIG_SGL_ALLOC
+
+/**
+ * sgl_alloc_order - allocate a scatterlist and its pages
+ * @length: Length in bytes of the scatterlist. Must be at least one
+ * @order: Second argument for alloc_pages()
+ * @chainable: Whether or not to allocate an extra element in the scatterlist
+ *	for scatterlist chaining purposes
+ * @gfp: Memory allocation flags
+ * @nent_p: [out] Number of entries in the scatterlist that have pages
+ *
+ * Returns: A pointer to an initialized scatterlist or %NULL upon failure.
+ */
+struct scatterlist *sgl_alloc_order(unsigned long long length,
+				    unsigned int order, bool chainable,
+				    gfp_t gfp, unsigned int *nent_p)
+{
+	struct scatterlist *sgl, *sg;
+	struct page *page;
+	unsigned int nent, nalloc;
+	u32 elem_len;
+
+	nent = round_up(length, PAGE_SIZE << order) >> (PAGE_SHIFT + order);
+	/* Check for integer overflow */
+	if (length > (nent << (PAGE_SHIFT + order)))
+		return NULL;
+	nalloc = nent;
+	if (chainable) {
+		/* Check for integer overflow */
+		if (nalloc + 1 < nalloc)
+			return NULL;
+		nalloc++;
+	}
+	sgl = kmalloc_array(nalloc, sizeof(struct scatterlist),
+			    (gfp & ~GFP_DMA) | __GFP_ZERO);
+	if (!sgl)
+		return NULL;
+
+	sg_init_table(sgl, nent);
+	sg = sgl;
+	while (length) {
+		elem_len = min_t(u64, length, PAGE_SIZE << order);
+		page = alloc_pages(gfp, order);
+		if (!page) {
+			sgl_free(sgl);
+			return NULL;
+		}
+
+		sg_set_page(sg, page, elem_len, 0);
+		length -= elem_len;
+		sg = sg_next(sg);
+	}
+	WARN_ON_ONCE(sg);
+	if (nent_p)
+		*nent_p = nent;
+	return sgl;
+}
+EXPORT_SYMBOL(sgl_alloc_order);
+
+/**
+ * sgl_alloc - allocate a scatterlist and its pages
+ * @length: Length in bytes of the scatterlist
+ * @gfp: Memory allocation flags
+ * @nent_p: [out] Number of entries in the scatterlist
+ *
+ * Returns: A pointer to an initialized scatterlist or %NULL upon failure.
+ */
+struct scatterlist *sgl_alloc(unsigned long long length, gfp_t gfp,
+			      unsigned int *nent_p)
+{
+	return sgl_alloc_order(length, 0, false, gfp, nent_p);
+}
+EXPORT_SYMBOL(sgl_alloc);
+
+/**
+ * sgl_free_order - free a scatterlist and its pages
+ * @sgl: Scatterlist with one or more elements
+ * @order: Second argument for __free_pages()
+ */
+void sgl_free_order(struct scatterlist *sgl, int order)
+{
+	struct scatterlist *sg;
+	struct page *page;
+
+	for (sg = sgl; sg; sg = sg_next(sg)) {
+		page = sg_page(sg);
+		if (page)
+			__free_pages(page, order);
+	}
+	kfree(sgl);
+}
+EXPORT_SYMBOL(sgl_free_order);
+
+/**
+ * sgl_free - free a scatterlist and its pages
+ * @sgl: Scatterlist with one or more elements
+ */
+void sgl_free(struct scatterlist *sgl)
+{
+	sgl_free_order(sgl, 0);
+}
+EXPORT_SYMBOL(sgl_free);
+
+#endif /* CONFIG_SGL_ALLOC */
+
 void __sg_page_iter_start(struct sg_page_iter *piter,
 			  struct scatterlist *sglist, unsigned int nents,
 			  unsigned long pgoffset)

commit 89d8589cd72c6f48b19c370517d16f3ee23909df
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Thu Aug 3 10:13:51 2017 +0100

    lib/scatterlist: Introduce and export __sg_alloc_table_from_pages
    
    Drivers like i915 benefit from being able to control the maxium
    size of the sg coalesced segment while building the scatter-
    gather list.
    
    Introduce and export the __sg_alloc_table_from_pages function
    which will allow it that control.
    
    v2: Reorder parameters. (Chris Wilson)
    v3: Fix incomplete reordering in v2.
    v4: max_segment needs to be page aligned.
    v5: Rebase.
    v6: Rebase.
    v7: Fix spelling in commit and mention max segment size in
        __sg_alloc_table_from_pages kerneldoc. (Andrew Morton)
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: https://patchwork.freedesktop.org/patch/msgid/20170803091351.23594-1-tvrtko.ursulin@linux.intel.com

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 7b2e74da2c44..7c1c55f7daaa 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -370,35 +370,38 @@ int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
 EXPORT_SYMBOL(sg_alloc_table);
 
 /**
- * sg_alloc_table_from_pages - Allocate and initialize an sg table from
- *			       an array of pages
- * @sgt:	The sg table header to use
- * @pages:	Pointer to an array of page pointers
- * @n_pages:	Number of pages in the pages array
- * @offset:     Offset from start of the first page to the start of a buffer
- * @size:       Number of valid bytes in the buffer (after offset)
- * @gfp_mask:	GFP allocation mask
+ * __sg_alloc_table_from_pages - Allocate and initialize an sg table from
+ *			         an array of pages
+ * @sgt:	 The sg table header to use
+ * @pages:	 Pointer to an array of page pointers
+ * @n_pages:	 Number of pages in the pages array
+ * @offset:      Offset from start of the first page to the start of a buffer
+ * @size:        Number of valid bytes in the buffer (after offset)
+ * @max_segment: Maximum size of a scatterlist node in bytes (page aligned)
+ * @gfp_mask:	 GFP allocation mask
  *
  *  Description:
  *    Allocate and initialize an sg table from a list of pages. Contiguous
- *    ranges of the pages are squashed into a single scatterlist node. A user
- *    may provide an offset at a start and a size of valid data in a buffer
- *    specified by the page array. The returned sg table is released by
- *    sg_free_table.
+ *    ranges of the pages are squashed into a single scatterlist node up to the
+ *    maximum size specified in @max_segment. An user may provide an offset at a
+ *    start and a size of valid data in a buffer specified by the page array.
+ *    The returned sg table is released by sg_free_table.
  *
  * Returns:
  *   0 on success, negative error on failure
  */
-int sg_alloc_table_from_pages(struct sg_table *sgt,
-	struct page **pages, unsigned int n_pages,
-	unsigned int offset, unsigned long size,
-	gfp_t gfp_mask)
+int __sg_alloc_table_from_pages(struct sg_table *sgt, struct page **pages,
+				unsigned int n_pages, unsigned int offset,
+				unsigned long size, unsigned int max_segment,
+				gfp_t gfp_mask)
 {
-	const unsigned int max_segment = SCATTERLIST_MAX_SEGMENT;
 	unsigned int chunks, cur_page, seg_len, i;
 	int ret;
 	struct scatterlist *s;
 
+	if (WARN_ON(!max_segment || offset_in_page(max_segment)))
+		return -EINVAL;
+
 	/* compute number of contiguous chunks */
 	chunks = 1;
 	seg_len = 0;
@@ -440,6 +443,35 @@ int sg_alloc_table_from_pages(struct sg_table *sgt,
 
 	return 0;
 }
+EXPORT_SYMBOL(__sg_alloc_table_from_pages);
+
+/**
+ * sg_alloc_table_from_pages - Allocate and initialize an sg table from
+ *			       an array of pages
+ * @sgt:	 The sg table header to use
+ * @pages:	 Pointer to an array of page pointers
+ * @n_pages:	 Number of pages in the pages array
+ * @offset:      Offset from start of the first page to the start of a buffer
+ * @size:        Number of valid bytes in the buffer (after offset)
+ * @gfp_mask:	 GFP allocation mask
+ *
+ *  Description:
+ *    Allocate and initialize an sg table from a list of pages. Contiguous
+ *    ranges of the pages are squashed into a single scatterlist node. A user
+ *    may provide an offset at a start and a size of valid data in a buffer
+ *    specified by the page array. The returned sg table is released by
+ *    sg_free_table.
+ *
+ * Returns:
+ *   0 on success, negative error on failure
+ */
+int sg_alloc_table_from_pages(struct sg_table *sgt, struct page **pages,
+			      unsigned int n_pages, unsigned int offset,
+			      unsigned long size, gfp_t gfp_mask)
+{
+	return __sg_alloc_table_from_pages(sgt, pages, n_pages, offset, size,
+					   SCATTERLIST_MAX_SEGMENT, gfp_mask);
+}
 EXPORT_SYMBOL(sg_alloc_table_from_pages);
 
 void __sg_page_iter_start(struct sg_page_iter *piter,

commit c125906b839b794c580a5de911de65bd2c63aaee
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Thu Aug 3 10:13:12 2017 +0100

    lib/scatterlist: Avoid potential scatterlist entry overflow
    
    Since the scatterlist length field is an unsigned int, make
    sure that sg_alloc_table_from_pages does not overflow it while
    coalescing pages to a single entry.
    
    v2: Drop reference to future use. Use UINT_MAX.
    v3: max_segment must be page aligned.
    v4: Do not rely on compiler to optimise out the rounddown.
        (Joonas Lahtinen)
    v5: Simplified loops and use post-increments rather than
        pre-increments. Use PAGE_MASK and fix comment typo.
        (Andy Shevchenko)
    v6: Commit spelling fix.
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: linux-kernel@vger.kernel.org
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Andy Shevchenko <andy.shevchenko@gmail.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20170803091312.22875-1-tvrtko.ursulin@linux.intel.com

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index dee0c5004e2f..7b2e74da2c44 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -394,17 +394,22 @@ int sg_alloc_table_from_pages(struct sg_table *sgt,
 	unsigned int offset, unsigned long size,
 	gfp_t gfp_mask)
 {
-	unsigned int chunks;
-	unsigned int i;
-	unsigned int cur_page;
+	const unsigned int max_segment = SCATTERLIST_MAX_SEGMENT;
+	unsigned int chunks, cur_page, seg_len, i;
 	int ret;
 	struct scatterlist *s;
 
 	/* compute number of contiguous chunks */
 	chunks = 1;
-	for (i = 1; i < n_pages; ++i)
-		if (page_to_pfn(pages[i]) != page_to_pfn(pages[i - 1]) + 1)
-			++chunks;
+	seg_len = 0;
+	for (i = 1; i < n_pages; i++) {
+		seg_len += PAGE_SIZE;
+		if (seg_len >= max_segment ||
+		    page_to_pfn(pages[i]) != page_to_pfn(pages[i - 1]) + 1) {
+			chunks++;
+			seg_len = 0;
+		}
+	}
 
 	ret = sg_alloc_table(sgt, chunks, gfp_mask);
 	if (unlikely(ret))
@@ -413,17 +418,21 @@ int sg_alloc_table_from_pages(struct sg_table *sgt,
 	/* merging chunks and putting them into the scatterlist */
 	cur_page = 0;
 	for_each_sg(sgt->sgl, s, sgt->orig_nents, i) {
-		unsigned long chunk_size;
-		unsigned int j;
+		unsigned int j, chunk_size;
 
 		/* look for the end of the current chunk */
-		for (j = cur_page + 1; j < n_pages; ++j)
-			if (page_to_pfn(pages[j]) !=
+		seg_len = 0;
+		for (j = cur_page + 1; j < n_pages; j++) {
+			seg_len += PAGE_SIZE;
+			if (seg_len >= max_segment ||
+			    page_to_pfn(pages[j]) !=
 			    page_to_pfn(pages[j - 1]) + 1)
 				break;
+		}
 
 		chunk_size = ((j - cur_page) << PAGE_SHIFT) - offset;
-		sg_set_page(s, pages[cur_page], min(size, chunk_size), offset);
+		sg_set_page(s, pages[cur_page],
+			    min_t(unsigned long, size, chunk_size), offset);
 		size -= chunk_size;
 		offset = 0;
 		cur_page = j;

commit c4860ad60564838994b74e7ee7dd12ceeda0f520
Author: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
Date:   Mon Jul 31 19:55:08 2017 +0100

    lib/scatterlist: Fix offset type in sg_alloc_table_from_pages
    
    Scatterlist entries have an unsigned int for the offset so
    correct the sg_alloc_table_from_pages function accordingly.
    
    Since these are offsets withing a page, unsigned int is
    wide enough.
    
    Also converts callers which were using unsigned long locally
    with the lower_32_bits annotation to make it explicitly
    clear what is happening.
    
    v2: Use offset_in_page. (Chris Wilson)
    
    Signed-off-by: Tvrtko Ursulin <tvrtko.ursulin@intel.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Pawel Osciak <pawel@osciak.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Tomasz Stanislawski <t.stanislaws@samsung.com>
    Cc: Matt Porter <mporter@kernel.crashing.org>
    Cc: Alexandre Bounine <alexandre.bounine@idt.com>
    Cc: linux-media@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com> (v1)
    Reviewed-by: Chris Wilson <chris@chris-wilson.co.uk>
    Reviewed-by: Mauro Carvalho Chehab <mchehab@s-opensource.com>
    Link: https://patchwork.freedesktop.org/patch/msgid/20170731185512.20010-1-tvrtko.ursulin@linux.intel.com

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index be7b4dd6b68d..dee0c5004e2f 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -391,7 +391,7 @@ EXPORT_SYMBOL(sg_alloc_table);
  */
 int sg_alloc_table_from_pages(struct sg_table *sgt,
 	struct page **pages, unsigned int n_pages,
-	unsigned long offset, unsigned long size,
+	unsigned int offset, unsigned long size,
 	gfp_t gfp_mask)
 {
 	unsigned int chunks;

commit 0945e56994ac855d01c4aecf69bded65c751b894
Author: Johannes Thumshirn <jthumshirn@suse.de>
Date:   Wed Jun 7 11:45:28 2017 +0200

    scatterlist: add sg_zero_buffer() helper
    
    The sg_zero_buffer() helper is used to zero fill an area in a SG
    list.
    
    Signed-off-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    [hch: renamed to sg_zero_buffer]
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index c6cf82242d65..be7b4dd6b68d 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -751,3 +751,38 @@ size_t sg_pcopy_to_buffer(struct scatterlist *sgl, unsigned int nents,
 	return sg_copy_buffer(sgl, nents, buf, buflen, skip, true);
 }
 EXPORT_SYMBOL(sg_pcopy_to_buffer);
+
+/**
+ * sg_zero_buffer - Zero-out a part of a SG list
+ * @sgl:		 The SG list
+ * @nents:		 Number of SG entries
+ * @buflen:		 The number of bytes to zero out
+ * @skip:		 Number of bytes to skip before zeroing
+ *
+ * Returns the number of bytes zeroed.
+ **/
+size_t sg_zero_buffer(struct scatterlist *sgl, unsigned int nents,
+		       size_t buflen, off_t skip)
+{
+	unsigned int offset = 0;
+	struct sg_mapping_iter miter;
+	unsigned int sg_flags = SG_MITER_ATOMIC | SG_MITER_TO_SG;
+
+	sg_miter_start(&miter, sgl, nents, sg_flags);
+
+	if (!sg_miter_skip(&miter, skip))
+		return false;
+
+	while (offset < buflen && sg_miter_next(&miter)) {
+		unsigned int len;
+
+		len = min(miter.length, buflen - offset);
+		memset(miter.addr, 0, len);
+
+		offset += len;
+	}
+
+	sg_miter_stop(&miter);
+	return offset;
+}
+EXPORT_SYMBOL(sg_zero_buffer);

commit d31712009709774506e536a3cc1f7c512f550a0c
Author: Gilad Ben-Yossef <gilad@benyossef.com>
Date:   Mon Feb 27 14:28:29 2017 -0800

    scatterlist: do not disable IRQs in sg_copy_buffer
    
    Commit 50bed2e2862a ("sg: disable interrupts inside sg_copy_buffer")
    introduced disabling interrupts in sg_copy_buffer() since atomic uses of
    miter required it due to use of kmap_atomic().
    
    However, as commit 8290e2d2dcbf ("scatterlist: atomic sg_mapping_iter()
    no longer needs disabled IRQs") acknowledges disabling interrupts is no
    longer needed for calls to kmap_atomic() and therefore unneeded for
    miter ops either, so remove it from sg_copy_buffer().
    
    Link: http://lkml.kernel.org/r/1486040150-14109-3-git-send-email-gilad@benyossef.com
    Signed-off-by: Gilad Ben-Yossef <gilad@benyossef.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: <ofir.drang@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 393920fddcb9..c6cf82242d65 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -651,7 +651,6 @@ size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents, void *buf,
 {
 	unsigned int offset = 0;
 	struct sg_mapping_iter miter;
-	unsigned long flags;
 	unsigned int sg_flags = SG_MITER_ATOMIC;
 
 	if (to_buffer)
@@ -664,8 +663,6 @@ size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents, void *buf,
 	if (!sg_miter_skip(&miter, skip))
 		return false;
 
-	local_irq_save(flags);
-
 	while ((offset < buflen) && sg_miter_next(&miter)) {
 		unsigned int len;
 
@@ -681,7 +678,6 @@ size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents, void *buf,
 
 	sg_miter_stop(&miter);
 
-	local_irq_restore(flags);
 	return offset;
 }
 EXPORT_SYMBOL(sg_copy_buffer);

commit 1d5210ef706523529d3cc16fd70be0a466253466
Author: Gilad Ben-Yossef <gilad@benyossef.com>
Date:   Mon Feb 27 14:28:27 2017 -0800

    scatterlist: reorder compound boolean expression
    
    Test the cheaper boolean expression with no side effects first.
    
    Link: http://lkml.kernel.org/r/1486040150-14109-2-git-send-email-gilad@benyossef.com
    Signed-off-by: Gilad Ben-Yossef <gilad@benyossef.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: <ofir.drang@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 004fc70fc56a..393920fddcb9 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -666,7 +666,7 @@ size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents, void *buf,
 
 	local_irq_save(flags);
 
-	while (sg_miter_next(&miter) && offset < buflen) {
+	while ((offset < buflen) && sg_miter_next(&miter)) {
 		unsigned int len;
 
 		len = min(miter.length, buflen - offset);

commit 4ba6a2b28f111e4c9621487612056d10f3f4a6ca
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Mon Feb 8 16:09:08 2016 +0900

    scatterlist: fix a typo in comment block of sg_miter_stop()
    
    Fix the doubled "started" and tidy up the following sentences.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index bafa9933fa76..004fc70fc56a 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -598,9 +598,9 @@ EXPORT_SYMBOL(sg_miter_next);
  *
  * Description:
  *   Stops mapping iterator @miter.  @miter should have been started
- *   started using sg_miter_start().  A stopped iteration can be
- *   resumed by calling sg_miter_next() on it.  This is useful when
- *   resources (kmap) need to be released during iteration.
+ *   using sg_miter_start().  A stopped iteration can be resumed by
+ *   calling sg_miter_next() on it.  This is useful when resources (kmap)
+ *   need to be released during iteration.
  *
  * Context:
  *   Preemption disabled if the SG_MITER_ATOMIC is set.  Don't care

commit 10c95ed9aa2970e05fedb4ac8b3ce1b934dab17b
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 7 18:15:11 2015 +0200

    scatterlist: allow limited chaining without ARCH_HAS_SG_CHAIN
    
    There are a couple of uses of struct scatterlist that never go to
    the dma_map_sg() helper and thus don't care about ARCH_HAS_SG_CHAIN
    which indicates that we can map chained S/G list.
    
    The most important one is the crypto code, which currently has
    to open code a few helpers to always allow chaining.  This patch
    removes a few #ifdef ARCH_HAS_SG_CHAIN statements so that we can
    switch the crypto code to these common helpers.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index d105a9f56878..bafa9933fa76 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -105,16 +105,12 @@ EXPORT_SYMBOL(sg_nents_for_len);
  **/
 struct scatterlist *sg_last(struct scatterlist *sgl, unsigned int nents)
 {
-#ifndef CONFIG_ARCH_HAS_SG_CHAIN
-	struct scatterlist *ret = &sgl[nents - 1];
-#else
 	struct scatterlist *sg, *ret = NULL;
 	unsigned int i;
 
 	for_each_sg(sgl, sg, nents, i)
 		ret = sg;
 
-#endif
 #ifdef CONFIG_DEBUG_SG
 	BUG_ON(sgl[0].sg_magic != SG_MAGIC);
 	BUG_ON(!sg_is_last(ret));

commit 386ecb1216f9e38947ce6a2af22e5e1e47256a97
Author: Dave Gordon <david.s.gordon@intel.com>
Date:   Tue Jun 30 14:58:57 2015 -0700

    drivers/scsi/scsi_debug.c: resolve sg buffer const-ness issue
    
    do_device_access() takes a separate parameter to indicate the direction of
    data transfer, which it used to use to select the appropriate function out
    of sg_pcopy_{to,from}_buffer().  However these two functions now have
    
    So this patch makes it bypass these wrappers and call the underlying
    function sg_copy_buffer() directly; this has the same calling style as
    do_device_access() i.e.  a separate direction-of-transfer parameter and no
    pointers-to-const, so skipping the wrappers not only eliminates the
    warning, it also make the code simpler :)
    
    [akpm@linux-foundation.org: fix very broken build]
    Signed-off-by: Dave Gordon <david.s.gordon@intel.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 317b62c6da3c..d105a9f56878 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -650,9 +650,8 @@ EXPORT_SYMBOL(sg_miter_stop);
  * Returns the number of copied bytes.
  *
  **/
-static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
-			     void *buf, size_t buflen, off_t skip,
-			     bool to_buffer)
+size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents, void *buf,
+		      size_t buflen, off_t skip, bool to_buffer)
 {
 	unsigned int offset = 0;
 	struct sg_mapping_iter miter;
@@ -689,6 +688,7 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
 	local_irq_restore(flags);
 	return offset;
 }
+EXPORT_SYMBOL(sg_copy_buffer);
 
 /**
  * sg_copy_from_buffer - Copy from a linear buffer to an SG list

commit 2a1bf8f93b33992bb0457512b28d046e279bbd7e
Author: Dave Gordon <david.s.gordon@intel.com>
Date:   Tue Jun 30 14:58:54 2015 -0700

    lib/scatterlist: mark input buffer parameters as 'const'
    
    The 'buf' parameter of sg(p)copy_from_buffer() can and should be
    const-qualified, although because of the shared implementation of
    _to_buffer() and _from_buffer(), we have to cast this away internally.
    
    This means that callers who have a 'const' buffer containing the data to
    be copied to the sg-list no longer have to cast away the const-ness
    themselves.  It also enables improved coverage by code analysis tools.
    
    Signed-off-by: Dave Gordon <david.s.gordon@intel.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 965c36e7a5a4..317b62c6da3c 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -701,9 +701,9 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
  *
  **/
 size_t sg_copy_from_buffer(struct scatterlist *sgl, unsigned int nents,
-			   void *buf, size_t buflen)
+			   const void *buf, size_t buflen)
 {
-	return sg_copy_buffer(sgl, nents, buf, buflen, 0, false);
+	return sg_copy_buffer(sgl, nents, (void *)buf, buflen, 0, false);
 }
 EXPORT_SYMBOL(sg_copy_from_buffer);
 
@@ -736,9 +736,9 @@ EXPORT_SYMBOL(sg_copy_to_buffer);
  *
  **/
 size_t sg_pcopy_from_buffer(struct scatterlist *sgl, unsigned int nents,
-			    void *buf, size_t buflen, off_t skip)
+			    const void *buf, size_t buflen, off_t skip)
 {
-	return sg_copy_buffer(sgl, nents, buf, buflen, skip, false);
+	return sg_copy_buffer(sgl, nents, (void *)buf, buflen, skip, false);
 }
 EXPORT_SYMBOL(sg_pcopy_from_buffer);
 

commit 4dc7daf843f3914c9c0c7efb71b8f251c9c4636f
Author: Dave Gordon <david.s.gordon@intel.com>
Date:   Tue Jun 30 14:58:52 2015 -0700

    lib/scatterlist.c: fix kerneldoc for sg_pcopy_{to,from}_buffer()
    
    The kerneldoc for the functions doesn't match the code; the last two
    parameters (buflen, skip) have been transposed, which is confusing,
    especially as they're both integral types and the compiler won't warn
    about swapping them.
    
    These functions and the kerneldoc were introduced in commit:
        df642cea lib/scatterlist: introduce sg_pcopy_from_buffer() ...
        Author: Akinobu Mita <akinobu.mita@gmail.com>
        Date:   Mon Jul 8 16:01:54 2013 -0700
    
        The only difference between sg_pcopy_{from,to}_buffer() and
        sg_copy_{from,to}_buffer() is an additional argument that
        specifies the number of bytes to skip the SG list before
        copying.
    
    The functions have the extra argument at the end, but the kerneldoc
    lists it in penultimate position.
    
    Signed-off-by: Dave Gordon <david.s.gordon@intel.com>
    Reviewed-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: "Martin K. Petersen" <martin.petersen@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 99fbc2f238c4..965c36e7a5a4 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -729,8 +729,8 @@ EXPORT_SYMBOL(sg_copy_to_buffer);
  * @sgl:		 The SG list
  * @nents:		 Number of SG entries
  * @buf:		 Where to copy from
- * @skip:		 Number of bytes to skip before copying
  * @buflen:		 The number of bytes to copy
+ * @skip:		 Number of bytes to skip before copying
  *
  * Returns the number of copied bytes.
  *
@@ -747,8 +747,8 @@ EXPORT_SYMBOL(sg_pcopy_from_buffer);
  * @sgl:		 The SG list
  * @nents:		 Number of SG entries
  * @buf:		 Where to copy to
- * @skip:		 Number of bytes to skip before copying
  * @buflen:		 The number of bytes to copy
+ * @skip:		 Number of bytes to skip before copying
  *
  * Returns the number of copied bytes.
  *

commit cfaed10d1f27d036b72bbdc6b1e59ea28c38ec7f
Author: Tom Lendacky <thomas.lendacky@amd.com>
Date:   Mon Jun 1 11:15:25 2015 -0500

    scatterlist: introduce sg_nents_for_len
    
    When performing a dma_map_sg() call, the number of sg entries to map is
    required. Using sg_nents to retrieve the number of sg entries will
    return the total number of entries in the sg list up to the entry marked
    as the end. If there happen to be unused entries in the list, these will
    still be counted. Some dma_map_sg() implementations will not handle the
    unused entries correctly (lib/swiotlb.c) and execute a BUG_ON.
    
    The sg_nents_for_len() function will traverse the sg list and return the
    number of entries required to satisfy the supplied length argument. This
    can then be supplied to the dma_map_sg() call to successfully map the
    sg.
    
    Signed-off-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index c9f2e8c6ccc9..99fbc2f238c4 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -56,6 +56,38 @@ int sg_nents(struct scatterlist *sg)
 }
 EXPORT_SYMBOL(sg_nents);
 
+/**
+ * sg_nents_for_len - return total count of entries in scatterlist
+ *                    needed to satisfy the supplied length
+ * @sg:		The scatterlist
+ * @len:	The total required length
+ *
+ * Description:
+ * Determines the number of entries in sg that are required to meet
+ * the supplied length, taking into acount chaining as well
+ *
+ * Returns:
+ *   the number of sg entries needed, negative error on failure
+ *
+ **/
+int sg_nents_for_len(struct scatterlist *sg, u64 len)
+{
+	int nents;
+	u64 total;
+
+	if (!len)
+		return 0;
+
+	for (nents = 0, total = 0; sg; sg = sg_next(sg)) {
+		nents++;
+		total += sg->length;
+		if (total >= len)
+			return nents;
+	}
+
+	return -EINVAL;
+}
+EXPORT_SYMBOL(sg_nents_for_len);
 
 /**
  * sg_last - return the last scatterlist entry in a list

commit c21e59d8dc04b2107bdb4ff0f412a9b7ae3349f3
Author: Tony Battersby <tonyb@cybernetics.com>
Date:   Thu Oct 23 15:10:21 2014 -0400

    lib/scatterlist: fix memory leak with scsi-mq
    
    Fix a memory leak with scsi-mq triggered by commands with large data
    transfer length.
    
    Fixes: c53c6d6a68b1 ("scatterlist: allow chaining to preallocated chunks")
    Cc: <stable@vger.kernel.org> # 3.17.x
    Signed-off-by: Tony Battersby <tonyb@cybernetics.com>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 9cdf62f8accd..c9f2e8c6ccc9 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -203,10 +203,10 @@ void __sg_free_table(struct sg_table *table, unsigned int max_ents,
 		}
 
 		table->orig_nents -= sg_size;
-		if (!skip_first_chunk) {
-			free_fn(sgl, alloc_size);
+		if (skip_first_chunk)
 			skip_first_chunk = false;
-		}
+		else
+			free_fn(sgl, alloc_size);
 		sgl = next;
 	}
 

commit 308c09f17da4adc53935115dbeb5bce4f067d8f9
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Aug 8 14:23:25 2014 -0700

    lib/scatterlist: make ARCH_HAS_SG_CHAIN an actual Kconfig
    
    Rather than have architectures #define ARCH_HAS_SG_CHAIN in an
    architecture specific scatterlist.h, make it a proper Kconfig option and
    use that instead.  At same time, remove the header files are are now
    mostly useless and just include asm-generic/scatterlist.h.
    
    [sfr@canb.auug.org.au: powerpc files now need asm/dma.h]
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>                  [x86]
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>     [powerpc]
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index b4415fceb7e7..9cdf62f8accd 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -73,7 +73,7 @@ EXPORT_SYMBOL(sg_nents);
  **/
 struct scatterlist *sg_last(struct scatterlist *sgl, unsigned int nents)
 {
-#ifndef ARCH_HAS_SG_CHAIN
+#ifndef CONFIG_ARCH_HAS_SG_CHAIN
 	struct scatterlist *ret = &sgl[nents - 1];
 #else
 	struct scatterlist *sg, *ret = NULL;
@@ -255,7 +255,7 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 
 	if (nents == 0)
 		return -EINVAL;
-#ifndef ARCH_HAS_SG_CHAIN
+#ifndef CONFIG_ARCH_HAS_SG_CHAIN
 	if (WARN_ON_ONCE(nents > max_ents))
 		return -EINVAL;
 #endif

commit c53c6d6a68b13b1dff2892551b56cfdc07887d9e
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Apr 15 14:38:31 2014 +0200

    scatterlist: allow chaining to preallocated chunks
    
    Blk-mq drivers usually preallocate their S/G list as part of the request,
    but if we want to support the very large S/G lists currently supported by
    the SCSI code that would tie up a lot of memory in the preallocated request
    pool.  Add support to the scatterlist code so that it can initialize a
    S/G list that uses a preallocated first chunks and dynamically allocated
    additional chunks.  That way the scsi-mq code can preallocate a first
    page worth of S/G entries as part of the request, and dynamically extend
    the S/G list when needed.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Martin K. Petersen <martin.petersen@oracle.com>
    Reviewed-by: Hannes Reinecke <hare@suse.de>
    Reviewed-by: Webb Scales <webbnh@hp.com>
    Acked-by: Jens Axboe <axboe@kernel.dk>
    Tested-by: Bart Van Assche <bvanassche@acm.org>
    Tested-by: Robert Elliott <elliott@hp.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 3a8e8e8fb2a5..b4415fceb7e7 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -165,6 +165,7 @@ static void sg_kfree(struct scatterlist *sg, unsigned int nents)
  * __sg_free_table - Free a previously mapped sg table
  * @table:	The sg table header to use
  * @max_ents:	The maximum number of entries per single scatterlist
+ * @skip_first_chunk: don't free the (preallocated) first scatterlist chunk
  * @free_fn:	Free function
  *
  *  Description:
@@ -174,7 +175,7 @@ static void sg_kfree(struct scatterlist *sg, unsigned int nents)
  *
  **/
 void __sg_free_table(struct sg_table *table, unsigned int max_ents,
-		     sg_free_fn *free_fn)
+		     bool skip_first_chunk, sg_free_fn *free_fn)
 {
 	struct scatterlist *sgl, *next;
 
@@ -202,7 +203,10 @@ void __sg_free_table(struct sg_table *table, unsigned int max_ents,
 		}
 
 		table->orig_nents -= sg_size;
-		free_fn(sgl, alloc_size);
+		if (!skip_first_chunk) {
+			free_fn(sgl, alloc_size);
+			skip_first_chunk = false;
+		}
 		sgl = next;
 	}
 
@@ -217,7 +221,7 @@ EXPORT_SYMBOL(__sg_free_table);
  **/
 void sg_free_table(struct sg_table *table)
 {
-	__sg_free_table(table, SG_MAX_SINGLE_ALLOC, sg_kfree);
+	__sg_free_table(table, SG_MAX_SINGLE_ALLOC, false, sg_kfree);
 }
 EXPORT_SYMBOL(sg_free_table);
 
@@ -241,8 +245,8 @@ EXPORT_SYMBOL(sg_free_table);
  *
  **/
 int __sg_alloc_table(struct sg_table *table, unsigned int nents,
-		     unsigned int max_ents, gfp_t gfp_mask,
-		     sg_alloc_fn *alloc_fn)
+		     unsigned int max_ents, struct scatterlist *first_chunk,
+		     gfp_t gfp_mask, sg_alloc_fn *alloc_fn)
 {
 	struct scatterlist *sg, *prv;
 	unsigned int left;
@@ -269,7 +273,12 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 
 		left -= sg_size;
 
-		sg = alloc_fn(alloc_size, gfp_mask);
+		if (first_chunk) {
+			sg = first_chunk;
+			first_chunk = NULL;
+		} else {
+			sg = alloc_fn(alloc_size, gfp_mask);
+		}
 		if (unlikely(!sg)) {
 			/*
 			 * Adjust entry count to reflect that the last
@@ -324,9 +333,9 @@ int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
 	int ret;
 
 	ret = __sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,
-			       gfp_mask, sg_kmalloc);
+			       NULL, gfp_mask, sg_kmalloc);
 	if (unlikely(ret))
-		__sg_free_table(table, SG_MAX_SINGLE_ALLOC, sg_kfree);
+		__sg_free_table(table, SG_MAX_SINGLE_ALLOC, false, sg_kfree);
 
 	return ret;
 }

commit 0d6077f8b48ed2dce8f2466a76c0d574a3b4dbe9
Author: Ming Lei <ming.lei@canonical.com>
Date:   Tue Nov 26 12:43:37 2013 +0800

    lib/scatterlist: export sg_miter_skip()
    
    sg_copy_buffer() can't meet demand for some drrivers(such usb
    mass storage), so we have to use the sg_miter_* APIs to access
    sg buffer, then need export sg_miter_skip() for these drivers.
    
    The API is needed for converting to sg_miter_* APIs in USB storage
    driver for accessing sg buffer.
    
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Reviewed-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index d16fa295ae1d..3a8e8e8fb2a5 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -495,7 +495,7 @@ static bool sg_miter_get_next_page(struct sg_mapping_iter *miter)
  *   true if @miter contains the valid mapping.  false if end of sg
  *   list is reached.
  */
-static bool sg_miter_skip(struct sg_mapping_iter *miter, off_t offset)
+bool sg_miter_skip(struct sg_mapping_iter *miter, off_t offset)
 {
 	sg_miter_stop(miter);
 
@@ -513,6 +513,7 @@ static bool sg_miter_skip(struct sg_mapping_iter *miter, off_t offset)
 
 	return true;
 }
+EXPORT_SYMBOL(sg_miter_skip);
 
 /**
  * sg_miter_next - proceed mapping iterator to the next mapping

commit 3d77b50c5874b7e923be946ba793644f82336b75
Author: Ming Lei <ming.lei@canonical.com>
Date:   Thu Oct 31 16:34:17 2013 -0700

    lib/scatterlist.c: don't flush_kernel_dcache_page on slab page
    
    Commit b1adaf65ba03 ("[SCSI] block: add sg buffer copy helper
    functions") introduces two sg buffer copy helpers, and calls
    flush_kernel_dcache_page() on pages in SG list after these pages are
    written to.
    
    Unfortunately, the commit may introduce a potential bug:
    
     - Before sending some SCSI commands, kmalloc() buffer may be passed to
       block layper, so flush_kernel_dcache_page() can see a slab page
       finally
    
     - According to cachetlb.txt, flush_kernel_dcache_page() is only called
       on "a user page", which surely can't be a slab page.
    
     - ARCH's implementation of flush_kernel_dcache_page() may use page
       mapping information to do optimization so page_mapping() will see the
       slab page, then VM_BUG_ON() is triggered.
    
    Aaro Koskinen reported the bug on ARM/kirkwood when DEBUG_VM is enabled,
    and this patch fixes the bug by adding test of '!PageSlab(miter->page)'
    before calling flush_kernel_dcache_page().
    
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Reported-by: Aaro Koskinen <aaro.koskinen@iki.fi>
    Tested-by: Simon Baatz <gmbnomis@gmail.com>
    Cc: Russell King - ARM Linux <linux@arm.linux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Aaro Koskinen <aaro.koskinen@iki.fi>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: <stable@vger.kernel.org>    [3.2+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index a685c8a79578..d16fa295ae1d 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -577,7 +577,8 @@ void sg_miter_stop(struct sg_mapping_iter *miter)
 		miter->__offset += miter->consumed;
 		miter->__remaining -= miter->consumed;
 
-		if (miter->__flags & SG_MITER_TO_SG)
+		if ((miter->__flags & SG_MITER_TO_SG) &&
+		    !PageSlab(miter->page))
 			flush_kernel_dcache_page(miter->page);
 
 		if (miter->__flags & SG_MITER_ATOMIC) {

commit 27daabd9b6a157c34a6e7a7f509fa26866e6420f
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Mon Jul 8 16:01:58 2013 -0700

    lib/scatterlist: error handling in __sg_alloc_table()
    
    I was reviewing code which I suspected might allocate a zero size SG
    table.  That will cause memory corruption.  Also we can't return before
    doing the memset or we could end up using uninitialized memory in the
    cleanup path.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Imre Deak <imre.deak@intel.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Maxim Levitsky <maximlevitsky@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 129a82f707df..a685c8a79578 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -247,13 +247,15 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 	struct scatterlist *sg, *prv;
 	unsigned int left;
 
+	memset(table, 0, sizeof(*table));
+
+	if (nents == 0)
+		return -EINVAL;
 #ifndef ARCH_HAS_SG_CHAIN
 	if (WARN_ON_ONCE(nents > max_ents))
 		return -EINVAL;
 #endif
 
-	memset(table, 0, sizeof(*table));
-
 	left = nents;
 	prv = NULL;
 	do {

commit df642cea25c90dc7d5dcd9d3b480b6b59de7d787
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Mon Jul 8 16:01:54 2013 -0700

    lib/scatterlist: introduce sg_pcopy_from_buffer() and sg_pcopy_to_buffer()
    
    The only difference between sg_pcopy_{from,to}_buffer() and
    sg_copy_{from,to}_buffer() is an additional argument that specifies the
    number of bytes to skip the SG list before copying.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Cc: Douglas Gilbert <dgilbert@interlog.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Horia Geanta <horia.geanta@freescale.com>
    Cc: Imre Deak <imre.deak@intel.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index c35b929435f3..129a82f707df 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -475,6 +475,43 @@ static bool sg_miter_get_next_page(struct sg_mapping_iter *miter)
 	return true;
 }
 
+/**
+ * sg_miter_skip - reposition mapping iterator
+ * @miter: sg mapping iter to be skipped
+ * @offset: number of bytes to plus the current location
+ *
+ * Description:
+ *   Sets the offset of @miter to its current location plus @offset bytes.
+ *   If mapping iterator @miter has been proceeded by sg_miter_next(), this
+ *   stops @miter.
+ *
+ * Context:
+ *   Don't care if @miter is stopped, or not proceeded yet.
+ *   Otherwise, preemption disabled if the SG_MITER_ATOMIC is set.
+ *
+ * Returns:
+ *   true if @miter contains the valid mapping.  false if end of sg
+ *   list is reached.
+ */
+static bool sg_miter_skip(struct sg_mapping_iter *miter, off_t offset)
+{
+	sg_miter_stop(miter);
+
+	while (offset) {
+		off_t consumed;
+
+		if (!sg_miter_get_next_page(miter))
+			return false;
+
+		consumed = min_t(off_t, offset, miter->__remaining);
+		miter->__offset += consumed;
+		miter->__remaining -= consumed;
+		offset -= consumed;
+	}
+
+	return true;
+}
+
 /**
  * sg_miter_next - proceed mapping iterator to the next mapping
  * @miter: sg mapping iter to proceed
@@ -561,14 +598,16 @@ EXPORT_SYMBOL(sg_miter_stop);
  * @nents:		 Number of SG entries
  * @buf:		 Where to copy from
  * @buflen:		 The number of bytes to copy
- * @to_buffer: 		 transfer direction (non zero == from an sg list to a
- * 			 buffer, 0 == from a buffer to an sg list
+ * @skip:		 Number of bytes to skip before copying
+ * @to_buffer:		 transfer direction (true == from an sg list to a
+ *			 buffer, false == from a buffer to an sg list
  *
  * Returns the number of copied bytes.
  *
  **/
 static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
-			     void *buf, size_t buflen, int to_buffer)
+			     void *buf, size_t buflen, off_t skip,
+			     bool to_buffer)
 {
 	unsigned int offset = 0;
 	struct sg_mapping_iter miter;
@@ -582,6 +621,9 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
 
 	sg_miter_start(&miter, sgl, nents, sg_flags);
 
+	if (!sg_miter_skip(&miter, skip))
+		return false;
+
 	local_irq_save(flags);
 
 	while (sg_miter_next(&miter) && offset < buflen) {
@@ -616,7 +658,7 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
 size_t sg_copy_from_buffer(struct scatterlist *sgl, unsigned int nents,
 			   void *buf, size_t buflen)
 {
-	return sg_copy_buffer(sgl, nents, buf, buflen, 0);
+	return sg_copy_buffer(sgl, nents, buf, buflen, 0, false);
 }
 EXPORT_SYMBOL(sg_copy_from_buffer);
 
@@ -633,6 +675,42 @@ EXPORT_SYMBOL(sg_copy_from_buffer);
 size_t sg_copy_to_buffer(struct scatterlist *sgl, unsigned int nents,
 			 void *buf, size_t buflen)
 {
-	return sg_copy_buffer(sgl, nents, buf, buflen, 1);
+	return sg_copy_buffer(sgl, nents, buf, buflen, 0, true);
 }
 EXPORT_SYMBOL(sg_copy_to_buffer);
+
+/**
+ * sg_pcopy_from_buffer - Copy from a linear buffer to an SG list
+ * @sgl:		 The SG list
+ * @nents:		 Number of SG entries
+ * @buf:		 Where to copy from
+ * @skip:		 Number of bytes to skip before copying
+ * @buflen:		 The number of bytes to copy
+ *
+ * Returns the number of copied bytes.
+ *
+ **/
+size_t sg_pcopy_from_buffer(struct scatterlist *sgl, unsigned int nents,
+			    void *buf, size_t buflen, off_t skip)
+{
+	return sg_copy_buffer(sgl, nents, buf, buflen, skip, false);
+}
+EXPORT_SYMBOL(sg_pcopy_from_buffer);
+
+/**
+ * sg_pcopy_to_buffer - Copy from an SG list to a linear buffer
+ * @sgl:		 The SG list
+ * @nents:		 Number of SG entries
+ * @buf:		 Where to copy to
+ * @skip:		 Number of bytes to skip before copying
+ * @buflen:		 The number of bytes to copy
+ *
+ * Returns the number of copied bytes.
+ *
+ **/
+size_t sg_pcopy_to_buffer(struct scatterlist *sgl, unsigned int nents,
+			  void *buf, size_t buflen, off_t skip)
+{
+	return sg_copy_buffer(sgl, nents, buf, buflen, skip, true);
+}
+EXPORT_SYMBOL(sg_pcopy_to_buffer);

commit 1105200480b4faeb673d1b23658650b003302c06
Author: Akinobu Mita <akinobu.mita@gmail.com>
Date:   Mon Jul 8 16:01:52 2013 -0700

    lib/scatterlist: factor out sg_miter_get_next_page() from sg_miter_next()
    
    This patchset introduces sg_pcopy_from_buffer() and sg_pcopy_to_buffer(),
    which copy data between a linear buffer and an SG list.
    
    The only difference between sg_pcopy_{from,to}_buffer() and
    sg_copy_{from,to}_buffer() is an additional argument that specifies the
    number of bytes to skip the SG list before copying.
    
    The main reason for introducing these functions is to fix a problem in
    scsi_debug module.  And there is a local function in crypto/talitos
    module, which can be replaced by sg_pcopy_to_buffer().
    
    This patch:
    
    sg_miter_get_next_page() is used to proceed page iterator to the next page
    if necessary, and will be used to implement the variants of
    sg_copy_{from,to}_buffer() later.
    
    Signed-off-by: Akinobu Mita <akinobu.mita@gmail.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Imre Deak <imre.deak@intel.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "James E.J. Bottomley" <JBottomley@parallels.com>
    Cc: Douglas Gilbert <dgilbert@interlog.com>
    Cc: Horia Geanta <horia.geanta@freescale.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index a1cf8cae60e7..c35b929435f3 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -453,6 +453,28 @@ void sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,
 }
 EXPORT_SYMBOL(sg_miter_start);
 
+static bool sg_miter_get_next_page(struct sg_mapping_iter *miter)
+{
+	if (!miter->__remaining) {
+		struct scatterlist *sg;
+		unsigned long pgoffset;
+
+		if (!__sg_page_iter_next(&miter->piter))
+			return false;
+
+		sg = miter->piter.sg;
+		pgoffset = miter->piter.sg_pgoffset;
+
+		miter->__offset = pgoffset ? 0 : sg->offset;
+		miter->__remaining = sg->offset + sg->length -
+				(pgoffset << PAGE_SHIFT) - miter->__offset;
+		miter->__remaining = min_t(unsigned long, miter->__remaining,
+					   PAGE_SIZE - miter->__offset);
+	}
+
+	return true;
+}
+
 /**
  * sg_miter_next - proceed mapping iterator to the next mapping
  * @miter: sg mapping iter to proceed
@@ -478,22 +500,9 @@ bool sg_miter_next(struct sg_mapping_iter *miter)
 	 * Get to the next page if necessary.
 	 * __remaining, __offset is adjusted by sg_miter_stop
 	 */
-	if (!miter->__remaining) {
-		struct scatterlist *sg;
-		unsigned long pgoffset;
-
-		if (!__sg_page_iter_next(&miter->piter))
-			return false;
-
-		sg = miter->piter.sg;
-		pgoffset = miter->piter.sg_pgoffset;
+	if (!sg_miter_get_next_page(miter))
+		return false;
 
-		miter->__offset = pgoffset ? 0 : sg->offset;
-		miter->__remaining = sg->offset + sg->length -
-				(pgoffset << PAGE_SHIFT) - miter->__offset;
-		miter->__remaining = min_t(unsigned long, miter->__remaining,
-					   PAGE_SIZE - miter->__offset);
-	}
 	miter->page = sg_page_iter_page(&miter->piter);
 	miter->consumed = miter->length = miter->__remaining;
 

commit 2db76d7c3c6db93058f983c8240f7c7c25e87ee6
Author: Imre Deak <imre.deak@intel.com>
Date:   Tue Mar 26 15:14:18 2013 +0200

    lib/scatterlist: sg_page_iter: support sg lists w/o backing pages
    
    The i915 driver uses sg lists for memory without backing 'struct page'
    pages, similarly to other IO memory regions, setting only the DMA
    address for these. It does this, so that it can program the HW MMU
    tables in a uniform way both for sg lists with and without backing pages.
    
    Without a valid page pointer we can't call nth_page to get the current
    page in __sg_page_iter_next, so add a helper that relevant users can
    call separately. Also add a helper to get the DMA address of the current
    page (idea from Daniel).
    
    Convert all places in i915, to use the new API.
    
    Signed-off-by: Imre Deak <imre.deak@intel.com>
    Reviewed-by: Damien Lespiau <damien.lespiau@intel.com>
    Signed-off-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index b83c144d731f..a1cf8cae60e7 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -401,7 +401,6 @@ void __sg_page_iter_start(struct sg_page_iter *piter,
 	piter->__pg_advance = 0;
 	piter->__nents = nents;
 
-	piter->page = NULL;
 	piter->sg = sglist;
 	piter->sg_pgoffset = pgoffset;
 }
@@ -426,7 +425,6 @@ bool __sg_page_iter_next(struct sg_page_iter *piter)
 		if (!--piter->__nents || !piter->sg)
 			return false;
 	}
-	piter->page = nth_page(sg_page(piter->sg), piter->sg_pgoffset);
 
 	return true;
 }
@@ -496,7 +494,7 @@ bool sg_miter_next(struct sg_mapping_iter *miter)
 		miter->__remaining = min_t(unsigned long, miter->__remaining,
 					   PAGE_SIZE - miter->__offset);
 	}
-	miter->page = miter->piter.page;
+	miter->page = sg_page_iter_page(&miter->piter);
 	miter->consumed = miter->length = miter->__remaining;
 
 	if (miter->__flags & SG_MITER_ATOMIC)

commit 4225fc8555a992c7f91d174ef424384d6781e144
Author: Imre Deak <imre.deak@intel.com>
Date:   Wed Feb 27 17:02:57 2013 -0800

    lib/scatterlist: use page iterator in the mapping iterator
    
    For better code reuse use the newly added page iterator to iterate
    through the pages.  The offset, length within the page is still
    calculated by the mapping iterator as well as the actual mapping.  Idea
    from Tejun Heo.
    
    Signed-off-by: Imre Deak <imre.deak@intel.com>
    Cc: Maxim Levitsky <maximlevitsky@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Stephen Warren <swarren@wwwdotorg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index a1d15647d7db..b83c144d731f 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -449,9 +449,7 @@ void sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,
 {
 	memset(miter, 0, sizeof(struct sg_mapping_iter));
 
-	miter->__sg = sgl;
-	miter->__nents = nents;
-	miter->__offset = 0;
+	__sg_page_iter_start(&miter->piter, sgl, nents, 0);
 	WARN_ON(!(flags & (SG_MITER_TO_SG | SG_MITER_FROM_SG)));
 	miter->__flags = flags;
 }
@@ -476,36 +474,35 @@ EXPORT_SYMBOL(sg_miter_start);
  */
 bool sg_miter_next(struct sg_mapping_iter *miter)
 {
-	unsigned int off, len;
-
-	/* check for end and drop resources from the last iteration */
-	if (!miter->__nents)
-		return false;
-
 	sg_miter_stop(miter);
 
-	/* get to the next sg if necessary.  __offset is adjusted by stop */
-	while (miter->__offset == miter->__sg->length) {
-		if (--miter->__nents) {
-			miter->__sg = sg_next(miter->__sg);
-			miter->__offset = 0;
-		} else
+	/*
+	 * Get to the next page if necessary.
+	 * __remaining, __offset is adjusted by sg_miter_stop
+	 */
+	if (!miter->__remaining) {
+		struct scatterlist *sg;
+		unsigned long pgoffset;
+
+		if (!__sg_page_iter_next(&miter->piter))
 			return false;
-	}
 
-	/* map the next page */
-	off = miter->__sg->offset + miter->__offset;
-	len = miter->__sg->length - miter->__offset;
+		sg = miter->piter.sg;
+		pgoffset = miter->piter.sg_pgoffset;
 
-	miter->page = nth_page(sg_page(miter->__sg), off >> PAGE_SHIFT);
-	off &= ~PAGE_MASK;
-	miter->length = min_t(unsigned int, len, PAGE_SIZE - off);
-	miter->consumed = miter->length;
+		miter->__offset = pgoffset ? 0 : sg->offset;
+		miter->__remaining = sg->offset + sg->length -
+				(pgoffset << PAGE_SHIFT) - miter->__offset;
+		miter->__remaining = min_t(unsigned long, miter->__remaining,
+					   PAGE_SIZE - miter->__offset);
+	}
+	miter->page = miter->piter.page;
+	miter->consumed = miter->length = miter->__remaining;
 
 	if (miter->__flags & SG_MITER_ATOMIC)
-		miter->addr = kmap_atomic(miter->page) + off;
+		miter->addr = kmap_atomic(miter->page) + miter->__offset;
 	else
-		miter->addr = kmap(miter->page) + off;
+		miter->addr = kmap(miter->page) + miter->__offset;
 
 	return true;
 }
@@ -532,6 +529,7 @@ void sg_miter_stop(struct sg_mapping_iter *miter)
 	/* drop resources from the last iteration */
 	if (miter->addr) {
 		miter->__offset += miter->consumed;
+		miter->__remaining -= miter->consumed;
 
 		if (miter->__flags & SG_MITER_TO_SG)
 			flush_kernel_dcache_page(miter->page);

commit a321e91b6d73ed011ffceed384c40d2785cf723b
Author: Imre Deak <imre.deak@intel.com>
Date:   Wed Feb 27 17:02:56 2013 -0800

    lib/scatterlist: add simple page iterator
    
    Add an iterator to walk through a scatter list a page at a time starting
    at a specific page offset.  As opposed to the mapping iterator this is
    meant to be small, performing well even in simple loops like collecting
    all pages on the scatterlist into an array or setting up an iommu table
    based on the pages' DMA address.
    
    Signed-off-by: Imre Deak <imre.deak@intel.com>
    Cc: Maxim Levitsky <maximlevitsky@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Tested-by: Stephen Warren <swarren@wwwdotorg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 7874b01e816e..a1d15647d7db 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -394,6 +394,44 @@ int sg_alloc_table_from_pages(struct sg_table *sgt,
 }
 EXPORT_SYMBOL(sg_alloc_table_from_pages);
 
+void __sg_page_iter_start(struct sg_page_iter *piter,
+			  struct scatterlist *sglist, unsigned int nents,
+			  unsigned long pgoffset)
+{
+	piter->__pg_advance = 0;
+	piter->__nents = nents;
+
+	piter->page = NULL;
+	piter->sg = sglist;
+	piter->sg_pgoffset = pgoffset;
+}
+EXPORT_SYMBOL(__sg_page_iter_start);
+
+static int sg_page_count(struct scatterlist *sg)
+{
+	return PAGE_ALIGN(sg->offset + sg->length) >> PAGE_SHIFT;
+}
+
+bool __sg_page_iter_next(struct sg_page_iter *piter)
+{
+	if (!piter->__nents || !piter->sg)
+		return false;
+
+	piter->sg_pgoffset += piter->__pg_advance;
+	piter->__pg_advance = 1;
+
+	while (piter->sg_pgoffset >= sg_page_count(piter->sg)) {
+		piter->sg_pgoffset -= sg_page_count(piter->sg);
+		piter->sg = sg_next(piter->sg);
+		if (!--piter->__nents || !piter->sg)
+			return false;
+	}
+	piter->page = nth_page(sg_page(piter->sg), piter->sg_pgoffset);
+
+	return true;
+}
+EXPORT_SYMBOL(__sg_page_iter_next);
+
 /**
  * sg_miter_start - start mapping iteration over a sg list
  * @miter: sg mapping iter to be started

commit 6fd59a83b9261fa53eaf98fb5514abba504a3ea3
Author: Nick Bowler <nbowler@elliptictech.com>
Date:   Mon Dec 17 16:05:20 2012 -0800

    scatterlist: don't BUG when we can trivially return a proper error.
    
    There is absolutely no reason to crash the kernel when we have a
    perfectly good return value already available to use for conveying
    failure status.
    
    Let's return an error code instead of crashing the kernel: that sounds
    like a much better plan.
    
    [akpm@linux-foundation.org: s/E2BIG/EINVAL/]
    Signed-off-by: Nick Bowler <nbowler@elliptictech.com>
    Cc: Maxim Levitsky <maximlevitsky@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 3675452b23ca..7874b01e816e 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -248,7 +248,8 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 	unsigned int left;
 
 #ifndef ARCH_HAS_SG_CHAIN
-	BUG_ON(nents > max_ents);
+	if (WARN_ON_ONCE(nents > max_ents))
+		return -EINVAL;
 #endif
 
 	memset(table, 0, sizeof(*table));

commit ce40be7a820bb393ac4ac69865f018d2f4038cf0
Merge: ba0a5a36f60e 02f3939e1a93
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 11 09:04:23 2012 +0900

    Merge branch 'for-3.7/core' of git://git.kernel.dk/linux-block
    
    Pull block IO update from Jens Axboe:
     "Core block IO bits for 3.7.  Not a huge round this time, it contains:
    
       - First series from Kent cleaning up and generalizing bio allocation
         and freeing.
    
       - WRITE_SAME support from Martin.
    
       - Mikulas patches to prevent O_DIRECT crashes when someone changes
         the block size of a device.
    
       - Make bio_split() work on data-less bio's (like trim/discards).
    
       - A few other minor fixups."
    
    Fixed up silent semantic mis-merge as per Mikulas Patocka and Andrew
    Morton.  It is due to the VM no longer using a prio-tree (see commit
    6b2dbba8b6ac: "mm: replace vma prio_tree with an interval tree").
    
    So make set_blocksize() use mapping_mapped() instead of open-coding the
    internal VM knowledge that has changed.
    
    * 'for-3.7/core' of git://git.kernel.dk/linux-block: (26 commits)
      block: makes bio_split support bio without data
      scatterlist: refactor the sg_nents
      scatterlist: add sg_nents
      fs: fix include/percpu-rwsem.h export error
      percpu-rw-semaphore: fix documentation typos
      fs/block_dev.c:1644:5: sparse: symbol 'blkdev_mmap' was not declared
      blockdev: turn a rw semaphore into a percpu rw semaphore
      Fix a crash when block device is read and block size is changed at the same time
      block: fix request_queue->flags initialization
      block: lift the initial queue bypass mode on blk_register_queue() instead of blk_init_allocated_queue()
      block: ioctl to zero block ranges
      block: Make blkdev_issue_zeroout use WRITE SAME
      block: Implement support for WRITE SAME
      block: Consolidate command flag and queue limit checks for merges
      block: Clean up special command handling logic
      block/blk-tag.c: Remove useless kfree
      block: remove the duplicated setting for congestion_threshold
      block: reject invalid queue attribute values
      block: Add bio_clone_bioset(), bio_clone_kmalloc()
      block: Consolidate bio_alloc_bioset(), bio_kmalloc()
      ...

commit 8290e2d2dcbf0d379d4b1379e17916515ee20a39
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Oct 4 17:13:28 2012 -0700

    scatterlist: atomic sg_mapping_iter() no longer needs disabled IRQs
    
    SG mapping iterator w/ SG_MITER_ATOMIC set required IRQ disabled because
    it originally used KM_BIO_SRC_IRQ to allow use from IRQ handlers.
    kmap_atomic() has long been updated to handle stacking atomic mapping
    requests on per-cpu basis and only requires not sleeping while mapped.
    
    Update sg_mapping_iter such that atomic iterators only require disabling
    preemption instead of disabling IRQ.
    
    While at it, convert wte weird @ARG@ notations to @ARG in the comment of
    sg_miter_start().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Maxim Levitsky <maximlevitsky@gmail.com>
    Cc: Alex Dubov <oakad@yahoo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index fadae774a20c..e76d85cf3175 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -404,14 +404,13 @@ EXPORT_SYMBOL(sg_miter_start);
  * @miter: sg mapping iter to proceed
  *
  * Description:
- *   Proceeds @miter@ to the next mapping.  @miter@ should have been
- *   started using sg_miter_start().  On successful return,
- *   @miter@->page, @miter@->addr and @miter@->length point to the
- *   current mapping.
+ *   Proceeds @miter to the next mapping.  @miter should have been started
+ *   using sg_miter_start().  On successful return, @miter->page,
+ *   @miter->addr and @miter->length point to the current mapping.
  *
  * Context:
- *   IRQ disabled if SG_MITER_ATOMIC.  IRQ must stay disabled till
- *   @miter@ is stopped.  May sleep if !SG_MITER_ATOMIC.
+ *   Preemption disabled if SG_MITER_ATOMIC.  Preemption must stay disabled
+ *   till @miter is stopped.  May sleep if !SG_MITER_ATOMIC.
  *
  * Returns:
  *   true if @miter contains the next mapping.  false if end of sg
@@ -465,7 +464,8 @@ EXPORT_SYMBOL(sg_miter_next);
  *   resources (kmap) need to be released during iteration.
  *
  * Context:
- *   IRQ disabled if the SG_MITER_ATOMIC is set.  Don't care otherwise.
+ *   Preemption disabled if the SG_MITER_ATOMIC is set.  Don't care
+ *   otherwise.
  */
 void sg_miter_stop(struct sg_mapping_iter *miter)
 {
@@ -479,7 +479,7 @@ void sg_miter_stop(struct sg_mapping_iter *miter)
 			flush_kernel_dcache_page(miter->page);
 
 		if (miter->__flags & SG_MITER_ATOMIC) {
-			WARN_ON(!irqs_disabled());
+			WARN_ON_ONCE(preemptible());
 			kunmap_atomic(miter->addr);
 		} else
 			kunmap(miter->page);

commit 232f1b51062553b7cf49f99719fbd1b8a8d80f29
Author: Maxim Levitsky <maximlevitsky@gmail.com>
Date:   Fri Sep 28 10:38:15 2012 +0200

    scatterlist: refactor the sg_nents
    
    Replace 'while' with 'for' as suggested by Tejun Heo
    
    Signed-off-by: Maxim Levitsky <maximlevitsky@gmail.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 1bf60efb5e02..8cd2ced68f36 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -49,12 +49,9 @@ EXPORT_SYMBOL(sg_next);
  **/
 int sg_nents(struct scatterlist *sg)
 {
-	int nents = 0;
-	while (sg) {
+	int nents;
+	for (nents = 0; sg; sg = sg_next(sg))
 		nents++;
-		sg = sg_next(sg);
-	}
-
 	return nents;
 }
 EXPORT_SYMBOL(sg_nents);

commit 2e484610296b25f0a04b516bc144a00731d1d845
Author: Maxim Levitsky <maximlevitsky@gmail.com>
Date:   Thu Sep 27 12:45:28 2012 +0200

    scatterlist: add sg_nents
    
    Useful helper to know the number of entries in scatterlist.
    
    Signed-off-by: Maxim Levitsky <maximlevitsky@gmail.com>
    Cc: Alex Dubov <oakad@yahoo.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index fadae774a20c..1bf60efb5e02 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -38,6 +38,28 @@ struct scatterlist *sg_next(struct scatterlist *sg)
 }
 EXPORT_SYMBOL(sg_next);
 
+/**
+ * sg_nents - return total count of entries in scatterlist
+ * @sg:		The scatterlist
+ *
+ * Description:
+ * Allows to know how many entries are in sg, taking into acount
+ * chaining as well
+ *
+ **/
+int sg_nents(struct scatterlist *sg)
+{
+	int nents = 0;
+	while (sg) {
+		nents++;
+		sg = sg_next(sg);
+	}
+
+	return nents;
+}
+EXPORT_SYMBOL(sg_nents);
+
+
 /**
  * sg_last - return the last scatterlist entry in a list
  * @sgl:	First entry in the scatterlist

commit 27c1ee3f929555b71fa39ec0d81a7e7185de1b16
Merge: 37cd9600a9e2 086ff4b3a7fb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 30 17:25:34 2012 -0700

    Merge branch 'akpm' (Andrew's patch-bomb)
    
    Merge Andrew's first set of patches:
     "Non-MM patches:
    
       - lots of misc bits
    
       - tree-wide have_clk() cleanups
    
       - quite a lot of printk tweaks.  I draw your attention to "printk:
         convert the format for KERN_<LEVEL> to a 2 byte pattern" which
         looks a bit scary.  But afaict it's solid.
    
       - backlight updates
    
       - lib/ feature work (notably the addition and use of memweight())
    
       - checkpatch updates
    
       - rtc updates
    
       - nilfs updates
    
       - fatfs updates (partial, still waiting for acks)
    
       - kdump, proc, fork, IPC, sysctl, taskstats, pps, etc
    
       - new fault-injection feature work"
    
    * Merge emailed patches from Andrew Morton <akpm@linux-foundation.org>: (128 commits)
      drivers/misc/lkdtm.c: fix missing allocation failure check
      lib/scatterlist: do not re-write gfp_flags in __sg_alloc_table()
      fault-injection: add tool to run command with failslab or fail_page_alloc
      fault-injection: add selftests for cpu and memory hotplug
      powerpc: pSeries reconfig notifier error injection module
      memory: memory notifier error injection module
      PM: PM notifier error injection module
      cpu: rewrite cpu-notifier-error-inject module
      fault-injection: notifier error injection
      c/r: fcntl: add F_GETOWNER_UIDS option
      resource: make sure requested range is included in the root range
      include/linux/aio.h: cpp->C conversions
      fs: cachefiles: add support for large files in filesystem caching
      pps: return PTR_ERR on error in device_create
      taskstats: check nla_reserve() return
      sysctl: suppress kmemleak messages
      ipc: use Kconfig options for __ARCH_WANT_[COMPAT_]IPC_PARSE_VERSION
      ipc: compat: use signed size_t types for msgsnd and msgrcv
      ipc: allow compat IPC version field parsing if !ARCH_WANT_OLD_COMPAT_IPC
      ipc: add COMPAT_SHMLBA support
      ...

commit e04f228335bde305d2833e40382ed37fc36f9efe
Author: Mandeep Singh Baines <msb@chromium.org>
Date:   Mon Jul 30 14:43:22 2012 -0700

    lib/scatterlist: do not re-write gfp_flags in __sg_alloc_table()
    
    We are seeing a lot of sg_alloc_table allocation failures using the new
    drm prime infrastructure.  We isolated the cause to code in
    __sg_alloc_table that was re-writing the gfp_flags.
    
    There is a comment in the code that suggest that there is an assumption
    about the allocation coming from a memory pool.  This was likely true
    when sg lists were primarily used for disk I/O.
    
    Signed-off-by: Mandeep Singh Baines <msb@chromium.org>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Cong Wang <amwang@redhat.com>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Rob Clark <rob.clark@linaro.org>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Inki Dae <inki.dae@samsung.com>
    Cc: Dave Airlie <airlied@redhat.com>
    Cc: Sonny Rao <sonnyrao@chromium.org>
    Cc: Olof Johansson <olofj@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 6096e89bee55..d09bdd8b40ce 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -279,14 +279,6 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 		if (!left)
 			sg_mark_end(&sg[sg_size - 1]);
 
-		/*
-		 * only really needed for mempool backed sg allocations (like
-		 * SCSI), a possible improvement here would be to pass the
-		 * table pointer into the allocator and let that clear these
-		 * flags
-		 */
-		gfp_mask &= ~__GFP_WAIT;
-		gfp_mask |= __GFP_HIGH;
 		prv = sg;
 	} while (left);
 

commit efc42bc98058a36d761b16a114823db1a902ed05
Author: Tomasz Stanislawski <t.stanislaws@samsung.com>
Date:   Mon Jun 18 09:25:01 2012 +0200

    scatterlist: add sg_alloc_table_from_pages function
    
    This patch adds a new constructor for an sg table. The table is constructed
    from an array of struct pages. All contiguous chunks of the pages are merged
    into a single sg nodes. A user may provide an offset and a size of a buffer if
    the buffer is not page-aligned.
    
    The function is dedicated for DMABUF exporters which often perform conversion
    from an page array to a scatterlist. Moreover the scatterlist should be
    squashed in order to save memory and to speed-up the process of DMA mapping
    using dma_map_sg.
    
    The code is based on the patch 'v4l: vb2-dma-contig: add support for
    scatterlist in userptr mode' and hints from Laurent Pinchart.
    
    Signed-off-by: Tomasz Stanislawski <t.stanislaws@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch>
    Acked-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    CC: Andrew Morton <akpm@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 6096e89bee55..e719adf695bf 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -318,6 +318,70 @@ int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
 }
 EXPORT_SYMBOL(sg_alloc_table);
 
+/**
+ * sg_alloc_table_from_pages - Allocate and initialize an sg table from
+ *			       an array of pages
+ * @sgt:	The sg table header to use
+ * @pages:	Pointer to an array of page pointers
+ * @n_pages:	Number of pages in the pages array
+ * @offset:     Offset from start of the first page to the start of a buffer
+ * @size:       Number of valid bytes in the buffer (after offset)
+ * @gfp_mask:	GFP allocation mask
+ *
+ *  Description:
+ *    Allocate and initialize an sg table from a list of pages. Contiguous
+ *    ranges of the pages are squashed into a single scatterlist node. A user
+ *    may provide an offset at a start and a size of valid data in a buffer
+ *    specified by the page array. The returned sg table is released by
+ *    sg_free_table.
+ *
+ * Returns:
+ *   0 on success, negative error on failure
+ */
+int sg_alloc_table_from_pages(struct sg_table *sgt,
+	struct page **pages, unsigned int n_pages,
+	unsigned long offset, unsigned long size,
+	gfp_t gfp_mask)
+{
+	unsigned int chunks;
+	unsigned int i;
+	unsigned int cur_page;
+	int ret;
+	struct scatterlist *s;
+
+	/* compute number of contiguous chunks */
+	chunks = 1;
+	for (i = 1; i < n_pages; ++i)
+		if (page_to_pfn(pages[i]) != page_to_pfn(pages[i - 1]) + 1)
+			++chunks;
+
+	ret = sg_alloc_table(sgt, chunks, gfp_mask);
+	if (unlikely(ret))
+		return ret;
+
+	/* merging chunks and putting them into the scatterlist */
+	cur_page = 0;
+	for_each_sg(sgt->sgl, s, sgt->orig_nents, i) {
+		unsigned long chunk_size;
+		unsigned int j;
+
+		/* look for the end of the current chunk */
+		for (j = cur_page + 1; j < n_pages; ++j)
+			if (page_to_pfn(pages[j]) !=
+			    page_to_pfn(pages[j - 1]) + 1)
+				break;
+
+		chunk_size = ((j - cur_page) << PAGE_SHIFT) - offset;
+		sg_set_page(s, pages[cur_page], min(size, chunk_size), offset);
+		size -= chunk_size;
+		offset = 0;
+		cur_page = j;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(sg_alloc_table_from_pages);
+
 /**
  * sg_miter_start - start mapping iteration over a sg list
  * @miter: sg mapping iter to be started

commit 11bcb32848ddb5ab28f09f142b625e2ba4d55c4c
Merge: ed2d265d1266 8bc3bcc93a2b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 24 10:24:31 2012 -0700

    Merge tag 'module-for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    Pull cleanup of fs/ and lib/ users of module.h from Paul Gortmaker:
     "Fix up files in fs/ and lib/ dirs to only use module.h if they really
      need it.
    
      These are trivial in scope vs the work done previously.  We now have
      things where any few remaining cleanups can be farmed out to arch or
      subsystem maintainers, and I have done so when possible.  What is
      remaining here represents the bits that don't clearly lie within a
      single arch/subsystem boundary, like the fs dir and the lib dir.
    
      Some duplicate includes arising from overlapping fixes from
      independent subsystem maintainer submissions are also quashed."
    
    Fix up trivial conflicts due to clashes with other include file cleanups
    (including some due to the previous bug.h cleanup pull).
    
    * tag 'module-for-3.4' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux:
      lib: reduce the use of module.h wherever possible
      fs: reduce the use of module.h wherever possible
      includecheck: delete any duplicate instances of module.h

commit c3eede8e0a1292d95c051cf947738687b9c42322
Author: Cong Wang <amwang@redhat.com>
Date:   Fri Nov 25 23:14:39 2011 +0800

    lib: remove the second argument of k[un]map_atomic()
    
    Signed-off-by: Cong Wang <amwang@redhat.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 4ceb05d772ae..33b2cbb97380 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -390,7 +390,7 @@ bool sg_miter_next(struct sg_mapping_iter *miter)
 	miter->consumed = miter->length;
 
 	if (miter->__flags & SG_MITER_ATOMIC)
-		miter->addr = kmap_atomic(miter->page, KM_BIO_SRC_IRQ) + off;
+		miter->addr = kmap_atomic(miter->page) + off;
 	else
 		miter->addr = kmap(miter->page) + off;
 
@@ -424,7 +424,7 @@ void sg_miter_stop(struct sg_mapping_iter *miter)
 
 		if (miter->__flags & SG_MITER_ATOMIC) {
 			WARN_ON(!irqs_disabled());
-			kunmap_atomic(miter->addr, KM_BIO_SRC_IRQ);
+			kunmap_atomic(miter->addr);
 		} else
 			kunmap(miter->page);
 

commit 8bc3bcc93a2b4e47d5d410146f6546bca6171663
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Nov 16 21:29:17 2011 -0500

    lib: reduce the use of module.h wherever possible
    
    For files only using THIS_MODULE and/or EXPORT_SYMBOL, map
    them onto including export.h -- or if the file isn't even
    using those, then just delete the include.  Fix up any implicit
    include dependencies that were being masked by module.h along
    the way.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 4ceb05d772ae..63a7f2ebbd01 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -6,7 +6,7 @@
  * This source code is licensed under the GNU General Public License,
  * Version 2. See the file COPYING for more details.
  */
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/slab.h>
 #include <linux/scatterlist.h>
 #include <linux/highmem.h>

commit edce6820a9fdda85521211cb334a183e34cc455e
Author: Jeffrey Carlyle <jeff.carlyle@motorola.com>
Date:   Mon Aug 30 19:55:09 2010 +0200

    scatterlist: prevent invalid free when alloc fails
    
    When alloc fails, free_table is being called. Depending on the number of
    bytes requested, we determine if we are going to call _get_free_page()
    or kmalloc(). When alloc fails, our math is wrong (due to sg_size - 1),
    and the last buffer is wrongfully assumed to have been allocated by
    kmalloc. Hence, kfree gets called and a panic occurs.
    
    Signed-off-by: Jeffrey Carlyle <jeff.carlyle@motorola.com>
    Signed-off-by: Olusanya Soyannwo <c23746@motorola.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jaxboe@fusionio.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index a5ec42868f99..4ceb05d772ae 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -248,8 +248,18 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents,
 		left -= sg_size;
 
 		sg = alloc_fn(alloc_size, gfp_mask);
-		if (unlikely(!sg))
-			return -ENOMEM;
+		if (unlikely(!sg)) {
+			/*
+			 * Adjust entry count to reflect that the last
+			 * entry of the previous table won't be used for
+			 * linkage.  Without this, sg_kfree() may get
+			 * confused.
+			 */
+			if (prv)
+				table->nents = ++table->orig_nents;
+
+ 			return -ENOMEM;
+		}
 
 		sg_init_table(sg, alloc_size);
 		table->nents = table->orig_nents += sg_size;

commit b94de9bb7519f597a3aed521d5eaeb5b02a7cbc0
Author: Chris Wilson <chris@chris-wilson.co.uk>
Date:   Wed Jul 28 22:59:02 2010 +0100

    lib/scatterlist: Hook sg_kmalloc into kmemleak (v2)
    
    kmemleak ignores page_alloc() and so believes the final sub-page
    allocation using the plain kmalloc is decoupled and lost. This leads to
    lots of false-positives with code that uses scatterlists.
    
    The options seem to be either to tell kmemleak that the kmalloc is not
    leaked or to notify kmemleak of the page allocations. The danger of the
    first approach is that we may hide a real leak, so choose the latter
    approach (of which I am not sure of the downsides).
    
    v2: Added comments on the suggestion of Catalin.
    
    Signed-off-by: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Jens Axboe <jaxboe@fusionio.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 9afa25b52a83..a5ec42868f99 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -10,6 +10,7 @@
 #include <linux/slab.h>
 #include <linux/scatterlist.h>
 #include <linux/highmem.h>
+#include <linux/kmemleak.h>
 
 /**
  * sg_next - return the next scatterlist entry in a list
@@ -115,17 +116,29 @@ EXPORT_SYMBOL(sg_init_one);
  */
 static struct scatterlist *sg_kmalloc(unsigned int nents, gfp_t gfp_mask)
 {
-	if (nents == SG_MAX_SINGLE_ALLOC)
-		return (struct scatterlist *) __get_free_page(gfp_mask);
-	else
+	if (nents == SG_MAX_SINGLE_ALLOC) {
+		/*
+		 * Kmemleak doesn't track page allocations as they are not
+		 * commonly used (in a raw form) for kernel data structures.
+		 * As we chain together a list of pages and then a normal
+		 * kmalloc (tracked by kmemleak), in order to for that last
+		 * allocation not to become decoupled (and thus a
+		 * false-positive) we need to inform kmemleak of all the
+		 * intermediate allocations.
+		 */
+		void *ptr = (void *) __get_free_page(gfp_mask);
+		kmemleak_alloc(ptr, PAGE_SIZE, 1, gfp_mask);
+		return ptr;
+	} else
 		return kmalloc(nents * sizeof(struct scatterlist), gfp_mask);
 }
 
 static void sg_kfree(struct scatterlist *sg, unsigned int nents)
 {
-	if (nents == SG_MAX_SINGLE_ALLOC)
+	if (nents == SG_MAX_SINGLE_ALLOC) {
+		kmemleak_free(sg);
 		free_page((unsigned long) sg);
-	else
+	} else
 		kfree(sg);
 }
 

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 0d475d8167bf..9afa25b52a83 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -7,6 +7,7 @@
  * Version 2. See the file COPYING for more details.
  */
 #include <linux/module.h>
+#include <linux/slab.h>
 #include <linux/scatterlist.h>
 #include <linux/highmem.h>
 

commit 6de7e356faf54aa75de5b624bbce28a5b776dfa8
Author: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
Date:   Thu Jun 18 10:19:12 2009 +0200

    lib/scatterlist: add a flags to signalize mapping direction
    
    sg_miter_start() is currently unaware of the direction of the copy
    process (to or from the scatter list). It is important to know the
    direction because the page has to be flushed in case the data written
    is seen on a different mapping in user land on cache incoherent
    architectures.
    
    Signed-off-by: Sebastian Andrzej Siewior <sebastian@breakpoint.cc>
    Acked-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Pierre Ossman <pierre@ossman.eu>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index a295e404e908..0d475d8167bf 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -314,6 +314,7 @@ void sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,
 	miter->__sg = sgl;
 	miter->__nents = nents;
 	miter->__offset = 0;
+	WARN_ON(!(flags & (SG_MITER_TO_SG | SG_MITER_FROM_SG)));
 	miter->__flags = flags;
 }
 EXPORT_SYMBOL(sg_miter_start);
@@ -394,6 +395,9 @@ void sg_miter_stop(struct sg_mapping_iter *miter)
 	if (miter->addr) {
 		miter->__offset += miter->consumed;
 
+		if (miter->__flags & SG_MITER_TO_SG)
+			flush_kernel_dcache_page(miter->page);
+
 		if (miter->__flags & SG_MITER_ATOMIC) {
 			WARN_ON(!irqs_disabled());
 			kunmap_atomic(miter->addr, KM_BIO_SRC_IRQ);
@@ -426,8 +430,14 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
 	unsigned int offset = 0;
 	struct sg_mapping_iter miter;
 	unsigned long flags;
+	unsigned int sg_flags = SG_MITER_ATOMIC;
+
+	if (to_buffer)
+		sg_flags |= SG_MITER_FROM_SG;
+	else
+		sg_flags |= SG_MITER_TO_SG;
 
-	sg_miter_start(&miter, sgl, nents, SG_MITER_ATOMIC);
+	sg_miter_start(&miter, sgl, nents, sg_flags);
 
 	local_irq_save(flags);
 
@@ -438,10 +448,8 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
 
 		if (to_buffer)
 			memcpy(buf + offset, miter.addr, len);
-		else {
+		else
 			memcpy(miter.addr, buf + offset, len);
-			flush_kernel_dcache_page(miter.page);
-		}
 
 		offset += len;
 	}

commit 23c560a99d78bddf5c251bfa97bce19e4da4b3f3
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Apr 15 22:10:23 2009 +0900

    scatterlist: make sure sg_miter_next() doesn't return 0 sized mappings
    
    Impact: fix not-so-critical but annoying bug
    
    sg_miter_next() returns 0 sized mapping if there is an zero sized sg
    entry in the list or at the end of each iteration.  As the users
    always check the ->length field, this bug shouldn't be critical other
    than causing unnecessary iteration.
    
    Fix it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index b7b449dafbe5..a295e404e908 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -347,9 +347,12 @@ bool sg_miter_next(struct sg_mapping_iter *miter)
 	sg_miter_stop(miter);
 
 	/* get to the next sg if necessary.  __offset is adjusted by stop */
-	if (miter->__offset == miter->__sg->length && --miter->__nents) {
-		miter->__sg = sg_next(miter->__sg);
-		miter->__offset = 0;
+	while (miter->__offset == miter->__sg->length) {
+		if (--miter->__nents) {
+			miter->__sg = sg_next(miter->__sg);
+			miter->__offset = 0;
+		} else
+			return false;
 	}
 
 	/* map the next page */

commit f652c521e0bec2e70cf123f47e80117a7e6ed139
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Wed Nov 19 15:36:19 2008 -0800

    lib/scatterlist.c: fix kunmap() argument in sg_miter_stop()
    
    kunmap() takes as argument the struct page that orginally got kmap()'d,
    however the sg_miter_stop() function passed it the kernel virtual address
    instead, resulting in weird stuff.
    
    Somehow I ended up fixing this bug by accident while looking for a bug in
    the same area.
    
    Reported-by: kerneloops.org
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Cc: <stable@kernel.org>         [2.6.27.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 8d2688ff1352..b7b449dafbe5 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -395,7 +395,7 @@ void sg_miter_stop(struct sg_mapping_iter *miter)
 			WARN_ON(!irqs_disabled());
 			kunmap_atomic(miter->addr, KM_BIO_SRC_IRQ);
 		} else
-			kunmap(miter->addr);
+			kunmap(miter->page);
 
 		miter->page = NULL;
 		miter->addr = NULL;

commit 50bed2e2862a8f3a4f7d683d0d27292e71ef18b9
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Thu Sep 11 18:35:39 2008 +0200

    sg: disable interrupts inside sg_copy_buffer
    
    The callers of sg_copy_buffer must disable interrupts before calling
    it (since it uses kmap_atomic). Some callers use it on
    interrupt-disabled code but some need to take the trouble to disable
    interrupts just for this. No wonder they forget about it and we hit a
    bug like:
    
    http://bugzilla.kernel.org/show_bug.cgi?id=11529
    
    James said that it might be better to disable interrupts inside the
    function rather than risk the callers getting it wrong.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 876ba6d5b670..8d2688ff1352 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -422,9 +422,12 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
 {
 	unsigned int offset = 0;
 	struct sg_mapping_iter miter;
+	unsigned long flags;
 
 	sg_miter_start(&miter, sgl, nents, SG_MITER_ATOMIC);
 
+	local_irq_save(flags);
+
 	while (sg_miter_next(&miter) && offset < buflen) {
 		unsigned int len;
 
@@ -442,6 +445,7 @@ static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
 
 	sg_miter_stop(&miter);
 
+	local_irq_restore(flags);
 	return offset;
 }
 

commit 137d3edb48425f82a6a4226b664f90ed5e42eea5
Author: Tejun Heo <htejun@gmail.com>
Date:   Sat Jul 19 23:03:35 2008 +0900

    sg: reimplement sg mapping iterator
    
    This is alternative implementation of sg content iterator introduced
    by commit 83e7d317... from Pierre Ossman in next-20080716.  As there's
    already an sg iterator which iterates over sg entries themselves, name
    this sg_mapping_iterator.
    
    Slightly edited description from the original implementation follows.
    
    Iteration over a sg list is not that trivial when you take into
    account that memory pages might have to be mapped before being used.
    Unfortunately, that means that some parts of the kernel restrict
    themselves to directly accesible memory just to not have to deal with
    the mess.
    
    This patch adds a simple iterator system that allows any code to
    easily traverse an sg list and not have to deal with all the details.
    The user can decide to consume part of the iteration.  Also, iteration
    can be stopped and resumed later if releasing the kmap between
    iteration steps is necessary.  These features are useful to implement
    piecemeal sg copying for interrupt drive PIO for example.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Pierre Ossman <drzeus@drzeus.cx>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index b80c21100d78..876ba6d5b670 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -294,6 +294,117 @@ int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
 }
 EXPORT_SYMBOL(sg_alloc_table);
 
+/**
+ * sg_miter_start - start mapping iteration over a sg list
+ * @miter: sg mapping iter to be started
+ * @sgl: sg list to iterate over
+ * @nents: number of sg entries
+ *
+ * Description:
+ *   Starts mapping iterator @miter.
+ *
+ * Context:
+ *   Don't care.
+ */
+void sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,
+		    unsigned int nents, unsigned int flags)
+{
+	memset(miter, 0, sizeof(struct sg_mapping_iter));
+
+	miter->__sg = sgl;
+	miter->__nents = nents;
+	miter->__offset = 0;
+	miter->__flags = flags;
+}
+EXPORT_SYMBOL(sg_miter_start);
+
+/**
+ * sg_miter_next - proceed mapping iterator to the next mapping
+ * @miter: sg mapping iter to proceed
+ *
+ * Description:
+ *   Proceeds @miter@ to the next mapping.  @miter@ should have been
+ *   started using sg_miter_start().  On successful return,
+ *   @miter@->page, @miter@->addr and @miter@->length point to the
+ *   current mapping.
+ *
+ * Context:
+ *   IRQ disabled if SG_MITER_ATOMIC.  IRQ must stay disabled till
+ *   @miter@ is stopped.  May sleep if !SG_MITER_ATOMIC.
+ *
+ * Returns:
+ *   true if @miter contains the next mapping.  false if end of sg
+ *   list is reached.
+ */
+bool sg_miter_next(struct sg_mapping_iter *miter)
+{
+	unsigned int off, len;
+
+	/* check for end and drop resources from the last iteration */
+	if (!miter->__nents)
+		return false;
+
+	sg_miter_stop(miter);
+
+	/* get to the next sg if necessary.  __offset is adjusted by stop */
+	if (miter->__offset == miter->__sg->length && --miter->__nents) {
+		miter->__sg = sg_next(miter->__sg);
+		miter->__offset = 0;
+	}
+
+	/* map the next page */
+	off = miter->__sg->offset + miter->__offset;
+	len = miter->__sg->length - miter->__offset;
+
+	miter->page = nth_page(sg_page(miter->__sg), off >> PAGE_SHIFT);
+	off &= ~PAGE_MASK;
+	miter->length = min_t(unsigned int, len, PAGE_SIZE - off);
+	miter->consumed = miter->length;
+
+	if (miter->__flags & SG_MITER_ATOMIC)
+		miter->addr = kmap_atomic(miter->page, KM_BIO_SRC_IRQ) + off;
+	else
+		miter->addr = kmap(miter->page) + off;
+
+	return true;
+}
+EXPORT_SYMBOL(sg_miter_next);
+
+/**
+ * sg_miter_stop - stop mapping iteration
+ * @miter: sg mapping iter to be stopped
+ *
+ * Description:
+ *   Stops mapping iterator @miter.  @miter should have been started
+ *   started using sg_miter_start().  A stopped iteration can be
+ *   resumed by calling sg_miter_next() on it.  This is useful when
+ *   resources (kmap) need to be released during iteration.
+ *
+ * Context:
+ *   IRQ disabled if the SG_MITER_ATOMIC is set.  Don't care otherwise.
+ */
+void sg_miter_stop(struct sg_mapping_iter *miter)
+{
+	WARN_ON(miter->consumed > miter->length);
+
+	/* drop resources from the last iteration */
+	if (miter->addr) {
+		miter->__offset += miter->consumed;
+
+		if (miter->__flags & SG_MITER_ATOMIC) {
+			WARN_ON(!irqs_disabled());
+			kunmap_atomic(miter->addr, KM_BIO_SRC_IRQ);
+		} else
+			kunmap(miter->addr);
+
+		miter->page = NULL;
+		miter->addr = NULL;
+		miter->length = 0;
+		miter->consumed = 0;
+	}
+}
+EXPORT_SYMBOL(sg_miter_stop);
+
 /**
  * sg_copy_buffer - Copy data between a linear buffer and an SG list
  * @sgl:		 The SG list
@@ -309,56 +420,29 @@ EXPORT_SYMBOL(sg_alloc_table);
 static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
 			     void *buf, size_t buflen, int to_buffer)
 {
-	struct scatterlist *sg;
-	size_t buf_off = 0;
-	int i;
-
-	WARN_ON(!irqs_disabled());
-
-	for_each_sg(sgl, sg, nents, i) {
-		struct page *page;
-		int n = 0;
-		unsigned int sg_off = sg->offset;
-		unsigned int sg_copy = sg->length;
-
-		if (sg_copy > buflen)
-			sg_copy = buflen;
-		buflen -= sg_copy;
-
-		while (sg_copy > 0) {
-			unsigned int page_copy;
-			void *p;
-
-			page_copy = PAGE_SIZE - sg_off;
-			if (page_copy > sg_copy)
-				page_copy = sg_copy;
-
-			page = nth_page(sg_page(sg), n);
-			p = kmap_atomic(page, KM_BIO_SRC_IRQ);
-
-			if (to_buffer)
-				memcpy(buf + buf_off, p + sg_off, page_copy);
-			else {
-				memcpy(p + sg_off, buf + buf_off, page_copy);
-				flush_kernel_dcache_page(page);
-			}
-
-			kunmap_atomic(p, KM_BIO_SRC_IRQ);
-
-			buf_off += page_copy;
-			sg_off += page_copy;
-			if (sg_off == PAGE_SIZE) {
-				sg_off = 0;
-				n++;
-			}
-			sg_copy -= page_copy;
+	unsigned int offset = 0;
+	struct sg_mapping_iter miter;
+
+	sg_miter_start(&miter, sgl, nents, SG_MITER_ATOMIC);
+
+	while (sg_miter_next(&miter) && offset < buflen) {
+		unsigned int len;
+
+		len = min(miter.length, buflen - offset);
+
+		if (to_buffer)
+			memcpy(buf + offset, miter.addr, len);
+		else {
+			memcpy(miter.addr, buf + offset, len);
+			flush_kernel_dcache_page(miter.page);
 		}
 
-		if (!buflen)
-			break;
+		offset += len;
 	}
 
-	return buf_off;
+	sg_miter_stop(&miter);
+
+	return offset;
 }
 
 /**

commit b1adaf65ba0398c9a1adc8f3a274533165a4df61
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Mar 18 00:15:03 2008 +0900

    [SCSI] block: add sg buffer copy helper functions
    
    This patch adds new three helper functions to copy data between an SG
    list and a linear buffer.
    
    - sg_copy_from_buffer copies data from linear buffer to an SG list
    
    - sg_copy_to_buffer copies data from an SG list to a linear buffer
    
    When the APIs copy data from a linear buffer to an SG list,
    flush_kernel_dcache_page is called. It's not necessary for everyone
    but it's a no-op on most architectures and in general the API is not
    used in performance critical path.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Acked-by: Jens Axboe <jens.axboe@oracle.com>
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index acca4901046c..b80c21100d78 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -8,6 +8,7 @@
  */
 #include <linux/module.h>
 #include <linux/scatterlist.h>
+#include <linux/highmem.h>
 
 /**
  * sg_next - return the next scatterlist entry in a list
@@ -292,3 +293,104 @@ int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
 	return ret;
 }
 EXPORT_SYMBOL(sg_alloc_table);
+
+/**
+ * sg_copy_buffer - Copy data between a linear buffer and an SG list
+ * @sgl:		 The SG list
+ * @nents:		 Number of SG entries
+ * @buf:		 Where to copy from
+ * @buflen:		 The number of bytes to copy
+ * @to_buffer: 		 transfer direction (non zero == from an sg list to a
+ * 			 buffer, 0 == from a buffer to an sg list
+ *
+ * Returns the number of copied bytes.
+ *
+ **/
+static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
+			     void *buf, size_t buflen, int to_buffer)
+{
+	struct scatterlist *sg;
+	size_t buf_off = 0;
+	int i;
+
+	WARN_ON(!irqs_disabled());
+
+	for_each_sg(sgl, sg, nents, i) {
+		struct page *page;
+		int n = 0;
+		unsigned int sg_off = sg->offset;
+		unsigned int sg_copy = sg->length;
+
+		if (sg_copy > buflen)
+			sg_copy = buflen;
+		buflen -= sg_copy;
+
+		while (sg_copy > 0) {
+			unsigned int page_copy;
+			void *p;
+
+			page_copy = PAGE_SIZE - sg_off;
+			if (page_copy > sg_copy)
+				page_copy = sg_copy;
+
+			page = nth_page(sg_page(sg), n);
+			p = kmap_atomic(page, KM_BIO_SRC_IRQ);
+
+			if (to_buffer)
+				memcpy(buf + buf_off, p + sg_off, page_copy);
+			else {
+				memcpy(p + sg_off, buf + buf_off, page_copy);
+				flush_kernel_dcache_page(page);
+			}
+
+			kunmap_atomic(p, KM_BIO_SRC_IRQ);
+
+			buf_off += page_copy;
+			sg_off += page_copy;
+			if (sg_off == PAGE_SIZE) {
+				sg_off = 0;
+				n++;
+			}
+			sg_copy -= page_copy;
+		}
+
+		if (!buflen)
+			break;
+	}
+
+	return buf_off;
+}
+
+/**
+ * sg_copy_from_buffer - Copy from a linear buffer to an SG list
+ * @sgl:		 The SG list
+ * @nents:		 Number of SG entries
+ * @buf:		 Where to copy from
+ * @buflen:		 The number of bytes to copy
+ *
+ * Returns the number of copied bytes.
+ *
+ **/
+size_t sg_copy_from_buffer(struct scatterlist *sgl, unsigned int nents,
+			   void *buf, size_t buflen)
+{
+	return sg_copy_buffer(sgl, nents, buf, buflen, 0);
+}
+EXPORT_SYMBOL(sg_copy_from_buffer);
+
+/**
+ * sg_copy_to_buffer - Copy from an SG list to a linear buffer
+ * @sgl:		 The SG list
+ * @nents:		 Number of SG entries
+ * @buf:		 Where to copy to
+ * @buflen:		 The number of bytes to copy
+ *
+ * Returns the number of copied bytes.
+ *
+ **/
+size_t sg_copy_to_buffer(struct scatterlist *sgl, unsigned int nents,
+			 void *buf, size_t buflen)
+{
+	return sg_copy_buffer(sgl, nents, buf, buflen, 1);
+}
+EXPORT_SYMBOL(sg_copy_to_buffer);

commit 7cedb1f17fb7f4374d11501f61656ae9d3ba47e9
Author: James Bottomley <James.Bottomley@HansenPartnership.com>
Date:   Sun Jan 13 14:15:28 2008 -0600

    SG: work with the SCSI fixed maximum allocations.
    
    SCSI sg table allocation has a maximum size (of SCSI_MAX_SG_SEGMENTS,
    currently 128) and this will cause a BUG_ON() in SCSI if something
    tries an allocation over it.  This patch adds a size limit to the
    chaining allocator to allow the specification of the maximum
    allocation size for chaining, so we always chain in units of the
    maximum SCSI allocation size.
    
    Signed-off-by: James Bottomley <James.Bottomley@HansenPartnership.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
index 02aaa27e010e..acca4901046c 100644
--- a/lib/scatterlist.c
+++ b/lib/scatterlist.c
@@ -130,13 +130,17 @@ static void sg_kfree(struct scatterlist *sg, unsigned int nents)
 /**
  * __sg_free_table - Free a previously mapped sg table
  * @table:	The sg table header to use
+ * @max_ents:	The maximum number of entries per single scatterlist
  * @free_fn:	Free function
  *
  *  Description:
- *    Free an sg table previously allocated and setup with __sg_alloc_table().
+ *    Free an sg table previously allocated and setup with
+ *    __sg_alloc_table().  The @max_ents value must be identical to
+ *    that previously used with __sg_alloc_table().
  *
  **/
-void __sg_free_table(struct sg_table *table, sg_free_fn *free_fn)
+void __sg_free_table(struct sg_table *table, unsigned int max_ents,
+		     sg_free_fn *free_fn)
 {
 	struct scatterlist *sgl, *next;
 
@@ -149,14 +153,14 @@ void __sg_free_table(struct sg_table *table, sg_free_fn *free_fn)
 		unsigned int sg_size;
 
 		/*
-		 * If we have more than SG_MAX_SINGLE_ALLOC segments left,
+		 * If we have more than max_ents segments left,
 		 * then assign 'next' to the sg table after the current one.
 		 * sg_size is then one less than alloc size, since the last
 		 * element is the chain pointer.
 		 */
-		if (alloc_size > SG_MAX_SINGLE_ALLOC) {
-			next = sg_chain_ptr(&sgl[SG_MAX_SINGLE_ALLOC - 1]);
-			alloc_size = SG_MAX_SINGLE_ALLOC;
+		if (alloc_size > max_ents) {
+			next = sg_chain_ptr(&sgl[max_ents - 1]);
+			alloc_size = max_ents;
 			sg_size = alloc_size - 1;
 		} else {
 			sg_size = alloc_size;
@@ -179,7 +183,7 @@ EXPORT_SYMBOL(__sg_free_table);
  **/
 void sg_free_table(struct sg_table *table)
 {
-	__sg_free_table(table, sg_kfree);
+	__sg_free_table(table, SG_MAX_SINGLE_ALLOC, sg_kfree);
 }
 EXPORT_SYMBOL(sg_free_table);
 
@@ -187,22 +191,30 @@ EXPORT_SYMBOL(sg_free_table);
  * __sg_alloc_table - Allocate and initialize an sg table with given allocator
  * @table:	The sg table header to use
  * @nents:	Number of entries in sg list
+ * @max_ents:	The maximum number of entries the allocator returns per call
  * @gfp_mask:	GFP allocation mask
  * @alloc_fn:	Allocator to use
  *
+ * Description:
+ *   This function returns a @table @nents long. The allocator is
+ *   defined to return scatterlist chunks of maximum size @max_ents.
+ *   Thus if @nents is bigger than @max_ents, the scatterlists will be
+ *   chained in units of @max_ents.
+ *
  * Notes:
  *   If this function returns non-0 (eg failure), the caller must call
  *   __sg_free_table() to cleanup any leftover allocations.
  *
  **/
-int __sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask,
+int __sg_alloc_table(struct sg_table *table, unsigned int nents,
+		     unsigned int max_ents, gfp_t gfp_mask,
 		     sg_alloc_fn *alloc_fn)
 {
 	struct scatterlist *sg, *prv;
 	unsigned int left;
 
 #ifndef ARCH_HAS_SG_CHAIN
-	BUG_ON(nents > SG_MAX_SINGLE_ALLOC);
+	BUG_ON(nents > max_ents);
 #endif
 
 	memset(table, 0, sizeof(*table));
@@ -212,8 +224,8 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask,
 	do {
 		unsigned int sg_size, alloc_size = left;
 
-		if (alloc_size > SG_MAX_SINGLE_ALLOC) {
-			alloc_size = SG_MAX_SINGLE_ALLOC;
+		if (alloc_size > max_ents) {
+			alloc_size = max_ents;
 			sg_size = alloc_size - 1;
 		} else
 			sg_size = alloc_size;
@@ -232,7 +244,7 @@ int __sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask,
 		 * If this is not the first mapping, chain previous part.
 		 */
 		if (prv)
-			sg_chain(prv, SG_MAX_SINGLE_ALLOC, sg);
+			sg_chain(prv, max_ents, sg);
 		else
 			table->sgl = sg;
 
@@ -272,9 +284,10 @@ int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
 {
 	int ret;
 
-	ret = __sg_alloc_table(table, nents, gfp_mask, sg_kmalloc);
+	ret = __sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,
+			       gfp_mask, sg_kmalloc);
 	if (unlikely(ret))
-		__sg_free_table(table, sg_kfree);
+		__sg_free_table(table, SG_MAX_SINGLE_ALLOC, sg_kfree);
 
 	return ret;
 }

commit 0db9299f48ebd4a860d6ad4e1d36ac50671d48e7
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Nov 30 09:16:50 2007 +0100

    SG: Move functions to lib/scatterlist.c and add sg chaining allocator helpers
    
    Manually doing chained sg lists is not trivial, so add some helpers
    to make sure that drivers get it right.
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/lib/scatterlist.c b/lib/scatterlist.c
new file mode 100644
index 000000000000..02aaa27e010e
--- /dev/null
+++ b/lib/scatterlist.c
@@ -0,0 +1,281 @@
+/*
+ * Copyright (C) 2007 Jens Axboe <jens.axboe@oracle.com>
+ *
+ * Scatterlist handling helpers.
+ *
+ * This source code is licensed under the GNU General Public License,
+ * Version 2. See the file COPYING for more details.
+ */
+#include <linux/module.h>
+#include <linux/scatterlist.h>
+
+/**
+ * sg_next - return the next scatterlist entry in a list
+ * @sg:		The current sg entry
+ *
+ * Description:
+ *   Usually the next entry will be @sg@ + 1, but if this sg element is part
+ *   of a chained scatterlist, it could jump to the start of a new
+ *   scatterlist array.
+ *
+ **/
+struct scatterlist *sg_next(struct scatterlist *sg)
+{
+#ifdef CONFIG_DEBUG_SG
+	BUG_ON(sg->sg_magic != SG_MAGIC);
+#endif
+	if (sg_is_last(sg))
+		return NULL;
+
+	sg++;
+	if (unlikely(sg_is_chain(sg)))
+		sg = sg_chain_ptr(sg);
+
+	return sg;
+}
+EXPORT_SYMBOL(sg_next);
+
+/**
+ * sg_last - return the last scatterlist entry in a list
+ * @sgl:	First entry in the scatterlist
+ * @nents:	Number of entries in the scatterlist
+ *
+ * Description:
+ *   Should only be used casually, it (currently) scans the entire list
+ *   to get the last entry.
+ *
+ *   Note that the @sgl@ pointer passed in need not be the first one,
+ *   the important bit is that @nents@ denotes the number of entries that
+ *   exist from @sgl@.
+ *
+ **/
+struct scatterlist *sg_last(struct scatterlist *sgl, unsigned int nents)
+{
+#ifndef ARCH_HAS_SG_CHAIN
+	struct scatterlist *ret = &sgl[nents - 1];
+#else
+	struct scatterlist *sg, *ret = NULL;
+	unsigned int i;
+
+	for_each_sg(sgl, sg, nents, i)
+		ret = sg;
+
+#endif
+#ifdef CONFIG_DEBUG_SG
+	BUG_ON(sgl[0].sg_magic != SG_MAGIC);
+	BUG_ON(!sg_is_last(ret));
+#endif
+	return ret;
+}
+EXPORT_SYMBOL(sg_last);
+
+/**
+ * sg_init_table - Initialize SG table
+ * @sgl:	   The SG table
+ * @nents:	   Number of entries in table
+ *
+ * Notes:
+ *   If this is part of a chained sg table, sg_mark_end() should be
+ *   used only on the last table part.
+ *
+ **/
+void sg_init_table(struct scatterlist *sgl, unsigned int nents)
+{
+	memset(sgl, 0, sizeof(*sgl) * nents);
+#ifdef CONFIG_DEBUG_SG
+	{
+		unsigned int i;
+		for (i = 0; i < nents; i++)
+			sgl[i].sg_magic = SG_MAGIC;
+	}
+#endif
+	sg_mark_end(&sgl[nents - 1]);
+}
+EXPORT_SYMBOL(sg_init_table);
+
+/**
+ * sg_init_one - Initialize a single entry sg list
+ * @sg:		 SG entry
+ * @buf:	 Virtual address for IO
+ * @buflen:	 IO length
+ *
+ **/
+void sg_init_one(struct scatterlist *sg, const void *buf, unsigned int buflen)
+{
+	sg_init_table(sg, 1);
+	sg_set_buf(sg, buf, buflen);
+}
+EXPORT_SYMBOL(sg_init_one);
+
+/*
+ * The default behaviour of sg_alloc_table() is to use these kmalloc/kfree
+ * helpers.
+ */
+static struct scatterlist *sg_kmalloc(unsigned int nents, gfp_t gfp_mask)
+{
+	if (nents == SG_MAX_SINGLE_ALLOC)
+		return (struct scatterlist *) __get_free_page(gfp_mask);
+	else
+		return kmalloc(nents * sizeof(struct scatterlist), gfp_mask);
+}
+
+static void sg_kfree(struct scatterlist *sg, unsigned int nents)
+{
+	if (nents == SG_MAX_SINGLE_ALLOC)
+		free_page((unsigned long) sg);
+	else
+		kfree(sg);
+}
+
+/**
+ * __sg_free_table - Free a previously mapped sg table
+ * @table:	The sg table header to use
+ * @free_fn:	Free function
+ *
+ *  Description:
+ *    Free an sg table previously allocated and setup with __sg_alloc_table().
+ *
+ **/
+void __sg_free_table(struct sg_table *table, sg_free_fn *free_fn)
+{
+	struct scatterlist *sgl, *next;
+
+	if (unlikely(!table->sgl))
+		return;
+
+	sgl = table->sgl;
+	while (table->orig_nents) {
+		unsigned int alloc_size = table->orig_nents;
+		unsigned int sg_size;
+
+		/*
+		 * If we have more than SG_MAX_SINGLE_ALLOC segments left,
+		 * then assign 'next' to the sg table after the current one.
+		 * sg_size is then one less than alloc size, since the last
+		 * element is the chain pointer.
+		 */
+		if (alloc_size > SG_MAX_SINGLE_ALLOC) {
+			next = sg_chain_ptr(&sgl[SG_MAX_SINGLE_ALLOC - 1]);
+			alloc_size = SG_MAX_SINGLE_ALLOC;
+			sg_size = alloc_size - 1;
+		} else {
+			sg_size = alloc_size;
+			next = NULL;
+		}
+
+		table->orig_nents -= sg_size;
+		free_fn(sgl, alloc_size);
+		sgl = next;
+	}
+
+	table->sgl = NULL;
+}
+EXPORT_SYMBOL(__sg_free_table);
+
+/**
+ * sg_free_table - Free a previously allocated sg table
+ * @table:	The mapped sg table header
+ *
+ **/
+void sg_free_table(struct sg_table *table)
+{
+	__sg_free_table(table, sg_kfree);
+}
+EXPORT_SYMBOL(sg_free_table);
+
+/**
+ * __sg_alloc_table - Allocate and initialize an sg table with given allocator
+ * @table:	The sg table header to use
+ * @nents:	Number of entries in sg list
+ * @gfp_mask:	GFP allocation mask
+ * @alloc_fn:	Allocator to use
+ *
+ * Notes:
+ *   If this function returns non-0 (eg failure), the caller must call
+ *   __sg_free_table() to cleanup any leftover allocations.
+ *
+ **/
+int __sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask,
+		     sg_alloc_fn *alloc_fn)
+{
+	struct scatterlist *sg, *prv;
+	unsigned int left;
+
+#ifndef ARCH_HAS_SG_CHAIN
+	BUG_ON(nents > SG_MAX_SINGLE_ALLOC);
+#endif
+
+	memset(table, 0, sizeof(*table));
+
+	left = nents;
+	prv = NULL;
+	do {
+		unsigned int sg_size, alloc_size = left;
+
+		if (alloc_size > SG_MAX_SINGLE_ALLOC) {
+			alloc_size = SG_MAX_SINGLE_ALLOC;
+			sg_size = alloc_size - 1;
+		} else
+			sg_size = alloc_size;
+
+		left -= sg_size;
+
+		sg = alloc_fn(alloc_size, gfp_mask);
+		if (unlikely(!sg))
+			return -ENOMEM;
+
+		sg_init_table(sg, alloc_size);
+		table->nents = table->orig_nents += sg_size;
+
+		/*
+		 * If this is the first mapping, assign the sg table header.
+		 * If this is not the first mapping, chain previous part.
+		 */
+		if (prv)
+			sg_chain(prv, SG_MAX_SINGLE_ALLOC, sg);
+		else
+			table->sgl = sg;
+
+		/*
+		 * If no more entries after this one, mark the end
+		 */
+		if (!left)
+			sg_mark_end(&sg[sg_size - 1]);
+
+		/*
+		 * only really needed for mempool backed sg allocations (like
+		 * SCSI), a possible improvement here would be to pass the
+		 * table pointer into the allocator and let that clear these
+		 * flags
+		 */
+		gfp_mask &= ~__GFP_WAIT;
+		gfp_mask |= __GFP_HIGH;
+		prv = sg;
+	} while (left);
+
+	return 0;
+}
+EXPORT_SYMBOL(__sg_alloc_table);
+
+/**
+ * sg_alloc_table - Allocate and initialize an sg table
+ * @table:	The sg table header to use
+ * @nents:	Number of entries in sg list
+ * @gfp_mask:	GFP allocation mask
+ *
+ *  Description:
+ *    Allocate and initialize an sg table. If @nents@ is larger than
+ *    SG_MAX_SINGLE_ALLOC a chained sg table will be setup.
+ *
+ **/
+int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
+{
+	int ret;
+
+	ret = __sg_alloc_table(table, nents, gfp_mask, sg_kmalloc);
+	if (unlikely(ret))
+		__sg_free_table(table, sg_kfree);
+
+	return ret;
+}
+EXPORT_SYMBOL(sg_alloc_table);
