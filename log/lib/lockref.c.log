commit 893a7d32e8e04ca4d6c882336b26ed660ca0a48d
Author: Jan Glauber <jglauber@cavium.com>
Date:   Wed Jun 5 15:48:49 2019 +0200

    lockref: Limit number of cmpxchg loop retries
    
    The lockref cmpxchg loop is unbound as long as the spinlock is not
    taken. Depending on the hardware implementation of compare-and-swap
    a high number of loop retries might happen.
    
    Add an upper bound to the loop to force the fallback to spinlocks
    after some time. A retry value of 100 should not impact any hardware
    that does not have this issue.
    
    With the retry limit the performance of an open-close testcase
    improved between 60-70% on ThunderX2.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Jan Glauber <jglauber@marvell.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index 3d468b53d4c9..5b34bbd3eba8 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -9,6 +9,7 @@
  * failure case.
  */
 #define CMPXCHG_LOOP(CODE, SUCCESS) do {					\
+	int retry = 100;							\
 	struct lockref old;							\
 	BUILD_BUG_ON(sizeof(old) != 8);						\
 	old.lock_count = READ_ONCE(lockref->lock_count);			\
@@ -21,6 +22,8 @@
 		if (likely(old.lock_count == prev.lock_count)) {		\
 			SUCCESS;						\
 		}								\
+		if (!--retry)							\
+			break;							\
 		cpu_relax();							\
 	}									\
 } while (0)

commit 450b1f6f56350c630e795f240dc5a77aa8aa2419
Author: Andreas Gruenbacher <agruenba@redhat.com>
Date:   Thu Mar 29 08:07:46 2018 +0100

    lockref: Add lockref_put_not_zero
    
    Put a lockref unless the lockref is dead or its count would become zero.
    This is the same as lockref_put_or_lock except that the lock is never
    left held.
    
    Signed-off-by: Andreas Gruenbacher <agruenba@redhat.com>
    Signed-off-by: Bob Peterson <rpeterso@redhat.com>

diff --git a/lib/lockref.c b/lib/lockref.c
index 47169ed7e964..3d468b53d4c9 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -80,6 +80,34 @@ int lockref_get_not_zero(struct lockref *lockref)
 }
 EXPORT_SYMBOL(lockref_get_not_zero);
 
+/**
+ * lockref_put_not_zero - Decrements count unless count <= 1 before decrement
+ * @lockref: pointer to lockref structure
+ * Return: 1 if count updated successfully or 0 if count would become zero
+ */
+int lockref_put_not_zero(struct lockref *lockref)
+{
+	int retval;
+
+	CMPXCHG_LOOP(
+		new.count--;
+		if (old.count <= 1)
+			return 0;
+	,
+		return 1;
+	);
+
+	spin_lock(&lockref->lock);
+	retval = 0;
+	if (lockref->count > 1) {
+		lockref->count--;
+		retval = 1;
+	}
+	spin_unlock(&lockref->lock);
+	return retval;
+}
+EXPORT_SYMBOL(lockref_put_not_zero);
+
 /**
  * lockref_get_or_lock - Increments count unless the count is 0 or dead
  * @lockref: pointer to lockref structure

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index c4bfcb8836cd..47169ed7e964 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 #include <linux/export.h>
 #include <linux/lockref.h>
 

commit f2f09a4cee3507dba0e24b87ba2961a5c377d3a7
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Oct 25 11:03:14 2016 +0200

    locking/core: Remove cpu_relax_lowlatency() users
    
    With the s390 special case of a yielding cpu_relax() implementation gone,
    we can now remove all users of cpu_relax_lowlatency() and replace them
    with cpu_relax().
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1477386195-32736-5-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index 5a92189ad711..c4bfcb8836cd 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -20,7 +20,7 @@
 		if (likely(old.lock_count == prev.lock_count)) {		\
 			SUCCESS;						\
 		}								\
-		cpu_relax_lowlatency();						\
+		cpu_relax();							\
 	}									\
 } while (0)
 

commit f5468ffde13fc991bd4d6bdec507ffd5777865bd
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Aug 6 17:54:40 2015 +0100

    locking/lockref: Remove homebrew cmpxchg64_relaxed() macro definition
    
    cmpxchg64_relaxed() is now defined by linux/atomic.h, so we can
    remove our local definition from the lockref code.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Waiman.Long@hp.com
    Cc: paulmck@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1438880084-18856-5-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index 494994bf17c8..5a92189ad711 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -3,14 +3,6 @@
 
 #if USE_CMPXCHG_LOCKREF
 
-/*
- * Allow weakly-ordered memory architectures to provide barrier-less
- * cmpxchg semantics for lockref updates.
- */
-#ifndef cmpxchg64_relaxed
-# define cmpxchg64_relaxed cmpxchg64
-#endif
-
 /*
  * Note that the "cmpxchg()" reloads the "old" value for the
  * failure case.

commit 4d3199e4ca8e6670b54dc5ee070ffd54385988e9
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Sun Feb 22 19:31:41 2015 -0800

    locking: Remove ACCESS_ONCE() usage
    
    With the new standardized functions, we can replace all
    ACCESS_ONCE() calls across relevant locking - this includes
    lockref and seqlock while at it.
    
    ACCESS_ONCE() does not work reliably on non-scalar types.
    For example gcc 4.6 and 4.7 might remove the volatile tag
    for such accesses during the SRA (scalar replacement of
    aggregates) step:
    
      https://gcc.gnu.org/bugzilla/show_bug.cgi?id=58145
    
    Update the new calls regardless of if it is a scalar type,
    this is cleaner than having three alternatives.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1424662301.6539.18.camel@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index ecb9a665ec19..494994bf17c8 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -18,7 +18,7 @@
 #define CMPXCHG_LOOP(CODE, SUCCESS) do {					\
 	struct lockref old;							\
 	BUILD_BUG_ON(sizeof(old) != 8);						\
-	old.lock_count = ACCESS_ONCE(lockref->lock_count);			\
+	old.lock_count = READ_ONCE(lockref->lock_count);			\
 	while (likely(arch_spin_value_unlocked(old.lock.rlock.raw_lock))) {  	\
 		struct lockref new = old, prev = old;				\
 		CODE								\

commit 360f54796ed65939093ae373b92ebd5ef3341776
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 9 15:19:03 2015 -0800

    dcache: let the dentry count go down to zero without taking d_lock
    
    We can be more aggressive about this, if we are clever and careful. This is subtle.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/lib/lockref.c b/lib/lockref.c
index d2233de9a86e..ecb9a665ec19 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -60,7 +60,7 @@ void lockref_get(struct lockref *lockref)
 EXPORT_SYMBOL(lockref_get);
 
 /**
- * lockref_get_not_zero - Increments count unless the count is 0
+ * lockref_get_not_zero - Increments count unless the count is 0 or dead
  * @lockref: pointer to lockref structure
  * Return: 1 if count updated successfully or 0 if count was zero
  */
@@ -70,7 +70,7 @@ int lockref_get_not_zero(struct lockref *lockref)
 
 	CMPXCHG_LOOP(
 		new.count++;
-		if (!old.count)
+		if (old.count <= 0)
 			return 0;
 	,
 		return 1;
@@ -78,7 +78,7 @@ int lockref_get_not_zero(struct lockref *lockref)
 
 	spin_lock(&lockref->lock);
 	retval = 0;
-	if (lockref->count) {
+	if (lockref->count > 0) {
 		lockref->count++;
 		retval = 1;
 	}
@@ -88,7 +88,7 @@ int lockref_get_not_zero(struct lockref *lockref)
 EXPORT_SYMBOL(lockref_get_not_zero);
 
 /**
- * lockref_get_or_lock - Increments count unless the count is 0
+ * lockref_get_or_lock - Increments count unless the count is 0 or dead
  * @lockref: pointer to lockref structure
  * Return: 1 if count updated successfully or 0 if count was zero
  * and we got the lock instead.
@@ -97,14 +97,14 @@ int lockref_get_or_lock(struct lockref *lockref)
 {
 	CMPXCHG_LOOP(
 		new.count++;
-		if (!old.count)
+		if (old.count <= 0)
 			break;
 	,
 		return 1;
 	);
 
 	spin_lock(&lockref->lock);
-	if (!lockref->count)
+	if (lockref->count <= 0)
 		return 0;
 	lockref->count++;
 	spin_unlock(&lockref->lock);
@@ -112,6 +112,26 @@ int lockref_get_or_lock(struct lockref *lockref)
 }
 EXPORT_SYMBOL(lockref_get_or_lock);
 
+/**
+ * lockref_put_return - Decrement reference count if possible
+ * @lockref: pointer to lockref structure
+ *
+ * Decrement the reference count and return the new value.
+ * If the lockref was dead or locked, return an error.
+ */
+int lockref_put_return(struct lockref *lockref)
+{
+	CMPXCHG_LOOP(
+		new.count--;
+		if (old.count <= 0)
+			return -1;
+	,
+		return new.count;
+	);
+	return -1;
+}
+EXPORT_SYMBOL(lockref_put_return);
+
 /**
  * lockref_put_or_lock - decrements count unless count <= 1 before decrement
  * @lockref: pointer to lockref structure
@@ -158,7 +178,7 @@ int lockref_get_not_dead(struct lockref *lockref)
 
 	CMPXCHG_LOOP(
 		new.count++;
-		if ((int)old.count < 0)
+		if (old.count < 0)
 			return 0;
 	,
 		return 1;
@@ -166,7 +186,7 @@ int lockref_get_not_dead(struct lockref *lockref)
 
 	spin_lock(&lockref->lock);
 	retval = 0;
-	if ((int) lockref->count >= 0) {
+	if (lockref->count >= 0) {
 		lockref->count++;
 		retval = 1;
 	}

commit 3a6bfbc91df04b081a44d419e0260bad54abddf7
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sun Jun 29 15:09:33 2014 -0700

    arch, locking: Ciao arch_mutex_cpu_relax()
    
    The arch_mutex_cpu_relax() function, introduced by 34b133f, is
    hacky and ugly. It was added a few years ago to address the fact
    that common cpu_relax() calls include yielding on s390, and thus
    impact the optimistic spinning functionality of mutexes. Nowadays
    we use this function well beyond mutexes: rwsem, qrwlock, mcs and
    lockref. Since the macro that defines the call is in the mutex header,
    any users must include mutex.h and the naming is misleading as well.
    
    This patch (i) renames the call to cpu_relax_lowlatency  ("relax, but
    only if you can do it with very low latency") and (ii) defines it in
    each arch's asm/processor.h local header, just like for regular cpu_relax
    functions. On all archs, except s390, cpu_relax_lowlatency is simply cpu_relax,
    and thus we can take it out of mutex.h. While this can seem redundant,
    I believe it is a good choice as it allows us to move out arch specific
    logic from generic locking primitives and enables future(?) archs to
    transparently define it, similarly to System Z.
    
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Bharat Bhushan <r65777@freescale.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Cc: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Joseph Myers <joseph@codesourcery.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Qais Yousef <qais.yousef@imgtec.com>
    Cc: Qiaowei Ren <qiaowei.ren@intel.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Stratos Karafotis <stratosk@semaphore.gr>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vasily Kulikov <segoon@openwall.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Vineet Gupta <Vineet.Gupta1@synopsys.com>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Wolfram Sang <wsa@the-dreams.de>
    Cc: adi-buildroot-devel@lists.sourceforge.net
    Cc: linux390@de.ibm.com
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-am33-list@redhat.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-cris-kernel@axis.com
    Cc: linux-hexagon@vger.kernel.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux@lists.openrisc.net
    Cc: linux-m32r-ja@ml.linux-m32r.org
    Cc: linux-m32r@ml.linux-m32r.org
    Cc: linux-m68k@lists.linux-m68k.org
    Cc: linux-metag@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Cc: linux-parisc@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-sh@vger.kernel.org
    Cc: linux-xtensa@linux-xtensa.org
    Cc: sparclinux@vger.kernel.org
    Link: http://lkml.kernel.org/r/1404079773.2619.4.camel@buesod1.americas.hpqcorp.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index f07a40d33871..d2233de9a86e 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -1,6 +1,5 @@
 #include <linux/export.h>
 #include <linux/lockref.h>
-#include <linux/mutex.h>
 
 #if USE_CMPXCHG_LOCKREF
 
@@ -29,7 +28,7 @@
 		if (likely(old.lock_count == prev.lock_count)) {		\
 			SUCCESS;						\
 		}								\
-		arch_mutex_cpu_relax();						\
+		cpu_relax_lowlatency();						\
 	}									\
 } while (0)
 

commit 14058d20c155ab3ff473fb60eca4fa7aa21a16ac
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Nov 27 13:52:53 2013 +0000

    lockref: include mutex.h rather than reinvent arch_mutex_cpu_relax
    
    arch_mutex_cpu_relax is already conditionally defined in mutex.h, so
    simply include that header rather than replicate the code here.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index d2b123f8456b..f07a40d33871 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -1,5 +1,6 @@
 #include <linux/export.h>
 #include <linux/lockref.h>
+#include <linux/mutex.h>
 
 #if USE_CMPXCHG_LOCKREF
 
@@ -11,14 +12,6 @@
 # define cmpxchg64_relaxed cmpxchg64
 #endif
 
-/*
- * Allow architectures to override the default cpu_relax() within CMPXCHG_LOOP.
- * This is useful for architectures with an expensive cpu_relax().
- */
-#ifndef arch_mutex_cpu_relax
-# define arch_mutex_cpu_relax() cpu_relax()
-#endif
-
 /*
  * Note that the "cmpxchg()" reloads the "old" value for the
  * failure case.

commit 57f4257eae33e036125973858934730250d464e3
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 14 14:31:54 2013 -0800

    lockref: use BLOATED_SPINLOCKS to avoid explicit config dependencies
    
    Avoid the fragile Kconfig construct guestimating spinlock_t sizes; use a
    friendly compile-time test to determine this.
    
    [kirill.shutemov@linux.intel.com: drop CONFIG_CMPXCHG_LOCKREF]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index af6e95d0bed6..d2b123f8456b 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -1,7 +1,7 @@
 #include <linux/export.h>
 #include <linux/lockref.h>
 
-#ifdef CONFIG_CMPXCHG_LOCKREF
+#if USE_CMPXCHG_LOCKREF
 
 /*
  * Allow weakly-ordered memory architectures to provide barrier-less

commit 8b5baa460b69c27389353eeff0dbe51dc695da60
Merge: 6c86ae2928f9 2147dbfd059e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 11 07:11:00 2013 +0900

    Merge tag 'gfs2-merge-window' of git://git.kernel.org/pub/scm/linux/kernel/git/steve/gfs2-3.0-nmw
    
    Pull gfs2 updates from Steven Whitehouse:
     "The main feature of interest this time is quota updates.  There are
      some clean ups and some patches to use the new generic lru list code.
    
      There is still plenty of scope for some further changes in due course -
      faster lookups of quota structures is very much on the todo list.
      Also, a start has been made towards the more tricky issue of using the
      generic lru code with glocks, but that will have to be completed in a
      subsequent merge window.
    
      The other, more minor feature, is that there have been a number of
      performance patches which relate to block allocation.  In particular
      they will improve performance when the disk is nearly full"
    
    * tag 'gfs2-merge-window' of git://git.kernel.org/pub/scm/linux/kernel/git/steve/gfs2-3.0-nmw:
      GFS2: Use generic list_lru for quota
      GFS2: Rename quota qd_lru_lock qd_lock
      GFS2: Use reflink for quota data cache
      GFS2: Use lockref for glocks
      GFS2: Protect quota sync generation
      GFS2: Inline qd_trylock into gfs2_quota_unlock
      GFS2: Make two similar quota code fragments into a function
      GFS2: Remove obsolete quota tunable
      GFS2: Move gfs2_icbit_munge into quota.c
      GFS2: Speed up starting point selection for block allocation
      GFS2: Add allocation parameters structure
      GFS2: Clean up reservation removal
      GFS2: fix dentry leaks
      GFS2: new function gfs2_rbm_incr
      GFS2: Introduce rbm field bii
      GFS2: Do not reset flags on active reservations
      GFS2: introduce bi_blocks for optimization
      GFS2: optimize rbm_from_block wrt bi_start
      GFS2: d_splice_alias() can't return error

commit e66cf161098a634dc96e32d0089c5767cf25668a
Author: Steven Whitehouse <swhiteho@redhat.com>
Date:   Tue Oct 15 15:18:08 2013 +0100

    GFS2: Use lockref for glocks
    
    Currently glocks have an atomic reference count and also a spinlock
    which covers various internal fields, such as the state. This intent of
    this patch is to replace the spinlock and the atomic reference count
    with a lockref structure. This contains a spinlock which we can continue
    to use as before, and a reference counter which is used in conjuction
    with the spinlock to replace the previous atomic counter.
    
    As a result of this there are some new rules for reference counting on
    glocks. We need to distinguish between reference count changes under
    gl_spin (which are now just increment or decrement of the new counter,
    provided the count cannot hit zero) and those which are outside of
    gl_spin, but which now take gl_spin internally.
    
    The conversion is relatively straight forward. There is probably some
    further clean up which can be done, but the priority at this stage is to
    make the change in as simple a manner as possible.
    
    A consequence of this change is that the reference count is being
    decoupled from the lru list processing. This should allow future
    adoption of the lru_list code with glocks in due course.
    
    The reason for using the "dead" state and not just relying on 0 being
    the "invalid state" is so that in due course 0 ref counts can be
    allowable. The intent is to eventually be able to remove the ref count
    changes which are currently hidden away in state_change().
    
    Signed-off-by: Steven Whitehouse <swhiteho@redhat.com>

diff --git a/lib/lockref.c b/lib/lockref.c
index e2cd2c0a8821..8ff162fe3413 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -136,6 +136,7 @@ void lockref_mark_dead(struct lockref *lockref)
 	assert_spin_locked(&lockref->lock);
 	lockref->count = -128;
 }
+EXPORT_SYMBOL(lockref_mark_dead);
 
 /**
  * lockref_get_not_dead - Increments count unless the ref is dead

commit 491f6f8e5fd9a57aaf03b6d6e3e153f1c27d8a46
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Sep 23 12:59:56 2013 +0200

    lockref: use arch_mutex_cpu_relax() in CMPXCHG_LOOP()
    
    Make use of arch_mutex_cpu_relax() so architectures can override the
    default cpu_relax() semantics.
    This is especially useful for s390, where cpu_relax() means that we
    yield() the current (virtual) cpu and therefore is very expensive,
    and would contradict the whole purpose of the lockless cmpxchg loop.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/lib/lockref.c b/lib/lockref.c
index e294ae445c9a..6f9d434c1521 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -11,6 +11,14 @@
 # define cmpxchg64_relaxed cmpxchg64
 #endif
 
+/*
+ * Allow architectures to override the default cpu_relax() within CMPXCHG_LOOP.
+ * This is useful for architectures with an expensive cpu_relax().
+ */
+#ifndef arch_mutex_cpu_relax
+# define arch_mutex_cpu_relax() cpu_relax()
+#endif
+
 /*
  * Note that the "cmpxchg()" reloads the "old" value for the
  * failure case.
@@ -28,7 +36,7 @@
 		if (likely(old.lock_count == prev.lock_count)) {		\
 			SUCCESS;						\
 		}								\
-		cpu_relax();							\
+		arch_mutex_cpu_relax();						\
 	}									\
 } while (0)
 

commit d2212b4dce596fee83e5c523400bf084f4cc816c
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Sep 26 17:27:00 2013 +0100

    lockref: allow relaxed cmpxchg64 variant for lockless updates
    
    The 64-bit cmpxchg operation on the lockref is ordered by virtue of
    hazarding between the cmpxchg operation and the reference count
    manipulation. On weakly ordered memory architectures (such as ARM), it
    can be of great benefit to omit the barrier instructions where they are
    not needed.
    
    This patch moves the lockless lockref code over to a cmpxchg64_relaxed
    operation, which doesn't provide barrier semantics. If the operation
    isn't defined, we simply #define it as the usual 64-bit cmpxchg macro.
    
    Cc: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index 677d036cf3c7..e294ae445c9a 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -3,6 +3,14 @@
 
 #ifdef CONFIG_CMPXCHG_LOCKREF
 
+/*
+ * Allow weakly-ordered memory architectures to provide barrier-less
+ * cmpxchg semantics for lockref updates.
+ */
+#ifndef cmpxchg64_relaxed
+# define cmpxchg64_relaxed cmpxchg64
+#endif
+
 /*
  * Note that the "cmpxchg()" reloads the "old" value for the
  * failure case.
@@ -14,8 +22,9 @@
 	while (likely(arch_spin_value_unlocked(old.lock.rlock.raw_lock))) {  	\
 		struct lockref new = old, prev = old;				\
 		CODE								\
-		old.lock_count = cmpxchg64(&lockref->lock_count,		\
-					   old.lock_count, new.lock_count);	\
+		old.lock_count = cmpxchg64_relaxed(&lockref->lock_count,	\
+						   old.lock_count,		\
+						   new.lock_count);		\
 		if (likely(old.lock_count == prev.lock_count)) {		\
 			SUCCESS;						\
 		}								\

commit 8f4c344696b9f9f8471d7f342076ef10ed7f66a5
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Sep 19 19:06:46 2013 +0100

    lockref: use cmpxchg64 explicitly for lockless updates
    
    The cmpxchg() function tends not to support 64-bit arguments on 32-bit
    architectures.  This could be either due to use of unsigned long
    arguments (like on ARM) or lack of instruction support (cmpxchgq on
    x86).  However, these architectures may implement a specific cmpxchg64()
    function to provide 64-bit cmpxchg support instead.
    
    Since the lockref code requires a 64-bit cmpxchg and relies on the
    architecture selecting ARCH_USE_CMPXCHG_LOCKREF, move to using cmpxchg64
    instead of cmpxchg and allow 32-bit architectures to make use of the
    lockless lockref implementation.
    
    Cc: Waiman Long <Waiman.Long@hp.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index e2cd2c0a8821..677d036cf3c7 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -14,8 +14,8 @@
 	while (likely(arch_spin_value_unlocked(old.lock.rlock.raw_lock))) {  	\
 		struct lockref new = old, prev = old;				\
 		CODE								\
-		old.lock_count = cmpxchg(&lockref->lock_count,			\
-					 old.lock_count, new.lock_count);	\
+		old.lock_count = cmpxchg64(&lockref->lock_count,		\
+					   old.lock_count, new.lock_count);	\
 		if (likely(old.lock_count == prev.lock_count)) {		\
 			SUCCESS;						\
 		}								\

commit e7d33bb5ea82922e6ddcfc6b28a630b1a4ced071
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 7 15:49:18 2013 -0700

    lockref: add ability to mark lockrefs "dead"
    
    The only actual current lockref user (dcache) uses zero reference counts
    even for perfectly live dentries, because it's a cache: there may not be
    any users, but that doesn't mean that we want to throw away the dentry.
    
    At the same time, the dentry cache does have a notion of a truly "dead"
    dentry that we must not even increment the reference count of, because
    we have pruned it and it is not valid.
    
    Currently that distinction is not visible in the lockref itself, and the
    dentry cache validation uses "lockref_get_or_lock()" to either get a new
    reference to a dentry that already had existing references (and thus
    cannot be dead), or get the dentry lock so that we can then verify the
    dentry and increment the reference count under the lock if that
    verification was successful.
    
    That's all somewhat complicated.
    
    This adds the concept of being "dead" to the lockref itself, by simply
    using a count that is negative.  This allows a usage scenario where we
    can increment the refcount of a dentry without having to validate it,
    and pushing the special "we killed it" case into the lockref code.
    
    The dentry code itself doesn't actually use this yet, and it's probably
    too late in the merge window to do that code (the dentry_kill() code
    with its "should I decrement the count" logic really is pretty complex
    code), but let's introduce the concept at the lockref level now.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index 7aae8df37f67..e2cd2c0a8821 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -126,3 +126,41 @@ int lockref_put_or_lock(struct lockref *lockref)
 	return 1;
 }
 EXPORT_SYMBOL(lockref_put_or_lock);
+
+/**
+ * lockref_mark_dead - mark lockref dead
+ * @lockref: pointer to lockref structure
+ */
+void lockref_mark_dead(struct lockref *lockref)
+{
+	assert_spin_locked(&lockref->lock);
+	lockref->count = -128;
+}
+
+/**
+ * lockref_get_not_dead - Increments count unless the ref is dead
+ * @lockref: pointer to lockref structure
+ * Return: 1 if count updated successfully or 0 if lockref was dead
+ */
+int lockref_get_not_dead(struct lockref *lockref)
+{
+	int retval;
+
+	CMPXCHG_LOOP(
+		new.count++;
+		if ((int)old.count < 0)
+			return 0;
+	,
+		return 1;
+	);
+
+	spin_lock(&lockref->lock);
+	retval = 0;
+	if ((int) lockref->count >= 0) {
+		lockref->count++;
+		retval = 1;
+	}
+	spin_unlock(&lockref->lock);
+	return retval;
+}
+EXPORT_SYMBOL(lockref_get_not_dead);

commit 44a0cf92926c343366a4986808d12ab068504eed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 7 15:30:29 2013 -0700

    lockref: fix docbook argument names
    
    The code got rewritten, but the comments got copied as-is from older
    versions, and as a result the argument name in the comment didn't
    actually match the code any more.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index 9d76f404ce9a..7aae8df37f67 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -31,7 +31,7 @@
 
 /**
  * lockref_get - Increments reference count unconditionally
- * @lockcnt: pointer to lockref structure
+ * @lockref: pointer to lockref structure
  *
  * This operation is only valid if you already hold a reference
  * to the object, so you know the count cannot be zero.
@@ -52,7 +52,7 @@ EXPORT_SYMBOL(lockref_get);
 
 /**
  * lockref_get_not_zero - Increments count unless the count is 0
- * @lockcnt: pointer to lockref structure
+ * @lockref: pointer to lockref structure
  * Return: 1 if count updated successfully or 0 if count was zero
  */
 int lockref_get_not_zero(struct lockref *lockref)
@@ -80,7 +80,7 @@ EXPORT_SYMBOL(lockref_get_not_zero);
 
 /**
  * lockref_get_or_lock - Increments count unless the count is 0
- * @lockcnt: pointer to lockref structure
+ * @lockref: pointer to lockref structure
  * Return: 1 if count updated successfully or 0 if count was zero
  * and we got the lock instead.
  */
@@ -105,7 +105,7 @@ EXPORT_SYMBOL(lockref_get_or_lock);
 
 /**
  * lockref_put_or_lock - decrements count unless count <= 1 before decrement
- * @lockcnt: pointer to lockref structure
+ * @lockref: pointer to lockref structure
  * Return: 1 if count updated successfully or 0 if count <= 1 and lock taken
  */
 int lockref_put_or_lock(struct lockref *lockref)

commit d472d9d98b463dd7a04f2bcdeafe4261686ce6ab
Author: Luck, Tony <tony.luck@intel.com>
Date:   Tue Sep 3 14:49:49 2013 -0700

    lockref: Relax in cmpxchg loop
    
    While we are likley to succeed and break out of this loop, it isn't
    guaranteed.  We should be power and thread friendly if we do have to
    go around for a second (or third, or more) attempt.
    
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index 7819c2d1d315..9d76f404ce9a 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -19,6 +19,7 @@
 		if (likely(old.lock_count == prev.lock_count)) {		\
 			SUCCESS;						\
 		}								\
+		cpu_relax();							\
 	}									\
 } while (0)
 

commit bc08b449ee14ace4d869adaa1bb35a44ce68d775
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 2 12:12:15 2013 -0700

    lockref: implement lockless reference count updates using cmpxchg()
    
    Instead of taking the spinlock, the lockless versions atomically check
    that the lock is not taken, and do the reference count update using a
    cmpxchg() loop.  This is semantically identical to doing the reference
    count update protected by the lock, but avoids the "wait for lock"
    contention that you get when accesses to the reference count are
    contended.
    
    Note that a "lockref" is absolutely _not_ equivalent to an atomic_t.
    Even when the lockref reference counts are updated atomically with
    cmpxchg, the fact that they also verify the state of the spinlock means
    that the lockless updates can never happen while somebody else holds the
    spinlock.
    
    So while "lockref_put_or_lock()" looks a lot like just another name for
    "atomic_dec_and_lock()", and both optimize to lockless updates, they are
    fundamentally different: the decrement done by atomic_dec_and_lock() is
    truly independent of any lock (as long as it doesn't decrement to zero),
    so a locked region can still see the count change.
    
    The lockref structure, in contrast, really is a *locked* reference
    count.  If you hold the spinlock, the reference count will be stable and
    you can modify the reference count without using atomics, because even
    the lockless updates will see and respect the state of the lock.
    
    In order to enable the cmpxchg lockless code, the architecture needs to
    do three things:
    
     (1) Make sure that the "arch_spinlock_t" and an "unsigned int" can fit
         in an aligned u64, and have a "cmpxchg()" implementation that works
         on such a u64 data type.
    
     (2) define a helper function to test for a spinlock being unlocked
         ("arch_spin_value_unlocked()")
    
     (3) select the "ARCH_USE_CMPXCHG_LOCKREF" config variable in its
         Kconfig file.
    
    This enables it for x86-64 (but not 32-bit, we'd need to make sure
    cmpxchg() turns into the proper cmpxchg8b in order to enable it for
    32-bit mode).
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
index a9a4f4e1eff5..7819c2d1d315 100644
--- a/lib/lockref.c
+++ b/lib/lockref.c
@@ -1,6 +1,33 @@
 #include <linux/export.h>
 #include <linux/lockref.h>
 
+#ifdef CONFIG_CMPXCHG_LOCKREF
+
+/*
+ * Note that the "cmpxchg()" reloads the "old" value for the
+ * failure case.
+ */
+#define CMPXCHG_LOOP(CODE, SUCCESS) do {					\
+	struct lockref old;							\
+	BUILD_BUG_ON(sizeof(old) != 8);						\
+	old.lock_count = ACCESS_ONCE(lockref->lock_count);			\
+	while (likely(arch_spin_value_unlocked(old.lock.rlock.raw_lock))) {  	\
+		struct lockref new = old, prev = old;				\
+		CODE								\
+		old.lock_count = cmpxchg(&lockref->lock_count,			\
+					 old.lock_count, new.lock_count);	\
+		if (likely(old.lock_count == prev.lock_count)) {		\
+			SUCCESS;						\
+		}								\
+	}									\
+} while (0)
+
+#else
+
+#define CMPXCHG_LOOP(CODE, SUCCESS) do { } while (0)
+
+#endif
+
 /**
  * lockref_get - Increments reference count unconditionally
  * @lockcnt: pointer to lockref structure
@@ -10,6 +37,12 @@
  */
 void lockref_get(struct lockref *lockref)
 {
+	CMPXCHG_LOOP(
+		new.count++;
+	,
+		return;
+	);
+
 	spin_lock(&lockref->lock);
 	lockref->count++;
 	spin_unlock(&lockref->lock);
@@ -23,9 +56,18 @@ EXPORT_SYMBOL(lockref_get);
  */
 int lockref_get_not_zero(struct lockref *lockref)
 {
-	int retval = 0;
+	int retval;
+
+	CMPXCHG_LOOP(
+		new.count++;
+		if (!old.count)
+			return 0;
+	,
+		return 1;
+	);
 
 	spin_lock(&lockref->lock);
+	retval = 0;
 	if (lockref->count) {
 		lockref->count++;
 		retval = 1;
@@ -43,6 +85,14 @@ EXPORT_SYMBOL(lockref_get_not_zero);
  */
 int lockref_get_or_lock(struct lockref *lockref)
 {
+	CMPXCHG_LOOP(
+		new.count++;
+		if (!old.count)
+			break;
+	,
+		return 1;
+	);
+
 	spin_lock(&lockref->lock);
 	if (!lockref->count)
 		return 0;
@@ -59,6 +109,14 @@ EXPORT_SYMBOL(lockref_get_or_lock);
  */
 int lockref_put_or_lock(struct lockref *lockref)
 {
+	CMPXCHG_LOOP(
+		new.count--;
+		if (old.count <= 1)
+			break;
+	,
+		return 1;
+	);
+
 	spin_lock(&lockref->lock);
 	if (lockref->count <= 1)
 		return 0;

commit 2f4f12e571c4e2f50f3818a3c2544929145f75dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 2 11:58:20 2013 -0700

    lockref: uninline lockref helper functions
    
    They aren't very good to inline, since they already call external
    functions (the spinlock code), and we're going to create rather more
    complicated versions of them that can do the reference count updates
    locklessly.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/lockref.c b/lib/lockref.c
new file mode 100644
index 000000000000..a9a4f4e1eff5
--- /dev/null
+++ b/lib/lockref.c
@@ -0,0 +1,69 @@
+#include <linux/export.h>
+#include <linux/lockref.h>
+
+/**
+ * lockref_get - Increments reference count unconditionally
+ * @lockcnt: pointer to lockref structure
+ *
+ * This operation is only valid if you already hold a reference
+ * to the object, so you know the count cannot be zero.
+ */
+void lockref_get(struct lockref *lockref)
+{
+	spin_lock(&lockref->lock);
+	lockref->count++;
+	spin_unlock(&lockref->lock);
+}
+EXPORT_SYMBOL(lockref_get);
+
+/**
+ * lockref_get_not_zero - Increments count unless the count is 0
+ * @lockcnt: pointer to lockref structure
+ * Return: 1 if count updated successfully or 0 if count was zero
+ */
+int lockref_get_not_zero(struct lockref *lockref)
+{
+	int retval = 0;
+
+	spin_lock(&lockref->lock);
+	if (lockref->count) {
+		lockref->count++;
+		retval = 1;
+	}
+	spin_unlock(&lockref->lock);
+	return retval;
+}
+EXPORT_SYMBOL(lockref_get_not_zero);
+
+/**
+ * lockref_get_or_lock - Increments count unless the count is 0
+ * @lockcnt: pointer to lockref structure
+ * Return: 1 if count updated successfully or 0 if count was zero
+ * and we got the lock instead.
+ */
+int lockref_get_or_lock(struct lockref *lockref)
+{
+	spin_lock(&lockref->lock);
+	if (!lockref->count)
+		return 0;
+	lockref->count++;
+	spin_unlock(&lockref->lock);
+	return 1;
+}
+EXPORT_SYMBOL(lockref_get_or_lock);
+
+/**
+ * lockref_put_or_lock - decrements count unless count <= 1 before decrement
+ * @lockcnt: pointer to lockref structure
+ * Return: 1 if count updated successfully or 0 if count <= 1 and lock taken
+ */
+int lockref_put_or_lock(struct lockref *lockref)
+{
+	spin_lock(&lockref->lock);
+	if (lockref->count <= 1)
+		return 0;
+	lockref->count--;
+	spin_unlock(&lockref->lock);
+	return 1;
+}
+EXPORT_SYMBOL(lockref_put_or_lock);
