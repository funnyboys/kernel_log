commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index ad485f08173b..5ee3526f71b8 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -12,7 +12,6 @@
 #include <linux/io.h>
 #include <linux/export.h>
 #include <asm/cacheflush.h>
-#include <asm/pgtable.h>
 
 #ifdef CONFIG_HAVE_ARCH_HUGE_VMAP
 static int __read_mostly ioremap_p4d_capable;

commit 6c0c7d2b365b21a413f6d75772a8a4a2c7d36916
Author: Joerg Roedel <jroedel@suse.de>
Date:   Mon Jun 1 21:52:26 2020 -0700

    mm/ioremap: track which page-table levels were modified
    
    Track at which levels in the page-table entries were modified by
    ioremap_page_range().
    
    After the page-table has been modified, use that information do decide
    whether the new arch_sync_kernel_mappings() needs to be called.  The
    iounmap path re-uses vunmap(), which has already been taken care of.
    
    Signed-off-by: Joerg Roedel <jroedel@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Matthew Wilcox (Oracle) <willy@infradead.org>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Link: http://lkml.kernel.org/r/20200515140023.25469-4-joro@8bytes.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 3f0e18543de8..ad485f08173b 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -61,13 +61,14 @@ static inline int ioremap_pmd_enabled(void) { return 0; }
 #endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
 
 static int ioremap_pte_range(pmd_t *pmd, unsigned long addr,
-		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot,
+		pgtbl_mod_mask *mask)
 {
 	pte_t *pte;
 	u64 pfn;
 
 	pfn = phys_addr >> PAGE_SHIFT;
-	pte = pte_alloc_kernel(pmd, addr);
+	pte = pte_alloc_kernel_track(pmd, addr, mask);
 	if (!pte)
 		return -ENOMEM;
 	do {
@@ -75,6 +76,7 @@ static int ioremap_pte_range(pmd_t *pmd, unsigned long addr,
 		set_pte_at(&init_mm, addr, pte, pfn_pte(pfn, prot));
 		pfn++;
 	} while (pte++, addr += PAGE_SIZE, addr != end);
+	*mask |= PGTBL_PTE_MODIFIED;
 	return 0;
 }
 
@@ -101,21 +103,24 @@ static int ioremap_try_huge_pmd(pmd_t *pmd, unsigned long addr,
 }
 
 static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
-		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot,
+		pgtbl_mod_mask *mask)
 {
 	pmd_t *pmd;
 	unsigned long next;
 
-	pmd = pmd_alloc(&init_mm, pud, addr);
+	pmd = pmd_alloc_track(&init_mm, pud, addr, mask);
 	if (!pmd)
 		return -ENOMEM;
 	do {
 		next = pmd_addr_end(addr, end);
 
-		if (ioremap_try_huge_pmd(pmd, addr, next, phys_addr, prot))
+		if (ioremap_try_huge_pmd(pmd, addr, next, phys_addr, prot)) {
+			*mask |= PGTBL_PMD_MODIFIED;
 			continue;
+		}
 
-		if (ioremap_pte_range(pmd, addr, next, phys_addr, prot))
+		if (ioremap_pte_range(pmd, addr, next, phys_addr, prot, mask))
 			return -ENOMEM;
 	} while (pmd++, phys_addr += (next - addr), addr = next, addr != end);
 	return 0;
@@ -144,21 +149,24 @@ static int ioremap_try_huge_pud(pud_t *pud, unsigned long addr,
 }
 
 static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,
-		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot,
+		pgtbl_mod_mask *mask)
 {
 	pud_t *pud;
 	unsigned long next;
 
-	pud = pud_alloc(&init_mm, p4d, addr);
+	pud = pud_alloc_track(&init_mm, p4d, addr, mask);
 	if (!pud)
 		return -ENOMEM;
 	do {
 		next = pud_addr_end(addr, end);
 
-		if (ioremap_try_huge_pud(pud, addr, next, phys_addr, prot))
+		if (ioremap_try_huge_pud(pud, addr, next, phys_addr, prot)) {
+			*mask |= PGTBL_PUD_MODIFIED;
 			continue;
+		}
 
-		if (ioremap_pmd_range(pud, addr, next, phys_addr, prot))
+		if (ioremap_pmd_range(pud, addr, next, phys_addr, prot, mask))
 			return -ENOMEM;
 	} while (pud++, phys_addr += (next - addr), addr = next, addr != end);
 	return 0;
@@ -187,21 +195,24 @@ static int ioremap_try_huge_p4d(p4d_t *p4d, unsigned long addr,
 }
 
 static inline int ioremap_p4d_range(pgd_t *pgd, unsigned long addr,
-		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot,
+		pgtbl_mod_mask *mask)
 {
 	p4d_t *p4d;
 	unsigned long next;
 
-	p4d = p4d_alloc(&init_mm, pgd, addr);
+	p4d = p4d_alloc_track(&init_mm, pgd, addr, mask);
 	if (!p4d)
 		return -ENOMEM;
 	do {
 		next = p4d_addr_end(addr, end);
 
-		if (ioremap_try_huge_p4d(p4d, addr, next, phys_addr, prot))
+		if (ioremap_try_huge_p4d(p4d, addr, next, phys_addr, prot)) {
+			*mask |= PGTBL_P4D_MODIFIED;
 			continue;
+		}
 
-		if (ioremap_pud_range(p4d, addr, next, phys_addr, prot))
+		if (ioremap_pud_range(p4d, addr, next, phys_addr, prot, mask))
 			return -ENOMEM;
 	} while (p4d++, phys_addr += (next - addr), addr = next, addr != end);
 	return 0;
@@ -214,6 +225,7 @@ int ioremap_page_range(unsigned long addr,
 	unsigned long start;
 	unsigned long next;
 	int err;
+	pgtbl_mod_mask mask = 0;
 
 	might_sleep();
 	BUG_ON(addr >= end);
@@ -222,13 +234,17 @@ int ioremap_page_range(unsigned long addr,
 	pgd = pgd_offset_k(addr);
 	do {
 		next = pgd_addr_end(addr, end);
-		err = ioremap_p4d_range(pgd, addr, next, phys_addr, prot);
+		err = ioremap_p4d_range(pgd, addr, next, phys_addr, prot,
+					&mask);
 		if (err)
 			break;
 	} while (pgd++, phys_addr += (next - addr), addr = next, addr != end);
 
 	flush_cache_vmap(start, end);
 
+	if (mask & ARCH_PAGE_TABLE_SYNC_MASK)
+		arch_sync_kernel_mappings(start, end);
+
 	return err;
 }
 

commit 80b0ca98f91ddbc09828aff5a00af1c73837713e
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Aug 13 11:24:04 2019 +0200

    lib: provide a simple generic ioremap implementation
    
    A lot of architectures reuse the same simple ioremap implementation, so
    start lifting the most simple variant to lib/ioremap.c.  It provides
    ioremap_prot and iounmap, plus a default ioremap that uses prot_noncached,
    although that can be overridden by asm/io.h.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Reviewed-by: Palmer Dabbelt <palmer@dabbelt.com>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 0a2ffadc6d71..3f0e18543de8 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -231,3 +231,42 @@ int ioremap_page_range(unsigned long addr,
 
 	return err;
 }
+
+#ifdef CONFIG_GENERIC_IOREMAP
+void __iomem *ioremap_prot(phys_addr_t addr, size_t size, unsigned long prot)
+{
+	unsigned long offset, vaddr;
+	phys_addr_t last_addr;
+	struct vm_struct *area;
+
+	/* Disallow wrap-around or zero size */
+	last_addr = addr + size - 1;
+	if (!size || last_addr < addr)
+		return NULL;
+
+	/* Page-align mappings */
+	offset = addr & (~PAGE_MASK);
+	addr -= offset;
+	size = PAGE_ALIGN(size + offset);
+
+	area = get_vm_area_caller(size, VM_IOREMAP,
+			__builtin_return_address(0));
+	if (!area)
+		return NULL;
+	vaddr = (unsigned long)area->addr;
+
+	if (ioremap_page_range(vaddr, vaddr + size, addr, __pgprot(prot))) {
+		free_vm_area(area);
+		return NULL;
+	}
+
+	return (void __iomem *)(vaddr + offset);
+}
+EXPORT_SYMBOL(ioremap_prot);
+
+void iounmap(volatile void __iomem *addr)
+{
+	vunmap((void *)((unsigned long)addr & PAGE_MASK));
+}
+EXPORT_SYMBOL(iounmap);
+#endif /* CONFIG_GENERIC_IOREMAP */

commit 0f472d04f59ff89d15b2a1c4eafde7317ddd67a2
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Jul 16 16:27:33 2019 -0700

    mm/ioremap: probe platform for p4d huge map support
    
    Finish up what commit c2febafc6773 ("mm: convert generic code to 5-level
    paging") started while levelling up P4D huge mapping support at par with
    PUD and PMD.  A new arch call back arch_ioremap_p4d_supported() is added
    which just maintains status quo (P4D huge map not supported) on x86,
    arm64 and powerpc.
    
    When HAVE_ARCH_HUGE_VMAP is enabled its just a simple check from the
    arch about the support, hence runtime effects are minimal.
    
    Link: http://lkml.kernel.org/r/1561699231-20991-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index a95161d9c883..0a2ffadc6d71 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -30,6 +30,8 @@ early_param("nohugeiomap", set_nohugeiomap);
 void __init ioremap_huge_init(void)
 {
 	if (!ioremap_huge_disabled) {
+		if (arch_ioremap_p4d_supported())
+			ioremap_p4d_capable = 1;
 		if (arch_ioremap_pud_supported())
 			ioremap_pud_capable = 1;
 		if (arch_ioremap_pmd_supported())

commit 6b95ab4218bfa59bc315105127ffe03aef3b5742
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Jul 16 16:27:30 2019 -0700

    mm/ioremap: check virtual address alignment while creating huge mappings
    
    Virtual address alignment is essential in ensuring correct clearing for
    all intermediate level pgtable entries and freeing associated pgtable
    pages.  An unaligned address can end up randomly freeing pgtable page
    that potentially still contains valid mappings.  Hence also check it's
    alignment along with existing phys_addr check.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Chintan Pandya <cpandya@codeaurora.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 063213685563..a95161d9c883 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -86,6 +86,9 @@ static int ioremap_try_huge_pmd(pmd_t *pmd, unsigned long addr,
 	if ((end - addr) != PMD_SIZE)
 		return 0;
 
+	if (!IS_ALIGNED(addr, PMD_SIZE))
+		return 0;
+
 	if (!IS_ALIGNED(phys_addr, PMD_SIZE))
 		return 0;
 
@@ -126,6 +129,9 @@ static int ioremap_try_huge_pud(pud_t *pud, unsigned long addr,
 	if ((end - addr) != PUD_SIZE)
 		return 0;
 
+	if (!IS_ALIGNED(addr, PUD_SIZE))
+		return 0;
+
 	if (!IS_ALIGNED(phys_addr, PUD_SIZE))
 		return 0;
 
@@ -166,6 +172,9 @@ static int ioremap_try_huge_p4d(p4d_t *p4d, unsigned long addr,
 	if ((end - addr) != P4D_SIZE)
 		return 0;
 
+	if (!IS_ALIGNED(addr, P4D_SIZE))
+		return 0;
+
 	if (!IS_ALIGNED(phys_addr, P4D_SIZE))
 		return 0;
 

commit 8e2d43405b22e98cf5f3730c1829ec1fdbe17ae7
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Dec 28 00:37:53 2018 -0800

    lib/ioremap: ensure break-before-make is used for huge p4d mappings
    
    Whilst no architectures actually enable support for huge p4d mappings in
    the vmap area, the code that is implemented should be using
    break-before-make, as we do for pud and pmd huge entries.
    
    Link: http://lkml.kernel.org/r/1544120495-17438-6-git-send-email-will.deacon@arm.com
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Toshi Kani <toshi.kani@hpe.com>
    Cc: Chintan Pandya <cpandya@codeaurora.org>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 10d7c5485c39..063213685563 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -156,6 +156,25 @@ static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,
 	return 0;
 }
 
+static int ioremap_try_huge_p4d(p4d_t *p4d, unsigned long addr,
+				unsigned long end, phys_addr_t phys_addr,
+				pgprot_t prot)
+{
+	if (!ioremap_p4d_enabled())
+		return 0;
+
+	if ((end - addr) != P4D_SIZE)
+		return 0;
+
+	if (!IS_ALIGNED(phys_addr, P4D_SIZE))
+		return 0;
+
+	if (p4d_present(*p4d) && !p4d_free_pud_page(p4d, addr))
+		return 0;
+
+	return p4d_set_huge(p4d, phys_addr, prot);
+}
+
 static inline int ioremap_p4d_range(pgd_t *pgd, unsigned long addr,
 		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
@@ -168,12 +187,8 @@ static inline int ioremap_p4d_range(pgd_t *pgd, unsigned long addr,
 	do {
 		next = p4d_addr_end(addr, end);
 
-		if (ioremap_p4d_enabled() &&
-		    ((next - addr) == P4D_SIZE) &&
-		    IS_ALIGNED(phys_addr, P4D_SIZE)) {
-			if (p4d_set_huge(p4d, phys_addr, prot))
-				continue;
-		}
+		if (ioremap_try_huge_p4d(p4d, addr, next, phys_addr, prot))
+			continue;
 
 		if (ioremap_pud_range(p4d, addr, next, phys_addr, prot))
 			return -ENOMEM;

commit 36ddc5a78c878e9b10c323d2fe88b0dae2f487eb
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Dec 28 00:37:49 2018 -0800

    lib/ioremap: ensure phys_addr actually corresponds to a physical address
    
    The current ioremap() code uses a phys_addr variable at each level of page
    table, which is confusingly offset by subtracting the base virtual address
    being mapped so that adding the current virtual address back on when
    iterating through the page table entries gives back the corresponding
    physical address.
    
    This is fairly confusing and results in all users of phys_addr having to
    add the current virtual address back on.  Instead, this patch just updates
    phys_addr when iterating over the page table entries, ensuring that it's
    always up-to-date and doesn't require explicit offsetting.
    
    Link: http://lkml.kernel.org/r/1544120495-17438-5-git-send-email-will.deacon@arm.com
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: Chintan Pandya <cpandya@codeaurora.org>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 6c72764af19c..10d7c5485c39 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -101,19 +101,18 @@ static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
 	pmd_t *pmd;
 	unsigned long next;
 
-	phys_addr -= addr;
 	pmd = pmd_alloc(&init_mm, pud, addr);
 	if (!pmd)
 		return -ENOMEM;
 	do {
 		next = pmd_addr_end(addr, end);
 
-		if (ioremap_try_huge_pmd(pmd, addr, next, phys_addr + addr, prot))
+		if (ioremap_try_huge_pmd(pmd, addr, next, phys_addr, prot))
 			continue;
 
-		if (ioremap_pte_range(pmd, addr, next, phys_addr + addr, prot))
+		if (ioremap_pte_range(pmd, addr, next, phys_addr, prot))
 			return -ENOMEM;
-	} while (pmd++, addr = next, addr != end);
+	} while (pmd++, phys_addr += (next - addr), addr = next, addr != end);
 	return 0;
 }
 
@@ -142,19 +141,18 @@ static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,
 	pud_t *pud;
 	unsigned long next;
 
-	phys_addr -= addr;
 	pud = pud_alloc(&init_mm, p4d, addr);
 	if (!pud)
 		return -ENOMEM;
 	do {
 		next = pud_addr_end(addr, end);
 
-		if (ioremap_try_huge_pud(pud, addr, next, phys_addr + addr, prot))
+		if (ioremap_try_huge_pud(pud, addr, next, phys_addr, prot))
 			continue;
 
-		if (ioremap_pmd_range(pud, addr, next, phys_addr + addr, prot))
+		if (ioremap_pmd_range(pud, addr, next, phys_addr, prot))
 			return -ENOMEM;
-	} while (pud++, addr = next, addr != end);
+	} while (pud++, phys_addr += (next - addr), addr = next, addr != end);
 	return 0;
 }
 
@@ -164,7 +162,6 @@ static inline int ioremap_p4d_range(pgd_t *pgd, unsigned long addr,
 	p4d_t *p4d;
 	unsigned long next;
 
-	phys_addr -= addr;
 	p4d = p4d_alloc(&init_mm, pgd, addr);
 	if (!p4d)
 		return -ENOMEM;
@@ -173,14 +170,14 @@ static inline int ioremap_p4d_range(pgd_t *pgd, unsigned long addr,
 
 		if (ioremap_p4d_enabled() &&
 		    ((next - addr) == P4D_SIZE) &&
-		    IS_ALIGNED(phys_addr + addr, P4D_SIZE)) {
-			if (p4d_set_huge(p4d, phys_addr + addr, prot))
+		    IS_ALIGNED(phys_addr, P4D_SIZE)) {
+			if (p4d_set_huge(p4d, phys_addr, prot))
 				continue;
 		}
 
-		if (ioremap_pud_range(p4d, addr, next, phys_addr + addr, prot))
+		if (ioremap_pud_range(p4d, addr, next, phys_addr, prot))
 			return -ENOMEM;
-	} while (p4d++, addr = next, addr != end);
+	} while (p4d++, phys_addr += (next - addr), addr = next, addr != end);
 	return 0;
 }
 
@@ -196,14 +193,13 @@ int ioremap_page_range(unsigned long addr,
 	BUG_ON(addr >= end);
 
 	start = addr;
-	phys_addr -= addr;
 	pgd = pgd_offset_k(addr);
 	do {
 		next = pgd_addr_end(addr, end);
-		err = ioremap_p4d_range(pgd, addr, next, phys_addr+addr, prot);
+		err = ioremap_p4d_range(pgd, addr, next, phys_addr, prot);
 		if (err)
 			break;
-	} while (pgd++, addr = next, addr != end);
+	} while (pgd++, phys_addr += (next - addr), addr = next, addr != end);
 
 	flush_cache_vmap(start, end);
 

commit d239865ac804c91a621294ca7bece4241b006fae
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Dec 28 00:37:38 2018 -0800

    ioremap: rework pXd_free_pYd_page() API
    
    The recently merged API for ensuring break-before-make on page-table
    entries when installing huge mappings in the vmalloc/ioremap region is
    fairly counter-intuitive, resulting in the arch freeing functions (e.g.
    pmd_free_pte_page()) being called even on entries that aren't present.
    This resulted in a minor bug in the arm64 implementation, giving rise to
    spurious VM_WARN messages.
    
    This patch moves the pXd_present() checks out into the core code,
    refactoring the callsites at the same time so that we avoid the complex
    conjunctions when determining whether or not we can put down a huge
    mapping.
    
    Link: http://lkml.kernel.org/r/1544120495-17438-2-git-send-email-will.deacon@arm.com
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Toshi Kani <toshi.kani@hpe.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Chintan Pandya <cpandya@codeaurora.org>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 517f5853ffed..6c72764af19c 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -76,6 +76,25 @@ static int ioremap_pte_range(pmd_t *pmd, unsigned long addr,
 	return 0;
 }
 
+static int ioremap_try_huge_pmd(pmd_t *pmd, unsigned long addr,
+				unsigned long end, phys_addr_t phys_addr,
+				pgprot_t prot)
+{
+	if (!ioremap_pmd_enabled())
+		return 0;
+
+	if ((end - addr) != PMD_SIZE)
+		return 0;
+
+	if (!IS_ALIGNED(phys_addr, PMD_SIZE))
+		return 0;
+
+	if (pmd_present(*pmd) && !pmd_free_pte_page(pmd, addr))
+		return 0;
+
+	return pmd_set_huge(pmd, phys_addr, prot);
+}
+
 static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
 		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
@@ -89,13 +108,8 @@ static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
 	do {
 		next = pmd_addr_end(addr, end);
 
-		if (ioremap_pmd_enabled() &&
-		    ((next - addr) == PMD_SIZE) &&
-		    IS_ALIGNED(phys_addr + addr, PMD_SIZE) &&
-		    pmd_free_pte_page(pmd, addr)) {
-			if (pmd_set_huge(pmd, phys_addr + addr, prot))
-				continue;
-		}
+		if (ioremap_try_huge_pmd(pmd, addr, next, phys_addr + addr, prot))
+			continue;
 
 		if (ioremap_pte_range(pmd, addr, next, phys_addr + addr, prot))
 			return -ENOMEM;
@@ -103,6 +117,25 @@ static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
 	return 0;
 }
 
+static int ioremap_try_huge_pud(pud_t *pud, unsigned long addr,
+				unsigned long end, phys_addr_t phys_addr,
+				pgprot_t prot)
+{
+	if (!ioremap_pud_enabled())
+		return 0;
+
+	if ((end - addr) != PUD_SIZE)
+		return 0;
+
+	if (!IS_ALIGNED(phys_addr, PUD_SIZE))
+		return 0;
+
+	if (pud_present(*pud) && !pud_free_pmd_page(pud, addr))
+		return 0;
+
+	return pud_set_huge(pud, phys_addr, prot);
+}
+
 static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,
 		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
@@ -116,13 +149,8 @@ static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,
 	do {
 		next = pud_addr_end(addr, end);
 
-		if (ioremap_pud_enabled() &&
-		    ((next - addr) == PUD_SIZE) &&
-		    IS_ALIGNED(phys_addr + addr, PUD_SIZE) &&
-		    pud_free_pmd_page(pud, addr)) {
-			if (pud_set_huge(pud, phys_addr + addr, prot))
-				continue;
-		}
+		if (ioremap_try_huge_pud(pud, addr, next, phys_addr + addr, prot))
+			continue;
 
 		if (ioremap_pmd_range(pud, addr, next, phys_addr + addr, prot))
 			return -ENOMEM;

commit 785a19f9d1dd8a4ab2d0633be4656653bd3de1fc
Author: Chintan Pandya <cpandya@codeaurora.org>
Date:   Wed Jun 27 08:13:47 2018 -0600

    ioremap: Update pgtable free interfaces with addr
    
    The following kernel panic was observed on ARM64 platform due to a stale
    TLB entry.
    
     1. ioremap with 4K size, a valid pte page table is set.
     2. iounmap it, its pte entry is set to 0.
     3. ioremap the same address with 2M size, update its pmd entry with
        a new value.
     4. CPU may hit an exception because the old pmd entry is still in TLB,
        which leads to a kernel panic.
    
    Commit b6bdb7517c3d ("mm/vmalloc: add interfaces to free unmapped page
    table") has addressed this panic by falling to pte mappings in the above
    case on ARM64.
    
    To support pmd mappings in all cases, TLB purge needs to be performed
    in this case on ARM64.
    
    Add a new arg, 'addr', to pud_free_pmd_page() and pmd_free_pte_page()
    so that TLB purge can be added later in seprate patches.
    
    [toshi.kani@hpe.com: merge changes, rewrite patch description]
    Fixes: 28ee90fe6048 ("x86/mm: implement free pmd/pte page interfaces")
    Signed-off-by: Chintan Pandya <cpandya@codeaurora.org>
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: mhocko@suse.com
    Cc: akpm@linux-foundation.org
    Cc: hpa@zytor.com
    Cc: linux-mm@kvack.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: stable@vger.kernel.org
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <stable@vger.kernel.org>
    Link: https://lkml.kernel.org/r/20180627141348.21777-3-toshi.kani@hpe.com

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 54e5bbaa3200..517f5853ffed 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -92,7 +92,7 @@ static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
 		if (ioremap_pmd_enabled() &&
 		    ((next - addr) == PMD_SIZE) &&
 		    IS_ALIGNED(phys_addr + addr, PMD_SIZE) &&
-		    pmd_free_pte_page(pmd)) {
+		    pmd_free_pte_page(pmd, addr)) {
 			if (pmd_set_huge(pmd, phys_addr + addr, prot))
 				continue;
 		}
@@ -119,7 +119,7 @@ static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,
 		if (ioremap_pud_enabled() &&
 		    ((next - addr) == PUD_SIZE) &&
 		    IS_ALIGNED(phys_addr + addr, PUD_SIZE) &&
-		    pud_free_pmd_page(pud)) {
+		    pud_free_pmd_page(pud, addr)) {
 			if (pud_set_huge(pud, phys_addr + addr, prot))
 				continue;
 		}

commit b6bdb7517c3d3f41f20e5c2948d6bc3f8897394e
Author: Toshi Kani <toshi.kani@hpe.com>
Date:   Thu Mar 22 16:17:20 2018 -0700

    mm/vmalloc: add interfaces to free unmapped page table
    
    On architectures with CONFIG_HAVE_ARCH_HUGE_VMAP set, ioremap() may
    create pud/pmd mappings.  A kernel panic was observed on arm64 systems
    with Cortex-A75 in the following steps as described by Hanjun Guo.
    
     1. ioremap a 4K size, valid page table will build,
     2. iounmap it, pte0 will set to 0;
     3. ioremap the same address with 2M size, pgd/pmd is unchanged,
        then set the a new value for pmd;
     4. pte0 is leaked;
     5. CPU may meet exception because the old pmd is still in TLB,
        which will lead to kernel panic.
    
    This panic is not reproducible on x86.  INVLPG, called from iounmap,
    purges all levels of entries associated with purged address on x86.  x86
    still has memory leak.
    
    The patch changes the ioremap path to free unmapped page table(s) since
    doing so in the unmap path has the following issues:
    
     - The iounmap() path is shared with vunmap(). Since vmap() only
       supports pte mappings, making vunmap() to free a pte page is an
       overhead for regular vmap users as they do not need a pte page freed
       up.
    
     - Checking if all entries in a pte page are cleared in the unmap path
       is racy, and serializing this check is expensive.
    
     - The unmap path calls free_vmap_area_noflush() to do lazy TLB purges.
       Clearing a pud/pmd entry before the lazy TLB purges needs extra TLB
       purge.
    
    Add two interfaces, pud_free_pmd_page() and pmd_free_pte_page(), which
    clear a given pud/pmd entry and free up a page for the lower level
    entries.
    
    This patch implements their stub functions on x86 and arm64, which work
    as workaround.
    
    [akpm@linux-foundation.org: fix typo in pmd_free_pte_page() stub]
    Link: http://lkml.kernel.org/r/20180314180155.19492-2-toshi.kani@hpe.com
    Fixes: e61ce6ade404e ("mm: change ioremap to set up huge I/O mappings")
    Reported-by: Lei Li <lious.lilei@hisilicon.com>
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Wang Xuefeng <wxf.wang@hisilicon.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Hanjun Guo <guohanjun@huawei.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Chintan Pandya <cpandya@codeaurora.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index b808a390e4c3..54e5bbaa3200 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -91,7 +91,8 @@ static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
 
 		if (ioremap_pmd_enabled() &&
 		    ((next - addr) == PMD_SIZE) &&
-		    IS_ALIGNED(phys_addr + addr, PMD_SIZE)) {
+		    IS_ALIGNED(phys_addr + addr, PMD_SIZE) &&
+		    pmd_free_pte_page(pmd)) {
 			if (pmd_set_huge(pmd, phys_addr + addr, prot))
 				continue;
 		}
@@ -117,7 +118,8 @@ static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,
 
 		if (ioremap_pud_enabled() &&
 		    ((next - addr) == PUD_SIZE) &&
-		    IS_ALIGNED(phys_addr + addr, PUD_SIZE)) {
+		    IS_ALIGNED(phys_addr + addr, PUD_SIZE) &&
+		    pud_free_pmd_page(pud)) {
 			if (pud_set_huge(pud, phys_addr + addr, prot))
 				continue;
 		}

commit ead751507de86d90fa250431e9990a8b881f713c
Merge: fdebad11e50e e2be04c7f995
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 2 10:04:46 2017 -0700

    Merge tag 'spdx_identifiers-4.14-rc8' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core
    
    Pull initial SPDX identifiers from Greg KH:
     "License cleanup: add SPDX license identifiers to some files
    
      Many source files in the tree are missing licensing information, which
      makes it harder for compliance tools to determine the correct license.
    
      By default all files without license information are under the default
      license of the kernel, which is GPL version 2.
    
      Update the files which contain no license information with the
      'GPL-2.0' SPDX license identifier. The SPDX identifier is a legally
      binding shorthand, which can be used instead of the full boiler plate
      text.
    
      This patch is based on work done by Thomas Gleixner and Kate Stewart
      and Philippe Ombredanne.
    
      How this work was done:
    
      Patches were generated and checked against linux-4.14-rc6 for a subset
      of the use cases:
    
       - file had no licensing information it it.
    
       - file was a */uapi/* one with no licensing information in it,
    
       - file was a */uapi/* one with existing licensing information,
    
      Further patches will be generated in subsequent months to fix up cases
      where non-standard license headers were used, and references to
      license had to be inferred by heuristics based on keywords.
    
      The analysis to determine which SPDX License Identifier to be applied
      to a file was done in a spreadsheet of side by side results from of
      the output of two independent scanners (ScanCode & Windriver)
      producing SPDX tag:value files created by Philippe Ombredanne.
      Philippe prepared the base worksheet, and did an initial spot review
      of a few 1000 files.
    
      The 4.13 kernel was the starting point of the analysis with 60,537
      files assessed. Kate Stewart did a file by file comparison of the
      scanner results in the spreadsheet to determine which SPDX license
      identifier(s) to be applied to the file. She confirmed any
      determination that was not immediately clear with lawyers working with
      the Linux Foundation.
    
      Criteria used to select files for SPDX license identifier tagging was:
    
       - Files considered eligible had to be source code files.
    
       - Make and config files were included as candidates if they contained
         >5 lines of source
    
       - File already had some variant of a license header in it (even if <5
         lines).
    
      All documentation files were explicitly excluded.
    
      The following heuristics were used to determine which SPDX license
      identifiers to apply.
    
       - when both scanners couldn't find any license traces, file was
         considered to have no license information in it, and the top level
         COPYING file license applied.
    
         For non */uapi/* files that summary was:
    
           SPDX license identifier                            # files
           ---------------------------------------------------|-------
           GPL-2.0                                              11139
    
         and resulted in the first patch in this series.
    
         If that file was a */uapi/* path one, it was "GPL-2.0 WITH
         Linux-syscall-note" otherwise it was "GPL-2.0". Results of that
         was:
    
           SPDX license identifier                            # files
           ---------------------------------------------------|-------
           GPL-2.0 WITH Linux-syscall-note                        930
    
         and resulted in the second patch in this series.
    
       - if a file had some form of licensing information in it, and was one
         of the */uapi/* ones, it was denoted with the Linux-syscall-note if
         any GPL family license was found in the file or had no licensing in
         it (per prior point). Results summary:
    
           SPDX license identifier                            # files
           ---------------------------------------------------|------
           GPL-2.0 WITH Linux-syscall-note                       270
           GPL-2.0+ WITH Linux-syscall-note                      169
           ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
           ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
           LGPL-2.1+ WITH Linux-syscall-note                      15
           GPL-1.0+ WITH Linux-syscall-note                       14
           ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
           LGPL-2.0+ WITH Linux-syscall-note                       4
           LGPL-2.1 WITH Linux-syscall-note                        3
           ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
           ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
         and that resulted in the third patch in this series.
    
       - when the two scanners agreed on the detected license(s), that
         became the concluded license(s).
    
       - when there was disagreement between the two scanners (one detected
         a license but the other didn't, or they both detected different
         licenses) a manual inspection of the file occurred.
    
       - In most cases a manual inspection of the information in the file
         resulted in a clear resolution of the license that should apply
         (and which scanner probably needed to revisit its heuristics).
    
       - When it was not immediately clear, the license identifier was
         confirmed with lawyers working with the Linux Foundation.
    
       - If there was any question as to the appropriate license identifier,
         the file was flagged for further research and to be revisited later
         in time.
    
      In total, over 70 hours of logged manual review was done on the
      spreadsheet to determine the SPDX license identifiers to apply to the
      source files by Kate, Philippe, Thomas and, in some cases,
      confirmation by lawyers working with the Linux Foundation.
    
      Kate also obtained a third independent scan of the 4.13 code base from
      FOSSology, and compared selected files where the other two scanners
      disagreed against that SPDX file, to see if there was new insights.
      The Windriver scanner is based on an older version of FOSSology in
      part, so they are related.
    
      Thomas did random spot checks in about 500 files from the spreadsheets
      for the uapi headers and agreed with SPDX license identifier in the
      files he inspected. For the non-uapi files Thomas did random spot
      checks in about 15000 files.
    
      In initial set of patches against 4.14-rc6, 3 files were found to have
      copy/paste license identifier errors, and have been fixed to reflect
      the correct identifier.
    
      Additionally Philippe spent 10 hours this week doing a detailed manual
      inspection and review of the 12,461 patched files from the initial
      patch version early this week with:
    
       - a full scancode scan run, collecting the matched texts, detected
         license ids and scores
    
       - reviewing anything where there was a license detected (about 500+
         files) to ensure that the applied SPDX license was correct
    
       - reviewing anything where there was no detection but the patch
         license was not GPL-2.0 WITH Linux-syscall-note to ensure that the
         applied SPDX license was correct
    
      This produced a worksheet with 20 files needing minor correction. This
      worksheet was then exported into 3 different .csv files for the
      different types of files to be modified.
    
      These .csv files were then reviewed by Greg. Thomas wrote a script to
      parse the csv files and add the proper SPDX tag to the file, in the
      format that the file expected. This script was further refined by Greg
      based on the output to detect more types of files automatically and to
      distinguish between header and source .c files (which need different
      comment types.) Finally Greg ran the script using the .csv files to
      generate the patches.
    
      Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
      Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
      Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
      Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>"
    
    * tag 'spdx_identifiers-4.14-rc8' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/driver-core:
      License cleanup: add SPDX license identifier to uapi header files with a license
      License cleanup: add SPDX license identifier to uapi header files with no license
      License cleanup: add SPDX GPL-2.0 license identifier to files with no license

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 4bb30206b942..ac7802b0a117 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Re-map IO memory to kernel address space so that we can access it.
  * This is needed for high PCI addresses that aren't mapped in the

commit b39ab98e2f4728d98973fd1bc531e3c4cbccb21c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 30 10:09:56 2017 -0700

    Mark 'ioremap_page_range()' as possibly sleeping
    
    It turns out that some drivers seem to think it's ok to remap page
    ranges from within interrupts and even NMI's.  That is definitely not
    the case, since the page table build-up is simply not interrupt-safe.
    
    This showed up in the zero-day robot that reported it for the ACPI APEI
    GHES ("Generic Hardware Error Source") driver.  Normally it had been
    hidden by the fact that no page table operations had been needed because
    the vmalloc area had been set up by other things.
    
    Apparently due to a recent change to the GHEI driver: commit
    77b246b32b2c ("acpi: apei: check for pending errors when probing GHES
    entries") 0day actually caught a case during bootup whenthe ioremap
    called down to page allocation.  But that recent change only showed the
    symptom, it wasn't the root cause of the problem.
    
    Hopefully it is limited to just that one driver.
    
    If you need to access random physical memory, you either need to ioremap
    in process context, or you need to use the FIXMAP facility to set one
    particular fixmap entry to the required mapping - that can be done safely.
    
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Tyler Baicar <tbaicar@codeaurora.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 4bb30206b942..c835f9080c43 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -161,6 +161,7 @@ int ioremap_page_range(unsigned long addr,
 	unsigned long next;
 	int err;
 
+	might_sleep();
 	BUG_ON(addr >= end);
 
 	start = addr;

commit c2febafc67734a62196c1b9dfba926412d4077ba
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Mar 9 17:24:07 2017 +0300

    mm: convert generic code to 5-level paging
    
    Convert all non-architecture-specific code to 5-level paging.
    
    It's mostly mechanical adding handling one more page table level in
    places where we deal with pud_t.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index a3e14ce92a56..4bb30206b942 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -14,6 +14,7 @@
 #include <asm/pgtable.h>
 
 #ifdef CONFIG_HAVE_ARCH_HUGE_VMAP
+static int __read_mostly ioremap_p4d_capable;
 static int __read_mostly ioremap_pud_capable;
 static int __read_mostly ioremap_pmd_capable;
 static int __read_mostly ioremap_huge_disabled;
@@ -35,6 +36,11 @@ void __init ioremap_huge_init(void)
 	}
 }
 
+static inline int ioremap_p4d_enabled(void)
+{
+	return ioremap_p4d_capable;
+}
+
 static inline int ioremap_pud_enabled(void)
 {
 	return ioremap_pud_capable;
@@ -46,6 +52,7 @@ static inline int ioremap_pmd_enabled(void)
 }
 
 #else	/* !CONFIG_HAVE_ARCH_HUGE_VMAP */
+static inline int ioremap_p4d_enabled(void) { return 0; }
 static inline int ioremap_pud_enabled(void) { return 0; }
 static inline int ioremap_pmd_enabled(void) { return 0; }
 #endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
@@ -94,14 +101,14 @@ static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
 	return 0;
 }
 
-static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,
+static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,
 		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
 	pud_t *pud;
 	unsigned long next;
 
 	phys_addr -= addr;
-	pud = pud_alloc(&init_mm, pgd, addr);
+	pud = pud_alloc(&init_mm, p4d, addr);
 	if (!pud)
 		return -ENOMEM;
 	do {
@@ -120,6 +127,32 @@ static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,
 	return 0;
 }
 
+static inline int ioremap_p4d_range(pgd_t *pgd, unsigned long addr,
+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
+{
+	p4d_t *p4d;
+	unsigned long next;
+
+	phys_addr -= addr;
+	p4d = p4d_alloc(&init_mm, pgd, addr);
+	if (!p4d)
+		return -ENOMEM;
+	do {
+		next = p4d_addr_end(addr, end);
+
+		if (ioremap_p4d_enabled() &&
+		    ((next - addr) == P4D_SIZE) &&
+		    IS_ALIGNED(phys_addr + addr, P4D_SIZE)) {
+			if (p4d_set_huge(p4d, phys_addr + addr, prot))
+				continue;
+		}
+
+		if (ioremap_pud_range(p4d, addr, next, phys_addr + addr, prot))
+			return -ENOMEM;
+	} while (p4d++, addr = next, addr != end);
+	return 0;
+}
+
 int ioremap_page_range(unsigned long addr,
 		       unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
@@ -135,7 +168,7 @@ int ioremap_page_range(unsigned long addr,
 	pgd = pgd_offset_k(addr);
 	do {
 		next = pgd_addr_end(addr, end);
-		err = ioremap_pud_range(pgd, addr, next, phys_addr+addr, prot);
+		err = ioremap_p4d_range(pgd, addr, next, phys_addr+addr, prot);
 		if (err)
 			break;
 	} while (pgd++, addr = next, addr != end);

commit 3277953de2f31dd03c6375e9a9f680ac37fc9d27
Author: zhong jiang <zhongjiang@huawei.com>
Date:   Tue Jan 24 15:18:52 2017 -0800

    mm: do not export ioremap_page_range symbol for external module
    
    Recently, I've found cases in which ioremap_page_range was used
    incorrectly, in external modules, leading to crashes.  This can be
    partly attributed to the fact that ioremap_page_range is lower-level,
    with fewer protections, as compared to the other functions that an
    external module would typically call.  Those include:
    
         ioremap_cache
         ioremap_nocache
         ioremap_prot
         ioremap_uc
         ioremap_wc
         ioremap_wt
    
    ...each of which wraps __ioremap_caller, which in turn provides a safer
    way to achieve the mapping.
    
    Therefore, stop EXPORT-ing ioremap_page_range.
    
    Link: http://lkml.kernel.org/r/1485173220-29010-1-git-send-email-zhongjiang@huawei.com
    Signed-off-by: zhong jiang <zhongjiang@huawei.com>
    Reviewed-by: John Hubbard <jhubbard@nvidia.com>
    Suggested-by: John Hubbard <jhubbard@nvidia.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 86c8911b0e3a..a3e14ce92a56 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -144,4 +144,3 @@ int ioremap_page_range(unsigned long addr,
 
 	return err;
 }
-EXPORT_SYMBOL_GPL(ioremap_page_range);

commit 6b6378355b925050eb6fa966742d8c2d65ff0d83
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Tue Apr 14 15:47:32 2015 -0700

    x86, mm: support huge KVA mappings on x86
    
    Implement huge KVA mapping interfaces on x86.
    
    On x86, MTRRs can override PAT memory types with a 4KB granularity.  When
    using a huge page, MTRRs can override the memory type of the huge page,
    which may lead a performance penalty.  The processor can also behave in an
    undefined manner if a huge page is mapped to a memory range that MTRRs
    have mapped with multiple different memory types.  Therefore, the mapping
    code falls back to use a smaller page size toward 4KB when a mapping range
    is covered by non-WB type of MTRRs.  The WB type of MTRRs has no affect on
    the PAT memory types.
    
    pud_set_huge() and pmd_set_huge() call mtrr_type_lookup() to see if a
    given range is covered by MTRRs.  MTRR_TYPE_WRBACK indicates that the
    range is either covered by WB or not covered and the MTRR default value is
    set to WB.  0xFF indicates that MTRRs are disabled.
    
    HAVE_ARCH_HUGE_VMAP is selected when X86_64 or X86_32 with X86_PAE is set.
     X86_32 without X86_PAE is not supported since such config can unlikey be
    benefited from this feature, and there was an issue found in testing.
    
    [fengguang.wu@intel.com: ioremap_pud_capable can be static]
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Robert Elliott <Elliott@hp.com>
    Signed-off-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index be249062ba6e..86c8911b0e3a 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -14,9 +14,9 @@
 #include <asm/pgtable.h>
 
 #ifdef CONFIG_HAVE_ARCH_HUGE_VMAP
-int __read_mostly ioremap_pud_capable;
-int __read_mostly ioremap_pmd_capable;
-int __read_mostly ioremap_huge_disabled;
+static int __read_mostly ioremap_pud_capable;
+static int __read_mostly ioremap_pmd_capable;
+static int __read_mostly ioremap_huge_disabled;
 
 static int __init set_nohugeiomap(char *str)
 {

commit e61ce6ade404ef17bd63d62bec78e29047a47b47
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Tue Apr 14 15:47:23 2015 -0700

    mm: change ioremap to set up huge I/O mappings
    
    ioremap_pud_range() and ioremap_pmd_range() are changed to create huge I/O
    mappings when their capability is enabled, and a request meets required
    conditions -- both virtual & physical addresses are aligned by their huge
    page size, and a requested range fufills their huge page size.  When
    pud_set_huge() or pmd_set_huge() returns zero, i.e.  no-operation is
    performed, the code simply falls back to the next level.
    
    The changes are only enabled when CONFIG_HAVE_ARCH_HUGE_VMAP is defined on
    the architecture.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Robert Elliott <Elliott@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 2008652c9a1f..be249062ba6e 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -80,6 +80,14 @@ static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
 		return -ENOMEM;
 	do {
 		next = pmd_addr_end(addr, end);
+
+		if (ioremap_pmd_enabled() &&
+		    ((next - addr) == PMD_SIZE) &&
+		    IS_ALIGNED(phys_addr + addr, PMD_SIZE)) {
+			if (pmd_set_huge(pmd, phys_addr + addr, prot))
+				continue;
+		}
+
 		if (ioremap_pte_range(pmd, addr, next, phys_addr + addr, prot))
 			return -ENOMEM;
 	} while (pmd++, addr = next, addr != end);
@@ -98,6 +106,14 @@ static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,
 		return -ENOMEM;
 	do {
 		next = pud_addr_end(addr, end);
+
+		if (ioremap_pud_enabled() &&
+		    ((next - addr) == PUD_SIZE) &&
+		    IS_ALIGNED(phys_addr + addr, PUD_SIZE)) {
+			if (pud_set_huge(pud, phys_addr + addr, prot))
+				continue;
+		}
+
 		if (ioremap_pmd_range(pud, addr, next, phys_addr + addr, prot))
 			return -ENOMEM;
 	} while (pud++, addr = next, addr != end);

commit 0ddab1d2ed664c85c95488eef569786a84aedf37
Author: Toshi Kani <toshi.kani@hp.com>
Date:   Tue Apr 14 15:47:20 2015 -0700

    lib/ioremap.c: add huge I/O map capability interfaces
    
    Add ioremap_pud_enabled() and ioremap_pmd_enabled(), which return 1 when
    I/O mappings with pud/pmd are enabled on the kernel.
    
    ioremap_huge_init() calls arch_ioremap_pud_supported() and
    arch_ioremap_pmd_supported() to initialize the capabilities at boot-time.
    
    A new kernel option "nohugeiomap" is also added, so that user can disable
    the huge I/O map capabilities when necessary.
    
    Signed-off-by: Toshi Kani <toshi.kani@hp.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Robert Elliott <Elliott@hp.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 0c9216c48762..2008652c9a1f 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -13,6 +13,43 @@
 #include <asm/cacheflush.h>
 #include <asm/pgtable.h>
 
+#ifdef CONFIG_HAVE_ARCH_HUGE_VMAP
+int __read_mostly ioremap_pud_capable;
+int __read_mostly ioremap_pmd_capable;
+int __read_mostly ioremap_huge_disabled;
+
+static int __init set_nohugeiomap(char *str)
+{
+	ioremap_huge_disabled = 1;
+	return 0;
+}
+early_param("nohugeiomap", set_nohugeiomap);
+
+void __init ioremap_huge_init(void)
+{
+	if (!ioremap_huge_disabled) {
+		if (arch_ioremap_pud_supported())
+			ioremap_pud_capable = 1;
+		if (arch_ioremap_pmd_supported())
+			ioremap_pmd_capable = 1;
+	}
+}
+
+static inline int ioremap_pud_enabled(void)
+{
+	return ioremap_pud_capable;
+}
+
+static inline int ioremap_pmd_enabled(void)
+{
+	return ioremap_pmd_capable;
+}
+
+#else	/* !CONFIG_HAVE_ARCH_HUGE_VMAP */
+static inline int ioremap_pud_enabled(void) { return 0; }
+static inline int ioremap_pmd_enabled(void) { return 0; }
+#endif	/* CONFIG_HAVE_ARCH_HUGE_VMAP */
+
 static int ioremap_pte_range(pmd_t *pmd, unsigned long addr,
 		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {

commit 8bc3bcc93a2b4e47d5d410146f6546bca6171663
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Nov 16 21:29:17 2011 -0500

    lib: reduce the use of module.h wherever possible
    
    For files only using THIS_MODULE and/or EXPORT_SYMBOL, map
    them onto including export.h -- or if the file isn't even
    using those, then just delete the include.  Fix up any implicit
    include dependencies that were being masked by module.h along
    the way.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index da4e2ad74b68..0c9216c48762 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -9,7 +9,7 @@
 #include <linux/mm.h>
 #include <linux/sched.h>
 #include <linux/io.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <asm/cacheflush.h>
 #include <asm/pgtable.h>
 

commit 81e88fdc432a1552401d6e91a984dcccce72b8dc
Author: Huang Ying <ying.huang@intel.com>
Date:   Wed Jan 12 14:44:55 2011 +0800

    ACPI, APEI, Generic Hardware Error Source POLL/IRQ/NMI notification type support
    
    Generic Hardware Error Source provides a way to report platform
    hardware errors (such as that from chipset). It works in so called
    "Firmware First" mode, that is, hardware errors are reported to
    firmware firstly, then reported to Linux by firmware. This way, some
    non-standard hardware error registers or non-standard hardware link
    can be checked by firmware to produce more valuable hardware error
    information for Linux.
    
    This patch adds POLL/IRQ/NMI notification types support.
    
    Because the memory area used to transfer hardware error information
    from BIOS to Linux can be determined only in NMI, IRQ or timer
    handler, but general ioremap can not be used in atomic context, so a
    special version of atomic ioremap is implemented for that.
    
    Known issue:
    
    - Error information can not be printed for recoverable errors notified
      via NMI, because printk is not NMI-safe. Will fix this via delay
      printing to IRQ context via irq_work or make printk NMI-safe.
    
    v2:
    
    - adjust printk format per comments.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 5730ecd3eb66..da4e2ad74b68 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -9,6 +9,7 @@
 #include <linux/mm.h>
 #include <linux/sched.h>
 #include <linux/io.h>
+#include <linux/module.h>
 #include <asm/cacheflush.h>
 #include <asm/pgtable.h>
 
@@ -90,3 +91,4 @@ int ioremap_page_range(unsigned long addr,
 
 	return err;
 }
+EXPORT_SYMBOL_GPL(ioremap_page_range);

commit ffa71f33a820d1ab3f2fc5723819ac60fb76080b
Author: Kenji Kaneshige <kaneshige.kenji@jp.fujitsu.com>
Date:   Fri Jun 18 12:22:40 2010 +0900

    x86, ioremap: Fix incorrect physical address handling in PAE mode
    
    Current x86 ioremap() doesn't handle physical address higher than
    32-bit properly in X86_32 PAE mode. When physical address higher than
    32-bit is passed to ioremap(), higher 32-bits in physical address is
    cleared wrongly. Due to this bug, ioremap() can map wrong address to
    linear address space.
    
    In my case, 64-bit MMIO region was assigned to a PCI device (ioat
    device) on my system. Because of the ioremap()'s bug, wrong physical
    address (instead of MMIO region) was mapped to linear address space.
    Because of this, loading ioatdma driver caused unexpected behavior
    (kernel panic, kernel hangup, ...).
    
    Signed-off-by: Kenji Kaneshige <kaneshige.kenji@jp.fujitsu.com>
    LKML-Reference: <4C1AE680.7090408@jp.fujitsu.com>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 14c6078f17a2..5730ecd3eb66 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -13,10 +13,10 @@
 #include <asm/pgtable.h>
 
 static int ioremap_pte_range(pmd_t *pmd, unsigned long addr,
-		unsigned long end, unsigned long phys_addr, pgprot_t prot)
+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
 	pte_t *pte;
-	unsigned long pfn;
+	u64 pfn;
 
 	pfn = phys_addr >> PAGE_SHIFT;
 	pte = pte_alloc_kernel(pmd, addr);
@@ -31,7 +31,7 @@ static int ioremap_pte_range(pmd_t *pmd, unsigned long addr,
 }
 
 static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
-		unsigned long end, unsigned long phys_addr, pgprot_t prot)
+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
 	pmd_t *pmd;
 	unsigned long next;
@@ -49,7 +49,7 @@ static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
 }
 
 static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,
-		unsigned long end, unsigned long phys_addr, pgprot_t prot)
+		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
 	pud_t *pud;
 	unsigned long next;
@@ -67,7 +67,7 @@ static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,
 }
 
 int ioremap_page_range(unsigned long addr,
-		       unsigned long end, unsigned long phys_addr, pgprot_t prot)
+		       unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
 {
 	pgd_t *pgd;
 	unsigned long start;

commit 53fa6645251727856d3554d64583fef59e6f1651
Author: Adrian Bunk <bunk@stusta.de>
Date:   Tue Oct 16 23:26:42 2007 -0700

    lib/ioremap.c should #include <linux/io.h>
    
    Every file should include the headers containing the prototypes for its global
    functions (in this case ioremap_page_range()).
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Acked-by: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 760521417b69..14c6078f17a2 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -8,6 +8,7 @@
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
 #include <linux/sched.h>
+#include <linux/io.h>
 #include <asm/cacheflush.h>
 #include <asm/pgtable.h>
 

commit e8edc6e03a5c8562dc70a6d969f732bdb355a7e7
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon May 21 01:22:52 2007 +0400

    Detach sched.h from mm.h
    
    First thing mm.h does is including sched.h solely for can_do_mlock() inline
    function which has "current" dereference inside. By dealing with can_do_mlock()
    mm.h can be detached from sched.h which is good. See below, why.
    
    This patch
    a) removes unconditional inclusion of sched.h from mm.h
    b) makes can_do_mlock() normal function in mm/mlock.c
    c) exports can_do_mlock() to not break compilation
    d) adds sched.h inclusions back to files that were getting it indirectly.
    e) adds less bloated headers to some files (asm/signal.h, jiffies.h) that were
       getting them indirectly
    
    Net result is:
    a) mm.h users would get less code to open, read, preprocess, parse, ... if
       they don't need sched.h
    b) sched.h stops being dependency for significant number of files:
       on x86_64 allmodconfig touching sched.h results in recompile of 4083 files,
       after patch it's only 3744 (-8.3%).
    
    Cross-compile tested on
    
            all arm defconfigs, all mips defconfigs, all powerpc defconfigs,
            alpha alpha-up
            arm
            i386 i386-up i386-defconfig i386-allnoconfig
            ia64 ia64-up
            m68k
            mips
            parisc parisc-up
            powerpc powerpc-up
            s390 s390-up
            sparc sparc-up
            sparc64 sparc64-up
            um-x86_64
            x86_64 x86_64-up x86_64-defconfig x86_64-allnoconfig
    
    as well as my two usual configs.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index a9e4415b02dc..760521417b69 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -7,7 +7,7 @@
  */
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
-
+#include <linux/sched.h>
 #include <asm/cacheflush.h>
 #include <asm/pgtable.h>
 

commit ee36c2bf8edb1c3e3855a928b348d29c6359093d
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Wed Dec 13 00:35:00 2006 -0800

    [PATCH] uml problems with linux/io.h
    
    Remove useless includes of linux/io.h, don't even try to build iomap_copy
    on uml (it doesn't have readb() et.al., so...)
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Acked-by: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 99fa277f9f7b..a9e4415b02dc 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -5,7 +5,6 @@
  *
  * (C) Copyright 1995 1996 Linus Torvalds
  */
-#include <linux/io.h>
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
 

commit db71daabad0821996483dfe309c4bc81d6755a70
Author: Haavard Skinnemoen <hskinnemoen@atmel.com>
Date:   Sat Sep 30 23:29:14 2006 -0700

    [PATCH] Generic ioremap_page_range: flush_cache_vmap
    
    The existing implementation of ioremap_page_range(), which was taken
    from i386, does this:
    
            flush_cache_all();
            /* modify page tables */
            flush_tlb_all();
    
    I think this is a bit defensive, so this patch changes the generic
    implementation to do:
    
            /* modify page tables */
            flush_cache_vmap(start, end);
    
    instead, which is similar to what vmalloc() does. This should still
    be correct because we never modify existing PTEs. According to
    James Bottomley:
    
    The problem the flush_tlb_all() is trying to solve is to avoid stale tlb
    entries in the ioremap area.  We're just being conservative by flushing
    on both map and unmap.  Technically what vmalloc/vfree does (only flush
    the tlb on unmap) is just fine because it means that the only tlb
    entries in the remap area must belong to in-use mappings.
    
    Signed-off-by: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: <linux-m32r@ml.linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Kyle McMartin <kyle@parisc-linux.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
index 29c810ec9813..99fa277f9f7b 100644
--- a/lib/ioremap.c
+++ b/lib/ioremap.c
@@ -76,8 +76,6 @@ int ioremap_page_range(unsigned long addr,
 
 	BUG_ON(addr >= end);
 
-	flush_cache_all();
-
 	start = addr;
 	phys_addr -= addr;
 	pgd = pgd_offset_k(addr);
@@ -88,7 +86,7 @@ int ioremap_page_range(unsigned long addr,
 			break;
 	} while (pgd++, addr = next, addr != end);
 
-	flush_tlb_all();
+	flush_cache_vmap(start, end);
 
 	return err;
 }

commit 74588d8ba34ff1bda027cfa737972af01ab00c8b
Author: Haavard Skinnemoen <hskinnemoen@atmel.com>
Date:   Sat Sep 30 23:29:12 2006 -0700

    [PATCH] Generic ioremap_page_range: implementation
    
    This patch adds a generic implementation of ioremap_page_range() in
    lib/ioremap.c based on the i386 implementation. It differs from the
    i386 version in the following ways:
    
      * The PTE flags are passed as a pgprot_t argument and must be
        determined up front by the arch-specific code. No additional
        PTE flags are added.
      * Uses set_pte_at() instead of set_pte()
    
    [bunk@stusta.de: warning fix]
    ]dhowells@redhat.com: nommu build fix]
    Signed-off-by: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Andi Kleen <ak@suse.de>
    Cc: <linux-m32r@ml.linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Kyle McMartin <kyle@parisc-linux.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/lib/ioremap.c b/lib/ioremap.c
new file mode 100644
index 000000000000..29c810ec9813
--- /dev/null
+++ b/lib/ioremap.c
@@ -0,0 +1,94 @@
+/*
+ * Re-map IO memory to kernel address space so that we can access it.
+ * This is needed for high PCI addresses that aren't mapped in the
+ * 640k-1MB IO memory area on PC's
+ *
+ * (C) Copyright 1995 1996 Linus Torvalds
+ */
+#include <linux/io.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+
+#include <asm/cacheflush.h>
+#include <asm/pgtable.h>
+
+static int ioremap_pte_range(pmd_t *pmd, unsigned long addr,
+		unsigned long end, unsigned long phys_addr, pgprot_t prot)
+{
+	pte_t *pte;
+	unsigned long pfn;
+
+	pfn = phys_addr >> PAGE_SHIFT;
+	pte = pte_alloc_kernel(pmd, addr);
+	if (!pte)
+		return -ENOMEM;
+	do {
+		BUG_ON(!pte_none(*pte));
+		set_pte_at(&init_mm, addr, pte, pfn_pte(pfn, prot));
+		pfn++;
+	} while (pte++, addr += PAGE_SIZE, addr != end);
+	return 0;
+}
+
+static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,
+		unsigned long end, unsigned long phys_addr, pgprot_t prot)
+{
+	pmd_t *pmd;
+	unsigned long next;
+
+	phys_addr -= addr;
+	pmd = pmd_alloc(&init_mm, pud, addr);
+	if (!pmd)
+		return -ENOMEM;
+	do {
+		next = pmd_addr_end(addr, end);
+		if (ioremap_pte_range(pmd, addr, next, phys_addr + addr, prot))
+			return -ENOMEM;
+	} while (pmd++, addr = next, addr != end);
+	return 0;
+}
+
+static inline int ioremap_pud_range(pgd_t *pgd, unsigned long addr,
+		unsigned long end, unsigned long phys_addr, pgprot_t prot)
+{
+	pud_t *pud;
+	unsigned long next;
+
+	phys_addr -= addr;
+	pud = pud_alloc(&init_mm, pgd, addr);
+	if (!pud)
+		return -ENOMEM;
+	do {
+		next = pud_addr_end(addr, end);
+		if (ioremap_pmd_range(pud, addr, next, phys_addr + addr, prot))
+			return -ENOMEM;
+	} while (pud++, addr = next, addr != end);
+	return 0;
+}
+
+int ioremap_page_range(unsigned long addr,
+		       unsigned long end, unsigned long phys_addr, pgprot_t prot)
+{
+	pgd_t *pgd;
+	unsigned long start;
+	unsigned long next;
+	int err;
+
+	BUG_ON(addr >= end);
+
+	flush_cache_all();
+
+	start = addr;
+	phys_addr -= addr;
+	pgd = pgd_offset_k(addr);
+	do {
+		next = pgd_addr_end(addr, end);
+		err = ioremap_pud_range(pgd, addr, next, phys_addr+addr, prot);
+		if (err)
+			break;
+	} while (pgd++, addr = next, addr != end);
+
+	flush_tlb_all();
+
+	return err;
+}
