commit 4505153954fdb1465d2b178288a9bf646f2a2166
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 29 16:57:47 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 333
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not write to the free
      software foundation inc 59 temple place suite 330 boston ma 02111
      1307 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 136 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190530000436.384967451@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/lib/llist.c b/lib/llist.c
index 7062e931a7bb..611ce4881a87 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Lock-less NULL terminated single linked list
  *
@@ -8,19 +9,6 @@
  *
  * Copyright 2010,2011 Intel Corp.
  *   Author: Huang Ying <ying.huang@intel.com>
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License version
- * 2 as published by the Free Software Foundation;
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
  */
 #include <linux/kernel.h>
 #include <linux/export.h>

commit 6aa7de059173a986114ac43b8f50b297a86f09a8
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Oct 23 14:07:29 2017 -0700

    locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE()
    
    Please do not apply this to mainline directly, instead please re-run the
    coccinelle script shown below and apply its output.
    
    For several reasons, it is desirable to use {READ,WRITE}_ONCE() in
    preference to ACCESS_ONCE(), and new code is expected to use one of the
    former. So far, there's been no reason to change most existing uses of
    ACCESS_ONCE(), as these aren't harmful, and changing them results in
    churn.
    
    However, for some features, the read/write distinction is critical to
    correct operation. To distinguish these cases, separate read/write
    accessors must be used. This patch migrates (most) remaining
    ACCESS_ONCE() instances to {READ,WRITE}_ONCE(), using the following
    coccinelle script:
    
    ----
    // Convert trivial ACCESS_ONCE() uses to equivalent READ_ONCE() and
    // WRITE_ONCE()
    
    // $ make coccicheck COCCI=/home/mark/once.cocci SPFLAGS="--include-headers" MODE=patch
    
    virtual patch
    
    @ depends on patch @
    expression E1, E2;
    @@
    
    - ACCESS_ONCE(E1) = E2
    + WRITE_ONCE(E1, E2)
    
    @ depends on patch @
    expression E;
    @@
    
    - ACCESS_ONCE(E)
    + READ_ONCE(E)
    ----
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: davem@davemloft.net
    Cc: linux-arch@vger.kernel.org
    Cc: mpe@ellerman.id.au
    Cc: shuah@kernel.org
    Cc: snitzer@redhat.com
    Cc: thor.thayer@linux.intel.com
    Cc: tj@kernel.org
    Cc: viro@zeniv.linux.org.uk
    Cc: will.deacon@arm.com
    Link: http://lkml.kernel.org/r/1508792849-3115-19-git-send-email-paulmck@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/lib/llist.c b/lib/llist.c
index ae5872b1df0c..7062e931a7bb 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -41,7 +41,7 @@ bool llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
 	struct llist_node *first;
 
 	do {
-		new_last->next = first = ACCESS_ONCE(head->first);
+		new_last->next = first = READ_ONCE(head->first);
 	} while (cmpxchg(&head->first, first, new_first) != first);
 
 	return !first;

commit 2cf12f821cd4f996bfabeec23d8f25e7a2052a28
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Fri Nov 6 16:31:17 2015 -0800

    lib/llist.c: fix data race in llist_del_first
    
    llist_del_first reads entry->next, but it did not acquire visibility over
    the entry node.  As the result it can get a stale value of entry->next
    (e.g.  NULL or whatever garbage was there before the appending thread
    wrote correct value).  And then commit that value as llist head with
    cmpxchg.  That will corrupt llist.
    
    Note there is a control-dependency between read of head->first and read of
    entry->next, but it does not make the code correct.  Kernel memory model
    unambiguously says: "A load-load control dependency requires a full read
    memory barrier".
    
    Use smp_load_acquire to acquire visibility over the entry node.
    
    The data race was found with KernelThreadSanitizer (KTSAN).
    
    Here is an example of KTSAN report:
    
    ThreadSanitizer: data-race in llist_del_first
    
    Read of size 1 by thread T389 (K2630, CPU0):
     [<ffffffff8156b8a9>] llist_del_first+0x39/0x70 lib/llist.c:74
     [<     inlined    >] tty_buffer_alloc drivers/tty/tty_buffer.c:181
     [<ffffffff81664af4>] __tty_buffer_request_room+0xb4/0x250 drivers/tty/tty_buffer.c:292
     [<ffffffff81664e6c>] tty_insert_flip_string_fixed_flag+0x6c/0x150 drivers/tty/tty_buffer.c:337
     [<     inlined    >] tty_insert_flip_string include/linux/tty_flip.h:35
     [<ffffffff81667422>] pty_write+0x72/0xc0 drivers/tty/pty.c:110
     [<     inlined    >] process_output_block drivers/tty/n_tty.c:611
     [<ffffffff8165c016>] n_tty_write+0x346/0x7f0 drivers/tty/n_tty.c:2401
     [<     inlined    >] do_tty_write drivers/tty/tty_io.c:1159
     [<ffffffff816568df>] tty_write+0x21f/0x3f0 drivers/tty/tty_io.c:1245
     [<ffffffff8125f00f>] __vfs_write+0x5f/0x1f0 fs/read_write.c:489
     [<ffffffff8125ff8f>] vfs_write+0xef/0x280 fs/read_write.c:538
     [<     inlined    >] SYSC_write fs/read_write.c:585
     [<ffffffff81261390>] SyS_write+0x70/0xe0 fs/read_write.c:577
     [<ffffffff81ee862e>] entry_SYSCALL_64_fastpath+0x12/0x71 arch/x86/entry/entry_64.S:186
    
    Previous write of size 8 by thread T226 (K761, CPU0):
     [<ffffffff8156b832>] llist_add_batch+0x32/0x70 lib/llist.c:44 (discriminator 16)
     [<     inlined    >] llist_add include/linux/llist.h:180
     [<ffffffff816649fc>] tty_buffer_free+0x6c/0xb0 drivers/tty/tty_buffer.c:221
     [<ffffffff816651e7>] flush_to_ldisc+0x107/0x300 drivers/tty/tty_buffer.c:514
     [<ffffffff810b20ee>] process_one_work+0x47e/0x930 kernel/workqueue.c:2036
     [<ffffffff810b2650>] worker_thread+0xb0/0x900 kernel/workqueue.c:2170
     [<ffffffff810bbe20>] kthread+0x150/0x170 kernel/kthread.c:209
     [<ffffffff81ee8a1f>] ret_from_fork+0x3f/0x70 arch/x86/entry/entry_64.S:526
    
    Signed-off-by: Dmitry Vyukov <dvyukov@google.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Konstantin Serebryany <kcc@google.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Alexander Potapenko <glider@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/llist.c b/lib/llist.c
index 0b0e9779d675..ae5872b1df0c 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -66,12 +66,12 @@ struct llist_node *llist_del_first(struct llist_head *head)
 {
 	struct llist_node *entry, *old_entry, *next;
 
-	entry = head->first;
+	entry = smp_load_acquire(&head->first);
 	for (;;) {
 		if (entry == NULL)
 			return NULL;
 		old_entry = entry;
-		next = entry->next;
+		next = READ_ONCE(entry->next);
 		entry = cmpxchg(&head->first, old_entry, next);
 		if (entry == old_entry)
 			break;

commit 9b40570bd986128bb8e3c5f6f1abc60e1ad89794
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Thu Feb 12 15:02:54 2015 -0800

    lib/llist.c: remove redundant include
    
    This file doesn't seem to use anything provided by linux/interrupt.h or
    anything recursively included through that.  Removing it produces
    byte-identical output, while reducing .llist.o.cmd from 541 to 156 lines.
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/llist.c b/lib/llist.c
index f76196d07409..0b0e9779d675 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -24,7 +24,6 @@
  */
 #include <linux/kernel.h>
 #include <linux/export.h>
-#include <linux/interrupt.h>
 #include <linux/llist.h>
 
 

commit 0791a6057cb60d12ec5e3182b99e6ffa8044ee3a
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu Nov 14 14:32:13 2013 -0800

    llists-move-llist_reverse_order-from-raid5-to-llistc-fix
    
    fix comment typo, per Jan
    
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/llist.c b/lib/llist.c
index ef48b87247ec..f76196d07409 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -86,7 +86,7 @@ EXPORT_SYMBOL_GPL(llist_del_first);
  * llist_reverse_order - reverse order of a llist chain
  * @head:	first item of the list to be reversed
  *
- * Reverse the oder of a chain of llist entries and return the
+ * Reverse the order of a chain of llist entries and return the
  * new first entry.
  */
 struct llist_node *llist_reverse_order(struct llist_node *head)

commit b89241e8cdb8321c20546d47645a9b65b58113b5
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Nov 14 14:32:11 2013 -0800

    llists: move llist_reverse_order from raid5 to llist.c
    
    Make this useful helper available for other users.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Neil Brown <neilb@suse.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/llist.c b/lib/llist.c
index 4a70d120138c..ef48b87247ec 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -81,3 +81,25 @@ struct llist_node *llist_del_first(struct llist_head *head)
 	return entry;
 }
 EXPORT_SYMBOL_GPL(llist_del_first);
+
+/**
+ * llist_reverse_order - reverse order of a llist chain
+ * @head:	first item of the list to be reversed
+ *
+ * Reverse the oder of a chain of llist entries and return the
+ * new first entry.
+ */
+struct llist_node *llist_reverse_order(struct llist_node *head)
+{
+	struct llist_node *new_head = NULL;
+
+	while (head) {
+		struct llist_node *tmp = head;
+		head = head->next;
+		tmp->next = new_head;
+		new_head = tmp;
+	}
+
+	return new_head;
+}
+EXPORT_SYMBOL_GPL(llist_reverse_order);

commit fb4214db50b00558cc6e274c88b3f7325068e942
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jul 8 14:24:18 2013 -0700

    llist: fix/simplify llist_add() and llist_add_batch()
    
    1. This is mostly theoretical, but llist_add*() need ACCESS_ONCE().
    
       Otherwise it is not guaranteed that the first cmpxchg() uses the
       same value for old_entry and new_last->next.
    
    2. These helpers cache the result of cmpxchg() and read the initial
       value of head->first before the main loop. I do not think this
       makes sense. In the likely case cmpxchg() succeeds, otherwise
       it doesn't hurt to reload head->first.
    
       I think it would be better to simplify the code and simply read
       ->first before cmpxchg().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andrey Vagin <avagin@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/lib/llist.c b/lib/llist.c
index 4a15115e90f8..4a70d120138c 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -39,18 +39,13 @@
 bool llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
 		     struct llist_head *head)
 {
-	struct llist_node *entry, *old_entry;
+	struct llist_node *first;
 
-	entry = head->first;
-	for (;;) {
-		old_entry = entry;
-		new_last->next = entry;
-		entry = cmpxchg(&head->first, old_entry, new_first);
-		if (entry == old_entry)
-			break;
-	}
+	do {
+		new_last->next = first = ACCESS_ONCE(head->first);
+	} while (cmpxchg(&head->first, first, new_first) != first);
 
-	return old_entry == NULL;
+	return !first;
 }
 EXPORT_SYMBOL_GPL(llist_add_batch);
 

commit 0195c00244dc2e9f522475868fa278c473ba7339
Merge: f21ce8f8447c 141124c02059
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 15:58:21 2012 -0700

    Merge tag 'split-asm_system_h-for-linus-20120328' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-asm_system
    
    Pull "Disintegrate and delete asm/system.h" from David Howells:
     "Here are a bunch of patches to disintegrate asm/system.h into a set of
      separate bits to relieve the problem of circular inclusion
      dependencies.
    
      I've built all the working defconfigs from all the arches that I can
      and made sure that they don't break.
    
      The reason for these patches is that I recently encountered a circular
      dependency problem that came about when I produced some patches to
      optimise get_order() by rewriting it to use ilog2().
    
      This uses bitops - and on the SH arch asm/bitops.h drags in
      asm-generic/get_order.h by a circuituous route involving asm/system.h.
    
      The main difficulty seems to be asm/system.h.  It holds a number of
      low level bits with no/few dependencies that are commonly used (eg.
      memory barriers) and a number of bits with more dependencies that
      aren't used in many places (eg.  switch_to()).
    
      These patches break asm/system.h up into the following core pieces:
    
        (1) asm/barrier.h
    
            Move memory barriers here.  This already done for MIPS and Alpha.
    
        (2) asm/switch_to.h
    
            Move switch_to() and related stuff here.
    
        (3) asm/exec.h
    
            Move arch_align_stack() here.  Other process execution related bits
            could perhaps go here from asm/processor.h.
    
        (4) asm/cmpxchg.h
    
            Move xchg() and cmpxchg() here as they're full word atomic ops and
            frequently used by atomic_xchg() and atomic_cmpxchg().
    
        (5) asm/bug.h
    
            Move die() and related bits.
    
        (6) asm/auxvec.h
    
            Move AT_VECTOR_SIZE_ARCH here.
    
      Other arch headers are created as needed on a per-arch basis."
    
    Fixed up some conflicts from other header file cleanups and moving code
    around that has happened in the meantime, so David's testing is somewhat
    weakened by that.  We'll find out anything that got broken and fix it..
    
    * tag 'split-asm_system_h-for-linus-20120328' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-asm_system: (38 commits)
      Delete all instances of asm/system.h
      Remove all #inclusions of asm/system.h
      Add #includes needed to permit the removal of asm/system.h
      Move all declarations of free_initmem() to linux/mm.h
      Disintegrate asm/system.h for OpenRISC
      Split arch_align_stack() out from asm-generic/system.h
      Split the switch_to() wrapper out of asm-generic/system.h
      Move the asm-generic/system.h xchg() implementation to asm-generic/cmpxchg.h
      Create asm-generic/barrier.h
      Make asm-generic/cmpxchg.h #include asm-generic/cmpxchg-local.h
      Disintegrate asm/system.h for Xtensa
      Disintegrate asm/system.h for Unicore32 [based on ver #3, changed by gxt]
      Disintegrate asm/system.h for Tile
      Disintegrate asm/system.h for Sparc
      Disintegrate asm/system.h for SH
      Disintegrate asm/system.h for Score
      Disintegrate asm/system.h for S390
      Disintegrate asm/system.h for PowerPC
      Disintegrate asm/system.h for PA-RISC
      Disintegrate asm/system.h for MN10300
      ...

commit 9ffc93f203c18a70623f21950f1dd473c9ec48cd
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:03 2012 +0100

    Remove all #inclusions of asm/system.h
    
    Remove all #inclusions of asm/system.h preparatory to splitting and killing
    it.  Performed with the following command:
    
    perl -p -i -e 's!^#\s*include\s*<asm/system[.]h>.*\n!!' `grep -Irl '^#\s*include\s*<asm/system[.]h>' *`
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/lib/llist.c b/lib/llist.c
index 700cff77a387..e429d2763480 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -27,7 +27,6 @@
 #include <linux/interrupt.h>
 #include <linux/llist.h>
 
-#include <asm/system.h>
 
 /**
  * llist_add_batch - add several linked entries in batch

commit 8bc3bcc93a2b4e47d5d410146f6546bca6171663
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Nov 16 21:29:17 2011 -0500

    lib: reduce the use of module.h wherever possible
    
    For files only using THIS_MODULE and/or EXPORT_SYMBOL, map
    them onto including export.h -- or if the file isn't even
    using those, then just delete the include.  Fix up any implicit
    include dependencies that were being masked by module.h along
    the way.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/lib/llist.c b/lib/llist.c
index 700cff77a387..8221b3d2fbf0 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -23,7 +23,7 @@
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
  */
 #include <linux/kernel.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/interrupt.h>
 #include <linux/llist.h>
 

commit f0f1d32f931b705c4ee5dd374074d34edf3eae14
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Sep 12 15:50:49 2011 +0200

    llist: Remove cpu_relax() usage in cmpxchg loops
    
    Initial benchmarks show they're a net loss:
    
     $ for i in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor ; do echo performance > $i; done
     $ echo 4096 32000 64 128 > /proc/sys/kernel/sem
     $ ./sembench -t 2048 -w 1900 -o 0
    
    Pre:
    
     run time 30 seconds 778936 worker burns per second
     run time 30 seconds 912190 worker burns per second
     run time 30 seconds 817506 worker burns per second
     run time 30 seconds 830870 worker burns per second
     run time 30 seconds 845056 worker burns per second
    
    Post:
    
     run time 30 seconds 905920 worker burns per second
     run time 30 seconds 849046 worker burns per second
     run time 30 seconds 886286 worker burns per second
     run time 30 seconds 822320 worker burns per second
     run time 30 seconds 900283 worker burns per second
    
    So about 4% faster. (!)
    
    cpu_relax() stalls the pipeline, therefore, when used in a tight loop
    it has the following benefits:
    
     - allows SMT siblings to have a go;
     - reduces pressure on the CPU interconnect.
    
    However, cmpxchg loops are unfair and thus have unbounded completion
    time, therefore we should avoid getting in such heavily contended
    situations where the above benefits make any difference.
    
    A typical cmpxchg loop should not go round more than a handfull of
    times at worst, therefore adding extra delays just slows things down.
    
    Since the llist primitives are new, there aren't any bad users yet,
    and we should avoid growing them. Heavily contended sites should
    generally be better off using the ticket locks for serialization since
    they provide bounded completion times (fifo-fair over the cpus).
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1315836358.26517.43.camel@twins
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/lib/llist.c b/lib/llist.c
index 878985c4d19d..700cff77a387 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -49,7 +49,6 @@ bool llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
 		entry = cmpxchg(&head->first, old_entry, new_first);
 		if (entry == old_entry)
 			break;
-		cpu_relax();
 	}
 
 	return old_entry == NULL;
@@ -83,7 +82,6 @@ struct llist_node *llist_del_first(struct llist_head *head)
 		entry = cmpxchg(&head->first, old_entry, next);
 		if (entry == old_entry)
 			break;
-		cpu_relax();
 	}
 
 	return entry;

commit 781f7fd916fc77a862e20063ed3aeedf173234f9
Author: Huang Ying <ying.huang@intel.com>
Date:   Thu Sep 8 14:00:45 2011 +0800

    llist: Return whether list is empty before adding in llist_add()
    
    Extend the llist_add*() functions to return a success indicator, this
    allows us in the scheduler code to send an IPI if the queue was empty.
    
    ( There's no effect on existing users, because the list_add_xxx() functions
      are inline, thus this will be optimized out by the compiler if not used
      by callers. )
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1315461646-1379-5-git-send-email-ying.huang@intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/lib/llist.c b/lib/llist.c
index 6c69f1d14c4b..878985c4d19d 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -34,8 +34,10 @@
  * @new_first:	first entry in batch to be added
  * @new_last:	last entry in batch to be added
  * @head:	the head for your lock-less list
+ *
+ * Return whether list is empty before adding.
  */
-void llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
+bool llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
 		     struct llist_head *head)
 {
 	struct llist_node *entry, *old_entry;
@@ -49,6 +51,8 @@ void llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
 			break;
 		cpu_relax();
 	}
+
+	return old_entry == NULL;
 }
 EXPORT_SYMBOL_GPL(llist_add_batch);
 

commit a3127336b71f6833d1483c856dce91fe558dc3a9
Author: Huang Ying <ying.huang@intel.com>
Date:   Thu Sep 8 14:00:44 2011 +0800

    llist: Move cpu_relax() to after the cmpxchg()
    
    If in llist_add()/etc. functions the first cmpxchg() call succeeds, it is
    not necessary to use cpu_relax() before the cmpxchg(). So cpu_relax() in
    a busy loop involving cmpxchg() should go after cmpxchg() instead of before
    that.
    
    This patch fixes this for all involved llist functions.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1315461646-1379-4-git-send-email-ying.huang@intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/lib/llist.c b/lib/llist.c
index b445f2c8596a..6c69f1d14c4b 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -41,11 +41,14 @@ void llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
 	struct llist_node *entry, *old_entry;
 
 	entry = head->first;
-	do {
+	for (;;) {
 		old_entry = entry;
 		new_last->next = entry;
+		entry = cmpxchg(&head->first, old_entry, new_first);
+		if (entry == old_entry)
+			break;
 		cpu_relax();
-	} while ((entry = cmpxchg(&head->first, old_entry, new_first)) != old_entry);
+	}
 }
 EXPORT_SYMBOL_GPL(llist_add_batch);
 
@@ -68,13 +71,16 @@ struct llist_node *llist_del_first(struct llist_head *head)
 	struct llist_node *entry, *old_entry, *next;
 
 	entry = head->first;
-	do {
+	for (;;) {
 		if (entry == NULL)
 			return NULL;
 		old_entry = entry;
 		next = entry->next;
+		entry = cmpxchg(&head->first, old_entry, next);
+		if (entry == old_entry)
+			break;
 		cpu_relax();
-	} while ((entry = cmpxchg(&head->first, old_entry, next)) != old_entry);
+	}
 
 	return entry;
 }

commit 2c30245c65e8ebc3080b75ce65572ab8140bad0b
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Oct 4 12:43:11 2011 +0200

    llist: Remove the platform-dependent NMI checks
    
    Remove the nmi() checks spread around the code. in_nmi() is not available
    on every architecture and it's a pretty obscure and ugly check in any case.
    
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1315461646-1379-3-git-send-email-ying.huang@intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/lib/llist.c b/lib/llist.c
index 3e3fa9139c41..b445f2c8596a 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -3,8 +3,8 @@
  *
  * The basic atomic operation of this list is cmpxchg on long.  On
  * architectures that don't have NMI-safe cmpxchg implementation, the
- * list can NOT be used in NMI handler.  So code uses the list in NMI
- * handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
+ * list can NOT be used in NMI handlers.  So code that uses the list in
+ * an NMI handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
  *
  * Copyright 2010,2011 Intel Corp.
  *   Author: Huang Ying <ying.huang@intel.com>
@@ -40,10 +40,6 @@ void llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
 {
 	struct llist_node *entry, *old_entry;
 
-#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
-	BUG_ON(in_nmi());
-#endif
-
 	entry = head->first;
 	do {
 		old_entry = entry;
@@ -71,10 +67,6 @@ struct llist_node *llist_del_first(struct llist_head *head)
 {
 	struct llist_node *entry, *old_entry, *next;
 
-#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
-	BUG_ON(in_nmi());
-#endif
-
 	entry = head->first;
 	do {
 		if (entry == NULL)

commit 1230db8e1543c0471dd165727d34647ab098cc1e
Author: Huang Ying <ying.huang@intel.com>
Date:   Thu Sep 8 14:00:42 2011 +0800

    llist: Make some llist functions inline
    
    Because llist code will be used in performance critical scheduler
    code path, make llist_add() and llist_del_all() inline to avoid
    function calling overhead and related 'glue' overhead.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1315461646-1379-2-git-send-email-ying.huang@intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/lib/llist.c b/lib/llist.c
index da445724fa1f..3e3fa9139c41 100644
--- a/lib/llist.c
+++ b/lib/llist.c
@@ -29,28 +29,6 @@
 
 #include <asm/system.h>
 
-/**
- * llist_add - add a new entry
- * @new:	new entry to be added
- * @head:	the head for your lock-less list
- */
-void llist_add(struct llist_node *new, struct llist_head *head)
-{
-	struct llist_node *entry, *old_entry;
-
-#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
-	BUG_ON(in_nmi());
-#endif
-
-	entry = head->first;
-	do {
-		old_entry = entry;
-		new->next = entry;
-		cpu_relax();
-	} while ((entry = cmpxchg(&head->first, old_entry, new)) != old_entry);
-}
-EXPORT_SYMBOL_GPL(llist_add);
-
 /**
  * llist_add_batch - add several linked entries in batch
  * @new_first:	first entry in batch to be added
@@ -109,21 +87,3 @@ struct llist_node *llist_del_first(struct llist_head *head)
 	return entry;
 }
 EXPORT_SYMBOL_GPL(llist_del_first);
-
-/**
- * llist_del_all - delete all entries from lock-less list
- * @head:	the head of lock-less list to delete all entries
- *
- * If list is empty, return NULL, otherwise, delete all entries and
- * return the pointer to the first entry.  The order of entries
- * deleted is from the newest to the oldest added one.
- */
-struct llist_node *llist_del_all(struct llist_head *head)
-{
-#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
-	BUG_ON(in_nmi());
-#endif
-
-	return xchg(&head->first, NULL);
-}
-EXPORT_SYMBOL_GPL(llist_del_all);

commit f49f23abf3dd786ddcac1c1e7db3c2013b07413f
Author: Huang Ying <ying.huang@intel.com>
Date:   Wed Jul 13 13:14:23 2011 +0800

    lib, Add lock-less NULL terminated single list
    
    Cmpxchg is used to implement adding new entry to the list, deleting
    all entries from the list, deleting first entry of the list and some
    other operations.
    
    Because this is a single list, so the tail can not be accessed in O(1).
    
    If there are multiple producers and multiple consumers, llist_add can
    be used in producers and llist_del_all can be used in consumers.  They
    can work simultaneously without lock.  But llist_del_first can not be
    used here.  Because llist_del_first depends on list->first->next does
    not changed if list->first is not changed during its operation, but
    llist_del_first, llist_add, llist_add (or llist_del_all, llist_add,
    llist_add) sequence in another consumer may violate that.
    
    If there are multiple producers and one consumer, llist_add can be
    used in producers and llist_del_all or llist_del_first can be used in
    the consumer.
    
    This can be summarized as follow:
    
               |   add    | del_first |  del_all
     add       |    -     |     -     |     -
     del_first |          |     L     |     L
     del_all   |          |           |     -
    
    Where "-" stands for no lock is needed, while "L" stands for lock is
    needed.
    
    The list entries deleted via llist_del_all can be traversed with
    traversing function such as llist_for_each etc.  But the list entries
    can not be traversed safely before deleted from the list.  The order
    of deleted entries is from the newest to the oldest added one.  If you
    want to traverse from the oldest to the newest, you must reverse the
    order by yourself before traversing.
    
    The basic atomic operation of this list is cmpxchg on long.  On
    architectures that don't have NMI-safe cmpxchg implementation, the
    list can NOT be used in NMI handler.  So code uses the list in NMI
    handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
    
    Signed-off-by: Huang Ying <ying.huang@intel.com>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Len Brown <len.brown@intel.com>

diff --git a/lib/llist.c b/lib/llist.c
new file mode 100644
index 000000000000..da445724fa1f
--- /dev/null
+++ b/lib/llist.c
@@ -0,0 +1,129 @@
+/*
+ * Lock-less NULL terminated single linked list
+ *
+ * The basic atomic operation of this list is cmpxchg on long.  On
+ * architectures that don't have NMI-safe cmpxchg implementation, the
+ * list can NOT be used in NMI handler.  So code uses the list in NMI
+ * handler should depend on CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG.
+ *
+ * Copyright 2010,2011 Intel Corp.
+ *   Author: Huang Ying <ying.huang@intel.com>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License version
+ * 2 as published by the Free Software Foundation;
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ */
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/llist.h>
+
+#include <asm/system.h>
+
+/**
+ * llist_add - add a new entry
+ * @new:	new entry to be added
+ * @head:	the head for your lock-less list
+ */
+void llist_add(struct llist_node *new, struct llist_head *head)
+{
+	struct llist_node *entry, *old_entry;
+
+#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
+	BUG_ON(in_nmi());
+#endif
+
+	entry = head->first;
+	do {
+		old_entry = entry;
+		new->next = entry;
+		cpu_relax();
+	} while ((entry = cmpxchg(&head->first, old_entry, new)) != old_entry);
+}
+EXPORT_SYMBOL_GPL(llist_add);
+
+/**
+ * llist_add_batch - add several linked entries in batch
+ * @new_first:	first entry in batch to be added
+ * @new_last:	last entry in batch to be added
+ * @head:	the head for your lock-less list
+ */
+void llist_add_batch(struct llist_node *new_first, struct llist_node *new_last,
+		     struct llist_head *head)
+{
+	struct llist_node *entry, *old_entry;
+
+#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
+	BUG_ON(in_nmi());
+#endif
+
+	entry = head->first;
+	do {
+		old_entry = entry;
+		new_last->next = entry;
+		cpu_relax();
+	} while ((entry = cmpxchg(&head->first, old_entry, new_first)) != old_entry);
+}
+EXPORT_SYMBOL_GPL(llist_add_batch);
+
+/**
+ * llist_del_first - delete the first entry of lock-less list
+ * @head:	the head for your lock-less list
+ *
+ * If list is empty, return NULL, otherwise, return the first entry
+ * deleted, this is the newest added one.
+ *
+ * Only one llist_del_first user can be used simultaneously with
+ * multiple llist_add users without lock.  Because otherwise
+ * llist_del_first, llist_add, llist_add (or llist_del_all, llist_add,
+ * llist_add) sequence in another user may change @head->first->next,
+ * but keep @head->first.  If multiple consumers are needed, please
+ * use llist_del_all or use lock between consumers.
+ */
+struct llist_node *llist_del_first(struct llist_head *head)
+{
+	struct llist_node *entry, *old_entry, *next;
+
+#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
+	BUG_ON(in_nmi());
+#endif
+
+	entry = head->first;
+	do {
+		if (entry == NULL)
+			return NULL;
+		old_entry = entry;
+		next = entry->next;
+		cpu_relax();
+	} while ((entry = cmpxchg(&head->first, old_entry, next)) != old_entry);
+
+	return entry;
+}
+EXPORT_SYMBOL_GPL(llist_del_first);
+
+/**
+ * llist_del_all - delete all entries from lock-less list
+ * @head:	the head of lock-less list to delete all entries
+ *
+ * If list is empty, return NULL, otherwise, delete all entries and
+ * return the pointer to the first entry.  The order of entries
+ * deleted is from the newest to the oldest added one.
+ */
+struct llist_node *llist_del_all(struct llist_head *head)
+{
+#ifndef CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG
+	BUG_ON(in_nmi());
+#endif
+
+	return xchg(&head->first, NULL);
+}
+EXPORT_SYMBOL_GPL(llist_del_all);
