commit 13ba17bee18e321b073b49a88dcab10881f757da
Author: Mukesh Ojha <mojha@codeaurora.org>
Date:   Fri Aug 24 18:03:53 2018 +0530

    notifier: Remove notifier header file wherever not used
    
    The conversion of the hotplug notifiers to a state machine left the
    notifier.h includes around in some places. Remove them.
    
    Signed-off-by: Mukesh Ojha <mojha@codeaurora.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/1535114033-4605-1-git-send-email-mojha@codeaurora.org

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index c72577e472f2..a66595ba5543 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -4,7 +4,6 @@
  */
 
 #include <linux/percpu_counter.h>
-#include <linux/notifier.h>
 #include <linux/mutex.h>
 #include <linux/init.h>
 #include <linux/cpu.h>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 3bf4a9984f4c..c72577e472f2 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Fast batching percpu counters.
  */

commit 3e8f399da490e6ac20a3cfd6aa404c9aa961a9a2
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Wed Jul 12 14:37:51 2017 -0700

    writeback: rework wb_[dec|inc]_stat family of functions
    
    Currently the writeback statistics code uses a percpu counters to hold
    various statistics.  Furthermore we have 2 families of functions - those
    which disable local irq and those which doesn't and whose names begin
    with double underscore.  However, they both end up calling
    __add_wb_stats which in turn calls percpu_counter_add_batch which is
    already irq-safe.
    
    Exploiting this fact allows to eliminated the __wb_* functions since
    they don't add any further protection than we already have.
    Furthermore, refactor the wb_* function to call __add_wb_stat directly
    without the irq-disabling dance.  This will likely result in better
    runtime of code which deals with modifying the stat counters.
    
    While at it also document why percpu_counter_add_batch is in fact
    preempt and irq-safe since at least 3 people got confused.
    
    Link: http://lkml.kernel.org/r/1498029937-27293-1-git-send-email-nborisov@suse.com
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Josef Bacik <jbacik@fb.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 8ee7e5ec21be..3bf4a9984f4c 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -72,6 +72,13 @@ void percpu_counter_set(struct percpu_counter *fbc, s64 amount)
 }
 EXPORT_SYMBOL(percpu_counter_set);
 
+/**
+ * This function is both preempt and irq safe. The former is due to explicit
+ * preemption disable. The latter is guaranteed by the fact that the slow path
+ * is explicitly protected by an irq-safe spinlock whereas the fast patch uses
+ * this_cpu_add which is irq-safe by definition. Hence there is no need muck
+ * with irq state before calling this one
+ */
 void percpu_counter_add_batch(struct percpu_counter *fbc, s64 amount, s32 batch)
 {
 	s64 count;

commit 104b4e5139fe384431ac11c3b8a6cf4a529edf4a
Author: Nikolay Borisov <nborisov@suse.com>
Date:   Tue Jun 20 21:01:20 2017 +0300

    percpu_counter: Rename __percpu_counter_add to percpu_counter_add_batch
    
    Currently, percpu_counter_add is a wrapper around __percpu_counter_add
    which is preempt safe due to explicit calls to preempt_disable.  Given
    how __ prefix is used in percpu related interfaces, the naming
    unfortunately creates the false sense that __percpu_counter_add is
    less safe than percpu_counter_add.  In terms of context-safety,
    they're equivalent.  The only difference is that the __ version takes
    a batch parameter.
    
    Make this a bit more explicit by just renaming __percpu_counter_add to
    percpu_counter_add_batch.
    
    This patch doesn't cause any functional changes.
    
    tj: Minor updates to patch description for clarity.  Cosmetic
        indentation updates.
    
    Signed-off-by: Nikolay Borisov <nborisov@suse.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <jbacik@fb.com>
    Cc: David Sterba <dsterba@suse.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Jan Kara <jack@suse.com>
    Cc: Jens Axboe <axboe@fb.com>
    Cc: linux-mm@kvack.org
    Cc: "David S. Miller" <davem@davemloft.net>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 9c21000df0b5..8ee7e5ec21be 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -72,7 +72,7 @@ void percpu_counter_set(struct percpu_counter *fbc, s64 amount)
 }
 EXPORT_SYMBOL(percpu_counter_set);
 
-void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
+void percpu_counter_add_batch(struct percpu_counter *fbc, s64 amount, s32 batch)
 {
 	s64 count;
 
@@ -89,7 +89,7 @@ void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 	}
 	preempt_enable();
 }
-EXPORT_SYMBOL(__percpu_counter_add);
+EXPORT_SYMBOL(percpu_counter_add_batch);
 
 /*
  * Add up all the per-cpu counts, return the result.  This is a more accurate

commit aaf0f2fa682861e47a4f6a8762d2b8a9a4a51077
Author: Eric Dumazet <edumazet@google.com>
Date:   Fri Jan 20 06:34:22 2017 -0800

    percpu_counter: percpu_counter_hotcpu_callback() cleanup
    
    In commit ebd8fef304f9 ("percpu_counter: make percpu_counters_lock
    irq-safe") we disabled irqs in percpu_counter_hotcpu_callback()
    
    We can grab every counter spinlock without having to disable
    irqs again.
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index c8cebb137076..9c21000df0b5 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -176,13 +176,12 @@ static int percpu_counter_cpu_dead(unsigned int cpu)
 	spin_lock_irq(&percpu_counters_lock);
 	list_for_each_entry(fbc, &percpu_counters, list) {
 		s32 *pcount;
-		unsigned long flags;
 
-		raw_spin_lock_irqsave(&fbc->lock, flags);
+		raw_spin_lock(&fbc->lock);
 		pcount = per_cpu_ptr(fbc->counters, cpu);
 		fbc->count += *pcount;
 		*pcount = 0;
-		raw_spin_unlock_irqrestore(&fbc->lock, flags);
+		raw_spin_unlock(&fbc->lock);
 	}
 	spin_unlock_irq(&percpu_counters_lock);
 #endif

commit 5588f5afb4cfc33eb377b751ba4b97184373e8d6
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Nov 3 15:50:00 2016 +0100

    lib/percpu_counter: Convert to hotplug state machine
    
    Install the callbacks via the state machine and let the core invoke
    the callbacks on the already online CPUs.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20161103145021.28528-5-bigeasy@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 72d36113ccaa..c8cebb137076 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -158,25 +158,21 @@ EXPORT_SYMBOL(percpu_counter_destroy);
 int percpu_counter_batch __read_mostly = 32;
 EXPORT_SYMBOL(percpu_counter_batch);
 
-static void compute_batch_value(void)
+static int compute_batch_value(unsigned int cpu)
 {
 	int nr = num_online_cpus();
 
 	percpu_counter_batch = max(32, nr*2);
+	return 0;
 }
 
-static int percpu_counter_hotcpu_callback(struct notifier_block *nb,
-					unsigned long action, void *hcpu)
+static int percpu_counter_cpu_dead(unsigned int cpu)
 {
 #ifdef CONFIG_HOTPLUG_CPU
-	unsigned int cpu;
 	struct percpu_counter *fbc;
 
-	compute_batch_value();
-	if (action != CPU_DEAD && action != CPU_DEAD_FROZEN)
-		return NOTIFY_OK;
+	compute_batch_value(cpu);
 
-	cpu = (unsigned long)hcpu;
 	spin_lock_irq(&percpu_counters_lock);
 	list_for_each_entry(fbc, &percpu_counters, list) {
 		s32 *pcount;
@@ -190,7 +186,7 @@ static int percpu_counter_hotcpu_callback(struct notifier_block *nb,
 	}
 	spin_unlock_irq(&percpu_counters_lock);
 #endif
-	return NOTIFY_OK;
+	return 0;
 }
 
 /*
@@ -222,8 +218,15 @@ EXPORT_SYMBOL(__percpu_counter_compare);
 
 static int __init percpu_counter_startup(void)
 {
-	compute_batch_value();
-	hotcpu_notifier(percpu_counter_hotcpu_callback, 0);
+	int ret;
+
+	ret = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, "lib/percpu_cnt:online",
+				compute_batch_value, NULL);
+	WARN_ON(ret < 0);
+	ret = cpuhp_setup_state_nocalls(CPUHP_PERCPU_CNT_DEAD,
+					"lib/percpu_cnt:dead", NULL,
+					percpu_counter_cpu_dead);
+	WARN_ON(ret < 0);
 	return 0;
 }
 module_init(percpu_counter_startup);

commit d99b1d8912654c4bdeb51063d2e934afc2372cc2
Author: Du, Changbin <changbin.du@intel.com>
Date:   Thu May 19 17:09:35 2016 -0700

    percpu_counter: update debugobjects fixup callbacks return type
    
    Update the return type to use bool instead of int, corresponding to
    cheange (debugobjects: make fixup functions return bool instead of int).
    
    Signed-off-by: Du, Changbin <changbin.du@intel.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Josh Triplett <josh@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index f051d69f0910..72d36113ccaa 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -19,7 +19,7 @@ static DEFINE_SPINLOCK(percpu_counters_lock);
 
 static struct debug_obj_descr percpu_counter_debug_descr;
 
-static int percpu_counter_fixup_free(void *addr, enum debug_obj_state state)
+static bool percpu_counter_fixup_free(void *addr, enum debug_obj_state state)
 {
 	struct percpu_counter *fbc = addr;
 
@@ -27,9 +27,9 @@ static int percpu_counter_fixup_free(void *addr, enum debug_obj_state state)
 	case ODEBUG_STATE_ACTIVE:
 		percpu_counter_destroy(fbc);
 		debug_object_free(fbc, &percpu_counter_debug_descr);
-		return 1;
+		return true;
 	default:
-		return 0;
+		return false;
 	}
 }
 

commit 80188b0d77d7426b494af739ac129e0e684acb84
Author: Dave Chinner <dchinner@redhat.com>
Date:   Fri May 29 07:39:34 2015 +1000

    percpu_counter: batch size aware __percpu_counter_compare()
    
    XFS uses non-stanard batch sizes for avoiding frequent global
    counter updates on it's allocated inode counters, as they increment
    or decrement in batches of 64 inodes. Hence the standard percpu
    counter batch of 32 means that the counter is effectively a global
    counter. Currently Xfs uses a batch size of 128 so that it doesn't
    take the global lock on every single modification.
    
    However, Xfs also needs to compare accurately against zero, which
    means we need to use percpu_counter_compare(), and that has a
    hard-coded batch size of 32, and hence will spuriously fail to
    detect when it is supposed to use precise comparisons and hence
    the accounting goes wrong.
    
    Add __percpu_counter_compare() to take a custom batch size so we can
    use it sanely in XFS and factor percpu_counter_compare() to use it.
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Dave Chinner <david@fromorbit.com>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 48144cdae819..f051d69f0910 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -197,13 +197,13 @@ static int percpu_counter_hotcpu_callback(struct notifier_block *nb,
  * Compare counter against given value.
  * Return 1 if greater, 0 if equal and -1 if less
  */
-int percpu_counter_compare(struct percpu_counter *fbc, s64 rhs)
+int __percpu_counter_compare(struct percpu_counter *fbc, s64 rhs, s32 batch)
 {
 	s64	count;
 
 	count = percpu_counter_read(fbc);
 	/* Check to see if rough count will be sufficient for comparison */
-	if (abs(count - rhs) > (percpu_counter_batch*num_online_cpus())) {
+	if (abs(count - rhs) > (batch * num_online_cpus())) {
 		if (count > rhs)
 			return 1;
 		else
@@ -218,7 +218,7 @@ int percpu_counter_compare(struct percpu_counter *fbc, s64 rhs)
 	else
 		return 0;
 }
-EXPORT_SYMBOL(percpu_counter_compare);
+EXPORT_SYMBOL(__percpu_counter_compare);
 
 static int __init percpu_counter_startup(void)
 {

commit 908c7f1949cb7cc6e92ba8f18f2998e87e265b8e
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Sep 8 09:51:29 2014 +0900

    percpu_counter: add @gfp to percpu_counter_init()
    
    Percpu allocator now supports allocation mask.  Add @gfp to
    percpu_counter_init() so that !GFP_KERNEL allocation masks can be used
    with percpu_counters too.
    
    We could have left percpu_counter_init() alone and added
    percpu_counter_init_gfp(); however, the number of users isn't that
    high and introducing _gfp variants to all percpu data structures would
    be quite ugly, so let's just do the conversion.  This is the one with
    the most users.  Other percpu data structures are a lot easier to
    convert.
    
    This patch doesn't make any functional difference.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Jan Kara <jack@suse.cz>
    Acked-by: "David S. Miller" <davem@davemloft.net>
    Cc: x86@kernel.org
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 3fde78275cd1..48144cdae819 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -112,7 +112,7 @@ s64 __percpu_counter_sum(struct percpu_counter *fbc)
 }
 EXPORT_SYMBOL(__percpu_counter_sum);
 
-int __percpu_counter_init(struct percpu_counter *fbc, s64 amount,
+int __percpu_counter_init(struct percpu_counter *fbc, s64 amount, gfp_t gfp,
 			  struct lock_class_key *key)
 {
 	unsigned long flags __maybe_unused;
@@ -120,7 +120,7 @@ int __percpu_counter_init(struct percpu_counter *fbc, s64 amount,
 	raw_spin_lock_init(&fbc->lock);
 	lockdep_set_class(&fbc->lock, key);
 	fbc->count = amount;
-	fbc->counters = alloc_percpu(s32);
+	fbc->counters = alloc_percpu_gfp(s32, gfp);
 	if (!fbc->counters)
 		return -ENOMEM;
 

commit ebd8fef304f99da84d4a52ad056f6137ac9652d4
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Sep 8 09:51:29 2014 +0900

    percpu_counter: make percpu_counters_lock irq-safe
    
    percpu_counter is scheduled to grow @gfp support to allow atomic
    initialization.  This patch makes percpu_counters_lock irq-safe so
    that it can be safely used from atomic contexts.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 7dd33577b905..3fde78275cd1 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -115,6 +115,8 @@ EXPORT_SYMBOL(__percpu_counter_sum);
 int __percpu_counter_init(struct percpu_counter *fbc, s64 amount,
 			  struct lock_class_key *key)
 {
+	unsigned long flags __maybe_unused;
+
 	raw_spin_lock_init(&fbc->lock);
 	lockdep_set_class(&fbc->lock, key);
 	fbc->count = amount;
@@ -126,9 +128,9 @@ int __percpu_counter_init(struct percpu_counter *fbc, s64 amount,
 
 #ifdef CONFIG_HOTPLUG_CPU
 	INIT_LIST_HEAD(&fbc->list);
-	spin_lock(&percpu_counters_lock);
+	spin_lock_irqsave(&percpu_counters_lock, flags);
 	list_add(&fbc->list, &percpu_counters);
-	spin_unlock(&percpu_counters_lock);
+	spin_unlock_irqrestore(&percpu_counters_lock, flags);
 #endif
 	return 0;
 }
@@ -136,15 +138,17 @@ EXPORT_SYMBOL(__percpu_counter_init);
 
 void percpu_counter_destroy(struct percpu_counter *fbc)
 {
+	unsigned long flags __maybe_unused;
+
 	if (!fbc->counters)
 		return;
 
 	debug_percpu_counter_deactivate(fbc);
 
 #ifdef CONFIG_HOTPLUG_CPU
-	spin_lock(&percpu_counters_lock);
+	spin_lock_irqsave(&percpu_counters_lock, flags);
 	list_del(&fbc->list);
-	spin_unlock(&percpu_counters_lock);
+	spin_unlock_irqrestore(&percpu_counters_lock, flags);
 #endif
 	free_percpu(fbc->counters);
 	fbc->counters = NULL;
@@ -173,7 +177,7 @@ static int percpu_counter_hotcpu_callback(struct notifier_block *nb,
 		return NOTIFY_OK;
 
 	cpu = (unsigned long)hcpu;
-	spin_lock(&percpu_counters_lock);
+	spin_lock_irq(&percpu_counters_lock);
 	list_for_each_entry(fbc, &percpu_counters, list) {
 		s32 *pcount;
 		unsigned long flags;
@@ -184,7 +188,7 @@ static int percpu_counter_hotcpu_callback(struct notifier_block *nb,
 		*pcount = 0;
 		raw_spin_unlock_irqrestore(&fbc->lock, flags);
 	}
-	spin_unlock(&percpu_counters_lock);
+	spin_unlock_irq(&percpu_counters_lock);
 #endif
 	return NOTIFY_OK;
 }

commit e39435ce68bb4685288f78b1a7e24311f7ef939f
Author: Jens Axboe <axboe@fb.com>
Date:   Tue Apr 8 16:04:12 2014 -0700

    lib/percpu_counter.c: fix bad percpu counter state during suspend
    
    I got a bug report yesterday from Laszlo Ersek in which he states that
    his kvm instance fails to suspend.  Laszlo bisected it down to this
    commit 1cf7e9c68fe8 ("virtio_blk: blk-mq support") where virtio-blk is
    converted to use the blk-mq infrastructure.
    
    After digging a bit, it became clear that the issue was with the queue
    drain.  blk-mq tracks queue usage in a percpu counter, which is
    incremented on request alloc and decremented when the request is freed.
    The initial hunt was for an inconsistency in blk-mq, but everything
    seemed fine.  In fact, the counter only returned crazy values when
    suspend was in progress.
    
    When a CPU is unplugged, the percpu counters merges that CPU state with
    the general state.  blk-mq takes care to register a hotcpu notifier with
    the appropriate priority, so we know it runs after the percpu counter
    notifier.  However, the percpu counter notifier only merges the state
    when the CPU is fully gone.  This leaves a state transition where the
    CPU going away is no longer in the online mask, yet it still holds
    private values.  This means that in this state, percpu_counter_sum()
    returns invalid results, and the suspend then hangs waiting for
    abs(dead-cpu-value) requests to complete which of course will never
    happen.
    
    Fix this by clearing the state earlier, so we never have a case where
    the CPU isn't in online mask but still holds private state.  This bug
    has been there since forever, I guess we don't have a lot of users where
    percpu counters needs to be reliable during the suspend cycle.
    
    Signed-off-by: Jens Axboe <axboe@fb.com>
    Reported-by: Laszlo Ersek <lersek@redhat.com>
    Tested-by: Laszlo Ersek <lersek@redhat.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 8280a5dd1727..7dd33577b905 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -169,7 +169,7 @@ static int percpu_counter_hotcpu_callback(struct notifier_block *nb,
 	struct percpu_counter *fbc;
 
 	compute_batch_value();
-	if (action != CPU_DEAD)
+	if (action != CPU_DEAD && action != CPU_DEAD_FROZEN)
 		return NOTIFY_OK;
 
 	cpu = (unsigned long)hcpu;

commit d1969a84dd6a44d375aa82bba7d6c38713a429c3
Author: Hugh Dickins <hughd@google.com>
Date:   Thu Jan 16 15:26:48 2014 -0800

    percpu_counter: unbreak __percpu_counter_add()
    
    Commit 74e72f894d56 ("lib/percpu_counter.c: fix __percpu_counter_add()")
    looked very plausible, but its arithmetic was badly wrong: obvious once
    you see the fix, but maddening to get there from the weird tmpfs ENOSPCs
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: Ming Lei <tom.leiming@gmail.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Shaohua Li <shli@fusionio.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Fan Du <fan.du@windriver.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 1da85bb1bc07..8280a5dd1727 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -82,7 +82,7 @@ void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 		unsigned long flags;
 		raw_spin_lock_irqsave(&fbc->lock, flags);
 		fbc->count += count;
-		 __this_cpu_sub(*fbc->counters, count);
+		__this_cpu_sub(*fbc->counters, count - amount);
 		raw_spin_unlock_irqrestore(&fbc->lock, flags);
 	} else {
 		this_cpu_add(*fbc->counters, amount);

commit 74e72f894d56eb9d2e1218530c658e7d297e002b
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Tue Jan 14 17:56:42 2014 -0800

    lib/percpu_counter.c: fix __percpu_counter_add()
    
    __percpu_counter_add() may be called in softirq/hardirq handler (such
    as, blk_mq_queue_exit() is typically called in hardirq/softirq handler),
    so we need to call this_cpu_add()(irq safe helper) to update percpu
    counter, otherwise counts may be lost.
    
    This fixes the problem that 'rmmod null_blk' hangs in blk_cleanup_queue()
    because of miscounting of request_queue->mq_usage_counter.
    
    This patch is the v1 of previous one of "lib/percpu_counter.c:
    disable local irq when updating percpu couter", and takes Andrew's
    approach which may be more efficient for ARCHs(x86, s390) that
    have optimized this_cpu_add().
    
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Shaohua Li <shli@fusionio.com>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Fan Du <fan.du@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 7473ee3b4ee7..1da85bb1bc07 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -82,10 +82,10 @@ void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 		unsigned long flags;
 		raw_spin_lock_irqsave(&fbc->lock, flags);
 		fbc->count += count;
+		 __this_cpu_sub(*fbc->counters, count);
 		raw_spin_unlock_irqrestore(&fbc->lock, flags);
-		__this_cpu_write(*fbc->counters, 0);
 	} else {
-		__this_cpu_write(*fbc->counters, count);
+		this_cpu_add(*fbc->counters, amount);
 	}
 	preempt_enable();
 }

commit 098faf5805c80f951ce5e8b4a6842382ad793c38
Author: Shaohua Li <shli@fusionio.com>
Date:   Thu Oct 24 09:06:45 2013 +0100

    percpu_counter: make APIs irq safe
    
    In my usage, sometimes the percpu APIs are called with irq locked,
    sometimes not. lockdep complains there is potential deadlock. Let's
    always use percpucounter lock in irq safe way. There should be no
    performance penality, as all those are slow code path.
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Shaohua Li <shli@fusionio.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 93c5d5ecff4e..7473ee3b4ee7 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -60,14 +60,15 @@ static inline void debug_percpu_counter_deactivate(struct percpu_counter *fbc)
 void percpu_counter_set(struct percpu_counter *fbc, s64 amount)
 {
 	int cpu;
+	unsigned long flags;
 
-	raw_spin_lock(&fbc->lock);
+	raw_spin_lock_irqsave(&fbc->lock, flags);
 	for_each_possible_cpu(cpu) {
 		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
 		*pcount = 0;
 	}
 	fbc->count = amount;
-	raw_spin_unlock(&fbc->lock);
+	raw_spin_unlock_irqrestore(&fbc->lock, flags);
 }
 EXPORT_SYMBOL(percpu_counter_set);
 
@@ -78,9 +79,10 @@ void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 	preempt_disable();
 	count = __this_cpu_read(*fbc->counters) + amount;
 	if (count >= batch || count <= -batch) {
-		raw_spin_lock(&fbc->lock);
+		unsigned long flags;
+		raw_spin_lock_irqsave(&fbc->lock, flags);
 		fbc->count += count;
-		raw_spin_unlock(&fbc->lock);
+		raw_spin_unlock_irqrestore(&fbc->lock, flags);
 		__this_cpu_write(*fbc->counters, 0);
 	} else {
 		__this_cpu_write(*fbc->counters, count);
@@ -97,14 +99,15 @@ s64 __percpu_counter_sum(struct percpu_counter *fbc)
 {
 	s64 ret;
 	int cpu;
+	unsigned long flags;
 
-	raw_spin_lock(&fbc->lock);
+	raw_spin_lock_irqsave(&fbc->lock, flags);
 	ret = fbc->count;
 	for_each_online_cpu(cpu) {
 		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
 		ret += *pcount;
 	}
-	raw_spin_unlock(&fbc->lock);
+	raw_spin_unlock_irqrestore(&fbc->lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL(__percpu_counter_sum);

commit 0db0628d90125193280eabb501c94feaf48fa9ab
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Jun 19 14:53:51 2013 -0400

    kernel: delete __cpuinit usage from all core kernel files
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    This removes all the uses of the __cpuinit macros from C files in
    the core kernel directories (kernel, init, lib, mm, and include)
    that don't really have a specific maintainer.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 1fc23a3277e1..93c5d5ecff4e 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -158,7 +158,7 @@ static void compute_batch_value(void)
 	percpu_counter_batch = max(32, nr*2);
 }
 
-static int __cpuinit percpu_counter_hotcpu_callback(struct notifier_block *nb,
+static int percpu_counter_hotcpu_callback(struct notifier_block *nb,
 					unsigned long action, void *hcpu)
 {
 #ifdef CONFIG_HOTPLUG_CPU

commit 64df3071a97f20767f63b88c573791691a855b5c
Author: Fan Du <fan.du@windriver.com>
Date:   Wed Jul 3 15:05:19 2013 -0700

    lib/percpu_counter.c: __this_cpu_write() doesn't need to be protected by spinlock
    
    __this_cpu_write doesn't need to be protected by spinlock, AS we are doing
    per cpu write with preempt disabled.  And another reason to remove
    __this_cpu_write outside of spinlock: __percpu_counter_sum is not an
    accurate counter.
    
    Signed-off-by: Fan Du <fan.du@windriver.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index ba6085d9c741..1fc23a3277e1 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -80,8 +80,8 @@ void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 	if (count >= batch || count <= -batch) {
 		raw_spin_lock(&fbc->lock);
 		fbc->count += count;
-		__this_cpu_write(*fbc->counters, 0);
 		raw_spin_unlock(&fbc->lock);
+		__this_cpu_write(*fbc->counters, 0);
 	} else {
 		__this_cpu_write(*fbc->counters, count);
 	}

commit d87aae2f3c8e90bd0fe03f5309b4d066b712b8ec
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Jul 31 09:28:31 2012 +0400

    switch the protection of percpu_counter list to spinlock
    
    ... making percpu_counter_destroy() non-blocking
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index f8a3f1a829b8..ba6085d9c741 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -12,7 +12,7 @@
 
 #ifdef CONFIG_HOTPLUG_CPU
 static LIST_HEAD(percpu_counters);
-static DEFINE_MUTEX(percpu_counters_lock);
+static DEFINE_SPINLOCK(percpu_counters_lock);
 #endif
 
 #ifdef CONFIG_DEBUG_OBJECTS_PERCPU_COUNTER
@@ -123,9 +123,9 @@ int __percpu_counter_init(struct percpu_counter *fbc, s64 amount,
 
 #ifdef CONFIG_HOTPLUG_CPU
 	INIT_LIST_HEAD(&fbc->list);
-	mutex_lock(&percpu_counters_lock);
+	spin_lock(&percpu_counters_lock);
 	list_add(&fbc->list, &percpu_counters);
-	mutex_unlock(&percpu_counters_lock);
+	spin_unlock(&percpu_counters_lock);
 #endif
 	return 0;
 }
@@ -139,9 +139,9 @@ void percpu_counter_destroy(struct percpu_counter *fbc)
 	debug_percpu_counter_deactivate(fbc);
 
 #ifdef CONFIG_HOTPLUG_CPU
-	mutex_lock(&percpu_counters_lock);
+	spin_lock(&percpu_counters_lock);
 	list_del(&fbc->list);
-	mutex_unlock(&percpu_counters_lock);
+	spin_unlock(&percpu_counters_lock);
 #endif
 	free_percpu(fbc->counters);
 	fbc->counters = NULL;
@@ -170,7 +170,7 @@ static int __cpuinit percpu_counter_hotcpu_callback(struct notifier_block *nb,
 		return NOTIFY_OK;
 
 	cpu = (unsigned long)hcpu;
-	mutex_lock(&percpu_counters_lock);
+	spin_lock(&percpu_counters_lock);
 	list_for_each_entry(fbc, &percpu_counters, list) {
 		s32 *pcount;
 		unsigned long flags;
@@ -181,7 +181,7 @@ static int __cpuinit percpu_counter_hotcpu_callback(struct notifier_block *nb,
 		*pcount = 0;
 		raw_spin_unlock_irqrestore(&fbc->lock, flags);
 	}
-	mutex_unlock(&percpu_counters_lock);
+	spin_unlock(&percpu_counters_lock);
 #endif
 	return NOTIFY_OK;
 }

commit 3a8495c739c1c773181a246dbb7c12b5b67a8325
Author: Glauber Costa <glommer@parallels.com>
Date:   Mon Oct 31 17:12:34 2011 -0700

    lib/percpu_counter.c: enclose hotplug only variables in hotplug ifdef
    
    These variables are only used when CONFIG_HOTPLUG_CPU is enabled, they are
    ifdef'ed everywhere else.  So don't define them when CONFIG_HOTPLUG_CPU is
    not enabled.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index f087105ed914..f8a3f1a829b8 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -10,8 +10,10 @@
 #include <linux/module.h>
 #include <linux/debugobjects.h>
 
+#ifdef CONFIG_HOTPLUG_CPU
 static LIST_HEAD(percpu_counters);
 static DEFINE_MUTEX(percpu_counters_lock);
+#endif
 
 #ifdef CONFIG_DEBUG_OBJECTS_PERCPU_COUNTER
 

commit f032a450812f6c7edd532772cc7c48091bca9f27
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jul 25 16:21:48 2009 +0200

    locking, percpu_counter: Annotate ::lock as raw
    
    The percpu_counter::lock can be taken in atomic context and therefore
    cannot be preempted on -rt - annotate it.
    
    In mainline this change documents the low level nature of
    the lock - otherwise there's no functional difference. Lockdep
    and Sparse checking will work as usual.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 28f2c33c6b53..f087105ed914 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -59,13 +59,13 @@ void percpu_counter_set(struct percpu_counter *fbc, s64 amount)
 {
 	int cpu;
 
-	spin_lock(&fbc->lock);
+	raw_spin_lock(&fbc->lock);
 	for_each_possible_cpu(cpu) {
 		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
 		*pcount = 0;
 	}
 	fbc->count = amount;
-	spin_unlock(&fbc->lock);
+	raw_spin_unlock(&fbc->lock);
 }
 EXPORT_SYMBOL(percpu_counter_set);
 
@@ -76,10 +76,10 @@ void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 	preempt_disable();
 	count = __this_cpu_read(*fbc->counters) + amount;
 	if (count >= batch || count <= -batch) {
-		spin_lock(&fbc->lock);
+		raw_spin_lock(&fbc->lock);
 		fbc->count += count;
 		__this_cpu_write(*fbc->counters, 0);
-		spin_unlock(&fbc->lock);
+		raw_spin_unlock(&fbc->lock);
 	} else {
 		__this_cpu_write(*fbc->counters, count);
 	}
@@ -96,13 +96,13 @@ s64 __percpu_counter_sum(struct percpu_counter *fbc)
 	s64 ret;
 	int cpu;
 
-	spin_lock(&fbc->lock);
+	raw_spin_lock(&fbc->lock);
 	ret = fbc->count;
 	for_each_online_cpu(cpu) {
 		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
 		ret += *pcount;
 	}
-	spin_unlock(&fbc->lock);
+	raw_spin_unlock(&fbc->lock);
 	return ret;
 }
 EXPORT_SYMBOL(__percpu_counter_sum);
@@ -110,7 +110,7 @@ EXPORT_SYMBOL(__percpu_counter_sum);
 int __percpu_counter_init(struct percpu_counter *fbc, s64 amount,
 			  struct lock_class_key *key)
 {
-	spin_lock_init(&fbc->lock);
+	raw_spin_lock_init(&fbc->lock);
 	lockdep_set_class(&fbc->lock, key);
 	fbc->count = amount;
 	fbc->counters = alloc_percpu(s32);
@@ -173,11 +173,11 @@ static int __cpuinit percpu_counter_hotcpu_callback(struct notifier_block *nb,
 		s32 *pcount;
 		unsigned long flags;
 
-		spin_lock_irqsave(&fbc->lock, flags);
+		raw_spin_lock_irqsave(&fbc->lock, flags);
 		pcount = per_cpu_ptr(fbc->counters, cpu);
 		fbc->count += *pcount;
 		*pcount = 0;
-		spin_unlock_irqrestore(&fbc->lock, flags);
+		raw_spin_unlock_irqrestore(&fbc->lock, flags);
 	}
 	mutex_unlock(&percpu_counters_lock);
 #endif

commit 819a72af8d6653daa48334f24ce0a935ccdd33c7
Author: Christoph Lameter <cl@linux.com>
Date:   Mon Dec 6 11:16:19 2010 -0600

    percpucounter: Optimize __percpu_counter_add a bit through the use of this_cpu() options.
    
    The this_cpu_* options can be used to optimize __percpu_counter_add a bit. Avoids
    some address arithmetic and saves 12 bytes.
    
    Before:
    
    
    00000000000001d3 <__percpu_counter_add>:
     1d3:   55                      push   %rbp
     1d4:   48 89 e5                mov    %rsp,%rbp
     1d7:   41 55                   push   %r13
     1d9:   41 54                   push   %r12
     1db:   53                      push   %rbx
     1dc:   48 89 fb                mov    %rdi,%rbx
     1df:   48 83 ec 08             sub    $0x8,%rsp
     1e3:   4c 8b 67 30             mov    0x30(%rdi),%r12
     1e7:   65 4c 03 24 25 00 00    add    %gs:0x0,%r12
     1ee:   00 00
     1f0:   4d 63 2c 24             movslq (%r12),%r13
     1f4:   48 63 c2                movslq %edx,%rax
     1f7:   49 01 f5                add    %rsi,%r13
     1fa:   49 39 c5                cmp    %rax,%r13
     1fd:   7d 0a                   jge    209 <__percpu_counter_add+0x36>
     1ff:   f7 da                   neg    %edx
     201:   48 63 d2                movslq %edx,%rdx
     204:   49 39 d5                cmp    %rdx,%r13
     207:   7f 1e                   jg     227 <__percpu_counter_add+0x54>
     209:   48 89 df                mov    %rbx,%rdi
     20c:   e8 00 00 00 00          callq  211 <__percpu_counter_add+0x3e>
     211:   4c 01 6b 18             add    %r13,0x18(%rbx)
     215:   48 89 df                mov    %rbx,%rdi
     218:   41 c7 04 24 00 00 00    movl   $0x0,(%r12)
     21f:   00
     220:   e8 00 00 00 00          callq  225 <__percpu_counter_add+0x52>
     225:   eb 04                   jmp    22b <__percpu_counter_add+0x58>
     227:   45 89 2c 24             mov    %r13d,(%r12)
     22b:   5b                      pop    %rbx
     22c:   5b                      pop    %rbx
     22d:   41 5c                   pop    %r12
     22f:   41 5d                   pop    %r13
     231:   c9                      leaveq
     232:   c3                      retq
    
    
    After:
    
    00000000000001d3 <__percpu_counter_add>:
     1d3:   55                      push   %rbp
     1d4:   48 63 ca                movslq %edx,%rcx
     1d7:   48 89 e5                mov    %rsp,%rbp
     1da:   41 54                   push   %r12
     1dc:   53                      push   %rbx
     1dd:   48 89 fb                mov    %rdi,%rbx
     1e0:   48 8b 47 30             mov    0x30(%rdi),%rax
     1e4:   65 44 8b 20             mov    %gs:(%rax),%r12d
     1e8:   4d 63 e4                movslq %r12d,%r12
     1eb:   49 01 f4                add    %rsi,%r12
     1ee:   49 39 cc                cmp    %rcx,%r12
     1f1:   7d 0a                   jge    1fd <__percpu_counter_add+0x2a>
     1f3:   f7 da                   neg    %edx
     1f5:   48 63 d2                movslq %edx,%rdx
     1f8:   49 39 d4                cmp    %rdx,%r12
     1fb:   7f 21                   jg     21e <__percpu_counter_add+0x4b>
     1fd:   48 89 df                mov    %rbx,%rdi
     200:   e8 00 00 00 00          callq  205 <__percpu_counter_add+0x32>
     205:   4c 01 63 18             add    %r12,0x18(%rbx)
     209:   48 8b 43 30             mov    0x30(%rbx),%rax
     20d:   48 89 df                mov    %rbx,%rdi
     210:   65 c7 00 00 00 00 00    movl   $0x0,%gs:(%rax)
     217:   e8 00 00 00 00          callq  21c <__percpu_counter_add+0x49>
     21c:   eb 04                   jmp    222 <__percpu_counter_add+0x4f>
     21e:   65 44 89 20             mov    %r12d,%gs:(%rax)
     222:   5b                      pop    %rbx
     223:   41 5c                   pop    %r12
     225:   c9                      leaveq
     226:   c3                      retq
    
    Reviewed-by: Pekka Enberg <penberg@kernel.org>
    Reviewed-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 604678d7d06d..28f2c33c6b53 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -72,18 +72,16 @@ EXPORT_SYMBOL(percpu_counter_set);
 void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 {
 	s64 count;
-	s32 *pcount;
 
 	preempt_disable();
-	pcount = this_cpu_ptr(fbc->counters);
-	count = *pcount + amount;
+	count = __this_cpu_read(*fbc->counters) + amount;
 	if (count >= batch || count <= -batch) {
 		spin_lock(&fbc->lock);
 		fbc->count += count;
-		*pcount = 0;
+		__this_cpu_write(*fbc->counters, 0);
 		spin_unlock(&fbc->lock);
 	} else {
-		*pcount = count;
+		__this_cpu_write(*fbc->counters, count);
 	}
 	preempt_enable();
 }

commit ea00c30b5b31baa91be29bee966204eccc15e9d3
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Oct 26 14:23:09 2010 -0700

    percpu_counter: use this_cpu_ptr() instead of per_cpu_ptr()
    
    this_cpu_ptr() avoids an array lookup and can use the percpu offset of the
    local cpu directly.
    
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 1d954ea72331..604678d7d06d 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -73,9 +73,9 @@ void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 {
 	s64 count;
 	s32 *pcount;
-	int cpu = get_cpu();
 
-	pcount = per_cpu_ptr(fbc->counters, cpu);
+	preempt_disable();
+	pcount = this_cpu_ptr(fbc->counters);
 	count = *pcount + amount;
 	if (count >= batch || count <= -batch) {
 		spin_lock(&fbc->lock);
@@ -85,7 +85,7 @@ void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 	} else {
 		*pcount = count;
 	}
-	put_cpu();
+	preempt_enable();
 }
 EXPORT_SYMBOL(__percpu_counter_add);
 

commit e2852ae825dba5ebc159788720baec1a28a57125
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Oct 26 14:23:05 2010 -0700

    percpu_counter: add debugobj support
    
    All percpu counters are linked to a global list on initialization and
    removed from it on destruction.  The list is walked during CPU up/down.
    If a percpu counter is freed without being properly destroyed, the system
    will oops only on the next CPU up/down making it pretty nasty to track
    down.  This patch adds debugobj support for percpu counters so that such
    problems can be found easily.
    
    As percpu counters don't make sense on stack and can't be statically
    initialized, debugobj support is pretty simple.  It's initialized and
    activated on counter initialization, and deactivatd and destroyed on
    counter destruction.  With this patch applied, the bug fixed by commit
    602586a83b719df0fbd94196a1359ed35aeb2df3 (shmem: put_super must
    percpu_counter_destroy) triggers the following warning on tmpfs unmount
    and the system won't oops on the next cpu up/down operation.
    
     ------------[ cut here ]------------
     WARNING: at lib/debugobjects.c:259 debug_print_object+0x5c/0x70()
     Hardware name: Bochs
     ODEBUG: free active (active state 0) object type: percpu_counter
     Modules linked in:
     Pid: 3999, comm: umount Not tainted 2.6.36-rc2-work+ #5
     Call Trace:
      [<ffffffff81083f7f>] warn_slowpath_common+0x7f/0xc0
      [<ffffffff81084076>] warn_slowpath_fmt+0x46/0x50
      [<ffffffff813b45cc>] debug_print_object+0x5c/0x70
      [<ffffffff813b50e5>] debug_check_no_obj_freed+0x125/0x210
      [<ffffffff811577d3>] kfree+0xb3/0x2f0
      [<ffffffff81132edd>] shmem_put_super+0x1d/0x30
      [<ffffffff81162e96>] generic_shutdown_super+0x56/0xe0
      [<ffffffff81162f86>] kill_anon_super+0x16/0x60
      [<ffffffff81162ff7>] kill_litter_super+0x27/0x30
      [<ffffffff81163295>] deactivate_locked_super+0x45/0x60
      [<ffffffff81163cfa>] deactivate_super+0x4a/0x70
      [<ffffffff8117d446>] mntput_no_expire+0x86/0xe0
      [<ffffffff8117df7f>] sys_umount+0x6f/0x360
      [<ffffffff8103f01b>] system_call_fastpath+0x16/0x1b
     ---[ end trace cce2a341ba3611a7 ]---
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Thomas Gleixner <tglxlinutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 209448e1d2b9..1d954ea72331 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -8,10 +8,53 @@
 #include <linux/init.h>
 #include <linux/cpu.h>
 #include <linux/module.h>
+#include <linux/debugobjects.h>
 
 static LIST_HEAD(percpu_counters);
 static DEFINE_MUTEX(percpu_counters_lock);
 
+#ifdef CONFIG_DEBUG_OBJECTS_PERCPU_COUNTER
+
+static struct debug_obj_descr percpu_counter_debug_descr;
+
+static int percpu_counter_fixup_free(void *addr, enum debug_obj_state state)
+{
+	struct percpu_counter *fbc = addr;
+
+	switch (state) {
+	case ODEBUG_STATE_ACTIVE:
+		percpu_counter_destroy(fbc);
+		debug_object_free(fbc, &percpu_counter_debug_descr);
+		return 1;
+	default:
+		return 0;
+	}
+}
+
+static struct debug_obj_descr percpu_counter_debug_descr = {
+	.name		= "percpu_counter",
+	.fixup_free	= percpu_counter_fixup_free,
+};
+
+static inline void debug_percpu_counter_activate(struct percpu_counter *fbc)
+{
+	debug_object_init(fbc, &percpu_counter_debug_descr);
+	debug_object_activate(fbc, &percpu_counter_debug_descr);
+}
+
+static inline void debug_percpu_counter_deactivate(struct percpu_counter *fbc)
+{
+	debug_object_deactivate(fbc, &percpu_counter_debug_descr);
+	debug_object_free(fbc, &percpu_counter_debug_descr);
+}
+
+#else	/* CONFIG_DEBUG_OBJECTS_PERCPU_COUNTER */
+static inline void debug_percpu_counter_activate(struct percpu_counter *fbc)
+{ }
+static inline void debug_percpu_counter_deactivate(struct percpu_counter *fbc)
+{ }
+#endif	/* CONFIG_DEBUG_OBJECTS_PERCPU_COUNTER */
+
 void percpu_counter_set(struct percpu_counter *fbc, s64 amount)
 {
 	int cpu;
@@ -75,6 +118,9 @@ int __percpu_counter_init(struct percpu_counter *fbc, s64 amount,
 	fbc->counters = alloc_percpu(s32);
 	if (!fbc->counters)
 		return -ENOMEM;
+
+	debug_percpu_counter_activate(fbc);
+
 #ifdef CONFIG_HOTPLUG_CPU
 	INIT_LIST_HEAD(&fbc->list);
 	mutex_lock(&percpu_counters_lock);
@@ -90,6 +136,8 @@ void percpu_counter_destroy(struct percpu_counter *fbc)
 	if (!fbc->counters)
 		return;
 
+	debug_percpu_counter_deactivate(fbc);
+
 #ifdef CONFIG_HOTPLUG_CPU
 	mutex_lock(&percpu_counters_lock);
 	list_del(&fbc->list);

commit 8474b591faf3bb0a1e08a60d21d6baac498f15e4
Author: Masanori ITOH <itoumsn@nttdata.co.jp>
Date:   Tue Oct 26 14:21:20 2010 -0700

    percpu: fix list_head init bug in __percpu_counter_init()
    
    WARNING: at lib/list_debug.c:26 __list_add+0x3f/0x81()
    Hardware name: Express5800/B120a [N8400-085]
    list_add corruption. next->prev should be prev (ffffffff81a7ea00), but was dead000000200200. (next=ffff88080b872d58).
    Modules linked in: aoe ipt_MASQUERADE iptable_nat nf_nat autofs4 sunrpc bridge 8021q garp stp llc ipv6 cpufreq_ondemand acpi_cpufreq freq_table dm_round_robin dm_multipath kvm_intel kvm uinput lpfc scsi_transport_fc igb ioatdma scsi_tgt i2c_i801 i2c_core dca iTCO_wdt iTCO_vendor_support pcspkr shpchp megaraid_sas [last unloaded: aoe]
    Pid: 54, comm: events/3 Tainted: G        W  2.6.34-vanilla1 #1
    Call Trace:
    [<ffffffff8104bd77>] warn_slowpath_common+0x7c/0x94
    [<ffffffff8104bde6>] warn_slowpath_fmt+0x41/0x43
    [<ffffffff8120fd2e>] __list_add+0x3f/0x81
    [<ffffffff81212a12>] __percpu_counter_init+0x59/0x6b
    [<ffffffff810d8499>] bdi_init+0x118/0x17e
    [<ffffffff811f2c50>] blk_alloc_queue_node+0x79/0x143
    [<ffffffff811f2d2b>] blk_alloc_queue+0x11/0x13
    [<ffffffffa02a931d>] aoeblk_gdalloc+0x8e/0x1c9 [aoe]
    [<ffffffffa02aa655>] aoecmd_sleepwork+0x25/0xa8 [aoe]
    [<ffffffff8106186c>] worker_thread+0x1a9/0x237
    [<ffffffffa02aa630>] ? aoecmd_sleepwork+0x0/0xa8 [aoe]
    [<ffffffff81065827>] ? autoremove_wake_function+0x0/0x39
    [<ffffffff810616c3>] ? worker_thread+0x0/0x237
    [<ffffffff810653ad>] kthread+0x7f/0x87
    [<ffffffff8100aa24>] kernel_thread_helper+0x4/0x10
    [<ffffffff8106532e>] ? kthread+0x0/0x87
    [<ffffffff8100aa20>] ? kernel_thread_helper+0x0/0x10
    
    It's because there is no initialization code for a list_head contained in
    the struct backing_dev_info under CONFIG_HOTPLUG_CPU, and the bug comes up
    when block device drivers calling blk_alloc_queue() are used.  In case of
    me, I got them by using aoe.
    
    Signed-off-by: Masanori Itoh <itoumsn@nttdata.co.jp>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: <stable@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index ec9048e74f44..209448e1d2b9 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -76,6 +76,7 @@ int __percpu_counter_init(struct percpu_counter *fbc, s64 amount,
 	if (!fbc->counters)
 		return -ENOMEM;
 #ifdef CONFIG_HOTPLUG_CPU
+	INIT_LIST_HEAD(&fbc->list);
 	mutex_lock(&percpu_counters_lock);
 	list_add(&fbc->list, &percpu_counters);
 	mutex_unlock(&percpu_counters_lock);

commit 27f5e0f694fd0600274a76854636c0749e3bb1f6
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Mon Aug 9 17:19:04 2010 -0700

    tmpfs: add accurate compare function to percpu_counter library
    
    Add percpu_counter_compare that allows for a quick but accurate comparison
    of percpu_counter with a given value.
    
    A rough count is provided by the count field in percpu_counter structure,
    without accounting for the other values stored in individual cpu counters.
    
    The actual count is a sum of count and the cpu counters.  However, count
    field is never different from the actual value by a factor of
    batch*num_online_cpu.  We do not need to get actual count for comparison
    if count is different from the given value by this factor and allows for
    quick comparison without summing up all the per cpu counters.
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index aeaa6d734447..ec9048e74f44 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -137,6 +137,33 @@ static int __cpuinit percpu_counter_hotcpu_callback(struct notifier_block *nb,
 	return NOTIFY_OK;
 }
 
+/*
+ * Compare counter against given value.
+ * Return 1 if greater, 0 if equal and -1 if less
+ */
+int percpu_counter_compare(struct percpu_counter *fbc, s64 rhs)
+{
+	s64	count;
+
+	count = percpu_counter_read(fbc);
+	/* Check to see if rough count will be sufficient for comparison */
+	if (abs(count - rhs) > (percpu_counter_batch*num_online_cpus())) {
+		if (count > rhs)
+			return 1;
+		else
+			return -1;
+	}
+	/* Need to use precise count */
+	count = percpu_counter_sum(fbc);
+	if (count > rhs)
+		return 1;
+	else if (count < rhs)
+		return -1;
+	else
+		return 0;
+}
+EXPORT_SYMBOL(percpu_counter_compare);
+
 static int __init percpu_counter_startup(void)
 {
 	compute_batch_value();

commit f94181da7192f4ed8ccb1b633ea4ce56954df130
Merge: 932adbed6d99 fdbc0450df12
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 6 17:10:04 2009 -0800

    Merge branch 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'core-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      rcu: fix rcutorture bug
      rcu: eliminate synchronize_rcu_xxx macro
      rcu: make treercu safe for suspend and resume
      rcu: fix rcutree grace-period-latency bug on small systems
      futex: catch certain assymetric (get|put)_futex_key calls
      futex: make futex_(get|put)_key() calls symmetric
      locking, percpu counters: introduce separate lock classes
      swiotlb: clean up EXPORT_SYMBOL usage
      swiotlb: remove unnecessary declaration
      swiotlb: replace architecture-specific swiotlb.h with linux/swiotlb.h
      swiotlb: add support for systems with highmem
      swiotlb: store phys address in io_tlb_orig_addr array
      swiotlb: add hwdev to swiotlb_phys_to_bus() / swiotlb_sg_to_bus()

commit 179f7ebff6be45738c6e2fa68c8d2cc5c2c6308e
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue Jan 6 14:41:04 2009 -0800

    percpu_counter: FBC_BATCH should be a variable
    
    For NR_CPUS >= 16 values, FBC_BATCH is 2*NR_CPUS
    
    Considering more and more distros are using high NR_CPUS values, it makes
    sense to use a more sensible value for FBC_BATCH, and get rid of NR_CPUS.
    
    A sensible value is 2*num_online_cpus(), with a minimum value of 32 (This
    minimum value helps branch prediction in __percpu_counter_add())
    
    We already have a hotcpu notifier, so we can adjust FBC_BATCH dynamically.
    
    We rename FBC_BATCH to percpu_counter_batch since its not a constant
    anymore.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index b255b939bc1b..a60bd8046095 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -9,10 +9,8 @@
 #include <linux/cpu.h>
 #include <linux/module.h>
 
-#ifdef CONFIG_HOTPLUG_CPU
 static LIST_HEAD(percpu_counters);
 static DEFINE_MUTEX(percpu_counters_lock);
-#endif
 
 void percpu_counter_set(struct percpu_counter *fbc, s64 amount)
 {
@@ -111,13 +109,24 @@ void percpu_counter_destroy(struct percpu_counter *fbc)
 }
 EXPORT_SYMBOL(percpu_counter_destroy);
 
-#ifdef CONFIG_HOTPLUG_CPU
+int percpu_counter_batch __read_mostly = 32;
+EXPORT_SYMBOL(percpu_counter_batch);
+
+static void compute_batch_value(void)
+{
+	int nr = num_online_cpus();
+
+	percpu_counter_batch = max(32, nr*2);
+}
+
 static int __cpuinit percpu_counter_hotcpu_callback(struct notifier_block *nb,
 					unsigned long action, void *hcpu)
 {
+#ifdef CONFIG_HOTPLUG_CPU
 	unsigned int cpu;
 	struct percpu_counter *fbc;
 
+	compute_batch_value();
 	if (action != CPU_DEAD)
 		return NOTIFY_OK;
 
@@ -134,13 +143,14 @@ static int __cpuinit percpu_counter_hotcpu_callback(struct notifier_block *nb,
 		spin_unlock_irqrestore(&fbc->lock, flags);
 	}
 	mutex_unlock(&percpu_counters_lock);
+#endif
 	return NOTIFY_OK;
 }
 
 static int __init percpu_counter_startup(void)
 {
+	compute_batch_value();
 	hotcpu_notifier(percpu_counter_hotcpu_callback, 0);
 	return 0;
 }
 module_init(percpu_counter_startup);
-#endif

commit fdbc0450df12cc9cb397f3497db4b0cad7c1a8ff
Merge: 46483d10e512 90621c40cc4a ea319518ba3d 238c6d54830c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 6 09:32:11 2009 +0100

    Merge branches 'core/futexes', 'core/locking', 'core/rcu' and 'linus' into core/urgent

commit ea319518ba3de282c13ae1cf4bf2215c5e03e67e
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Dec 26 15:08:55 2008 +0100

    locking, percpu counters: introduce separate lock classes
    
    Impact: fix lockdep false positives
    
    Classify percpu_counter instances similar to regular lock objects --
    that is, per instantiation site.
    
    The networking code has increased its use of percpu_counters, which
    leads to false positives if they are treated as a single class.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index a8663890a88c..c7fe2e4e8ed1 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -71,11 +71,11 @@ s64 __percpu_counter_sum(struct percpu_counter *fbc)
 }
 EXPORT_SYMBOL(__percpu_counter_sum);
 
-static struct lock_class_key percpu_counter_irqsafe;
-
-int percpu_counter_init(struct percpu_counter *fbc, s64 amount)
+int __percpu_counter_init(struct percpu_counter *fbc, s64 amount,
+			  struct lock_class_key *key)
 {
 	spin_lock_init(&fbc->lock);
+	lockdep_set_class(&fbc->lock, key);
 	fbc->count = amount;
 	fbc->counters = alloc_percpu(s32);
 	if (!fbc->counters)
@@ -87,17 +87,7 @@ int percpu_counter_init(struct percpu_counter *fbc, s64 amount)
 #endif
 	return 0;
 }
-EXPORT_SYMBOL(percpu_counter_init);
-
-int percpu_counter_init_irq(struct percpu_counter *fbc, s64 amount)
-{
-	int err;
-
-	err = percpu_counter_init(fbc, amount);
-	if (!err)
-		lockdep_set_class(&fbc->lock, &percpu_counter_irqsafe);
-	return err;
-}
+EXPORT_SYMBOL(__percpu_counter_init);
 
 void percpu_counter_destroy(struct percpu_counter *fbc)
 {

commit 02d211688727ad02bb4555b1aa8ae2de16b21b39
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Dec 9 13:14:14 2008 -0800

    revert "percpu_counter: new function percpu_counter_sum_and_set"
    
    Revert
    
        commit e8ced39d5e8911c662d4d69a342b9d053eaaac4e
        Author: Mingming Cao <cmm@us.ibm.com>
        Date:   Fri Jul 11 19:27:31 2008 -0400
    
            percpu_counter: new function percpu_counter_sum_and_set
    
    As described in
    
            revert "percpu counter: clean up percpu_counter_sum_and_set()"
    
    the new percpu_counter_sum_and_set() is racy against updates to the
    cpu-local accumulators on other CPUs.  Revert that change.
    
    This means that ext4 will be slow again.  But correct.
    
    Reported-by: Eric Dumazet <dada1@cosmosbay.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mingming Cao <cmm@us.ibm.com>
    Cc: <linux-ext4@vger.kernel.org>
    Cc: <stable@kernel.org>         [2.6.27.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index dba1530a5b29..b255b939bc1b 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -52,7 +52,7 @@ EXPORT_SYMBOL(__percpu_counter_add);
  * Add up all the per-cpu counts, return the result.  This is a more accurate
  * but much slower version of percpu_counter_read_positive()
  */
-s64 __percpu_counter_sum(struct percpu_counter *fbc, int set)
+s64 __percpu_counter_sum(struct percpu_counter *fbc)
 {
 	s64 ret;
 	int cpu;
@@ -62,12 +62,7 @@ s64 __percpu_counter_sum(struct percpu_counter *fbc, int set)
 	for_each_online_cpu(cpu) {
 		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
 		ret += *pcount;
-		if (set)
-			*pcount = 0;
 	}
-	if (set)
-		fbc->count = ret;
-
 	spin_unlock(&fbc->lock);
 	return ret;
 }

commit 71c5576fbd809f2015f4eddf72e501e298720cf3
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Tue Dec 9 13:14:13 2008 -0800

    revert "percpu counter: clean up percpu_counter_sum_and_set()"
    
    Revert
    
        commit 1f7c14c62ce63805f9574664a6c6de3633d4a354
        Author: Mingming Cao <cmm@us.ibm.com>
        Date:   Thu Oct 9 12:50:59 2008 -0400
    
            percpu counter: clean up percpu_counter_sum_and_set()
    
    Before this patch we had the following:
    
    percpu_counter_sum(): return the percpu_counter's value
    
    percpu_counter_sum_and_set(): return the percpu_counter's value, copying
    that value into the central value and zeroing the per-cpu counters before
    returning.
    
    After this patch, percpu_counter_sum_and_set() has gone, and
    percpu_counter_sum() gets the old percpu_counter_sum_and_set()
    functionality.
    
    Problem is, as Eric points out, the old percpu_counter_sum_and_set()
    functionality was racy and wrong.  It zeroes out counters on "other" cpus,
    without holding any locks which will prevent races agaist updates from
    those other CPUS.
    
    This patch reverts 1f7c14c62ce63805f9574664a6c6de3633d4a354.  This means
    that percpu_counter_sum_and_set() still has the race, but
    percpu_counter_sum() does not.
    
    Note that this is not a simple revert - ext4 has since started using
    percpu_counter_sum() for its dirty_blocks counter as well.
    
    Note that this revert patch changes percpu_counter_sum() semantics.
    
    Before the patch, a call to percpu_counter_sum() will bring the counter's
    central counter mostly up-to-date, so a following percpu_counter_read()
    will return a close value.
    
    After this patch, a call to percpu_counter_sum() will leave the counter's
    central accumulator unaltered, so a subsequent call to
    percpu_counter_read() can now return a significantly inaccurate result.
    
    If there is any code in the tree which was introduced after
    e8ced39d5e8911c662d4d69a342b9d053eaaac4e was merged, and which depends
    upon the new percpu_counter_sum() semantics, that code will break.
    
    Reported-by: Eric Dumazet <dada1@cosmosbay.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mingming Cao <cmm@us.ibm.com>
    Cc: <linux-ext4@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 71b265c330ce..dba1530a5b29 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -52,7 +52,7 @@ EXPORT_SYMBOL(__percpu_counter_add);
  * Add up all the per-cpu counts, return the result.  This is a more accurate
  * but much slower version of percpu_counter_read_positive()
  */
-s64 __percpu_counter_sum(struct percpu_counter *fbc)
+s64 __percpu_counter_sum(struct percpu_counter *fbc, int set)
 {
 	s64 ret;
 	int cpu;
@@ -62,9 +62,11 @@ s64 __percpu_counter_sum(struct percpu_counter *fbc)
 	for_each_online_cpu(cpu) {
 		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
 		ret += *pcount;
-		*pcount = 0;
+		if (set)
+			*pcount = 0;
 	}
-	fbc->count = ret;
+	if (set)
+		fbc->count = ret;
 
 	spin_unlock(&fbc->lock);
 	return ret;

commit fd3d664fef97cf01f8e28fe0b024ad52f3bbc1bc
Author: Eric Dumazet <dada1@cosmosbay.com>
Date:   Tue Dec 9 13:14:11 2008 -0800

    percpu_counter: fix CPU unplug race in percpu_counter_destroy()
    
    We should first delete the counter from percpu_counters list
    before freeing memory, or a percpu_counter_hotcpu_callback()
    could dereference a NULL pointer.
    
    Signed-off-by: Eric Dumazet <dada1@cosmosbay.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mingming Cao <cmm@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index a8663890a88c..71b265c330ce 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -104,13 +104,13 @@ void percpu_counter_destroy(struct percpu_counter *fbc)
 	if (!fbc->counters)
 		return;
 
-	free_percpu(fbc->counters);
-	fbc->counters = NULL;
 #ifdef CONFIG_HOTPLUG_CPU
 	mutex_lock(&percpu_counters_lock);
 	list_del(&fbc->list);
 	mutex_unlock(&percpu_counters_lock);
 #endif
+	free_percpu(fbc->counters);
+	fbc->counters = NULL;
 }
 EXPORT_SYMBOL(percpu_counter_destroy);
 

commit 1f7c14c62ce63805f9574664a6c6de3633d4a354
Author: Mingming Cao <cmm@us.ibm.com>
Date:   Thu Oct 9 12:50:59 2008 -0400

    percpu counter: clean up percpu_counter_sum_and_set()
    
    percpu_counter_sum_and_set() and percpu_counter_sum() is the same except
    the former updates the global counter after accounting.  Since we are
    taking the fbc->lock to calculate the precise value of the counter in
    percpu_counter_sum() anyway, it should simply set fbc->count too, as the
    percpu_counter_sum_and_set() does.
    
    This patch merges these two interfaces into one.
    
    Signed-off-by: Mingming Cao <cmm@us.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: <linux-ext4@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 4a8ba4bf5f6f..a8663890a88c 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -52,7 +52,7 @@ EXPORT_SYMBOL(__percpu_counter_add);
  * Add up all the per-cpu counts, return the result.  This is a more accurate
  * but much slower version of percpu_counter_read_positive()
  */
-s64 __percpu_counter_sum(struct percpu_counter *fbc, int set)
+s64 __percpu_counter_sum(struct percpu_counter *fbc)
 {
 	s64 ret;
 	int cpu;
@@ -62,11 +62,9 @@ s64 __percpu_counter_sum(struct percpu_counter *fbc, int set)
 	for_each_online_cpu(cpu) {
 		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
 		ret += *pcount;
-		if (set)
-			*pcount = 0;
+		*pcount = 0;
 	}
-	if (set)
-		fbc->count = ret;
+	fbc->count = ret;
 
 	spin_unlock(&fbc->lock);
 	return ret;

commit e8ced39d5e8911c662d4d69a342b9d053eaaac4e
Author: Mingming Cao <cmm@us.ibm.com>
Date:   Fri Jul 11 19:27:31 2008 -0400

    percpu_counter: new function percpu_counter_sum_and_set
    
    Delayed allocation need to check free blocks at every write time.
    percpu_counter_read_positive() is not quit accurate. delayed
    allocation need a more accurate accounting, but using
    percpu_counter_sum_positive() is frequently is quite expensive.
    
    This patch added a new function to update center counter when sum
    per-cpu counter, to increase the accurate rate for next
    percpu_counter_read() and require less calling expensive
    percpu_counter_sum().
    
    Signed-off-by: Mingming Cao <cmm@us.ibm.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 119174494cb5..4a8ba4bf5f6f 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -52,7 +52,7 @@ EXPORT_SYMBOL(__percpu_counter_add);
  * Add up all the per-cpu counts, return the result.  This is a more accurate
  * but much slower version of percpu_counter_read_positive()
  */
-s64 __percpu_counter_sum(struct percpu_counter *fbc)
+s64 __percpu_counter_sum(struct percpu_counter *fbc, int set)
 {
 	s64 ret;
 	int cpu;
@@ -62,7 +62,12 @@ s64 __percpu_counter_sum(struct percpu_counter *fbc)
 	for_each_online_cpu(cpu) {
 		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
 		ret += *pcount;
+		if (set)
+			*pcount = 0;
 	}
+	if (set)
+		fbc->count = ret;
+
 	spin_unlock(&fbc->lock);
 	return ret;
 }

commit cf0ca9fe5dd9e3693d935757a7b2fc50fc576554
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Apr 30 00:54:32 2008 -0700

    mm: bdi: export BDI attributes in sysfs
    
    Provide a place in sysfs (/sys/class/bdi) for the backing_dev_info object.
    This allows us to see and set the various BDI specific variables.
    
    In particular this properly exposes the read-ahead window for all relevant
    users and /sys/block/<block>/queue/read_ahead_kb should be deprecated.
    
    With patient help from Kay Sievers and Greg KH
    
    [mszeredi@suse.cz]
    
     - split off NFS and FUSE changes into separate patches
     - document new sysfs attributes under Documentation/ABI
     - do bdi_class_init as a core_initcall, otherwise the "default" BDI
       won't be initialized
     - remove bdi_init_fmt macro, it's not used very much
    
    [akpm@linux-foundation.org: fix ia64 warning]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Acked-by: Greg KH <greg@kroah.com>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Signed-off-by: Miklos Szeredi <mszeredi@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 393a0e915c23..119174494cb5 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -102,6 +102,7 @@ void percpu_counter_destroy(struct percpu_counter *fbc)
 		return;
 
 	free_percpu(fbc->counters);
+	fbc->counters = NULL;
 #ifdef CONFIG_HOTPLUG_CPU
 	mutex_lock(&percpu_counters_lock);
 	list_del(&fbc->list);

commit d2b20b11547cefc89d6c81937e81afaf3c62808b
Author: Gautham R Shenoy <ego@in.ibm.com>
Date:   Thu Oct 18 23:40:47 2007 -0700

    Add irq protection in the percpu-counters cpu-hotplug-callback path
    
    Some of the per-cpu counters and thus their locks are accessed from IRQ
    contexts.  This can cause a deadlock if it interrupts a cpu-offline thread
    which is transferring a dead-cpu's counts to the global counter.
    
    Add appropriate IRQ protection in the cpu-hotplug callback path.
    
    Signed-off-by: Gautham R Shenoy <ego@in.ibm.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 9659eabffc31..393a0e915c23 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -124,12 +124,13 @@ static int __cpuinit percpu_counter_hotcpu_callback(struct notifier_block *nb,
 	mutex_lock(&percpu_counters_lock);
 	list_for_each_entry(fbc, &percpu_counters, list) {
 		s32 *pcount;
+		unsigned long flags;
 
-		spin_lock(&fbc->lock);
+		spin_lock_irqsave(&fbc->lock, flags);
 		pcount = per_cpu_ptr(fbc->counters, cpu);
 		fbc->count += *pcount;
 		*pcount = 0;
-		spin_unlock(&fbc->lock);
+		spin_unlock_irqrestore(&fbc->lock, flags);
 	}
 	mutex_unlock(&percpu_counters_lock);
 	return NOTIFY_OK;

commit dc62a30e274d003a4d08fb888f1520add4b21373
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:46 2007 -0700

    lib: percpu_counter_init_irq
    
    provide a way to tell lockdep about percpu_counters that are supposed to be
    used from irq safe contexts.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 3a59d84b2d1e..9659eabffc31 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -68,6 +68,8 @@ s64 __percpu_counter_sum(struct percpu_counter *fbc)
 }
 EXPORT_SYMBOL(__percpu_counter_sum);
 
+static struct lock_class_key percpu_counter_irqsafe;
+
 int percpu_counter_init(struct percpu_counter *fbc, s64 amount)
 {
 	spin_lock_init(&fbc->lock);
@@ -84,6 +86,16 @@ int percpu_counter_init(struct percpu_counter *fbc, s64 amount)
 }
 EXPORT_SYMBOL(percpu_counter_init);
 
+int percpu_counter_init_irq(struct percpu_counter *fbc, s64 amount)
+{
+	int err;
+
+	err = percpu_counter_init(fbc, amount);
+	if (!err)
+		lockdep_set_class(&fbc->lock, &percpu_counter_irqsafe);
+	return err;
+}
+
 void percpu_counter_destroy(struct percpu_counter *fbc)
 {
 	if (!fbc->counters)

commit 833f4077bf7c2dcff6e31526e03ec2ad91c88581
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:45 2007 -0700

    lib: percpu_counter_init error handling
    
    alloc_percpu can fail, propagate that error.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 3b0ed80c1efd..3a59d84b2d1e 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -68,21 +68,27 @@ s64 __percpu_counter_sum(struct percpu_counter *fbc)
 }
 EXPORT_SYMBOL(__percpu_counter_sum);
 
-void percpu_counter_init(struct percpu_counter *fbc, s64 amount)
+int percpu_counter_init(struct percpu_counter *fbc, s64 amount)
 {
 	spin_lock_init(&fbc->lock);
 	fbc->count = amount;
 	fbc->counters = alloc_percpu(s32);
+	if (!fbc->counters)
+		return -ENOMEM;
 #ifdef CONFIG_HOTPLUG_CPU
 	mutex_lock(&percpu_counters_lock);
 	list_add(&fbc->list, &percpu_counters);
 	mutex_unlock(&percpu_counters_lock);
 #endif
+	return 0;
 }
 EXPORT_SYMBOL(percpu_counter_init);
 
 void percpu_counter_destroy(struct percpu_counter *fbc)
 {
+	if (!fbc->counters)
+		return;
+
 	free_percpu(fbc->counters);
 #ifdef CONFIG_HOTPLUG_CPU
 	mutex_lock(&percpu_counters_lock);

commit bf1d89c81352f6eca72d0c10cfee3dba29ef5efb
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:45 2007 -0700

    lib: percpu_count_sum()
    
    Provide an accurate version of percpu_counter_read.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index b0d80ea22a33..3b0ed80c1efd 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -52,7 +52,7 @@ EXPORT_SYMBOL(__percpu_counter_add);
  * Add up all the per-cpu counts, return the result.  This is a more accurate
  * but much slower version of percpu_counter_read_positive()
  */
-s64 percpu_counter_sum_positive(struct percpu_counter *fbc)
+s64 __percpu_counter_sum(struct percpu_counter *fbc)
 {
 	s64 ret;
 	int cpu;
@@ -64,9 +64,9 @@ s64 percpu_counter_sum_positive(struct percpu_counter *fbc)
 		ret += *pcount;
 	}
 	spin_unlock(&fbc->lock);
-	return ret < 0 ? 0 : ret;
+	return ret;
 }
-EXPORT_SYMBOL(percpu_counter_sum_positive);
+EXPORT_SYMBOL(__percpu_counter_sum);
 
 void percpu_counter_init(struct percpu_counter *fbc, s64 amount)
 {

commit 52d9f3b4090922f34497ace82bd062d80a465a29
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:44 2007 -0700

    lib: percpu_counter_sum_positive
    
     s/percpu_counter_sum/&_positive/
    
    Because its consitent with percpu_counter_read*
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index c9708db9b8d3..b0d80ea22a33 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -52,7 +52,7 @@ EXPORT_SYMBOL(__percpu_counter_add);
  * Add up all the per-cpu counts, return the result.  This is a more accurate
  * but much slower version of percpu_counter_read_positive()
  */
-s64 percpu_counter_sum(struct percpu_counter *fbc)
+s64 percpu_counter_sum_positive(struct percpu_counter *fbc)
 {
 	s64 ret;
 	int cpu;
@@ -66,7 +66,7 @@ s64 percpu_counter_sum(struct percpu_counter *fbc)
 	spin_unlock(&fbc->lock);
 	return ret < 0 ? 0 : ret;
 }
-EXPORT_SYMBOL(percpu_counter_sum);
+EXPORT_SYMBOL(percpu_counter_sum_positive);
 
 void percpu_counter_init(struct percpu_counter *fbc, s64 amount)
 {

commit 3a587f47b82f96f19318c036e7b979fcd5c3848f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:44 2007 -0700

    lib: percpu_counter_set
    
    Provide a method to set a percpu counter to a specified value.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index f7ac68c4c375..c9708db9b8d3 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -14,6 +14,20 @@ static LIST_HEAD(percpu_counters);
 static DEFINE_MUTEX(percpu_counters_lock);
 #endif
 
+void percpu_counter_set(struct percpu_counter *fbc, s64 amount)
+{
+	int cpu;
+
+	spin_lock(&fbc->lock);
+	for_each_possible_cpu(cpu) {
+		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
+		*pcount = 0;
+	}
+	fbc->count = amount;
+	spin_unlock(&fbc->lock);
+}
+EXPORT_SYMBOL(percpu_counter_set);
+
 void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 {
 	s64 count;

commit 20e89767096392a2cb2404437d3d181b8827af38
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:43 2007 -0700

    lib: make percpu_counter_add take s64
    
    percpu_counter is a s64 counter, make _add consitent.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index f736d67c64d7..f7ac68c4c375 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -14,9 +14,9 @@ static LIST_HEAD(percpu_counters);
 static DEFINE_MUTEX(percpu_counters_lock);
 #endif
 
-void __percpu_counter_add(struct percpu_counter *fbc, s32 amount, s32 batch)
+void __percpu_counter_add(struct percpu_counter *fbc, s64 amount, s32 batch)
 {
-	long count;
+	s64 count;
 	s32 *pcount;
 	int cpu = get_cpu();
 

commit 252e0ba6b77dcfae448fa2fbaf796e8a83839e75
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:43 2007 -0700

    lib: percpu_counter variable batch
    
    Because the current batch setup has an quadric error bound on the counter,
    allow for an alternative setup.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 5f36ad79a24a..f736d67c64d7 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -14,7 +14,7 @@ static LIST_HEAD(percpu_counters);
 static DEFINE_MUTEX(percpu_counters_lock);
 #endif
 
-void percpu_counter_add(struct percpu_counter *fbc, s32 amount)
+void __percpu_counter_add(struct percpu_counter *fbc, s32 amount, s32 batch)
 {
 	long count;
 	s32 *pcount;
@@ -22,7 +22,7 @@ void percpu_counter_add(struct percpu_counter *fbc, s32 amount)
 
 	pcount = per_cpu_ptr(fbc->counters, cpu);
 	count = *pcount + amount;
-	if (count >= FBC_BATCH || count <= -FBC_BATCH) {
+	if (count >= batch || count <= -batch) {
 		spin_lock(&fbc->lock);
 		fbc->count += count;
 		*pcount = 0;
@@ -32,7 +32,7 @@ void percpu_counter_add(struct percpu_counter *fbc, s32 amount)
 	}
 	put_cpu();
 }
-EXPORT_SYMBOL(percpu_counter_add);
+EXPORT_SYMBOL(__percpu_counter_add);
 
 /*
  * Add up all the per-cpu counts, return the result.  This is a more accurate

commit aa0dff2d09bfa50b7d02714a45920c64568e699d
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Oct 16 23:25:42 2007 -0700

    lib: percpu_counter_add
    
     s/percpu_counter_mod/percpu_counter_add/
    
    Because its a better name, _mod implies modulo.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index cf22c617baa4..5f36ad79a24a 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -14,7 +14,7 @@ static LIST_HEAD(percpu_counters);
 static DEFINE_MUTEX(percpu_counters_lock);
 #endif
 
-void percpu_counter_mod(struct percpu_counter *fbc, s32 amount)
+void percpu_counter_add(struct percpu_counter *fbc, s32 amount)
 {
 	long count;
 	s32 *pcount;
@@ -32,7 +32,7 @@ void percpu_counter_mod(struct percpu_counter *fbc, s32 amount)
 	}
 	put_cpu();
 }
-EXPORT_SYMBOL(percpu_counter_mod);
+EXPORT_SYMBOL(percpu_counter_add);
 
 /*
  * Add up all the per-cpu counts, return the result.  This is a more accurate

commit b4ef0296f214a1e0e65f161f88663b0ca1acca31
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Sun Jul 15 23:39:51 2007 -0700

    percpu_counters: use for_each_online_cpu()
    
    Now that we have implemented hotunplug-time counter spilling,
    percpu_counter_sum() only needs to look at online CPUs.
    
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 8901c4e9c2e6..cf22c617baa4 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -45,7 +45,7 @@ s64 percpu_counter_sum(struct percpu_counter *fbc)
 
 	spin_lock(&fbc->lock);
 	ret = fbc->count;
-	for_each_possible_cpu(cpu) {
+	for_each_online_cpu(cpu) {
 		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
 		ret += *pcount;
 	}

commit c67ad917cbf21b2862e2cf8e8b28339872ef7927
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Sun Jul 15 23:39:51 2007 -0700

    percpu_counters(): use cpu notifiers
    
    per-cpu counters presently must iterate over all possible CPUs in the
    exhaustive percpu_counter_sum().
    
    But it can be much better to only iterate over the presently-online CPUs.  To
    do this, we must arrange for an offlined CPU's count to be spilled into the
    counter's central count.
    
    We can do this for all percpu_counters in the machine by linking them into a
    single global list and walking that list at CPU_DEAD time.
    
    (I hope.  Might have race windows in which the percpu_counter_sum() count is
    inaccurate?)
    
    Cc: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 850449080e1c..8901c4e9c2e6 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -3,8 +3,17 @@
  */
 
 #include <linux/percpu_counter.h>
+#include <linux/notifier.h>
+#include <linux/mutex.h>
+#include <linux/init.h>
+#include <linux/cpu.h>
 #include <linux/module.h>
 
+#ifdef CONFIG_HOTPLUG_CPU
+static LIST_HEAD(percpu_counters);
+static DEFINE_MUTEX(percpu_counters_lock);
+#endif
+
 void percpu_counter_mod(struct percpu_counter *fbc, s32 amount)
 {
 	long count;
@@ -44,3 +53,60 @@ s64 percpu_counter_sum(struct percpu_counter *fbc)
 	return ret < 0 ? 0 : ret;
 }
 EXPORT_SYMBOL(percpu_counter_sum);
+
+void percpu_counter_init(struct percpu_counter *fbc, s64 amount)
+{
+	spin_lock_init(&fbc->lock);
+	fbc->count = amount;
+	fbc->counters = alloc_percpu(s32);
+#ifdef CONFIG_HOTPLUG_CPU
+	mutex_lock(&percpu_counters_lock);
+	list_add(&fbc->list, &percpu_counters);
+	mutex_unlock(&percpu_counters_lock);
+#endif
+}
+EXPORT_SYMBOL(percpu_counter_init);
+
+void percpu_counter_destroy(struct percpu_counter *fbc)
+{
+	free_percpu(fbc->counters);
+#ifdef CONFIG_HOTPLUG_CPU
+	mutex_lock(&percpu_counters_lock);
+	list_del(&fbc->list);
+	mutex_unlock(&percpu_counters_lock);
+#endif
+}
+EXPORT_SYMBOL(percpu_counter_destroy);
+
+#ifdef CONFIG_HOTPLUG_CPU
+static int __cpuinit percpu_counter_hotcpu_callback(struct notifier_block *nb,
+					unsigned long action, void *hcpu)
+{
+	unsigned int cpu;
+	struct percpu_counter *fbc;
+
+	if (action != CPU_DEAD)
+		return NOTIFY_OK;
+
+	cpu = (unsigned long)hcpu;
+	mutex_lock(&percpu_counters_lock);
+	list_for_each_entry(fbc, &percpu_counters, list) {
+		s32 *pcount;
+
+		spin_lock(&fbc->lock);
+		pcount = per_cpu_ptr(fbc->counters, cpu);
+		fbc->count += *pcount;
+		*pcount = 0;
+		spin_unlock(&fbc->lock);
+	}
+	mutex_unlock(&percpu_counters_lock);
+	return NOTIFY_OK;
+}
+
+static int __init percpu_counter_startup(void)
+{
+	hotcpu_notifier(percpu_counter_hotcpu_callback, 0);
+	return 0;
+}
+module_init(percpu_counter_startup);
+#endif

commit 0216bfcffe424a5473daa4da47440881b36c1f41
Author: Mingming Cao <cmm@us.ibm.com>
Date:   Fri Jun 23 02:05:41 2006 -0700

    [PATCH] percpu counter data type changes to suppport more than 2**31 ext3 free blocks counter
    
    The percpu counter data type are changed in this set of patches to support
    more users like ext3 who need more than 32 bit to store the free blocks
    total in the filesystem.
    
    - Generic perpcu counters data type changes.  The size of the global counter
      and local counter were explictly specified using s64 and s32.  The global
      counter is changed from long to s64, while the local counter is changed from
      long to s32, so we could avoid doing 64 bit update in most cases.
    
    - Users of the percpu counters are updated to make use of the new
      percpu_counter_init() routine now taking an additional parameter to allow
      users to pass the initial value of the global counter.
    
    Signed-off-by: Mingming Cao <cmm@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
index 7a87003f8e8f..850449080e1c 100644
--- a/lib/percpu_counter.c
+++ b/lib/percpu_counter.c
@@ -5,10 +5,10 @@
 #include <linux/percpu_counter.h>
 #include <linux/module.h>
 
-void percpu_counter_mod(struct percpu_counter *fbc, long amount)
+void percpu_counter_mod(struct percpu_counter *fbc, s32 amount)
 {
 	long count;
-	long *pcount;
+	s32 *pcount;
 	int cpu = get_cpu();
 
 	pcount = per_cpu_ptr(fbc->counters, cpu);
@@ -29,15 +29,15 @@ EXPORT_SYMBOL(percpu_counter_mod);
  * Add up all the per-cpu counts, return the result.  This is a more accurate
  * but much slower version of percpu_counter_read_positive()
  */
-long percpu_counter_sum(struct percpu_counter *fbc)
+s64 percpu_counter_sum(struct percpu_counter *fbc)
 {
-	long ret;
+	s64 ret;
 	int cpu;
 
 	spin_lock(&fbc->lock);
 	ret = fbc->count;
 	for_each_possible_cpu(cpu) {
-		long *pcount = per_cpu_ptr(fbc->counters, cpu);
+		s32 *pcount = per_cpu_ptr(fbc->counters, cpu);
 		ret += *pcount;
 	}
 	spin_unlock(&fbc->lock);

commit 3cbc564024d8f174202f023e8a2991782f6a9431
Author: Ravikiran G Thirumalai <kiran@scalex86.org>
Date:   Fri Jun 23 02:05:40 2006 -0700

    [PATCH] percpu_counters: create lib/percpu_counter.c
    
    - Move percpu_counter routines from mm/swap.c to lib/percpu_counter.c
    
    Signed-off-by: Ravikiran Thirumalai <kiran@scalex86.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/lib/percpu_counter.c b/lib/percpu_counter.c
new file mode 100644
index 000000000000..7a87003f8e8f
--- /dev/null
+++ b/lib/percpu_counter.c
@@ -0,0 +1,46 @@
+/*
+ * Fast batching percpu counters.
+ */
+
+#include <linux/percpu_counter.h>
+#include <linux/module.h>
+
+void percpu_counter_mod(struct percpu_counter *fbc, long amount)
+{
+	long count;
+	long *pcount;
+	int cpu = get_cpu();
+
+	pcount = per_cpu_ptr(fbc->counters, cpu);
+	count = *pcount + amount;
+	if (count >= FBC_BATCH || count <= -FBC_BATCH) {
+		spin_lock(&fbc->lock);
+		fbc->count += count;
+		*pcount = 0;
+		spin_unlock(&fbc->lock);
+	} else {
+		*pcount = count;
+	}
+	put_cpu();
+}
+EXPORT_SYMBOL(percpu_counter_mod);
+
+/*
+ * Add up all the per-cpu counts, return the result.  This is a more accurate
+ * but much slower version of percpu_counter_read_positive()
+ */
+long percpu_counter_sum(struct percpu_counter *fbc)
+{
+	long ret;
+	int cpu;
+
+	spin_lock(&fbc->lock);
+	ret = fbc->count;
+	for_each_possible_cpu(cpu) {
+		long *pcount = per_cpu_ptr(fbc->counters, cpu);
+		ret += *pcount;
+	}
+	spin_unlock(&fbc->lock);
+	return ret < 0 ? 0 : ret;
+}
+EXPORT_SYMBOL(percpu_counter_sum);
