commit 7dfaa98f646bbc29c3575bc184f2b37aa2dd62c5
Author: Yury Norov <yury.norov@gmail.com>
Date:   Thu Jan 30 22:16:47 2020 -0800

    lib/find_bit.c: uninline helper _find_next_bit()
    
    It saves 25% of .text for arm64, and more for BE architectures.
    
    Before:
      $ size lib/find_bit.o
         text    data     bss     dec     hex filename
         1012      56       0    1068     42c lib/find_bit.o
    
    After:
      $ size lib/find_bit.o
         text    data     bss     dec     hex filename
          776      56       0     832     340 lib/find_bit.o
    
    Link: http://lkml.kernel.org/r/20200103202846.21616-3-yury.norov@gmail.com
    Signed-off-by: Yury Norov <yury.norov@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Allison Randal <allison@lohutok.net>
    Cc: William Breathitt Gray <vilhelm.gray@gmail.com>
    Cc: Joe Perches <joe@perches.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/find_bit.c b/lib/find_bit.c
index c897c127316d..49f875f1baf7 100644
--- a/lib/find_bit.c
+++ b/lib/find_bit.c
@@ -27,7 +27,7 @@
  *    searching it for one bits.
  *  - The optional "addr2", which is anded with "addr1" if present.
  */
-static inline unsigned long _find_next_bit(const unsigned long *addr1,
+static unsigned long _find_next_bit(const unsigned long *addr1,
 		const unsigned long *addr2, unsigned long nbits,
 		unsigned long start, unsigned long invert, unsigned long le)
 {

commit b78c57135d470cfc5b461f2abde4eaeb07d100fb
Author: Yury Norov <yury.norov@gmail.com>
Date:   Thu Jan 30 22:16:43 2020 -0800

    lib/find_bit.c: join _find_next_bit{_le}
    
    _find_next_bit and _find_next_bit_le are very similar functions.  It's
    possible to join them by adding 1 parameter and a couple of simple
    checks.  It's simplify maintenance and make possible to shrink the size
    of .text by un-inlining the unified function (in the following patch).
    
    Link: http://lkml.kernel.org/r/20200103202846.21616-2-yury.norov@gmail.com
    Signed-off-by: Yury Norov <yury.norov@gmail.com>
    Cc: Allison Randal <allison@lohutok.net>
    Cc: Joe Perches <joe@perches.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: William Breathitt Gray <vilhelm.gray@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/find_bit.c b/lib/find_bit.c
index 06e503c339f3..c897c127316d 100644
--- a/lib/find_bit.c
+++ b/lib/find_bit.c
@@ -17,9 +17,9 @@
 #include <linux/export.h>
 #include <linux/kernel.h>
 
-#if !defined(find_next_bit) || !defined(find_next_zero_bit) || \
-		!defined(find_next_and_bit)
-
+#if !defined(find_next_bit) || !defined(find_next_zero_bit) ||			\
+	!defined(find_next_bit_le) || !defined(find_next_zero_bit_le) ||	\
+	!defined(find_next_and_bit)
 /*
  * This is a common helper function for find_next_bit, find_next_zero_bit, and
  * find_next_and_bit. The differences are:
@@ -29,9 +29,9 @@
  */
 static inline unsigned long _find_next_bit(const unsigned long *addr1,
 		const unsigned long *addr2, unsigned long nbits,
-		unsigned long start, unsigned long invert)
+		unsigned long start, unsigned long invert, unsigned long le)
 {
-	unsigned long tmp;
+	unsigned long tmp, mask;
 
 	if (unlikely(start >= nbits))
 		return nbits;
@@ -42,7 +42,12 @@ static inline unsigned long _find_next_bit(const unsigned long *addr1,
 	tmp ^= invert;
 
 	/* Handle 1st word. */
-	tmp &= BITMAP_FIRST_WORD_MASK(start);
+	mask = BITMAP_FIRST_WORD_MASK(start);
+	if (le)
+		mask = swab(mask);
+
+	tmp &= mask;
+
 	start = round_down(start, BITS_PER_LONG);
 
 	while (!tmp) {
@@ -56,6 +61,9 @@ static inline unsigned long _find_next_bit(const unsigned long *addr1,
 		tmp ^= invert;
 	}
 
+	if (le)
+		tmp = swab(tmp);
+
 	return min(start + __ffs(tmp), nbits);
 }
 #endif
@@ -67,7 +75,7 @@ static inline unsigned long _find_next_bit(const unsigned long *addr1,
 unsigned long find_next_bit(const unsigned long *addr, unsigned long size,
 			    unsigned long offset)
 {
-	return _find_next_bit(addr, NULL, size, offset, 0UL);
+	return _find_next_bit(addr, NULL, size, offset, 0UL, 0);
 }
 EXPORT_SYMBOL(find_next_bit);
 #endif
@@ -76,7 +84,7 @@ EXPORT_SYMBOL(find_next_bit);
 unsigned long find_next_zero_bit(const unsigned long *addr, unsigned long size,
 				 unsigned long offset)
 {
-	return _find_next_bit(addr, NULL, size, offset, ~0UL);
+	return _find_next_bit(addr, NULL, size, offset, ~0UL, 0);
 }
 EXPORT_SYMBOL(find_next_zero_bit);
 #endif
@@ -86,7 +94,7 @@ unsigned long find_next_and_bit(const unsigned long *addr1,
 		const unsigned long *addr2, unsigned long size,
 		unsigned long offset)
 {
-	return _find_next_bit(addr1, addr2, size, offset, 0UL);
+	return _find_next_bit(addr1, addr2, size, offset, 0UL, 0);
 }
 EXPORT_SYMBOL(find_next_and_bit);
 #endif
@@ -149,45 +157,11 @@ EXPORT_SYMBOL(find_last_bit);
 
 #ifdef __BIG_ENDIAN
 
-#if !defined(find_next_bit_le) || !defined(find_next_zero_bit_le)
-static inline unsigned long _find_next_bit_le(const unsigned long *addr1,
-		const unsigned long *addr2, unsigned long nbits,
-		unsigned long start, unsigned long invert)
-{
-	unsigned long tmp;
-
-	if (unlikely(start >= nbits))
-		return nbits;
-
-	tmp = addr1[start / BITS_PER_LONG];
-	if (addr2)
-		tmp &= addr2[start / BITS_PER_LONG];
-	tmp ^= invert;
-
-	/* Handle 1st word. */
-	tmp &= swab(BITMAP_FIRST_WORD_MASK(start));
-	start = round_down(start, BITS_PER_LONG);
-
-	while (!tmp) {
-		start += BITS_PER_LONG;
-		if (start >= nbits)
-			return nbits;
-
-		tmp = addr1[start / BITS_PER_LONG];
-		if (addr2)
-			tmp &= addr2[start / BITS_PER_LONG];
-		tmp ^= invert;
-	}
-
-	return min(start + __ffs(swab(tmp)), nbits);
-}
-#endif
-
 #ifndef find_next_zero_bit_le
 unsigned long find_next_zero_bit_le(const void *addr, unsigned
 		long size, unsigned long offset)
 {
-	return _find_next_bit_le(addr, NULL, size, offset, ~0UL);
+	return _find_next_bit(addr, NULL, size, offset, ~0UL, 1);
 }
 EXPORT_SYMBOL(find_next_zero_bit_le);
 #endif
@@ -196,7 +170,7 @@ EXPORT_SYMBOL(find_next_zero_bit_le);
 unsigned long find_next_bit_le(const void *addr, unsigned
 		long size, unsigned long offset)
 {
-	return _find_next_bit_le(addr, NULL, size, offset, 0UL);
+	return _find_next_bit(addr, NULL, size, offset, 0UL, 1);
 }
 EXPORT_SYMBOL(find_next_bit_le);
 #endif

commit d5767057c9a76a29f073dad66b7fa12a90e8c748
Author: Yury Norov <yury.norov@gmail.com>
Date:   Thu Jan 30 22:16:40 2020 -0800

    uapi: rename ext2_swab() to swab() and share globally in swab.h
    
    ext2_swab() is defined locally in lib/find_bit.c However it is not
    specific to ext2, neither to bitmaps.
    
    There are many potential users of it, so rename it to just swab() and
    move to include/uapi/linux/swab.h
    
    ABI guarantees that size of unsigned long corresponds to BITS_PER_LONG,
    therefore drop unneeded cast.
    
    Link: http://lkml.kernel.org/r/20200103202846.21616-1-yury.norov@gmail.com
    Signed-off-by: Yury Norov <yury.norov@gmail.com>
    Cc: Allison Randal <allison@lohutok.net>
    Cc: Joe Perches <joe@perches.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: William Breathitt Gray <vilhelm.gray@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/find_bit.c b/lib/find_bit.c
index e35a76b291e6..06e503c339f3 100644
--- a/lib/find_bit.c
+++ b/lib/find_bit.c
@@ -149,18 +149,6 @@ EXPORT_SYMBOL(find_last_bit);
 
 #ifdef __BIG_ENDIAN
 
-/* include/linux/byteorder does not support "unsigned long" type */
-static inline unsigned long ext2_swab(const unsigned long y)
-{
-#if BITS_PER_LONG == 64
-	return (unsigned long) __swab64((u64) y);
-#elif BITS_PER_LONG == 32
-	return (unsigned long) __swab32((u32) y);
-#else
-#error BITS_PER_LONG not defined
-#endif
-}
-
 #if !defined(find_next_bit_le) || !defined(find_next_zero_bit_le)
 static inline unsigned long _find_next_bit_le(const unsigned long *addr1,
 		const unsigned long *addr2, unsigned long nbits,
@@ -177,7 +165,7 @@ static inline unsigned long _find_next_bit_le(const unsigned long *addr1,
 	tmp ^= invert;
 
 	/* Handle 1st word. */
-	tmp &= ext2_swab(BITMAP_FIRST_WORD_MASK(start));
+	tmp &= swab(BITMAP_FIRST_WORD_MASK(start));
 	start = round_down(start, BITS_PER_LONG);
 
 	while (!tmp) {
@@ -191,7 +179,7 @@ static inline unsigned long _find_next_bit_le(const unsigned long *addr1,
 		tmp ^= invert;
 	}
 
-	return min(start + __ffs(ext2_swab(tmp)), nbits);
+	return min(start + __ffs(swab(tmp)), nbits);
 }
 #endif
 

commit 169c474fb22d8a5e909e172f177b957546d0519d
Author: William Breathitt Gray <vilhelm.gray@gmail.com>
Date:   Wed Dec 4 16:50:57 2019 -0800

    bitops: introduce the for_each_set_clump8 macro
    
    Pach series "Introduce the for_each_set_clump8 macro", v18.
    
    While adding GPIO get_multiple/set_multiple callback support for various
    drivers, I noticed a pattern of looping manifesting that would be useful
    standardized as a macro.
    
    This patchset introduces the for_each_set_clump8 macro and utilizes it
    in several GPIO drivers.  The for_each_set_clump macro8 facilitates a
    for-loop syntax that iterates over a memory region entire groups of set
    bits at a time.
    
    For example, suppose you would like to iterate over a 32-bit integer 8
    bits at a time, skipping over 8-bit groups with no set bit, where
    XXXXXXXX represents the current 8-bit group:
    
        Example:        10111110 00000000 11111111 00110011
        First loop:     10111110 00000000 11111111 XXXXXXXX
        Second loop:    10111110 00000000 XXXXXXXX 00110011
        Third loop:     XXXXXXXX 00000000 11111111 00110011
    
    Each iteration of the loop returns the next 8-bit group that has at
    least one set bit.
    
    The for_each_set_clump8 macro has four parameters:
    
        * start: set to the bit offset of the current clump
        * clump: set to the current clump value
        * bits: bitmap to search within
        * size: bitmap size in number of bits
    
    In this version of the patchset, the for_each_set_clump macro has been
    reimplemented and simplified based on the suggestions provided by Rasmus
    Villemoes and Andy Shevchenko in the version 4 submission.
    
    In particular, the function of the for_each_set_clump macro has been
    restricted to handle only 8-bit clumps; the drivers that use the
    for_each_set_clump macro only handle 8-bit ports so a generic
    for_each_set_clump implementation is not necessary.  Thus, a solution
    for large clumps (i.e.  those larger than the width of a bitmap word)
    can be postponed until a driver appears that actually requires such a
    generic for_each_set_clump implementation.
    
    For what it's worth, a semi-generic for_each_set_clump (i.e.  for clumps
    smaller than the width of a bitmap word) can be implemented by simply
    replacing the hardcoded '8' and '0xFF' instances with respective
    variables.  I have not yet had a need for such an implementation, and
    since it falls short of a true generic for_each_set_clump function, I
    have decided to forgo such an implementation for now.
    
    In addition, the bitmap_get_value8 and bitmap_set_value8 functions are
    introduced to get and set 8-bit values respectively.  Their use is based
    on the behavior suggested in the patchset version 4 review.
    
    This patch (of 14):
    
    This macro iterates for each 8-bit group of bits (clump) with set bits,
    within a bitmap memory region.  For each iteration, "start" is set to
    the bit offset of the found clump, while the respective clump value is
    stored to the location pointed by "clump".  Additionally, the
    bitmap_get_value8 and bitmap_set_value8 functions are introduced to
    respectively get and set an 8-bit value in a bitmap memory region.
    
    [gustavo@embeddedor.com: fix potential sign-extension overflow]
      Link: http://lkml.kernel.org/r/20191015184657.GA26541@embeddedor
    [akpm@linux-foundation.org: s/ULL/UL/, per Joe]
    [vilhelm.gray@gmail.com: add for_each_set_clump8 documentation]
      Link: http://lkml.kernel.org/r/20191016161825.301082-1-vilhelm.gray@gmail.com
    Link: http://lkml.kernel.org/r/893c3b4f03266c9496137cc98ac2b1bd27f92c73.1570641097.git.vilhelm.gray@gmail.com
    Signed-off-by: William Breathitt Gray <vilhelm.gray@gmail.com>
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>
    Suggested-by: Andy Shevchenko <andy.shevchenko@gmail.com>
    Suggested-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Suggested-by: Lukas Wunner <lukas@wunner.de>
    Tested-by: Andy Shevchenko <andriy.shevchenko@linux.intel.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Cc: Bartosz Golaszewski <bgolaszewski@baylibre.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Phil Reid <preid@electromag.com.au>
    Cc: Geert Uytterhoeven <geert+renesas@glider.be>
    Cc: Mathias Duckeck <m.duckeck@kunbus.de>
    Cc: Morten Hein Tiljeset <morten.tiljeset@prevas.dk>
    Cc: Sean Nyekjaer <sean.nyekjaer@prevas.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/find_bit.c b/lib/find_bit.c
index 5c51eb45178a..e35a76b291e6 100644
--- a/lib/find_bit.c
+++ b/lib/find_bit.c
@@ -214,3 +214,17 @@ EXPORT_SYMBOL(find_next_bit_le);
 #endif
 
 #endif /* __BIG_ENDIAN */
+
+unsigned long find_next_clump8(unsigned long *clump, const unsigned long *addr,
+			       unsigned long size, unsigned long offset)
+{
+	offset = find_next_bit(addr, size, offset);
+	if (offset == size)
+		return size;
+
+	offset = round_down(offset, 8);
+	*clump = bitmap_get_value8(addr, offset);
+
+	return offset;
+}
+EXPORT_SYMBOL(find_next_clump8);

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/lib/find_bit.c b/lib/find_bit.c
index ee3df93ba69a..5c51eb45178a 100644
--- a/lib/find_bit.c
+++ b/lib/find_bit.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /* bit search implementation
  *
  * Copyright (C) 2004 Red Hat, Inc. All Rights Reserved.
@@ -9,11 +10,6 @@
  *
  * Rewritten by Yury Norov <yury.norov@gmail.com> to decrease
  * size and improve performance, 2015.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #include <linux/bitops.h>

commit 0ade34c37012ea5c516d9aa4d19a56e9f40a55ed
Author: Clement Courbet <courbet@google.com>
Date:   Tue Feb 6 15:38:34 2018 -0800

    lib: optimize cpumask_next_and()
    
    We've measured that we spend ~0.6% of sys cpu time in cpumask_next_and().
    It's essentially a joined iteration in search for a non-zero bit, which is
    currently implemented as a lookup join (find a nonzero bit on the lhs,
    lookup the rhs to see if it's set there).
    
    Implement a direct join (find a nonzero bit on the incrementally built
    join).  Also add generic bitmap benchmarks in the new `test_find_bit`
    module for new function (see `find_next_and_bit` in [2] and [3] below).
    
    For cpumask_next_and, direct benchmarking shows that it's 1.17x to 14x
    faster with a geometric mean of 2.1 on 32 CPUs [1].  No impact on memory
    usage.  Note that on Arm, the new pure-C implementation still outperforms
    the old one that uses a mix of C and asm (`find_next_bit`) [3].
    
    [1] Approximate benchmark code:
    
    ```
      unsigned long src1p[nr_cpumask_longs] = {pattern1};
      unsigned long src2p[nr_cpumask_longs] = {pattern2};
      for (/*a bunch of repetitions*/) {
        for (int n = -1; n <= nr_cpu_ids; ++n) {
          asm volatile("" : "+rm"(src1p)); // prevent any optimization
          asm volatile("" : "+rm"(src2p));
          unsigned long result = cpumask_next_and(n, src1p, src2p);
          asm volatile("" : "+rm"(result));
        }
      }
    ```
    
    Results:
    pattern1    pattern2     time_before/time_after
    0x0000ffff  0x0000ffff   1.65
    0x0000ffff  0x00005555   2.24
    0x0000ffff  0x00001111   2.94
    0x0000ffff  0x00000000   14.0
    0x00005555  0x0000ffff   1.67
    0x00005555  0x00005555   1.71
    0x00005555  0x00001111   1.90
    0x00005555  0x00000000   6.58
    0x00001111  0x0000ffff   1.46
    0x00001111  0x00005555   1.49
    0x00001111  0x00001111   1.45
    0x00001111  0x00000000   3.10
    0x00000000  0x0000ffff   1.18
    0x00000000  0x00005555   1.18
    0x00000000  0x00001111   1.17
    0x00000000  0x00000000   1.25
    -----------------------------
                   geo.mean  2.06
    
    [2] test_find_next_bit, X86 (skylake)
    
     [ 3913.477422] Start testing find_bit() with random-filled bitmap
     [ 3913.477847] find_next_bit: 160868 cycles, 16484 iterations
     [ 3913.477933] find_next_zero_bit: 169542 cycles, 16285 iterations
     [ 3913.478036] find_last_bit: 201638 cycles, 16483 iterations
     [ 3913.480214] find_first_bit: 4353244 cycles, 16484 iterations
     [ 3913.480216] Start testing find_next_and_bit() with random-filled
     bitmap
     [ 3913.481074] find_next_and_bit: 89604 cycles, 8216 iterations
     [ 3913.481075] Start testing find_bit() with sparse bitmap
     [ 3913.481078] find_next_bit: 2536 cycles, 66 iterations
     [ 3913.481252] find_next_zero_bit: 344404 cycles, 32703 iterations
     [ 3913.481255] find_last_bit: 2006 cycles, 66 iterations
     [ 3913.481265] find_first_bit: 17488 cycles, 66 iterations
     [ 3913.481266] Start testing find_next_and_bit() with sparse bitmap
     [ 3913.481272] find_next_and_bit: 764 cycles, 1 iterations
    
    [3] test_find_next_bit, arm (v7 odroid XU3).
    
    [  267.206928] Start testing find_bit() with random-filled bitmap
    [  267.214752] find_next_bit: 4474 cycles, 16419 iterations
    [  267.221850] find_next_zero_bit: 5976 cycles, 16350 iterations
    [  267.229294] find_last_bit: 4209 cycles, 16419 iterations
    [  267.279131] find_first_bit: 1032991 cycles, 16420 iterations
    [  267.286265] Start testing find_next_and_bit() with random-filled
    bitmap
    [  267.302386] find_next_and_bit: 2290 cycles, 8140 iterations
    [  267.309422] Start testing find_bit() with sparse bitmap
    [  267.316054] find_next_bit: 191 cycles, 66 iterations
    [  267.322726] find_next_zero_bit: 8758 cycles, 32703 iterations
    [  267.329803] find_last_bit: 84 cycles, 66 iterations
    [  267.336169] find_first_bit: 4118 cycles, 66 iterations
    [  267.342627] Start testing find_next_and_bit() with sparse bitmap
    [  267.356919] find_next_and_bit: 91 cycles, 1 iterations
    
    [courbet@google.com: v6]
      Link: http://lkml.kernel.org/r/20171129095715.23430-1-courbet@google.com
    [geert@linux-m68k.org: m68k/bitops: always include <asm-generic/bitops/find.h>]
      Link: http://lkml.kernel.org/r/1512556816-28627-1-git-send-email-geert@linux-m68k.org
    Link: http://lkml.kernel.org/r/20171128131334.23491-1-courbet@google.com
    Signed-off-by: Clement Courbet <courbet@google.com>
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Yury Norov <ynorov@caviumnetworks.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/find_bit.c b/lib/find_bit.c
index 6ed74f78380c..ee3df93ba69a 100644
--- a/lib/find_bit.c
+++ b/lib/find_bit.c
@@ -21,22 +21,29 @@
 #include <linux/export.h>
 #include <linux/kernel.h>
 
-#if !defined(find_next_bit) || !defined(find_next_zero_bit)
+#if !defined(find_next_bit) || !defined(find_next_zero_bit) || \
+		!defined(find_next_and_bit)
 
 /*
- * This is a common helper function for find_next_bit and
- * find_next_zero_bit.  The difference is the "invert" argument, which
- * is XORed with each fetched word before searching it for one bits.
+ * This is a common helper function for find_next_bit, find_next_zero_bit, and
+ * find_next_and_bit. The differences are:
+ *  - The "invert" argument, which is XORed with each fetched word before
+ *    searching it for one bits.
+ *  - The optional "addr2", which is anded with "addr1" if present.
  */
-static unsigned long _find_next_bit(const unsigned long *addr,
-		unsigned long nbits, unsigned long start, unsigned long invert)
+static inline unsigned long _find_next_bit(const unsigned long *addr1,
+		const unsigned long *addr2, unsigned long nbits,
+		unsigned long start, unsigned long invert)
 {
 	unsigned long tmp;
 
 	if (unlikely(start >= nbits))
 		return nbits;
 
-	tmp = addr[start / BITS_PER_LONG] ^ invert;
+	tmp = addr1[start / BITS_PER_LONG];
+	if (addr2)
+		tmp &= addr2[start / BITS_PER_LONG];
+	tmp ^= invert;
 
 	/* Handle 1st word. */
 	tmp &= BITMAP_FIRST_WORD_MASK(start);
@@ -47,7 +54,10 @@ static unsigned long _find_next_bit(const unsigned long *addr,
 		if (start >= nbits)
 			return nbits;
 
-		tmp = addr[start / BITS_PER_LONG] ^ invert;
+		tmp = addr1[start / BITS_PER_LONG];
+		if (addr2)
+			tmp &= addr2[start / BITS_PER_LONG];
+		tmp ^= invert;
 	}
 
 	return min(start + __ffs(tmp), nbits);
@@ -61,7 +71,7 @@ static unsigned long _find_next_bit(const unsigned long *addr,
 unsigned long find_next_bit(const unsigned long *addr, unsigned long size,
 			    unsigned long offset)
 {
-	return _find_next_bit(addr, size, offset, 0UL);
+	return _find_next_bit(addr, NULL, size, offset, 0UL);
 }
 EXPORT_SYMBOL(find_next_bit);
 #endif
@@ -70,11 +80,21 @@ EXPORT_SYMBOL(find_next_bit);
 unsigned long find_next_zero_bit(const unsigned long *addr, unsigned long size,
 				 unsigned long offset)
 {
-	return _find_next_bit(addr, size, offset, ~0UL);
+	return _find_next_bit(addr, NULL, size, offset, ~0UL);
 }
 EXPORT_SYMBOL(find_next_zero_bit);
 #endif
 
+#if !defined(find_next_and_bit)
+unsigned long find_next_and_bit(const unsigned long *addr1,
+		const unsigned long *addr2, unsigned long size,
+		unsigned long offset)
+{
+	return _find_next_bit(addr1, addr2, size, offset, 0UL);
+}
+EXPORT_SYMBOL(find_next_and_bit);
+#endif
+
 #ifndef find_first_bit
 /*
  * Find the first set bit in a memory region.
@@ -146,15 +166,19 @@ static inline unsigned long ext2_swab(const unsigned long y)
 }
 
 #if !defined(find_next_bit_le) || !defined(find_next_zero_bit_le)
-static unsigned long _find_next_bit_le(const unsigned long *addr,
-		unsigned long nbits, unsigned long start, unsigned long invert)
+static inline unsigned long _find_next_bit_le(const unsigned long *addr1,
+		const unsigned long *addr2, unsigned long nbits,
+		unsigned long start, unsigned long invert)
 {
 	unsigned long tmp;
 
 	if (unlikely(start >= nbits))
 		return nbits;
 
-	tmp = addr[start / BITS_PER_LONG] ^ invert;
+	tmp = addr1[start / BITS_PER_LONG];
+	if (addr2)
+		tmp &= addr2[start / BITS_PER_LONG];
+	tmp ^= invert;
 
 	/* Handle 1st word. */
 	tmp &= ext2_swab(BITMAP_FIRST_WORD_MASK(start));
@@ -165,7 +189,10 @@ static unsigned long _find_next_bit_le(const unsigned long *addr,
 		if (start >= nbits)
 			return nbits;
 
-		tmp = addr[start / BITS_PER_LONG] ^ invert;
+		tmp = addr1[start / BITS_PER_LONG];
+		if (addr2)
+			tmp &= addr2[start / BITS_PER_LONG];
+		tmp ^= invert;
 	}
 
 	return min(start + __ffs(ext2_swab(tmp)), nbits);
@@ -176,7 +203,7 @@ static unsigned long _find_next_bit_le(const unsigned long *addr,
 unsigned long find_next_zero_bit_le(const void *addr, unsigned
 		long size, unsigned long offset)
 {
-	return _find_next_bit_le(addr, size, offset, ~0UL);
+	return _find_next_bit_le(addr, NULL, size, offset, ~0UL);
 }
 EXPORT_SYMBOL(find_next_zero_bit_le);
 #endif
@@ -185,7 +212,7 @@ EXPORT_SYMBOL(find_next_zero_bit_le);
 unsigned long find_next_bit_le(const void *addr, unsigned
 		long size, unsigned long offset)
 {
-	return _find_next_bit_le(addr, size, offset, 0UL);
+	return _find_next_bit_le(addr, NULL, size, offset, 0UL);
 }
 EXPORT_SYMBOL(find_next_bit_le);
 #endif

commit e4afd2e5567fc5d59988025f7528f9b4794d86a5
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Fri Feb 24 15:00:58 2017 -0800

    lib/find_bit.c: micro-optimise find_next_*_bit
    
    This saves 32 bytes on my x86-64 build, mostly due to alignment
    considerations and sharing more code between find_next_bit and
    find_next_zero_bit, but it does save a couple of instructions.
    
    There's really two parts to this commit:
     - First, the first half of the test: (!nbits || start >= nbits) is
       trivially a subset of the second half, since nbits and start are both
       unsigned
     - Second, while looking at the disassembly, I noticed that GCC was
       predicting the branch taken. Since this is a failure case, it's
       clearly the less likely of the two branches, so add an unlikely() to
       override GCC's heuristics.
    
    [mawilcox@microsoft.com: v2]
      Link: http://lkml.kernel.org/r/1483709016-1834-1-git-send-email-mawilcox@linuxonhyperv.com
    Link: http://lkml.kernel.org/r/1483709016-1834-1-git-send-email-mawilcox@linuxonhyperv.com
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Acked-by: Yury Norov <ynorov@caviumnetworks.com>
    Acked-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/find_bit.c b/lib/find_bit.c
index 18072ea9c20e..6ed74f78380c 100644
--- a/lib/find_bit.c
+++ b/lib/find_bit.c
@@ -33,7 +33,7 @@ static unsigned long _find_next_bit(const unsigned long *addr,
 {
 	unsigned long tmp;
 
-	if (!nbits || start >= nbits)
+	if (unlikely(start >= nbits))
 		return nbits;
 
 	tmp = addr[start / BITS_PER_LONG] ^ invert;
@@ -151,7 +151,7 @@ static unsigned long _find_next_bit_le(const unsigned long *addr,
 {
 	unsigned long tmp;
 
-	if (!nbits || start >= nbits)
+	if (unlikely(start >= nbits))
 		return nbits;
 
 	tmp = addr[start / BITS_PER_LONG] ^ invert;

commit 840620a1596a90636a44d6a593db4041bb28d52e
Author: Yury Norov <yury.norov@gmail.com>
Date:   Thu Apr 16 12:43:19 2015 -0700

    lib: rename lib/find_next_bit.c to lib/find_bit.c
    
    This file contains implementation for all find_*_bit{,_le}
    So giving it more generic name looks reasonable.
    
    Signed-off-by: Yury Norov <yury.norov@gmail.com>
    Reviewed-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Reviewed-by: George Spelvin <linux@horizon.com>
    Cc: Alexey Klimov <klimov.linux@gmail.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Daniel Borkmann <dborkman@redhat.com>
    Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Thomas Graf <tgraf@suug.ch>
    Cc: Valentin Rothberg <valentinrothberg@gmail.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/lib/find_bit.c b/lib/find_bit.c
new file mode 100644
index 000000000000..18072ea9c20e
--- /dev/null
+++ b/lib/find_bit.c
@@ -0,0 +1,193 @@
+/* bit search implementation
+ *
+ * Copyright (C) 2004 Red Hat, Inc. All Rights Reserved.
+ * Written by David Howells (dhowells@redhat.com)
+ *
+ * Copyright (C) 2008 IBM Corporation
+ * 'find_last_bit' is written by Rusty Russell <rusty@rustcorp.com.au>
+ * (Inspired by David Howell's find_next_bit implementation)
+ *
+ * Rewritten by Yury Norov <yury.norov@gmail.com> to decrease
+ * size and improve performance, 2015.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/bitops.h>
+#include <linux/bitmap.h>
+#include <linux/export.h>
+#include <linux/kernel.h>
+
+#if !defined(find_next_bit) || !defined(find_next_zero_bit)
+
+/*
+ * This is a common helper function for find_next_bit and
+ * find_next_zero_bit.  The difference is the "invert" argument, which
+ * is XORed with each fetched word before searching it for one bits.
+ */
+static unsigned long _find_next_bit(const unsigned long *addr,
+		unsigned long nbits, unsigned long start, unsigned long invert)
+{
+	unsigned long tmp;
+
+	if (!nbits || start >= nbits)
+		return nbits;
+
+	tmp = addr[start / BITS_PER_LONG] ^ invert;
+
+	/* Handle 1st word. */
+	tmp &= BITMAP_FIRST_WORD_MASK(start);
+	start = round_down(start, BITS_PER_LONG);
+
+	while (!tmp) {
+		start += BITS_PER_LONG;
+		if (start >= nbits)
+			return nbits;
+
+		tmp = addr[start / BITS_PER_LONG] ^ invert;
+	}
+
+	return min(start + __ffs(tmp), nbits);
+}
+#endif
+
+#ifndef find_next_bit
+/*
+ * Find the next set bit in a memory region.
+ */
+unsigned long find_next_bit(const unsigned long *addr, unsigned long size,
+			    unsigned long offset)
+{
+	return _find_next_bit(addr, size, offset, 0UL);
+}
+EXPORT_SYMBOL(find_next_bit);
+#endif
+
+#ifndef find_next_zero_bit
+unsigned long find_next_zero_bit(const unsigned long *addr, unsigned long size,
+				 unsigned long offset)
+{
+	return _find_next_bit(addr, size, offset, ~0UL);
+}
+EXPORT_SYMBOL(find_next_zero_bit);
+#endif
+
+#ifndef find_first_bit
+/*
+ * Find the first set bit in a memory region.
+ */
+unsigned long find_first_bit(const unsigned long *addr, unsigned long size)
+{
+	unsigned long idx;
+
+	for (idx = 0; idx * BITS_PER_LONG < size; idx++) {
+		if (addr[idx])
+			return min(idx * BITS_PER_LONG + __ffs(addr[idx]), size);
+	}
+
+	return size;
+}
+EXPORT_SYMBOL(find_first_bit);
+#endif
+
+#ifndef find_first_zero_bit
+/*
+ * Find the first cleared bit in a memory region.
+ */
+unsigned long find_first_zero_bit(const unsigned long *addr, unsigned long size)
+{
+	unsigned long idx;
+
+	for (idx = 0; idx * BITS_PER_LONG < size; idx++) {
+		if (addr[idx] != ~0UL)
+			return min(idx * BITS_PER_LONG + ffz(addr[idx]), size);
+	}
+
+	return size;
+}
+EXPORT_SYMBOL(find_first_zero_bit);
+#endif
+
+#ifndef find_last_bit
+unsigned long find_last_bit(const unsigned long *addr, unsigned long size)
+{
+	if (size) {
+		unsigned long val = BITMAP_LAST_WORD_MASK(size);
+		unsigned long idx = (size-1) / BITS_PER_LONG;
+
+		do {
+			val &= addr[idx];
+			if (val)
+				return idx * BITS_PER_LONG + __fls(val);
+
+			val = ~0ul;
+		} while (idx--);
+	}
+	return size;
+}
+EXPORT_SYMBOL(find_last_bit);
+#endif
+
+#ifdef __BIG_ENDIAN
+
+/* include/linux/byteorder does not support "unsigned long" type */
+static inline unsigned long ext2_swab(const unsigned long y)
+{
+#if BITS_PER_LONG == 64
+	return (unsigned long) __swab64((u64) y);
+#elif BITS_PER_LONG == 32
+	return (unsigned long) __swab32((u32) y);
+#else
+#error BITS_PER_LONG not defined
+#endif
+}
+
+#if !defined(find_next_bit_le) || !defined(find_next_zero_bit_le)
+static unsigned long _find_next_bit_le(const unsigned long *addr,
+		unsigned long nbits, unsigned long start, unsigned long invert)
+{
+	unsigned long tmp;
+
+	if (!nbits || start >= nbits)
+		return nbits;
+
+	tmp = addr[start / BITS_PER_LONG] ^ invert;
+
+	/* Handle 1st word. */
+	tmp &= ext2_swab(BITMAP_FIRST_WORD_MASK(start));
+	start = round_down(start, BITS_PER_LONG);
+
+	while (!tmp) {
+		start += BITS_PER_LONG;
+		if (start >= nbits)
+			return nbits;
+
+		tmp = addr[start / BITS_PER_LONG] ^ invert;
+	}
+
+	return min(start + __ffs(ext2_swab(tmp)), nbits);
+}
+#endif
+
+#ifndef find_next_zero_bit_le
+unsigned long find_next_zero_bit_le(const void *addr, unsigned
+		long size, unsigned long offset)
+{
+	return _find_next_bit_le(addr, size, offset, ~0UL);
+}
+EXPORT_SYMBOL(find_next_zero_bit_le);
+#endif
+
+#ifndef find_next_bit_le
+unsigned long find_next_bit_le(const void *addr, unsigned
+		long size, unsigned long offset)
+{
+	return _find_next_bit_le(addr, size, offset, 0UL);
+}
+EXPORT_SYMBOL(find_next_bit_le);
+#endif
+
+#endif /* __BIG_ENDIAN */
