commit d6f9469a03d832dcd17041ed67774ffb5f3e73b3
Merge: cadf32234b6f 34f853b849eb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 2 20:16:55 2020 -0700

    Merge tag 'erofs-for-5.8-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/xiang/erofs
    
    Pull erofs updates from Gao Xiang:
     "The most interesting part is the new mount api conversion, which is
      actually a old patch already pending for several cycles. And the
      others are recent trivial cleanups here.
    
      Summary:
    
       - Convert to use the new mount apis
    
       - Some random cleanup patches"
    
    * tag 'erofs-for-5.8-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/xiang/erofs:
      erofs: suppress false positive last_block warning
      erofs: convert to use the new mount fs_context api
      erofs: code cleanup by removing ifdef macro surrounding

commit 0c07a9f91ec0367925985944e288993759fb1b07
Author: Matthew Wilcox (Oracle) <willy@infradead.org>
Date:   Mon Jun 1 21:47:09 2020 -0700

    erofs: convert uncompressed files from readpages to readahead
    
    Use the new readahead operation in erofs
    
    Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: William Kucharski <william.kucharski@oracle.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Acked-by: Gao Xiang <gaoxiang25@huawei.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Dave Chinner <dchinner@redhat.com>
    Cc: Eric Biggers <ebiggers@google.com>
    Cc: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Joseph Qi <joseph.qi@linux.alibaba.com>
    Cc: Junxiao Bi <junxiao.bi@oracle.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Zi Yan <ziy@nvidia.com>
    Cc: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Cc: Miklos Szeredi <mszeredi@redhat.com>
    Link: http://lkml.kernel.org/r/20200414150233.24495-19-willy@infradead.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index fc3a8d8064f8..d0542151e8c4 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -280,47 +280,36 @@ static int erofs_raw_access_readpage(struct file *file, struct page *page)
 	return 0;
 }
 
-static int erofs_raw_access_readpages(struct file *filp,
-				      struct address_space *mapping,
-				      struct list_head *pages,
-				      unsigned int nr_pages)
+static void erofs_raw_access_readahead(struct readahead_control *rac)
 {
 	erofs_off_t last_block;
 	struct bio *bio = NULL;
-	gfp_t gfp = readahead_gfp_mask(mapping);
-	struct page *page = list_last_entry(pages, struct page, lru);
-
-	trace_erofs_readpages(mapping->host, page, nr_pages, true);
+	struct page *page;
 
-	for (; nr_pages; --nr_pages) {
-		page = list_entry(pages->prev, struct page, lru);
+	trace_erofs_readpages(rac->mapping->host, readahead_index(rac),
+			readahead_count(rac), true);
 
+	while ((page = readahead_page(rac))) {
 		prefetchw(&page->flags);
-		list_del(&page->lru);
 
-		if (!add_to_page_cache_lru(page, mapping, page->index, gfp)) {
-			bio = erofs_read_raw_page(bio, mapping, page,
-						  &last_block, nr_pages, true);
+		bio = erofs_read_raw_page(bio, rac->mapping, page, &last_block,
+				readahead_count(rac), true);
 
-			/* all the page errors are ignored when readahead */
-			if (IS_ERR(bio)) {
-				pr_err("%s, readahead error at page %lu of nid %llu\n",
-				       __func__, page->index,
-				       EROFS_I(mapping->host)->nid);
+		/* all the page errors are ignored when readahead */
+		if (IS_ERR(bio)) {
+			pr_err("%s, readahead error at page %lu of nid %llu\n",
+			       __func__, page->index,
+			       EROFS_I(rac->mapping->host)->nid);
 
-				bio = NULL;
-			}
+			bio = NULL;
 		}
 
-		/* pages could still be locked */
 		put_page(page);
 	}
-	DBG_BUGON(!list_empty(pages));
 
 	/* the rare case (end in gaps) */
 	if (bio)
 		submit_bio(bio);
-	return 0;
 }
 
 static int erofs_get_block(struct inode *inode, sector_t iblock,
@@ -358,7 +347,7 @@ static sector_t erofs_bmap(struct address_space *mapping, sector_t block)
 /* for uncompressed (aligned) files and raw access for other files */
 const struct address_space_operations erofs_raw_access_aops = {
 	.readpage = erofs_raw_access_readpage,
-	.readpages = erofs_raw_access_readpages,
+	.readahead = erofs_raw_access_readahead,
 	.bmap = erofs_bmap,
 };
 

commit 34f853b849eb6a509eb8f40f2f5946ebb1f62739
Author: Gao Xiang <hsiangkao@redhat.com>
Date:   Thu May 28 04:48:44 2020 -0400

    erofs: suppress false positive last_block warning
    
    As Andrew mentioned, some rare specific gcc versions could report
    last_block uninitialized warning. Actually last_block doesn't need
    to be uninitialized first from its implementation due to bio == NULL
    condition. After a bio is allocated, last_block will be assigned
    then.
    
    The detailed analysis is in this thread [1]. So let's silence those
    confusing gccs simply.
    
    [1] https://lore.kernel.org/r/20200421072839.GA13867@hsiangkao-HP-ZHAN-66-Pro-G1
    
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Link: https://lore.kernel.org/r/20200528084844.23359-1-hsiangkao@redhat.com
    Signed-off-by: Gao Xiang <hsiangkao@redhat.com>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index fc3a8d8064f8..2812645b361e 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -265,7 +265,7 @@ static inline struct bio *erofs_read_raw_page(struct bio *bio,
  */
 static int erofs_raw_access_readpage(struct file *file, struct page *page)
 {
-	erofs_off_t last_block;
+	erofs_off_t uninitialized_var(last_block);
 	struct bio *bio;
 
 	trace_erofs_readpage(page, true);
@@ -285,7 +285,7 @@ static int erofs_raw_access_readpages(struct file *filp,
 				      struct list_head *pages,
 				      unsigned int nr_pages)
 {
-	erofs_off_t last_block;
+	erofs_off_t uninitialized_var(last_block);
 	struct bio *bio = NULL;
 	gfp_t gfp = readahead_gfp_mask(mapping);
 	struct page *page = list_last_entry(pages, struct page, lru);

commit 55252ab72b774119afedfdc6d1f142ffa2a9b818
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Sun Sep 22 02:43:55 2019 +0800

    erofs: fix erofs_get_meta_page locking due to a cleanup
    
    After doing more drop_caches stress test on
    our products, I found the mistake introduced by
    a very recent cleanup [1].
    
    The current rule is that "erofs_get_meta_page"
    should be returned with page locked (although
    it's mostly unnecessary for read-only fs after
    pages are PG_uptodate), but a fix should be
    done for this.
    
    [1] https://lore.kernel.org/r/20190904020912.63925-26-gaoxiang25@huawei.com
    Fixes: 618f40ea026b ("erofs: use read_cache_page_gfp for erofs_get_meta_page")
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Link: https://lore.kernel.org/r/20190921184355.149928-1-gaoxiang25@huawei.com
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index 8a9fcbd0e8ac..fc3a8d8064f8 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -34,11 +34,15 @@ static void erofs_readendio(struct bio *bio)
 
 struct page *erofs_get_meta_page(struct super_block *sb, erofs_blk_t blkaddr)
 {
-	struct inode *const bd_inode = sb->s_bdev->bd_inode;
-	struct address_space *const mapping = bd_inode->i_mapping;
+	struct address_space *const mapping = sb->s_bdev->bd_inode->i_mapping;
+	struct page *page;
 
-	return read_cache_page_gfp(mapping, blkaddr,
+	page = read_cache_page_gfp(mapping, blkaddr,
 				   mapping_gfp_constraint(mapping, ~__GFP_FS));
+	/* should already be PageUptodate */
+	if (!IS_ERR(page))
+		lock_page(page);
+	return page;
 }
 
 static int erofs_map_blocks_flatmode(struct inode *inode,

commit 618f40ea026bda2866b2b1bc8ae2c7343a3d7a22
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Wed Sep 4 10:09:12 2019 +0800

    erofs: use read_cache_page_gfp for erofs_get_meta_page
    
    As Christoph said [1], "I'd much prefer to just use
    read_cache_page_gfp, and live with the fact that this
    allocates bufferheads behind you for now.  I'll try to
    speed up my attempts to get rid of the buffer heads on
    the block device mapping instead. "
    
    This simplifies the code a lot and a minor thing is
    "no REQ_META (e.g. for blktrace) on metadata at all..."
    
    [1] https://lore.kernel.org/r/20190903153704.GA2201@infradead.org/
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190904020912.63925-26-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index eb7bbae89ed0..8a9fcbd0e8ac 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -32,71 +32,13 @@ static void erofs_readendio(struct bio *bio)
 	bio_put(bio);
 }
 
-static struct bio *erofs_grab_raw_bio(struct super_block *sb,
-				      erofs_blk_t blkaddr,
-				      unsigned int nr_pages,
-				      bool ismeta)
-{
-	struct bio *bio = bio_alloc(GFP_NOIO, nr_pages);
-
-	bio->bi_end_io = erofs_readendio;
-	bio_set_dev(bio, sb->s_bdev);
-	bio->bi_iter.bi_sector = (sector_t)blkaddr << LOG_SECTORS_PER_BLOCK;
-	if (ismeta)
-		bio->bi_opf = REQ_OP_READ | REQ_META;
-	else
-		bio->bi_opf = REQ_OP_READ;
-
-	return bio;
-}
-
 struct page *erofs_get_meta_page(struct super_block *sb, erofs_blk_t blkaddr)
 {
 	struct inode *const bd_inode = sb->s_bdev->bd_inode;
 	struct address_space *const mapping = bd_inode->i_mapping;
-	const gfp_t gfp = mapping_gfp_constraint(mapping, ~__GFP_FS);
-	struct page *page;
-	int err;
-
-repeat:
-	page = find_or_create_page(mapping, blkaddr, gfp);
-	if (!page)
-		return ERR_PTR(-ENOMEM);
-
-	DBG_BUGON(!PageLocked(page));
-
-	if (!PageUptodate(page)) {
-		struct bio *bio;
-
-		bio = erofs_grab_raw_bio(sb, blkaddr, 1, true);
-
-		if (bio_add_page(bio, page, PAGE_SIZE, 0) != PAGE_SIZE) {
-			err = -EFAULT;
-			goto err_out;
-		}
-
-		submit_bio(bio);
-		lock_page(page);
 
-		/* this page has been truncated by others */
-		if (page->mapping != mapping) {
-			unlock_page(page);
-			put_page(page);
-			goto repeat;
-		}
-
-		/* more likely a read error */
-		if (!PageUptodate(page)) {
-			err = -EIO;
-			goto err_out;
-		}
-	}
-	return page;
-
-err_out:
-	unlock_page(page);
-	put_page(page);
-	return ERR_PTR(err);
+	return read_cache_page_gfp(mapping, blkaddr,
+				   mapping_gfp_constraint(mapping, ~__GFP_FS));
 }
 
 static int erofs_map_blocks_flatmode(struct inode *inode,
@@ -272,7 +214,13 @@ static inline struct bio *erofs_read_raw_page(struct bio *bio,
 		if (nblocks > BIO_MAX_PAGES)
 			nblocks = BIO_MAX_PAGES;
 
-		bio = erofs_grab_raw_bio(sb, blknr, nblocks, false);
+		bio = bio_alloc(GFP_NOIO, nblocks);
+
+		bio->bi_end_io = erofs_readendio;
+		bio_set_dev(bio, sb->s_bdev);
+		bio->bi_iter.bi_sector = (sector_t)blknr <<
+			LOG_SECTORS_PER_BLOCK;
+		bio->bi_opf = REQ_OP_READ;
 	}
 
 	err = bio_add_page(bio, page, PAGE_SIZE, 0);

commit 4f761fa253b49f657de7ef6f695a124e08e56c3a
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Wed Sep 4 10:09:09 2019 +0800

    erofs: rename errln/infoln/debugln to erofs_{err, info, dbg}
    
    Add prefix "erofs_" to these functions and print
    sb->s_id as a prefix to erofs_{err, info} so that
    the user knows which file system is affected.
    
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190904020912.63925-23-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index b5f5b8592d14..eb7bbae89ed0 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -137,8 +137,9 @@ static int erofs_map_blocks_flatmode(struct inode *inode,
 
 		/* inline data should be located in one meta block */
 		if (erofs_blkoff(map->m_pa) + map->m_plen > PAGE_SIZE) {
-			errln("inline data cross block boundary @ nid %llu",
-			      vi->nid);
+			erofs_err(inode->i_sb,
+				  "inline data cross block boundary @ nid %llu",
+				  vi->nid);
 			DBG_BUGON(1);
 			err = -EFSCORRUPTED;
 			goto err_out;
@@ -146,8 +147,9 @@ static int erofs_map_blocks_flatmode(struct inode *inode,
 
 		map->m_flags |= EROFS_MAP_META;
 	} else {
-		errln("internal error @ nid: %llu (size %llu), m_la 0x%llx",
-		      vi->nid, inode->i_size, map->m_la);
+		erofs_err(inode->i_sb,
+			  "internal error @ nid: %llu (size %llu), m_la 0x%llx",
+			  vi->nid, inode->i_size, map->m_la);
 		DBG_BUGON(1);
 		err = -EIO;
 		goto err_out;

commit e2c71e74b21e4053c18ac99ee93139d0aa95c53a
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Wed Sep 4 10:09:06 2019 +0800

    erofs: kill all erofs specific fault injection
    
    As Christoph suggested [1], "Please just use plain kmalloc
    everywhere and let the normal kernel error injection code
    take care of injeting any errors."
    
    [1] https://lore.kernel.org/r/20190829102426.GE20598@infradead.org/
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190904020912.63925-20-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index 3ce87a88452a..b5f5b8592d14 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -11,16 +11,10 @@
 
 static void erofs_readendio(struct bio *bio)
 {
-	struct super_block *const sb = bio->bi_private;
 	struct bio_vec *bvec;
 	blk_status_t err = bio->bi_status;
 	struct bvec_iter_all iter_all;
 
-	if (time_to_inject(EROFS_SB(sb), FAULT_READ_IO)) {
-		erofs_show_injection_info(FAULT_READ_IO);
-		err = BLK_STS_IOERR;
-	}
-
 	bio_for_each_segment_all(bvec, bio, iter_all) {
 		struct page *page = bvec->bv_page;
 
@@ -48,7 +42,6 @@ static struct bio *erofs_grab_raw_bio(struct super_block *sb,
 	bio->bi_end_io = erofs_readendio;
 	bio_set_dev(bio, sb->s_bdev);
 	bio->bi_iter.bi_sector = (sector_t)blkaddr << LOG_SECTORS_PER_BLOCK;
-	bio->bi_private = sb;
 	if (ismeta)
 		bio->bi_opf = REQ_OP_READ | REQ_META;
 	else

commit 99634bf388db04048b83a075358a1d166e7300fb
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Wed Sep 4 10:09:05 2019 +0800

    erofs: add "erofs_" prefix for common and short functions
    
    Add erofs_ prefix to free_inode, alloc_inode, ...
    
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190904020912.63925-19-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index 70b1e353756e..3ce87a88452a 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -9,7 +9,7 @@
 
 #include <trace/events/erofs.h>
 
-static inline void read_endio(struct bio *bio)
+static void erofs_readendio(struct bio *bio)
 {
 	struct super_block *const sb = bio->bi_private;
 	struct bio_vec *bvec;
@@ -45,7 +45,7 @@ static struct bio *erofs_grab_raw_bio(struct super_block *sb,
 {
 	struct bio *bio = bio_alloc(GFP_NOIO, nr_pages);
 
-	bio->bi_end_io = read_endio;
+	bio->bi_end_io = erofs_readendio;
 	bio_set_dev(bio, sb->s_bdev);
 	bio->bi_iter.bi_sector = (sector_t)blkaddr << LOG_SECTORS_PER_BLOCK;
 	bio->bi_private = sb;

commit 94e4e153b1c25a49b4953c424e6e2f66efb449f3
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Wed Sep 4 10:09:04 2019 +0800

    erofs: kill __submit_bio()
    
    As Christoph pointed out [1], "
    Why is there __submit_bio which really just obsfucates
    what is going on?  Also why is __submit_bio using
    bio_set_op_attrs instead of opencode it as the comment
    right next to it asks you to? "
    
    Let's use submit_bio directly instead.
    
    [1] https://lore.kernel.org/r/20190830162812.GA10694@infradead.org/
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190904020912.63925-18-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index 28eda71bb1a9..70b1e353756e 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -40,7 +40,8 @@ static inline void read_endio(struct bio *bio)
 
 static struct bio *erofs_grab_raw_bio(struct super_block *sb,
 				      erofs_blk_t blkaddr,
-				      unsigned int nr_pages)
+				      unsigned int nr_pages,
+				      bool ismeta)
 {
 	struct bio *bio = bio_alloc(GFP_NOIO, nr_pages);
 
@@ -48,6 +49,11 @@ static struct bio *erofs_grab_raw_bio(struct super_block *sb,
 	bio_set_dev(bio, sb->s_bdev);
 	bio->bi_iter.bi_sector = (sector_t)blkaddr << LOG_SECTORS_PER_BLOCK;
 	bio->bi_private = sb;
+	if (ismeta)
+		bio->bi_opf = REQ_OP_READ | REQ_META;
+	else
+		bio->bi_opf = REQ_OP_READ;
+
 	return bio;
 }
 
@@ -69,14 +75,14 @@ struct page *erofs_get_meta_page(struct super_block *sb, erofs_blk_t blkaddr)
 	if (!PageUptodate(page)) {
 		struct bio *bio;
 
-		bio = erofs_grab_raw_bio(sb, blkaddr, 1);
+		bio = erofs_grab_raw_bio(sb, blkaddr, 1, true);
 
 		if (bio_add_page(bio, page, PAGE_SIZE, 0) != PAGE_SIZE) {
 			err = -EFAULT;
 			goto err_out;
 		}
 
-		__submit_bio(bio, REQ_OP_READ, REQ_META);
+		submit_bio(bio);
 		lock_page(page);
 
 		/* this page has been truncated by others */
@@ -201,7 +207,7 @@ static inline struct bio *erofs_read_raw_page(struct bio *bio,
 	    /* not continuous */
 	    *last_block + 1 != current_block) {
 submit_bio_retry:
-		__submit_bio(bio, REQ_OP_READ, 0);
+		submit_bio(bio);
 		bio = NULL;
 	}
 
@@ -271,7 +277,7 @@ static inline struct bio *erofs_read_raw_page(struct bio *bio,
 		if (nblocks > BIO_MAX_PAGES)
 			nblocks = BIO_MAX_PAGES;
 
-		bio = erofs_grab_raw_bio(sb, blknr, nblocks);
+		bio = erofs_grab_raw_bio(sb, blknr, nblocks, false);
 	}
 
 	err = bio_add_page(bio, page, PAGE_SIZE, 0);
@@ -302,8 +308,7 @@ static inline struct bio *erofs_read_raw_page(struct bio *bio,
 	/* if updated manually, continuous pages has a gap */
 	if (bio)
 submit_bio_out:
-		__submit_bio(bio, REQ_OP_READ, 0);
-
+		submit_bio(bio);
 	return err ? ERR_PTR(err) : NULL;
 }
 
@@ -367,7 +372,7 @@ static int erofs_raw_access_readpages(struct file *filp,
 
 	/* the rare case (end in gaps) */
 	if (bio)
-		__submit_bio(bio, REQ_OP_READ, 0);
+		submit_bio(bio);
 	return 0;
 }
 

commit e655b5b3a29c5a16056f13854ac3db5b39c0b804
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Wed Sep 4 10:09:03 2019 +0800

    erofs: kill prio and nofail of erofs_get_meta_page()
    
    As Christoph pointed out [1],
    "Why is there __erofs_get_meta_page with the two weird
    booleans instead of a single erofs_get_meta_page that
    gets and gfp_t for additional flags and an unsigned int
    for additional bio op flags."
    
    And since all callers can handle errors, let's kill
    prio and nofail and erofs_get_inline_page() now.
    
    [1] https://lore.kernel.org/r/20190830162812.GA10694@infradead.org/
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190904020912.63925-17-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index 0136ea117934..28eda71bb1a9 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -51,25 +51,19 @@ static struct bio *erofs_grab_raw_bio(struct super_block *sb,
 	return bio;
 }
 
-/* prio -- true is used for dir */
-struct page *__erofs_get_meta_page(struct super_block *sb,
-				   erofs_blk_t blkaddr, bool prio, bool nofail)
+struct page *erofs_get_meta_page(struct super_block *sb, erofs_blk_t blkaddr)
 {
 	struct inode *const bd_inode = sb->s_bdev->bd_inode;
 	struct address_space *const mapping = bd_inode->i_mapping;
-	/* prefer retrying in the allocator to blindly looping below */
-	const gfp_t gfp = mapping_gfp_constraint(mapping, ~__GFP_FS) |
-		(nofail ? __GFP_NOFAIL : 0);
-	unsigned int io_retries = nofail ? EROFS_IO_MAX_RETRIES_NOFAIL : 0;
+	const gfp_t gfp = mapping_gfp_constraint(mapping, ~__GFP_FS);
 	struct page *page;
 	int err;
 
 repeat:
 	page = find_or_create_page(mapping, blkaddr, gfp);
-	if (!page) {
-		DBG_BUGON(nofail);
+	if (!page)
 		return ERR_PTR(-ENOMEM);
-	}
+
 	DBG_BUGON(!PageLocked(page));
 
 	if (!PageUptodate(page)) {
@@ -82,14 +76,11 @@ struct page *__erofs_get_meta_page(struct super_block *sb,
 			goto err_out;
 		}
 
-		__submit_bio(bio, REQ_OP_READ,
-			     REQ_META | (prio ? REQ_PRIO : 0));
-
+		__submit_bio(bio, REQ_OP_READ, REQ_META);
 		lock_page(page);
 
 		/* this page has been truncated by others */
 		if (page->mapping != mapping) {
-unlock_repeat:
 			unlock_page(page);
 			put_page(page);
 			goto repeat;
@@ -97,10 +88,6 @@ struct page *__erofs_get_meta_page(struct super_block *sb,
 
 		/* more likely a read error */
 		if (!PageUptodate(page)) {
-			if (io_retries) {
-				--io_retries;
-				goto unlock_repeat;
-			}
 			err = -EIO;
 			goto err_out;
 		}
@@ -251,7 +238,7 @@ static inline struct bio *erofs_read_raw_page(struct bio *bio,
 
 			DBG_BUGON(map.m_plen > PAGE_SIZE);
 
-			ipage = erofs_get_meta_page(inode->i_sb, blknr, 0);
+			ipage = erofs_get_meta_page(inode->i_sb, blknr);
 
 			if (IS_ERR(ipage)) {
 				err = PTR_ERR(ipage);

commit a5c0b7802cc9631e0dee67abd30c2f022621ae7c
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Wed Sep 4 10:09:02 2019 +0800

    erofs: localize erofs_grab_bio()
    
    As Christoph pointed out [1], "erofs_grab_bio tries to
    handle a bio_alloc failure, except that the function will
    not actually fail due the mempool backing it."
    
    Sorry about useless code, fix it now and
    localize erofs_grab_bio [2].
    
    [1] https://lore.kernel.org/r/20190830162812.GA10694@infradead.org/
    [2] https://lore.kernel.org/r/20190902122016.GL15931@infradead.org/
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190904020912.63925-16-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index be11b5ea9d2e..0136ea117934 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -38,6 +38,19 @@ static inline void read_endio(struct bio *bio)
 	bio_put(bio);
 }
 
+static struct bio *erofs_grab_raw_bio(struct super_block *sb,
+				      erofs_blk_t blkaddr,
+				      unsigned int nr_pages)
+{
+	struct bio *bio = bio_alloc(GFP_NOIO, nr_pages);
+
+	bio->bi_end_io = read_endio;
+	bio_set_dev(bio, sb->s_bdev);
+	bio->bi_iter.bi_sector = (sector_t)blkaddr << LOG_SECTORS_PER_BLOCK;
+	bio->bi_private = sb;
+	return bio;
+}
+
 /* prio -- true is used for dir */
 struct page *__erofs_get_meta_page(struct super_block *sb,
 				   erofs_blk_t blkaddr, bool prio, bool nofail)
@@ -62,12 +75,7 @@ struct page *__erofs_get_meta_page(struct super_block *sb,
 	if (!PageUptodate(page)) {
 		struct bio *bio;
 
-		bio = erofs_grab_bio(sb, blkaddr, 1, sb, read_endio, nofail);
-		if (IS_ERR(bio)) {
-			DBG_BUGON(nofail);
-			err = PTR_ERR(bio);
-			goto err_out;
-		}
+		bio = erofs_grab_raw_bio(sb, blkaddr, 1);
 
 		if (bio_add_page(bio, page, PAGE_SIZE, 0) != PAGE_SIZE) {
 			err = -EFAULT;
@@ -276,13 +284,7 @@ static inline struct bio *erofs_read_raw_page(struct bio *bio,
 		if (nblocks > BIO_MAX_PAGES)
 			nblocks = BIO_MAX_PAGES;
 
-		bio = erofs_grab_bio(sb, blknr, nblocks, sb,
-				     read_endio, false);
-		if (IS_ERR(bio)) {
-			err = PTR_ERR(bio);
-			bio = NULL;
-			goto err_out;
-		}
+		bio = erofs_grab_raw_bio(sb, blknr, nblocks);
 	}
 
 	err = bio_add_page(bio, page, PAGE_SIZE, 0);

commit a5876e24f13f13483fbd602b972d35801fb80b74
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Wed Sep 4 10:08:56 2019 +0800

    erofs: use erofs_inode naming
    
    As Christoph suggested [1], "Why is this called vnode instead
    of inode?  That seems like a rather odd naming for a Linux
    file system."
    
    [1] https://lore.kernel.org/r/20190829101545.GC20598@infradead.org/
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190904020912.63925-10-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index 4d9b07991d07..be11b5ea9d2e 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -112,7 +112,7 @@ static int erofs_map_blocks_flatmode(struct inode *inode,
 	int err = 0;
 	erofs_blk_t nblocks, lastblk;
 	u64 offset = map->m_la;
-	struct erofs_vnode *vi = EROFS_V(inode);
+	struct erofs_inode *vi = EROFS_I(inode);
 	bool tailendpacking = (vi->datalayout == EROFS_INODE_FLAT_INLINE);
 
 	trace_erofs_map_blocks_flatmode_enter(inode, map, flags);
@@ -170,7 +170,7 @@ static int erofs_map_blocks_flatmode(struct inode *inode,
 int erofs_map_blocks(struct inode *inode,
 		     struct erofs_map_blocks *map, int flags)
 {
-	if (erofs_inode_is_data_compressed(EROFS_V(inode)->datalayout)) {
+	if (erofs_inode_is_data_compressed(EROFS_I(inode)->datalayout)) {
 		int err = z_erofs_map_blocks_iter(inode, map, flags);
 
 		if (map->mpage) {
@@ -365,7 +365,7 @@ static int erofs_raw_access_readpages(struct file *filp,
 			if (IS_ERR(bio)) {
 				pr_err("%s, readahead error at page %lu of nid %llu\n",
 				       __func__, page->index,
-				       EROFS_V(mapping->host)->nid);
+				       EROFS_I(mapping->host)->nid);
 
 				bio = NULL;
 			}
@@ -404,7 +404,7 @@ static sector_t erofs_bmap(struct address_space *mapping, sector_t block)
 {
 	struct inode *inode = mapping->host;
 
-	if (EROFS_V(inode)->datalayout == EROFS_INODE_FLAT_INLINE) {
+	if (EROFS_I(inode)->datalayout == EROFS_INODE_FLAT_INLINE) {
 		erofs_blk_t blks = i_size_read(inode) >> LOG_BLOCK_SIZE;
 
 		if (block >> LOG_SECTORS_PER_BLOCK >= blks)

commit 8a76568225deae18cea1d1a21c2bade2f2ced2ba
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Wed Sep 4 10:08:54 2019 +0800

    erofs: better naming for erofs inode related stuffs
    
    updates inode naming
     - kill is_inode_layout_compression [1]
     - kill magic underscores [2] [3]
     - better naming for datamode & data_mapping_mode [3]
     - better naming erofs_inode_{compact, extended} [4]
    
    [1] https://lore.kernel.org/r/20190829102426.GE20598@infradead.org/
    [2] https://lore.kernel.org/r/20190829102426.GE20598@infradead.org/
    [3] https://lore.kernel.org/r/20190902122627.GN15931@infradead.org/
    [4] https://lore.kernel.org/r/20190902125438.GA17750@infradead.org/
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190904020912.63925-8-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index 0983807737fd..4d9b07991d07 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -113,11 +113,12 @@ static int erofs_map_blocks_flatmode(struct inode *inode,
 	erofs_blk_t nblocks, lastblk;
 	u64 offset = map->m_la;
 	struct erofs_vnode *vi = EROFS_V(inode);
+	bool tailendpacking = (vi->datalayout == EROFS_INODE_FLAT_INLINE);
 
 	trace_erofs_map_blocks_flatmode_enter(inode, map, flags);
 
 	nblocks = DIV_ROUND_UP(inode->i_size, PAGE_SIZE);
-	lastblk = nblocks - is_inode_flat_inline(inode);
+	lastblk = nblocks - tailendpacking;
 
 	if (offset >= inode->i_size) {
 		/* leave out-of-bound access unmapped */
@@ -132,7 +133,7 @@ static int erofs_map_blocks_flatmode(struct inode *inode,
 	if (offset < blknr_to_addr(lastblk)) {
 		map->m_pa = blknr_to_addr(vi->raw_blkaddr) + map->m_la;
 		map->m_plen = blknr_to_addr(lastblk) - offset;
-	} else if (is_inode_flat_inline(inode)) {
+	} else if (tailendpacking) {
 		/* 2 - inode inline B: inode, [xattrs], inline last blk... */
 		struct erofs_sb_info *sbi = EROFS_SB(inode->i_sb);
 
@@ -169,7 +170,7 @@ static int erofs_map_blocks_flatmode(struct inode *inode,
 int erofs_map_blocks(struct inode *inode,
 		     struct erofs_map_blocks *map, int flags)
 {
-	if (is_inode_layout_compression(inode)) {
+	if (erofs_inode_is_data_compressed(EROFS_V(inode)->datalayout)) {
 		int err = z_erofs_map_blocks_iter(inode, map, flags);
 
 		if (map->mpage) {
@@ -403,7 +404,7 @@ static sector_t erofs_bmap(struct address_space *mapping, sector_t block)
 {
 	struct inode *inode = mapping->host;
 
-	if (is_inode_flat_inline(inode)) {
+	if (EROFS_V(inode)->datalayout == EROFS_INODE_FLAT_INLINE) {
 		erofs_blk_t blks = i_size_read(inode) >> LOG_BLOCK_SIZE;
 
 		if (block >> LOG_SECTORS_PER_BLOCK >= blks)

commit 097a802ae15e2c6d17256a6fe0712a655ba10823
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Fri Aug 30 01:17:41 2019 +0800

    erofs: reduntant assignment in __erofs_get_meta_page()
    
    As Joe Perches suggested [1],
                    err = bio_add_page(bio, page, PAGE_SIZE, 0);
    -               if (unlikely(err != PAGE_SIZE)) {
    +               if (err != PAGE_SIZE) {
                            err = -EFAULT;
                            goto err_out;
                    }
    
    The initial assignment to err is odd as it's not
    actually an error value -E<FOO> but a int size
    from a unsigned int len.
    
    Here the return is either 0 or PAGE_SIZE.
    
    This would be more legible to me as:
    
                    if (bio_add_page(bio, page, PAGE_SIZE, 0) != PAGE_SIZE) {
                            err = -EFAULT;
                            goto err_out;
                    }
    
    [1] https://lore.kernel.org/r/74c4784319b40deabfbaea92468f7e3ef44f1c96.camel@perches.com/
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190829171741.225219-1-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index 0f2f1a839372..0983807737fd 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -69,8 +69,7 @@ struct page *__erofs_get_meta_page(struct super_block *sb,
 			goto err_out;
 		}
 
-		err = bio_add_page(bio, page, PAGE_SIZE, 0);
-		if (err != PAGE_SIZE) {
+		if (bio_add_page(bio, page, PAGE_SIZE, 0) != PAGE_SIZE) {
 			err = -EFAULT;
 			goto err_out;
 		}

commit 8d8a09b093d7073465c824f74caf315c073d3875
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Fri Aug 30 00:38:27 2019 +0800

    erofs: remove all likely/unlikely annotations
    
    As Dan Carpenter suggested [1], I have to remove
    all erofs likely/unlikely annotations.
    
    [1] https://lore.kernel.org/linux-fsdevel/20190829154346.GK23584@kadam/
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190829163827.203274-1-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
index fda16ec8863e..0f2f1a839372 100644
--- a/fs/erofs/data.c
+++ b/fs/erofs/data.c
@@ -27,7 +27,7 @@ static inline void read_endio(struct bio *bio)
 		/* page is already locked */
 		DBG_BUGON(PageUptodate(page));
 
-		if (unlikely(err))
+		if (err)
 			SetPageError(page);
 		else
 			SetPageUptodate(page);
@@ -53,7 +53,7 @@ struct page *__erofs_get_meta_page(struct super_block *sb,
 
 repeat:
 	page = find_or_create_page(mapping, blkaddr, gfp);
-	if (unlikely(!page)) {
+	if (!page) {
 		DBG_BUGON(nofail);
 		return ERR_PTR(-ENOMEM);
 	}
@@ -70,7 +70,7 @@ struct page *__erofs_get_meta_page(struct super_block *sb,
 		}
 
 		err = bio_add_page(bio, page, PAGE_SIZE, 0);
-		if (unlikely(err != PAGE_SIZE)) {
+		if (err != PAGE_SIZE) {
 			err = -EFAULT;
 			goto err_out;
 		}
@@ -81,7 +81,7 @@ struct page *__erofs_get_meta_page(struct super_block *sb,
 		lock_page(page);
 
 		/* this page has been truncated by others */
-		if (unlikely(page->mapping != mapping)) {
+		if (page->mapping != mapping) {
 unlock_repeat:
 			unlock_page(page);
 			put_page(page);
@@ -89,7 +89,7 @@ struct page *__erofs_get_meta_page(struct super_block *sb,
 		}
 
 		/* more likely a read error */
-		if (unlikely(!PageUptodate(page))) {
+		if (!PageUptodate(page)) {
 			if (io_retries) {
 				--io_retries;
 				goto unlock_repeat;
@@ -120,7 +120,7 @@ static int erofs_map_blocks_flatmode(struct inode *inode,
 	nblocks = DIV_ROUND_UP(inode->i_size, PAGE_SIZE);
 	lastblk = nblocks - is_inode_flat_inline(inode);
 
-	if (unlikely(offset >= inode->i_size)) {
+	if (offset >= inode->i_size) {
 		/* leave out-of-bound access unmapped */
 		map->m_flags = 0;
 		map->m_plen = 0;
@@ -170,7 +170,7 @@ static int erofs_map_blocks_flatmode(struct inode *inode,
 int erofs_map_blocks(struct inode *inode,
 		     struct erofs_map_blocks *map, int flags)
 {
-	if (unlikely(is_inode_layout_compression(inode))) {
+	if (is_inode_layout_compression(inode)) {
 		int err = z_erofs_map_blocks_iter(inode, map, flags);
 
 		if (map->mpage) {
@@ -218,11 +218,11 @@ static inline struct bio *erofs_read_raw_page(struct bio *bio,
 		unsigned int blkoff;
 
 		err = erofs_map_blocks(inode, &map, EROFS_GET_BLOCKS_RAW);
-		if (unlikely(err))
+		if (err)
 			goto err_out;
 
 		/* zero out the holed page */
-		if (unlikely(!(map.m_flags & EROFS_MAP_MAPPED))) {
+		if (!(map.m_flags & EROFS_MAP_MAPPED)) {
 			zero_user_segment(page, 0, PAGE_SIZE);
 			SetPageUptodate(page);
 
@@ -315,7 +315,7 @@ static inline struct bio *erofs_read_raw_page(struct bio *bio,
 submit_bio_out:
 		__submit_bio(bio, REQ_OP_READ, 0);
 
-	return unlikely(err) ? ERR_PTR(err) : NULL;
+	return err ? ERR_PTR(err) : NULL;
 }
 
 /*
@@ -377,7 +377,7 @@ static int erofs_raw_access_readpages(struct file *filp,
 	DBG_BUGON(!list_empty(pages));
 
 	/* the rare case (end in gaps) */
-	if (unlikely(bio))
+	if (bio)
 		__submit_bio(bio, REQ_OP_READ, 0);
 	return 0;
 }

commit 47e4937a4a7ca4184fd282791dfee76c6799966a
Author: Gao Xiang <hsiangkao@aol.com>
Date:   Fri Aug 23 05:36:59 2019 +0800

    erofs: move erofs out of staging
    
    EROFS filesystem has been merged into linux-staging for a year.
    
    EROFS is designed to be a better solution of saving extra storage
    space with guaranteed end-to-end performance for read-only files
    with the help of reduced metadata, fixed-sized output compression
    and decompression inplace technologies.
    
    In the past year, EROFS was greatly improved by many people as
    a staging driver, self-tested, betaed by a large number of our
    internal users, successfully applied to almost all in-service
    HUAWEI smartphones as the part of EMUI 9.1 and proven to be stable
    enough to be moved out of staging.
    
    EROFS is a self-contained filesystem driver. Although there are
    still some TODOs to be more generic, we have a dedicated team
    actively keeping on working on EROFS in order to make it better
    with the evolution of Linux kernel as the other in-kernel filesystems.
    
    As Pavel suggested, it's better to do as one commit since git
    can do moves and all histories will be saved in this way.
    
    Let's promote it from staging and enhance it more actively as
    a "real" part of kernel for more wider scenarios!
    
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Pavel Machek <pavel@denx.de>
    Cc: David Sterba <dsterba@suse.cz>
    Cc: Amir Goldstein <amir73il@gmail.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Darrick J . Wong <darrick.wong@oracle.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Chao Yu <yuchao0@huawei.com>
    Cc: Miao Xie <miaoxie@huawei.com>
    Cc: Li Guifu <bluce.liguifu@huawei.com>
    Cc: Fang Wei <fangwei1@huawei.com>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190822213659.5501-1-hsiangkao@aol.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/data.c b/fs/erofs/data.c
new file mode 100644
index 000000000000..fda16ec8863e
--- /dev/null
+++ b/fs/erofs/data.c
@@ -0,0 +1,423 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2017-2018 HUAWEI, Inc.
+ *             http://www.huawei.com/
+ * Created by Gao Xiang <gaoxiang25@huawei.com>
+ */
+#include "internal.h"
+#include <linux/prefetch.h>
+
+#include <trace/events/erofs.h>
+
+static inline void read_endio(struct bio *bio)
+{
+	struct super_block *const sb = bio->bi_private;
+	struct bio_vec *bvec;
+	blk_status_t err = bio->bi_status;
+	struct bvec_iter_all iter_all;
+
+	if (time_to_inject(EROFS_SB(sb), FAULT_READ_IO)) {
+		erofs_show_injection_info(FAULT_READ_IO);
+		err = BLK_STS_IOERR;
+	}
+
+	bio_for_each_segment_all(bvec, bio, iter_all) {
+		struct page *page = bvec->bv_page;
+
+		/* page is already locked */
+		DBG_BUGON(PageUptodate(page));
+
+		if (unlikely(err))
+			SetPageError(page);
+		else
+			SetPageUptodate(page);
+
+		unlock_page(page);
+		/* page could be reclaimed now */
+	}
+	bio_put(bio);
+}
+
+/* prio -- true is used for dir */
+struct page *__erofs_get_meta_page(struct super_block *sb,
+				   erofs_blk_t blkaddr, bool prio, bool nofail)
+{
+	struct inode *const bd_inode = sb->s_bdev->bd_inode;
+	struct address_space *const mapping = bd_inode->i_mapping;
+	/* prefer retrying in the allocator to blindly looping below */
+	const gfp_t gfp = mapping_gfp_constraint(mapping, ~__GFP_FS) |
+		(nofail ? __GFP_NOFAIL : 0);
+	unsigned int io_retries = nofail ? EROFS_IO_MAX_RETRIES_NOFAIL : 0;
+	struct page *page;
+	int err;
+
+repeat:
+	page = find_or_create_page(mapping, blkaddr, gfp);
+	if (unlikely(!page)) {
+		DBG_BUGON(nofail);
+		return ERR_PTR(-ENOMEM);
+	}
+	DBG_BUGON(!PageLocked(page));
+
+	if (!PageUptodate(page)) {
+		struct bio *bio;
+
+		bio = erofs_grab_bio(sb, blkaddr, 1, sb, read_endio, nofail);
+		if (IS_ERR(bio)) {
+			DBG_BUGON(nofail);
+			err = PTR_ERR(bio);
+			goto err_out;
+		}
+
+		err = bio_add_page(bio, page, PAGE_SIZE, 0);
+		if (unlikely(err != PAGE_SIZE)) {
+			err = -EFAULT;
+			goto err_out;
+		}
+
+		__submit_bio(bio, REQ_OP_READ,
+			     REQ_META | (prio ? REQ_PRIO : 0));
+
+		lock_page(page);
+
+		/* this page has been truncated by others */
+		if (unlikely(page->mapping != mapping)) {
+unlock_repeat:
+			unlock_page(page);
+			put_page(page);
+			goto repeat;
+		}
+
+		/* more likely a read error */
+		if (unlikely(!PageUptodate(page))) {
+			if (io_retries) {
+				--io_retries;
+				goto unlock_repeat;
+			}
+			err = -EIO;
+			goto err_out;
+		}
+	}
+	return page;
+
+err_out:
+	unlock_page(page);
+	put_page(page);
+	return ERR_PTR(err);
+}
+
+static int erofs_map_blocks_flatmode(struct inode *inode,
+				     struct erofs_map_blocks *map,
+				     int flags)
+{
+	int err = 0;
+	erofs_blk_t nblocks, lastblk;
+	u64 offset = map->m_la;
+	struct erofs_vnode *vi = EROFS_V(inode);
+
+	trace_erofs_map_blocks_flatmode_enter(inode, map, flags);
+
+	nblocks = DIV_ROUND_UP(inode->i_size, PAGE_SIZE);
+	lastblk = nblocks - is_inode_flat_inline(inode);
+
+	if (unlikely(offset >= inode->i_size)) {
+		/* leave out-of-bound access unmapped */
+		map->m_flags = 0;
+		map->m_plen = 0;
+		goto out;
+	}
+
+	/* there is no hole in flatmode */
+	map->m_flags = EROFS_MAP_MAPPED;
+
+	if (offset < blknr_to_addr(lastblk)) {
+		map->m_pa = blknr_to_addr(vi->raw_blkaddr) + map->m_la;
+		map->m_plen = blknr_to_addr(lastblk) - offset;
+	} else if (is_inode_flat_inline(inode)) {
+		/* 2 - inode inline B: inode, [xattrs], inline last blk... */
+		struct erofs_sb_info *sbi = EROFS_SB(inode->i_sb);
+
+		map->m_pa = iloc(sbi, vi->nid) + vi->inode_isize +
+			vi->xattr_isize + erofs_blkoff(map->m_la);
+		map->m_plen = inode->i_size - offset;
+
+		/* inline data should be located in one meta block */
+		if (erofs_blkoff(map->m_pa) + map->m_plen > PAGE_SIZE) {
+			errln("inline data cross block boundary @ nid %llu",
+			      vi->nid);
+			DBG_BUGON(1);
+			err = -EFSCORRUPTED;
+			goto err_out;
+		}
+
+		map->m_flags |= EROFS_MAP_META;
+	} else {
+		errln("internal error @ nid: %llu (size %llu), m_la 0x%llx",
+		      vi->nid, inode->i_size, map->m_la);
+		DBG_BUGON(1);
+		err = -EIO;
+		goto err_out;
+	}
+
+out:
+	map->m_llen = map->m_plen;
+
+err_out:
+	trace_erofs_map_blocks_flatmode_exit(inode, map, flags, 0);
+	return err;
+}
+
+int erofs_map_blocks(struct inode *inode,
+		     struct erofs_map_blocks *map, int flags)
+{
+	if (unlikely(is_inode_layout_compression(inode))) {
+		int err = z_erofs_map_blocks_iter(inode, map, flags);
+
+		if (map->mpage) {
+			put_page(map->mpage);
+			map->mpage = NULL;
+		}
+		return err;
+	}
+	return erofs_map_blocks_flatmode(inode, map, flags);
+}
+
+static inline struct bio *erofs_read_raw_page(struct bio *bio,
+					      struct address_space *mapping,
+					      struct page *page,
+					      erofs_off_t *last_block,
+					      unsigned int nblocks,
+					      bool ra)
+{
+	struct inode *const inode = mapping->host;
+	struct super_block *const sb = inode->i_sb;
+	erofs_off_t current_block = (erofs_off_t)page->index;
+	int err;
+
+	DBG_BUGON(!nblocks);
+
+	if (PageUptodate(page)) {
+		err = 0;
+		goto has_updated;
+	}
+
+	/* note that for readpage case, bio also equals to NULL */
+	if (bio &&
+	    /* not continuous */
+	    *last_block + 1 != current_block) {
+submit_bio_retry:
+		__submit_bio(bio, REQ_OP_READ, 0);
+		bio = NULL;
+	}
+
+	if (!bio) {
+		struct erofs_map_blocks map = {
+			.m_la = blknr_to_addr(current_block),
+		};
+		erofs_blk_t blknr;
+		unsigned int blkoff;
+
+		err = erofs_map_blocks(inode, &map, EROFS_GET_BLOCKS_RAW);
+		if (unlikely(err))
+			goto err_out;
+
+		/* zero out the holed page */
+		if (unlikely(!(map.m_flags & EROFS_MAP_MAPPED))) {
+			zero_user_segment(page, 0, PAGE_SIZE);
+			SetPageUptodate(page);
+
+			/* imply err = 0, see erofs_map_blocks */
+			goto has_updated;
+		}
+
+		/* for RAW access mode, m_plen must be equal to m_llen */
+		DBG_BUGON(map.m_plen != map.m_llen);
+
+		blknr = erofs_blknr(map.m_pa);
+		blkoff = erofs_blkoff(map.m_pa);
+
+		/* deal with inline page */
+		if (map.m_flags & EROFS_MAP_META) {
+			void *vsrc, *vto;
+			struct page *ipage;
+
+			DBG_BUGON(map.m_plen > PAGE_SIZE);
+
+			ipage = erofs_get_meta_page(inode->i_sb, blknr, 0);
+
+			if (IS_ERR(ipage)) {
+				err = PTR_ERR(ipage);
+				goto err_out;
+			}
+
+			vsrc = kmap_atomic(ipage);
+			vto = kmap_atomic(page);
+			memcpy(vto, vsrc + blkoff, map.m_plen);
+			memset(vto + map.m_plen, 0, PAGE_SIZE - map.m_plen);
+			kunmap_atomic(vto);
+			kunmap_atomic(vsrc);
+			flush_dcache_page(page);
+
+			SetPageUptodate(page);
+			/* TODO: could we unlock the page earlier? */
+			unlock_page(ipage);
+			put_page(ipage);
+
+			/* imply err = 0, see erofs_map_blocks */
+			goto has_updated;
+		}
+
+		/* pa must be block-aligned for raw reading */
+		DBG_BUGON(erofs_blkoff(map.m_pa));
+
+		/* max # of continuous pages */
+		if (nblocks > DIV_ROUND_UP(map.m_plen, PAGE_SIZE))
+			nblocks = DIV_ROUND_UP(map.m_plen, PAGE_SIZE);
+		if (nblocks > BIO_MAX_PAGES)
+			nblocks = BIO_MAX_PAGES;
+
+		bio = erofs_grab_bio(sb, blknr, nblocks, sb,
+				     read_endio, false);
+		if (IS_ERR(bio)) {
+			err = PTR_ERR(bio);
+			bio = NULL;
+			goto err_out;
+		}
+	}
+
+	err = bio_add_page(bio, page, PAGE_SIZE, 0);
+	/* out of the extent or bio is full */
+	if (err < PAGE_SIZE)
+		goto submit_bio_retry;
+
+	*last_block = current_block;
+
+	/* shift in advance in case of it followed by too many gaps */
+	if (bio->bi_iter.bi_size >= bio->bi_max_vecs * PAGE_SIZE) {
+		/* err should reassign to 0 after submitting */
+		err = 0;
+		goto submit_bio_out;
+	}
+
+	return bio;
+
+err_out:
+	/* for sync reading, set page error immediately */
+	if (!ra) {
+		SetPageError(page);
+		ClearPageUptodate(page);
+	}
+has_updated:
+	unlock_page(page);
+
+	/* if updated manually, continuous pages has a gap */
+	if (bio)
+submit_bio_out:
+		__submit_bio(bio, REQ_OP_READ, 0);
+
+	return unlikely(err) ? ERR_PTR(err) : NULL;
+}
+
+/*
+ * since we dont have write or truncate flows, so no inode
+ * locking needs to be held at the moment.
+ */
+static int erofs_raw_access_readpage(struct file *file, struct page *page)
+{
+	erofs_off_t last_block;
+	struct bio *bio;
+
+	trace_erofs_readpage(page, true);
+
+	bio = erofs_read_raw_page(NULL, page->mapping,
+				  page, &last_block, 1, false);
+
+	if (IS_ERR(bio))
+		return PTR_ERR(bio);
+
+	DBG_BUGON(bio);	/* since we have only one bio -- must be NULL */
+	return 0;
+}
+
+static int erofs_raw_access_readpages(struct file *filp,
+				      struct address_space *mapping,
+				      struct list_head *pages,
+				      unsigned int nr_pages)
+{
+	erofs_off_t last_block;
+	struct bio *bio = NULL;
+	gfp_t gfp = readahead_gfp_mask(mapping);
+	struct page *page = list_last_entry(pages, struct page, lru);
+
+	trace_erofs_readpages(mapping->host, page, nr_pages, true);
+
+	for (; nr_pages; --nr_pages) {
+		page = list_entry(pages->prev, struct page, lru);
+
+		prefetchw(&page->flags);
+		list_del(&page->lru);
+
+		if (!add_to_page_cache_lru(page, mapping, page->index, gfp)) {
+			bio = erofs_read_raw_page(bio, mapping, page,
+						  &last_block, nr_pages, true);
+
+			/* all the page errors are ignored when readahead */
+			if (IS_ERR(bio)) {
+				pr_err("%s, readahead error at page %lu of nid %llu\n",
+				       __func__, page->index,
+				       EROFS_V(mapping->host)->nid);
+
+				bio = NULL;
+			}
+		}
+
+		/* pages could still be locked */
+		put_page(page);
+	}
+	DBG_BUGON(!list_empty(pages));
+
+	/* the rare case (end in gaps) */
+	if (unlikely(bio))
+		__submit_bio(bio, REQ_OP_READ, 0);
+	return 0;
+}
+
+static int erofs_get_block(struct inode *inode, sector_t iblock,
+			   struct buffer_head *bh, int create)
+{
+	struct erofs_map_blocks map = {
+		.m_la = iblock << 9,
+	};
+	int err;
+
+	err = erofs_map_blocks(inode, &map, EROFS_GET_BLOCKS_RAW);
+	if (err)
+		return err;
+
+	if (map.m_flags & EROFS_MAP_MAPPED)
+		bh->b_blocknr = erofs_blknr(map.m_pa);
+
+	return err;
+}
+
+static sector_t erofs_bmap(struct address_space *mapping, sector_t block)
+{
+	struct inode *inode = mapping->host;
+
+	if (is_inode_flat_inline(inode)) {
+		erofs_blk_t blks = i_size_read(inode) >> LOG_BLOCK_SIZE;
+
+		if (block >> LOG_SECTORS_PER_BLOCK >= blks)
+			return 0;
+	}
+
+	return generic_block_bmap(mapping, block, erofs_get_block);
+}
+
+/* for uncompressed (aligned) files and raw access for other files */
+const struct address_space_operations erofs_raw_access_aops = {
+	.readpage = erofs_raw_access_readpage,
+	.readpages = erofs_raw_access_readpages,
+	.bmap = erofs_bmap,
+};
+
