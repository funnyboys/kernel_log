commit 3c597282887fd55181578996dca52ce697d985a5
Author: Gao Xiang <hsiangkao@redhat.com>
Date:   Fri Jun 19 07:43:49 2020 +0800

    erofs: fix partially uninitialized misuse in z_erofs_onlinepage_fixup
    
    Hongyu reported "id != index" in z_erofs_onlinepage_fixup() with
    specific aarch64 environment easily, which wasn't shown before.
    
    After digging into that, I found that high 32 bits of page->private
    was set to 0xaaaaaaaa rather than 0 (due to z_erofs_onlinepage_init
    behavior with specific compiler options). Actually we only use low
    32 bits to keep the page information since page->private is only 4
    bytes on most 32-bit platforms. However z_erofs_onlinepage_fixup()
    uses the upper 32 bits by mistake.
    
    Let's fix it now.
    
    Reported-and-tested-by: Hongyu Jin <hongyu.jin@unisoc.com>
    Fixes: 3883a79abd02 ("staging: erofs: introduce VLE decompression support")
    Cc: <stable@vger.kernel.org> # 4.19+
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Link: https://lore.kernel.org/r/20200618234349.22553-1-hsiangkao@aol.com
    Signed-off-by: Gao Xiang <hsiangkao@redhat.com>

diff --git a/fs/erofs/zdata.h b/fs/erofs/zdata.h
index 7824f5563a55..9b66c28b3ae9 100644
--- a/fs/erofs/zdata.h
+++ b/fs/erofs/zdata.h
@@ -144,22 +144,22 @@ static inline void z_erofs_onlinepage_init(struct page *page)
 static inline void z_erofs_onlinepage_fixup(struct page *page,
 	uintptr_t index, bool down)
 {
-	unsigned long *p, o, v, id;
-repeat:
-	p = &page_private(page);
-	o = READ_ONCE(*p);
+	union z_erofs_onlinepage_converter u = { .v = &page_private(page) };
+	int orig, orig_index, val;
 
-	id = o >> Z_EROFS_ONLINEPAGE_INDEX_SHIFT;
-	if (id) {
+repeat:
+	orig = atomic_read(u.o);
+	orig_index = orig >> Z_EROFS_ONLINEPAGE_INDEX_SHIFT;
+	if (orig_index) {
 		if (!index)
 			return;
 
-		DBG_BUGON(id != index);
+		DBG_BUGON(orig_index != index);
 	}
 
-	v = (index << Z_EROFS_ONLINEPAGE_INDEX_SHIFT) |
-		((o & Z_EROFS_ONLINEPAGE_COUNT_MASK) + (unsigned int)down);
-	if (cmpxchg(p, o, v) != o)
+	val = (index << Z_EROFS_ONLINEPAGE_INDEX_SHIFT) |
+		((orig & Z_EROFS_ONLINEPAGE_COUNT_MASK) + (unsigned int)down);
+	if (atomic_cmpxchg(u.o, orig, val) != orig)
 		goto repeat;
 }
 

commit a4b1fab121ec3235e6c5f3543ae1937426bd4eb4
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Tue Oct 8 20:56:15 2019 +0800

    erofs: clean up decompress queue stuffs
    
    Previously, both z_erofs_unzip_io and z_erofs_unzip_io_sb
    record decompress queues for backend to use.
    
    The only difference is that z_erofs_unzip_io is used for
    on-stack sync decompression so that it doesn't have a super
    block field (since the caller can pass it in its context),
    but it increases complexity with only a pointer saving.
    
    Rename z_erofs_unzip_io to z_erofs_decompressqueue with
    a fixed super_block member and kill the other entirely,
    and it can fallback to sync decompression if memory
    allocation failure.
    
    Link: https://lore.kernel.org/r/20191008125616.183715-4-gaoxiang25@huawei.com
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>

diff --git a/fs/erofs/zdata.h b/fs/erofs/zdata.h
index faf950189bd7..7824f5563a55 100644
--- a/fs/erofs/zdata.h
+++ b/fs/erofs/zdata.h
@@ -84,7 +84,8 @@ struct z_erofs_pcluster {
 
 #define Z_EROFS_WORKGROUP_SIZE  sizeof(struct z_erofs_pcluster)
 
-struct z_erofs_unzip_io {
+struct z_erofs_decompressqueue {
+	struct super_block *sb;
 	atomic_t pending_bios;
 	z_erofs_next_pcluster_t head;
 
@@ -94,11 +95,6 @@ struct z_erofs_unzip_io {
 	} u;
 };
 
-struct z_erofs_unzip_io_sb {
-	struct z_erofs_unzip_io io;
-	struct super_block *sb;
-};
-
 #define MNGD_MAPPING(sbi)	((sbi)->managed_cache->i_mapping)
 static inline bool erofs_page_is_managed(const struct erofs_sb_info *sbi,
 					 struct page *page)

commit 4f761fa253b49f657de7ef6f695a124e08e56c3a
Author: Gao Xiang <gaoxiang25@huawei.com>
Date:   Wed Sep 4 10:09:09 2019 +0800

    erofs: rename errln/infoln/debugln to erofs_{err, info, dbg}
    
    Add prefix "erofs_" to these functions and print
    sb->s_id as a prefix to erofs_{err, info} so that
    the user knows which file system is affected.
    
    Reported-by: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190904020912.63925-23-gaoxiang25@huawei.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/zdata.h b/fs/erofs/zdata.h
index 4fc547bc01f9..faf950189bd7 100644
--- a/fs/erofs/zdata.h
+++ b/fs/erofs/zdata.h
@@ -182,7 +182,7 @@ static inline void z_erofs_onlinepage_endio(struct page *page)
 			SetPageUptodate(page);
 		unlock_page(page);
 	}
-	debugln("%s, page %p value %x", __func__, page, atomic_read(u.o));
+	erofs_dbg("%s, page %p value %x", __func__, page, atomic_read(u.o));
 }
 
 #define Z_EROFS_VMAP_ONSTACK_PAGES	\

commit 47e4937a4a7ca4184fd282791dfee76c6799966a
Author: Gao Xiang <hsiangkao@aol.com>
Date:   Fri Aug 23 05:36:59 2019 +0800

    erofs: move erofs out of staging
    
    EROFS filesystem has been merged into linux-staging for a year.
    
    EROFS is designed to be a better solution of saving extra storage
    space with guaranteed end-to-end performance for read-only files
    with the help of reduced metadata, fixed-sized output compression
    and decompression inplace technologies.
    
    In the past year, EROFS was greatly improved by many people as
    a staging driver, self-tested, betaed by a large number of our
    internal users, successfully applied to almost all in-service
    HUAWEI smartphones as the part of EMUI 9.1 and proven to be stable
    enough to be moved out of staging.
    
    EROFS is a self-contained filesystem driver. Although there are
    still some TODOs to be more generic, we have a dedicated team
    actively keeping on working on EROFS in order to make it better
    with the evolution of Linux kernel as the other in-kernel filesystems.
    
    As Pavel suggested, it's better to do as one commit since git
    can do moves and all histories will be saved in this way.
    
    Let's promote it from staging and enhance it more actively as
    a "real" part of kernel for more wider scenarios!
    
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Pavel Machek <pavel@denx.de>
    Cc: David Sterba <dsterba@suse.cz>
    Cc: Amir Goldstein <amir73il@gmail.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Darrick J . Wong <darrick.wong@oracle.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Chao Yu <yuchao0@huawei.com>
    Cc: Miao Xie <miaoxie@huawei.com>
    Cc: Li Guifu <bluce.liguifu@huawei.com>
    Cc: Fang Wei <fangwei1@huawei.com>
    Signed-off-by: Gao Xiang <gaoxiang25@huawei.com>
    Link: https://lore.kernel.org/r/20190822213659.5501-1-hsiangkao@aol.com
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/erofs/zdata.h b/fs/erofs/zdata.h
new file mode 100644
index 000000000000..4fc547bc01f9
--- /dev/null
+++ b/fs/erofs/zdata.h
@@ -0,0 +1,193 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Copyright (C) 2018 HUAWEI, Inc.
+ *             http://www.huawei.com/
+ * Created by Gao Xiang <gaoxiang25@huawei.com>
+ */
+#ifndef __EROFS_FS_ZDATA_H
+#define __EROFS_FS_ZDATA_H
+
+#include "internal.h"
+#include "zpvec.h"
+
+#define Z_EROFS_NR_INLINE_PAGEVECS      3
+
+/*
+ * Structure fields follow one of the following exclusion rules.
+ *
+ * I: Modifiable by initialization/destruction paths and read-only
+ *    for everyone else;
+ *
+ * L: Field should be protected by pageset lock;
+ *
+ * A: Field should be accessed / updated in atomic for parallelized code.
+ */
+struct z_erofs_collection {
+	struct mutex lock;
+
+	/* I: page offset of start position of decompression */
+	unsigned short pageofs;
+
+	/* L: maximum relative page index in pagevec[] */
+	unsigned short nr_pages;
+
+	/* L: total number of pages in pagevec[] */
+	unsigned int vcnt;
+
+	union {
+		/* L: inline a certain number of pagevecs for bootstrap */
+		erofs_vtptr_t pagevec[Z_EROFS_NR_INLINE_PAGEVECS];
+
+		/* I: can be used to free the pcluster by RCU. */
+		struct rcu_head rcu;
+	};
+};
+
+#define Z_EROFS_PCLUSTER_FULL_LENGTH    0x00000001
+#define Z_EROFS_PCLUSTER_LENGTH_BIT     1
+
+/*
+ * let's leave a type here in case of introducing
+ * another tagged pointer later.
+ */
+typedef void *z_erofs_next_pcluster_t;
+
+struct z_erofs_pcluster {
+	struct erofs_workgroup obj;
+	struct z_erofs_collection primary_collection;
+
+	/* A: point to next chained pcluster or TAILs */
+	z_erofs_next_pcluster_t next;
+
+	/* A: compressed pages (including multi-usage pages) */
+	struct page *compressed_pages[Z_EROFS_CLUSTER_MAX_PAGES];
+
+	/* A: lower limit of decompressed length and if full length or not */
+	unsigned int length;
+
+	/* I: compression algorithm format */
+	unsigned char algorithmformat;
+	/* I: bit shift of physical cluster size */
+	unsigned char clusterbits;
+};
+
+#define z_erofs_primarycollection(pcluster) (&(pcluster)->primary_collection)
+
+/* let's avoid the valid 32-bit kernel addresses */
+
+/* the chained workgroup has't submitted io (still open) */
+#define Z_EROFS_PCLUSTER_TAIL           ((void *)0x5F0ECAFE)
+/* the chained workgroup has already submitted io */
+#define Z_EROFS_PCLUSTER_TAIL_CLOSED    ((void *)0x5F0EDEAD)
+
+#define Z_EROFS_PCLUSTER_NIL            (NULL)
+
+#define Z_EROFS_WORKGROUP_SIZE  sizeof(struct z_erofs_pcluster)
+
+struct z_erofs_unzip_io {
+	atomic_t pending_bios;
+	z_erofs_next_pcluster_t head;
+
+	union {
+		wait_queue_head_t wait;
+		struct work_struct work;
+	} u;
+};
+
+struct z_erofs_unzip_io_sb {
+	struct z_erofs_unzip_io io;
+	struct super_block *sb;
+};
+
+#define MNGD_MAPPING(sbi)	((sbi)->managed_cache->i_mapping)
+static inline bool erofs_page_is_managed(const struct erofs_sb_info *sbi,
+					 struct page *page)
+{
+	return page->mapping == MNGD_MAPPING(sbi);
+}
+
+#define Z_EROFS_ONLINEPAGE_COUNT_BITS   2
+#define Z_EROFS_ONLINEPAGE_COUNT_MASK   ((1 << Z_EROFS_ONLINEPAGE_COUNT_BITS) - 1)
+#define Z_EROFS_ONLINEPAGE_INDEX_SHIFT  (Z_EROFS_ONLINEPAGE_COUNT_BITS)
+
+/*
+ * waiters (aka. ongoing_packs): # to unlock the page
+ * sub-index: 0 - for partial page, >= 1 full page sub-index
+ */
+typedef atomic_t z_erofs_onlinepage_t;
+
+/* type punning */
+union z_erofs_onlinepage_converter {
+	z_erofs_onlinepage_t *o;
+	unsigned long *v;
+};
+
+static inline unsigned int z_erofs_onlinepage_index(struct page *page)
+{
+	union z_erofs_onlinepage_converter u;
+
+	DBG_BUGON(!PagePrivate(page));
+	u.v = &page_private(page);
+
+	return atomic_read(u.o) >> Z_EROFS_ONLINEPAGE_INDEX_SHIFT;
+}
+
+static inline void z_erofs_onlinepage_init(struct page *page)
+{
+	union {
+		z_erofs_onlinepage_t o;
+		unsigned long v;
+	/* keep from being unlocked in advance */
+	} u = { .o = ATOMIC_INIT(1) };
+
+	set_page_private(page, u.v);
+	smp_wmb();
+	SetPagePrivate(page);
+}
+
+static inline void z_erofs_onlinepage_fixup(struct page *page,
+	uintptr_t index, bool down)
+{
+	unsigned long *p, o, v, id;
+repeat:
+	p = &page_private(page);
+	o = READ_ONCE(*p);
+
+	id = o >> Z_EROFS_ONLINEPAGE_INDEX_SHIFT;
+	if (id) {
+		if (!index)
+			return;
+
+		DBG_BUGON(id != index);
+	}
+
+	v = (index << Z_EROFS_ONLINEPAGE_INDEX_SHIFT) |
+		((o & Z_EROFS_ONLINEPAGE_COUNT_MASK) + (unsigned int)down);
+	if (cmpxchg(p, o, v) != o)
+		goto repeat;
+}
+
+static inline void z_erofs_onlinepage_endio(struct page *page)
+{
+	union z_erofs_onlinepage_converter u;
+	unsigned int v;
+
+	DBG_BUGON(!PagePrivate(page));
+	u.v = &page_private(page);
+
+	v = atomic_dec_return(u.o);
+	if (!(v & Z_EROFS_ONLINEPAGE_COUNT_MASK)) {
+		ClearPagePrivate(page);
+		if (!PageError(page))
+			SetPageUptodate(page);
+		unlock_page(page);
+	}
+	debugln("%s, page %p value %x", __func__, page, atomic_read(u.o));
+}
+
+#define Z_EROFS_VMAP_ONSTACK_PAGES	\
+	min_t(unsigned int, THREAD_SIZE / 8 / sizeof(struct page *), 96U)
+#define Z_EROFS_VMAP_GLOBAL_PAGES	2048
+
+#endif
+
