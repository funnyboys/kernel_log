commit 6dcde60efd946e38fac8d276a6ca47492103e856
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Tue May 26 09:33:11 2020 -0700

    xfs: more lockdep whackamole with kmem_alloc*
    
    Dave Airlie reported the following lockdep complaint:
    
    >  ======================================================
    >  WARNING: possible circular locking dependency detected
    >  5.7.0-0.rc5.20200515git1ae7efb38854.1.fc33.x86_64 #1 Not tainted
    >  ------------------------------------------------------
    >  kswapd0/159 is trying to acquire lock:
    >  ffff9b38d01a4470 (&xfs_nondir_ilock_class){++++}-{3:3},
    >  at: xfs_ilock+0xde/0x2c0 [xfs]
    >
    >  but task is already holding lock:
    >  ffffffffbbb8bd00 (fs_reclaim){+.+.}-{0:0}, at:
    >  __fs_reclaim_acquire+0x5/0x30
    >
    >  which lock already depends on the new lock.
    >
    >
    >  the existing dependency chain (in reverse order) is:
    >
    >  -> #1 (fs_reclaim){+.+.}-{0:0}:
    >         fs_reclaim_acquire+0x34/0x40
    >         __kmalloc+0x4f/0x270
    >         kmem_alloc+0x93/0x1d0 [xfs]
    >         kmem_alloc_large+0x4c/0x130 [xfs]
    >         xfs_attr_copy_value+0x74/0xa0 [xfs]
    >         xfs_attr_get+0x9d/0xc0 [xfs]
    >         xfs_get_acl+0xb6/0x200 [xfs]
    >         get_acl+0x81/0x160
    >         posix_acl_xattr_get+0x3f/0xd0
    >         vfs_getxattr+0x148/0x170
    >         getxattr+0xa7/0x240
    >         path_getxattr+0x52/0x80
    >         do_syscall_64+0x5c/0xa0
    >         entry_SYSCALL_64_after_hwframe+0x49/0xb3
    >
    >  -> #0 (&xfs_nondir_ilock_class){++++}-{3:3}:
    >         __lock_acquire+0x1257/0x20d0
    >         lock_acquire+0xb0/0x310
    >         down_write_nested+0x49/0x120
    >         xfs_ilock+0xde/0x2c0 [xfs]
    >         xfs_reclaim_inode+0x3f/0x400 [xfs]
    >         xfs_reclaim_inodes_ag+0x20b/0x410 [xfs]
    >         xfs_reclaim_inodes_nr+0x31/0x40 [xfs]
    >         super_cache_scan+0x190/0x1e0
    >         do_shrink_slab+0x184/0x420
    >         shrink_slab+0x182/0x290
    >         shrink_node+0x174/0x680
    >         balance_pgdat+0x2d0/0x5f0
    >         kswapd+0x21f/0x510
    >         kthread+0x131/0x150
    >         ret_from_fork+0x3a/0x50
    >
    >  other info that might help us debug this:
    >
    >   Possible unsafe locking scenario:
    >
    >         CPU0                    CPU1
    >         ----                    ----
    >    lock(fs_reclaim);
    >                                 lock(&xfs_nondir_ilock_class);
    >                                 lock(fs_reclaim);
    >    lock(&xfs_nondir_ilock_class);
    >
    >   *** DEADLOCK ***
    >
    >  4 locks held by kswapd0/159:
    >   #0: ffffffffbbb8bd00 (fs_reclaim){+.+.}-{0:0}, at:
    >  __fs_reclaim_acquire+0x5/0x30
    >   #1: ffffffffbbb7cef8 (shrinker_rwsem){++++}-{3:3}, at:
    >  shrink_slab+0x115/0x290
    >   #2: ffff9b39f07a50e8
    >  (&type->s_umount_key#56){++++}-{3:3}, at: super_cache_scan+0x38/0x1e0
    >   #3: ffff9b39f077f258
    >  (&pag->pag_ici_reclaim_lock){+.+.}-{3:3}, at:
    >  xfs_reclaim_inodes_ag+0x82/0x410 [xfs]
    
    This is a known false positive because inodes cannot simultaneously be
    getting reclaimed and the target of a getxattr operation, but lockdep
    doesn't know that.  We can (selectively) shut up lockdep until either
    it gets smarter or we change inode reclaim not to require the ILOCK by
    applying a stupid GFP_NOLOCKDEP bandaid.
    
    Reported-by: Dave Airlie <airlied@gmail.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Tested-by: Dave Airlie <airlied@gmail.com>
    Reviewed-by: Brian Foster <bfoster@redhat.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index fc87ea9f6843..34cbcfde9228 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -19,6 +19,7 @@ typedef unsigned __bitwise xfs_km_flags_t;
 #define KM_NOFS		((__force xfs_km_flags_t)0x0004u)
 #define KM_MAYFAIL	((__force xfs_km_flags_t)0x0008u)
 #define KM_ZERO		((__force xfs_km_flags_t)0x0010u)
+#define KM_NOLOCKDEP	((__force xfs_km_flags_t)0x0020u)
 
 /*
  * We use a special process flag to avoid recursive callbacks into
@@ -30,7 +31,7 @@ kmem_flags_convert(xfs_km_flags_t flags)
 {
 	gfp_t	lflags;
 
-	BUG_ON(flags & ~(KM_NOFS|KM_MAYFAIL|KM_ZERO));
+	BUG_ON(flags & ~(KM_NOFS | KM_MAYFAIL | KM_ZERO | KM_NOLOCKDEP));
 
 	lflags = GFP_KERNEL | __GFP_NOWARN;
 	if (flags & KM_NOFS)
@@ -49,6 +50,9 @@ kmem_flags_convert(xfs_km_flags_t flags)
 	if (flags & KM_ZERO)
 		lflags |= __GFP_ZERO;
 
+	if (flags & KM_NOLOCKDEP)
+		lflags |= __GFP_NOLOCKDEP;
+
 	return lflags;
 }
 

commit 508578f2f5601816ea29bec5cda00ea7d95a856d
Author: Nishad Kamdar <nishadkamdar@gmail.com>
Date:   Tue May 12 16:54:17 2020 -0700

    xfs: Use the correct style for SPDX License Identifier
    
    This patch corrects the SPDX License Identifier style in header files
    related to XFS File System support. For C header files
    Documentation/process/license-rules.rst mandates C-like comments.
    (opposed to C source files where C++ style should be used).
    
    Changes made by using a script provided by Joe Perches here:
    https://lkml.org/lkml/2019/2/7/46.
    
    Suggested-by: Joe Perches <joe@perches.com>
    Signed-off-by: Nishad Kamdar <nishadkamdar@gmail.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 6143117770e9..fc87ea9f6843 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -1,4 +1,4 @@
-// SPDX-License-Identifier: GPL-2.0
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * Copyright (c) 2000-2005 Silicon Graphics, Inc.
  * All Rights Reserved.

commit 377bcd5f3b7f46f50fdad1fed639c07f8c9f68cb
Author: Carlos Maiolino <cmaiolino@redhat.com>
Date:   Thu Nov 14 12:43:04 2019 -0800

    xfs: Remove kmem_zone_free() wrapper
    
    We can remove it now, without needing to rework the KM_ flags.
    
    Use kmem_cache_free() directly.
    
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Carlos Maiolino <cmaiolino@redhat.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 70ed74c7f37e..6143117770e9 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -81,12 +81,6 @@ kmem_zalloc_large(size_t size, xfs_km_flags_t flags)
 #define kmem_zone	kmem_cache
 #define kmem_zone_t	struct kmem_cache
 
-static inline void
-kmem_zone_free(kmem_zone_t *zone, void *ptr)
-{
-	kmem_cache_free(zone, ptr);
-}
-
 extern void *kmem_zone_alloc(kmem_zone_t *, xfs_km_flags_t);
 
 static inline void *

commit aaf54eb8bc15de293b0fccf3be19100793b8ba67
Author: Carlos Maiolino <cmaiolino@redhat.com>
Date:   Thu Nov 14 12:43:04 2019 -0800

    xfs: Remove kmem_zone_destroy() wrapper
    
    Use kmem_cache_destroy directly
    
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Carlos Maiolino <cmaiolino@redhat.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 15c5800128b3..70ed74c7f37e 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -87,12 +87,6 @@ kmem_zone_free(kmem_zone_t *zone, void *ptr)
 	kmem_cache_free(zone, ptr);
 }
 
-static inline void
-kmem_zone_destroy(kmem_zone_t *zone)
-{
-	kmem_cache_destroy(zone);
-}
-
 extern void *kmem_zone_alloc(kmem_zone_t *, xfs_km_flags_t);
 
 static inline void *

commit b1231760e44324d4cdb1b02116670c1ad2126e54
Author: Carlos Maiolino <cmaiolino@redhat.com>
Date:   Thu Nov 14 12:43:03 2019 -0800

    xfs: Remove slab init wrappers
    
    Remove kmem_zone_init() and kmem_zone_init_flags() together with their
    specific KM_* to SLAB_* flag wrappers.
    
    Use kmem_cache_create() directly.
    
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Carlos Maiolino <cmaiolino@redhat.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 8170d95cf930..15c5800128b3 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -78,27 +78,9 @@ kmem_zalloc_large(size_t size, xfs_km_flags_t flags)
  * Zone interfaces
  */
 
-#define KM_ZONE_HWALIGN	SLAB_HWCACHE_ALIGN
-#define KM_ZONE_RECLAIM	SLAB_RECLAIM_ACCOUNT
-#define KM_ZONE_SPREAD	SLAB_MEM_SPREAD
-#define KM_ZONE_ACCOUNT	SLAB_ACCOUNT
-
 #define kmem_zone	kmem_cache
 #define kmem_zone_t	struct kmem_cache
 
-static inline kmem_zone_t *
-kmem_zone_init(int size, char *zone_name)
-{
-	return kmem_cache_create(zone_name, size, 0, 0, NULL);
-}
-
-static inline kmem_zone_t *
-kmem_zone_init_flags(int size, char *zone_name, slab_flags_t flags,
-		     void (*construct)(void *))
-{
-	return kmem_cache_create(zone_name, size, 0, flags, construct);
-}
-
 static inline void
 kmem_zone_free(kmem_zone_t *zone, void *ptr)
 {

commit f8f9ee479439c1be9e33c4404912a2a112c46200
Author: Dave Chinner <dchinner@redhat.com>
Date:   Mon Aug 26 12:08:39 2019 -0700

    xfs: add kmem_alloc_io()
    
    Memory we use to submit for IO needs strict alignment to the
    underlying driver contraints. Worst case, this is 512 bytes. Given
    that all allocations for IO are always a power of 2 multiple of 512
    bytes, the kernel heap provides natural alignment for objects of
    these sizes and that suffices.
    
    Until, of course, memory debugging of some kind is turned on (e.g.
    red zones, poisoning, KASAN) and then the alignment of the heap
    objects is thrown out the window. Then we get weird IO errors and
    data corruption problems because drivers don't validate alignment
    and do the wrong thing when passed unaligned memory buffers in bios.
    
    TO fix this, introduce kmem_alloc_io(), which will guaranteeat least
    512 byte alignment of buffers for IO, even if memory debugging
    options are turned on. It is assumed that the minimum allocation
    size will be 512 bytes, and that sizes will be power of 2 mulitples
    of 512 bytes.
    
    Use this everywhere we allocate buffers for IO.
    
    This no longer fails with log recovery errors when KASAN is enabled
    due to the brd driver not handling unaligned memory buffers:
    
    # mkfs.xfs -f /dev/ram0 ; mount /dev/ram0 /mnt/test
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index cb6fa7984ffa..8170d95cf930 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -53,6 +53,7 @@ kmem_flags_convert(xfs_km_flags_t flags)
 }
 
 extern void *kmem_alloc(size_t, xfs_km_flags_t);
+extern void *kmem_alloc_io(size_t size, int align_mask, xfs_km_flags_t flags);
 extern void *kmem_alloc_large(size_t size, xfs_km_flags_t);
 extern void *kmem_realloc(const void *, size_t, xfs_km_flags_t);
 static inline void  kmem_free(const void *ptr)

commit 707e0ddaf67e8942448ebdd16b523e409ebe40ce
Author: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
Date:   Mon Aug 26 12:06:22 2019 -0700

    fs: xfs: Remove KM_NOSLEEP and KM_SLEEP.
    
    Since no caller is using KM_NOSLEEP and no callee branches on KM_SLEEP,
    we can remove KM_NOSLEEP and replace KM_SLEEP with 0.
    
    Signed-off-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 267655acd426..cb6fa7984ffa 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -16,8 +16,6 @@
  */
 
 typedef unsigned __bitwise xfs_km_flags_t;
-#define KM_SLEEP	((__force xfs_km_flags_t)0x0001u)
-#define KM_NOSLEEP	((__force xfs_km_flags_t)0x0002u)
 #define KM_NOFS		((__force xfs_km_flags_t)0x0004u)
 #define KM_MAYFAIL	((__force xfs_km_flags_t)0x0008u)
 #define KM_ZERO		((__force xfs_km_flags_t)0x0010u)
@@ -32,15 +30,11 @@ kmem_flags_convert(xfs_km_flags_t flags)
 {
 	gfp_t	lflags;
 
-	BUG_ON(flags & ~(KM_SLEEP|KM_NOSLEEP|KM_NOFS|KM_MAYFAIL|KM_ZERO));
+	BUG_ON(flags & ~(KM_NOFS|KM_MAYFAIL|KM_ZERO));
 
-	if (flags & KM_NOSLEEP) {
-		lflags = GFP_ATOMIC | __GFP_NOWARN;
-	} else {
-		lflags = GFP_KERNEL | __GFP_NOWARN;
-		if (flags & KM_NOFS)
-			lflags &= ~__GFP_FS;
-	}
+	lflags = GFP_KERNEL | __GFP_NOWARN;
+	if (flags & KM_NOFS)
+		lflags &= ~__GFP_FS;
 
 	/*
 	 * Default page/slab allocator behavior is to retry for ever

commit 72945d86ddec143ee5064f36eae4b48b03ea1a3f
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Jun 28 19:27:19 2019 -0700

    xfs: make mem_to_page available outside of xfs_buf.c
    
    Rename the function to kmem_to_page and move it to kmem.h together
    with our kmem_large allocator that may either return kmalloced or
    vmalloc pages.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 8e6b3ba81c03..267655acd426 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -124,4 +124,12 @@ kmem_zone_zalloc(kmem_zone_t *zone, xfs_km_flags_t flags)
 	return kmem_zone_alloc(zone, flags | KM_ZERO);
 }
 
+static inline struct page *
+kmem_to_page(void *addr)
+{
+	if (is_vmalloc_addr(addr))
+		return vmalloc_to_page(addr);
+	return virt_to_page(addr);
+}
+
 #endif /* __XFS_SUPPORT_KMEM_H__ */

commit 0b61f8a4079d904b1b1d47946cca898313de8c26
Author: Dave Chinner <dchinner@redhat.com>
Date:   Tue Jun 5 19:42:14 2018 -0700

    xfs: convert to SPDX license tags
    
    Remove the verbose license text from XFS files and replace them
    with SPDX tags. This does not change the license of any of the code,
    merely refers to the common, up-to-date license files in LICENSES/
    
    This change was mostly scripted. fs/xfs/Makefile and
    fs/xfs/libxfs/xfs_fs.h were modified by hand, the rest were detected
    and modified by the following command:
    
    for f in `git grep -l "GNU General" fs/xfs/` ; do
            echo $f
            cat $f | awk -f hdr.awk > $f.new
            mv -f $f.new $f
    done
    
    And the hdr.awk script that did the modification (including
    detecting the difference between GPL-2.0 and GPL-2.0+ licenses)
    is as follows:
    
    $ cat hdr.awk
    BEGIN {
            hdr = 1.0
            tag = "GPL-2.0"
            str = ""
    }
    
    /^ \* This program is free software/ {
            hdr = 2.0;
            next
    }
    
    /any later version./ {
            tag = "GPL-2.0+"
            next
    }
    
    /^ \*\// {
            if (hdr > 0.0) {
                    print "// SPDX-License-Identifier: " tag
                    print str
                    print $0
                    str=""
                    hdr = 0.0
                    next
            }
            print $0
            next
    }
    
    /^ \* / {
            if (hdr > 1.0)
                    next
            if (hdr > 0.0) {
                    if (str != "")
                            str = str "\n"
                    str = str $0
                    next
            }
            print $0
            next
    }
    
    /^ \*/ {
            if (hdr > 0.0)
                    next
            print $0
            next
    }
    
    // {
            if (hdr > 0.0) {
                    if (str != "")
                            str = str "\n"
                    str = str $0
                    next
            }
            print $0
    }
    
    END { }
    $
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 6023b594ead7..8e6b3ba81c03 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -1,19 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (c) 2000-2005 Silicon Graphics, Inc.
  * All Rights Reserved.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it would be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write the Free Software Foundation,
- * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
  */
 #ifndef __XFS_SUPPORT_KMEM_H__
 #define __XFS_SUPPORT_KMEM_H__

commit cb0a8d23024e7bd234dea4d0fc5c4902a8dda766
Author: Dave Chinner <dchinner@redhat.com>
Date:   Tue Mar 6 17:03:28 2018 -0800

    xfs: fall back to vmalloc when allocation log vector buffers
    
    When using large directory blocks, we regularly see memory
    allocations of >64k being made for the shadow log vector buffer.
    When we are under memory pressure, kmalloc() may not be able to find
    contiguous memory chunks large enough to satisfy these allocations
    easily, and if memory is fragmented we can potentially stall here.
    
    TO avoid this problem, switch the log vector buffer allocation to
    use kmem_alloc_large(). This will allow failed allocations to fall
    back to vmalloc and so remove the dependency on large contiguous
    regions of memory being available. This should prevent slowdowns
    and potential stalls when memory is low and/or fragmented.
    
    Signed-Off-By: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 4b87472f35bc..6023b594ead7 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -71,7 +71,7 @@ kmem_flags_convert(xfs_km_flags_t flags)
 }
 
 extern void *kmem_alloc(size_t, xfs_km_flags_t);
-extern void *kmem_zalloc_large(size_t size, xfs_km_flags_t);
+extern void *kmem_alloc_large(size_t size, xfs_km_flags_t);
 extern void *kmem_realloc(const void *, size_t, xfs_km_flags_t);
 static inline void  kmem_free(const void *ptr)
 {
@@ -85,6 +85,12 @@ kmem_zalloc(size_t size, xfs_km_flags_t flags)
 	return kmem_alloc(size, flags | KM_ZERO);
 }
 
+static inline void *
+kmem_zalloc_large(size_t size, xfs_km_flags_t flags)
+{
+	return kmem_alloc_large(size, flags | KM_ZERO);
+}
+
 /*
  * Zone interfaces
  */

commit d50112edde1d0c621520e53747044009f11c656b
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Nov 15 17:32:18 2017 -0800

    slab, slub, slob: add slab_flags_t
    
    Add sparse-checked slab_flags_t for struct kmem_cache::flags (SLAB_POISON,
    etc).
    
    SLAB is bloated temporarily by switching to "unsigned long", but only
    temporarily.
    
    Link: http://lkml.kernel.org/r/20171021100225.GA22428@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Acked-by: Pekka Enberg <penberg@kernel.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 758f37ac5ad3..4b87472f35bc 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -104,7 +104,7 @@ kmem_zone_init(int size, char *zone_name)
 }
 
 static inline kmem_zone_t *
-kmem_zone_init_flags(int size, char *zone_name, unsigned long flags,
+kmem_zone_init_flags(int size, char *zone_name, slab_flags_t flags,
 		     void (*construct)(void *))
 {
 	return kmem_cache_create(zone_name, size, 0, flags, construct);

commit 478f8da0f7c95f847a02f8e3b808926c894e3830
Author: Tim Hansen <devtimhansen@gmail.com>
Date:   Wed Nov 8 12:00:40 2017 -0800

    fs/xfs: Remove NULL check before kmem_cache_destroy
    
    kmem_cache_destroy already checks for null values.
    
    Signed-off-by: Tim Hansen <devtimhansen@gmail.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 4d85992d75b2..758f37ac5ad3 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -119,8 +119,7 @@ kmem_zone_free(kmem_zone_t *zone, void *ptr)
 static inline void
 kmem_zone_destroy(kmem_zone_t *zone)
 {
-	if (zone)
-		kmem_cache_destroy(zone);
+	kmem_cache_destroy(zone);
 }
 
 extern void *kmem_zone_alloc(kmem_zone_t *, xfs_km_flags_t);

commit 91c63ecda75d3004da8ffd5d2590383e0ba47722
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Jul 12 14:36:49 2017 -0700

    xfs: map KM_MAYFAIL to __GFP_RETRY_MAYFAIL
    
    KM_MAYFAIL didn't have any suitable GFP_FOO counterpart until recently
    so it relied on the default page allocator behavior for the given set of
    flags.  This means that small allocations actually never failed.
    
    Now that we have __GFP_RETRY_MAYFAIL flag which works independently on
    the allocation request size we can map KM_MAYFAIL to it.  The allocator
    will try as hard as it can to fulfill the request but fails eventually
    if the progress cannot be made.  It does so without triggering the OOM
    killer which can be seen as an improvement because KM_MAYFAIL users
    should be able to deal with allocation failures.
    
    Link: http://lkml.kernel.org/r/20170623085345.11304-4-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Alex Belits <alex.belits@cavium.com>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: NeilBrown <neilb@suse.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index d6ea520162b2..4d85992d75b2 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -54,6 +54,16 @@ kmem_flags_convert(xfs_km_flags_t flags)
 			lflags &= ~__GFP_FS;
 	}
 
+	/*
+	 * Default page/slab allocator behavior is to retry for ever
+	 * for small allocations. We can override this behavior by using
+	 * __GFP_RETRY_MAYFAIL which will tell the allocator to retry as long
+	 * as it is feasible but rather fail than retry forever for all
+	 * request sizes.
+	 */
+	if (flags & KM_MAYFAIL)
+		lflags |= __GFP_RETRY_MAYFAIL;
+
 	if (flags & KM_ZERO)
 		lflags |= __GFP_ZERO;
 

commit 7dea19f9ee636cb244109a4dba426bbb3e5304b7
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed May 3 14:53:15 2017 -0700

    mm: introduce memalloc_nofs_{save,restore} API
    
    GFP_NOFS context is used for the following 5 reasons currently:
    
     - to prevent from deadlocks when the lock held by the allocation
       context would be needed during the memory reclaim
    
     - to prevent from stack overflows during the reclaim because the
       allocation is performed from a deep context already
    
     - to prevent lockups when the allocation context depends on other
       reclaimers to make a forward progress indirectly
    
     - just in case because this would be safe from the fs POV
    
     - silence lockdep false positives
    
    Unfortunately overuse of this allocation context brings some problems to
    the MM.  Memory reclaim is much weaker (especially during heavy FS
    metadata workloads), OOM killer cannot be invoked because the MM layer
    doesn't have enough information about how much memory is freeable by the
    FS layer.
    
    In many cases it is far from clear why the weaker context is even used
    and so it might be used unnecessarily.  We would like to get rid of
    those as much as possible.  One way to do that is to use the flag in
    scopes rather than isolated cases.  Such a scope is declared when really
    necessary, tracked per task and all the allocation requests from within
    the context will simply inherit the GFP_NOFS semantic.
    
    Not only this is easier to understand and maintain because there are
    much less problematic contexts than specific allocation requests, this
    also helps code paths where FS layer interacts with other layers (e.g.
    crypto, security modules, MM etc...) and there is no easy way to convey
    the allocation context between the layers.
    
    Introduce memalloc_nofs_{save,restore} API to control the scope of
    GFP_NOFS allocation context.  This is basically copying
    memalloc_noio_{save,restore} API we have for other restricted allocation
    context GFP_NOIO.  The PF_MEMALLOC_NOFS flag already exists and it is
    just an alias for PF_FSTRANS which has been xfs specific until recently.
    There are no more PF_FSTRANS users anymore so let's just drop it.
    
    PF_MEMALLOC_NOFS is now checked in the MM layer and drops __GFP_FS
    implicitly same as PF_MEMALLOC_NOIO drops __GFP_IO.  memalloc_noio_flags
    is renamed to current_gfp_context because it now cares about both
    PF_MEMALLOC_NOFS and PF_MEMALLOC_NOIO contexts.  Xfs code paths preserve
    their semantic.  kmem_flags_convert() doesn't need to evaluate the flag
    anymore.
    
    This patch shouldn't introduce any functional changes.
    
    Let's hope that filesystems will drop direct GFP_NOFS (resp.  ~__GFP_FS)
    usage as much as possible and only use a properly documented
    memalloc_nofs_{save,restore} checkpoints where they are appropriate.
    
    [akpm@linux-foundation.org: fix comment typo, reflow comment]
    Link: http://lkml.kernel.org/r/20170306131408.9828-5-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Chris Mason <clm@fb.com>
    Cc: David Sterba <dsterba@suse.cz>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Brian Foster <bfoster@redhat.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Nikolay Borisov <nborisov@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index a6c8da40c70d..d6ea520162b2 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -50,7 +50,7 @@ kmem_flags_convert(xfs_km_flags_t flags)
 		lflags = GFP_ATOMIC | __GFP_NOWARN;
 	} else {
 		lflags = GFP_KERNEL | __GFP_NOWARN;
-		if ((current->flags & PF_MEMALLOC_NOFS) || (flags & KM_NOFS))
+		if (flags & KM_NOFS)
 			lflags &= ~__GFP_FS;
 	}
 

commit 9070733b4efac4bf17f299a81b01c15e206f9ff5
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed May 3 14:53:12 2017 -0700

    xfs: abstract PF_FSTRANS to PF_MEMALLOC_NOFS
    
    xfs has defined PF_FSTRANS to declare a scope GFP_NOFS semantic quite
    some time ago.  We would like to make this concept more generic and use
    it for other filesystems as well.  Let's start by giving the flag a more
    generic name PF_MEMALLOC_NOFS which is in line with an exiting
    PF_MEMALLOC_NOIO already used for the same purpose for GFP_NOIO
    contexts.  Replace all PF_FSTRANS usage from the xfs code in the first
    step before we introduce a full API for it as xfs uses the flag directly
    anyway.
    
    This patch doesn't introduce any functional change.
    
    Link: http://lkml.kernel.org/r/20170306131408.9828-4-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Brian Foster <bfoster@redhat.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Chris Mason <clm@fb.com>
    Cc: David Sterba <dsterba@suse.cz>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Nikolay Borisov <nborisov@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index f0fc84fcaac2..a6c8da40c70d 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -50,7 +50,7 @@ kmem_flags_convert(xfs_km_flags_t flags)
 		lflags = GFP_ATOMIC | __GFP_NOWARN;
 	} else {
 		lflags = GFP_KERNEL | __GFP_NOWARN;
-		if ((current->flags & PF_FSTRANS) || (flags & KM_NOFS))
+		if ((current->flags & PF_MEMALLOC_NOFS) || (flags & KM_NOFS))
 			lflags &= ~__GFP_FS;
 	}
 

commit 08b005f1333154ae5b404ca28766e0ffb9f1c150
Author: Darrick J. Wong <darrick.wong@oracle.com>
Date:   Mon Mar 6 11:58:20 2017 -0800

    xfs: remove kmem_zalloc_greedy
    
    The sole remaining caller of kmem_zalloc_greedy is bulkstat, which uses
    it to grab 1-4 pages for staging of inobt records.  The infinite loop in
    the greedy allocation function is causing hangs[1] in generic/269, so
    just get rid of the greedy allocator in favor of kmem_zalloc_large.
    This makes bulkstat somewhat more likely to ENOMEM if there's really no
    pages to spare, but eliminates a source of hangs.
    
    [1] http://lkml.kernel.org/r/20170301044634.rgidgdqqiiwsmfpj%40XZHOUW.usersys.redhat.com
    
    Signed-off-by: Darrick J. Wong <darrick.wong@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    ---
    v2: remove single-page fallback

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 689f746224e7..f0fc84fcaac2 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -69,8 +69,6 @@ static inline void  kmem_free(const void *ptr)
 }
 
 
-extern void *kmem_zalloc_greedy(size_t *, size_t, size_t);
-
 static inline void *
 kmem_zalloc(size_t size, xfs_km_flags_t flags)
 {

commit 664b60f6babc98ee03c2ff15b9482cc8c5e15a83
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Apr 6 09:47:01 2016 +1000

    xfs: improve kmem_realloc
    
    Use krealloc to implement our realloc function.  This helps to avoid
    new allocations if we are still in the slab bucket.  At least for the
    bmap btree root that's actually the common case.
    
    This also allows removing the now unused oldsize argument.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Brian Foster <bfoster@redhat.com>
    Signed-off-by: Dave Chinner <david@fromorbit.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index d1c66e465ca5..689f746224e7 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -62,7 +62,7 @@ kmem_flags_convert(xfs_km_flags_t flags)
 
 extern void *kmem_alloc(size_t, xfs_km_flags_t);
 extern void *kmem_zalloc_large(size_t size, xfs_km_flags_t);
-extern void *kmem_realloc(const void *, size_t, size_t, xfs_km_flags_t);
+extern void *kmem_realloc(const void *, size_t, xfs_km_flags_t);
 static inline void  kmem_free(const void *ptr)
 {
 	kvfree(ptr);

commit 5d097056c9a017a3b720849efb5432f37acabbac
Author: Vladimir Davydov <vdavydov@virtuozzo.com>
Date:   Thu Jan 14 15:18:21 2016 -0800

    kmemcg: account certain kmem allocations to memcg
    
    Mark those kmem allocations that are known to be easily triggered from
    userspace as __GFP_ACCOUNT/SLAB_ACCOUNT, which makes them accounted to
    memcg.  For the list, see below:
    
     - threadinfo
     - task_struct
     - task_delay_info
     - pid
     - cred
     - mm_struct
     - vm_area_struct and vm_region (nommu)
     - anon_vma and anon_vma_chain
     - signal_struct
     - sighand_struct
     - fs_struct
     - files_struct
     - fdtable and fdtable->full_fds_bits
     - dentry and external_name
     - inode for all filesystems. This is the most tedious part, because
       most filesystems overwrite the alloc_inode method.
    
    The list is far from complete, so feel free to add more objects.
    Nevertheless, it should be close to "account everything" approach and
    keep most workloads within bounds.  Malevolent users will be able to
    breach the limit, but this was possible even with the former "account
    everything" approach (simply because it did not account everything in
    fact).
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Vladimir Davydov <vdavydov@virtuozzo.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Greg Thelen <gthelen@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index cc6b768fc068..d1c66e465ca5 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -84,6 +84,7 @@ kmem_zalloc(size_t size, xfs_km_flags_t flags)
 #define KM_ZONE_HWALIGN	SLAB_HWCACHE_ALIGN
 #define KM_ZONE_RECLAIM	SLAB_RECLAIM_ACCOUNT
 #define KM_ZONE_SPREAD	SLAB_MEM_SPREAD
+#define KM_ZONE_ACCOUNT	SLAB_ACCOUNT
 
 #define kmem_zone	kmem_cache
 #define kmem_zone_t	struct kmem_cache

commit f3d215526e6955028dfbbfd446db8716275fb0c7
Author: Wang, Yalin <Yalin.Wang@sonymobile.com>
Date:   Mon Feb 2 09:54:18 2015 +1100

    xfs: change kmem_free to use generic kvfree()
    
    Change kmem_free to use kvfree() generic function, remove the
    duplicated code.
    
    Signed-off-by: Yalin Wang <yalin.wang@sonymobile.com>
    Reviewed-by: Brian Foster <bfoster@redhat.com>
    Signed-off-by: Dave Chinner <david@fromorbit.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 64db0e53edea..cc6b768fc068 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -63,7 +63,10 @@ kmem_flags_convert(xfs_km_flags_t flags)
 extern void *kmem_alloc(size_t, xfs_km_flags_t);
 extern void *kmem_zalloc_large(size_t size, xfs_km_flags_t);
 extern void *kmem_realloc(const void *, size_t, size_t, xfs_km_flags_t);
-extern void  kmem_free(const void *);
+static inline void  kmem_free(const void *ptr)
+{
+	kvfree(ptr);
+}
 
 
 extern void *kmem_zalloc_greedy(size_t *, size_t, size_t);

commit 359d992bcd398273637cd9edde10afca953783c4
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Mon Nov 4 18:21:05 2013 +0800

    xfs: simplify kmem_{zone_}zalloc
    
    Introduce flag KM_ZERO which is used to alloc zeroed entry, and convert
    kmem_{zone_}zalloc to call kmem_{zone_}alloc() with KM_ZERO directly,
    in order to avoid the setting to zero step.
    And following Dave's suggestion, make kmem_{zone_}zalloc static inline
    into kmem.h as they're now just a simple wrapper.
    
    V2:
      Make kmem_{zone_}zalloc static inline into kmem.h as Dave suggested.
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Reviewed-by: Dave Chinner <dchinner@redhat.com>
    Signed-off-by: Ben Myers <bpm@sgi.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 3a7371cab508..64db0e53edea 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -32,6 +32,7 @@ typedef unsigned __bitwise xfs_km_flags_t;
 #define KM_NOSLEEP	((__force xfs_km_flags_t)0x0002u)
 #define KM_NOFS		((__force xfs_km_flags_t)0x0004u)
 #define KM_MAYFAIL	((__force xfs_km_flags_t)0x0008u)
+#define KM_ZERO		((__force xfs_km_flags_t)0x0010u)
 
 /*
  * We use a special process flag to avoid recursive callbacks into
@@ -43,7 +44,7 @@ kmem_flags_convert(xfs_km_flags_t flags)
 {
 	gfp_t	lflags;
 
-	BUG_ON(flags & ~(KM_SLEEP|KM_NOSLEEP|KM_NOFS|KM_MAYFAIL));
+	BUG_ON(flags & ~(KM_SLEEP|KM_NOSLEEP|KM_NOFS|KM_MAYFAIL|KM_ZERO));
 
 	if (flags & KM_NOSLEEP) {
 		lflags = GFP_ATOMIC | __GFP_NOWARN;
@@ -52,11 +53,14 @@ kmem_flags_convert(xfs_km_flags_t flags)
 		if ((current->flags & PF_FSTRANS) || (flags & KM_NOFS))
 			lflags &= ~__GFP_FS;
 	}
+
+	if (flags & KM_ZERO)
+		lflags |= __GFP_ZERO;
+
 	return lflags;
 }
 
 extern void *kmem_alloc(size_t, xfs_km_flags_t);
-extern void *kmem_zalloc(size_t, xfs_km_flags_t);
 extern void *kmem_zalloc_large(size_t size, xfs_km_flags_t);
 extern void *kmem_realloc(const void *, size_t, size_t, xfs_km_flags_t);
 extern void  kmem_free(const void *);
@@ -64,6 +68,12 @@ extern void  kmem_free(const void *);
 
 extern void *kmem_zalloc_greedy(size_t *, size_t, size_t);
 
+static inline void *
+kmem_zalloc(size_t size, xfs_km_flags_t flags)
+{
+	return kmem_alloc(size, flags | KM_ZERO);
+}
+
 /*
  * Zone interfaces
  */
@@ -102,6 +112,11 @@ kmem_zone_destroy(kmem_zone_t *zone)
 }
 
 extern void *kmem_zone_alloc(kmem_zone_t *, xfs_km_flags_t);
-extern void *kmem_zone_zalloc(kmem_zone_t *, xfs_km_flags_t);
+
+static inline void *
+kmem_zone_zalloc(kmem_zone_t *zone, xfs_km_flags_t flags)
+{
+	return kmem_zone_alloc(zone, flags | KM_ZERO);
+}
 
 #endif /* __XFS_SUPPORT_KMEM_H__ */

commit fdd3cceef46f2c18c618669cfae5c0f47d6982f9
Author: Dave Chinner <dchinner@redhat.com>
Date:   Mon Sep 2 20:53:00 2013 +1000

    xfs: factor all the kmalloc-or-vmalloc fallback allocations
    
    We have quite a few places now where we do:
    
            x = kmem_zalloc(large size)
            if (!x)
                    x = kmem_zalloc_large(large size)
    
    and do a similar dance when freeing the memory. kmem_free() already
    does the correct freeing dance, and kmem_zalloc_large() is only ever
    called in these constructs, so just factor it all into
    kmem_zalloc_large() and kmem_free().
    
    Signed-off-by: Dave Chinner <dchinner@redhat.com>
    Reviewed-by: Mark Tinguely <tinguely@sgi.com>
    Signed-off-by: Ben Myers <bpm@sgi.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index b2f2620f9a87..3a7371cab508 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -57,17 +57,10 @@ kmem_flags_convert(xfs_km_flags_t flags)
 
 extern void *kmem_alloc(size_t, xfs_km_flags_t);
 extern void *kmem_zalloc(size_t, xfs_km_flags_t);
+extern void *kmem_zalloc_large(size_t size, xfs_km_flags_t);
 extern void *kmem_realloc(const void *, size_t, size_t, xfs_km_flags_t);
 extern void  kmem_free(const void *);
 
-static inline void *kmem_zalloc_large(size_t size)
-{
-	return vzalloc(size);
-}
-static inline void kmem_free_large(void *ptr)
-{
-	vfree(ptr);
-}
 
 extern void *kmem_zalloc_greedy(size_t *, size_t, size_t);
 

commit 77ba78776e90e8de541f13b326e284c74286252f
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Apr 2 06:24:04 2012 -0400

    xfs: switch to proper __bitwise type for KM_... flags
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index ab7c53fe346e..b2f2620f9a87 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -27,10 +27,11 @@
  * General memory allocation interfaces
  */
 
-#define KM_SLEEP	0x0001u
-#define KM_NOSLEEP	0x0002u
-#define KM_NOFS		0x0004u
-#define KM_MAYFAIL	0x0008u
+typedef unsigned __bitwise xfs_km_flags_t;
+#define KM_SLEEP	((__force xfs_km_flags_t)0x0001u)
+#define KM_NOSLEEP	((__force xfs_km_flags_t)0x0002u)
+#define KM_NOFS		((__force xfs_km_flags_t)0x0004u)
+#define KM_MAYFAIL	((__force xfs_km_flags_t)0x0008u)
 
 /*
  * We use a special process flag to avoid recursive callbacks into
@@ -38,7 +39,7 @@
  * warnings, so we explicitly skip any generic ones (silly of us).
  */
 static inline gfp_t
-kmem_flags_convert(unsigned int __nocast flags)
+kmem_flags_convert(xfs_km_flags_t flags)
 {
 	gfp_t	lflags;
 
@@ -54,9 +55,9 @@ kmem_flags_convert(unsigned int __nocast flags)
 	return lflags;
 }
 
-extern void *kmem_alloc(size_t, unsigned int __nocast);
-extern void *kmem_zalloc(size_t, unsigned int __nocast);
-extern void *kmem_realloc(const void *, size_t, size_t, unsigned int __nocast);
+extern void *kmem_alloc(size_t, xfs_km_flags_t);
+extern void *kmem_zalloc(size_t, xfs_km_flags_t);
+extern void *kmem_realloc(const void *, size_t, size_t, xfs_km_flags_t);
 extern void  kmem_free(const void *);
 
 static inline void *kmem_zalloc_large(size_t size)
@@ -107,7 +108,7 @@ kmem_zone_destroy(kmem_zone_t *zone)
 		kmem_cache_destroy(zone);
 }
 
-extern void *kmem_zone_alloc(kmem_zone_t *, unsigned int __nocast);
-extern void *kmem_zone_zalloc(kmem_zone_t *, unsigned int __nocast);
+extern void *kmem_zone_alloc(kmem_zone_t *, xfs_km_flags_t);
+extern void *kmem_zone_zalloc(kmem_zone_t *, xfs_km_flags_t);
 
 #endif /* __XFS_SUPPORT_KMEM_H__ */

commit 04da0c8196ac0b12fb6b84f4b7a51ad2fa56d869
Author: Christoph Hellwig <hch@infradead.org>
Date:   Wed Feb 1 13:57:20 2012 +0000

    xfs: use a normal shrinker for the dquot freelist
    
    Stop reusing dquots from the freelist when allocating new ones directly, and
    implement a shrinker that actually follows the specifications for the
    interface.  The shrinker implementation is still highly suboptimal at this
    point, but we can gradually work on it.
    
    This also fixes an bug in the previous lock ordering, where we would take
    the hash and dqlist locks inside of the freelist lock against the normal
    lock ordering.  This is only solvable by introducing the dispose list,
    and thus not when using direct reclaim of unused dquots for new allocations.
    
    As a side-effect the quota upper bound and used to free ratio values in
    /proc/fs/xfs/xqm are set to 0 as these values don't make any sense in the
    new world order.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Ben Myers <bpm@sgi.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
index 292eff198030..ab7c53fe346e 100644
--- a/fs/xfs/kmem.h
+++ b/fs/xfs/kmem.h
@@ -110,10 +110,4 @@ kmem_zone_destroy(kmem_zone_t *zone)
 extern void *kmem_zone_alloc(kmem_zone_t *, unsigned int __nocast);
 extern void *kmem_zone_zalloc(kmem_zone_t *, unsigned int __nocast);
 
-static inline int
-kmem_shake_allow(gfp_t gfp_mask)
-{
-	return ((gfp_mask & __GFP_WAIT) && (gfp_mask & __GFP_FS));
-}
-
 #endif /* __XFS_SUPPORT_KMEM_H__ */

commit e060c38434b2caa78efe7cedaff4191040b65a15
Merge: 10e4ac572eef cc39c6a9bbde
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Thu Sep 15 15:08:05 2011 +0200

    Merge branch 'master' into for-next
    
    Fast-forward merge with Linus to be able to merge patches
    based on more recent version of the tree.

commit c59d87c460767bc35dafd490139d3cfe78fb8da4
Author: Christoph Hellwig <hch@infradead.org>
Date:   Fri Aug 12 16:21:35 2011 -0500

    xfs: remove subdirectories
    
    Use the move from Linux 2.6 to Linux 3.x as an excuse to kill the
    annoying subdirectories in the XFS source code.  Besides the large
    amount of file rename the only changes are to the Makefile, a few
    files including headers with the subdirectory prefix, and the binary
    sysctl compat code that includes a header under fs/xfs/ from
    kernel/.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Alex Elder <aelder@sgi.com>

diff --git a/fs/xfs/kmem.h b/fs/xfs/kmem.h
new file mode 100644
index 000000000000..f7c8f7a9ea6d
--- /dev/null
+++ b/fs/xfs/kmem.h
@@ -0,0 +1,124 @@
+/*
+ * Copyright (c) 2000-2005 Silicon Graphics, Inc.
+ * All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write the Free Software Foundation,
+ * Inc.,  51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
+ */
+#ifndef __XFS_SUPPORT_KMEM_H__
+#define __XFS_SUPPORT_KMEM_H__
+
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/vmalloc.h>
+
+/*
+ * General memory allocation interfaces
+ */
+
+#define KM_SLEEP	0x0001u
+#define KM_NOSLEEP	0x0002u
+#define KM_NOFS		0x0004u
+#define KM_MAYFAIL	0x0008u
+
+/*
+ * We use a special process flag to avoid recursive callbacks into
+ * the filesystem during transactions.  We will also issue our own
+ * warnings, so we explicitly skip any generic ones (silly of us).
+ */
+static inline gfp_t
+kmem_flags_convert(unsigned int __nocast flags)
+{
+	gfp_t	lflags;
+
+	BUG_ON(flags & ~(KM_SLEEP|KM_NOSLEEP|KM_NOFS|KM_MAYFAIL));
+
+	if (flags & KM_NOSLEEP) {
+		lflags = GFP_ATOMIC | __GFP_NOWARN;
+	} else {
+		lflags = GFP_KERNEL | __GFP_NOWARN;
+		if ((current->flags & PF_FSTRANS) || (flags & KM_NOFS))
+			lflags &= ~__GFP_FS;
+	}
+	return lflags;
+}
+
+extern void *kmem_alloc(size_t, unsigned int __nocast);
+extern void *kmem_zalloc(size_t, unsigned int __nocast);
+extern void *kmem_realloc(const void *, size_t, size_t, unsigned int __nocast);
+extern void  kmem_free(const void *);
+
+static inline void *kmem_zalloc_large(size_t size)
+{
+	void *ptr;
+
+	ptr = vmalloc(size);
+	if (ptr)
+		memset(ptr, 0, size);
+	return ptr;
+}
+static inline void kmem_free_large(void *ptr)
+{
+	vfree(ptr);
+}
+
+extern void *kmem_zalloc_greedy(size_t *, size_t, size_t);
+
+/*
+ * Zone interfaces
+ */
+
+#define KM_ZONE_HWALIGN	SLAB_HWCACHE_ALIGN
+#define KM_ZONE_RECLAIM	SLAB_RECLAIM_ACCOUNT
+#define KM_ZONE_SPREAD	SLAB_MEM_SPREAD
+
+#define kmem_zone	kmem_cache
+#define kmem_zone_t	struct kmem_cache
+
+static inline kmem_zone_t *
+kmem_zone_init(int size, char *zone_name)
+{
+	return kmem_cache_create(zone_name, size, 0, 0, NULL);
+}
+
+static inline kmem_zone_t *
+kmem_zone_init_flags(int size, char *zone_name, unsigned long flags,
+		     void (*construct)(void *))
+{
+	return kmem_cache_create(zone_name, size, 0, flags, construct);
+}
+
+static inline void
+kmem_zone_free(kmem_zone_t *zone, void *ptr)
+{
+	kmem_cache_free(zone, ptr);
+}
+
+static inline void
+kmem_zone_destroy(kmem_zone_t *zone)
+{
+	if (zone)
+		kmem_cache_destroy(zone);
+}
+
+extern void *kmem_zone_alloc(kmem_zone_t *, unsigned int __nocast);
+extern void *kmem_zone_zalloc(kmem_zone_t *, unsigned int __nocast);
+
+static inline int
+kmem_shake_allow(gfp_t gfp_mask)
+{
+	return ((gfp_mask & __GFP_WAIT) && (gfp_mask & __GFP_FS));
+}
+
+#endif /* __XFS_SUPPORT_KMEM_H__ */
