commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 111349f67d98..26af6fdf1538 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -1,12 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /* Cache page management and data I/O routines
  *
  * Copyright (C) 2004-2008 Red Hat, Inc. All Rights Reserved.
  * Written by David Howells (dhowells@redhat.com)
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #define FSCACHE_DEBUG_LEVEL PAGE

commit ee1235a9a06813429c201bf186397a6feeea07bf
Author: David Howells <dhowells@redhat.com>
Date:   Wed Apr 4 13:41:28 2018 +0100

    fscache: Pass object size in rather than calling back for it
    
    Pass the object size in to fscache_acquire_cookie() and
    fscache_write_page() rather than the netfs providing a callback by which it
    can be received.  This makes it easier to update the size of the object
    when a new page is written that extends the object.
    
    The current object size is also passed by fscache to the check_aux
    function, obviating the need to store it in the aux data.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Anna Schumaker <anna.schumaker@netapp.com>
    Tested-by: Steve Dickson <steved@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 810b33aced1c..111349f67d98 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -963,6 +963,7 @@ void fscache_invalidate_writes(struct fscache_cookie *cookie)
  */
 int __fscache_write_page(struct fscache_cookie *cookie,
 			 struct page *page,
+			 loff_t object_size,
 			 gfp_t gfp)
 {
 	struct fscache_storage *op;
@@ -1014,6 +1015,10 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	/* add the page to the pending-storage radix tree on the backing
 	 * object */
 	spin_lock(&object->lock);
+
+	if (object->store_limit_l != object_size)
+		fscache_set_store_limit(object, object_size);
+
 	spin_lock(&cookie->stores_lock);
 
 	_debug("store limit %llx", (unsigned long long) object->store_limit);

commit 08c2e3d087840cd1e7141b62d92f3dc897147984
Author: David Howells <dhowells@redhat.com>
Date:   Wed Apr 4 13:41:27 2018 +0100

    fscache: Add more tracepoints
    
    Add more tracepoints to fscache, including:
    
     (*) fscache_page - Tracks netfs pages known to fscache.
    
     (*) fscache_check_page - Tracks the netfs querying whether a page is
         pending storage.
    
     (*) fscache_wake_cookie - Tracks cookies being woken up after a page
         completes/aborts storage in the cache.
    
     (*) fscache_op - Tracks operations being initialised.
    
     (*) fscache_wrote_page - Tracks return of the backend write_page op.
    
     (*) fscache_gang_lookup - Tracks lookup of pages to be stored in the write
         operation.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index b9f18bf4227d..810b33aced1c 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -27,6 +27,7 @@ bool __fscache_check_page_write(struct fscache_cookie *cookie, struct page *page
 	rcu_read_lock();
 	val = radix_tree_lookup(&cookie->stores, page->index);
 	rcu_read_unlock();
+	trace_fscache_check_page(cookie, page, val, 0);
 
 	return val != NULL;
 }
@@ -39,6 +40,8 @@ void __fscache_wait_on_page_write(struct fscache_cookie *cookie, struct page *pa
 {
 	wait_queue_head_t *wq = bit_waitqueue(&cookie->flags, 0);
 
+	trace_fscache_page(cookie, page, fscache_page_write_wait);
+
 	wait_event(*wq, !__fscache_check_page_write(cookie, page));
 }
 EXPORT_SYMBOL(__fscache_wait_on_page_write);
@@ -69,6 +72,8 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 
 	_enter("%p,%p,%x", cookie, page, gfp);
 
+	trace_fscache_page(cookie, page, fscache_page_maybe_release);
+
 try_again:
 	rcu_read_lock();
 	val = radix_tree_lookup(&cookie->stores, page->index);
@@ -101,6 +106,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 	}
 
 	xpage = radix_tree_delete(&cookie->stores, page->index);
+	trace_fscache_page(cookie, page, fscache_page_radix_delete);
 	spin_unlock(&cookie->stores_lock);
 
 	if (xpage) {
@@ -112,6 +118,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 	}
 
 	wake_up_bit(&cookie->flags, 0);
+	trace_fscache_wake_cookie(cookie);
 	if (xpage)
 		put_page(xpage);
 	__fscache_uncache_page(cookie, page);
@@ -144,7 +151,7 @@ static void fscache_end_page_write(struct fscache_object *object,
 				   struct page *page)
 {
 	struct fscache_cookie *cookie;
-	struct page *xpage = NULL;
+	struct page *xpage = NULL, *val;
 
 	spin_lock(&object->lock);
 	cookie = object->cookie;
@@ -154,13 +161,24 @@ static void fscache_end_page_write(struct fscache_object *object,
 		spin_lock(&cookie->stores_lock);
 		radix_tree_tag_clear(&cookie->stores, page->index,
 				     FSCACHE_COOKIE_STORING_TAG);
+		trace_fscache_page(cookie, page, fscache_page_radix_clear_store);
 		if (!radix_tree_tag_get(&cookie->stores, page->index,
 					FSCACHE_COOKIE_PENDING_TAG)) {
 			fscache_stat(&fscache_n_store_radix_deletes);
 			xpage = radix_tree_delete(&cookie->stores, page->index);
+			trace_fscache_page(cookie, page, fscache_page_radix_delete);
+			trace_fscache_page(cookie, page, fscache_page_write_end);
+
+			val = radix_tree_lookup(&cookie->stores, page->index);
+			trace_fscache_check_page(cookie, page, val, 1);
+		} else {
+			trace_fscache_page(cookie, page, fscache_page_write_end_pend);
 		}
 		spin_unlock(&cookie->stores_lock);
 		wake_up_bit(&cookie->flags, 0);
+		trace_fscache_wake_cookie(cookie);
+	} else {
+		trace_fscache_page(cookie, page, fscache_page_write_end_noc);
 	}
 	spin_unlock(&object->lock);
 	if (xpage)
@@ -215,7 +233,8 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 		return -ENOMEM;
 	}
 
-	fscache_operation_init(op, fscache_attr_changed_op, NULL, NULL);
+	fscache_operation_init(cookie, op, fscache_attr_changed_op, NULL, NULL);
+	trace_fscache_page_op(cookie, NULL, op, fscache_page_op_attr_changed);
 	op->flags = FSCACHE_OP_ASYNC |
 		(1 << FSCACHE_OP_EXCLUSIVE) |
 		(1 << FSCACHE_OP_UNUSE_COOKIE);
@@ -299,7 +318,7 @@ static struct fscache_retrieval *fscache_alloc_retrieval(
 		return NULL;
 	}
 
-	fscache_operation_init(&op->op, NULL,
+	fscache_operation_init(cookie, &op->op, NULL,
 			       fscache_do_cancel_retrieval,
 			       fscache_release_retrieval_op);
 	op->op.flags	= FSCACHE_OP_MYTHREAD |
@@ -370,6 +389,7 @@ int fscache_wait_for_operation_activation(struct fscache_object *object,
 		fscache_stat(stat_op_waits);
 	if (wait_on_bit(&op->flags, FSCACHE_OP_WAITING,
 			TASK_INTERRUPTIBLE) != 0) {
+		trace_fscache_op(object->cookie, op, fscache_op_signal);
 		ret = fscache_cancel_op(op, false);
 		if (ret == 0)
 			return -ERESTARTSYS;
@@ -391,6 +411,7 @@ int fscache_wait_for_operation_activation(struct fscache_object *object,
 	if (unlikely(fscache_object_is_dying(object) ||
 		     fscache_cache_is_broken(object))) {
 		enum fscache_operation_state state = op->state;
+		trace_fscache_op(object->cookie, op, fscache_op_signal);
 		fscache_cancel_op(op, true);
 		if (stat_object_dead)
 			fscache_stat(stat_object_dead);
@@ -445,6 +466,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 		return -ENOMEM;
 	}
 	atomic_set(&op->n_pages, 1);
+	trace_fscache_page_op(cookie, page, &op->op, fscache_page_op_retr_one);
 
 	spin_lock(&cookie->lock);
 
@@ -573,6 +595,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	if (!op)
 		return -ENOMEM;
 	atomic_set(&op->n_pages, *nr_pages);
+	trace_fscache_page_op(cookie, NULL, &op->op, fscache_page_op_retr_multi);
 
 	spin_lock(&cookie->lock);
 
@@ -684,6 +707,7 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	if (!op)
 		return -ENOMEM;
 	atomic_set(&op->n_pages, 1);
+	trace_fscache_page_op(cookie, page, &op->op, fscache_page_op_alloc_one);
 
 	spin_lock(&cookie->lock);
 
@@ -813,9 +837,11 @@ static void fscache_write_op(struct fscache_operation *_op)
 	fscache_stat(&fscache_n_store_calls);
 
 	/* find a page to store */
+	results[0] = NULL;
 	page = NULL;
 	n = radix_tree_gang_lookup_tag(&cookie->stores, results, 0, 1,
 				       FSCACHE_COOKIE_PENDING_TAG);
+	trace_fscache_gang_lookup(cookie, &op->op, results, n, op->store_limit);
 	if (n != 1)
 		goto superseded;
 	page = results[0];
@@ -825,6 +851,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 			   FSCACHE_COOKIE_STORING_TAG);
 	radix_tree_tag_clear(&cookie->stores, page->index,
 			     FSCACHE_COOKIE_PENDING_TAG);
+	trace_fscache_page(cookie, page, fscache_page_radix_pend2store);
 
 	spin_unlock(&cookie->stores_lock);
 	spin_unlock(&object->lock);
@@ -836,6 +863,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 	fscache_stat(&fscache_n_cop_write_page);
 	ret = object->cache->ops->write_page(op, page);
 	fscache_stat_d(&fscache_n_cop_write_page);
+	trace_fscache_wrote_page(cookie, page, &op->op, ret);
 	fscache_end_page_write(object, page);
 	if (ret < 0) {
 		fscache_abort_object(object);
@@ -849,6 +877,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 
 discard_page:
 	fscache_stat(&fscache_n_store_pages_over_limit);
+	trace_fscache_wrote_page(cookie, page, &op->op, -ENOBUFS);
 	fscache_end_page_write(object, page);
 	goto again;
 
@@ -887,6 +916,8 @@ void fscache_invalidate_writes(struct fscache_cookie *cookie)
 		for (i = n - 1; i >= 0; i--) {
 			page = results[i];
 			radix_tree_delete(&cookie->stores, page->index);
+			trace_fscache_page(cookie, page, fscache_page_radix_delete);
+			trace_fscache_page(cookie, page, fscache_page_inval);
 		}
 
 		spin_unlock(&cookie->stores_lock);
@@ -896,6 +927,7 @@ void fscache_invalidate_writes(struct fscache_cookie *cookie)
 	}
 
 	wake_up_bit(&cookie->flags, 0);
+	trace_fscache_wake_cookie(cookie);
 
 	_leave("");
 }
@@ -954,7 +986,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	if (!op)
 		goto nomem;
 
-	fscache_operation_init(&op->op, fscache_write_op, NULL,
+	fscache_operation_init(cookie, &op->op, fscache_write_op, NULL,
 			       fscache_release_write_op);
 	op->op.flags = FSCACHE_OP_ASYNC |
 		(1 << FSCACHE_OP_WAITING) |
@@ -964,6 +996,8 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	if (ret < 0)
 		goto nomem_free;
 
+	trace_fscache_page_op(cookie, page, &op->op, fscache_page_op_write_one);
+
 	ret = -ENOBUFS;
 	spin_lock(&cookie->lock);
 
@@ -975,6 +1009,8 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	if (test_bit(FSCACHE_IOERROR, &object->cache->flags))
 		goto nobufs;
 
+	trace_fscache_page(cookie, page, fscache_page_write);
+
 	/* add the page to the pending-storage radix tree on the backing
 	 * object */
 	spin_lock(&object->lock);
@@ -990,8 +1026,10 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 		goto nobufs_unlock_obj;
 	}
 
+	trace_fscache_page(cookie, page, fscache_page_radix_insert);
 	radix_tree_tag_set(&cookie->stores, page->index,
 			   FSCACHE_COOKIE_PENDING_TAG);
+	trace_fscache_page(cookie, page, fscache_page_radix_set_pend);
 	get_page(page);
 
 	/* we only want one writer at a time, but we do need to queue new
@@ -1034,6 +1072,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 submit_failed:
 	spin_lock(&cookie->stores_lock);
 	radix_tree_delete(&cookie->stores, page->index);
+	trace_fscache_page(cookie, page, fscache_page_radix_delete);
 	spin_unlock(&cookie->stores_lock);
 	wake_cookie = __fscache_unuse_cookie(cookie);
 	put_page(page);
@@ -1080,6 +1119,8 @@ void __fscache_uncache_page(struct fscache_cookie *cookie, struct page *page)
 	if (!PageFsCache(page))
 		goto done;
 
+	trace_fscache_page(cookie, page, fscache_page_uncache);
+
 	/* get the object */
 	spin_lock(&cookie->lock);
 
@@ -1128,6 +1169,8 @@ void fscache_mark_page_cached(struct fscache_retrieval *op, struct page *page)
 	atomic_inc(&fscache_n_marks);
 #endif
 
+	trace_fscache_page(cookie, page, fscache_page_cached);
+
 	_debug("- mark %p{%lx}", page, page->index);
 	if (TestSetPageFsCache(page)) {
 		static bool once_only;

commit 2c98425720233ae3e135add0c7e869b32913502f
Author: David Howells <dhowells@redhat.com>
Date:   Wed Apr 4 13:41:26 2018 +0100

    fscache: Fix hanging wait on page discarded by writeback
    
    If the fscache asynchronous write operation elects to discard a page that's
    pending storage to the cache because the page would be over the store limit
    then it needs to wake the page as someone may be waiting on completion of
    the write.
    
    The problem is that the store limit may be updated by a different
    asynchronous operation - and so may miss the write - and that the store
    limit may not even get updated until later by the netfs.
    
    Fix the kernel hang by making fscache_write_op() mark as written any pages
    that are over the limit.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index adc39c0c62df..b9f18bf4227d 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -778,6 +778,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 
 	_enter("{OP%x,%d}", op->op.debug_id, atomic_read(&op->op.usage));
 
+again:
 	spin_lock(&object->lock);
 	cookie = object->cookie;
 
@@ -819,10 +820,6 @@ static void fscache_write_op(struct fscache_operation *_op)
 		goto superseded;
 	page = results[0];
 	_debug("gang %d [%lx]", n, page->index);
-	if (page->index >= op->store_limit) {
-		fscache_stat(&fscache_n_store_pages_over_limit);
-		goto superseded;
-	}
 
 	radix_tree_tag_set(&cookie->stores, page->index,
 			   FSCACHE_COOKIE_STORING_TAG);
@@ -832,6 +829,9 @@ static void fscache_write_op(struct fscache_operation *_op)
 	spin_unlock(&cookie->stores_lock);
 	spin_unlock(&object->lock);
 
+	if (page->index >= op->store_limit)
+		goto discard_page;
+
 	fscache_stat(&fscache_n_store_pages);
 	fscache_stat(&fscache_n_cop_write_page);
 	ret = object->cache->ops->write_page(op, page);
@@ -847,6 +847,11 @@ static void fscache_write_op(struct fscache_operation *_op)
 	_leave("");
 	return;
 
+discard_page:
+	fscache_stat(&fscache_n_store_pages_over_limit);
+	fscache_end_page_write(object, page);
+	goto again;
+
 superseded:
 	/* this writer is going away and there aren't any more things to
 	 * write */

commit b27ddd46245311850f850024df54d0537506f3c1
Author: David Howells <dhowells@redhat.com>
Date:   Wed Apr 4 13:41:26 2018 +0100

    fscache: Pass the correct cancelled indications to fscache_op_complete()
    
    The last parameter to fscache_op_complete() is a bool indicating whether or
    not the operation was cancelled.  A lot of the time the inverse value is
    given or no differentiation is made.  Fix this.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 961029e04027..adc39c0c62df 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -185,9 +185,11 @@ static void fscache_attr_changed_op(struct fscache_operation *op)
 		fscache_stat_d(&fscache_n_cop_attr_changed);
 		if (ret < 0)
 			fscache_abort_object(object);
+		fscache_op_complete(op, ret < 0);
+	} else {
+		fscache_op_complete(op, true);
 	}
 
-	fscache_op_complete(op, true);
 	_leave("");
 }
 
@@ -780,11 +782,12 @@ static void fscache_write_op(struct fscache_operation *_op)
 	cookie = object->cookie;
 
 	if (!fscache_object_is_active(object)) {
-		/* If we get here, then the on-disk cache object likely longer
-		 * exists, so we should just cancel this write operation.
+		/* If we get here, then the on-disk cache object likely no
+		 * longer exists, so we should just cancel this write
+		 * operation.
 		 */
 		spin_unlock(&object->lock);
-		fscache_op_complete(&op->op, false);
+		fscache_op_complete(&op->op, true);
 		_leave(" [inactive]");
 		return;
 	}
@@ -797,7 +800,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 		 * cancel this write operation.
 		 */
 		spin_unlock(&object->lock);
-		fscache_op_complete(&op->op, false);
+		fscache_op_complete(&op->op, true);
 		_leave(" [cancel] op{f=%lx s=%u} obj{s=%s f=%lx}",
 		       _op->flags, _op->state, object->state->short_name,
 		       object->flags);
@@ -851,7 +854,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 	spin_unlock(&cookie->stores_lock);
 	clear_bit(FSCACHE_OBJECT_PENDING_WRITE, &object->flags);
 	spin_unlock(&object->lock);
-	fscache_op_complete(&op->op, true);
+	fscache_op_complete(&op->op, false);
 	_leave("");
 }
 

commit 8667982014d6048e0b5e286b6247ff24f48d4cc6
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Wed Nov 15 17:37:52 2017 -0800

    mm, pagevec: remove cold parameter for pagevecs
    
    Every pagevec_init user claims the pages being released are hot even in
    cases where it is unlikely the pages are hot.  As no one cares about the
    hotness of pages being released to the allocator, just ditch the
    parameter.
    
    No performance impact is expected as the overhead is marginal.  The
    parameter is removed simply because it is a bit stupid to have a useless
    parameter copied everywhere.
    
    Link: http://lkml.kernel.org/r/20171018075952.10627-6-mgorman@techsingularity.net
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 0ad3fd3ad0b4..961029e04027 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -1175,7 +1175,7 @@ void __fscache_uncache_all_inode_pages(struct fscache_cookie *cookie,
 		return;
 	}
 
-	pagevec_init(&pvec, 0);
+	pagevec_init(&pvec);
 	next = 0;
 	do {
 		if (!pagevec_lookup(&pvec, mapping, &next))

commit 397162ffa2ed1cadffe05c324c6ddc53647f9c62
Author: Jan Kara <jack@suse.cz>
Date:   Wed Sep 6 16:21:43 2017 -0700

    mm: remove nr_pages argument from pagevec_lookup{,_range}()
    
    All users of pagevec_lookup() and pagevec_lookup_range() now pass
    PAGEVEC_SIZE as a desired number of pages.
    
    Just drop the argument.
    
    Link: http://lkml.kernel.org/r/20170726114704.7626-11-jack@suse.cz
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 83018861dcd2..0ad3fd3ad0b4 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -1178,7 +1178,7 @@ void __fscache_uncache_all_inode_pages(struct fscache_cookie *cookie,
 	pagevec_init(&pvec, 0);
 	next = 0;
 	do {
-		if (!pagevec_lookup(&pvec, mapping, &next, PAGEVEC_SIZE))
+		if (!pagevec_lookup(&pvec, mapping, &next))
 			break;
 		for (i = 0; i < pagevec_count(&pvec); i++) {
 			struct page *page = pvec.pages[i];

commit d72dc8a25afc71ce90ee92bdd77550e9beb85d4d
Author: Jan Kara <jack@suse.cz>
Date:   Wed Sep 6 16:21:18 2017 -0700

    mm: make pagevec_lookup() update index
    
    Make pagevec_lookup() (and underlying find_get_pages()) update index to
    the next page where iteration should continue.  Most callers want this
    and also pagevec_lookup_tag() already does this.
    
    Link: http://lkml.kernel.org/r/20170726114704.7626-3-jack@suse.cz
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index c8c4f79c7ce1..83018861dcd2 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -1178,11 +1178,10 @@ void __fscache_uncache_all_inode_pages(struct fscache_cookie *cookie,
 	pagevec_init(&pvec, 0);
 	next = 0;
 	do {
-		if (!pagevec_lookup(&pvec, mapping, next, PAGEVEC_SIZE))
+		if (!pagevec_lookup(&pvec, mapping, &next, PAGEVEC_SIZE))
 			break;
 		for (i = 0; i < pagevec_count(&pvec); i++) {
 			struct page *page = pvec.pages[i];
-			next = page->index;
 			if (PageFsCache(page)) {
 				__fscache_wait_on_page_write(cookie, page);
 				__fscache_uncache_page(cookie, page);
@@ -1190,7 +1189,7 @@ void __fscache_uncache_all_inode_pages(struct fscache_cookie *cookie,
 		}
 		pagevec_release(&pvec);
 		cond_resched();
-	} while (++next);
+	} while (next);
 
 	_leave("");
 }

commit d2138455286f9db8fbff9b50b53b51766f6c1f92
Author: Yan, Zheng <zyan@redhat.com>
Date:   Tue May 17 11:52:48 2016 +0800

    FS-Cache: wake write waiter after invalidating writes
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>
    Acked-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 3078b679fcd1..c8c4f79c7ce1 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -887,6 +887,8 @@ void fscache_invalidate_writes(struct fscache_cookie *cookie)
 			put_page(results[i]);
 	}
 
+	wake_up_bit(&cookie->flags, 0);
+
 	_leave("");
 }
 

commit 09cbfeaf1a5a67bfb3201e0c83c810cecb2efa5a
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Apr 1 15:29:47 2016 +0300

    mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros
    
    PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} macros were introduced *long* time
    ago with promise that one day it will be possible to implement page
    cache with bigger chunks than PAGE_SIZE.
    
    This promise never materialized.  And unlikely will.
    
    We have many places where PAGE_CACHE_SIZE assumed to be equal to
    PAGE_SIZE.  And it's constant source of confusion on whether
    PAGE_CACHE_* or PAGE_* constant should be used in a particular case,
    especially on the border between fs and mm.
    
    Global switching to PAGE_CACHE_SIZE != PAGE_SIZE would cause to much
    breakage to be doable.
    
    Let's stop pretending that pages in page cache are special.  They are
    not.
    
    The changes are pretty straight-forward:
    
     - <foo> << (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;
    
     - <foo> >> (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;
    
     - PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} -> PAGE_{SIZE,SHIFT,MASK,ALIGN};
    
     - page_cache_get() -> get_page();
    
     - page_cache_release() -> put_page();
    
    This patch contains automated changes generated with coccinelle using
    script below.  For some reason, coccinelle doesn't patch header files.
    I've called spatch for them manually.
    
    The only adjustment after coccinelle is revert of changes to
    PAGE_CAHCE_ALIGN definition: we are going to drop it later.
    
    There are few places in the code where coccinelle didn't reach.  I'll
    fix them manually in a separate patch.  Comments and documentation also
    will be addressed with the separate patch.
    
    virtual patch
    
    @@
    expression E;
    @@
    - E << (PAGE_CACHE_SHIFT - PAGE_SHIFT)
    + E
    
    @@
    expression E;
    @@
    - E >> (PAGE_CACHE_SHIFT - PAGE_SHIFT)
    + E
    
    @@
    @@
    - PAGE_CACHE_SHIFT
    + PAGE_SHIFT
    
    @@
    @@
    - PAGE_CACHE_SIZE
    + PAGE_SIZE
    
    @@
    @@
    - PAGE_CACHE_MASK
    + PAGE_MASK
    
    @@
    expression E;
    @@
    - PAGE_CACHE_ALIGN(E)
    + PAGE_ALIGN(E)
    
    @@
    expression E;
    @@
    - page_cache_get(E)
    + get_page(E)
    
    @@
    expression E;
    @@
    - page_cache_release(E)
    + put_page(E)
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 6b35fc4860a0..3078b679fcd1 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -113,7 +113,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 
 	wake_up_bit(&cookie->flags, 0);
 	if (xpage)
-		page_cache_release(xpage);
+		put_page(xpage);
 	__fscache_uncache_page(cookie, page);
 	return true;
 
@@ -164,7 +164,7 @@ static void fscache_end_page_write(struct fscache_object *object,
 	}
 	spin_unlock(&object->lock);
 	if (xpage)
-		page_cache_release(xpage);
+		put_page(xpage);
 }
 
 /*
@@ -884,7 +884,7 @@ void fscache_invalidate_writes(struct fscache_cookie *cookie)
 		spin_unlock(&cookie->stores_lock);
 
 		for (i = n - 1; i >= 0; i--)
-			page_cache_release(results[i]);
+			put_page(results[i]);
 	}
 
 	_leave("");
@@ -982,7 +982,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 
 	radix_tree_tag_set(&cookie->stores, page->index,
 			   FSCACHE_COOKIE_PENDING_TAG);
-	page_cache_get(page);
+	get_page(page);
 
 	/* we only want one writer at a time, but we do need to queue new
 	 * writers after exclusive ops */
@@ -1026,7 +1026,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	radix_tree_delete(&cookie->stores, page->index);
 	spin_unlock(&cookie->stores_lock);
 	wake_cookie = __fscache_unuse_cookie(cookie);
-	page_cache_release(page);
+	put_page(page);
 	ret = -ENOBUFS;
 	goto nobufs;
 

commit 102f4d900c9c8f5ed89ae4746d493fe3ebd7ba64
Author: David Howells <dhowells@redhat.com>
Date:   Wed Nov 4 15:20:42 2015 +0000

    FS-Cache: Handle a write to the page immediately beyond the EOF marker
    
    Handle a write being requested to the page immediately beyond the EOF
    marker on a cache object.  Currently this gets an assertion failure in
    CacheFiles because the EOF marker is used there to encode information about
    a partial page at the EOF - which could lead to an unknown blank spot in
    the file if we extend the file over it.
    
    The problem is actually in fscache where we check the index of the page
    being written against store_limit.  store_limit is set to the number of
    pages that we're allowed to store by fscache_set_store_limit() - which
    means it's one more than the index of the last page we're allowed to store.
    The problem is that we permit writing to a page with an index _equal_ to
    the store limit - when we should reject that case.
    
    Whilst we're at it, change the triggered assertion in CacheFiles to just
    return -ENOBUFS instead.
    
    The assertion failure looks something like this:
    
    CacheFiles: Assertion failed
    1000 < 7b1 is false
    ------------[ cut here ]------------
    kernel BUG at fs/cachefiles/rdwr.c:962!
    ...
    RIP: 0010:[<ffffffffa02c9e83>]  [<ffffffffa02c9e83>] cachefiles_write_page+0x273/0x2d0 [cachefiles]
    
    Cc: stable@vger.kernel.org # v2.6.31+; earlier - that + backport of a17754f (at least)
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 79483b3d8c6f..6b35fc4860a0 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -816,7 +816,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 		goto superseded;
 	page = results[0];
 	_debug("gang %d [%lx]", n, page->index);
-	if (page->index > op->store_limit) {
+	if (page->index >= op->store_limit) {
 		fscache_stat(&fscache_n_store_pages_over_limit);
 		goto superseded;
 	}

commit d0164adc89f6bb374d304ffcc375c6d2652fe67d
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Nov 6 16:28:21 2015 -0800

    mm, page_alloc: distinguish between being unable to sleep, unwilling to sleep and avoiding waking kswapd
    
    __GFP_WAIT has been used to identify atomic context in callers that hold
    spinlocks or are in interrupts.  They are expected to be high priority and
    have access one of two watermarks lower than "min" which can be referred
    to as the "atomic reserve".  __GFP_HIGH users get access to the first
    lower watermark and can be called the "high priority reserve".
    
    Over time, callers had a requirement to not block when fallback options
    were available.  Some have abused __GFP_WAIT leading to a situation where
    an optimisitic allocation with a fallback option can access atomic
    reserves.
    
    This patch uses __GFP_ATOMIC to identify callers that are truely atomic,
    cannot sleep and have no alternative.  High priority users continue to use
    __GFP_HIGH.  __GFP_DIRECT_RECLAIM identifies callers that can sleep and
    are willing to enter direct reclaim.  __GFP_KSWAPD_RECLAIM to identify
    callers that want to wake kswapd for background reclaim.  __GFP_WAIT is
    redefined as a caller that is willing to enter direct reclaim and wake
    kswapd for background reclaim.
    
    This patch then converts a number of sites
    
    o __GFP_ATOMIC is used by callers that are high priority and have memory
      pools for those requests. GFP_ATOMIC uses this flag.
    
    o Callers that have a limited mempool to guarantee forward progress clear
      __GFP_DIRECT_RECLAIM but keep __GFP_KSWAPD_RECLAIM. bio allocations fall
      into this category where kswapd will still be woken but atomic reserves
      are not used as there is a one-entry mempool to guarantee progress.
    
    o Callers that are checking if they are non-blocking should use the
      helper gfpflags_allow_blocking() where possible. This is because
      checking for __GFP_WAIT as was done historically now can trigger false
      positives. Some exceptions like dm-crypt.c exist where the code intent
      is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to
      flag manipulations.
    
    o Callers that built their own GFP flags instead of starting with GFP_KERNEL
      and friends now also need to specify __GFP_KSWAPD_RECLAIM.
    
    The first key hazard to watch out for is callers that removed __GFP_WAIT
    and was depending on access to atomic reserves for inconspicuous reasons.
    In some cases it may be appropriate for them to use __GFP_HIGH.
    
    The second key hazard is callers that assembled their own combination of
    GFP flags instead of starting with something like GFP_KERNEL.  They may
    now wish to specify __GFP_KSWAPD_RECLAIM.  It's almost certainly harmless
    if it's missed in most cases as other activity will wake kswapd.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vitaly Wool <vitalywool@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 483bbc613bf0..79483b3d8c6f 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -58,7 +58,7 @@ bool release_page_wait_timeout(struct fscache_cookie *cookie, struct page *page)
 
 /*
  * decide whether a page can be released, possibly by cancelling a store to it
- * - we're allowed to sleep if __GFP_WAIT is flagged
+ * - we're allowed to sleep if __GFP_DIRECT_RECLAIM is flagged
  */
 bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 				  struct page *page,
@@ -122,7 +122,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 	 * allocator as the work threads writing to the cache may all end up
 	 * sleeping on memory allocation, so we may need to impose a timeout
 	 * too. */
-	if (!(gfp & __GFP_WAIT) || !(gfp & __GFP_FS)) {
+	if (!(gfp & __GFP_DIRECT_RECLAIM) || !(gfp & __GFP_FS)) {
 		fscache_stat(&fscache_n_store_vmscan_busy);
 		return false;
 	}
@@ -132,7 +132,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 		_debug("fscache writeout timeout page: %p{%lx}",
 			page, page->index);
 
-	gfp &= ~__GFP_WAIT;
+	gfp &= ~__GFP_DIRECT_RECLAIM;
 	goto try_again;
 }
 EXPORT_SYMBOL(__fscache_maybe_release_page);

commit 4a47132ff472a0c2c5441baeb50cf97f2580bc43
Author: David Howells <dhowells@redhat.com>
Date:   Tue Feb 24 10:05:29 2015 +0000

    FS-Cache: Retain the netfs context in the retrieval op earlier
    
    Now that the retrieval operation may be disposed of by fscache_put_operation()
    before we actually set the context, the retrieval-specific cleanup operation
    can produce a NULL-pointer dereference when it tries to unconditionally clean
    up the netfs context.
    
    Given that it is expected that we'll get at least as far as the place where we
    currently set the context pointer and it is unlikely we'll go through the
    error handling paths prior to that point, retain the context right from the
    point that the retrieval op is allocated.
    
    Concomitant to this, we need to retain the cookie pointer in the retrieval op
    also so that we can call the netfs to release its context in the release
    method.
    
    In addition, we might now get into fscache_release_retrieval_op() with the op
    only initialised.  To this end, set the operation to DEAD only after the
    release method has been called and skip the n_pages test upon cleanup if the
    op is still in the INITIALISED state.
    
    Without these changes, the following oops might be seen:
    
            BUG: unable to handle kernel NULL pointer dereference at 00000000000000b8
            ...
            RIP: 0010:[<ffffffffa0089c98>] fscache_release_retrieval_op+0xae/0x100
            ...
            Call Trace:
             [<ffffffffa0088560>] fscache_put_operation+0x117/0x2e0
             [<ffffffffa008b8f5>] __fscache_read_or_alloc_pages+0x351/0x3ac
             [<ffffffffa00b761f>] __nfs_readpages_from_fscache+0x59/0xbf [nfs]
             [<ffffffffa00b06c5>] nfs_readpages+0x10c/0x185 [nfs]
             [<ffffffff81124925>] ? alloc_pages_current+0x119/0x13e
             [<ffffffff810ee5fd>] ? __page_cache_alloc+0xfb/0x10a
             [<ffffffff810f87f8>] __do_page_cache_readahead+0x188/0x22c
             [<ffffffff810f8b3a>] ondemand_readahead+0x29e/0x2af
             [<ffffffff810f8c92>] page_cache_sync_readahead+0x38/0x3a
             [<ffffffff810ef337>] generic_file_read_iter+0x1a2/0x55a
             [<ffffffffa00a9dff>] ? nfs_revalidate_mapping+0xd6/0x288 [nfs]
             [<ffffffffa00a6a23>] nfs_file_read+0x49/0x70 [nfs]
             [<ffffffff811363be>] new_sync_read+0x78/0x9c
             [<ffffffff81137164>] __vfs_read+0x13/0x38
             [<ffffffff8113721e>] vfs_read+0x95/0x121
             [<ffffffff811372f6>] SyS_read+0x4c/0x8a
             [<ffffffff81557a52>] system_call_fastpath+0x12/0x17
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Steve Dickson <steved@redhat.com>
    Acked-by: Jeff Layton <jeff.layton@primarydata.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index d1b23a67c031..483bbc613bf0 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -269,11 +269,12 @@ static void fscache_release_retrieval_op(struct fscache_operation *_op)
 
 	_enter("{OP%x}", op->op.debug_id);
 
-	ASSERTCMP(atomic_read(&op->n_pages), ==, 0);
+	ASSERTIFCMP(op->op.state != FSCACHE_OP_ST_INITIALISED,
+		    atomic_read(&op->n_pages), ==, 0);
 
 	fscache_hist(fscache_retrieval_histogram, op->start_time);
 	if (op->context)
-		fscache_put_context(op->op.object->cookie, op->context);
+		fscache_put_context(op->cookie, op->context);
 
 	_leave("");
 }
@@ -302,11 +303,18 @@ static struct fscache_retrieval *fscache_alloc_retrieval(
 	op->op.flags	= FSCACHE_OP_MYTHREAD |
 		(1UL << FSCACHE_OP_WAITING) |
 		(1UL << FSCACHE_OP_UNUSE_COOKIE);
+	op->cookie	= cookie;
 	op->mapping	= mapping;
 	op->end_io_func	= end_io_func;
 	op->context	= context;
 	op->start_time	= jiffies;
 	INIT_LIST_HEAD(&op->to_do);
+
+	/* Pin the netfs read context in case we need to do the actual netfs
+	 * read because we've encountered a cache read failure.
+	 */
+	if (context)
+		fscache_get_context(op->cookie, context);
 	return op;
 }
 
@@ -456,10 +464,6 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 
 	fscache_stat(&fscache_n_retrieval_ops);
 
-	/* pin the netfs read context in case we need to do the actual netfs
-	 * read because we've encountered a cache read failure */
-	fscache_get_context(object->cookie, op->context);
-
 	/* we wait for the operation to become active, and then process it
 	 * *here*, in this thread, and not in the thread pool */
 	ret = fscache_wait_for_operation_activation(
@@ -586,10 +590,6 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 
 	fscache_stat(&fscache_n_retrieval_ops);
 
-	/* pin the netfs read context in case we need to do the actual netfs
-	 * read because we've encountered a cache read failure */
-	fscache_get_context(object->cookie, op->context);
-
 	/* we wait for the operation to become active, and then process it
 	 * *here*, in this thread, and not in the thread pool */
 	ret = fscache_wait_for_operation_activation(

commit d3b97ca4a99e4e6c78f5a21c968eadf5c8ba9971
Author: David Howells <dhowells@redhat.com>
Date:   Tue Feb 24 10:05:29 2015 +0000

    FS-Cache: The operation cancellation method needs calling in more places
    
    Any time an incomplete operation is cancelled, the operation cancellation
    function needs to be called to clean up.  This is currently being passed
    directly to some of the functions that might want to call it, but not all.
    
    Instead, pass the cancellation method pointer to the fscache_operation_init()
    and have that cache it in the operation struct.  Further, plug in a dummy
    cancellation handler if the caller declines to set one as this allows us to
    call the function unconditionally (the extra overhead isn't worth bothering
    about as we don't expect to be calling this typically).
    
    The cancellation method must thence be called everywhere the CANCELLED state
    is set.  Note that we call it *before* setting the CANCELLED state such that
    the method can use the old state value to guide its operation.
    
    fscache_do_cancel_retrieval() needs moving higher up in the sources so that
    the init function can use it now.
    
    Without this, the following oops may be seen:
    
            FS-Cache: Assertion failed
            FS-Cache: 3 == 0 is false
            ------------[ cut here ]------------
            kernel BUG at ../fs/fscache/page.c:261!
            ...
            RIP: 0010:[<ffffffffa0089c1b>]  fscache_release_retrieval_op+0x77/0x100
             [<ffffffffa008853d>] fscache_put_operation+0x114/0x2da
             [<ffffffffa008b8c2>] __fscache_read_or_alloc_pages+0x358/0x3b3
             [<ffffffffa00b761f>] __nfs_readpages_from_fscache+0x59/0xbf [nfs]
             [<ffffffffa00b06c5>] nfs_readpages+0x10c/0x185 [nfs]
             [<ffffffff81124925>] ? alloc_pages_current+0x119/0x13e
             [<ffffffff810ee5fd>] ? __page_cache_alloc+0xfb/0x10a
             [<ffffffff810f87f8>] __do_page_cache_readahead+0x188/0x22c
             [<ffffffff810f8b3a>] ondemand_readahead+0x29e/0x2af
             [<ffffffff810f8c92>] page_cache_sync_readahead+0x38/0x3a
             [<ffffffff810ef337>] generic_file_read_iter+0x1a2/0x55a
             [<ffffffffa00a9dff>] ? nfs_revalidate_mapping+0xd6/0x288 [nfs]
             [<ffffffffa00a6a23>] nfs_file_read+0x49/0x70 [nfs]
             [<ffffffff811363be>] new_sync_read+0x78/0x9c
             [<ffffffff81137164>] __vfs_read+0x13/0x38
             [<ffffffff8113721e>] vfs_read+0x95/0x121
             [<ffffffff811372f6>] SyS_read+0x4c/0x8a
             [<ffffffff81557a52>] system_call_fastpath+0x12/0x17
    
    The assertion is showing that the remaining number of pages (n_pages) is not 0
    when the operation is being released.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Steve Dickson <steved@redhat.com>
    Acked-by: Jeff Layton <jeff.layton@primarydata.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 7db9752252ef..d1b23a67c031 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -213,7 +213,7 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 		return -ENOMEM;
 	}
 
-	fscache_operation_init(op, fscache_attr_changed_op, NULL);
+	fscache_operation_init(op, fscache_attr_changed_op, NULL, NULL);
 	op->flags = FSCACHE_OP_ASYNC |
 		(1 << FSCACHE_OP_EXCLUSIVE) |
 		(1 << FSCACHE_OP_UNUSE_COOKIE);
@@ -248,6 +248,17 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 }
 EXPORT_SYMBOL(__fscache_attr_changed);
 
+/*
+ * Handle cancellation of a pending retrieval op
+ */
+static void fscache_do_cancel_retrieval(struct fscache_operation *_op)
+{
+	struct fscache_retrieval *op =
+		container_of(_op, struct fscache_retrieval, op);
+
+	atomic_set(&op->n_pages, 0);
+}
+
 /*
  * release a retrieval op reference
  */
@@ -285,7 +296,9 @@ static struct fscache_retrieval *fscache_alloc_retrieval(
 		return NULL;
 	}
 
-	fscache_operation_init(&op->op, NULL, fscache_release_retrieval_op);
+	fscache_operation_init(&op->op, NULL,
+			       fscache_do_cancel_retrieval,
+			       fscache_release_retrieval_op);
 	op->op.flags	= FSCACHE_OP_MYTHREAD |
 		(1UL << FSCACHE_OP_WAITING) |
 		(1UL << FSCACHE_OP_UNUSE_COOKIE);
@@ -329,25 +342,13 @@ int fscache_wait_for_deferred_lookup(struct fscache_cookie *cookie)
 	return 0;
 }
 
-/*
- * Handle cancellation of a pending retrieval op
- */
-static void fscache_do_cancel_retrieval(struct fscache_operation *_op)
-{
-	struct fscache_retrieval *op =
-		container_of(_op, struct fscache_retrieval, op);
-
-	atomic_set(&op->n_pages, 0);
-}
-
 /*
  * wait for an object to become active (or dead)
  */
 int fscache_wait_for_operation_activation(struct fscache_object *object,
 					  struct fscache_operation *op,
 					  atomic_t *stat_op_waits,
-					  atomic_t *stat_object_dead,
-					  void (*do_cancel)(struct fscache_operation *))
+					  atomic_t *stat_object_dead)
 {
 	int ret;
 
@@ -359,7 +360,7 @@ int fscache_wait_for_operation_activation(struct fscache_object *object,
 		fscache_stat(stat_op_waits);
 	if (wait_on_bit(&op->flags, FSCACHE_OP_WAITING,
 			TASK_INTERRUPTIBLE) != 0) {
-		ret = fscache_cancel_op(op, do_cancel, false);
+		ret = fscache_cancel_op(op, false);
 		if (ret == 0)
 			return -ERESTARTSYS;
 
@@ -380,7 +381,7 @@ int fscache_wait_for_operation_activation(struct fscache_object *object,
 	if (unlikely(fscache_object_is_dying(object) ||
 		     fscache_cache_is_broken(object))) {
 		enum fscache_operation_state state = op->state;
-		fscache_cancel_op(op, do_cancel, true);
+		fscache_cancel_op(op, true);
 		if (stat_object_dead)
 			fscache_stat(stat_object_dead);
 		_leave(" = -ENOBUFS [obj dead %d]", state);
@@ -464,8 +465,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 	ret = fscache_wait_for_operation_activation(
 		object, &op->op,
 		__fscache_stat(&fscache_n_retrieval_op_waits),
-		__fscache_stat(&fscache_n_retrievals_object_dead),
-		fscache_do_cancel_retrieval);
+		__fscache_stat(&fscache_n_retrievals_object_dead));
 	if (ret < 0)
 		goto error;
 
@@ -595,8 +595,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	ret = fscache_wait_for_operation_activation(
 		object, &op->op,
 		__fscache_stat(&fscache_n_retrieval_op_waits),
-		__fscache_stat(&fscache_n_retrievals_object_dead),
-		fscache_do_cancel_retrieval);
+		__fscache_stat(&fscache_n_retrievals_object_dead));
 	if (ret < 0)
 		goto error;
 
@@ -702,8 +701,7 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	ret = fscache_wait_for_operation_activation(
 		object, &op->op,
 		__fscache_stat(&fscache_n_alloc_op_waits),
-		__fscache_stat(&fscache_n_allocs_object_dead),
-		fscache_do_cancel_retrieval);
+		__fscache_stat(&fscache_n_allocs_object_dead));
 	if (ret < 0)
 		goto error;
 
@@ -946,7 +944,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	if (!op)
 		goto nomem;
 
-	fscache_operation_init(&op->op, fscache_write_op,
+	fscache_operation_init(&op->op, fscache_write_op, NULL,
 			       fscache_release_write_op);
 	op->op.flags = FSCACHE_OP_ASYNC |
 		(1 << FSCACHE_OP_WAITING) |

commit a39caadf06879017cb9a8c5c5cb4fc4ccb213275
Author: David Howells <dhowells@redhat.com>
Date:   Wed Feb 25 14:22:40 2015 +0000

    FS-Cache: Put an aborted initialised op so that it is accounted correctly
    
    Call fscache_put_operation() or a wrapper on any op that has gone through
    fscache_operation_init() so that the accounting shown in /proc is done
    correctly, specifically fscache_n_op_release.
    
    fscache_put_operation() therefore now allows an op in the INITIALISED state as
    well as in the CANCELLED and COMPLETE states.
    
    Note that this means that an operation can get put that doesn't have its
    ->object pointer filled in, so anything that depends on the object needs to be
    conditional in fscache_put_operation().
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Steve Dickson <steved@redhat.com>
    Acked-by: Jeff Layton <jeff.layton@primarydata.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 433cae927eca..7db9752252ef 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -239,7 +239,7 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 	wake_cookie = __fscache_unuse_cookie(cookie);
 nobufs:
 	spin_unlock(&cookie->lock);
-	kfree(op);
+	fscache_put_operation(op);
 	if (wake_cookie)
 		__fscache_wake_unused_cookie(cookie);
 	fscache_stat(&fscache_n_attr_changed_nobufs);
@@ -505,7 +505,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 	spin_unlock(&cookie->lock);
 	if (wake_cookie)
 		__fscache_wake_unused_cookie(cookie);
-	kfree(op);
+	fscache_put_retrieval(op);
 nobufs:
 	fscache_stat(&fscache_n_retrievals_nobufs);
 	_leave(" = -ENOBUFS");
@@ -634,7 +634,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	wake_cookie = __fscache_unuse_cookie(cookie);
 nobufs_unlock:
 	spin_unlock(&cookie->lock);
-	kfree(op);
+	fscache_put_retrieval(op);
 	if (wake_cookie)
 		__fscache_wake_unused_cookie(cookie);
 nobufs:
@@ -728,7 +728,7 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	wake_cookie = __fscache_unuse_cookie(cookie);
 nobufs_unlock:
 	spin_unlock(&cookie->lock);
-	kfree(op);
+	fscache_put_retrieval(op);
 	if (wake_cookie)
 		__fscache_wake_unused_cookie(cookie);
 nobufs:
@@ -1018,7 +1018,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	spin_unlock(&object->lock);
 	spin_unlock(&cookie->lock);
 	radix_tree_preload_end();
-	kfree(op);
+	fscache_put_operation(&op->op);
 	fscache_stat(&fscache_n_stores_ok);
 	_leave(" = 0");
 	return 0;
@@ -1038,7 +1038,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 nobufs:
 	spin_unlock(&cookie->lock);
 	radix_tree_preload_end();
-	kfree(op);
+	fscache_put_operation(&op->op);
 	if (wake_cookie)
 		__fscache_wake_unused_cookie(cookie);
 	fscache_stat(&fscache_n_stores_nobufs);
@@ -1046,7 +1046,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	return -ENOBUFS;
 
 nomem_free:
-	kfree(op);
+	fscache_put_operation(&op->op);
 nomem:
 	fscache_stat(&fscache_n_stores_oom);
 	_leave(" = -ENOMEM");

commit 418b7eb9e1011bc11220a03ad0045885d04698d2
Author: David Howells <dhowells@redhat.com>
Date:   Tue Feb 24 10:05:28 2015 +0000

    FS-Cache: Permit fscache_cancel_op() to cancel in-progress operations too
    
    Currently, fscache_cancel_op() only cancels pending operations - attempts to
    cancel in-progress operations are ignored.  This leads to a problem in
    fscache_wait_for_operation_activation() whereby the wait is terminated, but
    the object has been killed.
    
    The check at the end of the function now triggers because it's no longer
    contingent on the cache having produced an I/O error since the commit that
    fixed the logic error in fscache_object_is_dead().
    
    The result of the check is that it tries to cancel the operation - but since
    the object may not be pending by this point, the cancellation request may be
    ignored - with the result that the the object is just put by the caller and
    fscache_put_operation has an assertion failure because the operation isn't in
    either the COMPLETE or the CANCELLED states.
    
    To fix this, we permit in-progress ops to be cancelled under some
    circumstances.
    
    The bug results in an oops that looks something like this:
    
            FS-Cache: fscache_wait_for_operation_activation() = -ENOBUFS [obj dead 3]
            FS-Cache:
            FS-Cache: Assertion failed
            FS-Cache: 3 == 5 is false
            ------------[ cut here ]------------
            kernel BUG at ../fs/fscache/operation.c:432!
            ...
            RIP: 0010:[<ffffffffa0088574>] fscache_put_operation+0xf2/0x2cd
            Call Trace:
             [<ffffffffa008b92a>] __fscache_read_or_alloc_pages+0x2ec/0x3b3
             [<ffffffffa00b761f>] __nfs_readpages_from_fscache+0x59/0xbf [nfs]
             [<ffffffffa00b06c5>] nfs_readpages+0x10c/0x185 [nfs]
             [<ffffffff81124925>] ? alloc_pages_current+0x119/0x13e
             [<ffffffff810ee5fd>] ? __page_cache_alloc+0xfb/0x10a
             [<ffffffff810f87f8>] __do_page_cache_readahead+0x188/0x22c
             [<ffffffff810f8b3a>] ondemand_readahead+0x29e/0x2af
             [<ffffffff810f8c92>] page_cache_sync_readahead+0x38/0x3a
             [<ffffffff810ef337>] generic_file_read_iter+0x1a2/0x55a
             [<ffffffffa00a9dff>] ? nfs_revalidate_mapping+0xd6/0x288 [nfs]
             [<ffffffffa00a6a23>] nfs_file_read+0x49/0x70 [nfs]
             [<ffffffff811363be>] new_sync_read+0x78/0x9c
             [<ffffffff81137164>] __vfs_read+0x13/0x38
             [<ffffffff8113721e>] vfs_read+0x95/0x121
             [<ffffffff811372f6>] SyS_read+0x4c/0x8a
             [<ffffffff81557a52>] system_call_fastpath+0x12/0x17
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Steve Dickson <steved@redhat.com>
    Acked-by: Jeff Layton <jeff.layton@primarydata.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index d0805e31361c..433cae927eca 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -359,7 +359,7 @@ int fscache_wait_for_operation_activation(struct fscache_object *object,
 		fscache_stat(stat_op_waits);
 	if (wait_on_bit(&op->flags, FSCACHE_OP_WAITING,
 			TASK_INTERRUPTIBLE) != 0) {
-		ret = fscache_cancel_op(op, do_cancel);
+		ret = fscache_cancel_op(op, do_cancel, false);
 		if (ret == 0)
 			return -ERESTARTSYS;
 
@@ -380,7 +380,7 @@ int fscache_wait_for_operation_activation(struct fscache_object *object,
 	if (unlikely(fscache_object_is_dying(object) ||
 		     fscache_cache_is_broken(object))) {
 		enum fscache_operation_state state = op->state;
-		fscache_cancel_op(op, do_cancel);
+		fscache_cancel_op(op, do_cancel, true);
 		if (stat_object_dead)
 			fscache_stat(stat_object_dead);
 		_leave(" = -ENOBUFS [obj dead %d]", state);

commit 87021526300f1a292dd966e141e183630ac95317
Author: David Howells <dhowells@redhat.com>
Date:   Tue Feb 24 10:52:51 2015 +0000

    FS-Cache: fscache_object_is_dead() has wrong logic, kill it
    
    fscache_object_is_dead() returns true only if the object is marked dead and
    the cache got an I/O error.  This should be a logical OR instead.  Since two
    of the callers got split up into handling for separate subcases, expand the
    other callers and kill the function.  This is probably the right thing to do
    anyway since one of the subcases isn't about the object at all, but rather
    about the cache.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Steve Dickson <steved@redhat.com>
    Acked-by: Jeff Layton <jeff.layton@primarydata.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index de33b3fccca6..d0805e31361c 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -377,11 +377,13 @@ int fscache_wait_for_operation_activation(struct fscache_object *object,
 		_leave(" = -ENOBUFS [cancelled]");
 		return -ENOBUFS;
 	}
-	if (unlikely(fscache_object_is_dead(object))) {
-		pr_err("%s() = -ENOBUFS [obj dead %d]\n", __func__, op->state);
+	if (unlikely(fscache_object_is_dying(object) ||
+		     fscache_cache_is_broken(object))) {
+		enum fscache_operation_state state = op->state;
 		fscache_cancel_op(op, do_cancel);
 		if (stat_object_dead)
 			fscache_stat(stat_object_dead);
+		_leave(" = -ENOBUFS [obj dead %d]", state);
 		return -ENOBUFS;
 	}
 	return 0;

commit 3e1199dcad004a40b55297a4736ccd1b9b81a952
Author: Milosz Tanski <milosz@adfin.com>
Date:   Wed Aug 13 12:58:26 2014 -0400

    FS-Cache: refcount becomes corrupt under vma pressure.
    
    In rare cases under heavy VMA pressure the ref count for a fscache cookie
    becomes corrupt. In this case we decrement ref count even if we fail before
    incrementing the refcount.
    
    FS-Cache: Assertion failed bnode-eca5f9c6/syslog
    0 > 0 is false
    ------------[ cut here ]------------
    kernel BUG at fs/fscache/cookie.c:519!
    invalid opcode: 0000 [#1] SMP
    Call Trace:
    [<ffffffffa01ba060>] __fscache_relinquish_cookie+0x50/0x220 [fscache]
    [<ffffffffa02d64ce>] ceph_fscache_unregister_inode_cookie+0x3e/0x50 [ceph]
    [<ffffffffa02ae1d3>] ceph_destroy_inode+0x33/0x200 [ceph]
    [<ffffffff811cf67e>] ? __fsnotify_inode_delete+0xe/0x10
    [<ffffffff811a9e0c>] destroy_inode+0x3c/0x70
    [<ffffffff811a9f51>] evict+0x111/0x180
    [<ffffffff811aa763>] iput+0x103/0x190
    [<ffffffff811a5de8>] __dentry_kill+0x1c8/0x220
    [<ffffffff811a5f31>] shrink_dentry_list+0xf1/0x250
    [<ffffffff811a762c>] prune_dcache_sb+0x4c/0x60
    [<ffffffff811930af>] super_cache_scan+0xff/0x170
    [<ffffffff8113d7a0>] shrink_slab_node+0x140/0x2c0
    [<ffffffff8113f2da>] shrink_slab+0x8a/0x130
    [<ffffffff81142572>] balance_pgdat+0x3e2/0x5d0
    [<ffffffff811428ca>] kswapd+0x16a/0x4a0
    [<ffffffff810a43f0>] ? __wake_up_sync+0x20/0x20
    [<ffffffff81142760>] ? balance_pgdat+0x5d0/0x5d0
    [<ffffffff81083e09>] kthread+0xc9/0xe0
    [<ffffffff81010000>] ? ftrace_raw_event_xen_mmu_release_ptpage+0x70/0x90
    [<ffffffff81083d40>] ? flush_kthread_worker+0xb0/0xb0
    [<ffffffff8159f63c>] ret_from_fork+0x7c/0xb0
    [<ffffffff81083d40>] ? flush_kthread_worker+0xb0/0xb0
    RIP [<ffffffffa01b984b>] __fscache_disable_cookie+0x1db/0x210 [fscache]
    RSP <ffff8803bc85f9b8>
    ---[ end trace 254d0d7c74a01f25 ]---
    
    Signed-off-by: Milosz Tanski <milosz@adfin.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 781ac7b0b53e..de33b3fccca6 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -198,7 +198,7 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 {
 	struct fscache_operation *op;
 	struct fscache_object *object;
-	bool wake_cookie;
+	bool wake_cookie = false;
 
 	_enter("%p", cookie);
 
@@ -228,15 +228,16 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 
 	__fscache_use_cookie(cookie);
 	if (fscache_submit_exclusive_op(object, op) < 0)
-		goto nobufs;
+		goto nobufs_dec;
 	spin_unlock(&cookie->lock);
 	fscache_stat(&fscache_n_attr_changed_ok);
 	fscache_put_operation(op);
 	_leave(" = 0");
 	return 0;
 
-nobufs:
+nobufs_dec:
 	wake_cookie = __fscache_unuse_cookie(cookie);
+nobufs:
 	spin_unlock(&cookie->lock);
 	kfree(op);
 	if (wake_cookie)

commit 9776de96e51565286da25c74d6f631abc50c63ef
Author: Milosz Tanski <milosz@adfin.com>
Date:   Wed Aug 13 12:58:16 2014 -0400

    FS-Cache: Timeout for releasepage()
    
    This is meant to avoid a recusive hang caused by underlying filesystem trying
    to grab a free page and causing a write-out.
    
    INFO: task kworker/u30:7:28375 blocked for more than 120 seconds.
          Not tainted 3.15.0-virtual #74
    "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    kworker/u30:7   D 0000000000000000     0 28375      2 0x00000000
    Workqueue: fscache_operation fscache_op_work_func [fscache]
     ffff88000b147148 0000000000000046 0000000000000000 ffff88000b1471c8
     ffff8807aa031820 0000000000014040 ffff88000b147fd8 0000000000014040
     ffff880f0c50c860 ffff8807aa031820 ffff88000b147158 ffff88007be59cd0
    Call Trace:
     [<ffffffff815930e9>] schedule+0x29/0x70
     [<ffffffffa018bed5>] __fscache_wait_on_page_write+0x55/0x90 [fscache]
     [<ffffffff810a4350>] ? __wake_up_sync+0x20/0x20
     [<ffffffffa018c135>] __fscache_maybe_release_page+0x65/0x1e0 [fscache]
     [<ffffffffa02ad813>] ceph_releasepage+0x83/0x100 [ceph]
     [<ffffffff811635b0>] ? anon_vma_fork+0x130/0x130
     [<ffffffff8112cdd2>] try_to_release_page+0x32/0x50
     [<ffffffff81140096>] shrink_page_list+0x7e6/0x9d0
     [<ffffffff8113f278>] ? isolate_lru_pages.isra.73+0x78/0x1e0
     [<ffffffff81140932>] shrink_inactive_list+0x252/0x4c0
     [<ffffffff811412b1>] shrink_lruvec+0x3e1/0x670
     [<ffffffff8114157f>] shrink_zone+0x3f/0x110
     [<ffffffff81141b06>] do_try_to_free_pages+0x1d6/0x450
     [<ffffffff8114a939>] ? zone_statistics+0x99/0xc0
     [<ffffffff81141e44>] try_to_free_pages+0xc4/0x180
     [<ffffffff81136982>] __alloc_pages_nodemask+0x6b2/0xa60
     [<ffffffff811c1d4e>] ? __find_get_block+0xbe/0x250
     [<ffffffff810a405e>] ? wake_up_bit+0x2e/0x40
     [<ffffffff811740c3>] alloc_pages_current+0xb3/0x180
     [<ffffffff8112cf07>] __page_cache_alloc+0xb7/0xd0
     [<ffffffff8112da6c>] grab_cache_page_write_begin+0x7c/0xe0
     [<ffffffff81214072>] ? ext4_mark_inode_dirty+0x82/0x220
     [<ffffffff81214a89>] ext4_da_write_begin+0x89/0x2d0
     [<ffffffff8112c6ee>] generic_perform_write+0xbe/0x1d0
     [<ffffffff811a96b1>] ? update_time+0x81/0xc0
     [<ffffffff811ad4c2>] ? mnt_clone_write+0x12/0x30
     [<ffffffff8112e80e>] __generic_file_aio_write+0x1ce/0x3f0
     [<ffffffff8112ea8e>] generic_file_aio_write+0x5e/0xe0
     [<ffffffff8120b94f>] ext4_file_write+0x9f/0x410
     [<ffffffff8120af56>] ? ext4_file_open+0x66/0x180
     [<ffffffff8118f0da>] do_sync_write+0x5a/0x90
     [<ffffffffa025c6c9>] cachefiles_write_page+0x149/0x430 [cachefiles]
     [<ffffffff812cf439>] ? radix_tree_gang_lookup_tag+0x89/0xd0
     [<ffffffffa018c512>] fscache_write_op+0x222/0x3b0 [fscache]
     [<ffffffffa018b35a>] fscache_op_work_func+0x3a/0x100 [fscache]
     [<ffffffff8107bfe9>] process_one_work+0x179/0x4a0
     [<ffffffff8107d47b>] worker_thread+0x11b/0x370
     [<ffffffff8107d360>] ? manage_workers.isra.21+0x2e0/0x2e0
     [<ffffffff81083d69>] kthread+0xc9/0xe0
     [<ffffffff81010000>] ? ftrace_raw_event_xen_mmu_release_ptpage+0x70/0x90
     [<ffffffff81083ca0>] ? flush_kthread_worker+0xb0/0xb0
     [<ffffffff8159eefc>] ret_from_fork+0x7c/0xb0
     [<ffffffff81083ca0>] ? flush_kthread_worker+0xb0/0xb0
    
    Signed-off-by: Milosz Tanski <milosz@adfin.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 85332b9d19d1..781ac7b0b53e 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -43,6 +43,19 @@ void __fscache_wait_on_page_write(struct fscache_cookie *cookie, struct page *pa
 }
 EXPORT_SYMBOL(__fscache_wait_on_page_write);
 
+/*
+ * wait for a page to finish being written to the cache. Put a timeout here
+ * since we might be called recursively via parent fs.
+ */
+static
+bool release_page_wait_timeout(struct fscache_cookie *cookie, struct page *page)
+{
+	wait_queue_head_t *wq = bit_waitqueue(&cookie->flags, 0);
+
+	return wait_event_timeout(*wq, !__fscache_check_page_write(cookie, page),
+				  HZ);
+}
+
 /*
  * decide whether a page can be released, possibly by cancelling a store to it
  * - we're allowed to sleep if __GFP_WAIT is flagged
@@ -115,7 +128,10 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 	}
 
 	fscache_stat(&fscache_n_store_vmscan_wait);
-	__fscache_wait_on_page_write(cookie, page);
+	if (!release_page_wait_timeout(cookie, page))
+		_debug("fscache writeout timeout page: %p{%lx}",
+			page, page->index);
+
 	gfp &= ~__GFP_WAIT;
 	goto try_again;
 }

commit 743162013d40ca612b4cb53d3a200dff2d9ab26e
Author: NeilBrown <neilb@suse.de>
Date:   Mon Jul 7 15:16:04 2014 +1000

    sched: Remove proliferation of wait_on_bit() action functions
    
    The current "wait_on_bit" interface requires an 'action'
    function to be provided which does the actual waiting.
    There are over 20 such functions, many of them identical.
    Most cases can be satisfied by one of just two functions, one
    which uses io_schedule() and one which just uses schedule().
    
    So:
     Rename wait_on_bit and        wait_on_bit_lock to
            wait_on_bit_action and wait_on_bit_lock_action
     to make it explicit that they need an action function.
    
     Introduce new wait_on_bit{,_lock} and wait_on_bit{,_lock}_io
     which are *not* given an action function but implicitly use
     a standard one.
     The decision to error-out if a signal is pending is now made
     based on the 'mode' argument rather than being encoded in the action
     function.
    
     All instances of the old wait_on_bit and wait_on_bit_lock which
     can use the new version have been changed accordingly and their
     action functions have been discarded.
     wait_on_bit{_lock} does not return any specific error code in the
     event of a signal so the caller must check for non-zero and
     interpolate their own error code as appropriate.
    
    The wait_on_bit() call in __fscache_wait_on_invalidate() was
    ambiguous as it specified TASK_UNINTERRUPTIBLE but used
    fscache_wait_bit_interruptible as an action function.
    David Howells confirms this should be uniformly
    "uninterruptible"
    
    The main remaining user of wait_on_bit{,_lock}_action is NFS
    which needs to use a freezer-aware schedule() call.
    
    A comment in fs/gfs2/glock.c notes that having multiple 'action'
    functions is useful as they display differently in the 'wchan'
    field of 'ps'. (and /proc/$PID/wchan).
    As the new bit_wait{,_io} functions are tagged "__sched", they
    will not show up at all, but something higher in the stack.  So
    the distinction will still be visible, only with different
    function names (gds2_glock_wait versus gfs2_glock_dq_wait in the
    gfs2/glock.c case).
    
    Since first version of this patch (against 3.15) two new action
    functions appeared, on in NFS and one in CIFS.  CIFS also now
    uses an action function that makes the same freezer aware
    schedule call as NFS.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Acked-by: David Howells <dhowells@redhat.com> (fscache, keys)
    Acked-by: Steven Whitehouse <swhiteho@redhat.com> (gfs2)
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steve French <sfrench@samba.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20140707051603.28027.72349.stgit@notabene.brown
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index ed70714503fa..85332b9d19d1 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -298,7 +298,6 @@ int fscache_wait_for_deferred_lookup(struct fscache_cookie *cookie)
 
 	jif = jiffies;
 	if (wait_on_bit(&cookie->flags, FSCACHE_COOKIE_LOOKING_UP,
-			fscache_wait_bit_interruptible,
 			TASK_INTERRUPTIBLE) != 0) {
 		fscache_stat(&fscache_n_retrievals_intr);
 		_leave(" = -ERESTARTSYS");
@@ -342,7 +341,6 @@ int fscache_wait_for_operation_activation(struct fscache_object *object,
 	if (stat_op_waits)
 		fscache_stat(stat_op_waits);
 	if (wait_on_bit(&op->flags, FSCACHE_OP_WAITING,
-			fscache_wait_bit_interruptible,
 			TASK_INTERRUPTIBLE) != 0) {
 		ret = fscache_cancel_op(op, do_cancel);
 		if (ret == 0)
@@ -351,7 +349,7 @@ int fscache_wait_for_operation_activation(struct fscache_object *object,
 		/* it's been removed from the pending queue by another party,
 		 * so we should get to run shortly */
 		wait_on_bit(&op->flags, FSCACHE_OP_WAITING,
-			    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+			    TASK_UNINTERRUPTIBLE);
 	}
 	_debug("<<< GO");
 

commit 36dfd116edd48fa6174d5694c143f1d4bd81aba8
Author: Fabian Frederick <fabf@skynet.be>
Date:   Wed Jun 4 16:05:38 2014 -0700

    fs/fscache: convert printk to pr_foo()
    
    All printk converted to pr_foo() except internal.h: printk(KERN_DEBUG
    
    Coalesce formats.
    
    Add pr_fmt
    
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Cc: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 7f5c658af755..ed70714503fa 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -1108,10 +1108,8 @@ void fscache_mark_page_cached(struct fscache_retrieval *op, struct page *page)
 		static bool once_only;
 		if (!once_only) {
 			once_only = true;
-			printk(KERN_WARNING "FS-Cache:"
-			       " Cookie type %s marked page %lx"
-			       " multiple times\n",
-			       cookie->def->name, page->index);
+			pr_warn("Cookie type %s marked page %lx multiple times\n",
+				cookie->def->name, page->index);
 		}
 	}
 

commit 94d30ae90a00cafe686c1057be57f4885f963abf
Author: David Howells <dhowells@redhat.com>
Date:   Sat Sep 21 00:09:31 2013 +0100

    FS-Cache: Provide the ability to enable/disable cookies
    
    Provide the ability to enable and disable fscache cookies.  A disabled cookie
    will reject or ignore further requests to:
    
            Acquire a child cookie
            Invalidate and update backing objects
            Check the consistency of a backing object
            Allocate storage for backing page
            Read backing pages
            Write to backing pages
    
    but still allows:
    
            Checks/waits on the completion of already in-progress objects
            Uncaching of pages
            Relinquishment of cookies
    
    Two new operations are provided:
    
     (1) Disable a cookie:
    
            void fscache_disable_cookie(struct fscache_cookie *cookie,
                                        bool invalidate);
    
         If the cookie is not already disabled, this locks the cookie against other
         dis/enablement ops, marks the cookie as being disabled, discards or
         invalidates any backing objects and waits for cessation of activity on any
         associated object.
    
         This is a wrapper around a chunk split out of fscache_relinquish_cookie(),
         but it reinitialises the cookie such that it can be reenabled.
    
         All possible failures are handled internally.  The caller should consider
         calling fscache_uncache_all_inode_pages() afterwards to make sure all page
         markings are cleared up.
    
     (2) Enable a cookie:
    
            void fscache_enable_cookie(struct fscache_cookie *cookie,
                                       bool (*can_enable)(void *data),
                                       void *data)
    
         If the cookie is not already enabled, this locks the cookie against other
         dis/enablement ops, invokes can_enable() and, if the cookie is not an
         index cookie, will begin the procedure of acquiring backing objects.
    
         The optional can_enable() function is passed the data argument and returns
         a ruling as to whether or not enablement should actually be permitted to
         begin.
    
         All possible failures are handled internally.  The cookie will only be
         marked as enabled if provisional backing objects are allocated.
    
    A later patch will introduce these to NFS.  Cookie enablement during nfs_open()
    is then contingent on i_writecount <= 0.  can_enable() checks for a race
    between open(O_RDONLY) and open(O_WRONLY/O_RDWR).  This simplifies NFS's cookie
    handling and allows us to get rid of open(O_RDONLY) accidentally introducing
    caching to an inode that's open for writing already.
    
    One operation has its API modified:
    
     (3) Acquire a cookie.
    
            struct fscache_cookie *fscache_acquire_cookie(
                    struct fscache_cookie *parent,
                    const struct fscache_cookie_def *def,
                    void *netfs_data,
                    bool enable);
    
         This now has an additional argument that indicates whether the requested
         cookie should be enabled by default.  It doesn't need the can_enable()
         function because the caller must prevent multiple calls for the same netfs
         object and it doesn't need to take the enablement lock because no one else
         can get at the cookie before this returns.
    
    Signed-off-by: David Howells <dhowells@redhat.com

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 0fe42a6d0e9c..7f5c658af755 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -204,7 +204,8 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 
 	spin_lock(&cookie->lock);
 
-	if (hlist_empty(&cookie->backing_objects))
+	if (!fscache_cookie_enabled(cookie) ||
+	    hlist_empty(&cookie->backing_objects))
 		goto nobufs;
 	object = hlist_entry(cookie->backing_objects.first,
 			     struct fscache_object, cookie_link);
@@ -410,7 +411,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 		return -ERESTARTSYS;
 
 	op = fscache_alloc_retrieval(cookie, page->mapping,
-				     end_io_func,context);
+				     end_io_func, context);
 	if (!op) {
 		_leave(" = -ENOMEM");
 		return -ENOMEM;
@@ -419,7 +420,8 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 
 	spin_lock(&cookie->lock);
 
-	if (hlist_empty(&cookie->backing_objects))
+	if (!fscache_cookie_enabled(cookie) ||
+	    hlist_empty(&cookie->backing_objects))
 		goto nobufs_unlock;
 	object = hlist_entry(cookie->backing_objects.first,
 			     struct fscache_object, cookie_link);
@@ -551,7 +553,8 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 
 	spin_lock(&cookie->lock);
 
-	if (hlist_empty(&cookie->backing_objects))
+	if (!fscache_cookie_enabled(cookie) ||
+	    hlist_empty(&cookie->backing_objects))
 		goto nobufs_unlock;
 	object = hlist_entry(cookie->backing_objects.first,
 			     struct fscache_object, cookie_link);
@@ -666,7 +669,8 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 
 	spin_lock(&cookie->lock);
 
-	if (hlist_empty(&cookie->backing_objects))
+	if (!fscache_cookie_enabled(cookie) ||
+	    hlist_empty(&cookie->backing_objects))
 		goto nobufs_unlock;
 	object = hlist_entry(cookie->backing_objects.first,
 			     struct fscache_object, cookie_link);
@@ -938,7 +942,8 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	ret = -ENOBUFS;
 	spin_lock(&cookie->lock);
 
-	if (hlist_empty(&cookie->backing_objects))
+	if (!fscache_cookie_enabled(cookie) ||
+	    hlist_empty(&cookie->backing_objects))
 		goto nobufs;
 	object = hlist_entry(cookie->backing_objects.first,
 			     struct fscache_object, cookie_link);

commit 8fb883f3e30065529e4f35d4b4f355193dcdb7a2
Author: David Howells <dhowells@redhat.com>
Date:   Sat Sep 21 00:09:31 2013 +0100

    FS-Cache: Add use/unuse/wake cookie wrappers
    
    Add wrapper functions for dealing with cookie->n_active:
    
     (*) __fscache_use_cookie() to increment it.
    
     (*) __fscache_unuse_cookie() to decrement and test against zero.
    
     (*) __fscache_wake_unused_cookie() to wake up anyone waiting for it to reach
         zero.
    
    The second and third are split so that the third can be done after cookie->lock
    has been released in case the waiter wakes up whilst we're still holding it and
    tries to get it.
    
    We will need to wake-on-zero once the cookie disablement patch is applied
    because it will then be possible to see n_active become zero without the cookie
    being relinquished.
    
    Also move the cookie usement out of fscache_attr_changed_op() and into
    fscache_attr_changed() and the operation struct so that cookie disablement
    will be able to track it.
    
    Whilst we're at it, only increment n_active if we're about to do
    fscache_submit_op() so that we don't have to deal with undoing it if anything
    earlier fails.  Possibly this should be moved into fscache_submit_op() which
    could look at FSCACHE_OP_UNUSE_COOKIE.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 73899c1c3449..0fe42a6d0e9c 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -163,12 +163,10 @@ static void fscache_attr_changed_op(struct fscache_operation *op)
 
 	fscache_stat(&fscache_n_attr_changed_calls);
 
-	if (fscache_object_is_active(object) &&
-	    fscache_use_cookie(object)) {
+	if (fscache_object_is_active(object)) {
 		fscache_stat(&fscache_n_cop_attr_changed);
 		ret = object->cache->ops->attr_changed(object);
 		fscache_stat_d(&fscache_n_cop_attr_changed);
-		fscache_unuse_cookie(object);
 		if (ret < 0)
 			fscache_abort_object(object);
 	}
@@ -184,6 +182,7 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 {
 	struct fscache_operation *op;
 	struct fscache_object *object;
+	bool wake_cookie;
 
 	_enter("%p", cookie);
 
@@ -199,7 +198,9 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 	}
 
 	fscache_operation_init(op, fscache_attr_changed_op, NULL);
-	op->flags = FSCACHE_OP_ASYNC | (1 << FSCACHE_OP_EXCLUSIVE);
+	op->flags = FSCACHE_OP_ASYNC |
+		(1 << FSCACHE_OP_EXCLUSIVE) |
+		(1 << FSCACHE_OP_UNUSE_COOKIE);
 
 	spin_lock(&cookie->lock);
 
@@ -208,6 +209,7 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 	object = hlist_entry(cookie->backing_objects.first,
 			     struct fscache_object, cookie_link);
 
+	__fscache_use_cookie(cookie);
 	if (fscache_submit_exclusive_op(object, op) < 0)
 		goto nobufs;
 	spin_unlock(&cookie->lock);
@@ -217,8 +219,11 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 	return 0;
 
 nobufs:
+	wake_cookie = __fscache_unuse_cookie(cookie);
 	spin_unlock(&cookie->lock);
 	kfree(op);
+	if (wake_cookie)
+		__fscache_wake_unused_cookie(cookie);
 	fscache_stat(&fscache_n_attr_changed_nobufs);
 	_leave(" = %d", -ENOBUFS);
 	return -ENOBUFS;
@@ -263,7 +268,6 @@ static struct fscache_retrieval *fscache_alloc_retrieval(
 	}
 
 	fscache_operation_init(&op->op, NULL, fscache_release_retrieval_op);
-	atomic_inc(&cookie->n_active);
 	op->op.flags	= FSCACHE_OP_MYTHREAD |
 		(1UL << FSCACHE_OP_WAITING) |
 		(1UL << FSCACHE_OP_UNUSE_COOKIE);
@@ -384,6 +388,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 {
 	struct fscache_retrieval *op;
 	struct fscache_object *object;
+	bool wake_cookie = false;
 	int ret;
 
 	_enter("%p,%p,,,", cookie, page);
@@ -421,6 +426,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 
 	ASSERT(test_bit(FSCACHE_OBJECT_IS_LOOKED_UP, &object->flags));
 
+	__fscache_use_cookie(cookie);
 	atomic_inc(&object->n_reads);
 	__set_bit(FSCACHE_OP_DEC_READ_CNT, &op->op.flags);
 
@@ -475,9 +481,11 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 
 nobufs_unlock_dec:
 	atomic_dec(&object->n_reads);
+	wake_cookie = __fscache_unuse_cookie(cookie);
 nobufs_unlock:
 	spin_unlock(&cookie->lock);
-	atomic_dec(&cookie->n_active);
+	if (wake_cookie)
+		__fscache_wake_unused_cookie(cookie);
 	kfree(op);
 nobufs:
 	fscache_stat(&fscache_n_retrievals_nobufs);
@@ -514,6 +522,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 {
 	struct fscache_retrieval *op;
 	struct fscache_object *object;
+	bool wake_cookie = false;
 	int ret;
 
 	_enter("%p,,%d,,,", cookie, *nr_pages);
@@ -547,6 +556,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	object = hlist_entry(cookie->backing_objects.first,
 			     struct fscache_object, cookie_link);
 
+	__fscache_use_cookie(cookie);
 	atomic_inc(&object->n_reads);
 	__set_bit(FSCACHE_OP_DEC_READ_CNT, &op->op.flags);
 
@@ -601,10 +611,12 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 
 nobufs_unlock_dec:
 	atomic_dec(&object->n_reads);
+	wake_cookie = __fscache_unuse_cookie(cookie);
 nobufs_unlock:
 	spin_unlock(&cookie->lock);
-	atomic_dec(&cookie->n_active);
 	kfree(op);
+	if (wake_cookie)
+		__fscache_wake_unused_cookie(cookie);
 nobufs:
 	fscache_stat(&fscache_n_retrievals_nobufs);
 	_leave(" = -ENOBUFS");
@@ -626,6 +638,7 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 {
 	struct fscache_retrieval *op;
 	struct fscache_object *object;
+	bool wake_cookie = false;
 	int ret;
 
 	_enter("%p,%p,,,", cookie, page);
@@ -658,8 +671,9 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	object = hlist_entry(cookie->backing_objects.first,
 			     struct fscache_object, cookie_link);
 
+	__fscache_use_cookie(cookie);
 	if (fscache_submit_op(object, &op->op) < 0)
-		goto nobufs_unlock;
+		goto nobufs_unlock_dec;
 	spin_unlock(&cookie->lock);
 
 	fscache_stat(&fscache_n_alloc_ops);
@@ -689,10 +703,13 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	_leave(" = %d", ret);
 	return ret;
 
+nobufs_unlock_dec:
+	wake_cookie = __fscache_unuse_cookie(cookie);
 nobufs_unlock:
 	spin_unlock(&cookie->lock);
-	atomic_dec(&cookie->n_active);
 	kfree(op);
+	if (wake_cookie)
+		__fscache_wake_unused_cookie(cookie);
 nobufs:
 	fscache_stat(&fscache_n_allocs_nobufs);
 	_leave(" = -ENOBUFS");
@@ -889,6 +906,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 {
 	struct fscache_storage *op;
 	struct fscache_object *object;
+	bool wake_cookie = false;
 	int ret;
 
 	_enter("%p,%x,", cookie, (u32) page->flags);
@@ -957,7 +975,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	op->op.debug_id	= atomic_inc_return(&fscache_op_debug_id);
 	op->store_limit = object->store_limit;
 
-	atomic_inc(&cookie->n_active);
+	__fscache_use_cookie(cookie);
 	if (fscache_submit_op(object, &op->op) < 0)
 		goto submit_failed;
 
@@ -984,10 +1002,10 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	return 0;
 
 submit_failed:
-	atomic_dec(&cookie->n_active);
 	spin_lock(&cookie->stores_lock);
 	radix_tree_delete(&cookie->stores, page->index);
 	spin_unlock(&cookie->stores_lock);
+	wake_cookie = __fscache_unuse_cookie(cookie);
 	page_cache_release(page);
 	ret = -ENOBUFS;
 	goto nobufs;
@@ -999,6 +1017,8 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	spin_unlock(&cookie->lock);
 	radix_tree_preload_end();
 	kfree(op);
+	if (wake_cookie)
+		__fscache_wake_unused_cookie(cookie);
 	fscache_stat(&fscache_n_stores_nobufs);
 	_leave(" = -ENOBUFS");
 	return -ENOBUFS;

commit 5e4c0d974139a98741b829b27cf38dc8f9284490
Author: Jan Kara <jack@suse.cz>
Date:   Wed Sep 11 14:26:05 2013 -0700

    lib/radix-tree.c: make radix_tree_node_alloc() work correctly within interrupt
    
    With users of radix_tree_preload() run from interrupt (block/blk-ioc.c is
    one such possible user), the following race can happen:
    
    radix_tree_preload()
    ...
    radix_tree_insert()
      radix_tree_node_alloc()
        if (rtp->nr) {
          ret = rtp->nodes[rtp->nr - 1];
    <interrupt>
    ...
    radix_tree_preload()
    ...
    radix_tree_insert()
      radix_tree_node_alloc()
        if (rtp->nr) {
          ret = rtp->nodes[rtp->nr - 1];
    
    And we give out one radix tree node twice.  That clearly results in radix
    tree corruption with different results (usually OOPS) depending on which
    two users of radix tree race.
    
    We fix the problem by making radix_tree_node_alloc() always allocate fresh
    radix tree nodes when in interrupt.  Using preloading when in interrupt
    doesn't make sense since all the allocations have to be atomic anyway and
    we cannot steal nodes from process-context users because some users rely
    on radix_tree_insert() succeeding after radix_tree_preload().
    in_interrupt() check is somewhat ugly but we cannot simply key off passed
    gfp_mask as that is acquired from root_gfp_mask() and thus the same for
    all preload users.
    
    Another part of the fix is to avoid node preallocation in
    radix_tree_preload() when passed gfp_mask doesn't allow waiting.  Again,
    preallocation in such case doesn't make sense and when preallocation would
    happen in interrupt we could possibly leak some allocated nodes.  However,
    some users of radix_tree_preload() require following radix_tree_insert()
    to succeed.  To avoid unexpected effects for these users,
    radix_tree_preload() only warns if passed gfp mask doesn't allow waiting
    and we provide a new function radix_tree_maybe_preload() for those users
    which get different gfp mask from different call sites and which are
    prepared to handle radix_tree_insert() failure.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <jaxboe@fusionio.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 8702b732109a..73899c1c3449 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -913,7 +913,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 		(1 << FSCACHE_OP_WAITING) |
 		(1 << FSCACHE_OP_UNUSE_COOKIE);
 
-	ret = radix_tree_preload(gfp & ~__GFP_HIGHMEM);
+	ret = radix_tree_maybe_preload(gfp & ~__GFP_HIGHMEM);
 	if (ret < 0)
 		goto nomem_free;
 

commit 5a6f282a2052bb13171b53f03b34501cf72c33f1
Author: Milosz Tanski <milosz@adfin.com>
Date:   Wed Aug 21 17:30:11 2013 -0400

    fscache: Netfs function for cleanup post readpages
    
    Currently the fscache code expect the netfs to call fscache_readpages_or_alloc
    inside the aops readpages callback.  It marks all the pages in the list
    provided by readahead with PG_private_2.  In the cases that the netfs fails to
    read all the pages (which is legal) it ends up returning to the readahead and
    triggering a BUG.  This happens because the page list still contains marked
    pages.
    
    This patch implements a simple fscache_readpages_cancel function that the netfs
    should call before returning from readpages.  It will revoke the pages from the
    underlying cache backend and unmark them.
    
    The problem was originally worked out in the Ceph devel tree, but it also
    occurs in CIFS.  It appears that NFS, AFS and 9P are okay as read_cache_pages()
    will clean up the unprocessed pages in the case of an error.
    
    This can be used to address the following oops:
    
    [12410647.597278] BUG: Bad page state in process petabucket  pfn:3d504e
    [12410647.597292] page:ffffea000f541380 count:0 mapcount:0 mapping:
            (null) index:0x0
    [12410647.597298] page flags: 0x200000000001000(private_2)
    
    ...
    
    [12410647.597334] Call Trace:
    [12410647.597345]  [<ffffffff815523f2>] dump_stack+0x19/0x1b
    [12410647.597356]  [<ffffffff8111def7>] bad_page+0xc7/0x120
    [12410647.597359]  [<ffffffff8111e49e>] free_pages_prepare+0x10e/0x120
    [12410647.597361]  [<ffffffff8111fc80>] free_hot_cold_page+0x40/0x170
    [12410647.597363]  [<ffffffff81123507>] __put_single_page+0x27/0x30
    [12410647.597365]  [<ffffffff81123df5>] put_page+0x25/0x40
    [12410647.597376]  [<ffffffffa02bdcf9>] ceph_readpages+0x2e9/0x6e0 [ceph]
    [12410647.597379]  [<ffffffff81122a8f>] __do_page_cache_readahead+0x1af/0x260
    [12410647.597382]  [<ffffffff81122ea1>] ra_submit+0x21/0x30
    [12410647.597384]  [<ffffffff81118f64>] filemap_fault+0x254/0x490
    [12410647.597387]  [<ffffffff8113a74f>] __do_fault+0x6f/0x4e0
    [12410647.597391]  [<ffffffff810125bd>] ? __switch_to+0x16d/0x4a0
    [12410647.597395]  [<ffffffff810865ba>] ? finish_task_switch+0x5a/0xc0
    [12410647.597398]  [<ffffffff8113d856>] handle_pte_fault+0xf6/0x930
    [12410647.597401]  [<ffffffff81008c33>] ? pte_mfn_to_pfn+0x93/0x110
    [12410647.597403]  [<ffffffff81008cce>] ? xen_pmd_val+0xe/0x10
    [12410647.597405]  [<ffffffff81005469>] ? __raw_callee_save_xen_pmd_val+0x11/0x1e
    [12410647.597407]  [<ffffffff8113f361>] handle_mm_fault+0x251/0x370
    [12410647.597411]  [<ffffffff812b0ac4>] ? call_rwsem_down_read_failed+0x14/0x30
    [12410647.597414]  [<ffffffff8155bffa>] __do_page_fault+0x1aa/0x550
    [12410647.597418]  [<ffffffff8108011d>] ? up_write+0x1d/0x20
    [12410647.597422]  [<ffffffff8113141c>] ? vm_mmap_pgoff+0xbc/0xe0
    [12410647.597425]  [<ffffffff81143bb8>] ? SyS_mmap_pgoff+0xd8/0x240
    [12410647.597427]  [<ffffffff8155c3ae>] do_page_fault+0xe/0x10
    [12410647.597431]  [<ffffffff81558818>] page_fault+0x28/0x30
    
    Signed-off-by: Milosz Tanski <milosz@adfin.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 793e3d5ca4b5..8702b732109a 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -700,6 +700,22 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 }
 EXPORT_SYMBOL(__fscache_alloc_page);
 
+/*
+ * Unmark pages allocate in the readahead code path (via:
+ * fscache_readpages_or_alloc) after delegating to the base filesystem
+ */
+void __fscache_readpages_cancel(struct fscache_cookie *cookie,
+				struct list_head *pages)
+{
+	struct page *page;
+
+	list_for_each_entry(page, pages, lru) {
+		if (PageFsCache(page))
+			__fscache_uncache_page(cookie, page);
+	}
+}
+EXPORT_SYMBOL(__fscache_readpages_cancel);
+
 /*
  * release a write op reference
  */

commit da9803bc8812f5bd3b26baaa90e515b843c65ff7
Author: David Howells <dhowells@redhat.com>
Date:   Wed Aug 21 17:29:38 2013 -0400

    FS-Cache: Add interface to check consistency of a cached object
    
    Extend the fscache netfs API so that the netfs can ask as to whether a cache
    object is up to date with respect to its corresponding netfs object:
    
            int fscache_check_consistency(struct fscache_cookie *cookie)
    
    This will call back to the netfs to check whether the auxiliary data associated
    with a cookie is correct.  It returns 0 if it is and -ESTALE if it isn't; it
    may also return -ENOMEM and -ERESTARTSYS.
    
    The backends now have to implement a mandatory operation pointer:
    
            int (*check_consistency)(struct fscache_object *object)
    
    that corresponds to the above API call.  FS-Cache takes care of pinning the
    object and the cookie in memory and managing this call with respect to the
    object state.
    
    Original-author: Hongyi Jia <jiayisuse@gmail.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Hongyi Jia <jiayisuse@gmail.com>
    cc: Milosz Tanski <milosz@adfin.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index d479ab3c63e4..793e3d5ca4b5 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -278,7 +278,7 @@ static struct fscache_retrieval *fscache_alloc_retrieval(
 /*
  * wait for a deferred lookup to complete
  */
-static int fscache_wait_for_deferred_lookup(struct fscache_cookie *cookie)
+int fscache_wait_for_deferred_lookup(struct fscache_cookie *cookie)
 {
 	unsigned long jif;
 
@@ -322,42 +322,46 @@ static void fscache_do_cancel_retrieval(struct fscache_operation *_op)
 /*
  * wait for an object to become active (or dead)
  */
-static int fscache_wait_for_retrieval_activation(struct fscache_object *object,
-						 struct fscache_retrieval *op,
-						 atomic_t *stat_op_waits,
-						 atomic_t *stat_object_dead)
+int fscache_wait_for_operation_activation(struct fscache_object *object,
+					  struct fscache_operation *op,
+					  atomic_t *stat_op_waits,
+					  atomic_t *stat_object_dead,
+					  void (*do_cancel)(struct fscache_operation *))
 {
 	int ret;
 
-	if (!test_bit(FSCACHE_OP_WAITING, &op->op.flags))
+	if (!test_bit(FSCACHE_OP_WAITING, &op->flags))
 		goto check_if_dead;
 
 	_debug(">>> WT");
-	fscache_stat(stat_op_waits);
-	if (wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+	if (stat_op_waits)
+		fscache_stat(stat_op_waits);
+	if (wait_on_bit(&op->flags, FSCACHE_OP_WAITING,
 			fscache_wait_bit_interruptible,
 			TASK_INTERRUPTIBLE) != 0) {
-		ret = fscache_cancel_op(&op->op, fscache_do_cancel_retrieval);
+		ret = fscache_cancel_op(op, do_cancel);
 		if (ret == 0)
 			return -ERESTARTSYS;
 
 		/* it's been removed from the pending queue by another party,
 		 * so we should get to run shortly */
-		wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+		wait_on_bit(&op->flags, FSCACHE_OP_WAITING,
 			    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
 	}
 	_debug("<<< GO");
 
 check_if_dead:
-	if (op->op.state == FSCACHE_OP_ST_CANCELLED) {
-		fscache_stat(stat_object_dead);
+	if (op->state == FSCACHE_OP_ST_CANCELLED) {
+		if (stat_object_dead)
+			fscache_stat(stat_object_dead);
 		_leave(" = -ENOBUFS [cancelled]");
 		return -ENOBUFS;
 	}
 	if (unlikely(fscache_object_is_dead(object))) {
-		pr_err("%s() = -ENOBUFS [obj dead %d]\n", __func__, op->op.state);
-		fscache_cancel_op(&op->op, fscache_do_cancel_retrieval);
-		fscache_stat(stat_object_dead);
+		pr_err("%s() = -ENOBUFS [obj dead %d]\n", __func__, op->state);
+		fscache_cancel_op(op, do_cancel);
+		if (stat_object_dead)
+			fscache_stat(stat_object_dead);
 		return -ENOBUFS;
 	}
 	return 0;
@@ -432,10 +436,11 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 
 	/* we wait for the operation to become active, and then process it
 	 * *here*, in this thread, and not in the thread pool */
-	ret = fscache_wait_for_retrieval_activation(
-		object, op,
+	ret = fscache_wait_for_operation_activation(
+		object, &op->op,
 		__fscache_stat(&fscache_n_retrieval_op_waits),
-		__fscache_stat(&fscache_n_retrievals_object_dead));
+		__fscache_stat(&fscache_n_retrievals_object_dead),
+		fscache_do_cancel_retrieval);
 	if (ret < 0)
 		goto error;
 
@@ -557,10 +562,11 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 
 	/* we wait for the operation to become active, and then process it
 	 * *here*, in this thread, and not in the thread pool */
-	ret = fscache_wait_for_retrieval_activation(
-		object, op,
+	ret = fscache_wait_for_operation_activation(
+		object, &op->op,
 		__fscache_stat(&fscache_n_retrieval_op_waits),
-		__fscache_stat(&fscache_n_retrievals_object_dead));
+		__fscache_stat(&fscache_n_retrievals_object_dead),
+		fscache_do_cancel_retrieval);
 	if (ret < 0)
 		goto error;
 
@@ -658,10 +664,11 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 
 	fscache_stat(&fscache_n_alloc_ops);
 
-	ret = fscache_wait_for_retrieval_activation(
-		object, op,
+	ret = fscache_wait_for_operation_activation(
+		object, &op->op,
 		__fscache_stat(&fscache_n_alloc_op_waits),
-		__fscache_stat(&fscache_n_allocs_object_dead));
+		__fscache_stat(&fscache_n_allocs_object_dead),
+		fscache_do_cancel_retrieval);
 	if (ret < 0)
 		goto error;
 

commit 1bb4b7f98f361132ea322834515334d95b93c184
Author: David Howells <dhowells@redhat.com>
Date:   Tue May 21 13:44:15 2013 +0100

    FS-Cache: The retrieval remaining-pages counter needs to be atomic_t
    
    struct fscache_retrieval contains a count of the number of pages that still
    need some processing (n_pages).  This is decremented as the pages are
    processed.
    
    However, this needs to be atomic as fscache_retrieval_complete() (I think) just
    occasionally may be called from cachefiles_read_backing_file() and
    cachefiles_read_copier() simultaneously.
    
    This happens when an fscache_read_or_alloc_pages() request containing a lot of
    pages (say a couple of hundred) is being processed.  The read on each backing
    page is dispatched individually because we need to insert a monitor into the
    waitqueue to catch when the read completes.  However, under low-memory
    conditions, we might be forced to wait in the allocator - and this gives the
    I/O on the backing page a chance to complete first.
    
    When the I/O completes, fscache_enqueue_retrieval() chucks the retrieval onto
    the workqueue without waiting for the operation to finish the initial I/O
    dispatch (we want to release any pages we can as soon as we can), thus both can
    end up running simultaneously and potentially attempting to partially complete
    the retrieval simultaneously (ENOMEM may occur, backing pages may already be in
    the page cache).
    
    This was demonstrated by parallelling the non-atomic counter with an atomic
    counter and printing both of them when the assertion fails.  At this point, the
    atomic counter has reached zero, but the non-atomic counter has not.
    
    To fix this, make the counter an atomic_t.
    
    This results in the following bug appearing
    
            FS-Cache: Assertion failed
            3 == 5 is false
            ------------[ cut here ]------------
            kernel BUG at fs/fscache/operation.c:421!
    
    or
    
            FS-Cache: Assertion failed
            3 == 5 is false
            ------------[ cut here ]------------
            kernel BUG at fs/fscache/operation.c:414!
    
    With a backtrace like the following:
    
    RIP: 0010:[<ffffffffa0211b1d>] fscache_put_operation+0x1ad/0x240 [fscache]
    Call Trace:
     [<ffffffffa0213185>] fscache_retrieval_work+0x55/0x270 [fscache]
     [<ffffffffa0213130>] ? fscache_retrieval_work+0x0/0x270 [fscache]
     [<ffffffff81090b10>] worker_thread+0x170/0x2a0
     [<ffffffff81096d10>] ? autoremove_wake_function+0x0/0x40
     [<ffffffff810909a0>] ? worker_thread+0x0/0x2a0
     [<ffffffff81096966>] kthread+0x96/0xa0
     [<ffffffff8100c0ca>] child_rip+0xa/0x20
     [<ffffffff810968d0>] ? kthread+0x0/0xa0
     [<ffffffff8100c0c0>] ? child_rip+0x0/0x20
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-and-tested-By: Milosz Tanski <milosz@adfin.com>
    Acked-by: Jeff Layton <jlayton@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 780bac6ffde5..d479ab3c63e4 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -235,7 +235,7 @@ static void fscache_release_retrieval_op(struct fscache_operation *_op)
 
 	_enter("{OP%x}", op->op.debug_id);
 
-	ASSERTCMP(op->n_pages, ==, 0);
+	ASSERTCMP(atomic_read(&op->n_pages), ==, 0);
 
 	fscache_hist(fscache_retrieval_histogram, op->start_time);
 	if (op->context)
@@ -316,7 +316,7 @@ static void fscache_do_cancel_retrieval(struct fscache_operation *_op)
 	struct fscache_retrieval *op =
 		container_of(_op, struct fscache_retrieval, op);
 
-	op->n_pages = 0;
+	atomic_set(&op->n_pages, 0);
 }
 
 /*
@@ -406,7 +406,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 		_leave(" = -ENOMEM");
 		return -ENOMEM;
 	}
-	op->n_pages = 1;
+	atomic_set(&op->n_pages, 1);
 
 	spin_lock(&cookie->lock);
 
@@ -533,7 +533,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	op = fscache_alloc_retrieval(cookie, mapping, end_io_func, context);
 	if (!op)
 		return -ENOMEM;
-	op->n_pages = *nr_pages;
+	atomic_set(&op->n_pages, *nr_pages);
 
 	spin_lock(&cookie->lock);
 
@@ -643,7 +643,7 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	op = fscache_alloc_retrieval(cookie, page->mapping, NULL, NULL);
 	if (!op)
 		return -ENOMEM;
-	op->n_pages = 1;
+	atomic_set(&op->n_pages, 1);
 
 	spin_lock(&cookie->lock);
 

commit 1362729b169b7903c7e739dbe7904994b0d8c47f
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 10 19:50:26 2013 +0100

    FS-Cache: Simplify cookie retention for fscache_objects, fixing oops
    
    Simplify the way fscache cache objects retain their cookie.  The way I
    implemented the cookie storage handling made synchronisation a pain (ie. the
    object state machine can't rely on the cookie actually still being there).
    
    Instead of the the object being detached from the cookie and the cookie being
    freed in __fscache_relinquish_cookie(), we defer both operations:
    
     (*) The detachment of the object from the list in the cookie now takes place
         in fscache_drop_object() and is thus governed by the object state machine
         (fscache_detach_from_cookie() has been removed).
    
     (*) The release of the cookie is now in fscache_object_destroy() - which is
         called by the cache backend just before it frees the object.
    
    This means that the fscache_cookie struct is now available to the cache all the
    way through from ->alloc_object() to ->drop_object() and ->put_object() -
    meaning that it's no longer necessary to take object->lock to guarantee access.
    
    However, __fscache_relinquish_cookie() doesn't wait for the object to go all
    the way through to destruction before letting the netfs proceed.  That would
    massively slow down the netfs.  Since __fscache_relinquish_cookie() leaves the
    cookie around, in must therefore break all attachments to the netfs - which
    includes ->def, ->netfs_data and any outstanding page read/writes.
    
    To handle this, struct fscache_cookie now has an n_active counter:
    
     (1) This starts off initialised to 1.
    
     (2) Any time the cache needs to get at the netfs data, it calls
         fscache_use_cookie() to increment it - if it is not zero.  If it was zero,
         then access is not permitted.
    
     (3) When the cache has finished with the data, it calls fscache_unuse_cookie()
         to decrement it.  This does a wake-up on it if it reaches 0.
    
     (4) __fscache_relinquish_cookie() decrements n_active and then waits for it to
         reach 0.  The initialisation to 1 in step (1) ensures that we only get
         wake ups when we're trying to get rid of the cookie.
    
    This leaves __fscache_relinquish_cookie() a lot simpler.
    
    
    ***
    This fixes a problem in the current code whereby if fscache_invalidate() is
    followed sufficiently quickly by fscache_relinquish_cookie() then it is
    possible for __fscache_relinquish_cookie() to have detached the cookie from the
    object and cleared the pointer before a thread is dispatched to process the
    invalidation state in the object state machine.
    
    Since the pending write clearance was deferred to the invalidation state to
    make it asynchronous, we need to either wait in relinquishment for the stores
    tree to be cleared in the invalidation state or we need to handle the clearance
    in relinquishment.
    
    Further, if the relinquishment code does clear the tree, then the invalidation
    state need to make the clearance contingent on still having the cookie to hand
    (since that's where the tree is rooted) and we have to prevent the cookie from
    disappearing for the duration.
    
    This can lead to an oops like the following:
    
    BUG: unable to handle kernel NULL pointer dereference at 000000000000000c
    ...
    RIP: 0010:[<ffffffff8151023e>] _spin_lock+0xe/0x30
    ...
    CR2: 000000000000000c ...
    ...
    Process kslowd002 (...)
    ....
    Call Trace:
     [<ffffffffa01c3278>] fscache_invalidate_writes+0x38/0xd0 [fscache]
     [<ffffffff810096f0>] ? __switch_to+0xd0/0x320
     [<ffffffff8105e759>] ? find_busiest_queue+0x69/0x150
     [<ffffffff8110ddd4>] ? slow_work_enqueue+0x104/0x180
     [<ffffffffa01c1303>] fscache_object_slow_work_execute+0x5e3/0x9d0 [fscache]
     [<ffffffff81096b67>] ? bit_waitqueue+0x17/0xd0
     [<ffffffff8110e233>] slow_work_execute+0x233/0x310
     [<ffffffff8110e515>] slow_work_thread+0x205/0x360
     [<ffffffff81096ca0>] ? autoremove_wake_function+0x0/0x40
     [<ffffffff8110e310>] ? slow_work_thread+0x0/0x360
     [<ffffffff81096936>] kthread+0x96/0xa0
     [<ffffffff8100c0ca>] child_rip+0xa/0x20
     [<ffffffff810968a0>] ? kthread+0x0/0xa0
     [<ffffffff8100c0c0>] ? child_rip+0x0/0x20
    
    The parameter to fscache_invalidate_writes() was object->cookie which is NULL.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-By: Milosz Tanski <milosz@adfin.com>
    Acked-by: Jeff Layton <jlayton@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index b4e4b424160a..780bac6ffde5 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -163,10 +163,12 @@ static void fscache_attr_changed_op(struct fscache_operation *op)
 
 	fscache_stat(&fscache_n_attr_changed_calls);
 
-	if (fscache_object_is_active(object)) {
+	if (fscache_object_is_active(object) &&
+	    fscache_use_cookie(object)) {
 		fscache_stat(&fscache_n_cop_attr_changed);
 		ret = object->cache->ops->attr_changed(object);
 		fscache_stat_d(&fscache_n_cop_attr_changed);
+		fscache_unuse_cookie(object);
 		if (ret < 0)
 			fscache_abort_object(object);
 	}
@@ -246,6 +248,7 @@ static void fscache_release_retrieval_op(struct fscache_operation *_op)
  * allocate a retrieval op
  */
 static struct fscache_retrieval *fscache_alloc_retrieval(
+	struct fscache_cookie *cookie,
 	struct address_space *mapping,
 	fscache_rw_complete_t end_io_func,
 	void *context)
@@ -260,7 +263,10 @@ static struct fscache_retrieval *fscache_alloc_retrieval(
 	}
 
 	fscache_operation_init(&op->op, NULL, fscache_release_retrieval_op);
-	op->op.flags	= FSCACHE_OP_MYTHREAD | (1 << FSCACHE_OP_WAITING);
+	atomic_inc(&cookie->n_active);
+	op->op.flags	= FSCACHE_OP_MYTHREAD |
+		(1UL << FSCACHE_OP_WAITING) |
+		(1UL << FSCACHE_OP_UNUSE_COOKIE);
 	op->mapping	= mapping;
 	op->end_io_func	= end_io_func;
 	op->context	= context;
@@ -394,7 +400,8 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 	if (fscache_wait_for_deferred_lookup(cookie) < 0)
 		return -ERESTARTSYS;
 
-	op = fscache_alloc_retrieval(page->mapping, end_io_func, context);
+	op = fscache_alloc_retrieval(cookie, page->mapping,
+				     end_io_func,context);
 	if (!op) {
 		_leave(" = -ENOMEM");
 		return -ENOMEM;
@@ -465,6 +472,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 	atomic_dec(&object->n_reads);
 nobufs_unlock:
 	spin_unlock(&cookie->lock);
+	atomic_dec(&cookie->n_active);
 	kfree(op);
 nobufs:
 	fscache_stat(&fscache_n_retrievals_nobufs);
@@ -522,7 +530,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	if (fscache_wait_for_deferred_lookup(cookie) < 0)
 		return -ERESTARTSYS;
 
-	op = fscache_alloc_retrieval(mapping, end_io_func, context);
+	op = fscache_alloc_retrieval(cookie, mapping, end_io_func, context);
 	if (!op)
 		return -ENOMEM;
 	op->n_pages = *nr_pages;
@@ -589,6 +597,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	atomic_dec(&object->n_reads);
 nobufs_unlock:
 	spin_unlock(&cookie->lock);
+	atomic_dec(&cookie->n_active);
 	kfree(op);
 nobufs:
 	fscache_stat(&fscache_n_retrievals_nobufs);
@@ -631,7 +640,7 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	if (fscache_wait_for_deferred_lookup(cookie) < 0)
 		return -ERESTARTSYS;
 
-	op = fscache_alloc_retrieval(page->mapping, NULL, NULL);
+	op = fscache_alloc_retrieval(cookie, page->mapping, NULL, NULL);
 	if (!op)
 		return -ENOMEM;
 	op->n_pages = 1;
@@ -675,6 +684,7 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 
 nobufs_unlock:
 	spin_unlock(&cookie->lock);
+	atomic_dec(&cookie->n_active);
 	kfree(op);
 nobufs:
 	fscache_stat(&fscache_n_allocs_nobufs);
@@ -876,7 +886,9 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 
 	fscache_operation_init(&op->op, fscache_write_op,
 			       fscache_release_write_op);
-	op->op.flags = FSCACHE_OP_ASYNC | (1 << FSCACHE_OP_WAITING);
+	op->op.flags = FSCACHE_OP_ASYNC |
+		(1 << FSCACHE_OP_WAITING) |
+		(1 << FSCACHE_OP_UNUSE_COOKIE);
 
 	ret = radix_tree_preload(gfp & ~__GFP_HIGHMEM);
 	if (ret < 0)
@@ -922,6 +934,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	op->op.debug_id	= atomic_inc_return(&fscache_op_debug_id);
 	op->store_limit = object->store_limit;
 
+	atomic_inc(&cookie->n_active);
 	if (fscache_submit_op(object, &op->op) < 0)
 		goto submit_failed;
 
@@ -948,6 +961,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	return 0;
 
 submit_failed:
+	atomic_dec(&cookie->n_active);
 	spin_lock(&cookie->stores_lock);
 	radix_tree_delete(&cookie->stores, page->index);
 	spin_unlock(&cookie->stores_lock);

commit caaef6900befb45689b1d1831ce3c7e7fb5b504f
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 10 19:50:26 2013 +0100

    FS-Cache: Fix object state machine to have separate work and wait states
    
    Fix object state machine to have separate work and wait states as that makes
    it easier to envision.
    
    There are now three kinds of state:
    
     (1) Work state.  This is an execution state.  No event processing is performed
         by a work state.  The function attached to a work state returns a pointer
         indicating the next state to which the OSM should transition.  Returning
         NO_TRANSIT repeats the current state, but goes back to the scheduler
         first.
    
     (2) Wait state.  This is an event processing state.  No execution is
         performed by a wait state.  Wait states are just tables of "if event X
         occurs, clear it and transition to state Y".  The dispatcher returns to
         the scheduler if none of the events in which the wait state has an
         interest are currently pending.
    
     (3) Out-of-band state.  This is a special work state.  Transitions to normal
         states can be overridden when an unexpected event occurs (eg. I/O error).
         Instead the dispatcher disables and clears the OOB event and transits to
         the specified work state.  This then acts as an ordinary work state,
         though object->state points to the overridden destination.  Returning
         NO_TRANSIT resumes the overridden transition.
    
    In addition, the states have names in their definitions, so there's no need for
    tables of state names.  Further, the EV_REQUEUE event is no longer necessary as
    that is automatic for work states.
    
    Since the states are now separate structs rather than values in an enum, it's
    not possible to use comparisons other than (non-)equality between them, so use
    some object->flags to indicate what phase an object is in.
    
    The EV_RELEASE, EV_RETIRE and EV_WITHDRAW events have been squished into one
    (EV_KILL).  An object flag now carries the information about retirement.
    
    Similarly, the RELEASING, RECYCLING and WITHDRAWING states have been merged
    into an KILL_OBJECT state and additional states have been added for handling
    waiting dependent objects (JUMPSTART_DEPS and KILL_DEPENDENTS).
    
    A state has also been added for synchronising with parent object initialisation
    (WAIT_FOR_PARENT) and another for initiating look up (PARENT_READY).
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-By: Milosz Tanski <milosz@adfin.com>
    Acked-by: Jeff Layton <jlayton@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 42f8f2d8a197..b4e4b424160a 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -408,7 +408,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 	object = hlist_entry(cookie->backing_objects.first,
 			     struct fscache_object, cookie_link);
 
-	ASSERTCMP(object->state, >, FSCACHE_OBJECT_LOOKING_UP);
+	ASSERT(test_bit(FSCACHE_OBJECT_IS_LOOKED_UP, &object->flags));
 
 	atomic_inc(&object->n_reads);
 	__set_bit(FSCACHE_OP_DEC_READ_CNT, &op->op.flags);
@@ -729,8 +729,9 @@ static void fscache_write_op(struct fscache_operation *_op)
 		 */
 		spin_unlock(&object->lock);
 		fscache_op_complete(&op->op, false);
-		_leave(" [cancel] op{f=%lx s=%u} obj{s=%u f=%lx}",
-		       _op->flags, _op->state, object->state, object->flags);
+		_leave(" [cancel] op{f=%lx s=%u} obj{s=%s f=%lx}",
+		       _op->flags, _op->state, object->state->short_name,
+		       object->flags);
 		return;
 	}
 
@@ -833,14 +834,12 @@ void fscache_invalidate_writes(struct fscache_cookie *cookie)
  *  (1) negative lookup, object not yet created (FSCACHE_COOKIE_CREATING is
  *      set)
  *
- *	(a) no writes yet (set FSCACHE_COOKIE_PENDING_FILL and queue deferred
- *	    fill op)
+ *	(a) no writes yet
  *
  *	(b) writes deferred till post-creation (mark page for writing and
  *	    return immediately)
  *
  *  (2) negative lookup, object created, initial fill being made from netfs
- *      (FSCACHE_COOKIE_INITIAL_FILL is set)
  *
  *	(a) fill point not yet reached this page (mark page for writing and
  *          return)

commit 0c59a95d90081e5d840649d4f872027123e3420c
Author: David Howells <dhowells@redhat.com>
Date:   Fri May 10 19:50:25 2013 +0100

    FS-Cache: Don't sleep in page release if __GFP_FS is not set
    
    Don't sleep in __fscache_maybe_release_page() if __GFP_FS is not set.  This
    goes some way towards mitigating fscache deadlocking against ext4 by way of
    the allocator, eg:
    
    INFO: task flush-8:0:24427 blocked for more than 120 seconds.
    "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    flush-8:0       D ffff88003e2b9fd8     0 24427      2 0x00000000
     ffff88003e2b9138 0000000000000046 ffff880012e3a040 ffff88003e2b9fd8
     0000000000011c80 ffff88003e2b9fd8 ffffffff81a10400 ffff880012e3a040
     0000000000000002 ffff880012e3a040 ffff88003e2b9098 ffffffff8106dcf5
    Call Trace:
     [<ffffffff8106dcf5>] ? __lock_is_held+0x31/0x53
     [<ffffffff81219b61>] ? radix_tree_lookup_element+0xf4/0x12a
     [<ffffffff81454bed>] schedule+0x60/0x62
     [<ffffffffa01d349c>] __fscache_wait_on_page_write+0x8b/0xa5 [fscache]
     [<ffffffff810498a8>] ? __init_waitqueue_head+0x4d/0x4d
     [<ffffffffa01d393a>] __fscache_maybe_release_page+0x30c/0x324 [fscache]
     [<ffffffffa01d369a>] ? __fscache_maybe_release_page+0x6c/0x324 [fscache]
     [<ffffffff81071b53>] ? trace_hardirqs_on_caller+0x114/0x170
     [<ffffffffa01fd7b2>] nfs_fscache_release_page+0x68/0x94 [nfs]
     [<ffffffffa01ef73e>] nfs_release_page+0x7e/0x86 [nfs]
     [<ffffffff810aa553>] try_to_release_page+0x32/0x3b
     [<ffffffff810b6c70>] shrink_page_list+0x535/0x71a
     [<ffffffff81071b53>] ? trace_hardirqs_on_caller+0x114/0x170
     [<ffffffff810b7352>] shrink_inactive_list+0x20a/0x2dd
     [<ffffffff81071a13>] ? mark_held_locks+0xbe/0xea
     [<ffffffff810b7a65>] shrink_lruvec+0x34c/0x3eb
     [<ffffffff810b7bd3>] do_try_to_free_pages+0xcf/0x355
     [<ffffffff810b7fc8>] try_to_free_pages+0x9a/0xa1
     [<ffffffff810b08d2>] __alloc_pages_nodemask+0x494/0x6f7
     [<ffffffff810d9a07>] kmem_getpages+0x58/0x155
     [<ffffffff810dc002>] fallback_alloc+0x120/0x1f3
     [<ffffffff8106db23>] ? trace_hardirqs_off+0xd/0xf
     [<ffffffff810dbed3>] ____cache_alloc_node+0x177/0x186
     [<ffffffff81162a6c>] ? ext4_init_io_end+0x1c/0x37
     [<ffffffff810dc403>] kmem_cache_alloc+0xf1/0x176
     [<ffffffff810b17ac>] ? test_set_page_writeback+0x101/0x113
     [<ffffffff81162a6c>] ext4_init_io_end+0x1c/0x37
     [<ffffffff81162ce4>] ext4_bio_write_page+0x20f/0x3af
     [<ffffffff8115cc02>] mpage_da_submit_io+0x26e/0x2f6
     [<ffffffff811088e5>] ? __find_get_block_slow+0x38/0x133
     [<ffffffff81161348>] mpage_da_map_and_submit+0x3a7/0x3bd
     [<ffffffff81161a60>] ext4_da_writepages+0x30d/0x426
     [<ffffffff810b3359>] do_writepages+0x1c/0x2a
     [<ffffffff81102f4d>] __writeback_single_inode+0x3e/0xe5
     [<ffffffff81103995>] writeback_sb_inodes+0x1bd/0x2f4
     [<ffffffff81103b3b>] __writeback_inodes_wb+0x6f/0xb4
     [<ffffffff81103c81>] wb_writeback+0x101/0x195
     [<ffffffff81071b53>] ? trace_hardirqs_on_caller+0x114/0x170
     [<ffffffff811043aa>] ? wb_do_writeback+0xaa/0x173
     [<ffffffff8110434a>] wb_do_writeback+0x4a/0x173
     [<ffffffff81071bbc>] ? trace_hardirqs_on+0xd/0xf
     [<ffffffff81038554>] ? del_timer+0x4b/0x5b
     [<ffffffff811044e0>] bdi_writeback_thread+0x6d/0x147
     [<ffffffff81104473>] ? wb_do_writeback+0x173/0x173
     [<ffffffff81048fbc>] kthread+0xd0/0xd8
     [<ffffffff81455eb2>] ? _raw_spin_unlock_irq+0x29/0x3e
     [<ffffffff81048eec>] ? __init_kthread_worker+0x55/0x55
     [<ffffffff81456aac>] ret_from_fork+0x7c/0xb0
     [<ffffffff81048eec>] ? __init_kthread_worker+0x55/0x55
    2 locks held by flush-8:0/24427:
     #0:  (&type->s_umount_key#41){.+.+..}, at: [<ffffffff810e3b73>] grab_super_passive+0x4c/0x76
     #1:  (jbd2_handle){+.+...}, at: [<ffffffff81190d81>] start_this_handle+0x475/0x4ea
    
    
    The problem here is that another thread, which is attempting to write the
    to-be-stored NFS page to the on-ext4 cache file is waiting for the journal
    lock, eg:
    
    INFO: task kworker/u:2:24437 blocked for more than 120 seconds.
    "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    kworker/u:2     D ffff880039589768     0 24437      2 0x00000000
     ffff8800395896d8 0000000000000046 ffff8800283bf040 ffff880039589fd8
     0000000000011c80 ffff880039589fd8 ffff880039f0b040 ffff8800283bf040
     0000000000000006 ffff8800283bf6b8 ffff880039589658 ffffffff81071a13
    Call Trace:
     [<ffffffff81071a13>] ? mark_held_locks+0xbe/0xea
     [<ffffffff81455e73>] ? _raw_spin_unlock_irqrestore+0x3a/0x50
     [<ffffffff81071b53>] ? trace_hardirqs_on_caller+0x114/0x170
     [<ffffffff81071bbc>] ? trace_hardirqs_on+0xd/0xf
     [<ffffffff81454bed>] schedule+0x60/0x62
     [<ffffffff81190c23>] start_this_handle+0x317/0x4ea
     [<ffffffff810498a8>] ? __init_waitqueue_head+0x4d/0x4d
     [<ffffffff81190fcc>] jbd2__journal_start+0xb3/0x12e
     [<ffffffff81176606>] __ext4_journal_start_sb+0xb2/0xc6
     [<ffffffff8115f137>] ext4_da_write_begin+0x109/0x233
     [<ffffffff810a964d>] generic_file_buffered_write+0x11a/0x264
     [<ffffffff811032cf>] ? __mark_inode_dirty+0x2d/0x1ee
     [<ffffffff810ab1ab>] __generic_file_aio_write+0x2a5/0x2d5
     [<ffffffff810ab24a>] generic_file_aio_write+0x6f/0xd0
     [<ffffffff81159a2c>] ext4_file_write+0x38c/0x3c4
     [<ffffffff810e0915>] do_sync_write+0x91/0xd1
     [<ffffffffa00a17f0>] cachefiles_write_page+0x26f/0x310 [cachefiles]
     [<ffffffffa01d470b>] fscache_write_op+0x21e/0x37a [fscache]
     [<ffffffff81455eb2>] ? _raw_spin_unlock_irq+0x29/0x3e
     [<ffffffffa01d2479>] fscache_op_work_func+0x78/0xd7 [fscache]
     [<ffffffff8104455a>] process_one_work+0x232/0x3a8
     [<ffffffff810444ff>] ? process_one_work+0x1d7/0x3a8
     [<ffffffff81044ee0>] worker_thread+0x214/0x303
     [<ffffffff81044ccc>] ? manage_workers+0x245/0x245
     [<ffffffff81048fbc>] kthread+0xd0/0xd8
     [<ffffffff81455eb2>] ? _raw_spin_unlock_irq+0x29/0x3e
     [<ffffffff81048eec>] ? __init_kthread_worker+0x55/0x55
     [<ffffffff81456aac>] ret_from_fork+0x7c/0xb0
     [<ffffffff81048eec>] ? __init_kthread_worker+0x55/0x55
    4 locks held by kworker/u:2/24437:
     #0:  (fscache_operation){.+.+.+}, at: [<ffffffff810444ff>] process_one_work+0x1d7/0x3a8
     #1:  ((&op->work)){+.+.+.}, at: [<ffffffff810444ff>] process_one_work+0x1d7/0x3a8
     #2:  (sb_writers#14){.+.+.+}, at: [<ffffffff810ab22c>] generic_file_aio_write+0x51/0xd0
     #3:  (&sb->s_type->i_mutex_key#19){+.+.+.}, at: [<ffffffff810ab236>] generic_file_aio_write+0x5b/0x
    
    fscache already tries to cancel pending stores, but it can't cancel a write
    for which I/O is already in progress.
    
    An alternative would be to accept writing garbage to the cache under extreme
    circumstances and to kill the afflicted cache object if we have to do this.
    However, we really need to know how strapped the allocator is before deciding
    to do that.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-By: Milosz Tanski <milosz@adfin.com>
    Acked-by: Jeff Layton <jlayton@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 4882c806253f..42f8f2d8a197 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -109,7 +109,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 	 * allocator as the work threads writing to the cache may all end up
 	 * sleeping on memory allocation, so we may need to impose a timeout
 	 * too. */
-	if (!(gfp & __GFP_WAIT)) {
+	if (!(gfp & __GFP_WAIT) || !(gfp & __GFP_FS)) {
 		fscache_stat(&fscache_n_store_vmscan_busy);
 		return false;
 	}

commit ee8be57bc331f00d520c4c8a78ffa9590ed41c2c
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Fri May 10 19:50:24 2013 +0100

    fs/fscache: remove spin_lock() from the condition in while()
    
    The spinlock() within the condition in while() will cause a compile error
    if it is not a function. This is not a problem on mainline but it does not
    look pretty and there is no reason to do it that way.
    That patch writes it a little differently and avoids the double condition.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-By: Milosz Tanski <milosz@adfin.com>
    Acked-by: Jeff Layton <jlayton@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index ff000e52072d..4882c806253f 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -796,11 +796,16 @@ void fscache_invalidate_writes(struct fscache_cookie *cookie)
 
 	_enter("");
 
-	while (spin_lock(&cookie->stores_lock),
-	       n = radix_tree_gang_lookup_tag(&cookie->stores, results, 0,
-					      ARRAY_SIZE(results),
-					      FSCACHE_COOKIE_PENDING_TAG),
-	       n > 0) {
+	for (;;) {
+		spin_lock(&cookie->stores_lock);
+		n = radix_tree_gang_lookup_tag(&cookie->stores, results, 0,
+					       ARRAY_SIZE(results),
+					       FSCACHE_COOKIE_PENDING_TAG);
+		if (n == 0) {
+			spin_unlock(&cookie->stores_lock);
+			break;
+		}
+
 		for (i = n - 1; i >= 0; i--) {
 			page = results[i];
 			radix_tree_delete(&cookie->stores, page->index);
@@ -812,7 +817,6 @@ void fscache_invalidate_writes(struct fscache_cookie *cookie)
 			page_cache_release(results[i]);
 	}
 
-	spin_unlock(&cookie->stores_lock);
 	_leave("");
 }
 

commit 91c7fbbf63f33c77d8d28de624834a21888842bb
Author: David Howells <dhowells@redhat.com>
Date:   Fri Dec 14 11:02:22 2012 +0000

    FS-Cache: Clear remaining page count on retrieval cancellation
    
    Provide fscache_cancel_op() with a pointer to a function it should invoke under
    lock if it cancels an operation.
    
    Use this to clear the remaining page count upon cancellation of a pending
    retrieval operation so that fscache_release_retrieval_op() doesn't get an
    assertion failure (see below).  This can happen when a signal occurs, say from
    CTRL-C being pressed during data retrieval.
    
    FS-Cache: Assertion failed
    3 == 0 is false
    ------------[ cut here ]------------
    kernel BUG at fs/fscache/page.c:237!
    invalid opcode: 0000 [#641] SMP
    Modules linked in: cachefiles(F) nfsv4(F) nfsv3(F) nfsv2(F) nfs(F) fscache(F) auth_rpcgss(F) nfs_acl(F) lockd(F) sunrpc(F)
    CPU 0
    Pid: 6075, comm: slurp-q Tainted: GF     D      3.7.0-rc8-fsdevel+ #411                  /DG965RY
    RIP: 0010:[<ffffffffa007f328>]  [<ffffffffa007f328>] fscache_release_retrieval_op+0x75/0xff [fscache]
    RSP: 0000:ffff88001c6d7988  EFLAGS: 00010296
    RAX: 000000000000000f RBX: ffff880014cdfe00 RCX: ffffffff6c102000
    RDX: ffffffff8102d1ad RSI: ffffffff6c102000 RDI: ffffffff8102d1d6
    RBP: ffff88001c6d7998 R08: 0000000000000002 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000000 R12: 00000000fffffe00
    R13: ffff88001c6d7ab4 R14: ffff88001a8638a0 R15: ffff88001552b190
    FS:  00007f877aaf0700(0000) GS:ffff88003bc00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    CR2: 00007fff11378fd2 CR3: 000000001c6c6000 CR4: 00000000000007f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    Process slurp-q (pid: 6075, threadinfo ffff88001c6d6000, task ffff88001c6c4080)
    Stack:
     ffffffffa007ec07 ffff880014cdfe00 ffff88001c6d79c8 ffffffffa007db4d
     ffffffffa007ec07 ffff880014cdfe00 00000000fffffe00 ffff88001c6d7ab4
     ffff88001c6d7a38 ffffffffa008116d 0000000000000000 ffff88001c6c4080
    Call Trace:
     [<ffffffffa007ec07>] ? fscache_cancel_op+0x194/0x1cf [fscache]
     [<ffffffffa007db4d>] fscache_put_operation+0x135/0x2ed [fscache]
     [<ffffffffa007ec07>] ? fscache_cancel_op+0x194/0x1cf [fscache]
     [<ffffffffa008116d>] __fscache_read_or_alloc_pages+0x413/0x4bc [fscache]
     [<ffffffff810ac8ae>] ? __alloc_pages_nodemask+0x195/0x75c
     [<ffffffffa00aab0f>] __nfs_readpages_from_fscache+0x86/0x13d [nfs]
     [<ffffffffa00a5fe0>] nfs_readpages+0x186/0x1bd [nfs]
     [<ffffffff810d23c8>] ? alloc_pages_current+0xc7/0xe4
     [<ffffffff810a68b5>] ? __page_cache_alloc+0x84/0x91
     [<ffffffff810af912>] ? __do_page_cache_readahead+0xa6/0x2e0
     [<ffffffff810afaa3>] __do_page_cache_readahead+0x237/0x2e0
     [<ffffffff810af912>] ? __do_page_cache_readahead+0xa6/0x2e0
     [<ffffffff810afe3e>] ra_submit+0x1c/0x20
     [<ffffffff810b019b>] ondemand_readahead+0x359/0x382
     [<ffffffff810b0279>] page_cache_sync_readahead+0x38/0x3a
     [<ffffffff810a77b5>] generic_file_aio_read+0x26b/0x637
     [<ffffffffa00f1852>] ? nfs_mark_delegation_referenced+0xb/0xb [nfsv4]
     [<ffffffffa009cc85>] nfs_file_read+0xaa/0xcf [nfs]
     [<ffffffff810db5b3>] do_sync_read+0x91/0xd1
     [<ffffffff810dbb8b>] vfs_read+0x9b/0x144
     [<ffffffff810dbc78>] sys_read+0x44/0x75
     [<ffffffff81422892>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 8a92b9fabe83..ff000e52072d 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -302,6 +302,17 @@ static int fscache_wait_for_deferred_lookup(struct fscache_cookie *cookie)
 	return 0;
 }
 
+/*
+ * Handle cancellation of a pending retrieval op
+ */
+static void fscache_do_cancel_retrieval(struct fscache_operation *_op)
+{
+	struct fscache_retrieval *op =
+		container_of(_op, struct fscache_retrieval, op);
+
+	op->n_pages = 0;
+}
+
 /*
  * wait for an object to become active (or dead)
  */
@@ -320,7 +331,7 @@ static int fscache_wait_for_retrieval_activation(struct fscache_object *object,
 	if (wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
 			fscache_wait_bit_interruptible,
 			TASK_INTERRUPTIBLE) != 0) {
-		ret = fscache_cancel_op(&op->op);
+		ret = fscache_cancel_op(&op->op, fscache_do_cancel_retrieval);
 		if (ret == 0)
 			return -ERESTARTSYS;
 
@@ -338,8 +349,8 @@ static int fscache_wait_for_retrieval_activation(struct fscache_object *object,
 		return -ENOBUFS;
 	}
 	if (unlikely(fscache_object_is_dead(object))) {
-		pr_err("%s() = -ENOBUFS [obj dead %d]", __func__, op->op.state);
-		fscache_cancel_op(&op->op);
+		pr_err("%s() = -ENOBUFS [obj dead %d]\n", __func__, op->op.state);
+		fscache_cancel_op(&op->op, fscache_do_cancel_retrieval);
 		fscache_stat(stat_object_dead);
 		return -ENOBUFS;
 	}

commit 1f372dff1da37e2b36ae9085368fa46896398598
Author: David Howells <dhowells@redhat.com>
Date:   Thu Dec 13 20:03:13 2012 +0000

    FS-Cache: Mark cancellation of in-progress operation
    
    Mark as cancelled an operation that is in progress rather than pending at the
    time it is cancelled, and call fscache_complete_op() to cancel an operation so
    that blocked ops can be started.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index ef0218f5080d..8a92b9fabe83 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -171,7 +171,7 @@ static void fscache_attr_changed_op(struct fscache_operation *op)
 			fscache_abort_object(object);
 	}
 
-	fscache_op_complete(op);
+	fscache_op_complete(op, true);
 	_leave("");
 }
 
@@ -704,7 +704,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 		 * exists, so we should just cancel this write operation.
 		 */
 		spin_unlock(&object->lock);
-		op->op.state = FSCACHE_OP_ST_CANCELLED;
+		fscache_op_complete(&op->op, false);
 		_leave(" [inactive]");
 		return;
 	}
@@ -717,7 +717,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 		 * cancel this write operation.
 		 */
 		spin_unlock(&object->lock);
-		op->op.state = FSCACHE_OP_ST_CANCELLED;
+		fscache_op_complete(&op->op, false);
 		_leave(" [cancel] op{f=%lx s=%u} obj{s=%u f=%lx}",
 		       _op->flags, _op->state, object->state, object->flags);
 		return;
@@ -755,7 +755,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 	fscache_end_page_write(object, page);
 	if (ret < 0) {
 		fscache_abort_object(object);
-		fscache_op_complete(&op->op);
+		fscache_op_complete(&op->op, true);
 	} else {
 		fscache_enqueue_operation(&op->op);
 	}
@@ -770,7 +770,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 	spin_unlock(&cookie->stores_lock);
 	clear_bit(FSCACHE_OBJECT_PENDING_WRITE, &object->flags);
 	spin_unlock(&object->lock);
-	fscache_op_complete(&op->op);
+	fscache_op_complete(&op->op, true);
 	_leave("");
 }
 

commit 7ef001e937e8b9cbedb2fc1c31dd681ac3b31927
Author: David Howells <dhowells@redhat.com>
Date:   Fri Dec 7 10:41:26 2012 +0000

    FS-Cache: One of the write operation paths doesn't set the object state
    
    In fscache_write_op(), if the object is determined to have become inactive or
    to have lost its cookie, we don't move the operation state from in-progress,
    and so an assertion in fscache_put_operation() fails with an assertion (see
    below).
    
    Instrumenting fscache_op_work_func() indicates that it called
    fscache_write_op() before calling fscache_put_operation() - where the assertion
    failed.  The assertion at line 433 indicates that the operation state is
    IN_PROGRESS rather than being COMPLETE or CANCELLED.
    
    Instrumenting fscache_write_op() showed that it was being called on an object
    that had had its cookie removed and that this was due to relinquishment of the
    cookie by the netfs.  At this point fscache no longer has access to the pages
    of netfs data that were requested to be written, and so simply cancelling the
    operation is the thing to do.
    
    FS-Cache: Assertion failed
    3 == 5 is false
    ------------[ cut here ]------------
    kernel BUG at fs/fscache/operation.c:433!
    invalid opcode: 0000 [#1] SMP
    Modules linked in: cachefiles(F) nfsv4(F) nfsv3(F) nfsv2(F) nfs(F) fscache(F) auth_rpcgss(F) nfs_acl(F) lockd(F) sunrpc(F)
    CPU 0
    Pid: 1035, comm: kworker/u:3 Tainted: GF            3.7.0-rc8-fsdevel+ #411                  /DG965RY
    RIP: 0010:[<ffffffffa007db22>]  [<ffffffffa007db22>] fscache_put_operation+0x11a/0x2ed [fscache]
    RSP: 0018:ffff88003e32bcf8  EFLAGS: 00010296
    RAX: 000000000000000f RBX: ffff88001818eb78 RCX: ffffffff6c102000
    RDX: ffffffff8102d1ad RSI: ffffffff6c102000 RDI: ffffffff8102d1d6
    RBP: ffff88003e32bd18 R08: 0000000000000002 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000000 R12: ffffffffa00811da
    R13: 0000000000000001 R14: 0000000100625d26 R15: 0000000000000000
    FS:  0000000000000000(0000) GS:ffff88003bc00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    CR2: 00007fff7dd31c68 CR3: 000000003d730000 CR4: 00000000000007f0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    Process kworker/u:3 (pid: 1035, threadinfo ffff88003e32a000, task ffff88003bb38080)
    Stack:
     ffffffff8102d1ad ffff88001818eb78 ffffffffa00811da 0000000000000001
     ffff88003e32bd48 ffffffffa007f0ad ffff88001818eb78 ffffffff819583c0
     ffff88003df24e00 ffff88003882c3e0 ffff88003e32bde8 ffffffff81042de0
    Call Trace:
     [<ffffffff8102d1ad>] ? vprintk_emit+0x3c6/0x41a
     [<ffffffffa00811da>] ? __fscache_read_or_alloc_pages+0x4bc/0x4bc [fscache]
     [<ffffffffa007f0ad>] fscache_op_work_func+0xec/0x123 [fscache]
     [<ffffffff81042de0>] process_one_work+0x21c/0x3b0
     [<ffffffff81042d82>] ? process_one_work+0x1be/0x3b0
     [<ffffffffa007efc1>] ? fscache_operation_gc+0x23e/0x23e [fscache]
     [<ffffffff8104332e>] worker_thread+0x202/0x2df
     [<ffffffff8104312c>] ? rescuer_thread+0x18e/0x18e
     [<ffffffff81047c1c>] kthread+0xd0/0xd8
     [<ffffffff81421bfa>] ? _raw_spin_unlock_irq+0x29/0x3e
     [<ffffffff81047b4c>] ? __init_kthread_worker+0x55/0x55
     [<ffffffff814227ec>] ret_from_fork+0x7c/0xb0
     [<ffffffff81047b4c>] ? __init_kthread_worker+0x55/0x55
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 5b5d9081c8b2..ef0218f5080d 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -699,9 +699,27 @@ static void fscache_write_op(struct fscache_operation *_op)
 	spin_lock(&object->lock);
 	cookie = object->cookie;
 
-	if (!fscache_object_is_active(object) || !cookie) {
+	if (!fscache_object_is_active(object)) {
+		/* If we get here, then the on-disk cache object likely longer
+		 * exists, so we should just cancel this write operation.
+		 */
 		spin_unlock(&object->lock);
-		_leave("");
+		op->op.state = FSCACHE_OP_ST_CANCELLED;
+		_leave(" [inactive]");
+		return;
+	}
+
+	if (!cookie) {
+		/* If we get here, then the cookie belonging to the object was
+		 * detached, probably by the cookie being withdrawn due to
+		 * memory pressure, which means that the pages we might write
+		 * to the cache from no longer exist - therefore, we can just
+		 * cancel this write operation.
+		 */
+		spin_unlock(&object->lock);
+		op->op.state = FSCACHE_OP_ST_CANCELLED;
+		_leave(" [cancel] op{f=%lx s=%u} obj{s=%u f=%lx}",
+		       _op->flags, _op->state, object->state, object->flags);
 		return;
 	}
 

commit 9c04caa81b876faee5f1cc6eaad76dd7021ab8ff
Author: David Howells <dhowells@redhat.com>
Date:   Fri Dec 7 18:08:02 2012 +0000

    FS-Cache: Fix signal handling during waits
    
    wait_on_bit() with TASK_INTERRUPTIBLE returns 1 rather than a negative error
    code, so change what we check for.  This means that the signal handling in
    fscache_wait_for_retrieval_activation()  should now work properly.
    
    Without this, the following bug can be seen if CTRL-C is pressed during
    fscache read operation:
    
    FS-Cache: Assertion failed
    2 == 3 is false
    ------------[ cut here ]------------
    kernel BUG at fs/fscache/page.c:347!
    invalid opcode: 0000 [#1] SMP
    Modules linked in: cachefiles(F) nfsv4(F) nfsv3(F) nfsv2(F) nfs(F) fscache(F) auth_rpcgss(F) nfs_acl(F) lockd(F) sunrpc(F)
    CPU 1
    Pid: 15006, comm: slurp-q Tainted: GF            3.7.0-rc8-fsdevel+ #411                  /DG965RY
    RIP: 0010:[<ffffffffa007fcb4>]  [<ffffffffa007fcb4>] fscache_wait_for_retrieval_activation+0x167/0x177 [fscache]
    RSP: 0018:ffff88002a4c39a8  EFLAGS: 00010292
    RAX: 000000000000001a RBX: ffff88002d3dc158 RCX: 0000000000008685
    RDX: ffffffff8102ccd6 RSI: 0000000000000001 RDI: ffffffff8102d1d6
    RBP: ffff88002a4c39c8 R08: 0000000000000002 R09: 0000000000000000
    R10: ffffffff8163afa0 R11: ffff88003bd11900 R12: ffffffffa00868c8
    R13: ffff880028306458 R14: ffff88002d3dc1b0 R15: ffff88001372e538
    FS:  00007f17426a0700(0000) GS:ffff88003bd00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    CR2: 00007f1742494a44 CR3: 0000000031bd7000 CR4: 00000000000007e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    Process slurp-q (pid: 15006, threadinfo ffff88002a4c2000, task ffff880023de3040)
    Stack:
     ffff88002d3dc158 ffff88001372e538 ffff88002a4c3ab4 ffff8800283064e0
     ffff88002a4c3a38 ffffffffa0080f6d 0000000000000000 ffff880023de3040
     ffff88002a4c3ac8 ffffffff810ac8ae ffff880028306458 ffff88002a4c3bc8
    Call Trace:
     [<ffffffffa0080f6d>] __fscache_read_or_alloc_pages+0x24f/0x4bc [fscache]
     [<ffffffff810ac8ae>] ? __alloc_pages_nodemask+0x195/0x75c
     [<ffffffffa00aab0f>] __nfs_readpages_from_fscache+0x86/0x13d [nfs]
     [<ffffffffa00a5fe0>] nfs_readpages+0x186/0x1bd [nfs]
     [<ffffffff810d23c8>] ? alloc_pages_current+0xc7/0xe4
     [<ffffffff810a68b5>] ? __page_cache_alloc+0x84/0x91
     [<ffffffff810af912>] ? __do_page_cache_readahead+0xa6/0x2e0
     [<ffffffff810afaa3>] __do_page_cache_readahead+0x237/0x2e0
     [<ffffffff810af912>] ? __do_page_cache_readahead+0xa6/0x2e0
     [<ffffffff810afe3e>] ra_submit+0x1c/0x20
     [<ffffffff810b019b>] ondemand_readahead+0x359/0x382
     [<ffffffff810b0279>] page_cache_sync_readahead+0x38/0x3a
     [<ffffffff810a77b5>] generic_file_aio_read+0x26b/0x637
     [<ffffffffa00f1852>] ? nfs_mark_delegation_referenced+0xb/0xb [nfsv4]
     [<ffffffffa009cc85>] nfs_file_read+0xaa/0xcf [nfs]
     [<ffffffff810db5b3>] do_sync_read+0x91/0xd1
     [<ffffffff810dbb8b>] vfs_read+0x9b/0x144
     [<ffffffff810dbc78>] sys_read+0x44/0x75
     [<ffffffff81422892>] system_call_fastpath+0x16/0x1b
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index f9b2fb3ae492..5b5d9081c8b2 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -319,7 +319,7 @@ static int fscache_wait_for_retrieval_activation(struct fscache_object *object,
 	fscache_stat(stat_op_waits);
 	if (wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
 			fscache_wait_bit_interruptible,
-			TASK_INTERRUPTIBLE) < 0) {
+			TASK_INTERRUPTIBLE) != 0) {
 		ret = fscache_cancel_op(&op->op);
 		if (ret == 0)
 			return -ERESTARTSYS;

commit 8c209ce721444a61b61d9e772746c721e4d8d1e8
Author: David Howells <dhowells@redhat.com>
Date:   Wed Dec 5 13:34:49 2012 +0000

    NFS: nfs_migrate_page() does not wait for FS-Cache to finish with a page
    
    nfs_migrate_page() does not wait for FS-Cache to finish with a page, probably
    leading to the following bad-page-state:
    
     BUG: Bad page state in process python-bin  pfn:17d39b
     page:ffffea00053649e8 flags:004000000000100c count:0 mapcount:0 mapping:(null)
    index:38686 (Tainted: G    B      ---------------- )
     Pid: 31053, comm: python-bin Tainted: G    B      ----------------
    2.6.32-71.24.1.el6.x86_64 #1
     Call Trace:
     [<ffffffff8111bfe7>] bad_page+0x107/0x160
     [<ffffffff8111ee69>] free_hot_cold_page+0x1c9/0x220
     [<ffffffff8111ef19>] __pagevec_free+0x59/0xb0
     [<ffffffff8104b988>] ? flush_tlb_others_ipi+0x128/0x130
     [<ffffffff8112230c>] release_pages+0x21c/0x250
     [<ffffffff8115b92a>] ? remove_migration_pte+0x28a/0x2b0
     [<ffffffff8115f3f8>] ? mem_cgroup_get_reclaim_stat_from_page+0x18/0x70
     [<ffffffff81122687>] ____pagevec_lru_add+0x167/0x180
     [<ffffffff811226f8>] __lru_cache_add+0x58/0x70
     [<ffffffff81122731>] lru_cache_add_lru+0x21/0x40
     [<ffffffff81123f49>] putback_lru_page+0x69/0x100
     [<ffffffff8115c0bd>] migrate_pages+0x13d/0x5d0
     [<ffffffff81122687>] ? ____pagevec_lru_add+0x167/0x180
     [<ffffffff81152ab0>] ? compaction_alloc+0x0/0x370
     [<ffffffff8115255c>] compact_zone+0x4cc/0x600
     [<ffffffff8111cfac>] ? get_page_from_freelist+0x15c/0x820
     [<ffffffff810672f4>] ? check_preempt_wakeup+0x1c4/0x3c0
     [<ffffffff8115290e>] compact_zone_order+0x7e/0xb0
     [<ffffffff81152a49>] try_to_compact_pages+0x109/0x170
     [<ffffffff8111e94d>] __alloc_pages_nodemask+0x5ed/0x850
     [<ffffffff814c9136>] ? thread_return+0x4e/0x778
     [<ffffffff81150d43>] alloc_pages_vma+0x93/0x150
     [<ffffffff81167ea5>] do_huge_pmd_anonymous_page+0x135/0x340
     [<ffffffff814cb6f6>] ? rwsem_down_read_failed+0x26/0x30
     [<ffffffff81136755>] handle_mm_fault+0x245/0x2b0
     [<ffffffff814ce383>] do_page_fault+0x123/0x3a0
     [<ffffffff814cbdf5>] page_fault+0x25/0x30
    
    nfs_migrate_page() calls nfs_fscache_release_page() which doesn't actually wait
    - even if __GFP_WAIT is set.  The reason that doesn't wait is that
    fscache_maybe_release_page() might deadlock the allocator as the work threads
    writing to the cache may all end up sleeping on memory allocation.
    
    However, I wonder if that is actually a problem.  There are a number of things
    I can do to deal with this:
    
     (1) Make nfs_migrate_page() wait.
    
     (2) Make fscache_maybe_release_page() honour the __GFP_WAIT flag.
    
     (3) Set a timeout around the wait.
    
     (4) Make nfs_migrate_page() return an error if the page is still busy.
    
    For the moment, I'll select (2) and (4).
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Jeff Layton <jlayton@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 4dbbca162620..f9b2fb3ae492 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -56,6 +56,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 
 	_enter("%p,%p,%x", cookie, page, gfp);
 
+try_again:
 	rcu_read_lock();
 	val = radix_tree_lookup(&cookie->stores, page->index);
 	if (!val) {
@@ -104,11 +105,19 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 	return true;
 
 page_busy:
-	/* we might want to wait here, but that could deadlock the allocator as
-	 * the work threads writing to the cache may all end up sleeping
-	 * on memory allocation */
-	fscache_stat(&fscache_n_store_vmscan_busy);
-	return false;
+	/* We will wait here if we're allowed to, but that could deadlock the
+	 * allocator as the work threads writing to the cache may all end up
+	 * sleeping on memory allocation, so we may need to impose a timeout
+	 * too. */
+	if (!(gfp & __GFP_WAIT)) {
+		fscache_stat(&fscache_n_store_vmscan_busy);
+		return false;
+	}
+
+	fscache_stat(&fscache_n_store_vmscan_wait);
+	__fscache_wait_on_page_write(cookie, page);
+	gfp &= ~__GFP_WAIT;
+	goto try_again;
 }
 EXPORT_SYMBOL(__fscache_maybe_release_page);
 

commit b4cf1e08c8ac95eff65faa53904f7f13ac78194b
Author: David Howells <dhowells@redhat.com>
Date:   Wed Dec 5 13:34:45 2012 +0000

    CacheFiles: Add missing retrieval completions
    
    CacheFiles is missing some calls to fscache_retrieval_complete() in the error
    handling/collision paths of its reader functions.
    
    This can be seen by the following assertion tripping in fscache_put_operation()
    whereby the operation being destroyed is still in the in-progress state and has
    not been cancelled or completed:
    
    FS-Cache: Assertion failed
    3 == 5 is false
    ------------[ cut here ]------------
    kernel BUG at fs/fscache/operation.c:408!
    invalid opcode: 0000 [#1] SMP
    CPU 2
    Modules linked in: xfs ioatdma dca loop joydev evdev
    psmouse dcdbas pcspkr serio_raw i5000_edac edac_core i5k_amb shpchp
    pci_hotplug sg sr_mod]
    
    Pid: 8062, comm: httpd Not tainted 3.1.0-rc8 #1 Dell Inc. PowerEdge 1950/0DT097
    RIP: 0010:[<ffffffff81197b24>]  [<ffffffff81197b24>] fscache_put_operation+0x304/0x330
    RSP: 0018:ffff880062f739d8  EFLAGS: 00010296
    RAX: 0000000000000025 RBX: ffff8800c5122e84 RCX: ffffffff81ddf040
    RDX: 00000000ffffffff RSI: 0000000000000082 RDI: ffffffff81ddef30
    RBP: ffff880062f739f8 R08: 0000000000000005 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000003 R12: ffff8800c5122e40
    R13: ffff880037a2cd20 R14: ffff880087c7a058 R15: ffff880087c7a000
    FS:  00007f63dcf636e0(0000) GS:ffff88022fc80000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007f0c0a91f000 CR3: 0000000062ec2000 CR4: 00000000000006e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    Process httpd (pid: 8062, threadinfo ffff880062f72000, task ffff880087e58000)
    Stack:
     ffff880062f73bf8 0000000000000000 ffff880062f73bf8 ffff880037a2cd20
     ffff880062f73a68 ffffffff8119aa7e ffff88006540e000 ffff880062f73ad4
     ffff88008e9a4308 ffff880037a2cd20 ffff880062f73a48 ffff8800c5122e40
    Call Trace:
     [<ffffffff8119aa7e>] __fscache_read_or_alloc_pages+0x1fe/0x530
     [<ffffffff81250780>] __nfs_readpages_from_fscache+0x70/0x1c0
     [<ffffffff8123142a>] nfs_readpages+0xca/0x1e0
     [<ffffffff815f3c06>] ? rpc_do_put_task+0x36/0x50
     [<ffffffff8122755b>] ? alloc_nfs_open_context+0x4b/0x110
     [<ffffffff815ecd1a>] ? rpc_call_sync+0x5a/0x70
     [<ffffffff810e7e9a>] __do_page_cache_readahead+0x1ca/0x270
     [<ffffffff810e7f61>] ra_submit+0x21/0x30
     [<ffffffff810e818d>] ondemand_readahead+0x11d/0x250
     [<ffffffff810e83b6>] page_cache_sync_readahead+0x36/0x60
     [<ffffffff810dffa4>] generic_file_aio_read+0x454/0x770
     [<ffffffff81224ce1>] nfs_file_read+0xe1/0x130
     [<ffffffff81121bd9>] do_sync_read+0xd9/0x120
     [<ffffffff8114088f>] ? mntput+0x1f/0x40
     [<ffffffff811238cb>] ? fput+0x1cb/0x260
     [<ffffffff81122938>] vfs_read+0xc8/0x180
     [<ffffffff81122af5>] sys_read+0x55/0x90
    
    Reported-by: Mark Moseley <moseleymark@gmail.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 7bf9d2557052..4dbbca162620 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -329,6 +329,8 @@ static int fscache_wait_for_retrieval_activation(struct fscache_object *object,
 		return -ENOBUFS;
 	}
 	if (unlikely(fscache_object_is_dead(object))) {
+		pr_err("%s() = -ENOBUFS [obj dead %d]", __func__, op->op.state);
+		fscache_cancel_op(&op->op);
 		fscache_stat(stat_object_dead);
 		return -ENOBUFS;
 	}

commit ef778e7ae67cd426c30cad43378b908f5eb0bad5
Author: David Howells <dhowells@redhat.com>
Date:   Thu Dec 20 21:52:36 2012 +0000

    FS-Cache: Provide proper invalidation
    
    Provide a proper invalidation method rather than relying on the netfs retiring
    the cookie it has and getting a new one.  The problem with this is that isn't
    easy for the netfs to make sure that it has completed/cancelled all its
    outstanding storage and retrieval operations on the cookie it is retiring.
    
    Instead, have the cache provide an invalidation method that will cancel or wait
    for all currently outstanding operations before invalidating the cache, and
    will cause new operations to queue up behind that.  Whilst invalidation is in
    progress, some requests will be rejected until the cache can stack a barrier on
    the operation queue to cause new operations to be deferred behind it.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index b38b13d2a555..7bf9d2557052 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -361,6 +361,11 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 	if (hlist_empty(&cookie->backing_objects))
 		goto nobufs;
 
+	if (test_bit(FSCACHE_COOKIE_INVALIDATING, &cookie->flags)) {
+		_leave(" = -ENOBUFS [invalidating]");
+		return -ENOBUFS;
+	}
+
 	ASSERTCMP(cookie->def->type, !=, FSCACHE_COOKIE_TYPE_INDEX);
 	ASSERTCMP(page, !=, NULL);
 
@@ -483,6 +488,11 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	if (hlist_empty(&cookie->backing_objects))
 		goto nobufs;
 
+	if (test_bit(FSCACHE_COOKIE_INVALIDATING, &cookie->flags)) {
+		_leave(" = -ENOBUFS [invalidating]");
+		return -ENOBUFS;
+	}
+
 	ASSERTCMP(cookie->def->type, !=, FSCACHE_COOKIE_TYPE_INDEX);
 	ASSERTCMP(*nr_pages, >, 0);
 	ASSERT(!list_empty(pages));
@@ -591,6 +601,11 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	ASSERTCMP(cookie->def->type, !=, FSCACHE_COOKIE_TYPE_INDEX);
 	ASSERTCMP(page, !=, NULL);
 
+	if (test_bit(FSCACHE_COOKIE_INVALIDATING, &cookie->flags)) {
+		_leave(" = -ENOBUFS [invalidating]");
+		return -ENOBUFS;
+	}
+
 	if (fscache_wait_for_deferred_lookup(cookie) < 0)
 		return -ERESTARTSYS;
 
@@ -730,6 +745,37 @@ static void fscache_write_op(struct fscache_operation *_op)
 	_leave("");
 }
 
+/*
+ * Clear the pages pending writing for invalidation
+ */
+void fscache_invalidate_writes(struct fscache_cookie *cookie)
+{
+	struct page *page;
+	void *results[16];
+	int n, i;
+
+	_enter("");
+
+	while (spin_lock(&cookie->stores_lock),
+	       n = radix_tree_gang_lookup_tag(&cookie->stores, results, 0,
+					      ARRAY_SIZE(results),
+					      FSCACHE_COOKIE_PENDING_TAG),
+	       n > 0) {
+		for (i = n - 1; i >= 0; i--) {
+			page = results[i];
+			radix_tree_delete(&cookie->stores, page->index);
+		}
+
+		spin_unlock(&cookie->stores_lock);
+
+		for (i = n - 1; i >= 0; i--)
+			page_cache_release(results[i]);
+	}
+
+	spin_unlock(&cookie->stores_lock);
+	_leave("");
+}
+
 /*
  * request a page be stored in the cache
  * - returns:
@@ -776,6 +822,11 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 
 	fscache_stat(&fscache_n_stores);
 
+	if (test_bit(FSCACHE_COOKIE_INVALIDATING, &cookie->flags)) {
+		_leave(" = -ENOBUFS [invalidating]");
+		return -ENOBUFS;
+	}
+
 	op = kzalloc(sizeof(*op), GFP_NOIO | __GFP_NOMEMALLOC | __GFP_NORETRY);
 	if (!op)
 		goto nomem;

commit 9f10523f891928330b7529da54c1a3cc65180b1a
Author: David Howells <dhowells@redhat.com>
Date:   Thu Dec 20 21:52:35 2012 +0000

    FS-Cache: Fix operation state management and accounting
    
    Fix the state management of internal fscache operations and the accounting of
    what operations are in what states.
    
    This is done by:
    
     (1) Give struct fscache_operation a enum variable that directly represents the
         state it's currently in, rather than spreading this knowledge over a bunch
         of flags, who's processing the operation at the moment and whether it is
         queued or not.
    
         This makes it easier to write assertions to check the state at various
         points and to prevent invalid state transitions.
    
     (2) Add an 'operation complete' state and supply a function to indicate the
         completion of an operation (fscache_op_complete()) and make things call
         it.  The final call to fscache_put_operation() can then check that an op
         in the appropriate state (complete or cancelled).
    
     (3) Adjust the use of object->n_ops, ->n_in_progress, ->n_exclusive to better
         govern the state of an object:
    
            (a) The ->n_ops is now the number of extant operations on the object
                and is now decremented by fscache_put_operation() only.
    
            (b) The ->n_in_progress is simply the number of objects that have been
                taken off of the object's pending queue for the purposes of being
                run.  This is decremented by fscache_op_complete() only.
    
            (c) The ->n_exclusive is the number of exclusive ops that have been
                submitted and queued or are in progress.  It is decremented by
                fscache_op_complete() and by fscache_cancel_op().
    
         fscache_put_operation() and fscache_operation_gc() now no longer try to
         clean up ->n_exclusive and ->n_in_progress.  That was leading to double
         decrements against fscache_cancel_op().
    
         fscache_cancel_op() now no longer decrements ->n_ops.  That was leading to
         double decrements against fscache_put_operation().
    
         fscache_submit_exclusive_op() now decides whether it has to queue an op
         based on ->n_in_progress being > 0 rather than ->n_ops > 0 as the latter
         will persist in being true even after all preceding operations have been
         cancelled or completed.  Furthermore, if an object is active and there are
         runnable ops against it, there must be at least one op running.
    
     (4) Add a remaining-pages counter (n_pages) to struct fscache_retrieval and
         provide a function to record completion of the pages as they complete.
    
         When n_pages reaches 0, the operation is deemed to be complete and
         fscache_op_complete() is called.
    
         Add calls to fscache_retrieval_complete() anywhere we've finished with a
         page we've been given to read or allocate for.  This includes places where
         we just return pages to the netfs for reading from the server and where
         accessing the cache fails and we discard the proposed netfs page.
    
    The bugs in the unfixed state management manifest themselves as oopses like the
    following where the operation completion gets out of sync with return of the
    cookie by the netfs.  This is possible because the cache unlocks and returns
    all the netfs pages before recording its completion - which means that there's
    nothing to stop the netfs discarding them and returning the cookie.
    
    
    FS-Cache: Cookie 'NFS.fh' still has outstanding reads
    ------------[ cut here ]------------
    kernel BUG at fs/fscache/cookie.c:519!
    invalid opcode: 0000 [#1] SMP
    CPU 1
    Modules linked in: cachefiles nfs fscache auth_rpcgss nfs_acl lockd sunrpc
    
    Pid: 400, comm: kswapd0 Not tainted 3.1.0-rc7-fsdevel+ #1090                  /DG965RY
    RIP: 0010:[<ffffffffa007050a>]  [<ffffffffa007050a>] __fscache_relinquish_cookie+0x170/0x343 [fscache]
    RSP: 0018:ffff8800368cfb00  EFLAGS: 00010282
    RAX: 000000000000003c RBX: ffff880023cc8790 RCX: 0000000000000000
    RDX: 0000000000002f2e RSI: 0000000000000001 RDI: ffffffff813ab86c
    RBP: ffff8800368cfb50 R08: 0000000000000002 R09: 0000000000000000
    R10: ffff88003a1b7890 R11: ffff88001df6e488 R12: ffff880023d8ed98
    R13: ffff880023cc8798 R14: 0000000000000004 R15: ffff88003b8bf370
    FS:  0000000000000000(0000) GS:ffff88003bd00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    CR2: 00000000008ba008 CR3: 0000000023d93000 CR4: 00000000000006e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
    Process kswapd0 (pid: 400, threadinfo ffff8800368ce000, task ffff88003b8bf040)
    Stack:
     ffff88003b8bf040 ffff88001df6e528 ffff88001df6e528 ffffffffa00b46b0
     ffff88003b8bf040 ffff88001df6e488 ffff88001df6e620 ffffffffa00b46b0
     ffff88001ebd04c8 0000000000000004 ffff8800368cfb70 ffffffffa00b2c91
    Call Trace:
     [<ffffffffa00b2c91>] nfs_fscache_release_inode_cookie+0x3b/0x47 [nfs]
     [<ffffffffa008f25f>] nfs_clear_inode+0x3c/0x41 [nfs]
     [<ffffffffa0090df1>] nfs4_evict_inode+0x2f/0x33 [nfs]
     [<ffffffff810d8d47>] evict+0xa1/0x15c
     [<ffffffff810d8e2e>] dispose_list+0x2c/0x38
     [<ffffffff810d9ebd>] prune_icache_sb+0x28c/0x29b
     [<ffffffff810c56b7>] prune_super+0xd5/0x140
     [<ffffffff8109b615>] shrink_slab+0x102/0x1ab
     [<ffffffff8109d690>] balance_pgdat+0x2f2/0x595
     [<ffffffff8103e009>] ? process_timeout+0xb/0xb
     [<ffffffff8109dba3>] kswapd+0x270/0x289
     [<ffffffff8104c5ea>] ? __init_waitqueue_head+0x46/0x46
     [<ffffffff8109d933>] ? balance_pgdat+0x595/0x595
     [<ffffffff8104bf7a>] kthread+0x7f/0x87
     [<ffffffff813ad6b4>] kernel_thread_helper+0x4/0x10
     [<ffffffff81026b98>] ? finish_task_switch+0x45/0xc0
     [<ffffffff813abcdd>] ? retint_restore_args+0xe/0xe
     [<ffffffff8104befb>] ? __init_kthread_worker+0x53/0x53
     [<ffffffff813ad6b0>] ? gs_change+0xb/0xb
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 248a12e22532..b38b13d2a555 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -162,6 +162,7 @@ static void fscache_attr_changed_op(struct fscache_operation *op)
 			fscache_abort_object(object);
 	}
 
+	fscache_op_complete(op);
 	_leave("");
 }
 
@@ -223,6 +224,8 @@ static void fscache_release_retrieval_op(struct fscache_operation *_op)
 
 	_enter("{OP%x}", op->op.debug_id);
 
+	ASSERTCMP(op->n_pages, ==, 0);
+
 	fscache_hist(fscache_retrieval_histogram, op->start_time);
 	if (op->context)
 		fscache_put_context(op->op.object->cookie, op->context);
@@ -320,6 +323,11 @@ static int fscache_wait_for_retrieval_activation(struct fscache_object *object,
 	_debug("<<< GO");
 
 check_if_dead:
+	if (op->op.state == FSCACHE_OP_ST_CANCELLED) {
+		fscache_stat(stat_object_dead);
+		_leave(" = -ENOBUFS [cancelled]");
+		return -ENOBUFS;
+	}
 	if (unlikely(fscache_object_is_dead(object))) {
 		fscache_stat(stat_object_dead);
 		return -ENOBUFS;
@@ -364,6 +372,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 		_leave(" = -ENOMEM");
 		return -ENOMEM;
 	}
+	op->n_pages = 1;
 
 	spin_lock(&cookie->lock);
 
@@ -375,10 +384,10 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 	ASSERTCMP(object->state, >, FSCACHE_OBJECT_LOOKING_UP);
 
 	atomic_inc(&object->n_reads);
-	set_bit(FSCACHE_OP_DEC_READ_CNT, &op->op.flags);
+	__set_bit(FSCACHE_OP_DEC_READ_CNT, &op->op.flags);
 
 	if (fscache_submit_op(object, &op->op) < 0)
-		goto nobufs_unlock;
+		goto nobufs_unlock_dec;
 	spin_unlock(&cookie->lock);
 
 	fscache_stat(&fscache_n_retrieval_ops);
@@ -425,6 +434,8 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 	_leave(" = %d", ret);
 	return ret;
 
+nobufs_unlock_dec:
+	atomic_dec(&object->n_reads);
 nobufs_unlock:
 	spin_unlock(&cookie->lock);
 	kfree(op);
@@ -482,6 +493,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	op = fscache_alloc_retrieval(mapping, end_io_func, context);
 	if (!op)
 		return -ENOMEM;
+	op->n_pages = *nr_pages;
 
 	spin_lock(&cookie->lock);
 
@@ -491,10 +503,10 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 			     struct fscache_object, cookie_link);
 
 	atomic_inc(&object->n_reads);
-	set_bit(FSCACHE_OP_DEC_READ_CNT, &op->op.flags);
+	__set_bit(FSCACHE_OP_DEC_READ_CNT, &op->op.flags);
 
 	if (fscache_submit_op(object, &op->op) < 0)
-		goto nobufs_unlock;
+		goto nobufs_unlock_dec;
 	spin_unlock(&cookie->lock);
 
 	fscache_stat(&fscache_n_retrieval_ops);
@@ -541,6 +553,8 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	_leave(" = %d", ret);
 	return ret;
 
+nobufs_unlock_dec:
+	atomic_dec(&object->n_reads);
 nobufs_unlock:
 	spin_unlock(&cookie->lock);
 	kfree(op);
@@ -583,6 +597,7 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	op = fscache_alloc_retrieval(page->mapping, NULL, NULL);
 	if (!op)
 		return -ENOMEM;
+	op->n_pages = 1;
 
 	spin_lock(&cookie->lock);
 
@@ -696,6 +711,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 	fscache_end_page_write(object, page);
 	if (ret < 0) {
 		fscache_abort_object(object);
+		fscache_op_complete(&op->op);
 	} else {
 		fscache_enqueue_operation(&op->op);
 	}
@@ -710,6 +726,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 	spin_unlock(&cookie->stores_lock);
 	clear_bit(FSCACHE_OBJECT_PENDING_WRITE, &object->flags);
 	spin_unlock(&object->lock);
+	fscache_op_complete(&op->op);
 	_leave("");
 }
 

commit 5f4f9f4af185d5e76c966d2d3420a61870c856e7
Author: David Howells <dhowells@redhat.com>
Date:   Thu Dec 20 21:52:33 2012 +0000

    CacheFiles: Downgrade the requirements passed to the allocator
    
    Downgrade the requirements passed to the allocator in the gfp flags parameter.
    FS-Cache/CacheFiles can handle OOM conditions simply by aborting the attempt to
    store an object or a page in the cache.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index d7c663cfc923..248a12e22532 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -759,7 +759,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 
 	fscache_stat(&fscache_n_stores);
 
-	op = kzalloc(sizeof(*op), GFP_NOIO);
+	op = kzalloc(sizeof(*op), GFP_NOIO | __GFP_NOMEMALLOC | __GFP_NORETRY);
 	if (!op)
 		goto nomem;
 

commit c4d6d8dbf335c7fa47341654a37c53a512b519bb
Author: David Howells <dhowells@redhat.com>
Date:   Thu Dec 20 21:52:32 2012 +0000

    CacheFiles: Fix the marking of cached pages
    
    Under some circumstances CacheFiles defers the marking of pages with PG_fscache
    so that it can take advantage of pagevecs to reduce the number of calls to
    fscache_mark_pages_cached() and the netfs's hook to keep track of this.
    
    There are, however, two problems with this:
    
     (1) It can lead to the PG_fscache mark being applied _after_ the page is set
         PG_uptodate and unlocked (by the call to fscache_end_io()).
    
     (2) CacheFiles's ref on the page is dropped immediately following
         fscache_end_io() - and so may not still be held when the mark is applied.
         This can lead to the page being passed back to the allocator before the
         mark is applied.
    
    Fix this by, where appropriate, marking the page before calling
    fscache_end_io() and releasing the page.  This means that we can't take
    advantage of pagevecs and have to make a separate call for each page to the
    marking routines.
    
    The symptoms of this are Bad Page state errors cropping up under memory
    pressure, for example:
    
    BUG: Bad page state in process tar  pfn:002da
    page:ffffea0000009fb0 count:0 mapcount:0 mapping:          (null) index:0x1447
    page flags: 0x1000(private_2)
    Pid: 4574, comm: tar Tainted: G        W   3.1.0-rc4-fsdevel+ #1064
    Call Trace:
     [<ffffffff8109583c>] ? dump_page+0xb9/0xbe
     [<ffffffff81095916>] bad_page+0xd5/0xea
     [<ffffffff81095d82>] get_page_from_freelist+0x35b/0x46a
     [<ffffffff810961f3>] __alloc_pages_nodemask+0x362/0x662
     [<ffffffff810989da>] __do_page_cache_readahead+0x13a/0x267
     [<ffffffff81098942>] ? __do_page_cache_readahead+0xa2/0x267
     [<ffffffff81098d7b>] ra_submit+0x1c/0x20
     [<ffffffff8109900a>] ondemand_readahead+0x28b/0x29a
     [<ffffffff81098ee2>] ? ondemand_readahead+0x163/0x29a
     [<ffffffff810990ce>] page_cache_sync_readahead+0x38/0x3a
     [<ffffffff81091d8a>] generic_file_aio_read+0x2ab/0x67e
     [<ffffffffa008cfbe>] nfs_file_read+0xa4/0xc9 [nfs]
     [<ffffffff810c22c4>] do_sync_read+0xba/0xfa
     [<ffffffff81177a47>] ? security_file_permission+0x7b/0x84
     [<ffffffff810c25dd>] ? rw_verify_area+0xab/0xc8
     [<ffffffff810c29a4>] vfs_read+0xaa/0x13a
     [<ffffffff810c2a79>] sys_read+0x45/0x6c
     [<ffffffff813ac37b>] system_call_fastpath+0x16/0x1b
    
    As can be seen, PG_private_2 (== PG_fscache) is set in the page flags.
    
    Instrumenting fscache_mark_pages_cached() to verify whether page->mapping was
    set appropriately showed that sometimes it wasn't.  This led to the discovery
    that sometimes the page has apparently been reclaimed by the time the marker
    got to see it.
    
    Reported-by: M. Stevens <m@tippett.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 3f7a59bfa7ad..d7c663cfc923 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -914,6 +914,40 @@ void __fscache_uncache_page(struct fscache_cookie *cookie, struct page *page)
 }
 EXPORT_SYMBOL(__fscache_uncache_page);
 
+/**
+ * fscache_mark_page_cached - Mark a page as being cached
+ * @op: The retrieval op pages are being marked for
+ * @page: The page to be marked
+ *
+ * Mark a netfs page as being cached.  After this is called, the netfs
+ * must call fscache_uncache_page() to remove the mark.
+ */
+void fscache_mark_page_cached(struct fscache_retrieval *op, struct page *page)
+{
+	struct fscache_cookie *cookie = op->op.object->cookie;
+
+#ifdef CONFIG_FSCACHE_STATS
+	atomic_inc(&fscache_n_marks);
+#endif
+
+	_debug("- mark %p{%lx}", page, page->index);
+	if (TestSetPageFsCache(page)) {
+		static bool once_only;
+		if (!once_only) {
+			once_only = true;
+			printk(KERN_WARNING "FS-Cache:"
+			       " Cookie type %s marked page %lx"
+			       " multiple times\n",
+			       cookie->def->name, page->index);
+		}
+	}
+
+	if (cookie->def->mark_page_cached)
+		cookie->def->mark_page_cached(cookie->netfs_data,
+					      op->mapping, page);
+}
+EXPORT_SYMBOL(fscache_mark_page_cached);
+
 /**
  * fscache_mark_pages_cached - Mark pages as being cached
  * @op: The retrieval op pages are being marked for
@@ -925,32 +959,11 @@ EXPORT_SYMBOL(__fscache_uncache_page);
 void fscache_mark_pages_cached(struct fscache_retrieval *op,
 			       struct pagevec *pagevec)
 {
-	struct fscache_cookie *cookie = op->op.object->cookie;
 	unsigned long loop;
 
-#ifdef CONFIG_FSCACHE_STATS
-	atomic_add(pagevec->nr, &fscache_n_marks);
-#endif
-
-	for (loop = 0; loop < pagevec->nr; loop++) {
-		struct page *page = pagevec->pages[loop];
-
-		_debug("- mark %p{%lx}", page, page->index);
-		if (TestSetPageFsCache(page)) {
-			static bool once_only;
-			if (!once_only) {
-				once_only = true;
-				printk(KERN_WARNING "FS-Cache:"
-				       " Cookie type %s marked page %lx"
-				       " multiple times\n",
-				       cookie->def->name, page->index);
-			}
-		}
-	}
+	for (loop = 0; loop < pagevec->nr; loop++)
+		fscache_mark_page_cached(op, pagevec->pages[loop]);
 
-	if (cookie->def->mark_pages_cached)
-		cookie->def->mark_pages_cached(cookie->netfs_data,
-					       op->mapping, pagevec);
 	pagevec_reinit(pagevec);
 }
 EXPORT_SYMBOL(fscache_mark_pages_cached);

commit b307d4655a71749ac3f91c6dbe33d28cc026ceeb
Author: Jan Beulich <JBeulich@novell.com>
Date:   Thu Jul 21 15:02:43 2011 +0100

    FS-Cache: Fix __fscache_uncache_all_inode_pages()'s outer loop
    
    The compiler, at least for ix86 and m68k, validly warns that the
    comparison:
    
            next <= (loff_t)-1
    
    is always true (and it's always true also for x86-64 and probably all
    other arches - as long as pgoff_t isn't wider than loff_t).  The
    intention appears to be to avoid wrapping of "next", so rather than
    eliminating the pointless comparison, fix the loop to indeed get exited
    when "next" would otherwise wrap.
    
    On m68k the following warning is observed:
    
      fs/fscache/page.c: In function '__fscache_uncache_all_inode_pages':
      fs/fscache/page.c:979: warning: comparison is always false due to limited range of data type
    
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Reported-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: Jan Beulich <jbeulich@novell.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Cc: Suresh Jayaraman <sjayaraman@suse.de>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 2f343b4d7a7d..3f7a59bfa7ad 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -976,16 +976,12 @@ void __fscache_uncache_all_inode_pages(struct fscache_cookie *cookie,
 
 	pagevec_init(&pvec, 0);
 	next = 0;
-	while (next <= (loff_t)-1 &&
-	       pagevec_lookup(&pvec, mapping, next, PAGEVEC_SIZE)
-	       ) {
+	do {
+		if (!pagevec_lookup(&pvec, mapping, next, PAGEVEC_SIZE))
+			break;
 		for (i = 0; i < pagevec_count(&pvec); i++) {
 			struct page *page = pvec.pages[i];
-			pgoff_t page_index = page->index;
-
-			ASSERTCMP(page_index, >=, next);
-			next = page_index + 1;
-
+			next = page->index;
 			if (PageFsCache(page)) {
 				__fscache_wait_on_page_write(cookie, page);
 				__fscache_uncache_page(cookie, page);
@@ -993,7 +989,7 @@ void __fscache_uncache_all_inode_pages(struct fscache_cookie *cookie,
 		}
 		pagevec_release(&pvec);
 		cond_resched();
-	}
+	} while (++next);
 
 	_leave("");
 }

commit c902ce1bfb40d8b049bd2319b388b4b68b04bc27
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jul 7 12:19:48 2011 +0100

    FS-Cache: Add a helper to bulk uncache pages on an inode
    
    Add an FS-Cache helper to bulk uncache pages on an inode.  This will
    only work for the circumstance where the pages in the cache correspond
    1:1 with the pages attached to an inode's page cache.
    
    This is required for CIFS and NFS: When disabling inode cookie, we were
    returning the cookie and setting cifsi->fscache to NULL but failed to
    invalidate any previously mapped pages.  This resulted in "Bad page
    state" errors and manifested in other kind of errors when running
    fsstress.  Fix it by uncaching mapped pages when we disable the inode
    cookie.
    
    This patch should fix the following oops and "Bad page state" errors
    seen during fsstress testing.
    
      ------------[ cut here ]------------
      kernel BUG at fs/cachefiles/namei.c:201!
      invalid opcode: 0000 [#1] SMP
      Pid: 5, comm: kworker/u:0 Not tainted 2.6.38.7-30.fc15.x86_64 #1 Bochs Bochs
      RIP: 0010: cachefiles_walk_to_object+0x436/0x745 [cachefiles]
      RSP: 0018:ffff88002ce6dd00  EFLAGS: 00010282
      RAX: ffff88002ef165f0 RBX: ffff88001811f500 RCX: 0000000000000000
      RDX: 0000000000000000 RSI: 0000000000000100 RDI: 0000000000000282
      RBP: ffff88002ce6dda0 R08: 0000000000000100 R09: ffffffff81b3a300
      R10: 0000ffff00066c0a R11: 0000000000000003 R12: ffff88002ae54840
      R13: ffff88002ae54840 R14: ffff880029c29c00 R15: ffff88001811f4b0
      FS:  00007f394dd32720(0000) GS:ffff88002ef00000(0000) knlGS:0000000000000000
      CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
      CR2: 00007fffcb62ddf8 CR3: 000000001825f000 CR4: 00000000000006e0
      DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
      DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
      Process kworker/u:0 (pid: 5, threadinfo ffff88002ce6c000, task ffff88002ce55cc0)
      Stack:
       0000000000000246 ffff88002ce55cc0 ffff88002ce6dd58 ffff88001815dc00
       ffff8800185246c0 ffff88001811f618 ffff880029c29d18 ffff88001811f380
       ffff88002ce6dd50 ffffffff814757e4 ffff88002ce6dda0 ffffffff8106ac56
      Call Trace:
       cachefiles_lookup_object+0x78/0xd4 [cachefiles]
       fscache_lookup_object+0x131/0x16d [fscache]
       fscache_object_work_func+0x1bc/0x669 [fscache]
       process_one_work+0x186/0x298
       worker_thread+0xda/0x15d
       kthread+0x84/0x8c
       kernel_thread_helper+0x4/0x10
      RIP  cachefiles_walk_to_object+0x436/0x745 [cachefiles]
      ---[ end trace 1d481c9af1804caa ]---
    
    I tested the uncaching by the following means:
    
     (1) Create a big file on my NFS server (104857600 bytes).
    
     (2) Read the file into the cache with md5sum on the NFS client.  Look in
         /proc/fs/fscache/stats:
    
            Pages  : mrk=25601 unc=0
    
     (3) Open the file for read/write ("bash 5<>/warthog/bigfile").  Look in proc
         again:
    
            Pages  : mrk=25601 unc=25601
    
    Reported-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-and-Tested-by: Suresh Jayaraman <sjayaraman@suse.de>
    cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index a2a5d19ece6a..2f343b4d7a7d 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -954,3 +954,47 @@ void fscache_mark_pages_cached(struct fscache_retrieval *op,
 	pagevec_reinit(pagevec);
 }
 EXPORT_SYMBOL(fscache_mark_pages_cached);
+
+/*
+ * Uncache all the pages in an inode that are marked PG_fscache, assuming them
+ * to be associated with the given cookie.
+ */
+void __fscache_uncache_all_inode_pages(struct fscache_cookie *cookie,
+				       struct inode *inode)
+{
+	struct address_space *mapping = inode->i_mapping;
+	struct pagevec pvec;
+	pgoff_t next;
+	int i;
+
+	_enter("%p,%p", cookie, inode);
+
+	if (!mapping || mapping->nrpages == 0) {
+		_leave(" [no pages]");
+		return;
+	}
+
+	pagevec_init(&pvec, 0);
+	next = 0;
+	while (next <= (loff_t)-1 &&
+	       pagevec_lookup(&pvec, mapping, next, PAGEVEC_SIZE)
+	       ) {
+		for (i = 0; i < pagevec_count(&pvec); i++) {
+			struct page *page = pvec.pages[i];
+			pgoff_t page_index = page->index;
+
+			ASSERTCMP(page_index, >=, next);
+			next = page_index + 1;
+
+			if (PageFsCache(page)) {
+				__fscache_wait_on_page_write(cookie, page);
+				__fscache_uncache_page(cookie, page);
+			}
+		}
+		pagevec_release(&pvec);
+		cond_resched();
+	}
+
+	_leave("");
+}
+EXPORT_SYMBOL(__fscache_uncache_all_inode_pages);

commit e50c1f609c63223adaa38f5a79b18759a00adf72
Author: Amerigo Wang <amwang@redhat.com>
Date:   Tue May 24 17:13:11 2011 -0700

    fscache: remove dead code under CONFIG_WORKQUEUE_DEBUGFS
    
    There is no CONFIG_WORKQUEUE_DEBUGFS any more, so this code is dead.
    
    Signed-off-by: WANG Cong <amwang@redhat.com>
    Cc: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 41c441c2058d..a2a5d19ece6a 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -155,11 +155,9 @@ static void fscache_attr_changed_op(struct fscache_operation *op)
 	fscache_stat(&fscache_n_attr_changed_calls);
 
 	if (fscache_object_is_active(object)) {
-		fscache_set_op_state(op, "CallFS");
 		fscache_stat(&fscache_n_cop_attr_changed);
 		ret = object->cache->ops->attr_changed(object);
 		fscache_stat_d(&fscache_n_cop_attr_changed);
-		fscache_set_op_state(op, "Done");
 		if (ret < 0)
 			fscache_abort_object(object);
 	}
@@ -190,7 +188,6 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 
 	fscache_operation_init(op, fscache_attr_changed_op, NULL);
 	op->flags = FSCACHE_OP_ASYNC | (1 << FSCACHE_OP_EXCLUSIVE);
-	fscache_set_op_name(op, "Attr");
 
 	spin_lock(&cookie->lock);
 
@@ -257,7 +254,6 @@ static struct fscache_retrieval *fscache_alloc_retrieval(
 	op->context	= context;
 	op->start_time	= jiffies;
 	INIT_LIST_HEAD(&op->to_do);
-	fscache_set_op_name(&op->op, "Retr");
 	return op;
 }
 
@@ -368,7 +364,6 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 		_leave(" = -ENOMEM");
 		return -ENOMEM;
 	}
-	fscache_set_op_name(&op->op, "RetrRA1");
 
 	spin_lock(&cookie->lock);
 
@@ -487,7 +482,6 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	op = fscache_alloc_retrieval(mapping, end_io_func, context);
 	if (!op)
 		return -ENOMEM;
-	fscache_set_op_name(&op->op, "RetrRAN");
 
 	spin_lock(&cookie->lock);
 
@@ -589,7 +583,6 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	op = fscache_alloc_retrieval(page->mapping, NULL, NULL);
 	if (!op)
 		return -ENOMEM;
-	fscache_set_op_name(&op->op, "RetrAL1");
 
 	spin_lock(&cookie->lock);
 
@@ -662,8 +655,6 @@ static void fscache_write_op(struct fscache_operation *_op)
 
 	_enter("{OP%x,%d}", op->op.debug_id, atomic_read(&op->op.usage));
 
-	fscache_set_op_state(&op->op, "GetPage");
-
 	spin_lock(&object->lock);
 	cookie = object->cookie;
 
@@ -698,15 +689,12 @@ static void fscache_write_op(struct fscache_operation *_op)
 	spin_unlock(&cookie->stores_lock);
 	spin_unlock(&object->lock);
 
-	fscache_set_op_state(&op->op, "Store");
 	fscache_stat(&fscache_n_store_pages);
 	fscache_stat(&fscache_n_cop_write_page);
 	ret = object->cache->ops->write_page(op, page);
 	fscache_stat_d(&fscache_n_cop_write_page);
-	fscache_set_op_state(&op->op, "EndWrite");
 	fscache_end_page_write(object, page);
 	if (ret < 0) {
-		fscache_set_op_state(&op->op, "Abort");
 		fscache_abort_object(object);
 	} else {
 		fscache_enqueue_operation(&op->op);
@@ -778,7 +766,6 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	fscache_operation_init(&op->op, fscache_write_op,
 			       fscache_release_write_op);
 	op->op.flags = FSCACHE_OP_ASYNC | (1 << FSCACHE_OP_WAITING);
-	fscache_set_op_name(&op->op, "Write1");
 
 	ret = radix_tree_preload(gfp & ~__GFP_HIGHMEM);
 	if (ret < 0)

commit 8af7c12436803291c90295259db23d371a7ad9cc
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Jul 20 22:09:01 2010 +0200

    fscache: convert operation to use workqueue instead of slow-work
    
    Make fscache operation to use only workqueue instead of combination of
    workqueue and slow-work.  FSCACHE_OP_SLOW is dropped and
    FSCACHE_OP_FAST is renamed to FSCACHE_OP_ASYNC and uses newly added
    fscache_op_wq workqueue to execute op->processor().
    fscache_operation_init_slow() is dropped and fscache_operation_init()
    now takes @processor argument directly.
    
    * Unbound workqueue is used.
    
    * fscache_retrieval_work() is no longer necessary as OP_ASYNC now does
      the equivalent thing.
    
    * sysctl fscache.operation_max_active added to control concurrency.
      The default value is nr_cpus clamped between 2 and
      WQ_UNBOUND_MAX_ACTIVE.
    
    * debugfs support is dropped for now.  Tracing API based debug
      facility is planned to be added.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 723b889fd219..41c441c2058d 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -105,7 +105,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 
 page_busy:
 	/* we might want to wait here, but that could deadlock the allocator as
-	 * the slow-work threads writing to the cache may all end up sleeping
+	 * the work threads writing to the cache may all end up sleeping
 	 * on memory allocation */
 	fscache_stat(&fscache_n_store_vmscan_busy);
 	return false;
@@ -188,9 +188,8 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 		return -ENOMEM;
 	}
 
-	fscache_operation_init(op, NULL);
-	fscache_operation_init_slow(op, fscache_attr_changed_op);
-	op->flags = FSCACHE_OP_SLOW | (1 << FSCACHE_OP_EXCLUSIVE);
+	fscache_operation_init(op, fscache_attr_changed_op, NULL);
+	op->flags = FSCACHE_OP_ASYNC | (1 << FSCACHE_OP_EXCLUSIVE);
 	fscache_set_op_name(op, "Attr");
 
 	spin_lock(&cookie->lock);
@@ -217,24 +216,6 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 }
 EXPORT_SYMBOL(__fscache_attr_changed);
 
-/*
- * handle secondary execution given to a retrieval op on behalf of the
- * cache
- */
-static void fscache_retrieval_work(struct work_struct *work)
-{
-	struct fscache_retrieval *op =
-		container_of(work, struct fscache_retrieval, op.fast_work);
-	unsigned long start;
-
-	_enter("{OP%x}", op->op.debug_id);
-
-	start = jiffies;
-	op->op.processor(&op->op);
-	fscache_hist(fscache_ops_histogram, start);
-	fscache_put_operation(&op->op);
-}
-
 /*
  * release a retrieval op reference
  */
@@ -269,13 +250,12 @@ static struct fscache_retrieval *fscache_alloc_retrieval(
 		return NULL;
 	}
 
-	fscache_operation_init(&op->op, fscache_release_retrieval_op);
+	fscache_operation_init(&op->op, NULL, fscache_release_retrieval_op);
 	op->op.flags	= FSCACHE_OP_MYTHREAD | (1 << FSCACHE_OP_WAITING);
 	op->mapping	= mapping;
 	op->end_io_func	= end_io_func;
 	op->context	= context;
 	op->start_time	= jiffies;
-	INIT_WORK(&op->op.fast_work, fscache_retrieval_work);
 	INIT_LIST_HEAD(&op->to_do);
 	fscache_set_op_name(&op->op, "Retr");
 	return op;
@@ -795,9 +775,9 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	if (!op)
 		goto nomem;
 
-	fscache_operation_init(&op->op, fscache_release_write_op);
-	fscache_operation_init_slow(&op->op, fscache_write_op);
-	op->op.flags = FSCACHE_OP_SLOW | (1 << FSCACHE_OP_WAITING);
+	fscache_operation_init(&op->op, fscache_write_op,
+			       fscache_release_write_op);
+	op->op.flags = FSCACHE_OP_ASYNC | (1 << FSCACHE_OP_WAITING);
 	fscache_set_op_name(&op->op, "Write1");
 
 	ret = radix_tree_preload(gfp & ~__GFP_HIGHMEM);
@@ -852,7 +832,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	fscache_stat(&fscache_n_store_ops);
 	fscache_stat(&fscache_n_stores_ok);
 
-	/* the slow work queue now carries its own ref on the object */
+	/* the work queue now carries its own ref on the object */
 	fscache_put_operation(&op->op);
 	_leave(" = 0");
 	return 0;

commit 08a66859e69264f3223560d06b88e80c1a6a6387
Author: Dan Carpenter <error27@gmail.com>
Date:   Tue Jun 1 20:58:22 2010 +0100

    FS-Cache: Remove unneeded null checks
    
    fscache_write_op() makes unnecessary checks of the page variable to see if it
    is NULL.  It can't be NULL at those points as the kernel would already have
    crashed a little higher up where we examined page->index.
    
    Furthermore, unless radix_tree_gang_lookup_tag() can return 1 but no page, a
    NULL pointer crash should not be encountered there as we can only get there if
    r_t_g_l_t() returned 1.
    
    Signed-off-by: Dan Carpenter <error27@gmail.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 47aefd376e54..723b889fd219 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -710,30 +710,26 @@ static void fscache_write_op(struct fscache_operation *_op)
 		goto superseded;
 	}
 
-	if (page) {
-		radix_tree_tag_set(&cookie->stores, page->index,
-				   FSCACHE_COOKIE_STORING_TAG);
-		radix_tree_tag_clear(&cookie->stores, page->index,
-				     FSCACHE_COOKIE_PENDING_TAG);
-	}
+	radix_tree_tag_set(&cookie->stores, page->index,
+			   FSCACHE_COOKIE_STORING_TAG);
+	radix_tree_tag_clear(&cookie->stores, page->index,
+			     FSCACHE_COOKIE_PENDING_TAG);
 
 	spin_unlock(&cookie->stores_lock);
 	spin_unlock(&object->lock);
 
-	if (page) {
-		fscache_set_op_state(&op->op, "Store");
-		fscache_stat(&fscache_n_store_pages);
-		fscache_stat(&fscache_n_cop_write_page);
-		ret = object->cache->ops->write_page(op, page);
-		fscache_stat_d(&fscache_n_cop_write_page);
-		fscache_set_op_state(&op->op, "EndWrite");
-		fscache_end_page_write(object, page);
-		if (ret < 0) {
-			fscache_set_op_state(&op->op, "Abort");
-			fscache_abort_object(object);
-		} else {
-			fscache_enqueue_operation(&op->op);
-		}
+	fscache_set_op_state(&op->op, "Store");
+	fscache_stat(&fscache_n_store_pages);
+	fscache_stat(&fscache_n_cop_write_page);
+	ret = object->cache->ops->write_page(op, page);
+	fscache_stat_d(&fscache_n_cop_write_page);
+	fscache_set_op_state(&op->op, "EndWrite");
+	fscache_end_page_write(object, page);
+	if (ret < 0) {
+		fscache_set_op_state(&op->op, "Abort");
+		fscache_abort_object(object);
+	} else {
+		fscache_enqueue_operation(&op->op);
 	}
 
 	_leave("");

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 69809024d71d..47aefd376e54 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -14,6 +14,7 @@
 #include <linux/fscache-cache.h>
 #include <linux/buffer_head.h>
 #include <linux/pagevec.h>
+#include <linux/slab.h>
 #include "internal.h"
 
 /*

commit 1147d0f915e3b4c5c4fa279dae2c40016b8f441d
Author: Dan Carpenter <error27@gmail.com>
Date:   Tue Mar 23 14:48:37 2010 +0000

    fscache: add missing unlock
    
    Sparse complained about this missing spin_unlock()
    
    Signed-off-by: Dan Carpenter <error27@gmail.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index c598ea4c4e7d..69809024d71d 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -881,6 +881,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	goto nobufs;
 
 nobufs_unlock_obj:
+	spin_unlock(&cookie->stores_lock);
 	spin_unlock(&object->lock);
 nobufs:
 	spin_unlock(&cookie->lock);

commit 60d543ca724be155c2b6166e36a00c80b21bd810
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 19 18:11:45 2009 +0000

    FS-Cache: Start processing an object's operations on that object's death
    
    Start processing an object's operations when that object moves into the DYING
    state as the object cannot be destroyed until all its outstanding operations
    have completed.
    
    Furthermore, make sure that read and allocation operations handle being woken
    up on a dead object.  Such events are recorded in the Allocs.abt and
    Retrvls.abt statistics as viewable through /proc/fs/fscache/stats.
    
    The code for waiting for object activation for the read and allocation
    operations is also extracted into its own function as it is much the same in
    all cases, differing only in the stats incremented.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index fc76798bd968..c598ea4c4e7d 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -313,6 +313,43 @@ static int fscache_wait_for_deferred_lookup(struct fscache_cookie *cookie)
 	return 0;
 }
 
+/*
+ * wait for an object to become active (or dead)
+ */
+static int fscache_wait_for_retrieval_activation(struct fscache_object *object,
+						 struct fscache_retrieval *op,
+						 atomic_t *stat_op_waits,
+						 atomic_t *stat_object_dead)
+{
+	int ret;
+
+	if (!test_bit(FSCACHE_OP_WAITING, &op->op.flags))
+		goto check_if_dead;
+
+	_debug(">>> WT");
+	fscache_stat(stat_op_waits);
+	if (wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+			fscache_wait_bit_interruptible,
+			TASK_INTERRUPTIBLE) < 0) {
+		ret = fscache_cancel_op(&op->op);
+		if (ret == 0)
+			return -ERESTARTSYS;
+
+		/* it's been removed from the pending queue by another party,
+		 * so we should get to run shortly */
+		wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+			    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+	}
+	_debug("<<< GO");
+
+check_if_dead:
+	if (unlikely(fscache_object_is_dead(object))) {
+		fscache_stat(stat_object_dead);
+		return -ENOBUFS;
+	}
+	return 0;
+}
+
 /*
  * read a page from the cache or allocate a block in which to store it
  * - we return:
@@ -376,25 +413,12 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 
 	/* we wait for the operation to become active, and then process it
 	 * *here*, in this thread, and not in the thread pool */
-	if (test_bit(FSCACHE_OP_WAITING, &op->op.flags)) {
-		_debug(">>> WT");
-		fscache_stat(&fscache_n_retrieval_op_waits);
-		if (wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
-				fscache_wait_bit_interruptible,
-				TASK_INTERRUPTIBLE) < 0) {
-			ret = fscache_cancel_op(&op->op);
-			if (ret == 0) {
-				ret = -ERESTARTSYS;
-				goto error;
-			}
-
-			/* it's been removed from the pending queue by another
-			 * party, so we should get to run shortly */
-			wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
-				    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
-		}
-		_debug("<<< GO");
-	}
+	ret = fscache_wait_for_retrieval_activation(
+		object, op,
+		__fscache_stat(&fscache_n_retrieval_op_waits),
+		__fscache_stat(&fscache_n_retrievals_object_dead));
+	if (ret < 0)
+		goto error;
 
 	/* ask the cache to honour the operation */
 	if (test_bit(FSCACHE_COOKIE_NO_DATA_YET, &object->cookie->flags)) {
@@ -506,25 +530,12 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 
 	/* we wait for the operation to become active, and then process it
 	 * *here*, in this thread, and not in the thread pool */
-	if (test_bit(FSCACHE_OP_WAITING, &op->op.flags)) {
-		_debug(">>> WT");
-		fscache_stat(&fscache_n_retrieval_op_waits);
-		if (wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
-				fscache_wait_bit_interruptible,
-				TASK_INTERRUPTIBLE) < 0) {
-			ret = fscache_cancel_op(&op->op);
-			if (ret == 0) {
-				ret = -ERESTARTSYS;
-				goto error;
-			}
-
-			/* it's been removed from the pending queue by another
-			 * party, so we should get to run shortly */
-			wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
-				    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
-		}
-		_debug("<<< GO");
-	}
+	ret = fscache_wait_for_retrieval_activation(
+		object, op,
+		__fscache_stat(&fscache_n_retrieval_op_waits),
+		__fscache_stat(&fscache_n_retrievals_object_dead));
+	if (ret < 0)
+		goto error;
 
 	/* ask the cache to honour the operation */
 	if (test_bit(FSCACHE_COOKIE_NO_DATA_YET, &object->cookie->flags)) {
@@ -612,25 +623,12 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 
 	fscache_stat(&fscache_n_alloc_ops);
 
-	if (test_bit(FSCACHE_OP_WAITING, &op->op.flags)) {
-		_debug(">>> WT");
-		fscache_stat(&fscache_n_alloc_op_waits);
-		if (wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
-				fscache_wait_bit_interruptible,
-				TASK_INTERRUPTIBLE) < 0) {
-			ret = fscache_cancel_op(&op->op);
-			if (ret == 0) {
-				ret = -ERESTARTSYS;
-				goto error;
-			}
-
-			/* it's been removed from the pending queue by another
-			 * party, so we should get to run shortly */
-			wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
-				    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
-		}
-		_debug("<<< GO");
-	}
+	ret = fscache_wait_for_retrieval_activation(
+		object, op,
+		__fscache_stat(&fscache_n_alloc_op_waits),
+		__fscache_stat(&fscache_n_allocs_object_dead));
+	if (ret < 0)
+		goto error;
 
 	/* ask the cache to honour the operation */
 	fscache_stat(&fscache_n_cop_allocate_page);

commit 201a15428bd54f83eccec8b7c64a04b8f9431204
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 19 18:11:35 2009 +0000

    FS-Cache: Handle pages pending storage that get evicted under OOM conditions
    
    Handle netfs pages that the vmscan algorithm wants to evict from the pagecache
    under OOM conditions, but that are waiting for write to the cache.  Under these
    conditions, vmscan calls the releasepage() function of the netfs, asking if a
    page can be discarded.
    
    The problem is typified by the following trace of a stuck process:
    
            kslowd005     D 0000000000000000     0  4253      2 0x00000080
             ffff88001b14f370 0000000000000046 ffff880020d0d000 0000000000000007
             0000000000000006 0000000000000001 ffff88001b14ffd8 ffff880020d0d2a8
             000000000000ddf0 00000000000118c0 00000000000118c0 ffff880020d0d2a8
            Call Trace:
             [<ffffffffa00782d8>] __fscache_wait_on_page_write+0x8b/0xa7 [fscache]
             [<ffffffff8104c0f1>] ? autoremove_wake_function+0x0/0x34
             [<ffffffffa0078240>] ? __fscache_check_page_write+0x63/0x70 [fscache]
             [<ffffffffa00b671d>] nfs_fscache_release_page+0x4e/0xc4 [nfs]
             [<ffffffffa00927f0>] nfs_release_page+0x3c/0x41 [nfs]
             [<ffffffff810885d3>] try_to_release_page+0x32/0x3b
             [<ffffffff81093203>] shrink_page_list+0x316/0x4ac
             [<ffffffff8109372b>] shrink_inactive_list+0x392/0x67c
             [<ffffffff813532fa>] ? __mutex_unlock_slowpath+0x100/0x10b
             [<ffffffff81058df0>] ? trace_hardirqs_on_caller+0x10c/0x130
             [<ffffffff8135330e>] ? mutex_unlock+0x9/0xb
             [<ffffffff81093aa2>] shrink_list+0x8d/0x8f
             [<ffffffff81093d1c>] shrink_zone+0x278/0x33c
             [<ffffffff81052d6c>] ? ktime_get_ts+0xad/0xba
             [<ffffffff81094b13>] try_to_free_pages+0x22e/0x392
             [<ffffffff81091e24>] ? isolate_pages_global+0x0/0x212
             [<ffffffff8108e743>] __alloc_pages_nodemask+0x3dc/0x5cf
             [<ffffffff81089529>] grab_cache_page_write_begin+0x65/0xaa
             [<ffffffff8110f8c0>] ext3_write_begin+0x78/0x1eb
             [<ffffffff81089ec5>] generic_file_buffered_write+0x109/0x28c
             [<ffffffff8103cb69>] ? current_fs_time+0x22/0x29
             [<ffffffff8108a509>] __generic_file_aio_write+0x350/0x385
             [<ffffffff8108a588>] ? generic_file_aio_write+0x4a/0xae
             [<ffffffff8108a59e>] generic_file_aio_write+0x60/0xae
             [<ffffffff810b2e82>] do_sync_write+0xe3/0x120
             [<ffffffff8104c0f1>] ? autoremove_wake_function+0x0/0x34
             [<ffffffff810b18e1>] ? __dentry_open+0x1a5/0x2b8
             [<ffffffff810b1a76>] ? dentry_open+0x82/0x89
             [<ffffffffa00e693c>] cachefiles_write_page+0x298/0x335 [cachefiles]
             [<ffffffffa0077147>] fscache_write_op+0x178/0x2c2 [fscache]
             [<ffffffffa0075656>] fscache_op_execute+0x7a/0xd1 [fscache]
             [<ffffffff81082093>] slow_work_execute+0x18f/0x2d1
             [<ffffffff8108239a>] slow_work_thread+0x1c5/0x308
             [<ffffffff8104c0f1>] ? autoremove_wake_function+0x0/0x34
             [<ffffffff810821d5>] ? slow_work_thread+0x0/0x308
             [<ffffffff8104be91>] kthread+0x7a/0x82
             [<ffffffff8100beda>] child_rip+0xa/0x20
             [<ffffffff8100b87c>] ? restore_args+0x0/0x30
             [<ffffffff8102ef83>] ? tg_shares_up+0x171/0x227
             [<ffffffff8104be17>] ? kthread+0x0/0x82
             [<ffffffff8100bed0>] ? child_rip+0x0/0x20
    
    In the above backtrace, the following is happening:
    
     (1) A page storage operation is being executed by a slow-work thread
         (fscache_write_op()).
    
     (2) FS-Cache farms the operation out to the cache to perform
         (cachefiles_write_page()).
    
     (3) CacheFiles is then calling Ext3 to perform the actual write, using Ext3's
         standard write (do_sync_write()) under KERNEL_DS directly from the netfs
         page.
    
     (4) However, for Ext3 to perform the write, it must allocate some memory, in
         particular, it must allocate at least one page cache page into which it
         can copy the data from the netfs page.
    
     (5) Under OOM conditions, the memory allocator can't immediately come up with
         a page, so it uses vmscan to find something to discard
         (try_to_free_pages()).
    
     (6) vmscan finds a clean netfs page it might be able to discard (possibly the
         one it's trying to write out).
    
     (7) The netfs is called to throw the page away (nfs_release_page()) - but it's
         called with __GFP_WAIT, so the netfs decides to wait for the store to
         complete (__fscache_wait_on_page_write()).
    
     (8) This blocks a slow-work processing thread - possibly against itself.
    
    The system ends up stuck because it can't write out any netfs pages to the
    cache without allocating more memory.
    
    To avoid this, we make FS-Cache cancel some writes that aren't in the middle of
    actually being performed.  This means that some data won't make it into the
    cache this time.  To support this, a new FS-Cache function is added
    fscache_maybe_release_page() that replaces what the netfs releasepage()
    functions used to do with respect to the cache.
    
    The decisions fscache_maybe_release_page() makes are counted and displayed
    through /proc/fs/fscache/stats on a line labelled "VmScan".  There are four
    counters provided: "nos=N" - pages that weren't pending storage; "gon=N" -
    pages that were pending storage when we first looked, but weren't by the time
    we got the object lock; "bsy=N" - pages that we ignored as they were actively
    being written when we looked; and "can=N" - pages that we cancelled the storage
    of.
    
    What I'd really like to do is alter the behaviour of the cancellation
    heuristics, depending on how necessary it is to expel pages.  If there are
    plenty of other pages that aren't waiting to be written to the cache that
    could be ejected first, then it would be nice to hold up on immediate
    cancellation of cache writes - but I don't see a way of doing that.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 022a5da8e130..fc76798bd968 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -42,6 +42,75 @@ void __fscache_wait_on_page_write(struct fscache_cookie *cookie, struct page *pa
 }
 EXPORT_SYMBOL(__fscache_wait_on_page_write);
 
+/*
+ * decide whether a page can be released, possibly by cancelling a store to it
+ * - we're allowed to sleep if __GFP_WAIT is flagged
+ */
+bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
+				  struct page *page,
+				  gfp_t gfp)
+{
+	struct page *xpage;
+	void *val;
+
+	_enter("%p,%p,%x", cookie, page, gfp);
+
+	rcu_read_lock();
+	val = radix_tree_lookup(&cookie->stores, page->index);
+	if (!val) {
+		rcu_read_unlock();
+		fscache_stat(&fscache_n_store_vmscan_not_storing);
+		__fscache_uncache_page(cookie, page);
+		return true;
+	}
+
+	/* see if the page is actually undergoing storage - if so we can't get
+	 * rid of it till the cache has finished with it */
+	if (radix_tree_tag_get(&cookie->stores, page->index,
+			       FSCACHE_COOKIE_STORING_TAG)) {
+		rcu_read_unlock();
+		goto page_busy;
+	}
+
+	/* the page is pending storage, so we attempt to cancel the store and
+	 * discard the store request so that the page can be reclaimed */
+	spin_lock(&cookie->stores_lock);
+	rcu_read_unlock();
+
+	if (radix_tree_tag_get(&cookie->stores, page->index,
+			       FSCACHE_COOKIE_STORING_TAG)) {
+		/* the page started to undergo storage whilst we were looking,
+		 * so now we can only wait or return */
+		spin_unlock(&cookie->stores_lock);
+		goto page_busy;
+	}
+
+	xpage = radix_tree_delete(&cookie->stores, page->index);
+	spin_unlock(&cookie->stores_lock);
+
+	if (xpage) {
+		fscache_stat(&fscache_n_store_vmscan_cancelled);
+		fscache_stat(&fscache_n_store_radix_deletes);
+		ASSERTCMP(xpage, ==, page);
+	} else {
+		fscache_stat(&fscache_n_store_vmscan_gone);
+	}
+
+	wake_up_bit(&cookie->flags, 0);
+	if (xpage)
+		page_cache_release(xpage);
+	__fscache_uncache_page(cookie, page);
+	return true;
+
+page_busy:
+	/* we might want to wait here, but that could deadlock the allocator as
+	 * the slow-work threads writing to the cache may all end up sleeping
+	 * on memory allocation */
+	fscache_stat(&fscache_n_store_vmscan_busy);
+	return false;
+}
+EXPORT_SYMBOL(__fscache_maybe_release_page);
+
 /*
  * note that a page has finished being written to the cache
  */
@@ -57,6 +126,8 @@ static void fscache_end_page_write(struct fscache_object *object,
 		/* delete the page from the tree if it is now no longer
 		 * pending */
 		spin_lock(&cookie->stores_lock);
+		radix_tree_tag_clear(&cookie->stores, page->index,
+				     FSCACHE_COOKIE_STORING_TAG);
 		if (!radix_tree_tag_get(&cookie->stores, page->index,
 					FSCACHE_COOKIE_PENDING_TAG)) {
 			fscache_stat(&fscache_n_store_radix_deletes);
@@ -640,8 +711,12 @@ static void fscache_write_op(struct fscache_operation *_op)
 		goto superseded;
 	}
 
-	radix_tree_tag_clear(&cookie->stores, page->index,
-			     FSCACHE_COOKIE_PENDING_TAG);
+	if (page) {
+		radix_tree_tag_set(&cookie->stores, page->index,
+				   FSCACHE_COOKIE_STORING_TAG);
+		radix_tree_tag_clear(&cookie->stores, page->index,
+				     FSCACHE_COOKIE_PENDING_TAG);
+	}
 
 	spin_unlock(&cookie->stores_lock);
 	spin_unlock(&object->lock);

commit 285e728b0ac55b53a673114096168d6f74930167
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 19 18:11:29 2009 +0000

    FS-Cache: Don't delete pending pages from the page-store tracking tree
    
    Don't delete pending pages from the page-store tracking tree, but rather send
    them for another write as they've presumably been updated.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 3ea8897bc217..022a5da8e130 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -57,8 +57,11 @@ static void fscache_end_page_write(struct fscache_object *object,
 		/* delete the page from the tree if it is now no longer
 		 * pending */
 		spin_lock(&cookie->stores_lock);
-		fscache_stat(&fscache_n_store_radix_deletes);
-		xpage = radix_tree_delete(&cookie->stores, page->index);
+		if (!radix_tree_tag_get(&cookie->stores, page->index,
+					FSCACHE_COOKIE_PENDING_TAG)) {
+			fscache_stat(&fscache_n_store_radix_deletes);
+			xpage = radix_tree_delete(&cookie->stores, page->index);
+		}
 		spin_unlock(&cookie->stores_lock);
 		wake_up_bit(&cookie->flags, 0);
 	}

commit 1bccf513ac49d44604ba1cddcc29f5886e70f1b6
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 19 18:11:25 2009 +0000

    FS-Cache: Fix lock misorder in fscache_write_op()
    
    FS-Cache has two structs internally for keeping track of the internal state of
    a cached file: the fscache_cookie struct, which represents the netfs's state,
    and fscache_object struct, which represents the cache's state.  Each has a
    pointer that points to the other (when both are in existence), and each has a
    spinlock for pointer maintenance.
    
    Since netfs operations approach these structures from the cookie side, they get
    the cookie lock first, then the object lock.  Cache operations, on the other
    hand, approach from the object side, and get the object lock first.  It is not
    then permitted for a cache operation to get the cookie lock whilst it is
    holding the object lock lest deadlock occur; instead, it must do one of two
    things:
    
     (1) increment the cookie usage counter, drop the object lock and then get both
         locks in order, or
    
     (2) simply hold the object lock as certain parts of the cookie may not be
         altered whilst the object lock is held.
    
    It is also not permitted to follow either pointer without holding the lock at
    the end you start with.  To break the pointers between the cookie and the
    object, both locks must be held.
    
    fscache_write_op(), however, violates the locking rules: It attempts to get the
    cookie lock without (a) checking that the cookie pointer is a valid pointer,
    and (b) holding the object lock to protect the cookie pointer whilst it follows
    it.  This is so that it can access the pending page store tree without
    interference from __fscache_write_page().
    
    This is fixed by splitting the cookie lock, such that the page store tracking
    tree is protected by its own lock, and checking that the cookie pointer is
    non-NULL before we attempt to follow it whilst holding the object lock.
    
    The new lock is subordinate to both the cookie lock and the object lock, and so
    should be taken after those.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index e6f2e61133a1..3ea8897bc217 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -45,16 +45,26 @@ EXPORT_SYMBOL(__fscache_wait_on_page_write);
 /*
  * note that a page has finished being written to the cache
  */
-static void fscache_end_page_write(struct fscache_cookie *cookie, struct page *page)
+static void fscache_end_page_write(struct fscache_object *object,
+				   struct page *page)
 {
-	struct page *xpage;
+	struct fscache_cookie *cookie;
+	struct page *xpage = NULL;
 
-	spin_lock(&cookie->lock);
-	xpage = radix_tree_delete(&cookie->stores, page->index);
-	spin_unlock(&cookie->lock);
-	ASSERT(xpage != NULL);
-
-	wake_up_bit(&cookie->flags, 0);
+	spin_lock(&object->lock);
+	cookie = object->cookie;
+	if (cookie) {
+		/* delete the page from the tree if it is now no longer
+		 * pending */
+		spin_lock(&cookie->stores_lock);
+		fscache_stat(&fscache_n_store_radix_deletes);
+		xpage = radix_tree_delete(&cookie->stores, page->index);
+		spin_unlock(&cookie->stores_lock);
+		wake_up_bit(&cookie->flags, 0);
+	}
+	spin_unlock(&object->lock);
+	if (xpage)
+		page_cache_release(xpage);
 }
 
 /*
@@ -591,7 +601,7 @@ static void fscache_write_op(struct fscache_operation *_op)
 	struct fscache_storage *op =
 		container_of(_op, struct fscache_storage, op);
 	struct fscache_object *object = op->op.object;
-	struct fscache_cookie *cookie = object->cookie;
+	struct fscache_cookie *cookie;
 	struct page *page;
 	unsigned n;
 	void *results[1];
@@ -601,16 +611,17 @@ static void fscache_write_op(struct fscache_operation *_op)
 
 	fscache_set_op_state(&op->op, "GetPage");
 
-	spin_lock(&cookie->lock);
 	spin_lock(&object->lock);
+	cookie = object->cookie;
 
-	if (!fscache_object_is_active(object)) {
+	if (!fscache_object_is_active(object) || !cookie) {
 		spin_unlock(&object->lock);
-		spin_unlock(&cookie->lock);
 		_leave("");
 		return;
 	}
 
+	spin_lock(&cookie->stores_lock);
+
 	fscache_stat(&fscache_n_store_calls);
 
 	/* find a page to store */
@@ -621,23 +632,25 @@ static void fscache_write_op(struct fscache_operation *_op)
 		goto superseded;
 	page = results[0];
 	_debug("gang %d [%lx]", n, page->index);
-	if (page->index > op->store_limit)
+	if (page->index > op->store_limit) {
+		fscache_stat(&fscache_n_store_pages_over_limit);
 		goto superseded;
+	}
 
 	radix_tree_tag_clear(&cookie->stores, page->index,
 			     FSCACHE_COOKIE_PENDING_TAG);
 
+	spin_unlock(&cookie->stores_lock);
 	spin_unlock(&object->lock);
-	spin_unlock(&cookie->lock);
 
 	if (page) {
 		fscache_set_op_state(&op->op, "Store");
+		fscache_stat(&fscache_n_store_pages);
 		fscache_stat(&fscache_n_cop_write_page);
 		ret = object->cache->ops->write_page(op, page);
 		fscache_stat_d(&fscache_n_cop_write_page);
 		fscache_set_op_state(&op->op, "EndWrite");
-		fscache_end_page_write(cookie, page);
-		page_cache_release(page);
+		fscache_end_page_write(object, page);
 		if (ret < 0) {
 			fscache_set_op_state(&op->op, "Abort");
 			fscache_abort_object(object);
@@ -653,9 +666,9 @@ static void fscache_write_op(struct fscache_operation *_op)
 	/* this writer is going away and there aren't any more things to
 	 * write */
 	_debug("cease");
+	spin_unlock(&cookie->stores_lock);
 	clear_bit(FSCACHE_OBJECT_PENDING_WRITE, &object->flags);
 	spin_unlock(&object->lock);
-	spin_unlock(&cookie->lock);
 	_leave("");
 }
 
@@ -731,6 +744,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	/* add the page to the pending-storage radix tree on the backing
 	 * object */
 	spin_lock(&object->lock);
+	spin_lock(&cookie->stores_lock);
 
 	_debug("store limit %llx", (unsigned long long) object->store_limit);
 
@@ -751,6 +765,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	if (test_and_set_bit(FSCACHE_OBJECT_PENDING_WRITE, &object->flags))
 		goto already_pending;
 
+	spin_unlock(&cookie->stores_lock);
 	spin_unlock(&object->lock);
 
 	op->op.debug_id	= atomic_inc_return(&fscache_op_debug_id);
@@ -772,6 +787,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 already_queued:
 	fscache_stat(&fscache_n_stores_again);
 already_pending:
+	spin_unlock(&cookie->stores_lock);
 	spin_unlock(&object->lock);
 	spin_unlock(&cookie->lock);
 	radix_tree_preload_end();
@@ -781,7 +797,9 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	return 0;
 
 submit_failed:
+	spin_lock(&cookie->stores_lock);
 	radix_tree_delete(&cookie->stores, page->index);
+	spin_unlock(&cookie->stores_lock);
 	page_cache_release(page);
 	ret = -ENOBUFS;
 	goto nobufs;

commit 5753c441889253e4323eee85f791a1d64cf08196
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 19 18:11:19 2009 +0000

    FS-Cache: Permit cache retrieval ops to be interrupted in the initial wait phase
    
    Permit the operations to retrieve data from the cache or to allocate space in
    the cache for future writes to be interrupted whilst they're waiting for
    permission for the operation to proceed.  Typically this wait occurs whilst the
    cache object is being looked up on disk in the background.
    
    If an interruption occurs, and the operation has not yet been given the
    go-ahead to run, the operation is dequeued and cancelled, and control returns
    to the read operation of the netfs routine with none of the requested pages
    having been read or in any way marked as known by the cache.
    
    This means that the initial wait is done interruptibly rather than
    uninterruptibly.
    
    In addition, extra stats values are made available to show the number of ops
    cancelled and the number of cache space allocations interrupted.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 250dfd34c07b..e6f2e61133a1 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -295,8 +295,20 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 	if (test_bit(FSCACHE_OP_WAITING, &op->op.flags)) {
 		_debug(">>> WT");
 		fscache_stat(&fscache_n_retrieval_op_waits);
-		wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
-			    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+		if (wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+				fscache_wait_bit_interruptible,
+				TASK_INTERRUPTIBLE) < 0) {
+			ret = fscache_cancel_op(&op->op);
+			if (ret == 0) {
+				ret = -ERESTARTSYS;
+				goto error;
+			}
+
+			/* it's been removed from the pending queue by another
+			 * party, so we should get to run shortly */
+			wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+				    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+		}
 		_debug("<<< GO");
 	}
 
@@ -313,6 +325,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 		fscache_stat_d(&fscache_n_cop_read_or_alloc_page);
 	}
 
+error:
 	if (ret == -ENOMEM)
 		fscache_stat(&fscache_n_retrievals_nomem);
 	else if (ret == -ERESTARTSYS)
@@ -412,8 +425,20 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	if (test_bit(FSCACHE_OP_WAITING, &op->op.flags)) {
 		_debug(">>> WT");
 		fscache_stat(&fscache_n_retrieval_op_waits);
-		wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
-			    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+		if (wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+				fscache_wait_bit_interruptible,
+				TASK_INTERRUPTIBLE) < 0) {
+			ret = fscache_cancel_op(&op->op);
+			if (ret == 0) {
+				ret = -ERESTARTSYS;
+				goto error;
+			}
+
+			/* it's been removed from the pending queue by another
+			 * party, so we should get to run shortly */
+			wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+				    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+		}
 		_debug("<<< GO");
 	}
 
@@ -430,6 +455,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 		fscache_stat_d(&fscache_n_cop_read_or_alloc_pages);
 	}
 
+error:
 	if (ret == -ENOMEM)
 		fscache_stat(&fscache_n_retrievals_nomem);
 	else if (ret == -ERESTARTSYS)
@@ -505,8 +531,20 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	if (test_bit(FSCACHE_OP_WAITING, &op->op.flags)) {
 		_debug(">>> WT");
 		fscache_stat(&fscache_n_alloc_op_waits);
-		wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
-			    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+		if (wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+				fscache_wait_bit_interruptible,
+				TASK_INTERRUPTIBLE) < 0) {
+			ret = fscache_cancel_op(&op->op);
+			if (ret == 0) {
+				ret = -ERESTARTSYS;
+				goto error;
+			}
+
+			/* it's been removed from the pending queue by another
+			 * party, so we should get to run shortly */
+			wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+				    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+		}
 		_debug("<<< GO");
 	}
 
@@ -515,7 +553,10 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	ret = object->cache->ops->allocate_page(op, page, gfp);
 	fscache_stat_d(&fscache_n_cop_allocate_page);
 
-	if (ret < 0)
+error:
+	if (ret == -ERESTARTSYS)
+		fscache_stat(&fscache_n_allocs_intr);
+	else if (ret < 0)
 		fscache_stat(&fscache_n_allocs_nobufs);
 	else
 		fscache_stat(&fscache_n_allocs_ok);

commit 52bd75fdb135d6133d878ae60c6e7e3f4ebc1cfc
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 19 18:11:08 2009 +0000

    FS-Cache: Add counters for entry/exit to/from cache operation functions
    
    Count entries to and exits from cache operation table functions.  Maintain
    these as a single counter that's added to or removed from as appropriate.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index c5973e38ce39..250dfd34c07b 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -71,7 +71,9 @@ static void fscache_attr_changed_op(struct fscache_operation *op)
 
 	if (fscache_object_is_active(object)) {
 		fscache_set_op_state(op, "CallFS");
+		fscache_stat(&fscache_n_cop_attr_changed);
 		ret = object->cache->ops->attr_changed(object);
+		fscache_stat_d(&fscache_n_cop_attr_changed);
 		fscache_set_op_state(op, "Done");
 		if (ret < 0)
 			fscache_abort_object(object);
@@ -300,11 +302,15 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 
 	/* ask the cache to honour the operation */
 	if (test_bit(FSCACHE_COOKIE_NO_DATA_YET, &object->cookie->flags)) {
+		fscache_stat(&fscache_n_cop_allocate_page);
 		ret = object->cache->ops->allocate_page(op, page, gfp);
+		fscache_stat_d(&fscache_n_cop_allocate_page);
 		if (ret == 0)
 			ret = -ENODATA;
 	} else {
+		fscache_stat(&fscache_n_cop_read_or_alloc_page);
 		ret = object->cache->ops->read_or_alloc_page(op, page, gfp);
+		fscache_stat_d(&fscache_n_cop_read_or_alloc_page);
 	}
 
 	if (ret == -ENOMEM)
@@ -358,7 +364,6 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 				  void *context,
 				  gfp_t gfp)
 {
-	fscache_pages_retrieval_func_t func;
 	struct fscache_retrieval *op;
 	struct fscache_object *object;
 	int ret;
@@ -413,11 +418,17 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	}
 
 	/* ask the cache to honour the operation */
-	if (test_bit(FSCACHE_COOKIE_NO_DATA_YET, &object->cookie->flags))
-		func = object->cache->ops->allocate_pages;
-	else
-		func = object->cache->ops->read_or_alloc_pages;
-	ret = func(op, pages, nr_pages, gfp);
+	if (test_bit(FSCACHE_COOKIE_NO_DATA_YET, &object->cookie->flags)) {
+		fscache_stat(&fscache_n_cop_allocate_pages);
+		ret = object->cache->ops->allocate_pages(
+			op, pages, nr_pages, gfp);
+		fscache_stat_d(&fscache_n_cop_allocate_pages);
+	} else {
+		fscache_stat(&fscache_n_cop_read_or_alloc_pages);
+		ret = object->cache->ops->read_or_alloc_pages(
+			op, pages, nr_pages, gfp);
+		fscache_stat_d(&fscache_n_cop_read_or_alloc_pages);
+	}
 
 	if (ret == -ENOMEM)
 		fscache_stat(&fscache_n_retrievals_nomem);
@@ -500,7 +511,9 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	}
 
 	/* ask the cache to honour the operation */
+	fscache_stat(&fscache_n_cop_allocate_page);
 	ret = object->cache->ops->allocate_page(op, page, gfp);
+	fscache_stat_d(&fscache_n_cop_allocate_page);
 
 	if (ret < 0)
 		fscache_stat(&fscache_n_allocs_nobufs);
@@ -578,7 +591,9 @@ static void fscache_write_op(struct fscache_operation *_op)
 
 	if (page) {
 		fscache_set_op_state(&op->op, "Store");
+		fscache_stat(&fscache_n_cop_write_page);
 		ret = object->cache->ops->write_page(op, page);
+		fscache_stat_d(&fscache_n_cop_write_page);
 		fscache_set_op_state(&op->op, "EndWrite");
 		fscache_end_page_write(cookie, page);
 		page_cache_release(page);
@@ -786,7 +801,9 @@ void __fscache_uncache_page(struct fscache_cookie *cookie, struct page *page)
 	if (TestClearPageFsCache(page) &&
 	    object->cache->ops->uncache_page) {
 		/* the cache backend releases the cookie lock */
+		fscache_stat(&fscache_n_cop_uncache_page);
 		object->cache->ops->uncache_page(object, page);
+		fscache_stat_d(&fscache_n_cop_uncache_page);
 		goto done;
 	}
 

commit 4fbf4291aa15926cd4fdca0ffe9122e89d0459db
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 19 18:11:04 2009 +0000

    FS-Cache: Allow the current state of all objects to be dumped
    
    Allow the current state of all fscache objects to be dumped by doing:
    
            cat /proc/fs/fscache/objects
    
    By default, all objects and all fields will be shown.  This can be restricted
    by adding a suitable key to one of the caller's keyrings (such as the session
    keyring):
    
            keyctl add user fscache:objlist "<restrictions>" @s
    
    The <restrictions> are:
    
            K       Show hexdump of object key (don't show if not given)
            A       Show hexdump of object aux data (don't show if not given)
    
    And paired restrictions:
    
            C       Show objects that have a cookie
            c       Show objects that don't have a cookie
            B       Show objects that are busy
            b       Show objects that aren't busy
            W       Show objects that have pending writes
            w       Show objects that don't have pending writes
            R       Show objects that have outstanding reads
            r       Show objects that don't have outstanding reads
            S       Show objects that have slow work queued
            s       Show objects that don't have slow work queued
    
    If neither side of a restriction pair is given, then both are implied.  For
    example:
    
            keyctl add user fscache:objlist KB @s
    
    shows objects that are busy, and lists their object keys, but does not dump
    their auxiliary data.  It also implies "CcWwRrSs", but as 'B' is given, 'b' is
    not implied.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index e8bbc395cef6..c5973e38ce39 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -275,6 +275,9 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 
 	ASSERTCMP(object->state, >, FSCACHE_OBJECT_LOOKING_UP);
 
+	atomic_inc(&object->n_reads);
+	set_bit(FSCACHE_OP_DEC_READ_CNT, &op->op.flags);
+
 	if (fscache_submit_op(object, &op->op) < 0)
 		goto nobufs_unlock;
 	spin_unlock(&cookie->lock);
@@ -386,6 +389,9 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	object = hlist_entry(cookie->backing_objects.first,
 			     struct fscache_object, cookie_link);
 
+	atomic_inc(&object->n_reads);
+	set_bit(FSCACHE_OP_DEC_READ_CNT, &op->op.flags);
+
 	if (fscache_submit_op(object, &op->op) < 0)
 		goto nobufs_unlock;
 	spin_unlock(&cookie->lock);

commit 440f0affe247e9990c8f8778f1861da4fd7d5e50
Author: David Howells <dhowells@redhat.com>
Date:   Thu Nov 19 18:11:01 2009 +0000

    FS-Cache: Annotate slow-work runqueue proc lines for FS-Cache work items
    
    Annotate slow-work runqueue proc lines for FS-Cache work items.  Objects
    include the object ID and the state.  Operations include the object ID, the
    operation ID and the operation type and state.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
index 2568e0eb644f..e8bbc395cef6 100644
--- a/fs/fscache/page.c
+++ b/fs/fscache/page.c
@@ -63,14 +63,19 @@ static void fscache_end_page_write(struct fscache_cookie *cookie, struct page *p
 static void fscache_attr_changed_op(struct fscache_operation *op)
 {
 	struct fscache_object *object = op->object;
+	int ret;
 
 	_enter("{OBJ%x OP%x}", object->debug_id, op->debug_id);
 
 	fscache_stat(&fscache_n_attr_changed_calls);
 
-	if (fscache_object_is_active(object) &&
-	    object->cache->ops->attr_changed(object) < 0)
-		fscache_abort_object(object);
+	if (fscache_object_is_active(object)) {
+		fscache_set_op_state(op, "CallFS");
+		ret = object->cache->ops->attr_changed(object);
+		fscache_set_op_state(op, "Done");
+		if (ret < 0)
+			fscache_abort_object(object);
+	}
 
 	_leave("");
 }
@@ -99,6 +104,7 @@ int __fscache_attr_changed(struct fscache_cookie *cookie)
 	fscache_operation_init(op, NULL);
 	fscache_operation_init_slow(op, fscache_attr_changed_op);
 	op->flags = FSCACHE_OP_SLOW | (1 << FSCACHE_OP_EXCLUSIVE);
+	fscache_set_op_name(op, "Attr");
 
 	spin_lock(&cookie->lock);
 
@@ -184,6 +190,7 @@ static struct fscache_retrieval *fscache_alloc_retrieval(
 	op->start_time	= jiffies;
 	INIT_WORK(&op->op.fast_work, fscache_retrieval_work);
 	INIT_LIST_HEAD(&op->to_do);
+	fscache_set_op_name(&op->op, "Retr");
 	return op;
 }
 
@@ -257,6 +264,7 @@ int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
 		_leave(" = -ENOMEM");
 		return -ENOMEM;
 	}
+	fscache_set_op_name(&op->op, "RetrRA1");
 
 	spin_lock(&cookie->lock);
 
@@ -369,6 +377,7 @@ int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
 	op = fscache_alloc_retrieval(mapping, end_io_func, context);
 	if (!op)
 		return -ENOMEM;
+	fscache_set_op_name(&op->op, "RetrRAN");
 
 	spin_lock(&cookie->lock);
 
@@ -461,6 +470,7 @@ int __fscache_alloc_page(struct fscache_cookie *cookie,
 	op = fscache_alloc_retrieval(page->mapping, NULL, NULL);
 	if (!op)
 		return -ENOMEM;
+	fscache_set_op_name(&op->op, "RetrAL1");
 
 	spin_lock(&cookie->lock);
 
@@ -529,6 +539,8 @@ static void fscache_write_op(struct fscache_operation *_op)
 
 	_enter("{OP%x,%d}", op->op.debug_id, atomic_read(&op->op.usage));
 
+	fscache_set_op_state(&op->op, "GetPage");
+
 	spin_lock(&cookie->lock);
 	spin_lock(&object->lock);
 
@@ -559,13 +571,17 @@ static void fscache_write_op(struct fscache_operation *_op)
 	spin_unlock(&cookie->lock);
 
 	if (page) {
+		fscache_set_op_state(&op->op, "Store");
 		ret = object->cache->ops->write_page(op, page);
+		fscache_set_op_state(&op->op, "EndWrite");
 		fscache_end_page_write(cookie, page);
 		page_cache_release(page);
-		if (ret < 0)
+		if (ret < 0) {
+			fscache_set_op_state(&op->op, "Abort");
 			fscache_abort_object(object);
-		else
+		} else {
 			fscache_enqueue_operation(&op->op);
+		}
 	}
 
 	_leave("");
@@ -634,6 +650,7 @@ int __fscache_write_page(struct fscache_cookie *cookie,
 	fscache_operation_init(&op->op, fscache_release_write_op);
 	fscache_operation_init_slow(&op->op, fscache_write_op);
 	op->op.flags = FSCACHE_OP_SLOW | (1 << FSCACHE_OP_WAITING);
+	fscache_set_op_name(&op->op, "Write1");
 
 	ret = radix_tree_preload(gfp & ~__GFP_HIGHMEM);
 	if (ret < 0)

commit b510882281d56873e1194021643b7c325336f84f
Author: David Howells <dhowells@redhat.com>
Date:   Fri Apr 3 16:42:39 2009 +0100

    FS-Cache: Implement data I/O part of netfs API
    
    Implement the data I/O part of the FS-Cache netfs API.  The documentation and
    API header file were added in a previous patch.
    
    This patch implements the following functions for the netfs to call:
    
     (*) fscache_attr_changed().
    
         Indicate that the object has changed its attributes.  The only attribute
         currently recorded is the file size.  Only pages within the set file size
         will be stored in the cache.
    
         This operation is submitted for asynchronous processing, and will return
         immediately.  It will return -ENOMEM if an out of memory error is
         encountered, -ENOBUFS if the object is not actually cached, or 0 if the
         operation is successfully queued.
    
     (*) fscache_read_or_alloc_page().
     (*) fscache_read_or_alloc_pages().
    
         Request data be fetched from the disk, and allocate internal metadata to
         track the netfs pages and reserve disk space for unknown pages.
    
         These operations perform semi-asynchronous data reads.  Upon returning
         they will indicate which pages they think can be retrieved from disk, and
         will have set in progress attempts to retrieve those pages.
    
         These will return, in order of preference, -ENOMEM on memory allocation
         error, -ERESTARTSYS if a signal interrupted proceedings, -ENODATA if one
         or more requested pages are not yet cached, -ENOBUFS if the object is not
         actually cached or if there isn't space for future pages to be cached on
         this object, or 0 if successful.
    
         In the case of the multipage function, the pages for which reads are set
         in progress will be removed from the list and the page count decreased
         appropriately.
    
         If any read operations should fail, the completion function will be given
         an error, and will also be passed contextual information to allow the
         netfs to fall back to querying the server for the absent pages.
    
         For each successful read, the page completion function will also be
         called.
    
         Any pages subsequently tracked by the cache will have PG_fscache set upon
         them on return.  fscache_uncache_page() must be called for such pages.
    
         If supplied by the netfs, the mark_pages_cached() cookie op will be
         invoked for any pages now tracked.
    
     (*) fscache_alloc_page().
    
         Allocate internal metadata to track a netfs page and reserve disk space.
    
         This will return -ENOMEM on memory allocation error, -ERESTARTSYS on
         signal, -ENOBUFS if the object isn't cached, or there isn't enough space
         in the cache, or 0 if successful.
    
         Any pages subsequently tracked by the cache will have PG_fscache set upon
         them on return.  fscache_uncache_page() must be called for such pages.
    
         If supplied by the netfs, the mark_pages_cached() cookie op will be
         invoked for any pages now tracked.
    
     (*) fscache_write_page().
    
         Request data be stored to disk.  This may only be called on pages that
         have been read or alloc'd by the above three functions and have not yet
         been uncached.
    
         This will return -ENOMEM on memory allocation error, -ERESTARTSYS on
         signal, -ENOBUFS if the object isn't cached, or there isn't immediately
         enough space in the cache, or 0 if successful.
    
         On a successful return, this operation will have queued the page for
         asynchronous writing to the cache.  The page will be returned with
         PG_fscache_write set until the write completes one way or another.  The
         caller will not be notified if the write fails due to an I/O error.  If
         that happens, the object will become available and all pending writes will
         be aborted.
    
         Note that the cache may batch up page writes, and so it may take a while
         to get around to writing them out.
    
         The caller must assume that until PG_fscache_write is cleared the page is
         use by the cache.  Any changes made to the page may be reflected on disk.
         The page may even be under DMA.
    
     (*) fscache_uncache_page().
    
         Indicate that the cache should stop tracking a page previously read or
         alloc'd from the cache.  If the page was alloc'd only, but unwritten, it
         will not appear on disk.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Steve Dickson <steved@redhat.com>
    Acked-by: Trond Myklebust <Trond.Myklebust@netapp.com>
    Acked-by: Al Viro <viro@zeniv.linux.org.uk>
    Tested-by: Daire Byrne <Daire.Byrne@framestore.com>

diff --git a/fs/fscache/page.c b/fs/fscache/page.c
new file mode 100644
index 000000000000..2568e0eb644f
--- /dev/null
+++ b/fs/fscache/page.c
@@ -0,0 +1,816 @@
+/* Cache page management and data I/O routines
+ *
+ * Copyright (C) 2004-2008 Red Hat, Inc. All Rights Reserved.
+ * Written by David Howells (dhowells@redhat.com)
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#define FSCACHE_DEBUG_LEVEL PAGE
+#include <linux/module.h>
+#include <linux/fscache-cache.h>
+#include <linux/buffer_head.h>
+#include <linux/pagevec.h>
+#include "internal.h"
+
+/*
+ * check to see if a page is being written to the cache
+ */
+bool __fscache_check_page_write(struct fscache_cookie *cookie, struct page *page)
+{
+	void *val;
+
+	rcu_read_lock();
+	val = radix_tree_lookup(&cookie->stores, page->index);
+	rcu_read_unlock();
+
+	return val != NULL;
+}
+EXPORT_SYMBOL(__fscache_check_page_write);
+
+/*
+ * wait for a page to finish being written to the cache
+ */
+void __fscache_wait_on_page_write(struct fscache_cookie *cookie, struct page *page)
+{
+	wait_queue_head_t *wq = bit_waitqueue(&cookie->flags, 0);
+
+	wait_event(*wq, !__fscache_check_page_write(cookie, page));
+}
+EXPORT_SYMBOL(__fscache_wait_on_page_write);
+
+/*
+ * note that a page has finished being written to the cache
+ */
+static void fscache_end_page_write(struct fscache_cookie *cookie, struct page *page)
+{
+	struct page *xpage;
+
+	spin_lock(&cookie->lock);
+	xpage = radix_tree_delete(&cookie->stores, page->index);
+	spin_unlock(&cookie->lock);
+	ASSERT(xpage != NULL);
+
+	wake_up_bit(&cookie->flags, 0);
+}
+
+/*
+ * actually apply the changed attributes to a cache object
+ */
+static void fscache_attr_changed_op(struct fscache_operation *op)
+{
+	struct fscache_object *object = op->object;
+
+	_enter("{OBJ%x OP%x}", object->debug_id, op->debug_id);
+
+	fscache_stat(&fscache_n_attr_changed_calls);
+
+	if (fscache_object_is_active(object) &&
+	    object->cache->ops->attr_changed(object) < 0)
+		fscache_abort_object(object);
+
+	_leave("");
+}
+
+/*
+ * notification that the attributes on an object have changed
+ */
+int __fscache_attr_changed(struct fscache_cookie *cookie)
+{
+	struct fscache_operation *op;
+	struct fscache_object *object;
+
+	_enter("%p", cookie);
+
+	ASSERTCMP(cookie->def->type, !=, FSCACHE_COOKIE_TYPE_INDEX);
+
+	fscache_stat(&fscache_n_attr_changed);
+
+	op = kzalloc(sizeof(*op), GFP_KERNEL);
+	if (!op) {
+		fscache_stat(&fscache_n_attr_changed_nomem);
+		_leave(" = -ENOMEM");
+		return -ENOMEM;
+	}
+
+	fscache_operation_init(op, NULL);
+	fscache_operation_init_slow(op, fscache_attr_changed_op);
+	op->flags = FSCACHE_OP_SLOW | (1 << FSCACHE_OP_EXCLUSIVE);
+
+	spin_lock(&cookie->lock);
+
+	if (hlist_empty(&cookie->backing_objects))
+		goto nobufs;
+	object = hlist_entry(cookie->backing_objects.first,
+			     struct fscache_object, cookie_link);
+
+	if (fscache_submit_exclusive_op(object, op) < 0)
+		goto nobufs;
+	spin_unlock(&cookie->lock);
+	fscache_stat(&fscache_n_attr_changed_ok);
+	fscache_put_operation(op);
+	_leave(" = 0");
+	return 0;
+
+nobufs:
+	spin_unlock(&cookie->lock);
+	kfree(op);
+	fscache_stat(&fscache_n_attr_changed_nobufs);
+	_leave(" = %d", -ENOBUFS);
+	return -ENOBUFS;
+}
+EXPORT_SYMBOL(__fscache_attr_changed);
+
+/*
+ * handle secondary execution given to a retrieval op on behalf of the
+ * cache
+ */
+static void fscache_retrieval_work(struct work_struct *work)
+{
+	struct fscache_retrieval *op =
+		container_of(work, struct fscache_retrieval, op.fast_work);
+	unsigned long start;
+
+	_enter("{OP%x}", op->op.debug_id);
+
+	start = jiffies;
+	op->op.processor(&op->op);
+	fscache_hist(fscache_ops_histogram, start);
+	fscache_put_operation(&op->op);
+}
+
+/*
+ * release a retrieval op reference
+ */
+static void fscache_release_retrieval_op(struct fscache_operation *_op)
+{
+	struct fscache_retrieval *op =
+		container_of(_op, struct fscache_retrieval, op);
+
+	_enter("{OP%x}", op->op.debug_id);
+
+	fscache_hist(fscache_retrieval_histogram, op->start_time);
+	if (op->context)
+		fscache_put_context(op->op.object->cookie, op->context);
+
+	_leave("");
+}
+
+/*
+ * allocate a retrieval op
+ */
+static struct fscache_retrieval *fscache_alloc_retrieval(
+	struct address_space *mapping,
+	fscache_rw_complete_t end_io_func,
+	void *context)
+{
+	struct fscache_retrieval *op;
+
+	/* allocate a retrieval operation and attempt to submit it */
+	op = kzalloc(sizeof(*op), GFP_NOIO);
+	if (!op) {
+		fscache_stat(&fscache_n_retrievals_nomem);
+		return NULL;
+	}
+
+	fscache_operation_init(&op->op, fscache_release_retrieval_op);
+	op->op.flags	= FSCACHE_OP_MYTHREAD | (1 << FSCACHE_OP_WAITING);
+	op->mapping	= mapping;
+	op->end_io_func	= end_io_func;
+	op->context	= context;
+	op->start_time	= jiffies;
+	INIT_WORK(&op->op.fast_work, fscache_retrieval_work);
+	INIT_LIST_HEAD(&op->to_do);
+	return op;
+}
+
+/*
+ * wait for a deferred lookup to complete
+ */
+static int fscache_wait_for_deferred_lookup(struct fscache_cookie *cookie)
+{
+	unsigned long jif;
+
+	_enter("");
+
+	if (!test_bit(FSCACHE_COOKIE_LOOKING_UP, &cookie->flags)) {
+		_leave(" = 0 [imm]");
+		return 0;
+	}
+
+	fscache_stat(&fscache_n_retrievals_wait);
+
+	jif = jiffies;
+	if (wait_on_bit(&cookie->flags, FSCACHE_COOKIE_LOOKING_UP,
+			fscache_wait_bit_interruptible,
+			TASK_INTERRUPTIBLE) != 0) {
+		fscache_stat(&fscache_n_retrievals_intr);
+		_leave(" = -ERESTARTSYS");
+		return -ERESTARTSYS;
+	}
+
+	ASSERT(!test_bit(FSCACHE_COOKIE_LOOKING_UP, &cookie->flags));
+
+	smp_rmb();
+	fscache_hist(fscache_retrieval_delay_histogram, jif);
+	_leave(" = 0 [dly]");
+	return 0;
+}
+
+/*
+ * read a page from the cache or allocate a block in which to store it
+ * - we return:
+ *   -ENOMEM	- out of memory, nothing done
+ *   -ERESTARTSYS - interrupted
+ *   -ENOBUFS	- no backing object available in which to cache the block
+ *   -ENODATA	- no data available in the backing object for this block
+ *   0		- dispatched a read - it'll call end_io_func() when finished
+ */
+int __fscache_read_or_alloc_page(struct fscache_cookie *cookie,
+				 struct page *page,
+				 fscache_rw_complete_t end_io_func,
+				 void *context,
+				 gfp_t gfp)
+{
+	struct fscache_retrieval *op;
+	struct fscache_object *object;
+	int ret;
+
+	_enter("%p,%p,,,", cookie, page);
+
+	fscache_stat(&fscache_n_retrievals);
+
+	if (hlist_empty(&cookie->backing_objects))
+		goto nobufs;
+
+	ASSERTCMP(cookie->def->type, !=, FSCACHE_COOKIE_TYPE_INDEX);
+	ASSERTCMP(page, !=, NULL);
+
+	if (fscache_wait_for_deferred_lookup(cookie) < 0)
+		return -ERESTARTSYS;
+
+	op = fscache_alloc_retrieval(page->mapping, end_io_func, context);
+	if (!op) {
+		_leave(" = -ENOMEM");
+		return -ENOMEM;
+	}
+
+	spin_lock(&cookie->lock);
+
+	if (hlist_empty(&cookie->backing_objects))
+		goto nobufs_unlock;
+	object = hlist_entry(cookie->backing_objects.first,
+			     struct fscache_object, cookie_link);
+
+	ASSERTCMP(object->state, >, FSCACHE_OBJECT_LOOKING_UP);
+
+	if (fscache_submit_op(object, &op->op) < 0)
+		goto nobufs_unlock;
+	spin_unlock(&cookie->lock);
+
+	fscache_stat(&fscache_n_retrieval_ops);
+
+	/* pin the netfs read context in case we need to do the actual netfs
+	 * read because we've encountered a cache read failure */
+	fscache_get_context(object->cookie, op->context);
+
+	/* we wait for the operation to become active, and then process it
+	 * *here*, in this thread, and not in the thread pool */
+	if (test_bit(FSCACHE_OP_WAITING, &op->op.flags)) {
+		_debug(">>> WT");
+		fscache_stat(&fscache_n_retrieval_op_waits);
+		wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+			    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+		_debug("<<< GO");
+	}
+
+	/* ask the cache to honour the operation */
+	if (test_bit(FSCACHE_COOKIE_NO_DATA_YET, &object->cookie->flags)) {
+		ret = object->cache->ops->allocate_page(op, page, gfp);
+		if (ret == 0)
+			ret = -ENODATA;
+	} else {
+		ret = object->cache->ops->read_or_alloc_page(op, page, gfp);
+	}
+
+	if (ret == -ENOMEM)
+		fscache_stat(&fscache_n_retrievals_nomem);
+	else if (ret == -ERESTARTSYS)
+		fscache_stat(&fscache_n_retrievals_intr);
+	else if (ret == -ENODATA)
+		fscache_stat(&fscache_n_retrievals_nodata);
+	else if (ret < 0)
+		fscache_stat(&fscache_n_retrievals_nobufs);
+	else
+		fscache_stat(&fscache_n_retrievals_ok);
+
+	fscache_put_retrieval(op);
+	_leave(" = %d", ret);
+	return ret;
+
+nobufs_unlock:
+	spin_unlock(&cookie->lock);
+	kfree(op);
+nobufs:
+	fscache_stat(&fscache_n_retrievals_nobufs);
+	_leave(" = -ENOBUFS");
+	return -ENOBUFS;
+}
+EXPORT_SYMBOL(__fscache_read_or_alloc_page);
+
+/*
+ * read a list of page from the cache or allocate a block in which to store
+ * them
+ * - we return:
+ *   -ENOMEM	- out of memory, some pages may be being read
+ *   -ERESTARTSYS - interrupted, some pages may be being read
+ *   -ENOBUFS	- no backing object or space available in which to cache any
+ *                pages not being read
+ *   -ENODATA	- no data available in the backing object for some or all of
+ *                the pages
+ *   0		- dispatched a read on all pages
+ *
+ * end_io_func() will be called for each page read from the cache as it is
+ * finishes being read
+ *
+ * any pages for which a read is dispatched will be removed from pages and
+ * nr_pages
+ */
+int __fscache_read_or_alloc_pages(struct fscache_cookie *cookie,
+				  struct address_space *mapping,
+				  struct list_head *pages,
+				  unsigned *nr_pages,
+				  fscache_rw_complete_t end_io_func,
+				  void *context,
+				  gfp_t gfp)
+{
+	fscache_pages_retrieval_func_t func;
+	struct fscache_retrieval *op;
+	struct fscache_object *object;
+	int ret;
+
+	_enter("%p,,%d,,,", cookie, *nr_pages);
+
+	fscache_stat(&fscache_n_retrievals);
+
+	if (hlist_empty(&cookie->backing_objects))
+		goto nobufs;
+
+	ASSERTCMP(cookie->def->type, !=, FSCACHE_COOKIE_TYPE_INDEX);
+	ASSERTCMP(*nr_pages, >, 0);
+	ASSERT(!list_empty(pages));
+
+	if (fscache_wait_for_deferred_lookup(cookie) < 0)
+		return -ERESTARTSYS;
+
+	op = fscache_alloc_retrieval(mapping, end_io_func, context);
+	if (!op)
+		return -ENOMEM;
+
+	spin_lock(&cookie->lock);
+
+	if (hlist_empty(&cookie->backing_objects))
+		goto nobufs_unlock;
+	object = hlist_entry(cookie->backing_objects.first,
+			     struct fscache_object, cookie_link);
+
+	if (fscache_submit_op(object, &op->op) < 0)
+		goto nobufs_unlock;
+	spin_unlock(&cookie->lock);
+
+	fscache_stat(&fscache_n_retrieval_ops);
+
+	/* pin the netfs read context in case we need to do the actual netfs
+	 * read because we've encountered a cache read failure */
+	fscache_get_context(object->cookie, op->context);
+
+	/* we wait for the operation to become active, and then process it
+	 * *here*, in this thread, and not in the thread pool */
+	if (test_bit(FSCACHE_OP_WAITING, &op->op.flags)) {
+		_debug(">>> WT");
+		fscache_stat(&fscache_n_retrieval_op_waits);
+		wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+			    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+		_debug("<<< GO");
+	}
+
+	/* ask the cache to honour the operation */
+	if (test_bit(FSCACHE_COOKIE_NO_DATA_YET, &object->cookie->flags))
+		func = object->cache->ops->allocate_pages;
+	else
+		func = object->cache->ops->read_or_alloc_pages;
+	ret = func(op, pages, nr_pages, gfp);
+
+	if (ret == -ENOMEM)
+		fscache_stat(&fscache_n_retrievals_nomem);
+	else if (ret == -ERESTARTSYS)
+		fscache_stat(&fscache_n_retrievals_intr);
+	else if (ret == -ENODATA)
+		fscache_stat(&fscache_n_retrievals_nodata);
+	else if (ret < 0)
+		fscache_stat(&fscache_n_retrievals_nobufs);
+	else
+		fscache_stat(&fscache_n_retrievals_ok);
+
+	fscache_put_retrieval(op);
+	_leave(" = %d", ret);
+	return ret;
+
+nobufs_unlock:
+	spin_unlock(&cookie->lock);
+	kfree(op);
+nobufs:
+	fscache_stat(&fscache_n_retrievals_nobufs);
+	_leave(" = -ENOBUFS");
+	return -ENOBUFS;
+}
+EXPORT_SYMBOL(__fscache_read_or_alloc_pages);
+
+/*
+ * allocate a block in the cache on which to store a page
+ * - we return:
+ *   -ENOMEM	- out of memory, nothing done
+ *   -ERESTARTSYS - interrupted
+ *   -ENOBUFS	- no backing object available in which to cache the block
+ *   0		- block allocated
+ */
+int __fscache_alloc_page(struct fscache_cookie *cookie,
+			 struct page *page,
+			 gfp_t gfp)
+{
+	struct fscache_retrieval *op;
+	struct fscache_object *object;
+	int ret;
+
+	_enter("%p,%p,,,", cookie, page);
+
+	fscache_stat(&fscache_n_allocs);
+
+	if (hlist_empty(&cookie->backing_objects))
+		goto nobufs;
+
+	ASSERTCMP(cookie->def->type, !=, FSCACHE_COOKIE_TYPE_INDEX);
+	ASSERTCMP(page, !=, NULL);
+
+	if (fscache_wait_for_deferred_lookup(cookie) < 0)
+		return -ERESTARTSYS;
+
+	op = fscache_alloc_retrieval(page->mapping, NULL, NULL);
+	if (!op)
+		return -ENOMEM;
+
+	spin_lock(&cookie->lock);
+
+	if (hlist_empty(&cookie->backing_objects))
+		goto nobufs_unlock;
+	object = hlist_entry(cookie->backing_objects.first,
+			     struct fscache_object, cookie_link);
+
+	if (fscache_submit_op(object, &op->op) < 0)
+		goto nobufs_unlock;
+	spin_unlock(&cookie->lock);
+
+	fscache_stat(&fscache_n_alloc_ops);
+
+	if (test_bit(FSCACHE_OP_WAITING, &op->op.flags)) {
+		_debug(">>> WT");
+		fscache_stat(&fscache_n_alloc_op_waits);
+		wait_on_bit(&op->op.flags, FSCACHE_OP_WAITING,
+			    fscache_wait_bit, TASK_UNINTERRUPTIBLE);
+		_debug("<<< GO");
+	}
+
+	/* ask the cache to honour the operation */
+	ret = object->cache->ops->allocate_page(op, page, gfp);
+
+	if (ret < 0)
+		fscache_stat(&fscache_n_allocs_nobufs);
+	else
+		fscache_stat(&fscache_n_allocs_ok);
+
+	fscache_put_retrieval(op);
+	_leave(" = %d", ret);
+	return ret;
+
+nobufs_unlock:
+	spin_unlock(&cookie->lock);
+	kfree(op);
+nobufs:
+	fscache_stat(&fscache_n_allocs_nobufs);
+	_leave(" = -ENOBUFS");
+	return -ENOBUFS;
+}
+EXPORT_SYMBOL(__fscache_alloc_page);
+
+/*
+ * release a write op reference
+ */
+static void fscache_release_write_op(struct fscache_operation *_op)
+{
+	_enter("{OP%x}", _op->debug_id);
+}
+
+/*
+ * perform the background storage of a page into the cache
+ */
+static void fscache_write_op(struct fscache_operation *_op)
+{
+	struct fscache_storage *op =
+		container_of(_op, struct fscache_storage, op);
+	struct fscache_object *object = op->op.object;
+	struct fscache_cookie *cookie = object->cookie;
+	struct page *page;
+	unsigned n;
+	void *results[1];
+	int ret;
+
+	_enter("{OP%x,%d}", op->op.debug_id, atomic_read(&op->op.usage));
+
+	spin_lock(&cookie->lock);
+	spin_lock(&object->lock);
+
+	if (!fscache_object_is_active(object)) {
+		spin_unlock(&object->lock);
+		spin_unlock(&cookie->lock);
+		_leave("");
+		return;
+	}
+
+	fscache_stat(&fscache_n_store_calls);
+
+	/* find a page to store */
+	page = NULL;
+	n = radix_tree_gang_lookup_tag(&cookie->stores, results, 0, 1,
+				       FSCACHE_COOKIE_PENDING_TAG);
+	if (n != 1)
+		goto superseded;
+	page = results[0];
+	_debug("gang %d [%lx]", n, page->index);
+	if (page->index > op->store_limit)
+		goto superseded;
+
+	radix_tree_tag_clear(&cookie->stores, page->index,
+			     FSCACHE_COOKIE_PENDING_TAG);
+
+	spin_unlock(&object->lock);
+	spin_unlock(&cookie->lock);
+
+	if (page) {
+		ret = object->cache->ops->write_page(op, page);
+		fscache_end_page_write(cookie, page);
+		page_cache_release(page);
+		if (ret < 0)
+			fscache_abort_object(object);
+		else
+			fscache_enqueue_operation(&op->op);
+	}
+
+	_leave("");
+	return;
+
+superseded:
+	/* this writer is going away and there aren't any more things to
+	 * write */
+	_debug("cease");
+	clear_bit(FSCACHE_OBJECT_PENDING_WRITE, &object->flags);
+	spin_unlock(&object->lock);
+	spin_unlock(&cookie->lock);
+	_leave("");
+}
+
+/*
+ * request a page be stored in the cache
+ * - returns:
+ *   -ENOMEM	- out of memory, nothing done
+ *   -ENOBUFS	- no backing object available in which to cache the page
+ *   0		- dispatched a write - it'll call end_io_func() when finished
+ *
+ * if the cookie still has a backing object at this point, that object can be
+ * in one of a few states with respect to storage processing:
+ *
+ *  (1) negative lookup, object not yet created (FSCACHE_COOKIE_CREATING is
+ *      set)
+ *
+ *	(a) no writes yet (set FSCACHE_COOKIE_PENDING_FILL and queue deferred
+ *	    fill op)
+ *
+ *	(b) writes deferred till post-creation (mark page for writing and
+ *	    return immediately)
+ *
+ *  (2) negative lookup, object created, initial fill being made from netfs
+ *      (FSCACHE_COOKIE_INITIAL_FILL is set)
+ *
+ *	(a) fill point not yet reached this page (mark page for writing and
+ *          return)
+ *
+ *	(b) fill point passed this page (queue op to store this page)
+ *
+ *  (3) object extant (queue op to store this page)
+ *
+ * any other state is invalid
+ */
+int __fscache_write_page(struct fscache_cookie *cookie,
+			 struct page *page,
+			 gfp_t gfp)
+{
+	struct fscache_storage *op;
+	struct fscache_object *object;
+	int ret;
+
+	_enter("%p,%x,", cookie, (u32) page->flags);
+
+	ASSERTCMP(cookie->def->type, !=, FSCACHE_COOKIE_TYPE_INDEX);
+	ASSERT(PageFsCache(page));
+
+	fscache_stat(&fscache_n_stores);
+
+	op = kzalloc(sizeof(*op), GFP_NOIO);
+	if (!op)
+		goto nomem;
+
+	fscache_operation_init(&op->op, fscache_release_write_op);
+	fscache_operation_init_slow(&op->op, fscache_write_op);
+	op->op.flags = FSCACHE_OP_SLOW | (1 << FSCACHE_OP_WAITING);
+
+	ret = radix_tree_preload(gfp & ~__GFP_HIGHMEM);
+	if (ret < 0)
+		goto nomem_free;
+
+	ret = -ENOBUFS;
+	spin_lock(&cookie->lock);
+
+	if (hlist_empty(&cookie->backing_objects))
+		goto nobufs;
+	object = hlist_entry(cookie->backing_objects.first,
+			     struct fscache_object, cookie_link);
+	if (test_bit(FSCACHE_IOERROR, &object->cache->flags))
+		goto nobufs;
+
+	/* add the page to the pending-storage radix tree on the backing
+	 * object */
+	spin_lock(&object->lock);
+
+	_debug("store limit %llx", (unsigned long long) object->store_limit);
+
+	ret = radix_tree_insert(&cookie->stores, page->index, page);
+	if (ret < 0) {
+		if (ret == -EEXIST)
+			goto already_queued;
+		_debug("insert failed %d", ret);
+		goto nobufs_unlock_obj;
+	}
+
+	radix_tree_tag_set(&cookie->stores, page->index,
+			   FSCACHE_COOKIE_PENDING_TAG);
+	page_cache_get(page);
+
+	/* we only want one writer at a time, but we do need to queue new
+	 * writers after exclusive ops */
+	if (test_and_set_bit(FSCACHE_OBJECT_PENDING_WRITE, &object->flags))
+		goto already_pending;
+
+	spin_unlock(&object->lock);
+
+	op->op.debug_id	= atomic_inc_return(&fscache_op_debug_id);
+	op->store_limit = object->store_limit;
+
+	if (fscache_submit_op(object, &op->op) < 0)
+		goto submit_failed;
+
+	spin_unlock(&cookie->lock);
+	radix_tree_preload_end();
+	fscache_stat(&fscache_n_store_ops);
+	fscache_stat(&fscache_n_stores_ok);
+
+	/* the slow work queue now carries its own ref on the object */
+	fscache_put_operation(&op->op);
+	_leave(" = 0");
+	return 0;
+
+already_queued:
+	fscache_stat(&fscache_n_stores_again);
+already_pending:
+	spin_unlock(&object->lock);
+	spin_unlock(&cookie->lock);
+	radix_tree_preload_end();
+	kfree(op);
+	fscache_stat(&fscache_n_stores_ok);
+	_leave(" = 0");
+	return 0;
+
+submit_failed:
+	radix_tree_delete(&cookie->stores, page->index);
+	page_cache_release(page);
+	ret = -ENOBUFS;
+	goto nobufs;
+
+nobufs_unlock_obj:
+	spin_unlock(&object->lock);
+nobufs:
+	spin_unlock(&cookie->lock);
+	radix_tree_preload_end();
+	kfree(op);
+	fscache_stat(&fscache_n_stores_nobufs);
+	_leave(" = -ENOBUFS");
+	return -ENOBUFS;
+
+nomem_free:
+	kfree(op);
+nomem:
+	fscache_stat(&fscache_n_stores_oom);
+	_leave(" = -ENOMEM");
+	return -ENOMEM;
+}
+EXPORT_SYMBOL(__fscache_write_page);
+
+/*
+ * remove a page from the cache
+ */
+void __fscache_uncache_page(struct fscache_cookie *cookie, struct page *page)
+{
+	struct fscache_object *object;
+
+	_enter(",%p", page);
+
+	ASSERTCMP(cookie->def->type, !=, FSCACHE_COOKIE_TYPE_INDEX);
+	ASSERTCMP(page, !=, NULL);
+
+	fscache_stat(&fscache_n_uncaches);
+
+	/* cache withdrawal may beat us to it */
+	if (!PageFsCache(page))
+		goto done;
+
+	/* get the object */
+	spin_lock(&cookie->lock);
+
+	if (hlist_empty(&cookie->backing_objects)) {
+		ClearPageFsCache(page);
+		goto done_unlock;
+	}
+
+	object = hlist_entry(cookie->backing_objects.first,
+			     struct fscache_object, cookie_link);
+
+	/* there might now be stuff on disk we could read */
+	clear_bit(FSCACHE_COOKIE_NO_DATA_YET, &cookie->flags);
+
+	/* only invoke the cache backend if we managed to mark the page
+	 * uncached here; this deals with synchronisation vs withdrawal */
+	if (TestClearPageFsCache(page) &&
+	    object->cache->ops->uncache_page) {
+		/* the cache backend releases the cookie lock */
+		object->cache->ops->uncache_page(object, page);
+		goto done;
+	}
+
+done_unlock:
+	spin_unlock(&cookie->lock);
+done:
+	_leave("");
+}
+EXPORT_SYMBOL(__fscache_uncache_page);
+
+/**
+ * fscache_mark_pages_cached - Mark pages as being cached
+ * @op: The retrieval op pages are being marked for
+ * @pagevec: The pages to be marked
+ *
+ * Mark a bunch of netfs pages as being cached.  After this is called,
+ * the netfs must call fscache_uncache_page() to remove the mark.
+ */
+void fscache_mark_pages_cached(struct fscache_retrieval *op,
+			       struct pagevec *pagevec)
+{
+	struct fscache_cookie *cookie = op->op.object->cookie;
+	unsigned long loop;
+
+#ifdef CONFIG_FSCACHE_STATS
+	atomic_add(pagevec->nr, &fscache_n_marks);
+#endif
+
+	for (loop = 0; loop < pagevec->nr; loop++) {
+		struct page *page = pagevec->pages[loop];
+
+		_debug("- mark %p{%lx}", page, page->index);
+		if (TestSetPageFsCache(page)) {
+			static bool once_only;
+			if (!once_only) {
+				once_only = true;
+				printk(KERN_WARNING "FS-Cache:"
+				       " Cookie type %s marked page %lx"
+				       " multiple times\n",
+				       cookie->def->name, page->index);
+			}
+		}
+	}
+
+	if (cookie->def->mark_pages_cached)
+		cookie->def->mark_pages_cached(cookie->netfs_data,
+					       op->mapping, pagevec);
+	pagevec_reinit(pagevec);
+}
+EXPORT_SYMBOL(fscache_mark_pages_cached);
