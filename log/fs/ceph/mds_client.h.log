commit e64f44a884657358812e6c057957be546db03cbe
Author: Xiubo Li <xiubli@redhat.com>
Date:   Wed May 27 09:09:27 2020 -0400

    ceph: skip checking caps when session reconnecting and releasing reqs
    
    It make no sense to check the caps when reconnecting to mds. And
    for the async dirop caps, they will be put by its _cb() function,
    so when releasing the requests, it will make no sense too.
    
    URL: https://tracker.ceph.com/issues/45635
    Signed-off-by: Xiubo Li <xiubli@redhat.com>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 43111e408fa2..5e0c4073a6be 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -507,6 +507,7 @@ extern int ceph_mdsc_do_request(struct ceph_mds_client *mdsc,
 				struct inode *dir,
 				struct ceph_mds_request *req);
 extern void ceph_mdsc_release_dir_caps(struct ceph_mds_request *req);
+extern void ceph_mdsc_release_dir_caps_no_check(struct ceph_mds_request *req);
 static inline void ceph_mdsc_get_request(struct ceph_mds_request *req)
 {
 	kref_get(&req->r_kref);

commit 829ad4db952aac86d11a62647d2516ab46c2fcd2
Author: Jeff Layton <jlayton@kernel.org>
Date:   Fri Apr 3 13:09:07 2020 -0400

    ceph: ceph_kick_flushing_caps needs the s_mutex
    
    The mdsc->cap_dirty_lock is not held while walking the list in
    ceph_kick_flushing_caps, which is not safe.
    
    ceph_early_kick_flushing_caps does something similar, but the
    s_mutex is held while it's called and I think that guards against
    changes to the list.
    
    Ensure we hold the s_mutex when calling ceph_kick_flushing_caps,
    and add some clarifying comments.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 788182adcc51..43111e408fa2 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -199,8 +199,10 @@ struct ceph_mds_session {
 	struct list_head  s_cap_releases; /* waiting cap_release messages */
 	struct work_struct s_cap_release_work;
 
-	/* both protected by s_mdsc->cap_dirty_lock */
+	/* See ceph_inode_info->i_dirty_item. */
 	struct list_head  s_cap_dirty;	      /* inodes w/ dirty caps */
+
+	/* See ceph_inode_info->i_flushing_item. */
 	struct list_head  s_cap_flushing;     /* inodes w/ flushing caps */
 
 	unsigned long     s_renew_requested; /* last time we sent a renew req */

commit 1cf03a68e791b1673bc4daaa88a0820f34f538f8
Author: Jeff Layton <jlayton@kernel.org>
Date:   Wed Apr 1 17:07:52 2020 -0400

    ceph: convert mdsc->cap_dirty to a per-session list
    
    This is a per-sb list now, but that makes it difficult to tell when
    the cap is the last dirty one associated with the session. Switch
    this to be a per-session list, but continue using the
    mdsc->cap_dirty_lock to protect the lists.
    
    This list is only ever walked in ceph_flush_dirty_caps, so change that
    to walk the sessions array and then flush the caps for inodes on each
    session's list.
    
    If the auth cap ever changes while the inode has dirty caps, then
    move the inode to the appropriate session for the new auth_cap. Also,
    ensure that we never remove an auth cap while the inode is still on the
    s_cap_dirty list.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index cceeb33f2ff9..788182adcc51 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -199,8 +199,10 @@ struct ceph_mds_session {
 	struct list_head  s_cap_releases; /* waiting cap_release messages */
 	struct work_struct s_cap_release_work;
 
-	/* protected by mutex */
+	/* both protected by s_mdsc->cap_dirty_lock */
+	struct list_head  s_cap_dirty;	      /* inodes w/ dirty caps */
 	struct list_head  s_cap_flushing;     /* inodes w/ flushing caps */
+
 	unsigned long     s_renew_requested; /* last time we sent a renew req */
 	u64               s_renew_seq;
 
@@ -424,7 +426,6 @@ struct ceph_mds_client {
 
 	u64               last_cap_flush_tid;
 	struct list_head  cap_flush_list;
-	struct list_head  cap_dirty;        /* inodes with dirty caps */
 	struct list_head  cap_dirty_migrating; /* ...that are migration... */
 	int               num_cap_flushing; /* # caps we are flushing */
 	spinlock_t        cap_dirty_lock;   /* protects above items */

commit 70c948206f0616c7e46130a26165b6a5d98bade4
Author: Xiubo Li <xiubli@redhat.com>
Date:   Thu Mar 19 23:45:02 2020 -0400

    ceph: add metadata perf metric support
    
    Add a new "r_ended" field to struct ceph_mds_request and use that to
    maintain the average latency of MDS requests.
    
    URL: https://tracker.ceph.com/issues/43215
    Signed-off-by: Xiubo Li <xiubli@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@kernel.org>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 605995ae3718..cceeb33f2ff9 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -10,6 +10,7 @@
 #include <linux/spinlock.h>
 #include <linux/refcount.h>
 #include <linux/utsname.h>
+#include <linux/ktime.h>
 
 #include <linux/ceph/types.h>
 #include <linux/ceph/messenger.h>
@@ -299,6 +300,8 @@ struct ceph_mds_request {
 
 	unsigned long r_timeout;  /* optional.  jiffies, 0 is "wait forever" */
 	unsigned long r_started;  /* start time to measure timeout against */
+	unsigned long r_start_latency;  /* start time to measure latency */
+	unsigned long r_end_latency;    /* finish time to measure latency */
 	unsigned long r_request_started; /* start time for mds request only,
 					    used to measure lease durations */
 

commit f9009efac49c830460f55b9f6c08ee0d76f31b0d
Author: Xiubo Li <xiubli@redhat.com>
Date:   Thu Mar 19 23:44:59 2020 -0400

    ceph: add dentry lease metric support
    
    For dentry leases, only count the hit/miss info triggered from the vfs
    calls. For the cases like request reply handling and ceph_trim_dentries,
    ignore them.
    
    For now, these are only viewable using debugfs. Future patches will
    allow the client to send the stats to the MDS.
    
    The output looks like:
    
    item          total           miss            hit
    -------------------------------------------------
    d_lease       11              7               141
    
    URL: https://tracker.ceph.com/issues/43215
    Signed-off-by: Xiubo Li <xiubli@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@kernel.org>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 903d9edfd4bf..605995ae3718 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -16,6 +16,8 @@
 #include <linux/ceph/mdsmap.h>
 #include <linux/ceph/auth.h>
 
+#include "metric.h"
+
 /* The first 8 bits are reserved for old ceph releases */
 enum ceph_feature_type {
 	CEPHFS_FEATURE_MIMIC = 8,
@@ -454,6 +456,8 @@ struct ceph_mds_client {
 	struct list_head  dentry_leases;     /* fifo list */
 	struct list_head  dentry_dir_leases; /* lru list */
 
+	struct ceph_client_metric metric;
+
 	spinlock_t		snapid_map_lock;
 	struct rb_root		snapid_map_tree;
 	struct list_head	snapid_map_lru;

commit 2a575f138d003fff0f4930b5cfae4a1c46343b8f
Author: Jeff Layton <jlayton@kernel.org>
Date:   Wed Apr 8 08:41:38 2020 -0400

    ceph: fix potential bad pointer deref in async dirops cb's
    
    The new async dirops callback routines can pass ERR_PTR values to
    ceph_mdsc_free_path, which could cause an oops. Make ceph_mdsc_free_path
    ignore ERR_PTR values. Also, ensure that the pr_warn messages look sane
    even if ceph_mdsc_build_path fails.
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: Ilya Dryomov <idryomov@gmail.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 4e5be79bf080..903d9edfd4bf 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -521,7 +521,7 @@ extern void ceph_mdsc_pre_umount(struct ceph_mds_client *mdsc);
 
 static inline void ceph_mdsc_free_path(char *path, int len)
 {
-	if (path)
+	if (!IS_ERR_OR_NULL(path))
 		__putname(path - (PATH_MAX - 1 - len));
 }
 

commit 6deb8008a8e64eec3b1e06cc4286905ee6b14e42
Author: Jeff Layton <jlayton@kernel.org>
Date:   Mon Jan 13 13:04:08 2020 -0500

    ceph: add new MDS req field to hold delegated inode number
    
    Add new request field to hold the delegated inode number. Encode that
    into the message when it's set.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 4c3b71707470..4e5be79bf080 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -313,6 +313,7 @@ struct ceph_mds_request {
 	int               r_num_fwd;    /* number of forward attempts */
 	int               r_resend_mds; /* mds to resend to next, if any*/
 	u32               r_sent_on_mseq; /* cap mseq request was sent at*/
+	u64		  r_deleg_ino;
 
 	struct list_head  r_wait;
 	struct completion r_completion;

commit d4846487870897a5a149a3220c95bfd5728f9247
Author: Jeff Layton <jlayton@kernel.org>
Date:   Fri Nov 15 11:51:55 2019 -0500

    ceph: decode interval_sets for delegated inos
    
    Starting in Octopus, the MDS will hand out caps that allow the client
    to do asynchronous file creates under certain conditions. As part of
    that, the MDS will delegate ranges of inode numbers to the client.
    
    Add the infrastructure to decode these ranges, and stuff them into an
    xarray for later consumption by the async creation code.
    
    Because the xarray code currently only handles unsigned long indexes,
    and those are 32-bits on 32-bit arches, we only enable the decoding when
    running on a 64-bit arch.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index f10d342ea585..4c3b71707470 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -23,8 +23,9 @@ enum ceph_feature_type {
 	CEPHFS_FEATURE_RECLAIM_CLIENT,
 	CEPHFS_FEATURE_LAZY_CAP_WANTED,
 	CEPHFS_FEATURE_MULTI_RECONNECT,
+	CEPHFS_FEATURE_DELEG_INO,
 
-	CEPHFS_FEATURE_MAX = CEPHFS_FEATURE_MULTI_RECONNECT,
+	CEPHFS_FEATURE_MAX = CEPHFS_FEATURE_DELEG_INO,
 };
 
 /*
@@ -37,6 +38,7 @@ enum ceph_feature_type {
 	CEPHFS_FEATURE_REPLY_ENCODING,		\
 	CEPHFS_FEATURE_LAZY_CAP_WANTED,		\
 	CEPHFS_FEATURE_MULTI_RECONNECT,		\
+	CEPHFS_FEATURE_DELEG_INO,		\
 						\
 	CEPHFS_FEATURE_MAX,			\
 }
@@ -201,6 +203,7 @@ struct ceph_mds_session {
 
 	struct list_head  s_waiting;  /* waiting requests */
 	struct list_head  s_unsafe;   /* unsafe requests */
+	struct xarray	  s_delegated_inos;
 };
 
 /*
@@ -542,6 +545,7 @@ extern void ceph_mdsc_open_export_target_sessions(struct ceph_mds_client *mdsc,
 extern int ceph_trim_caps(struct ceph_mds_client *mdsc,
 			  struct ceph_mds_session *session,
 			  int max_caps);
+
 static inline int ceph_wait_on_async_create(struct inode *inode)
 {
 	struct ceph_inode_info *ci = ceph_inode(inode);
@@ -549,4 +553,7 @@ static inline int ceph_wait_on_async_create(struct inode *inode)
 	return wait_on_bit(&ci->i_ceph_flags, CEPH_ASYNC_CREATE_BIT,
 			   TASK_INTERRUPTIBLE);
 }
+
+extern u64 ceph_get_deleg_ino(struct ceph_mds_session *session);
+extern int ceph_restore_deleg_ino(struct ceph_mds_session *session, u64 ino);
 #endif

commit a25949b99003b7e6c2604a3fc8b8d62385508477
Author: Jeff Layton <jlayton@kernel.org>
Date:   Tue Feb 18 14:12:45 2020 -0500

    ceph: cap tracking for async directory operations
    
    Track and correctly handle directory caps for asynchronous operations.
    Add aliases for Frc caps that we now designate at Dcu caps (when dealing
    with directories).
    
    Unlike file caps, we don't reclaim these when the session goes away, and
    instead preemptively release them. In-flight async dirops are instead
    handled during reconnect phase. The client needs to re-do a synchronous
    operation in order to re-get directory caps.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 8043f2b439b1..f10d342ea585 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -284,8 +284,11 @@ struct ceph_mds_request {
 	struct ceph_msg  *r_request;  /* original request */
 	struct ceph_msg  *r_reply;
 	struct ceph_mds_reply_info_parsed r_reply_info;
-	struct page *r_locked_page;
 	int r_err;
+
+
+	struct page *r_locked_page;
+	int r_dir_caps;
 	int r_num_caps;
 	u32               r_readdir_offset;
 
@@ -489,6 +492,7 @@ extern int ceph_mdsc_submit_request(struct ceph_mds_client *mdsc,
 extern int ceph_mdsc_do_request(struct ceph_mds_client *mdsc,
 				struct inode *dir,
 				struct ceph_mds_request *req);
+extern void ceph_mdsc_release_dir_caps(struct ceph_mds_request *req);
 static inline void ceph_mdsc_get_request(struct ceph_mds_request *req)
 {
 	kref_get(&req->r_kref);

commit 891f3f5a6a0615a2ed93cc495b54d1a8121d0968
Author: Jeff Layton <jlayton@kernel.org>
Date:   Tue Jan 14 15:06:40 2020 -0500

    ceph: add infrastructure for waiting for async create to complete
    
    When we issue an async create, we must ensure that any later on-the-wire
    requests involving it wait for the create reply.
    
    Expand i_ceph_flags to be an unsigned long, and add a new bit that
    MDS requests can wait on. If the bit is set in the inode when sending
    caps, then don't send it and just return that it has been delayed.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 95ac00e59e66..8043f2b439b1 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -538,4 +538,11 @@ extern void ceph_mdsc_open_export_target_sessions(struct ceph_mds_client *mdsc,
 extern int ceph_trim_caps(struct ceph_mds_client *mdsc,
 			  struct ceph_mds_session *session,
 			  int max_caps);
+static inline int ceph_wait_on_async_create(struct inode *inode)
+{
+	struct ceph_inode_info *ci = ceph_inode(inode);
+
+	return wait_on_bit(&ci->i_ceph_flags, CEPH_ASYNC_CREATE_BIT,
+			   TASK_INTERRUPTIBLE);
+}
 #endif

commit 3bb48b4142bbf72045af5ebe72e65ccff6d02680
Author: Jeff Layton <jlayton@kernel.org>
Date:   Mon Dec 2 13:47:57 2019 -0500

    ceph: add flag to designate that a request is asynchronous
    
    ...and ensure that such requests are never queued. The MDS has need to
    know that a request is asynchronous so add flags and proper
    infrastructure for that.
    
    Also, delegated inode numbers and directory caps are associated with the
    session, so ensure that async requests are always transmitted on the
    first attempt and are never queued to wait for session reestablishment.
    
    If it does end up looking like we'll need to queue the request, then
    have it return -EJUKEBOX so the caller can reattempt with a synchronous
    request.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index a0918d00117c..95ac00e59e66 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -255,6 +255,7 @@ struct ceph_mds_request {
 #define CEPH_MDS_R_GOT_RESULT		(5) /* got a result */
 #define CEPH_MDS_R_DID_PREPOPULATE	(6) /* prepopulated readdir */
 #define CEPH_MDS_R_PARENT_LOCKED	(7) /* is r_parent->i_rwsem wlocked? */
+#define CEPH_MDS_R_ASYNC		(8) /* async request */
 	unsigned long	r_req_flags;
 
 	struct mutex r_fill_mutex;

commit c36d641493c9d170adf895b7650c26c944d37636
Author: Jeff Layton <jlayton@kernel.org>
Date:   Mon Feb 17 10:19:14 2020 -0500

    ceph: reorganize fields in ceph_mds_request
    
    This shrinks the struct size by 16 bytes.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 27a7446e10d3..a0918d00117c 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -263,6 +263,7 @@ struct ceph_mds_request {
 	int r_fmode;        /* file mode, if expecting cap */
 	kuid_t r_uid;
 	kgid_t r_gid;
+	int r_request_release_offset;
 	struct timespec64 r_stamp;
 
 	/* for choosing which mds to send this request to */
@@ -280,11 +281,12 @@ struct ceph_mds_request {
 	int r_old_inode_drop, r_old_inode_unless;
 
 	struct ceph_msg  *r_request;  /* original request */
-	int r_request_release_offset;
 	struct ceph_msg  *r_reply;
 	struct ceph_mds_reply_info_parsed r_reply_info;
 	struct page *r_locked_page;
 	int r_err;
+	int r_num_caps;
+	u32               r_readdir_offset;
 
 	unsigned long r_timeout;  /* optional.  jiffies, 0 is "wait forever" */
 	unsigned long r_started;  /* start time to measure timeout against */
@@ -315,10 +317,8 @@ struct ceph_mds_request {
 	long long	  r_dir_release_cnt;
 	long long	  r_dir_ordered_cnt;
 	int		  r_readdir_cache_idx;
-	u32               r_readdir_offset;
 
 	struct ceph_cap_reservation r_caps_reservation;
-	int r_num_caps;
 };
 
 struct ceph_pool_perm {

commit 045100cd79f503487b95a1d11e96b221fe50693c
Author: Jeff Layton <jlayton@kernel.org>
Date:   Fri Nov 15 09:13:59 2019 -0500

    ceph: close holes in structs ceph_mds_session and ceph_mds_request
    
    Move s_ref up to plug a 4 byte hole, which plugs another.
    Move r_kref to shave 8 bytes off per request on x86_64.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index c950f8f88f58..27a7446e10d3 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -184,6 +184,7 @@ struct ceph_mds_session {
 
 	/* protected by s_cap_lock */
 	spinlock_t        s_cap_lock;
+	refcount_t        s_ref;
 	struct list_head  s_caps;     /* all caps issued by this session */
 	struct ceph_cap  *s_cap_iterator;
 	int               s_nr_caps;
@@ -198,7 +199,6 @@ struct ceph_mds_session {
 	unsigned long     s_renew_requested; /* last time we sent a renew req */
 	u64               s_renew_seq;
 
-	refcount_t        s_ref;
 	struct list_head  s_waiting;  /* waiting requests */
 	struct list_head  s_unsafe;   /* unsafe requests */
 };
@@ -234,6 +234,7 @@ struct ceph_mds_request {
 	struct rb_node r_node;
 	struct ceph_mds_client *r_mdsc;
 
+	struct kref       r_kref;
 	int r_op;                    /* mds op code */
 
 	/* operation on what? */
@@ -304,7 +305,6 @@ struct ceph_mds_request {
 	int               r_resend_mds; /* mds to resend to next, if any*/
 	u32               r_sent_on_mseq; /* cap mseq request was sent at*/
 
-	struct kref       r_kref;
 	struct list_head  r_wait;
 	struct completion r_completion;
 	struct completion r_safe_completion;

commit 9ba1e224538a021b989302bb2777abc7a3b3ec79
Author: Xiubo Li <xiubli@redhat.com>
Date:   Wed Jan 8 05:17:31 2020 -0500

    ceph: allocate the correct amount of extra bytes for the session features
    
    The total bytes may potentially be larger than 8.
    
    Signed-off-by: Xiubo Li <xiubli@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@kernel.org>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index c021df5f50ce..c950f8f88f58 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -17,22 +17,31 @@
 #include <linux/ceph/auth.h>
 
 /* The first 8 bits are reserved for old ceph releases */
-#define CEPHFS_FEATURE_MIMIC		8
-#define CEPHFS_FEATURE_REPLY_ENCODING	9
-#define CEPHFS_FEATURE_RECLAIM_CLIENT	10
-#define CEPHFS_FEATURE_LAZY_CAP_WANTED	11
-#define CEPHFS_FEATURE_MULTI_RECONNECT  12
+enum ceph_feature_type {
+	CEPHFS_FEATURE_MIMIC = 8,
+	CEPHFS_FEATURE_REPLY_ENCODING,
+	CEPHFS_FEATURE_RECLAIM_CLIENT,
+	CEPHFS_FEATURE_LAZY_CAP_WANTED,
+	CEPHFS_FEATURE_MULTI_RECONNECT,
+
+	CEPHFS_FEATURE_MAX = CEPHFS_FEATURE_MULTI_RECONNECT,
+};
 
-#define CEPHFS_FEATURES_CLIENT_SUPPORTED { 	\
+/*
+ * This will always have the highest feature bit value
+ * as the last element of the array.
+ */
+#define CEPHFS_FEATURES_CLIENT_SUPPORTED {	\
 	0, 1, 2, 3, 4, 5, 6, 7,			\
 	CEPHFS_FEATURE_MIMIC,			\
 	CEPHFS_FEATURE_REPLY_ENCODING,		\
 	CEPHFS_FEATURE_LAZY_CAP_WANTED,		\
 	CEPHFS_FEATURE_MULTI_RECONNECT,		\
+						\
+	CEPHFS_FEATURE_MAX,			\
 }
 #define CEPHFS_FEATURES_CLIENT_REQUIRED {}
 
-
 /*
  * Some lock dependencies:
  *

commit 5b3248c6772459a0737afe0c85bb45ee3ba79eeb
Author: Xiubo Li <xiubli@redhat.com>
Date:   Thu Dec 19 19:44:09 2019 -0500

    ceph: rename get_session and switch to use ceph_get_mds_session
    
    Just in case the session's refcount reach 0 and is releasing, and
    if we get the session without checking it, we may encounter kernel
    crash.
    
    Rename get_session to ceph_get_mds_session and make it global.
    
    Signed-off-by: Xiubo Li <xiubli@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@kernel.org>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index fe085e06adf5..c021df5f50ce 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -452,15 +452,10 @@ extern const char *ceph_mds_op_name(int op);
 extern struct ceph_mds_session *
 __ceph_lookup_mds_session(struct ceph_mds_client *, int mds);
 
-static inline struct ceph_mds_session *
-ceph_get_mds_session(struct ceph_mds_session *s)
-{
-	refcount_inc(&s->s_ref);
-	return s;
-}
-
 extern const char *ceph_session_state_name(int s);
 
+extern struct ceph_mds_session *
+ceph_get_mds_session(struct ceph_mds_session *s);
 extern void ceph_put_mds_session(struct ceph_mds_session *s);
 
 extern int ceph_send_msg_mds(struct ceph_mds_client *mdsc,

commit 4d681c2f9141cf50261eef85b3233151c83d068b
Author: Xiubo Li <xiubli@redhat.com>
Date:   Thu Dec 5 22:35:51 2019 -0500

    ceph: keep the session state until it is released
    
    When reconnecting the session but if it is denied by the MDS due
    to client was in blacklist or something else, kclient will receive
    a session close reply, and we will never see the important log:
    
    "ceph:  mds%d reconnect denied"
    
    And with the confusing log:
    
    "ceph:  handle_session mds0 close 0000000085804730 state ??? seq 0"
    
    Let's keep the session state until its memories is released.
    
    Signed-off-by: Xiubo Li <xiubli@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@kernel.org>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 14c7e8c49970..fe085e06adf5 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -151,7 +151,8 @@ enum {
 	CEPH_MDS_SESSION_RESTARTING = 5,
 	CEPH_MDS_SESSION_RECONNECTING = 6,
 	CEPH_MDS_SESSION_CLOSING = 7,
-	CEPH_MDS_SESSION_REJECTED = 8,
+	CEPH_MDS_SESSION_CLOSED = 8,
+	CEPH_MDS_SESSION_REJECTED = 9,
 };
 
 struct ceph_mds_session {

commit 3a3430affce5de301fc8e6e50fa3543d7597820e
Author: Jeff Layton <jlayton@kernel.org>
Date:   Wed Nov 20 12:00:59 2019 -0500

    ceph: show tasks waiting on caps in debugfs caps file
    
    Add some visibility of tasks that are waiting for caps to the "caps"
    debugfs file. Display the tgid of the waiting task, inode number, and
    the caps the task needs and wants.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 5cd131b41d84..14c7e8c49970 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -340,6 +340,14 @@ struct ceph_quotarealm_inode {
 	struct inode *inode;
 };
 
+struct cap_wait {
+	struct list_head	list;
+	unsigned long		ino;
+	pid_t			tgid;
+	int			need;
+	int			want;
+};
+
 /*
  * mds client state
  */
@@ -416,6 +424,7 @@ struct ceph_mds_client {
 	spinlock_t	caps_list_lock;
 	struct		list_head caps_list; /* unused (reserved or
 						unreserved) */
+	struct		list_head cap_wait_list;
 	int		caps_total_count;    /* total caps allocated */
 	int		caps_use_count;      /* in use */
 	int		caps_use_max;	     /* max used caps */

commit 533a2818dd1a00cdd32d638fea0178e25a683053
Author: Jeff Layton <jlayton@kernel.org>
Date:   Fri Jul 19 15:22:28 2019 -0400

    ceph: eliminate session->s_trim_caps
    
    It's only used to keep count of caps being trimmed, but that requires
    that we hold the session->s_mutex to prevent multiple trimming
    operations from running concurrently.
    
    We can achieve the same effect using an integer on the stack, which
    allows us to (eventually) not need the s_mutex.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 810fd8689dff..5cd131b41d84 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -176,7 +176,7 @@ struct ceph_mds_session {
 	spinlock_t        s_cap_lock;
 	struct list_head  s_caps;     /* all caps issued by this session */
 	struct ceph_cap  *s_cap_iterator;
-	int               s_nr_caps, s_trim_caps;
+	int               s_nr_caps;
 	int               s_num_cap_releases;
 	int		  s_cap_reconnect;
 	int		  s_readonly;

commit 7e6906c1e670f0b40554f42ba74cc043d094877c
Author: Yan, Zheng <zyan@redhat.com>
Date:   Thu Jul 25 20:16:41 2019 +0800

    ceph: allow closing session in restarting/reconnect state
    
    CEPH_MDS_SESSION_{RESTARTING,RECONNECTING} are for for mds failover,
    they are sub-states of CEPH_MDS_SESSION_OPEN. So __close_session()
    should send close request for session in these two state.
    
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@kernel.org>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index f7c8603484fe..810fd8689dff 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -148,9 +148,9 @@ enum {
 	CEPH_MDS_SESSION_OPENING = 2,
 	CEPH_MDS_SESSION_OPEN = 3,
 	CEPH_MDS_SESSION_HUNG = 4,
-	CEPH_MDS_SESSION_CLOSING = 5,
-	CEPH_MDS_SESSION_RESTARTING = 6,
-	CEPH_MDS_SESSION_RECONNECTING = 7,
+	CEPH_MDS_SESSION_RESTARTING = 5,
+	CEPH_MDS_SESSION_RECONNECTING = 6,
+	CEPH_MDS_SESSION_CLOSING = 7,
 	CEPH_MDS_SESSION_REJECTED = 8,
 };
 

commit a35ead314e0b9252a92a5e179a00b242d1af7bff
Author: Jeff Layton <jlayton@kernel.org>
Date:   Thu Jun 6 07:29:23 2019 -0400

    ceph: add change_attr field to ceph_inode_info
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index da2f53646217..f7c8603484fe 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -71,6 +71,7 @@ struct ceph_mds_reply_info_in {
 	s32 dir_pin;
 	struct ceph_timespec btime;
 	struct ceph_timespec snap_btime;
+	u64 change_attr;
 };
 
 struct ceph_mds_reply_dir_entry {

commit 245ce991cca55eb16cfc43d1655574121b8ed85f
Author: Jeff Layton <jlayton@kernel.org>
Date:   Wed May 29 11:19:42 2019 -0400

    ceph: add btime field to ceph_inode_info
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 330769ecb601..da2f53646217 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -69,6 +69,7 @@ struct ceph_mds_reply_info_in {
 	u64 max_bytes;
 	u64 max_files;
 	s32 dir_pin;
+	struct ceph_timespec btime;
 	struct ceph_timespec snap_btime;
 };
 

commit 8f2a98ef3c1adf815ce38d5cc2f4e2a8759e98c5
Author: Yan, Zheng <zyan@redhat.com>
Date:   Thu May 23 10:45:24 2019 +0800

    ceph: ensure d_name/d_parent stability in ceph_mdsc_lease_send_msg()
    
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 9c28b86abcf4..330769ecb601 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -505,7 +505,6 @@ extern char *ceph_mdsc_build_path(struct dentry *dentry, int *plen, u64 *base,
 
 extern void __ceph_mdsc_drop_dentry_lease(struct dentry *dentry);
 extern void ceph_mdsc_lease_send_msg(struct ceph_mds_session *session,
-				     struct inode *inode,
 				     struct dentry *dentry, char action,
 				     u32 seq);
 

commit 193e7b37628e97c6e66ec26a2c062dace68b4acd
Author: David Disseldorp <ddiss@suse.de>
Date:   Thu Apr 18 14:15:46 2019 +0200

    ceph: carry snapshot creation time with inodes
    
    MDS InodeStat v3 wire structures include a trailing snapshot creation
    time member. Unmarshall this and retain it for a future vxattr.
    
    Signed-off-by: David Disseldorp <ddiss@suse.de>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index a83f28bc2387..9c28b86abcf4 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -69,6 +69,7 @@ struct ceph_mds_reply_info_in {
 	u64 max_bytes;
 	u64 max_files;
 	s32 dir_pin;
+	struct ceph_timespec snap_btime;
 };
 
 struct ceph_mds_reply_dir_entry {

commit 86bda539fa90184ca404afb38cd015416bf81d15
Author: Jeff Layton <jlayton@kernel.org>
Date:   Tue Apr 2 09:24:36 2019 -0400

    ceph: have ceph_mdsc_do_request call ceph_mdsc_submit_request
    
    Nothing calls ceph_mdsc_submit_request today, but in later patches we'll
    need to be able to call this separately.
    
    Have the helper return an int so we can check the r_err under the mutex,
    and have the caller just check the error code from the submit. Also move
    the acquisition of CEPH_CAP_PIN references into the same function.
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index ebcad5afc87b..a83f28bc2387 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -465,8 +465,9 @@ extern int ceph_alloc_readdir_reply_buffer(struct ceph_mds_request *req,
 					   struct inode *dir);
 extern struct ceph_mds_request *
 ceph_mdsc_create_request(struct ceph_mds_client *mdsc, int op, int mode);
-extern void ceph_mdsc_submit_request(struct ceph_mds_client *mdsc,
-				     struct ceph_mds_request *req);
+extern int ceph_mdsc_submit_request(struct ceph_mds_client *mdsc,
+				    struct inode *dir,
+				    struct ceph_mds_request *req);
 extern int ceph_mdsc_do_request(struct ceph_mds_client *mdsc,
 				struct inode *dir,
 				struct ceph_mds_request *req);

commit f77f21bb28367d0ac4861a24da1db118bba850e6
Author: Jeff Layton <jlayton@kernel.org>
Date:   Mon Apr 29 12:13:14 2019 -0400

    ceph: use __getname/__putname in ceph_mdsc_build_path
    
    Al suggested we get rid of the kmalloc here and just use __getname
    and __putname to get a full PATH_MAX pathname buffer.
    
    Since we build the path in reverse, we continue to return a pointer
    to the beginning of the string and the length, and add a new helper
    to free the thing at the end.
    
    Suggested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 0d1f673a5689..ebcad5afc87b 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -492,6 +492,12 @@ extern int ceph_iterate_session_caps(struct ceph_mds_session *session,
 				     void *arg);
 extern void ceph_mdsc_pre_umount(struct ceph_mds_client *mdsc);
 
+static inline void ceph_mdsc_free_path(char *path, int len)
+{
+	if (path)
+		__putname(path - (PATH_MAX - 1 - len));
+}
+
 extern char *ceph_mdsc_build_path(struct dentry *dentry, int *plen, u64 *base,
 				  int stop_on_nosnap);
 

commit f5d7726900b66e38355db878ced6b13b00fa9201
Author: Jeff Layton <jlayton@kernel.org>
Date:   Wed Apr 24 12:09:04 2019 -0400

    ceph: make iterate_session_caps a public symbol
    
    Signed-off-by: Jeff Layton <jlayton@kernel.org>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 3f0029aa8a39..0d1f673a5689 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -486,6 +486,10 @@ extern void ceph_flush_cap_releases(struct ceph_mds_client *mdsc,
 				    struct ceph_mds_session *session);
 extern void ceph_queue_cap_reclaim_work(struct ceph_mds_client *mdsc);
 extern void ceph_reclaim_caps_nr(struct ceph_mds_client *mdsc, int nr);
+extern int ceph_iterate_session_caps(struct ceph_mds_session *session,
+				     int (*cb)(struct inode *,
+					       struct ceph_cap *, void *),
+				     void *arg);
 extern void ceph_mdsc_pre_umount(struct ceph_mds_client *mdsc);
 
 extern char *ceph_mdsc_build_path(struct dentry *dentry, int *plen, u64 *base,

commit 0c44a8e0fc55f56a70f72e67d7cc5b9341dae7d1
Author: Luis Henriques <lhenriques@suse.com>
Date:   Thu Mar 21 10:20:10 2019 +0000

    ceph: quota: fix quota subdir mounts
    
    The CephFS kernel client does not enforce quotas set in a directory that
    isn't visible from the mount point.  For example, given the path
    '/dir1/dir2', if quotas are set in 'dir1' and the filesystem is mounted with
    
      mount -t ceph <server>:<port>:/dir1/ /mnt
    
    then the client won't be able to access 'dir1' inode, even if 'dir2' belongs
    to a quota realm that points to it.
    
    This patch fixes this issue by simply doing an MDS LOOKUPINO operation for
    unknown inodes.  Any inode reference obtained this way will be added to a
    list in ceph_mds_client, and will only be released when the filesystem is
    umounted.
    
    Link: https://tracker.ceph.com/issues/38482
    Reported-by: Hendrik Peyerl <hpeyerl@plusline.net>
    Signed-off-by: Luis Henriques <lhenriques@suse.com>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 50385a481fdb..3f0029aa8a39 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -325,6 +325,18 @@ struct ceph_snapid_map {
 	unsigned long last_used;
 };
 
+/*
+ * node for list of quotarealm inodes that are not visible from the filesystem
+ * mountpoint, but required to handle, e.g. quotas.
+ */
+struct ceph_quotarealm_inode {
+	struct rb_node node;
+	u64 ino;
+	unsigned long timeout; /* last time a lookup failed for this inode */
+	struct mutex mutex;
+	struct inode *inode;
+};
+
 /*
  * mds client state
  */
@@ -344,6 +356,12 @@ struct ceph_mds_client {
 	int                     stopping;      /* true if shutting down */
 
 	atomic64_t		quotarealms_count; /* # realms with quota */
+	/*
+	 * We keep a list of inodes we don't see in the mountpoint but that we
+	 * need to track quota realms.
+	 */
+	struct rb_root		quotarealms_inodes;
+	struct mutex		quotarealms_inodes_mutex;
 
 	/*
 	 * snap_rwsem will cover cap linkage into snaprealms, and

commit fe33032daae2e584d9e7e33bab44c9eafced1f8f
Author: Yan, Zheng <zyan@redhat.com>
Date:   Fri Feb 1 14:57:15 2019 +0800

    ceph: add mount option to limit caps count
    
    If number of caps exceed the limit, ceph_trim_dentires() also trim
    dentries with valid leases. Trimming dentry releases references to
    associated inode, which may evict inode and release caps.
    
    By default, there is no limit for caps count.
    
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 580b235f343b..50385a481fdb 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -379,6 +379,7 @@ struct ceph_mds_client {
 	wait_queue_head_t cap_flushing_wq;
 
 	struct work_struct cap_reclaim_work;
+	atomic_t	   cap_reclaim_pending;
 
 	/*
 	 * Cap reservations
@@ -396,6 +397,7 @@ struct ceph_mds_client {
 						unreserved) */
 	int		caps_total_count;    /* total caps allocated */
 	int		caps_use_count;      /* in use */
+	int		caps_use_max;	     /* max used caps */
 	int		caps_reserve_count;  /* unused, reserved */
 	int		caps_avail_count;    /* unused, unreserved */
 	int		caps_min_count;      /* keep at least this many
@@ -465,6 +467,7 @@ extern void __ceph_queue_cap_release(struct ceph_mds_session *session,
 extern void ceph_flush_cap_releases(struct ceph_mds_client *mdsc,
 				    struct ceph_mds_session *session);
 extern void ceph_queue_cap_reclaim_work(struct ceph_mds_client *mdsc);
+extern void ceph_reclaim_caps_nr(struct ceph_mds_client *mdsc, int nr);
 extern void ceph_mdsc_pre_umount(struct ceph_mds_client *mdsc);
 
 extern char *ceph_mdsc_build_path(struct dentry *dentry, int *plen, u64 *base,

commit 37c4efc1ddf98ba8b234d116d863a9464445901e
Author: Yan, Zheng <zyan@redhat.com>
Date:   Thu Jan 31 16:55:51 2019 +0800

    ceph: periodically trim stale dentries
    
    Previous commit make VFS delete stale dentry when last reference is
    dropped. Lease also can become invalid when corresponding dentry has
    no reference. This patch make cephfs periodically scan lease list,
    delete corresponding dentry if lease is invalid.
    
    There are two types of lease, dentry lease and dir lease. dentry lease
    has life time and applies to singe dentry. Dentry lease is added to tail
    of a list when it's updated, leases at front of the list will expire
    first. Dir lease is CEPH_CAP_FILE_SHARED on directory inode, it applies
    to all dentries in the directory. Dentries have dir leases are added to
    another list. Dentries in the list are periodically checked in a round
    robin manner.
    
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 2147ecd0c9e5..580b235f343b 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -378,6 +378,8 @@ struct ceph_mds_client {
 	spinlock_t        cap_dirty_lock;   /* protects above items */
 	wait_queue_head_t cap_flushing_wq;
 
+	struct work_struct cap_reclaim_work;
+
 	/*
 	 * Cap reservations
 	 *
@@ -398,9 +400,9 @@ struct ceph_mds_client {
 	int		caps_avail_count;    /* unused, unreserved */
 	int		caps_min_count;      /* keep at least this many
 						(unreserved) */
-	spinlock_t	  dentry_lru_lock;
-	struct list_head  dentry_lru;
-	int		  num_dentry;
+	spinlock_t	  dentry_list_lock;
+	struct list_head  dentry_leases;     /* fifo list */
+	struct list_head  dentry_dir_leases; /* lru list */
 
 	spinlock_t		snapid_map_lock;
 	struct rb_root		snapid_map_tree;
@@ -462,6 +464,7 @@ extern void __ceph_queue_cap_release(struct ceph_mds_session *session,
 				    struct ceph_cap *cap);
 extern void ceph_flush_cap_releases(struct ceph_mds_client *mdsc,
 				    struct ceph_mds_session *session);
+extern void ceph_queue_cap_reclaim_work(struct ceph_mds_client *mdsc);
 extern void ceph_mdsc_pre_umount(struct ceph_mds_client *mdsc);
 
 extern char *ceph_mdsc_build_path(struct dentry *dentry, int *plen, u64 *base,

commit e3ec8d6898f71636a067dae683174ef9bf81bc96
Author: Yan, Zheng <zyan@redhat.com>
Date:   Mon Jan 14 17:21:19 2019 +0800

    ceph: send cap releases more aggressively
    
    When pending cap releases fill up one message, start a work to send
    cap release message. (old way is sending cap releases every 5 seconds)
    
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index af3b25e59e90..2147ecd0c9e5 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -172,12 +172,13 @@ struct ceph_mds_session {
 	/* protected by s_cap_lock */
 	spinlock_t        s_cap_lock;
 	struct list_head  s_caps;     /* all caps issued by this session */
+	struct ceph_cap  *s_cap_iterator;
 	int               s_nr_caps, s_trim_caps;
 	int               s_num_cap_releases;
 	int		  s_cap_reconnect;
 	int		  s_readonly;
 	struct list_head  s_cap_releases; /* waiting cap_release messages */
-	struct ceph_cap  *s_cap_iterator;
+	struct work_struct s_cap_release_work;
 
 	/* protected by mutex */
 	struct list_head  s_cap_flushing;     /* inodes w/ flushing caps */
@@ -457,9 +458,10 @@ static inline void ceph_mdsc_put_request(struct ceph_mds_request *req)
 	kref_put(&req->r_kref, ceph_mdsc_release_request);
 }
 
-extern void ceph_send_cap_releases(struct ceph_mds_client *mdsc,
-				   struct ceph_mds_session *session);
-
+extern void __ceph_queue_cap_release(struct ceph_mds_session *session,
+				    struct ceph_cap *cap);
+extern void ceph_flush_cap_releases(struct ceph_mds_client *mdsc,
+				    struct ceph_mds_session *session);
 extern void ceph_mdsc_pre_umount(struct ceph_mds_client *mdsc);
 
 extern char *ceph_mdsc_build_path(struct dentry *dentry, int *plen, u64 *base,

commit 08796873a5183bfaab52a3bd899fe82f9e64be94
Author: Yan, Zheng <zyan@redhat.com>
Date:   Wed Jan 9 11:07:02 2019 +0800

    ceph: support getting ceph.dir.pin vxattr
    
    Link: http://tracker.ceph.com/issues/37576
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 0919aacc1af3..af3b25e59e90 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -68,6 +68,7 @@ struct ceph_mds_reply_info_in {
 	char *pool_ns_data;
 	u64 max_bytes;
 	u64 max_files;
+	s32 dir_pin;
 };
 
 struct ceph_mds_reply_dir_entry {

commit b37fe1f923fb4b17dc7d63406ec8dc67f13c2799
Author: Yan, Zheng <zyan@redhat.com>
Date:   Wed Jan 9 10:10:17 2019 +0800

    ceph: support versioned reply
    
    In versioned reply, inodestat, dirstat and lease are encoded with
    version, compat_version and struct_len.
    
    Based on a patch from Jos Collin <jcollin@redhat.com>.
    
    Link: http://tracker.ceph.com/issues/26936
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index d3a5c4046316..0919aacc1af3 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -26,6 +26,7 @@
 #define CEPHFS_FEATURES_CLIENT_SUPPORTED { 	\
 	0, 1, 2, 3, 4, 5, 6, 7,			\
 	CEPHFS_FEATURE_MIMIC,			\
+	CEPHFS_FEATURE_REPLY_ENCODING,		\
 	CEPHFS_FEATURE_LAZY_CAP_WANTED,		\
 	CEPHFS_FEATURE_MULTI_RECONNECT,		\
 }

commit 75c9627efb7288e1725e9903ea275cc6b5992f17
Author: Yan, Zheng <zyan@redhat.com>
Date:   Thu Dec 14 15:11:09 2017 +0800

    ceph: map snapid to anonymous bdev ID
    
    ceph_getattr() return zero dev ID for head inodes and set dev ID to
    snapid directly for snaphost inodes. This is not good because userspace
    utilities may consider device ID of 0 as invalid, snapid may conflict
    with other device's ID.
    
    This patch introduces "snapids to anonymous bdev IDs" map. we create a
    new mapping when we see a snapid for the first time. we trim unused
    mapping after it is ilde for 5 minutes.
    
    Link: http://tracker.ceph.com/issues/22353
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Acked-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 4f962642fee4..d3a5c4046316 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -313,6 +313,15 @@ struct ceph_pool_perm {
 	char pool_ns[];
 };
 
+struct ceph_snapid_map {
+	struct rb_node node;
+	struct list_head lru;
+	atomic_t ref;
+	u64 snap;
+	dev_t dev;
+	unsigned long last_used;
+};
+
 /*
  * mds client state
  */
@@ -390,6 +399,10 @@ struct ceph_mds_client {
 	struct list_head  dentry_lru;
 	int		  num_dentry;
 
+	spinlock_t		snapid_map_lock;
+	struct rb_root		snapid_map_tree;
+	struct list_head	snapid_map_lru;
+
 	struct rw_semaphore     pool_perm_rwsem;
 	struct rb_root		pool_perm_tree;
 

commit 81c5a1487e52a316e5e7d79e9911376648a79e85
Author: Yan, Zheng <zyan@redhat.com>
Date:   Tue Jan 1 16:28:33 2019 +0800

    ceph: split large reconnect into multiple messages
    
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 0d3264cf3334..4f962642fee4 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -21,11 +21,13 @@
 #define CEPHFS_FEATURE_REPLY_ENCODING	9
 #define CEPHFS_FEATURE_RECLAIM_CLIENT	10
 #define CEPHFS_FEATURE_LAZY_CAP_WANTED	11
+#define CEPHFS_FEATURE_MULTI_RECONNECT  12
 
 #define CEPHFS_FEATURES_CLIENT_SUPPORTED { 	\
 	0, 1, 2, 3, 4, 5, 6, 7,			\
 	CEPHFS_FEATURE_MIMIC,			\
 	CEPHFS_FEATURE_LAZY_CAP_WANTED,		\
+	CEPHFS_FEATURE_MULTI_RECONNECT,		\
 }
 #define CEPHFS_FEATURES_CLIENT_REQUIRED {}
 
@@ -342,6 +344,7 @@ struct ceph_mds_client {
 	struct rw_semaphore     snap_rwsem;
 	struct rb_root          snap_realms;
 	struct list_head        snap_empty;
+	int			num_snap_realms;
 	spinlock_t              snap_empty_lock;  /* protect snap_empty */
 
 	u64                    last_tid;      /* most recent mds request */

commit 84bf39509bea5b9f936281c4c660e75099fcd15f
Author: Yan, Zheng <zyan@redhat.com>
Date:   Fri Dec 21 17:41:39 2018 +0800

    ceph: decode feature bits in session message
    
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 729da155ebf0..0d3264cf3334 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -152,6 +152,7 @@ struct ceph_mds_session {
 	int               s_mds;
 	int               s_state;
 	unsigned long     s_ttl;      /* time until mds kills us */
+	unsigned long	  s_features;
 	u64               s_seq;      /* incoming msg seq # */
 	struct mutex      s_mutex;    /* serialize session messages */
 
@@ -179,7 +180,7 @@ struct ceph_mds_session {
 	unsigned long     s_renew_requested; /* last time we sent a renew req */
 	u64               s_renew_seq;
 
-	refcount_t          s_ref;
+	refcount_t        s_ref;
 	struct list_head  s_waiting;  /* waiting requests */
 	struct list_head  s_unsafe;   /* unsafe requests */
 };

commit d2f8bb27c87945ab696bdaea25b0465dee94fb6d
Author: Yan, Zheng <zyan@redhat.com>
Date:   Mon Dec 10 16:35:09 2018 +0800

    ceph: update wanted caps after resuming stale session
    
    mds contains an optimization, it does not re-issue stale caps if
    client does not want any cap.
    
    A special case of the optimization is that client wants some caps,
    but skipped updating 'wanted'. For this case, client needs to update
    'wanted' when stale session get renewed.
    
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 32fcce0d4d3c..729da155ebf0 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -17,14 +17,16 @@
 #include <linux/ceph/auth.h>
 
 /* The first 8 bits are reserved for old ceph releases */
-#define CEPHFS_FEATURE_MIMIC    8
-
-#define CEPHFS_FEATURES_ALL {           \
-  0, 1, 2, 3, 4, 5, 6, 7,		\
-  CEPHFS_FEATURE_MIMIC,                 \
+#define CEPHFS_FEATURE_MIMIC		8
+#define CEPHFS_FEATURE_REPLY_ENCODING	9
+#define CEPHFS_FEATURE_RECLAIM_CLIENT	10
+#define CEPHFS_FEATURE_LAZY_CAP_WANTED	11
+
+#define CEPHFS_FEATURES_CLIENT_SUPPORTED { 	\
+	0, 1, 2, 3, 4, 5, 6, 7,			\
+	CEPHFS_FEATURE_MIMIC,			\
+	CEPHFS_FEATURE_LAZY_CAP_WANTED,		\
 }
-
-#define CEPHFS_FEATURES_CLIENT_SUPPORTED CEPHFS_FEATURES_ALL
 #define CEPHFS_FEATURES_CLIENT_REQUIRED {}
 
 

commit 342ce1823ebaec573ac269b56bca78c698fec5c3
Author: Yan, Zheng <zyan@redhat.com>
Date:   Fri May 11 18:47:29 2018 +0800

    ceph: support cephfs' own feature bits
    
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 80a7523e1523..32fcce0d4d3c 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -16,6 +16,18 @@
 #include <linux/ceph/mdsmap.h>
 #include <linux/ceph/auth.h>
 
+/* The first 8 bits are reserved for old ceph releases */
+#define CEPHFS_FEATURE_MIMIC    8
+
+#define CEPHFS_FEATURES_ALL {           \
+  0, 1, 2, 3, 4, 5, 6, 7,		\
+  CEPHFS_FEATURE_MIMIC,                 \
+}
+
+#define CEPHFS_FEATURES_CLIENT_SUPPORTED CEPHFS_FEATURES_ALL
+#define CEPHFS_FEATURES_CLIENT_REQUIRED {}
+
+
 /*
  * Some lock dependencies:
  *

commit 0ed1e90a09eb0e2863cab43e1ed5c5df89566772
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 13 22:18:38 2018 +0200

    ceph: use timespec64 for r_stamp
    
    The ceph_mds_request stamp still uses the deprecated timespec structure,
    this converts it over as well.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 2ec3b5b35067..80a7523e1523 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -229,7 +229,7 @@ struct ceph_mds_request {
 	int r_fmode;        /* file mode, if expecting cap */
 	kuid_t r_uid;
 	kgid_t r_gid;
-	struct timespec r_stamp;
+	struct timespec64 r_stamp;
 
 	/* for choosing which mds to send this request to */
 	int r_direct_mode;

commit d557c48db730eaab6b75d4af332c135309b7a6a4
Author: Luis Henriques <lhenriques@suse.com>
Date:   Fri Jan 12 17:19:29 2018 +0000

    ceph: quota: add counter for snaprealms with quota
    
    By keeping a counter with the number of snaprealms that have quota set
    allows to optimize the functions that need to walk throught the realms
    hierarchy looking for quotas.  Thus, if this counter is zero it's safe to
    assume that there are no realms with quota.
    
    Signed-off-by: Luis Henriques <lhenriques@suse.com>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 2a67c8b01ae6..2ec3b5b35067 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -314,6 +314,8 @@ struct ceph_mds_client {
 	int                     max_sessions;  /* len of s_mds_sessions */
 	int                     stopping;      /* true if shutting down */
 
+	atomic64_t		quotarealms_count; /* # realms with quota */
+
 	/*
 	 * snap_rwsem will cover cap linkage into snaprealms, and
 	 * realm snap contexts.  (later, we can do per-realm snap

commit fb18a57568c2b84cd611e242c0f6fa97b45e4907
Author: Luis Henriques <lhenriques@suse.com>
Date:   Fri Jan 5 10:47:18 2018 +0000

    ceph: quota: add initial infrastructure to support cephfs quotas
    
    This patch adds the infrastructure required to support cephfs quotas as it
    is currently implemented in the ceph fuse client.  Cephfs quotas can be
    set on any directory, and can restrict the number of bytes or the number
    of files stored beneath that point in the directory hierarchy.
    
    Quotas are set using the extended attributes 'ceph.quota.max_files' and
    'ceph.quota.max_bytes', and can be removed by setting these attributes to
    '0'.
    
    Link: http://tracker.ceph.com/issues/22372
    Signed-off-by: Luis Henriques <lhenriques@suse.com>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 71e3b783ee6f..2a67c8b01ae6 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -49,6 +49,8 @@ struct ceph_mds_reply_info_in {
 	char *inline_data;
 	u32 pool_ns_len;
 	char *pool_ns_data;
+	u64 max_bytes;
+	u64 max_files;
 };
 
 struct ceph_mds_reply_dir_entry {

commit e30ee58121e34831b9665934d70dbc72ab0fe2fb
Author: Zhi Zhang <zhang.david2011@gmail.com>
Date:   Wed Jan 24 21:24:33 2018 +0800

    ceph: try to allocate enough memory for reserved caps
    
    ceph_reserve_caps() may not reserve enough caps under high memory
    pressure, but it saved the needed caps number that expected to
    be reserved. When getting caps, crash would happen due to number
    mismatch.
    
    Now we will try to trim more caps when failing to allocate memory
    for caps need to be reserved, then try again. If still failing to
    allocate memory, return -ENOMEM.
    
    Signed-off-by: Zhi Zhang <zhang.david2011@gmail.com>
    Reviewed-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 837ac4b087a0..71e3b783ee6f 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -444,4 +444,7 @@ ceph_mdsc_open_export_target_session(struct ceph_mds_client *mdsc, int target);
 extern void ceph_mdsc_open_export_target_sessions(struct ceph_mds_client *mdsc,
 					  struct ceph_mds_session *session);
 
+extern int ceph_trim_caps(struct ceph_mds_client *mdsc,
+			  struct ceph_mds_session *session,
+			  int max_caps);
 #endif

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 636d6b2ec49c..837ac4b087a0 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _FS_CEPH_MDS_CLIENT_H
 #define _FS_CEPH_MDS_CLIENT_H
 

commit 717e6f2893eb35ce6728c3cacdc297b78d371b31
Author: Yan, Zheng <zyan@redhat.com>
Date:   Mon Sep 11 12:10:08 2017 +0800

    ceph: avoid panic in create_session_open_msg() if utsname() returns NULL
    
    utsname() can return NULL while process is exiting. Kernel releases
    file locks during process exits. We send request to mds when releasing
    file lock. So it's possible that we open mds session while process is
    exiting. utsname() is called in create_session_open_msg().
    
    Link: http://tracker.ceph.com/issues/21275
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Reviewed-by: Jeff Layton <jlayton@redhat.com>
    [idryomov@gmail.com: drop utsname.h include from mds_client.c]
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index db57ae98ed34..636d6b2ec49c 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -8,6 +8,7 @@
 #include <linux/rbtree.h>
 #include <linux/spinlock.h>
 #include <linux/refcount.h>
+#include <linux/utsname.h>
 
 #include <linux/ceph/types.h>
 #include <linux/ceph/messenger.h>
@@ -368,6 +369,8 @@ struct ceph_mds_client {
 
 	struct rw_semaphore     pool_perm_rwsem;
 	struct rb_root		pool_perm_tree;
+
+	char nodename[__NEW_UTS_LEN + 1];
 };
 
 extern const char *ceph_mds_op_name(int op);

commit 92475f05bdb6daefce3f55f46551153e7ed05f45
Author: Jeff Layton <jlayton@redhat.com>
Date:   Thu Apr 13 11:07:04 2017 -0400

    ceph: handle epoch barriers in cap messages
    
    Have the client store and update the osdc epoch_barrier when a cap
    message comes in with one.
    
    When sending cap messages, send the epoch barrier as well. This allows
    clients to inform servers that their released caps may not be used until
    a particular OSD map epoch.
    
    Signed-off-by: Jeff Layton <jlayton@redhat.com>
    Reviewed-by: "Yan, Zheng <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 3e67dd2169fa..db57ae98ed34 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -106,10 +106,13 @@ struct ceph_mds_reply_info_parsed {
 
 /*
  * cap releases are batched and sent to the MDS en masse.
+ *
+ * Account for per-message overhead of mds_cap_release header
+ * and __le32 for osd epoch barrier trailing field.
  */
-#define CEPH_CAPS_PER_RELEASE ((PAGE_SIZE -			\
+#define CEPH_CAPS_PER_RELEASE ((PAGE_SIZE - sizeof(u32) -		\
 				sizeof(struct ceph_mds_cap_release)) /	\
-			       sizeof(struct ceph_mds_cap_item))
+			        sizeof(struct ceph_mds_cap_item))
 
 
 /*

commit 79162547b76e4979b21ef80c9629ada94a51a59b
Author: Yan, Zheng <zyan@redhat.com>
Date:   Wed Apr 5 12:54:05 2017 -0400

    ceph: make seeky readdir more efficient
    
    Current cephfs client uses string to indicate start position of
    readdir. The string is last entry of previous readdir reply.
    This approach does not work for seeky readdir because we can
    not easily convert the new postion to a string. For seeky readdir,
    mds needs to return dentries from the beginning. Client keeps
    retrying if the reply does not contain the dentry it wants.
    
    In current version of ceph, mds sorts CDentry in its cache in
    hash order. Client also uses dentry hash to compose dir postion.
    For seeky readdir, if client passes the hash part of dir postion
    to mds. mds can avoid replying useless dentries.
    
    Signed-off-by: "Yan, Zheng" <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index bbebcd55d79e..3e67dd2169fa 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -83,9 +83,10 @@ struct ceph_mds_reply_info_parsed {
 			struct ceph_mds_reply_dirfrag *dir_dir;
 			size_t			      dir_buf_size;
 			int                           dir_nr;
-			bool			      dir_complete;
 			bool			      dir_end;
+			bool			      dir_complete;
 			bool			      hash_order;
+			bool			      offset_hash;
 			struct ceph_mds_reply_dir_entry  *dir_entries;
 		};
 

commit 3997c01d260ed00d712b051fdab022a08719441e
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Mar 3 11:15:06 2017 +0200

    ceph: convert ceph_mds_session.s_ref from atomic_t to refcount_t
    
    refcount_t type and corresponding API should be
    used instead of atomic_t when the variable is used as
    a reference counter. This allows to avoid accidental
    refcounter overflows that might lead to use-after-free
    situations.
    
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Hans Liljestrand <ishkamiel@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David Windsor <dwindsor@gmail.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index ac0475a2daa7..bbebcd55d79e 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -7,6 +7,7 @@
 #include <linux/mutex.h>
 #include <linux/rbtree.h>
 #include <linux/spinlock.h>
+#include <linux/refcount.h>
 
 #include <linux/ceph/types.h>
 #include <linux/ceph/messenger.h>
@@ -156,7 +157,7 @@ struct ceph_mds_session {
 	unsigned long     s_renew_requested; /* last time we sent a renew req */
 	u64               s_renew_seq;
 
-	atomic_t          s_ref;
+	refcount_t          s_ref;
 	struct list_head  s_waiting;  /* waiting requests */
 	struct list_head  s_unsafe;   /* unsafe requests */
 };
@@ -373,7 +374,7 @@ __ceph_lookup_mds_session(struct ceph_mds_client *, int mds);
 static inline struct ceph_mds_session *
 ceph_get_mds_session(struct ceph_mds_session *s)
 {
-	atomic_inc(&s->s_ref);
+	refcount_inc(&s->s_ref);
 	return s;
 }
 

commit 3dd69aabcef3d835446a9a1e11d2eab0e6e35e95
Author: Jeff Layton <jlayton@redhat.com>
Date:   Tue Jan 31 10:28:26 2017 -0500

    ceph: add a new flag to indicate whether parent is locked
    
    struct ceph_mds_request has an r_locked_dir pointer, which is set to
    indicate the parent inode and that its i_rwsem is locked.  In some
    critical places, we need to be able to indicate the parent inode to the
    request handling code, even when its i_rwsem may not be locked.
    
    Most of the code that operates on r_locked_dir doesn't require that the
    i_rwsem be locked. We only really need it to handle manipulation of the
    dcache. The rest (filling of the inode, updating dentry leases, etc.)
    already has its own locking.
    
    Add a new r_req_flags bit that indicates whether the parent is locked
    when doing the request, and rename the pointer to "r_parent". For now,
    all the places that set r_parent also set this flag, but that will
    change in a later patch.
    
    Signed-off-by: Jeff Layton <jlayton@redhat.com>
    Reviewed-by: Yan, Zheng <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 409b0e3c3b7a..ac0475a2daa7 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -202,7 +202,7 @@ struct ceph_mds_request {
 	char *r_path1, *r_path2;
 	struct ceph_vino r_ino1, r_ino2;
 
-	struct inode *r_locked_dir; /* dir (if any) i_mutex locked by vfs */
+	struct inode *r_parent;		    /* parent dir inode */
 	struct inode *r_target_inode;       /* resulting inode */
 
 #define CEPH_MDS_R_DIRECT_IS_HASH	(1) /* r_direct_hash is valid */
@@ -211,6 +211,7 @@ struct ceph_mds_request {
 #define CEPH_MDS_R_GOT_SAFE		(4) /* got a safe reply */
 #define CEPH_MDS_R_GOT_RESULT		(5) /* got a result */
 #define CEPH_MDS_R_DID_PREPOPULATE	(6) /* prepopulated readdir */
+#define CEPH_MDS_R_PARENT_LOCKED	(7) /* is r_parent->i_rwsem wlocked? */
 	unsigned long	r_req_flags;
 
 	struct mutex r_fill_mutex;

commit bc2de10dc4da5036ada3381775bd966f0c21c603
Author: Jeff Layton <jlayton@redhat.com>
Date:   Wed Feb 1 13:49:09 2017 -0500

    ceph: convert bools in ceph_mds_request to a new r_req_flags field
    
    Currently, we have a bunch of bool flags in struct ceph_mds_request. We
    need more flags though, but each bool takes (at least) a byte. Those
    add up over time.
    
    Merge all of the existing bools in this struct into a single unsigned
    long, and use the set/test/clear_bit macros to manipulate them. These
    are atomic operations, but that is required here to prevent
    load/modify/store races. The existing flags are protected by different
    locks, so we can't rely on them for that purpose.
    
    Signed-off-by: Jeff Layton <jlayton@redhat.com>
    Reviewed-by: Yan, Zheng <zyan@redhat.com>
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 3c6f77b7bb02..409b0e3c3b7a 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -205,6 +205,14 @@ struct ceph_mds_request {
 	struct inode *r_locked_dir; /* dir (if any) i_mutex locked by vfs */
 	struct inode *r_target_inode;       /* resulting inode */
 
+#define CEPH_MDS_R_DIRECT_IS_HASH	(1) /* r_direct_hash is valid */
+#define CEPH_MDS_R_ABORTED		(2) /* call was aborted */
+#define CEPH_MDS_R_GOT_UNSAFE		(3) /* got an unsafe reply */
+#define CEPH_MDS_R_GOT_SAFE		(4) /* got a safe reply */
+#define CEPH_MDS_R_GOT_RESULT		(5) /* got a result */
+#define CEPH_MDS_R_DID_PREPOPULATE	(6) /* prepopulated readdir */
+	unsigned long	r_req_flags;
+
 	struct mutex r_fill_mutex;
 
 	union ceph_mds_request_args r_args;
@@ -216,7 +224,6 @@ struct ceph_mds_request {
 	/* for choosing which mds to send this request to */
 	int r_direct_mode;
 	u32 r_direct_hash;      /* choose dir frag based on this dentry hash */
-	bool r_direct_is_hash;  /* true if r_direct_hash is valid */
 
 	/* data payload is used for xattr ops */
 	struct ceph_pagelist *r_pagelist;
@@ -234,7 +241,6 @@ struct ceph_mds_request {
 	struct ceph_mds_reply_info_parsed r_reply_info;
 	struct page *r_locked_page;
 	int r_err;
-	bool r_aborted;
 
 	unsigned long r_timeout;  /* optional.  jiffies, 0 is "wait forever" */
 	unsigned long r_started;  /* start time to measure timeout against */
@@ -262,9 +268,7 @@ struct ceph_mds_request {
 	ceph_mds_request_callback_t r_callback;
 	ceph_mds_request_wait_callback_t r_wait_for_completion;
 	struct list_head  r_unsafe_item;  /* per-session unsafe list item */
-	bool		  r_got_unsafe, r_got_safe, r_got_result;
 
-	bool              r_did_prepopulate;
 	long long	  r_dir_release_cnt;
 	long long	  r_dir_ordered_cnt;
 	int		  r_readdir_cache_idx;

commit fcff415c9421b417ef91d48f546f3c4566ddc358
Author: Yan, Zheng <zyan@redhat.com>
Date:   Wed Sep 14 16:39:51 2016 +0800

    ceph: handle CEPH_SESSION_REJECT message
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 6b3679737d4a..3c6f77b7bb02 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -121,6 +121,7 @@ enum {
 	CEPH_MDS_SESSION_CLOSING = 5,
 	CEPH_MDS_SESSION_RESTARTING = 6,
 	CEPH_MDS_SESSION_RECONNECTING = 7,
+	CEPH_MDS_SESSION_REJECTED = 8,
 };
 
 struct ceph_mds_session {

commit 0e2943878942aee7100c94d0d40c49087dac12cb
Author: Yan, Zheng <zyan@redhat.com>
Date:   Mon Jul 4 18:06:41 2016 +0800

    ceph: unify cap flush and snapcap flush
    
    This patch includes following changes
    - Assign flush tid to snapcap flush
    - Remove session's s_cap_snaps_flushing list. Add inode to session's
      s_cap_flushing list instead. Inode is removed from the list when
      there is no pending snapcap flush or cap flush.
    - make __kick_flushing_caps() re-send both snapcap flushes and cap
      flushes.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 93170b4b5d75..6b3679737d4a 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -152,7 +152,6 @@ struct ceph_mds_session {
 
 	/* protected by mutex */
 	struct list_head  s_cap_flushing;     /* inodes w/ flushing caps */
-	struct list_head  s_cap_snaps_flushing;
 	unsigned long     s_renew_requested; /* last time we sent a renew req */
 	u64               s_renew_seq;
 

commit e4500b5e35c213e0f97be7cb69328c0877203a79
Author: Yan, Zheng <zyan@redhat.com>
Date:   Wed Jul 6 11:12:56 2016 +0800

    ceph: use list instead of rbtree to track cap flushes
    
    We don't have requirement of searching cap flush by TID. In most cases,
    we just need to know TID of the oldest cap flush. List is ideal for this
    usage.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 3c154b8d49bf..93170b4b5d75 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -325,7 +325,7 @@ struct ceph_mds_client {
 	spinlock_t       snap_flush_lock;
 
 	u64               last_cap_flush_tid;
-	struct rb_root    cap_flush_tree;
+	struct list_head  cap_flush_list;
 	struct list_head  cap_dirty;        /* inodes with dirty caps */
 	struct list_head  cap_dirty_migrating; /* ...that are migration... */
 	int               num_cap_flushing; /* # caps we are flushing */

commit 430afbadd6c885557ef2fb8c454bd5bba23a9850
Author: Yan, Zheng <zyan@redhat.com>
Date:   Fri Jul 8 11:25:38 2016 +0800

    ceph: mount non-default filesystem by name
    
    To mount non-default filesytem, user currently needs to provide mds
    namespace ID. This is inconvenience.
    
    This patch makes user be able to mount filesystem by name. If user
    wants to mount non-default filesystem. Client first subscribes to
    fsmap.user. Subscribe to mdsmap.<ID> after getting ID of filesystem.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 9dd2c82379f8..3c154b8d49bf 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -293,6 +293,7 @@ struct ceph_mds_client {
 	struct completion       safe_umount_waiters;
 	wait_queue_head_t       session_close_wq;
 	struct list_head        waiting_for_map;
+	int 			mdsmap_err;
 
 	struct ceph_mds_session **sessions;    /* NULL for mds if no session */
 	atomic_t		num_sessions;
@@ -419,8 +420,10 @@ extern void ceph_mdsc_lease_send_msg(struct ceph_mds_session *session,
 				     struct dentry *dentry, char action,
 				     u32 seq);
 
-extern void ceph_mdsc_handle_map(struct ceph_mds_client *mdsc,
-				 struct ceph_msg *msg);
+extern void ceph_mdsc_handle_mdsmap(struct ceph_mds_client *mdsc,
+				    struct ceph_msg *msg);
+extern void ceph_mdsc_handle_fsmap(struct ceph_mds_client *mdsc,
+				   struct ceph_msg *msg);
 
 extern struct ceph_mds_session *
 ceph_mdsc_open_export_target_session(struct ceph_mds_client *mdsc, int target);

commit 8aa152c77890abd0731f119e4e6662375503e288
Author: Jeff Layton <jlayton@redhat.com>
Date:   Fri Jul 1 09:39:20 2016 -0400

    ceph: remove ceph_mdsc_lease_release
    
    Nothing calls it.
    
    Signed-off-by: Jeff Layton <jlayton@redhat.com>
    Reviewed-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 2ce8e9f9bfc9..9dd2c82379f8 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -385,10 +385,6 @@ extern void ceph_mdsc_destroy(struct ceph_fs_client *fsc);
 
 extern void ceph_mdsc_sync(struct ceph_mds_client *mdsc);
 
-extern void ceph_mdsc_lease_release(struct ceph_mds_client *mdsc,
-				    struct inode *inode,
-				    struct dentry *dn);
-
 extern void ceph_invalidate_dir_request(struct ceph_mds_request *req);
 extern int ceph_alloc_readdir_reply_buffer(struct ceph_mds_request *req,
 					   struct inode *dir);

commit 779fe0fb8e1883d5c479ac6bd85fbd237deed1f7
Author: Yan, Zheng <zyan@redhat.com>
Date:   Mon Mar 7 09:35:06 2016 +0800

    ceph: rados pool namespace support
    
    This patch adds codes that decode pool namespace information in
    cap message and request reply. Pool namespace is saved in i_layout,
    it will be passed to libceph when doing read/write.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 75ecf967d0b7..2ce8e9f9bfc9 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -45,6 +45,7 @@ struct ceph_mds_reply_info_in {
 	u32 inline_len;
 	char *inline_data;
 	u32 pool_ns_len;
+	char *pool_ns_data;
 };
 
 struct ceph_mds_reply_dir_entry {
@@ -277,6 +278,8 @@ struct ceph_pool_perm {
 	struct rb_node node;
 	int perm;
 	s64 pool;
+	size_t pool_ns_len;
+	char pool_ns[];
 };
 
 /*

commit 7627151ea30bce2051e3cb27d7bb2c30083f86a5
Author: Yan, Zheng <zyan@redhat.com>
Date:   Wed Feb 3 21:24:49 2016 +0800

    libceph: define new ceph_file_layout structure
    
    Define new ceph_file_layout structure and rename old ceph_file_layout
    to ceph_file_layout_legacy. This is preparation for adding namespace
    to ceph_file_layout structure.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index e7d38aac7109..75ecf967d0b7 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -275,8 +275,8 @@ struct ceph_mds_request {
 
 struct ceph_pool_perm {
 	struct rb_node node;
-	u32 pool;
 	int perm;
+	s64 pool;
 };
 
 /*

commit f3c4ebe65ea149ec892f94474233cfebe9cbe299
Author: Yan, Zheng <zyan@redhat.com>
Date:   Fri Apr 29 11:27:30 2016 +0800

    ceph: using hash value to compose dentry offset
    
    If MDS sorts dentries in dirfrag in hash order, we use hash value to
    compose dentry offset. dentry offset is:
    
      (0xff << 52) | ((24 bits hash) << 28) |
      (the nth entry hash hash collision)
    
    This offset is stable across directory fragmentation. This alos means
    there is no need to reset readdir offset if directory get fragmented
    in the middle of readdir.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 4ce19d852657..e7d38aac7109 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -81,7 +81,9 @@ struct ceph_mds_reply_info_parsed {
 			struct ceph_mds_reply_dirfrag *dir_dir;
 			size_t			      dir_buf_size;
 			int                           dir_nr;
-			bool			      dir_complete, dir_end;
+			bool			      dir_complete;
+			bool			      dir_end;
+			bool			      hash_order;
 			struct ceph_mds_reply_dir_entry  *dir_entries;
 		};
 

commit 8974eebd38737c9534d81c4131c5fdb1fe24d3e9
Author: Yan, Zheng <zyan@redhat.com>
Date:   Thu Apr 28 15:17:40 2016 +0800

    ceph: record 'offset' for each entry of readdir result
    
    This is preparation for using hash value as dentry 'offset'
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 2a865812a41b..4ce19d852657 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -52,6 +52,7 @@ struct ceph_mds_reply_dir_entry {
 	u32                           name_len;
 	struct ceph_mds_reply_lease   *lease;
 	struct ceph_mds_reply_info_in inode;
+	loff_t			      offset;
 };
 
 /*

commit 956d39d631dbcf7b57854873a24e309047f2a7f5
Author: Yan, Zheng <zyan@redhat.com>
Date:   Wed Apr 27 17:48:30 2016 +0800

    ceph: define 'end/complete' in readdir reply as bit flags
    
    Set a flag in readdir request, which indicates that client interprets
    'end/complete' as bit flags. So that mds can reply additional flags in
    readdir reply.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 0b84f9c0afa3..2a865812a41b 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -80,7 +80,7 @@ struct ceph_mds_reply_info_parsed {
 			struct ceph_mds_reply_dirfrag *dir_dir;
 			size_t			      dir_buf_size;
 			int                           dir_nr;
-			u8                            dir_complete, dir_end;
+			bool			      dir_complete, dir_end;
 			struct ceph_mds_reply_dir_entry  *dir_entries;
 		};
 

commit 2a5beea3f1b6544d6c72ea220e860a2eda2f9104
Author: Yan, Zheng <zyan@redhat.com>
Date:   Thu Apr 28 09:37:39 2016 +0800

    ceph: define struct for dir entry in readdir reply
    
    This avoids defining multiple arrays for entries in readdir reply
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index ee69a537dba5..0b84f9c0afa3 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -47,6 +47,13 @@ struct ceph_mds_reply_info_in {
 	u32 pool_ns_len;
 };
 
+struct ceph_mds_reply_dir_entry {
+	char                          *name;
+	u32                           name_len;
+	struct ceph_mds_reply_lease   *lease;
+	struct ceph_mds_reply_info_in inode;
+};
+
 /*
  * parsed info about an mds reply, including information about
  * either: 1) the target inode and/or its parent directory and dentry,
@@ -73,11 +80,8 @@ struct ceph_mds_reply_info_parsed {
 			struct ceph_mds_reply_dirfrag *dir_dir;
 			size_t			      dir_buf_size;
 			int                           dir_nr;
-			char                          **dir_dname;
-			u32                           *dir_dname_len;
-			struct ceph_mds_reply_lease   **dir_dlease;
-			struct ceph_mds_reply_info_in *dir_in;
 			u8                            dir_complete, dir_end;
+			struct ceph_mds_reply_dir_entry  *dir_entries;
 		};
 
 		/* for create results */

commit 09cbfeaf1a5a67bfb3201e0c83c810cecb2efa5a
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Apr 1 15:29:47 2016 +0300

    mm, fs: get rid of PAGE_CACHE_* and page_cache_{get,release} macros
    
    PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} macros were introduced *long* time
    ago with promise that one day it will be possible to implement page
    cache with bigger chunks than PAGE_SIZE.
    
    This promise never materialized.  And unlikely will.
    
    We have many places where PAGE_CACHE_SIZE assumed to be equal to
    PAGE_SIZE.  And it's constant source of confusion on whether
    PAGE_CACHE_* or PAGE_* constant should be used in a particular case,
    especially on the border between fs and mm.
    
    Global switching to PAGE_CACHE_SIZE != PAGE_SIZE would cause to much
    breakage to be doable.
    
    Let's stop pretending that pages in page cache are special.  They are
    not.
    
    The changes are pretty straight-forward:
    
     - <foo> << (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;
    
     - <foo> >> (PAGE_CACHE_SHIFT - PAGE_SHIFT) -> <foo>;
    
     - PAGE_CACHE_{SIZE,SHIFT,MASK,ALIGN} -> PAGE_{SIZE,SHIFT,MASK,ALIGN};
    
     - page_cache_get() -> get_page();
    
     - page_cache_release() -> put_page();
    
    This patch contains automated changes generated with coccinelle using
    script below.  For some reason, coccinelle doesn't patch header files.
    I've called spatch for them manually.
    
    The only adjustment after coccinelle is revert of changes to
    PAGE_CAHCE_ALIGN definition: we are going to drop it later.
    
    There are few places in the code where coccinelle didn't reach.  I'll
    fix them manually in a separate patch.  Comments and documentation also
    will be addressed with the separate patch.
    
    virtual patch
    
    @@
    expression E;
    @@
    - E << (PAGE_CACHE_SHIFT - PAGE_SHIFT)
    + E
    
    @@
    expression E;
    @@
    - E >> (PAGE_CACHE_SHIFT - PAGE_SHIFT)
    + E
    
    @@
    @@
    - PAGE_CACHE_SHIFT
    + PAGE_SHIFT
    
    @@
    @@
    - PAGE_CACHE_SIZE
    + PAGE_SIZE
    
    @@
    @@
    - PAGE_CACHE_MASK
    + PAGE_MASK
    
    @@
    expression E;
    @@
    - PAGE_CACHE_ALIGN(E)
    + PAGE_ALIGN(E)
    
    @@
    expression E;
    @@
    - page_cache_get(E)
    + get_page(E)
    
    @@
    expression E;
    @@
    - page_cache_release(E)
    + put_page(E)
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 37712ccffcc6..ee69a537dba5 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -97,7 +97,7 @@ struct ceph_mds_reply_info_parsed {
 /*
  * cap releases are batched and sent to the MDS en masse.
  */
-#define CEPH_CAPS_PER_RELEASE ((PAGE_CACHE_SIZE -			\
+#define CEPH_CAPS_PER_RELEASE ((PAGE_SIZE -			\
 				sizeof(struct ceph_mds_cap_release)) /	\
 			       sizeof(struct ceph_mds_cap_item))
 

commit 5ea5c5e0a7f70b256417d3b6e36bd9851504babd
Author: Yan, Zheng <zyan@redhat.com>
Date:   Sun Feb 14 18:06:41 2016 +0800

    ceph: initial CEPH_FEATURE_FS_FILE_LAYOUT_V2 support
    
    Add support for the format change of MClientReply/MclientCaps.
    Also add code that denies access to inodes with pool_ns layouts.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>
    Reviewed-by: Sage Weil <sage@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index ccf11ef0ca87..37712ccffcc6 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -44,6 +44,7 @@ struct ceph_mds_reply_info_in {
 	u64 inline_version;
 	u32 inline_len;
 	char *inline_data;
+	u32 pool_ns_len;
 };
 
 /*

commit 68cd5b4b7612c2956d8553dfb39490b29f32566d
Author: Yan, Zheng <zyan@redhat.com>
Date:   Tue Oct 27 18:36:06 2015 +0800

    ceph: make fsync() wait unsafe requests that created/modified inode
    
    If we get a unsafe reply for request that created/modified inode,
    add the unsafe request to a list in the newly created/modified
    inode. So we can make fsync() wait these unsafe requests.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index f575eafe2261..ccf11ef0ca87 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -236,6 +236,9 @@ struct ceph_mds_request {
 	struct inode	*r_unsafe_dir;
 	struct list_head r_unsafe_dir_item;
 
+	/* unsafe requests that modify the target inode */
+	struct list_head r_unsafe_target_item;
+
 	struct ceph_mds_session *r_session;
 
 	int               r_attempts;   /* resend attempts */

commit 48fec5d0a504dfbb302cb1dd24ebb0b82a46cce9
Author: Yan, Zheng <zyan@redhat.com>
Date:   Wed Jul 1 16:27:46 2015 +0800

    ceph: EIO all operations after forced umount
    
    This patch makes try_get_cap_refs() and __do_request() check
    if the file system was forced umount, and return -EIO if it was.
    This patch also adds a helper function to drops dirty caps and
    wakes up blocking operation.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 762757e6cebf..f575eafe2261 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -366,6 +366,7 @@ extern int ceph_send_msg_mds(struct ceph_mds_client *mdsc,
 
 extern int ceph_mdsc_init(struct ceph_fs_client *fsc);
 extern void ceph_mdsc_close_sessions(struct ceph_mds_client *mdsc);
+extern void ceph_mdsc_force_umount(struct ceph_mds_client *mdsc);
 extern void ceph_mdsc_destroy(struct ceph_fs_client *fsc);
 
 extern void ceph_mdsc_sync(struct ceph_mds_client *mdsc);

commit fdd4e15838e59c394a1ec4963b57c22c12608685
Author: Yan, Zheng <zyan@redhat.com>
Date:   Tue Jun 16 20:48:56 2015 +0800

    ceph: rework dcache readdir
    
    Previously our dcache readdir code relies on that child dentries in
    directory dentry's d_subdir list are sorted by dentry's offset in
    descending order. When adding dentries to the dcache, if a dentry
    already exists, our readdir code moves it to head of directory
    dentry's d_subdir list. This design relies on dcache internals.
    Al Viro suggests using ncpfs's approach: keeping array of pointers
    to dentries in page cache of directory inode. the validity of those
    pointers are presented by directory inode's complete and ordered
    flags. When a dentry gets pruned, we clear directory inode's complete
    flag in the d_prune() callback. Before moving a dentry to other
    directory, we clear the ordered flag for both old and new directory.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 470be4eb25f3..762757e6cebf 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -253,6 +253,9 @@ struct ceph_mds_request {
 	bool		  r_got_unsafe, r_got_safe, r_got_result;
 
 	bool              r_did_prepopulate;
+	long long	  r_dir_release_cnt;
+	long long	  r_dir_ordered_cnt;
+	int		  r_readdir_cache_idx;
 	u32               r_readdir_offset;
 
 	struct ceph_cap_reservation r_caps_reservation;

commit 8310b08913eca8aee98744c9aff1ec0d1f603b19
Author: Yan, Zheng <zyan@redhat.com>
Date:   Tue Jun 9 17:20:12 2015 +0800

    ceph: track pending caps flushing globally
    
    So we know TID of the oldest pending caps flushing. Later patch will
    send this information to MDS, so that MDS can trim its completed caps
    flush list.
    
    Tracking pending caps flushing globally also simplifies syncfs code.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 19f6084203f0..470be4eb25f3 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -306,8 +306,8 @@ struct ceph_mds_client {
 	struct list_head snap_flush_list;  /* cap_snaps ready to flush */
 	spinlock_t       snap_flush_lock;
 
-	u64               cap_flush_seq;
 	u64               last_cap_flush_tid;
+	struct rb_root    cap_flush_tree;
 	struct list_head  cap_dirty;        /* inodes with dirty caps */
 	struct list_head  cap_dirty_migrating; /* ...that are migration... */
 	int               num_cap_flushing; /* # caps we are flushing */

commit 553adfd941f8ca622965ef809553d918ea039929
Author: Yan, Zheng <zyan@redhat.com>
Date:   Tue Jun 9 15:48:57 2015 +0800

    ceph: track pending caps flushing accurately
    
    Previously we do not trace accurate TID for flushing caps. when
    MDS failovers, we have no choice but to re-send all flushing caps
    with a new TID. This can cause problem because MDS can has already
    flushed some caps and has issued the same caps to other client.
    The re-sent cap flush has a new TID, which makes MDS unable to
    detect if it has already processed the cap flush.
    
    This patch adds code to track pending caps flushing accurately.
    When re-sending cap flush is needed, we use its original flush
    TID.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 509d6822e9b1..19f6084203f0 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -307,6 +307,7 @@ struct ceph_mds_client {
 	spinlock_t       snap_flush_lock;
 
 	u64               cap_flush_seq;
+	u64               last_cap_flush_tid;
 	struct list_head  cap_dirty;        /* inodes with dirty caps */
 	struct list_head  cap_dirty_migrating; /* ...that are migration... */
 	int               num_cap_flushing; /* # caps we are flushing */

commit a319bf56a617354e62cf5f774d2ca4e1a8a3bff3
Author: Ilya Dryomov <idryomov@gmail.com>
Date:   Fri May 15 12:02:17 2015 +0300

    libceph: store timeouts in jiffies, verify user input
    
    There are currently three libceph-level timeouts that the user can
    specify on mount: mount_timeout, osd_idle_ttl and osdkeepalive.  All of
    these are in seconds and no checking is done on user input: negative
    values are accepted, we multiply them all by HZ which may or may not
    overflow, arbitrarily large jiffies then get added together, etc.
    
    There is also a bug in the way mount_timeout=0 is handled.  It's
    supposed to mean "infinite timeout", but that's not how wait.h APIs
    treat it and so __ceph_open_session() for example will busy loop
    without much chance of being interrupted if none of ceph-mons are
    there.
    
    Fix all this by verifying user input, storing timeouts capped by
    msecs_to_jiffies() in jiffies and using the new ceph_timeout_jiffies()
    helper for all user-specified waits to handle infinite timeouts
    correctly.
    
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>
    Reviewed-by: Alex Elder <elder@linaro.org>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 2ef799961ebb..509d6822e9b1 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -227,7 +227,7 @@ struct ceph_mds_request {
 	int r_err;
 	bool r_aborted;
 
-	unsigned long r_timeout;  /* optional.  jiffies */
+	unsigned long r_timeout;  /* optional.  jiffies, 0 is "wait forever" */
 	unsigned long r_started;  /* start time to measure timeout against */
 	unsigned long r_request_started; /* start time for mds request only,
 					    used to measure lease durations */

commit e8a7b8b12b13831467c6158c1e82801e25b5dd98
Author: Yan, Zheng <zyan@redhat.com>
Date:   Tue May 19 18:54:40 2015 +0800

    ceph: exclude setfilelock requests when calculating oldest tid
    
    setfilelock requests can block for a long time, which can prevent
    client from advancing its oldest tid.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 294fa23a7df6..2ef799961ebb 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -296,6 +296,8 @@ struct ceph_mds_client {
 	spinlock_t              snap_empty_lock;  /* protect snap_empty */
 
 	u64                    last_tid;      /* most recent mds request */
+	u64                    oldest_tid;    /* oldest incomplete mds request,
+						 excluding setfilelock requests */
 	struct rb_root         request_tree;  /* pending mds requests */
 	struct delayed_work    delayed_work;  /* delayed work */
 	unsigned long    last_renew_caps;  /* last time we renewed our caps */

commit 745a8e3bccbc6adae69a98ddc525e529aa44636e
Author: Yan, Zheng <zyan@redhat.com>
Date:   Thu May 14 17:22:42 2015 +0800

    ceph: don't pre-allocate space for cap release messages
    
    Previously we pre-allocate cap release messages for each caps. This
    wastes lots of memory when there are large amount of caps. This patch
    make the code not pre-allocate the cap release messages. Instead,
    we add the corresponding ceph_cap struct to a list when releasing a
    cap. Later when flush cap releases is needed, we allocate the cap
    release messages dynamically.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index bf24d88cfeb2..294fa23a7df6 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -139,7 +139,6 @@ struct ceph_mds_session {
 	int		  s_cap_reconnect;
 	int		  s_readonly;
 	struct list_head  s_cap_releases; /* waiting cap_release messages */
-	struct list_head  s_cap_releases_done; /* ready to send */
 	struct ceph_cap  *s_cap_iterator;
 
 	/* protected by mutex */
@@ -389,8 +388,6 @@ static inline void ceph_mdsc_put_request(struct ceph_mds_request *req)
 	kref_put(&req->r_kref, ceph_mdsc_release_request);
 }
 
-extern int ceph_add_cap_releases(struct ceph_mds_client *mdsc,
-				 struct ceph_mds_session *session);
 extern void ceph_send_cap_releases(struct ceph_mds_client *mdsc,
 				   struct ceph_mds_session *session);
 

commit affbc19a68f9966ad65a773db405f78e2bafc07b
Author: Yan, Zheng <zyan@redhat.com>
Date:   Tue May 5 21:22:13 2015 +0800

    ceph: make sure syncfs flushes all cap snaps
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index d474141c034a..bf24d88cfeb2 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -290,6 +290,7 @@ struct ceph_mds_client {
 	 * references (implying they contain no inodes with caps) that
 	 * should be destroyed.
 	 */
+	u64			last_snap_seq;
 	struct rw_semaphore     snap_rwsem;
 	struct rb_root          snap_realms;
 	struct list_head        snap_empty;

commit 10183a69551f76702ac68bc74a437b25419c6de0
Author: Yan, Zheng <zyan@redhat.com>
Date:   Mon Apr 27 15:33:28 2015 +0800

    ceph: check OSD caps before read/write
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 1875b5d985c6..d474141c034a 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -260,6 +260,12 @@ struct ceph_mds_request {
 	int r_num_caps;
 };
 
+struct ceph_pool_perm {
+	struct rb_node node;
+	u32 pool;
+	int perm;
+};
+
 /*
  * mds client state
  */
@@ -328,6 +334,9 @@ struct ceph_mds_client {
 	spinlock_t	  dentry_lru_lock;
 	struct list_head  dentry_lru;
 	int		  num_dentry;
+
+	struct rw_semaphore     pool_perm_rwsem;
+	struct rb_root		pool_perm_tree;
 };
 
 extern const char *ceph_mds_op_name(int op);

commit 86d8f67b26a8b30228b5177b7e594bbc89798a23
Author: Yan, Zheng <zyan@redhat.com>
Date:   Fri Jan 9 17:00:42 2015 +0800

    ceph: avoid block operation when !TASK_RUNNING (ceph_mdsc_close_sessions)
    
    use an atomic variable to track number of sessions, this can avoid block
    operation inside wait loops.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index a87b92f500bb..1875b5d985c6 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -273,6 +273,7 @@ struct ceph_mds_client {
 	struct list_head        waiting_for_map;
 
 	struct ceph_mds_session **sessions;    /* NULL for mds if no session */
+	atomic_t		num_sessions;
 	int                     max_sessions;  /* len of s_mds_sessions */
 	int                     stopping;      /* true if shutting down */
 

commit 03f4fcb02884859b584c709652bb48f8125ceb45
Author: Yan, Zheng <zyan@redhat.com>
Date:   Mon Jan 5 11:04:04 2015 +0800

    ceph: handle SESSION_FORCE_RO message
    
    mark session as readonly and wake up all cap waiters.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index e2817d00f7d9..a87b92f500bb 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -137,6 +137,7 @@ struct ceph_mds_session {
 	int               s_nr_caps, s_trim_caps;
 	int               s_num_cap_releases;
 	int		  s_cap_reconnect;
+	int		  s_readonly;
 	struct list_head  s_cap_releases; /* waiting cap_release messages */
 	struct list_head  s_cap_releases_done; /* ready to send */
 	struct ceph_cap  *s_cap_iterator;

commit 01deead041e03c9a6b4e1b2dd165dee4cced6112
Author: Yan, Zheng <zyan@redhat.com>
Date:   Fri Nov 14 21:56:29 2014 +0800

    ceph: use getattr request to fetch inline data
    
    Add a new parameter 'locked_page' to ceph_do_getattr(). If inline data
    in getattr reply will be copied to the page.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 01f5e4cfd684..e2817d00f7d9 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -223,6 +223,7 @@ struct ceph_mds_request {
 	int r_request_release_offset;
 	struct ceph_msg  *r_reply;
 	struct ceph_mds_reply_info_parsed r_reply_info;
+	struct page *r_locked_page;
 	int r_err;
 	bool r_aborted;
 

commit fb01d1f8b0343f1b19be878cee89d089f06e9f38
Author: Yan, Zheng <zyan@redhat.com>
Date:   Fri Nov 14 21:29:55 2014 +0800

    ceph: parse inline data in MClientReply and MClientCaps
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 230bda791d4f..01f5e4cfd684 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -41,6 +41,9 @@ struct ceph_mds_reply_info_in {
 	char *symlink;
 	u32 xattr_len;
 	char *xattr_data;
+	u64 inline_version;
+	u32 inline_len;
+	char *inline_data;
 };
 
 /*

commit 9280be24dc9c7aaee230de3ed33f8357386de9a2
Author: Yan, Zheng <zyan@redhat.com>
Date:   Tue Oct 14 10:33:35 2014 +0800

    ceph: fix file lock interruption
    
    When a lock operation is interrupted, current code sends a unlock request to
    MDS to undo the lock operation. This method does not work as expected because
    the unlock request can drop locks that have already been acquired.
    
    The fix is use the newly introduced CEPH_LOCK_FCNTL_INTR/CEPH_LOCK_FLOCK_INTR
    requests to interrupt blocked file lock request. These requests do not drop
    locks that have alread been acquired, they only interrupt blocked file lock
    request.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 3288359353e9..230bda791d4f 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -166,6 +166,11 @@ struct ceph_mds_client;
  */
 typedef void (*ceph_mds_request_callback_t) (struct ceph_mds_client *mdsc,
 					     struct ceph_mds_request *req);
+/*
+ * wait for request completion callback
+ */
+typedef int (*ceph_mds_request_wait_callback_t) (struct ceph_mds_client *mdsc,
+						 struct ceph_mds_request *req);
 
 /*
  * an in-flight mds request
@@ -239,6 +244,7 @@ struct ceph_mds_request {
 	struct completion r_completion;
 	struct completion r_safe_completion;
 	ceph_mds_request_callback_t r_callback;
+	ceph_mds_request_wait_callback_t r_wait_for_completion;
 	struct list_head  r_unsafe_item;  /* per-session unsafe list item */
 	bool		  r_got_unsafe, r_got_safe, r_got_result;
 

commit a687ecaf50f18329206c6b78764a8c7bd30a9df0
Author: John Spray <john.spray@redhat.com>
Date:   Fri Sep 19 13:51:08 2014 +0100

    ceph: export ceph_session_state_name function
    
    ...so that it can be used from the ceph debugfs
    code when dumping session info.
    
    Signed-off-by: John Spray <john.spray@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 23015f747061..3288359353e9 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -330,6 +330,8 @@ ceph_get_mds_session(struct ceph_mds_session *s)
 	return s;
 }
 
+extern const char *ceph_session_state_name(int s);
+
 extern void ceph_put_mds_session(struct ceph_mds_session *s);
 
 extern int ceph_send_msg_mds(struct ceph_mds_client *mdsc,

commit 25e6bae356502cde283f1804111b44e6fad20fc2
Author: Yan, Zheng <zyan@redhat.com>
Date:   Tue Sep 16 19:15:28 2014 +0800

    ceph: use pagelist to present MDS request data
    
    Current code uses page array to present MDS request data. Pages in the
    array are allocated/freed by caller of ceph_mdsc_do_request(). If request
    is interrupted, the pages can be freed while they are still being used by
    the request message.
    
    The fix is use pagelist to present MDS request data. Pagelist is
    reference counted.
    
    Signed-off-by: Yan, Zheng <zyan@redhat.com>
    Reviewed-by: Sage Weil <sage@redhat.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index e00737cf523c..23015f747061 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -202,9 +202,7 @@ struct ceph_mds_request {
 	bool r_direct_is_hash;  /* true if r_direct_hash is valid */
 
 	/* data payload is used for xattr ops */
-	struct page **r_pages;
-	int r_num_pages;
-	int r_data_len;
+	struct ceph_pagelist *r_pagelist;
 
 	/* what caps shall we drop? */
 	int r_inode_drop, r_inode_unless;

commit b8e69066d8afa8d2670dc697252ff0e5907aafad
Author: Sage Weil <sage@inktank.com>
Date:   Wed May 21 17:41:08 2014 -0700

    ceph: include time stamp in every MDS request
    
    We recently modified the client/MDS protocol to include a timestamp in the
    client request.  This allows ctime updates to follow the client's clock
    in most cases, which avoids subtle problems when clocks are out of sync
    and timestamps are updated sometimes by the MDS clock (for most requests)
    and sometimes by the client clock (for cap writeback).
    
    Signed-off-by: Sage Weil <sage@inktank.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index e90cfccf93bd..e00737cf523c 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -194,6 +194,7 @@ struct ceph_mds_request {
 	int r_fmode;        /* file mode, if expecting cap */
 	kuid_t r_uid;
 	kgid_t r_gid;
+	struct timespec r_stamp;
 
 	/* for choosing which mds to send this request to */
 	int r_direct_mode;

commit 54008399dc0ce511a07b87f1af3d1f5c791982a4
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Sat Mar 29 13:41:15 2014 +0800

    ceph: preallocate buffer for readdir reply
    
    Preallocate buffer for readdir reply. Limit number of entries in
    readdir reply according to the buffer size.
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 68288917c737..e90cfccf93bd 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -67,6 +67,7 @@ struct ceph_mds_reply_info_parsed {
 		/* for readdir results */
 		struct {
 			struct ceph_mds_reply_dirfrag *dir_dir;
+			size_t			      dir_buf_size;
 			int                           dir_nr;
 			char                          **dir_dname;
 			u32                           *dir_dname_len;
@@ -346,7 +347,8 @@ extern void ceph_mdsc_lease_release(struct ceph_mds_client *mdsc,
 				    struct dentry *dn);
 
 extern void ceph_invalidate_dir_request(struct ceph_mds_request *req);
-
+extern int ceph_alloc_readdir_reply_buffer(struct ceph_mds_request *req,
+					   struct inode *dir);
 extern struct ceph_mds_request *
 ceph_mdsc_create_request(struct ceph_mds_client *mdsc, int op, int mode);
 extern void ceph_mdsc_submit_request(struct ceph_mds_client *mdsc,

commit 5d72d13c425bb41f7752962f168fb402b86b7ac0
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Sun Nov 24 14:33:01 2013 +0800

    ceph: add open export target session helper
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 4c053d099ae4..68288917c737 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -383,6 +383,8 @@ extern void ceph_mdsc_lease_send_msg(struct ceph_mds_session *session,
 extern void ceph_mdsc_handle_map(struct ceph_mds_client *mdsc,
 				 struct ceph_msg *msg);
 
+extern struct ceph_mds_session *
+ceph_mdsc_open_export_target_session(struct ceph_mds_client *mdsc, int target);
 extern void ceph_mdsc_open_export_target_sessions(struct ceph_mds_client *mdsc,
 					  struct ceph_mds_session *session);
 

commit 99a9c273b94a087f8feaec6c5ffbe3205a2dbe51
Author: Yan, Zheng <zheng.z.yan@intel.com>
Date:   Sun Sep 22 11:08:14 2013 +0800

    ceph: handle race between cap reconnect and cap release
    
    When a cap get released while composing the cap reconnect message.
    We should skip queuing the release message if the cap hasn't been
    added to the cap reconnect message.
    
    Signed-off-by: Yan, Zheng <zheng.z.yan@intel.com>
    Reviewed-by: Sage Weil <sage@inktank.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index c2a19fbbe517..4c053d099ae4 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -132,6 +132,7 @@ struct ceph_mds_session {
 	struct list_head  s_caps;     /* all caps issued by this session */
 	int               s_nr_caps, s_trim_caps;
 	int               s_num_cap_releases;
+	int		  s_cap_reconnect;
 	struct list_head  s_cap_releases; /* waiting cap_release messages */
 	struct list_head  s_cap_releases_done; /* ready to send */
 	struct ceph_cap  *s_cap_iterator;

commit 1cf0209c431fa7790253c532039d53b0773193aa
Merge: de1a2262b006 83ca14fdd358
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 28 17:43:09 2013 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/sage/ceph-client
    
    Pull Ceph updates from Sage Weil:
     "A few groups of patches here.  Alex has been hard at work improving
      the RBD code, layout groundwork for understanding the new formats and
      doing layering.  Most of the infrastructure is now in place for the
      final bits that will come with the next window.
    
      There are a few changes to the data layout.  Jim Schutt's patch fixes
      some non-ideal CRUSH behavior, and a set of patches from me updates
      the client to speak a newer version of the protocol and implement an
      improved hashing strategy across storage nodes (when the server side
      supports it too).
    
      A pair of patches from Sam Lang fix the atomicity of open+create
      operations.  Several patches from Yan, Zheng fix various mds/client
      issues that turned up during multi-mds torture tests.
    
      A final set of patches expose file layouts via virtual xattrs, and
      allow the policies to be set on directories via xattrs as well
      (avoiding the awkward ioctl interface and providing a consistent
      interface for both kernel mount and ceph-fuse users)."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/sage/ceph-client: (143 commits)
      libceph: add support for HASHPSPOOL pool flag
      libceph: update osd request/reply encoding
      libceph: calculate placement based on the internal data types
      ceph: update support for PGID64, PGPOOL3, OSDENC protocol features
      ceph: update "ceph_features.h"
      libceph: decode into cpu-native ceph_pg type
      libceph: rename ceph_pg -> ceph_pg_v1
      rbd: pass length, not op for osd completions
      rbd: move rbd_osd_trivial_callback()
      libceph: use a do..while loop in con_work()
      libceph: use a flag to indicate a fault has occurred
      libceph: separate non-locked fault handling
      libceph: encapsulate connection backoff
      libceph: eliminate sparse warnings
      ceph: eliminate sparse warnings in fs code
      rbd: eliminate sparse warnings
      libceph: define connection flag helpers
      rbd: normalize dout() calls
      rbd: barriers are hard
      rbd: ignore zero-length requests
      ...

commit ff3d0046625c1b37df37beb8477135d44dae2823
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Jan 31 04:01:53 2013 -0800

    ceph: Convert struct ceph_mds_request to use kuid_t and kgid_t
    
    Hold the uid and gid for a pending ceph mds request using the types
    kuid_t and kgid_t.  When a request message is finally created convert
    the kuid_t and kgid_t values into uids and gids in the initial user
    namespace.
    
    Cc: Sage Weil <sage@inktank.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index dd26846dd71d..ff4188bf6199 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -184,8 +184,8 @@ struct ceph_mds_request {
 
 	union ceph_mds_request_args r_args;
 	int r_fmode;        /* file mode, if expecting cap */
-	uid_t r_uid;
-	gid_t r_gid;
+	kuid_t r_uid;
+	kgid_t r_gid;
 
 	/* for choosing which mds to send this request to */
 	int r_direct_mode;

commit 6e8575faa8fa680d59404a4d58d12190667be815
Author: Sam Lang <sam.lang@inktank.com>
Date:   Fri Dec 28 09:56:46 2012 -0800

    ceph: Check for created flag in response from mds
    
    The mds now sends back a created inode if the create request
    performed the create.  If the file already existed, no inode is
    returned in the reply.  This allows ceph to set the created flag
    in atomic_open so that permissions are properly checked in the case
    that the file wasn't created by the create call to the mds.
    
    To ensure compability with previous kernels, a feature for sending
    back the inode in the create reply was added, so that the mds will
    only send back the inode if the client indicates it supports the
    feature.
    
    Signed-off-by: Sam Lang <sam.lang@inktank.com>
    Reviewed-by: Sage Weil <sage@inktank.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index dd26846dd71d..567f7c60354e 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -74,6 +74,12 @@ struct ceph_mds_reply_info_parsed {
 			struct ceph_mds_reply_info_in *dir_in;
 			u8                            dir_complete, dir_end;
 		};
+
+		/* for create results */
+		struct {
+			bool has_create_ino;
+			u64 ino;
+		};
 	};
 
 	/* encoded blob describing snapshot contexts for certain

commit 6c4a19158b96ea1fb8acbe0c1d5493d9dcd2f147
Author: Alex Elder <elder@inktank.com>
Date:   Wed May 16 15:16:38 2012 -0500

    ceph: define ceph_auth_handshake type
    
    The definitions for the ceph_mds_session and ceph_osd both contain
    five fields related only to "authorizers."  Encapsulate those fields
    into their own struct type, allowing for better isolation in some
    upcoming patches.
    
    Fix the #includes in "linux/ceph/osd_client.h" to lay out their more
    complete canonical path.
    
    Signed-off-by: Alex Elder <elder@inktank.com>
    Reviewed-by: Sage Weil <sage@inktank.com>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 8c7c04ebb595..dd26846dd71d 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -11,6 +11,7 @@
 #include <linux/ceph/types.h>
 #include <linux/ceph/messenger.h>
 #include <linux/ceph/mdsmap.h>
+#include <linux/ceph/auth.h>
 
 /*
  * Some lock dependencies:
@@ -113,9 +114,7 @@ struct ceph_mds_session {
 
 	struct ceph_connection s_con;
 
-	struct ceph_authorizer *s_authorizer;
-	void             *s_authorizer_buf, *s_authorizer_reply_buf;
-	size_t            s_authorizer_buf_len, s_authorizer_reply_buf_len;
+	struct ceph_auth_handshake s_auth;
 
 	/* protected by s_gen_ttl_lock */
 	spinlock_t        s_gen_ttl_lock;

commit d8fb02abdc39f92a1066313e2b17047876afa8f9
Author: Alex Elder <elder@dreamhost.com>
Date:   Thu Jan 12 17:48:10 2012 -0800

    ceph: create a new session lock to avoid lock inversion
    
    Lockdep was reporting a possible circular lock dependency in
    dentry_lease_is_valid().  That function needs to sample the
    session's s_cap_gen and and s_cap_ttl fields coherently, but needs
    to do so while holding a dentry lock.  The s_cap_lock field was
    being used to protect the two fields, but that can't be taken while
    holding a lock on a dentry within the session.
    
    In most cases, the s_cap_gen and s_cap_ttl fields only get operated
    on separately.  But in three cases they need to be updated together.
    Implement a new lock to protect the spots updating both fields
    atomically is required.
    
    Signed-off-by: Alex Elder <elder@dreamhost.com>
    Reviewed-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index a50ca0e39475..8c7c04ebb595 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -117,10 +117,13 @@ struct ceph_mds_session {
 	void             *s_authorizer_buf, *s_authorizer_reply_buf;
 	size_t            s_authorizer_buf_len, s_authorizer_reply_buf_len;
 
-	/* protected by s_cap_lock */
-	spinlock_t        s_cap_lock;
+	/* protected by s_gen_ttl_lock */
+	spinlock_t        s_gen_ttl_lock;
 	u32               s_cap_gen;  /* inc each time we get mds stale msg */
 	unsigned long     s_cap_ttl;  /* when session caps expire */
+
+	/* protected by s_cap_lock */
+	spinlock_t        s_cap_lock;
 	struct list_head  s_caps;     /* all caps issued by this session */
 	int               s_nr_caps, s_trim_caps;
 	int               s_num_cap_releases;

commit be655596b3de5873f994ddbe205751a5ffb4de39
Author: Sage Weil <sage@newdream.net>
Date:   Wed Nov 30 09:47:09 2011 -0800

    ceph: use i_ceph_lock instead of i_lock
    
    We have been using i_lock to protect all kinds of data structures in the
    ceph_inode_info struct, including lists of inodes that we need to iterate
    over while avoiding races with inode destruction.  That requires grabbing
    a reference to the inode with the list lock protected, but igrab() now
    takes i_lock to check the inode flags.
    
    Changing the list lock ordering would be a painful process.
    
    However, using a ceph-specific i_ceph_lock in the ceph inode instead of
    i_lock is a simple mechanical change and avoids the ordering constraints
    imposed by igrab().
    
    Reported-by: Amon Ott <a.ott@m-privacy.de>
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 4bb239921dbd..a50ca0e39475 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -20,7 +20,7 @@
  *
  *         mdsc->snap_rwsem
  *
- *         inode->i_lock
+ *         ci->i_ceph_lock
  *                 mdsc->snap_flush_lock
  *                 mdsc->cap_delay_lock
  *

commit 41b02e1f9bb87b07d792b64aaeb7af3d00d69cd2
Author: Sage Weil <sage@newdream.net>
Date:   Tue Jul 26 11:31:14 2011 -0700

    ceph: explicitly reference rename old_dentry parent dir in request
    
    We carry a pin on the parent directory for the rename source and dest
    dentries.  For the source it's r_locked_dir; we need to explicitly
    reference the old_dentry parent as well, since the dentry's d_parent may
    change between when the request was created and pinned and when it is
    freed.
    
    Reviewed-by: Yehuda Sadeh <yehuda@hq.newdream.net>
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 8a40f0636ba9..4bb239921dbd 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -171,6 +171,7 @@ struct ceph_mds_request {
 	struct inode *r_inode;              /* arg1 */
 	struct dentry *r_dentry;            /* arg1 */
 	struct dentry *r_old_dentry;        /* arg2: rename from or link from */
+	struct inode *r_old_dentry_dir;     /* arg2: old dentry's parent dir */
 	char *r_path1, *r_path2;
 	struct ceph_vino r_ino1, r_ino2;
 

commit 2f90b852e3ae73889d7f6de6ecf429b9b6a6b103
Author: Sage Weil <sage@newdream.net>
Date:   Tue Jul 26 11:28:25 2011 -0700

    ceph: ignore lease mask
    
    The lease mask is no longer used (and it changed a while back).  Instead,
    use a non-zero duration to indicate that there is a lease being issued.
    
    Reviewed-by: Yehuda Sadeh <yehuda@hq.newdream.net>
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 7d8a0d662d56..8a40f0636ba9 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -333,7 +333,7 @@ extern void ceph_mdsc_sync(struct ceph_mds_client *mdsc);
 
 extern void ceph_mdsc_lease_release(struct ceph_mds_client *mdsc,
 				    struct inode *inode,
-				    struct dentry *dn, int mask);
+				    struct dentry *dn);
 
 extern void ceph_invalidate_dir_request(struct ceph_mds_request *req);
 

commit db3540522e955c1ebb391f4f5324dff4f20ecd09
Author: Sage Weil <sage@newdream.net>
Date:   Tue May 24 11:46:31 2011 -0700

    ceph: fix cap flush race reentrancy
    
    In e9964c10 we change cap flushing to do a delicate dance because some
    inodes on the cap_dirty list could be in a migrating state (got EXPORT but
    not IMPORT) in which we couldn't actually flush and move from
    dirty->flushing, breaking the while (!empty) { process first } loop
    structure.  It worked for a single sync thread, but was not reentrant and
    triggered infinite loops when multiple syncers came along.
    
    Instead, move inodes with dirty to a separate cap_dirty_migrating list
    when in the limbo export-but-no-import state, allowing us to go back to
    the simple loop structure (which was reentrant).  This is cleaner and more
    robust.
    
    Audited the cap_dirty users and this looks fine:
    list_empty(&ci->i_dirty_item) is still a reliable indicator of whether we
    have dirty caps (which list we're on is irrelevant) and list_del_init()
    calls still do the right thing.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 4e3a9cc0bba6..7d8a0d662d56 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -278,6 +278,7 @@ struct ceph_mds_client {
 
 	u64               cap_flush_seq;
 	struct list_head  cap_dirty;        /* inodes with dirty caps */
+	struct list_head  cap_dirty_migrating; /* ...that are migration... */
 	int               num_cap_flushing; /* # caps we are flushing */
 	spinlock_t        cap_dirty_lock;   /* protects above items */
 	wait_queue_head_t cap_flushing_wq;

commit 4af25fdda6943f311a63034f80933e4d6d6e3a19
Author: Sage Weil <sage@newdream.net>
Date:   Tue Nov 2 13:41:47 2010 -0700

    ceph: drop redundant r_mds field
    
    The r_mds field is redundant, since we can find the same information at
    r_session->s_mds, and when r_session is NULL then r_mds is meaningless.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index f8f27f6eaa90..4e3a9cc0bba6 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -166,7 +166,6 @@ struct ceph_mds_request {
 	struct ceph_mds_client *r_mdsc;
 
 	int r_op;                    /* mds op code */
-	int r_mds;
 
 	/* operation on what? */
 	struct inode *r_inode;              /* arg1 */

commit 14303d20f3ae3e6ab626c77a4aac202b3bafd377
Author: Sage Weil <sage@newdream.net>
Date:   Tue Dec 14 17:37:52 2010 -0800

    ceph: implement DIRLAYOUTHASH feature to get dir layout from MDS
    
    This implements the DIRLAYOUTHASH protocol feature, which passes the dir
    layout over the wire from the MDS.  This gives the client knowledge
    of the correct hash function to use for mapping dentries among dir
    fragments.
    
    Note that if this feature is _not_ present on the client but is on the
    MDS, the client may misdirect requests.  This will result in a forward
    and degrade performance.  It may also result in inaccurate NFS filehandle
    generation, which will prevent fh resolution when the inode is not present
    in the client cache and the parent directories have been fragmented.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index aabe563b54db..f8f27f6eaa90 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -35,6 +35,7 @@ struct ceph_cap;
  */
 struct ceph_mds_reply_info_in {
 	struct ceph_mds_reply_inode *in;
+	struct ceph_dir_layout dir_layout;
 	u32 symlink_len;
 	char *symlink;
 	u32 xattr_len;

commit 25933abdd8c562182ca6dc9f8c4c2cc8265c3a80
Author: Herb Shiu <herb_shiu@tcloudcomputing.com>
Date:   Wed Dec 1 14:14:38 2010 -0800

    ceph: Handle file locks in replies from the MDS.
    
    Previously the kernel client incorrectly assumed everything was a directory.
    
    Signed-off-by: Herb Shiu <herb_shiu@tcloudcomputing.com>
    Acked-by: Greg Farnum <gregf@hq.newdream.net>
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 9341fd4f1432..aabe563b54db 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -42,26 +42,37 @@ struct ceph_mds_reply_info_in {
 };
 
 /*
- * parsed info about an mds reply, including information about the
- * target inode and/or its parent directory and dentry, and directory
- * contents (for readdir results).
+ * parsed info about an mds reply, including information about
+ * either: 1) the target inode and/or its parent directory and dentry,
+ * and directory contents (for readdir results), or
+ * 2) the file range lock info (for fcntl F_GETLK results).
  */
 struct ceph_mds_reply_info_parsed {
 	struct ceph_mds_reply_head    *head;
 
+	/* trace */
 	struct ceph_mds_reply_info_in diri, targeti;
 	struct ceph_mds_reply_dirfrag *dirfrag;
 	char                          *dname;
 	u32                           dname_len;
 	struct ceph_mds_reply_lease   *dlease;
 
-	struct ceph_mds_reply_dirfrag *dir_dir;
-	int                           dir_nr;
-	char                          **dir_dname;
-	u32                           *dir_dname_len;
-	struct ceph_mds_reply_lease   **dir_dlease;
-	struct ceph_mds_reply_info_in *dir_in;
-	u8                            dir_complete, dir_end;
+	/* extra */
+	union {
+		/* for fcntl F_GETLK results */
+		struct ceph_filelock *filelock_reply;
+
+		/* for readdir results */
+		struct {
+			struct ceph_mds_reply_dirfrag *dir_dir;
+			int                           dir_nr;
+			char                          **dir_dname;
+			u32                           *dir_dname_len;
+			struct ceph_mds_reply_lease   **dir_dlease;
+			struct ceph_mds_reply_info_in *dir_in;
+			u8                            dir_complete, dir_end;
+		};
+	};
 
 	/* encoded blob describing snapshot contexts for certain
 	   operations (e.g., open) */

commit cb4276cca4695670916a82e359f2e3776f0a9138
Author: Sage Weil <sage@newdream.net>
Date:   Mon Nov 8 07:28:52 2010 -0800

    ceph: fix uid/gid on resent mds requests
    
    MDS requests can be rebuilt and resent in non-process context, but were
    filling in uid/gid from current_fsuid/gid.  Put that information in the
    request struct on request setup.
    
    This fixes incorrect (and root) uid/gid getting set for requests that
    are forwarded between MDSs, usually due to metadata migrations.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index d66d63c72355..9341fd4f1432 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -170,6 +170,8 @@ struct ceph_mds_request {
 
 	union ceph_mds_request_args r_args;
 	int r_fmode;        /* file mode, if expecting cap */
+	uid_t r_uid;
+	gid_t r_gid;
 
 	/* for choosing which mds to send this request to */
 	int r_direct_mode;

commit 3d14c5d2b6e15c21d8e5467dc62d33127c23a644
Author: Yehuda Sadeh <yehuda@hq.newdream.net>
Date:   Tue Apr 6 15:14:15 2010 -0700

    ceph: factor out libceph from Ceph file system
    
    This factors out protocol and low-level storage parts of ceph into a
    separate libceph module living in net/ceph and include/linux/ceph.  This
    is mostly a matter of moving files around.  However, a few key pieces
    of the interface change as well:
    
     - ceph_client becomes ceph_fs_client and ceph_client, where the latter
       captures the mon and osd clients, and the fs_client gets the mds client
       and file system specific pieces.
     - Mount option parsing and debugfs setup is correspondingly broken into
       two pieces.
     - The mon client gets a generic handler callback for otherwise unknown
       messages (mds map, in this case).
     - The basic supported/required feature bits can be expanded (and are by
       ceph_fs_client).
    
    No functional change, aside from some subtle error handling cases that got
    cleaned up in the refactoring process.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index c98267ce6d2a..d66d63c72355 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -8,9 +8,9 @@
 #include <linux/rbtree.h>
 #include <linux/spinlock.h>
 
-#include "types.h"
-#include "messenger.h"
-#include "mdsmap.h"
+#include <linux/ceph/types.h>
+#include <linux/ceph/messenger.h>
+#include <linux/ceph/mdsmap.h>
 
 /*
  * Some lock dependencies:
@@ -26,7 +26,7 @@
  *
  */
 
-struct ceph_client;
+struct ceph_fs_client;
 struct ceph_cap;
 
 /*
@@ -230,7 +230,7 @@ struct ceph_mds_request {
  * mds client state
  */
 struct ceph_mds_client {
-	struct ceph_client      *client;
+	struct ceph_fs_client  *fsc;
 	struct mutex            mutex;         /* all nested structures */
 
 	struct ceph_mdsmap      *mdsmap;
@@ -289,11 +289,6 @@ struct ceph_mds_client {
 	int		caps_avail_count;    /* unused, unreserved */
 	int		caps_min_count;      /* keep at least this many
 						(unreserved) */
-
-#ifdef CONFIG_DEBUG_FS
-	struct dentry 	  *debugfs_file;
-#endif
-
 	spinlock_t	  dentry_lru_lock;
 	struct list_head  dentry_lru;
 	int		  num_dentry;
@@ -316,10 +311,9 @@ extern void ceph_put_mds_session(struct ceph_mds_session *s);
 extern int ceph_send_msg_mds(struct ceph_mds_client *mdsc,
 			     struct ceph_msg *msg, int mds);
 
-extern int ceph_mdsc_init(struct ceph_mds_client *mdsc,
-			   struct ceph_client *client);
+extern int ceph_mdsc_init(struct ceph_fs_client *fsc);
 extern void ceph_mdsc_close_sessions(struct ceph_mds_client *mdsc);
-extern void ceph_mdsc_stop(struct ceph_mds_client *mdsc);
+extern void ceph_mdsc_destroy(struct ceph_fs_client *fsc);
 
 extern void ceph_mdsc_sync(struct ceph_mds_client *mdsc);
 

commit f3c60c5918f26ea16761ddc8b12d8401a3db626b
Author: Sage Weil <sage@newdream.net>
Date:   Wed Aug 11 14:51:23 2010 -0700

    ceph: fix multiple mds session shutdown
    
    The use of a completion when waiting for session shutdown during umount is
    inappropriate, given the complexity of the condition.  For multiple MDS's,
    this resulted in the umount thread spinning, often preventing the session
    close message from being processed in some cases.
    
    Switch to a waitqueue and defined a condition helper.  This cleans things
    up nicely.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index ab7e89f5e344..c98267ce6d2a 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -234,7 +234,8 @@ struct ceph_mds_client {
 	struct mutex            mutex;         /* all nested structures */
 
 	struct ceph_mdsmap      *mdsmap;
-	struct completion       safe_umount_waiters, session_close_waiters;
+	struct completion       safe_umount_waiters;
+	wait_queue_head_t       session_close_wq;
 	struct list_head        waiting_for_map;
 
 	struct ceph_mds_session **sessions;    /* NULL for mds if no session */

commit e55b71f802fd448a79275ba7b263fe1a8639be5f
Author: Greg Farnum <gregf@hq.newdream.net>
Date:   Tue Jun 22 15:58:01 2010 -0700

    ceph: handle ESTALE properly; on receipt send to authority if it wasn't
    
    Signed-off-by: Greg Farnum <gregf@hq.newdream.net>
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index c86be30e8707..ab7e89f5e344 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -208,8 +208,8 @@ struct ceph_mds_request {
 
 	int               r_attempts;   /* resend attempts */
 	int               r_num_fwd;    /* number of forward attempts */
-	int               r_num_stale;
 	int               r_resend_mds; /* mds to resend to next, if any*/
+	u32               r_sent_on_mseq; /* cap mseq request was sent at*/
 
 	struct kref       r_kref;
 	struct list_head  r_wait;

commit 154f42c2c3c3b66a7a63dad5648e8a9860a32af9
Author: Sage Weil <sage@newdream.net>
Date:   Mon Jun 21 13:45:04 2010 -0700

    ceph: connect to export targets on cap export
    
    When we get a cap EXPORT message, make sure we are connected to all export
    targets to ensure we can handle the matching IMPORT.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 8f2126321f2d..c86be30e8707 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -364,4 +364,7 @@ extern void ceph_mdsc_lease_send_msg(struct ceph_mds_session *session,
 extern void ceph_mdsc_handle_map(struct ceph_mds_client *mdsc,
 				 struct ceph_msg *msg);
 
+extern void ceph_mdsc_open_export_target_sessions(struct ceph_mds_client *mdsc,
+					  struct ceph_mds_session *session);
+
 #endif

commit 37151668bad3fd058368752bee476f2ba3645596
Author: Yehuda Sadeh <yehuda@hq.newdream.net>
Date:   Thu Jun 17 16:16:12 2010 -0700

    ceph: do caps accounting per mds_client
    
    Caps related accounting is now being done per mds client instead
    of just being global. This prepares ground work for a later revision
    of the caps preallocated reservation list.
    
    Signed-off-by: Yehuda Sadeh <yehuda@hq.newdream.net>
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index e389902db131..8f2126321f2d 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -151,6 +151,7 @@ typedef void (*ceph_mds_request_callback_t) (struct ceph_mds_client *mdsc,
 struct ceph_mds_request {
 	u64 r_tid;                   /* transaction id */
 	struct rb_node r_node;
+	struct ceph_mds_client *r_mdsc;
 
 	int r_op;                    /* mds op code */
 	int r_mds;
@@ -267,6 +268,27 @@ struct ceph_mds_client {
 	spinlock_t        cap_dirty_lock;   /* protects above items */
 	wait_queue_head_t cap_flushing_wq;
 
+	/*
+	 * Cap reservations
+	 *
+	 * Maintain a global pool of preallocated struct ceph_caps, referenced
+	 * by struct ceph_caps_reservations.  This ensures that we preallocate
+	 * memory needed to successfully process an MDS response.  (If an MDS
+	 * sends us cap information and we fail to process it, we will have
+	 * problems due to the client and MDS being out of sync.)
+	 *
+	 * Reservations are 'owned' by a ceph_cap_reservation context.
+	 */
+	spinlock_t	caps_list_lock;
+	struct		list_head caps_list; /* unused (reserved or
+						unreserved) */
+	int		caps_total_count;    /* total caps allocated */
+	int		caps_use_count;      /* in use */
+	int		caps_reserve_count;  /* unused, reserved */
+	int		caps_avail_count;    /* unused, unreserved */
+	int		caps_min_count;      /* keep at least this many
+						(unreserved) */
+
 #ifdef CONFIG_DEBUG_FS
 	struct dentry 	  *debugfs_file;
 #endif

commit ee6b272b9c3447a78fa831e37b925aefd5587ec9
Author: Sage Weil <sage@newdream.net>
Date:   Thu Jun 10 12:55:52 2010 -0700

    ceph: drop unused argument
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 952410c60d09..e389902db131 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -324,8 +324,7 @@ static inline void ceph_mdsc_put_request(struct ceph_mds_request *req)
 }
 
 extern int ceph_add_cap_releases(struct ceph_mds_client *mdsc,
-				 struct ceph_mds_session *session,
-				 int extra);
+				 struct ceph_mds_session *session);
 extern void ceph_send_cap_releases(struct ceph_mds_client *mdsc,
 				   struct ceph_mds_session *session);
 

commit e979cf50395e24c4bdd489f60e2d5dd5ae66d255
Author: Sage Weil <sage@newdream.net>
Date:   Thu Jul 15 14:58:39 2010 -0700

    ceph: do not include cap/dentry releases in replayed messages
    
    Strip the cap and dentry releases from replayed messages.  They can
    cause the shared state to get out of sync because they were generated
    (with the request message) earlier, and no longer reflect the current
    client state.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index b292fa42a66d..952410c60d09 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -188,6 +188,7 @@ struct ceph_mds_request {
 	int r_old_inode_drop, r_old_inode_unless;
 
 	struct ceph_msg  *r_request;  /* original request */
+	int r_request_release_offset;
 	struct ceph_msg  *r_reply;
 	struct ceph_mds_reply_info_parsed r_reply_info;
 	int r_err;

commit 2b2300d62ea413bec631d5b880effa2cc5363acb
Author: Sage Weil <sage@newdream.net>
Date:   Wed Jun 9 16:52:04 2010 -0700

    ceph: try to send partial cap release on cap message on missing inode
    
    If we have enough memory to allocate a new cap release message, do so, so
    that we can send a partial release message immediately.  This keeps us from
    making the MDS wait when the cap release it needs is in a partially full
    release message.
    
    If we fail because of ENOMEM, oh well, they'll just have to wait a bit
    longer.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index e43752b52635..b292fa42a66d 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -322,6 +322,9 @@ static inline void ceph_mdsc_put_request(struct ceph_mds_request *req)
 	kref_put(&req->r_kref, ceph_mdsc_release_request);
 }
 
+extern int ceph_add_cap_releases(struct ceph_mds_client *mdsc,
+				 struct ceph_mds_session *session,
+				 int extra);
 extern void ceph_send_cap_releases(struct ceph_mds_client *mdsc,
 				   struct ceph_mds_session *session);
 

commit 3d7ded4d81d807c2f75f310a8d74a5d72be13a1b
Author: Sage Weil <sage@newdream.net>
Date:   Wed Jun 9 16:47:10 2010 -0700

    ceph: release cap on import if we don't have the inode
    
    If we get an IMPORT that give us a cap, but we don't have the inode, queue
    a release (and try to send it immediately) so that the MDS doesn't get
    stuck waiting for us.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index d9936c4f1212..e43752b52635 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -322,6 +322,9 @@ static inline void ceph_mdsc_put_request(struct ceph_mds_request *req)
 	kref_put(&req->r_kref, ceph_mdsc_release_request);
 }
 
+extern void ceph_send_cap_releases(struct ceph_mds_client *mdsc,
+				   struct ceph_mds_session *session);
+
 extern void ceph_mdsc_pre_umount(struct ceph_mds_client *mdsc);
 
 extern char *ceph_mdsc_build_path(struct dentry *dentry, int *plen, u64 *base,

commit 167c9e352deb7e25568c926c49c3eafad69cbe76
Author: Sage Weil <sage@newdream.net>
Date:   Fri May 14 10:02:57 2010 -0700

    ceph: use common helper for aborted dir request invalidation
    
    We invalidate I_COMPLETE and dentry leases in two places: on aborted mds
    request and on request replay.  Use common helper to avoid duplicate code.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 141a265bda75..d9936c4f1212 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -303,6 +303,8 @@ extern void ceph_mdsc_lease_release(struct ceph_mds_client *mdsc,
 				    struct inode *inode,
 				    struct dentry *dn, int mask);
 
+extern void ceph_invalidate_dir_request(struct ceph_mds_request *req);
+
 extern struct ceph_mds_request *
 ceph_mdsc_create_request(struct ceph_mds_client *mdsc, int op, int mode);
 extern void ceph_mdsc_submit_request(struct ceph_mds_client *mdsc,

commit b4556396fac5b3f063d5b8ac54dc02f7612a75e1
Author: Sage Weil <sage@newdream.net>
Date:   Thu May 13 12:01:13 2010 -0700

    ceph: fix race between aborted requests and fill_trace
    
    When we abort requests we need to prevent fill_trace et al from doing
    anything that relies on locks held by the VFS caller.  This fixes a race
    between the reply handler and the abort code, ensuring that continue
    holding the dir mutex until the reply handler completes.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 0b1dd10be39a..141a265bda75 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -165,6 +165,8 @@ struct ceph_mds_request {
 	struct inode *r_locked_dir; /* dir (if any) i_mutex locked by vfs */
 	struct inode *r_target_inode;       /* resulting inode */
 
+	struct mutex r_fill_mutex;
+
 	union ceph_mds_request_args r_args;
 	int r_fmode;        /* file mode, if expecting cap */
 

commit e1518c7c0a67a75727f7285780dbef0ca7121cc9
Author: Sage Weil <sage@newdream.net>
Date:   Thu May 13 11:19:06 2010 -0700

    ceph: clean up mds reply, error handling
    
    We would occasionally BUG out in the reply handler because r_reply was
    nonzero, due to a race with ceph_mdsc_do_request temporarily setting
    r_reply to an ERR_PTR value.  This is unnecessary, messy, and also wrong
    in the EIO case.
    
    Clean up by consistently using r_err for errors and r_reply for messages.
    Also fix the abort logic to trigger consistently for all errors that return
    to the caller early (e.g., EIO from timeout case).  If an abort races with
    a reply, use the result from the reply.
    
    Also fix locking for r_err, r_reply update in the reply handler.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 961cc6f65878..0b1dd10be39a 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -213,7 +213,7 @@ struct ceph_mds_request {
 	struct completion r_safe_completion;
 	ceph_mds_request_callback_t r_callback;
 	struct list_head  r_unsafe_item;  /* per-session unsafe list item */
-	bool		  r_got_unsafe, r_got_safe;
+	bool		  r_got_unsafe, r_got_safe, r_got_result;
 
 	bool              r_did_prepopulate;
 	u32               r_readdir_offset;

commit 7c1332b8cb5b27656cf6ab1f5fe808a8eb8bb2c0
Author: Sage Weil <sage@newdream.net>
Date:   Tue Feb 16 11:39:45 2010 -0800

    ceph: fix iterate_caps removal race
    
    We need to be able to iterate over all caps on a session with a
    possibly slow callback on each cap.  To allow this, we used to
    prevent cap reordering while we were iterating.  However, we were
    not safe from races with removal: removing the 'next' cap would
    make the next pointer from list_for_each_entry_safe be invalid,
    and cause a lock up or similar badness.
    
    Instead, we keep an iterator pointer in the session pointing to
    the current cap.  As before, we avoid reordering.  For removal,
    if the cap isn't the current cap we are iterating over, we are
    fine.  If it is, we clear cap->ci (to mark the cap as pending
    removal) but leave it in the session list.  In iterate_caps, we
    can safely finish removal and get the next cap pointer.
    
    While we're at it, clean up put_cap to not take a cap reservation
    context, as it was never used.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 9d6b90173879..961cc6f65878 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -114,7 +114,7 @@ struct ceph_mds_session {
 	int               s_num_cap_releases;
 	struct list_head  s_cap_releases; /* waiting cap_release messages */
 	struct list_head  s_cap_releases_done; /* ready to send */
-	bool              s_iterating_caps;
+	struct ceph_cap  *s_cap_iterator;
 
 	/* protected by mutex */
 	struct list_head  s_cap_flushing;     /* inodes w/ flushing caps */

commit a105f00cf17d711e876b3dc67e15f9a89b7de5a3
Author: Sage Weil <sage@newdream.net>
Date:   Mon Feb 15 14:37:55 2010 -0800

    ceph: use rbtree for snap_realms
    
    Switch from radix tree to rbtree for snap realms.  This is much more
    appropriate given that realm keys are few and far between.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 98f09cd06006..9d6b90173879 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -5,7 +5,6 @@
 #include <linux/kref.h>
 #include <linux/list.h>
 #include <linux/mutex.h>
-#include <linux/radix-tree.h>
 #include <linux/rbtree.h>
 #include <linux/spinlock.h>
 
@@ -246,7 +245,7 @@ struct ceph_mds_client {
 	 * should be destroyed.
 	 */
 	struct rw_semaphore     snap_rwsem;
-	struct radix_tree_root  snap_realms;
+	struct rb_root          snap_realms;
 	struct list_head        snap_empty;
 	spinlock_t              snap_empty_lock;  /* protect snap_empty */
 

commit 44ca18f2682eb1cfbed153849adedb79e3e19790
Author: Sage Weil <sage@newdream.net>
Date:   Mon Feb 15 12:08:46 2010 -0800

    ceph: use rbtree for mds requests
    
    The rbtree is a more appropriate data structure than a radix_tree.  It
    avoids extra memory usage and simplifies the code.
    
    It also fixes a bug where the debugfs 'mdsc' file wasn't including the
    most recent mds request.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index ee71495e27c4..98f09cd06006 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -6,6 +6,7 @@
 #include <linux/list.h>
 #include <linux/mutex.h>
 #include <linux/radix-tree.h>
+#include <linux/rbtree.h>
 #include <linux/spinlock.h>
 
 #include "types.h"
@@ -150,6 +151,7 @@ typedef void (*ceph_mds_request_callback_t) (struct ceph_mds_client *mdsc,
  */
 struct ceph_mds_request {
 	u64 r_tid;                   /* transaction id */
+	struct rb_node r_node;
 
 	int r_op;                    /* mds op code */
 	int r_mds;
@@ -249,7 +251,7 @@ struct ceph_mds_client {
 	spinlock_t              snap_empty_lock;  /* protect snap_empty */
 
 	u64                    last_tid;      /* most recent mds request */
-	struct radix_tree_root request_tree;  /* pending mds requests */
+	struct rb_root         request_tree;  /* pending mds requests */
 	struct delayed_work    delayed_work;  /* delayed work */
 	unsigned long    last_renew_caps;  /* last time we renewed our caps */
 	struct list_head cap_delay_list;   /* caps with delayed release */

commit 5b1daecd59f95eb24dc629407ed80369c9929520
Author: Sage Weil <sage@newdream.net>
Date:   Mon Jan 25 11:33:08 2010 -0800

    ceph: properly handle aborted mds requests
    
    Previously, if the MDS request was interrupted, we would unregister the
    request and ignore any reply.  This could cause the caps or other cache
    state to become out of sync.  (For instance, aborting dbench and doing
    rm -r on clients would complain about a non-empty directory because the
    client didn't realize it's aborted file create request completed.)
    
    Even we don't unregister, we still can't process the reply normally because
    we are no longer holding the caller's locks (like the dir i_mutex).
    
    So, mark aborted operations with r_aborted, and in the reply handler, be
    sure to process all the caps.  Do not process the namespace changes,
    though, since we no longer will hold the dir i_mutex.  The dentry lease
    state can also be ignored as it's more forgiving.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index b1c2025227c5..ee71495e27c4 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -188,6 +188,7 @@ struct ceph_mds_request {
 	struct ceph_msg  *r_reply;
 	struct ceph_mds_reply_info_parsed r_reply_info;
 	int r_err;
+	bool r_aborted;
 
 	unsigned long r_timeout;  /* optional.  jiffies */
 	unsigned long r_started;  /* start time to measure timeout against */

commit 5dacf09121ffb2e5fc7d15b78cae0b77042a1935
Author: Sage Weil <sage@newdream.net>
Date:   Mon Dec 21 20:40:34 2009 -0800

    ceph: do not touch_caps while iterating over caps list
    
    Avoid confusing iterate_session_caps(), flag the session while we are
    iterating so that __touch_cap does not rearrange items on the list.
    
    All other modifiers of session->s_caps do so under the protection of
    s_mutex.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 41af5ca316e6..b1c2025227c5 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -114,6 +114,7 @@ struct ceph_mds_session {
 	int               s_num_cap_releases;
 	struct list_head  s_cap_releases; /* waiting cap_release messages */
 	struct list_head  s_cap_releases_done; /* ready to send */
+	bool              s_iterating_caps;
 
 	/* protected by mutex */
 	struct list_head  s_cap_flushing;     /* inodes w/ flushing caps */

commit 153c8e6bf7ffee561e046e60b26ef6486c6fc9f2
Author: Sage Weil <sage@newdream.net>
Date:   Mon Dec 7 12:31:09 2009 -0800

    ceph: use kref for struct ceph_mds_request
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 9faa1b2f79a7..41af5ca316e6 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -2,6 +2,7 @@
 #define _FS_CEPH_MDS_CLIENT_H
 
 #include <linux/completion.h>
+#include <linux/kref.h>
 #include <linux/list.h>
 #include <linux/mutex.h>
 #include <linux/radix-tree.h>
@@ -203,7 +204,7 @@ struct ceph_mds_request {
 	int               r_num_stale;
 	int               r_resend_mds; /* mds to resend to next, if any*/
 
-	atomic_t          r_ref;
+	struct kref       r_kref;
 	struct list_head  r_wait;
 	struct completion r_completion;
 	struct completion r_safe_completion;
@@ -306,9 +307,13 @@ extern int ceph_mdsc_do_request(struct ceph_mds_client *mdsc,
 				struct ceph_mds_request *req);
 static inline void ceph_mdsc_get_request(struct ceph_mds_request *req)
 {
-	atomic_inc(&req->r_ref);
+	kref_get(&req->r_kref);
+}
+extern void ceph_mdsc_release_request(struct kref *kref);
+static inline void ceph_mdsc_put_request(struct ceph_mds_request *req)
+{
+	kref_put(&req->r_kref, ceph_mdsc_release_request);
 }
-extern void ceph_mdsc_put_request(struct ceph_mds_request *req);
 
 extern void ceph_mdsc_pre_umount(struct ceph_mds_client *mdsc);
 

commit 4e7a5dcd1bbab6560fbc8ada29a840e7a20ed7bc
Author: Sage Weil <sage@newdream.net>
Date:   Wed Nov 18 16:19:57 2009 -0800

    ceph: negotiate authentication protocol; implement AUTH_NONE protocol
    
    When we open a monitor session, we send an initial AUTH message listing
    the auth protocols we support, our entity name, and (possibly) a previously
    assigned global_id.  The monitor chooses a protocol and responds with an
    initial message.
    
    Initially implement AUTH_NONE, a dummy protocol that provides no security,
    but works within the new framework.  It generates 'authorizers' that are
    used when connecting to (mds, osd) services that simply state our entity
    name and global_id.
    
    This is a wire protocol change.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 7c439488cfab..9faa1b2f79a7 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -100,6 +100,10 @@ struct ceph_mds_session {
 
 	struct ceph_connection s_con;
 
+	struct ceph_authorizer *s_authorizer;
+	void             *s_authorizer_buf, *s_authorizer_reply_buf;
+	size_t            s_authorizer_buf_len, s_authorizer_reply_buf_len;
+
 	/* protected by s_cap_lock */
 	spinlock_t        s_cap_lock;
 	u32               s_cap_gen;  /* inc each time we get mds stale msg */

commit 5f44f142601bf94c448e2d463f0f18fd159da164
Author: Sage Weil <sage@newdream.net>
Date:   Wed Nov 18 14:52:18 2009 -0800

    ceph: handle errors during osd client init
    
    Unwind initializing if we get ENOMEM during client initialization.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index 0751b821f231..7c439488cfab 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -282,7 +282,7 @@ extern void ceph_put_mds_session(struct ceph_mds_session *s);
 extern int ceph_send_msg_mds(struct ceph_mds_client *mdsc,
 			     struct ceph_msg *msg, int mds);
 
-extern void ceph_mdsc_init(struct ceph_mds_client *mdsc,
+extern int ceph_mdsc_init(struct ceph_mds_client *mdsc,
 			   struct ceph_client *client);
 extern void ceph_mdsc_close_sessions(struct ceph_mds_client *mdsc);
 extern void ceph_mdsc_stop(struct ceph_mds_client *mdsc);

commit 039934b895c89c2bb40aa5132efe00e60b70efca
Author: Sage Weil <sage@newdream.net>
Date:   Thu Nov 12 15:05:52 2009 -0800

    ceph: build cleanly without CONFIG_DEBUG_FS
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index f566e9c84295..0751b821f231 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -256,7 +256,9 @@ struct ceph_mds_client {
 	spinlock_t        cap_dirty_lock;   /* protects above items */
 	wait_queue_head_t cap_flushing_wq;
 
+#ifdef CONFIG_DEBUG_FS
 	struct dentry 	  *debugfs_file;
+#endif
 
 	spinlock_t	  dentry_lru_lock;
 	struct list_head  dentry_lru;

commit cdac830313fa6bf2831693af80fefe4aaac11b7d
Author: Sage Weil <sage@newdream.net>
Date:   Tue Nov 10 16:02:23 2009 -0800

    ceph: remove recon_gen logic
    
    We don't get an explicit affirmative confirmation that our caps reconnect,
    nor do we necessarily want to pay that cost.  So, take all this code out
    for now.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index c0846b1c482b..f566e9c84295 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -98,8 +98,6 @@ struct ceph_mds_session {
 	u64               s_seq;      /* incoming msg seq # */
 	struct mutex      s_mutex;    /* serialize session messages */
 
-	int               s_recon_gen; /* inc on reconnect to recovered mds */
-
 	struct ceph_connection s_con;
 
 	/* protected by s_cap_lock */

commit 685f9a5d14194fc35db73e5e7370740ccc14b64a
Author: Sage Weil <sage@newdream.net>
Date:   Mon Nov 9 12:05:48 2009 -0800

    ceph: do not confuse stale and dead (unreconnected) caps
    
    We were using the cap_gen to track both stale caps (caps that timed out
    due to temporarily losing touch with the mds) and dead caps that did not
    reconnect after an MDS failure.  Introduce a recon_gen counter to track
    reconnections to restarted MDSs and kill dead caps based on that instead.
    
    Rename gen to cap_gen while we're at it to make it more clear which is
    which.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
index f566e9c84295..c0846b1c482b 100644
--- a/fs/ceph/mds_client.h
+++ b/fs/ceph/mds_client.h
@@ -98,6 +98,8 @@ struct ceph_mds_session {
 	u64               s_seq;      /* incoming msg seq # */
 	struct mutex      s_mutex;    /* serialize session messages */
 
+	int               s_recon_gen; /* inc on reconnect to recovered mds */
+
 	struct ceph_connection s_con;
 
 	/* protected by s_cap_lock */

commit 2f2dc053404febedc9c273452d9d518fb31fde72
Author: Sage Weil <sage@newdream.net>
Date:   Tue Oct 6 11:31:09 2009 -0700

    ceph: MDS client
    
    The MDS (metadata server) client is responsible for submitting
    requests to the MDS cluster and parsing the response.  We decide which
    MDS to submit each request to based on cached information about the
    current partition of the directory hierarchy across the cluster.  A
    stateful session is opened with each MDS before we submit requests to
    it, and a mutex is used to control the ordering of messages within
    each session.
    
    An MDS request may generate two responses.  The first indicates the
    operation was a success and returns any result.  A second reply is
    sent when the operation commits to disk.  Note that locking on the MDS
    ensures that the results of updates are visible only to the updating
    client before the operation commits.  Requests are linked to the
    containing directory so that an fsync will wait for them to commit.
    
    If an MDS fails and/or recovers, we resubmit requests as needed.  We
    also reconnect existing capabilities to a recovering MDS to
    reestablish that shared session state.  Old dentry leases are
    invalidated.
    
    Signed-off-by: Sage Weil <sage@newdream.net>

diff --git a/fs/ceph/mds_client.h b/fs/ceph/mds_client.h
new file mode 100644
index 000000000000..f566e9c84295
--- /dev/null
+++ b/fs/ceph/mds_client.h
@@ -0,0 +1,321 @@
+#ifndef _FS_CEPH_MDS_CLIENT_H
+#define _FS_CEPH_MDS_CLIENT_H
+
+#include <linux/completion.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/radix-tree.h>
+#include <linux/spinlock.h>
+
+#include "types.h"
+#include "messenger.h"
+#include "mdsmap.h"
+
+/*
+ * Some lock dependencies:
+ *
+ * session->s_mutex
+ *         mdsc->mutex
+ *
+ *         mdsc->snap_rwsem
+ *
+ *         inode->i_lock
+ *                 mdsc->snap_flush_lock
+ *                 mdsc->cap_delay_lock
+ *
+ */
+
+struct ceph_client;
+struct ceph_cap;
+
+/*
+ * parsed info about a single inode.  pointers are into the encoded
+ * on-wire structures within the mds reply message payload.
+ */
+struct ceph_mds_reply_info_in {
+	struct ceph_mds_reply_inode *in;
+	u32 symlink_len;
+	char *symlink;
+	u32 xattr_len;
+	char *xattr_data;
+};
+
+/*
+ * parsed info about an mds reply, including information about the
+ * target inode and/or its parent directory and dentry, and directory
+ * contents (for readdir results).
+ */
+struct ceph_mds_reply_info_parsed {
+	struct ceph_mds_reply_head    *head;
+
+	struct ceph_mds_reply_info_in diri, targeti;
+	struct ceph_mds_reply_dirfrag *dirfrag;
+	char                          *dname;
+	u32                           dname_len;
+	struct ceph_mds_reply_lease   *dlease;
+
+	struct ceph_mds_reply_dirfrag *dir_dir;
+	int                           dir_nr;
+	char                          **dir_dname;
+	u32                           *dir_dname_len;
+	struct ceph_mds_reply_lease   **dir_dlease;
+	struct ceph_mds_reply_info_in *dir_in;
+	u8                            dir_complete, dir_end;
+
+	/* encoded blob describing snapshot contexts for certain
+	   operations (e.g., open) */
+	void *snapblob;
+	int snapblob_len;
+};
+
+
+/*
+ * cap releases are batched and sent to the MDS en masse.
+ */
+#define CEPH_CAPS_PER_RELEASE ((PAGE_CACHE_SIZE -			\
+				sizeof(struct ceph_mds_cap_release)) /	\
+			       sizeof(struct ceph_mds_cap_item))
+
+
+/*
+ * state associated with each MDS<->client session
+ */
+enum {
+	CEPH_MDS_SESSION_NEW = 1,
+	CEPH_MDS_SESSION_OPENING = 2,
+	CEPH_MDS_SESSION_OPEN = 3,
+	CEPH_MDS_SESSION_HUNG = 4,
+	CEPH_MDS_SESSION_CLOSING = 5,
+	CEPH_MDS_SESSION_RESTARTING = 6,
+	CEPH_MDS_SESSION_RECONNECTING = 7,
+};
+
+struct ceph_mds_session {
+	struct ceph_mds_client *s_mdsc;
+	int               s_mds;
+	int               s_state;
+	unsigned long     s_ttl;      /* time until mds kills us */
+	u64               s_seq;      /* incoming msg seq # */
+	struct mutex      s_mutex;    /* serialize session messages */
+
+	struct ceph_connection s_con;
+
+	/* protected by s_cap_lock */
+	spinlock_t        s_cap_lock;
+	u32               s_cap_gen;  /* inc each time we get mds stale msg */
+	unsigned long     s_cap_ttl;  /* when session caps expire */
+	struct list_head  s_caps;     /* all caps issued by this session */
+	int               s_nr_caps, s_trim_caps;
+	int               s_num_cap_releases;
+	struct list_head  s_cap_releases; /* waiting cap_release messages */
+	struct list_head  s_cap_releases_done; /* ready to send */
+
+	/* protected by mutex */
+	struct list_head  s_cap_flushing;     /* inodes w/ flushing caps */
+	struct list_head  s_cap_snaps_flushing;
+	unsigned long     s_renew_requested; /* last time we sent a renew req */
+	u64               s_renew_seq;
+
+	atomic_t          s_ref;
+	struct list_head  s_waiting;  /* waiting requests */
+	struct list_head  s_unsafe;   /* unsafe requests */
+};
+
+/*
+ * modes of choosing which MDS to send a request to
+ */
+enum {
+	USE_ANY_MDS,
+	USE_RANDOM_MDS,
+	USE_AUTH_MDS,   /* prefer authoritative mds for this metadata item */
+};
+
+struct ceph_mds_request;
+struct ceph_mds_client;
+
+/*
+ * request completion callback
+ */
+typedef void (*ceph_mds_request_callback_t) (struct ceph_mds_client *mdsc,
+					     struct ceph_mds_request *req);
+
+/*
+ * an in-flight mds request
+ */
+struct ceph_mds_request {
+	u64 r_tid;                   /* transaction id */
+
+	int r_op;                    /* mds op code */
+	int r_mds;
+
+	/* operation on what? */
+	struct inode *r_inode;              /* arg1 */
+	struct dentry *r_dentry;            /* arg1 */
+	struct dentry *r_old_dentry;        /* arg2: rename from or link from */
+	char *r_path1, *r_path2;
+	struct ceph_vino r_ino1, r_ino2;
+
+	struct inode *r_locked_dir; /* dir (if any) i_mutex locked by vfs */
+	struct inode *r_target_inode;       /* resulting inode */
+
+	union ceph_mds_request_args r_args;
+	int r_fmode;        /* file mode, if expecting cap */
+
+	/* for choosing which mds to send this request to */
+	int r_direct_mode;
+	u32 r_direct_hash;      /* choose dir frag based on this dentry hash */
+	bool r_direct_is_hash;  /* true if r_direct_hash is valid */
+
+	/* data payload is used for xattr ops */
+	struct page **r_pages;
+	int r_num_pages;
+	int r_data_len;
+
+	/* what caps shall we drop? */
+	int r_inode_drop, r_inode_unless;
+	int r_dentry_drop, r_dentry_unless;
+	int r_old_dentry_drop, r_old_dentry_unless;
+	struct inode *r_old_inode;
+	int r_old_inode_drop, r_old_inode_unless;
+
+	struct ceph_msg  *r_request;  /* original request */
+	struct ceph_msg  *r_reply;
+	struct ceph_mds_reply_info_parsed r_reply_info;
+	int r_err;
+
+	unsigned long r_timeout;  /* optional.  jiffies */
+	unsigned long r_started;  /* start time to measure timeout against */
+	unsigned long r_request_started; /* start time for mds request only,
+					    used to measure lease durations */
+
+	/* link unsafe requests to parent directory, for fsync */
+	struct inode	*r_unsafe_dir;
+	struct list_head r_unsafe_dir_item;
+
+	struct ceph_mds_session *r_session;
+
+	int               r_attempts;   /* resend attempts */
+	int               r_num_fwd;    /* number of forward attempts */
+	int               r_num_stale;
+	int               r_resend_mds; /* mds to resend to next, if any*/
+
+	atomic_t          r_ref;
+	struct list_head  r_wait;
+	struct completion r_completion;
+	struct completion r_safe_completion;
+	ceph_mds_request_callback_t r_callback;
+	struct list_head  r_unsafe_item;  /* per-session unsafe list item */
+	bool		  r_got_unsafe, r_got_safe;
+
+	bool              r_did_prepopulate;
+	u32               r_readdir_offset;
+
+	struct ceph_cap_reservation r_caps_reservation;
+	int r_num_caps;
+};
+
+/*
+ * mds client state
+ */
+struct ceph_mds_client {
+	struct ceph_client      *client;
+	struct mutex            mutex;         /* all nested structures */
+
+	struct ceph_mdsmap      *mdsmap;
+	struct completion       safe_umount_waiters, session_close_waiters;
+	struct list_head        waiting_for_map;
+
+	struct ceph_mds_session **sessions;    /* NULL for mds if no session */
+	int                     max_sessions;  /* len of s_mds_sessions */
+	int                     stopping;      /* true if shutting down */
+
+	/*
+	 * snap_rwsem will cover cap linkage into snaprealms, and
+	 * realm snap contexts.  (later, we can do per-realm snap
+	 * contexts locks..)  the empty list contains realms with no
+	 * references (implying they contain no inodes with caps) that
+	 * should be destroyed.
+	 */
+	struct rw_semaphore     snap_rwsem;
+	struct radix_tree_root  snap_realms;
+	struct list_head        snap_empty;
+	spinlock_t              snap_empty_lock;  /* protect snap_empty */
+
+	u64                    last_tid;      /* most recent mds request */
+	struct radix_tree_root request_tree;  /* pending mds requests */
+	struct delayed_work    delayed_work;  /* delayed work */
+	unsigned long    last_renew_caps;  /* last time we renewed our caps */
+	struct list_head cap_delay_list;   /* caps with delayed release */
+	spinlock_t       cap_delay_lock;   /* protects cap_delay_list */
+	struct list_head snap_flush_list;  /* cap_snaps ready to flush */
+	spinlock_t       snap_flush_lock;
+
+	u64               cap_flush_seq;
+	struct list_head  cap_dirty;        /* inodes with dirty caps */
+	int               num_cap_flushing; /* # caps we are flushing */
+	spinlock_t        cap_dirty_lock;   /* protects above items */
+	wait_queue_head_t cap_flushing_wq;
+
+	struct dentry 	  *debugfs_file;
+
+	spinlock_t	  dentry_lru_lock;
+	struct list_head  dentry_lru;
+	int		  num_dentry;
+};
+
+extern const char *ceph_mds_op_name(int op);
+
+extern struct ceph_mds_session *
+__ceph_lookup_mds_session(struct ceph_mds_client *, int mds);
+
+static inline struct ceph_mds_session *
+ceph_get_mds_session(struct ceph_mds_session *s)
+{
+	atomic_inc(&s->s_ref);
+	return s;
+}
+
+extern void ceph_put_mds_session(struct ceph_mds_session *s);
+
+extern int ceph_send_msg_mds(struct ceph_mds_client *mdsc,
+			     struct ceph_msg *msg, int mds);
+
+extern void ceph_mdsc_init(struct ceph_mds_client *mdsc,
+			   struct ceph_client *client);
+extern void ceph_mdsc_close_sessions(struct ceph_mds_client *mdsc);
+extern void ceph_mdsc_stop(struct ceph_mds_client *mdsc);
+
+extern void ceph_mdsc_sync(struct ceph_mds_client *mdsc);
+
+extern void ceph_mdsc_lease_release(struct ceph_mds_client *mdsc,
+				    struct inode *inode,
+				    struct dentry *dn, int mask);
+
+extern struct ceph_mds_request *
+ceph_mdsc_create_request(struct ceph_mds_client *mdsc, int op, int mode);
+extern void ceph_mdsc_submit_request(struct ceph_mds_client *mdsc,
+				     struct ceph_mds_request *req);
+extern int ceph_mdsc_do_request(struct ceph_mds_client *mdsc,
+				struct inode *dir,
+				struct ceph_mds_request *req);
+static inline void ceph_mdsc_get_request(struct ceph_mds_request *req)
+{
+	atomic_inc(&req->r_ref);
+}
+extern void ceph_mdsc_put_request(struct ceph_mds_request *req);
+
+extern void ceph_mdsc_pre_umount(struct ceph_mds_client *mdsc);
+
+extern char *ceph_mdsc_build_path(struct dentry *dentry, int *plen, u64 *base,
+				  int stop_on_nosnap);
+
+extern void __ceph_mdsc_drop_dentry_lease(struct dentry *dentry);
+extern void ceph_mdsc_lease_send_msg(struct ceph_mds_session *session,
+				     struct inode *inode,
+				     struct dentry *dentry, char action,
+				     u32 seq);
+
+extern void ceph_mdsc_handle_map(struct ceph_mds_client *mdsc,
+				 struct ceph_msg *msg);
+
+#endif
