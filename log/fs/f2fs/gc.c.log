commit 9c1223845a37ce09fd498b8c8ed061decff20eda
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Apr 23 18:03:06 2020 +0800

    f2fs: add compressed/gc data read IO stat
    
    in order to account data read IOs more accurately.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index f3c45ec2a7e7..5b95d5a146eb 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -740,6 +740,7 @@ static int ra_data_block(struct inode *inode, pgoff_t index)
 	f2fs_put_page(page, 1);
 
 	f2fs_update_iostat(sbi, FS_DATA_READ_IO, F2FS_BLKSIZE);
+	f2fs_update_iostat(sbi, FS_GDATA_READ_IO, F2FS_BLKSIZE);
 
 	return 0;
 put_encrypted_page:
@@ -846,6 +847,7 @@ static int move_data_block(struct inode *inode, block_t bidx,
 		}
 
 		f2fs_update_iostat(fio.sbi, FS_DATA_READ_IO, F2FS_BLKSIZE);
+		f2fs_update_iostat(fio.sbi, FS_GDATA_READ_IO, F2FS_BLKSIZE);
 
 		lock_page(mpage);
 		if (unlikely(mpage->mapping != META_MAPPING(fio.sbi) ||

commit b4b10061ef98c583bcf82a4200703fbaa98c18dc
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Mar 31 11:43:07 2020 -0700

    f2fs: refactor resize_fs to avoid meta updates in progress
    
    Sahitya raised an issue:
    - prevent meta updates while checkpoint is in progress
    
    allocate_segment_for_resize() can cause metapage updates if
    it requires to change the current node/data segments for resizing.
    Stop these meta updates when there is a checkpoint already
    in progress to prevent inconsistent CP data.
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 28a8c79c8bdc..f3c45ec2a7e7 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -13,6 +13,7 @@
 #include <linux/kthread.h>
 #include <linux/delay.h>
 #include <linux/freezer.h>
+#include <linux/sched/signal.h>
 
 #include "f2fs.h"
 #include "node.h"
@@ -1405,12 +1406,29 @@ void f2fs_build_gc_manager(struct f2fs_sb_info *sbi)
 				GET_SEGNO(sbi, FDEV(0).end_blk) + 1;
 }
 
-static int free_segment_range(struct f2fs_sb_info *sbi, unsigned int start,
-							unsigned int end)
+static int free_segment_range(struct f2fs_sb_info *sbi,
+				unsigned int secs, bool gc_only)
 {
-	int type;
-	unsigned int segno, next_inuse;
+	unsigned int segno, next_inuse, start, end;
+	struct cp_control cpc = { CP_RESIZE, 0, 0, 0 };
+	int gc_mode, gc_type;
 	int err = 0;
+	int type;
+
+	/* Force block allocation for GC */
+	MAIN_SECS(sbi) -= secs;
+	start = MAIN_SECS(sbi) * sbi->segs_per_sec;
+	end = MAIN_SEGS(sbi) - 1;
+
+	mutex_lock(&DIRTY_I(sbi)->seglist_lock);
+	for (gc_mode = 0; gc_mode < MAX_GC_POLICY; gc_mode++)
+		if (SIT_I(sbi)->last_victim[gc_mode] >= start)
+			SIT_I(sbi)->last_victim[gc_mode] = 0;
+
+	for (gc_type = BG_GC; gc_type <= FG_GC; gc_type++)
+		if (sbi->next_victim_seg[gc_type] >= start)
+			sbi->next_victim_seg[gc_type] = NULL_SEGNO;
+	mutex_unlock(&DIRTY_I(sbi)->seglist_lock);
 
 	/* Move out cursegs from the target range */
 	for (type = CURSEG_HOT_DATA; type < NR_CURSEG_TYPE; type++)
@@ -1423,18 +1441,24 @@ static int free_segment_range(struct f2fs_sb_info *sbi, unsigned int start,
 			.iroot = RADIX_TREE_INIT(gc_list.iroot, GFP_NOFS),
 		};
 
-		down_write(&sbi->gc_lock);
 		do_garbage_collect(sbi, segno, &gc_list, FG_GC);
-		up_write(&sbi->gc_lock);
 		put_gc_inode(&gc_list);
 
-		if (get_valid_blocks(sbi, segno, true))
-			return -EAGAIN;
+		if (!gc_only && get_valid_blocks(sbi, segno, true)) {
+			err = -EAGAIN;
+			goto out;
+		}
+		if (fatal_signal_pending(current)) {
+			err = -ERESTARTSYS;
+			goto out;
+		}
 	}
+	if (gc_only)
+		goto out;
 
-	err = f2fs_sync_fs(sbi->sb, 1);
+	err = f2fs_write_checkpoint(sbi, &cpc);
 	if (err)
-		return err;
+		goto out;
 
 	next_inuse = find_next_inuse(FREE_I(sbi), end + 1, start);
 	if (next_inuse <= end) {
@@ -1442,6 +1466,8 @@ static int free_segment_range(struct f2fs_sb_info *sbi, unsigned int start,
 			 next_inuse);
 		f2fs_bug_on(sbi, 1);
 	}
+out:
+	MAIN_SECS(sbi) += secs;
 	return err;
 }
 
@@ -1487,6 +1513,7 @@ static void update_fs_metadata(struct f2fs_sb_info *sbi, int secs)
 
 	SM_I(sbi)->segment_count = (int)SM_I(sbi)->segment_count + segs;
 	MAIN_SEGS(sbi) = (int)MAIN_SEGS(sbi) + segs;
+	MAIN_SECS(sbi) += secs;
 	FREE_I(sbi)->free_sections = (int)FREE_I(sbi)->free_sections + secs;
 	FREE_I(sbi)->free_segments = (int)FREE_I(sbi)->free_segments + segs;
 	F2FS_CKPT(sbi)->user_block_count = cpu_to_le64(user_block_count + blks);
@@ -1508,8 +1535,8 @@ static void update_fs_metadata(struct f2fs_sb_info *sbi, int secs)
 int f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count)
 {
 	__u64 old_block_count, shrunk_blocks;
+	struct cp_control cpc = { CP_RESIZE, 0, 0, 0 };
 	unsigned int secs;
-	int gc_mode, gc_type;
 	int err = 0;
 	__u32 rem;
 
@@ -1544,10 +1571,27 @@ int f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count)
 		return -EINVAL;
 	}
 
-	freeze_bdev(sbi->sb->s_bdev);
-
 	shrunk_blocks = old_block_count - block_count;
 	secs = div_u64(shrunk_blocks, BLKS_PER_SEC(sbi));
+
+	/* stop other GC */
+	if (!down_write_trylock(&sbi->gc_lock))
+		return -EAGAIN;
+
+	/* stop CP to protect MAIN_SEC in free_segment_range */
+	f2fs_lock_op(sbi);
+	err = free_segment_range(sbi, secs, true);
+	f2fs_unlock_op(sbi);
+	up_write(&sbi->gc_lock);
+	if (err)
+		return err;
+
+	set_sbi_flag(sbi, SBI_IS_RESIZEFS);
+
+	freeze_super(sbi->sb);
+	down_write(&sbi->gc_lock);
+	mutex_lock(&sbi->cp_mutex);
+
 	spin_lock(&sbi->stat_lock);
 	if (shrunk_blocks + valid_user_blocks(sbi) +
 		sbi->current_reserved_blocks + sbi->unusable_block_count +
@@ -1556,69 +1600,44 @@ int f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count)
 	else
 		sbi->user_block_count -= shrunk_blocks;
 	spin_unlock(&sbi->stat_lock);
-	if (err) {
-		thaw_bdev(sbi->sb->s_bdev, sbi->sb);
-		return err;
-	}
-
-	mutex_lock(&sbi->resize_mutex);
-	set_sbi_flag(sbi, SBI_IS_RESIZEFS);
-
-	mutex_lock(&DIRTY_I(sbi)->seglist_lock);
-
-	MAIN_SECS(sbi) -= secs;
-
-	for (gc_mode = 0; gc_mode < MAX_GC_POLICY; gc_mode++)
-		if (SIT_I(sbi)->last_victim[gc_mode] >=
-					MAIN_SECS(sbi) * sbi->segs_per_sec)
-			SIT_I(sbi)->last_victim[gc_mode] = 0;
-
-	for (gc_type = BG_GC; gc_type <= FG_GC; gc_type++)
-		if (sbi->next_victim_seg[gc_type] >=
-					MAIN_SECS(sbi) * sbi->segs_per_sec)
-			sbi->next_victim_seg[gc_type] = NULL_SEGNO;
-
-	mutex_unlock(&DIRTY_I(sbi)->seglist_lock);
+	if (err)
+		goto out_err;
 
-	err = free_segment_range(sbi, MAIN_SECS(sbi) * sbi->segs_per_sec,
-			MAIN_SEGS(sbi) - 1);
+	err = free_segment_range(sbi, secs, false);
 	if (err)
-		goto out;
+		goto recover_out;
 
 	update_sb_metadata(sbi, -secs);
 
 	err = f2fs_commit_super(sbi, false);
 	if (err) {
 		update_sb_metadata(sbi, secs);
-		goto out;
+		goto recover_out;
 	}
 
-	mutex_lock(&sbi->cp_mutex);
 	update_fs_metadata(sbi, -secs);
 	clear_sbi_flag(sbi, SBI_IS_RESIZEFS);
 	set_sbi_flag(sbi, SBI_IS_DIRTY);
-	mutex_unlock(&sbi->cp_mutex);
 
-	err = f2fs_sync_fs(sbi->sb, 1);
+	err = f2fs_write_checkpoint(sbi, &cpc);
 	if (err) {
-		mutex_lock(&sbi->cp_mutex);
 		update_fs_metadata(sbi, secs);
-		mutex_unlock(&sbi->cp_mutex);
 		update_sb_metadata(sbi, secs);
 		f2fs_commit_super(sbi, false);
 	}
-out:
+recover_out:
 	if (err) {
 		set_sbi_flag(sbi, SBI_NEED_FSCK);
 		f2fs_err(sbi, "resize_fs failed, should run fsck to repair!");
 
-		MAIN_SECS(sbi) += secs;
 		spin_lock(&sbi->stat_lock);
 		sbi->user_block_count += shrunk_blocks;
 		spin_unlock(&sbi->stat_lock);
 	}
+out_err:
+	mutex_unlock(&sbi->cp_mutex);
+	up_write(&sbi->gc_lock);
+	thaw_super(sbi->sb);
 	clear_sbi_flag(sbi, SBI_IS_RESIZEFS);
-	mutex_unlock(&sbi->resize_mutex);
-	thaw_bdev(sbi->sb->s_bdev, sbi->sb);
 	return err;
 }

commit 8b83ac81f4283ae3bd05c9a7e15dca721014dd03
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Apr 16 18:16:56 2020 +0800

    f2fs: support read iostat
    
    Adds to support accounting read IOs from userspace/kernel.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 26248c8936db..28a8c79c8bdc 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -737,6 +737,9 @@ static int ra_data_block(struct inode *inode, pgoff_t index)
 		goto put_encrypted_page;
 	f2fs_put_page(fio.encrypted_page, 0);
 	f2fs_put_page(page, 1);
+
+	f2fs_update_iostat(sbi, FS_DATA_READ_IO, F2FS_BLKSIZE);
+
 	return 0;
 put_encrypted_page:
 	f2fs_put_page(fio.encrypted_page, 1);
@@ -840,6 +843,9 @@ static int move_data_block(struct inode *inode, block_t bidx,
 			f2fs_put_page(mpage, 1);
 			goto up_out;
 		}
+
+		f2fs_update_iostat(fio.sbi, FS_DATA_READ_IO, F2FS_BLKSIZE);
+
 		lock_page(mpage);
 		if (unlikely(mpage->mapping != META_MAPPING(fio.sbi) ||
 						!PageUptodate(mpage))) {

commit 7bcd0cfa735d72037c71979f18a257b0c6a2b87f
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Mar 19 19:57:58 2020 +0800

    f2fs: don't trigger data flush in foreground operation
    
    Data flush can generate heavy IO and cause long latency during
    flush, so it's not appropriate to trigger it in foreground
    operation.
    
    And also, we may face below potential deadlock during data flush:
    - f2fs_write_multi_pages
     - f2fs_write_raw_pages
      - f2fs_write_single_data_page
       - f2fs_balance_fs
        - f2fs_balance_fs_bg
         - f2fs_sync_dirty_inodes
          - filemap_fdatawrite   -- stuck on flush same cluster
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index f122fe3dbba3..26248c8936db 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -113,7 +113,7 @@ static int gc_thread_func(void *data)
 				prefree_segments(sbi), free_segments(sbi));
 
 		/* balancing f2fs's metadata periodically */
-		f2fs_balance_fs_bg(sbi);
+		f2fs_balance_fs_bg(sbi, true);
 next:
 		sb_end_write(sbi->sb);
 

commit a4ba5dfc5c88e49bb03385abfdd28c5a0acfbb54
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Mar 3 20:09:25 2020 +0800

    f2fs: fix to update f2fs_super_block fields under sb_lock
    
    Fields in struct f2fs_super_block should be updated under coverage
    of sb_lock, fix to adjust update_sb_metadata() for that rule.
    
    Fixes: 04f0b2eaa3b3 ("f2fs: ioctl for removing a range from F2FS")
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index bc203e24a039..f122fe3dbba3 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1442,12 +1442,19 @@ static int free_segment_range(struct f2fs_sb_info *sbi, unsigned int start,
 static void update_sb_metadata(struct f2fs_sb_info *sbi, int secs)
 {
 	struct f2fs_super_block *raw_sb = F2FS_RAW_SUPER(sbi);
-	int section_count = le32_to_cpu(raw_sb->section_count);
-	int segment_count = le32_to_cpu(raw_sb->segment_count);
-	int segment_count_main = le32_to_cpu(raw_sb->segment_count_main);
-	long long block_count = le64_to_cpu(raw_sb->block_count);
+	int section_count;
+	int segment_count;
+	int segment_count_main;
+	long long block_count;
 	int segs = secs * sbi->segs_per_sec;
 
+	down_write(&sbi->sb_lock);
+
+	section_count = le32_to_cpu(raw_sb->section_count);
+	segment_count = le32_to_cpu(raw_sb->segment_count);
+	segment_count_main = le32_to_cpu(raw_sb->segment_count_main);
+	block_count = le64_to_cpu(raw_sb->block_count);
+
 	raw_sb->section_count = cpu_to_le32(section_count + secs);
 	raw_sb->segment_count = cpu_to_le32(segment_count + segs);
 	raw_sb->segment_count_main = cpu_to_le32(segment_count_main + segs);
@@ -1461,6 +1468,8 @@ static void update_sb_metadata(struct f2fs_sb_info *sbi, int secs)
 		raw_sb->devs[last_dev].total_segments =
 						cpu_to_le32(dev_segs + segs);
 	}
+
+	up_write(&sbi->sb_lock);
 }
 
 static void update_fs_metadata(struct f2fs_sb_info *sbi, int secs)

commit 682756827501dc52593bf490f2d437c65ec9efcb
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Tue Mar 3 19:59:25 2020 +0530

    f2fs: Fix mount failure due to SPO after a successful online resize FS
    
    Even though online resize is successfully done, a SPO immediately
    after resize, still causes below error in the next mount.
    
    [   11.294650] F2FS-fs (sda8): Wrong user_block_count: 2233856
    [   11.300272] F2FS-fs (sda8): Failed to get valid F2FS checkpoint
    
    This is because after FS metadata is updated in update_fs_metadata()
    if the SBI_IS_DIRTY is not dirty, then CP will not be done to reflect
    the new user_block_count.
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 9ead93fcf78a..bc203e24a039 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1578,11 +1578,17 @@ int f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count)
 		goto out;
 	}
 
+	mutex_lock(&sbi->cp_mutex);
 	update_fs_metadata(sbi, -secs);
 	clear_sbi_flag(sbi, SBI_IS_RESIZEFS);
+	set_sbi_flag(sbi, SBI_IS_DIRTY);
+	mutex_unlock(&sbi->cp_mutex);
+
 	err = f2fs_sync_fs(sbi->sb, 1);
 	if (err) {
+		mutex_lock(&sbi->cp_mutex);
 		update_fs_metadata(sbi, secs);
+		mutex_unlock(&sbi->cp_mutex);
 		update_sb_metadata(sbi, secs);
 		f2fs_commit_super(sbi, false);
 	}

commit dabfbbc8f914504670a7fbeaf933aa253cbb8acc
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sun Feb 9 13:28:45 2020 -0800

    f2fs: skip migration only when BG_GC is called
    
    FG_GC needs to move entire section more quickly.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index f6958ae8c157..9ead93fcf78a 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1211,7 +1211,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 
 		if (get_valid_blocks(sbi, segno, false) == 0)
 			goto freed;
-		if (__is_large_section(sbi) &&
+		if (gc_type == BG_GC && __is_large_section(sbi) &&
 				migrated >= sbi->migration_granularity)
 			goto skip;
 		if (!PageUptodate(sum_page) || unlikely(f2fs_cp_error(sbi)))

commit 5df7731f60c2a933695a68d732f8b39fca788de6
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Feb 17 17:45:44 2020 +0800

    f2fs: introduce DEFAULT_IO_TIMEOUT
    
    As Geert Uytterhoeven reported:
    
    for parameter HZ/50 in congestion_wait(BLK_RW_ASYNC, HZ/50);
    
    On some platforms, HZ can be less than 50, then unexpected 0 timeout
    jiffies will be set in congestion_wait().
    
    This patch introduces a macro DEFAULT_IO_TIMEOUT to wrap a determinate
    value with msecs_to_jiffies(20) to instead HZ/50 to avoid such issue.
    
    Quoted from Geert Uytterhoeven:
    
    "A timeout of HZ means 1 second.
    HZ/50 means 20 ms, but has the risk of being zero, if HZ < 50.
    
    If you want to use a timeout of 20 ms, you best use msecs_to_jiffies(20),
    as that takes care of the special cases, and never returns 0."
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 0dfdec4926e4..f6958ae8c157 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -977,7 +977,8 @@ static int move_data_page(struct inode *inode, block_t bidx, int gc_type,
 		if (err) {
 			clear_cold_data(page);
 			if (err == -ENOMEM) {
-				congestion_wait(BLK_RW_ASYNC, HZ/50);
+				congestion_wait(BLK_RW_ASYNC,
+						DEFAULT_IO_TIMEOUT);
 				goto retry;
 			}
 			if (is_dirty)

commit 2bac07635ddf9ed59268e61e415d8de9c5eaded7
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sun Feb 9 13:23:28 2020 -0800

    f2fs: skip GC when section is full
    
    This fixes skipping GC when segment is full in large section.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index bb0b6435dcf7..0dfdec4926e4 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1025,8 +1025,8 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		 * race condition along with SSR block allocation.
 		 */
 		if ((gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0)) ||
-				get_valid_blocks(sbi, segno, false) ==
-							sbi->blocks_per_seg)
+				get_valid_blocks(sbi, segno, true) ==
+							BLKS_PER_SEC(sbi))
 			return submitted;
 
 		if (check_valid_map(sbi, segno, off) == 0)

commit 8c7b9ac129d0962faa6f8b092424eab3427cc364
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sun Feb 9 13:27:09 2020 -0800

    f2fs: add migration count iff migration happens
    
    If first segment is empty and migration_granularity is 1, we can't move this
    at all.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index a19966b0e018..bb0b6435dcf7 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1240,12 +1240,12 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 							segno, gc_type);
 
 		stat_inc_seg_count(sbi, type, gc_type);
+		migrated++;
 
 freed:
 		if (gc_type == FG_GC &&
 				get_valid_blocks(sbi, segno, false) == 0)
 			seg_freed++;
-		migrated++;
 
 		if (__is_large_section(sbi) && segno + 1 < end_segno)
 			sbi->next_victim_seg[gc_type] = segno + 1;

commit bbbc34fd66625473d20f89def2703f4f140d2ce8
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Feb 14 17:44:13 2020 +0800

    f2fs: clean up bggc mount option
    
    There are three status for background gc: on, off and sync, it's
    a little bit confused to use test_opt(BG_GC) and test_opt(FORCE_FG_GC)
    combinations to indicate status of background gc.
    
    So let's remove F2FS_MOUNT_BG_GC and F2FS_MOUNT_FORCE_FG_GC mount
    options, and add F2FS_OPTION().bggc_mode with below three status
    to clean up codes and enhance bggc mode's scalability.
    
    enum {
            BGGC_MODE_ON,           /* background gc is on */
            BGGC_MODE_OFF,          /* background gc is off */
            BGGC_MODE_SYNC,         /*
                                     * background gc is on, migrating blocks
                                     * like foreground gc
                                     */
    };
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index d883943f7f46..a19966b0e018 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -31,6 +31,8 @@ static int gc_thread_func(void *data)
 
 	set_freezable();
 	do {
+		bool sync_mode;
+
 		wait_event_interruptible_timeout(*wq,
 				kthread_should_stop() || freezing(current) ||
 				gc_th->gc_wake,
@@ -101,8 +103,10 @@ static int gc_thread_func(void *data)
 do_gc:
 		stat_inc_bggc_count(sbi->stat_info);
 
+		sync_mode = F2FS_OPTION(sbi).bggc_mode == BGGC_MODE_SYNC;
+
 		/* if return value is not zero, no victim was selected */
-		if (f2fs_gc(sbi, test_opt(sbi, FORCE_FG_GC), true, NULL_SEGNO))
+		if (f2fs_gc(sbi, sync_mode, true, NULL_SEGNO))
 			wait_ms = gc_th->no_gc_sleep_time;
 
 		trace_f2fs_background_gc(sbi->sb, wait_ms,

commit b0332a0f957ca818642cfafdb9515d4fd3b24663
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Feb 14 17:44:12 2020 +0800

    f2fs: clean up lfs/adaptive mount option
    
    This patch removes F2FS_MOUNT_ADAPTIVE and F2FS_MOUNT_LFS mount options,
    and add F2FS_OPTION.fs_mode with below two status to indicate filesystem
    mode.
    
    enum {
            FS_MODE_ADAPTIVE,       /* use both lfs/ssr allocation */
            FS_MODE_LFS,            /* use lfs allocation only */
    };
    
    It can enhance code readability and fs mode's scalability.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 3fdc3d6a874c..d883943f7f46 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -765,7 +765,7 @@ static int move_data_block(struct inode *inode, block_t bidx,
 	struct page *page, *mpage;
 	block_t newaddr;
 	int err = 0;
-	bool lfs_mode = test_opt(fio.sbi, LFS);
+	bool lfs_mode = f2fs_lfs_mode(fio.sbi);
 
 	/* do not read out */
 	page = f2fs_grab_cache_page(inode->i_mapping, bidx, false);

commit a2ced1ce1087a19361b7845c85a2d910fc591344
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Feb 14 17:44:10 2020 +0800

    f2fs: clean up codes with {f2fs_,}data_blkaddr()
    
    - rename datablock_addr() to data_blkaddr().
    - wrap data_blkaddr() with f2fs_data_blkaddr() to clean up
    parameters.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 608d04950f05..3fdc3d6a874c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -637,7 +637,7 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	}
 
 	*nofs = ofs_of_node(node_page);
-	source_blkaddr = datablock_addr(NULL, node_page, ofs_in_node);
+	source_blkaddr = data_blkaddr(NULL, node_page, ofs_in_node);
 	f2fs_put_page(node_page, 1);
 
 	if (source_blkaddr != blkaddr) {

commit 7a88ddb56077d07257a5d0393a4be13e424ca755
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Feb 27 19:30:05 2020 +0800

    f2fs: fix inconsistent comments
    
    Lack of maintenance on comments may mislead developers, fix them.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index db8725d473b5..608d04950f05 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -192,7 +192,10 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 		p->ofs_unit = sbi->segs_per_sec;
 	}
 
-	/* we need to check every dirty segments in the FG_GC case */
+	/*
+	 * adjust candidates range, should select all dirty segments for
+	 * foreground GC and urgent GC cases.
+	 */
 	if (gc_type != FG_GC &&
 			(sbi->gc_mode != GC_URGENT) &&
 			p->max_search > sbi->max_victim_search)

commit fc7100ea2a52fcf200be75421bfd32652827d287
Author: Hridya Valsaraju <hridya@google.com>
Date:   Wed Jan 22 10:51:16 2020 -0800

    f2fs: Add f2fs stats to sysfs
    
    Currently f2fs stats are only available from /d/f2fs/status. This patch
    adds some of the f2fs stats to sysfs so that they are accessible even
    when debugfs is not mounted.
    
    The following sysfs nodes are added:
    -/sys/fs/f2fs/<disk>/free_segments
    -/sys/fs/f2fs/<disk>/cp_foreground_calls
    -/sys/fs/f2fs/<disk>/cp_background_calls
    -/sys/fs/f2fs/<disk>/gc_foreground_calls
    -/sys/fs/f2fs/<disk>/gc_background_calls
    -/sys/fs/f2fs/<disk>/moved_blocks_foreground
    -/sys/fs/f2fs/<disk>/moved_blocks_background
    -/sys/fs/f2fs/<disk>/avg_vblocks
    
    Signed-off-by: Hridya Valsaraju <hridya@google.com>
    [Jaegeuk Kim: allow STAT_FS without DEBUG_FS]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 67eca7c2d983..db8725d473b5 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -99,7 +99,7 @@ static int gc_thread_func(void *data)
 		else
 			increase_sleep_time(gc_th, &wait_ms);
 do_gc:
-		stat_inc_bggc_count(sbi);
+		stat_inc_bggc_count(sbi->stat_info);
 
 		/* if return value is not zero, no victim was selected */
 		if (f2fs_gc(sbi, test_opt(sbi, FORCE_FG_GC), true, NULL_SEGNO))

commit fb24fea75ca5ceef59f753494b2efd453606e08a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Jan 14 19:36:50 2020 +0800

    f2fs: change to use rwsem for gc_mutex
    
    Mutex lock won't serialize callers, in order to avoid starving of unlucky
    caller, let's use rwsem lock instead.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c43181ef98c4..67eca7c2d983 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -78,18 +78,18 @@ static int gc_thread_func(void *data)
 		 */
 		if (sbi->gc_mode == GC_URGENT) {
 			wait_ms = gc_th->urgent_sleep_time;
-			mutex_lock(&sbi->gc_mutex);
+			down_write(&sbi->gc_lock);
 			goto do_gc;
 		}
 
-		if (!mutex_trylock(&sbi->gc_mutex)) {
+		if (!down_write_trylock(&sbi->gc_lock)) {
 			stat_other_skip_bggc_count(sbi);
 			goto next;
 		}
 
 		if (!is_idle(sbi, GC_TIME)) {
 			increase_sleep_time(gc_th, &wait_ms);
-			mutex_unlock(&sbi->gc_mutex);
+			up_write(&sbi->gc_lock);
 			stat_io_skip_bggc_count(sbi);
 			goto next;
 		}
@@ -1370,7 +1370,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 				reserved_segments(sbi),
 				prefree_segments(sbi));
 
-	mutex_unlock(&sbi->gc_mutex);
+	up_write(&sbi->gc_lock);
 
 	put_gc_inode(&gc_list);
 
@@ -1409,9 +1409,9 @@ static int free_segment_range(struct f2fs_sb_info *sbi, unsigned int start,
 			.iroot = RADIX_TREE_INIT(gc_list.iroot, GFP_NOFS),
 		};
 
-		mutex_lock(&sbi->gc_mutex);
+		down_write(&sbi->gc_lock);
 		do_garbage_collect(sbi, segno, &gc_list, FG_GC);
-		mutex_unlock(&sbi->gc_mutex);
+		up_write(&sbi->gc_lock);
 		put_gc_inode(&gc_list);
 
 		if (get_valid_blocks(sbi, segno, true))

commit 4eea93e3ff98aa185302d562f0df1ad501c51e70
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Dec 20 17:20:05 2019 -0800

    f2fs: run fsck when getting bad inode during GC
    
    This is to avoid inifinite GC when trying to disable checkpoint.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b3d399623290..c43181ef98c4 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1049,8 +1049,10 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 		if (phase == 3) {
 			inode = f2fs_iget(sb, dni.ino);
-			if (IS_ERR(inode) || is_bad_inode(inode))
+			if (IS_ERR(inode) || is_bad_inode(inode)) {
+				set_sbi_flag(sbi, SBI_NEED_FSCK);
 				continue;
+			}
 
 			if (!down_write_trylock(
 				&F2FS_I(inode)->i_gc_rwsem[WRITE])) {

commit 803e74be04b32f7785742dcabfc62116718fbb06
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Nov 22 12:02:06 2019 -0800

    f2fs: stop GC when the victim becomes fully valid
    
    We must stop GC, once the segment becomes fully valid. Otherwise, it can
    produce another dirty segments by moving valid blocks in the segment partially.
    
    Ramon hit no free segment panic sometimes and saw this case happens when
    validating reliable file pinning feature.
    
    Signed-off-by: Ramon Pantin <pantin@google.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 24a3b6b52210..b3d399623290 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1012,8 +1012,14 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		block_t start_bidx;
 		nid_t nid = le32_to_cpu(entry->nid);
 
-		/* stop BG_GC if there is not enough free sections. */
-		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0))
+		/*
+		 * stop BG_GC if there is not enough free sections.
+		 * Or, stop GC if the segment becomes fully valid caused by
+		 * race condition along with SSR block allocation.
+		 */
+		if ((gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0)) ||
+				get_valid_blocks(sbi, segno, false) ==
+							sbi->blocks_per_seg)
 			return submitted;
 
 		if (check_valid_map(sbi, segno, off) == 0)

commit c45d6002ff7a322022560e9b19ad867b01fec77f
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Nov 1 17:53:23 2019 +0800

    f2fs: show f2fs instance in printk_ratelimited
    
    As Eric mentioned, bare printk{,_ratelimited} won't show which
    filesystem instance these message is coming from, this patch tries
    to show fs instance with sb->s_id field in all places we missed
    before.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index ef7686a82722..24a3b6b52210 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -54,7 +54,7 @@ static int gc_thread_func(void *data)
 		}
 
 		if (time_to_inject(sbi, FAULT_CHECKPOINT)) {
-			f2fs_show_injection_info(FAULT_CHECKPOINT);
+			f2fs_show_injection_info(sbi, FAULT_CHECKPOINT);
 			f2fs_stop_checkpoint(sbi, false);
 		}
 

commit 46d9ce195a2b1b8aceeafae1d8f407383a117b0e
Author: Qiuyang Sun <sunqiuyang@huawei.com>
Date:   Mon Sep 23 12:21:39 2019 +0800

    f2fs: update multi-dev metadata in resize_fs
    
    Multi-device metadata should be updated in resize_fs as well.
    
    Also, we check that the new FS size still reaches the last device.
    
    Signed-off-by: Qiuyang Sun <sunqiuyang@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 5877bd729689..ef7686a82722 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1437,11 +1437,20 @@ static void update_sb_metadata(struct f2fs_sb_info *sbi, int secs)
 	raw_sb->segment_count_main = cpu_to_le32(segment_count_main + segs);
 	raw_sb->block_count = cpu_to_le64(block_count +
 					(long long)segs * sbi->blocks_per_seg);
+	if (f2fs_is_multi_device(sbi)) {
+		int last_dev = sbi->s_ndevs - 1;
+		int dev_segs =
+			le32_to_cpu(raw_sb->devs[last_dev].total_segments);
+
+		raw_sb->devs[last_dev].total_segments =
+						cpu_to_le32(dev_segs + segs);
+	}
 }
 
 static void update_fs_metadata(struct f2fs_sb_info *sbi, int secs)
 {
 	int segs = secs * sbi->segs_per_sec;
+	long long blks = (long long)segs * sbi->blocks_per_seg;
 	long long user_block_count =
 				le64_to_cpu(F2FS_CKPT(sbi)->user_block_count);
 
@@ -1449,8 +1458,20 @@ static void update_fs_metadata(struct f2fs_sb_info *sbi, int secs)
 	MAIN_SEGS(sbi) = (int)MAIN_SEGS(sbi) + segs;
 	FREE_I(sbi)->free_sections = (int)FREE_I(sbi)->free_sections + secs;
 	FREE_I(sbi)->free_segments = (int)FREE_I(sbi)->free_segments + segs;
-	F2FS_CKPT(sbi)->user_block_count = cpu_to_le64(user_block_count +
-					(long long)segs * sbi->blocks_per_seg);
+	F2FS_CKPT(sbi)->user_block_count = cpu_to_le64(user_block_count + blks);
+
+	if (f2fs_is_multi_device(sbi)) {
+		int last_dev = sbi->s_ndevs - 1;
+
+		FDEV(last_dev).total_segments =
+				(int)FDEV(last_dev).total_segments + segs;
+		FDEV(last_dev).end_blk =
+				(long long)FDEV(last_dev).end_blk + blks;
+#ifdef CONFIG_BLK_DEV_ZONED
+		FDEV(last_dev).nr_blkz = (int)FDEV(last_dev).nr_blkz +
+					(int)(blks >> sbi->log_blocks_per_blkz);
+#endif
+	}
 }
 
 int f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count)
@@ -1465,6 +1486,15 @@ int f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count)
 	if (block_count > old_block_count)
 		return -EINVAL;
 
+	if (f2fs_is_multi_device(sbi)) {
+		int last_dev = sbi->s_ndevs - 1;
+		__u64 last_segs = FDEV(last_dev).total_segments;
+
+		if (block_count + last_segs * sbi->blocks_per_seg <=
+								old_block_count)
+			return -EINVAL;
+	}
+
 	/* new fs size should align to section size */
 	div_u64_rem(block_count, BLKS_PER_SEC(sbi), &rem);
 	if (rem)

commit 957fa47823dfe449c5a15a944e4e7a299a6601db
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Mon Jul 29 10:50:26 2019 +0530

    f2fs: Fix indefinite loop in f2fs_gc()
    
    Policy - foreground GC, LFS mode and greedy GC mode.
    
    Under this policy, f2fs_gc() loops forever to GC as it doesn't have
    enough free segements to proceed and thus it keeps calling gc_more
    for the same victim segment.  This can happen if the selected victim
    segment could not be GC'd due to failed blkaddr validity check i.e.
    is_alive() returns false for the blocks set in current validity map.
    
    Fix this by not resetting the sbi->cur_victim_sec to NULL_SEGNO, when
    the segment selected could not be GC'd. This helps to select another
    segment for GC and thus helps to proceed forward with GC.
    
    [Note]
    This can happen due to is_alive as well as atomic_file which skipps
    GC.
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e88f98ddf396..5877bd729689 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1326,7 +1326,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 		round++;
 	}
 
-	if (gc_type == FG_GC)
+	if (gc_type == FG_GC && seg_freed)
 		sbi->cur_victim_sec = NULL_SEGNO;
 
 	if (sync)

commit bbf9f7d90f21e05e31b7cdd95b32f64dd2819dfe
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Wed Aug 7 19:10:32 2019 +0530

    f2fs: Fix indefinite loop in f2fs_gc()
    
    Policy - Foreground GC, LFS and greedy GC mode.
    
    Under this policy, f2fs_gc() loops forever to GC as it doesn't have
    enough free segements to proceed and thus it keeps calling gc_more
    for the same victim segment.  This can happen if the selected victim
    segment could not be GC'd due to failed blkaddr validity check i.e.
    is_alive() returns false for the blocks set in current validity map.
    
    Fix this by keeping track of such invalid segments and skip those
    segments for selection in get_victim_by_default() to avoid endless
    GC loop under such error scenarios. Currently, add this logic under
    CONFIG_F2FS_CHECK_FS to be able to root cause the issue in debug
    version.
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    [Jaegeuk Kim: fix wrong bitmap size]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 8974672db78f..e88f98ddf396 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -382,6 +382,16 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 			nsearched++;
 		}
 
+#ifdef CONFIG_F2FS_CHECK_FS
+		/*
+		 * skip selecting the invalid segno (that is failed due to block
+		 * validity check failure during GC) to avoid endless GC loop in
+		 * such cases.
+		 */
+		if (test_bit(segno, sm->invalid_segmap))
+			goto next;
+#endif
+
 		secno = GET_SEC_FROM_SEG(sbi, segno);
 
 		if (sec_usage_check(sbi, secno))
@@ -627,8 +637,21 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	source_blkaddr = datablock_addr(NULL, node_page, ofs_in_node);
 	f2fs_put_page(node_page, 1);
 
-	if (source_blkaddr != blkaddr)
+	if (source_blkaddr != blkaddr) {
+#ifdef CONFIG_F2FS_CHECK_FS
+		unsigned int segno = GET_SEGNO(sbi, blkaddr);
+		unsigned long offset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);
+
+		if (unlikely(check_valid_map(sbi, segno, offset))) {
+			if (!test_and_set_bit(segno, SIT_I(sbi)->invalid_segmap)) {
+				f2fs_err(sbi, "mismatched blkaddr %u (source_blkaddr %u) in seg %u\n",
+						blkaddr, source_blkaddr, segno);
+				f2fs_bug_on(sbi, 1);
+			}
+		}
+#endif
 		return false;
+	}
 	return true;
 }
 

commit 543b8c468f55f27f3c0178a22a91a51aabbbc428
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jul 17 18:31:53 2019 -0700

    f2fs: fix to read source block before invalidating it
    
    f2fs_allocate_data_block() invalidates old block address and enable new block
    address. Then, if we try to read old block by f2fs_submit_page_bio(), it will
    give WARN due to reading invalid blocks.
    
    Let's make the order sanely back.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 6691f526fa40..8974672db78f 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -796,6 +796,29 @@ static int move_data_block(struct inode *inode, block_t bidx,
 	if (lfs_mode)
 		down_write(&fio.sbi->io_order_lock);
 
+	mpage = f2fs_grab_cache_page(META_MAPPING(fio.sbi),
+					fio.old_blkaddr, false);
+	if (!mpage)
+		goto up_out;
+
+	fio.encrypted_page = mpage;
+
+	/* read source block in mpage */
+	if (!PageUptodate(mpage)) {
+		err = f2fs_submit_page_bio(&fio);
+		if (err) {
+			f2fs_put_page(mpage, 1);
+			goto up_out;
+		}
+		lock_page(mpage);
+		if (unlikely(mpage->mapping != META_MAPPING(fio.sbi) ||
+						!PageUptodate(mpage))) {
+			err = -EIO;
+			f2fs_put_page(mpage, 1);
+			goto up_out;
+		}
+	}
+
 	f2fs_allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &newaddr,
 					&sum, CURSEG_COLD_DATA, NULL, false);
 
@@ -803,44 +826,18 @@ static int move_data_block(struct inode *inode, block_t bidx,
 				newaddr, FGP_LOCK | FGP_CREAT, GFP_NOFS);
 	if (!fio.encrypted_page) {
 		err = -ENOMEM;
-		goto recover_block;
-	}
-
-	mpage = f2fs_pagecache_get_page(META_MAPPING(fio.sbi),
-					fio.old_blkaddr, FGP_LOCK, GFP_NOFS);
-	if (mpage) {
-		bool updated = false;
-
-		if (PageUptodate(mpage)) {
-			memcpy(page_address(fio.encrypted_page),
-					page_address(mpage), PAGE_SIZE);
-			updated = true;
-		}
 		f2fs_put_page(mpage, 1);
-		invalidate_mapping_pages(META_MAPPING(fio.sbi),
-					fio.old_blkaddr, fio.old_blkaddr);
-		if (updated)
-			goto write_page;
-	}
-
-	err = f2fs_submit_page_bio(&fio);
-	if (err)
-		goto put_page_out;
-
-	/* write page */
-	lock_page(fio.encrypted_page);
-
-	if (unlikely(fio.encrypted_page->mapping != META_MAPPING(fio.sbi))) {
-		err = -EIO;
-		goto put_page_out;
-	}
-	if (unlikely(!PageUptodate(fio.encrypted_page))) {
-		err = -EIO;
-		goto put_page_out;
+		goto recover_block;
 	}
 
-write_page:
+	/* write target block */
 	f2fs_wait_on_page_writeback(fio.encrypted_page, DATA, true, true);
+	memcpy(page_address(fio.encrypted_page),
+				page_address(mpage), PAGE_SIZE);
+	f2fs_put_page(mpage, 1);
+	invalidate_mapping_pages(META_MAPPING(fio.sbi),
+				fio.old_blkaddr, fio.old_blkaddr);
+
 	set_page_dirty(fio.encrypted_page);
 	if (clear_page_dirty_for_io(fio.encrypted_page))
 		dec_page_count(fio.sbi, F2FS_DIRTY_META);
@@ -871,11 +868,12 @@ static int move_data_block(struct inode *inode, block_t bidx,
 put_page_out:
 	f2fs_put_page(fio.encrypted_page, 1);
 recover_block:
-	if (lfs_mode)
-		up_write(&fio.sbi->io_order_lock);
 	if (err)
 		f2fs_do_replace_block(fio.sbi, &sum, newaddr, fio.old_blkaddr,
 								true, true);
+up_out:
+	if (lfs_mode)
+		up_write(&fio.sbi->io_order_lock);
 put_out:
 	f2fs_put_dnode(&dn);
 out:

commit 10f966bbf521bb9b2e497bbca496a5141f4071d0
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Jun 20 11:36:14 2019 +0800

    f2fs: use generic EFSBADCRC/EFSCORRUPTED
    
    f2fs uses EFAULT as error number to indicate filesystem is corrupted
    all the time, but generic filesystems use EUCLEAN for such condition,
    we need to change to follow others.
    
    This patch adds two new macros as below to wrap more generic error
    code macros, and spread them in code.
    
    EFSBADCRC       EBADMSG         /* Bad CRC detected */
    EFSCORRUPTED    EUCLEAN         /* Filesystem is corrupted */
    
    Reported-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c65f87f11de0..6691f526fa40 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -660,7 +660,7 @@ static int ra_data_block(struct inode *inode, pgoff_t index)
 		dn.data_blkaddr = ei.blk + index - ei.fofs;
 		if (unlikely(!f2fs_is_valid_blkaddr(sbi, dn.data_blkaddr,
 						DATA_GENERIC_ENHANCE_READ))) {
-			err = -EFAULT;
+			err = -EFSCORRUPTED;
 			goto put_page;
 		}
 		goto got_it;
@@ -678,7 +678,7 @@ static int ra_data_block(struct inode *inode, pgoff_t index)
 	}
 	if (unlikely(!f2fs_is_valid_blkaddr(sbi, dn.data_blkaddr,
 						DATA_GENERIC_ENHANCE))) {
-		err = -EFAULT;
+		err = -EFSCORRUPTED;
 		goto put_page;
 	}
 got_it:
@@ -1454,7 +1454,7 @@ int f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count)
 
 	if (is_sbi_flag_set(sbi, SBI_NEED_FSCK)) {
 		f2fs_err(sbi, "Should run fsck to repair first.");
-		return -EINVAL;
+		return -EFSCORRUPTED;
 	}
 
 	if (test_opt(sbi, DISABLE_CHECKPOINT)) {

commit dcbb4c10e6d9693cc9d6fa493b4d130b66a60c7d
Author: Joe Perches <joe@perches.com>
Date:   Tue Jun 18 17:48:42 2019 +0800

    f2fs: introduce f2fs_<level> macros to wrap f2fs_printk()
    
    - Add and use f2fs_<level> macros
    - Convert f2fs_msg to f2fs_printk
    - Remove level from f2fs_printk and embed the level in the format
    - Coalesce formats and align multi-line arguments
    - Remove unnecessary duplicate extern f2fs_msg f2fs.h
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e19b49b02d1b..c65f87f11de0 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -618,9 +618,8 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	}
 
 	if (sum->version != dni->version) {
-		f2fs_msg(sbi->sb, KERN_WARNING,
-				"%s: valid data with mismatched node version.",
-				__func__);
+		f2fs_warn(sbi, "%s: valid data with mismatched node version.",
+			  __func__);
 		set_sbi_flag(sbi, SBI_NEED_FSCK);
 	}
 
@@ -1183,9 +1182,8 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 
 		sum = page_address(sum_page);
 		if (type != GET_SUM_TYPE((&sum->footer))) {
-			f2fs_msg(sbi->sb, KERN_ERR, "Inconsistent segment (%u) "
-				"type [%d, %d] in SSA and SIT",
-				segno, type, GET_SUM_TYPE((&sum->footer)));
+			f2fs_err(sbi, "Inconsistent segment (%u) type [%d, %d] in SSA and SIT",
+				 segno, type, GET_SUM_TYPE((&sum->footer)));
 			set_sbi_flag(sbi, SBI_NEED_FSCK);
 			f2fs_stop_checkpoint(sbi, false);
 			goto skip;
@@ -1397,8 +1395,8 @@ static int free_segment_range(struct f2fs_sb_info *sbi, unsigned int start,
 
 	next_inuse = find_next_inuse(FREE_I(sbi), end + 1, start);
 	if (next_inuse <= end) {
-		f2fs_msg(sbi->sb, KERN_ERR,
-			"segno %u should be free but still inuse!", next_inuse);
+		f2fs_err(sbi, "segno %u should be free but still inuse!",
+			 next_inuse);
 		f2fs_bug_on(sbi, 1);
 	}
 	return err;
@@ -1455,14 +1453,12 @@ int f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count)
 		return 0;
 
 	if (is_sbi_flag_set(sbi, SBI_NEED_FSCK)) {
-		f2fs_msg(sbi->sb, KERN_ERR,
-			"Should run fsck to repair first.");
+		f2fs_err(sbi, "Should run fsck to repair first.");
 		return -EINVAL;
 	}
 
 	if (test_opt(sbi, DISABLE_CHECKPOINT)) {
-		f2fs_msg(sbi->sb, KERN_ERR,
-			"Checkpoint should be enabled.");
+		f2fs_err(sbi, "Checkpoint should be enabled.");
 		return -EINVAL;
 	}
 
@@ -1526,8 +1522,7 @@ int f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count)
 out:
 	if (err) {
 		set_sbi_flag(sbi, SBI_NEED_FSCK);
-		f2fs_msg(sbi->sb, KERN_ERR,
-				"resize_fs failed, should run fsck to repair!");
+		f2fs_err(sbi, "resize_fs failed, should run fsck to repair!");
 
 		MAIN_SECS(sbi) += secs;
 		spin_lock(&sbi->stat_lock);

commit 04f0b2eaa3b3ee243df6040617b4bfbbc0404854
Author: Qiuyang Sun <sunqiuyang@huawei.com>
Date:   Wed Jun 5 11:33:25 2019 +0800

    f2fs: ioctl for removing a range from F2FS
    
    This ioctl shrinks a given length (aligned to sections) from end of the
    main area. Any cursegs and valid blocks will be moved out before
    invalidating the range.
    
    This feature can be used for adjusting partition sizes online.
    
    History of the patch:
    
    Sahitya Tummala:
     - Add this ioctl for f2fs_compat_ioctl() as well.
     - Fix debugfs status to reflect the online resize changes.
     - Fix potential race between online resize path and allocate new data
       block path or gc path.
    
    Others:
     - Rename some identifiers.
     - Add some error handling branches.
     - Clear sbi->next_victim_seg[BG_GC/FG_GC] in shrinking range.
     - Implement this interface as ext4's, and change the parameter from shrunk
    bytes to new block count of F2FS.
     - During resizing, force to empty sit_journal and forbid adding new
       entries to it, in order to avoid invalid segno in journal after resize.
     - Reduce sbi->user_block_count before resize starts.
     - Commit the updated superblock first, and then update in-memory metadata
       only when the former succeeds.
     - Target block count must align to sections.
     - Write checkpoint before and after committing the new superblock, w/o
    CP_FSCK_FLAG respectively, so that the FS can be fixed by fsck even if
    resize fails after the new superblock is committed.
     - In free_segment_range(), reduce granularity of gc_mutex.
     - Add protection on curseg migration.
     - Add freeze_bdev() and thaw_bdev() for resize fs.
     - Remove CUR_MAIN_SECS and use MAIN_SECS directly for allocation.
     - Recover super_block and FS metadata when resize fails.
     - No need to clear CP_FSCK_FLAG in update_ckpt_flags().
     - Clean up the sb and fs metadata update functions for resize_fs.
    
    Geert Uytterhoeven:
     - Use div_u64*() for 64-bit divisions
    
    Arnd Bergmann:
     - Not all architectures support get_user() with a 64-bit argument:
        ERROR: "__get_user_bad" [fs/f2fs/f2fs.ko] undefined!
        Use copy_from_user() here, this will always work.
    
    Signed-off-by: Qiuyang Sun <sunqiuyang@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 1e029da26053..e19b49b02d1b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -311,10 +311,11 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	struct sit_info *sm = SIT_I(sbi);
 	struct victim_sel_policy p;
 	unsigned int secno, last_victim;
-	unsigned int last_segment = MAIN_SEGS(sbi);
+	unsigned int last_segment;
 	unsigned int nsearched = 0;
 
 	mutex_lock(&dirty_i->seglist_lock);
+	last_segment = MAIN_SECS(sbi) * sbi->segs_per_sec;
 
 	p.alloc_mode = alloc_mode;
 	select_policy(sbi, gc_type, type, &p);
@@ -405,7 +406,8 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 				sm->last_victim[p.gc_mode] = last_victim + 1;
 			else
 				sm->last_victim[p.gc_mode] = segno + 1;
-			sm->last_victim[p.gc_mode] %= MAIN_SEGS(sbi);
+			sm->last_victim[p.gc_mode] %=
+				(MAIN_SECS(sbi) * sbi->segs_per_sec);
 			break;
 		}
 	}
@@ -1361,3 +1363,179 @@ void f2fs_build_gc_manager(struct f2fs_sb_info *sbi)
 		SIT_I(sbi)->last_victim[ALLOC_NEXT] =
 				GET_SEGNO(sbi, FDEV(0).end_blk) + 1;
 }
+
+static int free_segment_range(struct f2fs_sb_info *sbi, unsigned int start,
+							unsigned int end)
+{
+	int type;
+	unsigned int segno, next_inuse;
+	int err = 0;
+
+	/* Move out cursegs from the target range */
+	for (type = CURSEG_HOT_DATA; type < NR_CURSEG_TYPE; type++)
+		allocate_segment_for_resize(sbi, type, start, end);
+
+	/* do GC to move out valid blocks in the range */
+	for (segno = start; segno <= end; segno += sbi->segs_per_sec) {
+		struct gc_inode_list gc_list = {
+			.ilist = LIST_HEAD_INIT(gc_list.ilist),
+			.iroot = RADIX_TREE_INIT(gc_list.iroot, GFP_NOFS),
+		};
+
+		mutex_lock(&sbi->gc_mutex);
+		do_garbage_collect(sbi, segno, &gc_list, FG_GC);
+		mutex_unlock(&sbi->gc_mutex);
+		put_gc_inode(&gc_list);
+
+		if (get_valid_blocks(sbi, segno, true))
+			return -EAGAIN;
+	}
+
+	err = f2fs_sync_fs(sbi->sb, 1);
+	if (err)
+		return err;
+
+	next_inuse = find_next_inuse(FREE_I(sbi), end + 1, start);
+	if (next_inuse <= end) {
+		f2fs_msg(sbi->sb, KERN_ERR,
+			"segno %u should be free but still inuse!", next_inuse);
+		f2fs_bug_on(sbi, 1);
+	}
+	return err;
+}
+
+static void update_sb_metadata(struct f2fs_sb_info *sbi, int secs)
+{
+	struct f2fs_super_block *raw_sb = F2FS_RAW_SUPER(sbi);
+	int section_count = le32_to_cpu(raw_sb->section_count);
+	int segment_count = le32_to_cpu(raw_sb->segment_count);
+	int segment_count_main = le32_to_cpu(raw_sb->segment_count_main);
+	long long block_count = le64_to_cpu(raw_sb->block_count);
+	int segs = secs * sbi->segs_per_sec;
+
+	raw_sb->section_count = cpu_to_le32(section_count + secs);
+	raw_sb->segment_count = cpu_to_le32(segment_count + segs);
+	raw_sb->segment_count_main = cpu_to_le32(segment_count_main + segs);
+	raw_sb->block_count = cpu_to_le64(block_count +
+					(long long)segs * sbi->blocks_per_seg);
+}
+
+static void update_fs_metadata(struct f2fs_sb_info *sbi, int secs)
+{
+	int segs = secs * sbi->segs_per_sec;
+	long long user_block_count =
+				le64_to_cpu(F2FS_CKPT(sbi)->user_block_count);
+
+	SM_I(sbi)->segment_count = (int)SM_I(sbi)->segment_count + segs;
+	MAIN_SEGS(sbi) = (int)MAIN_SEGS(sbi) + segs;
+	FREE_I(sbi)->free_sections = (int)FREE_I(sbi)->free_sections + secs;
+	FREE_I(sbi)->free_segments = (int)FREE_I(sbi)->free_segments + segs;
+	F2FS_CKPT(sbi)->user_block_count = cpu_to_le64(user_block_count +
+					(long long)segs * sbi->blocks_per_seg);
+}
+
+int f2fs_resize_fs(struct f2fs_sb_info *sbi, __u64 block_count)
+{
+	__u64 old_block_count, shrunk_blocks;
+	unsigned int secs;
+	int gc_mode, gc_type;
+	int err = 0;
+	__u32 rem;
+
+	old_block_count = le64_to_cpu(F2FS_RAW_SUPER(sbi)->block_count);
+	if (block_count > old_block_count)
+		return -EINVAL;
+
+	/* new fs size should align to section size */
+	div_u64_rem(block_count, BLKS_PER_SEC(sbi), &rem);
+	if (rem)
+		return -EINVAL;
+
+	if (block_count == old_block_count)
+		return 0;
+
+	if (is_sbi_flag_set(sbi, SBI_NEED_FSCK)) {
+		f2fs_msg(sbi->sb, KERN_ERR,
+			"Should run fsck to repair first.");
+		return -EINVAL;
+	}
+
+	if (test_opt(sbi, DISABLE_CHECKPOINT)) {
+		f2fs_msg(sbi->sb, KERN_ERR,
+			"Checkpoint should be enabled.");
+		return -EINVAL;
+	}
+
+	freeze_bdev(sbi->sb->s_bdev);
+
+	shrunk_blocks = old_block_count - block_count;
+	secs = div_u64(shrunk_blocks, BLKS_PER_SEC(sbi));
+	spin_lock(&sbi->stat_lock);
+	if (shrunk_blocks + valid_user_blocks(sbi) +
+		sbi->current_reserved_blocks + sbi->unusable_block_count +
+		F2FS_OPTION(sbi).root_reserved_blocks > sbi->user_block_count)
+		err = -ENOSPC;
+	else
+		sbi->user_block_count -= shrunk_blocks;
+	spin_unlock(&sbi->stat_lock);
+	if (err) {
+		thaw_bdev(sbi->sb->s_bdev, sbi->sb);
+		return err;
+	}
+
+	mutex_lock(&sbi->resize_mutex);
+	set_sbi_flag(sbi, SBI_IS_RESIZEFS);
+
+	mutex_lock(&DIRTY_I(sbi)->seglist_lock);
+
+	MAIN_SECS(sbi) -= secs;
+
+	for (gc_mode = 0; gc_mode < MAX_GC_POLICY; gc_mode++)
+		if (SIT_I(sbi)->last_victim[gc_mode] >=
+					MAIN_SECS(sbi) * sbi->segs_per_sec)
+			SIT_I(sbi)->last_victim[gc_mode] = 0;
+
+	for (gc_type = BG_GC; gc_type <= FG_GC; gc_type++)
+		if (sbi->next_victim_seg[gc_type] >=
+					MAIN_SECS(sbi) * sbi->segs_per_sec)
+			sbi->next_victim_seg[gc_type] = NULL_SEGNO;
+
+	mutex_unlock(&DIRTY_I(sbi)->seglist_lock);
+
+	err = free_segment_range(sbi, MAIN_SECS(sbi) * sbi->segs_per_sec,
+			MAIN_SEGS(sbi) - 1);
+	if (err)
+		goto out;
+
+	update_sb_metadata(sbi, -secs);
+
+	err = f2fs_commit_super(sbi, false);
+	if (err) {
+		update_sb_metadata(sbi, secs);
+		goto out;
+	}
+
+	update_fs_metadata(sbi, -secs);
+	clear_sbi_flag(sbi, SBI_IS_RESIZEFS);
+	err = f2fs_sync_fs(sbi->sb, 1);
+	if (err) {
+		update_fs_metadata(sbi, secs);
+		update_sb_metadata(sbi, secs);
+		f2fs_commit_super(sbi, false);
+	}
+out:
+	if (err) {
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		f2fs_msg(sbi->sb, KERN_ERR,
+				"resize_fs failed, should run fsck to repair!");
+
+		MAIN_SECS(sbi) += secs;
+		spin_lock(&sbi->stat_lock);
+		sbi->user_block_count += shrunk_blocks;
+		spin_unlock(&sbi->stat_lock);
+	}
+	clear_sbi_flag(sbi, SBI_IS_RESIZEFS);
+	mutex_unlock(&sbi->resize_mutex);
+	thaw_bdev(sbi->sb->s_bdev, sbi->sb);
+	return err;
+}

commit 49dd883c421a4529968887b0ecd321b23ebf3326
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon May 20 15:54:49 2019 -0700

    f2fs: allow ssr block allocation during checkpoint=disable period
    
    This patch allows to use ssr during checkpoint is disabled.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 963fb4571fd9..1e029da26053 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -387,7 +387,8 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 			goto next;
 		/* Don't touch checkpointed data */
 		if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED) &&
-					get_ckpt_valid_blocks(sbi, segno)))
+					get_ckpt_valid_blocks(sbi, segno) &&
+					p.alloc_mode != SSR))
 			goto next;
 		if (gc_type == BG_GC && test_bit(secno, dirty_i->victim_secmap))
 			goto next;

commit 93770ab7a6e963147a5dbca25278b69ba6c8f8c5
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Apr 15 15:26:32 2019 +0800

    f2fs: introduce DATA_GENERIC_ENHANCE
    
    Previously, f2fs_is_valid_blkaddr(, blkaddr, DATA_GENERIC) will check
    whether @blkaddr locates in main area or not.
    
    That check is weak, since the block address in range of main area can
    point to the address which is not valid in segment info table, and we
    can not detect such condition, we may suffer worse corruption as system
    continues running.
    
    So this patch introduce DATA_GENERIC_ENHANCE to enhance the sanity check
    which trigger SIT bitmap check rather than only range check.
    
    This patch did below changes as wel:
    - set SBI_NEED_FSCK in f2fs_is_valid_blkaddr().
    - get rid of is_valid_data_blkaddr() to avoid panic if blkaddr is invalid.
    - introduce verify_fio_blkaddr() to wrap fio {new,old}_blkaddr validation check.
    - spread blkaddr check in:
     * f2fs_get_node_info()
     * __read_out_blkaddrs()
     * f2fs_submit_page_read()
     * ra_data_block()
     * do_recover_data()
    
    This patch can fix bug reported from bugzilla below:
    
    https://bugzilla.kernel.org/show_bug.cgi?id=203215
    https://bugzilla.kernel.org/show_bug.cgi?id=203223
    https://bugzilla.kernel.org/show_bug.cgi?id=203231
    https://bugzilla.kernel.org/show_bug.cgi?id=203235
    https://bugzilla.kernel.org/show_bug.cgi?id=203241
    
    = Update by Jaegeuk Kim =
    
    DATA_GENERIC_ENHANCE enhanced to validate block addresses on read/write paths.
    But, xfstest/generic/446 compalins some generated kernel messages saying invalid
    bitmap was detected when reading a block. The reaons is, when we get the
    block addresses from extent_cache, there is no lock to synchronize it from
    truncating the blocks in parallel.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index ba30ec90952f..963fb4571fd9 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -656,6 +656,11 @@ static int ra_data_block(struct inode *inode, pgoff_t index)
 
 	if (f2fs_lookup_extent_cache(inode, index, &ei)) {
 		dn.data_blkaddr = ei.blk + index - ei.fofs;
+		if (unlikely(!f2fs_is_valid_blkaddr(sbi, dn.data_blkaddr,
+						DATA_GENERIC_ENHANCE_READ))) {
+			err = -EFAULT;
+			goto put_page;
+		}
 		goto got_it;
 	}
 
@@ -665,8 +670,12 @@ static int ra_data_block(struct inode *inode, pgoff_t index)
 		goto put_page;
 	f2fs_put_dnode(&dn);
 
+	if (!__is_valid_data_blkaddr(dn.data_blkaddr)) {
+		err = -ENOENT;
+		goto put_page;
+	}
 	if (unlikely(!f2fs_is_valid_blkaddr(sbi, dn.data_blkaddr,
-						DATA_GENERIC))) {
+						DATA_GENERIC_ENHANCE))) {
 		err = -EFAULT;
 		goto put_page;
 	}

commit d02a6e6174a772fa90c5efa51a55a04e6d8c0006
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Mar 25 21:08:19 2019 +0800

    f2fs: allow address pointer number of dnode aligning to specified size
    
    This patch expands scalability of dnode layout, it allows address pointer
    number of dnode aligning to specified size (now, the size is one byte by
    default), and later the number can align to compress cluster size
    (1 << n bytes, n=[2,..)), it can avoid cluster acrossing two dnode, making
    design of compress meta layout simple.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index a66a8752e5f6..ba30ec90952f 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -591,7 +591,7 @@ block_t f2fs_start_bidx_of_node(unsigned int node_ofs, struct inode *inode)
 		int dec = (node_ofs - indirect_blks - 3) / (NIDS_PER_BLOCK + 1);
 		bidx = node_ofs - 5 - dec;
 	}
-	return bidx * ADDRS_PER_BLOCK + ADDRS_PER_INODE(inode);
+	return bidx * ADDRS_PER_BLOCK(inode) + ADDRS_PER_INODE(inode);
 }
 
 static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,

commit 793ab1c8a792f8bccd7ae4c5be02bd275410b3af
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Apr 10 18:45:50 2019 +0800

    f2fs: fix to avoid deadloop in foreground GC
    
    As Jungyeon reported in bugzilla:
    
    https://bugzilla.kernel.org/show_bug.cgi?id=203211
    
    - Overview
    When mounting the attached crafted image and making a new file, I got this error and the error messages keep repeating.
    
    The image is intentionally fuzzed from a normal f2fs image for testing and I run with option CONFIG_F2FS_CHECK_FS on.
    
    - Reproduces
    mkdir test
    mount -t f2fs tmp.img test
    cd test
    touch t
    
    - Messages
    [   58.820451] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.821485] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.822530] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.823571] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.824616] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.825640] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.826663] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.827698] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.828719] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.829759] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.830783] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.831828] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.832869] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.833888] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.834945] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.835996] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.837028] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.838051] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.839072] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.840100] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.841147] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.842186] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.843214] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.844267] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.845282] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.846305] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    [   58.847341] F2FS-fs (sdb): Inconsistent segment (1) type [1, 0] in SSA and SIT
    ... (repeating)
    
    During GC, if segment type stored in SSA and SIT is inconsistent, we just
    skip migrating current segment directly, since we need to know the exact
    type to decide the migration function we use.
    
    So in foreground GC, we will easily run into a infinite loop as we may
    select the same victim segment which has inconsistent type due to greedy
    policy. In order to end up this, we choose to shutdown filesystem. For
    backgrond GC, we need to do that as well, so that we can avoid latter
    potential infinite looped foreground GC.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index ab764bd106de..a66a8752e5f6 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1175,6 +1175,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 				"type [%d, %d] in SSA and SIT",
 				segno, type, GET_SUM_TYPE((&sum->footer)));
 			set_sbi_flag(sbi, SBI_NEED_FSCK);
+			f2fs_stop_checkpoint(sbi, false);
 			goto skip;
 		}
 

commit 0916878da355650d7e77104a7ac0fa1784eca852
Author: Damien Le Moal <damien.lemoal@wdc.com>
Date:   Sat Mar 16 09:13:06 2019 +0900

    f2fs: Fix use of number of devices
    
    For a single device mount using a zoned block device, the zone
    information for the device is stored in the sbi->devs single entry
    array and sbi->s_ndevs is set to 1. This differs from a single device
    mount using a regular block device which does not allocate sbi->devs
    and sets sbi->s_ndevs to 0.
    
    However, sbi->s_devs == 0 condition is used throughout the code to
    differentiate a single device mount from a multi-device mount where
    sbi->s_ndevs is always larger than 1. This results in problems with
    single zoned block device volumes as these are treated as multi-device
    mounts but do not have the start_blk and end_blk information set. One
    of the problem observed is skipping of zone discard issuing resulting in
    write commands being issued to full zones or unaligned to a zone write
    pointer.
    
    Fix this problem by simply treating the cases sbi->s_ndevs == 0 (single
    regular block device mount) and sbi->s_ndevs == 1 (single zoned block
    device mount) in the same manner. This is done by introducing the
    helper function f2fs_is_multi_device() and using this helper in place
    of direct tests of sbi->s_ndevs value, improving code readability.
    
    Fixes: 7bb3a371d199 ("f2fs: Fix zoned block device support")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Damien Le Moal <damien.lemoal@wdc.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 195cf0f9d9ef..ab764bd106de 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1346,7 +1346,7 @@ void f2fs_build_gc_manager(struct f2fs_sb_info *sbi)
 	sbi->gc_pin_file_threshold = DEF_GC_FAILED_PINNED_FILES;
 
 	/* give warm/cold data area from slower device */
-	if (sbi->s_ndevs && !__is_large_section(sbi))
+	if (f2fs_is_multi_device(sbi) && !__is_large_section(sbi))
 		SIT_I(sbi)->last_victim[ALLOC_NEXT] =
 				GET_SEGNO(sbi, FDEV(0).end_blk) + 1;
 }

commit bae0ee7a767ceeea6d8e170da3f228fbc7480331
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Dec 25 17:43:42 2018 +0800

    f2fs: check PageWriteback flag for ordered case
    
    For all ordered cases in f2fs_wait_on_page_writeback(), we need to
    check PageWriteback status, so let's clean up to relocate the check
    into f2fs_wait_on_page_writeback().
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index aed7084e8443..195cf0f9d9ef 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -679,7 +679,7 @@ static int ra_data_block(struct inode *inode, pgoff_t index)
 	 * don't cache encrypted data into meta inode until previous dirty
 	 * data were writebacked to avoid racing between GC and flush.
 	 */
-	f2fs_wait_on_page_writeback(page, DATA, true);
+	f2fs_wait_on_page_writeback(page, DATA, true, true);
 
 	f2fs_wait_on_block_writeback(inode, dn.data_blkaddr);
 
@@ -768,7 +768,7 @@ static int move_data_block(struct inode *inode, block_t bidx,
 	 * don't cache encrypted data into meta inode until previous dirty
 	 * data were writebacked to avoid racing between GC and flush.
 	 */
-	f2fs_wait_on_page_writeback(page, DATA, true);
+	f2fs_wait_on_page_writeback(page, DATA, true, true);
 
 	f2fs_wait_on_block_writeback(inode, dn.data_blkaddr);
 
@@ -829,7 +829,7 @@ static int move_data_block(struct inode *inode, block_t bidx,
 	}
 
 write_page:
-	f2fs_wait_on_page_writeback(fio.encrypted_page, DATA, true);
+	f2fs_wait_on_page_writeback(fio.encrypted_page, DATA, true, true);
 	set_page_dirty(fio.encrypted_page);
 	if (clear_page_dirty_for_io(fio.encrypted_page))
 		dec_page_count(fio.sbi, F2FS_DIRTY_META);
@@ -838,7 +838,7 @@ static int move_data_block(struct inode *inode, block_t bidx,
 	ClearPageError(page);
 
 	/* allocate block address */
-	f2fs_wait_on_page_writeback(dn.node_page, NODE, true);
+	f2fs_wait_on_page_writeback(dn.node_page, NODE, true, true);
 
 	fio.op = REQ_OP_WRITE;
 	fio.op_flags = REQ_SYNC;
@@ -924,7 +924,7 @@ static int move_data_page(struct inode *inode, block_t bidx, int gc_type,
 		bool is_dirty = PageDirty(page);
 
 retry:
-		f2fs_wait_on_page_writeback(page, DATA, true);
+		f2fs_wait_on_page_writeback(page, DATA, true, true);
 
 		set_page_dirty(page);
 		if (clear_page_dirty_for_io(page)) {

commit 5222595d093ebe80329d38d255d14316257afb3e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Dec 13 18:38:33 2018 -0800

    f2fs: use kvmalloc, if kmalloc is failed
    
    One report says memalloc failure during mount.
    
     (unwind_backtrace) from [<c010cd4c>] (show_stack+0x10/0x14)
     (show_stack) from [<c049c6b8>] (dump_stack+0x8c/0xa0)
     (dump_stack) from [<c024fcf0>] (warn_alloc+0xc4/0x160)
     (warn_alloc) from [<c0250218>] (__alloc_pages_nodemask+0x3f4/0x10d0)
     (__alloc_pages_nodemask) from [<c0270450>] (kmalloc_order_trace+0x2c/0x120)
     (kmalloc_order_trace) from [<c03fa748>] (build_node_manager+0x35c/0x688)
     (build_node_manager) from [<c03de494>] (f2fs_fill_super+0xf0c/0x16cc)
     (f2fs_fill_super) from [<c02a5864>] (mount_bdev+0x15c/0x188)
     (mount_bdev) from [<c03da624>] (f2fs_mount+0x18/0x20)
     (f2fs_mount) from [<c02a68b8>] (mount_fs+0x158/0x19c)
     (mount_fs) from [<c02c3c9c>] (vfs_kern_mount+0x78/0x134)
     (vfs_kern_mount) from [<c02c76ac>] (do_mount+0x474/0xca4)
     (do_mount) from [<c02c8264>] (SyS_mount+0x94/0xbc)
     (SyS_mount) from [<c0108180>] (ret_fast_syscall+0x0/0x48)
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 666518eb7293..aed7084e8443 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -142,7 +142,7 @@ int f2fs_start_gc_thread(struct f2fs_sb_info *sbi)
 			"f2fs_gc-%u:%u", MAJOR(dev), MINOR(dev));
 	if (IS_ERR(gc_th->f2fs_gc_task)) {
 		err = PTR_ERR(gc_th->f2fs_gc_task);
-		kfree(gc_th);
+		kvfree(gc_th);
 		sbi->gc_thread = NULL;
 	}
 out:
@@ -155,7 +155,7 @@ void f2fs_stop_gc_thread(struct f2fs_sb_info *sbi)
 	if (!gc_th)
 		return;
 	kthread_stop(gc_th->f2fs_gc_task);
-	kfree(gc_th);
+	kvfree(gc_th);
 	sbi->gc_thread = NULL;
 }
 

commit 8d64d365aed355b2e2465d19ba61df02462fff4d
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Dec 12 18:12:30 2018 +0800

    f2fs: fix to reorder set_page_dirty and wait_on_page_writeback
    
    This patch reorders flow from
    
    - update page
    - set_page_dirty
    - wait_on_page_writeback
    
    to
    
    - wait_on_page_writeback
    - update page
    - set_page_dirty
    
    The reason is:
    - set_page_dirty will increase reference of dirty page, the reference
    should be cleared before wait_on_page_writeback to keep its consistency.
    - some devices need stable page during page writebacking, so we
    should not change page's data.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 71462f2e47d4..666518eb7293 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -829,8 +829,8 @@ static int move_data_block(struct inode *inode, block_t bidx,
 	}
 
 write_page:
-	set_page_dirty(fio.encrypted_page);
 	f2fs_wait_on_page_writeback(fio.encrypted_page, DATA, true);
+	set_page_dirty(fio.encrypted_page);
 	if (clear_page_dirty_for_io(fio.encrypted_page))
 		dec_page_count(fio.sbi, F2FS_DIRTY_META);
 
@@ -924,8 +924,9 @@ static int move_data_page(struct inode *inode, block_t bidx, int gc_type,
 		bool is_dirty = PageDirty(page);
 
 retry:
-		set_page_dirty(page);
 		f2fs_wait_on_page_writeback(page, DATA, true);
+
+		set_page_dirty(page);
 		if (clear_page_dirty_for_io(page)) {
 			inode_dec_dirty_pages(inode);
 			f2fs_remove_dirty_inode(inode);

commit e3c59108da8655de78fe59843372b8cfab1a8e33
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Mon Nov 26 13:31:42 2018 +0530

    f2fs: adjust trace print in f2fs_get_victim() to cover all paths
    
    Adjust the trace print in f2fs_get_victim() to cover GC done by
    F2FS_IOC_GARBAGE_COLLECT_RANGE.
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 9a60801ab1c5..71462f2e47d4 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -420,11 +420,12 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 				set_bit(secno, dirty_i->victim_secmap);
 		}
 
+	}
+out:
+	if (p.min_segno != NULL_SEGNO)
 		trace_f2fs_get_victim(sbi->sb, type, gc_type, &p,
 				sbi->cur_victim_sec,
 				prefree_segments(sbi), free_segments(sbi));
-	}
-out:
 	mutex_unlock(&dirty_i->seglist_lock);
 
 	return (p.min_segno == NULL_SEGNO) ? 0 : 1;

commit 08ac9a3870f6babb2b1fff46118536ca8a71ef19
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Mon Nov 26 13:31:41 2018 +0530

    f2fs: fix to allow node segment for GC by ioctl path
    
    Allow node type segments also to be GC'd via f2fs ioctl
    F2FS_IOC_GARBAGE_COLLECT_RANGE.
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 8606ebf509cb..9a60801ab1c5 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -323,8 +323,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	p.min_cost = get_max_cost(sbi, &p);
 
 	if (*result != NULL_SEGNO) {
-		if (IS_DATASEG(get_seg_entry(sbi, *result)->type) &&
-			get_valid_blocks(sbi, *result, false) &&
+		if (get_valid_blocks(sbi, *result, false) &&
 			!sec_usage_check(sbi, GET_SEC_FROM_SEG(sbi, *result)))
 			p.min_segno = *result;
 		goto out;

commit e3080b0120a15e648f59491919cb5da0bc9802c3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Oct 24 18:37:27 2018 +0800

    f2fs: support subsectional garbage collection
    
    Section is minimal garbage collection unit of f2fs, in zoned block
    device, or ancient block mapping flash device, in order to improve
    GC efficiency, we can align GC unit to lower device erase unit,
    normally, it consists of multiple of segments.
    
    Once background or foreground GC triggers, it brings a large number
    of IOs, which will impact user IO, and also occupy cpu/memory resource
    intensively.
    
    So, to reduce impact of GC on large size section, this patch supports
    subsectional GC, in one cycle of GC, it only migrate partial segment{s}
    in victim section. Currently, by default, we use sbi->segs_per_sec as
    migration granularity.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 84f49cb3147c..8606ebf509cb 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -333,6 +333,22 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	if (p.max_search == 0)
 		goto out;
 
+	if (__is_large_section(sbi) && p.alloc_mode == LFS) {
+		if (sbi->next_victim_seg[BG_GC] != NULL_SEGNO) {
+			p.min_segno = sbi->next_victim_seg[BG_GC];
+			*result = p.min_segno;
+			sbi->next_victim_seg[BG_GC] = NULL_SEGNO;
+			goto got_result;
+		}
+		if (gc_type == FG_GC &&
+				sbi->next_victim_seg[FG_GC] != NULL_SEGNO) {
+			p.min_segno = sbi->next_victim_seg[FG_GC];
+			*result = p.min_segno;
+			sbi->next_victim_seg[FG_GC] = NULL_SEGNO;
+			goto got_result;
+		}
+	}
+
 	last_victim = sm->last_victim[p.gc_mode];
 	if (p.alloc_mode == LFS && gc_type == FG_GC) {
 		p.min_segno = check_bg_victims(sbi);
@@ -395,6 +411,8 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	}
 	if (p.min_segno != NULL_SEGNO) {
 got_it:
+		*result = (p.min_segno / p.ofs_unit) * p.ofs_unit;
+got_result:
 		if (p.alloc_mode == LFS) {
 			secno = GET_SEC_FROM_SEG(sbi, p.min_segno);
 			if (gc_type == FG_GC)
@@ -402,7 +420,6 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 			else
 				set_bit(secno, dirty_i->victim_secmap);
 		}
-		*result = (p.min_segno / p.ofs_unit) * p.ofs_unit;
 
 		trace_f2fs_get_victim(sbi->sb, type, gc_type, &p,
 				sbi->cur_victim_sec,
@@ -1103,15 +1120,18 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	struct blk_plug plug;
 	unsigned int segno = start_segno;
 	unsigned int end_segno = start_segno + sbi->segs_per_sec;
-	int seg_freed = 0;
+	int seg_freed = 0, migrated = 0;
 	unsigned char type = IS_DATASEG(get_seg_entry(sbi, segno)->type) ?
 						SUM_TYPE_DATA : SUM_TYPE_NODE;
 	int submitted = 0;
 
+	if (__is_large_section(sbi))
+		end_segno = rounddown(end_segno, sbi->segs_per_sec);
+
 	/* readahead multi ssa blocks those have contiguous address */
 	if (__is_large_section(sbi))
 		f2fs_ra_meta_pages(sbi, GET_SUM_BLOCK(sbi, segno),
-					sbi->segs_per_sec, META_SSA, true);
+					end_segno - segno, META_SSA, true);
 
 	/* reference all summary page */
 	while (segno < end_segno) {
@@ -1142,8 +1162,11 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 
 		if (get_valid_blocks(sbi, segno, false) == 0)
 			goto freed;
+		if (__is_large_section(sbi) &&
+				migrated >= sbi->migration_granularity)
+			goto skip;
 		if (!PageUptodate(sum_page) || unlikely(f2fs_cp_error(sbi)))
-			goto next;
+			goto skip;
 
 		sum = page_address(sum_page);
 		if (type != GET_SUM_TYPE((&sum->footer))) {
@@ -1151,7 +1174,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 				"type [%d, %d] in SSA and SIT",
 				segno, type, GET_SUM_TYPE((&sum->footer)));
 			set_sbi_flag(sbi, SBI_NEED_FSCK);
-			goto next;
+			goto skip;
 		}
 
 		/*
@@ -1174,7 +1197,11 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 		if (gc_type == FG_GC &&
 				get_valid_blocks(sbi, segno, false) == 0)
 			seg_freed++;
-next:
+		migrated++;
+
+		if (__is_large_section(sbi) && segno + 1 < end_segno)
+			sbi->next_victim_seg[gc_type] = segno + 1;
+skip:
 		f2fs_put_page(sum_page, 0);
 	}
 

commit 2c70c5e3874e8cf2f39f4ce4e2b832f4380a0c1b
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Oct 24 18:37:26 2018 +0800

    f2fs: introduce __is_large_section() for cleanup
    
    Introduce a wrapper __is_large_section() to clean up codes.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 9707773fdaac..84f49cb3147c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1109,7 +1109,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	int submitted = 0;
 
 	/* readahead multi ssa blocks those have contiguous address */
-	if (sbi->segs_per_sec > 1)
+	if (__is_large_section(sbi))
 		f2fs_ra_meta_pages(sbi, GET_SUM_BLOCK(sbi, segno),
 					sbi->segs_per_sec, META_SSA, true);
 
@@ -1318,7 +1318,7 @@ void f2fs_build_gc_manager(struct f2fs_sb_info *sbi)
 	sbi->gc_pin_file_threshold = DEF_GC_FAILED_PINNED_FILES;
 
 	/* give warm/cold data area from slower device */
-	if (sbi->s_ndevs && sbi->segs_per_sec == 1)
+	if (sbi->s_ndevs && !__is_large_section(sbi))
 		SIT_I(sbi)->last_victim[ALLOC_NEXT] =
 				GET_SEGNO(sbi, FDEV(0).end_blk) + 1;
 }

commit d6c66cd19ef322fe0d51ba09ce1b7f386acab04a
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Wed Oct 24 16:08:30 2018 +0800

    f2fs: fix count of seg_freed to make sec_freed correct
    
    When sbi->segs_per_sec > 1, and if some segno has 0 valid blocks before
    gc starts, do_garbage_collect will skip counting seg_freed++, and this
    will cause seg_freed < sbi->segs_per_sec and finally skip sec_freed++.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c96e7c6354ef..9707773fdaac 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1140,9 +1140,9 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 					GET_SUM_BLOCK(sbi, segno));
 		f2fs_put_page(sum_page, 0);
 
-		if (get_valid_blocks(sbi, segno, false) == 0 ||
-				!PageUptodate(sum_page) ||
-				unlikely(f2fs_cp_error(sbi)))
+		if (get_valid_blocks(sbi, segno, false) == 0)
+			goto freed;
+		if (!PageUptodate(sum_page) || unlikely(f2fs_cp_error(sbi)))
 			goto next;
 
 		sum = page_address(sum_page);
@@ -1170,6 +1170,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 
 		stat_inc_seg_count(sbi, type, gc_type);
 
+freed:
 		if (gc_type == FG_GC &&
 				get_valid_blocks(sbi, segno, false) == 0)
 			seg_freed++;

commit 9bf1a3f73927492c8be127b642197125e9d52be8
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Sep 18 20:39:53 2018 +0800

    f2fs: avoid GC causing encrypted file corrupted
    
    The encrypted file may be corrupted by GC in following case:
    
    Time 1: | segment 1 blkaddr = A |  GC -> | segment 2 blkaddr = B |
    Encrypted block 1 is moved from blkaddr A of segment 1 to blkaddr B of
    segment 2,
    
    Time 2: | segment 1 blkaddr = B |  GC -> | segment 3 blkaddr = C |
    
    Before page 1 is written back and if segment 2 become a victim, then
    page 1 is moved from blkaddr B of segment 2 to blkaddr Cof segment 3,
    during the GC process of Time 2, f2fs should wait for page 1 written back
    before reading it, or move_data_block will read a garbage block from
    blkaddr B since page is not written back to blkaddr B yet.
    
    Commit 6aa58d8a ("f2fs: readahead encrypted block during GC") introduce
    ra_data_block to read encrypted block, but it forgets to add
    f2fs_wait_on_page_writeback to avoid racing between GC and flush.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index a07241fb8537..c96e7c6354ef 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -658,6 +658,14 @@ static int ra_data_block(struct inode *inode, pgoff_t index)
 	fio.page = page;
 	fio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;
 
+	/*
+	 * don't cache encrypted data into meta inode until previous dirty
+	 * data were writebacked to avoid racing between GC and flush.
+	 */
+	f2fs_wait_on_page_writeback(page, DATA, true);
+
+	f2fs_wait_on_block_writeback(inode, dn.data_blkaddr);
+
 	fio.encrypted_page = f2fs_pagecache_get_page(META_MAPPING(sbi),
 					dn.data_blkaddr,
 					FGP_LOCK | FGP_CREAT, GFP_NOFS);
@@ -745,6 +753,8 @@ static int move_data_block(struct inode *inode, block_t bidx,
 	 */
 	f2fs_wait_on_page_writeback(page, DATA, true);
 
+	f2fs_wait_on_block_writeback(inode, dn.data_blkaddr);
+
 	err = f2fs_get_node_info(fio.sbi, dn.nid, &ni);
 	if (err)
 		goto put_out;

commit 48018b4cfd07dd2df9a067fb3a6a3221c19eed11
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Sep 13 07:40:53 2018 +0800

    f2fs: submit cached bio to avoid endless PageWriteback
    
    When migrating encrypted block from background GC thread, we only add
    them into f2fs inner bio cache, but forget to submit the cached bio, it
    may cause potential deadlock when we are waiting page writebacked, fix
    it.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 78288c54b68c..a07241fb8537 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -473,7 +473,7 @@ static int check_valid_map(struct f2fs_sb_info *sbi,
  * On validity, copy that node with cold status, otherwise (invalid node)
  * ignore that.
  */
-static void gc_node_segment(struct f2fs_sb_info *sbi,
+static int gc_node_segment(struct f2fs_sb_info *sbi,
 		struct f2fs_summary *sum, unsigned int segno, int gc_type)
 {
 	struct f2fs_summary *entry;
@@ -481,6 +481,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 	int off;
 	int phase = 0;
 	bool fggc = (gc_type == FG_GC);
+	int submitted = 0;
 
 	start_addr = START_BLOCK(sbi, segno);
 
@@ -494,10 +495,11 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 		nid_t nid = le32_to_cpu(entry->nid);
 		struct page *node_page;
 		struct node_info ni;
+		int err;
 
 		/* stop BG_GC if there is not enough free sections. */
 		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0))
-			return;
+			return submitted;
 
 		if (check_valid_map(sbi, segno, off) == 0)
 			continue;
@@ -534,7 +536,9 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 			continue;
 		}
 
-		f2fs_move_node_page(node_page, gc_type);
+		err = f2fs_move_node_page(node_page, gc_type);
+		if (!err && gc_type == FG_GC)
+			submitted++;
 		stat_inc_node_blk_count(sbi, 1, gc_type);
 	}
 
@@ -543,6 +547,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 
 	if (fggc)
 		atomic_dec(&sbi->wb_sync_req[NODE]);
+	return submitted;
 }
 
 /*
@@ -678,7 +683,7 @@ static int ra_data_block(struct inode *inode, pgoff_t index)
  * Move data block via META_MAPPING while keeping locked data page.
  * This can be used to move blocks, aka LBAs, directly on disk.
  */
-static void move_data_block(struct inode *inode, block_t bidx,
+static int move_data_block(struct inode *inode, block_t bidx,
 				int gc_type, unsigned int segno, int off)
 {
 	struct f2fs_io_info fio = {
@@ -697,25 +702,29 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	struct node_info ni;
 	struct page *page, *mpage;
 	block_t newaddr;
-	int err;
+	int err = 0;
 	bool lfs_mode = test_opt(fio.sbi, LFS);
 
 	/* do not read out */
 	page = f2fs_grab_cache_page(inode->i_mapping, bidx, false);
 	if (!page)
-		return;
+		return -ENOMEM;
 
-	if (!check_valid_map(F2FS_I_SB(inode), segno, off))
+	if (!check_valid_map(F2FS_I_SB(inode), segno, off)) {
+		err = -ENOENT;
 		goto out;
+	}
 
 	if (f2fs_is_atomic_file(inode)) {
 		F2FS_I(inode)->i_gc_failures[GC_FAILURE_ATOMIC]++;
 		F2FS_I_SB(inode)->skipped_atomic_files[gc_type]++;
+		err = -EAGAIN;
 		goto out;
 	}
 
 	if (f2fs_is_pinned_file(inode)) {
 		f2fs_pin_file_control(inode, true);
+		err = -EAGAIN;
 		goto out;
 	}
 
@@ -726,6 +735,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 
 	if (unlikely(dn.data_blkaddr == NULL_ADDR)) {
 		ClearPageUptodate(page);
+		err = -ENOENT;
 		goto put_out;
 	}
 
@@ -808,6 +818,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	fio.new_blkaddr = newaddr;
 	f2fs_submit_page_write(&fio);
 	if (fio.retry) {
+		err = -EAGAIN;
 		if (PageWriteback(fio.encrypted_page))
 			end_page_writeback(fio.encrypted_page);
 		goto put_page_out;
@@ -831,34 +842,42 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	f2fs_put_dnode(&dn);
 out:
 	f2fs_put_page(page, 1);
+	return err;
 }
 
-static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
+static int move_data_page(struct inode *inode, block_t bidx, int gc_type,
 							unsigned int segno, int off)
 {
 	struct page *page;
+	int err = 0;
 
 	page = f2fs_get_lock_data_page(inode, bidx, true);
 	if (IS_ERR(page))
-		return;
+		return PTR_ERR(page);
 
-	if (!check_valid_map(F2FS_I_SB(inode), segno, off))
+	if (!check_valid_map(F2FS_I_SB(inode), segno, off)) {
+		err = -ENOENT;
 		goto out;
+	}
 
 	if (f2fs_is_atomic_file(inode)) {
 		F2FS_I(inode)->i_gc_failures[GC_FAILURE_ATOMIC]++;
 		F2FS_I_SB(inode)->skipped_atomic_files[gc_type]++;
+		err = -EAGAIN;
 		goto out;
 	}
 	if (f2fs_is_pinned_file(inode)) {
 		if (gc_type == FG_GC)
 			f2fs_pin_file_control(inode, true);
+		err = -EAGAIN;
 		goto out;
 	}
 
 	if (gc_type == BG_GC) {
-		if (PageWriteback(page))
+		if (PageWriteback(page)) {
+			err = -EAGAIN;
 			goto out;
+		}
 		set_page_dirty(page);
 		set_cold_data(page);
 	} else {
@@ -876,7 +895,6 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 			.io_type = FS_GC_DATA_IO,
 		};
 		bool is_dirty = PageDirty(page);
-		int err;
 
 retry:
 		set_page_dirty(page);
@@ -901,6 +919,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 	}
 out:
 	f2fs_put_page(page, 1);
+	return err;
 }
 
 /*
@@ -910,7 +929,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
  * If the parent node is not valid or the data block address is different,
  * the victim data block is ignored.
  */
-static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
+static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		struct gc_inode_list *gc_list, unsigned int segno, int gc_type)
 {
 	struct super_block *sb = sbi->sb;
@@ -918,6 +937,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	block_t start_addr;
 	int off;
 	int phase = 0;
+	int submitted = 0;
 
 	start_addr = START_BLOCK(sbi, segno);
 
@@ -934,7 +954,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 		/* stop BG_GC if there is not enough free sections. */
 		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0))
-			return;
+			return submitted;
 
 		if (check_valid_map(sbi, segno, off) == 0)
 			continue;
@@ -1006,6 +1026,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		if (inode) {
 			struct f2fs_inode_info *fi = F2FS_I(inode);
 			bool locked = false;
+			int err;
 
 			if (S_ISREG(inode->i_mode)) {
 				if (!down_write_trylock(&fi->i_gc_rwsem[READ]))
@@ -1025,12 +1046,16 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			start_bidx = f2fs_start_bidx_of_node(nofs, inode)
 								+ ofs_in_node;
 			if (f2fs_post_read_required(inode))
-				move_data_block(inode, start_bidx, gc_type,
-								segno, off);
+				err = move_data_block(inode, start_bidx,
+							gc_type, segno, off);
 			else
-				move_data_page(inode, start_bidx, gc_type,
+				err = move_data_page(inode, start_bidx, gc_type,
 								segno, off);
 
+			if (!err && (gc_type == FG_GC ||
+					f2fs_post_read_required(inode)))
+				submitted++;
+
 			if (locked) {
 				up_write(&fi->i_gc_rwsem[WRITE]);
 				up_write(&fi->i_gc_rwsem[READ]);
@@ -1042,6 +1067,8 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 	if (++phase < 5)
 		goto next_step;
+
+	return submitted;
 }
 
 static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
@@ -1069,6 +1096,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	int seg_freed = 0;
 	unsigned char type = IS_DATASEG(get_seg_entry(sbi, segno)->type) ?
 						SUM_TYPE_DATA : SUM_TYPE_NODE;
+	int submitted = 0;
 
 	/* readahead multi ssa blocks those have contiguous address */
 	if (sbi->segs_per_sec > 1)
@@ -1124,10 +1152,11 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 		 *                                  - lock_page(sum_page)
 		 */
 		if (type == SUM_TYPE_NODE)
-			gc_node_segment(sbi, sum->entries, segno, gc_type);
-		else
-			gc_data_segment(sbi, sum->entries, gc_list, segno,
+			submitted += gc_node_segment(sbi, sum->entries, segno,
 								gc_type);
+		else
+			submitted += gc_data_segment(sbi, sum->entries, gc_list,
+							segno, gc_type);
 
 		stat_inc_seg_count(sbi, type, gc_type);
 
@@ -1138,7 +1167,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 		f2fs_put_page(sum_page, 0);
 	}
 
-	if (gc_type == FG_GC)
+	if (submitted)
 		f2fs_submit_merged_write(sbi,
 				(type == SUM_TYPE_NODE) ? NODE : DATA);
 

commit 4354994f097d068a894aa1a0860da54571df3582
Author: Daniel Rosenberg <drosen@google.com>
Date:   Mon Aug 20 19:21:43 2018 -0700

    f2fs: checkpoint disabling
    
    Note that, it requires "f2fs: return correct errno in f2fs_gc".
    
    This adds a lightweight non-persistent snapshotting scheme to f2fs.
    
    To use, mount with the option checkpoint=disable, and to return to
    normal operation, remount with checkpoint=enable. If the filesystem
    is shut down before remounting with checkpoint=enable, it will revert
    back to its apparent state when it was first mounted with
    checkpoint=disable. This is useful for situations where you wish to be
    able to roll back the state of the disk in case of some critical
    failure.
    
    Signed-off-by: Daniel Rosenberg <drosen@google.com>
    [Jaegeuk Kim: use SB_RDONLY instead of MS_RDONLY]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 99ed8a5d9249..78288c54b68c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -370,6 +370,10 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 
 		if (sec_usage_check(sbi, secno))
 			goto next;
+		/* Don't touch checkpointed data */
+		if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED) &&
+					get_ckpt_valid_blocks(sbi, segno)))
+			goto next;
 		if (gc_type == BG_GC && test_bit(secno, dirty_i->victim_secmap))
 			goto next;
 
@@ -1189,7 +1193,8 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 		 * threshold, we can make them free by checkpoint. Then, we
 		 * secure free segments which doesn't need fggc any more.
 		 */
-		if (prefree_segments(sbi)) {
+		if (prefree_segments(sbi) &&
+				!is_sbi_flag_set(sbi, SBI_CP_DISABLED)) {
 			ret = f2fs_write_checkpoint(sbi, &cpc);
 			if (ret)
 				goto stop;
@@ -1241,7 +1246,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 			segno = NULL_SEGNO;
 			goto gc_more;
 		}
-		if (gc_type == FG_GC)
+		if (gc_type == FG_GC && !is_sbi_flag_set(sbi, SBI_CP_DISABLED))
 			ret = f2fs_write_checkpoint(sbi, &cpc);
 	}
 stop:

commit 274bd9ba39425610fdb9a6827602197a5cd27cd8
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Sep 29 18:31:28 2018 +0800

    f2fs: add to account skip count of background GC
    
    This patch adds to account skip count of background GC, and show stat
    info via 'status' debugfs entry.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index f541aa3ff7d5..99ed8a5d9249 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -40,13 +40,16 @@ static int gc_thread_func(void *data)
 		if (gc_th->gc_wake)
 			gc_th->gc_wake = 0;
 
-		if (try_to_freeze())
+		if (try_to_freeze()) {
+			stat_other_skip_bggc_count(sbi);
 			continue;
+		}
 		if (kthread_should_stop())
 			break;
 
 		if (sbi->sb->s_writers.frozen >= SB_FREEZE_WRITE) {
 			increase_sleep_time(gc_th, &wait_ms);
+			stat_other_skip_bggc_count(sbi);
 			continue;
 		}
 
@@ -55,8 +58,10 @@ static int gc_thread_func(void *data)
 			f2fs_stop_checkpoint(sbi, false);
 		}
 
-		if (!sb_start_write_trylock(sbi->sb))
+		if (!sb_start_write_trylock(sbi->sb)) {
+			stat_other_skip_bggc_count(sbi);
 			continue;
+		}
 
 		/*
 		 * [GC triggering condition]
@@ -77,12 +82,15 @@ static int gc_thread_func(void *data)
 			goto do_gc;
 		}
 
-		if (!mutex_trylock(&sbi->gc_mutex))
+		if (!mutex_trylock(&sbi->gc_mutex)) {
+			stat_other_skip_bggc_count(sbi);
 			goto next;
+		}
 
 		if (!is_idle(sbi, GC_TIME)) {
 			increase_sleep_time(gc_th, &wait_ms);
 			mutex_unlock(&sbi->gc_mutex);
+			stat_io_skip_bggc_count(sbi);
 			goto next;
 		}
 

commit 61f7725aa148ee870436a29d3a24d5c00ab7e9af
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 25 15:25:21 2018 -0700

    f2fs: return correct errno in f2fs_gc
    
    This fixes overriding error number in f2fs_gc.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 247b31e73303..f541aa3ff7d5 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1253,7 +1253,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 
 	put_gc_inode(&gc_list);
 
-	if (sync)
+	if (sync && !ret)
 		ret = sec_freed ? 0 : -EAGAIN;
 	return ret;
 }

commit edc55aaf0d1712b54a3704dd58423c7e495534fe
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Sep 17 17:36:06 2018 -0700

    f2fs: avoid f2fs_bug_on if f2fs_get_meta_page_nofail got EIO
    
    This patch avoids BUG_ON when f2fs_get_meta_page_nofail got EIO during
    xfstests/generic/475.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 77ffa8045a3b..247b31e73303 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1066,6 +1066,18 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	/* reference all summary page */
 	while (segno < end_segno) {
 		sum_page = f2fs_get_sum_page(sbi, segno++);
+		if (IS_ERR(sum_page)) {
+			int err = PTR_ERR(sum_page);
+
+			end_segno = segno - 1;
+			for (segno = start_segno; segno < end_segno; segno++) {
+				sum_page = find_get_page(META_MAPPING(sbi),
+						GET_SUM_BLOCK(sbi, segno));
+				f2fs_put_page(sum_page, 0);
+				f2fs_put_page(sum_page, 0);
+			}
+			return err;
+		}
 		unlock_page(sum_page);
 	}
 

commit a7d10cf3e4e3e308da01462a1ef8008233ee523d
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Wed Sep 19 14:18:47 2018 +0530

    f2fs: add new idle interval timing for discard and gc paths
    
    This helps to control the frequency of submission of discard and
    GC requests independently, based on the need.
    
    Suggested-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index a4c1a419611d..77ffa8045a3b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -80,7 +80,7 @@ static int gc_thread_func(void *data)
 		if (!mutex_trylock(&sbi->gc_mutex))
 			goto next;
 
-		if (!is_idle(sbi)) {
+		if (!is_idle(sbi, GC_TIME)) {
 			increase_sleep_time(gc_th, &wait_ms);
 			mutex_unlock(&sbi->gc_mutex);
 			goto next;

commit 7c1a000d466235c875a989971cfda344e6bb1166
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Sep 12 09:16:07 2018 +0800

    f2fs: add SPDX license identifiers
    
    Remove the verbose license text from f2fs files and replace them with
    SPDX tags.  This does not change the license of any of the code.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 5c8d00422237..a4c1a419611d 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1,12 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * fs/f2fs/gc.c
  *
  * Copyright (c) 2012 Samsung Electronics Co., Ltd.
  *             http://www.samsung.com/
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #include <linux/fs.h>
 #include <linux/module.h>

commit 6aa58d8ad20a3323f42274c25820a6f54192422d
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Aug 14 22:37:25 2018 +0800

    f2fs: readahead encrypted block during GC
    
    During GC, for each encrypted block, we will read block synchronously
    into meta page, and then submit it into current cold data log area.
    
    So this block read model with 4k granularity can make poor performance,
    like migrating non-encrypted block, let's readahead encrypted block
    as well to improve migration performance.
    
    To implement this, we choose meta page that its index is old block
    address of the encrypted block, and readahead ciphertext into this
    page, later, if readaheaded page is still updated, we will load its
    data into target meta page, and submit the write IO.
    
    Note that for OPU, truncation, deletion, we need to invalid meta
    page after we invalid old block address, to make sure we won't load
    invalid data from target meta page during encrypted block migration.
    
    for ((i = 0; i < 1000; i++))
    do {
            xfs_io -f /mnt/f2fs/dir/$i -c "pwrite 0 128k" -c "fsync";
    } done
    
    for ((i = 0; i < 1000; i+=2))
    do {
            rm /mnt/f2fs/dir/$i;
    } done
    
    ret = ioctl(fd, F2FS_IOC_GARBAGE_COLLECT, 0);
    
    Before:
                  gc-6549  [001] d..1 214682.212797: block_rq_insert: 8,32 RA 32768 () 786400 + 64 [gc]
                  gc-6549  [001] d..1 214682.212802: block_unplug: [gc] 1
                  gc-6549  [001] .... 214682.213892: block_bio_queue: 8,32 R 67494144 + 8 [gc]
                  gc-6549  [001] .... 214682.213899: block_getrq: 8,32 R 67494144 + 8 [gc]
                  gc-6549  [001] .... 214682.213902: block_plug: [gc]
                  gc-6549  [001] d..1 214682.213905: block_rq_insert: 8,32 R 4096 () 67494144 + 8 [gc]
                  gc-6549  [001] d..1 214682.213908: block_unplug: [gc] 1
                  gc-6549  [001] .... 214682.226405: block_bio_queue: 8,32 R 67494152 + 8 [gc]
                  gc-6549  [001] .... 214682.226412: block_getrq: 8,32 R 67494152 + 8 [gc]
                  gc-6549  [001] .... 214682.226414: block_plug: [gc]
                  gc-6549  [001] d..1 214682.226417: block_rq_insert: 8,32 R 4096 () 67494152 + 8 [gc]
                  gc-6549  [001] d..1 214682.226420: block_unplug: [gc] 1
                  gc-6549  [001] .... 214682.226904: block_bio_queue: 8,32 R 67494160 + 8 [gc]
                  gc-6549  [001] .... 214682.226910: block_getrq: 8,32 R 67494160 + 8 [gc]
                  gc-6549  [001] .... 214682.226911: block_plug: [gc]
                  gc-6549  [001] d..1 214682.226914: block_rq_insert: 8,32 R 4096 () 67494160 + 8 [gc]
                  gc-6549  [001] d..1 214682.226916: block_unplug: [gc] 1
    
    After:
                  gc-5678  [003] .... 214327.025906: block_bio_queue: 8,32 R 67493824 + 8 [gc]
                  gc-5678  [003] .... 214327.025908: block_bio_backmerge: 8,32 R 67493824 + 8 [gc]
                  gc-5678  [003] .... 214327.025915: block_bio_queue: 8,32 R 67493832 + 8 [gc]
                  gc-5678  [003] .... 214327.025917: block_bio_backmerge: 8,32 R 67493832 + 8 [gc]
                  gc-5678  [003] .... 214327.025923: block_bio_queue: 8,32 R 67493840 + 8 [gc]
                  gc-5678  [003] .... 214327.025925: block_bio_backmerge: 8,32 R 67493840 + 8 [gc]
                  gc-5678  [003] .... 214327.025932: block_bio_queue: 8,32 R 67493848 + 8 [gc]
                  gc-5678  [003] .... 214327.025934: block_bio_backmerge: 8,32 R 67493848 + 8 [gc]
                  gc-5678  [003] .... 214327.025941: block_bio_queue: 8,32 R 67493856 + 8 [gc]
                  gc-5678  [003] .... 214327.025943: block_bio_backmerge: 8,32 R 67493856 + 8 [gc]
                  gc-5678  [003] .... 214327.025953: block_bio_queue: 8,32 R 67493864 + 8 [gc]
                  gc-5678  [003] .... 214327.025955: block_bio_backmerge: 8,32 R 67493864 + 8 [gc]
                  gc-5678  [003] .... 214327.025962: block_bio_queue: 8,32 R 67493872 + 8 [gc]
                  gc-5678  [003] .... 214327.025964: block_bio_backmerge: 8,32 R 67493872 + 8 [gc]
                  gc-5678  [003] .... 214327.025970: block_bio_queue: 8,32 R 67493880 + 8 [gc]
                  gc-5678  [003] .... 214327.025972: block_bio_backmerge: 8,32 R 67493880 + 8 [gc]
                  gc-5678  [003] .... 214327.026000: block_bio_queue: 8,32 WS 34123776 + 2048 [gc]
                  gc-5678  [003] .... 214327.026019: block_getrq: 8,32 WS 34123776 + 2048 [gc]
                  gc-5678  [003] d..1 214327.026021: block_rq_insert: 8,32 R 131072 () 67493632 + 256 [gc]
                  gc-5678  [003] d..1 214327.026023: block_unplug: [gc] 1
                  gc-5678  [003] d..1 214327.026026: block_rq_issue: 8,32 R 131072 () 67493632 + 256 [gc]
                  gc-5678  [003] .... 214327.026046: block_plug: [gc]
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c598ae5ecbfa..5c8d00422237 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -599,6 +599,72 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	return true;
 }
 
+static int ra_data_block(struct inode *inode, pgoff_t index)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct address_space *mapping = inode->i_mapping;
+	struct dnode_of_data dn;
+	struct page *page;
+	struct extent_info ei = {0, 0, 0};
+	struct f2fs_io_info fio = {
+		.sbi = sbi,
+		.ino = inode->i_ino,
+		.type = DATA,
+		.temp = COLD,
+		.op = REQ_OP_READ,
+		.op_flags = 0,
+		.encrypted_page = NULL,
+		.in_list = false,
+		.retry = false,
+	};
+	int err;
+
+	page = f2fs_grab_cache_page(mapping, index, true);
+	if (!page)
+		return -ENOMEM;
+
+	if (f2fs_lookup_extent_cache(inode, index, &ei)) {
+		dn.data_blkaddr = ei.blk + index - ei.fofs;
+		goto got_it;
+	}
+
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	err = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);
+	if (err)
+		goto put_page;
+	f2fs_put_dnode(&dn);
+
+	if (unlikely(!f2fs_is_valid_blkaddr(sbi, dn.data_blkaddr,
+						DATA_GENERIC))) {
+		err = -EFAULT;
+		goto put_page;
+	}
+got_it:
+	/* read page */
+	fio.page = page;
+	fio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;
+
+	fio.encrypted_page = f2fs_pagecache_get_page(META_MAPPING(sbi),
+					dn.data_blkaddr,
+					FGP_LOCK | FGP_CREAT, GFP_NOFS);
+	if (!fio.encrypted_page) {
+		err = -ENOMEM;
+		goto put_page;
+	}
+
+	err = f2fs_submit_page_bio(&fio);
+	if (err)
+		goto put_encrypted_page;
+	f2fs_put_page(fio.encrypted_page, 0);
+	f2fs_put_page(page, 1);
+	return 0;
+put_encrypted_page:
+	f2fs_put_page(fio.encrypted_page, 1);
+put_page:
+	f2fs_put_page(page, 1);
+	return err;
+}
+
 /*
  * Move data block via META_MAPPING while keeping locked data page.
  * This can be used to move blocks, aka LBAs, directly on disk.
@@ -620,7 +686,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	struct dnode_of_data dn;
 	struct f2fs_summary sum;
 	struct node_info ni;
-	struct page *page;
+	struct page *page, *mpage;
 	block_t newaddr;
 	int err;
 	bool lfs_mode = test_opt(fio.sbi, LFS);
@@ -683,6 +749,23 @@ static void move_data_block(struct inode *inode, block_t bidx,
 		goto recover_block;
 	}
 
+	mpage = f2fs_pagecache_get_page(META_MAPPING(fio.sbi),
+					fio.old_blkaddr, FGP_LOCK, GFP_NOFS);
+	if (mpage) {
+		bool updated = false;
+
+		if (PageUptodate(mpage)) {
+			memcpy(page_address(fio.encrypted_page),
+					page_address(mpage), PAGE_SIZE);
+			updated = true;
+		}
+		f2fs_put_page(mpage, 1);
+		invalidate_mapping_pages(META_MAPPING(fio.sbi),
+					fio.old_blkaddr, fio.old_blkaddr);
+		if (updated)
+			goto write_page;
+	}
+
 	err = f2fs_submit_page_bio(&fio);
 	if (err)
 		goto put_page_out;
@@ -699,6 +782,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 		goto put_page_out;
 	}
 
+write_page:
 	set_page_dirty(fio.encrypted_page);
 	f2fs_wait_on_page_writeback(fio.encrypted_page, DATA, true);
 	if (clear_page_dirty_for_io(fio.encrypted_page))
@@ -873,12 +957,6 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			if (IS_ERR(inode) || is_bad_inode(inode))
 				continue;
 
-			/* if inode uses special I/O path, let's go phase 3 */
-			if (f2fs_post_read_required(inode)) {
-				add_gc_inode(gc_list, inode);
-				continue;
-			}
-
 			if (!down_write_trylock(
 				&F2FS_I(inode)->i_gc_rwsem[WRITE])) {
 				iput(inode);
@@ -886,10 +964,23 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 				continue;
 			}
 
-			start_bidx = f2fs_start_bidx_of_node(nofs, inode);
+			start_bidx = f2fs_start_bidx_of_node(nofs, inode) +
+								ofs_in_node;
+
+			if (f2fs_post_read_required(inode)) {
+				int err = ra_data_block(inode, start_bidx);
+
+				up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+				if (err) {
+					iput(inode);
+					continue;
+				}
+				add_gc_inode(gc_list, inode);
+				continue;
+			}
+
 			data_page = f2fs_get_read_data_page(inode,
-					start_bidx + ofs_in_node, REQ_RAHEAD,
-					true);
+						start_bidx, REQ_RAHEAD, true);
 			up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
 			if (IS_ERR(data_page)) {
 				iput(inode);

commit 6f8d4455060dfb0e32dfb8e685b97caf4ed1be41
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jul 25 12:11:56 2018 +0900

    f2fs: avoid fi->i_gc_rwsem[WRITE] lock in f2fs_gc
    
    The f2fs_gc() called by f2fs_balance_fs() requires to be called outside of
    fi->i_gc_rwsem[WRITE], since f2fs_gc() can try to grab it in a loop.
    
    If it hits the miximum retrials in GC, let's give a chance to release
    gc_mutex for a short time in order not to go into live lock in the worst
    case.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 76a22b3773bc..c598ae5ecbfa 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -882,6 +882,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			if (!down_write_trylock(
 				&F2FS_I(inode)->i_gc_rwsem[WRITE])) {
 				iput(inode);
+				sbi->skipped_gc_rwsem++;
 				continue;
 			}
 
@@ -911,6 +912,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 					continue;
 				if (!down_write_trylock(
 						&fi->i_gc_rwsem[WRITE])) {
+					sbi->skipped_gc_rwsem++;
 					up_write(&fi->i_gc_rwsem[READ]);
 					continue;
 				}
@@ -1048,6 +1050,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 		.iroot = RADIX_TREE_INIT(gc_list.iroot, GFP_NOFS),
 	};
 	unsigned long long last_skipped = sbi->skipped_atomic_files[FG_GC];
+	unsigned long long first_skipped;
 	unsigned int skipped_round = 0, round = 0;
 
 	trace_f2fs_gc_begin(sbi->sb, sync, background,
@@ -1060,6 +1063,8 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 				prefree_segments(sbi));
 
 	cpc.reason = __get_cp_reason(sbi);
+	sbi->skipped_gc_rwsem = 0;
+	first_skipped = last_skipped;
 gc_more:
 	if (unlikely(!(sbi->sb->s_flags & SB_ACTIVE))) {
 		ret = -EINVAL;
@@ -1101,7 +1106,8 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 	total_freed += seg_freed;
 
 	if (gc_type == FG_GC) {
-		if (sbi->skipped_atomic_files[FG_GC] > last_skipped)
+		if (sbi->skipped_atomic_files[FG_GC] > last_skipped ||
+						sbi->skipped_gc_rwsem)
 			skipped_round++;
 		last_skipped = sbi->skipped_atomic_files[FG_GC];
 		round++;
@@ -1110,15 +1116,23 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 	if (gc_type == FG_GC)
 		sbi->cur_victim_sec = NULL_SEGNO;
 
-	if (!sync) {
-		if (has_not_enough_free_secs(sbi, sec_freed, 0)) {
-			if (skipped_round > MAX_SKIP_ATOMIC_COUNT &&
-				skipped_round * 2 >= round)
-				f2fs_drop_inmem_pages_all(sbi, true);
+	if (sync)
+		goto stop;
+
+	if (has_not_enough_free_secs(sbi, sec_freed, 0)) {
+		if (skipped_round <= MAX_SKIP_GC_COUNT ||
+					skipped_round * 2 < round) {
 			segno = NULL_SEGNO;
 			goto gc_more;
 		}
 
+		if (first_skipped < last_skipped &&
+				(last_skipped - first_skipped) >
+						sbi->skipped_gc_rwsem) {
+			f2fs_drop_inmem_pages_all(sbi, true);
+			segno = NULL_SEGNO;
+			goto gc_more;
+		}
 		if (gc_type == FG_GC)
 			ret = f2fs_write_checkpoint(sbi, &cpc);
 	}

commit 7fa750a163089cf96866de402314d853a96cb342
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Aug 13 23:38:06 2018 +0200

    f2fs: rework fault injection handling to avoid a warning
    
    When CONFIG_F2FS_FAULT_INJECTION is disabled, we get a warning about an
    unused label:
    
    fs/f2fs/segment.c: In function '__submit_discard_cmd':
    fs/f2fs/segment.c:1059:1: error: label 'submit' defined but not used [-Werror=unused-label]
    
    This could be fixed by adding another #ifdef around it, but the more
    reliable way of doing this seems to be to remove the other #ifdefs
    where that is easily possible.
    
    By defining time_to_inject() as a trivial stub, most of the checks for
    CONFIG_F2FS_FAULT_INJECTION can go away. This also leads to nicer
    formatting of the code.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e352fbd33848..76a22b3773bc 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -53,12 +53,10 @@ static int gc_thread_func(void *data)
 			continue;
 		}
 
-#ifdef CONFIG_F2FS_FAULT_INJECTION
 		if (time_to_inject(sbi, FAULT_CHECKPOINT)) {
 			f2fs_show_injection_info(FAULT_CHECKPOINT);
 			f2fs_stop_checkpoint(sbi, false);
 		}
-#endif
 
 		if (!sb_start_write_trylock(sbi->sb))
 			continue;

commit 7735730d39d75e70476c1b01435b9b1f41637f0e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Jul 17 00:02:17 2018 +0800

    f2fs: fix to propagate error from __get_meta_page()
    
    If caller of __get_meta_page() can handle error, let's propagate error
    from __get_meta_page().
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 37ab2d10a872..e352fbd33848 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -517,7 +517,11 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 			continue;
 		}
 
-		f2fs_get_node_info(sbi, nid, &ni);
+		if (f2fs_get_node_info(sbi, nid, &ni)) {
+			f2fs_put_page(node_page, 1);
+			continue;
+		}
+
 		if (ni.blk_addr != start_addr + off) {
 			f2fs_put_page(node_page, 1);
 			continue;
@@ -576,7 +580,10 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	if (IS_ERR(node_page))
 		return false;
 
-	f2fs_get_node_info(sbi, nid, dni);
+	if (f2fs_get_node_info(sbi, nid, dni)) {
+		f2fs_put_page(node_page, 1);
+		return false;
+	}
 
 	if (sum->version != dni->version) {
 		f2fs_msg(sbi->sb, KERN_WARNING,
@@ -655,7 +662,10 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	 */
 	f2fs_wait_on_page_writeback(page, DATA, true);
 
-	f2fs_get_node_info(fio.sbi, dn.nid, &ni);
+	err = f2fs_get_node_info(fio.sbi, dn.nid, &ni);
+	if (err)
+		goto put_out;
+
 	set_summary(&sum, dn.nid, dn.ofs_in_node, ni.version);
 
 	/* read page */

commit 10d255c3540239c7920f52d2eb223756e186af56
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jul 4 21:20:05 2018 +0800

    f2fs: fix to skip GC if type in SSA and SIT is inconsistent
    
    If segment type in SSA and SIT is inconsistent, we will encounter below
    BUG_ON during GC, to avoid this panic, let's just skip doing GC on such
    segment.
    
    The bug is triggered with image reported in below link:
    
    https://bugzilla.kernel.org/show_bug.cgi?id=200223
    
    [  388.060262] ------------[ cut here ]------------
    [  388.060268] kernel BUG at /home/y00370721/git/devf2fs/gc.c:989!
    [  388.061172] invalid opcode: 0000 [#1] SMP
    [  388.061773] Modules linked in: f2fs(O) bluetooth ecdh_generic xt_tcpudp iptable_filter ip_tables x_tables lp ttm drm_kms_helper drm intel_rapl sb_edac crct10dif_pclmul crc32_pclmul ghash_clmulni_intel pcbc aesni_intel fb_sys_fops ppdev aes_x86_64 syscopyarea crypto_simd sysfillrect parport_pc joydev sysimgblt glue_helper parport cryptd i2c_piix4 serio_raw mac_hid btrfs hid_generic usbhid hid raid6_pq psmouse pata_acpi floppy
    [  388.064247] CPU: 7 PID: 4151 Comm: f2fs_gc-7:0 Tainted: G           O    4.13.0-rc1+ #26
    [  388.065306] Hardware name: Xen HVM domU, BIOS 4.1.2_115-900.260_ 11/06/2015
    [  388.066058] task: ffff880201583b80 task.stack: ffffc90004d7c000
    [  388.069948] RIP: 0010:do_garbage_collect+0xcc8/0xcd0 [f2fs]
    [  388.070766] RSP: 0018:ffffc90004d7fc68 EFLAGS: 00010202
    [  388.071783] RAX: ffff8801ed227000 RBX: 0000000000000001 RCX: ffffea0007b489c0
    [  388.072700] RDX: ffff880000000000 RSI: 0000000000000001 RDI: ffffea0007b489c0
    [  388.073607] RBP: ffffc90004d7fd58 R08: 0000000000000003 R09: ffffea0007b489dc
    [  388.074619] R10: 0000000000000000 R11: 0052782ab317138d R12: 0000000000000018
    [  388.075625] R13: 0000000000000018 R14: ffff880211ceb000 R15: ffff880211ceb000
    [  388.076687] FS:  0000000000000000(0000) GS:ffff880214fc0000(0000) knlGS:0000000000000000
    [  388.083277] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  388.084536] CR2: 0000000000e18c60 CR3: 00000001ecf2e000 CR4: 00000000001406e0
    [  388.085748] Call Trace:
    [  388.086690]  ? find_next_bit+0xb/0x10
    [  388.088091]  f2fs_gc+0x1a8/0x9d0 [f2fs]
    [  388.088888]  ? lock_timer_base+0x7d/0xa0
    [  388.090213]  ? try_to_del_timer_sync+0x44/0x60
    [  388.091698]  gc_thread_func+0x342/0x4b0 [f2fs]
    [  388.092892]  ? wait_woken+0x80/0x80
    [  388.094098]  kthread+0x109/0x140
    [  388.095010]  ? f2fs_gc+0x9d0/0x9d0 [f2fs]
    [  388.096043]  ? kthread_park+0x60/0x60
    [  388.097281]  ret_from_fork+0x25/0x30
    [  388.098401] Code: ff ff 48 83 e8 01 48 89 44 24 58 e9 27 f8 ff ff 48 83 e8 01 e9 78 fc ff ff 48 8d 78 ff e9 17 fb ff ff 48 83 ef 01 e9 4d f4 ff ff <0f> 0b 66 0f 1f 44 00 00 0f 1f 44 00 00 55 48 89 e5 41 56 41 55
    [  388.100864] RIP: do_garbage_collect+0xcc8/0xcd0 [f2fs] RSP: ffffc90004d7fc68
    [  388.101810] ---[ end trace 81c73d6e6b7da61d ]---
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 9093be6e7a7d..37ab2d10a872 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -986,7 +986,13 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 			goto next;
 
 		sum = page_address(sum_page);
-		f2fs_bug_on(sbi, type != GET_SUM_TYPE((&sum->footer)));
+		if (type != GET_SUM_TYPE((&sum->footer))) {
+			f2fs_msg(sbi->sb, KERN_ERR, "Inconsistent segment (%u) "
+				"type [%d, %d] in SSA and SIT",
+				segno, type, GET_SUM_TYPE((&sum->footer)));
+			set_sbi_flag(sbi, SBI_NEED_FSCK);
+			goto next;
+		}
 
 		/*
 		 * this is to avoid deadlock:

commit c29fd0c0e26dacb7a33ad166587059818a94b4e0
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Jun 4 23:20:36 2018 +0800

    f2fs: let sync node IO interrupt async one
    
    Although mixed sync/async IOs can have continuous LBA, as they have
    different IO priority, block IO scheduler will add them into different
    queues and commit them separately, result in splited IOs which causes
    wrose performance.
    
    This patch gives high priority to synchronous IO of nodes, means that
    once synchronous flow starts, it can interrupt asynchronous writeback
    flow of system flusher, so more big IOs can be expected.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 2a97ce7bc91d..9093be6e7a7d 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -473,12 +473,16 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 	block_t start_addr;
 	int off;
 	int phase = 0;
+	bool fggc = (gc_type == FG_GC);
 
 	start_addr = START_BLOCK(sbi, segno);
 
 next_step:
 	entry = sum;
 
+	if (fggc && phase == 2)
+		atomic_inc(&sbi->wb_sync_req[NODE]);
+
 	for (off = 0; off < sbi->blocks_per_seg; off++, entry++) {
 		nid_t nid = le32_to_cpu(entry->nid);
 		struct page *node_page;
@@ -525,6 +529,9 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 
 	if (++phase < 3)
 		goto next_step;
+
+	if (fggc)
+		atomic_dec(&sbi->wb_sync_req[NODE]);
 }
 
 /*

commit 4d57b86dd86404fd8bb4f87d277d5a86a7fe537e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed May 30 00:20:41 2018 +0800

    f2fs: clean up symbol namespace
    
    As Ted reported:
    
    "Hi, I was looking at f2fs's sources recently, and I noticed that there
    is a very large number of non-static symbols which don't have a f2fs
    prefix.  There's well over a hundred (see attached below).
    
    As one example, in fs/f2fs/dir.c there is:
    
    unsigned char get_de_type(struct f2fs_dir_entry *de)
    
    This function is clearly only useful for f2fs, but it has a generic
    name.  This means that if any other file system tries to have the same
    symbol name, there will be a symbol conflict and the kernel would not
    successfully build.  It also means that when someone is looking f2fs
    sources, it's not at all obvious whether a function such as
    read_data_page(), invalidate_blocks(), is a generic kernel function
    found in the fs, mm, or block layers, or a f2fs specific function.
    
    You might want to fix this at some point.  Hopefully Kent's bcachefs
    isn't similarly using genericly named functions, since that might
    cause conflicts with f2fs's functions --- but just as this would be a
    problem that we would rightly insist that Kent fix, this is something
    that we should have rightly insisted that f2fs should have fixed
    before it was integrated into the mainline kernel.
    
    acquire_orphan_inode
    add_ino_entry
    add_orphan_inode
    allocate_data_block
    allocate_new_segments
    alloc_nid
    alloc_nid_done
    alloc_nid_failed
    available_free_memory
    ...."
    
    This patch adds "f2fs_" prefix for all non-static symbols in order to:
    a) avoid conflict with other kernel generic symbols;
    b) to indicate the function is f2fs specific one instead of generic
    one;
    
    Reported-by: Theodore Ts'o <tytso@mit.edu>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 5e632ed27dcd..2a97ce7bc91d 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -114,7 +114,7 @@ static int gc_thread_func(void *data)
 	return 0;
 }
 
-int start_gc_thread(struct f2fs_sb_info *sbi)
+int f2fs_start_gc_thread(struct f2fs_sb_info *sbi)
 {
 	struct f2fs_gc_kthread *gc_th;
 	dev_t dev = sbi->sb->s_bdev->bd_dev;
@@ -146,7 +146,7 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 	return err;
 }
 
-void stop_gc_thread(struct f2fs_sb_info *sbi)
+void f2fs_stop_gc_thread(struct f2fs_sb_info *sbi)
 {
 	struct f2fs_gc_kthread *gc_th = sbi->gc_thread;
 	if (!gc_th)
@@ -429,7 +429,7 @@ static void add_gc_inode(struct gc_inode_list *gc_list, struct inode *inode)
 		iput(inode);
 		return;
 	}
-	new_ie = f2fs_kmem_cache_alloc(inode_entry_slab, GFP_NOFS);
+	new_ie = f2fs_kmem_cache_alloc(f2fs_inode_entry_slab, GFP_NOFS);
 	new_ie->inode = inode;
 
 	f2fs_radix_tree_insert(&gc_list->iroot, inode->i_ino, new_ie);
@@ -443,7 +443,7 @@ static void put_gc_inode(struct gc_inode_list *gc_list)
 		radix_tree_delete(&gc_list->iroot, ie->inode->i_ino);
 		iput(ie->inode);
 		list_del(&ie->list);
-		kmem_cache_free(inode_entry_slab, ie);
+		kmem_cache_free(f2fs_inode_entry_slab, ie);
 	}
 }
 
@@ -492,34 +492,34 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 			continue;
 
 		if (phase == 0) {
-			ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,
+			f2fs_ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,
 							META_NAT, true);
 			continue;
 		}
 
 		if (phase == 1) {
-			ra_node_page(sbi, nid);
+			f2fs_ra_node_page(sbi, nid);
 			continue;
 		}
 
 		/* phase == 2 */
-		node_page = get_node_page(sbi, nid);
+		node_page = f2fs_get_node_page(sbi, nid);
 		if (IS_ERR(node_page))
 			continue;
 
-		/* block may become invalid during get_node_page */
+		/* block may become invalid during f2fs_get_node_page */
 		if (check_valid_map(sbi, segno, off) == 0) {
 			f2fs_put_page(node_page, 1);
 			continue;
 		}
 
-		get_node_info(sbi, nid, &ni);
+		f2fs_get_node_info(sbi, nid, &ni);
 		if (ni.blk_addr != start_addr + off) {
 			f2fs_put_page(node_page, 1);
 			continue;
 		}
 
-		move_node_page(node_page, gc_type);
+		f2fs_move_node_page(node_page, gc_type);
 		stat_inc_node_blk_count(sbi, 1, gc_type);
 	}
 
@@ -534,7 +534,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
  * as indirect or double indirect node blocks, are given, it must be a caller's
  * bug.
  */
-block_t start_bidx_of_node(unsigned int node_ofs, struct inode *inode)
+block_t f2fs_start_bidx_of_node(unsigned int node_ofs, struct inode *inode)
 {
 	unsigned int indirect_blks = 2 * NIDS_PER_BLOCK + 4;
 	unsigned int bidx;
@@ -565,11 +565,11 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	nid = le32_to_cpu(sum->nid);
 	ofs_in_node = le16_to_cpu(sum->ofs_in_node);
 
-	node_page = get_node_page(sbi, nid);
+	node_page = f2fs_get_node_page(sbi, nid);
 	if (IS_ERR(node_page))
 		return false;
 
-	get_node_info(sbi, nid, dni);
+	f2fs_get_node_info(sbi, nid, dni);
 
 	if (sum->version != dni->version) {
 		f2fs_msg(sbi->sb, KERN_WARNING,
@@ -633,7 +633,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	}
 
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	err = get_dnode_of_data(&dn, bidx, LOOKUP_NODE);
+	err = f2fs_get_dnode_of_data(&dn, bidx, LOOKUP_NODE);
 	if (err)
 		goto out;
 
@@ -648,7 +648,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	 */
 	f2fs_wait_on_page_writeback(page, DATA, true);
 
-	get_node_info(fio.sbi, dn.nid, &ni);
+	f2fs_get_node_info(fio.sbi, dn.nid, &ni);
 	set_summary(&sum, dn.nid, dn.ofs_in_node, ni.version);
 
 	/* read page */
@@ -658,7 +658,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	if (lfs_mode)
 		down_write(&fio.sbi->io_order_lock);
 
-	allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &newaddr,
+	f2fs_allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &newaddr,
 					&sum, CURSEG_COLD_DATA, NULL, false);
 
 	fio.encrypted_page = f2fs_pagecache_get_page(META_MAPPING(fio.sbi),
@@ -717,7 +717,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	if (lfs_mode)
 		up_write(&fio.sbi->io_order_lock);
 	if (err)
-		__f2fs_replace_block(fio.sbi, &sum, newaddr, fio.old_blkaddr,
+		f2fs_do_replace_block(fio.sbi, &sum, newaddr, fio.old_blkaddr,
 								true, true);
 put_out:
 	f2fs_put_dnode(&dn);
@@ -730,7 +730,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 {
 	struct page *page;
 
-	page = get_lock_data_page(inode, bidx, true);
+	page = f2fs_get_lock_data_page(inode, bidx, true);
 	if (IS_ERR(page))
 		return;
 
@@ -775,12 +775,12 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 		f2fs_wait_on_page_writeback(page, DATA, true);
 		if (clear_page_dirty_for_io(page)) {
 			inode_dec_dirty_pages(inode);
-			remove_dirty_inode(inode);
+			f2fs_remove_dirty_inode(inode);
 		}
 
 		set_cold_data(page);
 
-		err = do_write_data_page(&fio);
+		err = f2fs_do_write_data_page(&fio);
 		if (err) {
 			clear_cold_data(page);
 			if (err == -ENOMEM) {
@@ -832,13 +832,13 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			continue;
 
 		if (phase == 0) {
-			ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,
+			f2fs_ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,
 							META_NAT, true);
 			continue;
 		}
 
 		if (phase == 1) {
-			ra_node_page(sbi, nid);
+			f2fs_ra_node_page(sbi, nid);
 			continue;
 		}
 
@@ -847,7 +847,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			continue;
 
 		if (phase == 2) {
-			ra_node_page(sbi, dni.ino);
+			f2fs_ra_node_page(sbi, dni.ino);
 			continue;
 		}
 
@@ -870,8 +870,8 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 				continue;
 			}
 
-			start_bidx = start_bidx_of_node(nofs, inode);
-			data_page = get_read_data_page(inode,
+			start_bidx = f2fs_start_bidx_of_node(nofs, inode);
+			data_page = f2fs_get_read_data_page(inode,
 					start_bidx + ofs_in_node, REQ_RAHEAD,
 					true);
 			up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
@@ -905,7 +905,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 				inode_dio_wait(inode);
 			}
 
-			start_bidx = start_bidx_of_node(nofs, inode)
+			start_bidx = f2fs_start_bidx_of_node(nofs, inode)
 								+ ofs_in_node;
 			if (f2fs_post_read_required(inode))
 				move_data_block(inode, start_bidx, gc_type,
@@ -955,12 +955,12 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 
 	/* readahead multi ssa blocks those have contiguous address */
 	if (sbi->segs_per_sec > 1)
-		ra_meta_pages(sbi, GET_SUM_BLOCK(sbi, segno),
+		f2fs_ra_meta_pages(sbi, GET_SUM_BLOCK(sbi, segno),
 					sbi->segs_per_sec, META_SSA, true);
 
 	/* reference all summary page */
 	while (segno < end_segno) {
-		sum_page = get_sum_page(sbi, segno++);
+		sum_page = f2fs_get_sum_page(sbi, segno++);
 		unlock_page(sum_page);
 	}
 
@@ -1056,7 +1056,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 		 * secure free segments which doesn't need fggc any more.
 		 */
 		if (prefree_segments(sbi)) {
-			ret = write_checkpoint(sbi, &cpc);
+			ret = f2fs_write_checkpoint(sbi, &cpc);
 			if (ret)
 				goto stop;
 		}
@@ -1093,13 +1093,13 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 		if (has_not_enough_free_secs(sbi, sec_freed, 0)) {
 			if (skipped_round > MAX_SKIP_ATOMIC_COUNT &&
 				skipped_round * 2 >= round)
-				drop_inmem_pages_all(sbi, true);
+				f2fs_drop_inmem_pages_all(sbi, true);
 			segno = NULL_SEGNO;
 			goto gc_more;
 		}
 
 		if (gc_type == FG_GC)
-			ret = write_checkpoint(sbi, &cpc);
+			ret = f2fs_write_checkpoint(sbi, &cpc);
 	}
 stop:
 	SIT_I(sbi)->last_victim[ALLOC_NEXT] = 0;
@@ -1123,7 +1123,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 	return ret;
 }
 
-void build_gc_manager(struct f2fs_sb_info *sbi)
+void f2fs_build_gc_manager(struct f2fs_sb_info *sbi)
 {
 	DIRTY_I(sbi)->v_ops = &default_v_ops;
 

commit fe16efe6a7952f59d596a02e4fc57f966fafdafe
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon May 28 23:47:18 2018 +0800

    f2fs: fix to let caller retry allocating block address
    
    Configure io_bits with 2 and enable LFS mode, generic/013 reports below dmesg:
    
    BUG: unable to handle kernel NULL pointer dereference at 00000104
    *pdpt = 0000000029b7b001 *pde = 0000000000000000
    Oops: 0002 [#1] PREEMPT SMP
    Modules linked in: crc32_generic zram f2fs(O) rfcomm bnep bluetooth ecdh_generic snd_intel8x0 snd_ac97_codec ac97_bus snd_pcm snd_seq_midi snd_seq_midi_event snd_rawmidi snd_seq pcbc joydev snd_seq_device aesni_intel snd_timer aes_i586 snd crypto_simd cryptd soundcore i2c_piix4 serio_raw mac_hid video parport_pc ppdev lp parport hid_generic psmouse usbhid hid e1000
    CPU: 0 PID: 11161 Comm: fsstress Tainted: G           O      4.17.0-rc2 #38
    Hardware name: innotek GmbH VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006
    EIP: f2fs_submit_page_write+0x28d/0x550 [f2fs]
    EFLAGS: 00010206 CPU: 0
    EAX: e863dcd8 EBX: 00000000 ECX: 00000100 EDX: 00000200
    ESI: e863dcf4 EDI: f6f82768 EBP: e863dbb0 ESP: e863db74
     DS: 007b ES: 007b FS: 00d8 GS: 0033 SS: 0068
    CR0: 80050033 CR2: 00000104 CR3: 29a62020 CR4: 000406f0
    Call Trace:
     do_write_page+0x6f/0xc0 [f2fs]
     write_data_page+0x4a/0xd0 [f2fs]
     do_write_data_page+0x327/0x630 [f2fs]
     __write_data_page+0x34b/0x820 [f2fs]
     __f2fs_write_data_pages+0x42d/0x8c0 [f2fs]
     f2fs_write_data_pages+0x27/0x30 [f2fs]
     do_writepages+0x1a/0x70
     __filemap_fdatawrite_range+0x94/0xd0
     filemap_write_and_wait_range+0x3d/0xa0
     __generic_file_write_iter+0x11a/0x1f0
     f2fs_file_write_iter+0xdd/0x3b0 [f2fs]
     __vfs_write+0xd2/0x150
     vfs_write+0x9b/0x190
     ksys_write+0x45/0x90
     sys_write+0x16/0x20
     do_fast_syscall_32+0xaa/0x22c
     entry_SYSENTER_32+0x4c/0x7b
    EIP: 0xb7fc8c51
    EFLAGS: 00000246 CPU: 0
    EAX: ffffffda EBX: 00000003 ECX: 09cde000 EDX: 00001000
    ESI: 00000003 EDI: 00001000 EBP: 00000000 ESP: bfbded38
     DS: 007b ES: 007b FS: 0000 GS: 0033 SS: 007b
    Code: e8 f9 77 34 c9 8b 45 e0 8b 80 b8 00 00 00 39 45 d8 0f 84 bb 02 00 00 8b 45 e0 8b 80 b8 00 00 00 8d 50 d8 8b 08 89 55 f0 8b 50 04 <89> 51 04 89 0a c7 00 00 01 00 00 c7 40 04 00 02 00 00 8b 45 dc
    EIP: f2fs_submit_page_write+0x28d/0x550 [f2fs] SS:ESP: 0068:e863db74
    CR2: 0000000000000104
    ---[ end trace 4cac79c0d1305ee6 ]---
    
    allocate_data_block will submit all sequential pending IOs sorted by a
    FIFO list, If we failed to submit other user's IO due to unaligned write,
    we will retry to allocate new block address for current IO, then it will
    initialize fio.list again, if fio was in the list before, it can break
    FIFO list, result in above panic.
    
    Thread A                        Thread B
    - do_write_page
     - allocate_data_block
      - list_add_tail
      : fioA cached in FIFO list.
                                    - do_write_page
                                     - allocate_data_block
                                      - list_add_tail
                                      : fioB cached in FIFO list.
                                     - f2fs_submit_page_write
                                     : fail to submit IO
                                     - allocate_data_block
                                      - INIT_LIST_HEAD
     - f2fs_submit_page_write
      - list_del  <-- NULL pointer dereference
    
    This patch adds fio.retry parameter to indicate failure status for each
    IO, and avoid bailing out if there is still pending IO in FIFO list for
    fixing.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 885032fc3a61..5e632ed27dcd 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -603,6 +603,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 		.op_flags = 0,
 		.encrypted_page = NULL,
 		.in_list = false,
+		.retry = false,
 	};
 	struct dnode_of_data dn;
 	struct f2fs_summary sum;
@@ -697,8 +698,8 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	fio.op = REQ_OP_WRITE;
 	fio.op_flags = REQ_SYNC;
 	fio.new_blkaddr = newaddr;
-	err = f2fs_submit_page_write(&fio);
-	if (err) {
+	f2fs_submit_page_write(&fio);
+	if (fio.retry) {
 		if (PageWriteback(fio.encrypted_page))
 			end_page_writeback(fio.encrypted_page);
 		goto put_page_out;

commit 14a28559f43ac7c0b98dd1b0e73ec9ec8ab4fc45
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon May 28 16:59:27 2018 +0800

    f2fs: fix error path of move_data_page
    
    This patch fixes error path of move_data_page:
    - clear cold data flag if it fails to write page.
    - redirty page for non-ENOMEM case.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 50bb8fc25275..885032fc3a61 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -780,9 +780,14 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 		set_cold_data(page);
 
 		err = do_write_data_page(&fio);
-		if (err == -ENOMEM && is_dirty) {
-			congestion_wait(BLK_RW_ASYNC, HZ/50);
-			goto retry;
+		if (err) {
+			clear_cold_data(page);
+			if (err == -ENOMEM) {
+				congestion_wait(BLK_RW_ASYNC, HZ/50);
+				goto retry;
+			}
+			if (is_dirty)
+				set_page_dirty(page);
 		}
 	}
 out:

commit 2ef79ecb5e906d87475d3e0c49b22425499a89f3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon May 7 20:28:54 2018 +0800

    f2fs: avoid stucking GC due to atomic write
    
    f2fs doesn't allow abuse on atomic write class interface, so except
    limiting in-mem pages' total memory usage capacity, we need to limit
    atomic-write usage as well when filesystem is seriously fragmented,
    otherwise we may run into infinite loop during foreground GC because
    target blocks in victim segment are belong to atomic opened file for
    long time.
    
    Now, we will detect failure due to atomic write in foreground GC, if
    the count exceeds threshold, we will drop all atomic written data in
    cache, by this, I expect it can keep our system running safely to
    prevent Dos attack.
    
    In addition, his patch adds to show GC skip information in debugfs,
    now it just shows count of skipped caused by atomic write.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index f02a9fee4b4b..50bb8fc25275 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -592,7 +592,7 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
  * This can be used to move blocks, aka LBAs, directly on disk.
  */
 static void move_data_block(struct inode *inode, block_t bidx,
-					unsigned int segno, int off)
+				int gc_type, unsigned int segno, int off)
 {
 	struct f2fs_io_info fio = {
 		.sbi = F2FS_I_SB(inode),
@@ -620,8 +620,11 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	if (!check_valid_map(F2FS_I_SB(inode), segno, off))
 		goto out;
 
-	if (f2fs_is_atomic_file(inode))
+	if (f2fs_is_atomic_file(inode)) {
+		F2FS_I(inode)->i_gc_failures[GC_FAILURE_ATOMIC]++;
+		F2FS_I_SB(inode)->skipped_atomic_files[gc_type]++;
 		goto out;
+	}
 
 	if (f2fs_is_pinned_file(inode)) {
 		f2fs_pin_file_control(inode, true);
@@ -733,8 +736,11 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 	if (!check_valid_map(F2FS_I_SB(inode), segno, off))
 		goto out;
 
-	if (f2fs_is_atomic_file(inode))
+	if (f2fs_is_atomic_file(inode)) {
+		F2FS_I(inode)->i_gc_failures[GC_FAILURE_ATOMIC]++;
+		F2FS_I_SB(inode)->skipped_atomic_files[gc_type]++;
 		goto out;
+	}
 	if (f2fs_is_pinned_file(inode)) {
 		if (gc_type == FG_GC)
 			f2fs_pin_file_control(inode, true);
@@ -896,7 +902,8 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			start_bidx = start_bidx_of_node(nofs, inode)
 								+ ofs_in_node;
 			if (f2fs_post_read_required(inode))
-				move_data_block(inode, start_bidx, segno, off);
+				move_data_block(inode, start_bidx, gc_type,
+								segno, off);
 			else
 				move_data_page(inode, start_bidx, gc_type,
 								segno, off);
@@ -1013,6 +1020,8 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 		.ilist = LIST_HEAD_INIT(gc_list.ilist),
 		.iroot = RADIX_TREE_INIT(gc_list.iroot, GFP_NOFS),
 	};
+	unsigned long long last_skipped = sbi->skipped_atomic_files[FG_GC];
+	unsigned int skipped_round = 0, round = 0;
 
 	trace_f2fs_gc_begin(sbi->sb, sync, background,
 				get_pages(sbi, F2FS_DIRTY_NODES),
@@ -1064,11 +1073,21 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 		sec_freed++;
 	total_freed += seg_freed;
 
+	if (gc_type == FG_GC) {
+		if (sbi->skipped_atomic_files[FG_GC] > last_skipped)
+			skipped_round++;
+		last_skipped = sbi->skipped_atomic_files[FG_GC];
+		round++;
+	}
+
 	if (gc_type == FG_GC)
 		sbi->cur_victim_sec = NULL_SEGNO;
 
 	if (!sync) {
 		if (has_not_enough_free_secs(sbi, sec_freed, 0)) {
+			if (skipped_round > MAX_SKIP_ATOMIC_COUNT &&
+				skipped_round * 2 >= round)
+				drop_inmem_pages_all(sbi, true);
 			segno = NULL_SEGNO;
 			goto gc_more;
 		}

commit 5b0e95398e2bcc18e871758221cc712be4a0a39a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon May 7 14:22:40 2018 -0700

    f2fs: introduce sbi->gc_mode to determine the policy
    
    This is to avoid sbi->gc_thread pointer access.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index ec9b1579c510..f02a9fee4b4b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -76,7 +76,7 @@ static int gc_thread_func(void *data)
 		 * invalidated soon after by user update or deletion.
 		 * So, I'd like to wait some time to collect dirty segments.
 		 */
-		if (gc_th->gc_urgent) {
+		if (sbi->gc_mode == GC_URGENT) {
 			wait_ms = gc_th->urgent_sleep_time;
 			mutex_lock(&sbi->gc_mutex);
 			goto do_gc;
@@ -131,8 +131,6 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 	gc_th->max_sleep_time = DEF_GC_THREAD_MAX_SLEEP_TIME;
 	gc_th->no_gc_sleep_time = DEF_GC_THREAD_NOGC_SLEEP_TIME;
 
-	gc_th->gc_idle = 0;
-	gc_th->gc_urgent = 0;
 	gc_th->gc_wake= 0;
 
 	sbi->gc_thread = gc_th;
@@ -158,21 +156,19 @@ void stop_gc_thread(struct f2fs_sb_info *sbi)
 	sbi->gc_thread = NULL;
 }
 
-static int select_gc_type(struct f2fs_gc_kthread *gc_th, int gc_type)
+static int select_gc_type(struct f2fs_sb_info *sbi, int gc_type)
 {
 	int gc_mode = (gc_type == BG_GC) ? GC_CB : GC_GREEDY;
 
-	if (!gc_th)
-		return gc_mode;
-
-	if (gc_th->gc_idle) {
-		if (gc_th->gc_idle == 1)
-			gc_mode = GC_CB;
-		else if (gc_th->gc_idle == 2)
-			gc_mode = GC_GREEDY;
-	}
-	if (gc_th->gc_urgent)
+	switch (sbi->gc_mode) {
+	case GC_IDLE_CB:
+		gc_mode = GC_CB;
+		break;
+	case GC_IDLE_GREEDY:
+	case GC_URGENT:
 		gc_mode = GC_GREEDY;
+		break;
+	}
 	return gc_mode;
 }
 
@@ -187,7 +183,7 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 		p->max_search = dirty_i->nr_dirty[type];
 		p->ofs_unit = 1;
 	} else {
-		p->gc_mode = select_gc_type(sbi->gc_thread, gc_type);
+		p->gc_mode = select_gc_type(sbi, gc_type);
 		p->dirty_segmap = dirty_i->dirty_segmap[DIRTY];
 		p->max_search = dirty_i->nr_dirty[DIRTY];
 		p->ofs_unit = sbi->segs_per_sec;
@@ -195,7 +191,7 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 
 	/* we need to check every dirty segments in the FG_GC case */
 	if (gc_type != FG_GC &&
-			(sbi->gc_thread && !sbi->gc_thread->gc_urgent) &&
+			(sbi->gc_mode != GC_URGENT) &&
 			p->max_search > sbi->max_victim_search)
 		p->max_search = sbi->max_victim_search;
 

commit 107a805de87ec071dab602071c14e67b98b0c519
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat May 26 09:00:13 2018 +0800

    f2fs: keep migration IO order in LFS mode
    
    For non-migration IO, we will keep order of data/node blocks' submitting
    as allocation sequence by sorting IOs in per log io_list list, but for
    migration IO, it could be out-of-order.
    
    In LFS mode, we should keep all IOs including migration IO be ordered,
    so that this patch fixes to add an additional lock to keep submitting
    order.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 9bb2ddbbed1e..ec9b1579c510 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -614,6 +614,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	struct page *page;
 	block_t newaddr;
 	int err;
+	bool lfs_mode = test_opt(fio.sbi, LFS);
 
 	/* do not read out */
 	page = f2fs_grab_cache_page(inode->i_mapping, bidx, false);
@@ -654,6 +655,9 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	fio.page = page;
 	fio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;
 
+	if (lfs_mode)
+		down_write(&fio.sbi->io_order_lock);
+
 	allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &newaddr,
 					&sum, CURSEG_COLD_DATA, NULL, false);
 
@@ -710,6 +714,8 @@ static void move_data_block(struct inode *inode, block_t bidx,
 put_page_out:
 	f2fs_put_page(fio.encrypted_page, 1);
 recover_block:
+	if (lfs_mode)
+		up_write(&fio.sbi->io_order_lock);
 	if (err)
 		__f2fs_replace_block(fio.sbi, &sum, newaddr, fio.old_blkaddr,
 								true, true);

commit 299254d85dec4ac0515a14e6d96ff70841302719
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Apr 26 17:05:51 2018 +0800

    Revert "f2fs: add ovp valid_blocks check for bg gc victim to fg_gc"
    
    For extreme case:
    10 section, op = 10%, no_fggc_threshold = 90%
    All section usage: 85% 85% 85% 85% 90% 90% 95% 95% 95% 95%
    
    During foreground GC, if we skip select dirty section whose usage
    is larger than no_fggc_threshold, we can only recycle 80% invalid
    space from four 85% usage sections and two 90% usage sections,
    result in encountering out-of-space issue.
    
    This reverts commit e93b9865251a0503d83fd570e7d5a7c8bc351715 to
    fix this issue, besides, we keep the logic that we scan all dirty
    section when searching a victim, so that GC can select victim with
    least valid blocks.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 94746d5ac141..9bb2ddbbed1e 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -234,10 +234,6 @@ static unsigned int check_bg_victims(struct f2fs_sb_info *sbi)
 	for_each_set_bit(secno, dirty_i->victim_secmap, MAIN_SECS(sbi)) {
 		if (sec_usage_check(sbi, secno))
 			continue;
-
-		if (no_fggc_candidate(sbi, secno))
-			continue;
-
 		clear_bit(secno, dirty_i->victim_secmap);
 		return GET_SEG_FROM_SEC(sbi, secno);
 	}
@@ -377,9 +373,6 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 			goto next;
 		if (gc_type == BG_GC && test_bit(secno, dirty_i->victim_secmap))
 			goto next;
-		if (gc_type == FG_GC && p.alloc_mode == LFS &&
-					no_fggc_candidate(sbi, secno))
-			goto next;
 
 		cost = get_gc_cost(sbi, segno, &p);
 
@@ -1105,17 +1098,8 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 
 void build_gc_manager(struct f2fs_sb_info *sbi)
 {
-	u64 main_count, resv_count, ovp_count;
-
 	DIRTY_I(sbi)->v_ops = &default_v_ops;
 
-	/* threshold of # of valid blocks in a section for victims of FG_GC */
-	main_count = SM_I(sbi)->main_segments << sbi->log_blocks_per_seg;
-	resv_count = SM_I(sbi)->reserved_segments << sbi->log_blocks_per_seg;
-	ovp_count = SM_I(sbi)->ovp_segments << sbi->log_blocks_per_seg;
-
-	sbi->fggc_threshold = div64_u64((main_count - ovp_count) *
-				BLKS_PER_SEC(sbi), (main_count - resv_count));
 	sbi->gc_pin_file_threshold = DEF_GC_FAILED_PINNED_FILES;
 
 	/* give warm/cold data area from slower device */

commit b2532c694033fb6762478846c457382061f9f630
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Apr 24 10:55:28 2018 +0800

    f2fs: rename dio_rwsem to i_gc_rwsem
    
    RW semphore dio_rwsem in struct f2fs_inode_info is introduced to avoid
    race between dio and data gc, but now, it is more wildly used to avoid
    foreground operation vs data gc. So rename it to i_gc_rwsem to improve
    its readability.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index a7de8b3431a9..94746d5ac141 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -858,7 +858,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			}
 
 			if (!down_write_trylock(
-				&F2FS_I(inode)->dio_rwsem[WRITE])) {
+				&F2FS_I(inode)->i_gc_rwsem[WRITE])) {
 				iput(inode);
 				continue;
 			}
@@ -867,7 +867,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			data_page = get_read_data_page(inode,
 					start_bidx + ofs_in_node, REQ_RAHEAD,
 					true);
-			up_write(&F2FS_I(inode)->dio_rwsem[WRITE]);
+			up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
 			if (IS_ERR(data_page)) {
 				iput(inode);
 				continue;
@@ -885,11 +885,11 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			bool locked = false;
 
 			if (S_ISREG(inode->i_mode)) {
-				if (!down_write_trylock(&fi->dio_rwsem[READ]))
+				if (!down_write_trylock(&fi->i_gc_rwsem[READ]))
 					continue;
 				if (!down_write_trylock(
-						&fi->dio_rwsem[WRITE])) {
-					up_write(&fi->dio_rwsem[READ]);
+						&fi->i_gc_rwsem[WRITE])) {
+					up_write(&fi->i_gc_rwsem[READ]);
 					continue;
 				}
 				locked = true;
@@ -907,8 +907,8 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 								segno, off);
 
 			if (locked) {
-				up_write(&fi->dio_rwsem[WRITE]);
-				up_write(&fi->dio_rwsem[READ]);
+				up_write(&fi->i_gc_rwsem[WRITE]);
+				up_write(&fi->i_gc_rwsem[READ]);
 			}
 
 			stat_inc_data_blk_count(sbi, 1, gc_type);

commit 17c500350b3e1a1430cbcc7efb54eb859446fc8a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Apr 11 23:09:04 2018 -0700

    f2fs: clear PageError on writepage
    
    This patch clears PageError in some pages tagged by read path, but when we
    write the pages with valid contents, writepage should clear the bit likewise
    ext4.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 70418b34c5f6..a7de8b3431a9 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -693,6 +693,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 		dec_page_count(fio.sbi, F2FS_DIRTY_META);
 
 	set_page_writeback(fio.encrypted_page);
+	ClearPageError(page);
 
 	/* allocate block address */
 	f2fs_wait_on_page_writeback(dn.node_page, NODE, true);

commit 6dbb17961f46b2eafcea2f2627aabb309553e068
Author: Eric Biggers <ebiggers@google.com>
Date:   Wed Apr 18 11:09:48 2018 -0700

    f2fs: refactor read path to allow multiple postprocessing steps
    
    Currently f2fs's ->readpage() and ->readpages() assume that either the
    data undergoes no postprocessing, or decryption only.  But with
    fs-verity, there will be an additional authenticity verification step,
    and it may be needed either by itself, or combined with decryption.
    
    To support this, store a 'struct bio_post_read_ctx' in ->bi_private
    which contains a work struct, a bitmask of postprocessing steps that are
    enabled, and an indicator of the current step.  The bio completion
    routine, if there was no I/O error, enqueues the first postprocessing
    step.  When that completes, it continues to the next step.  Pages that
    fail any postprocessing step have PageError set.  Once all steps have
    completed, pages without PageError set are set Uptodate, and all pages
    are unlocked.
    
    Also replace f2fs_encrypted_file() with a new function
    f2fs_post_read_required() in places like direct I/O and garbage
    collection that really should be testing whether the file needs special
    I/O processing, not whether it is encrypted specifically.
    
    This may also be useful for other future f2fs features such as
    compression.
    
    Signed-off-by: Eric Biggers <ebiggers@google.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 9327411fd93b..70418b34c5f6 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -850,8 +850,8 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			if (IS_ERR(inode) || is_bad_inode(inode))
 				continue;
 
-			/* if encrypted inode, let's go phase 3 */
-			if (f2fs_encrypted_file(inode)) {
+			/* if inode uses special I/O path, let's go phase 3 */
+			if (f2fs_post_read_required(inode)) {
 				add_gc_inode(gc_list, inode);
 				continue;
 			}
@@ -899,7 +899,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 			start_bidx = start_bidx_of_node(nofs, inode)
 								+ ofs_in_node;
-			if (f2fs_encrypted_file(inode))
+			if (f2fs_post_read_required(inode))
 				move_data_block(inode, start_bidx, segno, off);
 			else
 				move_data_page(inode, start_bidx, gc_type,

commit f6bb2a2c0b81c47282ddb7883f92e65a063c27dd
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Tue Apr 10 16:36:52 2018 -0700

    xarray: add the xa_lock to the radix_tree_root
    
    This results in no change in structure size on 64-bit machines as it
    fits in the padding between the gfp_t and the void *.  32-bit machines
    will grow the structure from 8 to 12 bytes.  Almost all radix trees are
    protected with (at least) a spinlock, so as they are converted from
    radix trees to xarrays, the data structures will shrink again.
    
    Initialising the spinlock requires a name for the benefit of lockdep, so
    RADIX_TREE_INIT() now needs to know the name of the radix tree it's
    initialising, and so do IDR_INIT() and IDA_INIT().
    
    Also add the xa_lock() and xa_unlock() family of wrappers to make it
    easier to use the lock.  If we could rely on -fplan9-extensions in the
    compiler, we could avoid all of this syntactic sugar, but that wasn't
    added until gcc 4.6.
    
    Link: http://lkml.kernel.org/r/20180313132639.17387-8-willy@infradead.org
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Reviewed-by: Jeff Layton <jlayton@kernel.org>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Ryusuke Konishi <konishi.ryusuke@lab.ntt.co.jp>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index bfb7a4a3a929..9327411fd93b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1015,7 +1015,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 	unsigned int init_segno = segno;
 	struct gc_inode_list gc_list = {
 		.ilist = LIST_HEAD_INIT(gc_list.ilist),
-		.iroot = RADIX_TREE_INIT(GFP_NOFS),
+		.iroot = RADIX_TREE_INIT(gc_list.iroot, GFP_NOFS),
 	};
 
 	trace_f2fs_gc_begin(sbi->sb, sync, background,

commit b27bc8091ccf22f66ab105d10fa4f5cdeaef05a2
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Feb 26 15:40:30 2018 -0800

    f2fs: do gc in greedy mode for whole range if gc_urgent mode is set
    
    Otherwise, f2fs conducts GC on 8GB range only based on slow cost-benefit.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index bc9420ce2275..bfb7a4a3a929 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -162,12 +162,17 @@ static int select_gc_type(struct f2fs_gc_kthread *gc_th, int gc_type)
 {
 	int gc_mode = (gc_type == BG_GC) ? GC_CB : GC_GREEDY;
 
-	if (gc_th && gc_th->gc_idle) {
+	if (!gc_th)
+		return gc_mode;
+
+	if (gc_th->gc_idle) {
 		if (gc_th->gc_idle == 1)
 			gc_mode = GC_CB;
 		else if (gc_th->gc_idle == 2)
 			gc_mode = GC_GREEDY;
 	}
+	if (gc_th->gc_urgent)
+		gc_mode = GC_GREEDY;
 	return gc_mode;
 }
 
@@ -189,7 +194,9 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 	}
 
 	/* we need to check every dirty segments in the FG_GC case */
-	if (gc_type != FG_GC && p->max_search > sbi->max_victim_search)
+	if (gc_type != FG_GC &&
+			(sbi->gc_thread && !sbi->gc_thread->gc_urgent) &&
+			p->max_search > sbi->max_victim_search)
 		p->max_search = sbi->max_victim_search;
 
 	/* let's select beginning hot/small space first in no_heap mode*/

commit 69babac019337b35aae5644ccf8089243c749d4c
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Feb 26 09:19:47 2018 -0800

    f2fs: don't stop GC if GC is contended
    
    Let's do GC as much as possible, while gc_urgent is set.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b9d93fd532a9..bc9420ce2275 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -76,14 +76,15 @@ static int gc_thread_func(void *data)
 		 * invalidated soon after by user update or deletion.
 		 * So, I'd like to wait some time to collect dirty segments.
 		 */
-		if (!mutex_trylock(&sbi->gc_mutex))
-			goto next;
-
 		if (gc_th->gc_urgent) {
 			wait_ms = gc_th->urgent_sleep_time;
+			mutex_lock(&sbi->gc_mutex);
 			goto do_gc;
 		}
 
+		if (!mutex_trylock(&sbi->gc_mutex))
+			goto next;
+
 		if (!is_idle(sbi)) {
 			increase_sleep_time(gc_th, &wait_ms);
 			mutex_unlock(&sbi->gc_mutex);

commit b94929d975c8423defc9aededb0f499ff936b509
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Mon Jan 29 11:37:45 2018 +0800

    f2fs: fix heap mode to reset it back
    
    Commit 7a20b8a61eff81bdb7097a578752a74860e9d142 ("f2fs: allocate node
    and hot data in the beginning of partition") introduces another mount
    option, heap, to reset it back. But it does not do anything for heap
    mode, so fix it.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index aa720cc44509..b9d93fd532a9 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -191,8 +191,9 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 	if (gc_type != FG_GC && p->max_search > sbi->max_victim_search)
 		p->max_search = sbi->max_victim_search;
 
-	/* let's select beginning hot/small space first */
-	if (type == CURSEG_HOT_DATA || IS_NODESEG(type))
+	/* let's select beginning hot/small space first in no_heap mode*/
+	if (test_opt(sbi, NOHEAP) &&
+		(type == CURSEG_HOT_DATA || IS_NODESEG(type)))
 		p->offset = 0;
 	else
 		p->offset = SIT_I(sbi)->last_victim[p->gc_mode];

commit a9d572c7550044d5b217b5287d99a2e6d34b97b0
Author: Sheng Yong <shengyong1@huawei.com>
Date:   Wed Jan 17 12:11:31 2018 +0800

    f2fs: avoid hungtask when GC encrypted block if io_bits is set
    
    When io_bits is set, GCing encrypted block may hit the following hungtask.
    Since io_bits requires aligned block address, f2fs_submit_page_write may
    return -EAGAIN if new_blkaddr does not satisify io_bits alignment. As a
    result, the encrypted page will never be writtenback.
    
    This patch makes move_data_block aware the EAGAIN error and cancel the
    writeback.
    
    [  246.751371] INFO: task kworker/u4:4:797 blocked for more than 90 seconds.
    [  246.752423]       Not tainted 4.15.0-rc4+ #11
    [  246.754176] "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    [  246.755336] kworker/u4:4    D25448   797      2 0x80000000
    [  246.755597] Workqueue: writeback wb_workfn (flush-7:0)
    [  246.755616] Call Trace:
    [  246.755695]  ? __schedule+0x322/0xa90
    [  246.755761]  ? blk_init_request_from_bio+0x120/0x120
    [  246.755773]  ? pci_mmcfg_check_reserved+0xb0/0xb0
    [  246.755801]  ? __radix_tree_create+0x19e/0x200
    [  246.755813]  ? delete_node+0x136/0x370
    [  246.755838]  schedule+0x43/0xc0
    [  246.755904]  io_schedule+0x17/0x40
    [  246.755939]  wait_on_page_bit_common+0x17b/0x240
    [  246.755950]  ? wake_page_function+0xa0/0xa0
    [  246.755961]  ? add_to_page_cache_lru+0x160/0x160
    [  246.755972]  ? page_cache_tree_insert+0x170/0x170
    [  246.755983]  ? __lru_cache_add+0x96/0xb0
    [  246.756086]  __filemap_fdatawait_range+0x14f/0x1c0
    [  246.756097]  ? wait_on_page_bit_common+0x240/0x240
    [  246.756120]  ? __wake_up_locked_key_bookmark+0x20/0x20
    [  246.756167]  ? wait_on_all_pages_writeback+0xc9/0x100
    [  246.756179]  ? __remove_ino_entry+0x120/0x120
    [  246.756192]  ? wait_woken+0x100/0x100
    [  246.756204]  filemap_fdatawait_range+0x9/0x20
    [  246.756216]  write_checkpoint+0x18a1/0x1f00
    [  246.756254]  ? blk_get_request+0x10/0x10
    [  246.756265]  ? cpumask_next_and+0x43/0x60
    [  246.756279]  ? f2fs_sync_inode_meta+0x160/0x160
    [  246.756289]  ? remove_element.isra.4+0xa0/0xa0
    [  246.756300]  ? __put_compound_page+0x40/0x40
    [  246.756310]  ? f2fs_sync_fs+0xec/0x1c0
    [  246.756320]  ? f2fs_sync_fs+0x120/0x1c0
    [  246.756329]  f2fs_sync_fs+0x120/0x1c0
    [  246.756357]  ? trace_event_raw_event_f2fs__page+0x260/0x260
    [  246.756393]  ? ata_build_rw_tf+0x173/0x410
    [  246.756397]  f2fs_balance_fs_bg+0x198/0x390
    [  246.756405]  ? drop_inmem_page+0x230/0x230
    [  246.756415]  ? ahci_qc_prep+0x1bb/0x2e0
    [  246.756418]  ? ahci_qc_issue+0x1df/0x290
    [  246.756422]  ? __accumulate_pelt_segments+0x42/0xd0
    [  246.756426]  ? f2fs_write_node_pages+0xd1/0x380
    [  246.756429]  f2fs_write_node_pages+0xd1/0x380
    [  246.756437]  ? sync_node_pages+0x8f0/0x8f0
    [  246.756440]  ? update_curr+0x53/0x220
    [  246.756444]  ? __accumulate_pelt_segments+0xa2/0xd0
    [  246.756448]  ? __update_load_avg_se.isra.39+0x349/0x360
    [  246.756452]  ? do_writepages+0x2a/0xa0
    [  246.756456]  do_writepages+0x2a/0xa0
    [  246.756460]  __writeback_single_inode+0x70/0x490
    [  246.756463]  ? check_preempt_wakeup+0x199/0x310
    [  246.756467]  writeback_sb_inodes+0x2a2/0x660
    [  246.756471]  ? is_empty_dir_inode+0x40/0x40
    [  246.756474]  ? __writeback_single_inode+0x490/0x490
    [  246.756477]  ? string+0xbf/0xf0
    [  246.756480]  ? down_read_trylock+0x35/0x60
    [  246.756484]  __writeback_inodes_wb+0x9f/0xf0
    [  246.756488]  wb_writeback+0x41d/0x4b0
    [  246.756492]  ? writeback_inodes_wb.constprop.55+0x150/0x150
    [  246.756498]  ? set_worker_desc+0xf7/0x130
    [  246.756502]  ? current_is_workqueue_rescuer+0x60/0x60
    [  246.756511]  ? _find_next_bit+0x2c/0xa0
    [  246.756514]  ? wb_workfn+0x400/0x5d0
    [  246.756518]  wb_workfn+0x400/0x5d0
    [  246.756521]  ? finish_task_switch+0xdf/0x2a0
    [  246.756525]  ? inode_wait_for_writeback+0x30/0x30
    [  246.756529]  process_one_work+0x3a7/0x6f0
    [  246.756533]  worker_thread+0x82/0x750
    [  246.756537]  kthread+0x16f/0x1c0
    [  246.756541]  ? trace_event_raw_event_workqueue_work+0x110/0x110
    [  246.756544]  ? kthread_create_worker_on_cpu+0xb0/0xb0
    [  246.756548]  ret_from_fork+0x1f/0x30
    
    Signed-off-by: Sheng Yong <shengyong1@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 33e79697e41c..aa720cc44509 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -691,7 +691,12 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	fio.op = REQ_OP_WRITE;
 	fio.op_flags = REQ_SYNC;
 	fio.new_blkaddr = newaddr;
-	f2fs_submit_page_write(&fio);
+	err = f2fs_submit_page_write(&fio);
+	if (err) {
+		if (PageWriteback(fio.encrypted_page))
+			end_page_writeback(fio.encrypted_page);
+		goto put_page_out;
+	}
 
 	f2fs_update_iostat(fio.sbi, FS_GC_DATA_IO, F2FS_BLKSIZE);
 

commit 1ad71a27124caf0b68ddd3c92be01aa2b2a72b2a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Dec 7 16:25:39 2017 -0800

    f2fs: add an ioctl to disable GC for specific file
    
    This patch gives a flag to disable GC on given file, which would be useful, when
    user wants to keep its block map. It also conducts in-place-update for dontmove
    file.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index d844dcb80570..33e79697e41c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -624,6 +624,11 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	if (f2fs_is_atomic_file(inode))
 		goto out;
 
+	if (f2fs_is_pinned_file(inode)) {
+		f2fs_pin_file_control(inode, true);
+		goto out;
+	}
+
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, bidx, LOOKUP_NODE);
 	if (err)
@@ -720,6 +725,11 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 
 	if (f2fs_is_atomic_file(inode))
 		goto out;
+	if (f2fs_is_pinned_file(inode)) {
+		if (gc_type == FG_GC)
+			f2fs_pin_file_control(inode, true);
+		goto out;
+	}
 
 	if (gc_type == BG_GC) {
 		if (PageWriteback(page))
@@ -1091,6 +1101,7 @@ void build_gc_manager(struct f2fs_sb_info *sbi)
 
 	sbi->fggc_threshold = div64_u64((main_count - ovp_count) *
 				BLKS_PER_SEC(sbi), (main_count - resv_count));
+	sbi->gc_pin_file_threshold = DEF_GC_FAILED_PINNED_FILES;
 
 	/* give warm/cold data area from slower device */
 	if (sbi->s_ndevs && sbi->segs_per_sec == 1)

commit 1751e8a6cb935e555fcdbcb9ab4f0446e322ca3e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 27 13:05:09 2017 -0800

    Rename superblock flags (MS_xyz -> SB_xyz)
    
    This is a pure automated search-and-replace of the internal kernel
    superblock flags.
    
    The s_flags are now called SB_*, with the names and the values for the
    moment mirroring the MS_* flags that they're equivalent to.
    
    Note how the MS_xyz flags are the ones passed to the mount system call,
    while the SB_xyz flags are what we then use in sb->s_flags.
    
    The script to do this was:
    
        # places to look in; re security/*: it generally should *not* be
        # touched (that stuff parses mount(2) arguments directly), but
        # there are two places where we really deal with superblock flags.
        FILES="drivers/mtd drivers/staging/lustre fs ipc mm \
                include/linux/fs.h include/uapi/linux/bfs_fs.h \
                security/apparmor/apparmorfs.c security/apparmor/include/lib.h"
        # the list of MS_... constants
        SYMS="RDONLY NOSUID NODEV NOEXEC SYNCHRONOUS REMOUNT MANDLOCK \
              DIRSYNC NOATIME NODIRATIME BIND MOVE REC VERBOSE SILENT \
              POSIXACL UNBINDABLE PRIVATE SLAVE SHARED RELATIME KERNMOUNT \
              I_VERSION STRICTATIME LAZYTIME SUBMOUNT NOREMOTELOCK NOSEC BORN \
              ACTIVE NOUSER"
    
        SED_PROG=
        for i in $SYMS; do SED_PROG="$SED_PROG -e s/MS_$i/SB_$i/g"; done
    
        # we want files that contain at least one of MS_...,
        # with fs/namespace.c and fs/pnode.c excluded.
        L=$(for i in $SYMS; do git grep -w -l MS_$i $FILES; done| sort|uniq|grep -v '^fs/namespace.c'|grep -v '^fs/pnode.c')
    
        for f in $L; do sed -i $f $SED_PROG; done
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 5d5bba462f26..d844dcb80570 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1005,7 +1005,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 
 	cpc.reason = __get_cp_reason(sbi);
 gc_more:
-	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE))) {
+	if (unlikely(!(sbi->sb->s_flags & SB_ACTIVE))) {
 		ret = -EINVAL;
 		goto stop;
 	}

commit bb06664a534ba4833373f19ae018efff91ab2908
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Nov 3 10:21:05 2017 +0800

    f2fs: avoid race in between GC and block exchange
    
    During block exchange in {insert,collapse,move}_range, page-block mapping
    is unstable due to mapping moving or recovery, so there should be no
    concurrent cache read operation rely on such mapping, nor cache write
    operation to mess up block exchange.
    
    So this patch let background GC be aware of that.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index ff8f0012888d..5d5bba462f26 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -832,10 +832,17 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 				continue;
 			}
 
+			if (!down_write_trylock(
+				&F2FS_I(inode)->dio_rwsem[WRITE])) {
+				iput(inode);
+				continue;
+			}
+
 			start_bidx = start_bidx_of_node(nofs, inode);
 			data_page = get_read_data_page(inode,
 					start_bidx + ofs_in_node, REQ_RAHEAD,
 					true);
+			up_write(&F2FS_I(inode)->dio_rwsem[WRITE]);
 			if (IS_ERR(data_page)) {
 				iput(inode);
 				continue;

commit 3d26fa6be3c487fac7d87dc4a6f02a9ff0f6b1ef
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Oct 30 17:49:53 2017 +0800

    f2fs: use rw_semaphore to protect SIT cache
    
    There are some cases user didn't update SIT cache under this lock,
    so let's use rw_semaphore instead of mutex to enhance concurrently
    accessing.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c7b1d704846a..ff8f0012888d 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -456,10 +456,10 @@ static int check_valid_map(struct f2fs_sb_info *sbi,
 	struct seg_entry *sentry;
 	int ret;
 
-	mutex_lock(&sit_i->sentry_lock);
+	down_read(&sit_i->sentry_lock);
 	sentry = get_seg_entry(sbi, segno);
 	ret = f2fs_test_bit(offset, sentry->cur_valid_map);
-	mutex_unlock(&sit_i->sentry_lock);
+	up_read(&sit_i->sentry_lock);
 	return ret;
 }
 
@@ -893,10 +893,10 @@ static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
 	struct sit_info *sit_i = SIT_I(sbi);
 	int ret;
 
-	mutex_lock(&sit_i->sentry_lock);
+	down_write(&sit_i->sentry_lock);
 	ret = DIRTY_I(sbi)->v_ops->get_victim(sbi, victim, gc_type,
 					      NO_CHECK_TYPE, LFS);
-	mutex_unlock(&sit_i->sentry_lock);
+	up_write(&sit_i->sentry_lock);
 	return ret;
 }
 
@@ -944,8 +944,8 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 		/*
 		 * this is to avoid deadlock:
 		 * - lock_page(sum_page)         - f2fs_replace_block
-		 *  - check_valid_map()            - mutex_lock(sentry_lock)
-		 *   - mutex_lock(sentry_lock)     - change_curseg()
+		 *  - check_valid_map()            - down_write(sentry_lock)
+		 *   - down_read(sentry_lock)     - change_curseg()
 		 *                                  - lock_page(sum_page)
 		 */
 		if (type == SUM_TYPE_NODE)

commit 01eccef7930f137bed9501bf0923931f45909b94
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Oct 28 16:52:30 2017 +0800

    f2fs: support get_page error injection
    
    This patch adds to support get_page error injection to simulate
    out-of-memory test scenario.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 197ebf45e4e4..c7b1d704846a 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -650,8 +650,8 @@ static void move_data_block(struct inode *inode, block_t bidx,
 	allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &newaddr,
 					&sum, CURSEG_COLD_DATA, NULL, false);
 
-	fio.encrypted_page = pagecache_get_page(META_MAPPING(fio.sbi), newaddr,
-					FGP_LOCK | FGP_CREAT, GFP_NOFS);
+	fio.encrypted_page = f2fs_pagecache_get_page(META_MAPPING(fio.sbi),
+				newaddr, FGP_LOCK | FGP_CREAT, GFP_NOFS);
 	if (!fio.encrypted_page) {
 		err = -ENOMEM;
 		goto recover_block;

commit 39d787bec4f792e69e24b11aa3d61ae1c0e4830b
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Sep 29 13:59:38 2017 +0800

    f2fs: enhance multiple device flush
    
    When multiple device feature is enabled, during ->fsync we will issue
    flush in all devices to make sure node/data of the file being persisted
    into storage. But some flushes of device could be unneeded as file's
    data may be not writebacked into those devices. So this patch adds and
    manage bitmap per inode in global cache to indicate which device is
    dirty and it needs to issue flush during ->fsync, hence, we could improve
    performance of fsync in scenario of multiple device.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index f777e073e509..197ebf45e4e4 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -598,6 +598,7 @@ static void move_data_block(struct inode *inode, block_t bidx,
 {
 	struct f2fs_io_info fio = {
 		.sbi = F2FS_I_SB(inode),
+		.ino = inode->i_ino,
 		.type = DATA,
 		.temp = COLD,
 		.op = REQ_OP_READ,
@@ -728,6 +729,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 	} else {
 		struct f2fs_io_info fio = {
 			.sbi = F2FS_I_SB(inode),
+			.ino = inode->i_ino,
 			.type = DATA,
 			.temp = COLD,
 			.op = REQ_OP_WRITE,

commit 91f4382b50ee9954f5dad459803200ca2a8bd6fb
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Sat Sep 23 17:02:18 2017 +0800

    Revert "f2fs: node segment is prior to data segment selected victim"
    
    This reverts commit b9cd20619e359d199b755543474c3d853c8e3415.
    
    That patch causes much fewer node segments (which can be used for SSR)
    than before, and in the corner case (e.g. create and delete *.txt files in
    one same directory, there will be very few node segments but many data
    segments), if the reserved free segments are all used up during gc, then
    the write_checkpoint can still flush dentry pages to data ssr segments,
    but will probably fail to flush node pages to node ssr segments, since
    there are not enough node ssr segments left (the left ones are all
    full).
    
    So revert this patch to give a fair chance to let node segments remain
    for SSR, which provides more robustness for corner cases.
    
    Conflicts:
            fs/f2fs/gc.c
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index bfe6a8ccc3a0..f777e073e509 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -267,16 +267,6 @@ static unsigned int get_cb_cost(struct f2fs_sb_info *sbi, unsigned int segno)
 	return UINT_MAX - ((100 * (100 - u) * age) / (100 + u));
 }
 
-static unsigned int get_greedy_cost(struct f2fs_sb_info *sbi,
-						unsigned int segno)
-{
-	unsigned int valid_blocks =
-			get_valid_blocks(sbi, segno, true);
-
-	return IS_DATASEG(get_seg_entry(sbi, segno)->type) ?
-				valid_blocks * 2 : valid_blocks;
-}
-
 static inline unsigned int get_gc_cost(struct f2fs_sb_info *sbi,
 			unsigned int segno, struct victim_sel_policy *p)
 {
@@ -285,7 +275,7 @@ static inline unsigned int get_gc_cost(struct f2fs_sb_info *sbi,
 
 	/* alloc_mode == LFS */
 	if (p->gc_mode == GC_GREEDY)
-		return get_greedy_cost(sbi, segno);
+		return get_valid_blocks(sbi, segno, true);
 	else
 		return get_cb_cost(sbi, segno);
 }

commit d4c759ee5faa51e0b0ee55d8a229ba5b80c4917e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 5 17:04:35 2017 -0700

    f2fs: use generic terms used for encrypted block management
    
    This patch renames functions regarding to buffer management via META_MAPPING
    used for encrypted blocks especially. We can actually use them in generic way.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 6a12f33d0cdd..bfe6a8ccc3a0 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -599,8 +599,12 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	return true;
 }
 
-static void move_encrypted_block(struct inode *inode, block_t bidx,
-							unsigned int segno, int off)
+/*
+ * Move data block via META_MAPPING while keeping locked data page.
+ * This can be used to move blocks, aka LBAs, directly on disk.
+ */
+static void move_data_block(struct inode *inode, block_t bidx,
+					unsigned int segno, int off)
 {
 	struct f2fs_io_info fio = {
 		.sbi = F2FS_I_SB(inode),
@@ -873,9 +877,10 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			start_bidx = start_bidx_of_node(nofs, inode)
 								+ ofs_in_node;
 			if (f2fs_encrypted_file(inode))
-				move_encrypted_block(inode, start_bidx, segno, off);
+				move_data_block(inode, start_bidx, segno, off);
 			else
-				move_data_page(inode, start_bidx, gc_type, segno, off);
+				move_data_page(inode, start_bidx, gc_type,
+								segno, off);
 
 			if (locked) {
 				up_write(&fi->dio_rwsem[WRITE]);

commit 1958593e4fa9fa9e3e2d03b27e72af314c2891be
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 5 16:54:24 2017 -0700

    f2fs: introduce f2fs_encrypted_file for clean-up
    
    This patch replaces (f2fs_encrypted_inode() && S_ISREG()) with
    f2fs_encrypted_file(), which gives no functional change.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b226760afba8..6a12f33d0cdd 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -831,8 +831,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 				continue;
 
 			/* if encrypted inode, let's go phase 3 */
-			if (f2fs_encrypted_inode(inode) &&
-						S_ISREG(inode->i_mode)) {
+			if (f2fs_encrypted_file(inode)) {
 				add_gc_inode(gc_list, inode);
 				continue;
 			}
@@ -873,7 +872,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 			start_bidx = start_bidx_of_node(nofs, inode)
 								+ ofs_in_node;
-			if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
+			if (f2fs_encrypted_file(inode))
 				move_encrypted_block(inode, start_bidx, segno, off);
 			else
 				move_data_page(inode, start_bidx, gc_type, segno, off);

commit 2afce76a1151fe2f1104962c327d8f85047045e6
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Mon Sep 4 11:10:18 2017 +0800

    Revert "f2fs: add a new function get_ssr_cost"
    
    This reverts commit b7b7c4cf1c9ef0272a65f1480457cbfdadcda19d.
    
    se->ckpt_valid_blocks will never be smaller than se->valid_blocks, so just
    remove get_ssr_cost.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index cd147e7c71e8..b226760afba8 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -277,20 +277,11 @@ static unsigned int get_greedy_cost(struct f2fs_sb_info *sbi,
 				valid_blocks * 2 : valid_blocks;
 }
 
-static unsigned int get_ssr_cost(struct f2fs_sb_info *sbi,
-						unsigned int segno)
-{
-	struct seg_entry *se = get_seg_entry(sbi, segno);
-
-	return se->ckpt_valid_blocks > se->valid_blocks ?
-				se->ckpt_valid_blocks : se->valid_blocks;
-}
-
 static inline unsigned int get_gc_cost(struct f2fs_sb_info *sbi,
 			unsigned int segno, struct victim_sel_policy *p)
 {
 	if (p->alloc_mode == SSR)
-		return get_ssr_cost(sbi, segno);
+		return get_seg_entry(sbi, segno)->ckpt_valid_blocks;
 
 	/* alloc_mode == LFS */
 	if (p->gc_mode == GC_GREEDY)

commit 73ac2f4e8256b9605c84364011322f015b31f499
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Aug 23 18:23:24 2017 +0800

    f2fs: fix to avoid race in between aio and gc
    
    We won't wait DIO synchronously when doing AIO, so there will be potential
    IO reorder in between AIO and GC, which will cause data corruption.
    
    This patch adds inode_dio_wait to serialize aio and data GC to avoid this
    issue.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 57bea2182f30..cd147e7c71e8 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -875,6 +875,9 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 					continue;
 				}
 				locked = true;
+
+				/* wait for all inflight aio data */
+				inode_dio_wait(inode);
 			}
 
 			start_bidx = start_bidx_of_node(nofs, inode)

commit c56f16dab0dfc8a37bc6133f2c2a02ffb873648b
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Aug 11 18:00:15 2017 +0800

    f2fs: add tracepoint for f2fs_gc
    
    This patch adds tracepoint for f2fs_gc.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e60480f71bb5..57bea2182f30 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -919,7 +919,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	struct blk_plug plug;
 	unsigned int segno = start_segno;
 	unsigned int end_segno = start_segno + sbi->segs_per_sec;
-	int sec_freed = 0;
+	int seg_freed = 0;
 	unsigned char type = IS_DATASEG(get_seg_entry(sbi, segno)->type) ?
 						SUM_TYPE_DATA : SUM_TYPE_NODE;
 
@@ -965,6 +965,10 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 								gc_type);
 
 		stat_inc_seg_count(sbi, type, gc_type);
+
+		if (gc_type == FG_GC &&
+				get_valid_blocks(sbi, segno, false) == 0)
+			seg_freed++;
 next:
 		f2fs_put_page(sum_page, 0);
 	}
@@ -975,21 +979,17 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 
 	blk_finish_plug(&plug);
 
-	if (gc_type == FG_GC &&
-		get_valid_blocks(sbi, start_segno, true) == 0)
-		sec_freed = 1;
-
 	stat_inc_call_count(sbi->stat_info);
 
-	return sec_freed;
+	return seg_freed;
 }
 
 int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 			bool background, unsigned int segno)
 {
 	int gc_type = sync ? FG_GC : BG_GC;
-	int sec_freed = 0;
-	int ret;
+	int sec_freed = 0, seg_freed = 0, total_freed = 0;
+	int ret = 0;
 	struct cp_control cpc;
 	unsigned int init_segno = segno;
 	struct gc_inode_list gc_list = {
@@ -997,6 +997,15 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 		.iroot = RADIX_TREE_INIT(GFP_NOFS),
 	};
 
+	trace_f2fs_gc_begin(sbi->sb, sync, background,
+				get_pages(sbi, F2FS_DIRTY_NODES),
+				get_pages(sbi, F2FS_DIRTY_DENTS),
+				get_pages(sbi, F2FS_DIRTY_IMETA),
+				free_sections(sbi),
+				free_segments(sbi),
+				reserved_segments(sbi),
+				prefree_segments(sbi));
+
 	cpc.reason = __get_cp_reason(sbi);
 gc_more:
 	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE))) {
@@ -1023,17 +1032,20 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 			gc_type = FG_GC;
 	}
 
-	ret = -EINVAL;
 	/* f2fs_balance_fs doesn't need to do BG_GC in critical path. */
-	if (gc_type == BG_GC && !background)
+	if (gc_type == BG_GC && !background) {
+		ret = -EINVAL;
 		goto stop;
-	if (!__get_victim(sbi, &segno, gc_type))
+	}
+	if (!__get_victim(sbi, &segno, gc_type)) {
+		ret = -ENODATA;
 		goto stop;
-	ret = 0;
+	}
 
-	if (do_garbage_collect(sbi, segno, &gc_list, gc_type) &&
-			gc_type == FG_GC)
+	seg_freed = do_garbage_collect(sbi, segno, &gc_list, gc_type);
+	if (gc_type == FG_GC && seg_freed == sbi->segs_per_sec)
 		sec_freed++;
+	total_freed += seg_freed;
 
 	if (gc_type == FG_GC)
 		sbi->cur_victim_sec = NULL_SEGNO;
@@ -1050,6 +1062,16 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 stop:
 	SIT_I(sbi)->last_victim[ALLOC_NEXT] = 0;
 	SIT_I(sbi)->last_victim[FLUSH_DEVICE] = init_segno;
+
+	trace_f2fs_gc_end(sbi->sb, ret, total_freed, sec_freed,
+				get_pages(sbi, F2FS_DIRTY_NODES),
+				get_pages(sbi, F2FS_DIRTY_DENTS),
+				get_pages(sbi, F2FS_DIRTY_IMETA),
+				free_sections(sbi),
+				free_segments(sbi),
+				reserved_segments(sbi),
+				prefree_segments(sbi));
+
 	mutex_unlock(&sbi->gc_mutex);
 
 	put_gc_inode(&gc_list);

commit b8c502b81e3f899c6488967dec61eed0f5907db3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Aug 7 23:12:46 2017 +0800

    f2fs: fix potential overflow when adjusting GC cycle
    
    While comparing signed and unsigned variables, compiler will converts the
    signed value to unsigned one, due to this reason, {in,de}crease_sleep_time
    may return overflowed result.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 8da7c14a9d29..e60480f71bb5 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -28,7 +28,7 @@ static int gc_thread_func(void *data)
 	struct f2fs_sb_info *sbi = data;
 	struct f2fs_gc_kthread *gc_th = sbi->gc_thread;
 	wait_queue_head_t *wq = &sbi->gc_thread->gc_wait_queue_head;
-	long wait_ms;
+	unsigned int wait_ms;
 
 	wait_ms = gc_th->min_sleep_time;
 

commit d9872a698c393e0d1abca86bf05b62712cbfc581
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sun Aug 6 22:09:00 2017 -0700

    f2fs: introduce gc_urgent mode for background GC
    
    This patch adds a sysfs entry to control urgent mode for background GC.
    If this is set, background GC thread conducts GC with gc_urgent_sleep_time
    all the time.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 620dca443b29..8da7c14a9d29 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -35,9 +35,14 @@ static int gc_thread_func(void *data)
 	set_freezable();
 	do {
 		wait_event_interruptible_timeout(*wq,
-				kthread_should_stop() || freezing(current),
+				kthread_should_stop() || freezing(current) ||
+				gc_th->gc_wake,
 				msecs_to_jiffies(wait_ms));
 
+		/* give it a try one time */
+		if (gc_th->gc_wake)
+			gc_th->gc_wake = 0;
+
 		if (try_to_freeze())
 			continue;
 		if (kthread_should_stop())
@@ -74,6 +79,11 @@ static int gc_thread_func(void *data)
 		if (!mutex_trylock(&sbi->gc_mutex))
 			goto next;
 
+		if (gc_th->gc_urgent) {
+			wait_ms = gc_th->urgent_sleep_time;
+			goto do_gc;
+		}
+
 		if (!is_idle(sbi)) {
 			increase_sleep_time(gc_th, &wait_ms);
 			mutex_unlock(&sbi->gc_mutex);
@@ -84,7 +94,7 @@ static int gc_thread_func(void *data)
 			decrease_sleep_time(gc_th, &wait_ms);
 		else
 			increase_sleep_time(gc_th, &wait_ms);
-
+do_gc:
 		stat_inc_bggc_count(sbi);
 
 		/* if return value is not zero, no victim was selected */
@@ -115,11 +125,14 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 		goto out;
 	}
 
+	gc_th->urgent_sleep_time = DEF_GC_THREAD_URGENT_SLEEP_TIME;
 	gc_th->min_sleep_time = DEF_GC_THREAD_MIN_SLEEP_TIME;
 	gc_th->max_sleep_time = DEF_GC_THREAD_MAX_SLEEP_TIME;
 	gc_th->no_gc_sleep_time = DEF_GC_THREAD_NOGC_SLEEP_TIME;
 
 	gc_th->gc_idle = 0;
+	gc_th->gc_urgent = 0;
+	gc_th->gc_wake= 0;
 
 	sbi->gc_thread = gc_th;
 	init_waitqueue_head(&sbi->gc_thread->gc_wait_queue_head);

commit b0af6d491a6b5f5622fa91ac75f34f3640f862c4
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Aug 2 23:21:48 2017 +0800

    f2fs: add app/fs io stat
    
    This patch enables inner app/fs io stats and introduces below virtual fs
    nodes for exposing stats info:
    /sys/fs/f2fs/<dev>/iostat_enable
    /proc/fs/f2fs/<dev>/iostat_info
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    [Jaegeuk Kim: fix wrong stat assignment]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index f57cadae1a30..620dca443b29 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -689,6 +689,8 @@ static void move_encrypted_block(struct inode *inode, block_t bidx,
 	fio.new_blkaddr = newaddr;
 	f2fs_submit_page_write(&fio);
 
+	f2fs_update_iostat(fio.sbi, FS_GC_DATA_IO, F2FS_BLKSIZE);
+
 	f2fs_update_data_blkaddr(&dn, newaddr);
 	set_inode_flag(inode, FI_APPEND_WRITE);
 	if (page->index == 0)
@@ -736,6 +738,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 			.page = page,
 			.encrypted_page = NULL,
 			.need_lock = LOCK_REQ,
+			.io_type = FS_GC_DATA_IO,
 		};
 		bool is_dirty = PageDirty(page);
 		int err;

commit 7a2af766af15887754f7f7a0869b4603b390876a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jul 19 00:19:06 2017 +0800

    f2fs: enhance on-disk inode structure scalability
    
    This patch add new flag F2FS_EXTRA_ATTR storing in inode.i_inline
    to indicate that on-disk structure of current inode is extended.
    
    In order to extend, we changed the inode structure a bit:
    
    Original one:
    
    struct f2fs_inode {
            ...
            struct f2fs_extent i_ext;
            __le32 i_addr[DEF_ADDRS_PER_INODE];
            __le32 i_nid[DEF_NIDS_PER_INODE];
    }
    
    Extended one:
    
    struct f2fs_inode {
            ...
            struct f2fs_extent i_ext;
            union {
                    struct {
                            __le16 i_extra_isize;
                            __le16 i_padding;
                            __le32 i_extra_end[0];
                    };
                    __le32 i_addr[DEF_ADDRS_PER_INODE];
            };
            __le32 i_nid[DEF_NIDS_PER_INODE];
    }
    
    Once F2FS_EXTRA_ATTR is set, we will steal four bytes in the head of
    i_addr field for storing i_extra_isize and i_padding. with i_extra_isize,
    we can calculate actual size of reserved space in i_addr, available
    attribute fields included in total extra attribute fields for current
    inode can be described as below:
    
      +--------------------+
      | .i_mode            |
      | ...                |
      | .i_ext             |
      +--------------------+
      | .i_extra_isize     |-----+
      | .i_padding         |     |
      | .i_prjid           |     |
      | .i_atime_extra     |     |
      | .i_ctime_extra     |     |
      | .i_mtime_extra     |<----+
      | .i_inode_cs        |<----- store blkaddr/inline from here
      | .i_xattr_cs        |
      | ...                |
      +--------------------+
      |                    |
      |    block address   |
      |                    |
      +--------------------+
      | .i_nid             |
      +--------------------+
      |   node_footer      |
      | (nid, ino, offset) |
      +--------------------+
    
    Hence, with this patch, we would enhance scalability of f2fs inode for
    storing more newly added attribute.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 41c649c9c55c..f57cadae1a30 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -587,7 +587,7 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	}
 
 	*nofs = ofs_of_node(node_page);
-	source_blkaddr = datablock_addr(node_page, ofs_in_node);
+	source_blkaddr = datablock_addr(NULL, node_page, ofs_in_node);
 	f2fs_put_page(node_page, 1);
 
 	if (source_blkaddr != blkaddr)

commit dc6febb6bcec7ff1b4a4d306411013b5f648f27e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Jul 22 08:52:23 2017 +0800

    f2fs: make background threads of f2fs being aware of freezing
    
    When ->freeze_fs is called from lvm for doing snapshot, it needs to
    make sure there will be no more changes in filesystem's data, however,
    previously, background threads like GC thread wasn't aware of freezing,
    so in environment with active background threads, data of snapshot
    becomes unstable.
    
    This patch fixes this issue by adding sb_{start,end}_intwrite in
    below background threads:
    - GC thread
    - flush thread
    - discard thread
    
    Note that, don't use sb_start_intwrite() in gc_thread_func() due to:
    
    generic/241 reports below bug:
    
     ======================================================
     WARNING: possible circular locking dependency detected
     4.13.0-rc1+ #32 Tainted: G           O
     ------------------------------------------------------
     f2fs_gc-250:0/22186 is trying to acquire lock:
      (&sbi->gc_mutex){+.+...}, at: [<f8fa7f0b>] f2fs_sync_fs+0x7b/0x1b0 [f2fs]
    
     but task is already holding lock:
      (sb_internal#2){++++.-}, at: [<f8fb5609>] gc_thread_func+0x159/0x4a0 [f2fs]
    
     which lock already depends on the new lock.
    
     the existing dependency chain (in reverse order) is:
    
     -> #2 (sb_internal#2){++++.-}:
            __lock_acquire+0x405/0x7b0
            lock_acquire+0xae/0x220
            __sb_start_write+0x11d/0x1f0
            f2fs_evict_inode+0x2d6/0x4e0 [f2fs]
            evict+0xa8/0x170
            iput+0x1fb/0x2c0
            f2fs_sync_inode_meta+0x3f/0xf0 [f2fs]
            write_checkpoint+0x1b1/0x750 [f2fs]
            f2fs_sync_fs+0x85/0x1b0 [f2fs]
            f2fs_do_sync_file.isra.24+0x137/0xa30 [f2fs]
            f2fs_sync_file+0x34/0x40 [f2fs]
            vfs_fsync_range+0x4a/0xa0
            do_fsync+0x3c/0x60
            SyS_fdatasync+0x15/0x20
            do_fast_syscall_32+0xa1/0x1b0
            entry_SYSENTER_32+0x4c/0x7b
    
     -> #1 (&sbi->cp_mutex){+.+...}:
            __lock_acquire+0x405/0x7b0
            lock_acquire+0xae/0x220
            __mutex_lock+0x4f/0x830
            mutex_lock_nested+0x25/0x30
            write_checkpoint+0x2f/0x750 [f2fs]
            f2fs_sync_fs+0x85/0x1b0 [f2fs]
            sync_filesystem+0x67/0x80
            generic_shutdown_super+0x27/0x100
            kill_block_super+0x22/0x50
            kill_f2fs_super+0x3a/0x40 [f2fs]
            deactivate_locked_super+0x3d/0x70
            deactivate_super+0x40/0x60
            cleanup_mnt+0x39/0x70
            __cleanup_mnt+0x10/0x20
            task_work_run+0x69/0x80
            exit_to_usermode_loop+0x57/0x92
            do_fast_syscall_32+0x18c/0x1b0
            entry_SYSENTER_32+0x4c/0x7b
    
     -> #0 (&sbi->gc_mutex){+.+...}:
            validate_chain.isra.36+0xc50/0xdb0
            __lock_acquire+0x405/0x7b0
            lock_acquire+0xae/0x220
            __mutex_lock+0x4f/0x830
            mutex_lock_nested+0x25/0x30
            f2fs_sync_fs+0x7b/0x1b0 [f2fs]
            f2fs_balance_fs_bg+0xb9/0x200 [f2fs]
            gc_thread_func+0x302/0x4a0 [f2fs]
            kthread+0xe9/0x120
            ret_from_fork+0x19/0x24
    
     other info that might help us debug this:
    
     Chain exists of:
       &sbi->gc_mutex --> &sbi->cp_mutex --> sb_internal#2
    
      Possible unsafe locking scenario:
    
            CPU0                    CPU1
            ----                    ----
       lock(sb_internal#2);
                                    lock(&sbi->cp_mutex);
                                    lock(sb_internal#2);
       lock(&sbi->gc_mutex);
    
      *** DEADLOCK ***
    
     1 lock held by f2fs_gc-250:0/22186:
      #0:  (sb_internal#2){++++.-}, at: [<f8fb5609>] gc_thread_func+0x159/0x4a0 [f2fs]
    
     stack backtrace:
     CPU: 2 PID: 22186 Comm: f2fs_gc-250:0 Tainted: G           O    4.13.0-rc1+ #32
     Hardware name: innotek GmbH VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006
     Call Trace:
      dump_stack+0x5f/0x92
      print_circular_bug+0x1b3/0x1bd
      validate_chain.isra.36+0xc50/0xdb0
      ? __this_cpu_preempt_check+0xf/0x20
      __lock_acquire+0x405/0x7b0
      lock_acquire+0xae/0x220
      ? f2fs_sync_fs+0x7b/0x1b0 [f2fs]
      __mutex_lock+0x4f/0x830
      ? f2fs_sync_fs+0x7b/0x1b0 [f2fs]
      mutex_lock_nested+0x25/0x30
      ? f2fs_sync_fs+0x7b/0x1b0 [f2fs]
      f2fs_sync_fs+0x7b/0x1b0 [f2fs]
      f2fs_balance_fs_bg+0xb9/0x200 [f2fs]
      gc_thread_func+0x302/0x4a0 [f2fs]
      ? preempt_schedule_common+0x2f/0x4d
      ? f2fs_gc+0x540/0x540 [f2fs]
      kthread+0xe9/0x120
      ? f2fs_gc+0x540/0x540 [f2fs]
      ? kthread_create_on_node+0x30/0x30
      ret_from_fork+0x19/0x24
    
    The deadlock occurs in below condition:
    GC Thread                       Thread B
    - sb_start_intwrite
                                    - f2fs_sync_file
                                     - f2fs_sync_fs
                                      - mutex_lock(&sbi->gc_mutex)
                                       - write_checkpoint
                                        - block_operations
                                         - f2fs_sync_inode_meta
                                          - iput
                                           - sb_start_intwrite
     - mutex_lock(&sbi->gc_mutex)
    
    Fix this by altering sb_start_intwrite to sb_start_write_trylock.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index fa3d2e2df8e7..41c649c9c55c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -55,6 +55,9 @@ static int gc_thread_func(void *data)
 		}
 #endif
 
+		if (!sb_start_write_trylock(sbi->sb))
+			continue;
+
 		/*
 		 * [GC triggering condition]
 		 * 0. GC is not conducted currently.
@@ -69,12 +72,12 @@ static int gc_thread_func(void *data)
 		 * So, I'd like to wait some time to collect dirty segments.
 		 */
 		if (!mutex_trylock(&sbi->gc_mutex))
-			continue;
+			goto next;
 
 		if (!is_idle(sbi)) {
 			increase_sleep_time(gc_th, &wait_ms);
 			mutex_unlock(&sbi->gc_mutex);
-			continue;
+			goto next;
 		}
 
 		if (has_enough_invalid_blocks(sbi))
@@ -93,6 +96,8 @@ static int gc_thread_func(void *data)
 
 		/* balancing f2fs's metadata periodically */
 		f2fs_balance_fs_bg(sbi);
+next:
+		sb_end_write(sbi->sb);
 
 	} while (!kthread_should_stop());
 	return 0;

commit fb830fc5cfc90ba8236921aacb72c6d70bf78af7
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri May 19 23:37:01 2017 +0800

    f2fs: introduce io_list for serialize data/node IOs
    
    Serialize data/node IOs by using fifo list instead of mutex lock,
    it will help to enhance concurrency of f2fs, meanwhile keeping LFS
    IO semantics.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 570480571d72..fa3d2e2df8e7 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -600,6 +600,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx,
 		.op = REQ_OP_READ,
 		.op_flags = 0,
 		.encrypted_page = NULL,
+		.in_list = false,
 	};
 	struct dnode_of_data dn;
 	struct f2fs_summary sum;
@@ -643,7 +644,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx,
 	fio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;
 
 	allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &newaddr,
-							&sum, CURSEG_COLD_DATA);
+					&sum, CURSEG_COLD_DATA, NULL, false);
 
 	fio.encrypted_page = pagecache_get_page(META_MAPPING(fio.sbi), newaddr,
 					FGP_LOCK | FGP_CREAT, GFP_NOFS);

commit 1d7be2708277edfef95171d52fb65ee26eaa076b
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed May 17 10:36:58 2017 -0700

    f2fs: try to freeze in gc and discard threads
    
    This allows to freeze gc and discard threads.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 81392970fb2d..570480571d72 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -32,13 +32,14 @@ static int gc_thread_func(void *data)
 
 	wait_ms = gc_th->min_sleep_time;
 
+	set_freezable();
 	do {
+		wait_event_interruptible_timeout(*wq,
+				kthread_should_stop() || freezing(current),
+				msecs_to_jiffies(wait_ms));
+
 		if (try_to_freeze())
 			continue;
-		else
-			wait_event_interruptible_timeout(*wq,
-						kthread_should_stop(),
-						msecs_to_jiffies(wait_ms));
 		if (kthread_should_stop())
 			break;
 

commit b7b7c4cf1c9ef0272a65f1480457cbfdadcda19d
Author: Yunlei He <heyunlei@huawei.com>
Date:   Wed May 17 17:22:51 2017 +0800

    f2fs: add a new function get_ssr_cost
    
    This patch add a new method get_ssr_cost to select
    SSR segment more accurately.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 14c71ac76062..81392970fb2d 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -258,11 +258,20 @@ static unsigned int get_greedy_cost(struct f2fs_sb_info *sbi,
 				valid_blocks * 2 : valid_blocks;
 }
 
+static unsigned int get_ssr_cost(struct f2fs_sb_info *sbi,
+						unsigned int segno)
+{
+	struct seg_entry *se = get_seg_entry(sbi, segno);
+
+	return se->ckpt_valid_blocks > se->valid_blocks ?
+				se->ckpt_valid_blocks : se->valid_blocks;
+}
+
 static inline unsigned int get_gc_cost(struct f2fs_sb_info *sbi,
 			unsigned int segno, struct victim_sel_policy *p)
 {
 	if (p->alloc_mode == SSR)
-		return get_seg_entry(sbi, segno)->ckpt_valid_blocks;
+		return get_ssr_cost(sbi, segno);
 
 	/* alloc_mode == LFS */
 	if (p->gc_mode == GC_GREEDY)

commit cc15620bc826b14006956fd321e026ae96aff53a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 12 13:51:34 2017 -0700

    f2fs: avoid f2fs_lock_op for IPU writes
    
    Currently, if we do get_node_of_data before f2fs_lock_op, there may be dead lock
    as follows, where process A would be in infinite loop, and B will NOT be awaked.
    
    Process A(cp):            Process B:
    f2fs_lock_all(sbi)
                            get_dnode_of_data <---- lock dn.node_page
    flush_nodes             f2fs_lock_op
    
    So, this patch adds f2fs_trylock_op to avoid f2fs_lock_op done by IPU.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e2b13558a915..14c71ac76062 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -719,7 +719,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 			.old_blkaddr = NULL_ADDR,
 			.page = page,
 			.encrypted_page = NULL,
-			.need_lock = true,
+			.need_lock = LOCK_REQ,
 		};
 		bool is_dirty = PageDirty(page);
 		int err;

commit a912b54d3aaa011266dc266e3694f782f27233cf
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed May 10 11:18:25 2017 -0700

    f2fs: split bio cache
    
    Split DATA/NODE type bio cache according to different temperature,
    so write IOs with the same temperature can be merged in corresponding
    bio cache as much as possible, otherwise, different temperature write
    IOs submitting into one bio cache will always cause split of bio.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 67b87155bc48..e2b13558a915 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -586,6 +586,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx,
 	struct f2fs_io_info fio = {
 		.sbi = F2FS_I_SB(inode),
 		.type = DATA,
+		.temp = COLD,
 		.op = REQ_OP_READ,
 		.op_flags = 0,
 		.encrypted_page = NULL,
@@ -712,6 +713,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 		struct f2fs_io_info fio = {
 			.sbi = F2FS_I_SB(inode),
 			.type = DATA,
+			.temp = COLD,
 			.op = REQ_OP_WRITE,
 			.op_flags = REQ_SYNC,
 			.old_blkaddr = NULL_ADDR,

commit b9109b0e49b93b0ae663330acb36561b8f4f6905
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed May 10 11:28:38 2017 -0700

    f2fs: remove unnecessary read cases in merged IO flow
    
    Merged IO flow doesn't need to care about read IOs.
    
    f2fs_submit_merged_bio -> f2fs_submit_merged_write
    f2fs_submit_merged_bios -> f2fs_submit_merged_writes
    f2fs_submit_merged_bio_cond -> f2fs_submit_merged_write_cond
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 965fbf5d0a2e..67b87155bc48 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -670,7 +670,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx,
 	fio.op = REQ_OP_WRITE;
 	fio.op_flags = REQ_SYNC;
 	fio.new_blkaddr = newaddr;
-	f2fs_submit_page_mbio(&fio);
+	f2fs_submit_page_write(&fio);
 
 	f2fs_update_data_blkaddr(&dn, newaddr);
 	set_inode_flag(inode, FI_APPEND_WRITE);
@@ -936,8 +936,8 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	}
 
 	if (gc_type == FG_GC)
-		f2fs_submit_merged_bio(sbi,
-				(type == SUM_TYPE_NODE) ? NODE : DATA, WRITE);
+		f2fs_submit_merged_write(sbi,
+				(type == SUM_TYPE_NODE) ? NODE : DATA);
 
 	blk_finish_plug(&plug);
 

commit e5dbd9563e5528f98728ba0bc8361f804ace5aae
Author: Weichao Guo <guoweichao@huawei.com>
Date:   Thu May 11 04:28:00 2017 +0800

    f2fs: make sure f2fs_gc returns consistent errno
    
    By default, f2fs_gc returns -EINVAL in general error cases, e.g., no victim
    was selected. However, the default errno may be overwritten in two cases:
    gc_more and BG_GC -> FG_GC. We should return consistent errno in such cases.
    
    Signed-off-by: Weichao Guo <guoweichao@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 026522107ca3..965fbf5d0a2e 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -955,7 +955,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 {
 	int gc_type = sync ? FG_GC : BG_GC;
 	int sec_freed = 0;
-	int ret = -EINVAL;
+	int ret;
 	struct cp_control cpc;
 	unsigned int init_segno = segno;
 	struct gc_inode_list gc_list = {
@@ -965,8 +965,10 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 
 	cpc.reason = __get_cp_reason(sbi);
 gc_more:
-	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE)))
+	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE))) {
+		ret = -EINVAL;
 		goto stop;
+	}
 	if (unlikely(f2fs_cp_error(sbi))) {
 		ret = -EIO;
 		goto stop;
@@ -987,6 +989,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
 			gc_type = FG_GC;
 	}
 
+	ret = -EINVAL;
 	/* f2fs_balance_fs doesn't need to do BG_GC in critical path. */
 	if (gc_type == BG_GC && !background)
 		goto stop;

commit 279d6df20c94079d35e012f1602d40c42632e8f3
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Thu Apr 27 00:17:21 2017 +0800

    f2fs: release cp and dnode lock before IPU
    
    We don't need to rewrite the page under cp_rwsem and dnode locks.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c2a9ae8397d3..026522107ca3 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -717,6 +717,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 			.old_blkaddr = NULL_ADDR,
 			.page = page,
 			.encrypted_page = NULL,
+			.need_lock = true,
 		};
 		bool is_dirty = PageDirty(page);
 		int err;

commit e959c8f543e11dadf7f6923427fb3acb452a0de6
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Tue Apr 25 12:45:13 2017 +0000

    f2fs: lookup extent cache first under IPU scenario
    
    If a page is cold, NOT atomit written and need_ipu now, there is
    a high probability that IPU should be adapted. For IPU, we try to
    check extent tree to get the block index first, instead of reading
    the dnode page, where may lead to an useless dnode IO, since no need to
    update the dnode index for IPU.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 84db41ca27c1..c2a9ae8397d3 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -714,6 +714,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 			.type = DATA,
 			.op = REQ_OP_WRITE,
 			.op_flags = REQ_SYNC,
+			.old_blkaddr = NULL_ADDR,
 			.page = page,
 			.encrypted_page = NULL,
 		};

commit d579324998a39fa6e13edea2f06506840df9b729
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Apr 18 15:03:15 2017 -0700

    f2fs: assign allocation hint for warm/cold data
    
    This patch gives slower device region to warm/cold data area more eagerly.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 74a10b7675f5..84db41ca27c1 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1033,4 +1033,9 @@ void build_gc_manager(struct f2fs_sb_info *sbi)
 
 	sbi->fggc_threshold = div64_u64((main_count - ovp_count) *
 				BLKS_PER_SEC(sbi), (main_count - resv_count));
+
+	/* give warm/cold data area from slower device */
+	if (sbi->s_ndevs && sbi->segs_per_sec == 1)
+		SIT_I(sbi)->last_victim[ALLOC_NEXT] =
+				GET_SEGNO(sbi, FDEV(0).end_blk) + 1;
 }

commit e066b83c9b40f3a6951fb693ef0943fa1dfc40c2
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 13 15:17:00 2017 -0700

    f2fs: add ioctl to flush data from faster device to cold area
    
    This patch adds an ioctl to flush data in faster device to cold area. User can
    give device number and number of segments to move. It doesn't move it if there
    is only one device.
    
    The parameter looks like:
    
    struct f2fs_flush_device {
            u32 dev_num;            /* device number to flush */
            u32 segments;           /* # of segments to flush */
    };
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 9172112d6246..74a10b7675f5 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -84,7 +84,7 @@ static int gc_thread_func(void *data)
 		stat_inc_bggc_count(sbi);
 
 		/* if return value is not zero, no victim was selected */
-		if (f2fs_gc(sbi, test_opt(sbi, FORCE_FG_GC), true))
+		if (f2fs_gc(sbi, test_opt(sbi, FORCE_FG_GC), true, NULL_SEGNO))
 			wait_ms = gc_th->no_gc_sleep_time;
 
 		trace_f2fs_background_gc(sbi->sb, wait_ms,
@@ -176,7 +176,7 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 	if (type == CURSEG_HOT_DATA || IS_NODESEG(type))
 		p->offset = 0;
 	else
-		p->offset = sbi->last_victim[p->gc_mode];
+		p->offset = SIT_I(sbi)->last_victim[p->gc_mode];
 }
 
 static unsigned int get_max_cost(struct f2fs_sb_info *sbi,
@@ -295,6 +295,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 		unsigned int *result, int gc_type, int type, char alloc_mode)
 {
 	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
+	struct sit_info *sm = SIT_I(sbi);
 	struct victim_sel_policy p;
 	unsigned int secno, last_victim;
 	unsigned int last_segment = MAIN_SEGS(sbi);
@@ -308,10 +309,18 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	p.min_segno = NULL_SEGNO;
 	p.min_cost = get_max_cost(sbi, &p);
 
+	if (*result != NULL_SEGNO) {
+		if (IS_DATASEG(get_seg_entry(sbi, *result)->type) &&
+			get_valid_blocks(sbi, *result, false) &&
+			!sec_usage_check(sbi, GET_SEC_FROM_SEG(sbi, *result)))
+			p.min_segno = *result;
+		goto out;
+	}
+
 	if (p.max_search == 0)
 		goto out;
 
-	last_victim = sbi->last_victim[p.gc_mode];
+	last_victim = sm->last_victim[p.gc_mode];
 	if (p.alloc_mode == LFS && gc_type == FG_GC) {
 		p.min_segno = check_bg_victims(sbi);
 		if (p.min_segno != NULL_SEGNO)
@@ -324,9 +333,10 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 
 		segno = find_next_bit(p.dirty_segmap, last_segment, p.offset);
 		if (segno >= last_segment) {
-			if (sbi->last_victim[p.gc_mode]) {
-				last_segment = sbi->last_victim[p.gc_mode];
-				sbi->last_victim[p.gc_mode] = 0;
+			if (sm->last_victim[p.gc_mode]) {
+				last_segment =
+					sm->last_victim[p.gc_mode];
+				sm->last_victim[p.gc_mode] = 0;
 				p.offset = 0;
 				continue;
 			}
@@ -361,11 +371,11 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 		}
 next:
 		if (nsearched >= p.max_search) {
-			if (!sbi->last_victim[p.gc_mode] && segno <= last_victim)
-				sbi->last_victim[p.gc_mode] = last_victim + 1;
+			if (!sm->last_victim[p.gc_mode] && segno <= last_victim)
+				sm->last_victim[p.gc_mode] = last_victim + 1;
 			else
-				sbi->last_victim[p.gc_mode] = segno + 1;
-			sbi->last_victim[p.gc_mode] %= MAIN_SEGS(sbi);
+				sm->last_victim[p.gc_mode] = segno + 1;
+			sm->last_victim[p.gc_mode] %= MAIN_SEGS(sbi);
 			break;
 		}
 	}
@@ -912,7 +922,6 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 		 *   - mutex_lock(sentry_lock)     - change_curseg()
 		 *                                  - lock_page(sum_page)
 		 */
-
 		if (type == SUM_TYPE_NODE)
 			gc_node_segment(sbi, sum->entries, segno, gc_type);
 		else
@@ -939,13 +948,14 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	return sec_freed;
 }
 
-int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
+int f2fs_gc(struct f2fs_sb_info *sbi, bool sync,
+			bool background, unsigned int segno)
 {
-	unsigned int segno;
 	int gc_type = sync ? FG_GC : BG_GC;
 	int sec_freed = 0;
 	int ret = -EINVAL;
 	struct cp_control cpc;
+	unsigned int init_segno = segno;
 	struct gc_inode_list gc_list = {
 		.ilist = LIST_HEAD_INIT(gc_list.ilist),
 		.iroot = RADIX_TREE_INIT(GFP_NOFS),
@@ -990,13 +1000,17 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
 		sbi->cur_victim_sec = NULL_SEGNO;
 
 	if (!sync) {
-		if (has_not_enough_free_secs(sbi, sec_freed, 0))
+		if (has_not_enough_free_secs(sbi, sec_freed, 0)) {
+			segno = NULL_SEGNO;
 			goto gc_more;
+		}
 
 		if (gc_type == FG_GC)
 			ret = write_checkpoint(sbi, &cpc);
 	}
 stop:
+	SIT_I(sbi)->last_victim[ALLOC_NEXT] = 0;
+	SIT_I(sbi)->last_victim[FLUSH_DEVICE] = init_segno;
 	mutex_unlock(&sbi->gc_mutex);
 
 	put_gc_inode(&gc_list);

commit 8fd5a37efa0b036353df253e20dabe8773c039cd
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Apr 7 17:25:54 2017 -0700

    f2fs: avoid frequent checkpoint during f2fs_gc
    
    Now we're doing SSR aggressively more than ever before, so once we reach to
    the reserved_segment, f2fs_balance_fs will call f2fs_gc, which triggers
    checkpoint everytime. We actually must avoid that.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e2f9b2b12b74..9172112d6246 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -966,9 +966,11 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
 		 * threshold, we can make them free by checkpoint. Then, we
 		 * secure free segments which doesn't need fggc any more.
 		 */
-		ret = write_checkpoint(sbi, &cpc);
-		if (ret)
-			goto stop;
+		if (prefree_segments(sbi)) {
+			ret = write_checkpoint(sbi, &cpc);
+			if (ret)
+				goto stop;
+		}
 		if (has_not_enough_free_secs(sbi, 0, 0))
 			gc_type = FG_GC;
 	}

commit 4ddb1a4d4dc20642073b7d92400a67b67601fe6f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Apr 7 15:08:17 2017 -0700

    f2fs: clean up some macros in terms of GET_SEGNO
    
    This patch cleans several macros by introducing:
    - BLKS_PER_SEC
    - GET_SEC_FROM_SEG
    - GET_SEG_FROM_SEC
    - GET_ZONE_FROM_SEC
    - GET_ZONE_FROM_SEG
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 439887c3aaf4..e2f9b2b12b74 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -211,7 +211,7 @@ static unsigned int check_bg_victims(struct f2fs_sb_info *sbi)
 			continue;
 
 		clear_bit(secno, dirty_i->victim_secmap);
-		return secno * sbi->segs_per_sec;
+		return GET_SEG_FROM_SEC(sbi, secno);
 	}
 	return NULL_SEGNO;
 }
@@ -219,8 +219,8 @@ static unsigned int check_bg_victims(struct f2fs_sb_info *sbi)
 static unsigned int get_cb_cost(struct f2fs_sb_info *sbi, unsigned int segno)
 {
 	struct sit_info *sit_i = SIT_I(sbi);
-	unsigned int secno = GET_SECNO(sbi, segno);
-	unsigned int start = secno * sbi->segs_per_sec;
+	unsigned int secno = GET_SEC_FROM_SEG(sbi, segno);
+	unsigned int start = GET_SEG_FROM_SEC(sbi, secno);
 	unsigned long long mtime = 0;
 	unsigned int vblocks;
 	unsigned char age = 0;
@@ -343,7 +343,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 			nsearched++;
 		}
 
-		secno = GET_SECNO(sbi, segno);
+		secno = GET_SEC_FROM_SEG(sbi, segno);
 
 		if (sec_usage_check(sbi, secno))
 			goto next;
@@ -372,7 +372,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	if (p.min_segno != NULL_SEGNO) {
 got_it:
 		if (p.alloc_mode == LFS) {
-			secno = GET_SECNO(sbi, p.min_segno);
+			secno = GET_SEC_FROM_SEG(sbi, p.min_segno);
 			if (gc_type == FG_GC)
 				sbi->cur_victim_sec = secno;
 			else
@@ -1006,7 +1006,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
 
 void build_gc_manager(struct f2fs_sb_info *sbi)
 {
-	u64 main_count, resv_count, ovp_count, blocks_per_sec;
+	u64 main_count, resv_count, ovp_count;
 
 	DIRTY_I(sbi)->v_ops = &default_v_ops;
 
@@ -1014,8 +1014,7 @@ void build_gc_manager(struct f2fs_sb_info *sbi)
 	main_count = SM_I(sbi)->main_segments << sbi->log_blocks_per_seg;
 	resv_count = SM_I(sbi)->reserved_segments << sbi->log_blocks_per_seg;
 	ovp_count = SM_I(sbi)->ovp_segments << sbi->log_blocks_per_seg;
-	blocks_per_sec = sbi->blocks_per_seg * sbi->segs_per_sec;
 
-	sbi->fggc_threshold = div64_u64((main_count - ovp_count) * blocks_per_sec,
-					(main_count - resv_count));
+	sbi->fggc_threshold = div64_u64((main_count - ovp_count) *
+				BLKS_PER_SEC(sbi), (main_count - resv_count));
 }

commit 302bd34882b1e20797f08cc13ef060ec972d0acb
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Apr 7 14:33:22 2017 -0700

    f2fs: clean up get_valid_blocks with consistent parameter
    
    This patch cleans up get_valid_blocks, which has no functional change.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c52656ccbde5..439887c3aaf4 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -229,7 +229,7 @@ static unsigned int get_cb_cost(struct f2fs_sb_info *sbi, unsigned int segno)
 
 	for (i = 0; i < sbi->segs_per_sec; i++)
 		mtime += get_seg_entry(sbi, start + i)->mtime;
-	vblocks = get_valid_blocks(sbi, segno, sbi->segs_per_sec);
+	vblocks = get_valid_blocks(sbi, segno, true);
 
 	mtime = div_u64(mtime, sbi->segs_per_sec);
 	vblocks = div_u64(vblocks, sbi->segs_per_sec);
@@ -252,7 +252,7 @@ static unsigned int get_greedy_cost(struct f2fs_sb_info *sbi,
 						unsigned int segno)
 {
 	unsigned int valid_blocks =
-			get_valid_blocks(sbi, segno, sbi->segs_per_sec);
+			get_valid_blocks(sbi, segno, true);
 
 	return IS_DATASEG(get_seg_entry(sbi, segno)->type) ?
 				valid_blocks * 2 : valid_blocks;
@@ -897,7 +897,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 					GET_SUM_BLOCK(sbi, segno));
 		f2fs_put_page(sum_page, 0);
 
-		if (get_valid_blocks(sbi, segno, 1) == 0 ||
+		if (get_valid_blocks(sbi, segno, false) == 0 ||
 				!PageUptodate(sum_page) ||
 				unlikely(f2fs_cp_error(sbi)))
 			goto next;
@@ -931,7 +931,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	blk_finish_plug(&plug);
 
 	if (gc_type == FG_GC &&
-		get_valid_blocks(sbi, start_segno, sbi->segs_per_sec) == 0)
+		get_valid_blocks(sbi, start_segno, true) == 0)
 		sec_freed = 1;
 
 	stat_inc_call_count(sbi->stat_info);

commit c13ff37e359bb3eacf4e1760dcea8d9760aa7459
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Mar 21 10:59:50 2017 -0400

    f2fs: relax node version check for victim data in gc
    
    - has_not_enough_free_secs
    node_secs: 0  dent_secs: 0  freed:0  free_segments:103  reserved:104
    
              - f2fs_gc
                 - get_victim_by_default
    alloc_mode 0, gc_mode 1, max_search 2672, offset 4654, ofs_unit 1
    
                    - do_garbage_collect
    start_segno 3976, end_segno 3977   type 0
    
                      - is_alive
    nid 22797, blkaddr 2131882, ofs_in_node 0, version 0x8/0x0
    
                       - gc_data_segment 766, segno 3976, block 512/426 not alive
    
    So, this patch fixes subtle corrupted case where node version does not match
    to summary version which results in infinite loop by gc.
    
    Reported-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 63fefef04184..c52656ccbde5 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -555,8 +555,10 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	get_node_info(sbi, nid, dni);
 
 	if (sum->version != dni->version) {
-		f2fs_put_page(node_page, 1);
-		return false;
+		f2fs_msg(sbi->sb, KERN_WARNING,
+				"%s: valid data with mismatched node version.",
+				__func__);
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
 	}
 
 	*nofs = ofs_of_node(node_page);

commit 7a20b8a61eff81bdb7097a578752a74860e9d142
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Mar 24 20:41:45 2017 -0400

    f2fs: allocate node and hot data in the beginning of partition
    
    In order to give more spatial locality, this patch changes the block allocation
    policy which assigns beginning of partition for small and hot data/node blocks.
    In order to do this, we set noheap allocation by default and introduce another
    mount option, heap, to reset it back.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 704bea678d37..63fefef04184 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -172,7 +172,11 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 	if (gc_type != FG_GC && p->max_search > sbi->max_victim_search)
 		p->max_search = sbi->max_victim_search;
 
-	p->offset = sbi->last_victim[p->gc_mode];
+	/* let's select beginning hot/small space first */
+	if (type == CURSEG_HOT_DATA || IS_NODESEG(type))
+		p->offset = 0;
+	else
+		p->offset = sbi->last_victim[p->gc_mode];
 }
 
 static unsigned int get_max_cost(struct f2fs_sb_info *sbi,

commit c541a51b8ce81d003b02ed67ad3604a2e6220e3e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Mar 25 00:03:02 2017 -0700

    f2fs: fix wrong max cost initialization
    
    This patch fixes missing increased max cost caused by a patch that we increased
    cose of data segments in greedy algorithm.
    
    Cc: <stable@vger.kernel.org> # v4.10+
    Fixes: b9cd20619 "f2fs: node segment is prior to data segment selected victim"
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 939be88a8833..704bea678d37 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -182,7 +182,7 @@ static unsigned int get_max_cost(struct f2fs_sb_info *sbi,
 	if (p->alloc_mode == SSR)
 		return sbi->blocks_per_seg;
 	if (p->gc_mode == GC_GREEDY)
-		return sbi->blocks_per_seg * p->ofs_unit;
+		return 2 * sbi->blocks_per_seg * p->ofs_unit;
 	else if (p->gc_mode == GC_CB)
 		return UINT_MAX;
 	else /* No other gc_mode */

commit 9897159a7b1aa98ec0bc8fc053ab822e6634e7fa
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Mar 21 20:09:45 2017 +0800

    f2fs: fix recording invalid last_victim
    
    When doing garbage collection, we try to record segment offset which
    locates at next one of last victim, using it as the start offset in
    next searching.
    
    But in some corner cases, recorded offset may cross the end of main
    segment area, it will cause incorrectly searching in dirty_segmap
    bitmap. This patch adds modular operation to avoid this issue.
    
    Reported-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 418fd9881646..939be88a8833 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -361,6 +361,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 				sbi->last_victim[p.gc_mode] = last_victim + 1;
 			else
 				sbi->last_victim[p.gc_mode] = segno + 1;
+			sbi->last_victim[p.gc_mode] %= MAIN_SEGS(sbi);
 			break;
 		}
 	}

commit 37e79cd31c2c4dcd7e149958ca3cae9d180c63c7
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Mon Feb 27 13:02:59 2017 +0000

    f2fs: fix a plint compile warning
    
    fix such pclint warning:
    ...
    Loss of precision (arg. no. 2) (unsigned long long to unsigned int))
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 023f043edefd..418fd9881646 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1009,6 +1009,6 @@ void build_gc_manager(struct f2fs_sb_info *sbi)
 	ovp_count = SM_I(sbi)->ovp_segments << sbi->log_blocks_per_seg;
 	blocks_per_sec = sbi->blocks_per_seg * sbi->segs_per_sec;
 
-	sbi->fggc_threshold = div_u64((main_count - ovp_count) * blocks_per_sec,
+	sbi->fggc_threshold = div64_u64((main_count - ovp_count) * blocks_per_sec,
 					(main_count - resv_count));
 }

commit 19f4e688f89a9ce07b86d06d3df23c1cd877ab4e
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Sat Feb 25 03:57:38 2017 +0000

    f2fs: avoid bggc->fggc when enough free segments are avaliable after cp
    
    We use has_not_enough_free_secs to check if there are enough free segments,
    
            (free_sections(sbi) + freed) <=
                    (node_secs + 2 * dent_secs + imeta_secs +
                                     reserved_sections(sbi) + needed);
    
    Under scenario with large number of dirty nodes, these nodes would be flushed
    during cp, as a result, right side of the inequality would be decreased, while
    left side stays unchanged if these nodes are flushed in SSR way, which means
    there are enough free segments after this cp.
    
    For this case, we just do a bggc instead of fggc.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 8be5144da8e6..023f043edefd 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -953,21 +953,22 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
 		goto stop;
 	}
 
-	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, sec_freed, 0)) {
-		gc_type = FG_GC;
+	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0)) {
 		/*
-		 * If there is no victim and no prefree segment but still not
-		 * enough free sections, we should flush dent/node blocks and do
-		 * garbage collections.
+		 * For example, if there are many prefree_segments below given
+		 * threshold, we can make them free by checkpoint. Then, we
+		 * secure free segments which doesn't need fggc any more.
 		 */
 		ret = write_checkpoint(sbi, &cpc);
 		if (ret)
 			goto stop;
-	} else if (gc_type == BG_GC && !background) {
-		/* f2fs_balance_fs doesn't need to do BG_GC in critical path. */
-		goto stop;
+		if (has_not_enough_free_secs(sbi, 0, 0))
+			gc_type = FG_GC;
 	}
 
+	/* f2fs_balance_fs doesn't need to do BG_GC in critical path. */
+	if (gc_type == BG_GC && !background)
+		goto stop;
 	if (!__get_victim(sbi, &segno, gc_type))
 		goto stop;
 	ret = 0;

commit 55523519bc7227e651fd4febeb3aafdd22b8af1c
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Feb 25 11:08:28 2017 +0800

    f2fs: show simple call stack in fault injection message
    
    Previously kernel message can show that in which function we do the
    injection, but unfortunately, most of the caller are the same, for
    tracking more information of injection path, it needs to show upper
    caller's name. This patch supports that ability.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 6c996e39b59a..8be5144da8e6 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -48,8 +48,10 @@ static int gc_thread_func(void *data)
 		}
 
 #ifdef CONFIG_F2FS_FAULT_INJECTION
-		if (time_to_inject(sbi, FAULT_CHECKPOINT))
+		if (time_to_inject(sbi, FAULT_CHECKPOINT)) {
+			f2fs_show_injection_info(FAULT_CHECKPOINT);
 			f2fs_stop_checkpoint(sbi, false);
+		}
 #endif
 
 		/*

commit 77190e1f313f5ef18a3742aa4c5b790456a5825b
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Tue Feb 21 20:43:48 2017 +0800

    f2fs: remove unnecessary condition check for write_checkpoint in f2fs_gc
    
    Since has_not_enough_free_secs(sbi, 0, 0) must be true if has_not_enough_
    free_secs(sbi, sec_freed, 0) is true, write_checkpoint is sure to execute in
    both conditions.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 11416c7cb705..6c996e39b59a 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -958,15 +958,9 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
 		 * enough free sections, we should flush dent/node blocks and do
 		 * garbage collections.
 		 */
-		if (dirty_segments(sbi) || prefree_segments(sbi)) {
-			ret = write_checkpoint(sbi, &cpc);
-			if (ret)
-				goto stop;
-		} else if (has_not_enough_free_secs(sbi, 0, 0)) {
-			ret = write_checkpoint(sbi, &cpc);
-			if (ret)
-				goto stop;
-		}
+		ret = write_checkpoint(sbi, &cpc);
+		if (ret)
+			goto stop;
 	} else if (gc_type == BG_GC && !background) {
 		/* f2fs_balance_fs doesn't need to do BG_GC in critical path. */
 		goto stop;

commit b9cd20619e359d199b755543474c3d853c8e3415
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Wed Feb 22 10:28:59 2017 +0000

    f2fs: node segment is prior to data segment selected victim
    
    As data segment gc may lead dnode dirty, so the greedy cost for data segment
    should be valid blocks * 2, that is data segment is prior to node segment.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e93aecb0138b..11416c7cb705 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -242,6 +242,16 @@ static unsigned int get_cb_cost(struct f2fs_sb_info *sbi, unsigned int segno)
 	return UINT_MAX - ((100 * (100 - u) * age) / (100 + u));
 }
 
+static unsigned int get_greedy_cost(struct f2fs_sb_info *sbi,
+						unsigned int segno)
+{
+	unsigned int valid_blocks =
+			get_valid_blocks(sbi, segno, sbi->segs_per_sec);
+
+	return IS_DATASEG(get_seg_entry(sbi, segno)->type) ?
+				valid_blocks * 2 : valid_blocks;
+}
+
 static inline unsigned int get_gc_cost(struct f2fs_sb_info *sbi,
 			unsigned int segno, struct victim_sel_policy *p)
 {
@@ -250,7 +260,7 @@ static inline unsigned int get_gc_cost(struct f2fs_sb_info *sbi,
 
 	/* alloc_mode == LFS */
 	if (p->gc_mode == GC_GREEDY)
-		return get_valid_blocks(sbi, segno, sbi->segs_per_sec);
+		return get_greedy_cost(sbi, segno);
 	else
 		return get_cb_cost(sbi, segno);
 }

commit e93b9865251a0503d83fd570e7d5a7c8bc351715
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Thu Feb 16 12:34:31 2017 +0000

    f2fs: add ovp valid_blocks check for bg gc victim to fg_gc
    
    For foreground gc, greedy algorithm should be adapted, which makes
    this formula work well:
    
            (2 * (100 / config.overprovision + 1) + 6)
    
    But currently, we fg_gc have a prior to select bg_gc victim segments to gc
    first, these victims are selected by cost-benefit algorithm, we can't guarantee
    such segments have the small valid blocks, which may destroy the f2fs rule, on
    the worstest case, would consume all the free segments.
    
    This patch fix this by add a filter in check_bg_victims, if segment's has # of
    valid blocks over overprovision ratio, skip such segments.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 5ee258ebf6ca..e93aecb0138b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -166,7 +166,8 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 		p->ofs_unit = sbi->segs_per_sec;
 	}
 
-	if (p->max_search > sbi->max_victim_search)
+	/* we need to check every dirty segments in the FG_GC case */
+	if (gc_type != FG_GC && p->max_search > sbi->max_victim_search)
 		p->max_search = sbi->max_victim_search;
 
 	p->offset = sbi->last_victim[p->gc_mode];
@@ -199,6 +200,10 @@ static unsigned int check_bg_victims(struct f2fs_sb_info *sbi)
 	for_each_set_bit(secno, dirty_i->victim_secmap, MAIN_SECS(sbi)) {
 		if (sec_usage_check(sbi, secno))
 			continue;
+
+		if (no_fggc_candidate(sbi, secno))
+			continue;
+
 		clear_bit(secno, dirty_i->victim_secmap);
 		return secno * sbi->segs_per_sec;
 	}
@@ -322,13 +327,15 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 			nsearched++;
 		}
 
-
 		secno = GET_SECNO(sbi, segno);
 
 		if (sec_usage_check(sbi, secno))
 			goto next;
 		if (gc_type == BG_GC && test_bit(secno, dirty_i->victim_secmap))
 			goto next;
+		if (gc_type == FG_GC && p.alloc_mode == LFS &&
+					no_fggc_candidate(sbi, secno))
+			goto next;
 
 		cost = get_gc_cost(sbi, segno, &p);
 
@@ -985,5 +992,16 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
 
 void build_gc_manager(struct f2fs_sb_info *sbi)
 {
+	u64 main_count, resv_count, ovp_count, blocks_per_sec;
+
 	DIRTY_I(sbi)->v_ops = &default_v_ops;
+
+	/* threshold of # of valid blocks in a section for victims of FG_GC */
+	main_count = SM_I(sbi)->main_segments << sbi->log_blocks_per_seg;
+	resv_count = SM_I(sbi)->reserved_segments << sbi->log_blocks_per_seg;
+	ovp_count = SM_I(sbi)->ovp_segments << sbi->log_blocks_per_seg;
+	blocks_per_sec = sbi->blocks_per_seg * sbi->segs_per_sec;
+
+	sbi->fggc_threshold = div_u64((main_count - ovp_count) * blocks_per_sec,
+					(main_count - resv_count));
 }

commit 05eeb118a0e9fdc675a1f9db05039693da3ea25e
Author: Yunlei He <heyunlei@huawei.com>
Date:   Fri Feb 17 17:16:38 2017 +0800

    f2fs: replace __get_victim by dirty_segments in FG_GC
    
    In FG_GC process, it will search victim section twice. This will
    cause some dirty section with less valid blocks skip garbage
    collection.
    
    section # 26425 : valid blocks # 3
    142.037567: get_victim_by_default: victim 26425 : valid blocks # 3
    142.037585: f2fs_get_victim: dev = (259,30), type = No TYPE, policy = (Foreground GC, LFS-mode, Greedy), victim = 26425 ofs_unit = 1, pre_victim_secno = 26425, prefree = 0, free = 244
    142.039494: f2fs_get_victim: dev = (259,30), type = Hot DATA, policy = (Background GC, SSR-mode, Greedy), victim = 19022 ofs_unit = 1, pre_victim_secno = 26425, prefree = 0, free = 24
    142.070247: new_curseg: Debug: alloc new segment 26746
    142.244341: f2fs_get_victim: dev = (259,30), type = No TYPE, policy = (Foreground GC, LFS-mode, Greedy), victim = 26054 ofs_unit = 1, pre_victim_secno = 26054, prefree = 0, free = 243
    142.254475: do_garbage_collect: Debug: FG_GC, seg_freed = 1
    142.293131: f2fs_get_victim: dev = (259,30), type = Warm DATA, policy = (Background GC, SSR-mode, Greedy), victim = 23466 ofs_unit = 1, pre_victim_secno = -1, prefree = 0, free = 244
    142.319001: f2fs_get_victim: dev = (259,30), type = Warm DATA, policy = (Background GC, SSR-mode, Greedy), victim = 23467 ofs_unit = 1, pre_victim_secno = -1, prefree = 0, free = 244
    142.368879: get_victim_by_default: victim 26425 : valid blocks # 3
    142.368894: f2fs_get_victim: dev = (259,30), type = No TYPE, policy = (Foreground GC, LFS-mode, Greedy), victim = 26425 ofs_unit = 1, pre_victim_secno = 26425, prefree = 0, free = 244
    142.378127: f2fs_get_victim: dev = (259,30), type = Hot DATA, policy = (Background GC, SSR-mode, Greedy), victim = 19612 ofs_unit = 1, pre_victim_secno = 26425, prefree = 0, free = 24
    142.416917: new_curseg: Debug: alloc new segment 26054
    142.656794: f2fs_get_victim: dev = (259,30), type = No TYPE, policy = (Foreground GC, LFS-mode, Greedy), victim = 25404 ofs_unit = 1, pre_victim_secno = 25404, prefree = 0, free = 243
    142.662139: do_garbage_collect: Debug: FG_GC, seg_freed = 1
    142.684159: new_curseg: Debug: alloc new segment 25197
    142.685059: get_victim_by_default: victim 26425 : valid blocks # 3
    142.685079: f2fs_get_victim: dev = (259,30), type = No TYPE, policy = (Foreground GC, LFS-mode, Greedy), victim = 26425 ofs_unit = 1, pre_victim_secno = 26425, prefree = 0, free = 243
    142.701427: f2fs_get_victim: dev = (259,30), type = No TYPE, policy = (Foreground GC, LFS-mode, Greedy), victim = 26238 ofs_unit = 1, pre_victim_secno = 26238, prefree = 0, free = 243
    142.707105: do_garbage_collect: Debug: FG_GC, seg_freed = 1
    142.802444: f2fs_get_victim: dev = (259,30), type = Warm DATA, policy = (Background GC, SSR-mode, Greedy), victim = 23473 ofs_unit = 1, pre_victim_secno = -1, prefree = 0, free = 244
    142.804422: get_victim_by_default: victim 26425 : valid blocks # 3
    142.804443: f2fs_get_victim: dev = (259,30), type = No TYPE, policy = (Foreground GC, LFS-mode, Greedy), victim = 26425 ofs_unit = 1, pre_victim_secno = 26425, prefree = 0, free = 244
    142.851567: f2fs_get_victim: dev = (259,30), type = Hot DATA, policy = (Background GC, SSR-mode, Greedy), victim = 19092 ofs_unit = 1, pre_victim_secno = 26425, prefree = 0, free = 24
    142.865014: new_curseg: Debug: alloc new segment 26238
    143.082245: f2fs_get_victim: dev = (259,30), type = No TYPE, policy = (Foreground GC, LFS-mode, Greedy), victim = 26307 ofs_unit = 1, pre_victim_secno = 26307, prefree = 0, free = 244
    143.088252: do_garbage_collect: Debug: FG_GC, seg_freed = 1
    143.128307: new_curseg: Debug: alloc new segment 25404
    143.181846: get_victim_by_default: victim 26425 : valid blocks # 3
    143.181872: f2fs_get_victim: dev = (259,30), type = No TYPE, policy = (Foreground GC, LFS-mode, Greedy), victim = 26425 ofs_unit = 1, pre_victim_secno = 26425, prefree = 0, free = 244
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 88e5e7b10ab6..5ee258ebf6ca 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -927,8 +927,6 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
 
 	cpc.reason = __get_cp_reason(sbi);
 gc_more:
-	segno = NULL_SEGNO;
-
 	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE)))
 		goto stop;
 	if (unlikely(f2fs_cp_error(sbi))) {
@@ -943,12 +941,10 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
 		 * enough free sections, we should flush dent/node blocks and do
 		 * garbage collections.
 		 */
-		if (__get_victim(sbi, &segno, gc_type) ||
-						prefree_segments(sbi)) {
+		if (dirty_segments(sbi) || prefree_segments(sbi)) {
 			ret = write_checkpoint(sbi, &cpc);
 			if (ret)
 				goto stop;
-			segno = NULL_SEGNO;
 		} else if (has_not_enough_free_secs(sbi, 0, 0)) {
 			ret = write_checkpoint(sbi, &cpc);
 			if (ret)
@@ -959,7 +955,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
 		goto stop;
 	}
 
-	if (segno == NULL_SEGNO && !__get_victim(sbi, &segno, gc_type))
+	if (!__get_victim(sbi, &segno, gc_type))
 		goto stop;
 	ret = 0;
 

commit 5fe457430e554a2f5188f13c1a2e36ad845640c5
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Jan 7 18:50:26 2017 +0800

    f2fs: introduce FI_ATOMIC_COMMIT
    
    This patch introduces a new flag to indicate inode status of doing atomic
    write committing, so that, we can keep atomic write status for inode
    during atomic committing, then we can skip GCing pages of atomic write inode,
    that avoids random GCed datas being mixed with current transaction, so
    isolation of transaction can be kept.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 88bfc3dff496..88e5e7b10ab6 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -569,6 +569,9 @@ static void move_encrypted_block(struct inode *inode, block_t bidx,
 	if (!check_valid_map(F2FS_I_SB(inode), segno, off))
 		goto out;
 
+	if (f2fs_is_atomic_file(inode))
+		goto out;
+
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, bidx, LOOKUP_NODE);
 	if (err)
@@ -661,6 +664,9 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 	if (!check_valid_map(F2FS_I_SB(inode), segno, off))
 		goto out;
 
+	if (f2fs_is_atomic_file(inode))
+		goto out;
+
 	if (gc_type == BG_GC) {
 		if (PageWriteback(page))
 			goto out;

commit 09cb6464fe5e7fcd5177911429badd139c4481b7
Merge: 19d37ce2a715 c0ed4405a99e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 14 09:07:36 2016 -0800

    Merge tag 'for-f2fs-4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "This patch series contains several performance tuning patches
      regarding to the IO submission flow, in addition to supporting new
      features such as a ZBC-base drive and multiple devices.
    
      It also includes some major bug fixes such as:
       - checkpoint version control
       - fdatasync-related roll-forward recovery routine
       - memory boundary or null-pointer access in corner cases
       - missing error cases
    
      It has various minor clean-up patches as well"
    
    * tag 'for-f2fs-4.10' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (66 commits)
      f2fs: fix a missing size change in f2fs_setattr
      f2fs: fix to access nullified flush_cmd_control pointer
      f2fs: free meta pages if sanity check for ckpt is failed
      f2fs: detect wrong layout
      f2fs: call sync_fs when f2fs is idle
      Revert "f2fs: use percpu_counter for # of dirty pages in inode"
      f2fs: return AOP_WRITEPAGE_ACTIVATE for writepage
      f2fs: do not activate auto_recovery for fallocated i_size
      f2fs: fix to determine start_cp_addr by sbi->cur_cp_pack
      f2fs: fix 32-bit build
      f2fs: set ->owner for debugfs status file's file_operations
      f2fs: fix incorrect free inode count in ->statfs
      f2fs: drop duplicate header timer.h
      f2fs: fix wrong AUTO_RECOVER condition
      f2fs: do not recover i_size if it's valid
      f2fs: fix fdatasync
      f2fs: fix to account total free nid correctly
      f2fs: fix an infinite loop when flush nodes in cp
      f2fs: don't wait writeback for datas during checkpoint
      f2fs: fix wrong written_valid_blocks counting
      ...

commit 36869cb93d36269f34800b3384ba7991060a69cf
Merge: 9439b3710df6 7cd54aa84389
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 13 10:19:16 2016 -0800

    Merge branch 'for-4.10/block' of git://git.kernel.dk/linux-block
    
    Pull block layer updates from Jens Axboe:
     "This is the main block pull request this series. Contrary to previous
      release, I've kept the core and driver changes in the same branch. We
      always ended up having dependencies between the two for obvious
      reasons, so makes more sense to keep them together. That said, I'll
      probably try and keep more topical branches going forward, especially
      for cycles that end up being as busy as this one.
    
      The major parts of this pull request is:
    
       - Improved support for O_DIRECT on block devices, with a small
         private implementation instead of using the pig that is
         fs/direct-io.c. From Christoph.
    
       - Request completion tracking in a scalable fashion. This is utilized
         by two components in this pull, the new hybrid polling and the
         writeback queue throttling code.
    
       - Improved support for polling with O_DIRECT, adding a hybrid mode
         that combines pure polling with an initial sleep. From me.
    
       - Support for automatic throttling of writeback queues on the block
         side. This uses feedback from the device completion latencies to
         scale the queue on the block side up or down. From me.
    
       - Support from SMR drives in the block layer and for SD. From Hannes
         and Shaun.
    
       - Multi-connection support for nbd. From Josef.
    
       - Cleanup of request and bio flags, so we have a clear split between
         which are bio (or rq) private, and which ones are shared. From
         Christoph.
    
       - A set of patches from Bart, that improve how we handle queue
         stopping and starting in blk-mq.
    
       - Support for WRITE_ZEROES from Chaitanya.
    
       - Lightnvm updates from Javier/Matias.
    
       - Supoort for FC for the nvme-over-fabrics code. From James Smart.
    
       - A bunch of fixes from a whole slew of people, too many to name
         here"
    
    * 'for-4.10/block' of git://git.kernel.dk/linux-block: (182 commits)
      blk-stat: fix a few cases of missing batch flushing
      blk-flush: run the queue when inserting blk-mq flush
      elevator: make the rqhash helpers exported
      blk-mq: abstract out blk_mq_dispatch_rq_list() helper
      blk-mq: add blk_mq_start_stopped_hw_queue()
      block: improve handling of the magic discard payload
      blk-wbt: don't throttle discard or write zeroes
      nbd: use dev_err_ratelimited in io path
      nbd: reset the setup task for NBD_CLEAR_SOCK
      nvme-fabrics: Add FC LLDD loopback driver to test FC-NVME
      nvme-fabrics: Add target support for FC transport
      nvme-fabrics: Add host support for FC transport
      nvme-fabrics: Add FC transport LLDD api definitions
      nvme-fabrics: Add FC transport FC-NVME definitions
      nvme-fabrics: Add FC transport error codes to nvme.h
      Add type 0x28 NVME type code to scsi fc headers
      nvme-fabrics: patch target code in prep for FC transport support
      nvme-fabrics: set sqe.command_id in core not transports
      parser: add u64 number parser
      nvme-rdma: align to generic ib_event logging helper
      ...

commit 36951b38d13ac7cce9fcf89e0e01c22ed0d05688
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Nov 16 10:41:20 2016 +0800

    f2fs: don't wait writeback for datas during checkpoint
    
    Normally, while committing checkpoint, we will wait on all pages to be
    writebacked no matter the page is data or metadata, so in scenario where
    there are lots of data IO being submitted with metadata, we may suffer
    long latency for waiting writeback during checkpoint.
    
    Indeed, we only care about persistence for pages with metadata, but not
    pages with data, as file system consistent are only related to metadate,
    so in order to avoid encountering long latency in above scenario, let's
    recognize and reference metadata in submitted IOs, wait writeback only
    for metadatas.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 36f6cfc613d3..5c2fe68be549 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -693,8 +693,6 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
 			congestion_wait(BLK_RW_ASYNC, HZ/50);
 			goto retry;
 		}
-
-		clear_cold_data(page);
 	}
 out:
 	f2fs_put_page(page, 1);

commit 7702bdbe505a22380dd958e2ee35124c7c414806
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Nov 14 17:38:35 2016 -0800

    f2fs: avoid BG_GC in f2fs_balance_fs
    
    If many threads hit has_not_enough_free_secs() in f2fs_balance_fs() at the same
    time, all the threads would do FG_GC or BG_GC.
    In this critical path, we totally don't need to do BG_GC at all.
    Let's avoid that.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index a7b2c13404f8..36f6cfc613d3 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -82,7 +82,7 @@ static int gc_thread_func(void *data)
 		stat_inc_bggc_count(sbi);
 
 		/* if return value is not zero, no victim was selected */
-		if (f2fs_gc(sbi, test_opt(sbi, FORCE_FG_GC)))
+		if (f2fs_gc(sbi, test_opt(sbi, FORCE_FG_GC), true))
 			wait_ms = gc_th->no_gc_sleep_time;
 
 		trace_f2fs_background_gc(sbi->sb, wait_ms,
@@ -909,7 +909,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	return sec_freed;
 }
 
-int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
+int f2fs_gc(struct f2fs_sb_info *sbi, bool sync, bool background)
 {
 	unsigned int segno;
 	int gc_type = sync ? FG_GC : BG_GC;
@@ -950,6 +950,9 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 			if (ret)
 				goto stop;
 		}
+	} else if (gc_type == BG_GC && !background) {
+		/* f2fs_balance_fs doesn't need to do BG_GC in critical path. */
+		goto stop;
 	}
 
 	if (segno == NULL_SEGNO && !__get_victim(sbi, &segno, gc_type))

commit 206147112860418fa9363cf94ec2c172bdf4313a
Author: Yunlei He <heyunlei@huawei.com>
Date:   Mon Nov 7 21:22:31 2016 +0800

    f2fs: return directly if block has been removed from the victim
    
    If one block has been to written to a new place, just return
    in move data process. This patch check it again with holding
    page lock.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 9c189171b89c..a7b2c13404f8 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -544,7 +544,8 @@ static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	return true;
 }
 
-static void move_encrypted_block(struct inode *inode, block_t bidx)
+static void move_encrypted_block(struct inode *inode, block_t bidx,
+							unsigned int segno, int off)
 {
 	struct f2fs_io_info fio = {
 		.sbi = F2FS_I_SB(inode),
@@ -565,6 +566,9 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	if (!page)
 		return;
 
+	if (!check_valid_map(F2FS_I_SB(inode), segno, off))
+		goto out;
+
 	set_new_dnode(&dn, inode, NULL, NULL, 0);
 	err = get_dnode_of_data(&dn, bidx, LOOKUP_NODE);
 	if (err)
@@ -645,7 +649,8 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	f2fs_put_page(page, 1);
 }
 
-static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
+static void move_data_page(struct inode *inode, block_t bidx, int gc_type,
+							unsigned int segno, int off)
 {
 	struct page *page;
 
@@ -653,6 +658,9 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 	if (IS_ERR(page))
 		return;
 
+	if (!check_valid_map(F2FS_I_SB(inode), segno, off))
+		goto out;
+
 	if (gc_type == BG_GC) {
 		if (PageWriteback(page))
 			goto out;
@@ -796,9 +804,9 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			start_bidx = start_bidx_of_node(nofs, inode)
 								+ ofs_in_node;
 			if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
-				move_encrypted_block(inode, start_bidx);
+				move_encrypted_block(inode, start_bidx, segno, off);
 			else
-				move_data_page(inode, start_bidx, gc_type);
+				move_data_page(inode, start_bidx, gc_type, segno, off);
 
 			if (locked) {
 				up_write(&fi->dio_rwsem[WRITE]);

commit 933439c8f3474e329709b715b43b0b8168bbecf8
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Oct 11 22:57:01 2016 +0800

    f2fs: give a chance to detach from dirty list
    
    If there is no dirty pages in inode, we should give a chance to detach
    the inode from global dirty list, otherwise it needs to call another
    unnecessary .writepages for detaching.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 6f14ee923acd..9c189171b89c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -673,8 +673,10 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 retry:
 		set_page_dirty(page);
 		f2fs_wait_on_page_writeback(page, DATA, true);
-		if (clear_page_dirty_for_io(page))
+		if (clear_page_dirty_for_io(page)) {
 			inode_dec_dirty_pages(inode);
+			remove_dirty_inode(inode);
+		}
 
 		set_cold_data(page);
 

commit 70fd76140a6cb63262bd47b68d57b42e889c10ee
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Nov 1 07:40:10 2016 -0600

    block,fs: use REQ_* flags directly
    
    Remove the WRITE_* and READ_SYNC wrappers, and just use the flags
    directly.  Where applicable this also drops usage of the
    bio_set_op_attrs wrapper.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 93985c64d8a8..9eb11b2244ea 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -550,7 +550,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 		.sbi = F2FS_I_SB(inode),
 		.type = DATA,
 		.op = REQ_OP_READ,
-		.op_flags = READ_SYNC,
+		.op_flags = 0,
 		.encrypted_page = NULL,
 	};
 	struct dnode_of_data dn;
@@ -625,7 +625,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	f2fs_wait_on_page_writeback(dn.node_page, NODE, true);
 
 	fio.op = REQ_OP_WRITE;
-	fio.op_flags = WRITE_SYNC;
+	fio.op_flags = REQ_SYNC;
 	fio.new_blkaddr = newaddr;
 	f2fs_submit_page_mbio(&fio);
 
@@ -663,7 +663,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 			.sbi = F2FS_I_SB(inode),
 			.type = DATA,
 			.op = REQ_OP_WRITE,
-			.op_flags = WRITE_SYNC,
+			.op_flags = REQ_SYNC,
 			.page = page,
 			.encrypted_page = NULL,
 		};

commit de0dcc40f6e24d6bac6b60e36eac4659bbbd3f00
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Oct 12 13:38:41 2016 -0700

    f2fs: fix wrong sum_page pointer in f2fs_gc
    
    This patch fixes using a wrong pointer for sum_page in f2fs_gc.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 93985c64d8a8..6f14ee923acd 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -852,16 +852,16 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 
 	for (segno = start_segno; segno < end_segno; segno++) {
 
-		if (get_valid_blocks(sbi, segno, 1) == 0 ||
-					unlikely(f2fs_cp_error(sbi)))
-			goto next;
-
 		/* find segment summary of victim */
 		sum_page = find_get_page(META_MAPPING(sbi),
 					GET_SUM_BLOCK(sbi, segno));
-		f2fs_bug_on(sbi, !PageUptodate(sum_page));
 		f2fs_put_page(sum_page, 0);
 
+		if (get_valid_blocks(sbi, segno, 1) == 0 ||
+				!PageUptodate(sum_page) ||
+				unlikely(f2fs_cp_error(sbi)))
+			goto next;
+
 		sum = page_address(sum_page);
 		f2fs_bug_on(sbi, type != GET_SUM_TYPE((&sum->footer)));
 

commit 3fa565039e3338f60d7e7a8e818835dabdea764b
Author: Sheng Yong <shengyong1@huawei.com>
Date:   Thu Sep 29 18:37:31 2016 +0800

    f2fs: remove dead variable
    
    Signed-off-by: Sheng Yong <shengyong1@huawei.com>
    Acked-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c9b8a67671f1..93985c64d8a8 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -275,7 +275,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 {
 	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
 	struct victim_sel_policy p;
-	unsigned int secno, max_cost, last_victim;
+	unsigned int secno, last_victim;
 	unsigned int last_segment = MAIN_SEGS(sbi);
 	unsigned int nsearched = 0;
 
@@ -285,7 +285,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	select_policy(sbi, gc_type, type, &p);
 
 	p.min_segno = NULL_SEGNO;
-	p.min_cost = max_cost = get_max_cost(sbi, &p);
+	p.min_cost = get_max_cost(sbi, &p);
 
 	if (p.max_search == 0)
 		goto out;

commit 0f34802858e74e708c6d42209811f6d264892c8f
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Sep 26 19:45:55 2016 +0800

    f2fs: support checkpoint error injection
    
    This patch adds to support checkpoint error injection in f2fs for testing
    fatal error tolerance, it will be useful that it can simulate abnormal
    power off by f2fs itself instead of calling godown ioctl by running apps.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index a5c4175376ab..c9b8a67671f1 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -47,6 +47,11 @@ static int gc_thread_func(void *data)
 			continue;
 		}
 
+#ifdef CONFIG_F2FS_FAULT_INJECTION
+		if (time_to_inject(sbi, FAULT_CHECKPOINT))
+			f2fs_stop_checkpoint(sbi, false);
+#endif
+
 		/*
 		 * [GC triggering condition]
 		 * 0. GC is not conducted currently.

commit 1ecc0c5c50ce8834f7e35b63be7480bf1aaa4155
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Sep 23 21:30:09 2016 +0800

    f2fs: support configuring fault injection per superblock
    
    Previously, we only support global fault injection configuration, so that
    when we configure type/rate of fault injection through sysfs, mount
    option, it will influence all f2fs partition which is being used.
    
    It is not make sence, since it will be not convenient if developer want
    to test separated partitions with different fault injection rate/type
    simultaneously, also it's not possible to enable fault injection in one
    partition and disable fault injection in other one.
    
    >From now on, we move global configuration of fault injection in module
    into per-superblock, hence injection testing can be more flexible.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b9d6c4250efa..a5c4175376ab 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -96,7 +96,7 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 	dev_t dev = sbi->sb->s_bdev->bd_dev;
 	int err = 0;
 
-	gc_th = f2fs_kmalloc(sizeof(struct f2fs_gc_kthread), GFP_KERNEL);
+	gc_th = f2fs_kmalloc(sbi, sizeof(struct f2fs_gc_kthread), GFP_KERNEL);
 	if (!gc_th) {
 		err = -ENOMEM;
 		goto out;

commit 646e759a4d09062df943eaf61cb8141a91204380
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Sep 21 09:37:23 2016 -0700

    f2fs: avoid gc in cp_error case
    
    Otherwise, we can hit
            f2fs_bug_on(sbi, !PageUptodate(sum_page));
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index a9a3c9f19032..b9d6c4250efa 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -847,7 +847,8 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 
 	for (segno = start_segno; segno < end_segno; segno++) {
 
-		if (get_valid_blocks(sbi, segno, 1) == 0)
+		if (get_valid_blocks(sbi, segno, 1) == 0 ||
+					unlikely(f2fs_cp_error(sbi)))
 			goto next;
 
 		/* find segment summary of victim */

commit f6fe2be3c6d6f0127742ae1cc2e3ffe9ad31ea8b
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Sep 21 09:34:48 2016 -0700

    f2fs: should put_page for summary page
    
    We should call put_page for preloaded summary pages in do_garbage_collect.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 400bc6d78372..a9a3c9f19032 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -848,7 +848,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	for (segno = start_segno; segno < end_segno; segno++) {
 
 		if (get_valid_blocks(sbi, segno, 1) == 0)
-			continue;
+			goto next;
 
 		/* find segment summary of victim */
 		sum_page = find_get_page(META_MAPPING(sbi),
@@ -874,7 +874,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 								gc_type);
 
 		stat_inc_seg_count(sbi, type, gc_type);
-
+next:
 		f2fs_put_page(sum_page, 0);
 	}
 

commit 2956e450fa08669ebf1541acb07843b5aa6acf96
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Sep 21 09:28:06 2016 -0700

    f2fs: assign return value in f2fs_gc
    
    This patch adds a return value of write_checkpoint for f2fs_gc.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 24acbbbd0b1d..400bc6d78372 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -925,10 +925,14 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 		 */
 		if (__get_victim(sbi, &segno, gc_type) ||
 						prefree_segments(sbi)) {
-			write_checkpoint(sbi, &cpc);
+			ret = write_checkpoint(sbi, &cpc);
+			if (ret)
+				goto stop;
 			segno = NULL_SEGNO;
 		} else if (has_not_enough_free_secs(sbi, 0, 0)) {
-			write_checkpoint(sbi, &cpc);
+			ret = write_checkpoint(sbi, &cpc);
+			if (ret)
+				goto stop;
 		}
 	}
 
@@ -948,7 +952,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 			goto gc_more;
 
 		if (gc_type == FG_GC)
-			write_checkpoint(sbi, &cpc);
+			ret = write_checkpoint(sbi, &cpc);
 	}
 stop:
 	mutex_unlock(&sbi->gc_mutex);

commit 7f3037a5ec0672e03f96d4b0b86169c4c48e479e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Sep 1 12:02:51 2016 -0700

    f2fs: check free_sections for defragmentation
    
    Fix wrong condition check for defragmentation of a file.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index cdc44a67485f..24acbbbd0b1d 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -439,7 +439,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 		struct node_info ni;
 
 		/* stop BG_GC if there is not enough free sections. */
-		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0))
+		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0))
 			return;
 
 		if (check_valid_map(sbi, segno, off) == 0)
@@ -715,7 +715,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		nid_t nid = le32_to_cpu(entry->nid);
 
 		/* stop BG_GC if there is not enough free sections. */
-		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0))
+		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0, 0))
 			return;
 
 		if (check_valid_map(sbi, segno, off) == 0)
@@ -916,7 +916,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 		goto stop;
 	}
 
-	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, sec_freed)) {
+	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, sec_freed, 0)) {
 		gc_type = FG_GC;
 		/*
 		 * If there is no victim and no prefree segment but still not
@@ -927,7 +927,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 						prefree_segments(sbi)) {
 			write_checkpoint(sbi, &cpc);
 			segno = NULL_SEGNO;
-		} else if (has_not_enough_free_secs(sbi, 0)) {
+		} else if (has_not_enough_free_secs(sbi, 0, 0)) {
 			write_checkpoint(sbi, &cpc);
 		}
 	}
@@ -944,7 +944,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 		sbi->cur_victim_sec = NULL_SEGNO;
 
 	if (!sync) {
-		if (has_not_enough_free_secs(sbi, sec_freed))
+		if (has_not_enough_free_secs(sbi, sec_freed, 0))
 			goto gc_more;
 
 		if (gc_type == FG_GC)

commit 7ea984b0604ac37e806ddc34baf950230bfdaadd
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Aug 27 00:14:31 2016 +0800

    f2fs: do in batch synchronously readahead during GC
    
    In order to enhance performance, we try to readahead node page during
    GC, but before loading node page we should get block address of node page
    which is stored in NAT table, so synchronously read of single NAT page
    block our readahead flow.
    
    f2fs_submit_page_bio: dev = (251,0), ino = 2, page_index = 0xa1e, oldaddr = 0xa1e, newaddr = 0xa1e, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_bio: dev = (251,0), ino = 1, page_index = 0x35e9, oldaddr = 0x72d7a, newaddr = 0x72d7a, rw = READAHEAD ^H, type = NODE
    f2fs_submit_page_bio: dev = (251,0), ino = 2, page_index = 0xc1f, oldaddr = 0xc1f, newaddr = 0xc1f, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_bio: dev = (251,0), ino = 1, page_index = 0x389d, oldaddr = 0x72d7d, newaddr = 0x72d7d, rw = READAHEAD ^H, type = NODE
    f2fs_submit_page_bio: dev = (251,0), ino = 1, page_index = 0x3a82, oldaddr = 0x72d7f, newaddr = 0x72d7f, rw = READAHEAD ^H, type = NODE
    f2fs_submit_page_bio: dev = (251,0), ino = 1, page_index = 0x3bfa, oldaddr = 0x72d86, newaddr = 0x72d86, rw = READAHEAD ^H, type = NODE
    
    This patch adds one phase that do readahead NAT pages in batch before
    readahead node page for more effeciently.
    
    f2fs_submit_page_bio: dev = (251,0), ino = 2, page_index = 0x1952, oldaddr = 0x1952, newaddr = 0x1952, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_mbio: dev = (251,0), ino = 2, page_index = 0xc34, oldaddr = 0xc34, newaddr = 0xc34, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_mbio: dev = (251,0), ino = 2, page_index = 0xa33, oldaddr = 0xa33, newaddr = 0xa33, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_mbio: dev = (251,0), ino = 2, page_index = 0xc30, oldaddr = 0xc30, newaddr = 0xc30, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_mbio: dev = (251,0), ino = 2, page_index = 0xc32, oldaddr = 0xc32, newaddr = 0xc32, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_mbio: dev = (251,0), ino = 2, page_index = 0xc26, oldaddr = 0xc26, newaddr = 0xc26, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_mbio: dev = (251,0), ino = 2, page_index = 0xa2b, oldaddr = 0xa2b, newaddr = 0xa2b, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_mbio: dev = (251,0), ino = 2, page_index = 0xc23, oldaddr = 0xc23, newaddr = 0xc23, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_mbio: dev = (251,0), ino = 2, page_index = 0xc24, oldaddr = 0xc24, newaddr = 0xc24, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_mbio: dev = (251,0), ino = 2, page_index = 0xa10, oldaddr = 0xa10, newaddr = 0xa10, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_mbio: dev = (251,0), ino = 2, page_index = 0xc2c, oldaddr = 0xc2c, newaddr = 0xc2c, rw = READ_SYNC(MP), type = META
    f2fs_submit_page_bio: dev = (251,0), ino = 1, page_index = 0x5db7, oldaddr = 0x6be00, newaddr = 0x6be00, rw = READAHEAD ^H, type = NODE
    f2fs_submit_page_bio: dev = (251,0), ino = 1, page_index = 0x5db9, oldaddr = 0x6be17, newaddr = 0x6be17, rw = READAHEAD ^H, type = NODE
    f2fs_submit_page_bio: dev = (251,0), ino = 1, page_index = 0x5dbc, oldaddr = 0x6be1a, newaddr = 0x6be1a, rw = READAHEAD ^H, type = NODE
    f2fs_submit_page_bio: dev = (251,0), ino = 1, page_index = 0x5dc3, oldaddr = 0x6be20, newaddr = 0x6be20, rw = READAHEAD ^H, type = NODE
    f2fs_submit_page_bio: dev = (251,0), ino = 1, page_index = 0x5dc7, oldaddr = 0x6be24, newaddr = 0x6be24, rw = READAHEAD ^H, type = NODE
    f2fs_submit_page_bio: dev = (251,0), ino = 1, page_index = 0x5dc9, oldaddr = 0x6be25, newaddr = 0x6be25, rw = READAHEAD ^H, type = NODE
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c1599b48859b..cdc44a67485f 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -423,10 +423,10 @@ static int check_valid_map(struct f2fs_sb_info *sbi,
 static void gc_node_segment(struct f2fs_sb_info *sbi,
 		struct f2fs_summary *sum, unsigned int segno, int gc_type)
 {
-	bool initial = true;
 	struct f2fs_summary *entry;
 	block_t start_addr;
 	int off;
+	int phase = 0;
 
 	start_addr = START_BLOCK(sbi, segno);
 
@@ -445,10 +445,18 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 		if (check_valid_map(sbi, segno, off) == 0)
 			continue;
 
-		if (initial) {
+		if (phase == 0) {
+			ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,
+							META_NAT, true);
+			continue;
+		}
+
+		if (phase == 1) {
 			ra_node_page(sbi, nid);
 			continue;
 		}
+
+		/* phase == 2 */
 		node_page = get_node_page(sbi, nid);
 		if (IS_ERR(node_page))
 			continue;
@@ -469,10 +477,8 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 		stat_inc_node_blk_count(sbi, 1, gc_type);
 	}
 
-	if (initial) {
-		initial = false;
+	if (++phase < 3)
 		goto next_step;
-	}
 }
 
 /*
@@ -706,6 +712,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		struct node_info dni; /* dnode info for the data */
 		unsigned int ofs_in_node, nofs;
 		block_t start_bidx;
+		nid_t nid = le32_to_cpu(entry->nid);
 
 		/* stop BG_GC if there is not enough free sections. */
 		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0))
@@ -715,7 +722,13 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			continue;
 
 		if (phase == 0) {
-			ra_node_page(sbi, le32_to_cpu(entry->nid));
+			ra_meta_pages(sbi, NAT_BLOCK_OFFSET(nid), 1,
+							META_NAT, true);
+			continue;
+		}
+
+		if (phase == 1) {
+			ra_node_page(sbi, nid);
 			continue;
 		}
 
@@ -723,14 +736,14 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		if (!is_alive(sbi, entry, &dni, start_addr + off, &nofs))
 			continue;
 
-		if (phase == 1) {
+		if (phase == 2) {
 			ra_node_page(sbi, dni.ino);
 			continue;
 		}
 
 		ofs_in_node = le16_to_cpu(entry->ofs_in_node);
 
-		if (phase == 2) {
+		if (phase == 3) {
 			inode = f2fs_iget(sb, dni.ino);
 			if (IS_ERR(inode) || is_bad_inode(inode))
 				continue;
@@ -756,7 +769,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			continue;
 		}
 
-		/* phase 3 */
+		/* phase 4 */
 		inode = find_gc_inode(gc_list, dni.ino);
 		if (inode) {
 			struct f2fs_inode_info *fi = F2FS_I(inode);
@@ -789,7 +802,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		}
 	}
 
-	if (++phase < 4)
+	if (++phase < 5)
 		goto next_step;
 }
 

commit 43ced84ec8a7cb1b2e56dd1e262a0c63db79c3c1
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Aug 19 23:13:46 2016 +0800

    f2fs: clean up foreground GC flow
    
    This patch changes to check valid block number of one GCed section
    directly instead of checking the number in all segments of section
    one by one in order to clean up codes of foreground GC.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 8f7fa326ce95..c1599b48859b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -815,7 +815,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	struct blk_plug plug;
 	unsigned int segno = start_segno;
 	unsigned int end_segno = start_segno + sbi->segs_per_sec;
-	int seg_freed = 0;
+	int sec_freed = 0;
 	unsigned char type = IS_DATASEG(get_seg_entry(sbi, segno)->type) ?
 						SUM_TYPE_DATA : SUM_TYPE_NODE;
 
@@ -871,22 +871,20 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 
 	blk_finish_plug(&plug);
 
-	if (gc_type == FG_GC) {
-		while (start_segno < end_segno)
-			if (get_valid_blocks(sbi, start_segno++, 1) == 0)
-				seg_freed++;
-	}
+	if (gc_type == FG_GC &&
+		get_valid_blocks(sbi, start_segno, sbi->segs_per_sec) == 0)
+		sec_freed = 1;
 
 	stat_inc_call_count(sbi->stat_info);
 
-	return seg_freed;
+	return sec_freed;
 }
 
 int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 {
 	unsigned int segno;
 	int gc_type = sync ? FG_GC : BG_GC;
-	int sec_freed = 0, seg_freed;
+	int sec_freed = 0;
 	int ret = -EINVAL;
 	struct cp_control cpc;
 	struct gc_inode_list gc_list = {
@@ -925,9 +923,8 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 		goto stop;
 	ret = 0;
 
-	seg_freed = do_garbage_collect(sbi, segno, &gc_list, gc_type);
-
-	if (gc_type == FG_GC && seg_freed == sbi->segs_per_sec)
+	if (do_garbage_collect(sbi, segno, &gc_list, gc_type) &&
+			gc_type == FG_GC)
 		sec_freed++;
 
 	if (gc_type == FG_GC)

commit 4fc29c1aa375353ffe7c8fa171bf941b71ce29ef
Merge: 0e6acf0204da 5302fb000def
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 27 10:36:31 2016 -0700

    Merge tag 'for-f2fs-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "The major change in this version is mitigating cpu overheads on write
      paths by replacing redundant inode page updates with mark_inode_dirty
      calls.  And we tried to reduce lock contentions as well to improve
      filesystem scalability.  Other feature is setting F2FS automatically
      when detecting host-managed SMR.
    
      Enhancements:
       - ioctl to move a range of data between files
       - inject orphan inode errors
       - avoid flush commands congestion
       - support lazytime
    
      Bug fixes:
       - return proper results for some dentry operations
       - fix deadlock in add_link failure
       - disable extent_cache for fcollapse/finsert"
    
    * tag 'for-f2fs-4.8' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (68 commits)
      f2fs: clean up coding style and redundancy
      f2fs: get victim segment again after new cp
      f2fs: handle error case with f2fs_bug_on
      f2fs: avoid data race when deciding checkpoin in f2fs_sync_file
      f2fs: support an ioctl to move a range of data blocks
      f2fs: fix to report error number of f2fs_find_entry
      f2fs: avoid memory allocation failure due to a long length
      f2fs: reset default idle interval value
      f2fs: use blk_plug in all the possible paths
      f2fs: fix to avoid data update racing between GC and DIO
      f2fs: add maximum prefree segments
      f2fs: disable extent_cache for fcollapse/finsert inodes
      f2fs: refactor __exchange_data_block for speed up
      f2fs: fix ERR_PTR returned by bio
      f2fs: avoid mark_inode_dirty
      f2fs: move i_size_write in f2fs_write_end
      f2fs: fix to avoid redundant discard during fstrim
      f2fs: avoid mismatching block range for discard
      f2fs: fix incorrect f_bfree calculation in ->statfs
      f2fs: use percpu_rw_semaphore
      ...

commit fe94793e555f650fab656649521fc38aaab4874e
Author: Yunlei He <heyunlei@huawei.com>
Date:   Fri Jul 22 19:08:31 2016 +0800

    f2fs: get victim segment again after new cp
    
    Previous selected segment may become free after write_checkpoint,
    if we do garbage collect on this segment, and then new_curseg happen
    to reuse it, it may cause f2fs_bug_on as below.
    
            panic+0x154/0x29c
            do_garbage_collect+0x15c/0xaf4
            f2fs_gc+0x2dc/0x444
            f2fs_balance_fs.part.22+0xcc/0x14c
            f2fs_balance_fs+0x28/0x34
            f2fs_map_blocks+0x5ec/0x790
            f2fs_preallocate_blocks+0xe0/0x100
            f2fs_file_write_iter+0x64/0x11c
            new_sync_write+0xac/0x11c
            vfs_write+0x144/0x1e4
            SyS_write+0x60/0xc0
    
    Here, maybe we check sit and ssa type during reset_curseg. So, we check
    segment is stale or not, and select a new victim to avoid this.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index de6c41c32c62..06cfb94cc3db 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -908,10 +908,13 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 		 * enough free sections, we should flush dent/node blocks and do
 		 * garbage collections.
 		 */
-		if (__get_victim(sbi, &segno, gc_type) || prefree_segments(sbi))
+		if (__get_victim(sbi, &segno, gc_type) ||
+						prefree_segments(sbi)) {
 			write_checkpoint(sbi, &cpc);
-		else if (has_not_enough_free_secs(sbi, 0))
+			segno = NULL_SEGNO;
+		} else if (has_not_enough_free_secs(sbi, 0)) {
 			write_checkpoint(sbi, &cpc);
+		}
 	}
 
 	if (segno == NULL_SEGNO && !__get_victim(sbi, &segno, gc_type))

commit 70246286e94c335b5bea0cbc68a17a96dd620281
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jul 19 11:28:41 2016 +0200

    block: get rid of bio_rw and READA
    
    These two are confusing leftover of the old world order, combining
    values of the REQ_OP_ and REQ_ namespaces.  For callers that don't
    special case we mostly just replace bi_rw with bio_data_dir or
    op_is_write, except for the few cases where a switch over the REQ_OP_
    values makes more sense.  Any check for READA is replaced with an
    explicit check for REQ_RAHEAD.  Also remove the READA alias for
    REQ_RAHEAD.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Johannes Thumshirn <jthumshirn@suse.de>
    Reviewed-by: Mike Christie <mchristi@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 3649d86bb431..f06ed73adf99 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -733,7 +733,8 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 			start_bidx = start_bidx_of_node(nofs, inode);
 			data_page = get_read_data_page(inode,
-					start_bidx + ofs_in_node, READA, true);
+					start_bidx + ofs_in_node, REQ_RAHEAD,
+					true);
 			if (IS_ERR(data_page)) {
 				iput(inode);
 				continue;

commit 9dfa1baff76d08843aaf5e3c78f6da6950957702
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jul 13 19:33:19 2016 -0700

    f2fs: use blk_plug in all the possible paths
    
    This patch reverts 19a5f5e2ef37 (f2fs: drop any block plugging),
    and adds blk_plug in write paths additionally.
    
    The main reason is that blk_start_plug can be used to wake up from low-power
    mode before submitting further bios.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 5c8acf754513..de6c41c32c62 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -808,6 +808,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 {
 	struct page *sum_page;
 	struct f2fs_summary_block *sum;
+	struct blk_plug plug;
 	unsigned int segno = start_segno;
 	unsigned int end_segno = start_segno + sbi->segs_per_sec;
 	int seg_freed = 0;
@@ -825,6 +826,8 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 		unlock_page(sum_page);
 	}
 
+	blk_start_plug(&plug);
+
 	for (segno = start_segno; segno < end_segno; segno++) {
 
 		if (get_valid_blocks(sbi, segno, 1) == 0)
@@ -862,6 +865,8 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 		f2fs_submit_merged_bio(sbi,
 				(type == SUM_TYPE_NODE) ? NODE : DATA, WRITE);
 
+	blk_finish_plug(&plug);
+
 	if (gc_type == FG_GC) {
 		while (start_segno < end_segno)
 			if (get_valid_blocks(sbi, start_segno++, 1) == 0)

commit 82e0a5aa5ddf794b3e1b21fcd091228736871882
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jul 13 09:18:29 2016 +0800

    f2fs: fix to avoid data update racing between GC and DIO
    
    Datas in file can be operated by GC and DIO simultaneously, so we will
    face race case as below:
    
    For write case:
    Thread A                                Thread B
    - generic_file_direct_write
     - invalidate_inode_pages2_range
     - f2fs_direct_IO
      - do_blockdev_direct_IO
       - do_direct_IO
        - get_more_blocks
                                            - f2fs_gc
                                             - do_garbage_collect
                                              - gc_data_segment
                                               - move_data_page
                                                - do_write_data_page
                                                migrate data block to new block address
       - dio_bio_submit
       update user data to old block address
    
    For read case:
    Thread A                                Thread B
    - generic_file_direct_write
     - invalidate_inode_pages2_range
     - f2fs_direct_IO
      - do_blockdev_direct_IO
       - do_direct_IO
        - get_more_blocks
                                            - f2fs_balance_fs
                                             - f2fs_gc
                                              - do_garbage_collect
                                               - gc_data_segment
                                                - move_data_page
                                                 - do_write_data_page
                                                 migrate data block to new block address
                                              - write_checkpoint
                                               - do_checkpoint
                                                - clear_prefree_segments
                                                 - f2fs_issue_discard
                                                 discard old block adress
       - dio_bio_submit
       update user buffer from obsolete block address
    
    In order to fix this, for one file, we should let DIO and GC getting exclusion
    against with each other.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c61213785914..5c8acf754513 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -755,12 +755,32 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		/* phase 3 */
 		inode = find_gc_inode(gc_list, dni.ino);
 		if (inode) {
+			struct f2fs_inode_info *fi = F2FS_I(inode);
+			bool locked = false;
+
+			if (S_ISREG(inode->i_mode)) {
+				if (!down_write_trylock(&fi->dio_rwsem[READ]))
+					continue;
+				if (!down_write_trylock(
+						&fi->dio_rwsem[WRITE])) {
+					up_write(&fi->dio_rwsem[READ]);
+					continue;
+				}
+				locked = true;
+			}
+
 			start_bidx = start_bidx_of_node(nofs, inode)
 								+ ofs_in_node;
 			if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 				move_encrypted_block(inode, start_bidx);
 			else
 				move_data_page(inode, start_bidx, gc_type);
+
+			if (locked) {
+				up_write(&fi->dio_rwsem[WRITE]);
+				up_write(&fi->dio_rwsem[READ]);
+			}
+
 			stat_inc_data_blk_count(sbi, 1, gc_type);
 		}
 	}

commit 72e1c797b57da42ce8f75c5636720f40c4f607a2
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sun Jul 3 22:05:13 2016 +0800

    f2fs: fix to redirty page if fail to gc data page
    
    If we fail to move data page during foreground GC, we should give another
    chance to writeback that page which was set dirty previously by writer.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c9602d0dc57a..c61213785914 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -653,12 +653,23 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 			.page = page,
 			.encrypted_page = NULL,
 		};
+		bool is_dirty = PageDirty(page);
+		int err;
+
+retry:
 		set_page_dirty(page);
 		f2fs_wait_on_page_writeback(page, DATA, true);
 		if (clear_page_dirty_for_io(page))
 			inode_dec_dirty_pages(inode);
+
 		set_cold_data(page);
-		do_write_data_page(&fio);
+
+		err = do_write_data_page(&fio);
+		if (err == -ENOMEM && is_dirty) {
+			congestion_wait(BLK_RW_ASYNC, HZ/50);
+			goto retry;
+		}
+
 		clear_cold_data(page);
 	}
 out:

commit 1563ac75e7e45adcdc1271e6bb55fe27a23d4e4e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sun Jul 3 22:05:12 2016 +0800

    f2fs: fix to detect truncation prior rather than EIO during read
    
    In procedure of synchonized read, after sending out the read request, reader
    will try to lock the page for waiting device to finish the read jobs and
    unlock the page, but meanwhile, truncater will race with reader, so after
    reader get lock of the page, it should check page's mapping to detect
    whether someone has truncated the page in advance, then reader has the
    chance to do the retry if truncation was done, otherwise read can be failed
    due to previous condition check.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e1d274cdecb8..c9602d0dc57a 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -593,11 +593,11 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	/* write page */
 	lock_page(fio.encrypted_page);
 
-	if (unlikely(!PageUptodate(fio.encrypted_page))) {
+	if (unlikely(fio.encrypted_page->mapping != META_MAPPING(fio.sbi))) {
 		err = -EIO;
 		goto put_page_out;
 	}
-	if (unlikely(fio.encrypted_page->mapping != META_MAPPING(fio.sbi))) {
+	if (unlikely(!PageUptodate(fio.encrypted_page))) {
 		err = -EIO;
 		goto put_page_out;
 	}

commit aa987273290d206b298e9d09db83e32ead661098
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jun 6 18:49:54 2016 -0700

    f2fs: skip clean segment for gc
    
    If a segment in a section is clean or prefreed, we don't need to get its summary
    and do gc.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 67fd2855ccc9..e1d274cdecb8 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -795,6 +795,10 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 	}
 
 	for (segno = start_segno; segno < end_segno; segno++) {
+
+		if (get_valid_blocks(sbi, segno, 1) == 0)
+			continue;
+
 		/* find segment summary of victim */
 		sum_page = find_get_page(META_MAPPING(sbi),
 					GET_SUM_BLOCK(sbi, segno));

commit 19a5f5e2ef37f032efd840ada257bce2e91c8066
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Jun 4 14:25:24 2016 -0700

    f2fs: drop any block plugging
    
    In f2fs, we don't need to keep block plugging for NODE and DATA writes, since
    we already merged bios as much as possible.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 4a03076074af..67fd2855ccc9 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -777,7 +777,6 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 {
 	struct page *sum_page;
 	struct f2fs_summary_block *sum;
-	struct blk_plug plug;
 	unsigned int segno = start_segno;
 	unsigned int end_segno = start_segno + sbi->segs_per_sec;
 	int seg_freed = 0;
@@ -795,8 +794,6 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 		unlock_page(sum_page);
 	}
 
-	blk_start_plug(&plug);
-
 	for (segno = start_segno; segno < end_segno; segno++) {
 		/* find segment summary of victim */
 		sum_page = find_get_page(META_MAPPING(sbi),
@@ -830,8 +827,6 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 		f2fs_submit_merged_bio(sbi,
 				(type == SUM_TYPE_NODE) ? NODE : DATA, WRITE);
 
-	blk_finish_plug(&plug);
-
 	if (gc_type == FG_GC) {
 		while (start_segno < end_segno)
 			if (get_valid_blocks(sbi, start_segno++, 1) == 0)

commit 04d328defd06257bf386d58f359013e0ef329226
Author: Mike Christie <mchristi@redhat.com>
Date:   Sun Jun 5 14:31:55 2016 -0500

    f2fs: use bio op accessors
    
    Separate the op from the rq_flag_bits and have f2fs
    set/get the bio using bio_set_op_attrs/bio_op.
    
    Signed-off-by: Mike Christie <mchristi@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Hannes Reinecke <hare@suse.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 38d56f678912..3649d86bb431 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -538,7 +538,8 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	struct f2fs_io_info fio = {
 		.sbi = F2FS_I_SB(inode),
 		.type = DATA,
-		.rw = READ_SYNC,
+		.op = REQ_OP_READ,
+		.op_flags = READ_SYNC,
 		.encrypted_page = NULL,
 	};
 	struct dnode_of_data dn;
@@ -612,7 +613,8 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	/* allocate block address */
 	f2fs_wait_on_page_writeback(dn.node_page, NODE, true);
 
-	fio.rw = WRITE_SYNC;
+	fio.op = REQ_OP_WRITE;
+	fio.op_flags = WRITE_SYNC;
 	fio.new_blkaddr = newaddr;
 	f2fs_submit_page_mbio(&fio);
 
@@ -649,7 +651,8 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 		struct f2fs_io_info fio = {
 			.sbi = F2FS_I_SB(inode),
 			.type = DATA,
-			.rw = WRITE_SYNC,
+			.op = REQ_OP_WRITE,
+			.op_flags = WRITE_SYNC,
 			.page = page,
 			.encrypted_page = NULL,
 		};

commit 91942321e4c9f8460f260cdfcf0a7a48a73a84a4
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 20 10:13:22 2016 -0700

    f2fs: use inode pointer for {set, clear}_inode_flag
    
    This patch refactors to use inode pointer for set_inode_flag and
    clear_inode_flag.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 38d56f678912..4a03076074af 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -617,9 +617,9 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	f2fs_submit_page_mbio(&fio);
 
 	f2fs_update_data_blkaddr(&dn, newaddr);
-	set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
+	set_inode_flag(inode, FI_APPEND_WRITE);
 	if (page->index == 0)
-		set_inode_flag(F2FS_I(inode), FI_FIRST_BLOCK_WRITTEN);
+		set_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);
 put_page_out:
 	f2fs_put_page(fio.encrypted_page, 1);
 recover_block:

commit 0414b004a894746921bbc05f05dced1e7907b092
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Apr 29 15:16:42 2016 -0700

    f2fs: introduce f2fs_kmalloc to wrap kmalloc
    
    This patch adds f2fs_kmalloc.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 19e9fafc5c70..38d56f678912 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -96,7 +96,7 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 	dev_t dev = sbi->sb->s_bdev->bd_dev;
 	int err = 0;
 
-	gc_th = kmalloc(sizeof(struct f2fs_gc_kthread), GFP_KERNEL);
+	gc_th = f2fs_kmalloc(sizeof(struct f2fs_gc_kthread), GFP_KERNEL);
 	if (!gc_th) {
 		err = -ENOMEM;
 		goto out;

commit da011cc0da8cf4a60ddf4d2ae8b42902a3d71e5f
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Apr 27 21:40:15 2016 +0800

    f2fs: move node pages only in victim section during GC
    
    For foreground GC, we cache node blocks in victim section and set them
    dirty, then we call sync_node_pages to flush these node pages, but
    meanwhile, those node pages which does not locate in victim section
    will be flushed together, so more bandwidth and continuous free space
    would be occupied.
    
    So for this condition, it's better to leave those unrelated node page
    in cache for further write hit, and let CP or VM to flush them afterward.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e82046523186..19e9fafc5c70 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -465,15 +465,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 			continue;
 		}
 
-		/* set page dirty and write it */
-		if (gc_type == FG_GC) {
-			f2fs_wait_on_page_writeback(node_page, NODE, true);
-			set_page_dirty(node_page);
-		} else {
-			if (!PageWriteback(node_page))
-				set_page_dirty(node_page);
-		}
-		f2fs_put_page(node_page, 1);
+		move_node_page(node_page, gc_type);
 		stat_inc_node_blk_count(sbi, 1, gc_type);
 	}
 
@@ -834,18 +826,9 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 		f2fs_put_page(sum_page, 0);
 	}
 
-	if (gc_type == FG_GC) {
-		if (type == SUM_TYPE_NODE) {
-			struct writeback_control wbc = {
-				.sync_mode = WB_SYNC_ALL,
-				.nr_to_write = LONG_MAX,
-				.for_reclaim = 0,
-			};
-			sync_node_pages(sbi, &wbc);
-		} else {
-			f2fs_submit_merged_bio(sbi, DATA, WRITE);
-		}
-	}
+	if (gc_type == FG_GC)
+		f2fs_submit_merged_bio(sbi,
+				(type == SUM_TYPE_NODE) ? NODE : DATA, WRITE);
 
 	blk_finish_plug(&plug);
 

commit 5268137564920843e581304d9bfb06fb9502cf24
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Apr 13 16:24:44 2016 -0700

    f2fs: split sync_node_pages with fsync_node_pages
    
    This patch splits the existing sync_node_pages into (f)sync_node_pages.
    The fsync_node_pages is used for f2fs_sync_file only.
    
    Acked-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b0051a97824c..e82046523186 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -841,7 +841,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 				.nr_to_write = LONG_MAX,
 				.for_reclaim = 0,
 			};
-			sync_node_pages(sbi, 0, &wbc);
+			sync_node_pages(sbi, &wbc);
 		} else {
 			f2fs_submit_merged_bio(sbi, DATA, WRITE);
 		}

commit f28b3434afb8bb586965970039e46ffb6a1be033
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Feb 24 17:16:47 2016 +0800

    f2fs: introduce f2fs_update_data_blkaddr for cleanup
    
    Add a new help f2fs_update_data_blkaddr to clean up redundant codes.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index d4bf60f128cf..b0051a97824c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -624,9 +624,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	fio.new_blkaddr = newaddr;
 	f2fs_submit_page_mbio(&fio);
 
-	dn.data_blkaddr = fio.new_blkaddr;
-	set_data_blkaddr(&dn);
-	f2fs_update_extent_cache(&dn);
+	f2fs_update_data_blkaddr(&dn, newaddr);
 	set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
 	if (page->index == 0)
 		set_inode_flag(F2FS_I(inode), FI_FIRST_BLOCK_WRITTEN);

commit 4356e48e64374ceac6e4313244eb65158a954b40
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Feb 23 17:52:43 2016 +0800

    f2fs crypto: fix incorrect positioning for GCing encrypted data page
    
    For now, flow of GCing an encrypted data page:
    1) try to grab meta page in meta inode's mapping with index of old block
    address of that data page
    2) load data of ciphertext into meta page
    3) allocate new block address
    4) write the meta page into new block address
    5) update block address pointer in direct node page.
    
    Other reader/writer will use f2fs_wait_on_encrypted_page_writeback to
    check and wait on GCed encrypted data cached in meta page writebacked
    in order to avoid inconsistence among data page cache, meta page cache
    and data on-disk when updating.
    
    However, we will use new block address updated in step 5) as an index to
    lookup meta page in inner bio buffer. That would be wrong, and we will
    never find the GCing meta page, since we use the old block address as
    index of that page in step 1).
    
    This patch fixes the issue by adjust the order of step 1) and step 3),
    and in step 1) grab page with index generated in step 3).
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 4bd0c91d7b5f..d4bf60f128cf 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -553,6 +553,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	struct f2fs_summary sum;
 	struct node_info ni;
 	struct page *page;
+	block_t newaddr;
 	int err;
 
 	/* do not read out */
@@ -583,12 +584,15 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	fio.page = page;
 	fio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;
 
-	fio.encrypted_page = pagecache_get_page(META_MAPPING(fio.sbi),
-					fio.new_blkaddr,
-					FGP_LOCK|FGP_CREAT,
-					GFP_NOFS);
-	if (!fio.encrypted_page)
-		goto put_out;
+	allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &newaddr,
+							&sum, CURSEG_COLD_DATA);
+
+	fio.encrypted_page = pagecache_get_page(META_MAPPING(fio.sbi), newaddr,
+					FGP_LOCK | FGP_CREAT, GFP_NOFS);
+	if (!fio.encrypted_page) {
+		err = -ENOMEM;
+		goto recover_block;
+	}
 
 	err = f2fs_submit_page_bio(&fio);
 	if (err)
@@ -597,10 +601,14 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	/* write page */
 	lock_page(fio.encrypted_page);
 
-	if (unlikely(!PageUptodate(fio.encrypted_page)))
+	if (unlikely(!PageUptodate(fio.encrypted_page))) {
+		err = -EIO;
 		goto put_page_out;
-	if (unlikely(fio.encrypted_page->mapping != META_MAPPING(fio.sbi)))
+	}
+	if (unlikely(fio.encrypted_page->mapping != META_MAPPING(fio.sbi))) {
+		err = -EIO;
 		goto put_page_out;
+	}
 
 	set_page_dirty(fio.encrypted_page);
 	f2fs_wait_on_page_writeback(fio.encrypted_page, DATA, true);
@@ -611,9 +619,9 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 
 	/* allocate block address */
 	f2fs_wait_on_page_writeback(dn.node_page, NODE, true);
-	allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &fio.new_blkaddr,
-							&sum, CURSEG_COLD_DATA);
+
 	fio.rw = WRITE_SYNC;
+	fio.new_blkaddr = newaddr;
 	f2fs_submit_page_mbio(&fio);
 
 	dn.data_blkaddr = fio.new_blkaddr;
@@ -624,6 +632,10 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 		set_inode_flag(F2FS_I(inode), FI_FIRST_BLOCK_WRITTEN);
 put_page_out:
 	f2fs_put_page(fio.encrypted_page, 1);
+recover_block:
+	if (err)
+		__f2fs_replace_block(fio.sbi, &sum, newaddr, fio.old_blkaddr,
+								true, true);
 put_out:
 	f2fs_put_dnode(&dn);
 out:

commit 7a9d75481b85d59204d76097d41a28db663a7a43
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Feb 22 18:36:38 2016 +0800

    f2fs: trace old block address for CoWed page
    
    This patch enables to trace old block address of CoWed page for better
    debugging.
    
    f2fs_submit_page_mbio: dev = (1,0), ino = 1, page_index = 0x1d4f0, oldaddr = 0xfe8ab, newaddr = 0xfee90 rw = WRITE_SYNC, type = NODE
    f2fs_submit_page_mbio: dev = (1,0), ino = 1, page_index = 0x1d4f8, oldaddr = 0xfe8b0, newaddr = 0xfee91 rw = WRITE_SYNC, type = NODE
    f2fs_submit_page_mbio: dev = (1,0), ino = 1, page_index = 0x1d4fa, oldaddr = 0xfe8ae, newaddr = 0xfee92 rw = WRITE_SYNC, type = NODE
    
    f2fs_submit_page_mbio: dev = (1,0), ino = 134824, page_index = 0x96, oldaddr = 0xf049b, newaddr = 0x2bbe rw = WRITE, type = DATA
    f2fs_submit_page_mbio: dev = (1,0), ino = 134824, page_index = 0x97, oldaddr = 0xf049c, newaddr = 0x2bbf rw = WRITE, type = DATA
    f2fs_submit_page_mbio: dev = (1,0), ino = 134824, page_index = 0x98, oldaddr = 0xf049d, newaddr = 0x2bc0 rw = WRITE, type = DATA
    
    f2fs_submit_page_mbio: dev = (1,0), ino = 135260, page_index = 0x47, oldaddr = 0xffffffff, newaddr = 0xf2631 rw = WRITE, type = DATA
    f2fs_submit_page_mbio: dev = (1,0), ino = 135260, page_index = 0x48, oldaddr = 0xffffffff, newaddr = 0xf2632 rw = WRITE, type = DATA
    f2fs_submit_page_mbio: dev = (1,0), ino = 135260, page_index = 0x49, oldaddr = 0xffffffff, newaddr = 0xf2633 rw = WRITE, type = DATA
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 8d63fc0b84ea..4bd0c91d7b5f 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -581,10 +581,10 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 
 	/* read page */
 	fio.page = page;
-	fio.blk_addr = dn.data_blkaddr;
+	fio.new_blkaddr = fio.old_blkaddr = dn.data_blkaddr;
 
 	fio.encrypted_page = pagecache_get_page(META_MAPPING(fio.sbi),
-					fio.blk_addr,
+					fio.new_blkaddr,
 					FGP_LOCK|FGP_CREAT,
 					GFP_NOFS);
 	if (!fio.encrypted_page)
@@ -611,12 +611,12 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 
 	/* allocate block address */
 	f2fs_wait_on_page_writeback(dn.node_page, NODE, true);
-	allocate_data_block(fio.sbi, NULL, fio.blk_addr,
-					&fio.blk_addr, &sum, CURSEG_COLD_DATA);
+	allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &fio.new_blkaddr,
+							&sum, CURSEG_COLD_DATA);
 	fio.rw = WRITE_SYNC;
 	f2fs_submit_page_mbio(&fio);
 
-	dn.data_blkaddr = fio.blk_addr;
+	dn.data_blkaddr = fio.new_blkaddr;
 	set_data_blkaddr(&dn);
 	f2fs_update_extent_cache(&dn);
 	set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);

commit 17d899df4678a19b6715e225e2c2d175151887a7
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Feb 22 18:32:13 2016 +0800

    f2fs: fix the wrong stat count of calling gc
    
    With a partition which was formated as multi segments in one section,
    we stated incorrectly for count of gc operation.
    
    e.g., for a partition with segs_per_sec = 4
    
    cat /sys/kernel/debug/f2fs/status
    
    GC calls: 208 (BG: 7)
      - data segments : 104 (52)
      - node segments : 104 (24)
    
    GC called count should be (104 (data segs) + 104 (node segs)) / 4 = 52,
    rather than 208. Fix it.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c01353429ba0..8d63fc0b84ea 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -820,7 +820,6 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 								gc_type);
 
 		stat_inc_seg_count(sbi, type, gc_type);
-		stat_inc_call_count(sbi->stat_info);
 
 		f2fs_put_page(sum_page, 0);
 	}
@@ -845,6 +844,9 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi,
 			if (get_valid_blocks(sbi, start_segno++, 1) == 0)
 				seg_freed++;
 	}
+
+	stat_inc_call_count(sbi->stat_info);
+
 	return seg_freed;
 }
 

commit 4ce537763eeb6b9d453f84b70c69c609973ccc1e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Feb 18 16:34:38 2016 -0800

    f2fs: remain last victim segment number ascending order
    
    This patch avoids to remain inefficient victim segment number selected by
    a victim.
    
    For example, if all the dirty segments has same valid blocks, we can get
    the victim segments descending order due to keeping wrong last segment number.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 47ade3542fbd..c01353429ba0 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -270,7 +270,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 {
 	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
 	struct victim_sel_policy p;
-	unsigned int secno, max_cost;
+	unsigned int secno, max_cost, last_victim;
 	unsigned int last_segment = MAIN_SEGS(sbi);
 	unsigned int nsearched = 0;
 
@@ -285,6 +285,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	if (p.max_search == 0)
 		goto out;
 
+	last_victim = sbi->last_victim[p.gc_mode];
 	if (p.alloc_mode == LFS && gc_type == FG_GC) {
 		p.min_segno = check_bg_victims(sbi);
 		if (p.min_segno != NULL_SEGNO)
@@ -332,7 +333,10 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 		}
 next:
 		if (nsearched >= p.max_search) {
-			sbi->last_victim[p.gc_mode] = segno;
+			if (!sbi->last_victim[p.gc_mode] && segno <= last_victim)
+				sbi->last_victim[p.gc_mode] = last_victim + 1;
+			else
+				sbi->last_victim[p.gc_mode] = segno + 1;
 			break;
 		}
 	}

commit 81ca7350ce5ed438547ea769b0c33cb0abbd74ba
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Jan 26 15:39:35 2016 +0800

    f2fs: remove unneeded pointer conversion
    
    There are redundant pointer conversion in following call stack:
     - at position a, inode was been converted to f2fs_file_info.
     - at position b, f2fs_file_info was been converted to inode again.
    
     - truncate_blocks(inode,..)
      - fi = F2FS_I(inode)          ---a
      - ADDRS_PER_PAGE(node_page, fi)
       - addrs_per_inode(fi)
        - inode = &fi->vfs_inode    ---b
        - f2fs_has_inline_xattr(inode)
         - fi = F2FS_I(inode)
         - is_inode_flag_set(fi,..)
    
    In order to avoid unneeded conversion, alter ADDRS_PER_PAGE and
    addrs_per_inode to acept parameter with type of inode pointer.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 645273c8b341..47ade3542fbd 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -486,7 +486,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
  * as indirect or double indirect node blocks, are given, it must be a caller's
  * bug.
  */
-block_t start_bidx_of_node(unsigned int node_ofs, struct f2fs_inode_info *fi)
+block_t start_bidx_of_node(unsigned int node_ofs, struct inode *inode)
 {
 	unsigned int indirect_blks = 2 * NIDS_PER_BLOCK + 4;
 	unsigned int bidx;
@@ -503,7 +503,7 @@ block_t start_bidx_of_node(unsigned int node_ofs, struct f2fs_inode_info *fi)
 		int dec = (node_ofs - indirect_blks - 3) / (NIDS_PER_BLOCK + 1);
 		bidx = node_ofs - 5 - dec;
 	}
-	return bidx * ADDRS_PER_BLOCK + ADDRS_PER_INODE(fi);
+	return bidx * ADDRS_PER_BLOCK + ADDRS_PER_INODE(inode);
 }
 
 static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
@@ -722,7 +722,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 				continue;
 			}
 
-			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode));
+			start_bidx = start_bidx_of_node(nofs, inode);
 			data_page = get_read_data_page(inode,
 					start_bidx + ofs_in_node, READA, true);
 			if (IS_ERR(data_page)) {
@@ -738,7 +738,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		/* phase 3 */
 		inode = find_gc_inode(gc_list, dni.ino);
 		if (inode) {
-			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode))
+			start_bidx = start_bidx_of_node(nofs, inode)
 								+ ofs_in_node;
 			if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
 				move_encrypted_block(inode, start_bidx);

commit 688159b6db47a9ebc0b34402bf5df157c45f6a46
Author: Fan Li <fanofcode.li@samsung.com>
Date:   Wed Feb 3 16:21:57 2016 +0800

    f2fs: avoid unnecessary search while finding victim in gc
    
    variable nsearched in get_victim_by_default() indicates the number of
    dirty segments we already checked. There are 2 problems about the way
    it updates:
    1. When p.ofs_unit is greater than 1, the victim we find consists
       of multiple segments, possibly more than 1 dirty segment.
       But nsearched always increases by 1.
    2. If segments have been found but not been chosen, nsearched won't
       increase. So even we have checked all dirty segments, nsearched
       may still less than p.max_search.
    All these problems could cause unnecessary search after all dirty
    segments have already been checked.
    
    Signed-off-by: Fan li <fanofcode.li@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index d57245deca99..645273c8b341 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -245,6 +245,18 @@ static inline unsigned int get_gc_cost(struct f2fs_sb_info *sbi,
 		return get_cb_cost(sbi, segno);
 }
 
+static unsigned int count_bits(const unsigned long *addr,
+				unsigned int offset, unsigned int len)
+{
+	unsigned int end = offset + len, sum = 0;
+
+	while (offset < end) {
+		if (test_bit(offset++, addr))
+			++sum;
+	}
+	return sum;
+}
+
 /*
  * This function is called from two paths.
  * One is garbage collection and the other is SSR segment selection.
@@ -260,7 +272,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	struct victim_sel_policy p;
 	unsigned int secno, max_cost;
 	unsigned int last_segment = MAIN_SEGS(sbi);
-	int nsearched = 0;
+	unsigned int nsearched = 0;
 
 	mutex_lock(&dirty_i->seglist_lock);
 
@@ -295,26 +307,31 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 		}
 
 		p.offset = segno + p.ofs_unit;
-		if (p.ofs_unit > 1)
+		if (p.ofs_unit > 1) {
 			p.offset -= segno % p.ofs_unit;
+			nsearched += count_bits(p.dirty_segmap,
+						p.offset - p.ofs_unit,
+						p.ofs_unit);
+		} else {
+			nsearched++;
+		}
+
 
 		secno = GET_SECNO(sbi, segno);
 
 		if (sec_usage_check(sbi, secno))
-			continue;
+			goto next;
 		if (gc_type == BG_GC && test_bit(secno, dirty_i->victim_secmap))
-			continue;
+			goto next;
 
 		cost = get_gc_cost(sbi, segno, &p);
 
 		if (p.min_cost > cost) {
 			p.min_segno = segno;
 			p.min_cost = cost;
-		} else if (unlikely(cost == max_cost)) {
-			continue;
 		}
-
-		if (nsearched++ >= p.max_search) {
+next:
+		if (nsearched >= p.max_search) {
 			sbi->last_victim[p.gc_mode] = segno;
 			break;
 		}

commit fec1d6576cdf2ce13f84fcdf7b20d02a05f76fc6
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jan 20 23:43:51 2016 +0800

    f2fs: use wait_for_stable_page to avoid contention
    
    In write_begin, if storage supports stable_page, we don't need to wait for
    writeback to update its contents.
    This patch introduces to use wait_for_stable_page instead of
    wait_on_page_writeback.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c1f363d2b17c..d57245deca99 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -446,7 +446,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 
 		/* set page dirty and write it */
 		if (gc_type == FG_GC) {
-			f2fs_wait_on_page_writeback(node_page, NODE);
+			f2fs_wait_on_page_writeback(node_page, NODE, true);
 			set_page_dirty(node_page);
 		} else {
 			if (!PageWriteback(node_page))
@@ -553,7 +553,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	 * don't cache encrypted data into meta inode until previous dirty
 	 * data were writebacked to avoid racing between GC and flush.
 	 */
-	f2fs_wait_on_page_writeback(page, DATA);
+	f2fs_wait_on_page_writeback(page, DATA, true);
 
 	get_node_info(fio.sbi, dn.nid, &ni);
 	set_summary(&sum, dn.nid, dn.ofs_in_node, ni.version);
@@ -582,14 +582,14 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 		goto put_page_out;
 
 	set_page_dirty(fio.encrypted_page);
-	f2fs_wait_on_page_writeback(fio.encrypted_page, DATA);
+	f2fs_wait_on_page_writeback(fio.encrypted_page, DATA, true);
 	if (clear_page_dirty_for_io(fio.encrypted_page))
 		dec_page_count(fio.sbi, F2FS_DIRTY_META);
 
 	set_page_writeback(fio.encrypted_page);
 
 	/* allocate block address */
-	f2fs_wait_on_page_writeback(dn.node_page, NODE);
+	f2fs_wait_on_page_writeback(dn.node_page, NODE, true);
 	allocate_data_block(fio.sbi, NULL, fio.blk_addr,
 					&fio.blk_addr, &sum, CURSEG_COLD_DATA);
 	fio.rw = WRITE_SYNC;
@@ -631,7 +631,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 			.encrypted_page = NULL,
 		};
 		set_page_dirty(page);
-		f2fs_wait_on_page_writeback(page, DATA);
+		f2fs_wait_on_page_writeback(page, DATA, true);
 		if (clear_page_dirty_for_io(page))
 			inode_dec_dirty_pages(inode);
 		set_cold_data(page);

commit 718e53fa633f84d09acb4b76f1ad572ccbf75a12
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sat Jan 23 16:23:55 2016 +0800

    f2fs: enhance foreground GC
    
    If we configure section consist of multiple segments, foreground GC will
    do the garbage collection with following approach:
    
            for each segment in victim section
                    blk_start_plug
                    for each valid block in segment
                            write out by OPU method
                    submit bio cache   <---
                    blk_finish_plug   <---
    
    There are two issue:
    1) for most of the time, 'submit bio cache' will break the merging in
    current bio buffer from writes of next segments, making a smaller bio
    submitting.
    2) block plug only cover IO submitting in one segment, which reduce
    opportunity of merging IOs in plug with multiple segments.
    
    So refactor the code as below structure to strive for biggest
    opportunity of merging IOs:
    
            blk_start_plug
            for each segment in victim section
                    for each valid block in segment
                            write out by OPU method
            submit bio cache
            blk_finish_plug
    
    Test method:
    1. mkfs.f2fs -s 8 /dev/sdX
    2. touch 32 files
    3. write 2M data into each file
    4. punch 1.5M data from offset 0 for each file
    5. trigger foreground gc through ioctl
    
    Before patch, there are totoally 40 bios submitted.
    f2fs_submit_write_bio: dev = (8,32), WRITE_SYNC, DATA, sector = 65536, size = 122880
    f2fs_submit_write_bio: dev = (8,32), WRITE_SYNC, DATA, sector = 65776, size = 122880
    f2fs_submit_write_bio: dev = (8,32), WRITE_SYNC, DATA, sector = 66016, size = 122880
    f2fs_submit_write_bio: dev = (8,32), WRITE_SYNC, DATA, sector = 66256, size = 122880
    f2fs_submit_write_bio: dev = (8,32), WRITE_SYNC, DATA, sector = 66496, size = 32768
    ----repeat for 8 times
    
    After patch, there are totally 35 bios submitted.
    f2fs_submit_write_bio: dev = (8,32), WRITE_SYNC, DATA, sector = 65536, size = 122880
    ----repeat 34 times
    f2fs_submit_write_bio: dev = (8,32), WRITE_SYNC, DATA, sector = 73696, size = 16384
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 4a704a16bf17..c1f363d2b17c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -399,7 +399,7 @@ static int check_valid_map(struct f2fs_sb_info *sbi,
  * On validity, copy that node with cold status, otherwise (invalid node)
  * ignore that.
  */
-static int gc_node_segment(struct f2fs_sb_info *sbi,
+static void gc_node_segment(struct f2fs_sb_info *sbi,
 		struct f2fs_summary *sum, unsigned int segno, int gc_type)
 {
 	bool initial = true;
@@ -419,7 +419,7 @@ static int gc_node_segment(struct f2fs_sb_info *sbi,
 
 		/* stop BG_GC if there is not enough free sections. */
 		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0))
-			return 0;
+			return;
 
 		if (check_valid_map(sbi, segno, off) == 0)
 			continue;
@@ -460,20 +460,6 @@ static int gc_node_segment(struct f2fs_sb_info *sbi,
 		initial = false;
 		goto next_step;
 	}
-
-	if (gc_type == FG_GC) {
-		struct writeback_control wbc = {
-			.sync_mode = WB_SYNC_ALL,
-			.nr_to_write = LONG_MAX,
-			.for_reclaim = 0,
-		};
-		sync_node_pages(sbi, 0, &wbc);
-
-		/* return 1 only if FG_GC succefully reclaimed one */
-		if (get_valid_blocks(sbi, segno, 1) == 0)
-			return 1;
-	}
-	return 0;
 }
 
 /*
@@ -663,7 +649,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
  * If the parent node is not valid or the data block address is different,
  * the victim data block is ignored.
  */
-static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
+static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		struct gc_inode_list *gc_list, unsigned int segno, int gc_type)
 {
 	struct super_block *sb = sbi->sb;
@@ -686,7 +672,7 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 		/* stop BG_GC if there is not enough free sections. */
 		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0))
-			return 0;
+			return;
 
 		if (check_valid_map(sbi, segno, off) == 0)
 			continue;
@@ -747,15 +733,6 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 	if (++phase < 4)
 		goto next_step;
-
-	if (gc_type == FG_GC) {
-		f2fs_submit_merged_bio(sbi, DATA, WRITE);
-
-		/* return 1 only if FG_GC succefully reclaimed one */
-		if (get_valid_blocks(sbi, segno, 1) == 0)
-			return 1;
-	}
-	return 0;
 }
 
 static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
@@ -771,53 +748,90 @@ static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
 	return ret;
 }
 
-static int do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
+static int do_garbage_collect(struct f2fs_sb_info *sbi,
+				unsigned int start_segno,
 				struct gc_inode_list *gc_list, int gc_type)
 {
 	struct page *sum_page;
 	struct f2fs_summary_block *sum;
 	struct blk_plug plug;
-	int nfree = 0;
+	unsigned int segno = start_segno;
+	unsigned int end_segno = start_segno + sbi->segs_per_sec;
+	int seg_freed = 0;
+	unsigned char type = IS_DATASEG(get_seg_entry(sbi, segno)->type) ?
+						SUM_TYPE_DATA : SUM_TYPE_NODE;
 
-	/* read segment summary of victim */
-	sum_page = get_sum_page(sbi, segno);
+	/* readahead multi ssa blocks those have contiguous address */
+	if (sbi->segs_per_sec > 1)
+		ra_meta_pages(sbi, GET_SUM_BLOCK(sbi, segno),
+					sbi->segs_per_sec, META_SSA, true);
+
+	/* reference all summary page */
+	while (segno < end_segno) {
+		sum_page = get_sum_page(sbi, segno++);
+		unlock_page(sum_page);
+	}
 
 	blk_start_plug(&plug);
 
-	sum = page_address(sum_page);
+	for (segno = start_segno; segno < end_segno; segno++) {
+		/* find segment summary of victim */
+		sum_page = find_get_page(META_MAPPING(sbi),
+					GET_SUM_BLOCK(sbi, segno));
+		f2fs_bug_on(sbi, !PageUptodate(sum_page));
+		f2fs_put_page(sum_page, 0);
 
-	/*
-	 * this is to avoid deadlock:
-	 * - lock_page(sum_page)         - f2fs_replace_block
-	 *  - check_valid_map()            - mutex_lock(sentry_lock)
-	 *   - mutex_lock(sentry_lock)     - change_curseg()
-	 *                                  - lock_page(sum_page)
-	 */
-	unlock_page(sum_page);
-
-	switch (GET_SUM_TYPE((&sum->footer))) {
-	case SUM_TYPE_NODE:
-		nfree = gc_node_segment(sbi, sum->entries, segno, gc_type);
-		break;
-	case SUM_TYPE_DATA:
-		nfree = gc_data_segment(sbi, sum->entries, gc_list,
-							segno, gc_type);
-		break;
+		sum = page_address(sum_page);
+		f2fs_bug_on(sbi, type != GET_SUM_TYPE((&sum->footer)));
+
+		/*
+		 * this is to avoid deadlock:
+		 * - lock_page(sum_page)         - f2fs_replace_block
+		 *  - check_valid_map()            - mutex_lock(sentry_lock)
+		 *   - mutex_lock(sentry_lock)     - change_curseg()
+		 *                                  - lock_page(sum_page)
+		 */
+
+		if (type == SUM_TYPE_NODE)
+			gc_node_segment(sbi, sum->entries, segno, gc_type);
+		else
+			gc_data_segment(sbi, sum->entries, gc_list, segno,
+								gc_type);
+
+		stat_inc_seg_count(sbi, type, gc_type);
+		stat_inc_call_count(sbi->stat_info);
+
+		f2fs_put_page(sum_page, 0);
+	}
+
+	if (gc_type == FG_GC) {
+		if (type == SUM_TYPE_NODE) {
+			struct writeback_control wbc = {
+				.sync_mode = WB_SYNC_ALL,
+				.nr_to_write = LONG_MAX,
+				.for_reclaim = 0,
+			};
+			sync_node_pages(sbi, 0, &wbc);
+		} else {
+			f2fs_submit_merged_bio(sbi, DATA, WRITE);
+		}
 	}
-	blk_finish_plug(&plug);
 
-	stat_inc_seg_count(sbi, GET_SUM_TYPE((&sum->footer)), gc_type);
-	stat_inc_call_count(sbi->stat_info);
+	blk_finish_plug(&plug);
 
-	f2fs_put_page(sum_page, 0);
-	return nfree;
+	if (gc_type == FG_GC) {
+		while (start_segno < end_segno)
+			if (get_valid_blocks(sbi, start_segno++, 1) == 0)
+				seg_freed++;
+	}
+	return seg_freed;
 }
 
 int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 {
-	unsigned int segno, i;
+	unsigned int segno;
 	int gc_type = sync ? FG_GC : BG_GC;
-	int sec_freed = 0;
+	int sec_freed = 0, seg_freed;
 	int ret = -EINVAL;
 	struct cp_control cpc;
 	struct gc_inode_list gc_list = {
@@ -853,22 +867,9 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 		goto stop;
 	ret = 0;
 
-	/* readahead multi ssa blocks those have contiguous address */
-	if (sbi->segs_per_sec > 1)
-		ra_meta_pages(sbi, GET_SUM_BLOCK(sbi, segno), sbi->segs_per_sec,
-							META_SSA, true);
-
-	for (i = 0; i < sbi->segs_per_sec; i++) {
-		/*
-		 * for FG_GC case, halt gcing left segments once failed one
-		 * of segments in selected section to avoid long latency.
-		 */
-		if (!do_garbage_collect(sbi, segno + i, &gc_list, gc_type) &&
-				gc_type == FG_GC)
-			break;
-	}
+	seg_freed = do_garbage_collect(sbi, segno, &gc_list, gc_type);
 
-	if (i == sbi->segs_per_sec && gc_type == FG_GC)
+	if (gc_type == FG_GC && seg_freed == sbi->segs_per_sec)
 		sec_freed++;
 
 	if (gc_type == FG_GC)

commit 6e17bfbc75a5cb9c363ba488bf57cd0152bc3ec0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Jan 23 22:00:57 2016 +0800

    f2fs: fix to overcome inline_data floods
    
    The scenario is:
    1. create lots of node blocks
    2. sync
    3. write lots of inline_data
    -> got panic due to no free space
    
    In that case, we should flush node blocks when writing inline_data in #3,
    and trigger gc as well.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index f610c2a9bdde..4a704a16bf17 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -838,8 +838,15 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 
 	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, sec_freed)) {
 		gc_type = FG_GC;
+		/*
+		 * If there is no victim and no prefree segment but still not
+		 * enough free sections, we should flush dent/node blocks and do
+		 * garbage collections.
+		 */
 		if (__get_victim(sbi, &segno, gc_type) || prefree_segments(sbi))
 			write_checkpoint(sbi, &cpc);
+		else if (has_not_enough_free_secs(sbi, 0))
+			write_checkpoint(sbi, &cpc);
 	}
 
 	if (segno == NULL_SEGNO && !__get_victim(sbi, &segno, gc_type))

commit d0239e1bf5204d602281f93c01d46bcf3531098d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jan 8 16:57:48 2016 -0800

    f2fs: detect idle time depending on user behavior
    
    This patch adds last time that user requested filesystem operations.
    This information is used to detect whether system is idle or not later.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c09be339569c..f610c2a9bdde 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -16,7 +16,6 @@
 #include <linux/kthread.h>
 #include <linux/delay.h>
 #include <linux/freezer.h>
-#include <linux/blkdev.h>
 
 #include "f2fs.h"
 #include "node.h"

commit 6d5a1495eebd441216dc96913a4270100b26e104
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Dec 24 18:04:56 2015 +0800

    f2fs: let user being aware of IO error
    
    Sometimes we keep dumb when IO error occur in lower layer device, so user
    will not receive any error return value for some operation, but actually,
    the operation did not succeed.
    
    This sould be avoided, so this patch reports such kind of error to user.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index ce350c44b5cf..c09be339569c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -832,8 +832,10 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 
 	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE)))
 		goto stop;
-	if (unlikely(f2fs_cp_error(sbi)))
+	if (unlikely(f2fs_cp_error(sbi))) {
+		ret = -EIO;
 		goto stop;
+	}
 
 	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, sec_freed)) {
 		gc_type = FG_GC;

commit 3519e3f992995d46c200134cfbf84c61b7a01f4c
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Dec 1 11:56:52 2015 +0800

    f2fs: use sbi->blocks_per_seg to avoid unnecessary calculation
    
    Use sbi->blocks_per_seg directly to avoid unnecessary calculation when using
    1 << sbi->log_blocks_per_seg.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index fedbf67a0842..ce350c44b5cf 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -173,9 +173,9 @@ static unsigned int get_max_cost(struct f2fs_sb_info *sbi,
 {
 	/* SSR allocates in a segment unit */
 	if (p->alloc_mode == SSR)
-		return 1 << sbi->log_blocks_per_seg;
+		return sbi->blocks_per_seg;
 	if (p->gc_mode == GC_GREEDY)
-		return (1 << sbi->log_blocks_per_seg) * p->ofs_unit;
+		return sbi->blocks_per_seg * p->ofs_unit;
 	else if (p->gc_mode == GC_CB)
 		return UINT_MAX;
 	else /* No other gc_mode */

commit 84e4214f0868ae77771837d0ed4cc6eff10738ba
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Oct 13 10:00:53 2015 -0700

    f2fs: relocate the tracepoint for background_gc
    
    Once f2fs_gc is done, wait_ms is changed once more.
    So, its tracepoint would be located after it.
    
    Reported-by: He YunLei <heyunlei@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index af7c24caeef9..fedbf67a0842 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -77,13 +77,13 @@ static int gc_thread_func(void *data)
 
 		stat_inc_bggc_count(sbi);
 
-		trace_f2fs_background_gc(sbi->sb, wait_ms,
-				prefree_segments(sbi), free_segments(sbi));
-
 		/* if return value is not zero, no victim was selected */
 		if (f2fs_gc(sbi, test_opt(sbi, FORCE_FG_GC)))
 			wait_ms = gc_th->no_gc_sleep_time;
 
+		trace_f2fs_background_gc(sbi->sb, wait_ms,
+				prefree_segments(sbi), free_segments(sbi));
+
 		/* balancing f2fs's metadata periodically */
 		f2fs_balance_fs_bg(sbi);
 

commit 08b39fbd59781729da9fb6367decaf4804a22721
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Oct 8 13:27:34 2015 +0800

    f2fs crypto: fix racing of accessing encrypted page among
    
     different competitors
    
    Since we use different page cache (normally inode's page cache for R/W
    and meta inode's page cache for GC) to cache the same physical block
    which is belong to an encrypted inode. Writeback of these two page
    cache should be exclusive, but now we didn't handle writeback state
    well, so there may be potential racing problem:
    
    a)
    kworker:                                f2fs_gc:
     - f2fs_write_data_pages
      - f2fs_write_data_page
       - do_write_data_page
        - write_data_page
         - f2fs_submit_page_mbio
    (page#1 in inode's page cache was queued
    in f2fs bio cache, and be ready to write
    to new blkaddr)
                                             - gc_data_segment
                                              - move_encrypted_block
                                               - pagecache_get_page
                                            (page#2 in meta inode's page cache
                                            was cached with the invalid datas
                                            of physical block located in new
                                            blkaddr)
                                               - f2fs_submit_page_mbio
                                            (page#1 was submitted, later, page#2
                                            with invalid data will be submitted)
    
    b)
    f2fs_gc:
     - gc_data_segment
      - move_encrypted_block
       - f2fs_submit_page_mbio
    (page#1 in meta inode's page cache was
    queued in f2fs bio cache, and be ready
    to write to new blkaddr)
                                            user thread:
                                             - f2fs_write_begin
                                              - f2fs_submit_page_bio
                                            (we submit the request to block layer
                                            to update page#2 in inode's page cache
                                            with physical block located in new
                                            blkaddr, so here we may read gabbage
                                            data from new blkaddr since GC hasn't
                                            writebacked the page#1 yet)
    
    This patch fixes above potential racing problem for encrypted inode.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 3f19634e2aa5..af7c24caeef9 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -559,8 +559,16 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	if (err)
 		goto out;
 
-	if (unlikely(dn.data_blkaddr == NULL_ADDR))
+	if (unlikely(dn.data_blkaddr == NULL_ADDR)) {
+		ClearPageUptodate(page);
 		goto put_out;
+	}
+
+	/*
+	 * don't cache encrypted data into meta inode until previous dirty
+	 * data were writebacked to avoid racing between GC and flush.
+	 */
+	f2fs_wait_on_page_writeback(page, DATA);
 
 	get_node_info(fio.sbi, dn.nid, &ni);
 	set_summary(&sum, dn.nid, dn.ofs_in_node, ni.version);
@@ -589,7 +597,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 		goto put_page_out;
 
 	set_page_dirty(fio.encrypted_page);
-	f2fs_wait_on_page_writeback(fio.encrypted_page, META);
+	f2fs_wait_on_page_writeback(fio.encrypted_page, DATA);
 	if (clear_page_dirty_for_io(fio.encrypted_page))
 		dec_page_count(fio.sbi, F2FS_DIRTY_META);
 

commit 26879fb101f28c554294eaf25ac7817a2825b180
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Oct 12 17:05:59 2015 +0800

    f2fs: support lower priority asynchronous readahead in ra_meta_pages
    
    Now, we use ra_meta_pages to reads continuous physical blocks as much as
    possible to improve performance of following reads. However, ra_meta_pages
    uses a synchronous readahead approach by submitting bio with READ, as READ
    is with high priority, it can not be used in the case of preloading blocks,
    and it's not sure when these RAed pages will be used.
    
    This patch supports asynchronous readahead in ra_meta_pages by tagging bio
    with READA flag in order to allow preloading.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 94278dbbdf7c..3f19634e2aa5 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -840,7 +840,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 	/* readahead multi ssa blocks those have contiguous address */
 	if (sbi->segs_per_sec > 1)
 		ra_meta_pages(sbi, GET_SUM_BLOCK(sbi, segno), sbi->segs_per_sec,
-								META_SSA);
+							META_SSA, true);
 
 	for (i = 0; i < sbi->segs_per_sec; i++) {
 		/*

commit a56c7c6fb3c60857c1335bcb8b914e6f65655486
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Oct 9 15:11:38 2015 -0700

    f2fs: set GFP_NOFS for grab_cache_page
    
    For normal inodes, their pages are allocated with __GFP_FS, which can cause
    filesystem calls when reclaiming memory.
    This can incur a dead lock condition accordingly.
    
    So, this patch addresses this problem by introducing
    f2fs_grab_cache_page(.., bool for_write), which calls
    grab_cache_page_write_begin() with AOP_FLAG_NOFS.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e7cec86f0747..94278dbbdf7c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -550,7 +550,7 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	int err;
 
 	/* do not read out */
-	page = grab_cache_page(inode->i_mapping, bidx);
+	page = f2fs_grab_cache_page(inode->i_mapping, bidx, false);
 	if (!page)
 		return;
 
@@ -620,7 +620,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 {
 	struct page *page;
 
-	page = get_lock_data_page(inode, bidx);
+	page = get_lock_data_page(inode, bidx, true);
 	if (IS_ERR(page))
 		return;
 
@@ -714,7 +714,7 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode));
 			data_page = get_read_data_page(inode,
-					start_bidx + ofs_in_node, READA);
+					start_bidx + ofs_in_node, READA, true);
 			if (IS_ERR(data_page)) {
 				iput(inode);
 				continue;

commit 5c2674347466d5c2d5169214e95f4ad6dc09e9b6
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Oct 5 11:32:34 2015 -0700

    f2fs: add a tracepoint for background gc
    
    This patch introduces a tracepoint to monitor background gc behaviors.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e627c19f7301..e7cec86f0747 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -77,6 +77,9 @@ static int gc_thread_func(void *data)
 
 		stat_inc_bggc_count(sbi);
 
+		trace_f2fs_background_gc(sbi->sb, wait_ms,
+				prefree_segments(sbi), free_segments(sbi));
+
 		/* if return value is not zero, no victim was selected */
 		if (f2fs_gc(sbi, test_opt(sbi, FORCE_FG_GC)))
 			wait_ms = gc_th->no_gc_sleep_time;

commit 6aefd93b01379bf0b62f8b38dcf7a21397893833
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Oct 5 11:02:54 2015 -0700

    f2fs: introduce background_gc=sync mount option
    
    This patch introduce background_gc=sync enabling synchronous cleaning in
    background.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 830d27770a32..e627c19f7301 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -78,7 +78,7 @@ static int gc_thread_func(void *data)
 		stat_inc_bggc_count(sbi);
 
 		/* if return value is not zero, no victim was selected */
-		if (f2fs_gc(sbi, false))
+		if (f2fs_gc(sbi, test_opt(sbi, FORCE_FG_GC)))
 			wait_ms = gc_th->no_gc_sleep_time;
 
 		/* balancing f2fs's metadata periodically */

commit d530d4d8e237f4d12c93bb76df40b69b8b8a1dcd
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Oct 5 22:22:44 2015 +0800

    f2fs: support synchronous gc in ioctl
    
    This patch drops in batches gc triggered through ioctl, since user
    can easily control the gc by designing the loop around the ->ioctl.
    
    We support synchronous gc by forcing using FG_GC in f2fs_gc, so with
    it, user can make sure that in this round all blocks gced were
    persistent in the device until ioctl returned.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index d844a8028527..830d27770a32 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -78,7 +78,7 @@ static int gc_thread_func(void *data)
 		stat_inc_bggc_count(sbi);
 
 		/* if return value is not zero, no victim was selected */
-		if (f2fs_gc(sbi))
+		if (f2fs_gc(sbi, false))
 			wait_ms = gc_th->no_gc_sleep_time;
 
 		/* balancing f2fs's metadata periodically */
@@ -803,12 +803,12 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 	return nfree;
 }
 
-int f2fs_gc(struct f2fs_sb_info *sbi)
+int f2fs_gc(struct f2fs_sb_info *sbi, bool sync)
 {
 	unsigned int segno, i;
-	int gc_type = BG_GC;
+	int gc_type = sync ? FG_GC : BG_GC;
 	int sec_freed = 0;
-	int ret = -1;
+	int ret = -EINVAL;
 	struct cp_control cpc;
 	struct gc_inode_list gc_list = {
 		.ilist = LIST_HEAD_INIT(gc_list.ilist),
@@ -855,15 +855,20 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 	if (gc_type == FG_GC)
 		sbi->cur_victim_sec = NULL_SEGNO;
 
-	if (has_not_enough_free_secs(sbi, sec_freed))
-		goto gc_more;
+	if (!sync) {
+		if (has_not_enough_free_secs(sbi, sec_freed))
+			goto gc_more;
 
-	if (gc_type == FG_GC)
-		write_checkpoint(sbi, &cpc);
+		if (gc_type == FG_GC)
+			write_checkpoint(sbi, &cpc);
+	}
 stop:
 	mutex_unlock(&sbi->gc_mutex);
 
 	put_gc_inode(&gc_list);
+
+	if (sync)
+		ret = sec_freed ? 0 : -EAGAIN;
 	return ret;
 }
 

commit 3342bb303bf48dd8bb5ac94c3356489ff53e4d04
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Oct 5 22:20:40 2015 +0800

    f2fs: skip searching dirty map if dirty segment is not exist
    
    When searching victim during gc, if there are no dirty segments in
    filesystem, we will still take the time to search the whole dirty segment
    map, it's not needed, it's better to skip in this condition.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e5c255ba227b..d844a8028527 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -268,6 +268,9 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	p.min_segno = NULL_SEGNO;
 	p.min_cost = max_cost = get_max_cost(sbi, &p);
 
+	if (p.max_search == 0)
+		goto out;
+
 	if (p.alloc_mode == LFS && gc_type == FG_GC) {
 		p.min_segno = check_bg_victims(sbi);
 		if (p.min_segno != NULL_SEGNO)
@@ -329,6 +332,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 				sbi->cur_victim_sec,
 				prefree_segments(sbi), free_segments(sbi));
 	}
+out:
 	mutex_unlock(&dirty_i->seglist_lock);
 
 	return (p.min_segno == NULL_SEGNO) ? 0 : 1;

commit a43f7ec327b0d86cbb80d0841673038c0706e714
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Oct 5 22:19:24 2015 +0800

    f2fs: fix to avoid redundant searching in dirty map during gc
    
    When doing gc, we search a victim in dirty map, starting from position of
    last victim, we will reset the current searching position until we touch
    the end of dirty map, and then search the whole diryt map. So sometimes we
    will search the range [victim, last] twice, it's redundant, this patch
    avoids this issue.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 343b096cb654..e5c255ba227b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -257,6 +257,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
 	struct victim_sel_policy p;
 	unsigned int secno, max_cost;
+	unsigned int last_segment = MAIN_SEGS(sbi);
 	int nsearched = 0;
 
 	mutex_lock(&dirty_i->seglist_lock);
@@ -277,9 +278,10 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 		unsigned long cost;
 		unsigned int segno;
 
-		segno = find_next_bit(p.dirty_segmap, MAIN_SEGS(sbi), p.offset);
-		if (segno >= MAIN_SEGS(sbi)) {
+		segno = find_next_bit(p.dirty_segmap, last_segment, p.offset);
+		if (segno >= last_segment) {
 			if (sbi->last_victim[p.gc_mode]) {
+				last_segment = sbi->last_victim[p.gc_mode];
 				sbi->last_victim[p.gc_mode] = 0;
 				p.offset = 0;
 				continue;

commit ab126cfc3090fa5af2a87cc0698f793aebbec7d0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Sep 18 17:33:00 2015 -0700

    f2fs: should get a victim from retrials
    
    If we do not call get_victim first, we cannot get a new victim for retrial
    path.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b6e03ebc703c..343b096cb654 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -799,8 +799,7 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 
 int f2fs_gc(struct f2fs_sb_info *sbi)
 {
-	unsigned int segno = NULL_SEGNO;
-	unsigned int i;
+	unsigned int segno, i;
 	int gc_type = BG_GC;
 	int sec_freed = 0;
 	int ret = -1;
@@ -812,6 +811,8 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 
 	cpc.reason = __get_cp_reason(sbi);
 gc_more:
+	segno = NULL_SEGNO;
+
 	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE)))
 		goto stop;
 	if (unlikely(f2fs_cp_error(sbi)))

commit 45fe8492ccbe561c4b8918c2d4c83a0501e50646
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Sep 28 17:42:24 2015 +0800

    f2fs: fix to correct freed section number during gc
    
    This patch fixes to maintain the right section count freed in garbage
    collecting when triggering a foreground gc.
    
    Besides, when a foreground gc is running on current selected section, once
    we fail to gc one segment, it's better to abandon gcing the left segments
    in current section, because anyway we will select next victim for
    foreground gc, so gc on the left segments in previous section will become
    overhead and also cause the long latency for caller.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 782b8e72c094..b6e03ebc703c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -802,7 +802,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 	unsigned int segno = NULL_SEGNO;
 	unsigned int i;
 	int gc_type = BG_GC;
-	int nfree = 0;
+	int sec_freed = 0;
 	int ret = -1;
 	struct cp_control cpc;
 	struct gc_inode_list gc_list = {
@@ -817,7 +817,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 	if (unlikely(f2fs_cp_error(sbi)))
 		goto stop;
 
-	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, nfree)) {
+	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, sec_freed)) {
 		gc_type = FG_GC;
 		if (__get_victim(sbi, &segno, gc_type) || prefree_segments(sbi))
 			write_checkpoint(sbi, &cpc);
@@ -832,13 +832,23 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 		ra_meta_pages(sbi, GET_SUM_BLOCK(sbi, segno), sbi->segs_per_sec,
 								META_SSA);
 
-	for (i = 0; i < sbi->segs_per_sec; i++)
-		nfree += do_garbage_collect(sbi, segno + i, &gc_list, gc_type);
+	for (i = 0; i < sbi->segs_per_sec; i++) {
+		/*
+		 * for FG_GC case, halt gcing left segments once failed one
+		 * of segments in selected section to avoid long latency.
+		 */
+		if (!do_garbage_collect(sbi, segno + i, &gc_list, gc_type) &&
+				gc_type == FG_GC)
+			break;
+	}
+
+	if (i == sbi->segs_per_sec && gc_type == FG_GC)
+		sec_freed++;
 
 	if (gc_type == FG_GC)
 		sbi->cur_victim_sec = NULL_SEGNO;
 
-	if (has_not_enough_free_secs(sbi, nfree))
+	if (has_not_enough_free_secs(sbi, sec_freed))
 		goto gc_more;
 
 	if (gc_type == FG_GC)

commit 5ee5293c3290a8e710d75977418f954e62c3dfdf
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Aug 15 22:06:08 2015 -0700

    f2fs: retry gc if one section is not successfully reclaimed
    
    If FG_GC failed to reclaim one section, let's retry with another section
    from the start, since we can get anoterh good candidate.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 0a5d573e2574..782b8e72c094 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -391,7 +391,7 @@ static int check_valid_map(struct f2fs_sb_info *sbi,
  * On validity, copy that node with cold status, otherwise (invalid node)
  * ignore that.
  */
-static void gc_node_segment(struct f2fs_sb_info *sbi,
+static int gc_node_segment(struct f2fs_sb_info *sbi,
 		struct f2fs_summary *sum, unsigned int segno, int gc_type)
 {
 	bool initial = true;
@@ -411,7 +411,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 
 		/* stop BG_GC if there is not enough free sections. */
 		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0))
-			return;
+			return 0;
 
 		if (check_valid_map(sbi, segno, off) == 0)
 			continue;
@@ -461,13 +461,11 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 		};
 		sync_node_pages(sbi, 0, &wbc);
 
-		/*
-		 * In the case of FG_GC, it'd be better to reclaim this victim
-		 * completely.
-		 */
-		if (get_valid_blocks(sbi, segno, 1) != 0)
-			goto next_step;
+		/* return 1 only if FG_GC succefully reclaimed one */
+		if (get_valid_blocks(sbi, segno, 1) == 0)
+			return 1;
 	}
+	return 0;
 }
 
 /*
@@ -649,7 +647,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
  * If the parent node is not valid or the data block address is different,
  * the victim data block is ignored.
  */
-static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
+static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		struct gc_inode_list *gc_list, unsigned int segno, int gc_type)
 {
 	struct super_block *sb = sbi->sb;
@@ -672,7 +670,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 		/* stop BG_GC if there is not enough free sections. */
 		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0))
-			return;
+			return 0;
 
 		if (check_valid_map(sbi, segno, off) == 0)
 			continue;
@@ -737,15 +735,11 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	if (gc_type == FG_GC) {
 		f2fs_submit_merged_bio(sbi, DATA, WRITE);
 
-		/*
-		 * In the case of FG_GC, it'd be better to reclaim this victim
-		 * completely.
-		 */
-		if (get_valid_blocks(sbi, segno, 1) != 0) {
-			phase = 2;
-			goto next_step;
-		}
+		/* return 1 only if FG_GC succefully reclaimed one */
+		if (get_valid_blocks(sbi, segno, 1) == 0)
+			return 1;
 	}
+	return 0;
 }
 
 static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
@@ -761,12 +755,13 @@ static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
 	return ret;
 }
 
-static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
+static int do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 				struct gc_inode_list *gc_list, int gc_type)
 {
 	struct page *sum_page;
 	struct f2fs_summary_block *sum;
 	struct blk_plug plug;
+	int nfree = 0;
 
 	/* read segment summary of victim */
 	sum_page = get_sum_page(sbi, segno);
@@ -786,10 +781,11 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 
 	switch (GET_SUM_TYPE((&sum->footer))) {
 	case SUM_TYPE_NODE:
-		gc_node_segment(sbi, sum->entries, segno, gc_type);
+		nfree = gc_node_segment(sbi, sum->entries, segno, gc_type);
 		break;
 	case SUM_TYPE_DATA:
-		gc_data_segment(sbi, sum->entries, gc_list, segno, gc_type);
+		nfree = gc_data_segment(sbi, sum->entries, gc_list,
+							segno, gc_type);
 		break;
 	}
 	blk_finish_plug(&plug);
@@ -798,6 +794,7 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 	stat_inc_call_count(sbi->stat_info);
 
 	f2fs_put_page(sum_page, 0);
+	return nfree;
 }
 
 int f2fs_gc(struct f2fs_sb_info *sbi)
@@ -836,13 +833,10 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 								META_SSA);
 
 	for (i = 0; i < sbi->segs_per_sec; i++)
-		do_garbage_collect(sbi, segno + i, &gc_list, gc_type);
+		nfree += do_garbage_collect(sbi, segno + i, &gc_list, gc_type);
 
-	if (gc_type == FG_GC) {
+	if (gc_type == FG_GC)
 		sbi->cur_victim_sec = NULL_SEGNO;
-		nfree++;
-		WARN_ON(get_valid_blocks(sbi, segno, sbi->segs_per_sec));
-	}
 
 	if (has_not_enough_free_secs(sbi, nfree))
 		goto gc_more;

commit 26d5859974bb817f7615be90199a8e82e3f0a0ed
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Aug 14 14:37:50 2015 -0700

    f2fs: avoid garbage collecting already moved node blocks
    
    If node blocks were already moved, we don't need to move them again.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 81de28d8326f..0a5d573e2574 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -396,14 +396,18 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 {
 	bool initial = true;
 	struct f2fs_summary *entry;
+	block_t start_addr;
 	int off;
 
+	start_addr = START_BLOCK(sbi, segno);
+
 next_step:
 	entry = sum;
 
 	for (off = 0; off < sbi->blocks_per_seg; off++, entry++) {
 		nid_t nid = le32_to_cpu(entry->nid);
 		struct page *node_page;
+		struct node_info ni;
 
 		/* stop BG_GC if there is not enough free sections. */
 		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0))
@@ -426,6 +430,12 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 			continue;
 		}
 
+		get_node_info(sbi, nid, &ni);
+		if (ni.blk_addr != start_addr + off) {
+			f2fs_put_page(node_page, 1);
+			continue;
+		}
+
 		/* set page dirty and write it */
 		if (gc_type == FG_GC) {
 			f2fs_wait_on_page_writeback(node_page, NODE);

commit 798c1b16d1a6171587ff46c74ede8092e66f72f7
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Aug 11 21:59:49 2015 -0700

    f2fs: skip checkpoint if there is no dirty and prefree segments
    
    We should avoid needless checkpoints when there is no dirty and prefree segment.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index fcb263af58b3..81de28d8326f 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -792,7 +792,8 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 
 int f2fs_gc(struct f2fs_sb_info *sbi)
 {
-	unsigned int segno, i;
+	unsigned int segno = NULL_SEGNO;
+	unsigned int i;
 	int gc_type = BG_GC;
 	int nfree = 0;
 	int ret = -1;
@@ -811,10 +812,11 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 
 	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, nfree)) {
 		gc_type = FG_GC;
-		write_checkpoint(sbi, &cpc);
+		if (__get_victim(sbi, &segno, gc_type) || prefree_segments(sbi))
+			write_checkpoint(sbi, &cpc);
 	}
 
-	if (!__get_victim(sbi, &segno, gc_type))
+	if (segno == NULL_SEGNO && !__get_victim(sbi, &segno, gc_type))
 		goto stop;
 	ret = 0;
 

commit 1b77c416e7dfe317277057c32baa67ea9e486ae7
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jul 13 18:31:24 2015 -0700

    f2fs: use a page temporarily for encrypted gced page
    
    That encrypted page is used temporarily, so we don't need to mark it accessed.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 2701e05af991..fcb263af58b3 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -552,7 +552,10 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	fio.page = page;
 	fio.blk_addr = dn.data_blkaddr;
 
-	fio.encrypted_page = grab_cache_page(META_MAPPING(fio.sbi), fio.blk_addr);
+	fio.encrypted_page = pagecache_get_page(META_MAPPING(fio.sbi),
+					fio.blk_addr,
+					FGP_LOCK|FGP_CREAT,
+					GFP_NOFS);
 	if (!fio.encrypted_page)
 		goto put_out;
 

commit c1079892f4e8ecfd1bbc525cbfc1bd46b470888e
Author: Nicholas Krause <xerofoify@gmail.com>
Date:   Tue Jun 30 21:37:21 2015 -0400

    f2fs: make the function check_dnode have a return type of bool and change it's name to is_alive
    
    This makes the function check_dnode have a return type of bool
    due to this particular function only ever returning either one
    or zero as its return value and changes the name of the function
    to is_alive in order to better explain this function's intended
    work of checking if a dnode is still in use by the filesystem.
    
    Signed-off-by: Nicholas Krause <xerofoify@gmail.com>
    [Jaegeuk Kim: change the return value check for the renamed function]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 22fb5ef37966..2701e05af991 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -487,7 +487,7 @@ block_t start_bidx_of_node(unsigned int node_ofs, struct f2fs_inode_info *fi)
 	return bidx * ADDRS_PER_BLOCK + ADDRS_PER_INODE(fi);
 }
 
-static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
+static bool is_alive(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		struct node_info *dni, block_t blkaddr, unsigned int *nofs)
 {
 	struct page *node_page;
@@ -500,13 +500,13 @@ static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 	node_page = get_node_page(sbi, nid);
 	if (IS_ERR(node_page))
-		return 0;
+		return false;
 
 	get_node_info(sbi, nid, dni);
 
 	if (sum->version != dni->version) {
 		f2fs_put_page(node_page, 1);
-		return 0;
+		return false;
 	}
 
 	*nofs = ofs_of_node(node_page);
@@ -514,8 +514,8 @@ static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	f2fs_put_page(node_page, 1);
 
 	if (source_blkaddr != blkaddr)
-		return 0;
-	return 1;
+		return false;
+	return true;
 }
 
 static void move_encrypted_block(struct inode *inode, block_t bidx)
@@ -670,7 +670,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		}
 
 		/* Get an inode by ino with checking validity */
-		if (check_dnode(sbi, entry, &dni, start_addr + off, &nofs) == 0)
+		if (!is_alive(sbi, entry, &dni, start_addr + off, &nofs))
 			continue;
 
 		if (phase == 1) {

commit 6282adbf932c226f76e1b83e074448c79976fe75
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Jul 25 00:29:17 2015 -0700

    f2fs: call set_page_dirty to attach i_wb for cgroup
    
    The cgroup attaches inode->i_wb via mark_inode_dirty and when set_page_writeback
    is called, __inc_wb_stat() updates i_wb's stat.
    
    So, we need to explicitly call set_page_dirty->__mark_inode_dirty in prior to
    any writebacking pages.
    
    This patch should resolve the following kernel panic reported by Andreas Reis.
    
    https://bugzilla.kernel.org/show_bug.cgi?id=101801
    
    --- Comment #2 from Andreas Reis <andreas.reis@gmail.com> ---
    BUG: unable to handle kernel NULL pointer dereference at 00000000000000a8
    IP: [<ffffffff8149deea>] __percpu_counter_add+0x1a/0x90
    PGD 2951ff067 PUD 2df43f067 PMD 0
    Oops: 0000 [#1] PREEMPT SMP
    Modules linked in:
    CPU: 7 PID: 10356 Comm: gcc Tainted: G        W       4.2.0-1-cu #1
    Hardware name: Gigabyte Technology Co., Ltd. G1.Sniper M5/G1.Sniper M5, BIOS
    T01 02/03/2015
    task: ffff880295044f80 ti: ffff880295140000 task.ti: ffff880295140000
    RIP: 0010:[<ffffffff8149deea>]  [<ffffffff8149deea>]
    __percpu_counter_add+0x1a/0x90
    RSP: 0018:ffff880295143ac8  EFLAGS: 00010082
    RAX: 0000000000000003 RBX: ffffea000a526d40 RCX: 0000000000000001
    RDX: 0000000000000020 RSI: 0000000000000001 RDI: 0000000000000088
    RBP: ffff880295143ae8 R08: 0000000000000000 R09: ffff88008f69bb30
    R10: 00000000fffffffa R11: 0000000000000000 R12: 0000000000000088
    R13: 0000000000000001 R14: ffff88041d099000 R15: ffff880084a205d0
    FS:  00007f8549374700(0000) GS:ffff88042f3c0000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00000000000000a8 CR3: 000000033e1d5000 CR4: 00000000001406e0
    DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
    DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
    Stack:
     0000000000000000 ffffea000a526d40 ffff880084a20738 ffff880084a20750
     ffff880295143b48 ffffffff811cc91e ffff880000000000 0000000000000296
     0000000000000000 ffff880417090198 0000000000000000 ffffea000a526d40
    Call Trace:
     [<ffffffff811cc91e>] __test_set_page_writeback+0xde/0x1d0
     [<ffffffff813fee87>] do_write_data_page+0xe7/0x3a0
     [<ffffffff813faeea>] gc_data_segment+0x5aa/0x640
     [<ffffffff813fb0b8>] do_garbage_collect+0x138/0x150
     [<ffffffff813fb3fe>] f2fs_gc+0x1be/0x3e0
     [<ffffffff81405541>] f2fs_balance_fs+0x81/0x90
     [<ffffffff813ee357>] f2fs_unlink+0x47/0x1d0
     [<ffffffff81239329>] vfs_unlink+0x109/0x1b0
     [<ffffffff8123e3d7>] do_unlinkat+0x287/0x2c0
     [<ffffffff8123ebc6>] SyS_unlink+0x16/0x20
     [<ffffffff81942e2e>] entry_SYSCALL_64_fastpath+0x12/0x71
    Code: 41 5e 5d c3 0f 1f 00 66 2e 0f 1f 84 00 00 00 00 00 55 48 89 e5 41 55 49
    89 f5 41 54 49 89 fc 53 48 83 ec 08 65 ff 05 e6 d9 b6 7e <48> 8b 47 20 48 63 ca
    65 8b 18 48 63 db 48 01 f3 48 39 cb 7d 0a
    RIP  [<ffffffff8149deea>] __percpu_counter_add+0x1a/0x90
     RSP <ffff880295143ac8>
    CR2: 00000000000000a8
    ---[ end trace 5132449a58ed93a3 ]---
    note: gcc[10356] exited with preempt_count 2
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 883a841dfc6d..22fb5ef37966 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -568,6 +568,11 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	if (unlikely(fio.encrypted_page->mapping != META_MAPPING(fio.sbi)))
 		goto put_page_out;
 
+	set_page_dirty(fio.encrypted_page);
+	f2fs_wait_on_page_writeback(fio.encrypted_page, META);
+	if (clear_page_dirty_for_io(fio.encrypted_page))
+		dec_page_count(fio.sbi, F2FS_DIRTY_META);
+
 	set_page_writeback(fio.encrypted_page);
 
 	/* allocate block address */
@@ -612,8 +617,8 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 			.page = page,
 			.encrypted_page = NULL,
 		};
+		set_page_dirty(page);
 		f2fs_wait_on_page_writeback(page, DATA);
-
 		if (clear_page_dirty_for_io(page))
 			inode_dec_dirty_pages(inode);
 		set_cold_data(page);

commit 548aedac5128e4bb256bab5c8e1db88e626697fc
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jul 13 17:44:14 2015 -0700

    f2fs: handle error cases in move_encrypted_block
    
    This patch fixes some missing error handlers.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e1e73617d13b..883a841dfc6d 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -556,27 +556,34 @@ static void move_encrypted_block(struct inode *inode, block_t bidx)
 	if (!fio.encrypted_page)
 		goto put_out;
 
-	f2fs_submit_page_bio(&fio);
+	err = f2fs_submit_page_bio(&fio);
+	if (err)
+		goto put_page_out;
+
+	/* write page */
+	lock_page(fio.encrypted_page);
+
+	if (unlikely(!PageUptodate(fio.encrypted_page)))
+		goto put_page_out;
+	if (unlikely(fio.encrypted_page->mapping != META_MAPPING(fio.sbi)))
+		goto put_page_out;
+
+	set_page_writeback(fio.encrypted_page);
 
 	/* allocate block address */
 	f2fs_wait_on_page_writeback(dn.node_page, NODE);
-
 	allocate_data_block(fio.sbi, NULL, fio.blk_addr,
 					&fio.blk_addr, &sum, CURSEG_COLD_DATA);
-	dn.data_blkaddr = fio.blk_addr;
-
-	/* write page */
-	lock_page(fio.encrypted_page);
-	set_page_writeback(fio.encrypted_page);
 	fio.rw = WRITE_SYNC;
 	f2fs_submit_page_mbio(&fio);
 
+	dn.data_blkaddr = fio.blk_addr;
 	set_data_blkaddr(&dn);
 	f2fs_update_extent_cache(&dn);
 	set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
 	if (page->index == 0)
 		set_inode_flag(F2FS_I(inode), FI_FIRST_BLOCK_WRITTEN);
-
+put_page_out:
 	f2fs_put_page(fio.encrypted_page, 1);
 put_out:
 	f2fs_put_dnode(&dn);

commit 9236cac5666ea8b3a3b92b132a046c200b99dca8
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu May 28 18:19:17 2015 -0700

    f2fs: fix a deadlock for summary page lock vs. sentry_lock
    
    In f2fs_gc:                      In f2fs_replace_block:
     - lock_page(sum_page)
      - check_valid_map()            - mutex_lock(sentry_lock)
       - mutex_lock(sentry_lock)     - change_curseg()
                                      - lock_page(sum_page)
    
    This patch fixes the deadlock condition.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 43354cb3ce94..e1e73617d13b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -750,6 +750,15 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 
 	sum = page_address(sum_page);
 
+	/*
+	 * this is to avoid deadlock:
+	 * - lock_page(sum_page)         - f2fs_replace_block
+	 *  - check_valid_map()            - mutex_lock(sentry_lock)
+	 *   - mutex_lock(sentry_lock)     - change_curseg()
+	 *                                  - lock_page(sum_page)
+	 */
+	unlock_page(sum_page);
+
 	switch (GET_SUM_TYPE((&sum->footer))) {
 	case SUM_TYPE_NODE:
 		gc_node_segment(sbi, sum->entries, segno, gc_type);
@@ -763,7 +772,7 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 	stat_inc_seg_count(sbi, GET_SUM_TYPE((&sum->footer)), gc_type);
 	stat_inc_call_count(sbi->stat_info);
 
-	f2fs_put_page(sum_page, 1);
+	f2fs_put_page(sum_page, 0);
 }
 
 int f2fs_gc(struct f2fs_sb_info *sbi)

commit 4375a33664de17af9032b5f491a49bd256670927
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 23 12:04:33 2015 -0700

    f2fs crypto: add encryption support in read/write paths
    
    This patch adds encryption support in read and write paths.
    
    Note that, in f2fs, we need to consider cleaning operation.
    In cleaning procedure, we must avoid encrypting and decrypting written blocks.
    So, this patch implements move_encrypted_block().
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 2e2afebd9d0f..43354cb3ce94 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -518,6 +518,72 @@ static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	return 1;
 }
 
+static void move_encrypted_block(struct inode *inode, block_t bidx)
+{
+	struct f2fs_io_info fio = {
+		.sbi = F2FS_I_SB(inode),
+		.type = DATA,
+		.rw = READ_SYNC,
+		.encrypted_page = NULL,
+	};
+	struct dnode_of_data dn;
+	struct f2fs_summary sum;
+	struct node_info ni;
+	struct page *page;
+	int err;
+
+	/* do not read out */
+	page = grab_cache_page(inode->i_mapping, bidx);
+	if (!page)
+		return;
+
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	err = get_dnode_of_data(&dn, bidx, LOOKUP_NODE);
+	if (err)
+		goto out;
+
+	if (unlikely(dn.data_blkaddr == NULL_ADDR))
+		goto put_out;
+
+	get_node_info(fio.sbi, dn.nid, &ni);
+	set_summary(&sum, dn.nid, dn.ofs_in_node, ni.version);
+
+	/* read page */
+	fio.page = page;
+	fio.blk_addr = dn.data_blkaddr;
+
+	fio.encrypted_page = grab_cache_page(META_MAPPING(fio.sbi), fio.blk_addr);
+	if (!fio.encrypted_page)
+		goto put_out;
+
+	f2fs_submit_page_bio(&fio);
+
+	/* allocate block address */
+	f2fs_wait_on_page_writeback(dn.node_page, NODE);
+
+	allocate_data_block(fio.sbi, NULL, fio.blk_addr,
+					&fio.blk_addr, &sum, CURSEG_COLD_DATA);
+	dn.data_blkaddr = fio.blk_addr;
+
+	/* write page */
+	lock_page(fio.encrypted_page);
+	set_page_writeback(fio.encrypted_page);
+	fio.rw = WRITE_SYNC;
+	f2fs_submit_page_mbio(&fio);
+
+	set_data_blkaddr(&dn);
+	f2fs_update_extent_cache(&dn);
+	set_inode_flag(F2FS_I(inode), FI_APPEND_WRITE);
+	if (page->index == 0)
+		set_inode_flag(F2FS_I(inode), FI_FIRST_BLOCK_WRITTEN);
+
+	f2fs_put_page(fio.encrypted_page, 1);
+put_out:
+	f2fs_put_dnode(&dn);
+out:
+	f2fs_put_page(page, 1);
+}
+
 static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 {
 	struct page *page;
@@ -537,6 +603,7 @@ static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 			.type = DATA,
 			.rw = WRITE_SYNC,
 			.page = page,
+			.encrypted_page = NULL,
 		};
 		f2fs_wait_on_page_writeback(page, DATA);
 
@@ -606,6 +673,13 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			if (IS_ERR(inode) || is_bad_inode(inode))
 				continue;
 
+			/* if encrypted inode, let's go phase 3 */
+			if (f2fs_encrypted_inode(inode) &&
+						S_ISREG(inode->i_mode)) {
+				add_gc_inode(gc_list, inode);
+				continue;
+			}
+
 			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode));
 			data_page = get_read_data_page(inode,
 					start_bidx + ofs_in_node, READA);
@@ -624,7 +698,10 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		if (inode) {
 			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode))
 								+ ofs_in_node;
-			move_data_page(inode, start_bidx, gc_type);
+			if (f2fs_encrypted_inode(inode) && S_ISREG(inode->i_mode))
+				move_encrypted_block(inode, start_bidx);
+			else
+				move_data_page(inode, start_bidx, gc_type);
 			stat_inc_data_blk_count(sbi, 1, gc_type);
 		}
 	}

commit 43f3eae1d3b1de6a4f7e39ef9c363ec6f8b9c8d4
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 30 17:00:33 2015 -0700

    f2fs: split find_data_page according to specific purposes
    
    This patch splits find_data_page as follows.
    
    1. f2fs_gc
     - use get_read_data_page() with read only
    
    2. find_in_level
     - use find_data_page without locked page
    
    3. truncate_partial_page
     - In the case cache_only mode, just drop cached page.
     - Ohterwise, use get_lock_data_page() and guarantee to truncate
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 1bd11f017a23..2e2afebd9d0f 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -607,9 +607,8 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 				continue;
 
 			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode));
-
-			data_page = find_data_page(inode,
-					start_bidx + ofs_in_node, false);
+			data_page = get_read_data_page(inode,
+					start_bidx + ofs_in_node, READA);
 			if (IS_ERR(data_page)) {
 				iput(inode);
 				continue;

commit c879f90da96c6369080be868162ef96aeef4a439
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Apr 24 14:34:30 2015 -0700

    f2fs: move get_page for gc victims
    
    This patch moves getting victim page into move_data_page.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 72667a54ac5f..1bd11f017a23 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -518,14 +518,13 @@ static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	return 1;
 }
 
-static void move_data_page(struct inode *inode, struct page *page, int gc_type)
+static void move_data_page(struct inode *inode, block_t bidx, int gc_type)
 {
-	struct f2fs_io_info fio = {
-		.sbi = F2FS_I_SB(inode),
-		.type = DATA,
-		.rw = WRITE_SYNC,
-		.page = page,
-	};
+	struct page *page;
+
+	page = get_lock_data_page(inode, bidx);
+	if (IS_ERR(page))
+		return;
 
 	if (gc_type == BG_GC) {
 		if (PageWriteback(page))
@@ -533,6 +532,12 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 		set_page_dirty(page);
 		set_cold_data(page);
 	} else {
+		struct f2fs_io_info fio = {
+			.sbi = F2FS_I_SB(inode),
+			.type = DATA,
+			.rw = WRITE_SYNC,
+			.page = page,
+		};
 		f2fs_wait_on_page_writeback(page, DATA);
 
 		if (clear_page_dirty_for_io(page))
@@ -618,12 +623,9 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		/* phase 3 */
 		inode = find_gc_inode(gc_list, dni.ino);
 		if (inode) {
-			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode));
-			data_page = get_lock_data_page(inode,
-						start_bidx + ofs_in_node);
-			if (IS_ERR(data_page))
-				continue;
-			move_data_page(inode, data_page, gc_type);
+			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode))
+								+ ofs_in_node;
+			move_data_page(inode, start_bidx, gc_type);
 			stat_inc_data_blk_count(sbi, 1, gc_type);
 		}
 	}

commit 05ca3632e5a73b493b27ec3e2a337885563abff0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 23 14:38:15 2015 -0700

    f2fs: add sbi and page pointer in f2fs_io_info
    
    This patch adds f2fs_sb_info and page pointers in f2fs_io_info structure.
    With this change, we can reduce a lot of parameters for IO functions.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index ed58211fe79b..72667a54ac5f 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -521,8 +521,10 @@ static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 {
 	struct f2fs_io_info fio = {
+		.sbi = F2FS_I_SB(inode),
 		.type = DATA,
 		.rw = WRITE_SYNC,
+		.page = page,
 	};
 
 	if (gc_type == BG_GC) {
@@ -536,7 +538,7 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 		if (clear_page_dirty_for_io(page))
 			inode_dec_dirty_pages(inode);
 		set_cold_data(page);
-		do_write_data_page(page, &fio);
+		do_write_data_page(&fio);
 		clear_cold_data(page);
 	}
 out:

commit e1235983e385afafb33bab3578bfc83a7d871ce1
Author: Changman Lee <cm224.lee@samsung.com>
Date:   Tue Dec 23 08:37:39 2014 +0900

    f2fs: add stat info for moved blocks by background gc
    
    This patch is for looking into gc performance of f2fs in detail.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    [Jaegeuk Kim: fix build errors]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 76adbc3641f1..ed58211fe79b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -435,7 +435,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 				set_page_dirty(node_page);
 		}
 		f2fs_put_page(node_page, 1);
-		stat_inc_node_blk_count(sbi, 1);
+		stat_inc_node_blk_count(sbi, 1, gc_type);
 	}
 
 	if (initial) {
@@ -622,7 +622,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			if (IS_ERR(data_page))
 				continue;
 			move_data_page(inode, data_page, gc_type);
-			stat_inc_data_blk_count(sbi, 1);
+			stat_inc_data_blk_count(sbi, 1, gc_type);
 		}
 	}
 
@@ -680,7 +680,7 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 	}
 	blk_finish_plug(&plug);
 
-	stat_inc_seg_count(sbi, GET_SUM_TYPE((&sum->footer)));
+	stat_inc_seg_count(sbi, GET_SUM_TYPE((&sum->footer)), gc_type);
 	stat_inc_call_count(sbi->stat_info);
 
 	f2fs_put_page(sum_page, 1);

commit 119ee9144534141822462e3e8a5ccc8dc537f712
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jan 29 11:45:33 2015 -0800

    f2fs: split UMOUNT and FASTBOOT flags
    
    This patch adds FASTBOOT flag into checkpoint as follows.
    
     - CP_UMOUNT_FLAG is set when system is umounted.
     - CP_FASTBOOT_FLAG is set when intermediate checkpoint having node summaries
       was done.
    
    So, if you get CP_UMOUNT_FLAG from checkpoint, the system was umounted cleanly.
    Instead, if there was sudden-power-off, you can get CP_FASTBOOT_FLAG or nothing.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index ba89e27f394f..76adbc3641f1 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -698,8 +698,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 		.iroot = RADIX_TREE_INIT(GFP_NOFS),
 	};
 
-	cpc.reason = test_opt(sbi, FASTBOOT) ? CP_UMOUNT : CP_SYNC;
-
+	cpc.reason = __get_cp_reason(sbi);
 gc_more:
 	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE)))
 		goto stop;

commit 88dd8934194f6d1db7f824c03d1eee169cb891b0
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Jan 26 20:24:21 2015 +0800

    f2fs: clean up {in,de}create_sleep_time
    
    Use pointer parameter @wait to pass result in {in,de}create_sleep_time for
    cleanup.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 67860b6712f3..ba89e27f394f 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -44,7 +44,7 @@ static int gc_thread_func(void *data)
 			break;
 
 		if (sbi->sb->s_writers.frozen >= SB_FREEZE_WRITE) {
-			wait_ms = increase_sleep_time(gc_th, wait_ms);
+			increase_sleep_time(gc_th, &wait_ms);
 			continue;
 		}
 
@@ -65,15 +65,15 @@ static int gc_thread_func(void *data)
 			continue;
 
 		if (!is_idle(sbi)) {
-			wait_ms = increase_sleep_time(gc_th, wait_ms);
+			increase_sleep_time(gc_th, &wait_ms);
 			mutex_unlock(&sbi->gc_mutex);
 			continue;
 		}
 
 		if (has_enough_invalid_blocks(sbi))
-			wait_ms = decrease_sleep_time(gc_th, wait_ms);
+			decrease_sleep_time(gc_th, &wait_ms);
 		else
-			wait_ms = increase_sleep_time(gc_th, wait_ms);
+			increase_sleep_time(gc_th, &wait_ms);
 
 		stat_inc_bggc_count(sbi);
 

commit f28e503429644a794d9bf2793189ff0b0887eb21
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Jan 23 20:37:53 2015 +0800

    f2fs: use f2fs_radix_tree_insert to clean codes
    
    No modification in functionality, just clean codes with f2fs_radix_tree_insert.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 40887d3c9d01..67860b6712f3 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -356,11 +356,8 @@ static void add_gc_inode(struct gc_inode_list *gc_list, struct inode *inode)
 	}
 	new_ie = f2fs_kmem_cache_alloc(inode_entry_slab, GFP_NOFS);
 	new_ie->inode = inode;
-retry:
-	if (radix_tree_insert(&gc_list->iroot, inode->i_ino, new_ie)) {
-		cond_resched();
-		goto retry;
-	}
+
+	f2fs_radix_tree_insert(&gc_list->iroot, inode->i_ino, new_ie);
 	list_add_tail(&new_ie->list, &gc_list->ilist);
 }
 

commit 062920734c0de9dd4f0a9bdc36fdcabc2751eb34
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Dec 29 15:56:18 2014 +0800

    f2fs: reuse inode_entry_slab in gc procedure for using slab more effectively
    
    There are two slab cache inode_entry_slab and winode_slab using the same
    structure as below:
    
    struct dir_inode_entry {
            struct list_head list;  /* list head */
            struct inode *inode;    /* vfs inode pointer */
    };
    
    struct inode_entry {
            struct list_head list;
            struct inode *inode;
    };
    
    It's a little waste that the two cache can not share their memory space for each
    other.
    So in this patch we remove one redundant winode_slab slab cache, then use more
    universal name struct inode_entry as remaining data structure name of slab,
    finally we reuse the inode_entry_slab to store dirty dir item and gc item for
    more effective.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index eec0933a4819..40887d3c9d01 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -24,8 +24,6 @@
 #include "gc.h"
 #include <trace/events/f2fs.h>
 
-static struct kmem_cache *winode_slab;
-
 static int gc_thread_func(void *data)
 {
 	struct f2fs_sb_info *sbi = data;
@@ -356,7 +354,7 @@ static void add_gc_inode(struct gc_inode_list *gc_list, struct inode *inode)
 		iput(inode);
 		return;
 	}
-	new_ie = f2fs_kmem_cache_alloc(winode_slab, GFP_NOFS);
+	new_ie = f2fs_kmem_cache_alloc(inode_entry_slab, GFP_NOFS);
 	new_ie->inode = inode;
 retry:
 	if (radix_tree_insert(&gc_list->iroot, inode->i_ino, new_ie)) {
@@ -373,7 +371,7 @@ static void put_gc_inode(struct gc_inode_list *gc_list)
 		radix_tree_delete(&gc_list->iroot, ie->inode->i_ino);
 		iput(ie->inode);
 		list_del(&ie->list);
-		kmem_cache_free(winode_slab, ie);
+		kmem_cache_free(inode_entry_slab, ie);
 	}
 }
 
@@ -750,17 +748,3 @@ void build_gc_manager(struct f2fs_sb_info *sbi)
 {
 	DIRTY_I(sbi)->v_ops = &default_v_ops;
 }
-
-int __init create_gc_caches(void)
-{
-	winode_slab = f2fs_kmem_cache_create("f2fs_gc_inodes",
-			sizeof(struct inode_entry));
-	if (!winode_slab)
-		return -ENOMEM;
-	return 0;
-}
-
-void destroy_gc_caches(void)
-{
-	kmem_cache_destroy(winode_slab);
-}

commit 9be32d72becca41d7d9b010d7d9be1d39489414f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Dec 5 10:39:49 2014 -0800

    f2fs: do retry operations with cond_resched
    
    This patch revists retrial paths in f2fs.
    The basic idea is to use cond_resched instead of retrying from the very early
    stage.
    
    Suggested-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 2c58c587a3c6..eec0933a4819 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -356,12 +356,11 @@ static void add_gc_inode(struct gc_inode_list *gc_list, struct inode *inode)
 		iput(inode);
 		return;
 	}
-retry:
 	new_ie = f2fs_kmem_cache_alloc(winode_slab, GFP_NOFS);
 	new_ie->inode = inode;
-
+retry:
 	if (radix_tree_insert(&gc_list->iroot, inode->i_ino, new_ie)) {
-		kmem_cache_free(winode_slab, new_ie);
+		cond_resched();
 		goto retry;
 	}
 	list_add_tail(&new_ie->list, &gc_list->ilist);

commit 769ec6e5b7d4a8115447736871be8bffaaba3a7d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Dec 3 20:47:26 2014 -0800

    f2fs: call radix_tree_preload before radix_tree_insert
    
    This patch tries to fix:
    
     BUG: using smp_processor_id() in preemptible [00000000] code: f2fs_gc-254:0/384
      (radix_tree_node_alloc+0x14/0x74) from [<c033d8a0>] (radix_tree_insert+0x110/0x200)
      (radix_tree_insert+0x110/0x200) from [<c02e8264>] (gc_data_segment+0x340/0x52c)
      (gc_data_segment+0x340/0x52c) from [<c02e8658>] (f2fs_gc+0x208/0x400)
      (f2fs_gc+0x208/0x400) from [<c02e8a98>] (gc_thread_func+0x248/0x28c)
      (gc_thread_func+0x248/0x28c) from [<c0139944>] (kthread+0xa0/0xac)
      (kthread+0xa0/0xac) from [<c0105ef8>] (ret_from_fork+0x14/0x3c)
    
    The reason is that f2fs calls radix_tree_insert under enabled preemption.
    So, before calling it, we need to call radix_tree_preload.
    
    Otherwise, we should use _GFP_WAIT for the radix tree, and use mutex or
    semaphore to cover the radix tree operations.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index a1af74f1a1d9..2c58c587a3c6 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -351,7 +351,6 @@ static struct inode *find_gc_inode(struct gc_inode_list *gc_list, nid_t ino)
 static void add_gc_inode(struct gc_inode_list *gc_list, struct inode *inode)
 {
 	struct inode_entry *new_ie;
-	int ret;
 
 	if (inode == find_gc_inode(gc_list, inode->i_ino)) {
 		iput(inode);
@@ -361,8 +360,7 @@ static void add_gc_inode(struct gc_inode_list *gc_list, struct inode *inode)
 	new_ie = f2fs_kmem_cache_alloc(winode_slab, GFP_NOFS);
 	new_ie->inode = inode;
 
-	ret = radix_tree_insert(&gc_list->iroot, inode->i_ino, new_ie);
-	if (ret) {
+	if (radix_tree_insert(&gc_list->iroot, inode->i_ino, new_ie)) {
 		kmem_cache_free(winode_slab, new_ie);
 		goto retry;
 	}
@@ -703,7 +701,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 	struct cp_control cpc;
 	struct gc_inode_list gc_list = {
 		.ilist = LIST_HEAD_INIT(gc_list.ilist),
-		.iroot = RADIX_TREE_INIT(GFP_ATOMIC),
+		.iroot = RADIX_TREE_INIT(GFP_NOFS),
 	};
 
 	cpc.reason = test_opt(sbi, FASTBOOT) ? CP_UMOUNT : CP_SYNC;

commit 7dda2af83b2b7593458828d4f15443167b3da8c4
Author: Changman Lee <cm224.lee@samsung.com>
Date:   Fri Nov 28 15:49:40 2014 +0000

    f2fs: more fast lookup for gc_inode list
    
    If there are many inodes that have data blocks in victim segment,
    it takes long time to find a inode in gc_inode list.
    Let's use radix_tree to reduce lookup time.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 6acd5f240224..a1af74f1a1d9 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -338,34 +338,42 @@ static const struct victim_selection default_v_ops = {
 	.get_victim = get_victim_by_default,
 };
 
-static struct inode *find_gc_inode(nid_t ino, struct list_head *ilist)
+static struct inode *find_gc_inode(struct gc_inode_list *gc_list, nid_t ino)
 {
 	struct inode_entry *ie;
 
-	list_for_each_entry(ie, ilist, list)
-		if (ie->inode->i_ino == ino)
-			return ie->inode;
+	ie = radix_tree_lookup(&gc_list->iroot, ino);
+	if (ie)
+		return ie->inode;
 	return NULL;
 }
 
-static void add_gc_inode(struct inode *inode, struct list_head *ilist)
+static void add_gc_inode(struct gc_inode_list *gc_list, struct inode *inode)
 {
 	struct inode_entry *new_ie;
+	int ret;
 
-	if (inode == find_gc_inode(inode->i_ino, ilist)) {
+	if (inode == find_gc_inode(gc_list, inode->i_ino)) {
 		iput(inode);
 		return;
 	}
-
+retry:
 	new_ie = f2fs_kmem_cache_alloc(winode_slab, GFP_NOFS);
 	new_ie->inode = inode;
-	list_add_tail(&new_ie->list, ilist);
+
+	ret = radix_tree_insert(&gc_list->iroot, inode->i_ino, new_ie);
+	if (ret) {
+		kmem_cache_free(winode_slab, new_ie);
+		goto retry;
+	}
+	list_add_tail(&new_ie->list, &gc_list->ilist);
 }
 
-static void put_gc_inode(struct list_head *ilist)
+static void put_gc_inode(struct gc_inode_list *gc_list)
 {
 	struct inode_entry *ie, *next_ie;
-	list_for_each_entry_safe(ie, next_ie, ilist, list) {
+	list_for_each_entry_safe(ie, next_ie, &gc_list->ilist, list) {
+		radix_tree_delete(&gc_list->iroot, ie->inode->i_ino);
 		iput(ie->inode);
 		list_del(&ie->list);
 		kmem_cache_free(winode_slab, ie);
@@ -551,7 +559,7 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
  * the victim data block is ignored.
  */
 static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
-		struct list_head *ilist, unsigned int segno, int gc_type)
+		struct gc_inode_list *gc_list, unsigned int segno, int gc_type)
 {
 	struct super_block *sb = sbi->sb;
 	struct f2fs_summary *entry;
@@ -609,12 +617,12 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			}
 
 			f2fs_put_page(data_page, 0);
-			add_gc_inode(inode, ilist);
+			add_gc_inode(gc_list, inode);
 			continue;
 		}
 
 		/* phase 3 */
-		inode = find_gc_inode(dni.ino, ilist);
+		inode = find_gc_inode(gc_list, dni.ino);
 		if (inode) {
 			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode));
 			data_page = get_lock_data_page(inode,
@@ -657,7 +665,7 @@ static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
 }
 
 static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
-				struct list_head *ilist, int gc_type)
+				struct gc_inode_list *gc_list, int gc_type)
 {
 	struct page *sum_page;
 	struct f2fs_summary_block *sum;
@@ -675,7 +683,7 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 		gc_node_segment(sbi, sum->entries, segno, gc_type);
 		break;
 	case SUM_TYPE_DATA:
-		gc_data_segment(sbi, sum->entries, ilist, segno, gc_type);
+		gc_data_segment(sbi, sum->entries, gc_list, segno, gc_type);
 		break;
 	}
 	blk_finish_plug(&plug);
@@ -688,16 +696,18 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 
 int f2fs_gc(struct f2fs_sb_info *sbi)
 {
-	struct list_head ilist;
 	unsigned int segno, i;
 	int gc_type = BG_GC;
 	int nfree = 0;
 	int ret = -1;
 	struct cp_control cpc;
+	struct gc_inode_list gc_list = {
+		.ilist = LIST_HEAD_INIT(gc_list.ilist),
+		.iroot = RADIX_TREE_INIT(GFP_ATOMIC),
+	};
 
 	cpc.reason = test_opt(sbi, FASTBOOT) ? CP_UMOUNT : CP_SYNC;
 
-	INIT_LIST_HEAD(&ilist);
 gc_more:
 	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE)))
 		goto stop;
@@ -719,7 +729,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 								META_SSA);
 
 	for (i = 0; i < sbi->segs_per_sec; i++)
-		do_garbage_collect(sbi, segno + i, &ilist, gc_type);
+		do_garbage_collect(sbi, segno + i, &gc_list, gc_type);
 
 	if (gc_type == FG_GC) {
 		sbi->cur_victim_sec = NULL_SEGNO;
@@ -735,7 +745,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 stop:
 	mutex_unlock(&sbi->gc_mutex);
 
-	put_gc_inode(&ilist);
+	put_gc_inode(&gc_list);
 	return ret;
 }
 

commit 31a3268839c1aa808a5109111ec847b95e1bb114
Author: Changman Lee <cm224.lee@samsung.com>
Date:   Thu Nov 27 16:03:08 2014 +0900

    f2fs: cleanup if-statement of phase in gc_data_segment
    
    Little cleanup to distinguish each phase easily
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    [Jaegeuk Kim: modify indentation for code readability]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 657683c9ee48..6acd5f240224 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -603,27 +603,27 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 			data_page = find_data_page(inode,
 					start_bidx + ofs_in_node, false);
-			if (IS_ERR(data_page))
-				goto next_iput;
+			if (IS_ERR(data_page)) {
+				iput(inode);
+				continue;
+			}
 
 			f2fs_put_page(data_page, 0);
 			add_gc_inode(inode, ilist);
-		} else {
-			inode = find_gc_inode(dni.ino, ilist);
-			if (inode) {
-				start_bidx = start_bidx_of_node(nofs,
-								F2FS_I(inode));
-				data_page = get_lock_data_page(inode,
+			continue;
+		}
+
+		/* phase 3 */
+		inode = find_gc_inode(dni.ino, ilist);
+		if (inode) {
+			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode));
+			data_page = get_lock_data_page(inode,
 						start_bidx + ofs_in_node);
-				if (IS_ERR(data_page))
-					continue;
-				move_data_page(inode, data_page, gc_type);
-				stat_inc_data_blk_count(sbi, 1);
-			}
+			if (IS_ERR(data_page))
+				continue;
+			move_data_page(inode, data_page, gc_type);
+			stat_inc_data_blk_count(sbi, 1);
 		}
-		continue;
-next_iput:
-		iput(inode);
 	}
 
 	if (++phase < 4)

commit 6c0299320318c8154a20a3d9e73cbd1fc58d96e1
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Nov 18 11:16:01 2014 +0800

    f2fs: avoid unable to restart gc thread in remount
    
    In f2fs_remount, we will stop gc thread and set need_restart_gc as true when new
    option is set without BG_GC, then if any error occurred in the following
    procedure, we can restore to start the gc thread.
    But after that, We will fail to restore gc thread in start_gc_thread as BG_GC is
    not set in new option, so we'd better move this condition judgment out of
    start_gc_thread to fix this issue.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b197a2f2993a..657683c9ee48 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -96,8 +96,6 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 	dev_t dev = sbi->sb->s_bdev->bd_dev;
 	int err = 0;
 
-	if (!test_opt(sbi, BG_GC))
-		goto out;
 	gc_th = kmalloc(sizeof(struct f2fs_gc_kthread), GFP_KERNEL);
 	if (!gc_th) {
 		err = -ENOMEM;

commit d5053a34a9cc797b9d5d77574354b5555848c43c
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Oct 30 22:47:03 2014 -0700

    f2fs: introduce -o fastboot for reducing booting time only
    
    If a system wants to reduce the booting time as a top priority, now we can
    use a mount option, -o fastboot.
    With this option, f2fs conducts a little bit slow write_checkpoint, but
    it can avoid the node page reads during the next mount time.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 7151d7de7d95..b197a2f2993a 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -695,9 +695,9 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 	int gc_type = BG_GC;
 	int nfree = 0;
 	int ret = -1;
-	struct cp_control cpc = {
-		.reason = CP_SYNC,
-	};
+	struct cp_control cpc;
+
+	cpc.reason = test_opt(sbi, FASTBOOT) ? CP_UMOUNT : CP_SYNC;
 
 	INIT_LIST_HEAD(&ilist);
 gc_more:

commit 8a2d0ace3af74825e4f1e6fb962f7ee8ef4d9281
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Mon Oct 20 17:45:48 2014 +0800

    f2fs: remove the seems unneeded argument 'type' from __get_victim
    
    Remove the unneeded argument 'type' from __get_victim, use
    NO_CHECK_TYPE directly when calling v_ops->get_victim().
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 2a8f4acdb86b..7151d7de7d95 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -646,12 +646,14 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 }
 
 static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
-						int gc_type, int type)
+			int gc_type)
 {
 	struct sit_info *sit_i = SIT_I(sbi);
 	int ret;
+
 	mutex_lock(&sit_i->sentry_lock);
-	ret = DIRTY_I(sbi)->v_ops->get_victim(sbi, victim, gc_type, type, LFS);
+	ret = DIRTY_I(sbi)->v_ops->get_victim(sbi, victim, gc_type,
+					      NO_CHECK_TYPE, LFS);
 	mutex_unlock(&sit_i->sentry_lock);
 	return ret;
 }
@@ -709,7 +711,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 		write_checkpoint(sbi, &cpc);
 	}
 
-	if (!__get_victim(sbi, &segno, gc_type, NO_CHECK_TYPE))
+	if (!__get_victim(sbi, &segno, gc_type))
 		goto stop;
 	ret = 0;
 

commit 7cd8558baa4e4588a80ecb31cb30784195763cdd
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 23 11:23:01 2014 -0700

    f2fs: check the use of macros on block counts and addresses
    
    This patch cleans up the existing and new macros for readability.
    
    Rule is like this.
    
             ,-----------------------------------------> MAX_BLKADDR -,
             |  ,------------- TOTAL_BLKS ----------------------------,
             |  |                                                     |
             |  ,- seg0_blkaddr   ,----- sit/nat/ssa/main blkaddress  |
    block    |  | (SEG0_BLKADDR)  | | | |   (e.g., MAIN_BLKADDR)      |
    address  0..x................ a b c d .............................
                |                                                     |
    global seg# 0...................... m .............................
                |                       |                             |
                |                       `------- MAIN_SEGS -----------'
                `-------------- TOTAL_SEGS ---------------------------'
                                        |                             |
     seg#                               0..........xx..................
    
    = Note =
     o GET_SEGNO_FROM_SEG0 : blk address -> global segno
     o GET_SEGNO           : blk address -> segno
     o START_BLOCK         : segno -> starting block address
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e88fcf65aa7f..2a8f4acdb86b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -193,7 +193,7 @@ static unsigned int check_bg_victims(struct f2fs_sb_info *sbi)
 	 * selected by background GC before.
 	 * Those segments guarantee they have small valid blocks.
 	 */
-	for_each_set_bit(secno, dirty_i->victim_secmap, TOTAL_SECS(sbi)) {
+	for_each_set_bit(secno, dirty_i->victim_secmap, MAIN_SECS(sbi)) {
 		if (sec_usage_check(sbi, secno))
 			continue;
 		clear_bit(secno, dirty_i->victim_secmap);
@@ -281,9 +281,8 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 		unsigned long cost;
 		unsigned int segno;
 
-		segno = find_next_bit(p.dirty_segmap,
-						TOTAL_SEGS(sbi), p.offset);
-		if (segno >= TOTAL_SEGS(sbi)) {
+		segno = find_next_bit(p.dirty_segmap, MAIN_SEGS(sbi), p.offset);
+		if (segno >= MAIN_SEGS(sbi)) {
 			if (sbi->last_victim[p.gc_mode]) {
 				sbi->last_victim[p.gc_mode] = 0;
 				p.offset = 0;

commit 75ab4cb8301adb3a02a96c5c03c837ed941f1bc5
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Sep 20 21:57:51 2014 -0700

    f2fs: introduce cp_control structure
    
    This patch add a new data structure to control checkpoint parameters.
    Currently, it presents the reason of checkpoint such as is_umount and normal
    sync.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 7bf8392d555f..e88fcf65aa7f 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -694,6 +694,9 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 	int gc_type = BG_GC;
 	int nfree = 0;
 	int ret = -1;
+	struct cp_control cpc = {
+		.reason = CP_SYNC,
+	};
 
 	INIT_LIST_HEAD(&ilist);
 gc_more:
@@ -704,7 +707,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 
 	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, nfree)) {
 		gc_type = FG_GC;
-		write_checkpoint(sbi, false);
+		write_checkpoint(sbi, &cpc);
 	}
 
 	if (!__get_victim(sbi, &segno, gc_type, NO_CHECK_TYPE))
@@ -729,7 +732,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 		goto gc_more;
 
 	if (gc_type == FG_GC)
-		write_checkpoint(sbi, false);
+		write_checkpoint(sbi, &cpc);
 stop:
 	mutex_unlock(&sbi->gc_mutex);
 

commit 210f41bc048263d572515e1e0edc28d362ce673e
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Sep 15 18:05:44 2014 +0800

    f2fs: fix to search whole dirty segmap when get_victim
    
    In ->get_victim we get max_search value from dirty_i->nr_dirty without
    protection of seglist_lock, after that, nr_dirty can be increased/decreased
    before we hold seglist_lock lock.
    Then in main loop we attempt to traverse all dirty section one time to find
    victim section, but it's not accurate to use max_search as the total loop count,
    because we might lose checking several sections or check sections redundantly
    for the case of nr_dirty are increased or decreased previously.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index dca7818c0662..7bf8392d555f 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -263,14 +263,14 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	unsigned int secno, max_cost;
 	int nsearched = 0;
 
+	mutex_lock(&dirty_i->seglist_lock);
+
 	p.alloc_mode = alloc_mode;
 	select_policy(sbi, gc_type, type, &p);
 
 	p.min_segno = NULL_SEGNO;
 	p.min_cost = max_cost = get_max_cost(sbi, &p);
 
-	mutex_lock(&dirty_i->seglist_lock);
-
 	if (p.alloc_mode == LFS && gc_type == FG_GC) {
 		p.min_segno = check_bg_victims(sbi);
 		if (p.min_segno != NULL_SEGNO)

commit a7ffdbe22cecaed59b5d76a5f003d68907d64240
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Sep 12 15:53:45 2014 -0700

    f2fs: expand counting dirty pages in the inode page cache
    
    Previously f2fs only counts dirty dentry pages, but there is no reason not to
    expand the scope.
    
    This patch changes the names on the management of dirty pages and to count
    dirty pages in each inode info as well.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 075ea1eb8fa0..dca7818c0662 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -537,7 +537,7 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 		f2fs_wait_on_page_writeback(page, DATA);
 
 		if (clear_page_dirty_for_io(page))
-			inode_dec_dirty_dents(inode);
+			inode_dec_dirty_pages(inode);
 		set_cold_data(page);
 		do_write_data_page(page, &fio);
 		clear_cold_data(page);

commit 9a01b56b1a79e210e9d2d67c9b18906d31d536c4
Author: Huang Ying <ying.huang@intel.com>
Date:   Sun Sep 7 11:05:20 2014 +0800

    f2fs: avoid node page to be written twice in gc_node_segment
    
    In gc_node_segment, if node page gc is run concurrently with node page
    writeback, and check_valid_map and get_node_page run after page locked
    and before cur_valid_map is updated as below, it is possible for the
    page to be written twice unnecessarily.
    
                            sync_node_pages
                              try_lock_page
                              ...
    check_valid_map           f2fs_write_node_page
                                ...
                                write_node_page
                                  do_write_page
                                    allocate_data_block
                                      ...
                                      refresh_sit_entry /* update cur_valid_map */
                                      ...
                                ...
                                unlock_page
    get_node_page
    ...
    set_page_dirty
    ...
    f2fs_put_page
      unlock_page
    
    This can be solved via calling check_valid_map after get_node_page again.
    
    Signed-off-by: Huang, Ying <ying.huang@intel.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 943a31db7cc3..075ea1eb8fa0 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -423,6 +423,12 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 		if (IS_ERR(node_page))
 			continue;
 
+		/* block may become invalid during get_node_page */
+		if (check_valid_map(sbi, segno, off) == 0) {
+			f2fs_put_page(node_page, 1);
+			continue;
+		}
+
 		/* set page dirty and write it */
 		if (gc_type == FG_GC) {
 			f2fs_wait_on_page_writeback(node_page, NODE);

commit b73e52824c8920a5ff754e3c8ff68466a7dd61f9
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sat Aug 30 09:52:34 2014 +0800

    f2fs: reposition unlock_new_inode to prevent accessing invalid inode
    
    As the race condition on the inode cache, following scenario can appear:
    [Thread a]                              [Thread b]
                                            ->f2fs_mkdir
                                              ->f2fs_add_link
                                                ->__f2fs_add_link
                                                  ->init_inode_metadata failed here
    ->gc_thread_func
      ->f2fs_gc
        ->do_garbage_collect
          ->gc_data_segment
            ->f2fs_iget
              ->iget_locked
                ->wait_on_inode
                                              ->unlock_new_inode
            ->move_data_page
                                              ->make_bad_inode
                                              ->iput
    
    When we fail in create/symlink/mkdir/mknod/tmpfile, the new allocated inode
    should be set as bad to avoid being accessed by other thread. But in above
    scenario, it allows f2fs to access the invalid inode before this inode was set
    as bad.
    This patch fix the potential problem, and this issue was found by code review.
    
    change log from v1:
     o Add condition judgment in gc_data_segment() suggested by Changman Lee.
     o use iget_failed to simplify code.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e8507b1c8759..943a31db7cc3 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -593,7 +593,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 		if (phase == 2) {
 			inode = f2fs_iget(sb, dni.ino);
-			if (IS_ERR(inode))
+			if (IS_ERR(inode) || is_bad_inode(inode))
 				continue;
 
 			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode));

commit 1e968fdfe69e4060f05fa04059ecad93a0284e32
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Aug 11 16:49:25 2014 -0700

    f2fs: introduce f2fs_cp_error for readability
    
    This patch adds f2fs_cp_error for readability.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 87c50c507461..e8507b1c8759 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -693,7 +693,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 gc_more:
 	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE)))
 		goto stop;
-	if (unlikely(is_set_ckpt_flags(F2FS_CKPT(sbi), CP_ERROR_FLAG)))
+	if (unlikely(f2fs_cp_error(sbi)))
 		goto stop;
 
 	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, nfree)) {

commit e1c42045203071c4634b89e696037357810d3083
Author: arter97 <qkrwngud825@gmail.com>
Date:   Wed Aug 6 23:22:50 2014 +0900

    f2fs: fix typo
    
    Fix typo and some grammatical errors.
    
    The words "filesystem" and "readahead" are being used without the space treewide.
    
    Signed-off-by: Park Ju Hyung <qkrwngud825@gmail.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index d7947d90ccc3..87c50c507461 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -58,7 +58,7 @@ static int gc_thread_func(void *data)
 		 * 3. IO subsystem is idle by checking the # of requests in
 		 *    bdev's request list.
 		 *
-		 * Note) We have to avoid triggering GCs too much frequently.
+		 * Note) We have to avoid triggering GCs frequently.
 		 * Because it is possible that some segments can be
 		 * invalidated soon after by user update or deletion.
 		 * So, I'd like to wait some time to collect dirty segments.
@@ -222,7 +222,7 @@ static unsigned int get_cb_cost(struct f2fs_sb_info *sbi, unsigned int segno)
 
 	u = (vblocks * 100) >> sbi->log_blocks_per_seg;
 
-	/* Handle if the system time is changed by user */
+	/* Handle if the system time has changed by the user */
 	if (mtime < sit_i->min_mtime)
 		sit_i->min_mtime = mtime;
 	if (mtime > sit_i->max_mtime)

commit b65ee14818e67127aa242fe1dbd3711b9c095cc0
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Aug 4 10:10:07 2014 +0800

    f2fs: use for_each_set_bit to simplify the code
    
    This patch uses for_each_set_bit to simplify some codes in f2fs.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b90dbe55403a..d7947d90ccc3 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -186,7 +186,6 @@ static unsigned int get_max_cost(struct f2fs_sb_info *sbi,
 static unsigned int check_bg_victims(struct f2fs_sb_info *sbi)
 {
 	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
-	unsigned int hint = 0;
 	unsigned int secno;
 
 	/*
@@ -194,11 +193,9 @@ static unsigned int check_bg_victims(struct f2fs_sb_info *sbi)
 	 * selected by background GC before.
 	 * Those segments guarantee they have small valid blocks.
 	 */
-next:
-	secno = find_next_bit(dirty_i->victim_secmap, TOTAL_SECS(sbi), hint++);
-	if (secno < TOTAL_SECS(sbi)) {
+	for_each_set_bit(secno, dirty_i->victim_secmap, TOTAL_SECS(sbi)) {
 		if (sec_usage_check(sbi, secno))
-			goto next;
+			continue;
 		clear_bit(secno, dirty_i->victim_secmap);
 		return secno * sbi->segs_per_sec;
 	}

commit e8512d2e0c4eb38cd78b1499bb08d7d8eea6c723
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Fri Mar 7 18:43:28 2014 +0800

    f2fs: remove the unused ctor argument of f2fs_kmem_cache_create()
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index d94acbc3d928..b90dbe55403a 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -742,7 +742,7 @@ void build_gc_manager(struct f2fs_sb_info *sbi)
 int __init create_gc_caches(void)
 {
 	winode_slab = f2fs_kmem_cache_create("f2fs_gc_inodes",
-			sizeof(struct inode_entry), NULL);
+			sizeof(struct inode_entry));
 	if (!winode_slab)
 		return -ENOMEM;
 	return 0;

commit 81c1a0f13e6306a76fc3743b8504085d96659a5f
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Feb 27 19:12:24 2014 +0800

    f2fs: readahead contiguous SSA blocks for f2fs_gc
    
    If there are multi segments in one section, we will read those SSA blocks which
    have contiguous address one by one in f2fs_gc. It may lost performance, let's
    read ahead SSA blocks by merge multi read request.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b161db4a96a4..d94acbc3d928 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -708,6 +708,11 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 		goto stop;
 	ret = 0;
 
+	/* readahead multi ssa blocks those have contiguous address */
+	if (sbi->segs_per_sec > 1)
+		ra_meta_pages(sbi, GET_SUM_BLOCK(sbi, segno), sbi->segs_per_sec,
+								META_SSA);
+
 	for (i = 0; i < sbi->segs_per_sec; i++)
 		do_garbage_collect(sbi, segno + i, &ilist, gc_type);
 

commit 1fe54f9dd3acfaa3ed4e1d1e3278fd0f1d1e98cd
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Feb 7 10:00:06 2014 +0900

    f2fs: clean up redundant function call
    
    This patch integrates inode_[inc|dec]_dirty_dents with inc_page_count to remove
    redundant calls.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b0f57628fe55..b161db4a96a4 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -531,15 +531,10 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 		set_page_dirty(page);
 		set_cold_data(page);
 	} else {
-		struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
-
 		f2fs_wait_on_page_writeback(page, DATA);
 
-		if (clear_page_dirty_for_io(page) &&
-			S_ISDIR(inode->i_mode)) {
-			dec_page_count(sbi, F2FS_DIRTY_DENTS);
+		if (clear_page_dirty_for_io(page))
 			inode_dec_dirty_dents(inode);
-		}
 		set_cold_data(page);
 		do_write_data_page(page, &fio);
 		clear_cold_data(page);

commit 203681f65b07055259bd475a6281136615b4e9a4
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Feb 5 13:03:57 2014 +0900

    f2fs: fix f2fs_write_meta_page at no checkpoint status
    
    If f2fs entered errorneous checkpoint status, it should skip writing meta
    pages instead of redirtying the pages out.
    Otherwise, it cannot unmount the partition even though f2fs is under read-only
    status.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index ea0371e854b4..b0f57628fe55 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -701,6 +701,8 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 gc_more:
 	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE)))
 		goto stop;
+	if (unlikely(is_set_ckpt_flags(F2FS_CKPT(sbi), CP_ERROR_FLAG)))
+		goto stop;
 
 	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, nfree)) {
 		gc_type = FG_GC;

commit 5514f0aadddcdfaaaea697b60203f5402552eb7b
Author: Yuan Zhong <yuan.mark.zhong@samsung.com>
Date:   Fri Jan 10 07:26:14 2014 +0000

    f2fs: remove the needless parameter of f2fs_wait_on_page_writeback
    
    "boo sync" parameter is never referenced in f2fs_wait_on_page_writeback.
    We should remove this parameter.
    
    Signed-off-by: Yuan Zhong <yuan.mark.zhong@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 9117ccaf254a..ea0371e854b4 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -428,7 +428,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 
 		/* set page dirty and write it */
 		if (gc_type == FG_GC) {
-			f2fs_wait_on_page_writeback(node_page, NODE, true);
+			f2fs_wait_on_page_writeback(node_page, NODE);
 			set_page_dirty(node_page);
 		} else {
 			if (!PageWriteback(node_page))
@@ -533,7 +533,7 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 	} else {
 		struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 
-		f2fs_wait_on_page_writeback(page, DATA, true);
+		f2fs_wait_on_page_writeback(page, DATA);
 
 		if (clear_page_dirty_for_io(page) &&
 			S_ISDIR(inode->i_mode)) {

commit b1c57c1caa753cec299e62bb4272da0e85a01ef0
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Jan 8 13:45:08 2014 +0900

    f2fs: add a sysfs entry to control max_victim_search
    
    Previously during SSR and GC, the maximum number of retrials to find a victim
    segment was hard-coded by MAX_VICTIM_SEARCH, 4096 by default.
    
    This number makes an effect on IO locality, when SSR mode is activated, which
    results in performance fluctuation on some low-end devices.
    
    If max_victim_search = 4, the victim will be searched like below.
    ("D" represents a dirty segment, and "*" indicates a selected victim segment.)
    
     D1 D2 D3 D4 D5 D6 D7 D8 D9
    [   *       ]
          [   *    ]
                [         * ]
                            [ ....]
    
    This patch adds a sysfs entry to control the number dynamically through:
      /sys/fs/f2fs/$dev/max_victim_search
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 599f546d042c..9117ccaf254a 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -163,8 +163,8 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 		p->ofs_unit = sbi->segs_per_sec;
 	}
 
-	if (p->max_search > MAX_VICTIM_SEARCH)
-		p->max_search = MAX_VICTIM_SEARCH;
+	if (p->max_search > sbi->max_victim_search)
+		p->max_search = sbi->max_victim_search;
 
 	p->offset = sbi->last_victim[p->gc_mode];
 }

commit 7e8f23081ab3a11de90d7389f2c6fd44676c8df9
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Fri Dec 20 18:17:49 2013 +0800

    f2fs: remove the rw_flag domain from f2fs_io_info
    
    When using the f2fs_io_info in the low level, we still need to merge the
    rw and rw_flag, so use the rw to hold all the io flags directly,
    and remove the rw_flag field.
    
    ps.It is based on the previous patch:
    f2fs: move all the bio initialization into __bio_alloc
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 69c18e399014..599f546d042c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -523,7 +523,6 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 	struct f2fs_io_info fio = {
 		.type = DATA,
 		.rw = WRITE_SYNC,
-		.rw_flag = 0,
 	};
 
 	if (gc_type == BG_GC) {

commit 458e6197c37de53f7be0a837644daabb900c3036
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Dec 11 13:54:01 2013 +0900

    f2fs: refactor bio->rw handling
    
    This patch introduces f2fs_io_info to mitigate the complex parameter list.
    
    struct f2fs_io_info {
            enum page_type type;            /* contains DATA/NODE/META/META_FLUSH */
            int rw;                         /* contains R/RS/W/WS */
            int rw_flag;                    /* contains REQ_META/REQ_PRIO */
    }
    
    1. f2fs_write_data_pages
     - DATA
     - WRITE_SYNC is set when wbc->WB_SYNC_ALL.
    
    2. sync_node_pages
     - NODE
     - WRITE_SYNC all the time
    
    3. sync_meta_pages
     - META
     - WRITE_SYNC all the time
     - REQ_META | REQ_PRIO all the time
    
     ** f2fs_submit_merged_bio() handles META_FLUSH.
    
    4. ra_nat_pages, ra_sit_pages, ra_sum_pages
     - META
     - READ_SYNC
    
    Cc: Fan Li <fanofcode.li@samsung.com>
    Cc: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c68fba5ffadb..69c18e399014 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -520,8 +520,10 @@ static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 {
-	struct writeback_control wbc = {
-		.sync_mode = 1,
+	struct f2fs_io_info fio = {
+		.type = DATA,
+		.rw = WRITE_SYNC,
+		.rw_flag = 0,
 	};
 
 	if (gc_type == BG_GC) {
@@ -540,7 +542,7 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 			inode_dec_dirty_dents(inode);
 		}
 		set_cold_data(page);
-		do_write_data_page(page, &wbc);
+		do_write_data_page(page, &fio);
 		clear_cold_data(page);
 	}
 out:
@@ -634,7 +636,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		goto next_step;
 
 	if (gc_type == FG_GC) {
-		f2fs_submit_merged_bio(sbi, DATA, true, WRITE);
+		f2fs_submit_merged_bio(sbi, DATA, WRITE);
 
 		/*
 		 * In the case of FG_GC, it'd be better to reclaim this victim

commit 63a0b7cb33d85aeb0df39b984c08e234db4925d1
Author: Fan Li <fanofcode.li@samsung.com>
Date:   Mon Dec 9 16:09:00 2013 +0800

    f2fs: merge pages with the same sync_mode flag
    
    Previously f2fs submits most of write requests using WRITE_SYNC, but f2fs_write_data_pages
    submits last write requests by sync_mode flags callers pass.
    
    This causes a performance problem since continuous pages with different sync flags
    can't be merged in cfq IO scheduler(thanks yu chao for pointing it out), and synchronous
    requests often take more time.
    
    This patch makes the following modifies to DATA writebacks:
    
    1. every page will be written back using the sync mode caller pass.
    2. only pages with the same sync mode can be merged in one bio request.
    
    These changes are restricted to DATA pages.Other types of writebacks are modified
    To remain synchronous.
    
    In my test with tiotest, f2fs sequence write performance is improved by about 7%-10% ,
    and this patch has no obvious impact on other performance tests.
    
    Signed-off-by: Fan Li <fanofcode.li@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 29ceb9d71c8c..c68fba5ffadb 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -520,6 +520,10 @@ static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 {
+	struct writeback_control wbc = {
+		.sync_mode = 1,
+	};
+
 	if (gc_type == BG_GC) {
 		if (PageWriteback(page))
 			goto out;
@@ -536,7 +540,7 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 			inode_dec_dirty_dents(inode);
 		}
 		set_cold_data(page);
-		do_write_data_page(page);
+		do_write_data_page(page, &wbc);
 		clear_cold_data(page);
 	}
 out:

commit 6bacf52fb58aeb3e89d9a62970b85a5570aa8ace
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Dec 6 15:00:58 2013 +0900

    f2fs: add unlikely() macro for compiler more aggressively
    
    This patch adds unlikely() macro into the most of codes.
    The basic rule is to add that when:
    - checking unusual errors,
    - checking page mappings,
    - and the other unlikely conditions.
    
    Change log from v1:
     - Don't add unlikely for the NULL test and error test: advised by Andi Kleen.
    
    Cc: Chao Yu <chao2.yu@samsung.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 2886aef35d59..29ceb9d71c8c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -119,7 +119,6 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 		kfree(gc_th);
 		sbi->gc_thread = NULL;
 	}
-
 out:
 	return err;
 }
@@ -695,7 +694,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 
 	INIT_LIST_HEAD(&ilist);
 gc_more:
-	if (!(sbi->sb->s_flags & MS_ACTIVE))
+	if (unlikely(!(sbi->sb->s_flags & MS_ACTIVE)))
 		goto stop;
 
 	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, nfree)) {

commit 93dfe2ac516250755f7d5edd438b0ce67c0e3aa6
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Sat Nov 30 12:51:14 2013 +0900

    f2fs: refactor bio-related operations
    
    This patch integrates redundant bio operations on read and write IOs.
    
    1. Move bio-related codes to the top of data.c.
    2. Replace f2fs_submit_bio with f2fs_submit_merged_bio, which handles read
       bios additionally.
    3. Introduce __submit_merged_bio to submit the merged bio.
    4. Change f2fs_readpage to f2fs_submit_page_bio.
    5. Introduce f2fs_submit_page_mbio to integrate previous submit_read_page and
       submit_write_page.
    
    Reviewed-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Reviewed-by: Chao Yu <chao2.yu@samsung.com >
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 5fa54c1ca33b..2886aef35d59 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -631,7 +631,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		goto next_step;
 
 	if (gc_type == FG_GC) {
-		f2fs_submit_bio(sbi, DATA, true);
+		f2fs_submit_merged_bio(sbi, DATA, true, WRITE);
 
 		/*
 		 * In the case of FG_GC, it'd be better to reclaim this victim

commit 031fa8cc9ba45c14f440b9cf71d09950fbe5eb9b
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Nov 28 12:55:13 2013 +0900

    f2fs: remove unnecessary condition checks
    
    This patch removes the unnecessary condition checks on:
    
    fs/f2fs/gc.c:667 do_garbage_collect() warn: 'sum_page' isn't an ERR_PTR
    fs/f2fs/f2fs.h:795 f2fs_put_page() warn: 'page' isn't an ERR_PTR
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b7ad1ec7e4cc..5fa54c1ca33b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -664,8 +664,6 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 
 	/* read segment summary of victim */
 	sum_page = get_sum_page(sbi, segno);
-	if (IS_ERR(sum_page))
-		return;
 
 	blk_start_plug(&plug);
 

commit 4660f9c0fe484353b17a4b9d1cc2b036fa895f76
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Oct 24 14:19:18 2013 +0900

    f2fs: introduce f2fs_balance_fs_bg for some background jobs
    
    This patch merges some background jobs into this new function.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 783c6cc6253c..b7ad1ec7e4cc 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -83,9 +83,8 @@ static int gc_thread_func(void *data)
 		if (f2fs_gc(sbi))
 			wait_ms = gc_th->no_gc_sleep_time;
 
-		/* balancing prefree segments */
-		if (excess_prefree_segs(sbi))
-			f2fs_sync_fs(sbi->sb, true);
+		/* balancing f2fs's metadata periodically */
+		f2fs_balance_fs_bg(sbi);
 
 	} while (!kthread_should_stop());
 	return 0;

commit 81eb8d6e2869b119d4a7b8c02091c3779733a3ac
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Oct 24 13:31:34 2013 +0900

    f2fs: reclaim prefree segments periodically
    
    Previously, f2fs postpones reclaiming prefree segments into free segments
    as much as possible.
    However, if user writes and deletes a bunch of data without any sync or fsync
    calls, some flash storages can suffer from garbage collections.
    
    So, this patch adds the reclaiming codes to f2fs_write_node_pages and background
    GC thread.
    
    If there are a lot of prefree segments, let's do checkpoint so that f2fs
    submits discard commands for the prefree regions to the flash storage.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index cb286d7b02b2..783c6cc6253c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -82,6 +82,11 @@ static int gc_thread_func(void *data)
 		/* if return value is not zero, no victim was selected */
 		if (f2fs_gc(sbi))
 			wait_ms = gc_th->no_gc_sleep_time;
+
+		/* balancing prefree segments */
+		if (excess_prefree_segs(sbi))
+			f2fs_sync_fs(sbi->sb, true);
+
 	} while (!kthread_should_stop());
 	return 0;
 }

commit dcdfff65276fdc6dfe5eb1d0aff802dfa7a95e15
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Oct 22 20:56:10 2013 +0900

    f2fs: clean up several status-related operations
    
    This patch cleans up improper definitions that update some status information.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 7914b92a6967..cb286d7b02b2 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -77,9 +77,7 @@ static int gc_thread_func(void *data)
 		else
 			wait_ms = increase_sleep_time(gc_th, wait_ms);
 
-#ifdef CONFIG_F2FS_STAT_FS
-		sbi->bg_gc++;
-#endif
+		stat_inc_bggc_count(sbi);
 
 		/* if return value is not zero, no victim was selected */
 		if (f2fs_gc(sbi))

commit 7bd59381c82defe19875284c48b1ac9dacd16e8f
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Tue Oct 22 14:52:26 2013 +0800

    f2fs: introduce f2fs_kmem_cache_alloc to hide the unfailed, kmem cache allocation
    
    Introduce the unfailed version of kmem_cache_alloc named f2fs_kmem_cache_alloc
    to hide the retry routine and make the code a bit cleaner.
    
    v2:
       Fix the wrong use of 'retry' tag pointed out by Gao feng.
       Use more neat code to remove redundant tag suggested by Haicheng Li.
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index fbad96846c7c..7914b92a6967 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -361,12 +361,8 @@ static void add_gc_inode(struct inode *inode, struct list_head *ilist)
 		iput(inode);
 		return;
 	}
-repeat:
-	new_ie = kmem_cache_alloc(winode_slab, GFP_NOFS);
-	if (!new_ie) {
-		cond_resched();
-		goto repeat;
-	}
+
+	new_ie = f2fs_kmem_cache_alloc(winode_slab, GFP_NOFS);
 	new_ie->inode = inode;
 	list_add_tail(&new_ie->list, ilist);
 }

commit a57e564d14d9d123b2dd4ff1c933da0d900e0b1d
Author: Jin Xu <jinuxstyle@gmail.com>
Date:   Fri Sep 13 08:38:54 2013 +0800

    f2fs: optimize the victim searching loop slightly
    
    Since the MAX_VICTIM_SEARCH has been enlarged from 20 to 4096,
    the victim searching overhead will be increased much than before,
    especially for SSR that searches victim for use quiet often.
    This patch intends to reduce the overhead a little bit by:
    - make the get_gc_cost a inline routine to reduce function call
      overhead
    - reduce multiplication and division operations
    - reduce unnecessary comparison operation
    
    Signed-off-by: Jin Xu <jinuxstyle@gmail.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 2f157e883687..fbad96846c7c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -236,8 +236,8 @@ static unsigned int get_cb_cost(struct f2fs_sb_info *sbi, unsigned int segno)
 	return UINT_MAX - ((100 * (100 - u) * age) / (100 + u));
 }
 
-static unsigned int get_gc_cost(struct f2fs_sb_info *sbi, unsigned int segno,
-					struct victim_sel_policy *p)
+static inline unsigned int get_gc_cost(struct f2fs_sb_info *sbi,
+			unsigned int segno, struct victim_sel_policy *p)
 {
 	if (p->alloc_mode == SSR)
 		return get_seg_entry(sbi, segno)->ckpt_valid_blocks;
@@ -293,7 +293,11 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 			}
 			break;
 		}
-		p.offset = ((segno / p.ofs_unit) * p.ofs_unit) + p.ofs_unit;
+
+		p.offset = segno + p.ofs_unit;
+		if (p.ofs_unit > 1)
+			p.offset -= segno % p.ofs_unit;
+
 		secno = GET_SECNO(sbi, segno);
 
 		if (sec_usage_check(sbi, secno))
@@ -306,10 +310,9 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 		if (p.min_cost > cost) {
 			p.min_segno = segno;
 			p.min_cost = cost;
-		}
-
-		if (cost == max_cost)
+		} else if (unlikely(cost == max_cost)) {
 			continue;
+		}
 
 		if (nsearched++ >= p.max_search) {
 			sbi->last_victim[p.gc_mode] = segno;

commit a26b7c8a0149ce1e3b6a10f2801aada6e447e4e7
Author: Jin Xu <jinuxstyle@gmail.com>
Date:   Thu Sep 5 12:45:26 2013 +0800

    f2fs: optimize gc for better performance
    
    This patch improves the gc efficiency by optimizing the victim
    selection policy. With this optimization, the random re-write
    performance could increase up to 20%.
    
    For f2fs, when disk is in shortage of free spaces, gc will selects
    dirty segments and moves valid blocks around for making more space
    available. The gc cost of a segment is determined by the valid blocks
    in the segment. The less the valid blocks, the higher the efficiency.
    The ideal victim segment is the one that has the most garbage blocks.
    
    Currently, it searches up to 20 dirty segments for a victim segment.
    The selected victim is not likely the best victim for gc when there
    are much more dirty segments. Why not searching more dirty segments
    for a better victim? The cost of searching dirty segments is
    negligible in comparison to moving blocks.
    
    In this patch, it enlarges the MAX_VICTIM_SEARCH to 4096 to make
    the search more aggressively for a possible better victim. Since
    it also applies to victim selection for SSR, it will likely improve
    the SSR efficiency as well.
    
    The test case is simple. It creates as many files until the disk full.
    The size for each file is 32KB. Then it writes as many as 100000
    records of 4KB size to random offsets of random files in sync mode.
    The testing was done on a 2GB partition of a SDHC card. Let's see the
    test result of f2fs without and with the patch.
    
    ---------------------------------------
    2GB partition, SDHC
    create 52023 files of size 32768 bytes
    random re-write 100000 records of 4KB
    ---------------------------------------
    | file creation (s) | rewrite time (s) | gc count | gc garbage blocks |
    [no patch]  341         4227             1174          174840
    [patched]   324         2958             645           106682
    
    It's obvious that, with the patch, f2fs finishes the test in 20+% less
    time than without the patch. And internally it does much less gc with
    higher efficiency than before.
    
    Since the performance improvement is related to gc, it might not be so
    obvious for other tests that do not trigger gc as often as this one (
    This is because f2fs selects dirty segments for SSR use most of the
    time when free space is in shortage). The well-known iozone test tool
    was not used for benchmarking the patch becuase it seems do not have
    a test case that performs random re-write on a full disk.
    
    This patch is the revised version based on the suggestion from
    Jaegeuk Kim.
    
    Signed-off-by: Jin Xu <jinuxstyle@gmail.com>
    [Jaegeuk Kim: suggested simpler solution]
    Reviewed-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index eb89037e3312..2f157e883687 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -153,12 +153,18 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 	if (p->alloc_mode == SSR) {
 		p->gc_mode = GC_GREEDY;
 		p->dirty_segmap = dirty_i->dirty_segmap[type];
+		p->max_search = dirty_i->nr_dirty[type];
 		p->ofs_unit = 1;
 	} else {
 		p->gc_mode = select_gc_type(sbi->gc_thread, gc_type);
 		p->dirty_segmap = dirty_i->dirty_segmap[DIRTY];
+		p->max_search = dirty_i->nr_dirty[DIRTY];
 		p->ofs_unit = sbi->segs_per_sec;
 	}
+
+	if (p->max_search > MAX_VICTIM_SEARCH)
+		p->max_search = MAX_VICTIM_SEARCH;
+
 	p->offset = sbi->last_victim[p->gc_mode];
 }
 
@@ -305,7 +311,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 		if (cost == max_cost)
 			continue;
 
-		if (nsearched++ >= MAX_VICTIM_SEARCH) {
+		if (nsearched++ >= p.max_search) {
 			sbi->last_victim[p.gc_mode] = segno;
 			break;
 		}

commit de93653fe31fc9439971296842dcd0280f8ab5f4
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Aug 12 21:08:03 2013 +0900

    f2fs: reserve the xattr space dynamically
    
    This patch enables the number of direct pointers inside on-disk inode block to
    be changed dynamically according to the size of inline xattr space.
    
    The number of direct pointers, ADDRS_PER_INODE, can be changed only if the file
    has inline xattr flag.
    
    The number of direct pointers that will be used by inline xattrs is defined as
    F2FS_INLINE_XATTR_ADDRS.
    Current patch assigns F2FS_INLINE_XATTR_ADDRS to 0 temporarily.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e6b3ffd5ff6a..eb89037e3312 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -461,7 +461,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
  * as indirect or double indirect node blocks, are given, it must be a caller's
  * bug.
  */
-block_t start_bidx_of_node(unsigned int node_ofs)
+block_t start_bidx_of_node(unsigned int node_ofs, struct f2fs_inode_info *fi)
 {
 	unsigned int indirect_blks = 2 * NIDS_PER_BLOCK + 4;
 	unsigned int bidx;
@@ -478,7 +478,7 @@ block_t start_bidx_of_node(unsigned int node_ofs)
 		int dec = (node_ofs - indirect_blks - 3) / (NIDS_PER_BLOCK + 1);
 		bidx = node_ofs - 5 - dec;
 	}
-	return bidx * ADDRS_PER_BLOCK + ADDRS_PER_INODE;
+	return bidx * ADDRS_PER_BLOCK + ADDRS_PER_INODE(fi);
 }
 
 static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
@@ -586,7 +586,6 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			continue;
 		}
 
-		start_bidx = start_bidx_of_node(nofs);
 		ofs_in_node = le16_to_cpu(entry->ofs_in_node);
 
 		if (phase == 2) {
@@ -594,6 +593,8 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 			if (IS_ERR(inode))
 				continue;
 
+			start_bidx = start_bidx_of_node(nofs, F2FS_I(inode));
+
 			data_page = find_data_page(inode,
 					start_bidx + ofs_in_node, false);
 			if (IS_ERR(data_page))
@@ -604,6 +605,8 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		} else {
 			inode = find_gc_inode(dni.ino, ilist);
 			if (inode) {
+				start_bidx = start_bidx_of_node(nofs,
+								F2FS_I(inode));
 				data_page = get_lock_data_page(inode,
 						start_bidx + ofs_in_node);
 				if (IS_ERR(data_page))

commit a569469e967022d9ceeaa4b73619f96614087d2d
Author: Jin Xu <jinuxstyle@gmail.com>
Date:   Mon Aug 5 20:02:04 2013 +0800

    f2fs: fix a deadlock in fsync
    
    This patch fixes a deadlock bug that occurs quite often when there are
    concurrent write and fsync on a same file.
    
    Following is the simplified call trace when tasks get hung.
    
    fsync thread:
    - f2fs_sync_file
     ...
     - f2fs_write_data_pages
     ...
      - update_extent_cache
      ...
       - update_inode
        - wait_on_page_writeback
    
    bdi writeback thread
    - __writeback_single_inode
     - f2fs_write_data_pages
      - mutex_lock(sbi->writepages)
    
    The deadlock happens when the fsync thread waits on a inode page that has
    been added to the f2fs' cached bio sbi->bio[NODE], and unfortunately,
    no one else could be able to submit the cached bio to block layer for
    writeback. This is because the fsync thread already hold a sbi->fs_lock and
    the sbi->writepages lock, causing the bdi thread being blocked when attempt
    to write data pages for the same inode. At the same time, f2fs_gc thread
    does not notice the situation and could not help. Even the sync syscall
    gets blocked.
    
    To fix it, we could submit the cached bio first before waiting on a inode page
    that is being written back.
    
    Signed-off-by: Jin Xu <jinuxstyle@gmail.com>
    [Jaegeuk Kim: add more cases to use f2fs_wait_on_page_writeback]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index d286d8be8e68..e6b3ffd5ff6a 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -422,8 +422,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 
 		/* set page dirty and write it */
 		if (gc_type == FG_GC) {
-			f2fs_submit_bio(sbi, NODE, true);
-			wait_on_page_writeback(node_page);
+			f2fs_wait_on_page_writeback(node_page, NODE, true);
 			set_page_dirty(node_page);
 		} else {
 			if (!PageWriteback(node_page))
@@ -523,10 +522,7 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 	} else {
 		struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 
-		if (PageWriteback(page)) {
-			f2fs_submit_bio(sbi, DATA, true);
-			wait_on_page_writeback(page);
-		}
+		f2fs_wait_on_page_writeback(page, DATA, true);
 
 		if (clear_page_dirty_for_io(page) &&
 			S_ISDIR(inode->i_mode)) {

commit d2dc095f4280ad5fdea33769e8e119fd16648426
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sun Aug 4 23:10:15 2013 +0900

    f2fs: add sysfs entries to select the gc policy
    
    Add sysfs entry gc_idle to control the gc policy. Where
    gc_idle = 1 corresponds to selecting a cost benefit approach,
    while gc_idle = 2 corresponds to selecting a greedy approach
    to garbage collection. The selection is mutually exclusive one
    approach will work at any point. If gc_idle = 0, then this
    option is disabled.
    
    Cc: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Pankaj Kumar <pankaj.km@samsung.com>
    Reviewed-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    [Jaegeuk Kim: change the select_gc_type() flow slightly]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 60d4f674efa7..d286d8be8e68 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -106,6 +106,8 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 	gc_th->max_sleep_time = DEF_GC_THREAD_MAX_SLEEP_TIME;
 	gc_th->no_gc_sleep_time = DEF_GC_THREAD_NOGC_SLEEP_TIME;
 
+	gc_th->gc_idle = 0;
+
 	sbi->gc_thread = gc_th;
 	init_waitqueue_head(&sbi->gc_thread->gc_wait_queue_head);
 	sbi->gc_thread->f2fs_gc_task = kthread_run(gc_thread_func, sbi,
@@ -130,9 +132,17 @@ void stop_gc_thread(struct f2fs_sb_info *sbi)
 	sbi->gc_thread = NULL;
 }
 
-static int select_gc_type(int gc_type)
+static int select_gc_type(struct f2fs_gc_kthread *gc_th, int gc_type)
 {
-	return (gc_type == BG_GC) ? GC_CB : GC_GREEDY;
+	int gc_mode = (gc_type == BG_GC) ? GC_CB : GC_GREEDY;
+
+	if (gc_th && gc_th->gc_idle) {
+		if (gc_th->gc_idle == 1)
+			gc_mode = GC_CB;
+		else if (gc_th->gc_idle == 2)
+			gc_mode = GC_GREEDY;
+	}
+	return gc_mode;
 }
 
 static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
@@ -145,7 +155,7 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 		p->dirty_segmap = dirty_i->dirty_segmap[type];
 		p->ofs_unit = 1;
 	} else {
-		p->gc_mode = select_gc_type(gc_type);
+		p->gc_mode = select_gc_type(sbi->gc_thread, gc_type);
 		p->dirty_segmap = dirty_i->dirty_segmap[DIRTY];
 		p->ofs_unit = sbi->segs_per_sec;
 	}

commit b59d0bae6ca30c496f298881616258f9cde0d9c6
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sun Aug 4 23:09:40 2013 +0900

    f2fs: add sysfs support for controlling the gc_thread
    
    Add sysfs entries to control the timing parameters for
    f2fs gc thread.
    
    Various Sysfs options introduced are:
    gc_min_sleep_time: Min Sleep time for GC in ms
    gc_max_sleep_time: Max Sleep time for GC in ms
    gc_no_gc_sleep_time: Default Sleep time for GC in ms
    
    Cc: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Pankaj Kumar <pankaj.km@samsung.com>
    Reviewed-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    [Jaegeuk Kim: fix an umount bug and some minor changes]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 35f9b1a196aa..60d4f674efa7 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -29,10 +29,11 @@ static struct kmem_cache *winode_slab;
 static int gc_thread_func(void *data)
 {
 	struct f2fs_sb_info *sbi = data;
+	struct f2fs_gc_kthread *gc_th = sbi->gc_thread;
 	wait_queue_head_t *wq = &sbi->gc_thread->gc_wait_queue_head;
 	long wait_ms;
 
-	wait_ms = GC_THREAD_MIN_SLEEP_TIME;
+	wait_ms = gc_th->min_sleep_time;
 
 	do {
 		if (try_to_freeze())
@@ -45,7 +46,7 @@ static int gc_thread_func(void *data)
 			break;
 
 		if (sbi->sb->s_writers.frozen >= SB_FREEZE_WRITE) {
-			wait_ms = GC_THREAD_MAX_SLEEP_TIME;
+			wait_ms = increase_sleep_time(gc_th, wait_ms);
 			continue;
 		}
 
@@ -66,15 +67,15 @@ static int gc_thread_func(void *data)
 			continue;
 
 		if (!is_idle(sbi)) {
-			wait_ms = increase_sleep_time(wait_ms);
+			wait_ms = increase_sleep_time(gc_th, wait_ms);
 			mutex_unlock(&sbi->gc_mutex);
 			continue;
 		}
 
 		if (has_enough_invalid_blocks(sbi))
-			wait_ms = decrease_sleep_time(wait_ms);
+			wait_ms = decrease_sleep_time(gc_th, wait_ms);
 		else
-			wait_ms = increase_sleep_time(wait_ms);
+			wait_ms = increase_sleep_time(gc_th, wait_ms);
 
 #ifdef CONFIG_F2FS_STAT_FS
 		sbi->bg_gc++;
@@ -82,7 +83,7 @@ static int gc_thread_func(void *data)
 
 		/* if return value is not zero, no victim was selected */
 		if (f2fs_gc(sbi))
-			wait_ms = GC_THREAD_NOGC_SLEEP_TIME;
+			wait_ms = gc_th->no_gc_sleep_time;
 	} while (!kthread_should_stop());
 	return 0;
 }
@@ -101,6 +102,10 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 		goto out;
 	}
 
+	gc_th->min_sleep_time = DEF_GC_THREAD_MIN_SLEEP_TIME;
+	gc_th->max_sleep_time = DEF_GC_THREAD_MAX_SLEEP_TIME;
+	gc_th->no_gc_sleep_time = DEF_GC_THREAD_NOGC_SLEEP_TIME;
+
 	sbi->gc_thread = gc_th;
 	init_waitqueue_head(&sbi->gc_thread->gc_wait_queue_head);
 	sbi->gc_thread->f2fs_gc_task = kthread_run(gc_thread_func, sbi,

commit 6cc4af56066d8e9c62584cf61c6ce50fd0ab139a
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Thu Jun 20 17:52:39 2013 +0800

    f2fs: code cleanup and simplify in func {find/add}_gc_inode
    
    This patch simplifies list operations in find_gc_inode and add_gc_inode.
    Just simple code cleanup.
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    [Jaegeuk Kim: add description]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 3a9df36491a5..35f9b1a196aa 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -321,28 +321,21 @@ static const struct victim_selection default_v_ops = {
 
 static struct inode *find_gc_inode(nid_t ino, struct list_head *ilist)
 {
-	struct list_head *this;
 	struct inode_entry *ie;
 
-	list_for_each(this, ilist) {
-		ie = list_entry(this, struct inode_entry, list);
+	list_for_each_entry(ie, ilist, list)
 		if (ie->inode->i_ino == ino)
 			return ie->inode;
-	}
 	return NULL;
 }
 
 static void add_gc_inode(struct inode *inode, struct list_head *ilist)
 {
-	struct list_head *this;
-	struct inode_entry *new_ie, *ie;
+	struct inode_entry *new_ie;
 
-	list_for_each(this, ilist) {
-		ie = list_entry(this, struct inode_entry, list);
-		if (ie->inode == inode) {
-			iput(inode);
-			return;
-		}
+	if (inode == find_gc_inode(inode->i_ino, ilist)) {
+		iput(inode);
+		return;
 	}
 repeat:
 	new_ie = kmem_cache_alloc(winode_slab, GFP_NOFS);

commit b2b3460a9404136e0a99b9f7cb56e08ec41ea933
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sat Jun 1 16:20:26 2013 +0900

    f2fs: reorganise the function get_victim_by_default
    
    Fix the function get_victim_by_default, where it checks
    for the condition  that p.min_segno != NULL_SEGNO as
    shown:
    
    if (p.min_segno != NULL_SEGNO)
               goto got_it;
    
    and if above condition is true then
    
    got_it:
            if (p.min_segno != NULL_SEGNO) {
    
    So this condition is being checked twice. Hence move the goto
    statement after the if condition so that duplication of condition
    check is avoided.
    
    Also this function makes a call to get_max_cost() to compute
    the max cost based on the f2fs_sbi_info and victim policy. Since
    get_max_cost depends on on three parameters of victim_sel_policy
    => alloc_mode, gc_mode & ofs_unit, once this victim policy is
    initialised, these value will not change till the execution
    time of get_victim_by_default() & also f2fs_sbi_info structure
    parameters will not change.
    
    Hence making calls to get_max_cost() in while loop does not seems to
    be a good point. Instead we can call it once in begining and store
    the results in local variable, which later can serve our purpose
    for comparing the cost with max cost inside the while loop.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Pankaj Kumar <pankaj.km@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index ddc2c6750eee..3a9df36491a5 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -241,14 +241,14 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 {
 	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
 	struct victim_sel_policy p;
-	unsigned int secno;
+	unsigned int secno, max_cost;
 	int nsearched = 0;
 
 	p.alloc_mode = alloc_mode;
 	select_policy(sbi, gc_type, type, &p);
 
 	p.min_segno = NULL_SEGNO;
-	p.min_cost = get_max_cost(sbi, &p);
+	p.min_cost = max_cost = get_max_cost(sbi, &p);
 
 	mutex_lock(&dirty_i->seglist_lock);
 
@@ -287,7 +287,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 			p.min_cost = cost;
 		}
 
-		if (cost == get_max_cost(sbi, &p))
+		if (cost == max_cost)
 			continue;
 
 		if (nsearched++ >= MAX_VICTIM_SEARCH) {
@@ -295,8 +295,8 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 			break;
 		}
 	}
-got_it:
 	if (p.min_segno != NULL_SEGNO) {
+got_it:
 		if (p.alloc_mode == LFS) {
 			secno = GET_SECNO(sbi, p.min_segno);
 			if (gc_type == FG_GC)

commit 7a267f8d7463346a139e49c8beac1b8bfe32ef97
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sun May 26 11:05:32 2013 +0900

    f2fs: return proper error from start_gc_thread
    
    when there is an error from kthread_run, then return proper error
    rather than returning -ENOMEM.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 25b083c81d50..ddc2c6750eee 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -91,23 +91,28 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 {
 	struct f2fs_gc_kthread *gc_th;
 	dev_t dev = sbi->sb->s_bdev->bd_dev;
+	int err = 0;
 
 	if (!test_opt(sbi, BG_GC))
-		return 0;
+		goto out;
 	gc_th = kmalloc(sizeof(struct f2fs_gc_kthread), GFP_KERNEL);
-	if (!gc_th)
-		return -ENOMEM;
+	if (!gc_th) {
+		err = -ENOMEM;
+		goto out;
+	}
 
 	sbi->gc_thread = gc_th;
 	init_waitqueue_head(&sbi->gc_thread->gc_wait_queue_head);
 	sbi->gc_thread->f2fs_gc_task = kthread_run(gc_thread_func, sbi,
 			"f2fs_gc-%u:%u", MAJOR(dev), MINOR(dev));
 	if (IS_ERR(gc_th->f2fs_gc_task)) {
+		err = PTR_ERR(gc_th->f2fs_gc_task);
 		kfree(gc_th);
 		sbi->gc_thread = NULL;
-		return -ENOMEM;
 	}
-	return 0;
+
+out:
+	return err;
 }
 
 void stop_gc_thread(struct f2fs_sb_info *sbi)

commit 35b09d82c3cf3fc0b8b6d923e7fd82ff7926aafc
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Thu May 23 22:57:53 2013 +0900

    f2fs: push some variables to debug part
    
    Some, counters are needed only for the statistical information
    while debugging.
    So, those can be controlled using CONFIG_F2FS_STAT_FS,
    pushing the usage for few variables under this flag.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 14961593e93c..25b083c81d50 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -76,7 +76,9 @@ static int gc_thread_func(void *data)
 		else
 			wait_ms = increase_sleep_time(wait_ms);
 
+#ifdef CONFIG_F2FS_STAT_FS
 		sbi->bg_gc++;
+#endif
 
 		/* if return value is not zero, no victim was selected */
 		if (f2fs_gc(sbi))

commit b743ba78ae4c7c6a6e08e623af824b6208f58019
Author: Haicheng Li <haicheng.li@linux.intel.com>
Date:   Sun Apr 28 19:16:07 2013 +0800

    f2fs: remove useless #include <linux/proc_fs.h> as we're now using sysfs as debug entry.
    
    Signed-off-by: Haicheng Li <haicheng.li@linux.intel.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 25a1f7e593e0..14961593e93c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -11,7 +11,6 @@
 #include <linux/fs.h>
 #include <linux/module.h>
 #include <linux/backing-dev.h>
-#include <linux/proc_fs.h>
 #include <linux/init.h>
 #include <linux/f2fs_fs.h>
 #include <linux/kthread.h>

commit c718379b6b0954a04a153d7e5dc8b3136a301ee6
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Apr 24 13:19:56 2013 +0900

    f2fs: give a chance to merge IOs by IO scheduler
    
    Previously, background GC submits many 4KB read requests to load victim blocks
    and/or its (i)node blocks.
    
    ...
    f2fs_gc : f2fs_readpage: ino = 1, page_index = 0xb61, blkaddr = 0x3b964ed
    f2fs_gc : block_rq_complete: 8,16 R () 499854968 + 8 [0]
    f2fs_gc : f2fs_readpage: ino = 1, page_index = 0xb6f, blkaddr = 0x3b964ee
    f2fs_gc : block_rq_complete: 8,16 R () 499854976 + 8 [0]
    f2fs_gc : f2fs_readpage: ino = 1, page_index = 0xb79, blkaddr = 0x3b964ef
    f2fs_gc : block_rq_complete: 8,16 R () 499854984 + 8 [0]
    ...
    
    However, by the fact that many IOs are sequential, we can give a chance to merge
    the IOs by IO scheduler.
    In order to do that, let's use blk_plug.
    
    ...
    f2fs_gc : f2fs_iget: ino = 143
    f2fs_gc : f2fs_readpage: ino = 143, page_index = 0x1c6, blkaddr = 0x2e6ee
    f2fs_gc : f2fs_iget: ino = 143
    f2fs_gc : f2fs_readpage: ino = 143, page_index = 0x1c7, blkaddr = 0x2e6ef
    <idle> : block_rq_complete: 8,16 R () 1519616 + 8 [0]
    <idle> : block_rq_complete: 8,16 R () 1519848 + 8 [0]
    <idle> : block_rq_complete: 8,16 R () 1520432 + 96 [0]
    <idle> : block_rq_complete: 8,16 R () 1520536 + 104 [0]
    <idle> : block_rq_complete: 8,16 R () 1521008 + 112 [0]
    <idle> : block_rq_complete: 8,16 R () 1521440 + 152 [0]
    <idle> : block_rq_complete: 8,16 R () 1521688 + 144 [0]
    <idle> : block_rq_complete: 8,16 R () 1522128 + 192 [0]
    <idle> : block_rq_complete: 8,16 R () 1523256 + 328 [0]
    ...
    
    Note that this issue should be addressed in checkpoint, and some readahead
    flows too.
    
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 6ed3263eeee8..25a1f7e593e0 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -386,6 +386,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 
 next_step:
 	entry = sum;
+
 	for (off = 0; off < sbi->blocks_per_seg; off++, entry++) {
 		nid_t nid = le32_to_cpu(entry->nid);
 		struct page *node_page;
@@ -417,6 +418,7 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 		f2fs_put_page(node_page, 1);
 		stat_inc_node_blk_count(sbi, 1);
 	}
+
 	if (initial) {
 		initial = false;
 		goto next_step;
@@ -545,6 +547,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 next_step:
 	entry = sum;
+
 	for (off = 0; off < sbi->blocks_per_seg; off++, entry++) {
 		struct page *data_page;
 		struct inode *inode;
@@ -582,7 +585,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 				continue;
 
 			data_page = find_data_page(inode,
-					start_bidx + ofs_in_node);
+					start_bidx + ofs_in_node, false);
 			if (IS_ERR(data_page))
 				goto next_iput;
 
@@ -603,6 +606,7 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 next_iput:
 		iput(inode);
 	}
+
 	if (++phase < 4)
 		goto next_step;
 
@@ -636,12 +640,15 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 {
 	struct page *sum_page;
 	struct f2fs_summary_block *sum;
+	struct blk_plug plug;
 
 	/* read segment summary of victim */
 	sum_page = get_sum_page(sbi, segno);
 	if (IS_ERR(sum_page))
 		return;
 
+	blk_start_plug(&plug);
+
 	sum = page_address(sum_page);
 
 	switch (GET_SUM_TYPE((&sum->footer))) {
@@ -652,6 +659,8 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 		gc_data_segment(sbi, sum->entries, ilist, segno, gc_type);
 		break;
 	}
+	blk_finish_plug(&plug);
+
 	stat_inc_seg_count(sbi, GET_SUM_TYPE((&sum->footer)));
 	stat_inc_call_count(sbi->stat_info);
 

commit 6cb968d9b0358c7e807416a85699a526e820083c
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Apr 24 13:00:14 2013 +0900

    f2fs: avoid frequent background GC
    
    If there is no victim segments selected by background GC, let's wait
    a little bit longer time to collect dirty segments.
    By default, let's give 5 minutes.
    
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 1ca332455ee2..6ed3263eeee8 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -82,9 +82,6 @@ static int gc_thread_func(void *data)
 		/* if return value is not zero, no victim was selected */
 		if (f2fs_gc(sbi))
 			wait_ms = GC_THREAD_NOGC_SLEEP_TIME;
-		else if (wait_ms == GC_THREAD_NOGC_SLEEP_TIME)
-			wait_ms = GC_THREAD_MAX_SLEEP_TIME;
-
 	} while (!kthread_should_stop());
 	return 0;
 }

commit 8e46b3ed11b750a740fec0a313ad9118059fc37b
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Tue Apr 23 16:42:53 2013 +0900

    f2fs: add tracepoints for GC threads
    
    Add tracepoints for tracing the garbage collector
    threads in f2fs with status of collection & type.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Pankaj Kumar <pankaj.km@samsung.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    [Jaegeuk: modify slightly to show information]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 37b05e1c574c..1ca332455ee2 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -23,6 +23,7 @@
 #include "node.h"
 #include "segment.h"
 #include "gc.h"
+#include <trace/events/f2fs.h>
 
 static struct kmem_cache *winode_slab;
 
@@ -301,6 +302,10 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 				set_bit(secno, dirty_i->victim_secmap);
 		}
 		*result = (p.min_segno / p.ofs_unit) * p.ofs_unit;
+
+		trace_f2fs_get_victim(sbi->sb, type, gc_type, &p,
+				sbi->cur_victim_sec,
+				prefree_segments(sbi), free_segments(sbi));
 	}
 	mutex_unlock(&dirty_i->seglist_lock);
 

commit d64f80473bb58f5c8bfcb63220c31a573a07752f
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Apr 8 16:01:00 2013 +0900

    f2fs: write checkpoint before starting FG_GC
    
    In order to be aware of prefree and free sections during FG_GC, let's start with
    write_checkpoint().
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 83cec8f868c6..37b05e1c574c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -669,8 +669,10 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 	if (!(sbi->sb->s_flags & MS_ACTIVE))
 		goto stop;
 
-	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, nfree))
+	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, nfree)) {
 		gc_type = FG_GC;
+		write_checkpoint(sbi, false);
+	}
 
 	if (!__get_victim(sbi, &segno, gc_type, NO_CHECK_TYPE))
 		goto stop;

commit 399368372ed9f3c396eadb5c2bbc98be8c774a39
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Nov 22 16:21:29 2012 +0900

    f2fs: introduce a new global lock scheme
    
    In the previous version, f2fs uses global locks according to the usage types,
    such as directory operations, block allocation, block write, and so on.
    
    Reference the following lock types in f2fs.h.
    enum lock_type {
            RENAME,         /* for renaming operations */
            DENTRY_OPS,     /* for directory operations */
            DATA_WRITE,     /* for data write */
            DATA_NEW,       /* for data allocation */
            DATA_TRUNC,     /* for data truncate */
            NODE_NEW,       /* for node allocation */
            NODE_TRUNC,     /* for node truncate */
            NODE_WRITE,     /* for node write */
            NR_LOCK_TYPE,
    };
    
    In that case, we lose the performance under the multi-threading environment,
    since every types of operations must be conducted one at a time.
    
    In order to address the problem, let's share the locks globally with a mutex
    array regardless of any types.
    So, let users grab a mutex and perform their jobs in parallel as much as
    possbile.
    
    For this, I propose a new global lock scheme as follows.
    
    0. Data structure
     - f2fs_sb_info -> mutex_lock[NR_GLOBAL_LOCKS]
     - f2fs_sb_info -> node_write
    
    1. mutex_lock_op(sbi)
     - try to get an avaiable lock from the array.
     - returns the index of the gottern lock variable.
    
    2. mutex_unlock_op(sbi, index of the lock)
     - unlock the given index of the lock.
    
    3. mutex_lock_all(sbi)
     - grab all the locks in the array before the checkpoint.
    
    4. mutex_unlock_all(sbi)
     - release all the locks in the array after checkpoint.
    
    5. block_operations()
     - call mutex_lock_all()
     - sync_dirty_dir_inodes()
     - grab node_write
     - sync_node_pages()
    
    Note that,
     the pairs of mutex_lock_op()/mutex_unlock_op() and
     mutex_lock_all()/mutex_unlock_all() should be used together.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index e97f30157aa6..83cec8f868c6 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -510,7 +510,6 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 			wait_on_page_writeback(page);
 		}
 
-		mutex_lock_op(sbi, DATA_WRITE);
 		if (clear_page_dirty_for_io(page) &&
 			S_ISDIR(inode->i_mode)) {
 			dec_page_count(sbi, F2FS_DIRTY_DENTS);
@@ -518,7 +517,6 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 		}
 		set_cold_data(page);
 		do_write_data_page(page);
-		mutex_unlock_op(sbi, DATA_WRITE);
 		clear_cold_data(page);
 	}
 out:

commit b74737541c5190ab2ad3ee0d7b323e860b988df1
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Apr 1 08:32:21 2013 +0900

    f2fs: avoid race for summary information
    
    In order to do GC more reliably, I'd like to lock the vicitm summary page
    until its GC is completed, and also prevent any checkpoint process.
    
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 136c0f7a670b..e97f30157aa6 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -642,12 +642,6 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 	if (IS_ERR(sum_page))
 		return;
 
-	/*
-	 * CP needs to lock sum_page. In this time, we don't need
-	 * to lock this page, because this summary page is not gone anywhere.
-	 * Also, this page is not gonna be updated before GC is done.
-	 */
-	unlock_page(sum_page);
 	sum = page_address(sum_page);
 
 	switch (GET_SUM_TYPE((&sum->footer))) {
@@ -661,7 +655,7 @@ static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 	stat_inc_seg_count(sbi, GET_SUM_TYPE((&sum->footer)));
 	stat_inc_call_count(sbi->stat_info);
 
-	f2fs_put_page(sum_page, 0);
+	f2fs_put_page(sum_page, 1);
 }
 
 int f2fs_gc(struct f2fs_sb_info *sbi)

commit 4ebefc4443898f5429185ef96d85cfce0fbcc16a
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Sun Mar 31 13:49:18 2013 +0900

    f2fs: check completion of foreground GC
    
    The foreground GCs are triggered under not enough free sections.
    So, we should not skip moving valid blocks in the victim segments.
    
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 09b8a907400b..136c0f7a670b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -131,7 +131,7 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 {
 	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
 
-	if (p->alloc_mode) {
+	if (p->alloc_mode == SSR) {
 		p->gc_mode = GC_GREEDY;
 		p->dirty_segmap = dirty_i->dirty_segmap[type];
 		p->ofs_unit = 1;
@@ -404,8 +404,14 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 			continue;
 
 		/* set page dirty and write it */
-		if (!PageWriteback(node_page))
+		if (gc_type == FG_GC) {
+			f2fs_submit_bio(sbi, NODE, true);
+			wait_on_page_writeback(node_page);
 			set_page_dirty(node_page);
+		} else {
+			if (!PageWriteback(node_page))
+				set_page_dirty(node_page);
+		}
 		f2fs_put_page(node_page, 1);
 		stat_inc_node_blk_count(sbi, 1);
 	}
@@ -421,6 +427,13 @@ static void gc_node_segment(struct f2fs_sb_info *sbi,
 			.for_reclaim = 0,
 		};
 		sync_node_pages(sbi, 0, &wbc);
+
+		/*
+		 * In the case of FG_GC, it'd be better to reclaim this victim
+		 * completely.
+		 */
+		if (get_valid_blocks(sbi, segno, 1) != 0)
+			goto next_step;
 	}
 }
 
@@ -484,20 +497,19 @@ static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 {
-	if (page->mapping != inode->i_mapping)
-		goto out;
-
-	if (inode != page->mapping->host)
-		goto out;
-
-	if (PageWriteback(page))
-		goto out;
-
 	if (gc_type == BG_GC) {
+		if (PageWriteback(page))
+			goto out;
 		set_page_dirty(page);
 		set_cold_data(page);
 	} else {
 		struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+
+		if (PageWriteback(page)) {
+			f2fs_submit_bio(sbi, DATA, true);
+			wait_on_page_writeback(page);
+		}
+
 		mutex_lock_op(sbi, DATA_WRITE);
 		if (clear_page_dirty_for_io(page) &&
 			S_ISDIR(inode->i_mode)) {
@@ -594,8 +606,18 @@ static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	if (++phase < 4)
 		goto next_step;
 
-	if (gc_type == FG_GC)
+	if (gc_type == FG_GC) {
 		f2fs_submit_bio(sbi, DATA, true);
+
+		/*
+		 * In the case of FG_GC, it'd be better to reclaim this victim
+		 * completely.
+		 */
+		if (get_valid_blocks(sbi, segno, 1) != 0) {
+			phase = 2;
+			goto next_step;
+		}
+	}
 }
 
 static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,

commit 5ec4e49f9bd753e2a6857a96e01f8ae5ff00b459
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Sun Mar 31 13:26:03 2013 +0900

    f2fs: change GC bitmaps to apply the section granularity
    
    This patch removes a bitmap for victim segments selected by foreground GC, and
    modifies the other bitmap for victim segments selected by background GC.
    
    1) foreground GC bitmap
     : We don't need to manage this, since we just only one previous victim section
       number instead of the whole victim history.
       The f2fs uses the victim section number in order not to allocate currently
       GC'ed section to current active logs.
    
    2) background GC bitmap
     : This bitmap is used to avoid selecting victims repeatedly by background GCs.
       In addition, the victims are able to be selected by foreground GCs, since
       there is no need to read victim blocks during foreground GCs.
    
       By the fact that the foreground GC reclaims segments in a section unit, it'd
       be better to manage this bitmap based on the section granularity.
    
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 2e3eb2d4fc30..09b8a907400b 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -160,18 +160,21 @@ static unsigned int get_max_cost(struct f2fs_sb_info *sbi,
 static unsigned int check_bg_victims(struct f2fs_sb_info *sbi)
 {
 	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
-	unsigned int segno;
+	unsigned int hint = 0;
+	unsigned int secno;
 
 	/*
 	 * If the gc_type is FG_GC, we can select victim segments
 	 * selected by background GC before.
 	 * Those segments guarantee they have small valid blocks.
 	 */
-	segno = find_next_bit(dirty_i->victim_segmap[BG_GC],
-						TOTAL_SEGS(sbi), 0);
-	if (segno < TOTAL_SEGS(sbi)) {
-		clear_bit(segno, dirty_i->victim_segmap[BG_GC]);
-		return segno;
+next:
+	secno = find_next_bit(dirty_i->victim_secmap, TOTAL_SECS(sbi), hint++);
+	if (secno < TOTAL_SECS(sbi)) {
+		if (sec_usage_check(sbi, secno))
+			goto next;
+		clear_bit(secno, dirty_i->victim_secmap);
+		return secno * sbi->segs_per_sec;
 	}
 	return NULL_SEGNO;
 }
@@ -234,7 +237,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 {
 	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
 	struct victim_sel_policy p;
-	unsigned int segno;
+	unsigned int secno;
 	int nsearched = 0;
 
 	p.alloc_mode = alloc_mode;
@@ -253,6 +256,7 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 
 	while (1) {
 		unsigned long cost;
+		unsigned int segno;
 
 		segno = find_next_bit(p.dirty_segmap,
 						TOTAL_SEGS(sbi), p.offset);
@@ -265,13 +269,11 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 			break;
 		}
 		p.offset = ((segno / p.ofs_unit) * p.ofs_unit) + p.ofs_unit;
+		secno = GET_SECNO(sbi, segno);
 
-		if (test_bit(segno, dirty_i->victim_segmap[FG_GC]))
+		if (sec_usage_check(sbi, secno))
 			continue;
-		if (gc_type == BG_GC &&
-				test_bit(segno, dirty_i->victim_segmap[BG_GC]))
-			continue;
-		if (IS_CURSEC(sbi, GET_SECNO(sbi, segno)))
+		if (gc_type == BG_GC && test_bit(secno, dirty_i->victim_secmap))
 			continue;
 
 		cost = get_gc_cost(sbi, segno, &p);
@@ -291,13 +293,14 @@ static int get_victim_by_default(struct f2fs_sb_info *sbi,
 	}
 got_it:
 	if (p.min_segno != NULL_SEGNO) {
-		*result = (p.min_segno / p.ofs_unit) * p.ofs_unit;
 		if (p.alloc_mode == LFS) {
-			int i;
-			for (i = 0; i < p.ofs_unit; i++)
-				set_bit(*result + i,
-					dirty_i->victim_segmap[gc_type]);
+			secno = GET_SECNO(sbi, p.min_segno);
+			if (gc_type == FG_GC)
+				sbi->cur_victim_sec = secno;
+			else
+				set_bit(secno, dirty_i->victim_secmap);
 		}
+		*result = (p.min_segno / p.ofs_unit) * p.ofs_unit;
 	}
 	mutex_unlock(&dirty_i->seglist_lock);
 
@@ -662,9 +665,11 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 	for (i = 0; i < sbi->segs_per_sec; i++)
 		do_garbage_collect(sbi, segno + i, &ilist, gc_type);
 
-	if (gc_type == FG_GC &&
-			get_valid_blocks(sbi, segno, sbi->segs_per_sec) == 0)
+	if (gc_type == FG_GC) {
+		sbi->cur_victim_sec = NULL_SEGNO;
 		nfree++;
+		WARN_ON(get_valid_blocks(sbi, segno, sbi->segs_per_sec));
+	}
 
 	if (has_not_enough_free_secs(sbi, nfree))
 		goto gc_more;

commit 111d2495a8a8fbd8e3bb0f1c1c60f977b1386249
Author: Masanari Iida <standby24x7@gmail.com>
Date:   Tue Mar 19 08:03:35 2013 +0900

    f2fs: fix typo in comments
    
    Correct spelling typo in comments
    
    Signed-off-by: Masanari Iida <standby24x7@gmail.com>
    Acked-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 94b8a0c48453..2e3eb2d4fc30 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -222,7 +222,7 @@ static unsigned int get_gc_cost(struct f2fs_sb_info *sbi, unsigned int segno,
 }
 
 /*
- * This function is called from two pathes.
+ * This function is called from two paths.
  * One is garbage collection and the other is SSR segment selection.
  * When it is called during GC, it just gets a victim segment
  * and it does not remove it from dirty seglist.

commit b7250d2d845822466356f7f22a650bf807090d7e
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Feb 5 13:19:28 2013 +0900

    f2fs: fix calculation of max. gc cost in the SSR case
    
    In the SSR case, the max gc cost should be the number of pages in a segment.
    Otherwise, f2fs is able to fail getting dirty segments frequently for SSR.
    
    In get_victim_by_default() previously,
    
    while(1) {
       ...
       cost = get_gc_cost(); <- cost is between 0 ~ 512.
       ...
       if (cost == get_max_cost(sbi, &p)) <- max cost is UINT_MAX due to GC_CB type
            continue;
    
       if (nsearched++ >= MAX_VICTIM_SEARCH)
            break;
    }
    
    So, if there are a number of fully valid segments in series, f2fs cannot skip
    those segments by comparing the cost and max cost of each segment.
    
    Note that, the cost is the number of valid blocks at the time of the last
    checkpoint.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 52d3a391b922..94b8a0c48453 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -146,6 +146,9 @@ static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
 static unsigned int get_max_cost(struct f2fs_sb_info *sbi,
 				struct victim_sel_policy *p)
 {
+	/* SSR allocates in a segment unit */
+	if (p->alloc_mode == SSR)
+		return 1 << sbi->log_blocks_per_seg;
 	if (p->gc_mode == GC_GREEDY)
 		return (1 << sbi->log_blocks_per_seg) * p->ofs_unit;
 	else if (p->gc_mode == GC_CB)

commit 437275272f9e635673f065300e5d95226a25cb06
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Feb 4 15:11:17 2013 +0900

    f2fs: clarify and enhance the f2fs_gc flow
    
    This patch makes clearer the ambiguous f2fs_gc flow as follows.
    
    1. Remove intermediate checkpoint condition during f2fs_gc
     (i.e., should_do_checkpoint() and GC_BLOCKED)
    
    2. Remove unnecessary return values of f2fs_gc because of #1.
     (i.e., GC_NODE, GC_OK, etc)
    
    3. Simplify write_checkpoint() because of #2.
    
    4. Clarify the main f2fs_gc flow.
     o monitor how many freed sections during one iteration of do_garbage_collect().
     o do GC more without checkpoints if we can't get enough free sections.
     o do checkpoint once we've got enough free sections through forground GCs.
    
    5. Adopt thread-logging (Slack-Space-Recycle) scheme more aggressively on data
      log types. See. get_ssr_segement()
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 16fdec355201..52d3a391b922 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -78,7 +78,8 @@ static int gc_thread_func(void *data)
 
 		sbi->bg_gc++;
 
-		if (f2fs_gc(sbi) == GC_NONE)
+		/* if return value is not zero, no victim was selected */
+		if (f2fs_gc(sbi))
 			wait_ms = GC_THREAD_NOGC_SLEEP_TIME;
 		else if (wait_ms == GC_THREAD_NOGC_SLEEP_TIME)
 			wait_ms = GC_THREAD_MAX_SLEEP_TIME;
@@ -360,7 +361,7 @@ static int check_valid_map(struct f2fs_sb_info *sbi,
 	sentry = get_seg_entry(sbi, segno);
 	ret = f2fs_test_bit(offset, sentry->cur_valid_map);
 	mutex_unlock(&sit_i->sentry_lock);
-	return ret ? GC_OK : GC_NEXT;
+	return ret;
 }
 
 /*
@@ -368,7 +369,7 @@ static int check_valid_map(struct f2fs_sb_info *sbi,
  * On validity, copy that node with cold status, otherwise (invalid node)
  * ignore that.
  */
-static int gc_node_segment(struct f2fs_sb_info *sbi,
+static void gc_node_segment(struct f2fs_sb_info *sbi,
 		struct f2fs_summary *sum, unsigned int segno, int gc_type)
 {
 	bool initial = true;
@@ -380,21 +381,12 @@ static int gc_node_segment(struct f2fs_sb_info *sbi,
 	for (off = 0; off < sbi->blocks_per_seg; off++, entry++) {
 		nid_t nid = le32_to_cpu(entry->nid);
 		struct page *node_page;
-		int err;
 
-		/*
-		 * It makes sure that free segments are able to write
-		 * all the dirty node pages before CP after this CP.
-		 * So let's check the space of dirty node pages.
-		 */
-		if (should_do_checkpoint(sbi)) {
-			mutex_lock(&sbi->cp_mutex);
-			block_operations(sbi);
-			return GC_BLOCKED;
-		}
+		/* stop BG_GC if there is not enough free sections. */
+		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0))
+			return;
 
-		err = check_valid_map(sbi, segno, off);
-		if (err == GC_NEXT)
+		if (check_valid_map(sbi, segno, off) == 0)
 			continue;
 
 		if (initial) {
@@ -424,7 +416,6 @@ static int gc_node_segment(struct f2fs_sb_info *sbi,
 		};
 		sync_node_pages(sbi, 0, &wbc);
 	}
-	return GC_DONE;
 }
 
 /*
@@ -467,13 +458,13 @@ static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 	node_page = get_node_page(sbi, nid);
 	if (IS_ERR(node_page))
-		return GC_NEXT;
+		return 0;
 
 	get_node_info(sbi, nid, dni);
 
 	if (sum->version != dni->version) {
 		f2fs_put_page(node_page, 1);
-		return GC_NEXT;
+		return 0;
 	}
 
 	*nofs = ofs_of_node(node_page);
@@ -481,8 +472,8 @@ static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	f2fs_put_page(node_page, 1);
 
 	if (source_blkaddr != blkaddr)
-		return GC_NEXT;
-	return GC_OK;
+		return 0;
+	return 1;
 }
 
 static void move_data_page(struct inode *inode, struct page *page, int gc_type)
@@ -523,13 +514,13 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
  * If the parent node is not valid or the data block address is different,
  * the victim data block is ignored.
  */
-static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
+static void gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		struct list_head *ilist, unsigned int segno, int gc_type)
 {
 	struct super_block *sb = sbi->sb;
 	struct f2fs_summary *entry;
 	block_t start_addr;
-	int err, off;
+	int off;
 	int phase = 0;
 
 	start_addr = START_BLOCK(sbi, segno);
@@ -543,20 +534,11 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		unsigned int ofs_in_node, nofs;
 		block_t start_bidx;
 
-		/*
-		 * It makes sure that free segments are able to write
-		 * all the dirty node pages before CP after this CP.
-		 * So let's check the space of dirty node pages.
-		 */
-		if (should_do_checkpoint(sbi)) {
-			mutex_lock(&sbi->cp_mutex);
-			block_operations(sbi);
-			err = GC_BLOCKED;
-			goto stop;
-		}
+		/* stop BG_GC if there is not enough free sections. */
+		if (gc_type == BG_GC && has_not_enough_free_secs(sbi, 0))
+			return;
 
-		err = check_valid_map(sbi, segno, off);
-		if (err == GC_NEXT)
+		if (check_valid_map(sbi, segno, off) == 0)
 			continue;
 
 		if (phase == 0) {
@@ -565,8 +547,7 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		}
 
 		/* Get an inode by ino with checking validity */
-		err = check_dnode(sbi, entry, &dni, start_addr + off, &nofs);
-		if (err == GC_NEXT)
+		if (check_dnode(sbi, entry, &dni, start_addr + off, &nofs) == 0)
 			continue;
 
 		if (phase == 1) {
@@ -606,11 +587,9 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 	}
 	if (++phase < 4)
 		goto next_step;
-	err = GC_DONE;
-stop:
+
 	if (gc_type == FG_GC)
 		f2fs_submit_bio(sbi, DATA, true);
-	return err;
 }
 
 static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
@@ -624,17 +603,16 @@ static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
 	return ret;
 }
 
-static int do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
+static void do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 				struct list_head *ilist, int gc_type)
 {
 	struct page *sum_page;
 	struct f2fs_summary_block *sum;
-	int ret = GC_DONE;
 
 	/* read segment summary of victim */
 	sum_page = get_sum_page(sbi, segno);
 	if (IS_ERR(sum_page))
-		return GC_ERROR;
+		return;
 
 	/*
 	 * CP needs to lock sum_page. In this time, we don't need
@@ -646,17 +624,16 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 
 	switch (GET_SUM_TYPE((&sum->footer))) {
 	case SUM_TYPE_NODE:
-		ret = gc_node_segment(sbi, sum->entries, segno, gc_type);
+		gc_node_segment(sbi, sum->entries, segno, gc_type);
 		break;
 	case SUM_TYPE_DATA:
-		ret = gc_data_segment(sbi, sum->entries, ilist, segno, gc_type);
+		gc_data_segment(sbi, sum->entries, ilist, segno, gc_type);
 		break;
 	}
 	stat_inc_seg_count(sbi, GET_SUM_TYPE((&sum->footer)));
 	stat_inc_call_count(sbi->stat_info);
 
 	f2fs_put_page(sum_page, 0);
-	return ret;
 }
 
 int f2fs_gc(struct f2fs_sb_info *sbi)
@@ -664,40 +641,38 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 	struct list_head ilist;
 	unsigned int segno, i;
 	int gc_type = BG_GC;
-	int gc_status = GC_NONE;
+	int nfree = 0;
+	int ret = -1;
 
 	INIT_LIST_HEAD(&ilist);
 gc_more:
 	if (!(sbi->sb->s_flags & MS_ACTIVE))
 		goto stop;
 
-	if (gc_type == BG_GC && has_not_enough_free_secs(sbi))
+	if (gc_type == BG_GC && has_not_enough_free_secs(sbi, nfree))
 		gc_type = FG_GC;
 
 	if (!__get_victim(sbi, &segno, gc_type, NO_CHECK_TYPE))
 		goto stop;
+	ret = 0;
 
-	for (i = 0; i < sbi->segs_per_sec; i++) {
-		/*
-		 * do_garbage_collect will give us three gc_status:
-		 * GC_ERROR, GC_DONE, and GC_BLOCKED.
-		 * If GC is finished uncleanly, we have to return
-		 * the victim to dirty segment list.
-		 */
-		gc_status = do_garbage_collect(sbi, segno + i, &ilist, gc_type);
-		if (gc_status != GC_DONE)
-			break;
-	}
-	if (has_not_enough_free_secs(sbi)) {
-		write_checkpoint(sbi, (gc_status == GC_BLOCKED), false);
-		if (has_not_enough_free_secs(sbi))
-			goto gc_more;
-	}
+	for (i = 0; i < sbi->segs_per_sec; i++)
+		do_garbage_collect(sbi, segno + i, &ilist, gc_type);
+
+	if (gc_type == FG_GC &&
+			get_valid_blocks(sbi, segno, sbi->segs_per_sec) == 0)
+		nfree++;
+
+	if (has_not_enough_free_secs(sbi, nfree))
+		goto gc_more;
+
+	if (gc_type == FG_GC)
+		write_checkpoint(sbi, false);
 stop:
 	mutex_unlock(&sbi->gc_mutex);
 
 	put_gc_inode(&ilist);
-	return gc_status;
+	return ret;
 }
 
 void build_gc_manager(struct f2fs_sb_info *sbi)

commit 25718423ea6f7118f9c173adff0fac52414571b8
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sat Feb 2 23:52:42 2013 +0900

    f2fs: mark gc_thread as NULL when thread creation is failed
    
    When gc thread creation is failed, mark gc_thread as NULL to avoid
    crash while trying to stop invalid thread in stop_gc_thread->kthread_stop.
    Instead make it return from:
            if (!gc_th)
           return;
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b1e498503e59..16fdec355201 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -104,6 +104,7 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 			"f2fs_gc-%u:%u", MAJOR(dev), MINOR(dev));
 	if (IS_ERR(gc_th->f2fs_gc_task)) {
 		kfree(gc_th);
+		sbi->gc_thread = NULL;
 		return -ENOMEM;
 	}
 	return 0;

commit ec7b1f2dd180afb1ce23aa44a7451e59bf2f3eb3
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sat Feb 2 23:52:28 2013 +0900

    f2fs: name gc task as per the block device
    
    Currently GC task is started for each f2fs formatted/mounted device.
    But, when we check the task list, using 'ps', there is no distinguishing
    factor between the tasks. So, name the task as per the block device just
    like the flusher threads.
    Also, remove the macro GC_THREAD_NAME and instead use the name: f2fs_gc
    to avoid name length truncation, as the command length is 16
    -> TASK_COMM_LEN 16 and example name like:
    f2fs_gc_task:8:16 -> this exceeds name length
    
    Before Patch for 2 F2FS formatted partitions:
    root  28061  0.0  0.0  0 0 ? S 10:31   0:00 [f2fs_gc_task]
    root  28087  0.0  0.0  0 0 ? S 10:32   0:00 [f2fs_gc_task]
    
    After Patch:
    root  16756  0.0  0.0  0  0 ?  S  14:57   0:00 [f2fs_gc-8:18]
    root  16765  0.0  0.0  0  0 ?  S  14:57   0:00 [f2fs_gc-8:19]
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 8d293cb685ba..b1e498503e59 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -90,6 +90,7 @@ static int gc_thread_func(void *data)
 int start_gc_thread(struct f2fs_sb_info *sbi)
 {
 	struct f2fs_gc_kthread *gc_th;
+	dev_t dev = sbi->sb->s_bdev->bd_dev;
 
 	if (!test_opt(sbi, BG_GC))
 		return 0;
@@ -100,7 +101,7 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 	sbi->gc_thread = gc_th;
 	init_waitqueue_head(&sbi->gc_thread->gc_wait_queue_head);
 	sbi->gc_thread->f2fs_gc_task = kthread_run(gc_thread_func, sbi,
-				GC_THREAD_NAME);
+			"f2fs_gc-%u:%u", MAJOR(dev), MINOR(dev));
 	if (IS_ERR(gc_th->f2fs_gc_task)) {
 		kfree(gc_th);
 		return -ENOMEM;

commit 48600e44c18e4eb0f7c02ec8633c4c56aef292f0
Author: Changman Lee <cm224.lee@samsung.com>
Date:   Mon Feb 4 10:05:09 2013 +0900

    f2fs: remove unnecessary gc option check and balance_fs
    
     1. If f2fs is mounted with background_gc_off option, checking
        BG_GC is not redundant.
     2. f2fs_balance_fs is checked in f2fs_gc, so this is also redundant.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 375e69e2c6f1..8d293cb685ba 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -49,11 +49,6 @@ static int gc_thread_func(void *data)
 			continue;
 		}
 
-		f2fs_balance_fs(sbi);
-
-		if (!test_opt(sbi, BG_GC))
-			continue;
-
 		/*
 		 * [GC triggering condition]
 		 * 0. GC is not conducted currently.
@@ -96,6 +91,8 @@ int start_gc_thread(struct f2fs_sb_info *sbi)
 {
 	struct f2fs_gc_kthread *gc_th;
 
+	if (!test_opt(sbi, BG_GC))
+		return 0;
 	gc_th = kmalloc(sizeof(struct f2fs_gc_kthread), GFP_KERNEL);
 	if (!gc_th)
 		return -ENOMEM;

commit d4686d56ec912d55fd8a9d6d509de50de24e90ab
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Jan 31 15:36:04 2013 +0900

    f2fs: avoid balanc_fs during evict_inode
    
    1. Background
    
    Previously, if f2fs tries to move data blocks of an *evicting* inode during the
    cleaning process, it stops the process incompletely and then restarts the whole
    process, since it needs a locked inode to grab victim data pages in its address
    space. In order to get a locked inode, iget_locked() by f2fs_iget() is normally
    used, but, it waits if the inode is on freeing.
    
    So, here is a deadlock scenario.
    1. f2fs_evict_inode()       <- inode "A"
      2. f2fs_balance_fs()
        3. f2fs_gc()
          4. gc_data_segment()
            5. f2fs_iget()      <- inode "A" too!
    
    If step #1 and #5 treat a same inode "A", step #5 would fall into deadlock since
    the inode "A" is on freeing. In order to resolve this, f2fs_iget_nowait() which
    skips __wait_on_freeing_inode() was introduced in step #5, and stops f2fs_gc()
    to complete f2fs_evict_inode().
    
    1. f2fs_evict_inode()           <- inode "A"
      2. f2fs_balance_fs()
        3. f2fs_gc()
          4. gc_data_segment()
            5. f2fs_iget_nowait()   <- inode "A", then stop f2fs_gc() w/ -ENOENT
    
    2. Problem and Solution
    
    In the above scenario, however, f2fs cannot finish f2fs_evict_inode() only if:
     o there are not enough free sections, and
     o f2fs_gc() tries to move data blocks of the *evicting* inode repeatedly.
    
    So, the final solution is to use f2fs_iget() and remove f2fs_balance_fs() in
    f2fs_evict_inode().
    The f2fs_evict_inode() actually truncates all the data and node blocks, which
    means that it doesn't produce any dirty node pages accordingly.
    So, we don't need to do f2fs_balance_fs() in practical.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index fb03be68cb20..375e69e2c6f1 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -579,7 +579,7 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		ofs_in_node = le16_to_cpu(entry->ofs_in_node);
 
 		if (phase == 2) {
-			inode = f2fs_iget_nowait(sb, dni.ino);
+			inode = f2fs_iget(sb, dni.ino);
 			if (IS_ERR(inode))
 				continue;
 

commit 3786dfdf4f00755ce5797c0e87eac8f898acb52c
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Wed Jan 30 22:47:02 2013 +0900

    f2fs: avoid redundant call to has_not_enough_free_secs in f2fs_gc
    
    After doing a write_checkpoint from garbage collection path if there is still
    need to do more garbage collection, gc_more label is used to jump and start
    the process again. And in that process, first step before getting victim is to
    check if there are not enough free sections, which is already done before
    doing a jump to gc_more. We can avoid the redundant call to check free
    sections, by checking the gc_type flag which will remain FG_GC(value 1) under
    this condition.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 9b5d0aad5daf..fb03be68cb20 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -672,7 +672,7 @@ int f2fs_gc(struct f2fs_sb_info *sbi)
 	if (!(sbi->sb->s_flags & MS_ACTIVE))
 		goto stop;
 
-	if (has_not_enough_free_secs(sbi))
+	if (gc_type == BG_GC && has_not_enough_free_secs(sbi))
 		gc_type = FG_GC;
 
 	if (!__get_victim(sbi, &segno, gc_type, NO_CHECK_TYPE))

commit d6212a5f18c8f9f9cc884070a96e11907711217f
Author: Changman Lee <cm224.lee@samsung.com>
Date:   Tue Jan 29 18:30:07 2013 +0900

    f2fs: add un/freeze_fs into super_operations
    
    This patch supports ioctl FIFREEZE and FITHAW to snapshot filesystem.
    Before calling f2fs_freeze, all writers would be suspended and sync_fs
    would be completed. So no f2fs has to do something.
    Just background gc operation should be skipped due to generate dirty
    nodes and data until unfreeze.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index c386910dacc5..9b5d0aad5daf 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -44,6 +44,11 @@ static int gc_thread_func(void *data)
 		if (kthread_should_stop())
 			break;
 
+		if (sbi->sb->s_writers.frozen >= SB_FREEZE_WRITE) {
+			wait_ms = GC_THREAD_MAX_SLEEP_TIME;
+			continue;
+		}
+
 		f2fs_balance_fs(sbi);
 
 		if (!test_opt(sbi, BG_GC))

commit 9af45ef5ab8ce4a13c553200dc15509441fbd68f
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Jan 21 17:34:21 2013 +0900

    f2fs: add comments of start_bidx_of_node
    
    The caller of start_bidx_of_node() should give proper node offsets which
    point only direct node blocks. Otherwise, it is a caller's bug.
    This patch adds comments to make it clear.
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 809cfec6683c..c386910dacc5 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -424,7 +424,11 @@ static int gc_node_segment(struct f2fs_sb_info *sbi,
 }
 
 /*
- * Calculate start block index that this node page contains
+ * Calculate start block index indicating the given node offset.
+ * Be careful, caller should give this node offset only indicating direct node
+ * blocks. If any node offsets, which point the other types of node blocks such
+ * as indirect or double indirect node blocks, are given, it must be a caller's
+ * bug.
  */
 block_t start_bidx_of_node(unsigned int node_ofs)
 {

commit 6e6093a8f144414d904575da5fdea40cf14fb63e
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Thu Jan 17 00:08:30 2013 +0900

    f2fs: add __init to functions in init_f2fs_fs
    
    Add __init to functions in init_f2fs_fs for code consistency.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b4dd90cf1f18..809cfec6683c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -697,7 +697,7 @@ void build_gc_manager(struct f2fs_sb_info *sbi)
 	DIRTY_I(sbi)->v_ops = &default_v_ops;
 }
 
-int create_gc_caches(void)
+int __init create_gc_caches(void)
 {
 	winode_slab = f2fs_kmem_cache_create("f2fs_gc_inodes",
 			sizeof(struct inode_entry), NULL);

commit 408e9375610cca6d54e9c654cbe05a647687e12e
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Jan 3 17:55:52 2013 +0900

    f2fs: revisit the f2fs_gc flow
    
    I'd like to revisit the f2fs_gc flow and rewrite as follows.
    
    1. In practical, the nGC parameter of f2fs_gc is meaningless. So, let's
      remove it.
    2. Background GC marks victim blocks as dirty one at a time.
    3. Foreground GC should do cleaning job until acquiring enough free
      sections. Afterwards, it needs to do checkpoint.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index b0ec721e984a..b4dd90cf1f18 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -78,7 +78,7 @@ static int gc_thread_func(void *data)
 
 		sbi->bg_gc++;
 
-		if (f2fs_gc(sbi, 1) == GC_NONE)
+		if (f2fs_gc(sbi) == GC_NONE)
 			wait_ms = GC_THREAD_NOGC_SLEEP_TIME;
 		else if (wait_ms == GC_THREAD_NOGC_SLEEP_TIME)
 			wait_ms = GC_THREAD_MAX_SLEEP_TIME;
@@ -651,62 +651,44 @@ static int do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
 	return ret;
 }
 
-int f2fs_gc(struct f2fs_sb_info *sbi, int nGC)
+int f2fs_gc(struct f2fs_sb_info *sbi)
 {
-	unsigned int segno;
-	int old_free_secs, cur_free_secs;
-	int gc_status, nfree;
 	struct list_head ilist;
+	unsigned int segno, i;
 	int gc_type = BG_GC;
+	int gc_status = GC_NONE;
 
 	INIT_LIST_HEAD(&ilist);
 gc_more:
-	nfree = 0;
-	gc_status = GC_NONE;
+	if (!(sbi->sb->s_flags & MS_ACTIVE))
+		goto stop;
 
 	if (has_not_enough_free_secs(sbi))
-		old_free_secs = reserved_sections(sbi);
-	else
-		old_free_secs = free_sections(sbi);
-
-	while (sbi->sb->s_flags & MS_ACTIVE) {
-		int i;
-		if (has_not_enough_free_secs(sbi))
-			gc_type = FG_GC;
+		gc_type = FG_GC;
 
-		cur_free_secs = free_sections(sbi) + nfree;
+	if (!__get_victim(sbi, &segno, gc_type, NO_CHECK_TYPE))
+		goto stop;
 
-		/* We got free space successfully. */
-		if (nGC < cur_free_secs - old_free_secs)
-			break;
-
-		if (!__get_victim(sbi, &segno, gc_type, NO_CHECK_TYPE))
+	for (i = 0; i < sbi->segs_per_sec; i++) {
+		/*
+		 * do_garbage_collect will give us three gc_status:
+		 * GC_ERROR, GC_DONE, and GC_BLOCKED.
+		 * If GC is finished uncleanly, we have to return
+		 * the victim to dirty segment list.
+		 */
+		gc_status = do_garbage_collect(sbi, segno + i, &ilist, gc_type);
+		if (gc_status != GC_DONE)
 			break;
-
-		for (i = 0; i < sbi->segs_per_sec; i++) {
-			/*
-			 * do_garbage_collect will give us three gc_status:
-			 * GC_ERROR, GC_DONE, and GC_BLOCKED.
-			 * If GC is finished uncleanly, we have to return
-			 * the victim to dirty segment list.
-			 */
-			gc_status = do_garbage_collect(sbi, segno + i,
-					&ilist, gc_type);
-			if (gc_status != GC_DONE)
-				goto stop;
-			nfree++;
-		}
 	}
-stop:
-	if (has_not_enough_free_secs(sbi) || gc_status == GC_BLOCKED) {
+	if (has_not_enough_free_secs(sbi)) {
 		write_checkpoint(sbi, (gc_status == GC_BLOCKED), false);
-		if (nfree)
+		if (has_not_enough_free_secs(sbi))
 			goto gc_more;
 	}
+stop:
 	mutex_unlock(&sbi->gc_mutex);
 
 	put_gc_inode(&ilist);
-	BUG_ON(!list_empty(&ilist));
 	return gc_status;
 }
 

commit 2b50638decdb9a8585654a5acf1c8ce5962f1951
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Dec 26 14:39:50 2012 +0900

    f2fs: clean up unused variables and return values
    
    This patch cleans up a couple of unnecessary codes related to unused variables
    and return values.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index eda8230deb0c..b0ec721e984a 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -390,9 +390,7 @@ static int gc_node_segment(struct f2fs_sb_info *sbi,
 		}
 
 		err = check_valid_map(sbi, segno, off);
-		if (err == GC_ERROR)
-			return err;
-		else if (err == GC_NEXT)
+		if (err == GC_NEXT)
 			continue;
 
 		if (initial) {
@@ -550,9 +548,7 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		}
 
 		err = check_valid_map(sbi, segno, off);
-		if (err == GC_ERROR)
-			goto stop;
-		else if (err == GC_NEXT)
+		if (err == GC_NEXT)
 			continue;
 
 		if (phase == 0) {
@@ -562,9 +558,7 @@ static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 
 		/* Get an inode by ino with checking validity */
 		err = check_dnode(sbi, entry, &dni, start_addr + off, &nofs);
-		if (err == GC_ERROR)
-			goto stop;
-		else if (err == GC_NEXT)
+		if (err == GC_NEXT)
 			continue;
 
 		if (phase == 1) {

commit ce19a5d4321911f98d42e4d724630ae48f413719
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Dec 26 12:03:22 2012 +0900

    f2fs: clean up the start_bidx_of_node function
    
    This patch also resolves the following warning reported by kbuild test robot.
    
    fs/f2fs/gc.c: In function 'start_bidx_of_node':
    fs/f2fs/gc.c:453:21: warning: 'bidx' may be used uninitialized in this function
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 644aa3808273..eda8230deb0c 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -430,28 +430,22 @@ static int gc_node_segment(struct f2fs_sb_info *sbi,
  */
 block_t start_bidx_of_node(unsigned int node_ofs)
 {
-	block_t start_bidx;
-	unsigned int bidx, indirect_blks;
-	int dec;
+	unsigned int indirect_blks = 2 * NIDS_PER_BLOCK + 4;
+	unsigned int bidx;
 
-	indirect_blks = 2 * NIDS_PER_BLOCK + 4;
+	if (node_ofs == 0)
+		return 0;
 
-	start_bidx = 1;
-	if (node_ofs == 0) {
-		start_bidx = 0;
-	} else if (node_ofs <= 2) {
+	if (node_ofs <= 2) {
 		bidx = node_ofs - 1;
 	} else if (node_ofs <= indirect_blks) {
-		dec = (node_ofs - 4) / (NIDS_PER_BLOCK + 1);
+		int dec = (node_ofs - 4) / (NIDS_PER_BLOCK + 1);
 		bidx = node_ofs - 2 - dec;
 	} else {
-		dec = (node_ofs - indirect_blks - 3) / (NIDS_PER_BLOCK + 1);
+		int dec = (node_ofs - indirect_blks - 3) / (NIDS_PER_BLOCK + 1);
 		bidx = node_ofs - 5 - dec;
 	}
-
-	if (start_bidx)
-		start_bidx = bidx * ADDRS_PER_BLOCK + ADDRS_PER_INODE;
-	return start_bidx;
+	return bidx * ADDRS_PER_BLOCK + ADDRS_PER_INODE;
 }
 
 static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,

commit 1042d60f917d78ef1a6eaea297a1020484d4bf74
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sat Dec 1 10:56:13 2012 +0900

    f2fs: remove unneeded initialization
    
    No need to initialize  "struct f2fs_gc_kthread *gc_th = NULL",
    as gc_th = NULL, will be taken care by the return values of kmalloc().
    And fix codes in other places.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 3271be42c0b6..644aa3808273 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -89,7 +89,7 @@ static int gc_thread_func(void *data)
 
 int start_gc_thread(struct f2fs_sb_info *sbi)
 {
-	struct f2fs_gc_kthread *gc_th = NULL;
+	struct f2fs_gc_kthread *gc_th;
 
 	gc_th = kmalloc(sizeof(struct f2fs_gc_kthread), GFP_KERNEL);
 	if (!gc_th)

commit 0a8165d7c2cf1395059db20ab07665baf3758fcd
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Nov 29 13:28:09 2012 +0900

    f2fs: adjust kernel coding style
    
    As pointed out by Randy Dunlap, this patch removes all usage of "/**" for comment
    blocks. Instead, just use "/*".
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
index 46774ce3ae03..3271be42c0b6 100644
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1,4 +1,4 @@
-/**
+/*
  * fs/f2fs/gc.c
  *
  * Copyright (c) 2012 Samsung Electronics Co., Ltd.
@@ -213,7 +213,7 @@ static unsigned int get_gc_cost(struct f2fs_sb_info *sbi, unsigned int segno,
 		return get_cb_cost(sbi, segno);
 }
 
-/**
+/*
  * This function is called from two pathes.
  * One is garbage collection and the other is SSR segment selection.
  * When it is called during GC, it just gets a victim segment
@@ -359,7 +359,7 @@ static int check_valid_map(struct f2fs_sb_info *sbi,
 	return ret ? GC_OK : GC_NEXT;
 }
 
-/**
+/*
  * This function compares node address got in summary with that in NAT.
  * On validity, copy that node with cold status, otherwise (invalid node)
  * ignore that.
@@ -425,7 +425,7 @@ static int gc_node_segment(struct f2fs_sb_info *sbi,
 	return GC_DONE;
 }
 
-/**
+/*
  * Calculate start block index that this node page contains
  */
 block_t start_bidx_of_node(unsigned int node_ofs)
@@ -516,7 +516,7 @@ static void move_data_page(struct inode *inode, struct page *page, int gc_type)
 	f2fs_put_page(page, 1);
 }
 
-/**
+/*
  * This function tries to get parent node of victim data block, and identifies
  * data block validity. If the block is valid, copy that with cold status and
  * modify parent node.

commit 7bc0900347e069a1676d28ad6f98cafaf8cfd6e9
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Nov 2 17:13:01 2012 +0900

    f2fs: add garbage collection functions
    
    This adds on-demand and background cleaning functions.
    
    - The basic background cleaning policy is trying to do cleaning jobs as much as
      possible whenever the system is idle. Once the background cleaning is done,
      the cleaner sleeps an amount of time not to interfere with VFS calls. The time
      is dynamically adjusted according to the status of whole segments, which is
      decreased when the following conditions are satisfied.
    
      . GC is not conducted currently, and
      . IO subsystem is idle by checking the number of requets in bdev's request
         list, and
      . There are enough dirty segments.
    
      Otherwise, the time is increased incrementally until to the maximum time.
      Note that, min and max times are 10 secs and 30 secs by default.
    
    - F2FS adopts a default victim selection policy where background cleaning uses
      a cost-benefit algorithm, while on-demand cleaning uses a greedy algorithm.
    
    - The method of moving data during the cleaning is slightly different between
      background and on-demand cleaning schemes. In the case of background cleaning,
      F2FS loads the data, and marks them as dirty. Then, F2FS expects that the data
      will be moved by flusher or VM. In the case of on-demand cleaning, F2FS should
      move the data right away.
    
    - In order to identify valid blocks in a victim segment, F2FS scans the bitmap
      of the segment managed as an SIT entry.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
new file mode 100644
index 000000000000..46774ce3ae03
--- /dev/null
+++ b/fs/f2fs/gc.c
@@ -0,0 +1,742 @@
+/**
+ * fs/f2fs/gc.c
+ *
+ * Copyright (c) 2012 Samsung Electronics Co., Ltd.
+ *             http://www.samsung.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/fs.h>
+#include <linux/module.h>
+#include <linux/backing-dev.h>
+#include <linux/proc_fs.h>
+#include <linux/init.h>
+#include <linux/f2fs_fs.h>
+#include <linux/kthread.h>
+#include <linux/delay.h>
+#include <linux/freezer.h>
+#include <linux/blkdev.h>
+
+#include "f2fs.h"
+#include "node.h"
+#include "segment.h"
+#include "gc.h"
+
+static struct kmem_cache *winode_slab;
+
+static int gc_thread_func(void *data)
+{
+	struct f2fs_sb_info *sbi = data;
+	wait_queue_head_t *wq = &sbi->gc_thread->gc_wait_queue_head;
+	long wait_ms;
+
+	wait_ms = GC_THREAD_MIN_SLEEP_TIME;
+
+	do {
+		if (try_to_freeze())
+			continue;
+		else
+			wait_event_interruptible_timeout(*wq,
+						kthread_should_stop(),
+						msecs_to_jiffies(wait_ms));
+		if (kthread_should_stop())
+			break;
+
+		f2fs_balance_fs(sbi);
+
+		if (!test_opt(sbi, BG_GC))
+			continue;
+
+		/*
+		 * [GC triggering condition]
+		 * 0. GC is not conducted currently.
+		 * 1. There are enough dirty segments.
+		 * 2. IO subsystem is idle by checking the # of writeback pages.
+		 * 3. IO subsystem is idle by checking the # of requests in
+		 *    bdev's request list.
+		 *
+		 * Note) We have to avoid triggering GCs too much frequently.
+		 * Because it is possible that some segments can be
+		 * invalidated soon after by user update or deletion.
+		 * So, I'd like to wait some time to collect dirty segments.
+		 */
+		if (!mutex_trylock(&sbi->gc_mutex))
+			continue;
+
+		if (!is_idle(sbi)) {
+			wait_ms = increase_sleep_time(wait_ms);
+			mutex_unlock(&sbi->gc_mutex);
+			continue;
+		}
+
+		if (has_enough_invalid_blocks(sbi))
+			wait_ms = decrease_sleep_time(wait_ms);
+		else
+			wait_ms = increase_sleep_time(wait_ms);
+
+		sbi->bg_gc++;
+
+		if (f2fs_gc(sbi, 1) == GC_NONE)
+			wait_ms = GC_THREAD_NOGC_SLEEP_TIME;
+		else if (wait_ms == GC_THREAD_NOGC_SLEEP_TIME)
+			wait_ms = GC_THREAD_MAX_SLEEP_TIME;
+
+	} while (!kthread_should_stop());
+	return 0;
+}
+
+int start_gc_thread(struct f2fs_sb_info *sbi)
+{
+	struct f2fs_gc_kthread *gc_th = NULL;
+
+	gc_th = kmalloc(sizeof(struct f2fs_gc_kthread), GFP_KERNEL);
+	if (!gc_th)
+		return -ENOMEM;
+
+	sbi->gc_thread = gc_th;
+	init_waitqueue_head(&sbi->gc_thread->gc_wait_queue_head);
+	sbi->gc_thread->f2fs_gc_task = kthread_run(gc_thread_func, sbi,
+				GC_THREAD_NAME);
+	if (IS_ERR(gc_th->f2fs_gc_task)) {
+		kfree(gc_th);
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+void stop_gc_thread(struct f2fs_sb_info *sbi)
+{
+	struct f2fs_gc_kthread *gc_th = sbi->gc_thread;
+	if (!gc_th)
+		return;
+	kthread_stop(gc_th->f2fs_gc_task);
+	kfree(gc_th);
+	sbi->gc_thread = NULL;
+}
+
+static int select_gc_type(int gc_type)
+{
+	return (gc_type == BG_GC) ? GC_CB : GC_GREEDY;
+}
+
+static void select_policy(struct f2fs_sb_info *sbi, int gc_type,
+			int type, struct victim_sel_policy *p)
+{
+	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
+
+	if (p->alloc_mode) {
+		p->gc_mode = GC_GREEDY;
+		p->dirty_segmap = dirty_i->dirty_segmap[type];
+		p->ofs_unit = 1;
+	} else {
+		p->gc_mode = select_gc_type(gc_type);
+		p->dirty_segmap = dirty_i->dirty_segmap[DIRTY];
+		p->ofs_unit = sbi->segs_per_sec;
+	}
+	p->offset = sbi->last_victim[p->gc_mode];
+}
+
+static unsigned int get_max_cost(struct f2fs_sb_info *sbi,
+				struct victim_sel_policy *p)
+{
+	if (p->gc_mode == GC_GREEDY)
+		return (1 << sbi->log_blocks_per_seg) * p->ofs_unit;
+	else if (p->gc_mode == GC_CB)
+		return UINT_MAX;
+	else /* No other gc_mode */
+		return 0;
+}
+
+static unsigned int check_bg_victims(struct f2fs_sb_info *sbi)
+{
+	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
+	unsigned int segno;
+
+	/*
+	 * If the gc_type is FG_GC, we can select victim segments
+	 * selected by background GC before.
+	 * Those segments guarantee they have small valid blocks.
+	 */
+	segno = find_next_bit(dirty_i->victim_segmap[BG_GC],
+						TOTAL_SEGS(sbi), 0);
+	if (segno < TOTAL_SEGS(sbi)) {
+		clear_bit(segno, dirty_i->victim_segmap[BG_GC]);
+		return segno;
+	}
+	return NULL_SEGNO;
+}
+
+static unsigned int get_cb_cost(struct f2fs_sb_info *sbi, unsigned int segno)
+{
+	struct sit_info *sit_i = SIT_I(sbi);
+	unsigned int secno = GET_SECNO(sbi, segno);
+	unsigned int start = secno * sbi->segs_per_sec;
+	unsigned long long mtime = 0;
+	unsigned int vblocks;
+	unsigned char age = 0;
+	unsigned char u;
+	unsigned int i;
+
+	for (i = 0; i < sbi->segs_per_sec; i++)
+		mtime += get_seg_entry(sbi, start + i)->mtime;
+	vblocks = get_valid_blocks(sbi, segno, sbi->segs_per_sec);
+
+	mtime = div_u64(mtime, sbi->segs_per_sec);
+	vblocks = div_u64(vblocks, sbi->segs_per_sec);
+
+	u = (vblocks * 100) >> sbi->log_blocks_per_seg;
+
+	/* Handle if the system time is changed by user */
+	if (mtime < sit_i->min_mtime)
+		sit_i->min_mtime = mtime;
+	if (mtime > sit_i->max_mtime)
+		sit_i->max_mtime = mtime;
+	if (sit_i->max_mtime != sit_i->min_mtime)
+		age = 100 - div64_u64(100 * (mtime - sit_i->min_mtime),
+				sit_i->max_mtime - sit_i->min_mtime);
+
+	return UINT_MAX - ((100 * (100 - u) * age) / (100 + u));
+}
+
+static unsigned int get_gc_cost(struct f2fs_sb_info *sbi, unsigned int segno,
+					struct victim_sel_policy *p)
+{
+	if (p->alloc_mode == SSR)
+		return get_seg_entry(sbi, segno)->ckpt_valid_blocks;
+
+	/* alloc_mode == LFS */
+	if (p->gc_mode == GC_GREEDY)
+		return get_valid_blocks(sbi, segno, sbi->segs_per_sec);
+	else
+		return get_cb_cost(sbi, segno);
+}
+
+/**
+ * This function is called from two pathes.
+ * One is garbage collection and the other is SSR segment selection.
+ * When it is called during GC, it just gets a victim segment
+ * and it does not remove it from dirty seglist.
+ * When it is called from SSR segment selection, it finds a segment
+ * which has minimum valid blocks and removes it from dirty seglist.
+ */
+static int get_victim_by_default(struct f2fs_sb_info *sbi,
+		unsigned int *result, int gc_type, int type, char alloc_mode)
+{
+	struct dirty_seglist_info *dirty_i = DIRTY_I(sbi);
+	struct victim_sel_policy p;
+	unsigned int segno;
+	int nsearched = 0;
+
+	p.alloc_mode = alloc_mode;
+	select_policy(sbi, gc_type, type, &p);
+
+	p.min_segno = NULL_SEGNO;
+	p.min_cost = get_max_cost(sbi, &p);
+
+	mutex_lock(&dirty_i->seglist_lock);
+
+	if (p.alloc_mode == LFS && gc_type == FG_GC) {
+		p.min_segno = check_bg_victims(sbi);
+		if (p.min_segno != NULL_SEGNO)
+			goto got_it;
+	}
+
+	while (1) {
+		unsigned long cost;
+
+		segno = find_next_bit(p.dirty_segmap,
+						TOTAL_SEGS(sbi), p.offset);
+		if (segno >= TOTAL_SEGS(sbi)) {
+			if (sbi->last_victim[p.gc_mode]) {
+				sbi->last_victim[p.gc_mode] = 0;
+				p.offset = 0;
+				continue;
+			}
+			break;
+		}
+		p.offset = ((segno / p.ofs_unit) * p.ofs_unit) + p.ofs_unit;
+
+		if (test_bit(segno, dirty_i->victim_segmap[FG_GC]))
+			continue;
+		if (gc_type == BG_GC &&
+				test_bit(segno, dirty_i->victim_segmap[BG_GC]))
+			continue;
+		if (IS_CURSEC(sbi, GET_SECNO(sbi, segno)))
+			continue;
+
+		cost = get_gc_cost(sbi, segno, &p);
+
+		if (p.min_cost > cost) {
+			p.min_segno = segno;
+			p.min_cost = cost;
+		}
+
+		if (cost == get_max_cost(sbi, &p))
+			continue;
+
+		if (nsearched++ >= MAX_VICTIM_SEARCH) {
+			sbi->last_victim[p.gc_mode] = segno;
+			break;
+		}
+	}
+got_it:
+	if (p.min_segno != NULL_SEGNO) {
+		*result = (p.min_segno / p.ofs_unit) * p.ofs_unit;
+		if (p.alloc_mode == LFS) {
+			int i;
+			for (i = 0; i < p.ofs_unit; i++)
+				set_bit(*result + i,
+					dirty_i->victim_segmap[gc_type]);
+		}
+	}
+	mutex_unlock(&dirty_i->seglist_lock);
+
+	return (p.min_segno == NULL_SEGNO) ? 0 : 1;
+}
+
+static const struct victim_selection default_v_ops = {
+	.get_victim = get_victim_by_default,
+};
+
+static struct inode *find_gc_inode(nid_t ino, struct list_head *ilist)
+{
+	struct list_head *this;
+	struct inode_entry *ie;
+
+	list_for_each(this, ilist) {
+		ie = list_entry(this, struct inode_entry, list);
+		if (ie->inode->i_ino == ino)
+			return ie->inode;
+	}
+	return NULL;
+}
+
+static void add_gc_inode(struct inode *inode, struct list_head *ilist)
+{
+	struct list_head *this;
+	struct inode_entry *new_ie, *ie;
+
+	list_for_each(this, ilist) {
+		ie = list_entry(this, struct inode_entry, list);
+		if (ie->inode == inode) {
+			iput(inode);
+			return;
+		}
+	}
+repeat:
+	new_ie = kmem_cache_alloc(winode_slab, GFP_NOFS);
+	if (!new_ie) {
+		cond_resched();
+		goto repeat;
+	}
+	new_ie->inode = inode;
+	list_add_tail(&new_ie->list, ilist);
+}
+
+static void put_gc_inode(struct list_head *ilist)
+{
+	struct inode_entry *ie, *next_ie;
+	list_for_each_entry_safe(ie, next_ie, ilist, list) {
+		iput(ie->inode);
+		list_del(&ie->list);
+		kmem_cache_free(winode_slab, ie);
+	}
+}
+
+static int check_valid_map(struct f2fs_sb_info *sbi,
+				unsigned int segno, int offset)
+{
+	struct sit_info *sit_i = SIT_I(sbi);
+	struct seg_entry *sentry;
+	int ret;
+
+	mutex_lock(&sit_i->sentry_lock);
+	sentry = get_seg_entry(sbi, segno);
+	ret = f2fs_test_bit(offset, sentry->cur_valid_map);
+	mutex_unlock(&sit_i->sentry_lock);
+	return ret ? GC_OK : GC_NEXT;
+}
+
+/**
+ * This function compares node address got in summary with that in NAT.
+ * On validity, copy that node with cold status, otherwise (invalid node)
+ * ignore that.
+ */
+static int gc_node_segment(struct f2fs_sb_info *sbi,
+		struct f2fs_summary *sum, unsigned int segno, int gc_type)
+{
+	bool initial = true;
+	struct f2fs_summary *entry;
+	int off;
+
+next_step:
+	entry = sum;
+	for (off = 0; off < sbi->blocks_per_seg; off++, entry++) {
+		nid_t nid = le32_to_cpu(entry->nid);
+		struct page *node_page;
+		int err;
+
+		/*
+		 * It makes sure that free segments are able to write
+		 * all the dirty node pages before CP after this CP.
+		 * So let's check the space of dirty node pages.
+		 */
+		if (should_do_checkpoint(sbi)) {
+			mutex_lock(&sbi->cp_mutex);
+			block_operations(sbi);
+			return GC_BLOCKED;
+		}
+
+		err = check_valid_map(sbi, segno, off);
+		if (err == GC_ERROR)
+			return err;
+		else if (err == GC_NEXT)
+			continue;
+
+		if (initial) {
+			ra_node_page(sbi, nid);
+			continue;
+		}
+		node_page = get_node_page(sbi, nid);
+		if (IS_ERR(node_page))
+			continue;
+
+		/* set page dirty and write it */
+		if (!PageWriteback(node_page))
+			set_page_dirty(node_page);
+		f2fs_put_page(node_page, 1);
+		stat_inc_node_blk_count(sbi, 1);
+	}
+	if (initial) {
+		initial = false;
+		goto next_step;
+	}
+
+	if (gc_type == FG_GC) {
+		struct writeback_control wbc = {
+			.sync_mode = WB_SYNC_ALL,
+			.nr_to_write = LONG_MAX,
+			.for_reclaim = 0,
+		};
+		sync_node_pages(sbi, 0, &wbc);
+	}
+	return GC_DONE;
+}
+
+/**
+ * Calculate start block index that this node page contains
+ */
+block_t start_bidx_of_node(unsigned int node_ofs)
+{
+	block_t start_bidx;
+	unsigned int bidx, indirect_blks;
+	int dec;
+
+	indirect_blks = 2 * NIDS_PER_BLOCK + 4;
+
+	start_bidx = 1;
+	if (node_ofs == 0) {
+		start_bidx = 0;
+	} else if (node_ofs <= 2) {
+		bidx = node_ofs - 1;
+	} else if (node_ofs <= indirect_blks) {
+		dec = (node_ofs - 4) / (NIDS_PER_BLOCK + 1);
+		bidx = node_ofs - 2 - dec;
+	} else {
+		dec = (node_ofs - indirect_blks - 3) / (NIDS_PER_BLOCK + 1);
+		bidx = node_ofs - 5 - dec;
+	}
+
+	if (start_bidx)
+		start_bidx = bidx * ADDRS_PER_BLOCK + ADDRS_PER_INODE;
+	return start_bidx;
+}
+
+static int check_dnode(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
+		struct node_info *dni, block_t blkaddr, unsigned int *nofs)
+{
+	struct page *node_page;
+	nid_t nid;
+	unsigned int ofs_in_node;
+	block_t source_blkaddr;
+
+	nid = le32_to_cpu(sum->nid);
+	ofs_in_node = le16_to_cpu(sum->ofs_in_node);
+
+	node_page = get_node_page(sbi, nid);
+	if (IS_ERR(node_page))
+		return GC_NEXT;
+
+	get_node_info(sbi, nid, dni);
+
+	if (sum->version != dni->version) {
+		f2fs_put_page(node_page, 1);
+		return GC_NEXT;
+	}
+
+	*nofs = ofs_of_node(node_page);
+	source_blkaddr = datablock_addr(node_page, ofs_in_node);
+	f2fs_put_page(node_page, 1);
+
+	if (source_blkaddr != blkaddr)
+		return GC_NEXT;
+	return GC_OK;
+}
+
+static void move_data_page(struct inode *inode, struct page *page, int gc_type)
+{
+	if (page->mapping != inode->i_mapping)
+		goto out;
+
+	if (inode != page->mapping->host)
+		goto out;
+
+	if (PageWriteback(page))
+		goto out;
+
+	if (gc_type == BG_GC) {
+		set_page_dirty(page);
+		set_cold_data(page);
+	} else {
+		struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+		mutex_lock_op(sbi, DATA_WRITE);
+		if (clear_page_dirty_for_io(page) &&
+			S_ISDIR(inode->i_mode)) {
+			dec_page_count(sbi, F2FS_DIRTY_DENTS);
+			inode_dec_dirty_dents(inode);
+		}
+		set_cold_data(page);
+		do_write_data_page(page);
+		mutex_unlock_op(sbi, DATA_WRITE);
+		clear_cold_data(page);
+	}
+out:
+	f2fs_put_page(page, 1);
+}
+
+/**
+ * This function tries to get parent node of victim data block, and identifies
+ * data block validity. If the block is valid, copy that with cold status and
+ * modify parent node.
+ * If the parent node is not valid or the data block address is different,
+ * the victim data block is ignored.
+ */
+static int gc_data_segment(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
+		struct list_head *ilist, unsigned int segno, int gc_type)
+{
+	struct super_block *sb = sbi->sb;
+	struct f2fs_summary *entry;
+	block_t start_addr;
+	int err, off;
+	int phase = 0;
+
+	start_addr = START_BLOCK(sbi, segno);
+
+next_step:
+	entry = sum;
+	for (off = 0; off < sbi->blocks_per_seg; off++, entry++) {
+		struct page *data_page;
+		struct inode *inode;
+		struct node_info dni; /* dnode info for the data */
+		unsigned int ofs_in_node, nofs;
+		block_t start_bidx;
+
+		/*
+		 * It makes sure that free segments are able to write
+		 * all the dirty node pages before CP after this CP.
+		 * So let's check the space of dirty node pages.
+		 */
+		if (should_do_checkpoint(sbi)) {
+			mutex_lock(&sbi->cp_mutex);
+			block_operations(sbi);
+			err = GC_BLOCKED;
+			goto stop;
+		}
+
+		err = check_valid_map(sbi, segno, off);
+		if (err == GC_ERROR)
+			goto stop;
+		else if (err == GC_NEXT)
+			continue;
+
+		if (phase == 0) {
+			ra_node_page(sbi, le32_to_cpu(entry->nid));
+			continue;
+		}
+
+		/* Get an inode by ino with checking validity */
+		err = check_dnode(sbi, entry, &dni, start_addr + off, &nofs);
+		if (err == GC_ERROR)
+			goto stop;
+		else if (err == GC_NEXT)
+			continue;
+
+		if (phase == 1) {
+			ra_node_page(sbi, dni.ino);
+			continue;
+		}
+
+		start_bidx = start_bidx_of_node(nofs);
+		ofs_in_node = le16_to_cpu(entry->ofs_in_node);
+
+		if (phase == 2) {
+			inode = f2fs_iget_nowait(sb, dni.ino);
+			if (IS_ERR(inode))
+				continue;
+
+			data_page = find_data_page(inode,
+					start_bidx + ofs_in_node);
+			if (IS_ERR(data_page))
+				goto next_iput;
+
+			f2fs_put_page(data_page, 0);
+			add_gc_inode(inode, ilist);
+		} else {
+			inode = find_gc_inode(dni.ino, ilist);
+			if (inode) {
+				data_page = get_lock_data_page(inode,
+						start_bidx + ofs_in_node);
+				if (IS_ERR(data_page))
+					continue;
+				move_data_page(inode, data_page, gc_type);
+				stat_inc_data_blk_count(sbi, 1);
+			}
+		}
+		continue;
+next_iput:
+		iput(inode);
+	}
+	if (++phase < 4)
+		goto next_step;
+	err = GC_DONE;
+stop:
+	if (gc_type == FG_GC)
+		f2fs_submit_bio(sbi, DATA, true);
+	return err;
+}
+
+static int __get_victim(struct f2fs_sb_info *sbi, unsigned int *victim,
+						int gc_type, int type)
+{
+	struct sit_info *sit_i = SIT_I(sbi);
+	int ret;
+	mutex_lock(&sit_i->sentry_lock);
+	ret = DIRTY_I(sbi)->v_ops->get_victim(sbi, victim, gc_type, type, LFS);
+	mutex_unlock(&sit_i->sentry_lock);
+	return ret;
+}
+
+static int do_garbage_collect(struct f2fs_sb_info *sbi, unsigned int segno,
+				struct list_head *ilist, int gc_type)
+{
+	struct page *sum_page;
+	struct f2fs_summary_block *sum;
+	int ret = GC_DONE;
+
+	/* read segment summary of victim */
+	sum_page = get_sum_page(sbi, segno);
+	if (IS_ERR(sum_page))
+		return GC_ERROR;
+
+	/*
+	 * CP needs to lock sum_page. In this time, we don't need
+	 * to lock this page, because this summary page is not gone anywhere.
+	 * Also, this page is not gonna be updated before GC is done.
+	 */
+	unlock_page(sum_page);
+	sum = page_address(sum_page);
+
+	switch (GET_SUM_TYPE((&sum->footer))) {
+	case SUM_TYPE_NODE:
+		ret = gc_node_segment(sbi, sum->entries, segno, gc_type);
+		break;
+	case SUM_TYPE_DATA:
+		ret = gc_data_segment(sbi, sum->entries, ilist, segno, gc_type);
+		break;
+	}
+	stat_inc_seg_count(sbi, GET_SUM_TYPE((&sum->footer)));
+	stat_inc_call_count(sbi->stat_info);
+
+	f2fs_put_page(sum_page, 0);
+	return ret;
+}
+
+int f2fs_gc(struct f2fs_sb_info *sbi, int nGC)
+{
+	unsigned int segno;
+	int old_free_secs, cur_free_secs;
+	int gc_status, nfree;
+	struct list_head ilist;
+	int gc_type = BG_GC;
+
+	INIT_LIST_HEAD(&ilist);
+gc_more:
+	nfree = 0;
+	gc_status = GC_NONE;
+
+	if (has_not_enough_free_secs(sbi))
+		old_free_secs = reserved_sections(sbi);
+	else
+		old_free_secs = free_sections(sbi);
+
+	while (sbi->sb->s_flags & MS_ACTIVE) {
+		int i;
+		if (has_not_enough_free_secs(sbi))
+			gc_type = FG_GC;
+
+		cur_free_secs = free_sections(sbi) + nfree;
+
+		/* We got free space successfully. */
+		if (nGC < cur_free_secs - old_free_secs)
+			break;
+
+		if (!__get_victim(sbi, &segno, gc_type, NO_CHECK_TYPE))
+			break;
+
+		for (i = 0; i < sbi->segs_per_sec; i++) {
+			/*
+			 * do_garbage_collect will give us three gc_status:
+			 * GC_ERROR, GC_DONE, and GC_BLOCKED.
+			 * If GC is finished uncleanly, we have to return
+			 * the victim to dirty segment list.
+			 */
+			gc_status = do_garbage_collect(sbi, segno + i,
+					&ilist, gc_type);
+			if (gc_status != GC_DONE)
+				goto stop;
+			nfree++;
+		}
+	}
+stop:
+	if (has_not_enough_free_secs(sbi) || gc_status == GC_BLOCKED) {
+		write_checkpoint(sbi, (gc_status == GC_BLOCKED), false);
+		if (nfree)
+			goto gc_more;
+	}
+	mutex_unlock(&sbi->gc_mutex);
+
+	put_gc_inode(&ilist);
+	BUG_ON(!list_empty(&ilist));
+	return gc_status;
+}
+
+void build_gc_manager(struct f2fs_sb_info *sbi)
+{
+	DIRTY_I(sbi)->v_ops = &default_v_ops;
+}
+
+int create_gc_caches(void)
+{
+	winode_slab = f2fs_kmem_cache_create("f2fs_gc_inodes",
+			sizeof(struct inode_entry), NULL);
+	if (!winode_slab)
+		return -ENOMEM;
+	return 0;
+}
+
+void destroy_gc_caches(void)
+{
+	kmem_cache_destroy(winode_slab);
+}
