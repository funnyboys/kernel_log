commit d29fbcdb051fbe8e6aee87dd5a7df723f299494d
Author: Nishad Kamdar <nishadkamdar@gmail.com>
Date:   Sat Apr 25 18:49:08 2020 +0530

    f2fs: Use the correct style for SPDX License Identifier
    
    This patch corrects the SPDX License Identifier style in
    header files related to F2FS File System support.
    For C header files Documentation/process/license-rules.rst
    mandates C-like comments (opposed to C source files where
    C++ style should be used).
    
    Changes made by using a script provided by Joe Perches here:
    https://lkml.org/lkml/2019/2/7/46.
    
    Suggested-by: Joe Perches <joe@perches.com>
    Signed-off-by: Nishad Kamdar <nishadkamdar@gmail.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 7a83bd530812..cba16cca5189 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -1,4 +1,4 @@
-// SPDX-License-Identifier: GPL-2.0
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  * fs/f2fs/segment.h
  *

commit a7e679b53393c76d4427f4085ec9d114f06cac5a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Feb 25 19:08:16 2020 -0800

    f2fs: show mounted time
    
    Let's show mounted time.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 459dc3901a57..7a83bd530812 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -756,7 +756,7 @@ static inline unsigned long long get_mtime(struct f2fs_sb_info *sbi,
 						bool base_time)
 {
 	struct sit_info *sit_i = SIT_I(sbi);
-	time64_t diff, now = ktime_get_real_seconds();
+	time64_t diff, now = ktime_get_boottime_seconds();
 
 	if (now >= sit_i->mounted_time)
 		return sit_i->elapsed_time + now - sit_i->mounted_time;

commit d7b0a23d81507deb095859250cd7dd4aa5d8875e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jan 8 15:10:02 2020 -0800

    f2fs: update f2fs document regarding to fsync_mode
    
    This patch adds missing fsync_mode entry in f2fs document.
    
    Fixes: 04485987f053 ("f2fs: introduce async IPU policy")
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 5e6cd8d8411d..459dc3901a57 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -607,9 +607,10 @@ static inline int utilization(struct f2fs_sb_info *sbi)
  *                     threashold,
  * F2FS_IPU_FSYNC - activated in fsync path only for high performance flash
  *                     storages. IPU will be triggered only if the # of dirty
- *                     pages over min_fsync_blocks.
+ *                     pages over min_fsync_blocks. (=default option)
+ * F2FS_IPU_ASYNC - do IPU given by asynchronous write requests.
  * F2FS_IPU_NOCACHE - disable IPU bio cache.
- * F2FS_IPUT_DISABLE - disable IPU. (=default option)
+ * F2FS_IPUT_DISABLE - disable IPU. (=default option in LFS mode)
  */
 #define DEF_MIN_IPU_UTIL	70
 #define DEF_MIN_FSYNC_BLOCKS	8

commit 0e7f41974e9c6ceac3ac9b5d0377c5f74b1bc4fc
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Jan 6 16:43:09 2020 -0800

    f2fs: add a way to turn off ipu bio cache
    
    Setting 0x40 in /sys/fs/f2fs/dev/ipu_policy gives a way to turn off
    bio cache, which is useufl to check whether block layer using hardware
    encryption engine merges IOs correctly.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index a1b3951367cd..5e6cd8d8411d 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -608,6 +608,7 @@ static inline int utilization(struct f2fs_sb_info *sbi)
  * F2FS_IPU_FSYNC - activated in fsync path only for high performance flash
  *                     storages. IPU will be triggered only if the # of dirty
  *                     pages over min_fsync_blocks.
+ * F2FS_IPU_NOCACHE - disable IPU bio cache.
  * F2FS_IPUT_DISABLE - disable IPU. (=default option)
  */
 #define DEF_MIN_IPU_UTIL	70
@@ -623,6 +624,7 @@ enum {
 	F2FS_IPU_SSR_UTIL,
 	F2FS_IPU_FSYNC,
 	F2FS_IPU_ASYNC,
+	F2FS_IPU_NOCACHE,
 };
 
 static inline unsigned int curseg_segno(struct f2fs_sb_info *sbi,

commit 4c8ff7095bef64fc47e996a938f7d57f9e077da3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Nov 1 18:07:14 2019 +0800

    f2fs: support data compression
    
    This patch tries to support compression in f2fs.
    
    - New term named cluster is defined as basic unit of compression, file can
    be divided into multiple clusters logically. One cluster includes 4 << n
    (n >= 0) logical pages, compression size is also cluster size, each of
    cluster can be compressed or not.
    
    - In cluster metadata layout, one special flag is used to indicate cluster
    is compressed one or normal one, for compressed cluster, following metadata
    maps cluster to [1, 4 << n - 1] physical blocks, in where f2fs stores
    data including compress header and compressed data.
    
    - In order to eliminate write amplification during overwrite, F2FS only
    support compression on write-once file, data can be compressed only when
    all logical blocks in file are valid and cluster compress ratio is lower
    than specified threshold.
    
    - To enable compression on regular inode, there are three ways:
    * chattr +c file
    * chattr +c dir; touch dir/file
    * mount w/ -o compress_extension=ext; touch file.ext
    
    Compress metadata layout:
                                 [Dnode Structure]
                 +-----------------------------------------------+
                 | cluster 1 | cluster 2 | ......... | cluster N |
                 +-----------------------------------------------+
                 .           .                       .           .
           .                       .                .                      .
      .         Compressed Cluster       .        .        Normal Cluster            .
    +----------+---------+---------+---------+  +---------+---------+---------+---------+
    |compr flag| block 1 | block 2 | block 3 |  | block 1 | block 2 | block 3 | block 4 |
    +----------+---------+---------+---------+  +---------+---------+---------+---------+
               .                             .
             .                                           .
           .                                                           .
          +-------------+-------------+----------+----------------------------+
          | data length | data chksum | reserved |      compressed data       |
          +-------------+-------------+----------+----------------------------+
    
    Changelog:
    
    20190326:
    - fix error handling of read_end_io().
    - remove unneeded comments in f2fs_encrypt_one_page().
    
    20190327:
    - fix wrong use of f2fs_cluster_is_full() in f2fs_mpage_readpages().
    - don't jump into loop directly to avoid uninitialized variables.
    - add TODO tag in error path of f2fs_write_cache_pages().
    
    20190328:
    - fix wrong merge condition in f2fs_read_multi_pages().
    - check compressed file in f2fs_post_read_required().
    
    20190401
    - allow overwrite on non-compressed cluster.
    - check cluster meta before writing compressed data.
    
    20190402
    - don't preallocate blocks for compressed file.
    
    - add lz4 compress algorithm
    - process multiple post read works in one workqueue
      Now f2fs supports processing post read work in multiple workqueue,
      it shows low performance due to schedule overhead of multiple
      workqueue executing orderly.
    
    20190921
    - compress: support buffered overwrite
    C: compress cluster flag
    V: valid block address
    N: NEW_ADDR
    
    One cluster contain 4 blocks
    
     before overwrite   after overwrite
    
    - VVVV          ->      CVNN
    - CVNN          ->      VVVV
    
    - CVNN          ->      CVNN
    - CVNN          ->      CVVV
    
    - CVVV          ->      CVNN
    - CVVV          ->      CVVV
    
    20191029
    - add kconfig F2FS_FS_COMPRESSION to isolate compression related
    codes, add kconfig F2FS_FS_{LZO,LZ4} to cover backend algorithm.
    note that: will remove lzo backend if Jaegeuk agreed that too.
    - update codes according to Eric's comments.
    
    20191101
    - apply fixes from Jaegeuk
    
    20191113
    - apply fixes from Jaegeuk
    - split workqueue for fsverity
    
    20191216
    - apply fixes from Jaegeuk
    
    20200117
    - fix to avoid NULL pointer dereference
    
    [Jaegeuk Kim]
    - add tracepoint for f2fs_{,de}compress_pages()
    - fix many bugs and add some compression stats
    - fix overwrite/mmap bugs
    - address 32bit build error, reported by Geert.
    - bug fixes when handling errors and i_compressed_blocks
    
    Reported-by: <noreply@ellerman.id.au>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index a95467b202ea..a1b3951367cd 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -200,18 +200,6 @@ struct segment_allocation {
 	void (*allocate_segment)(struct f2fs_sb_info *, int, bool);
 };
 
-/*
- * this value is set in page as a private data which indicate that
- * the page is atomically written, and it is in inmem_pages list.
- */
-#define ATOMIC_WRITTEN_PAGE		((unsigned long)-1)
-#define DUMMY_WRITTEN_PAGE		((unsigned long)-2)
-
-#define IS_ATOMIC_WRITTEN_PAGE(page)			\
-		(page_private(page) == (unsigned long)ATOMIC_WRITTEN_PAGE)
-#define IS_DUMMY_WRITTEN_PAGE(page)			\
-		(page_private(page) == (unsigned long)DUMMY_WRITTEN_PAGE)
-
 #define MAX_SKIP_GC_COUNT			16
 
 struct inmem_pages {

commit f5a53edcf01eae21dc3ef1845515229e8459e5cc
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Oct 18 10:06:40 2019 -0700

    f2fs: support aligned pinned file
    
    This patch supports 2MB-aligned pinned file, which can guarantee no GC at all
    by allocating fully valid 2MB segment.
    
    Check free segments by has_not_enough_free_secs() with large budget.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 325781a1ae4d..a95467b202ea 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -313,6 +313,8 @@ struct sit_entry_set {
  */
 static inline struct curseg_info *CURSEG_I(struct f2fs_sb_info *sbi, int type)
 {
+	if (type == CURSEG_COLD_DATA_PINNED)
+		type = CURSEG_COLD_DATA;
 	return (struct curseg_info *)(SM_I(sbi)->curseg_array + type);
 }
 

commit 00e09c0bccc71825ca9a659eb145ed7c4dc95588
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Aug 23 17:58:36 2019 +0800

    f2fs: enhance f2fs_is_checkpoint_ready()'s readability
    
    This patch changes sematics of f2fs_is_checkpoint_ready()'s return
    value as: return true when checkpoint is ready, other return false,
    it can improve readability of below conditions.
    
    f2fs_submit_page_write()
    ...
            if (is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN) ||
                                    !f2fs_is_checkpoint_ready(sbi))
                    __submit_merged_bio(io);
    
    f2fs_balance_fs()
    ...
            if (!f2fs_is_checkpoint_ready(sbi))
                    return;
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index b219009c3e20..325781a1ae4d 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -586,13 +586,13 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi,
 		reserved_sections(sbi) + needed);
 }
 
-static inline int f2fs_is_checkpoint_ready(struct f2fs_sb_info *sbi)
+static inline bool f2fs_is_checkpoint_ready(struct f2fs_sb_info *sbi)
 {
 	if (likely(!is_sbi_flag_set(sbi, SBI_CP_DISABLED)))
-		return 0;
+		return true;
 	if (likely(!has_not_enough_free_secs(sbi, 0, 0)))
-		return 0;
-	return -ENOSPC;
+		return true;
+	return false;
 }
 
 static inline bool excess_prefree_segs(struct f2fs_sb_info *sbi)

commit bbf9f7d90f21e05e31b7cdd95b32f64dd2819dfe
Author: Sahitya Tummala <stummala@codeaurora.org>
Date:   Wed Aug 7 19:10:32 2019 +0530

    f2fs: Fix indefinite loop in f2fs_gc()
    
    Policy - Foreground GC, LFS and greedy GC mode.
    
    Under this policy, f2fs_gc() loops forever to GC as it doesn't have
    enough free segements to proceed and thus it keeps calling gc_more
    for the same victim segment.  This can happen if the selected victim
    segment could not be GC'd due to failed blkaddr validity check i.e.
    is_alive() returns false for the blocks set in current validity map.
    
    Fix this by keeping track of such invalid segments and skip those
    segments for selection in get_victim_by_default() to avoid endless
    GC loop under such error scenarios. Currently, add this logic under
    CONFIG_F2FS_CHECK_FS to be able to root cause the issue in debug
    version.
    
    Signed-off-by: Sahitya Tummala <stummala@codeaurora.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    [Jaegeuk Kim: fix wrong bitmap size]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index ec4d568fd58c..b219009c3e20 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -230,6 +230,9 @@ struct sit_info {
 	char *sit_bitmap;		/* SIT bitmap pointer */
 #ifdef CONFIG_F2FS_CHECK_FS
 	char *sit_bitmap_mir;		/* SIT bitmap mirror */
+
+	/* bitmap of segments to be ignored by GC in case of errors */
+	unsigned long *invalid_segmap;
 #endif
 	unsigned int bitmap_size;	/* SIT bitmap size */
 

commit 2fde3dd14efe8ceb21a92c2749394a5df61cb7df
Author: Chao Yu <yuchao0@huawei.com>
Date:   Fri Jul 26 15:41:20 2019 +0800

    f2fs: allocate memory in batch in build_sit_info()
    
    build_sit_info() allocate all bitmaps for each segment one by one,
    it's quite low efficiency, this pach changes to allocate large
    continuous memory at a time, and divide it and assign for each bitmaps
    of segment. For large size image, it can expect improving its mount
    speed.
    
    Signed-off-by: Chen Gong <gongchen4@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index b74602813a05..ec4d568fd58c 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -226,6 +226,7 @@ struct sit_info {
 	block_t sit_base_addr;		/* start block address of SIT area */
 	block_t sit_blocks;		/* # of blocks used by SIT area */
 	block_t written_valid_blocks;	/* # of valid blocks in main area */
+	char *bitmap;			/* all bitmaps pointer */
 	char *sit_bitmap;		/* SIT bitmap pointer */
 #ifdef CONFIG_F2FS_CHECK_FS
 	char *sit_bitmap_mir;		/* SIT bitmap mirror */

commit 10f966bbf521bb9b2e497bbca496a5141f4071d0
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Jun 20 11:36:14 2019 +0800

    f2fs: use generic EFSBADCRC/EFSCORRUPTED
    
    f2fs uses EFAULT as error number to indicate filesystem is corrupted
    all the time, but generic filesystems use EUCLEAN for such condition,
    we need to change to follow others.
    
    This patch adds two new macros as below to wrap more generic error
    code macros, and spread them in code.
    
    EFSBADCRC       EBADMSG         /* Bad CRC detected */
    EFSCORRUPTED    EUCLEAN         /* Filesystem is corrupted */
    
    Reported-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 2ae6df03b998..b74602813a05 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -696,7 +696,7 @@ static inline int check_block_count(struct f2fs_sb_info *sbi,
 		f2fs_err(sbi, "Mismatch valid blocks %d vs. %d",
 			 GET_SIT_VBLOCKS(raw_sit), valid_blocks);
 		set_sbi_flag(sbi, SBI_NEED_FSCK);
-		return -EINVAL;
+		return -EFSCORRUPTED;
 	}
 
 	/* check segment usage, and check boundary of a given segment number */
@@ -705,7 +705,7 @@ static inline int check_block_count(struct f2fs_sb_info *sbi,
 		f2fs_err(sbi, "Wrong valid blocks %d or segno %u",
 			 GET_SIT_VBLOCKS(raw_sit), segno);
 		set_sbi_flag(sbi, SBI_NEED_FSCK);
-		return -EINVAL;
+		return -EFSCORRUPTED;
 	}
 	return 0;
 }

commit f91108b801ad7d7668c95841f4cb76494b2a9c8a
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Thu Jun 20 16:42:08 2019 +0200

    f2fs: Use DIV_ROUND_UP() instead of open-coding
    
    Replace the open-coded divisions with round-up by calls to the
    DIV_ROUND_UP() helper macro.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 166ac0f07a4e..2ae6df03b998 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -109,7 +109,7 @@
 #define	START_SEGNO(segno)		\
 	(SIT_BLOCK_OFFSET(segno) * SIT_ENTRY_PER_BLOCK)
 #define SIT_BLK_CNT(sbi)			\
-	((MAIN_SEGS(sbi) + SIT_ENTRY_PER_BLOCK - 1) / SIT_ENTRY_PER_BLOCK)
+	DIV_ROUND_UP(MAIN_SEGS(sbi), SIT_ENTRY_PER_BLOCK)
 #define f2fs_bitmap_size(nr)			\
 	(BITS_TO_LONGS(nr) * sizeof(unsigned long))
 

commit dcbb4c10e6d9693cc9d6fa493b4d130b66a60c7d
Author: Joe Perches <joe@perches.com>
Date:   Tue Jun 18 17:48:42 2019 +0800

    f2fs: introduce f2fs_<level> macros to wrap f2fs_printk()
    
    - Add and use f2fs_<level> macros
    - Convert f2fs_msg to f2fs_printk
    - Remove level from f2fs_printk and embed the level in the format
    - Coalesce formats and align multi-line arguments
    - Remove unnecessary duplicate extern f2fs_msg f2fs.h
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 429007b8036e..166ac0f07a4e 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -693,9 +693,8 @@ static inline int check_block_count(struct f2fs_sb_info *sbi,
 	} while (cur_pos < sbi->blocks_per_seg);
 
 	if (unlikely(GET_SIT_VBLOCKS(raw_sit) != valid_blocks)) {
-		f2fs_msg(sbi->sb, KERN_ERR,
-				"Mismatch valid blocks %d vs. %d",
-					GET_SIT_VBLOCKS(raw_sit), valid_blocks);
+		f2fs_err(sbi, "Mismatch valid blocks %d vs. %d",
+			 GET_SIT_VBLOCKS(raw_sit), valid_blocks);
 		set_sbi_flag(sbi, SBI_NEED_FSCK);
 		return -EINVAL;
 	}
@@ -703,9 +702,8 @@ static inline int check_block_count(struct f2fs_sb_info *sbi,
 	/* check segment usage, and check boundary of a given segment number */
 	if (unlikely(GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg
 					|| segno > TOTAL_SEGS(sbi) - 1)) {
-		f2fs_msg(sbi->sb, KERN_ERR,
-				"Wrong valid blocks %d or segno %u",
-					GET_SIT_VBLOCKS(raw_sit), segno);
+		f2fs_err(sbi, "Wrong valid blocks %d or segno %u",
+			 GET_SIT_VBLOCKS(raw_sit), segno);
 		set_sbi_flag(sbi, SBI_NEED_FSCK);
 		return -EINVAL;
 	}

commit 93770ab7a6e963147a5dbca25278b69ba6c8f8c5
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Apr 15 15:26:32 2019 +0800

    f2fs: introduce DATA_GENERIC_ENHANCE
    
    Previously, f2fs_is_valid_blkaddr(, blkaddr, DATA_GENERIC) will check
    whether @blkaddr locates in main area or not.
    
    That check is weak, since the block address in range of main area can
    point to the address which is not valid in segment info table, and we
    can not detect such condition, we may suffer worse corruption as system
    continues running.
    
    So this patch introduce DATA_GENERIC_ENHANCE to enhance the sanity check
    which trigger SIT bitmap check rather than only range check.
    
    This patch did below changes as wel:
    - set SBI_NEED_FSCK in f2fs_is_valid_blkaddr().
    - get rid of is_valid_data_blkaddr() to avoid panic if blkaddr is invalid.
    - introduce verify_fio_blkaddr() to wrap fio {new,old}_blkaddr validation check.
    - spread blkaddr check in:
     * f2fs_get_node_info()
     * __read_out_blkaddrs()
     * f2fs_submit_page_read()
     * ra_data_block()
     * do_recover_data()
    
    This patch can fix bug reported from bugzilla below:
    
    https://bugzilla.kernel.org/show_bug.cgi?id=203215
    https://bugzilla.kernel.org/show_bug.cgi?id=203223
    https://bugzilla.kernel.org/show_bug.cgi?id=203231
    https://bugzilla.kernel.org/show_bug.cgi?id=203235
    https://bugzilla.kernel.org/show_bug.cgi?id=203241
    
    = Update by Jaegeuk Kim =
    
    DATA_GENERIC_ENHANCE enhanced to validate block addresses on read/write paths.
    But, xfstest/generic/446 compalins some generated kernel messages saying invalid
    bitmap was detected when reading a block. The reaons is, when we get the
    block addresses from extent_cache, there is no lock to synchronize it from
    truncating the blocks in parallel.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 6f48e0763279..429007b8036e 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -82,7 +82,7 @@
 	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) & ((sbi)->blocks_per_seg - 1))
 
 #define GET_SEGNO(sbi, blk_addr)					\
-	((!is_valid_data_blkaddr(sbi, blk_addr)) ?			\
+	((!__is_valid_data_blkaddr(blk_addr)) ?			\
 	NULL_SEGNO : GET_L2R_SEGNO(FREE_I(sbi),			\
 		GET_SEGNO_FROM_SEG0(sbi, blk_addr)))
 #define BLKS_PER_SEC(sbi)					\
@@ -656,14 +656,15 @@ static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
 	f2fs_bug_on(sbi, segno > TOTAL_SEGS(sbi) - 1);
 }
 
-static inline void verify_block_addr(struct f2fs_io_info *fio, block_t blk_addr)
+static inline void verify_fio_blkaddr(struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
 
-	if (__is_meta_io(fio))
-		verify_blkaddr(sbi, blk_addr, META_GENERIC);
-	else
-		verify_blkaddr(sbi, blk_addr, DATA_GENERIC);
+	if (__is_valid_data_blkaddr(fio->old_blkaddr))
+		verify_blkaddr(sbi, fio->old_blkaddr, __is_meta_io(fio) ?
+					META_GENERIC : DATA_GENERIC);
+	verify_blkaddr(sbi, fio->new_blkaddr, __is_meta_io(fio) ?
+					META_GENERIC : DATA_GENERIC_ENHANCE);
 }
 
 /*

commit e95bcdb2fefa129f37bd9035af1d234ca92ee4ef
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Apr 15 15:30:51 2019 +0800

    f2fs: fix to do sanity check on valid block count of segment
    
    As Jungyeon reported in bugzilla:
    
    https://bugzilla.kernel.org/show_bug.cgi?id=203233
    
    - Overview
    When mounting the attached crafted image and running program, following errors are reported.
    Additionally, it hangs on sync after running program.
    
    The image is intentionally fuzzed from a normal f2fs image for testing.
    Compile options for F2FS are as follows.
    CONFIG_F2FS_FS=y
    CONFIG_F2FS_STAT_FS=y
    CONFIG_F2FS_FS_XATTR=y
    CONFIG_F2FS_FS_POSIX_ACL=y
    CONFIG_F2FS_CHECK_FS=y
    
    - Reproduces
    cc poc_13.c
    mkdir test
    mount -t f2fs tmp.img test
    cp a.out test
    cd test
    sudo ./a.out
    sync
    
    - Kernel messages
     F2FS-fs (sdb): Bitmap was wrongly set, blk:4608
     kernel BUG at fs/f2fs/segment.c:2102!
     RIP: 0010:update_sit_entry+0x394/0x410
     Call Trace:
      f2fs_allocate_data_block+0x16f/0x660
      do_write_page+0x62/0x170
      f2fs_do_write_node_page+0x33/0xa0
      __write_node_page+0x270/0x4e0
      f2fs_sync_node_pages+0x5df/0x670
      f2fs_write_checkpoint+0x372/0x1400
      f2fs_sync_fs+0xa3/0x130
      f2fs_do_sync_file+0x1a6/0x810
      do_fsync+0x33/0x60
      __x64_sys_fsync+0xb/0x10
      do_syscall_64+0x43/0xf0
      entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    sit.vblocks and sum valid block count in sit.valid_map may be
    inconsistent, segment w/ zero vblocks will be treated as free
    segment, while allocating in free segment, we may allocate a
    free block, if its bitmap is valid previously, it can cause
    kernel crash due to bitmap verification failure.
    
    Anyway, to avoid further serious metadata inconsistence and
    corruption, it is necessary and worth to detect SIT
    inconsistence. So let's enable check_block_count() to verify
    vblocks and valid_map all the time rather than do it only
    CONFIG_F2FS_CHECK_FS is enabled.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 5c7ed0442d6e..6f48e0763279 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -672,7 +672,6 @@ static inline void verify_block_addr(struct f2fs_io_info *fio, block_t blk_addr)
 static inline int check_block_count(struct f2fs_sb_info *sbi,
 		int segno, struct f2fs_sit_entry *raw_sit)
 {
-#ifdef CONFIG_F2FS_CHECK_FS
 	bool is_valid  = test_bit_le(0, raw_sit->valid_map) ? true : false;
 	int valid_blocks = 0;
 	int cur_pos = 0, next_pos;
@@ -699,7 +698,7 @@ static inline int check_block_count(struct f2fs_sb_info *sbi,
 		set_sbi_flag(sbi, SBI_NEED_FSCK);
 		return -EINVAL;
 	}
-#endif
+
 	/* check segment usage, and check boundary of a given segment number */
 	if (unlikely(GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg
 					|| segno > TOTAL_SEGS(sbi) - 1)) {

commit b460866d27089ffaf915a5a3340b4d8c9f079d33
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jan 25 10:26:39 2019 -0800

    f2fs: don't wake up too frequently, if there is lots of IOs
    
    Otherwise, it wakes up discard thread which will sleep again by busy IOs
    in a loop.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index a77f76f528b6..5c7ed0442d6e 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -865,7 +865,7 @@ static inline void wake_up_discard_thread(struct f2fs_sb_info *sbi, bool force)
 		}
 	}
 	mutex_unlock(&dcc->cmd_lock);
-	if (!wakeup)
+	if (!wakeup || !is_idle(sbi, DISCARD_TIME))
 		return;
 wake_up:
 	dcc->discard_wake = 1;

commit 2c70c5e3874e8cf2f39f4ce4e2b832f4380a0c1b
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Oct 24 18:37:26 2018 +0800

    f2fs: introduce __is_large_section() for cleanup
    
    Introduce a wrapper __is_large_section() to clean up codes.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index ab3465faddf1..a77f76f528b6 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -333,7 +333,7 @@ static inline unsigned int get_valid_blocks(struct f2fs_sb_info *sbi,
 	 * In order to get # of valid blocks in a section instantly from many
 	 * segments, f2fs manages two counting structures separately.
 	 */
-	if (use_section && sbi->segs_per_sec > 1)
+	if (use_section && __is_large_section(sbi))
 		return get_sec_entry(sbi, segno)->valid_blocks;
 	else
 		return get_seg_entry(sbi, segno)->valid_blocks;

commit 4354994f097d068a894aa1a0860da54571df3582
Author: Daniel Rosenberg <drosen@google.com>
Date:   Mon Aug 20 19:21:43 2018 -0700

    f2fs: checkpoint disabling
    
    Note that, it requires "f2fs: return correct errno in f2fs_gc".
    
    This adds a lightweight non-persistent snapshotting scheme to f2fs.
    
    To use, mount with the option checkpoint=disable, and to return to
    normal operation, remount with checkpoint=enable. If the filesystem
    is shut down before remounting with checkpoint=enable, it will revert
    back to its apparent state when it was first mounted with
    checkpoint=disable. This is useful for situations where you wish to be
    able to roll back the state of the disk in case of some critical
    failure.
    
    Signed-off-by: Daniel Rosenberg <drosen@google.com>
    [Jaegeuk Kim: use SB_RDONLY instead of MS_RDONLY]
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 086150028c6d..ab3465faddf1 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -339,6 +339,12 @@ static inline unsigned int get_valid_blocks(struct f2fs_sb_info *sbi,
 		return get_seg_entry(sbi, segno)->valid_blocks;
 }
 
+static inline unsigned int get_ckpt_valid_blocks(struct f2fs_sb_info *sbi,
+				unsigned int segno)
+{
+	return get_seg_entry(sbi, segno)->ckpt_valid_blocks;
+}
+
 static inline void seg_info_from_raw_sit(struct seg_entry *se,
 					struct f2fs_sit_entry *rs)
 {
@@ -576,6 +582,15 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi,
 		reserved_sections(sbi) + needed);
 }
 
+static inline int f2fs_is_checkpoint_ready(struct f2fs_sb_info *sbi)
+{
+	if (likely(!is_sbi_flag_set(sbi, SBI_CP_DISABLED)))
+		return 0;
+	if (likely(!has_not_enough_free_secs(sbi, 0, 0)))
+		return 0;
+	return -ENOSPC;
+}
+
 static inline bool excess_prefree_segs(struct f2fs_sb_info *sbi)
 {
 	return prefree_segments(sbi) > SM_I(sbi)->rec_prefree_segments;

commit 7c1a000d466235c875a989971cfda344e6bb1166
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Sep 12 09:16:07 2018 +0800

    f2fs: add SPDX license identifiers
    
    Remove the verbose license text from f2fs files and replace them with
    SPDX tags.  This does not change the license of any of the code.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index b3d9e317ff0c..086150028c6d 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -1,12 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * fs/f2fs/segment.h
  *
  * Copyright (c) 2012 Samsung Electronics Co., Ltd.
  *             http://www.samsung.com/
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #include <linux/blkdev.h>
 #include <linux/backing-dev.h>

commit 6f8d4455060dfb0e32dfb8e685b97caf4ed1be41
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jul 25 12:11:56 2018 +0900

    f2fs: avoid fi->i_gc_rwsem[WRITE] lock in f2fs_gc
    
    The f2fs_gc() called by f2fs_balance_fs() requires to be called outside of
    fi->i_gc_rwsem[WRITE], since f2fs_gc() can try to grab it in a loop.
    
    If it hits the miximum retrials in GC, let's give a chance to release
    gc_mutex for a short time in order not to go into live lock in the worst
    case.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 50495515f0a0..b3d9e317ff0c 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -215,7 +215,7 @@ struct segment_allocation {
 #define IS_DUMMY_WRITTEN_PAGE(page)			\
 		(page_private(page) == (unsigned long)DUMMY_WRITTEN_PAGE)
 
-#define MAX_SKIP_ATOMIC_COUNT			16
+#define MAX_SKIP_GC_COUNT			16
 
 struct inmem_pages {
 	struct list_head list;

commit 3611ce9911267cb93d364bd71ddea6821278d11f
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Thu Jul 12 23:09:26 2018 +0800

    f2fs: do not set free of current section
    
    For the case when sbi->segs_per_sec > 1, take section:segment = 5 for
    example, if segment 1 is just used and allocate new segment 2, and the
    blocks of segment 1 is invalidated, at this time, the previous code will
    use __set_test_and_free to free the free_secmap and free_sections++,
    this is not correct since it is still a current section, so fix it.
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index b5bd3287e104..50495515f0a0 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -448,6 +448,8 @@ static inline void __set_test_and_free(struct f2fs_sb_info *sbi,
 	if (test_and_clear_bit(segno, free_i->free_segmap)) {
 		free_i->free_segments++;
 
+		if (IS_CURSEC(sbi, secno))
+			goto skip_free;
 		next = find_next_bit(free_i->free_segmap,
 				start_segno + sbi->segs_per_sec, start_segno);
 		if (next >= start_segno + sbi->segs_per_sec) {
@@ -455,6 +457,7 @@ static inline void __set_test_and_free(struct f2fs_sb_info *sbi,
 				free_i->free_sections++;
 		}
 	}
+skip_free:
 	spin_unlock(&free_i->segmap_lock);
 }
 

commit c9b60788fc760d136211853f10ce73dc152d1f4a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Aug 1 19:13:44 2018 +0800

    f2fs: fix to do sanity check with block address in main area
    
    This patch add to do sanity check with below field:
    - cp_pack_total_block_count
    - blkaddr of data/node
    - extent info
    
    - Overview
    BUG() in verify_block_addr() when writing to a corrupted f2fs image
    
    - Reproduce (4.18 upstream kernel)
    
    - POC (poc.c)
    
    static void activity(char *mpoint) {
    
      char *foo_bar_baz;
      int err;
    
      static int buf[8192];
      memset(buf, 0, sizeof(buf));
    
      err = asprintf(&foo_bar_baz, "%s/foo/bar/baz", mpoint);
    
      int fd = open(foo_bar_baz, O_RDWR | O_TRUNC, 0777);
      if (fd >= 0) {
        write(fd, (char *)buf, sizeof(buf));
        fdatasync(fd);
        close(fd);
      }
    }
    
    int main(int argc, char *argv[]) {
      activity(argv[1]);
      return 0;
    }
    
    - Kernel message
    [  689.349473] F2FS-fs (loop0): Mounted with checkpoint version = 3
    [  699.728662] WARNING: CPU: 0 PID: 1309 at fs/f2fs/segment.c:2860 f2fs_inplace_write_data+0x232/0x240
    [  699.728670] Modules linked in: snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hwdep snd_hda_core snd_pcm snd_timer snd mac_hid i2c_piix4 soundcore ib_iser rdma_cm iw_cm ib_cm ib_core iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx raid1 raid0 multipath linear 8139too crct10dif_pclmul crc32_pclmul qxl drm_kms_helper syscopyarea aesni_intel sysfillrect sysimgblt fb_sys_fops ttm drm aes_x86_64 crypto_simd cryptd 8139cp glue_helper mii pata_acpi floppy
    [  699.729056] CPU: 0 PID: 1309 Comm: a.out Not tainted 4.18.0-rc1+ #4
    [  699.729064] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  699.729074] RIP: 0010:f2fs_inplace_write_data+0x232/0x240
    [  699.729076] Code: ff e9 cf fe ff ff 49 8d 7d 10 e8 39 45 ad ff 4d 8b 7d 10 be 04 00 00 00 49 8d 7f 48 e8 07 49 ad ff 45 8b 7f 48 e9 fb fe ff ff <0f> 0b f0 41 80 4d 48 04 e9 65 fe ff ff 90 66 66 66 66 90 55 48 8d
    [  699.729130] RSP: 0018:ffff8801f43af568 EFLAGS: 00010202
    [  699.729139] RAX: 000000000000003f RBX: ffff8801f43af7b8 RCX: ffffffffb88c9113
    [  699.729142] RDX: 0000000000000003 RSI: dffffc0000000000 RDI: ffff8802024e5540
    [  699.729144] RBP: ffff8801f43af590 R08: 0000000000000009 R09: ffffffffffffffe8
    [  699.729147] R10: 0000000000000001 R11: ffffed0039b0596a R12: ffff8802024e5540
    [  699.729149] R13: ffff8801f0335500 R14: ffff8801e3e7a700 R15: ffff8801e1ee4450
    [  699.729154] FS:  00007f9bf97f5700(0000) GS:ffff8801f6e00000(0000) knlGS:0000000000000000
    [  699.729156] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  699.729159] CR2: 00007f9bf925d170 CR3: 00000001f0c34000 CR4: 00000000000006f0
    [  699.729171] Call Trace:
    [  699.729192]  f2fs_do_write_data_page+0x2e2/0xe00
    [  699.729203]  ? f2fs_should_update_outplace+0xd0/0xd0
    [  699.729238]  ? memcg_drain_all_list_lrus+0x280/0x280
    [  699.729269]  ? __radix_tree_replace+0xa3/0x120
    [  699.729276]  __write_data_page+0x5c7/0xe30
    [  699.729291]  ? kasan_check_read+0x11/0x20
    [  699.729310]  ? page_mapped+0x8a/0x110
    [  699.729321]  ? page_mkclean+0xe9/0x160
    [  699.729327]  ? f2fs_do_write_data_page+0xe00/0xe00
    [  699.729331]  ? invalid_page_referenced_vma+0x130/0x130
    [  699.729345]  ? clear_page_dirty_for_io+0x332/0x450
    [  699.729351]  f2fs_write_cache_pages+0x4ca/0x860
    [  699.729358]  ? __write_data_page+0xe30/0xe30
    [  699.729374]  ? percpu_counter_add_batch+0x22/0xa0
    [  699.729380]  ? kasan_check_write+0x14/0x20
    [  699.729391]  ? _raw_spin_lock+0x17/0x40
    [  699.729403]  ? f2fs_mark_inode_dirty_sync.part.18+0x16/0x30
    [  699.729413]  ? iov_iter_advance+0x113/0x640
    [  699.729418]  ? f2fs_write_end+0x133/0x2e0
    [  699.729423]  ? balance_dirty_pages_ratelimited+0x239/0x640
    [  699.729428]  f2fs_write_data_pages+0x329/0x520
    [  699.729433]  ? generic_perform_write+0x250/0x320
    [  699.729438]  ? f2fs_write_cache_pages+0x860/0x860
    [  699.729454]  ? current_time+0x110/0x110
    [  699.729459]  ? f2fs_preallocate_blocks+0x1ef/0x370
    [  699.729464]  do_writepages+0x37/0xb0
    [  699.729468]  ? f2fs_write_cache_pages+0x860/0x860
    [  699.729472]  ? do_writepages+0x37/0xb0
    [  699.729478]  __filemap_fdatawrite_range+0x19a/0x1f0
    [  699.729483]  ? delete_from_page_cache_batch+0x4e0/0x4e0
    [  699.729496]  ? __vfs_write+0x2b2/0x410
    [  699.729501]  file_write_and_wait_range+0x66/0xb0
    [  699.729506]  f2fs_do_sync_file+0x1f9/0xd90
    [  699.729511]  ? truncate_partial_data_page+0x290/0x290
    [  699.729521]  ? __sb_end_write+0x30/0x50
    [  699.729526]  ? vfs_write+0x20f/0x260
    [  699.729530]  f2fs_sync_file+0x9a/0xb0
    [  699.729534]  ? f2fs_do_sync_file+0xd90/0xd90
    [  699.729548]  vfs_fsync_range+0x68/0x100
    [  699.729554]  ? __fget_light+0xc9/0xe0
    [  699.729558]  do_fsync+0x3d/0x70
    [  699.729562]  __x64_sys_fdatasync+0x24/0x30
    [  699.729585]  do_syscall_64+0x78/0x170
    [  699.729595]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [  699.729613] RIP: 0033:0x7f9bf930d800
    [  699.729615] Code: 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 83 3d 49 bf 2c 00 00 75 10 b8 4b 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 31 c3 48 83 ec 08 e8 be 78 01 00 48 89 04 24
    [  699.729668] RSP: 002b:00007ffee3606c68 EFLAGS: 00000246 ORIG_RAX: 000000000000004b
    [  699.729673] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f9bf930d800
    [  699.729675] RDX: 0000000000008000 RSI: 00000000006010a0 RDI: 0000000000000003
    [  699.729678] RBP: 00007ffee3606ca0 R08: 0000000001503010 R09: 0000000000000000
    [  699.729680] R10: 00000000000002e8 R11: 0000000000000246 R12: 0000000000400610
    [  699.729683] R13: 00007ffee3606da0 R14: 0000000000000000 R15: 0000000000000000
    [  699.729687] ---[ end trace 4ce02f25ff7d3df5 ]---
    [  699.729782] ------------[ cut here ]------------
    [  699.729785] kernel BUG at fs/f2fs/segment.h:654!
    [  699.731055] invalid opcode: 0000 [#1] SMP KASAN PTI
    [  699.732104] CPU: 0 PID: 1309 Comm: a.out Tainted: G        W         4.18.0-rc1+ #4
    [  699.733684] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  699.735611] RIP: 0010:f2fs_submit_page_bio+0x29b/0x730
    [  699.736649] Code: 54 49 8d bd 18 04 00 00 e8 b2 59 af ff 41 8b 8d 18 04 00 00 8b 45 b8 41 d3 e6 44 01 f0 4c 8d 73 14 41 39 c7 0f 82 37 fe ff ff <0f> 0b 65 8b 05 2c 04 77 47 89 c0 48 0f a3 05 52 c1 d5 01 0f 92 c0
    [  699.740524] RSP: 0018:ffff8801f43af508 EFLAGS: 00010283
    [  699.741573] RAX: 0000000000000000 RBX: ffff8801f43af7b8 RCX: ffffffffb88a7cef
    [  699.743006] RDX: 0000000000000007 RSI: dffffc0000000000 RDI: ffff8801e3e7a64c
    [  699.744426] RBP: ffff8801f43af558 R08: ffffed003e066b55 R09: ffffed003e066b55
    [  699.745833] R10: 0000000000000001 R11: ffffed003e066b54 R12: ffffea0007876940
    [  699.747256] R13: ffff8801f0335500 R14: ffff8801e3e7a600 R15: 0000000000000001
    [  699.748683] FS:  00007f9bf97f5700(0000) GS:ffff8801f6e00000(0000) knlGS:0000000000000000
    [  699.750293] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  699.751462] CR2: 00007f9bf925d170 CR3: 00000001f0c34000 CR4: 00000000000006f0
    [  699.752874] Call Trace:
    [  699.753386]  ? f2fs_inplace_write_data+0x93/0x240
    [  699.754341]  f2fs_inplace_write_data+0xd2/0x240
    [  699.755271]  f2fs_do_write_data_page+0x2e2/0xe00
    [  699.756214]  ? f2fs_should_update_outplace+0xd0/0xd0
    [  699.757215]  ? memcg_drain_all_list_lrus+0x280/0x280
    [  699.758209]  ? __radix_tree_replace+0xa3/0x120
    [  699.759164]  __write_data_page+0x5c7/0xe30
    [  699.760002]  ? kasan_check_read+0x11/0x20
    [  699.760823]  ? page_mapped+0x8a/0x110
    [  699.761573]  ? page_mkclean+0xe9/0x160
    [  699.762345]  ? f2fs_do_write_data_page+0xe00/0xe00
    [  699.763332]  ? invalid_page_referenced_vma+0x130/0x130
    [  699.764374]  ? clear_page_dirty_for_io+0x332/0x450
    [  699.765347]  f2fs_write_cache_pages+0x4ca/0x860
    [  699.766276]  ? __write_data_page+0xe30/0xe30
    [  699.767161]  ? percpu_counter_add_batch+0x22/0xa0
    [  699.768112]  ? kasan_check_write+0x14/0x20
    [  699.768951]  ? _raw_spin_lock+0x17/0x40
    [  699.769739]  ? f2fs_mark_inode_dirty_sync.part.18+0x16/0x30
    [  699.770885]  ? iov_iter_advance+0x113/0x640
    [  699.771743]  ? f2fs_write_end+0x133/0x2e0
    [  699.772569]  ? balance_dirty_pages_ratelimited+0x239/0x640
    [  699.773680]  f2fs_write_data_pages+0x329/0x520
    [  699.774603]  ? generic_perform_write+0x250/0x320
    [  699.775544]  ? f2fs_write_cache_pages+0x860/0x860
    [  699.776510]  ? current_time+0x110/0x110
    [  699.777299]  ? f2fs_preallocate_blocks+0x1ef/0x370
    [  699.778279]  do_writepages+0x37/0xb0
    [  699.779026]  ? f2fs_write_cache_pages+0x860/0x860
    [  699.779978]  ? do_writepages+0x37/0xb0
    [  699.780755]  __filemap_fdatawrite_range+0x19a/0x1f0
    [  699.781746]  ? delete_from_page_cache_batch+0x4e0/0x4e0
    [  699.782820]  ? __vfs_write+0x2b2/0x410
    [  699.783597]  file_write_and_wait_range+0x66/0xb0
    [  699.784540]  f2fs_do_sync_file+0x1f9/0xd90
    [  699.785381]  ? truncate_partial_data_page+0x290/0x290
    [  699.786415]  ? __sb_end_write+0x30/0x50
    [  699.787204]  ? vfs_write+0x20f/0x260
    [  699.787941]  f2fs_sync_file+0x9a/0xb0
    [  699.788694]  ? f2fs_do_sync_file+0xd90/0xd90
    [  699.789572]  vfs_fsync_range+0x68/0x100
    [  699.790360]  ? __fget_light+0xc9/0xe0
    [  699.791128]  do_fsync+0x3d/0x70
    [  699.791779]  __x64_sys_fdatasync+0x24/0x30
    [  699.792614]  do_syscall_64+0x78/0x170
    [  699.793371]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
    [  699.794406] RIP: 0033:0x7f9bf930d800
    [  699.795134] Code: 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 83 3d 49 bf 2c 00 00 75 10 b8 4b 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 31 c3 48 83 ec 08 e8 be 78 01 00 48 89 04 24
    [  699.798960] RSP: 002b:00007ffee3606c68 EFLAGS: 00000246 ORIG_RAX: 000000000000004b
    [  699.800483] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f9bf930d800
    [  699.801923] RDX: 0000000000008000 RSI: 00000000006010a0 RDI: 0000000000000003
    [  699.803373] RBP: 00007ffee3606ca0 R08: 0000000001503010 R09: 0000000000000000
    [  699.804798] R10: 00000000000002e8 R11: 0000000000000246 R12: 0000000000400610
    [  699.806233] R13: 00007ffee3606da0 R14: 0000000000000000 R15: 0000000000000000
    [  699.807667] Modules linked in: snd_hda_codec_generic snd_hda_intel snd_hda_codec snd_hwdep snd_hda_core snd_pcm snd_timer snd mac_hid i2c_piix4 soundcore ib_iser rdma_cm iw_cm ib_cm ib_core iscsi_tcp libiscsi_tcp libiscsi scsi_transport_iscsi raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx raid1 raid0 multipath linear 8139too crct10dif_pclmul crc32_pclmul qxl drm_kms_helper syscopyarea aesni_intel sysfillrect sysimgblt fb_sys_fops ttm drm aes_x86_64 crypto_simd cryptd 8139cp glue_helper mii pata_acpi floppy
    [  699.817079] ---[ end trace 4ce02f25ff7d3df6 ]---
    [  699.818068] RIP: 0010:f2fs_submit_page_bio+0x29b/0x730
    [  699.819114] Code: 54 49 8d bd 18 04 00 00 e8 b2 59 af ff 41 8b 8d 18 04 00 00 8b 45 b8 41 d3 e6 44 01 f0 4c 8d 73 14 41 39 c7 0f 82 37 fe ff ff <0f> 0b 65 8b 05 2c 04 77 47 89 c0 48 0f a3 05 52 c1 d5 01 0f 92 c0
    [  699.822919] RSP: 0018:ffff8801f43af508 EFLAGS: 00010283
    [  699.823977] RAX: 0000000000000000 RBX: ffff8801f43af7b8 RCX: ffffffffb88a7cef
    [  699.825436] RDX: 0000000000000007 RSI: dffffc0000000000 RDI: ffff8801e3e7a64c
    [  699.826881] RBP: ffff8801f43af558 R08: ffffed003e066b55 R09: ffffed003e066b55
    [  699.828292] R10: 0000000000000001 R11: ffffed003e066b54 R12: ffffea0007876940
    [  699.829750] R13: ffff8801f0335500 R14: ffff8801e3e7a600 R15: 0000000000000001
    [  699.831192] FS:  00007f9bf97f5700(0000) GS:ffff8801f6e00000(0000) knlGS:0000000000000000
    [  699.832793] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  699.833981] CR2: 00007f9bf925d170 CR3: 00000001f0c34000 CR4: 00000000000006f0
    [  699.835556] ==================================================================
    [  699.837029] BUG: KASAN: stack-out-of-bounds in update_stack_state+0x38c/0x3e0
    [  699.838462] Read of size 8 at addr ffff8801f43af970 by task a.out/1309
    
    [  699.840086] CPU: 0 PID: 1309 Comm: a.out Tainted: G      D W         4.18.0-rc1+ #4
    [  699.841603] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
    [  699.843475] Call Trace:
    [  699.843982]  dump_stack+0x7b/0xb5
    [  699.844661]  print_address_description+0x70/0x290
    [  699.845607]  kasan_report+0x291/0x390
    [  699.846351]  ? update_stack_state+0x38c/0x3e0
    [  699.853831]  __asan_load8+0x54/0x90
    [  699.854569]  update_stack_state+0x38c/0x3e0
    [  699.855428]  ? __read_once_size_nocheck.constprop.7+0x20/0x20
    [  699.856601]  ? __save_stack_trace+0x5e/0x100
    [  699.857476]  unwind_next_frame.part.5+0x18e/0x490
    [  699.858448]  ? unwind_dump+0x290/0x290
    [  699.859217]  ? clear_page_dirty_for_io+0x332/0x450
    [  699.860185]  __unwind_start+0x106/0x190
    [  699.860974]  __save_stack_trace+0x5e/0x100
    [  699.861808]  ? __save_stack_trace+0x5e/0x100
    [  699.862691]  ? unlink_anon_vmas+0xba/0x2c0
    [  699.863525]  save_stack_trace+0x1f/0x30
    [  699.864312]  save_stack+0x46/0xd0
    [  699.864993]  ? __alloc_pages_slowpath+0x1420/0x1420
    [  699.865990]  ? flush_tlb_mm_range+0x15e/0x220
    [  699.866889]  ? kasan_check_write+0x14/0x20
    [  699.867724]  ? __dec_node_state+0x92/0xb0
    [  699.868543]  ? lock_page_memcg+0x85/0xf0
    [  699.869350]  ? unlock_page_memcg+0x16/0x80
    [  699.870185]  ? page_remove_rmap+0x198/0x520
    [  699.871048]  ? mark_page_accessed+0x133/0x200
    [  699.871930]  ? _cond_resched+0x1a/0x50
    [  699.872700]  ? unmap_page_range+0xcd4/0xe50
    [  699.873551]  ? rb_next+0x58/0x80
    [  699.874217]  ? rb_next+0x58/0x80
    [  699.874895]  __kasan_slab_free+0x13c/0x1a0
    [  699.875734]  ? unlink_anon_vmas+0xba/0x2c0
    [  699.876563]  kasan_slab_free+0xe/0x10
    [  699.877315]  kmem_cache_free+0x89/0x1e0
    [  699.878095]  unlink_anon_vmas+0xba/0x2c0
    [  699.878913]  free_pgtables+0x101/0x1b0
    [  699.879677]  exit_mmap+0x146/0x2a0
    [  699.880378]  ? __ia32_sys_munmap+0x50/0x50
    [  699.881214]  ? kasan_check_read+0x11/0x20
    [  699.882052]  ? mm_update_next_owner+0x322/0x380
    [  699.882985]  mmput+0x8b/0x1d0
    [  699.883602]  do_exit+0x43a/0x1390
    [  699.884288]  ? mm_update_next_owner+0x380/0x380
    [  699.885212]  ? f2fs_sync_file+0x9a/0xb0
    [  699.885995]  ? f2fs_do_sync_file+0xd90/0xd90
    [  699.886877]  ? vfs_fsync_range+0x68/0x100
    [  699.887694]  ? __fget_light+0xc9/0xe0
    [  699.888442]  ? do_fsync+0x3d/0x70
    [  699.889118]  ? __x64_sys_fdatasync+0x24/0x30
    [  699.889996]  rewind_stack_do_exit+0x17/0x20
    [  699.890860] RIP: 0033:0x7f9bf930d800
    [  699.891585] Code: Bad RIP value.
    [  699.892268] RSP: 002b:00007ffee3606c68 EFLAGS: 00000246 ORIG_RAX: 000000000000004b
    [  699.893781] RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f9bf930d800
    [  699.895220] RDX: 0000000000008000 RSI: 00000000006010a0 RDI: 0000000000000003
    [  699.896643] RBP: 00007ffee3606ca0 R08: 0000000001503010 R09: 0000000000000000
    [  699.898069] R10: 00000000000002e8 R11: 0000000000000246 R12: 0000000000400610
    [  699.899505] R13: 00007ffee3606da0 R14: 0000000000000000 R15: 0000000000000000
    
    [  699.901241] The buggy address belongs to the page:
    [  699.902215] page:ffffea0007d0ebc0 count:0 mapcount:0 mapping:0000000000000000 index:0x0
    [  699.903811] flags: 0x2ffff0000000000()
    [  699.904585] raw: 02ffff0000000000 0000000000000000 ffffffff07d00101 0000000000000000
    [  699.906125] raw: 0000000000000000 0000000000240000 00000000ffffffff 0000000000000000
    [  699.907673] page dumped because: kasan: bad access detected
    
    [  699.909108] Memory state around the buggy address:
    [  699.910077]  ffff8801f43af800: 00 f1 f1 f1 f1 00 f4 f4 f4 f3 f3 f3 f3 00 00 00
    [  699.911528]  ffff8801f43af880: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    [  699.912953] >ffff8801f43af900: 00 00 00 00 00 00 00 00 f1 01 f4 f4 f4 f2 f2 f2
    [  699.914392]                                                              ^
    [  699.915758]  ffff8801f43af980: f2 00 f4 f4 00 00 00 00 f2 00 00 00 00 00 00 00
    [  699.917193]  ffff8801f43afa00: 00 00 00 00 00 00 00 00 00 f3 f3 f3 00 00 00 00
    [  699.918634] ==================================================================
    
    - Location
    https://elixir.bootlin.com/linux/v4.18-rc1/source/fs/f2fs/segment.h#L644
    
    Reported-by Wen Xu <wen.xu@gatech.edu>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index a7460da9af43..b5bd3287e104 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -645,8 +645,7 @@ static inline void verify_block_addr(struct f2fs_io_info *fio, block_t blk_addr)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
 
-	if (PAGE_TYPE_OF_BIO(fio->type) == META &&
-				(!is_read_io(fio->op) || fio->is_meta))
+	if (__is_meta_io(fio))
 		verify_blkaddr(sbi, blk_addr, META_GENERIC);
 	else
 		verify_blkaddr(sbi, blk_addr, DATA_GENERIC);

commit e1da7872f6eda977bd812346bf588c35e4495a1e
Author: Chao Yu <yuchao0@huawei.com>
Date:   Tue Jun 5 17:44:11 2018 +0800

    f2fs: introduce and spread verify_blkaddr
    
    This patch introduces verify_blkaddr to check meta/data block address
    with valid range to detect bug earlier.
    
    In addition, once we encounter an invalid blkaddr, notice user to run
    fsck to fix, and let the kernel panic.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index f18fc82fbe99..a7460da9af43 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -85,7 +85,7 @@
 	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) & ((sbi)->blocks_per_seg - 1))
 
 #define GET_SEGNO(sbi, blk_addr)					\
-	((!is_valid_blkaddr(blk_addr)) ?			\
+	((!is_valid_data_blkaddr(sbi, blk_addr)) ?			\
 	NULL_SEGNO : GET_L2R_SEGNO(FREE_I(sbi),			\
 		GET_SEGNO_FROM_SEG0(sbi, blk_addr)))
 #define BLKS_PER_SEC(sbi)					\
@@ -647,11 +647,9 @@ static inline void verify_block_addr(struct f2fs_io_info *fio, block_t blk_addr)
 
 	if (PAGE_TYPE_OF_BIO(fio->type) == META &&
 				(!is_read_io(fio->op) || fio->is_meta))
-		BUG_ON(blk_addr < SEG0_BLKADDR(sbi) ||
-				blk_addr >= MAIN_BLKADDR(sbi));
+		verify_blkaddr(sbi, blk_addr, META_GENERIC);
 	else
-		BUG_ON(blk_addr < MAIN_BLKADDR(sbi) ||
-				blk_addr >= MAX_BLKADDR(sbi));
+		verify_blkaddr(sbi, blk_addr, DATA_GENERIC);
 }
 
 /*

commit a1f72ac2c0a8d8a3488b31867c9255264331a4db
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Jun 4 23:20:17 2018 +0800

    f2fs: fix to update mtime correctly
    
    If we change system time to the past, get_mtime() will return a
    overflowed time, and SIT_I(sbi)->max_mtime will be udpated
    incorrectly, this patch fixes the two issues.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index c574131ac7e1..f18fc82fbe99 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -745,12 +745,23 @@ static inline void set_to_next_sit(struct sit_info *sit_i, unsigned int start)
 #endif
 }
 
-static inline unsigned long long get_mtime(struct f2fs_sb_info *sbi)
+static inline unsigned long long get_mtime(struct f2fs_sb_info *sbi,
+						bool base_time)
 {
 	struct sit_info *sit_i = SIT_I(sbi);
-	time64_t now = ktime_get_real_seconds();
+	time64_t diff, now = ktime_get_real_seconds();
 
-	return sit_i->elapsed_time + now - sit_i->mounted_time;
+	if (now >= sit_i->mounted_time)
+		return sit_i->elapsed_time + now - sit_i->mounted_time;
+
+	/* system time is set to the past */
+	if (!base_time) {
+		diff = sit_i->mounted_time - now;
+		if (sit_i->elapsed_time >= diff)
+			return sit_i->elapsed_time - diff;
+		return 0;
+	}
+	return sit_i->elapsed_time;
 }
 
 static inline void set_summary(struct f2fs_summary *sum, nid_t nid,

commit 2ef79ecb5e906d87475d3e0c49b22425499a89f3
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon May 7 20:28:54 2018 +0800

    f2fs: avoid stucking GC due to atomic write
    
    f2fs doesn't allow abuse on atomic write class interface, so except
    limiting in-mem pages' total memory usage capacity, we need to limit
    atomic-write usage as well when filesystem is seriously fragmented,
    otherwise we may run into infinite loop during foreground GC because
    target blocks in victim segment are belong to atomic opened file for
    long time.
    
    Now, we will detect failure due to atomic write in foreground GC, if
    the count exceeds threshold, we will drop all atomic written data in
    cache, by this, I expect it can keep our system running safely to
    prevent Dos attack.
    
    In addition, his patch adds to show GC skip information in debugfs,
    now it just shows count of skipped caused by atomic write.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 86bbc189f1e0..c574131ac7e1 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -215,6 +215,8 @@ struct segment_allocation {
 #define IS_DUMMY_WRITTEN_PAGE(page)			\
 		(page_private(page) == (unsigned long)DUMMY_WRITTEN_PAGE)
 
+#define MAX_SKIP_ATOMIC_COUNT			16
+
 struct inmem_pages {
 	struct list_head list;
 	struct page *page;

commit 7b525dd01365c6764018e374d391c92466be1b7a
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed May 23 22:25:08 2018 +0800

    f2fs: clean up with is_valid_blkaddr()
    
    - rename is_valid_blkaddr() to is_valid_meta_blkaddr() for readability.
    - introduce is_valid_blkaddr() for cleanup.
    
    No logic change in this patch.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 5b7c35fa4bd2..86bbc189f1e0 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -85,7 +85,7 @@
 	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) & ((sbi)->blocks_per_seg - 1))
 
 #define GET_SEGNO(sbi, blk_addr)					\
-	((((blk_addr) == NULL_ADDR) || ((blk_addr) == NEW_ADDR)) ?	\
+	((!is_valid_blkaddr(blk_addr)) ?			\
 	NULL_SEGNO : GET_L2R_SEGNO(FREE_I(sbi),			\
 		GET_SEGNO_FROM_SEG0(sbi, blk_addr)))
 #define BLKS_PER_SEC(sbi)					\

commit 299254d85dec4ac0515a14e6d96ff70841302719
Author: Chao Yu <yuchao0@huawei.com>
Date:   Thu Apr 26 17:05:51 2018 +0800

    Revert "f2fs: add ovp valid_blocks check for bg gc victim to fg_gc"
    
    For extreme case:
    10 section, op = 10%, no_fggc_threshold = 90%
    All section usage: 85% 85% 85% 85% 90% 90% 95% 95% 95% 95%
    
    During foreground GC, if we skip select dirty section whose usage
    is larger than no_fggc_threshold, we can only recycle 80% invalid
    space from four 85% usage sections and two 90% usage sections,
    result in encountering out-of-space issue.
    
    This reverts commit e93b9865251a0503d83fd570e7d5a7c8bc351715 to
    fix this issue, besides, we keep the logic that we scan all dirty
    section when searching a victim, so that GC can select victim with
    least valid blocks.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 492ad0c86fa9..5b7c35fa4bd2 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -772,15 +772,6 @@ static inline block_t sum_blk_addr(struct f2fs_sb_info *sbi, int base, int type)
 				- (base + 1) + type;
 }
 
-static inline bool no_fggc_candidate(struct f2fs_sb_info *sbi,
-						unsigned int secno)
-{
-	if (get_valid_blocks(sbi, GET_SEG_FROM_SEC(sbi, secno), true) >
-						sbi->fggc_threshold)
-		return true;
-	return false;
-}
-
 static inline bool sec_usage_check(struct f2fs_sb_info *sbi, unsigned int secno)
 {
 	if (IS_CURSEC(sbi, secno) || (sbi->cur_victim_sec == secno))

commit 81114baa835b59ed02d14aa1d67f91ea874077cd
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Apr 9 20:25:06 2018 +0800

    f2fs: don't use GFP_ZERO for page caches
    
    Related to https://lkml.org/lkml/2018/4/8/661
    
    Sometimes, we need to write meta data to new allocated block address,
    then we will allocate a zeroed page in inner inode's address space, and
    fill partial data in it, and leave other place with zero value which means
    some fields are initial status.
    
    There are two inner inodes (meta inode and node inode) setting __GFP_ZERO,
    I have just checked them, for both of them, we can avoid using __GFP_ZERO,
    and do initialization by ourselves to avoid unneeded/redundant zeroing
    from mm.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 3325d0769723..492ad0c86fa9 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -375,6 +375,7 @@ static inline void seg_info_to_sit_page(struct f2fs_sb_info *sbi,
 	int i;
 
 	raw_sit = (struct f2fs_sit_block *)page_address(page);
+	memset(raw_sit, 0, PAGE_SIZE);
 	for (i = 0; i < end - start; i++) {
 		rs = &raw_sit->entries[i];
 		se = get_seg_entry(sbi, start + i);

commit 0833721ec3658a4e9d5e58b6fa82cf9edc431e59
Author: Yunlei He <heyunlei@huawei.com>
Date:   Thu Mar 8 16:29:13 2018 +0800

    f2fs: check blkaddr more accuratly before issue a bio
    
    This patch check blkaddr more accuratly before issue a
    write or read bio.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index dbb774aaf387..3325d0769723 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -53,13 +53,19 @@
 	 ((secno) == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno /		\
 	  (sbi)->segs_per_sec))	\
 
-#define MAIN_BLKADDR(sbi)	(SM_I(sbi)->main_blkaddr)
-#define SEG0_BLKADDR(sbi)	(SM_I(sbi)->seg0_blkaddr)
+#define MAIN_BLKADDR(sbi)						\
+	(SM_I(sbi) ? SM_I(sbi)->main_blkaddr : 				\
+		le32_to_cpu(F2FS_RAW_SUPER(sbi)->main_blkaddr))
+#define SEG0_BLKADDR(sbi)						\
+	(SM_I(sbi) ? SM_I(sbi)->seg0_blkaddr : 				\
+		le32_to_cpu(F2FS_RAW_SUPER(sbi)->segment0_blkaddr))
 
 #define MAIN_SEGS(sbi)	(SM_I(sbi)->main_segments)
 #define MAIN_SECS(sbi)	((sbi)->total_sections)
 
-#define TOTAL_SEGS(sbi)	(SM_I(sbi)->segment_count)
+#define TOTAL_SEGS(sbi)							\
+	(SM_I(sbi) ? SM_I(sbi)->segment_count : 				\
+		le32_to_cpu(F2FS_RAW_SUPER(sbi)->segment_count))
 #define TOTAL_BLKS(sbi)	(TOTAL_SEGS(sbi) << (sbi)->log_blocks_per_seg)
 
 #define MAX_BLKADDR(sbi)	(SEG0_BLKADDR(sbi) + TOTAL_BLKS(sbi))
@@ -632,10 +638,17 @@ static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
 	f2fs_bug_on(sbi, segno > TOTAL_SEGS(sbi) - 1);
 }
 
-static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
+static inline void verify_block_addr(struct f2fs_io_info *fio, block_t blk_addr)
 {
-	BUG_ON(blk_addr < SEG0_BLKADDR(sbi)
-			|| blk_addr >= MAX_BLKADDR(sbi));
+	struct f2fs_sb_info *sbi = fio->sbi;
+
+	if (PAGE_TYPE_OF_BIO(fio->type) == META &&
+				(!is_read_io(fio->op) || fio->is_meta))
+		BUG_ON(blk_addr < SEG0_BLKADDR(sbi) ||
+				blk_addr >= MAIN_BLKADDR(sbi));
+	else
+		BUG_ON(blk_addr < MAIN_BLKADDR(sbi) ||
+				blk_addr >= MAX_BLKADDR(sbi));
 }
 
 /*

commit 84b89e5d943d8d6b997fcebb4609373e1b3e4b03
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Feb 22 14:09:30 2018 -0800

    f2fs: add auto tuning for small devices
    
    If f2fs is running on top of very small devices, it's worth to avoid abusing
    free LBAs. In order to achieve that, this patch introduces some parameter
    tuning.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index f11c4bc82c78..dbb774aaf387 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -596,6 +596,8 @@ static inline int utilization(struct f2fs_sb_info *sbi)
 #define DEF_MIN_FSYNC_BLOCKS	8
 #define DEF_MIN_HOT_BLOCKS	16
 
+#define SMALL_VOLUME_SEGMENTS	(16 * 512)	/* 16GB */
+
 enum {
 	F2FS_IPU_FORCE,
 	F2FS_IPU_SSR,

commit 068c3cd85866785303f18960a4c00a182515b963
Author: Yunlei He <heyunlei@huawei.com>
Date:   Thu Jan 25 17:27:11 2018 +0800

    f2fs: rebuild sit page from sit info in mem
    
    This patch rebuild sit page from sit info in mem instead
    of issue a read io.
    
    I test this method and the result is as below:
    
    Pre:
     mmc_perf_test-12061 [001] ...1   976.819992: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
     mmc_perf_test-12061 [001] ...1   976.856446: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
     mmc_perf_test-12061 [003] ...1   998.976946: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
     mmc_perf_test-12061 [003] ...1   999.023269: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
     mmc_perf_test-12061 [003] ...1  1022.060772: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
     mmc_perf_test-12061 [003] ...1  1022.111034: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
     mmc_perf_test-12061 [002] ...1  1070.127643: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
     mmc_perf_test-12061 [003] ...1  1070.187352: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
     mmc_perf_test-12061 [003] ...1  1095.942124: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
     mmc_perf_test-12061 [003] ...1  1095.995975: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
     mmc_perf_test-12061 [003] ...1  1122.535091: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
     mmc_perf_test-12061 [003] ...1  1122.586521: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
     mmc_perf_test-12061 [001] ...1  1147.897487: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
     mmc_perf_test-12061 [001] ...1  1147.959438: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
     mmc_perf_test-12061 [003] ...1  1177.926951: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
     mmc_perf_test-12061 [002] ...1  1177.976823: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
     mmc_perf_test-12061 [002] ...1  1204.176087: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
     mmc_perf_test-12061 [002] ...1  1204.239046: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    
    Some sit flush consume more than 50ms.
    
    Now:
    mmc_perf_test-2187  [007] ...1   196.840684: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
    mmc_perf_test-2187  [007] ...1   196.841258: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    mmc_perf_test-2187  [007] ...1   219.430582: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
    mmc_perf_test-2187  [007] ...1   219.431144: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    mmc_perf_test-2187  [002] ...1   243.638678: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
    mmc_perf_test-2187  [000] ...1   243.638980: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    mmc_perf_test-2187  [002] ...1   265.392180: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
    mmc_perf_test-2187  [002] ...1   265.392245: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    mmc_perf_test-2187  [000] ...1   290.309051: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
    mmc_perf_test-2187  [000] ...1   290.309116: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    mmc_perf_test-2187  [003] ...1   317.144209: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
    mmc_perf_test-2187  [003] ...1   317.145913: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    mmc_perf_test-2187  [005] ...1   343.224954: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
    mmc_perf_test-2187  [005] ...1   343.225574: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    mmc_perf_test-2187  [000] ...1   370.239846: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
    mmc_perf_test-2187  [000] ...1   370.241138: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    mmc_perf_test-2187  [001] ...1   397.029043: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
    mmc_perf_test-2187  [001] ...1   397.030750: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    mmc_perf_test-2187  [003] ...1   425.386377: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = start flush sit
    mmc_perf_test-2187  [003] ...1   425.387735: f2fs_write_checkpoint: dev = (259,44), checkpoint for Sync, state = end flush sit
    
    Most sit flush consume no more than 1ms.
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 1a807e6e7c97..f11c4bc82c78 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -348,16 +348,41 @@ static inline void seg_info_from_raw_sit(struct seg_entry *se,
 	se->mtime = le64_to_cpu(rs->mtime);
 }
 
-static inline void seg_info_to_raw_sit(struct seg_entry *se,
+static inline void __seg_info_to_raw_sit(struct seg_entry *se,
 					struct f2fs_sit_entry *rs)
 {
 	unsigned short raw_vblocks = (se->type << SIT_VBLOCKS_SHIFT) |
 					se->valid_blocks;
 	rs->vblocks = cpu_to_le16(raw_vblocks);
 	memcpy(rs->valid_map, se->cur_valid_map, SIT_VBLOCK_MAP_SIZE);
+	rs->mtime = cpu_to_le64(se->mtime);
+}
+
+static inline void seg_info_to_sit_page(struct f2fs_sb_info *sbi,
+				struct page *page, unsigned int start)
+{
+	struct f2fs_sit_block *raw_sit;
+	struct seg_entry *se;
+	struct f2fs_sit_entry *rs;
+	unsigned int end = min(start + SIT_ENTRY_PER_BLOCK,
+					(unsigned long)MAIN_SEGS(sbi));
+	int i;
+
+	raw_sit = (struct f2fs_sit_block *)page_address(page);
+	for (i = 0; i < end - start; i++) {
+		rs = &raw_sit->entries[i];
+		se = get_seg_entry(sbi, start + i);
+		__seg_info_to_raw_sit(se, rs);
+	}
+}
+
+static inline void seg_info_to_raw_sit(struct seg_entry *se,
+					struct f2fs_sit_entry *rs)
+{
+	__seg_info_to_raw_sit(se, rs);
+
 	memcpy(se->ckpt_valid_map, rs->valid_map, SIT_VBLOCK_MAP_SIZE);
 	se->ckpt_valid_blocks = se->valid_blocks;
-	rs->mtime = cpu_to_le64(se->mtime);
 }
 
 static inline unsigned int find_next_inuse(struct free_segmap_info *free_i,

commit bb9e3bb8dbf5975a493e4e2dfbb900018579890c
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Jan 17 16:31:38 2018 +0800

    f2fs: split need_inplace_update
    
    This patch splits need_inplace_update to two functions:
    a. should_update_inplace() includes all conditions that we must use IPU.
    b. should_update_outplace() includes all conditions that we must use OPU.
    
    So that, in f2fs_ioc_set_pin_file() and f2fs_defragment_range(), we can
    use corresponding function to check whether we can trigger OPU/IPU or not.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 71a2aaa286df..1a807e6e7c97 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -580,47 +580,6 @@ enum {
 	F2FS_IPU_ASYNC,
 };
 
-static inline bool need_inplace_update_policy(struct inode *inode,
-				struct f2fs_io_info *fio)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	unsigned int policy = SM_I(sbi)->ipu_policy;
-
-	if (test_opt(sbi, LFS))
-		return false;
-
-	/* if this is cold file, we should overwrite to avoid fragmentation */
-	if (file_is_cold(inode))
-		return true;
-
-	if (policy & (0x1 << F2FS_IPU_FORCE))
-		return true;
-	if (policy & (0x1 << F2FS_IPU_SSR) && need_SSR(sbi))
-		return true;
-	if (policy & (0x1 << F2FS_IPU_UTIL) &&
-			utilization(sbi) > SM_I(sbi)->min_ipu_util)
-		return true;
-	if (policy & (0x1 << F2FS_IPU_SSR_UTIL) && need_SSR(sbi) &&
-			utilization(sbi) > SM_I(sbi)->min_ipu_util)
-		return true;
-
-	/*
-	 * IPU for rewrite async pages
-	 */
-	if (policy & (0x1 << F2FS_IPU_ASYNC) &&
-			fio && fio->op == REQ_OP_WRITE &&
-			!(fio->op_flags & REQ_SYNC) &&
-			!f2fs_encrypted_inode(inode))
-		return true;
-
-	/* this is only set during fdatasync */
-	if (policy & (0x1 << F2FS_IPU_FSYNC) &&
-			is_inode_flag_set(inode, FI_NEED_IPU))
-		return true;
-
-	return false;
-}
-
 static inline unsigned int curseg_segno(struct f2fs_sb_info *sbi,
 		int type)
 {

commit c39a1b348c4fe172729eff77c533dabc3c7cdaa7
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Dec 19 19:16:34 2017 -0800

    f2fs: return error during fill_super
    
    Let's avoid BUG_ON during fill_super, when on-disk was totall corrupted.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index d1d394cdf61d..71a2aaa286df 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -655,7 +655,7 @@ static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 /*
  * Summary block is always treated as an invalid block
  */
-static inline void check_block_count(struct f2fs_sb_info *sbi,
+static inline int check_block_count(struct f2fs_sb_info *sbi,
 		int segno, struct f2fs_sit_entry *raw_sit)
 {
 #ifdef CONFIG_F2FS_CHECK_FS
@@ -677,11 +677,25 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 		cur_pos = next_pos;
 		is_valid = !is_valid;
 	} while (cur_pos < sbi->blocks_per_seg);
-	BUG_ON(GET_SIT_VBLOCKS(raw_sit) != valid_blocks);
+
+	if (unlikely(GET_SIT_VBLOCKS(raw_sit) != valid_blocks)) {
+		f2fs_msg(sbi->sb, KERN_ERR,
+				"Mismatch valid blocks %d vs. %d",
+					GET_SIT_VBLOCKS(raw_sit), valid_blocks);
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		return -EINVAL;
+	}
 #endif
 	/* check segment usage, and check boundary of a given segment number */
-	f2fs_bug_on(sbi, GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg
-					|| segno > TOTAL_SEGS(sbi) - 1);
+	if (unlikely(GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg
+					|| segno > TOTAL_SEGS(sbi) - 1)) {
+		f2fs_msg(sbi->sb, KERN_ERR,
+				"Wrong valid blocks %d or segno %u",
+					GET_SIT_VBLOCKS(raw_sit), segno);
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		return -EINVAL;
+	}
+	return 0;
 }
 
 static inline pgoff_t current_sit_addr(struct f2fs_sb_info *sbi,

commit bf34c93d264509422e9eb11e2dbcea8d8e875efd
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Oct 30 17:49:54 2017 +0800

    f2fs: check curseg space before foreground GC
    
    When we are closing to trigger foreground GC, if there are only a few
    of dirty metas, we can log these dirty metas in left space of opened
    segments instead of triggering foreground GC.
    
    With this patch, total count of foreground GC triggered by
    test/generic/* of fstest suit reduce from 254 to 184.
    
    So let's do the check before foreground GC anyway.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 245acb34cab5..d1d394cdf61d 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -497,6 +497,33 @@ static inline int reserved_sections(struct f2fs_sb_info *sbi)
 	return GET_SEC_FROM_SEG(sbi, (unsigned int)reserved_segments(sbi));
 }
 
+static inline bool has_curseg_enough_space(struct f2fs_sb_info *sbi)
+{
+	unsigned int node_blocks = get_pages(sbi, F2FS_DIRTY_NODES) +
+					get_pages(sbi, F2FS_DIRTY_DENTS);
+	unsigned int dent_blocks = get_pages(sbi, F2FS_DIRTY_DENTS);
+	unsigned int segno, left_blocks;
+	int i;
+
+	/* check current node segment */
+	for (i = CURSEG_HOT_NODE; i <= CURSEG_COLD_NODE; i++) {
+		segno = CURSEG_I(sbi, i)->segno;
+		left_blocks = sbi->blocks_per_seg -
+			get_seg_entry(sbi, segno)->ckpt_valid_blocks;
+
+		if (node_blocks > left_blocks)
+			return false;
+	}
+
+	/* check current data segment */
+	segno = CURSEG_I(sbi, CURSEG_HOT_DATA)->segno;
+	left_blocks = sbi->blocks_per_seg -
+			get_seg_entry(sbi, segno)->ckpt_valid_blocks;
+	if (dent_blocks > left_blocks)
+		return false;
+	return true;
+}
+
 static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi,
 					int freed, int needed)
 {
@@ -507,6 +534,9 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi,
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		return false;
 
+	if (free_sections(sbi) + freed == reserved_sections(sbi) + needed &&
+			has_curseg_enough_space(sbi))
+		return false;
 	return (free_sections(sbi) + freed) <=
 		(node_secs + 2 * dent_secs + imeta_secs +
 		reserved_sections(sbi) + needed);

commit 3d26fa6be3c487fac7d87dc4a6f02a9ff0f6b1ef
Author: Chao Yu <yuchao0@huawei.com>
Date:   Mon Oct 30 17:49:53 2017 +0800

    f2fs: use rw_semaphore to protect SIT cache
    
    There are some cases user didn't update SIT cache under this lock,
    so let's use rw_semaphore instead of mutex to enhance concurrently
    accessing.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 8d93652d5b6a..245acb34cab5 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -231,7 +231,7 @@ struct sit_info {
 	unsigned long *dirty_sentries_bitmap;	/* bitmap for dirty sentries */
 	unsigned int dirty_sentries;		/* # of dirty sentries */
 	unsigned int sents_per_block;		/* # of SIT entries per block */
-	struct mutex sentry_lock;		/* to protect SIT cache */
+	struct rw_semaphore sentry_lock;	/* to protect SIT cache */
 	struct seg_entry *sentries;		/* SIT segment-level cache */
 	struct sec_entry *sec_entries;		/* SIT section-level cache */
 

commit 7e515b31d44dcd20a98c938dfdc21877a30042a0
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Oct 18 10:34:14 2017 +0800

    f2fs: fix to correct no_fggc_candidate
    
    There may be extreme case as below:
    
    For one section contains one segment, and there are total 100 segments
    with 10% over-privision ratio in f2fs partition, fggc_threshold will
    be rounded down to 460 instead of 460.8 as below caclulation:
    
    sbi->fggc_threshold = div_u64((u64)(main_count - ovp_count) *
                            BLKS_PER_SEC(sbi), (main_count - resv_count));
    
    If section usage is as:
    60 segments which contain 460 valid blocks
    40 segments which contain 462 valid blocks
    
    As valid block number in all sections is large than fggc_threshold, so
    none of them will be chosen as candidate due to incorrect fggc_threshold.
    
    Let's just soften the term of choosing foreground GC candidates.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 5a1f7b9c8a72..8d93652d5b6a 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -731,7 +731,7 @@ static inline block_t sum_blk_addr(struct f2fs_sb_info *sbi, int base, int type)
 static inline bool no_fggc_candidate(struct f2fs_sb_info *sbi,
 						unsigned int secno)
 {
-	if (get_valid_blocks(sbi, GET_SEG_FROM_SEC(sbi, secno), true) >=
+	if (get_valid_blocks(sbi, GET_SEG_FROM_SEC(sbi, secno), true) >
 						sbi->fggc_threshold)
 		return true;
 	return false;

commit 78997b569f5625f2762b3c4e743c33becdd5d99d
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed Oct 4 09:08:34 2017 +0800

    f2fs: split discard policy
    
    There are many different scenarios such as fstrim, umount, urgent or
    background where we will issue discards, actually, they need use
    different policy in aspect of io aware, discard granularity, delay
    interval and so on. But now they just share one common discard policy,
    so there will be race when changing policy in between these scenarios,
    the interference of changing discard policy will be very serious.
    
    This patch changes to split discard policy for different scenarios.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index e0a6cc23ace3..5a1f7b9c8a72 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -796,8 +796,9 @@ static inline void wake_up_discard_thread(struct f2fs_sb_info *sbi, bool force)
 		goto wake_up;
 
 	mutex_lock(&dcc->cmd_lock);
-	for (i = MAX_PLIST_NUM - 1;
-			i >= 0 && plist_issue(dcc->pend_list_tag[i]); i--) {
+	for (i = MAX_PLIST_NUM - 1; i >= 0; i--) {
+		if (i + 1 < dcc->discard_granularity)
+			break;
 		if (!list_empty(&dcc->pend_list[i])) {
 			wakeup = true;
 			break;

commit b3a97a2a9a7b2d50bcf13d32857cd6f5695c6b65
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Sep 9 11:11:04 2017 -0700

    f2fs: speed up gc_urgent mode with SSR
    
    This patch activates SSR in gc_urgent mode.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 613b2fa7b1c1..e0a6cc23ace3 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -497,19 +497,6 @@ static inline int reserved_sections(struct f2fs_sb_info *sbi)
 	return GET_SEC_FROM_SEG(sbi, (unsigned int)reserved_segments(sbi));
 }
 
-static inline bool need_SSR(struct f2fs_sb_info *sbi)
-{
-	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
-	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
-	int imeta_secs = get_blocktype_secs(sbi, F2FS_DIRTY_IMETA);
-
-	if (test_opt(sbi, LFS))
-		return false;
-
-	return free_sections(sbi) <= (node_secs + 2 * dent_secs + imeta_secs +
-						2 * reserved_sections(sbi));
-}
-
 static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi,
 					int freed, int needed)
 {

commit 01983c715ad0e78842a885f361ad927a3a985994
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Aug 22 21:15:43 2017 -0700

    f2fs: wake up discard_thread iff there is a candidate
    
    This patch fixes to avoid needless wake ups.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index b9b4f85ffeb6..613b2fa7b1c1 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -798,3 +798,28 @@ static inline long nr_pages_to_write(struct f2fs_sb_info *sbi, int type,
 	wbc->nr_to_write = desired;
 	return desired - nr_to_write;
 }
+
+static inline void wake_up_discard_thread(struct f2fs_sb_info *sbi, bool force)
+{
+	struct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;
+	bool wakeup = false;
+	int i;
+
+	if (force)
+		goto wake_up;
+
+	mutex_lock(&dcc->cmd_lock);
+	for (i = MAX_PLIST_NUM - 1;
+			i >= 0 && plist_issue(dcc->pend_list_tag[i]); i--) {
+		if (!list_empty(&dcc->pend_list[i])) {
+			wakeup = true;
+			break;
+		}
+	}
+	mutex_unlock(&dcc->cmd_lock);
+	if (!wakeup)
+		return;
+wake_up:
+	dcc->discard_wake = 1;
+	wake_up_interruptible_all(&dcc->discard_wait_queue);
+}

commit f24b150a63b208f4437efc7dc7a42954363641ca
Author: Yunlong Song <yunlong.song@huawei.com>
Date:   Mon Aug 14 16:52:43 2017 +0800

    f2fs: remove unused function overprovision_sections
    
    Signed-off-by: Yunlong Song <yunlong.song@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 7f700e54b77d..b9b4f85ffeb6 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -492,11 +492,6 @@ static inline int overprovision_segments(struct f2fs_sb_info *sbi)
 	return SM_I(sbi)->ovp_segments;
 }
 
-static inline int overprovision_sections(struct f2fs_sb_info *sbi)
-{
-	return GET_SEC_FROM_SEG(sbi, (unsigned int)overprovision_segments(sbi));
-}
-
 static inline int reserved_sections(struct f2fs_sb_info *sbi)
 {
 	return GET_SEC_FROM_SEG(sbi, (unsigned int)reserved_segments(sbi));

commit 3537581a722402a1e6f0942cd0653035a9e09e21
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Sat Aug 5 14:25:08 2017 -0700

    f2fs: use IPU for cold files
    
    We expect cold files write data sequentially, but sometimes some of small data
    can be updated, which incurs fragmentation.
    Let's avoid that.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 6b871b492fd5..7f700e54b77d 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -577,6 +577,10 @@ static inline bool need_inplace_update_policy(struct inode *inode,
 	if (test_opt(sbi, LFS))
 		return false;
 
+	/* if this is cold file, we should overwrite to avoid fragmentation */
+	if (file_is_cold(inode))
+		return true;
+
 	if (policy & (0x1 << F2FS_IPU_FORCE))
 		return true;
 	if (policy & (0x1 << F2FS_IPU_SSR) && need_SSR(sbi))

commit 5cdd4c046864827e7ac140eed081c6768a4dbb16
Merge: 7cee9384cb3e 0abd675e97e6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 10 14:29:45 2017 -0700

    Merge tag 'for-f2fs-4.13' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs
    
    Pull f2fs updates from Jaegeuk Kim:
     "In this round, we've added new features such as disk quota and statx,
      and modified internal bio management flow to merge more IOs depending
      on block types. We've also made internal threads freezeable for
      Android battery life. In addition to them, there are some patches to
      avoid lock contention as well as a couple of deadlock conditions.
    
      Enhancements:
       - support usrquota, grpquota, and statx
       - manage DATA/NODE typed bios separately to serialize more IOs
       - modify f2fs_lock_op/wio_mutex to avoid lock contention
       - prevent lock contention in migratepage
    
      Bug fixes:
       - fix missing load of written inode flag
       - fix worst case victim selection in GC
       - freezeable GC and discard threads for Android battery life
       - sanitize f2fs metadata to deal with security hole
       - clean up sysfs-related code and docs"
    
    * tag 'for-f2fs-4.13' of git://git.kernel.org/pub/scm/linux/kernel/git/jaegeuk/f2fs: (59 commits)
      f2fs: support plain user/group quota
      f2fs: avoid deadlock caused by lock order of page and lock_op
      f2fs: use spin_{,un}lock_irq{save,restore}
      f2fs: relax migratepage for atomic written page
      f2fs: don't count inode block in in-memory inode.i_blocks
      Revert "f2fs: fix to clean previous mount option when remount_fs"
      f2fs: do not set LOST_PINO for renamed dir
      f2fs: do not set LOST_PINO for newly created dir
      f2fs: skip ->writepages for {mete,node}_inode during recovery
      f2fs: introduce __check_sit_bitmap
      f2fs: stop gc/discard thread in prior during umount
      f2fs: introduce reserved_blocks in sysfs
      f2fs: avoid redundant f2fs_flush after remount
      f2fs: report # of free inodes more precisely
      f2fs: add ioctl to do gc with target block address
      f2fs: don't need to check encrypted inode for partial truncation
      f2fs: measure inode.i_blocks as generic filesystem
      f2fs: set CP_TRIMMED_FLAG correctly
      f2fs: require key for truncate(2) of encrypted file
      f2fs: move sysfs code from super.c to fs/f2fs/sysfs.c
      ...

commit a912b54d3aaa011266dc266e3694f782f27233cf
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed May 10 11:18:25 2017 -0700

    f2fs: split bio cache
    
    Split DATA/NODE type bio cache according to different temperature,
    so write IOs with the same temperature can be merged in corresponding
    bio cache as much as possible, otherwise, different temperature write
    IOs submitting into one bio cache will always cause split of bio.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 10bf05d4cff4..e9ba1f1d9723 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -27,6 +27,10 @@
 #define IS_DATASEG(t)	((t) <= CURSEG_COLD_DATA)
 #define IS_NODESEG(t)	((t) >= CURSEG_HOT_NODE)
 
+#define IS_HOT(t)	((t) == CURSEG_HOT_NODE || (t) == CURSEG_HOT_DATA)
+#define IS_WARM(t)	((t) == CURSEG_WARM_NODE || (t) == CURSEG_WARM_DATA)
+#define IS_COLD(t)	((t) == CURSEG_COLD_NODE || (t) == CURSEG_COLD_DATA)
+
 #define IS_CURSEG(sbi, seg)						\
 	(((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno) ||	\
 	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno) ||	\

commit bf5f89463f5b3109a72ed13ca62b57e90213387d
Merge: 2d3e4866dea9 4d2b5bcab53f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 8 18:17:56 2017 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
    
     - the rest of MM
    
     - various misc things
    
     - procfs updates
    
     - lib/ updates
    
     - checkpatch updates
    
     - kdump/kexec updates
    
     - add kvmalloc helpers, use them
    
     - time helper updates for Y2038 issues. We're almost ready to remove
       current_fs_time() but that awaits a btrfs merge.
    
     - add tracepoints to DAX
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (114 commits)
      drivers/staging/ccree/ssi_hash.c: fix build with gcc-4.4.4
      selftests/vm: add a test for virtual address range mapping
      dax: add tracepoint to dax_insert_mapping()
      dax: add tracepoint to dax_writeback_one()
      dax: add tracepoints to dax_writeback_mapping_range()
      dax: add tracepoints to dax_load_hole()
      dax: add tracepoints to dax_pfn_mkwrite()
      dax: add tracepoints to dax_iomap_pte_fault()
      mtd: nand: nandsim: convert to memalloc_noreclaim_*()
      treewide: convert PF_MEMALLOC manipulations to new helpers
      mm: introduce memalloc_noreclaim_{save,restore}
      mm: prevent potential recursive reclaim due to clearing PF_MEMALLOC
      mm/huge_memory.c: deposit a pgtable for DAX PMD faults when required
      mm/huge_memory.c: use zap_deposited_table() more
      time: delete CURRENT_TIME_SEC and CURRENT_TIME
      gfs2: replace CURRENT_TIME with current_time
      apparmorfs: replace CURRENT_TIME with current_time()
      lustre: replace CURRENT_TIME macro
      fs: ubifs: replace CURRENT_TIME_SEC with current_time
      fs: ufs: use ktime_get_real_ts64() for birthtime
      ...

commit 48fbfe50f1d5fef51bac98d105d2a28df42a1205
Author: Deepa Dinamani <deepa.kernel@gmail.com>
Date:   Mon May 8 15:59:10 2017 -0700

    fs: f2fs: use ktime_get_real_seconds for sit_info times
    
    CURRENT_TIME_SEC is not y2038 safe.
    
    Replace use of CURRENT_TIME_SEC with ktime_get_real_seconds in segment
    timestamps used by GC algorithm including the segment mtime timestamps.
    
    Link: http://lkml.kernel.org/r/1491613030-11599-2-git-send-email-deepa.kernel@gmail.com
    Signed-off-by: Deepa Dinamani <deepa.kernel@gmail.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 5e8ad4280a50..313b99040aa6 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -691,8 +691,9 @@ static inline void set_to_next_sit(struct sit_info *sit_i, unsigned int start)
 static inline unsigned long long get_mtime(struct f2fs_sb_info *sbi)
 {
 	struct sit_info *sit_i = SIT_I(sbi);
-	return sit_i->elapsed_time + CURRENT_TIME_SEC.tv_sec -
-						sit_i->mounted_time;
+	time64_t now = ktime_get_real_seconds();
+
+	return sit_i->elapsed_time + now - sit_i->mounted_time;
 }
 
 static inline void set_summary(struct f2fs_summary *sum, nid_t nid,

commit 7eab0c0df8d1a8c460f7d660d3ffd06fd448e590
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Tue Apr 25 12:45:12 2017 +0000

    f2fs: reconstruct code to write a data page
    
    This patch introduces encrypt_one_page which encrypts one data page before
    submit_bio, and change the use of need_inplace_update.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 8ad22b8cbba7..10bf05d4cff4 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -564,16 +564,12 @@ enum {
 	F2FS_IPU_ASYNC,
 };
 
-static inline bool need_inplace_update(struct inode *inode,
+static inline bool need_inplace_update_policy(struct inode *inode,
 				struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	unsigned int policy = SM_I(sbi)->ipu_policy;
 
-	/* IPU can be done only for the user data */
-	if (S_ISDIR(inode->i_mode) || f2fs_is_atomic_file(inode))
-		return false;
-
 	if (test_opt(sbi, LFS))
 		return false;
 

commit 4086d3f61b6573f65ddc13fc375c0c7b0ac482a0
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Fri Apr 21 12:41:48 2017 +0000

    f2fs: skip encrypted inode in ASYNC IPU policy
    
    Async request may be throttled in block layer, so page for async may keep WRITE_BACK
    for a long time.
    
    For encrytped inode, we need wait on page writeback no matter if the device supports
    BDI_CAP_STABLE_WRITES. This may result in a higher waiting page writeback time for
    async encrypted inode page.
    
    This patch skips IPU for encrypted inode's updating write.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 93cc4e504aab..8ad22b8cbba7 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -593,7 +593,8 @@ static inline bool need_inplace_update(struct inode *inode,
 	 */
 	if (policy & (0x1 << F2FS_IPU_ASYNC) &&
 			fio && fio->op == REQ_OP_WRITE &&
-			!(fio->op_flags & REQ_SYNC))
+			!(fio->op_flags & REQ_SYNC) &&
+			!f2fs_encrypted_inode(inode))
 		return true;
 
 	/* this is only set during fdatasync */

commit e066b83c9b40f3a6951fb693ef0943fa1dfc40c2
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 13 15:17:00 2017 -0700

    f2fs: add ioctl to flush data from faster device to cold area
    
    This patch adds an ioctl to flush data in faster device to cold area. User can
    give device number and number of segments to move. It doesn't move it if there
    is only one device.
    
    The parameter looks like:
    
    struct f2fs_flush_device {
            u32 dev_num;            /* device number to flush */
            u32 segments;           /* # of segments to flush */
    };
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 3cd780a42f51..93cc4e504aab 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -138,7 +138,10 @@ enum {
  */
 enum {
 	GC_CB = 0,
-	GC_GREEDY
+	GC_GREEDY,
+	ALLOC_NEXT,
+	FLUSH_DEVICE,
+	MAX_GC_POLICY,
 };
 
 /*
@@ -233,6 +236,8 @@ struct sit_info {
 	unsigned long long mounted_time;	/* mount time */
 	unsigned long long min_mtime;		/* min. modification time */
 	unsigned long long max_mtime;		/* max. modification time */
+
+	unsigned int last_victim[MAX_GC_POLICY]; /* last victim segment # */
 };
 
 struct free_segmap_info {

commit 04485987f05388ffec04cdab7808ba26db30c9b8
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Tue Apr 18 11:57:16 2017 +0000

    f2fs: introduce async IPU policy
    
    This patch introduces an ASYNC IPU policy.
    
    Under senario of large # of async updating(e.g. log writing in Android),
    disk would be seriously fragmented, and higher frequent gc would be triggered.
    
    This patch uses IPU to rewrite the async update writting, since async is
    NOT sensitive to io latency.
    
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 5f6ef163aa8f..3cd780a42f51 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -556,9 +556,11 @@ enum {
 	F2FS_IPU_UTIL,
 	F2FS_IPU_SSR_UTIL,
 	F2FS_IPU_FSYNC,
+	F2FS_IPU_ASYNC,
 };
 
-static inline bool need_inplace_update(struct inode *inode)
+static inline bool need_inplace_update(struct inode *inode,
+				struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	unsigned int policy = SM_I(sbi)->ipu_policy;
@@ -581,6 +583,14 @@ static inline bool need_inplace_update(struct inode *inode)
 			utilization(sbi) > SM_I(sbi)->min_ipu_util)
 		return true;
 
+	/*
+	 * IPU for rewrite async pages
+	 */
+	if (policy & (0x1 << F2FS_IPU_ASYNC) &&
+			fio && fio->op == REQ_OP_WRITE &&
+			!(fio->op_flags & REQ_SYNC))
+		return true;
+
 	/* this is only set during fdatasync */
 	if (policy & (0x1 << F2FS_IPU_FSYNC) &&
 			is_inode_flag_set(inode, FI_NEED_IPU))

commit 4ddb1a4d4dc20642073b7d92400a67b67601fe6f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Apr 7 15:08:17 2017 -0700

    f2fs: clean up some macros in terms of GET_SEGNO
    
    This patch cleans several macros by introducing:
    - BLKS_PER_SEC
    - GET_SEC_FROM_SEG
    - GET_SEG_FROM_SEC
    - GET_ZONE_FROM_SEC
    - GET_ZONE_FROM_SEG
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 053166038bfe..5f6ef163aa8f 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -78,12 +78,16 @@
 	((((blk_addr) == NULL_ADDR) || ((blk_addr) == NEW_ADDR)) ?	\
 	NULL_SEGNO : GET_L2R_SEGNO(FREE_I(sbi),			\
 		GET_SEGNO_FROM_SEG0(sbi, blk_addr)))
-#define GET_SECNO(sbi, segno)					\
+#define BLKS_PER_SEC(sbi)					\
+	((sbi)->segs_per_sec * (sbi)->blocks_per_seg)
+#define GET_SEC_FROM_SEG(sbi, segno)				\
 	((segno) / (sbi)->segs_per_sec)
-#define GET_SEGNO_FROM_SECNO(sbi, secno)				\
+#define GET_SEG_FROM_SEC(sbi, secno)				\
 	((secno) * (sbi)->segs_per_sec)
-#define GET_ZONENO_FROM_SEGNO(sbi, segno)				\
-	(((segno) / (sbi)->segs_per_sec) / (sbi)->secs_per_zone)
+#define GET_ZONE_FROM_SEC(sbi, secno)				\
+	((secno) / (sbi)->secs_per_zone)
+#define GET_ZONE_FROM_SEG(sbi, segno)				\
+	GET_ZONE_FROM_SEC(sbi, GET_SEC_FROM_SEG(sbi, segno))
 
 #define GET_SUM_BLOCK(sbi, segno)				\
 	((sbi)->sm_info->ssa_blkaddr + (segno))
@@ -305,7 +309,7 @@ static inline struct sec_entry *get_sec_entry(struct f2fs_sb_info *sbi,
 						unsigned int segno)
 {
 	struct sit_info *sit_i = SIT_I(sbi);
-	return &sit_i->sec_entries[GET_SECNO(sbi, segno)];
+	return &sit_i->sec_entries[GET_SEC_FROM_SEG(sbi, segno)];
 }
 
 static inline unsigned int get_valid_blocks(struct f2fs_sb_info *sbi,
@@ -360,8 +364,8 @@ static inline unsigned int find_next_inuse(struct free_segmap_info *free_i,
 static inline void __set_free(struct f2fs_sb_info *sbi, unsigned int segno)
 {
 	struct free_segmap_info *free_i = FREE_I(sbi);
-	unsigned int secno = segno / sbi->segs_per_sec;
-	unsigned int start_segno = secno * sbi->segs_per_sec;
+	unsigned int secno = GET_SEC_FROM_SEG(sbi, segno);
+	unsigned int start_segno = GET_SEG_FROM_SEC(sbi, secno);
 	unsigned int next;
 
 	spin_lock(&free_i->segmap_lock);
@@ -381,7 +385,8 @@ static inline void __set_inuse(struct f2fs_sb_info *sbi,
 		unsigned int segno)
 {
 	struct free_segmap_info *free_i = FREE_I(sbi);
-	unsigned int secno = segno / sbi->segs_per_sec;
+	unsigned int secno = GET_SEC_FROM_SEG(sbi, segno);
+
 	set_bit(segno, free_i->free_segmap);
 	free_i->free_segments--;
 	if (!test_and_set_bit(secno, free_i->free_secmap))
@@ -392,8 +397,8 @@ static inline void __set_test_and_free(struct f2fs_sb_info *sbi,
 		unsigned int segno)
 {
 	struct free_segmap_info *free_i = FREE_I(sbi);
-	unsigned int secno = segno / sbi->segs_per_sec;
-	unsigned int start_segno = secno * sbi->segs_per_sec;
+	unsigned int secno = GET_SEC_FROM_SEG(sbi, segno);
+	unsigned int start_segno = GET_SEG_FROM_SEC(sbi, secno);
 	unsigned int next;
 
 	spin_lock(&free_i->segmap_lock);
@@ -414,7 +419,8 @@ static inline void __set_test_and_inuse(struct f2fs_sb_info *sbi,
 		unsigned int segno)
 {
 	struct free_segmap_info *free_i = FREE_I(sbi);
-	unsigned int secno = segno / sbi->segs_per_sec;
+	unsigned int secno = GET_SEC_FROM_SEG(sbi, segno);
+
 	spin_lock(&free_i->segmap_lock);
 	if (!test_and_set_bit(segno, free_i->free_segmap)) {
 		free_i->free_segments--;
@@ -479,12 +485,12 @@ static inline int overprovision_segments(struct f2fs_sb_info *sbi)
 
 static inline int overprovision_sections(struct f2fs_sb_info *sbi)
 {
-	return ((unsigned int) overprovision_segments(sbi)) / sbi->segs_per_sec;
+	return GET_SEC_FROM_SEG(sbi, (unsigned int)overprovision_segments(sbi));
 }
 
 static inline int reserved_sections(struct f2fs_sb_info *sbi)
 {
-	return ((unsigned int) reserved_segments(sbi)) / sbi->segs_per_sec;
+	return GET_SEC_FROM_SEG(sbi, (unsigned int)reserved_segments(sbi));
 }
 
 static inline bool need_SSR(struct f2fs_sb_info *sbi)
@@ -722,7 +728,7 @@ static inline block_t sum_blk_addr(struct f2fs_sb_info *sbi, int base, int type)
 static inline bool no_fggc_candidate(struct f2fs_sb_info *sbi,
 						unsigned int secno)
 {
-	if (get_valid_blocks(sbi, GET_SEGNO_FROM_SECNO(sbi, secno), true) >=
+	if (get_valid_blocks(sbi, GET_SEG_FROM_SEC(sbi, secno), true) >=
 						sbi->fggc_threshold)
 		return true;
 	return false;

commit 302bd34882b1e20797f08cc13ef060ec972d0acb
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Apr 7 14:33:22 2017 -0700

    f2fs: clean up get_valid_blocks with consistent parameter
    
    This patch cleans up get_valid_blocks, which has no functional change.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 39ef9cc0093b..053166038bfe 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -309,13 +309,13 @@ static inline struct sec_entry *get_sec_entry(struct f2fs_sb_info *sbi,
 }
 
 static inline unsigned int get_valid_blocks(struct f2fs_sb_info *sbi,
-				unsigned int segno, int section)
+				unsigned int segno, bool use_section)
 {
 	/*
 	 * In order to get # of valid blocks in a section instantly from many
 	 * segments, f2fs manages two counting structures separately.
 	 */
-	if (section > 1)
+	if (use_section && sbi->segs_per_sec > 1)
 		return get_sec_entry(sbi, segno)->valid_blocks;
 	else
 		return get_seg_entry(sbi, segno)->valid_blocks;
@@ -722,8 +722,8 @@ static inline block_t sum_blk_addr(struct f2fs_sb_info *sbi, int base, int type)
 static inline bool no_fggc_candidate(struct f2fs_sb_info *sbi,
 						unsigned int secno)
 {
-	if (get_valid_blocks(sbi, GET_SEGNO_FROM_SECNO(sbi, secno),
-				sbi->segs_per_sec) >= sbi->fggc_threshold)
+	if (get_valid_blocks(sbi, GET_SEGNO_FROM_SECNO(sbi, secno), true) >=
+						sbi->fggc_threshold)
 		return true;
 	return false;
 }

commit 63fcf8e8d6bff2e8228f348c78b4a4fcca44401a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Apr 7 14:27:07 2017 -0700

    f2fs: use segment number for get_valid_blocks
    
    This patch fixes to submit a segment number for get_valid_blocks.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index b8a1bac9355d..39ef9cc0093b 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -80,6 +80,8 @@
 		GET_SEGNO_FROM_SEG0(sbi, blk_addr)))
 #define GET_SECNO(sbi, segno)					\
 	((segno) / (sbi)->segs_per_sec)
+#define GET_SEGNO_FROM_SECNO(sbi, secno)				\
+	((secno) * (sbi)->segs_per_sec)
 #define GET_ZONENO_FROM_SEGNO(sbi, segno)				\
 	(((segno) / (sbi)->segs_per_sec) / (sbi)->secs_per_zone)
 
@@ -720,8 +722,8 @@ static inline block_t sum_blk_addr(struct f2fs_sb_info *sbi, int base, int type)
 static inline bool no_fggc_candidate(struct f2fs_sb_info *sbi,
 						unsigned int secno)
 {
-	if (get_valid_blocks(sbi, secno, sbi->segs_per_sec) >=
-						sbi->fggc_threshold)
+	if (get_valid_blocks(sbi, GET_SEGNO_FROM_SECNO(sbi, secno),
+				sbi->segs_per_sec) >= sbi->fggc_threshold)
 		return true;
 	return false;
 }

commit 68afcf2d38cd7544817558757e57e7b9d5c4e72c
Author: Tomohiro Kusumi <tkusumi@tuxera.com>
Date:   Sun Apr 9 02:11:36 2017 +0300

    f2fs: guard macro variables with braces
    
    Add braces around variables used within macros for those make sense
    to do it. Many of the macros in f2fs already do this. What this commit
    doesn't do is anything that changes line# as a result of adding braces,
    which usually affects the binary via __LINE__.
    
    Confirmed no diff in fs/f2fs/f2fs.ko before/after this commit on x86_64,
    to make sure this has no functional change as well as there's been no
    unexpected side effect due to callers' arithmetics within the existing
    code.
    
    Signed-off-by: Tomohiro Kusumi <tkusumi@tuxera.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 57e36c1ce7bd..b8a1bac9355d 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -21,78 +21,78 @@
 #define F2FS_MIN_SEGMENTS	9 /* SB + 2 (CP + SIT + NAT) + SSA + MAIN */
 
 /* L: Logical segment # in volume, R: Relative segment # in main area */
-#define GET_L2R_SEGNO(free_i, segno)	(segno - free_i->start_segno)
-#define GET_R2L_SEGNO(free_i, segno)	(segno + free_i->start_segno)
+#define GET_L2R_SEGNO(free_i, segno)	((segno) - (free_i)->start_segno)
+#define GET_R2L_SEGNO(free_i, segno)	((segno) + (free_i)->start_segno)
 
-#define IS_DATASEG(t)	(t <= CURSEG_COLD_DATA)
-#define IS_NODESEG(t)	(t >= CURSEG_HOT_NODE)
+#define IS_DATASEG(t)	((t) <= CURSEG_COLD_DATA)
+#define IS_NODESEG(t)	((t) >= CURSEG_HOT_NODE)
 
 #define IS_CURSEG(sbi, seg)						\
-	((seg == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno) ||	\
-	 (seg == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno) ||	\
-	 (seg == CURSEG_I(sbi, CURSEG_COLD_DATA)->segno) ||	\
-	 (seg == CURSEG_I(sbi, CURSEG_HOT_NODE)->segno) ||	\
-	 (seg == CURSEG_I(sbi, CURSEG_WARM_NODE)->segno) ||	\
-	 (seg == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno))
+	(((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_COLD_DATA)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_NODE)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_NODE)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno))
 
 #define IS_CURSEC(sbi, secno)						\
-	((secno == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno /		\
-	  sbi->segs_per_sec) ||	\
-	 (secno == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno /		\
-	  sbi->segs_per_sec) ||	\
-	 (secno == CURSEG_I(sbi, CURSEG_COLD_DATA)->segno /		\
-	  sbi->segs_per_sec) ||	\
-	 (secno == CURSEG_I(sbi, CURSEG_HOT_NODE)->segno /		\
-	  sbi->segs_per_sec) ||	\
-	 (secno == CURSEG_I(sbi, CURSEG_WARM_NODE)->segno /		\
-	  sbi->segs_per_sec) ||	\
-	 (secno == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno /		\
-	  sbi->segs_per_sec))	\
+	(((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_COLD_DATA)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_NODE)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_NODE)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno /		\
+	  (sbi)->segs_per_sec))	\
 
 #define MAIN_BLKADDR(sbi)	(SM_I(sbi)->main_blkaddr)
 #define SEG0_BLKADDR(sbi)	(SM_I(sbi)->seg0_blkaddr)
 
 #define MAIN_SEGS(sbi)	(SM_I(sbi)->main_segments)
-#define MAIN_SECS(sbi)	(sbi->total_sections)
+#define MAIN_SECS(sbi)	((sbi)->total_sections)
 
 #define TOTAL_SEGS(sbi)	(SM_I(sbi)->segment_count)
-#define TOTAL_BLKS(sbi)	(TOTAL_SEGS(sbi) << sbi->log_blocks_per_seg)
+#define TOTAL_BLKS(sbi)	(TOTAL_SEGS(sbi) << (sbi)->log_blocks_per_seg)
 
 #define MAX_BLKADDR(sbi)	(SEG0_BLKADDR(sbi) + TOTAL_BLKS(sbi))
-#define SEGMENT_SIZE(sbi)	(1ULL << (sbi->log_blocksize +		\
-					sbi->log_blocks_per_seg))
+#define SEGMENT_SIZE(sbi)	(1ULL << ((sbi)->log_blocksize +	\
+					(sbi)->log_blocks_per_seg))
 
 #define START_BLOCK(sbi, segno)	(SEG0_BLKADDR(sbi) +			\
-	 (GET_R2L_SEGNO(FREE_I(sbi), segno) << sbi->log_blocks_per_seg))
+	 (GET_R2L_SEGNO(FREE_I(sbi), segno) << (sbi)->log_blocks_per_seg))
 
 #define NEXT_FREE_BLKADDR(sbi, curseg)					\
-	(START_BLOCK(sbi, curseg->segno) + curseg->next_blkoff)
+	(START_BLOCK(sbi, (curseg)->segno) + (curseg)->next_blkoff)
 
 #define GET_SEGOFF_FROM_SEG0(sbi, blk_addr)	((blk_addr) - SEG0_BLKADDR(sbi))
 #define GET_SEGNO_FROM_SEG0(sbi, blk_addr)				\
-	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) >> sbi->log_blocks_per_seg)
+	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) >> (sbi)->log_blocks_per_seg)
 #define GET_BLKOFF_FROM_SEG0(sbi, blk_addr)				\
-	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) & (sbi->blocks_per_seg - 1))
+	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) & ((sbi)->blocks_per_seg - 1))
 
 #define GET_SEGNO(sbi, blk_addr)					\
-	(((blk_addr == NULL_ADDR) || (blk_addr == NEW_ADDR)) ?		\
+	((((blk_addr) == NULL_ADDR) || ((blk_addr) == NEW_ADDR)) ?	\
 	NULL_SEGNO : GET_L2R_SEGNO(FREE_I(sbi),			\
 		GET_SEGNO_FROM_SEG0(sbi, blk_addr)))
 #define GET_SECNO(sbi, segno)					\
-	((segno) / sbi->segs_per_sec)
+	((segno) / (sbi)->segs_per_sec)
 #define GET_ZONENO_FROM_SEGNO(sbi, segno)				\
-	((segno / sbi->segs_per_sec) / sbi->secs_per_zone)
+	(((segno) / (sbi)->segs_per_sec) / (sbi)->secs_per_zone)
 
 #define GET_SUM_BLOCK(sbi, segno)				\
-	((sbi->sm_info->ssa_blkaddr) + segno)
+	((sbi)->sm_info->ssa_blkaddr + (segno))
 
 #define GET_SUM_TYPE(footer) ((footer)->entry_type)
-#define SET_SUM_TYPE(footer, type) ((footer)->entry_type = type)
+#define SET_SUM_TYPE(footer, type) ((footer)->entry_type = (type))
 
 #define SIT_ENTRY_OFFSET(sit_i, segno)					\
-	(segno % sit_i->sents_per_block)
+	((segno) % (sit_i)->sents_per_block)
 #define SIT_BLOCK_OFFSET(segno)					\
-	(segno / SIT_ENTRY_PER_BLOCK)
+	((segno) / SIT_ENTRY_PER_BLOCK)
 #define	START_SEGNO(segno)		\
 	(SIT_BLOCK_OFFSET(segno) * SIT_ENTRY_PER_BLOCK)
 #define SIT_BLK_CNT(sbi)			\
@@ -103,7 +103,7 @@
 #define SECTOR_FROM_BLOCK(blk_addr)					\
 	(((sector_t)blk_addr) << F2FS_LOG_SECTORS_PER_BLOCK)
 #define SECTOR_TO_BLOCK(sectors)					\
-	(sectors >> F2FS_LOG_SECTORS_PER_BLOCK)
+	((sectors) >> F2FS_LOG_SECTORS_PER_BLOCK)
 
 /*
  * indicate a block allocation direction: RIGHT and LEFT.

commit ef095d19e82f25bbdead472b8b71f4ef3b7a636d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Mar 24 20:05:13 2017 -0400

    f2fs: write small sized IO to hot log
    
    It would better split small and large IOs separately in order to get more
    consecutive big writes.
    
    The default threshold is set to 64KB, but configurable by sysfs/min_hot_blocks.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 31846b0fcb95..57e36c1ce7bd 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -540,6 +540,7 @@ static inline int utilization(struct f2fs_sb_info *sbi)
  */
 #define DEF_MIN_IPU_UTIL	70
 #define DEF_MIN_FSYNC_BLOCKS	8
+#define DEF_MIN_HOT_BLOCKS	16
 
 enum {
 	F2FS_IPU_FORCE,

commit 796dbbfe4e52cc3ae860794fc20ea157391c4d93
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Mar 24 21:08:56 2017 -0400

    f2fs: start SSR much eariler to avoid FG_GC
    
    This patch initiates SSR much eariler, resulting in less FG_GC.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 5e8ad4280a50..31846b0fcb95 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -495,7 +495,7 @@ static inline bool need_SSR(struct f2fs_sb_info *sbi)
 		return false;
 
 	return free_sections(sbi) <= (node_secs + 2 * dent_secs + imeta_secs +
-						reserved_sections(sbi) + 1);
+						2 * reserved_sections(sbi));
 }
 
 static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi,

commit 727ebb091e1778cc3a4842ae04f8f54da111eee0
Author: Kinglong Mee <kinglongmee@gmail.com>
Date:   Sat Feb 25 19:32:21 2017 +0800

    f2fs: update the comment of default nr_pages to skipping
    
    Fixes: 2c237ebaa4 ("f2fs: avoid writing node/metapages during writes")
    Signed-off-by: Kinglong Mee <kinglongmee@gmail.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index f4020f141d83..5e8ad4280a50 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -736,8 +736,8 @@ static inline bool sec_usage_check(struct f2fs_sb_info *sbi, unsigned int secno)
  * It is very important to gather dirty pages and write at once, so that we can
  * submit a big bio without interfering other data writes.
  * By default, 512 pages for directory data,
- * 512 pages (2MB) * 3 for three types of nodes, and
- * max_bio_blocks for meta are set.
+ * 512 pages (2MB) * 8 for nodes, and
+ * 256 pages * 8 for meta are set.
  */
 static inline int nr_pages_to_skip(struct f2fs_sb_info *sbi, int type)
 {

commit e93b9865251a0503d83fd570e7d5a7c8bc351715
Author: Hou Pengyang <houpengyang@huawei.com>
Date:   Thu Feb 16 12:34:31 2017 +0000

    f2fs: add ovp valid_blocks check for bg gc victim to fg_gc
    
    For foreground gc, greedy algorithm should be adapted, which makes
    this formula work well:
    
            (2 * (100 / config.overprovision + 1) + 6)
    
    But currently, we fg_gc have a prior to select bg_gc victim segments to gc
    first, these victims are selected by cost-benefit algorithm, we can't guarantee
    such segments have the small valid blocks, which may destroy the f2fs rule, on
    the worstest case, would consume all the free segments.
    
    This patch fix this by add a filter in check_bg_victims, if segment's has # of
    valid blocks over overprovision ratio, skip such segments.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Hou Pengyang <houpengyang@huawei.com>
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 5cb5755c75d9..f4020f141d83 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -716,6 +716,15 @@ static inline block_t sum_blk_addr(struct f2fs_sb_info *sbi, int base, int type)
 				- (base + 1) + type;
 }
 
+static inline bool no_fggc_candidate(struct f2fs_sb_info *sbi,
+						unsigned int secno)
+{
+	if (get_valid_blocks(sbi, secno, sbi->segs_per_sec) >=
+						sbi->fggc_threshold)
+		return true;
+	return false;
+}
+
 static inline bool sec_usage_check(struct f2fs_sb_info *sbi, unsigned int secno)
 {
 	if (IS_CURSEC(sbi, secno) || (sbi->cur_victim_sec == secno))

commit ae27d62e6befd3cac4ffa702e644cc52019642e8
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Jan 7 18:52:34 2017 +0800

    f2fs: check in-memory sit version bitmap
    
    This patch adds a mirror for sit version bitmap, and use it to detect
    in-memory bitmap corruption which may be caused by bit-transition of
    cache or memory overflow.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 9af95194db06..5cb5755c75d9 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -209,6 +209,9 @@ struct sit_info {
 	block_t sit_blocks;		/* # of blocks used by SIT area */
 	block_t written_valid_blocks;	/* # of valid blocks in main area */
 	char *sit_bitmap;		/* SIT bitmap pointer */
+#ifdef CONFIG_F2FS_CHECK_FS
+	char *sit_bitmap_mir;		/* SIT bitmap mirror */
+#endif
 	unsigned int bitmap_size;	/* SIT bitmap size */
 
 	unsigned long *tmp_map;			/* bitmap for temporal use */
@@ -423,6 +426,12 @@ static inline void get_sit_bitmap(struct f2fs_sb_info *sbi,
 		void *dst_addr)
 {
 	struct sit_info *sit_i = SIT_I(sbi);
+
+#ifdef CONFIG_F2FS_CHECK_FS
+	if (memcmp(sit_i->sit_bitmap, sit_i->sit_bitmap_mir,
+						sit_i->bitmap_size))
+		f2fs_bug_on(sbi, 1);
+#endif
 	memcpy(dst_addr, sit_i->sit_bitmap, sit_i->bitmap_size);
 }
 
@@ -643,6 +652,12 @@ static inline pgoff_t current_sit_addr(struct f2fs_sb_info *sbi,
 
 	check_seg_range(sbi, start);
 
+#ifdef CONFIG_F2FS_CHECK_FS
+	if (f2fs_test_bit(offset, sit_i->sit_bitmap) !=
+			f2fs_test_bit(offset, sit_i->sit_bitmap_mir))
+		f2fs_bug_on(sbi, 1);
+#endif
+
 	/* calculate sit block address */
 	if (f2fs_test_bit(offset, sit_i->sit_bitmap))
 		blk_addr += sit_i->sit_blocks;
@@ -668,6 +683,9 @@ static inline void set_to_next_sit(struct sit_info *sit_i, unsigned int start)
 	unsigned int block_off = SIT_BLOCK_OFFSET(start);
 
 	f2fs_change_bit(block_off, sit_i->sit_bitmap);
+#ifdef CONFIG_F2FS_CHECK_FS
+	f2fs_change_bit(block_off, sit_i->sit_bitmap_mir);
+#endif
 }
 
 static inline unsigned long long get_mtime(struct f2fs_sb_info *sbi)

commit 355e78913c0d57492076d545b6f44b94fec2bf6b
Author: Chao Yu <yuchao0@huawei.com>
Date:   Sat Jan 7 18:51:01 2017 +0800

    f2fs: check in-memory block bitmap
    
    This patch adds a mirror for valid block bitmap, and use it to detect
    in-memory bitmap corruption which may be caused by bit-transition of
    cache or memory overflow.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 08f1455c812c..9af95194db06 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -164,6 +164,9 @@ struct seg_entry {
 	unsigned int ckpt_valid_blocks:10;	/* # of valid blocks last cp */
 	unsigned int padding:6;		/* padding */
 	unsigned char *cur_valid_map;	/* validity bitmap of blocks */
+#ifdef CONFIG_F2FS_CHECK_FS
+	unsigned char *cur_valid_map_mir;	/* mirror of current valid bitmap */
+#endif
 	/*
 	 * # of valid blocks and the validity bitmap stored in the the last
 	 * checkpoint pack. This information is used by the SSR mode.
@@ -320,6 +323,9 @@ static inline void seg_info_from_raw_sit(struct seg_entry *se,
 	se->ckpt_valid_blocks = GET_SIT_VBLOCKS(rs);
 	memcpy(se->cur_valid_map, rs->valid_map, SIT_VBLOCK_MAP_SIZE);
 	memcpy(se->ckpt_valid_map, rs->valid_map, SIT_VBLOCK_MAP_SIZE);
+#ifdef CONFIG_F2FS_CHECK_FS
+	memcpy(se->cur_valid_map_mir, rs->valid_map, SIT_VBLOCK_MAP_SIZE);
+#endif
 	se->type = GET_SIT_TYPE(rs);
 	se->mtime = le64_to_cpu(rs->mtime);
 }

commit 0a595ebaaa6b53a2226d3fee2a2fd616ea5ba378
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Dec 14 10:12:56 2016 -0800

    f2fs: support IO alignment for DATA and NODE writes
    
    This patch implements IO alignment by filling dummy blocks in DATA and NODE
    write bios. If we can guarantee, for example, 32KB or 64KB for such the IOs,
    we can eliminate underlying dummy page problem which FTL conducts in order to
    close MLC or TLC partial written pages.
    
    Note that,
     - it requires "-o mode=lfs".
     - IO size should be power of 2, not exceed BIO_MAX_PAGES, 256.
     - read IO is still 4KB.
     - do checkpoint at fsync, if dummy NODE page was written.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 9d44ce83acb2..08f1455c812c 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -186,9 +186,12 @@ struct segment_allocation {
  * the page is atomically written, and it is in inmem_pages list.
  */
 #define ATOMIC_WRITTEN_PAGE		((unsigned long)-1)
+#define DUMMY_WRITTEN_PAGE		((unsigned long)-2)
 
 #define IS_ATOMIC_WRITTEN_PAGE(page)			\
 		(page_private(page) == (unsigned long)ATOMIC_WRITTEN_PAGE)
+#define IS_DUMMY_WRITTEN_PAGE(page)			\
+		(page_private(page) == (unsigned long)DUMMY_WRITTEN_PAGE)
 
 struct inmem_pages {
 	struct list_head list;

commit 2040fce83fe17763b07c97c1f691da2bb85e4135
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Dec 5 13:56:04 2016 -0800

    f2fs: detect wrong layout
    
    Previous mkfs.f2fs allows small partition inappropriately, so f2fs should detect
    that as well.
    
    Refer this in f2fs-tools.
    
    mkfs.f2fs: detect small partition by overprovision ratio and # of segments
    
    Reported-and-Tested-by: Eric Biggers <ebiggers@google.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 89ab4301ef02..9d44ce83acb2 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -18,6 +18,8 @@
 #define DEF_RECLAIM_PREFREE_SEGMENTS	5	/* 5% over total segments */
 #define DEF_MAX_RECLAIM_PREFREE_SEGMENTS	4096	/* 8GB in maximum */
 
+#define F2FS_MIN_SEGMENTS	9 /* SB + 2 (CP + SIT + NAT) + SSA + MAIN */
+
 /* L: Logical segment # in volume, R: Relative segment # in main area */
 #define GET_L2R_SEGNO(free_i, segno)	(segno - free_i->start_segno)
 #define GET_R2L_SEGNO(free_i, segno)	(segno + free_i->start_segno)

commit 664ba972df9b96942191db3068274cc1db899774
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Oct 18 11:07:45 2016 -0700

    f2fs: use BIO_MAX_PAGES for bio allocation
    
    We don't need to allocate bio partially in order to maximize sequential writes.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 762743988426..89ab4301ef02 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -102,8 +102,6 @@
 	(((sector_t)blk_addr) << F2FS_LOG_SECTORS_PER_BLOCK)
 #define SECTOR_TO_BLOCK(sectors)					\
 	(sectors >> F2FS_LOG_SECTORS_PER_BLOCK)
-#define MAX_BIO_BLOCKS(sbi)						\
-	((int)min((int)max_hw_blocks(sbi), BIO_MAX_PAGES))
 
 /*
  * indicate a block allocation direction: RIGHT and LEFT.
@@ -696,13 +694,6 @@ static inline bool sec_usage_check(struct f2fs_sb_info *sbi, unsigned int secno)
 	return false;
 }
 
-static inline unsigned int max_hw_blocks(struct f2fs_sb_info *sbi)
-{
-	struct block_device *bdev = sbi->sb->s_bdev;
-	struct request_queue *q = bdev_get_queue(bdev);
-	return SECTOR_TO_BLOCK(queue_max_sectors(q));
-}
-
 /*
  * It is very important to gather dirty pages and write at once, so that we can
  * submit a big bio without interfering other data writes.
@@ -720,7 +711,7 @@ static inline int nr_pages_to_skip(struct f2fs_sb_info *sbi, int type)
 	else if (type == NODE)
 		return 8 * sbi->blocks_per_seg;
 	else if (type == META)
-		return 8 * MAX_BIO_BLOCKS(sbi);
+		return 8 * BIO_MAX_PAGES;
 	else
 		return 0;
 }
@@ -737,11 +728,9 @@ static inline long nr_pages_to_write(struct f2fs_sb_info *sbi, int type,
 		return 0;
 
 	nr_to_write = wbc->nr_to_write;
-
+	desired = BIO_MAX_PAGES;
 	if (type == NODE)
-		desired = 2 * max_hw_blocks(sbi);
-	else
-		desired = MAX_BIO_BLOCKS(sbi);
+		desired <<= 1;
 
 	wbc->nr_to_write = desired;
 	return desired - nr_to_write;

commit b9610bdfcbdbb6017802ec6d1e073f445c98157d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Oct 14 13:28:05 2016 -0700

    f2fs: count dirty inodes to flush node pages during checkpoint
    
    If there are a lot of dirty inodes, we need to flush all of them when doing
    checkpoint. So, we need to count this for enough free space.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index fecb856ad874..762743988426 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -471,11 +471,12 @@ static inline bool need_SSR(struct f2fs_sb_info *sbi)
 {
 	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
 	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
+	int imeta_secs = get_blocktype_secs(sbi, F2FS_DIRTY_IMETA);
 
 	if (test_opt(sbi, LFS))
 		return false;
 
-	return free_sections(sbi) <= (node_secs + 2 * dent_secs +
+	return free_sections(sbi) <= (node_secs + 2 * dent_secs + imeta_secs +
 						reserved_sections(sbi) + 1);
 }
 
@@ -484,14 +485,14 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi,
 {
 	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
 	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
-
-	node_secs += get_blocktype_secs(sbi, F2FS_DIRTY_IMETA);
+	int imeta_secs = get_blocktype_secs(sbi, F2FS_DIRTY_IMETA);
 
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		return false;
 
 	return (free_sections(sbi) + freed) <=
-		(node_secs + 2 * dent_secs + reserved_sections(sbi) + needed);
+		(node_secs + 2 * dent_secs + imeta_secs +
+		reserved_sections(sbi) + needed);
 }
 
 static inline bool excess_prefree_segs(struct f2fs_sb_info *sbi)

commit 7f3037a5ec0672e03f96d4b0b86169c4c48e479e
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Sep 1 12:02:51 2016 -0700

    f2fs: check free_sections for defragmentation
    
    Fix wrong condition check for defragmentation of a file.
    
    Reviewed-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 87156c739796..fecb856ad874 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -479,7 +479,8 @@ static inline bool need_SSR(struct f2fs_sb_info *sbi)
 						reserved_sections(sbi) + 1);
 }
 
-static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
+static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi,
+					int freed, int needed)
 {
 	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
 	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
@@ -489,8 +490,8 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		return false;
 
-	return (free_sections(sbi) + freed) <= (node_secs + 2 * dent_secs +
-						reserved_sections(sbi));
+	return (free_sections(sbi) + freed) <=
+		(node_secs + 2 * dent_secs + reserved_sections(sbi) + needed);
 }
 
 static inline bool excess_prefree_segs(struct f2fs_sb_info *sbi)

commit bb413d6acd4e1c361daebf8486efc3923f429792
Author: Yunlei He <heyunlei@huawei.com>
Date:   Thu Jul 28 12:12:38 2016 +0800

    f2fs: not allow to write illegal blkaddr
    
    we came across an error as below:
    
    [build_nat_area_bitmap:1710] nid[0x    1718] addr[0x         1c18ddc] ino[0x    1718]
    [build_nat_area_bitmap:1710] nid[0x    1719] addr[0x         1c193d5] ino[0x    1719]
    [build_nat_area_bitmap:1710] nid[0x    171a] addr[0x         1c1736e] ino[0x    171a]
    [build_nat_area_bitmap:1710] nid[0x    171b] addr[0x        58b3ee8f] ino[0x815f92ed]
    [build_nat_area_bitmap:1710] nid[0x    171c] addr[0x         fcdc94b] ino[0x49366377]
    [build_nat_area_bitmap:1710] nid[0x    171d] addr[0x        7cd2facf] ino[0xb3c55300]
    [build_nat_area_bitmap:1710] nid[0x    171e] addr[0x        bd4e25d0] ino[0x77c34c09]
    
    ... ...
    
    [build_nat_area_bitmap:1710] nid[0x    1718] addr[0x         1c18ddc] ino[0x    1718]
    [build_nat_area_bitmap:1710] nid[0x    1719] addr[0x         1c193d5] ino[0x    1719]
    [build_nat_area_bitmap:1710] nid[0x    171a] addr[0x         1c1736e] ino[0x    171a]
    [build_nat_area_bitmap:1710] nid[0x    171b] addr[0x        58b3ee8f] ino[0x815f92ed]
    [build_nat_area_bitmap:1710] nid[0x    171c] addr[0x         fcdc94b] ino[0x49366377]
    [build_nat_area_bitmap:1710] nid[0x    171d] addr[0x        7cd2facf] ino[0xb3c55300]
    [build_nat_area_bitmap:1710] nid[0x    171e] addr[0x        bd4e25d0] ino[0x77c34c09]
    
    One nat block may be stepped by a data block, so this patch forbid to
    write if the blkaddr is illegal
    
    Signed-off-by: Yunlei He <heyunlei@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index b33f73ec60a4..87156c739796 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -587,8 +587,8 @@ static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
 
 static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 {
-	f2fs_bug_on(sbi, blk_addr < SEG0_BLKADDR(sbi)
-					|| blk_addr >= MAX_BLKADDR(sbi));
+	BUG_ON(blk_addr < SEG0_BLKADDR(sbi)
+			|| blk_addr >= MAX_BLKADDR(sbi));
 }
 
 /*

commit 44a83499dda714d9262a9bf4fdac8c077893c9e6
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Jul 13 18:23:35 2016 -0700

    f2fs: add maximum prefree segments
    
    In 1TB storage, we need to admit 22841 prefree segments, which can consume
    too much segments.
    This patch sets 8GB in max. prefree segments in that case.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 57d450fb4643..b33f73ec60a4 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -16,6 +16,7 @@
 #define NULL_SECNO			((unsigned int)(~0))
 
 #define DEF_RECLAIM_PREFREE_SEGMENTS	5	/* 5% over total segments */
+#define DEF_MAX_RECLAIM_PREFREE_SEGMENTS	4096	/* 8GB in maximum */
 
 /* L: Logical segment # in volume, R: Relative segment # in main area */
 #define GET_L2R_SEGNO(free_i, segno)	(segno - free_i->start_segno)

commit 2c237ebaa440b8c641c61cf66802521a917fc30c
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jun 16 16:44:11 2016 -0700

    f2fs: avoid writing node/metapages during writes
    
    Let's keep more node/meta pages in run time.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index d74cc330ae13..57d450fb4643 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -715,9 +715,9 @@ static inline int nr_pages_to_skip(struct f2fs_sb_info *sbi, int type)
 	if (type == DATA)
 		return sbi->blocks_per_seg;
 	else if (type == NODE)
-		return 3 * sbi->blocks_per_seg;
+		return 8 * sbi->blocks_per_seg;
 	else if (type == META)
-		return MAX_BIO_BLOCKS(sbi);
+		return 8 * MAX_BIO_BLOCKS(sbi);
 	else
 		return 0;
 }
@@ -736,7 +736,7 @@ static inline long nr_pages_to_write(struct f2fs_sb_info *sbi, int type,
 	nr_to_write = wbc->nr_to_write;
 
 	if (type == NODE)
-		desired = 3 * max_hw_blocks(sbi);
+		desired = 2 * max_hw_blocks(sbi);
 	else
 		desired = MAX_BIO_BLOCKS(sbi);
 

commit 36abef4e796d382e81a0c2d21ea5327481dd7154
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jun 3 19:29:38 2016 -0700

    f2fs: introduce mode=lfs mount option
    
    This mount option is to enable original log-structured filesystem forcefully.
    So, there should be no random writes for main area.
    
    Especially, this supports host-managed SMR device.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 890bb28d2082..d74cc330ae13 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -470,6 +470,10 @@ static inline bool need_SSR(struct f2fs_sb_info *sbi)
 {
 	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
 	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
+
+	if (test_opt(sbi, LFS))
+		return false;
+
 	return free_sections(sbi) <= (node_secs + 2 * dent_secs +
 						reserved_sections(sbi) + 1);
 }
@@ -533,6 +537,9 @@ static inline bool need_inplace_update(struct inode *inode)
 	if (S_ISDIR(inode->i_mode) || f2fs_is_atomic_file(inode))
 		return false;
 
+	if (test_opt(sbi, LFS))
+		return false;
+
 	if (policy & (0x1 << F2FS_IPU_FORCE))
 		return true;
 	if (policy & (0x1 << F2FS_IPU_SSR) && need_SSR(sbi))

commit 28ea6162e29ba0db87a512dda2bb6d6e63a6006f
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed May 25 17:17:56 2016 -0700

    f2fs: do not skip writing data pages
    
    For data pages, let's try to flush as much as possible in background.
    
    On /dev/pmem0,
    
    1. dd if=/dev/zero of=/mnt/test/testfile bs=1M count=2048 conv=fsync
     Before : 800 MB/s
     After  : 1.1 GB/s
    
    2. dd if=/dev/zero of=/mnt/test/testfile bs=1M count=2048
     Before : 1.3 GB/s
     After  : 2.2 GB/s
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 5d016a1edf21..890bb28d2082 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -728,9 +728,7 @@ static inline long nr_pages_to_write(struct f2fs_sb_info *sbi, int type,
 
 	nr_to_write = wbc->nr_to_write;
 
-	if (type == DATA)
-		desired = 4096;
-	else if (type == NODE)
+	if (type == NODE)
 		desired = 3 * max_hw_blocks(sbi);
 	else
 		desired = MAX_BIO_BLOCKS(sbi);

commit 0f18b462b2e5aff64b8638e8a47284b907351ef3
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 20 11:10:10 2016 -0700

    f2fs: flush inode metadata when checkpoint is doing
    
    This patch registers all the inodes which have dirty metadata to sync when
    checkpoint is doing.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index fcdd7310b961..5d016a1edf21 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -479,6 +479,8 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
 	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
 	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
 
+	node_secs += get_blocktype_secs(sbi, F2FS_DIRTY_IMETA);
+
 	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		return false;
 

commit 91942321e4c9f8460f260cdfcf0a7a48a73a84a4
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri May 20 10:13:22 2016 -0700

    f2fs: use inode pointer for {set, clear}_inode_flag
    
    This patch refactors to use inode pointer for set_inode_flag and
    clear_inode_flag.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 7a756ff5a36d..fcdd7310b961 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -544,7 +544,7 @@ static inline bool need_inplace_update(struct inode *inode)
 
 	/* this is only set during fdatasync */
 	if (policy & (0x1 << F2FS_IPU_FSYNC) &&
-			is_inode_flag_set(F2FS_I(inode), FI_NEED_IPU))
+			is_inode_flag_set(inode, FI_NEED_IPU))
 		return true;
 
 	return false;

commit f51b4ce6c1759a64d68d314545854dc226ab69a0
Author: Chao Yu <yuchao0@huawei.com>
Date:   Wed May 4 23:19:48 2016 +0800

    f2fs: shrink size of struct seg_entry
    
    Restructure struct seg_entry to eliminate holes in it, after that,
    in 32-bits machine, it reduces size from 32 bytes to 24 bytes; in
    64-bits machine, it reduces size from 56 bytes to 40 bytes.
    
    Signed-off-by: Chao Yu <yuchao0@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 975c33df65c7..7a756ff5a36d 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -158,16 +158,17 @@ struct victim_sel_policy {
 };
 
 struct seg_entry {
-	unsigned short valid_blocks;	/* # of valid blocks */
+	unsigned int type:6;		/* segment type like CURSEG_XXX_TYPE */
+	unsigned int valid_blocks:10;	/* # of valid blocks */
+	unsigned int ckpt_valid_blocks:10;	/* # of valid blocks last cp */
+	unsigned int padding:6;		/* padding */
 	unsigned char *cur_valid_map;	/* validity bitmap of blocks */
 	/*
 	 * # of valid blocks and the validity bitmap stored in the the last
 	 * checkpoint pack. This information is used by the SSR mode.
 	 */
-	unsigned short ckpt_valid_blocks;
-	unsigned char *ckpt_valid_map;
+	unsigned char *ckpt_valid_map;	/* validity bitmap of blocks last cp */
 	unsigned char *discard_map;
-	unsigned char type;		/* segment type like CURSEG_XXX_TYPE */
 	unsigned long long mtime;	/* modification time of the segment */
 };
 

commit b7ad7512b84b26f1c0ec823647a387627c138d32
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Feb 19 18:08:46 2016 +0800

    f2fs: split journal cache from curseg cache
    
    In curseg cache, f2fs caches two different parts:
     - datas of current summay block, i.e. summary entries, footer info.
     - journal info, i.e. sparse nat/sit entries or io stat info.
    
    With this approach, 1) it may cause higher lock contention when we access
    or update both of the parts of cache since we use the same mutex lock
    curseg_mutex to protect the cache. 2) current summary block with last
    journal info will be writebacked into device as a normal summary block
    when flushing, however, we treat journal info as valid one only in current
    summary, so most normal summary blocks contain junk journal data, it wastes
    remaining space of summary block.
    
    So, in order to fix above issues, we split curseg cache into two parts:
    a) current summary block, protected by original mutex lock curseg_mutex
    b) journal cache, protected by newly introduced r/w semaphore journal_rwsem
    
    When loading curseg cache during ->mount, we store summary info and
    journal info into different caches; When doing checkpoint, we combine
    datas of two cache into current summary block for persisting.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 2f1a4220f14b..975c33df65c7 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -258,6 +258,8 @@ struct victim_selection {
 struct curseg_info {
 	struct mutex curseg_mutex;		/* lock for consistency */
 	struct f2fs_summary_block *sum_blk;	/* cached summary block */
+	struct rw_semaphore journal_rwsem;	/* protect journal area */
+	struct f2fs_journal *journal;		/* cached journal info */
 	unsigned char alloc_type;		/* current allocation type */
 	unsigned int segno;			/* current segment number */
 	unsigned short next_blkoff;		/* next block offset to write */

commit 28bc106b2346a7348706bf86d9efbe31920c69f3
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sat Feb 6 14:40:34 2016 +0800

    f2fs: support revoking atomic written pages
    
    f2fs support atomic write with following semantics:
    1. open db file
    2. ioctl start atomic write
    3. (write db file) * n
    4. ioctl commit atomic write
    5. close db file
    
    With this flow we can avoid file becoming corrupted when abnormal power
    cut, because we hold data of transaction in referenced pages linked in
    inmem_pages list of inode, but without setting them dirty, so these data
    won't be persisted unless we commit them in step 4.
    
    But we should still hold journal db file in memory by using volatile
    write, because our semantics of 'atomic write support' is incomplete, in
    step 4, we could fail to submit all dirty data of transaction, once
    partial dirty data was committed in storage, then after a checkpoint &
    abnormal power-cut, db file will be corrupted forever.
    
    So this patch tries to improve atomic write flow by adding a revoking flow,
    once inner error occurs in committing, this gives another chance to try to
    revoke these partial submitted data of current transaction, it makes
    committing operation more like aotmical one.
    
    If we're not lucky, once revoking operation was failed, EAGAIN will be
    reported to user for suggesting doing the recovery with held journal file,
    or retrying current transaction again.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index cd7111b9a664..2f1a4220f14b 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -191,6 +191,7 @@ struct segment_allocation {
 struct inmem_pages {
 	struct list_head list;
 	struct page *page;
+	block_t old_addr;		/* for revoking when fail to commit */
 };
 
 struct sit_info {

commit d48dfc2073e31f76262204a4c8dc1c7da3834b7c
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Jan 29 16:21:15 2016 -0800

    f2fs: fix conflict on page->private usage
    
    This patch fixes confilct on page->private value between f2fs_trace_pid and
    atomic page.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index ee44d346ea44..cd7111b9a664 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -183,7 +183,7 @@ struct segment_allocation {
  * this value is set in page as a private data which indicate that
  * the page is atomically written, and it is in inmem_pages list.
  */
-#define ATOMIC_WRITTEN_PAGE		0x0000ffff
+#define ATOMIC_WRITTEN_PAGE		((unsigned long)-1)
 
 #define IS_ATOMIC_WRITTEN_PAGE(page)			\
 		(page_private(page) == (unsigned long)ATOMIC_WRITTEN_PAGE)

commit a125702326d9c3b753fe9c9b9727d3b3dd1cba4a
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Oct 8 10:40:07 2015 -0700

    Revert "f2fs: do not skip dentry block writes"
    
    The periodic checkpoint can resolve the previous issue.
    So, now we can use this again to improve the reported performance regression:
    
    https://lkml.org/lkml/2015/10/8/20
    
    This reverts commit 15bec0ff5a9ba6d203178fa8772259df6207942a.

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index e9afb5884312..ee44d346ea44 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -699,7 +699,9 @@ static inline int nr_pages_to_skip(struct f2fs_sb_info *sbi, int type)
 	if (sbi->sb->s_bdi->wb.dirty_exceeded)
 		return 0;
 
-	if (type == NODE)
+	if (type == DATA)
+		return sbi->blocks_per_seg;
+	else if (type == NODE)
 		return 3 * sbi->blocks_per_seg;
 	else if (type == META)
 		return MAX_BIO_BLOCKS(sbi);

commit 6aefd93b01379bf0b62f8b38dcf7a21397893833
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Oct 5 11:02:54 2015 -0700

    f2fs: introduce background_gc=sync mount option
    
    This patch introduce background_gc=sync enabling synchronous cleaning in
    background.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index a294da70a7b5..e9afb5884312 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -137,10 +137,12 @@ enum {
 /*
  * BG_GC means the background cleaning job.
  * FG_GC means the on-demand cleaning job.
+ * FORCE_FG_GC means on-demand cleaning job in background.
  */
 enum {
 	BG_GC = 0,
-	FG_GC
+	FG_GC,
+	FORCE_FG_GC,
 };
 
 /* for a function parameter to select a victim segment */

commit 90b803e6fb6243922bff9ddd8a6205c17cb93b31
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Fri Sep 25 19:34:50 2015 -0700

    f2fs: do not skip dentry block writes
    
    Previously, we skip dentry block writes when wbc is SYNC_NONE with no memory
    pressure and the number of dirty pages is pretty small.
    
    But, we didn't skip for normal data writes, which gives us not much big impact
    on overall performance.
    Moreover, by skipping some data writes, kworker falls into infinite loop to try
    to write blocks, when many dir inodes have only one dentry block.
    
    So, this patch removes skipping data writes.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index b6e4ed15c698..a294da70a7b5 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -697,9 +697,7 @@ static inline int nr_pages_to_skip(struct f2fs_sb_info *sbi, int type)
 	if (sbi->sb->s_bdi->wb.dirty_exceeded)
 		return 0;
 
-	if (type == DATA)
-		return sbi->blocks_per_seg;
-	else if (type == NODE)
+	if (type == NODE)
 		return 3 * sbi->blocks_per_seg;
 	else if (type == META)
 		return MAX_BIO_BLOCKS(sbi);

commit 4c278394b0feb7aadc538be12ab0474b106a7255
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Aug 11 16:01:30 2015 -0700

    f2fs: avoid a build warning
    
    If F2FS_CHECK_FS is turned off, we can get a build warning for unused variable.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index d0bd952b7065..b6e4ed15c698 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -581,15 +581,11 @@ static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 static inline void check_block_count(struct f2fs_sb_info *sbi,
 		int segno, struct f2fs_sit_entry *raw_sit)
 {
+#ifdef CONFIG_F2FS_CHECK_FS
 	bool is_valid  = test_bit_le(0, raw_sit->valid_map) ? true : false;
 	int valid_blocks = 0;
 	int cur_pos = 0, next_pos;
 
-	/* check segment usage, and check boundary of a given segment number */
-	f2fs_bug_on(sbi, GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg
-					|| segno > TOTAL_SEGS(sbi) - 1);
-
-#ifdef CONFIG_F2FS_CHECK_FS
 	/* check bitmap with valid block count */
 	do {
 		if (is_valid) {
@@ -606,6 +602,9 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 	} while (cur_pos < sbi->blocks_per_seg);
 	BUG_ON(GET_SIT_VBLOCKS(raw_sit) != valid_blocks);
 #endif
+	/* check segment usage, and check boundary of a given segment number */
+	f2fs_bug_on(sbi, GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg
+					|| segno > TOTAL_SEGS(sbi) - 1);
 }
 
 static inline pgoff_t current_sit_addr(struct f2fs_sb_info *sbi,

commit decd36b6c43a1051bab97571cf4c0ec8450268b0
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Aug 7 18:42:09 2015 +0800

    f2fs: remove inmem radix tree
    
    Previously, we use radix tree to index all registered page entries for
    atomic file, but now we only use radix tree to see whether current page
    is indexed or not, since the other user of radix tree is gone in commit
    042b7816aaeb ("f2fs: remove unnecessary call to invalidate inmemory pages").
    
    So in this patch, we try to use one more efficient way:
    Introducing a macro ATOMIC_WRITTEN_PAGE, and setting it as page private
    value to indicate page indexing status. By using this way, we can save
    memory and lookup time.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 230f9cd9fa2a..d0bd952b7065 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -177,6 +177,15 @@ struct segment_allocation {
 	void (*allocate_segment)(struct f2fs_sb_info *, int, bool);
 };
 
+/*
+ * this value is set in page as a private data which indicate that
+ * the page is atomically written, and it is in inmem_pages list.
+ */
+#define ATOMIC_WRITTEN_PAGE		0x0000ffff
+
+#define IS_ATOMIC_WRITTEN_PAGE(page)			\
+		(page_private(page) == (unsigned long)ATOMIC_WRITTEN_PAGE)
+
 struct inmem_pages {
 	struct list_head list;
 	struct page *page;

commit 7a04f64d4d5367ade827d75388d66054b535e201
Author: Liu Xue <liuxueliu.liu@huawei.com>
Date:   Mon Jul 27 10:17:59 2015 +0000

    f2fs: unify f2fs_bug_on when check blocks and segment
    
    Replace BUG_ON with f2fs_bug_on to deal with
    block and segment validity check failed.
    
    Signed-off-by: Xue Liu <liuxueliu.liu@huawei.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 79e7b879a753..230f9cd9fa2a 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -555,16 +555,15 @@ static inline unsigned short curseg_blkoff(struct f2fs_sb_info *sbi, int type)
 	return curseg->next_blkoff;
 }
 
-#ifdef CONFIG_F2FS_CHECK_FS
 static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
 {
-	BUG_ON(segno > TOTAL_SEGS(sbi) - 1);
+	f2fs_bug_on(sbi, segno > TOTAL_SEGS(sbi) - 1);
 }
 
 static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 {
-	BUG_ON(blk_addr < SEG0_BLKADDR(sbi));
-	BUG_ON(blk_addr >= MAX_BLKADDR(sbi));
+	f2fs_bug_on(sbi, blk_addr < SEG0_BLKADDR(sbi)
+					|| blk_addr >= MAX_BLKADDR(sbi));
 }
 
 /*
@@ -577,12 +576,11 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 	int valid_blocks = 0;
 	int cur_pos = 0, next_pos;
 
-	/* check segment usage */
-	BUG_ON(GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg);
-
-	/* check boundary of a given segment number */
-	BUG_ON(segno > TOTAL_SEGS(sbi) - 1);
+	/* check segment usage, and check boundary of a given segment number */
+	f2fs_bug_on(sbi, GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg
+					|| segno > TOTAL_SEGS(sbi) - 1);
 
+#ifdef CONFIG_F2FS_CHECK_FS
 	/* check bitmap with valid block count */
 	do {
 		if (is_valid) {
@@ -598,35 +596,8 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 		is_valid = !is_valid;
 	} while (cur_pos < sbi->blocks_per_seg);
 	BUG_ON(GET_SIT_VBLOCKS(raw_sit) != valid_blocks);
-}
-#else
-static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
-{
-	if (segno > TOTAL_SEGS(sbi) - 1)
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-}
-
-static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
-{
-	if (blk_addr < SEG0_BLKADDR(sbi) || blk_addr >= MAX_BLKADDR(sbi))
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-}
-
-/*
- * Summary block is always treated as an invalid block
- */
-static inline void check_block_count(struct f2fs_sb_info *sbi,
-		int segno, struct f2fs_sit_entry *raw_sit)
-{
-	/* check segment usage */
-	if (GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg)
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-
-	/* check boundary of a given segment number */
-	if (segno > TOTAL_SEGS(sbi) - 1)
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-}
 #endif
+}
 
 static inline pgoff_t current_sit_addr(struct f2fs_sb_info *sbi,
 						unsigned int start)

commit e4bc13adfd016fc1036838170288b5680d1a98b0
Merge: ad90fb97515b 3e1534cf4a2a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 25 16:00:17 2015 -0700

    Merge branch 'for-4.2/writeback' of git://git.kernel.dk/linux-block
    
    Pull cgroup writeback support from Jens Axboe:
     "This is the big pull request for adding cgroup writeback support.
    
      This code has been in development for a long time, and it has been
      simmering in for-next for a good chunk of this cycle too.  This is one
      of those problems that has been talked about for at least half a
      decade, finally there's a solution and code to go with it.
    
      Also see last weeks writeup on LWN:
    
            http://lwn.net/Articles/648292/"
    
    * 'for-4.2/writeback' of git://git.kernel.dk/linux-block: (85 commits)
      writeback, blkio: add documentation for cgroup writeback support
      vfs, writeback: replace FS_CGROUP_WRITEBACK with SB_I_CGROUPWB
      writeback: do foreign inode detection iff cgroup writeback is enabled
      v9fs: fix error handling in v9fs_session_init()
      bdi: fix wrong error return value in cgwb_create()
      buffer: remove unusued 'ret' variable
      writeback: disassociate inodes from dying bdi_writebacks
      writeback: implement foreign cgroup inode bdi_writeback switching
      writeback: add lockdep annotation to inode_to_wb()
      writeback: use unlocked_inode_to_wb transaction in inode_congested()
      writeback: implement unlocked_inode_to_wb transaction and use it for stat updates
      writeback: implement [locked_]inode_to_wb_and_lock_list()
      writeback: implement foreign cgroup inode detection
      writeback: make writeback_control track the inode being written back
      writeback: relocate wb[_try]_get(), wb_put(), inode_{attach|detach}_wb()
      mm: vmscan: disable memcg direct reclaim stalling if cgroup writeback support is in use
      writeback: implement memcg writeback domain based throttling
      writeback: reset wb_domain->dirty_limit[_tstmp] when memcg domain size changes
      writeback: implement memcg wb_domain
      writeback: update wb_over_bg_thresh() to use wb_domain aware operations
      ...

commit 66114cad64bf76a155fec1f0fff0de771cf909d5
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:32 2015 -0400

    writeback: separate out include/linux/backing-dev-defs.h
    
    With the planned cgroup writeback support, backing-dev related
    declarations will be more widely used across block and cgroup;
    unfortunately, including backing-dev.h from include/linux/blkdev.h
    makes cyclic include dependency quite likely.
    
    This patch separates out backing-dev-defs.h which only has the
    essential definitions and updates blkdev.h to include it.  c files
    which need access to more backing-dev details now include
    backing-dev.h directly.  This takes backing-dev.h off the common
    include dependency chain making it a lot easier to use it across block
    and cgroup.
    
    v2: fs/fat build failure fixed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 6408989857e0..aba72f7a8ac4 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -9,6 +9,7 @@
  * published by the Free Software Foundation.
  */
 #include <linux/blkdev.h>
+#include <linux/backing-dev.h>
 
 /* constant macro */
 #define NULL_SEGNO			((unsigned int)(~0))

commit a88a341a73be4ef035ca26170c849f002797da27
Author: Tejun Heo <tj@kernel.org>
Date:   Fri May 22 17:13:28 2015 -0400

    writeback: move bandwidth related fields from backing_dev_info into bdi_writeback
    
    Currently, a bdi (backing_dev_info) embeds single wb (bdi_writeback)
    and the role of the separation is unclear.  For cgroup support for
    writeback IOs, a bdi will be updated to host multiple wb's where each
    wb serves writeback IOs of a different cgroup on the bdi.  To achieve
    that, a wb should carry all states necessary for servicing writeback
    IOs for a cgroup independently.
    
    This patch moves bandwidth related fields from backing_dev_info into
    bdi_writeback.
    
    * The moved fields are: bw_time_stamp, dirtied_stamp, written_stamp,
      write_bandwidth, avg_write_bandwidth, dirty_ratelimit,
      balanced_dirty_ratelimit, completions and dirty_exceeded.
    
    * writeback_chunk_size() and over_bground_thresh() now take @wb
      instead of @bdi.
    
    * bdi_writeout_fraction(bdi, ...)       -> wb_writeout_fraction(wb, ...)
      bdi_dirty_limit(bdi, ...)             -> wb_dirty_limit(wb, ...)
      bdi_position_ration(bdi, ...)         -> wb_position_ratio(wb, ...)
      bdi_update_writebandwidth(bdi, ...)   -> wb_update_write_bandwidth(wb, ...)
      [__]bdi_update_bandwidth(bdi, ...)    -> [__]wb_update_bandwidth(wb, ...)
      bdi_{max|min}_pause(bdi, ...)         -> wb_{max|min}_pause(wb, ...)
      bdi_dirty_limits(bdi, ...)            -> wb_dirty_limits(wb, ...)
    
    * Init/exits of the relocated fields are moved to bdi_wb_init/exit()
      respectively.  Note that explicit zeroing is dropped in the process
      as wb's are cleared in entirety anyway.
    
    * As there's still only one bdi_writeback per backing_dev_info, all
      uses of bdi->stat[] are mechanically replaced with bdi->wb.stat[]
      introducing no behavior changes.
    
    v2: Typo in description fixed as suggested by Jan.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Jaegeuk Kim <jaegeuk@kernel.org>
    Cc: Steven Whitehouse <swhiteho@redhat.com>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 85d7fa7514b2..6408989857e0 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -713,7 +713,7 @@ static inline unsigned int max_hw_blocks(struct f2fs_sb_info *sbi)
  */
 static inline int nr_pages_to_skip(struct f2fs_sb_info *sbi, int type)
 {
-	if (sbi->sb->s_bdi->dirty_exceeded)
+	if (sbi->sb->s_bdi->wb.dirty_exceeded)
 		return 0;
 
 	if (type == DATA)

commit a66cdd985532e69999b79249cd3b4a6bccd0f84b
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Apr 30 22:37:50 2015 -0700

    f2fs: introduce discard_map for f2fs_trim_fs
    
    This patch adds a bitmap for discard issues from f2fs_trim_fs.
    There-in rule is to issue discard commands only for invalidated blocks
    after mount.
    Once mount is done, f2fs_trim_fs trims out whole invalid area.
    After ehn, it will not issue and discrads redundantly.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 85d7fa7514b2..849635778118 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -163,6 +163,7 @@ struct seg_entry {
 	 */
 	unsigned short ckpt_valid_blocks;
 	unsigned char *ckpt_valid_map;
+	unsigned char *discard_map;
 	unsigned char type;		/* segment type like CURSEG_XXX_TYPE */
 	unsigned long long mtime;	/* modification time of the segment */
 };

commit 7fd97019b8db0668a4eb8b08d707bdce9f51d58e
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Fri Mar 6 15:00:55 2015 +0800

    f2fs: reduce searching region of segmap when set free section
    
    In __set_free we will check whether all segment are free in one section
    when free one segment, in order to set section to free status. But the
    searching region of segmap is from start segno to last segno of main
    area, it's not necessary. So let's just only check all segment bitmap
    of target section.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 7fd35111cf62..85d7fa7514b2 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -336,7 +336,8 @@ static inline void __set_free(struct f2fs_sb_info *sbi, unsigned int segno)
 	clear_bit(segno, free_i->free_segmap);
 	free_i->free_segments++;
 
-	next = find_next_bit(free_i->free_segmap, MAIN_SEGS(sbi), start_segno);
+	next = find_next_bit(free_i->free_segmap,
+			start_segno + sbi->segs_per_sec, start_segno);
 	if (next >= start_segno + sbi->segs_per_sec) {
 		clear_bit(secno, free_i->free_secmap);
 		free_i->free_sections++;

commit 1a118ccfd60fc78e64c0a3ab9e85075545839d6e
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Feb 11 18:20:38 2015 +0800

    f2fs: use spinlock for segmap_lock instead of rwlock
    
    rwlock can provide better concurrency when there are much more readers than
    writers because readers can hold the rwlock simultaneously.
    
    But now, for segmap_lock rwlock in struct free_segmap_info, there is only one
    reader 'mount' from below call path:
    ->f2fs_fill_super
      ->build_segment_manager
        ->build_dirty_segmap
          ->init_dirty_segmap
            ->find_next_inuse
              read_lock
              ...
              read_unlock
    
    Now that our concurrency can not be improved since there is no other reader for
    this lock, we do not need to use rwlock_t type for segmap_lock, let's replace it
    with spinlock_t type.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index ba858ca01445..7fd35111cf62 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -208,7 +208,7 @@ struct free_segmap_info {
 	unsigned int start_segno;	/* start segment number logically */
 	unsigned int free_segments;	/* # of free segments */
 	unsigned int free_sections;	/* # of free sections */
-	rwlock_t segmap_lock;		/* free segmap lock */
+	spinlock_t segmap_lock;		/* free segmap lock */
 	unsigned long *free_segmap;	/* free segment bitmap */
 	unsigned long *free_secmap;	/* free section bitmap */
 };
@@ -319,9 +319,9 @@ static inline unsigned int find_next_inuse(struct free_segmap_info *free_i,
 		unsigned int max, unsigned int segno)
 {
 	unsigned int ret;
-	read_lock(&free_i->segmap_lock);
+	spin_lock(&free_i->segmap_lock);
 	ret = find_next_bit(free_i->free_segmap, max, segno);
-	read_unlock(&free_i->segmap_lock);
+	spin_unlock(&free_i->segmap_lock);
 	return ret;
 }
 
@@ -332,7 +332,7 @@ static inline void __set_free(struct f2fs_sb_info *sbi, unsigned int segno)
 	unsigned int start_segno = secno * sbi->segs_per_sec;
 	unsigned int next;
 
-	write_lock(&free_i->segmap_lock);
+	spin_lock(&free_i->segmap_lock);
 	clear_bit(segno, free_i->free_segmap);
 	free_i->free_segments++;
 
@@ -341,7 +341,7 @@ static inline void __set_free(struct f2fs_sb_info *sbi, unsigned int segno)
 		clear_bit(secno, free_i->free_secmap);
 		free_i->free_sections++;
 	}
-	write_unlock(&free_i->segmap_lock);
+	spin_unlock(&free_i->segmap_lock);
 }
 
 static inline void __set_inuse(struct f2fs_sb_info *sbi,
@@ -363,7 +363,7 @@ static inline void __set_test_and_free(struct f2fs_sb_info *sbi,
 	unsigned int start_segno = secno * sbi->segs_per_sec;
 	unsigned int next;
 
-	write_lock(&free_i->segmap_lock);
+	spin_lock(&free_i->segmap_lock);
 	if (test_and_clear_bit(segno, free_i->free_segmap)) {
 		free_i->free_segments++;
 
@@ -374,7 +374,7 @@ static inline void __set_test_and_free(struct f2fs_sb_info *sbi,
 				free_i->free_sections++;
 		}
 	}
-	write_unlock(&free_i->segmap_lock);
+	spin_unlock(&free_i->segmap_lock);
 }
 
 static inline void __set_test_and_inuse(struct f2fs_sb_info *sbi,
@@ -382,13 +382,13 @@ static inline void __set_test_and_inuse(struct f2fs_sb_info *sbi,
 {
 	struct free_segmap_info *free_i = FREE_I(sbi);
 	unsigned int secno = segno / sbi->segs_per_sec;
-	write_lock(&free_i->segmap_lock);
+	spin_lock(&free_i->segmap_lock);
 	if (!test_and_set_bit(segno, free_i->free_segmap)) {
 		free_i->free_segments--;
 		if (!test_and_set_bit(secno, free_i->free_secmap))
 			free_i->free_sections--;
 	}
-	write_unlock(&free_i->segmap_lock);
+	spin_unlock(&free_i->segmap_lock);
 }
 
 static inline void get_sit_bitmap(struct f2fs_sb_info *sbi,

commit 60a3b782b1aaf6e5f8c4f92e99302c48a26d475b
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Feb 10 16:44:29 2015 -0800

    f2fs: avoid variable length array
    
    Instead of using variable length array, this patch let preallocate memory for
    them.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 421d5794b0c8..ba858ca01445 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -189,6 +189,7 @@ struct sit_info {
 	char *sit_bitmap;		/* SIT bitmap pointer */
 	unsigned int bitmap_size;	/* SIT bitmap size */
 
+	unsigned long *tmp_map;			/* bitmap for temporal use */
 	unsigned long *dirty_sentries_bitmap;	/* bitmap for dirty sentries */
 	unsigned int dirty_sentries;		/* # of dirty sentries */
 	unsigned int sents_per_block;		/* # of SIT entries per block */

commit caf0047e7e1e60a7ad1d655d3b81b32e2dfb6095
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Wed Jan 28 17:48:42 2015 +0800

    f2fs: merge flags in struct f2fs_sb_info
    
    Currently, there are several variables with Boolean type as below:
    
    struct f2fs_sb_info {
    ...
            int s_dirty;
            bool need_fsck;
            bool s_closing;
    ...
            bool por_doing;
    ...
    }
    
    For this there are some issues:
    1. there are some space of f2fs_sb_info is wasted due to aligning after Boolean
       type variables by compiler.
    2. if we continuously add new flag into f2fs_sb_info, structure will be messed
       up.
    
    So in this patch, we try to:
    1. switch s_dirty to Boolean type variable since it has two status 0/1.
    2. merge s_dirty/need_fsck/s_closing/por_doing variables into s_flag.
    3. introduce an enum type which can indicate different states of sbi.
    4. use new introduced universal interfaces is_sbi_flag_set/{set,clear}_sbi_flag
       to operate flags for sbi.
    
    After that, above issues will be fixed.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 7f327c0ba4e3..421d5794b0c8 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -460,7 +460,7 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
 	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
 	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
 
-	if (unlikely(sbi->por_doing))
+	if (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))
 		return false;
 
 	return (free_sections(sbi) + freed) <= (node_secs + 2 * dent_secs +
@@ -599,13 +599,13 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
 {
 	if (segno > TOTAL_SEGS(sbi) - 1)
-		sbi->need_fsck = true;
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
 }
 
 static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 {
 	if (blk_addr < SEG0_BLKADDR(sbi) || blk_addr >= MAX_BLKADDR(sbi))
-		sbi->need_fsck = true;
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
 }
 
 /*
@@ -616,11 +616,11 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 {
 	/* check segment usage */
 	if (GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg)
-		sbi->need_fsck = true;
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
 
 	/* check boundary of a given segment number */
 	if (segno > TOTAL_SEGS(sbi) - 1)
-		sbi->need_fsck = true;
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
 }
 #endif
 

commit 510184c89f21f40634e1585790714be060cd49c5
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Nov 6 17:23:08 2014 -0800

    f2fs: do not skip any writes under memory pressure
    
    Under memory pressure, let's avoid skipping data writes.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 6723ccc17585..7f327c0ba4e3 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -711,6 +711,9 @@ static inline unsigned int max_hw_blocks(struct f2fs_sb_info *sbi)
  */
 static inline int nr_pages_to_skip(struct f2fs_sb_info *sbi, int type)
 {
+	if (sbi->sb->s_bdi->dirty_exceeded)
+		return 0;
+
 	if (type == DATA)
 		return sbi->blocks_per_seg;
 	else if (type == NODE)

commit c6ac4c0ec416e77cab09cac6cee2d100fbd7fc82
Author: Gu Zheng <guz.fnst@cn.fujitsu.com>
Date:   Mon Oct 20 17:45:50 2014 +0800

    f2fs: introduce f2fs_change_bit to simplify the change bit logic
    
    Introduce f2fs_change_bit to simplify the change bit logic in
    function set_to_next_nat{sit}.
    
    Signed-off-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 2495bec1c621..6723ccc17585 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -657,10 +657,7 @@ static inline void set_to_next_sit(struct sit_info *sit_i, unsigned int start)
 {
 	unsigned int block_off = SIT_BLOCK_OFFSET(start);
 
-	if (f2fs_test_bit(block_off, sit_i->sit_bitmap))
-		f2fs_clear_bit(block_off, sit_i->sit_bitmap);
-	else
-		f2fs_set_bit(block_off, sit_i->sit_bitmap);
+	f2fs_change_bit(block_off, sit_i->sit_bitmap);
 }
 
 static inline unsigned long long get_mtime(struct f2fs_sb_info *sbi)

commit 88b88a66797159949cec32eaab12b4968f6fae2d
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Oct 6 17:39:50 2014 -0700

    f2fs: support atomic writes
    
    This patch introduces a very limited functionality for atomic write support.
    In order to support atomic write, this patch adds two ioctls:
     o F2FS_IOC_START_ATOMIC_WRITE
     o F2FS_IOC_COMMIT_ATOMIC_WRITE
    
    The database engine should be aware of the following sequence.
    1. open
     -> ioctl(F2FS_IOC_START_ATOMIC_WRITE);
    2. writes
      : all the written data will be treated as atomic pages.
    3. commit
     -> ioctl(F2FS_IOC_COMMIT_ATOMIC_WRITE);
      : this flushes all the data blocks to the disk, which will be shown all or
      nothing by f2fs recovery procedure.
    4. repeat to #2.
    
    The IO pattens should be:
    
      ,- START_ATOMIC_WRITE                  ,- COMMIT_ATOMIC_WRITE
     CP | D D D D D D | FSYNC | D D D D | FSYNC ...
                          `- COMMIT_ATOMIC_WRITE
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index afb73627a8ec..2495bec1c621 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -175,6 +175,11 @@ struct segment_allocation {
 	void (*allocate_segment)(struct f2fs_sb_info *, int, bool);
 };
 
+struct inmem_pages {
+	struct list_head list;
+	struct page *page;
+};
+
 struct sit_info {
 	const struct segment_allocation *s_ops;
 
@@ -504,7 +509,7 @@ static inline bool need_inplace_update(struct inode *inode)
 	unsigned int policy = SM_I(sbi)->ipu_policy;
 
 	/* IPU can be done only for the user data */
-	if (S_ISDIR(inode->i_mode))
+	if (S_ISDIR(inode->i_mode) || f2fs_is_atomic_file(inode))
 		return false;
 
 	if (policy & (0x1 << F2FS_IPU_FORCE))

commit 8a21984d5d86540ba32b4b346d8fe6426018b7a7
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Thu Sep 25 14:39:17 2014 +0300

    f2fs: potential shift wrapping buf in f2fs_trim_fs()
    
    My static checker complains that segment is a u64 but only the lower 31
    bits can be used before we hit a shift wrapping bug.
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index a7e3e9112100..afb73627a8ec 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -55,7 +55,7 @@
 #define TOTAL_BLKS(sbi)	(TOTAL_SEGS(sbi) << sbi->log_blocks_per_seg)
 
 #define MAX_BLKADDR(sbi)	(SEG0_BLKADDR(sbi) + TOTAL_BLKS(sbi))
-#define SEGMENT_SIZE(sbi)	(1 << (sbi->log_blocksize +		\
+#define SEGMENT_SIZE(sbi)	(1ULL << (sbi->log_blocksize +		\
 					sbi->log_blocks_per_seg))
 
 #define START_BLOCK(sbi, segno)	(SEG0_BLKADDR(sbi) +			\

commit 7cd8558baa4e4588a80ecb31cb30784195763cdd
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 23 11:23:01 2014 -0700

    f2fs: check the use of macros on block counts and addresses
    
    This patch cleans up the existing and new macros for readability.
    
    Rule is like this.
    
             ,-----------------------------------------> MAX_BLKADDR -,
             |  ,------------- TOTAL_BLKS ----------------------------,
             |  |                                                     |
             |  ,- seg0_blkaddr   ,----- sit/nat/ssa/main blkaddress  |
    block    |  | (SEG0_BLKADDR)  | | | |   (e.g., MAIN_BLKADDR)      |
    address  0..x................ a b c d .............................
                |                                                     |
    global seg# 0...................... m .............................
                |                       |                             |
                |                       `------- MAIN_SEGS -----------'
                `-------------- TOTAL_SEGS ---------------------------'
                                        |                             |
     seg#                               0..........xx..................
    
    = Note =
     o GET_SEGNO_FROM_SEG0 : blk address -> global segno
     o GET_SEGNO           : blk address -> segno
     o START_BLOCK         : segno -> starting block address
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 4588545b3030..a7e3e9112100 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -45,16 +45,26 @@
 	 (secno == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno /		\
 	  sbi->segs_per_sec))	\
 
-#define START_BLOCK(sbi, segno)						\
-	(SM_I(sbi)->seg0_blkaddr +					\
+#define MAIN_BLKADDR(sbi)	(SM_I(sbi)->main_blkaddr)
+#define SEG0_BLKADDR(sbi)	(SM_I(sbi)->seg0_blkaddr)
+
+#define MAIN_SEGS(sbi)	(SM_I(sbi)->main_segments)
+#define MAIN_SECS(sbi)	(sbi->total_sections)
+
+#define TOTAL_SEGS(sbi)	(SM_I(sbi)->segment_count)
+#define TOTAL_BLKS(sbi)	(TOTAL_SEGS(sbi) << sbi->log_blocks_per_seg)
+
+#define MAX_BLKADDR(sbi)	(SEG0_BLKADDR(sbi) + TOTAL_BLKS(sbi))
+#define SEGMENT_SIZE(sbi)	(1 << (sbi->log_blocksize +		\
+					sbi->log_blocks_per_seg))
+
+#define START_BLOCK(sbi, segno)	(SEG0_BLKADDR(sbi) +			\
 	 (GET_R2L_SEGNO(FREE_I(sbi), segno) << sbi->log_blocks_per_seg))
+
 #define NEXT_FREE_BLKADDR(sbi, curseg)					\
 	(START_BLOCK(sbi, curseg->segno) + curseg->next_blkoff)
 
-#define MAIN_BASE_BLOCK(sbi)	(SM_I(sbi)->main_blkaddr)
-
-#define GET_SEGOFF_FROM_SEG0(sbi, blk_addr)				\
-	((blk_addr) - SM_I(sbi)->seg0_blkaddr)
+#define GET_SEGOFF_FROM_SEG0(sbi, blk_addr)	((blk_addr) - SEG0_BLKADDR(sbi))
 #define GET_SEGNO_FROM_SEG0(sbi, blk_addr)				\
 	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) >> sbi->log_blocks_per_seg)
 #define GET_BLKOFF_FROM_SEG0(sbi, blk_addr)				\
@@ -82,12 +92,9 @@
 #define	START_SEGNO(segno)		\
 	(SIT_BLOCK_OFFSET(segno) * SIT_ENTRY_PER_BLOCK)
 #define SIT_BLK_CNT(sbi)			\
-	((TOTAL_SEGS(sbi) + SIT_ENTRY_PER_BLOCK - 1) / SIT_ENTRY_PER_BLOCK)
+	((MAIN_SEGS(sbi) + SIT_ENTRY_PER_BLOCK - 1) / SIT_ENTRY_PER_BLOCK)
 #define f2fs_bitmap_size(nr)			\
 	(BITS_TO_LONGS(nr) * sizeof(unsigned long))
-#define TOTAL_SEGS(sbi)	(SM_I(sbi)->main_segments)
-#define TOTAL_SECS(sbi)	(sbi->total_sections)
-#define TOTAL_BLKS(sbi)	(SM_I(sbi)->segment_count << sbi->log_blocks_per_seg)
 
 #define SECTOR_FROM_BLOCK(blk_addr)					\
 	(((sector_t)blk_addr) << F2FS_LOG_SECTORS_PER_BLOCK)
@@ -323,7 +330,7 @@ static inline void __set_free(struct f2fs_sb_info *sbi, unsigned int segno)
 	clear_bit(segno, free_i->free_segmap);
 	free_i->free_segments++;
 
-	next = find_next_bit(free_i->free_segmap, TOTAL_SEGS(sbi), start_segno);
+	next = find_next_bit(free_i->free_segmap, MAIN_SEGS(sbi), start_segno);
 	if (next >= start_segno + sbi->segs_per_sec) {
 		clear_bit(secno, free_i->free_secmap);
 		free_i->free_sections++;
@@ -542,18 +549,13 @@ static inline unsigned short curseg_blkoff(struct f2fs_sb_info *sbi, int type)
 #ifdef CONFIG_F2FS_CHECK_FS
 static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
 {
-	unsigned int end_segno = SM_I(sbi)->segment_count - 1;
-	BUG_ON(segno > end_segno);
+	BUG_ON(segno > TOTAL_SEGS(sbi) - 1);
 }
 
 static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 {
-	struct f2fs_sm_info *sm_info = SM_I(sbi);
-	block_t total_blks = TOTAL_BLKS(sbi);
-	block_t start_addr = sm_info->seg0_blkaddr;
-	block_t end_addr = start_addr + total_blks - 1;
-	BUG_ON(blk_addr < start_addr);
-	BUG_ON(blk_addr > end_addr);
+	BUG_ON(blk_addr < SEG0_BLKADDR(sbi));
+	BUG_ON(blk_addr >= MAX_BLKADDR(sbi));
 }
 
 /*
@@ -562,8 +564,6 @@ static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 static inline void check_block_count(struct f2fs_sb_info *sbi,
 		int segno, struct f2fs_sit_entry *raw_sit)
 {
-	struct f2fs_sm_info *sm_info = SM_I(sbi);
-	unsigned int end_segno = sm_info->segment_count - 1;
 	bool is_valid  = test_bit_le(0, raw_sit->valid_map) ? true : false;
 	int valid_blocks = 0;
 	int cur_pos = 0, next_pos;
@@ -572,7 +572,7 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 	BUG_ON(GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg);
 
 	/* check boundary of a given segment number */
-	BUG_ON(segno > end_segno);
+	BUG_ON(segno > TOTAL_SEGS(sbi) - 1);
 
 	/* check bitmap with valid block count */
 	do {
@@ -593,20 +593,13 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 #else
 static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
 {
-	unsigned int end_segno = SM_I(sbi)->segment_count - 1;
-
-	if (segno > end_segno)
+	if (segno > TOTAL_SEGS(sbi) - 1)
 		sbi->need_fsck = true;
 }
 
 static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 {
-	struct f2fs_sm_info *sm_info = SM_I(sbi);
-	block_t total_blks = TOTAL_BLKS(sbi);
-	block_t start_addr = sm_info->seg0_blkaddr;
-	block_t end_addr = start_addr + total_blks - 1;
-
-	if (blk_addr < start_addr || blk_addr > end_addr)
+	if (blk_addr < SEG0_BLKADDR(sbi) || blk_addr >= MAX_BLKADDR(sbi))
 		sbi->need_fsck = true;
 }
 
@@ -616,14 +609,12 @@ static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 static inline void check_block_count(struct f2fs_sb_info *sbi,
 		int segno, struct f2fs_sit_entry *raw_sit)
 {
-	unsigned int end_segno = SM_I(sbi)->segment_count - 1;
-
 	/* check segment usage */
 	if (GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg)
 		sbi->need_fsck = true;
 
 	/* check boundary of a given segment number */
-	if (segno > end_segno)
+	if (segno > TOTAL_SEGS(sbi) - 1)
 		sbi->need_fsck = true;
 }
 #endif

commit 95dd89730119b97d82f9edc806757cef737703e5
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Sep 17 17:52:58 2014 -0700

    f2fs: use more free segments until SSR is activated
    
    Previously, f2fs activates SSR if the # of free segments reaches to the # of
    overprovisioned segments.
    In this case, SSR starts to use dirty segments only, so that the overprovisoned
    space cannot be selected for new data.
    This means that we have no chance to utilizae the overprovisioned space at all.
    
    This patch fixes that by allowing LFS allocations until the # of free segments
    reaches to the last threshold, reserved space.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index d317b61f83a5..4588545b3030 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -437,8 +437,10 @@ static inline int reserved_sections(struct f2fs_sb_info *sbi)
 
 static inline bool need_SSR(struct f2fs_sb_info *sbi)
 {
-	return (prefree_segments(sbi) / sbi->segs_per_sec)
-			+ free_sections(sbi) < overprovision_sections(sbi);
+	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
+	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
+	return free_sections(sbi) <= (node_secs + 2 * dent_secs +
+						reserved_sections(sbi) + 1);
 }
 
 static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)

commit 9b5f136fd41658f384a5b4ea49d8ef37036e15f5
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 16 18:30:54 2014 -0700

    f2fs: change the ipu_policy option to enable combinations
    
    This patch changes the ipu_policy setting to use any combination of orthogonal policies.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 032c0905a12b..d317b61f83a5 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -487,40 +487,33 @@ enum {
 	F2FS_IPU_UTIL,
 	F2FS_IPU_SSR_UTIL,
 	F2FS_IPU_FSYNC,
-	F2FS_IPU_DISABLE,
 };
 
 static inline bool need_inplace_update(struct inode *inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	unsigned int policy = SM_I(sbi)->ipu_policy;
 
 	/* IPU can be done only for the user data */
 	if (S_ISDIR(inode->i_mode))
 		return false;
 
-	switch (SM_I(sbi)->ipu_policy) {
-	case F2FS_IPU_FORCE:
+	if (policy & (0x1 << F2FS_IPU_FORCE))
 		return true;
-	case F2FS_IPU_SSR:
-		if (need_SSR(sbi))
-			return true;
-		break;
-	case F2FS_IPU_UTIL:
-		if (utilization(sbi) > SM_I(sbi)->min_ipu_util)
-			return true;
-		break;
-	case F2FS_IPU_SSR_UTIL:
-		if (need_SSR(sbi) && utilization(sbi) > SM_I(sbi)->min_ipu_util)
-			return true;
-		break;
-	case F2FS_IPU_FSYNC:
-		/* this is only set during fdatasync */
-		if (is_inode_flag_set(F2FS_I(inode), FI_NEED_IPU))
-			return true;
-		break;
-	case F2FS_IPU_DISABLE:
-		break;
-	}
+	if (policy & (0x1 << F2FS_IPU_SSR) && need_SSR(sbi))
+		return true;
+	if (policy & (0x1 << F2FS_IPU_UTIL) &&
+			utilization(sbi) > SM_I(sbi)->min_ipu_util)
+		return true;
+	if (policy & (0x1 << F2FS_IPU_SSR_UTIL) && need_SSR(sbi) &&
+			utilization(sbi) > SM_I(sbi)->min_ipu_util)
+		return true;
+
+	/* this is only set during fdatasync */
+	if (policy & (0x1 << F2FS_IPU_FSYNC) &&
+			is_inode_flag_set(F2FS_I(inode), FI_NEED_IPU))
+		return true;
+
 	return false;
 }
 

commit 55cf9cb63f0e5439f208d78ed944de9a8df65011
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Sep 15 18:01:10 2014 +0800

    f2fs: support large sector size
    
    Block size in f2fs is 4096 bytes, so theoretically, f2fs can support 4096 bytes
    sector device at maximum. But now f2fs only support 512 bytes size sector, so
    block device such as zRAM which uses page cache as its block storage space will
    not be mounted successfully as mismatch between sector size of zRAM and sector
    size of f2fs supported.
    
    In this patch we support large sector size in f2fs, so block device with sector
    size of 512/1024/2048/4096 bytes can be supported in f2fs.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 848ac519958f..032c0905a12b 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -89,10 +89,10 @@
 #define TOTAL_SECS(sbi)	(sbi->total_sections)
 #define TOTAL_BLKS(sbi)	(SM_I(sbi)->segment_count << sbi->log_blocks_per_seg)
 
-#define SECTOR_FROM_BLOCK(sbi, blk_addr)				\
-	(((sector_t)blk_addr) << (sbi)->log_sectors_per_block)
-#define SECTOR_TO_BLOCK(sbi, sectors)					\
-	(sectors >> (sbi)->log_sectors_per_block)
+#define SECTOR_FROM_BLOCK(blk_addr)					\
+	(((sector_t)blk_addr) << F2FS_LOG_SECTORS_PER_BLOCK)
+#define SECTOR_TO_BLOCK(sectors)					\
+	(sectors >> F2FS_LOG_SECTORS_PER_BLOCK)
 #define MAX_BIO_BLOCKS(sbi)						\
 	((int)min((int)max_hw_blocks(sbi), BIO_MAX_PAGES))
 
@@ -711,7 +711,7 @@ static inline unsigned int max_hw_blocks(struct f2fs_sb_info *sbi)
 {
 	struct block_device *bdev = sbi->sb->s_bdev;
 	struct request_queue *q = bdev_get_queue(bdev);
-	return SECTOR_TO_BLOCK(sbi, queue_max_sectors(q));
+	return SECTOR_TO_BLOCK(queue_max_sectors(q));
 }
 
 /*

commit 90a893c749f4582f21e97639f4e85e7f2362c2f0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Mon Sep 22 16:21:07 2014 -0700

    f2fs: use MAX_BIO_BLOCKS(sbi)
    
    This patch cleans up a simple macro.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index c6f37f2fe50e..848ac519958f 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -93,8 +93,8 @@
 	(((sector_t)blk_addr) << (sbi)->log_sectors_per_block)
 #define SECTOR_TO_BLOCK(sbi, sectors)					\
 	(sectors >> (sbi)->log_sectors_per_block)
-#define MAX_BIO_BLOCKS(max_hw_blocks)					\
-	(min((int)max_hw_blocks, BIO_MAX_PAGES))
+#define MAX_BIO_BLOCKS(sbi)						\
+	((int)min((int)max_hw_blocks(sbi), BIO_MAX_PAGES))
 
 /*
  * indicate a block allocation direction: RIGHT and LEFT.
@@ -728,7 +728,7 @@ static inline int nr_pages_to_skip(struct f2fs_sb_info *sbi, int type)
 	else if (type == NODE)
 		return 3 * sbi->blocks_per_seg;
 	else if (type == META)
-		return MAX_BIO_BLOCKS(max_hw_blocks(sbi));
+		return MAX_BIO_BLOCKS(sbi);
 	else
 		return 0;
 }
@@ -751,7 +751,7 @@ static inline long nr_pages_to_write(struct f2fs_sb_info *sbi, int type,
 	else if (type == NODE)
 		desired = 3 * max_hw_blocks(sbi);
 	else
-		desired = MAX_BIO_BLOCKS(max_hw_blocks(sbi));
+		desired = MAX_BIO_BLOCKS(sbi);
 
 	wbc->nr_to_write = desired;
 	return desired - nr_to_write;

commit 4c521f493b625c7982cf2eae246e86c893f62dfa
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Sep 11 13:49:55 2014 -0700

    f2fs: use meta_inode cache to improve roll-forward speed
    
    Previously, all the dnode pages should be read during the roll-forward recovery.
    Even worsely, whole the chain was traversed twice.
    This patch removes that redundant and costly read operations by using page cache
    of meta_inode and readahead function as well.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 013aed2f3539..c6f37f2fe50e 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -87,6 +87,7 @@
 	(BITS_TO_LONGS(nr) * sizeof(unsigned long))
 #define TOTAL_SEGS(sbi)	(SM_I(sbi)->main_segments)
 #define TOTAL_SECS(sbi)	(sbi->total_sections)
+#define TOTAL_BLKS(sbi)	(SM_I(sbi)->segment_count << sbi->log_blocks_per_seg)
 
 #define SECTOR_FROM_BLOCK(sbi, blk_addr)				\
 	(((sector_t)blk_addr) << (sbi)->log_sectors_per_block)
@@ -553,7 +554,7 @@ static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
 static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 {
 	struct f2fs_sm_info *sm_info = SM_I(sbi);
-	block_t total_blks = sm_info->segment_count << sbi->log_blocks_per_seg;
+	block_t total_blks = TOTAL_BLKS(sbi);
 	block_t start_addr = sm_info->seg0_blkaddr;
 	block_t end_addr = start_addr + total_blks - 1;
 	BUG_ON(blk_addr < start_addr);
@@ -606,7 +607,7 @@ static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
 static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 {
 	struct f2fs_sm_info *sm_info = SM_I(sbi);
-	block_t total_blks = sm_info->segment_count << sbi->log_blocks_per_seg;
+	block_t total_blks = TOTAL_BLKS(sbi);
 	block_t start_addr = sm_info->seg0_blkaddr;
 	block_t end_addr = start_addr + total_blks - 1;
 

commit c1ce1b02bb25640567dc484dc94d3a195d21e705
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Wed Sep 10 16:53:02 2014 -0700

    f2fs: give an option to enable in-place-updates during fsync to users
    
    If user wrote F2FS_IPU_FSYNC:4 in /sys/fs/f2fs/ipu_policy, f2fs_sync_file
    only starts to try in-place-updates.
    And, if the number of dirty pages is over /sys/fs/f2fs/min_fsync_blocks, it
    keeps out-of-order manner. Otherwise, it triggers in-place-updates.
    
    This may be used by storage showing very high random write performance.
    
    For example, it can be used when,
    
    Seq. writes (Data) + wait + Seq. writes (Node)
    
    is pretty much slower than,
    
    Rand. writes (Data)
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index bed0dc967f29..013aed2f3539 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -472,15 +472,20 @@ static inline int utilization(struct f2fs_sb_info *sbi)
  * F2FS_IPU_UTIL - if FS utilization is over threashold,
  * F2FS_IPU_SSR_UTIL - if SSR mode is activated and FS utilization is over
  *                     threashold,
+ * F2FS_IPU_FSYNC - activated in fsync path only for high performance flash
+ *                     storages. IPU will be triggered only if the # of dirty
+ *                     pages over min_fsync_blocks.
  * F2FS_IPUT_DISABLE - disable IPU. (=default option)
  */
 #define DEF_MIN_IPU_UTIL	70
+#define DEF_MIN_FSYNC_BLOCKS	8
 
 enum {
 	F2FS_IPU_FORCE,
 	F2FS_IPU_SSR,
 	F2FS_IPU_UTIL,
 	F2FS_IPU_SSR_UTIL,
+	F2FS_IPU_FSYNC,
 	F2FS_IPU_DISABLE,
 };
 
@@ -492,10 +497,6 @@ static inline bool need_inplace_update(struct inode *inode)
 	if (S_ISDIR(inode->i_mode))
 		return false;
 
-	/* this is only set during fdatasync */
-	if (is_inode_flag_set(F2FS_I(inode), FI_NEED_IPU))
-		return true;
-
 	switch (SM_I(sbi)->ipu_policy) {
 	case F2FS_IPU_FORCE:
 		return true;
@@ -511,6 +512,11 @@ static inline bool need_inplace_update(struct inode *inode)
 		if (need_SSR(sbi) && utilization(sbi) > SM_I(sbi)->min_ipu_util)
 			return true;
 		break;
+	case F2FS_IPU_FSYNC:
+		/* this is only set during fdatasync */
+		if (is_inode_flag_set(F2FS_I(inode), FI_NEED_IPU))
+			return true;
+		break;
 	case F2FS_IPU_DISABLE:
 		break;
 	}

commit 184a5cd2ce281f1207d72adb9ae18e416ca371db
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Sep 4 18:13:01 2014 +0800

    f2fs: refactor flush_sit_entries codes for reducing SIT writes
    
    In commit aec71382c681 ("f2fs: refactor flush_nat_entries codes for reducing NAT
    writes"), we descripte the issue as below:
    
    "Although building NAT journal in cursum reduce the read/write work for NAT
    block, but previous design leave us lower performance when write checkpoint
    frequently for these cases:
    1. if journal in cursum has already full, it's a bit of waste that we flush all
       nat entries to page for persistence, but not to cache any entries.
    2. if journal in cursum is not full, we fill nat entries to journal util
       journal is full, then flush the left dirty entries to disk without merge
       journaled entries, so these journaled entries may be flushed to disk at next
       checkpoint but lost chance to flushed last time."
    
    Actually, we have the same problem in using SIT journal area.
    
    In this patch, firstly we will update sit journal with dirty entries as many as
    possible. Secondly if there is no space in sit journal, we will remove all
    entries in journal and walk through the whole dirty entry bitmap of sit,
    accounting dirty sit entries located in same SIT block to sit entry set. All
    entry sets are linked to list sit_entry_set in sm_info, sorted ascending order
    by count of entries in set. Later we flush entries in set which have fewest
    entries into journal as many as we can, and then flush dense set with merged
    entries to disk.
    
    In this way we can use sit journal area more effectively, also we will reduce
    SIT update, result in gaining in performance and saving lifetime of flash
    device.
    
    In my testing environment, it shows this patch can help to reduce SIT block
    update obviously.
    
    virtual machine + hard disk:
    fsstress -p 20 -n 400 -l 5
                    sit page num    cp count        sit pages/cp
    based           2006.50         1349.75         1.486
    patched         1566.25         1463.25         1.070
    
    Our latency of merging op is small when handling a great number of dirty SIT
    entries in flush_sit_entries:
    latency(ns)     dirty sit count
    36038           2151
    49168           2123
    37174           2232
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 2548bfdf0240..bed0dc967f29 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -237,6 +237,12 @@ struct curseg_info {
 	unsigned int next_segno;		/* preallocated segment */
 };
 
+struct sit_entry_set {
+	struct list_head set_list;	/* link with all sit sets */
+	unsigned int start_segno;	/* start segno of sits in set */
+	unsigned int entry_cnt;		/* the # of sit entries in set */
+};
+
 /*
  * inline functions
  */

commit d3a14afd5ed1970519a2d6ed59f4062ec3ba821f
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Sep 4 18:11:47 2014 +0800

    f2fs: remove unneeded sit_i in macro SIT_BLOCK_OFFSET/START_SEGNO
    
    sit_i in macro SIT_BLOCK_OFFSET/START_SEGNO is not used, remove it.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index d163fc854ac1..2548bfdf0240 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -77,10 +77,10 @@
 
 #define SIT_ENTRY_OFFSET(sit_i, segno)					\
 	(segno % sit_i->sents_per_block)
-#define SIT_BLOCK_OFFSET(sit_i, segno)					\
+#define SIT_BLOCK_OFFSET(segno)					\
 	(segno / SIT_ENTRY_PER_BLOCK)
-#define	START_SEGNO(sit_i, segno)		\
-	(SIT_BLOCK_OFFSET(sit_i, segno) * SIT_ENTRY_PER_BLOCK)
+#define	START_SEGNO(segno)		\
+	(SIT_BLOCK_OFFSET(segno) * SIT_ENTRY_PER_BLOCK)
 #define SIT_BLK_CNT(sbi)			\
 	((TOTAL_SEGS(sbi) + SIT_ENTRY_PER_BLOCK - 1) / SIT_ENTRY_PER_BLOCK)
 #define f2fs_bitmap_size(nr)			\
@@ -624,7 +624,7 @@ static inline pgoff_t current_sit_addr(struct f2fs_sb_info *sbi,
 						unsigned int start)
 {
 	struct sit_info *sit_i = SIT_I(sbi);
-	unsigned int offset = SIT_BLOCK_OFFSET(sit_i, start);
+	unsigned int offset = SIT_BLOCK_OFFSET(start);
 	block_t blk_addr = sit_i->sit_base_addr + offset;
 
 	check_seg_range(sbi, start);
@@ -651,7 +651,7 @@ static inline pgoff_t next_sit_addr(struct f2fs_sb_info *sbi,
 
 static inline void set_to_next_sit(struct sit_info *sit_i, unsigned int start)
 {
-	unsigned int block_off = SIT_BLOCK_OFFSET(sit_i, start);
+	unsigned int block_off = SIT_BLOCK_OFFSET(start);
 
 	if (f2fs_test_bit(block_off, sit_i->sit_bitmap))
 		f2fs_clear_bit(block_off, sit_i->sit_bitmap);

commit 05796763b8d19b48bb4149bfb1aa1a91dd9faee6
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 2 16:05:00 2014 -0700

    f2fs: add BUG cases to initiate fsck.f2fs
    
    This patch replaces BUG cases with f2fs_bug_on to remain fsck.f2fs information.
    And it implements some void functions to initiate fsck.f2fs too.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 793ca27b9c88..d163fc854ac1 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -583,9 +583,41 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 	BUG_ON(GET_SIT_VBLOCKS(raw_sit) != valid_blocks);
 }
 #else
-#define check_seg_range(sbi, segno)
-#define verify_block_addr(sbi, blk_addr)
-#define check_block_count(sbi, segno, raw_sit)
+static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
+{
+	unsigned int end_segno = SM_I(sbi)->segment_count - 1;
+
+	if (segno > end_segno)
+		sbi->need_fsck = true;
+}
+
+static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
+{
+	struct f2fs_sm_info *sm_info = SM_I(sbi);
+	block_t total_blks = sm_info->segment_count << sbi->log_blocks_per_seg;
+	block_t start_addr = sm_info->seg0_blkaddr;
+	block_t end_addr = start_addr + total_blks - 1;
+
+	if (blk_addr < start_addr || blk_addr > end_addr)
+		sbi->need_fsck = true;
+}
+
+/*
+ * Summary block is always treated as an invalid block
+ */
+static inline void check_block_count(struct f2fs_sb_info *sbi,
+		int segno, struct f2fs_sit_entry *raw_sit)
+{
+	unsigned int end_segno = SM_I(sbi)->segment_count - 1;
+
+	/* check segment usage */
+	if (GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg)
+		sbi->need_fsck = true;
+
+	/* check boundary of a given segment number */
+	if (segno > end_segno)
+		sbi->need_fsck = true;
+}
 #endif
 
 static inline pgoff_t current_sit_addr(struct f2fs_sb_info *sbi,

commit 4081363fbe84a7ebac6d3339dd2775df45d856d0
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Tue Sep 2 15:31:18 2014 -0700

    f2fs: introduce F2FS_I_SB, F2FS_M_SB, and F2FS_P_SB
    
    This patch adds three inline functions to clean up dirty casting codes.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index ff483257283b..793ca27b9c88 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -480,7 +480,7 @@ enum {
 
 static inline bool need_inplace_update(struct inode *inode)
 {
-	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
 	/* IPU can be done only for the user data */
 	if (S_ISDIR(inode->i_mode))

commit e1c42045203071c4634b89e696037357810d3083
Author: arter97 <qkrwngud825@gmail.com>
Date:   Wed Aug 6 23:22:50 2014 +0900

    f2fs: fix typo
    
    Fix typo and some grammatical errors.
    
    The words "filesystem" and "readahead" are being used without the space treewide.
    
    Signed-off-by: Park Ju Hyung <qkrwngud825@gmail.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 55973f7b0330..ff483257283b 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -549,7 +549,7 @@ static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 }
 
 /*
- * Summary block is always treated as invalid block
+ * Summary block is always treated as an invalid block
  */
 static inline void check_block_count(struct f2fs_sb_info *sbi,
 		int segno, struct f2fs_sit_entry *raw_sit)

commit ea1aa12ca237149227ef68af50c9a1acf027b625
Author: Jaegeuk Kim <jaegeuk@kernel.org>
Date:   Thu Jul 24 19:11:43 2014 -0700

    f2fs: enable in-place-update for fdatasync
    
    This patch enforces in-place-updates only when fdatasync is requested.
    If we adopt this in-place-updates for the fdatasync, we can skip to write the
    recovery information.
    
    Reviewed-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index ee5c75e08d9c..55973f7b0330 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -486,6 +486,10 @@ static inline bool need_inplace_update(struct inode *inode)
 	if (S_ISDIR(inode->i_mode))
 		return false;
 
+	/* this is only set during fdatasync */
+	if (is_inode_flag_set(F2FS_I(inode), FI_NEED_IPU))
+		return true;
+
 	switch (SM_I(sbi)->ipu_policy) {
 	case F2FS_IPU_FORCE:
 		return true;

commit f1121ab0ba9ab9c1592049533b511877600f409e
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Mon Jul 14 16:45:15 2014 +0800

    f2fs: reduce searching region of segmap when free section
    
    In __set_test_and_free we will check whether all segment are free in one section
    When free one segment, in order to set section to free status.
    But the searching region of segmap is from start segno to last segno of f2fs,
    it's not necessary. So let's just only check all segment bitmap of target
    section.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk@kernel.org>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 7091204680f4..ee5c75e08d9c 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -347,8 +347,8 @@ static inline void __set_test_and_free(struct f2fs_sb_info *sbi,
 	if (test_and_clear_bit(segno, free_i->free_segmap)) {
 		free_i->free_segments++;
 
-		next = find_next_bit(free_i->free_segmap, TOTAL_SEGS(sbi),
-								start_segno);
+		next = find_next_bit(free_i->free_segmap,
+				start_segno + sbi->segs_per_sec, start_segno);
 		if (next >= start_segno + sbi->segs_per_sec) {
 			if (test_and_clear_bit(secno, free_i->free_secmap))
 				free_i->free_sections++;

commit 58c410351eba3d24f741c85a0eb9eaf15c94047d
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Mar 19 14:17:21 2014 +0900

    f2fs: change reclaim rate in percentage
    
    It is more reasonable to determine the reclaiming rate of prefree segments
    according to the volume size, which is set to 5% by default.
    For example, if the volume is 128GB, the prefree segments are reclaimed
    when the number reaches to 6.4GB.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 9fc46ee27bb8..7091204680f4 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -14,7 +14,7 @@
 #define NULL_SEGNO			((unsigned int)(~0))
 #define NULL_SECNO			((unsigned int)(~0))
 
-#define DEF_RECLAIM_PREFREE_SEGMENTS	100	/* 200MB of prefree segments */
+#define DEF_RECLAIM_PREFREE_SEGMENTS	5	/* 5% over total segments */
 
 /* L: Logical segment # in volume, R: Relative segment # in main area */
 #define GET_L2R_SEGNO(free_i, segno)	(segno - free_i->start_segno)

commit 50c8cdb35ad8016c52fb2326ef9d65542e3a3e1b
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Mar 18 13:47:11 2014 +0900

    f2fs: introduce nr_pages_to_write for segment alignment
    
    This patch introduces nr_pages_to_write to align page writes to the segment
    or other operational unit size, which can be tuned according to the system
    environment.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index bbd976100d14..9fc46ee27bb8 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -683,3 +683,27 @@ static inline int nr_pages_to_skip(struct f2fs_sb_info *sbi, int type)
 	else
 		return 0;
 }
+
+/*
+ * When writing pages, it'd better align nr_to_write for segment size.
+ */
+static inline long nr_pages_to_write(struct f2fs_sb_info *sbi, int type,
+					struct writeback_control *wbc)
+{
+	long nr_to_write, desired;
+
+	if (wbc->sync_mode != WB_SYNC_NONE)
+		return 0;
+
+	nr_to_write = wbc->nr_to_write;
+
+	if (type == DATA)
+		desired = 4096;
+	else if (type == NODE)
+		desired = 3 * max_hw_blocks(sbi);
+	else
+		desired = MAX_BIO_BLOCKS(max_hw_blocks(sbi));
+
+	wbc->nr_to_write = desired;
+	return desired - nr_to_write;
+}

commit 87d6f890944d092c4ef5b84053f0d0d5d8137b0b
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Mar 18 12:40:49 2014 +0900

    f2fs: avoid small data writes by skipping writepages
    
    This patch introduces nr_pages_to_skip(sbi, type) to determine writepages can
    be skipped.
    The dentry, node, and meta pages can be conrolled by F2FS without breaking the
    FS consistency.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index c3d5e3689ffc..bbd976100d14 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -664,3 +664,22 @@ static inline unsigned int max_hw_blocks(struct f2fs_sb_info *sbi)
 	struct request_queue *q = bdev_get_queue(bdev);
 	return SECTOR_TO_BLOCK(sbi, queue_max_sectors(q));
 }
+
+/*
+ * It is very important to gather dirty pages and write at once, so that we can
+ * submit a big bio without interfering other data writes.
+ * By default, 512 pages for directory data,
+ * 512 pages (2MB) * 3 for three types of nodes, and
+ * max_bio_blocks for meta are set.
+ */
+static inline int nr_pages_to_skip(struct f2fs_sb_info *sbi, int type)
+{
+	if (type == DATA)
+		return sbi->blocks_per_seg;
+	else if (type == NODE)
+		return 3 * sbi->blocks_per_seg;
+	else if (type == META)
+		return MAX_BIO_BLOCKS(max_hw_blocks(sbi));
+	else
+		return 0;
+}

commit 8b8343fa9d503894ece57acbe46cb36883646685
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Feb 24 13:00:13 2014 +0900

    f2fs: implement a lock-free stat_show
    
    The stat_show is just to show the current status of f2fs.
    So, we can remove all the there-in locks.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 4024546b6361..c3d5e3689ffc 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -380,26 +380,12 @@ static inline void get_sit_bitmap(struct f2fs_sb_info *sbi,
 
 static inline block_t written_block_count(struct f2fs_sb_info *sbi)
 {
-	struct sit_info *sit_i = SIT_I(sbi);
-	block_t vblocks;
-
-	mutex_lock(&sit_i->sentry_lock);
-	vblocks = sit_i->written_valid_blocks;
-	mutex_unlock(&sit_i->sentry_lock);
-
-	return vblocks;
+	return SIT_I(sbi)->written_valid_blocks;
 }
 
 static inline unsigned int free_segments(struct f2fs_sb_info *sbi)
 {
-	struct free_segmap_info *free_i = FREE_I(sbi);
-	unsigned int free_segs;
-
-	read_lock(&free_i->segmap_lock);
-	free_segs = free_i->free_segments;
-	read_unlock(&free_i->segmap_lock);
-
-	return free_segs;
+	return FREE_I(sbi)->free_segments;
 }
 
 static inline int reserved_segments(struct f2fs_sb_info *sbi)
@@ -409,14 +395,7 @@ static inline int reserved_segments(struct f2fs_sb_info *sbi)
 
 static inline unsigned int free_sections(struct f2fs_sb_info *sbi)
 {
-	struct free_segmap_info *free_i = FREE_I(sbi);
-	unsigned int free_secs;
-
-	read_lock(&free_i->segmap_lock);
-	free_secs = free_i->free_sections;
-	read_unlock(&free_i->segmap_lock);
-
-	return free_secs;
+	return FREE_I(sbi)->free_sections;
 }
 
 static inline unsigned int prefree_segments(struct f2fs_sb_info *sbi)

commit 491c0854b41380f48e422c00ae7e25ae4d02cecc
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Feb 4 13:01:10 2014 +0900

    f2fs: clean up with a macro
    
    This patch adds GET_BLKOFF_FROM_SEG0 to clean up some codes.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 5731682d7516..4024546b6361 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -57,6 +57,9 @@
 	((blk_addr) - SM_I(sbi)->seg0_blkaddr)
 #define GET_SEGNO_FROM_SEG0(sbi, blk_addr)				\
 	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) >> sbi->log_blocks_per_seg)
+#define GET_BLKOFF_FROM_SEG0(sbi, blk_addr)				\
+	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) & (sbi->blocks_per_seg - 1))
+
 #define GET_SEGNO(sbi, blk_addr)					\
 	(((blk_addr == NULL_ADDR) || (blk_addr == NEW_ADDR)) ?		\
 	NULL_SEGNO : GET_L2R_SEGNO(FREE_I(sbi),			\

commit 6c311ec6c2d9e015d454b4e3fda8008b5bebf316
Author: Chris Fries <cfries@motorola.com>
Date:   Fri Jan 17 14:44:39 2014 -0600

    f2fs: clean checkpatch warnings
    
    Fixed a variety of trivial checkpatch warnings.  The only delta should
    be some minor formatting on log strings that were split / too long.
    
    Signed-off-by: Chris Fries <cfries@motorola.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index e9a10bdd7fed..5731682d7516 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -448,8 +448,8 @@ static inline int reserved_sections(struct f2fs_sb_info *sbi)
 
 static inline bool need_SSR(struct f2fs_sb_info *sbi)
 {
-	return ((prefree_segments(sbi) / sbi->segs_per_sec)
-			+ free_sections(sbi) < overprovision_sections(sbi));
+	return (prefree_segments(sbi) / sbi->segs_per_sec)
+			+ free_sections(sbi) < overprovision_sections(sbi);
 }
 
 static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
@@ -460,18 +460,19 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
 	if (unlikely(sbi->por_doing))
 		return false;
 
-	return ((free_sections(sbi) + freed) <= (node_secs + 2 * dent_secs +
-						reserved_sections(sbi)));
+	return (free_sections(sbi) + freed) <= (node_secs + 2 * dent_secs +
+						reserved_sections(sbi));
 }
 
 static inline bool excess_prefree_segs(struct f2fs_sb_info *sbi)
 {
-	return (prefree_segments(sbi) > SM_I(sbi)->rec_prefree_segments);
+	return prefree_segments(sbi) > SM_I(sbi)->rec_prefree_segments;
 }
 
 static inline int utilization(struct f2fs_sb_info *sbi)
 {
-	return div_u64((u64)valid_user_blocks(sbi) * 100, sbi->user_block_count);
+	return div_u64((u64)valid_user_blocks(sbi) * 100,
+					sbi->user_block_count);
 }
 
 /*

commit 216fbd64437452d23db54ae845916facd7215caa
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Nov 7 13:13:42 2013 +0900

    f2fs: introduce sysfs entry to control in-place-update policy
    
    This patch introduces new sysfs entries for users to control the policy of
    in-place-updates, namely IPU, in f2fs.
    
    Sometimes f2fs suffers from performance degradation due to its out-of-place
    update policy that produces many additional node block writes.
    If the storage performance is very dependant on the amount of data writes
    instead of IO patterns, we'd better drop this out-of-place update policy.
    
    This patch suggests 5 polcies and their triggering conditions as follows.
    
    [sysfs entry name = ipu_policy]
    
    0: F2FS_IPU_FORCE       all the time,
    1: F2FS_IPU_SSR         if SSR mode is activated,
    2: F2FS_IPU_UTIL        if FS utilization is over threashold,
    3: F2FS_IPU_SSR_UTIL    if SSR mode is activated and FS utilization is over
                            threashold,
    4: F2FS_IPU_DISABLE    disable IPU. (=default option)
    
    [sysfs entry name = min_ipu_util]
    
    This parameter controls the threshold to trigger in-place-updates.
    The number indicates percentage of the filesystem utilization, and used by
    F2FS_IPU_UTIL and F2FS_IPU_SSR_UTIL policies.
    
    For more details, see need_inplace_update() in segment.h.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index ea563760f4b7..e9a10bdd7fed 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -476,19 +476,51 @@ static inline int utilization(struct f2fs_sb_info *sbi)
 
 /*
  * Sometimes f2fs may be better to drop out-of-place update policy.
- * So, if fs utilization is over MIN_IPU_UTIL, then f2fs tries to write
- * data in the original place likewise other traditional file systems.
- * But, currently set 100 in percentage, which means it is disabled.
- * See below need_inplace_update().
+ * And, users can control the policy through sysfs entries.
+ * There are five policies with triggering conditions as follows.
+ * F2FS_IPU_FORCE - all the time,
+ * F2FS_IPU_SSR - if SSR mode is activated,
+ * F2FS_IPU_UTIL - if FS utilization is over threashold,
+ * F2FS_IPU_SSR_UTIL - if SSR mode is activated and FS utilization is over
+ *                     threashold,
+ * F2FS_IPUT_DISABLE - disable IPU. (=default option)
  */
-#define MIN_IPU_UTIL		100
+#define DEF_MIN_IPU_UTIL	70
+
+enum {
+	F2FS_IPU_FORCE,
+	F2FS_IPU_SSR,
+	F2FS_IPU_UTIL,
+	F2FS_IPU_SSR_UTIL,
+	F2FS_IPU_DISABLE,
+};
+
 static inline bool need_inplace_update(struct inode *inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+
+	/* IPU can be done only for the user data */
 	if (S_ISDIR(inode->i_mode))
 		return false;
-	if (need_SSR(sbi) && utilization(sbi) > MIN_IPU_UTIL)
+
+	switch (SM_I(sbi)->ipu_policy) {
+	case F2FS_IPU_FORCE:
 		return true;
+	case F2FS_IPU_SSR:
+		if (need_SSR(sbi))
+			return true;
+		break;
+	case F2FS_IPU_UTIL:
+		if (utilization(sbi) > SM_I(sbi)->min_ipu_util)
+			return true;
+		break;
+	case F2FS_IPU_SSR_UTIL:
+		if (need_SSR(sbi) && utilization(sbi) > SM_I(sbi)->min_ipu_util)
+			return true;
+		break;
+	case F2FS_IPU_DISABLE:
+		break;
+	}
 	return false;
 }
 

commit cfb271d485d0ec31eb92b51f4fbe54bf6542e8e6
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Dec 5 17:15:22 2013 +0800

    f2fs: add unlikely() macro for compiler optimization
    
    As we know, some of our branch condition will rarely be true. So we could add
    'unlikely' to let compiler optimize these code, by this way we could drop
    unneeded 'jump' assemble code to improve performance.
    
    change log:
     o add *unlikely* as many as possible across the whole source files at once
       suggested by Jaegeuk Kim.
    
    Suggested-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 26812fc0fa12..ea563760f4b7 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -457,7 +457,7 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
 	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
 	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
 
-	if (sbi->por_doing)
+	if (unlikely(sbi->por_doing))
 		return false;
 
 	return ((free_sections(sbi) + freed) <= (node_secs + 2 * dent_secs +

commit 187b5b8b3dfcfc73126f2743c89cc47df3bf07be
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Sat Nov 30 10:10:31 2013 +0900

    f2fs: remove the own bi_private allocation
    
    Previously f2fs allocates its own bi_private data structure all the time even
    though we don't use it. But, can we remove this bi_private allocation?
    
    This patch removes such the additional bi_private allocation.
    
    1. Retrieve f2fs_sb_info from its page->mapping->host->i_sb.
     - This removes the usecases of bi_private in end_io.
    
    2. Use bi_private only when we really need it.
     - The bi_private is used only when the checkpoint procedure is conducted.
     - When conducting the checkpoint, f2fs submits a META_FLUSH bio to wait its bio
    completion.
     - Since we have no dependancies to remove bi_private now, let's just use
     bi_private pointer as the completion pointer.
    
    Reviewed-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 7fea2ee8a5da..26812fc0fa12 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -92,13 +92,6 @@
 #define MAX_BIO_BLOCKS(max_hw_blocks)					\
 	(min((int)max_hw_blocks, BIO_MAX_PAGES))
 
-/* during checkpoint, bio_private is used to synchronize the last bio */
-struct bio_private {
-	struct f2fs_sb_info *sbi;
-	bool is_sync;
-	struct completion *wait;
-};
-
 /*
  * indicate a block allocation direction: RIGHT and LEFT.
  * RIGHT means allocating new sections towards the end of volume.

commit aac44046a2cdf70474fee8931bc8a591fd16c8dc
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Thu Nov 28 15:41:39 2013 +0800

    f2fs: correct type of wait in struct bio_private
    
    The void *wait in bio_private is used for waiting completion of checkpoint bio.
    So we don't need to use its type as void, but declare it as completion type.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    [Jaegeuk Kim: add description]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 07887e1cc704..7fea2ee8a5da 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -96,7 +96,7 @@
 struct bio_private {
 	struct f2fs_sb_info *sbi;
 	bool is_sync;
-	void *wait;
+	struct completion *wait;
 };
 
 /*

commit f9a4e6df52edf8ce1040d1b8d340d31234a1bce3
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Nov 28 12:44:05 2013 +0900

    f2fs: bug fix on bit overflow from 32bits to 64bits
    
    This patch fixes some bit overflows by the shift operations.
    
    Dan Carpenter reported potential bugs on bit overflows as follows.
    
    fs/f2fs/segment.c:910 submit_write_page()
            warn: should 'blk_addr << ((sbi)->log_blocksize - 9)' be a 64 bit type?
    fs/f2fs/checkpoint.c:429 get_valid_checkpoint()
            warn: should '1 << ()' be a 64 bit type?
    fs/f2fs/data.c:408 f2fs_readpage()
            warn: should 'blk_addr << ((sbi)->log_blocksize - 9)' be a 64 bit type?
    fs/f2fs/data.c:457 submit_read_page()
            warn: should 'blk_addr << ((sbi)->log_blocksize - 9)' be a 64 bit type?
    fs/f2fs/data.c:525 get_data_block_ro()
            warn: should 'i << blkbits' be a 64 bit type?
    
    Bug-Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index b84dd2396665..07887e1cc704 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -86,9 +86,9 @@
 #define TOTAL_SECS(sbi)	(sbi->total_sections)
 
 #define SECTOR_FROM_BLOCK(sbi, blk_addr)				\
-	(blk_addr << ((sbi)->log_blocksize - F2FS_LOG_SECTOR_SIZE))
+	(((sector_t)blk_addr) << (sbi)->log_sectors_per_block)
 #define SECTOR_TO_BLOCK(sbi, sectors)					\
-	(sectors >> ((sbi)->log_blocksize - F2FS_LOG_SECTOR_SIZE))
+	(sectors >> (sbi)->log_sectors_per_block)
 #define MAX_BIO_BLOCKS(max_hw_blocks)					\
 	(min((int)max_hw_blocks, BIO_MAX_PAGES))
 

commit 74de593af77b109f202c47e090c9e134c8882869
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Fri Nov 22 09:09:59 2013 +0800

    f2fs: read contiguous sit entry pages by merging for mount performance
    
    Previously we read sit entries page one by one, this method lost the chance
    of reading contiguous page together. So we read pages as contiguous as
    possible for better mount performance.
    
    change log:
     o merge judgements/use 'Continue' or 'Break' instead of 'Goto' as Gu Zheng
       suggested.
     o add mark_page_accessed() before release page to delay VM reclaiming.
     o remove '*order' for simplification of function as Jaegeuk Kim suggested.
    
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    [Jaegeuk Kim: fix a bug on the block address calculation]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 38f6196493ff..b84dd2396665 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -78,6 +78,8 @@
 	(segno / SIT_ENTRY_PER_BLOCK)
 #define	START_SEGNO(sit_i, segno)		\
 	(SIT_BLOCK_OFFSET(sit_i, segno) * SIT_ENTRY_PER_BLOCK)
+#define SIT_BLK_CNT(sbi)			\
+	((TOTAL_SEGS(sbi) + SIT_ENTRY_PER_BLOCK - 1) / SIT_ENTRY_PER_BLOCK)
 #define f2fs_bitmap_size(nr)			\
 	(BITS_TO_LONGS(nr) * sizeof(unsigned long))
 #define TOTAL_SEGS(sbi)	(SM_I(sbi)->main_segments)

commit 61ae45c8803262a96f3af200489344f339992291
Author: Changman Lee <cm224.lee@samsung.com>
Date:   Thu Nov 21 20:04:21 2013 +0900

    f2fs: simplify IS_DATASEG and IS_NODESEG macro
    
    It is not efficient comparing each segment type to find node or data.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    [Jaegeuk Kim: remove unnecessary white spaces]
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 269f690b4e24..38f6196493ff 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -20,13 +20,8 @@
 #define GET_L2R_SEGNO(free_i, segno)	(segno - free_i->start_segno)
 #define GET_R2L_SEGNO(free_i, segno)	(segno + free_i->start_segno)
 
-#define IS_DATASEG(t)							\
-	((t == CURSEG_HOT_DATA) || (t == CURSEG_COLD_DATA) ||		\
-	(t == CURSEG_WARM_DATA))
-
-#define IS_NODESEG(t)							\
-	((t == CURSEG_HOT_NODE) || (t == CURSEG_COLD_NODE) ||		\
-	(t == CURSEG_WARM_NODE))
+#define IS_DATASEG(t)	(t <= CURSEG_COLD_DATA)
+#define IS_NODESEG(t)	(t >= CURSEG_HOT_NODE)
 
 #define IS_CURSEG(sbi, seg)						\
 	((seg == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno) ||	\

commit 44c60bf2b9f2808e127fa4cd3b11b50a46cf5817
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Tue Oct 29 14:50:40 2013 +0800

    f2fs: check all ones or zeros bitmap with bitops for better mount performance
    
    Previously, check_block_count check valid_map with bit data type in common
    scenario that sit has all ones or zeros bitmap, it makes low mount performance.
    So let's check the special bitmap with integer data type instead of the bit one.
    
    v1-->v2:
     o use find_next_{zero_}bit_le for better performance and readable as Jaegeuk
       suggested.
     o use neat logogram in comment as Gu Zheng suggested.
     o search continuous ones or zeros for better performance when checking mixed
       bitmap.
    
    Suggested-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Signed-off-by: Shu Tan <shu.tan@samsung.com>
    Signed-off-by: Chao Yu <chao2.yu@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 001d4c4d97fd..269f690b4e24 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -547,8 +547,9 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 {
 	struct f2fs_sm_info *sm_info = SM_I(sbi);
 	unsigned int end_segno = sm_info->segment_count - 1;
+	bool is_valid  = test_bit_le(0, raw_sit->valid_map) ? true : false;
 	int valid_blocks = 0;
-	int i;
+	int cur_pos = 0, next_pos;
 
 	/* check segment usage */
 	BUG_ON(GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg);
@@ -557,9 +558,19 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 	BUG_ON(segno > end_segno);
 
 	/* check bitmap with valid block count */
-	for (i = 0; i < sbi->blocks_per_seg; i++)
-		if (f2fs_test_bit(i, raw_sit->valid_map))
-			valid_blocks++;
+	do {
+		if (is_valid) {
+			next_pos = find_next_zero_bit_le(&raw_sit->valid_map,
+					sbi->blocks_per_seg,
+					cur_pos);
+			valid_blocks += next_pos - cur_pos;
+		} else
+			next_pos = find_next_bit_le(&raw_sit->valid_map,
+					sbi->blocks_per_seg,
+					cur_pos);
+		cur_pos = next_pos;
+		is_valid = !is_valid;
+	} while (cur_pos < sbi->blocks_per_seg);
 	BUG_ON(GET_SIT_VBLOCKS(raw_sit) != valid_blocks);
 }
 #else

commit 5d56b6718a0f4e5c58cdd3cb6b7a472d7c5671b9
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Oct 29 15:14:54 2013 +0900

    f2fs: add an option to avoid unnecessary BUG_ONs
    
    If you want to remove unnecessary BUG_ONs, you can just turn off F2FS_CHECK_FS
    in your kernel config.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index abe7094c4f7a..001d4c4d97fd 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -522,16 +522,13 @@ static inline unsigned short curseg_blkoff(struct f2fs_sb_info *sbi, int type)
 	return curseg->next_blkoff;
 }
 
+#ifdef CONFIG_F2FS_CHECK_FS
 static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
 {
 	unsigned int end_segno = SM_I(sbi)->segment_count - 1;
 	BUG_ON(segno > end_segno);
 }
 
-/*
- * This function is used for only debugging.
- * NOTE: In future, we have to remove this function.
- */
 static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
 {
 	struct f2fs_sm_info *sm_info = SM_I(sbi);
@@ -565,6 +562,11 @@ static inline void check_block_count(struct f2fs_sb_info *sbi,
 			valid_blocks++;
 	BUG_ON(GET_SIT_VBLOCKS(raw_sit) != valid_blocks);
 }
+#else
+#define check_seg_range(sbi, segno)
+#define verify_block_addr(sbi, blk_addr)
+#define check_block_count(sbi, segno, raw_sit)
+#endif
 
 static inline pgoff_t current_sit_addr(struct f2fs_sb_info *sbi,
 						unsigned int start)

commit 81eb8d6e2869b119d4a7b8c02091c3779733a3ac
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Oct 24 13:31:34 2013 +0900

    f2fs: reclaim prefree segments periodically
    
    Previously, f2fs postpones reclaiming prefree segments into free segments
    as much as possible.
    However, if user writes and deletes a bunch of data without any sync or fsync
    calls, some flash storages can suffer from garbage collections.
    
    So, this patch adds the reclaiming codes to f2fs_write_node_pages and background
    GC thread.
    
    If there are a lot of prefree segments, let's do checkpoint so that f2fs
    submits discard commands for the prefree regions to the flash storage.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 7f94d78cda3d..abe7094c4f7a 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -14,6 +14,8 @@
 #define NULL_SEGNO			((unsigned int)(~0))
 #define NULL_SECNO			((unsigned int)(~0))
 
+#define DEF_RECLAIM_PREFREE_SEGMENTS	100	/* 200MB of prefree segments */
+
 /* L: Logical segment # in volume, R: Relative segment # in main area */
 #define GET_L2R_SEGNO(free_i, segno)	(segno - free_i->start_segno)
 #define GET_R2L_SEGNO(free_i, segno)	(segno + free_i->start_segno)
@@ -472,6 +474,11 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
 						reserved_sections(sbi)));
 }
 
+static inline bool excess_prefree_segs(struct f2fs_sb_info *sbi)
+{
+	return (prefree_segments(sbi) > SM_I(sbi)->rec_prefree_segments);
+}
+
 static inline int utilization(struct f2fs_sb_info *sbi)
 {
 	return div_u64((u64)valid_user_blocks(sbi) * 100, sbi->user_block_count);

commit cc7b1bb173676621b092b61d22d8d12b05efb5e8
Author: Chao Yu <chao2.yu@samsung.com>
Date:   Sun Sep 22 15:50:50 2013 +0800

    f2fs: avoid allocating failure in bio_alloc
    
    This patch add macro MAX_BIO_BLOCKS to limit value of npages in
    f2fs_bio_alloc, it can avoid allocating failure in bio_alloc caused by
    npages is larger than BIO_MAX_PAGES.
    
    Signed-off-by: Yu Chao <chao2.yu@samsung.com>
    Reviewed-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index bdd10eab8c40..7f94d78cda3d 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -90,6 +90,8 @@
 	(blk_addr << ((sbi)->log_blocksize - F2FS_LOG_SECTOR_SIZE))
 #define SECTOR_TO_BLOCK(sbi, sectors)					\
 	(sectors >> ((sbi)->log_blocksize - F2FS_LOG_SECTOR_SIZE))
+#define MAX_BIO_BLOCKS(max_hw_blocks)					\
+	(min((int)max_hw_blocks, BIO_MAX_PAGES))
 
 /* during checkpoint, bio_private is used to synchronize the last bio */
 struct bio_private {

commit a26b7c8a0149ce1e3b6a10f2801aada6e447e4e7
Author: Jin Xu <jinuxstyle@gmail.com>
Date:   Thu Sep 5 12:45:26 2013 +0800

    f2fs: optimize gc for better performance
    
    This patch improves the gc efficiency by optimizing the victim
    selection policy. With this optimization, the random re-write
    performance could increase up to 20%.
    
    For f2fs, when disk is in shortage of free spaces, gc will selects
    dirty segments and moves valid blocks around for making more space
    available. The gc cost of a segment is determined by the valid blocks
    in the segment. The less the valid blocks, the higher the efficiency.
    The ideal victim segment is the one that has the most garbage blocks.
    
    Currently, it searches up to 20 dirty segments for a victim segment.
    The selected victim is not likely the best victim for gc when there
    are much more dirty segments. Why not searching more dirty segments
    for a better victim? The cost of searching dirty segments is
    negligible in comparison to moving blocks.
    
    In this patch, it enlarges the MAX_VICTIM_SEARCH to 4096 to make
    the search more aggressively for a possible better victim. Since
    it also applies to victim selection for SSR, it will likely improve
    the SSR efficiency as well.
    
    The test case is simple. It creates as many files until the disk full.
    The size for each file is 32KB. Then it writes as many as 100000
    records of 4KB size to random offsets of random files in sync mode.
    The testing was done on a 2GB partition of a SDHC card. Let's see the
    test result of f2fs without and with the patch.
    
    ---------------------------------------
    2GB partition, SDHC
    create 52023 files of size 32768 bytes
    random re-write 100000 records of 4KB
    ---------------------------------------
    | file creation (s) | rewrite time (s) | gc count | gc garbage blocks |
    [no patch]  341         4227             1174          174840
    [patched]   324         2958             645           106682
    
    It's obvious that, with the patch, f2fs finishes the test in 20+% less
    time than without the patch. And internally it does much less gc with
    higher efficiency than before.
    
    Since the performance improvement is related to gc, it might not be so
    obvious for other tests that do not trigger gc as often as this one (
    This is because f2fs selects dirty segments for SSR use most of the
    time when free space is in shortage). The well-known iozone test tool
    was not used for benchmarking the patch becuase it seems do not have
    a test case that performs random re-write on a full disk.
    
    This patch is the revised version based on the suggestion from
    Jaegeuk Kim.
    
    Signed-off-by: Jin Xu <jinuxstyle@gmail.com>
    [Jaegeuk Kim: suggested simpler solution]
    Reviewed-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index e0d6d3abf396..bdd10eab8c40 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -142,6 +142,7 @@ struct victim_sel_policy {
 	int alloc_mode;			/* LFS or SSR */
 	int gc_mode;			/* GC_CB or GC_GREEDY */
 	unsigned long *dirty_segmap;	/* dirty segment bitmap */
+	unsigned int max_search;	/* maximum # of segments to search */
 	unsigned int offset;		/* last scanned bitmap offset */
 	unsigned int ofs_unit;		/* bitmap search unit */
 	unsigned int min_cost;		/* minimum cost */

commit 222cbdc4835f8151b886b049d6ad56b18f88d470
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Sep 3 13:41:37 2013 +0900

    f2fs: avoid an overflow during utilization calculation
    
    The current f2fs uses all the block counts with 32 bit numbers, which is able to
    cover about 15TB volume.
    
    But in calculation of utilization, f2fs multiplies the count by 100 which can
    induce overflow.
    This patch fixes this.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index cd9bd93212ad..e0d6d3abf396 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -471,7 +471,7 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
 
 static inline int utilization(struct f2fs_sb_info *sbi)
 {
-	return div_u64(valid_user_blocks(sbi) * 100, sbi->user_block_count);
+	return div_u64((u64)valid_user_blocks(sbi) * 100, sbi->user_block_count);
 }
 
 /*

commit c34e333fd5ca41317c3dab69ed8a212acacd4aeb
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Tue Sep 3 09:46:45 2013 +0900

    f2fs: trigger GC when there are prefree segments
    
    Previously, f2fs conducts SSR when free_sections() < overprovision_sections.
    But, even though there are a lot of prefree segments, it can consider SSR only.
    So, let's consider the number of prefree segments too for triggering SSR.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 062424a0e4c3..cd9bd93212ad 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -453,7 +453,8 @@ static inline int reserved_sections(struct f2fs_sb_info *sbi)
 
 static inline bool need_SSR(struct f2fs_sb_info *sbi)
 {
-	return (free_sections(sbi) < overprovision_sections(sbi));
+	return ((prefree_segments(sbi) / sbi->segs_per_sec)
+			+ free_sections(sbi) < overprovision_sections(sbi));
 }
 
 static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)

commit ac5d156c78a68b39955ee9b09498ba93831c77d7
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Apr 29 16:58:39 2013 +0900

    f2fs: modify the number of issued pages to merge IOs
    
    When testing f2fs on an SSD, I found some 128 page IOs followed by 1 page IO
    were issued by f2fs_write_node_pages.
    This means that there were some mishandling flows which degrades performance.
    
    Previous f2fs_write_node_pages determines the number of pages to be written,
    nr_to_write, as follows.
    
    1. The bio_get_nr_vecs returns 129 pages.
    2. The bio_alloc makes a room for 128 pages.
    3. The initial 128 pages go into one bio.
    4. The existing bio is submitted, and a new bio is prepared for the last 1 page.
    5. Finally, sync_node_pages submits the last 1 page bio.
    
    The problem is from the use of bio_get_nr_vecs, so this patch replace it
    with max_hw_blocks using queue_max_sectors.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 26fc0540f144..062424a0e4c3 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -8,6 +8,8 @@
  * it under the terms of the GNU General Public License version 2 as
  * published by the Free Software Foundation.
  */
+#include <linux/blkdev.h>
+
 /* constant macro */
 #define NULL_SEGNO			((unsigned int)(~0))
 #define NULL_SECNO			((unsigned int)(~0))
@@ -86,6 +88,8 @@
 
 #define SECTOR_FROM_BLOCK(sbi, blk_addr)				\
 	(blk_addr << ((sbi)->log_blocksize - F2FS_LOG_SECTOR_SIZE))
+#define SECTOR_TO_BLOCK(sbi, sectors)					\
+	(sectors >> ((sbi)->log_blocksize - F2FS_LOG_SECTOR_SIZE))
 
 /* during checkpoint, bio_private is used to synchronize the last bio */
 struct bio_private {
@@ -624,3 +628,10 @@ static inline bool sec_usage_check(struct f2fs_sb_info *sbi, unsigned int secno)
 		return true;
 	return false;
 }
+
+static inline unsigned int max_hw_blocks(struct f2fs_sb_info *sbi)
+{
+	struct block_device *bdev = sbi->sb->s_bdev;
+	struct request_queue *q = bdev_get_queue(bdev);
+	return SECTOR_TO_BLOCK(sbi, queue_max_sectors(q));
+}

commit 6224da875ee0cb702c6ffe3456307701106dffb5
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sat Apr 6 14:44:32 2013 +0900

    f2fs: fix typo mistakes
    
    Fix typo mistakes.
    1. I think that it should be 'L' instead of 'V'.
    2. and try to fix 'Front' instead of 'Frone'
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 994bb7bd7b70..26fc0540f144 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -12,7 +12,7 @@
 #define NULL_SEGNO			((unsigned int)(~0))
 #define NULL_SECNO			((unsigned int)(~0))
 
-/* V: Logical segment # in volume, R: Relative segment # in main area */
+/* L: Logical segment # in volume, R: Relative segment # in main area */
 #define GET_L2R_SEGNO(free_i, segno)	(segno - free_i->start_segno)
 #define GET_R2L_SEGNO(free_i, segno)	(segno + free_i->start_segno)
 

commit 5ec4e49f9bd753e2a6857a96e01f8ae5ff00b459
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Sun Mar 31 13:26:03 2013 +0900

    f2fs: change GC bitmaps to apply the section granularity
    
    This patch removes a bitmap for victim segments selected by foreground GC, and
    modifies the other bitmap for victim segments selected by background GC.
    
    1) foreground GC bitmap
     : We don't need to manage this, since we just only one previous victim section
       number instead of the whole victim history.
       The f2fs uses the victim section number in order not to allocate currently
       GC'ed section to current active logs.
    
    2) background GC bitmap
     : This bitmap is used to avoid selecting victims repeatedly by background GCs.
       In addition, the victims are able to be selected by foreground GCs, since
       there is no need to read victim blocks during foreground GCs.
    
       By the fact that the foreground GC reclaims segments in a section unit, it'd
       be better to manage this bitmap based on the section granularity.
    
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index fea9245d4774..994bb7bd7b70 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -10,6 +10,7 @@
  */
 /* constant macro */
 #define NULL_SEGNO			((unsigned int)(~0))
+#define NULL_SECNO			((unsigned int)(~0))
 
 /* V: Logical segment # in volume, R: Relative segment # in main area */
 #define GET_L2R_SEGNO(free_i, segno)	(segno - free_i->start_segno)
@@ -214,7 +215,7 @@ struct dirty_seglist_info {
 	unsigned long *dirty_segmap[NR_DIRTY_TYPE];
 	struct mutex seglist_lock;		/* lock for segment bitmaps */
 	int nr_dirty[NR_DIRTY_TYPE];		/* # of dirty segments */
-	unsigned long *victim_segmap[2];	/* BG_GC, FG_GC */
+	unsigned long *victim_secmap;		/* background GC victims */
 };
 
 /* victim selection function for cleaning and SSR */
@@ -616,3 +617,10 @@ static inline block_t sum_blk_addr(struct f2fs_sb_info *sbi, int base, int type)
 		le32_to_cpu(F2FS_CKPT(sbi)->cp_pack_total_block_count)
 				- (base + 1) + type;
 }
+
+static inline bool sec_usage_check(struct f2fs_sb_info *sbi, unsigned int secno)
+{
+	if (IS_CURSEC(sbi, secno) || (sbi->cur_victim_sec == secno))
+		return true;
+	return false;
+}

commit 53cf95222fad7a962cc03fb61a33e37bcf4f5c9d
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Sun Mar 31 12:39:49 2013 +0900

    f2fs: introduce TOTAL_SECS macro
    
    Let's use a macro to get the total number of sections.
    
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index c0d774076ab9..fea9245d4774 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -81,6 +81,7 @@
 #define f2fs_bitmap_size(nr)			\
 	(BITS_TO_LONGS(nr) * sizeof(unsigned long))
 #define TOTAL_SEGS(sbi)	(SM_I(sbi)->main_segments)
+#define TOTAL_SECS(sbi)	(sbi->total_sections)
 
 #define SECTOR_FROM_BLOCK(sbi, blk_addr)				\
 	(blk_addr << ((sbi)->log_blocksize - F2FS_LOG_SECTOR_SIZE))

commit 5c773ba33a29c0dcddac7cfaa39fc63a7137130d
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Sun Mar 31 12:30:04 2013 +0900

    f2fs: do not use duplicate names in a macro
    
    A macro should not use duplicate parameter names.
    
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index e399bd4d3af8..c0d774076ab9 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -23,13 +23,13 @@
 	((t == CURSEG_HOT_NODE) || (t == CURSEG_COLD_NODE) ||		\
 	(t == CURSEG_WARM_NODE))
 
-#define IS_CURSEG(sbi, segno)						\
-	((segno == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno) ||	\
-	 (segno == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno) ||	\
-	 (segno == CURSEG_I(sbi, CURSEG_COLD_DATA)->segno) ||	\
-	 (segno == CURSEG_I(sbi, CURSEG_HOT_NODE)->segno) ||	\
-	 (segno == CURSEG_I(sbi, CURSEG_WARM_NODE)->segno) ||	\
-	 (segno == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno))
+#define IS_CURSEG(sbi, seg)						\
+	((seg == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno) ||	\
+	 (seg == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno) ||	\
+	 (seg == CURSEG_I(sbi, CURSEG_COLD_DATA)->segno) ||	\
+	 (seg == CURSEG_I(sbi, CURSEG_HOT_NODE)->segno) ||	\
+	 (seg == CURSEG_I(sbi, CURSEG_WARM_NODE)->segno) ||	\
+	 (seg == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno))
 
 #define IS_CURSEC(sbi, secno)						\
 	((secno == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno /		\

commit 12fc760fd632a96f49e96f519c4aed4eb279bb61
Author: Changman Lee <cm224.lee@samsung.com>
Date:   Mon Feb 25 17:38:02 2013 +0900

    f2fs: fix overflow when calculating utilization on 32-bit
    
    Use div_u64 to fix overflow when calculating utilization.
    *long int* is 4-bytes on 32-bit so (user blocks * 100) might be
    overflow if disk size is over e.g. 512GB.
    
    Signed-off-by: Changman Lee <cm224.lee@samsung.com>
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 552dadbb2327..e399bd4d3af8 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -464,8 +464,7 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
 
 static inline int utilization(struct f2fs_sb_info *sbi)
 {
-	return (long int)valid_user_blocks(sbi) * 100 /
-			(long int)sbi->user_block_count;
+	return div_u64(valid_user_blocks(sbi) * 100, sbi->user_block_count);
 }
 
 /*

commit 437275272f9e635673f065300e5d95226a25cb06
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Feb 4 15:11:17 2013 +0900

    f2fs: clarify and enhance the f2fs_gc flow
    
    This patch makes clearer the ambiguous f2fs_gc flow as follows.
    
    1. Remove intermediate checkpoint condition during f2fs_gc
     (i.e., should_do_checkpoint() and GC_BLOCKED)
    
    2. Remove unnecessary return values of f2fs_gc because of #1.
     (i.e., GC_NODE, GC_OK, etc)
    
    3. Simplify write_checkpoint() because of #2.
    
    4. Clarify the main f2fs_gc flow.
     o monitor how many freed sections during one iteration of do_garbage_collect().
     o do GC more without checkpoints if we can't get enough free sections.
     o do checkpoint once we've got enough free sections through forground GCs.
    
    5. Adopt thread-logging (Slack-Space-Recycle) scheme more aggressively on data
      log types. See. get_ssr_segement()
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 458bf5c726f7..552dadbb2327 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -450,21 +450,15 @@ static inline bool need_SSR(struct f2fs_sb_info *sbi)
 	return (free_sections(sbi) < overprovision_sections(sbi));
 }
 
-static inline int get_ssr_segment(struct f2fs_sb_info *sbi, int type)
-{
-	struct curseg_info *curseg = CURSEG_I(sbi, type);
-	return DIRTY_I(sbi)->v_ops->get_victim(sbi,
-				&(curseg)->next_segno, BG_GC, type, SSR);
-}
-
-static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi)
+static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi, int freed)
 {
 	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
 	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
+
 	if (sbi->por_doing)
 		return false;
 
-	return (free_sections(sbi) <= (node_secs + 2 * dent_secs +
+	return ((free_sections(sbi) + freed) <= (node_secs + 2 * dent_secs +
 						reserved_sections(sbi)));
 }
 

commit b1f1daf8c72d615b64163e26488d8effeed29b60
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sat Feb 2 23:53:15 2013 +0900

    f2fs: optimize the return condition for has_not_enough_free_secs
    
    Instead of evaluating the free_sections and then deciding to return
    true/false from that path. We can directly use the evaluation condition
    for returning proper value.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index a9417b94d6bc..458bf5c726f7 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -464,10 +464,8 @@ static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi)
 	if (sbi->por_doing)
 		return false;
 
-	if (free_sections(sbi) <= (node_secs + 2 * dent_secs +
-						reserved_sections(sbi)))
-		return true;
-	return false;
+	return (free_sections(sbi) <= (node_secs + 2 * dent_secs +
+						reserved_sections(sbi)));
 }
 
 static inline int utilization(struct f2fs_sb_info *sbi)

commit 5ac206cf4f9aad871aa1f1dd653a3404092db56c
Author: Namjae Jeon <namjae.jeon@samsung.com>
Date:   Sat Feb 2 23:52:59 2013 +0900

    f2fs: make an accessor to get sections for particular block type
    
    Introduce accessor to get the sections based upon the block type
    (node,dents...) and modify the functions : should_do_checkpoint,
    has_not_enough_free_secs to use this accessor function to get
    the node sections and dent sections.
    
    Signed-off-by: Namjae Jeon <namjae.jeon@samsung.com>
    Signed-off-by: Amit Sahrawat <a.sahrawat@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 66a288a52fd3..a9417b94d6bc 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -459,13 +459,8 @@ static inline int get_ssr_segment(struct f2fs_sb_info *sbi, int type)
 
 static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi)
 {
-	unsigned int pages_per_sec = (1 << sbi->log_blocks_per_seg) *
-			sbi->segs_per_sec;
-	int node_secs = ((get_pages(sbi, F2FS_DIRTY_NODES) + pages_per_sec - 1)
-			>> sbi->log_blocks_per_seg) / sbi->segs_per_sec;
-	int dent_secs = ((get_pages(sbi, F2FS_DIRTY_DENTS) + pages_per_sec - 1)
-			>> sbi->log_blocks_per_seg) / sbi->segs_per_sec;
-
+	int node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);
+	int dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);
 	if (sbi->por_doing)
 		return false;
 

commit 029cd28c1f739bbfc5105035696d5f1f4e45d161
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Fri Dec 21 17:20:21 2012 +0900

    f2fs: fix equation of has_not_enough_free_secs()
    
    Practically, has_not_enough_free_secs() should calculate with the numbers of
    current node and directory data blocks together.
    Actually the equation was implemented in need_to_flush().
    
    So, this patch removes need_flush() and moves the equation into
    has_not_enough_free_secs().
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 0948405af6f5..66a288a52fd3 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -459,7 +459,20 @@ static inline int get_ssr_segment(struct f2fs_sb_info *sbi, int type)
 
 static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi)
 {
-	return free_sections(sbi) <= reserved_sections(sbi);
+	unsigned int pages_per_sec = (1 << sbi->log_blocks_per_seg) *
+			sbi->segs_per_sec;
+	int node_secs = ((get_pages(sbi, F2FS_DIRTY_NODES) + pages_per_sec - 1)
+			>> sbi->log_blocks_per_seg) / sbi->segs_per_sec;
+	int dent_secs = ((get_pages(sbi, F2FS_DIRTY_DENTS) + pages_per_sec - 1)
+			>> sbi->log_blocks_per_seg) / sbi->segs_per_sec;
+
+	if (sbi->por_doing)
+		return false;
+
+	if (free_sections(sbi) <= (node_secs + 2 * dent_secs +
+						reserved_sections(sbi)))
+		return true;
+	return false;
 }
 
 static inline int utilization(struct f2fs_sb_info *sbi)

commit 3cd8a23948b29301f8f67b8d70c5c18fabbc05e1
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Mon Dec 10 09:26:05 2012 +0900

    f2fs: cleanup the f2fs_bio_alloc routine
    
    Do cleanup more for better code readability.
    
    - Change the parameter set of f2fs_bio_alloc()
      This function should allocate a bio only since it is not something like
      f2fs_bio_init(). Instead, the caller should initialize the allocated bio.
    
    - Introduce SECTOR_FROM_BLOCK
      This macro translates a block address to its sector address.
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>
    Reviewed-by: Namjae Jeon <namjae.jeon@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 2c445f8947c9..0948405af6f5 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -82,6 +82,9 @@
 	(BITS_TO_LONGS(nr) * sizeof(unsigned long))
 #define TOTAL_SEGS(sbi)	(SM_I(sbi)->main_segments)
 
+#define SECTOR_FROM_BLOCK(sbi, blk_addr)				\
+	(blk_addr << ((sbi)->log_blocksize - F2FS_LOG_SECTOR_SIZE))
+
 /* during checkpoint, bio_private is used to synchronize the last bio */
 struct bio_private {
 	struct f2fs_sb_info *sbi;

commit 0a8165d7c2cf1395059db20ab07665baf3758fcd
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Thu Nov 29 13:28:09 2012 +0900

    f2fs: adjust kernel coding style
    
    As pointed out by Randy Dunlap, this patch removes all usage of "/**" for comment
    blocks. Instead, just use "/*".
    
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index e380a8ef13f5..2c445f8947c9 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -1,4 +1,4 @@
-/**
+/*
  * fs/f2fs/segment.h
  *
  * Copyright (c) 2012 Samsung Electronics Co., Ltd.

commit 39a53e0ce0df01b3cf4bb898c7ae2fd2189647d5
Author: Jaegeuk Kim <jaegeuk.kim@samsung.com>
Date:   Wed Nov 28 13:37:31 2012 +0900

    f2fs: add superblock and major in-memory structure
    
    This adds the following major in-memory structures in f2fs.
    
    - f2fs_sb_info:
      contains f2fs-specific information, two special inode pointers for node and
      meta address spaces, and orphan inode management.
    
    - f2fs_inode_info:
      contains vfs_inode and other fs-specific information.
    
    - f2fs_nm_info:
      contains node manager information such as NAT entry cache, free nid list,
      and NAT page management.
    
    - f2fs_node_info:
      represents a node as node id, inode number, block address, and its version.
    
    - f2fs_sm_info:
      contains segment manager information such as SIT entry cache, free segment
      map, current active logs, dirty segment management, and segment utilization.
      The specific structures are sit_info, free_segmap_info, dirty_seglist_info,
      curseg_info.
    
    In addition, add F2FS_SUPER_MAGIC in magic.h.
    
    Signed-off-by: Chul Lee <chur.lee@samsung.com>
    Signed-off-by: Jaegeuk Kim <jaegeuk.kim@samsung.com>

diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
new file mode 100644
index 000000000000..e380a8ef13f5
--- /dev/null
+++ b/fs/f2fs/segment.h
@@ -0,0 +1,615 @@
+/**
+ * fs/f2fs/segment.h
+ *
+ * Copyright (c) 2012 Samsung Electronics Co., Ltd.
+ *             http://www.samsung.com/
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+/* constant macro */
+#define NULL_SEGNO			((unsigned int)(~0))
+
+/* V: Logical segment # in volume, R: Relative segment # in main area */
+#define GET_L2R_SEGNO(free_i, segno)	(segno - free_i->start_segno)
+#define GET_R2L_SEGNO(free_i, segno)	(segno + free_i->start_segno)
+
+#define IS_DATASEG(t)							\
+	((t == CURSEG_HOT_DATA) || (t == CURSEG_COLD_DATA) ||		\
+	(t == CURSEG_WARM_DATA))
+
+#define IS_NODESEG(t)							\
+	((t == CURSEG_HOT_NODE) || (t == CURSEG_COLD_NODE) ||		\
+	(t == CURSEG_WARM_NODE))
+
+#define IS_CURSEG(sbi, segno)						\
+	((segno == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno) ||	\
+	 (segno == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno) ||	\
+	 (segno == CURSEG_I(sbi, CURSEG_COLD_DATA)->segno) ||	\
+	 (segno == CURSEG_I(sbi, CURSEG_HOT_NODE)->segno) ||	\
+	 (segno == CURSEG_I(sbi, CURSEG_WARM_NODE)->segno) ||	\
+	 (segno == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno))
+
+#define IS_CURSEC(sbi, secno)						\
+	((secno == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno /		\
+	  sbi->segs_per_sec) ||	\
+	 (secno == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno /		\
+	  sbi->segs_per_sec) ||	\
+	 (secno == CURSEG_I(sbi, CURSEG_COLD_DATA)->segno /		\
+	  sbi->segs_per_sec) ||	\
+	 (secno == CURSEG_I(sbi, CURSEG_HOT_NODE)->segno /		\
+	  sbi->segs_per_sec) ||	\
+	 (secno == CURSEG_I(sbi, CURSEG_WARM_NODE)->segno /		\
+	  sbi->segs_per_sec) ||	\
+	 (secno == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno /		\
+	  sbi->segs_per_sec))	\
+
+#define START_BLOCK(sbi, segno)						\
+	(SM_I(sbi)->seg0_blkaddr +					\
+	 (GET_R2L_SEGNO(FREE_I(sbi), segno) << sbi->log_blocks_per_seg))
+#define NEXT_FREE_BLKADDR(sbi, curseg)					\
+	(START_BLOCK(sbi, curseg->segno) + curseg->next_blkoff)
+
+#define MAIN_BASE_BLOCK(sbi)	(SM_I(sbi)->main_blkaddr)
+
+#define GET_SEGOFF_FROM_SEG0(sbi, blk_addr)				\
+	((blk_addr) - SM_I(sbi)->seg0_blkaddr)
+#define GET_SEGNO_FROM_SEG0(sbi, blk_addr)				\
+	(GET_SEGOFF_FROM_SEG0(sbi, blk_addr) >> sbi->log_blocks_per_seg)
+#define GET_SEGNO(sbi, blk_addr)					\
+	(((blk_addr == NULL_ADDR) || (blk_addr == NEW_ADDR)) ?		\
+	NULL_SEGNO : GET_L2R_SEGNO(FREE_I(sbi),			\
+		GET_SEGNO_FROM_SEG0(sbi, blk_addr)))
+#define GET_SECNO(sbi, segno)					\
+	((segno) / sbi->segs_per_sec)
+#define GET_ZONENO_FROM_SEGNO(sbi, segno)				\
+	((segno / sbi->segs_per_sec) / sbi->secs_per_zone)
+
+#define GET_SUM_BLOCK(sbi, segno)				\
+	((sbi->sm_info->ssa_blkaddr) + segno)
+
+#define GET_SUM_TYPE(footer) ((footer)->entry_type)
+#define SET_SUM_TYPE(footer, type) ((footer)->entry_type = type)
+
+#define SIT_ENTRY_OFFSET(sit_i, segno)					\
+	(segno % sit_i->sents_per_block)
+#define SIT_BLOCK_OFFSET(sit_i, segno)					\
+	(segno / SIT_ENTRY_PER_BLOCK)
+#define	START_SEGNO(sit_i, segno)		\
+	(SIT_BLOCK_OFFSET(sit_i, segno) * SIT_ENTRY_PER_BLOCK)
+#define f2fs_bitmap_size(nr)			\
+	(BITS_TO_LONGS(nr) * sizeof(unsigned long))
+#define TOTAL_SEGS(sbi)	(SM_I(sbi)->main_segments)
+
+/* during checkpoint, bio_private is used to synchronize the last bio */
+struct bio_private {
+	struct f2fs_sb_info *sbi;
+	bool is_sync;
+	void *wait;
+};
+
+/*
+ * indicate a block allocation direction: RIGHT and LEFT.
+ * RIGHT means allocating new sections towards the end of volume.
+ * LEFT means the opposite direction.
+ */
+enum {
+	ALLOC_RIGHT = 0,
+	ALLOC_LEFT
+};
+
+/*
+ * In the victim_sel_policy->alloc_mode, there are two block allocation modes.
+ * LFS writes data sequentially with cleaning operations.
+ * SSR (Slack Space Recycle) reuses obsolete space without cleaning operations.
+ */
+enum {
+	LFS = 0,
+	SSR
+};
+
+/*
+ * In the victim_sel_policy->gc_mode, there are two gc, aka cleaning, modes.
+ * GC_CB is based on cost-benefit algorithm.
+ * GC_GREEDY is based on greedy algorithm.
+ */
+enum {
+	GC_CB = 0,
+	GC_GREEDY
+};
+
+/*
+ * BG_GC means the background cleaning job.
+ * FG_GC means the on-demand cleaning job.
+ */
+enum {
+	BG_GC = 0,
+	FG_GC
+};
+
+/* for a function parameter to select a victim segment */
+struct victim_sel_policy {
+	int alloc_mode;			/* LFS or SSR */
+	int gc_mode;			/* GC_CB or GC_GREEDY */
+	unsigned long *dirty_segmap;	/* dirty segment bitmap */
+	unsigned int offset;		/* last scanned bitmap offset */
+	unsigned int ofs_unit;		/* bitmap search unit */
+	unsigned int min_cost;		/* minimum cost */
+	unsigned int min_segno;		/* segment # having min. cost */
+};
+
+struct seg_entry {
+	unsigned short valid_blocks;	/* # of valid blocks */
+	unsigned char *cur_valid_map;	/* validity bitmap of blocks */
+	/*
+	 * # of valid blocks and the validity bitmap stored in the the last
+	 * checkpoint pack. This information is used by the SSR mode.
+	 */
+	unsigned short ckpt_valid_blocks;
+	unsigned char *ckpt_valid_map;
+	unsigned char type;		/* segment type like CURSEG_XXX_TYPE */
+	unsigned long long mtime;	/* modification time of the segment */
+};
+
+struct sec_entry {
+	unsigned int valid_blocks;	/* # of valid blocks in a section */
+};
+
+struct segment_allocation {
+	void (*allocate_segment)(struct f2fs_sb_info *, int, bool);
+};
+
+struct sit_info {
+	const struct segment_allocation *s_ops;
+
+	block_t sit_base_addr;		/* start block address of SIT area */
+	block_t sit_blocks;		/* # of blocks used by SIT area */
+	block_t written_valid_blocks;	/* # of valid blocks in main area */
+	char *sit_bitmap;		/* SIT bitmap pointer */
+	unsigned int bitmap_size;	/* SIT bitmap size */
+
+	unsigned long *dirty_sentries_bitmap;	/* bitmap for dirty sentries */
+	unsigned int dirty_sentries;		/* # of dirty sentries */
+	unsigned int sents_per_block;		/* # of SIT entries per block */
+	struct mutex sentry_lock;		/* to protect SIT cache */
+	struct seg_entry *sentries;		/* SIT segment-level cache */
+	struct sec_entry *sec_entries;		/* SIT section-level cache */
+
+	/* for cost-benefit algorithm in cleaning procedure */
+	unsigned long long elapsed_time;	/* elapsed time after mount */
+	unsigned long long mounted_time;	/* mount time */
+	unsigned long long min_mtime;		/* min. modification time */
+	unsigned long long max_mtime;		/* max. modification time */
+};
+
+struct free_segmap_info {
+	unsigned int start_segno;	/* start segment number logically */
+	unsigned int free_segments;	/* # of free segments */
+	unsigned int free_sections;	/* # of free sections */
+	rwlock_t segmap_lock;		/* free segmap lock */
+	unsigned long *free_segmap;	/* free segment bitmap */
+	unsigned long *free_secmap;	/* free section bitmap */
+};
+
+/* Notice: The order of dirty type is same with CURSEG_XXX in f2fs.h */
+enum dirty_type {
+	DIRTY_HOT_DATA,		/* dirty segments assigned as hot data logs */
+	DIRTY_WARM_DATA,	/* dirty segments assigned as warm data logs */
+	DIRTY_COLD_DATA,	/* dirty segments assigned as cold data logs */
+	DIRTY_HOT_NODE,		/* dirty segments assigned as hot node logs */
+	DIRTY_WARM_NODE,	/* dirty segments assigned as warm node logs */
+	DIRTY_COLD_NODE,	/* dirty segments assigned as cold node logs */
+	DIRTY,			/* to count # of dirty segments */
+	PRE,			/* to count # of entirely obsolete segments */
+	NR_DIRTY_TYPE
+};
+
+struct dirty_seglist_info {
+	const struct victim_selection *v_ops;	/* victim selction operation */
+	unsigned long *dirty_segmap[NR_DIRTY_TYPE];
+	struct mutex seglist_lock;		/* lock for segment bitmaps */
+	int nr_dirty[NR_DIRTY_TYPE];		/* # of dirty segments */
+	unsigned long *victim_segmap[2];	/* BG_GC, FG_GC */
+};
+
+/* victim selection function for cleaning and SSR */
+struct victim_selection {
+	int (*get_victim)(struct f2fs_sb_info *, unsigned int *,
+							int, int, char);
+};
+
+/* for active log information */
+struct curseg_info {
+	struct mutex curseg_mutex;		/* lock for consistency */
+	struct f2fs_summary_block *sum_blk;	/* cached summary block */
+	unsigned char alloc_type;		/* current allocation type */
+	unsigned int segno;			/* current segment number */
+	unsigned short next_blkoff;		/* next block offset to write */
+	unsigned int zone;			/* current zone number */
+	unsigned int next_segno;		/* preallocated segment */
+};
+
+/*
+ * inline functions
+ */
+static inline struct curseg_info *CURSEG_I(struct f2fs_sb_info *sbi, int type)
+{
+	return (struct curseg_info *)(SM_I(sbi)->curseg_array + type);
+}
+
+static inline struct seg_entry *get_seg_entry(struct f2fs_sb_info *sbi,
+						unsigned int segno)
+{
+	struct sit_info *sit_i = SIT_I(sbi);
+	return &sit_i->sentries[segno];
+}
+
+static inline struct sec_entry *get_sec_entry(struct f2fs_sb_info *sbi,
+						unsigned int segno)
+{
+	struct sit_info *sit_i = SIT_I(sbi);
+	return &sit_i->sec_entries[GET_SECNO(sbi, segno)];
+}
+
+static inline unsigned int get_valid_blocks(struct f2fs_sb_info *sbi,
+				unsigned int segno, int section)
+{
+	/*
+	 * In order to get # of valid blocks in a section instantly from many
+	 * segments, f2fs manages two counting structures separately.
+	 */
+	if (section > 1)
+		return get_sec_entry(sbi, segno)->valid_blocks;
+	else
+		return get_seg_entry(sbi, segno)->valid_blocks;
+}
+
+static inline void seg_info_from_raw_sit(struct seg_entry *se,
+					struct f2fs_sit_entry *rs)
+{
+	se->valid_blocks = GET_SIT_VBLOCKS(rs);
+	se->ckpt_valid_blocks = GET_SIT_VBLOCKS(rs);
+	memcpy(se->cur_valid_map, rs->valid_map, SIT_VBLOCK_MAP_SIZE);
+	memcpy(se->ckpt_valid_map, rs->valid_map, SIT_VBLOCK_MAP_SIZE);
+	se->type = GET_SIT_TYPE(rs);
+	se->mtime = le64_to_cpu(rs->mtime);
+}
+
+static inline void seg_info_to_raw_sit(struct seg_entry *se,
+					struct f2fs_sit_entry *rs)
+{
+	unsigned short raw_vblocks = (se->type << SIT_VBLOCKS_SHIFT) |
+					se->valid_blocks;
+	rs->vblocks = cpu_to_le16(raw_vblocks);
+	memcpy(rs->valid_map, se->cur_valid_map, SIT_VBLOCK_MAP_SIZE);
+	memcpy(se->ckpt_valid_map, rs->valid_map, SIT_VBLOCK_MAP_SIZE);
+	se->ckpt_valid_blocks = se->valid_blocks;
+	rs->mtime = cpu_to_le64(se->mtime);
+}
+
+static inline unsigned int find_next_inuse(struct free_segmap_info *free_i,
+		unsigned int max, unsigned int segno)
+{
+	unsigned int ret;
+	read_lock(&free_i->segmap_lock);
+	ret = find_next_bit(free_i->free_segmap, max, segno);
+	read_unlock(&free_i->segmap_lock);
+	return ret;
+}
+
+static inline void __set_free(struct f2fs_sb_info *sbi, unsigned int segno)
+{
+	struct free_segmap_info *free_i = FREE_I(sbi);
+	unsigned int secno = segno / sbi->segs_per_sec;
+	unsigned int start_segno = secno * sbi->segs_per_sec;
+	unsigned int next;
+
+	write_lock(&free_i->segmap_lock);
+	clear_bit(segno, free_i->free_segmap);
+	free_i->free_segments++;
+
+	next = find_next_bit(free_i->free_segmap, TOTAL_SEGS(sbi), start_segno);
+	if (next >= start_segno + sbi->segs_per_sec) {
+		clear_bit(secno, free_i->free_secmap);
+		free_i->free_sections++;
+	}
+	write_unlock(&free_i->segmap_lock);
+}
+
+static inline void __set_inuse(struct f2fs_sb_info *sbi,
+		unsigned int segno)
+{
+	struct free_segmap_info *free_i = FREE_I(sbi);
+	unsigned int secno = segno / sbi->segs_per_sec;
+	set_bit(segno, free_i->free_segmap);
+	free_i->free_segments--;
+	if (!test_and_set_bit(secno, free_i->free_secmap))
+		free_i->free_sections--;
+}
+
+static inline void __set_test_and_free(struct f2fs_sb_info *sbi,
+		unsigned int segno)
+{
+	struct free_segmap_info *free_i = FREE_I(sbi);
+	unsigned int secno = segno / sbi->segs_per_sec;
+	unsigned int start_segno = secno * sbi->segs_per_sec;
+	unsigned int next;
+
+	write_lock(&free_i->segmap_lock);
+	if (test_and_clear_bit(segno, free_i->free_segmap)) {
+		free_i->free_segments++;
+
+		next = find_next_bit(free_i->free_segmap, TOTAL_SEGS(sbi),
+								start_segno);
+		if (next >= start_segno + sbi->segs_per_sec) {
+			if (test_and_clear_bit(secno, free_i->free_secmap))
+				free_i->free_sections++;
+		}
+	}
+	write_unlock(&free_i->segmap_lock);
+}
+
+static inline void __set_test_and_inuse(struct f2fs_sb_info *sbi,
+		unsigned int segno)
+{
+	struct free_segmap_info *free_i = FREE_I(sbi);
+	unsigned int secno = segno / sbi->segs_per_sec;
+	write_lock(&free_i->segmap_lock);
+	if (!test_and_set_bit(segno, free_i->free_segmap)) {
+		free_i->free_segments--;
+		if (!test_and_set_bit(secno, free_i->free_secmap))
+			free_i->free_sections--;
+	}
+	write_unlock(&free_i->segmap_lock);
+}
+
+static inline void get_sit_bitmap(struct f2fs_sb_info *sbi,
+		void *dst_addr)
+{
+	struct sit_info *sit_i = SIT_I(sbi);
+	memcpy(dst_addr, sit_i->sit_bitmap, sit_i->bitmap_size);
+}
+
+static inline block_t written_block_count(struct f2fs_sb_info *sbi)
+{
+	struct sit_info *sit_i = SIT_I(sbi);
+	block_t vblocks;
+
+	mutex_lock(&sit_i->sentry_lock);
+	vblocks = sit_i->written_valid_blocks;
+	mutex_unlock(&sit_i->sentry_lock);
+
+	return vblocks;
+}
+
+static inline unsigned int free_segments(struct f2fs_sb_info *sbi)
+{
+	struct free_segmap_info *free_i = FREE_I(sbi);
+	unsigned int free_segs;
+
+	read_lock(&free_i->segmap_lock);
+	free_segs = free_i->free_segments;
+	read_unlock(&free_i->segmap_lock);
+
+	return free_segs;
+}
+
+static inline int reserved_segments(struct f2fs_sb_info *sbi)
+{
+	return SM_I(sbi)->reserved_segments;
+}
+
+static inline unsigned int free_sections(struct f2fs_sb_info *sbi)
+{
+	struct free_segmap_info *free_i = FREE_I(sbi);
+	unsigned int free_secs;
+
+	read_lock(&free_i->segmap_lock);
+	free_secs = free_i->free_sections;
+	read_unlock(&free_i->segmap_lock);
+
+	return free_secs;
+}
+
+static inline unsigned int prefree_segments(struct f2fs_sb_info *sbi)
+{
+	return DIRTY_I(sbi)->nr_dirty[PRE];
+}
+
+static inline unsigned int dirty_segments(struct f2fs_sb_info *sbi)
+{
+	return DIRTY_I(sbi)->nr_dirty[DIRTY_HOT_DATA] +
+		DIRTY_I(sbi)->nr_dirty[DIRTY_WARM_DATA] +
+		DIRTY_I(sbi)->nr_dirty[DIRTY_COLD_DATA] +
+		DIRTY_I(sbi)->nr_dirty[DIRTY_HOT_NODE] +
+		DIRTY_I(sbi)->nr_dirty[DIRTY_WARM_NODE] +
+		DIRTY_I(sbi)->nr_dirty[DIRTY_COLD_NODE];
+}
+
+static inline int overprovision_segments(struct f2fs_sb_info *sbi)
+{
+	return SM_I(sbi)->ovp_segments;
+}
+
+static inline int overprovision_sections(struct f2fs_sb_info *sbi)
+{
+	return ((unsigned int) overprovision_segments(sbi)) / sbi->segs_per_sec;
+}
+
+static inline int reserved_sections(struct f2fs_sb_info *sbi)
+{
+	return ((unsigned int) reserved_segments(sbi)) / sbi->segs_per_sec;
+}
+
+static inline bool need_SSR(struct f2fs_sb_info *sbi)
+{
+	return (free_sections(sbi) < overprovision_sections(sbi));
+}
+
+static inline int get_ssr_segment(struct f2fs_sb_info *sbi, int type)
+{
+	struct curseg_info *curseg = CURSEG_I(sbi, type);
+	return DIRTY_I(sbi)->v_ops->get_victim(sbi,
+				&(curseg)->next_segno, BG_GC, type, SSR);
+}
+
+static inline bool has_not_enough_free_secs(struct f2fs_sb_info *sbi)
+{
+	return free_sections(sbi) <= reserved_sections(sbi);
+}
+
+static inline int utilization(struct f2fs_sb_info *sbi)
+{
+	return (long int)valid_user_blocks(sbi) * 100 /
+			(long int)sbi->user_block_count;
+}
+
+/*
+ * Sometimes f2fs may be better to drop out-of-place update policy.
+ * So, if fs utilization is over MIN_IPU_UTIL, then f2fs tries to write
+ * data in the original place likewise other traditional file systems.
+ * But, currently set 100 in percentage, which means it is disabled.
+ * See below need_inplace_update().
+ */
+#define MIN_IPU_UTIL		100
+static inline bool need_inplace_update(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
+	if (S_ISDIR(inode->i_mode))
+		return false;
+	if (need_SSR(sbi) && utilization(sbi) > MIN_IPU_UTIL)
+		return true;
+	return false;
+}
+
+static inline unsigned int curseg_segno(struct f2fs_sb_info *sbi,
+		int type)
+{
+	struct curseg_info *curseg = CURSEG_I(sbi, type);
+	return curseg->segno;
+}
+
+static inline unsigned char curseg_alloc_type(struct f2fs_sb_info *sbi,
+		int type)
+{
+	struct curseg_info *curseg = CURSEG_I(sbi, type);
+	return curseg->alloc_type;
+}
+
+static inline unsigned short curseg_blkoff(struct f2fs_sb_info *sbi, int type)
+{
+	struct curseg_info *curseg = CURSEG_I(sbi, type);
+	return curseg->next_blkoff;
+}
+
+static inline void check_seg_range(struct f2fs_sb_info *sbi, unsigned int segno)
+{
+	unsigned int end_segno = SM_I(sbi)->segment_count - 1;
+	BUG_ON(segno > end_segno);
+}
+
+/*
+ * This function is used for only debugging.
+ * NOTE: In future, we have to remove this function.
+ */
+static inline void verify_block_addr(struct f2fs_sb_info *sbi, block_t blk_addr)
+{
+	struct f2fs_sm_info *sm_info = SM_I(sbi);
+	block_t total_blks = sm_info->segment_count << sbi->log_blocks_per_seg;
+	block_t start_addr = sm_info->seg0_blkaddr;
+	block_t end_addr = start_addr + total_blks - 1;
+	BUG_ON(blk_addr < start_addr);
+	BUG_ON(blk_addr > end_addr);
+}
+
+/*
+ * Summary block is always treated as invalid block
+ */
+static inline void check_block_count(struct f2fs_sb_info *sbi,
+		int segno, struct f2fs_sit_entry *raw_sit)
+{
+	struct f2fs_sm_info *sm_info = SM_I(sbi);
+	unsigned int end_segno = sm_info->segment_count - 1;
+	int valid_blocks = 0;
+	int i;
+
+	/* check segment usage */
+	BUG_ON(GET_SIT_VBLOCKS(raw_sit) > sbi->blocks_per_seg);
+
+	/* check boundary of a given segment number */
+	BUG_ON(segno > end_segno);
+
+	/* check bitmap with valid block count */
+	for (i = 0; i < sbi->blocks_per_seg; i++)
+		if (f2fs_test_bit(i, raw_sit->valid_map))
+			valid_blocks++;
+	BUG_ON(GET_SIT_VBLOCKS(raw_sit) != valid_blocks);
+}
+
+static inline pgoff_t current_sit_addr(struct f2fs_sb_info *sbi,
+						unsigned int start)
+{
+	struct sit_info *sit_i = SIT_I(sbi);
+	unsigned int offset = SIT_BLOCK_OFFSET(sit_i, start);
+	block_t blk_addr = sit_i->sit_base_addr + offset;
+
+	check_seg_range(sbi, start);
+
+	/* calculate sit block address */
+	if (f2fs_test_bit(offset, sit_i->sit_bitmap))
+		blk_addr += sit_i->sit_blocks;
+
+	return blk_addr;
+}
+
+static inline pgoff_t next_sit_addr(struct f2fs_sb_info *sbi,
+						pgoff_t block_addr)
+{
+	struct sit_info *sit_i = SIT_I(sbi);
+	block_addr -= sit_i->sit_base_addr;
+	if (block_addr < sit_i->sit_blocks)
+		block_addr += sit_i->sit_blocks;
+	else
+		block_addr -= sit_i->sit_blocks;
+
+	return block_addr + sit_i->sit_base_addr;
+}
+
+static inline void set_to_next_sit(struct sit_info *sit_i, unsigned int start)
+{
+	unsigned int block_off = SIT_BLOCK_OFFSET(sit_i, start);
+
+	if (f2fs_test_bit(block_off, sit_i->sit_bitmap))
+		f2fs_clear_bit(block_off, sit_i->sit_bitmap);
+	else
+		f2fs_set_bit(block_off, sit_i->sit_bitmap);
+}
+
+static inline unsigned long long get_mtime(struct f2fs_sb_info *sbi)
+{
+	struct sit_info *sit_i = SIT_I(sbi);
+	return sit_i->elapsed_time + CURRENT_TIME_SEC.tv_sec -
+						sit_i->mounted_time;
+}
+
+static inline void set_summary(struct f2fs_summary *sum, nid_t nid,
+			unsigned int ofs_in_node, unsigned char version)
+{
+	sum->nid = cpu_to_le32(nid);
+	sum->ofs_in_node = cpu_to_le16(ofs_in_node);
+	sum->version = version;
+}
+
+static inline block_t start_sum_block(struct f2fs_sb_info *sbi)
+{
+	return __start_cp_addr(sbi) +
+		le32_to_cpu(F2FS_CKPT(sbi)->cp_pack_start_sum);
+}
+
+static inline block_t sum_blk_addr(struct f2fs_sb_info *sbi, int base, int type)
+{
+	return __start_cp_addr(sbi) +
+		le32_to_cpu(F2FS_CKPT(sbi)->cp_pack_total_block_count)
+				- (base + 1) + type;
+}
